- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: AI for Autonomous Vehicles – Build a Self-Driving Car
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自主车辆的人工智能——构建一辆自动驾驶汽车
- en: I'm really pumped up for you to start this new chapter. It's probably the most
    challenging, and most fun, adventure we'll have in this book. You're literally
    about to build a self-driving car from scratch, on a 2D map, using the powerful
    deep Q-learning model. I think that's incredibly exciting!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我真是非常激动，期待你开始这个新章节。这可能是本书中最具挑战性，同时也是最有趣的冒险。你将从零开始，使用强大的深度 Q 学习模型，构建一辆 2D 地图上的自动驾驶汽车。我觉得这真是令人兴奋！
- en: Think fast; what's our first step?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 快点想想，第一步是什么？
- en: If you answered "building the environment," you're absolutely right. I hope
    that's getting so familiar to you that you answered before I even finished the
    question. Let's start by building an environment in which a car can learn how
    to drive by itself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回答了“构建环境”，你完全正确。我希望这个答案已经让你感到如此熟悉，以至于我还没问完你就回答出来了。让我们从构建一个汽车可以自己学习如何驾驶的环境开始。
- en: Building the environment
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建环境
- en: This time, we have much more to define than just the states, actions, and rewards.
    Building a self-driving car is a seriously complex problem. Now, I'm not going
    to ask you to go to your garage and turn yourself into a hybrid AI mechanic; you're
    simply going to build a virtual self-driving car that moves around a 2D map.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们需要定义的内容远不止状态、动作和奖励。构建一辆自动驾驶汽车是一个非常复杂的问题。现在，我不会要求你去车库，把自己变成一位混合型 AI 机械师；你只需在一个
    2D 地图上构建一个虚拟的自动驾驶汽车，让它四处移动。
- en: 'You''ll build this 2D map inside a Kivy web app. Kivy is a free and open source
    Python framework, used for the development of applications like games, or really
    any kind of mobile app. Check out the website here: [https://kivy.org/#home](https://kivy.org/#home).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在 Kivy 网络应用中构建这个 2D 地图。Kivy 是一个免费的开源 Python 框架，用于开发类似游戏的应用，或者任何种类的移动应用。你可以在这里查看网站：[https://kivy.org/#home](https://kivy.org/#home)。
- en: The whole environment for this project is built with Kivy, from start to finish.
    The development of the map and the virtual car has nothing to do with AI, so we
    won't go line by line through the code that implements it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的整个环境都是用 Kivy 构建的，从头到尾。地图和虚拟汽车的开发与 AI 无关，因此我们不会逐行讲解实现它的代码。
- en: However, I am going to describe the features of the map. For those of you curious
    to know about exactly how the map is built, I've provided a fully commented Python
    file in the GitHub named `map_commented.py` that builds the environment from scratch
    with a full explanation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我将描述一下地图的功能。对于那些好奇地图是如何构建的朋友，我已经在 GitHub 上提供了一个完全注释过的 Python 文件，名为 `map_commented.py`，它从头开始构建这个环境，并提供了详细的解释。
- en: 'Before we look at all the features, let''s have a look at this map with the
    little virtual car inside:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看所有功能之前，让我们先看一下这个地图，上面有一辆小虚拟车：
- en: '![](img/B14110_10_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_01.png)'
- en: 'Figure 1: The map'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：地图
- en: The first thing you'll notice is a black screen, which is the Kivy user interface.
    You build your games or apps inside this interface. As you might guess, it's actually
    the container of the whole environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到的是一个黑色的屏幕，那就是 Kivy 用户界面。你在这个界面内构建你的游戏或应用。正如你可能猜到的，它实际上是整个环境的容器。
- en: You can see something weird inside, a white rectangle with three colored dots
    in front of it. Well, that's the car! My apologies for not being a better artist,
    but it's important to keep things simple. The white little rectangle is the shape
    of the car, and the three little dots are the sensors of the car. Why do we need
    sensors? Because on this map, we will have the option to build roads, delimited
    by sand, which the car will have to avoid going through.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到里面有些奇怪的东西，一个白色的矩形，前面有三个彩色的圆点。嗯，那就是汽车！抱歉，我不是个更好的艺术家，但保持简单是很重要的。那个白色的小矩形就是汽车的形状，而那三个小圆点是汽车的传感器。为什么我们需要传感器呢？因为在这个地图上，我们将有选项构建被沙地隔开的道路，而汽车必须避免通过这些沙地。
- en: To put some sand on the map, simply keep pressing left with your mouse and draw
    whatever you want. It doesn't have to just be roads; you can add some obstacles
    as well. In any case, the car will have to avoid going through the sand.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要在地图上放一些沙子，只需持续按住鼠标左键并绘制你想要的任何内容。它不一定只是道路；你还可以添加一些障碍物。无论如何，汽车必须避免通过沙地。
- en: 'If you remember that everything works from the rewards, I''m sure you already
    know how to make that happen; it''s by penalizing the self-driving car with a
    bad reward when it goes onto the sand. We''ll take care of that later. In the
    meantime, let''s have a look at one of my nice drawings of roads with sand:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得一切都来自于奖励，我相信你已经知道如何实现这一点；那就是当自动驾驶汽车驶上沙地时，通过给予它一个不好的奖励来进行惩罚。我们稍后会处理这个问题。与此同时，让我们看一下我绘制的带有沙子的道路示意图：
- en: '![](img/B14110_10_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_02.png)'
- en: 'Figure 2: Map with a drawn road'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：带有绘制道路的地图
- en: The sensors are there to detect the sand, so the car can avoid it. The blue
    sensor covers an area at the left of the car, the red sensor covers an area at
    the front of the car, and the yellow sensor covers an area at the right of the
    car.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器用来检测沙子，以便汽车能够避免。蓝色传感器覆盖汽车左侧的区域，红色传感器覆盖汽车前方的区域，黄色传感器覆盖汽车右侧的区域。
- en: 'Finally, there are three buttons to click on at the bottom left corner of the
    screen, which are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在屏幕的左下角有三个按钮可以点击，它们是：
- en: '**clear**: Removes all the sand drawn on the map'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**clear**: 移除地图上绘制的所有沙子'
- en: '**save**: Saves the weights (parameters) of the AI'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**save**: 保存AI的权重（参数）'
- en: '**load**: Loads the last saved weights'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**load**: 加载最后保存的权重'
- en: Now we've had a look at our little map, let's move on to defining our goals.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了这张小地图，接下来让我们定义一下我们的目标。
- en: Defining the goal
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义目标
- en: We understand that our goal is to build a self-driving car. Good. But how are
    we going to formalize that goal, in terms of AI and reinforcement learning? Your
    intuition should hopefully make you think about the rewards we're going to set.
    I agree—we're going to give a high reward to our car if it manages to self-drive.
    But how can we tell that it's managing to self-drive?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解我们的目标是构建一辆自动驾驶汽车。很好。那么我们如何在AI和强化学习的框架下对这个目标进行形式化定义呢？希望你的直觉已经让你想到了我们将设定的奖励。我同意——如果我们的汽车成功实现自动驾驶，我们会给予它一个高奖励。但是，我们怎么判断它已经成功实现了自动驾驶呢？
- en: 'We''ve got plenty of ways to evaluate this. For example, we could simply draw
    some obstacles on the map, and train our self-driving car to move around the map
    without hitting the obstacles. That''s a simple challenge, but we could try something
    a little more fun. Remember the road I drew earlier? How about we train our car
    to go from the upper left corner of the map, to the bottom right corner, through
    any road we build between these two spots? That''s a real challenge, and that''s
    what we''ll do. Let''s imagine that the map is a city, where the upper left corner
    is the Airport, and the bottom right corner is Downtown:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有很多方法可以评估这个问题。例如，我们可以在地图上画一些障碍物，并训练我们的自动驾驶汽车在不碰到障碍物的情况下绕过它们。这是一个简单的挑战，但我们可以尝试一些更有趣的挑战。还记得我之前画的那条路吗？那我们来训练汽车从地图的左上角到右下角，沿着我们在这两个点之间建立的任何道路行驶。这才是一个真正的挑战，这就是我们要做的。假设这张地图是一个城市，左上角是机场，右下角是市中心：
- en: '![](img/B14110_10_03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_03.png)'
- en: 'Figure 3: The two destinations – Airport and Downtown'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：两个目的地——机场和市中心
- en: 'Now we can clearly formulate a goal; to train the self-driving car to make
    round trips between the Airport and Downtown. As soon as it reaches the Airport,
    it will then have to go to Downtown, and as soon as it reaches Downtown, it will
    then have to go to the Airport. More than that, it should be able to make these
    round trips along any road connecting these two locations. It should also be able
    to cope with any obstacles along that road it has to avoid. Here is an example
    of another, more challenging road:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以清楚地制定一个目标；训练自动驾驶汽车在机场和市中心之间往返。当它到达机场时，就必须前往市中心；而当它到达市中心时，又必须返回机场。更重要的是，它应该能够沿着任何连接这两个地点的道路完成这些往返。它还应该能够应对道路上任何需要避免的障碍。这里是另一条更具挑战性的道路示例：
- en: '![](img/B14110_10_04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_04.png)'
- en: 'Figure 4: A more challenging road'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：更具挑战性的道路
- en: 'If you think that road look too easy, here''s a more challenging example; this
    time with not only a more difficult road but also many obstacles:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这条路看起来太简单，这里有一个更具挑战性的例子；这次不仅是更难的道路，而且还有许多障碍：
- en: '![](img/B14110_10_05.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_05.png)'
- en: 'Figure 5: An even more challenging road'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：一条更具挑战性的道路
- en: 'As a final example, I want to share this last map, designed by one of my students,
    which could belong in the movie *Inception*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个例子，我想分享这张由我的一位学生设计的地图，它可能出现在电影*盗梦空间*中：
- en: '![](img/B14110_10_06.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_06.png)'
- en: 'Figure 6: The most challenging road ever'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：有史以来最具挑战性的道路
- en: If you look closely, it's still a path that goes from Airport to Downtown and
    vice versa, just much more challenging. The AI we create will be able to cope
    with any of these maps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看，它仍然是从机场到市中心来回的道路，只不过现在变得更具挑战性了。我们创建的 AI 将能够应对这些地图中的任何一种。
- en: I hope you find that as exciting as I do! Keep that level of energy up, because
    we have quite a lot of work to do.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能和我一样感到兴奋！保持这种能量，因为我们还有很多工作要做。
- en: Setting the parameters
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置参数
- en: 'Before you define the input states, the output actions, and the rewards, you
    must set all of the parameters of the map and the car that will be part of your
    environment. The inputs, outputs, and rewards are all functions of these parameters.
    Let''s list them all, using the same names as in the code, so that you can easily
    understand the file `map.py`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在你定义输入状态、输出动作和奖励之前，必须先设置所有关于地图和汽车的参数，这些参数将成为你环境的一部分。输入、输出和奖励都是这些参数的函数。让我们列出它们，使用和代码中相同的名称，这样你就能轻松理解文件
    `map.py`：
- en: '**angle**: The angle between the *x*-axis of the map and the axis of the car'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**angle**：地图的 *x*-轴与汽车轴之间的角度'
- en: '**rotation**: The last rotation made by the car (we will see later that when
    playing an action, the car makes a rotation)'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**rotation**：汽车做的最后一次旋转（我们稍后会看到，执行一个动作时，汽车会进行旋转）'
- en: '**pos = (self.car.x, self.car.y)**: The position of the car (`self.car.x` is
    the *x*-coordinate of the car, `self.car.y` is the *y*-coordinate of the car)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**pos = (self.car.x, self.car.y)**：汽车的位置（`self.car.x` 是汽车的 *x*-坐标，`self.car.y`
    是汽车的 *y*-坐标）'
- en: '**velocity = (velocity_x, velocity_y)**: The velocity vector of the car'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**velocity = (velocity_x, velocity_y)**：汽车的速度向量'
- en: '**sensor1 = (sensor1_x, sensor1_y)**: The position of the first sensor'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**sensor1 = (sensor1_x, sensor1_y)**：第一个传感器的位置'
- en: '**sensor2 = (sensor2_x, sensor2_y)**: The position of the second sensor'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**sensor2 = (sensor2_x, sensor2_y)**：第二个传感器的位置'
- en: '**sensor3 = (sensor3_x, sensor3_y)**: The position of the third sensor'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**sensor3 = (sensor3_x, sensor3_y)**：第三个传感器的位置'
- en: '**signal1**: The signal received by sensor 1'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**signal1**：传感器 1 接收到的信号'
- en: '**signal2**: The signal received by sensor 2'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**signal2**：传感器 2 接收到的信号'
- en: '**signal3**: The signal received by sensor 3'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**signal3**：传感器 3 接收到的信号'
- en: Now let's slow down; we've got to define how these signals are computed. The
    signals are a measure of the density of sand around their sensor. How are you
    going to compute that density? You start by introducing a new variable, called
    `sand`, which you initialize as an array that has as many cells as our graphic
    interface has pixels. Simply put, the `sand` array is the black map itself and
    the pixels are the cells of the array. Then, each cell of the `sand` array will
    get a 1 if there is sand, and a 0 if there is not.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们放慢一下进度；我们得定义这些信号是如何计算的。这些信号是传感器周围沙子密度的度量。你打算如何计算这种密度呢？你首先引入一个新变量，叫做 `sand`，它初始化为一个数组，数组的单元格数量与我们的图形界面的像素数量相同。简单来说，`sand`
    数组就是黑色地图本身，像素则是数组的单元格。然后，如果某个位置有沙子，`sand` 数组中的相应单元格会被赋值为 1，如果没有沙子，则赋值为 0。
- en: 'For example, here the `sand` array has only 1s in its first few rows, and the
    rest is all 0s:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里 `sand` 数组的前几行只有 1，其余部分全是 0：
- en: '![](img/B14110_10_07.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_07.png)'
- en: 'Figure 7: The map with only sand in the first rows'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：只有前几行有沙子的地图
- en: I know the border is a little wobbly—like I said, I'm no great artist—and that
    just means those rows of the `sand` array would have 1s where the sand is and
    0s where there's no sand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道边界有点晃动——就像我说的，我不是伟大的艺术家——这只是意味着 `sand` 数组的那些行会在有沙子的地方显示 1，而没有沙子的地方显示 0。
- en: Now that you have this `sand` array it's very easy to compute the density of
    sand around each sensor. You surround your sensor by a square of 20 by 20 cells
    (which the sensor reads from the `sand` array), then you count the number of ones
    in these cells, and finally you divide that number by the total number of cells
    in that square, that is, 20 x 20 = 400 cells.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了这个 `sand` 数组，计算每个传感器周围的沙子密度变得非常简单。你将传感器周围放置一个 20x20 单元格的正方形（传感器从 `sand`
    数组中读取这些单元格），然后计算这些单元格中 1 的数量，最后将这个数字除以该正方形中的总单元格数，即 20 x 20 = 400 个单元格。
- en: 'Since the `sand` array only contains 1s (where there''s sand) and 0s (where
    there''s no sand), we can very easily count the number of 1s by simply summing
    the cells of the `sand` array in this 20 by 20 square. That gives us exactly the
    density of sand around each sensor, and that''s what''s computed at lines 81,
    82, and 83 in the `map.py` file:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`sand`数组只包含1（表示有沙子）和0（表示没有沙子），我们可以通过简单地对这个20x20平方中的`sand`数组单元格求和，轻松地计算出1的数量。这就给出了每个传感器周围的沙子密度，而这正是在`map.py`文件中的第81、82和83行计算的内容：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we''ve covered how the signals are computed, let''s continue with
    the rest of the parameters. The last parameters, which I''ve highlighted in the
    list below, are important because they''re the last pieces that we need to reveal
    the final input state vector. Here they are:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了信号是如何计算的，让我们继续讨论其余的参数。下面我标出的一些最后的参数很重要，因为它们是我们需要的最后几块拼图，以揭示最终的输入状态向量。它们是：
- en: '**goal_x**: The *x*-coordinate of the goal (which can either be the Airport
    or Downtown)'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**goal_x**：目标的*x*坐标（可以是机场或市中心）'
- en: '**goal_y**: The *y*-coordinate of the goal (which can either be the Airport
    or Downtown)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**goal_y**：目标的*y*坐标（可以是机场或市中心）'
- en: '**xx = (goal_x - self.car.x)**: The difference of *x*-coordinates between the
    goal and the car'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**xx = (goal_x - self.car.x)**：目标和汽车之间的*x*坐标差'
- en: '**yy = (goal_y - self.car.y)**: The difference of *y*-coordinates between the
    goal and the car'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**yy = (goal_y - self.car.y)**：目标和汽车之间的*y*坐标差'
- en: '**orientation**: The angle that measures the direction of the car with respect
    to the goal'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**orientation**：测量汽车相对于目标方向的角度'
- en: 'Let''s slow down again for a moment. We need to know how orientation is computed;
    it''s the angle between the axis of the car (the `velocity` vector from our first
    list of parameters) and the axis that joins the goal and the center of the car.
    The goal has the coordinates (`goal_x`, `goal_y`) and the center of the car has
    the coordinates (`self.car.x`, `self.car.y`). For example, if the car is heading
    perfectly toward the goal, then orientation = 0°. If you''re curious as to how
    we can compute the angle between the two axes in Python, here''s the code that
    gets the `orientation` (lines 126, 127, and 128 in the `map.py` file):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再慢下来一下。我们需要知道如何计算方向；它是汽车的轴（来自我们参数列表中的`velocity`向量）与连接目标和汽车中心的轴之间的角度。目标的坐标是（`goal_x`，`goal_y`），汽车中心的坐标是（`self.car.x`，`self.car.y`）。例如，如果汽车正朝目标完全前进，那么方向
    = 0°。如果你对如何在Python中计算这两个轴之间的角度感兴趣，这里是获取`orientation`的代码（`map.py`文件中的第126、127和128行）：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Good news—we're finally ready to define the main pillars of the environment.
    I'm talking, of course, about the input states, the actions, and the rewards.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息——我们终于准备好定义环境的主要支柱了。我说的当然是输入状态、动作和奖励。
- en: 'Before I define them, try to guess what they''re going to be. Check out all
    the preceding parameters again, and remember the goal: making round trips between
    two locations, the Airport and Downtown, while avoiding any obstacles along the
    road. The solution''s in the next section.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我定义它们之前，试着猜猜它们是什么。再检查一下所有前面的参数，并记住目标：在两个地点之间来回旅行，即机场和市中心，同时避免道路上的任何障碍。解决方案在下一节。
- en: The input states
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入状态
- en: 'What do you think the input states are? You might have answered "the position
    of the car." In that case, the input state would be a vector of two elements,
    the coordinates of the car: `self.car.x` and `self.car.y`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为输入状态是什么？你可能回答了“汽车的位置”。在这种情况下，输入状态将是一个包含两个元素的向量，汽车的坐标：`self.car.x`和`self.car.y`。
- en: That's a good start. From the intuition and foundation techniques of deep Q-learning
    you learned in *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning*,
    you know that when you're doing deep Q-learning, the input state doesn't have
    to be a single element as in Q-learning. In fact, in deep Q-learning the input
    state can be a vector of many elements, allowing you to supply many sources of
    information to your AI to help it predict smart actions to play.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的开始。从你在*第9章*《成为人工智能专家 - 深度Q学习》中学到的深度Q学习的直觉和基础技术来看，你知道在做深度Q学习时，输入状态不一定像Q学习那样是单一的元素。实际上，在深度Q学习中，输入状态可以是多个元素的向量，从而允许你为AI提供多个信息来源，帮助它预测智能的动作来执行。
- en: 'The input state can even be bigger than a simple vector: it can be an image!
    In that case, the AI model is called **deep convolutional Q-learning**. It''s
    the same as deep Q-learning, except that you add a convolutional neural network
    at the entrance of the neural network that allows your AI (machine) to visualize
    images. We''ll cover this technique in *Chapter 12*, *Deep Convolution Q-Learning*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入状态甚至可以比一个简单的向量更大：它可以是一个图像！在这种情况下，AI模型被称为**深度卷积Q学习**。它和深度Q学习相同，只不过在神经网络的入口处加入了卷积神经网络，让你的AI（机器）能够处理图像。我们将在*第12章*，*深度卷积Q学习*中介绍这项技术。
- en: We can do better than just supplying the car position coordinates. They tell
    us where the self-driving car is located, but there's another parameter that's
    better, simpler, and more directly related to the goal. I'm talking about the
    `orientation` variable. The orientation is a single input that directly tells
    us if we are pointed in the right direction, toward the goal. If we have that
    orientation, we don't need the car position coordinates at all to navigate toward
    the goal; we can just change the orientation by a certain angle to point the car
    more in the direction of the goal. The actions that the AI performs will be what
    changes that orientation. We'll discuss those in the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做得比仅仅提供汽车位置坐标更好。位置坐标告诉我们自动驾驶汽车的位置，但有一个更好的参数，它更简单，并且与目标更直接相关。我说的是`orientation`变量。方向是一个单一的输入，它直接告诉我们是否朝着正确的方向前进，朝着目标。如果我们有了这个方向，我们就不再需要汽车的位置坐标来导航到目标；我们只需要改变方向一定角度，就可以让汽车更朝着目标的方向行驶。AI执行的操作将是改变这个方向的动作。我们将在下一节讨论这些操作。
- en: 'We have the first element of our input state: the orientation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了输入状态的第一个元素：方向。
- en: But that's not enough. Remember that we also have another goal, or, should I
    say, constraint. Our car needs to stay on the road and avoid any obstacles along
    that road.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不够。记住，我们还有另一个目标，或者说是约束。我们的车需要保持在道路上，并避开道路上的任何障碍物。
- en: In the input state, we need information telling the AI whether it is about to
    move off the road or hit an obstacle. Try and work it out for yourself—do we have
    a way to get this information?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入状态中，我们需要一些信息来告诉AI它是否即将驶出道路或撞到障碍物。试着自己推理一下——我们有办法获取这些信息吗？
- en: 'The solution is the sensors. Remember that our car has three sensors giving
    us signals about how much sand is around them. The blue sensor tells us if there''s
    any sand at the left of the car, the red sensor tells us if there is any sand
    in front of the car, and the yellow sensor tells us if there is any sand at the
    right of the car. The signals of these sensors are already coded into three variables:
    `signal1`, `signal2`, and `signal3`. These signals will tell the AI if it''s about
    to hit some obstacle or about to get out of the road, since the road is delimited
    by sand.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案就是传感器。记住，我们的车有三个传感器，给我们提供关于周围沙子多少的信号。蓝色传感器告诉我们车左边是否有沙子，红色传感器告诉我们车前方是否有沙子，黄色传感器告诉我们车右边是否有沙子。这些传感器的信号已经被编码成三个变量：`signal1`、`signal2`和`signal3`。这些信号将告诉AI它是否即将撞到障碍物或驶出道路，因为道路是由沙子界定的。
- en: That's the rest of the information you need for your input state. With these
    four elements, `signal1`, `signal2`, `signal3`, and `orientation`, you have everything
    you need to be able to drive from one location to another, while staying on the
    road, and without hitting any obstacles.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你所需的输入状态的其余信息。通过这四个元素，`signal1`、`signal2`、`signal3`和`orientation`，你拥有了足够的信息，能够从一个位置驾驶到另一个位置，同时保持在道路上，并避免撞到任何障碍物。
- en: 'In conclusion, here''s what the input state is going to be at each time:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，下面是每次的输入状态：
- en: Input state = (`orientation`, `signal1`, `signal2`, `signal3`)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入状态 = (`orientation`, `signal1`, `signal2`, `signal3`)
- en: 'And that''s exactly what''s coded at line 129 in the `map.py` file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 而这正是`map.py`文件中第129行的编码内容：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`state` is the variable name given to the input state.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`state`是给输入状态指定的变量名。'
- en: Don't worry too much about the code syntax difference between `signal`, `self.signal`,
    and `self.car.signal`; they're all the same. The reason we use these different
    variables is because the AI is coded with classes (as in **Object Oriented Programming**
    (**OOP**)), which allows us to create several self-driving cars on the same map.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不必太担心`signal`、`self.signal`和`self.car.signal`之间的代码语法差异；它们是一样的。我们使用这些不同的变量是因为AI是用类（如**面向对象编程**（**OOP**））编写的，这样我们就可以在同一张地图上创建多个自动驾驶汽车。
- en: If you do want to have several self-driving cars on your map, for example, if
    you want them racing, then you can distinguish the cars better thanks to `self.car.signal`.
    For example, if you have two cars, you can name the two objects `car1` and `car2`
    so that you can distinguish the first sensor signals of the two cars, by using
    `self.car1.signal1` and `self.car2.signal1`. In this chapter, we just have one
    car, so whether we use `signal1`, `car.signal1` or `self.car.signal1`, we get
    the same thing.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在地图上拥有多辆自动驾驶汽车，例如，如果你想让它们进行比赛，那么你可以通过`self.car.signal`更好地区分这些车。例如，如果你有两辆车，你可以将这两个对象命名为`car1`和`car2`，然后通过使用`self.car1.signal1`和`self.car2.signal1`来区分这两辆车的第一个传感器信号。在本章中，我们只有一辆车，因此无论是使用`signal1`、`car.signal1`还是`self.car.signal1`，结果都是一样的。
- en: We've covered the input state; now let's tackle the actions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过输入状态；现在让我们来处理动作。
- en: The output actions
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出动作
- en: 'I''ve already briefly mentioned or suggested what the actions are going to
    be. Given our input state, it''s easy to guess. Naturally, since you''re building
    a self-driving car, you might think that the actions should be: move forward,
    turn left, or turn right. You''d be absolutely right! That''s exactly what the
    actions are going to be.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经简要提到或暗示了这些动作会是什么。根据我们的输入状态，猜测这些动作是很容易的。自然地，因为你正在构建一辆自动驾驶汽车，你可能会认为动作应该是：前进、左转或右转。你完全正确！这正是这些动作会是的内容。
- en: Not only is this intuitive, but it aligns extremely well with our choice of
    input states. They contain the `orientation` variable that tells us if we're aimed
    in the right direction toward the goal. Simply put, if the `orientation` input
    tells us our car is pointed in the right direction, we perform the action of moving
    forward. If the `orientation` input tells us that the goal is on the right of
    our car, we perform the action of turning right. Finally, if the `orientation`
    tells us that the goal is on the left of our car, we perform the action of turning
    left.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅直观，而且与我们选择的输入状态极其吻合。输入状态包含了`orientation`变量，它告诉我们汽车是否朝着正确的方向驶向目标。简单来说，如果`orientation`输入告诉我们我们的车指向正确的方向，我们就执行前进的动作。如果`orientation`输入告诉我们目标在车的右侧，我们就执行右转的动作。最后，如果`orientation`告诉我们目标在车的左侧，我们就执行左转的动作。
- en: 'At the same time, if any of the signals spot some sand around the car, the
    car will turn left or right to avoid it. The three possible actions of move forward,
    turn left, and turn right make logical sense with the goal, constraint, and input
    states we have, and we can define them as the three following rotations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果任何信号检测到车周围有沙子，汽车会向左或向右转弯以避开它。前进、左转和右转这三种可能的动作与我们所设定的目标、约束和输入状态是逻辑一致的，我们可以将它们定义为以下三个旋转：
- en: '*rotations* = [turn 0° (that is, move forward), turn 20° to the left, turn
    20° to the right]'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*旋转* = [转动0°（即前进）、向左转20°、向右转20°]'
- en: The choice of 20° is quite arbitrary. You could very well choose 10°, 30°, or
    40°. I'd avoid more than 40°, because then your car would have twitchy, fidgety
    movements, and wouldn't look like a smoothly moving car.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择20°是相当任意的。你完全可以选择10°、30°或40°。我建议避免选择超过40°，因为那样你的车子会有不稳定、抖动的动作，看起来就不像是一辆平稳行驶的车了。
- en: However, the actions the ANN outputs will not be 0°, 20°, and -20°; they will
    be 0, 1 and 2.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人工神经网络输出的动作不会是0°、20°和-20°，它们将是0、1和2。
- en: '*actions* = [0, 1, 2]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*动作* = [0, 1, 2]'
- en: It's always better to use simple categories like those when you're dealing with
    the output of an artificial neural network. Since 0, 1, and 2 will be the actions
    the AI returns, how do you think we end up with the rotations?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理人工神经网络的输出时，使用像这样的简单类别总是更好的。由于0、1和2将是AI返回的动作，那么你认为我们是如何得到旋转角度的呢？
- en: 'You''ll use a simple mapping, called `action2rotation` in our code, which maps
    the actions 0, 1, 2 to the respective rotations of 0°, 20°, -20°. This is exactly
    what''s coded on lines 34 and 131 of the `map.py` file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用一个简单的映射，代码中叫做`action2rotation`，它将动作0、1、2映射到相应的旋转角度0°、20°、-20°。这正是`map.py`文件中第34行和第131行的代码：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let's move on to the rewards. This one's going to be fun, because this
    is where you decide how you want to reward or punish your car. Try to figure out
    how by yourself first, and then take a look at the solution in the following section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讨论奖励。这个部分会很有趣，因为在这里你决定如何奖励或惩罚你的汽车。先尝试自己思考一下，然后再看看下一节中的解决方案。
- en: The rewards
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励
- en: 'To define the system of rewards, we have to answer the following questions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义奖励系统，我们必须回答以下问题：
- en: In which cases do we give the AI a good reward? How good for each case?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在哪些情况下我们会给AI一个好奖励？每种情况下的奖励有多好？
- en: In which cases do we give the AI a bad reward? How bad for each case?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在哪些情况下我们会给AI一个坏奖励？每种情况下的奖励有多坏？
- en: 'To answer these questions, we must simply remember what the goal and constraints are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我们只需要记住目标和约束是什么：
- en: The goal is to make round trips between the Airport and Downtown.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是进行机场和市区之间的往返。
- en: The constraints are to stay on the road and avoid obstacles if any. In other
    words, the constraint is to stay away from the sand.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约束是保持在道路上，并避免任何障碍物。换句话说，约束是远离沙地。
- en: 'Hence, based on this goal and constraints, the answers to our preceding questions are:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于这个目标和约束，我们之前问题的答案是：
- en: We give the AI a good reward when it gets closer to the destination.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当AI向目标靠近时，我们会给予它一个好奖励。
- en: We give the AI a bad reward when it gets further away from the destination.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当AI离目标越来越远时，我们会给予它一个坏奖励。
- en: We give the AI a bad reward if it's about to drive onto some sand.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI即将驶入沙地，我们会给予它一个坏奖励。
- en: That's it! That should work, because these good and bad rewards have a direct
    effect on the goal and constraints.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这样应该能奏效，因为这些好坏奖励直接影响目标和约束。
- en: To answer the second part of each question, how good and how bad the reward
    should be for each case, we'll play the tough card; it's often more effective.
    The tough card consists of punishing the car more when it makes mistakes than
    we reward it when it does well. In other words, the bad reward is going to be
    stronger than the good reward.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答每个问题的第二部分，即对于每种情况，奖励应该有多好或多坏，我们将采取强硬手段；这通常更有效。强硬手段包括在汽车犯错时给予比表现好时更多的惩罚。换句话说，坏奖励将比好奖励更强烈。
- en: This works well in reinforcement learning, but that doesn't mean you should
    do the same with your dog or your kids. When you're dealing with a biological
    system, the other way around (high good reward and small bad reward) is a much
    more effective way to train or educate. Just food for thought.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在强化学习中效果很好，但这并不意味着你应该用同样的方法训练你的狗或孩子。当你面对一个生物系统时，反过来（高好奖励和小坏奖励）是一种更有效的训练或教育方式。仅供参考。
- en: 'On that note, here are the rewards we''ll give in each case:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，以下是每种情况下我们会给予的奖励：
- en: The AI gets a bad reward of -1 if it drives onto some sand. Nasty!
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI驶入沙地，它会得到-1的坏奖励。真讨厌！
- en: The AI gets a bad reward of -0.2 if it moves away from the destination.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI远离目标，它会得到-0.2的坏奖励。
- en: The AI gets a good reward of 0.1 if it moves closer to the destination.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI向目标靠近，它会得到一个0.1的好奖励。
- en: The reason we attribute the worst reward (-1) to the case when the car drives
    onto some sand makes sense. Driving onto sand is what we absolutely want to avoid.
    The sand on the map represents obstacles in real life; in real life, you would
    train your self-driving car not to hit any obstacle, so as to avoid any accident.
    To do so, we penalize the AI with a highly bad reward when it does hit an obstacle
    during its training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最差奖励（-1）赋给汽车驶入沙地的情况，这是合理的。驶入沙地是我们绝对希望避免的。地图上的沙地代表现实生活中的障碍物；在现实生活中，你会训练自动驾驶汽车避开任何障碍物，以避免事故发生。为了实现这一点，当AI在训练过程中撞到障碍物时，我们会给予它极其严重的坏奖励。
- en: 'How''s that translated that into code? That''s easy; you just take your `sand`
    array and check if the car has just moved onto a cell that contains a 1\. If it
    does, that means the car has moved onto some sand and must therefore get a bad
    reward of -1\. That''s exactly what''s coded here at lines 138, 139, and 140 of
    the `map.py` file (including an update of the car velocity vector, which not only
    updates the speed by slowing the car down to 1, but also updates the direction
    of the car by a certain angle, `self.car.angle`):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何将其转化为代码呢？很简单；你只需要检查`sand`数组，看汽车是否刚刚驶入包含1的格子。如果是，那就意味着汽车驶入了沙地，因此必须获得-1的坏奖励。这正是`map.py`文件中第138、139和140行的代码（包括更新汽车速度向量，不仅通过将汽车速度减慢到1来更新速度，还通过一定角度更新汽车方向`self.car.angle`）。
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then for the other reward attributions, you just have to complete the `if` condition
    preceding with an `else`, which will say what happens in the case where the car
    has not driven onto some sand.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他的奖励分配，你只需要在前面的`if`条件后面加一个`else`，这将说明在汽车没有驶入沙地的情况下会发生什么。
- en: 'In that case, you start a new `if` and `else` condition, saying that if the
    car has moved away from the destination, you give it a bad reward of `-0.2`, and,
    if the car has moved closer to the destination, you give it a good reward of `0.1`.
    The way you measure if the car is getting away from or closer to the goal is by
    comparing two distances put into two separate variables: `last_distance`, which
    is the previous distance between the car and the destination at time *t*-1, and
    `distance`, which is the current distance between the car and the destination
    at time *t*. If you put all that together, you get the following code, which completes
    the preceding lines of code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你开始一个新的 `if` 和 `else` 条件，表示如果汽车远离目标，你给予一个坏的奖励 `-0.2`，如果汽车更接近目标，则给予一个好的奖励
    `0.1`。衡量汽车是否远离或接近目标的方法是通过比较两个距离，这两个距离分别存储在两个变量中：`last_distance`，表示在时刻 *t*-1 时汽车与目标之间的距离，以及
    `distance`，表示在时刻 *t* 时汽车与目标之间的当前距离。如果将这些组合在一起，你会得到以下代码，完成前面的代码行：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To keep the car from trying to veer off the map, lines 147 to 158 of the `map.py`
    file punish the AI with a bad reward of `-1` if the self-driving car gets within
    `10` pixels of any of the map's 4 borders of the map. Finally, lines 160 to 162
    of the `map.py` file update the goal, switching it from the Airport to Downtown,
    or vice versa, anytime the car gets within 100 pixels of the current goal.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止汽车尝试驶出地图，`map.py` 文件的第147行到158行会惩罚AI，如果自动驾驶汽车距离地图的任意4条边界 `10` 像素以内，它会被赋予一个坏奖励
    `-1`。最后，`map.py` 文件的第160行到162行会在汽车距离当前目标100像素以内时，更新目标，将其从机场切换到市区，反之亦然。
- en: AI solution refresher
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI解决方案回顾
- en: Let's refresh our memory by reminding ourselves of the steps of the deep Q-learning
    process, while adapting them to our self-driving car application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾深度Q学习过程的步骤，来刷新一下记忆，同时将其适应到我们的自动驾驶汽车应用中。
- en: 'Initialization:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：
- en: The memory of the experience replay is initialized to an empty list, called
    **memory** in the code.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经验重放的记忆被初始化为空列表，在代码中称为 **memory**。
- en: The maximum size of the memory is set, called **capacity** in the code.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记忆的最大容量被设置，在代码中称为 **capacity**。
- en: 'At each time *t*, the AI repeats the following process, until the end of the
    epoch:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时刻 *t*，AI重复以下过程，直到本轮结束：
- en: The AI predicts the Q-values of the current state *S[t]*. Therefore, since three
    actions can be played (0 <-> 0°, 1 <-> 20°, or 2 <-> -20°), it gets three predicted
    Q-values.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI预测当前状态 *S[t]* 的Q值。因此，由于可以执行三种动作（0 <-> 0°，1 <-> 20°，或2 <-> -20°），它得到了三个预测的Q值。
- en: The AI performs an action selected by the Softmax method (see *Chapter 5*, *Your
    First AI Model – Beware the Bandits!*):![](img/B14110_10_002.png)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI执行一个通过Softmax方法选定的动作（参见 *第5章*，*你的第一个AI模型 – 当心土匪!*）：![](img/B14110_10_002.png)
- en: The AI receives a reward ![](img/B14110_10_003.png), which is one of -1, -0.2
    or +0.1.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI收到一个奖励 ![](img/B14110_10_003.png)，奖励值可能是-1、-0.2 或 +0.1。
- en: The AI reaches the next state ![](img/B14110_10_004.png), which is composed
    of the next three signals from the three sensors, plus the orientation of the
    car.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI到达下一个状态 ![](img/B14110_10_004.png)，该状态由三个传感器的下一个信号以及汽车的方向组成。
- en: The AI appends the transition ![](img/B14110_10_005.png) to the memory.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI将转换 ![](img/B14110_10_005.png) 添加到记忆中。
- en: 'The AI takes a random batch ![](img/B14110_10_006.png) of transitions. For
    all the transitions ![](img/B14110_10_007.png) of the random batch ![](img/B14110_10_008.png):'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI获取一个随机批次 ![](img/B14110_10_006.png) 的转换。对于随机批次中的所有转换 ![](img/B14110_10_007.png)
    ![](img/B14110_10_008.png)：
- en: 'The AI gets the predictions: ![](img/B14110_10_009.png)'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI获取预测值：![](img/B14110_10_009.png)
- en: 'The AI gets the targets: ![](img/B14110_10_010.png)'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI获取目标值：![](img/B14110_10_010.png)
- en: The AI computes the loss between the predictions and the targets over the whole
    batch ![](img/B14110_10_011.png):![](img/B14110_10_012.png)
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI计算整个批次中预测值与目标值之间的损失 ![](img/B14110_10_011.png):![](img/B14110_10_012.png)
- en: Finally, the AI backpropagates this loss error into the neural network, and
    through stochastic gradient descent updates the weights according to how much
    they contributed to the loss error.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，AI将这个损失误差反向传播到神经网络中，并通过随机梯度下降法根据每个权重对损失误差的贡献来更新权重。
- en: Implementation
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Now it's time for the implementation! The first thing you need is a professional
    toolkit, because you're not going to build an artificial brain with simple Python
    libraries. What you need is an advanced framework, which allows fast computation
    for the training of neural networks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始实现了！你首先需要一套专业的工具包，因为你不可能仅仅用简单的Python库来构建一个人工大脑。你需要的是一个高级框架，它能够快速计算神经网络的训练过程。
- en: Today, the best frameworks to build and train AIs are **TensorFlow** (by Google)
    and **PyTorch** (by Facebook). How should you choose between the two? They're
    both great to work with and equally powerful. They both have dynamic graphs, which
    allow the fast computation of the gradients of complex functions needed to train
    the model during backpropagation with mini-batch gradient descent. Really, it
    doesn't matter which framework you choose; both work very well for our self-driving
    car. As far as I'm concerned, I have slightly more experience with PyTorch, so
    I'm going to opt for PyTorch and that's how the example in this chapter will continue
    to play out.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，构建和训练AI的最佳框架是**TensorFlow**（由Google开发）和**PyTorch**（由Facebook开发）。你应该如何在这两者之间做出选择？它们都非常适合使用，且功能强大。它们都有动态计算图，可以快速计算训练模型时反向传播和小批量梯度下降所需的复杂函数的梯度。实际上，选择哪一个框架并不重要；两者都非常适合我们的自动驾驶汽车。就我个人而言，我在PyTorch方面稍有更多经验，因此我将选择PyTorch，本章中的示例也将继续使用PyTorch进行演示。
- en: 'To take a step back, our self-driving car implementation is composed of three
    Python files:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 回过头来看，我们的自动驾驶汽车实现由三个Python文件组成：
- en: '`car.kv`, which contains the Kivy objects (rectangle shape of the car and the
    three sensors)'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`car.kv`，包含Kivy对象（汽车的矩形形状和三个传感器）'
- en: '`map.py`, which builds the environment (map, car, input states, output actions,
    rewards)'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`map.py`，用于构建环境（地图、汽车、输入状态、输出动作、奖励）'
- en: '`deep_q_learning.py`, which builds and trains the AI through deep Q-learning'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`deep_q_learning.py`，用于通过深度Q学习来构建和训练AI'
- en: We've already covered the major elements of `map.py`, and now we're about to
    tackle `deep_q_learning.py`, where you'll not only build an artificial neural
    network, but also implement the deep Q-learning training process. Let's get started!
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了`map.py`的主要元素，现在我们将开始处理`deep_q_learning.py`，在这里你不仅要构建一个人工神经网络，还要实现深度Q学习训练过程。让我们开始吧！
- en: Step 1 – Importing the libraries
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步 – 导入库
- en: 'As usual, you start by importing the libraries and modules you need to build
    your AI. These include:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，你需要通过导入必要的库和模块来开始构建AI。这些包括：
- en: '`os`: The operating system library, used to load the saved AI models.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`os`：操作系统库，用于加载保存的AI模型。'
- en: '`random`: Used to sample some random transitions from the memory for experience
    replay.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`random`：用于从记忆中抽取一些随机转移进行经验回放。'
- en: '`torch`: The main library from PyTorch, which will be used to build our neural
    network with tensors, as opposed to simple matrices like `numpy` arrays. While
    a matrix is a 2-D array, a tensor can be a *n*-dimensional array, with more than
    just a single number in its cells. Here''s a diagram so you can clearly understand
    the difference between a matrix and a tensor:![](img/B14110_10_013.png)'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch`：PyTorch的主要库，将用于通过张量构建我们的神经网络，而不是像`numpy`数组那样使用简单的矩阵。矩阵是二维数组，而张量可以是*n*维数组，其单元格中不仅包含一个数字。以下是一个图示，帮助你清楚地理解矩阵和张量之间的区别：![](img/B14110_10_013.png)'
- en: '`torch.nn`: The `nn` module from the torch library, used to build the fully
    connected layers in the artificial neural network of our AI.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.nn`：PyTorch库中的`nn`模块，用于构建我们AI的人工神经网络中的全连接层。'
- en: '`torch.nn.functional`: The `functional` sub-module from the `nn` module, used
    to call the activation functions (rectifier and Softmax), as well as the loss
    function for backpropagation.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.nn.functional`：`nn`模块中的`functional`子模块，用于调用激活函数（修正线性单元和Softmax），以及用于反向传播的损失函数。'
- en: '`torch.optim`: The `optim` module from the torch library, used to call the
    Adam optimizer, which computes the gradients of the loss with respect to the weights
    and updates those weights in directions that reduce the loss.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.optim`：PyTorch库中的`optim`模块，用于调用Adam优化器，它计算损失函数相对于权重的梯度，并在减小损失的方向上更新这些权重。'
- en: '`torch.autograd`: The `autograd` module from the torch library, used to call
    the `Variable` class, which associates each tensor and its gradient into the same
    variable.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.autograd`：PyTorch库中的`autograd`模块，用于调用`Variable`类，该类将每个张量及其梯度关联到同一个变量中。'
- en: 'That makes up your first code section:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这构成了你的第一段代码：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Step 2 – Creating the architecture of the neural network
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步 – 创建神经网络的架构
- en: This code section is where you really become the architect of the brain in your
    AI. You're about to build the input layer, the fully connected layers, and the
    output layer, while choosing some activation functions that will forward-propagate
    the signal inside the brain.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这一段代码是你真正开始构建AI大脑的地方。你将要构建输入层、全连接层和输出层，并选择一些激活函数，用于在大脑内部进行前向传播信号。
- en: First, you build this brain inside a class, which we are going to call `Network`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将把这个大脑构建在一个类里面，我们将这个类称为`Network`。
- en: What is a class? Let's explain that before we explain why you're using one.
    A class is an advanced structure in Python that contains the instructions of an
    object we want to build. Taking the example of your neural network (the object),
    these instructions include how many layers you want, how many neurons you want
    inside each layer, which activation function you choose, and so on. These parameters
    define your artificial brain and are all gathered in what we call the `__init__()`
    method, which is what we always start with when building a class. But that's not
    all—a class can also contain tools, called methods, which are functions that either
    perform some operations or return something. Your `Network` class will contain
    one method, which forward-propagates the signal inside the neural network and
    returns the predicted Q-values. Call this method `forward`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是类？在我们解释为什么使用类之前，先来解释一下什么是类。类是Python中的一种高级结构，它包含了我们想要构建的对象的指令。以神经网络（即对象）为例，这些指令包括你想要多少层、每一层里面有多少个神经元、你选择了哪种激活函数，等等。这些参数定义了你的人工大脑，并且都聚集在我们称之为`__init__()`的方法中，这也是我们在构建类时总是从这里开始的。但这还不是全部——类还可以包含工具，称为方法，这些方法是执行某些操作或返回某些东西的函数。你的`Network`类将包含一个方法，用于在神经网络中进行前向传播并返回预测的Q值。我们将这个方法命名为`forward`。
- en: 'Now, why use a class? That''s because building a class allows you to create
    as many objects (also called instances) as you want, and easily switch from one
    to another by just changing the arguments of the class. For example, your `Network`
    class contains two arguments: `input_size` (the number of inputs) and `nb_actions`
    (the number of actions). If you ever want to build an AI with more inputs (besides
    the signals and the orientation) or more outputs (you could add an action that
    brakes the car), you''ll do it in a flash thanks to the advanced structure of
    the class. It''s super practical, and if you''re not already familiar with classes
    you''ll have to get familiar with them. Nearly all AI implementations are done
    with classes.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么使用类呢？这是因为构建类可以让你创建任意多个对象（也叫实例），并且只需改变类的参数就能轻松切换。举个例子，你的`Network`类包含两个参数：`input_size`（输入的数量）和`nb_actions`（动作的数量）。如果你将来想要构建一个拥有更多输入（除了信号和方向）或更多输出（例如添加一个能够刹车的动作）的AI，得益于类的高级结构，你可以快速实现。这非常实用，如果你还不熟悉类的概念，你将需要尽快掌握它们。几乎所有的AI实现都使用类。
- en: 'That was just a short technical aside to make sure I don''t lose anybody on
    the way. Now let''s build this class. As there are many important elements to
    explain in the code, and since you''re probably new to PyTorch, I''ll show you
    the code first and then explain it line by line from the `deep_q_learning.py`
    file:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简短的技术说明，确保我在讲解过程中没有让任何人迷失。现在，让我们开始构建这个类。由于代码中有许多重要的元素需要解释，并且你可能对PyTorch不太熟悉，所以我会先展示代码，然后逐行解释来自`deep_q_learning.py`文件的内容：
- en: '[PRE7]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Line 15**: You introduce the `Network` class. In the parenthesis of this
    class, you can see `nn.Module`. That means you''re calling the `Module` class,
    which is an existing class taken from the `nn` module, in order to get all the
    properties and tools of the `Module` class and use them inside your `Network`
    class. This trick of calling another existing class inside a new class is called
    **inheritance**.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**第15行**：你引入了`Network`类。在这个类的括号内，你可以看到`nn.Module`。这意味着你调用了`Module`类，它是从`nn`模块中提取的一个现有类，用来获取`Module`类的所有属性和工具，并在你的`Network`类中使用它们。这个在新类中调用另一个现有类的技巧叫做**继承**。'
- en: '**Line 17**: You start with the `__init__()` method, which defines all the
    parameters (number of inputs, number of outputs, and so on) of your artificial
    neural network. You can see three arguments: `self`, `input_size`, and `nb_action.self`
    refer to the object, that is, to the future instance of the class that will be
    created after the class is done. Any time you see `self` before a variable, and
    separated by a dot (like `self.variable`), that means the variable belongs to
    the object. That should clear up any mystery about `self`!'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**第17行**：你从`__init__()`方法开始，它定义了人工神经网络的所有参数（输入数量、输出数量等）。你可以看到三个参数：`self`、`input_size`和`nb_action`。`self`指代对象，即类创建后将要生成的未来实例。每当你看到`self`出现在变量前面，并且通过点（`.`）与变量分隔时，意味着该变量属于该对象。这应该能解开关于`self`的一切谜团！'
- en: Then, `input_size` is the number of inputs in your input state vector (thus
    4), and `nb_action` is the number of output actions (thus 3). What's important
    to understand is that the arguments (other than self) of the `__init__()` method
    are the ones you will enter when creating the future object, which is the future
    artificial brain of your AI.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`input_size`是输入状态向量中的输入数量（因此是4），`nb_action`是输出动作的数量（因此是3）。重要的是要理解，`__init__()`方法中的参数（除了`self`）是你在创建未来对象时会输入的参数，也就是未来你的AI人工大脑。
- en: '**Line 18**: You use the `super()` function to activate the inheritance (explained
    in Line 15), inside the `__init__()` method.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**第18行**：你使用`super()`函数来激活继承（如第15行所述），该函数位于`__init__()`方法内。'
- en: '**Line 19**: Here you introduce the first object variable, `self.input_size`,
    set equal to the argument `input_size` (which will later be entered as `4`, since
    the input state has 4 elements).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**第19行**：这里你引入了第一个对象变量`self.input_size`，并将其设置为与参数`input_size`相等（稍后将输入为`4`，因为输入状态有4个元素）。'
- en: '**Line 20**: You introduce the second object variable, `self.nb_action`, set
    equal to the argument `nb_action` (which will later be entered as `3`, since there
    are three actions that can be performed).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**第20行**：你引入了第二个对象变量`self.nb_action`，并将其设置为与参数`nb_action`相等（稍后将输入为`3`，因为可以执行三个动作）。'
- en: '**Line 21**: You introduce the third object variable, `self.fc1`, which is
    the first full connection between the input layer (composed of the input state)
    and the hidden layer. That first full connection is created as an object of the
    `nn.Linear` class, which takes two arguments: the first one is the number of elements
    in the left layer (the input layer), so `input_size` is the right argument to
    use, and the second one is the number of hidden neurons in the right layer (the
    hidden layer). Here, you choose to have 30 neurons, and therefore the second argument
    is `30`. The choice of 30 is purely arbitrary, and the self-driving car could
    work well with any other numbers.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**第21行**：你引入了第三个对象变量`self.fc1`，它是输入层（由输入状态组成）与隐藏层之间的第一个全连接。这个第一个全连接作为`nn.Linear`类的对象创建，它接受两个参数：第一个是左侧层（输入层）中的元素数量，因此应该使用`input_size`作为参数，第二个是右侧层（隐藏层）中的隐藏神经元数量。在这里，你选择了30个神经元，因此第二个参数是`30`。选择30只是一个任意的决定，自动驾驶汽车也可以在其他数量下正常工作。'
- en: '**Line 22**: You introduce the fourth object variable, `self.fc2`, which is
    the second full connection between the hidden layer (composed of 30 hidden neurons)
    and the output layer. It could have been a full connection with a new hidden layer,
    but your problem is not complex enough to need more than one hidden layer, so
    you''ll just have one hidden layer in your artificial brain. Just like before,
    that second full connection is created as an object of the `nn.Linear` class,
    which takes two arguments: the first one is the number of elements in the left
    layer (the hidden layer), therefore `30`, and the second one is the number of
    hidden neurons in the right layer (the output layer), therefore `3`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**第22行**：你引入了第四个对象变量`self.fc2`，它是隐藏层（由30个隐藏神经元组成）与输出层之间的第二个全连接。它本来也可以是与新隐藏层的全连接，但你的问题不复杂到需要多个隐藏层，因此你在人工大脑中只会有一个隐藏层。和之前一样，这个第二个全连接作为`nn.Linear`类的对象创建，它接受两个参数：第一个是左侧层（隐藏层）中的元素数量，因此是`30`，第二个是右侧层（输出层）中的隐藏神经元数量，因此是`3`。'
- en: '**Line 24**: You start building the first and only method of the class, the
    `forward` method, which will propagate the signal from the input layer to the
    output layer, after which it will return the predicted Q-values. This `forward`
    method takes two arguments: `self`, because you''ll use the object variables inside
    the `forward` method, and `state`, the input state vector composed of four elements
    (orientation plus the three signals).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**第24行**：你开始构建类的第一个也是唯一的方法——`forward` 方法，该方法将信号从输入层传播到输出层，然后返回预测的 Q 值。这个 `forward`
    方法接受两个参数：`self`，因为你将在 `forward` 方法中使用对象变量，以及 `state`，输入状态向量，由四个元素（方位加上三个信号）组成。'
- en: '**Line 25**: You forward propagate the signal from the input layer to the hidden
    layer while activating the signal with a rectifier activation function, also called
    **ReLU** (**Rectified Linear Unit**). You do this in two steps. First, the forward
    propagation from the input layer to the hidden layer is done by calling the first
    full connection `self.fc1` with the input state vector `state` as input: `self.fc1(state)`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**第25行**：你通过一个整流激活函数（也叫 **ReLU**（**整流线性单元**））将信号从输入层前向传播到隐藏层并激活信号。这个过程分为两步。首先，通过调用第一个全连接层
    `self.fc1`，并将输入状态向量 `state` 作为输入，完成从输入层到隐藏层的前向传播：`self.fc1(state)`。'
- en: 'That returns the hidden layer. And then we call the `relu` function with that
    hidden layer as input to break the linearity of the signal the following way:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回隐藏层。然后我们调用 `relu` 函数，将该隐藏层作为输入，以以下方式打破信号的线性关系：
- en: '![](img/B14110_10_08.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_08.png)'
- en: 'Figure 8: The Rectifier activation function'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：整流激活函数
- en: The purpose of the ReLU layer is to break linearity by creating non-linear operations
    along the fully connected layers. You'll want to have that, because you're trying
    to solve a nonlinear problem. Finally, `F.relu(self.fc1(state))` returns `x`,
    the hidden layer with a nonlinear signal.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 层的目的是通过在全连接层上创建非线性操作来打破线性关系。你希望实现这一点，因为你在解决的是一个非线性问题。最后，`F.relu(self.fc1(state))`
    返回 `x`，即带有非线性信号的隐藏层。
- en: '**Line 26**: You forward-propagate the signal from the hidden layer to the
    output layer containing the Q-values. In the same way as the previous line, this
    is done by calling the second full connection `self.fc2` with the hidden layer
    `x` as input: `self.fc2(x)`. That returns the Q-values, which you name `q_values`.
    Here, no activation function is needed because you''ll select the action to play
    with Softmax, later, in another class.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**第26行**：你将信号从隐藏层前向传播到包含 Q 值的输出层。和上一行一样，这也是通过调用第二个全连接层 `self.fc2`，并将隐藏层 `x`
    作为输入来完成的：`self.fc2(x)`。这将返回 Q 值，命名为 `q_values`。这里不需要激活函数，因为你稍后将在另一个类中使用 Softmax
    选择要执行的动作。'
- en: '**Line 27**: Finally, here, the `forward` method returns the Q-values.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**第27行**：最后，`forward` 方法返回 Q 值。'
- en: Let's have a look at what you've just created!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一看你刚刚创建的内容！
- en: '![](img/B14110_10_09.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_09.png)'
- en: 'Figure 9: The neural network (the brain) of our AI'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：我们 AI 的神经网络（大脑）
- en: '`self.fc1` are all the blue connection lines between the **Input Layer** and
    the **Hidden Layer**.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`self.fc1` 是 **输入层** 和 **隐藏层** 之间所有的蓝色连接线。'
- en: '`self.fc2` are all the blue connection lines between the **Hidden Layer** and
    the **Output Layer**.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`self.fc2` 是 **隐藏层** 和 **输出层** 之间所有的蓝色连接线。'
- en: That should help you visualize the full connections better. Great job!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该能帮助你更好地可视化完整的连接。干得不错！
- en: Step 3 – Implementing experience replay
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步 – 实现经验回放
- en: Time for the next step! You'll now build another class, which builds the memory
    object for experience replay (as seen in *Chapter 5*, *Your First AI Model – Beware
    the Bandits!*). Call this class `ReplayMemory`. Let's have a look at the code
    first and then I'll explain everything line by line from the `deep_q_learning.py`
    file.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进入下一步了！你现在将构建另一个类，来构建用于经验回放的内存对象（如 *第5章* 所示，*你的第一个 AI 模型 - 小心强盗!*）。首先，我们来看一下代码，然后我将逐行解释
    `deep_q_learning.py` 文件中的所有内容。
- en: '[PRE8]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Line 31**: You introduce the `ReplayMemory` class. This time you don''t need
    to inherit from any other class, so just input `object` in the parenthesis of
    the class.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**第31行**：你引入了 `ReplayMemory` 类。这次你不需要从任何其他类继承，因此只需在类的括号中输入 `object` 即可。'
- en: '**Line 33**: As always, you start with the `__init__()` method, which only
    takes two arguments: `self`, the object, and `capacity`, the maximum size of the
    memory.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**第33行**：和往常一样，你从 `__init__()` 方法开始，该方法只接受两个参数：`self`，对象本身，以及 `capacity`，内存的最大容量。'
- en: '**Line 34**: You introduce the first object variable, `self.capacity`, set
    equal to the argument `capacity`, which will be entered later when creating an
    object of the class.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**第34行**：你引入了第一个对象变量`self.capacity`，并将其设置为参数`capacity`，该参数将在稍后创建类的对象时传入。'
- en: '**Line 35**: You introduce the second object variable, `self.memory`, initialized
    as an empty list.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**第35行**：你引入了第二个对象变量`self.memory`，并将其初始化为空列表。'
- en: '**Line 37**: You start building the first tool of the class, the `push` method,
    which takes a transition as input and adds it to the memory. However, if adding
    that transition exceeds the memory''s capacity, the `push` method also deletes
    the first element of the memory. The `event` argument you can see is the transition
    to be added.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**第37行**：你开始构建类的第一个工具——`push` 方法，该方法以一个过渡作为输入并将其添加到记忆中。然而，如果添加该过渡会超出记忆的容量，`push`
    方法还会删除记忆中的第一个元素。你看到的`event`参数是要添加的过渡。'
- en: '**Line 38**: Using the `append` function, you add the transition to the memory.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**第38行**：使用`append`函数，你将过渡添加到记忆中。'
- en: '**Line 39**: You start an `if` condition that checks if the length of the memory
    (meaning its number of transitions) is larger than the capacity.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**第39行**：你开始了一个`if`条件语句，用来检查记忆的长度（即它的过渡数量）是否大于容量。'
- en: '**Line 40**: If that is indeed the case, you delete the first element of the
    memory.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**第40行**：如果真是这样，你会删除记忆中的第一个元素。'
- en: '**Line 42**: You start building the second tool of the class, the `sample`
    method, which samples some random transitions from the experience replay memory.
    It takes `batch_size` as input, which is the size of the batches of transitions
    with which you''ll train your neural network.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**第42行**：你开始构建类的第二个工具——`sample`方法，该方法从经验回放记忆中抽取一些随机过渡。它以`batch_size`为输入，表示用于训练神经网络的过渡批次大小。'
- en: 'Remember how it works: instead of forward-propagating single input states into
    the neural network and updating the weights after each transition resulting from
    the input state, you forward-propagate small batches of input states and update
    the weights after backpropagating the same whole batches of transitions with mini-batch
    gradient descent. That''s different from stochastic gradient descent (weight update
    every single input) and batch gradient descent (weight update every batch of inputs)
    as explained in *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning*:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 记住它是如何工作的：你不是将单一输入状态正向传播到神经网络并在每次由输入状态导致的过渡后更新权重，而是正向传播小批量的输入状态，并在反向传播相同的整个批次的过渡后通过小批量梯度下降更新权重。这与随机梯度下降（每次输入时更新权重）和批量梯度下降（每批输入时更新权重）不同，正如在*第9章*，*与人工大脑同行
    – 深度Q学习*中解释的那样：
- en: '![](img/B14110_10_10.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_10.png)'
- en: 'Figure 10: Batch gradient descent versus stochastic gradient descent'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：批量梯度下降与随机梯度下降
- en: '**Line 43**: You sample some random transitions from the memory and put them
    into a batch of size `batch_size`. For example, if `batch_size = 100`, you sample
    100 random transitions from the memory. The sampling is done with the `sample()`
    function from the random library. Then, `zip(*list)` is used to regroup the states,
    actions, and rewards into separate batches of the same size (`batch_size`), in
    order to put the sampled transitions into the format expected by PyTorch (the
    `Variable` format, which comes next in Line 44).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**第43行**：你从记忆中随机抽取一些过渡，并将它们放入一个大小为`batch_size`的批次中。例如，如果`batch_size = 100`，你将从记忆中抽取100个随机过渡。抽取过程使用随机库中的`sample()`函数完成。然后，`zip(*list)`被用来将状态、动作和奖励重新分组为相同大小的独立批次（`batch_size`），以便将抽取的过渡格式化为PyTorch所期望的格式（即接下来在第44行的`Variable`格式）。'
- en: 'This is probably a good time to take a step back. Let''s see what Line 43 gives
    you:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能是个不错的时机，退一步来看一下第43行的内容：
- en: '![](img/B14110_10_11.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_11.png)'
- en: 'Figure 11: The batches of last states, actions, rewards, and next states'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：最后状态、动作、奖励和下一状态的批次
- en: '**Line 44**: Using the `map()` function, wrap each sample into a `torch Variable`
    object (as `Variable()` is actually a class), so that each tensor inside the samples
    is associated to a gradient. Simply put, you can see a `torch Variable` as an
    advanced structure that encompasses a tensor and a gradient.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**第44行**：使用`map()`函数，将每个样本包装成一个`torch Variable`对象（因为`Variable()`实际上是一个类），这样样本中的每个张量都会与梯度关联。简单来说，`torch
    Variable`可以看作是一个包含张量和梯度的高级结构。'
- en: This is the beauty of PyTorch. These `torch Variables` are all in a dynamic
    graph which allows fast computation of the gradient of complex functions. Those
    fast computations are required for the weight updates happening during backpropagation
    with mini-batch gradient descent. Inside the `Variable` class we see `torch.cat(x,0)`.
    That's just a concatenation trick, along the vertical axis, to put the samples
    in the format expected by the `Variable` class.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 PyTorch 的魅力所在。这些 `torch Variables` 都位于一个动态计算图中，这使得我们能够快速计算复杂函数的梯度。这样的快速计算是反向传播过程中通过小批量梯度下降更新权重所必需的。在
    `Variable` 类内部，我们看到 `torch.cat(x,0)`。这只是一个拼接技巧，沿着垂直轴将样本格式化为 `Variable` 类所期望的格式。
- en: 'The most important thing to remember is this: when training a neural network
    with PyTorch, we always work with `torch Variables`, as opposed to just tensors.
    You can find more details about this in the PyTorch documentation.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的最重要的事情是：在使用 PyTorch 训练神经网络时，我们始终使用`torch Variables`，而不仅仅是张量。你可以在 PyTorch
    文档中找到更多相关细节。
- en: Step 4 – Implementing deep Q-learning
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 4 步 – 实现深度 Q 学习
- en: You've made it! You're finally about to start coding the whole deep Q-learning
    process. Again, you'll wrap all of it into a class, this time called `Dqn`, as
    in deep Q-network. This is your final run before the finish line. Let's smash
    this.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你成功了！你终于要开始编写整个深度 Q 学习的代码了。再次强调，你将把它全部封装成一个类，这次叫做 `Dqn`，也就是深度 Q 网络。这是你在终点线之前的最后一次冲刺。让我们加油，冲刺到底！
- en: 'This time, the class is quite long so I''ll show and explain the lines of code
    method by method from the `deep_q_learning.py` file. Here''s the first one, the
    `__init__()` method:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，类比较长，所以我将逐行展示并解释来自 `deep_q_learning.py` 文件的代码，按方法来逐一说明。这是第一行，`__init__()`
    方法：
- en: '[PRE9]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Line 48**: You introduce the `Dqn` class. You don''t need to inherit from
    any other class so just input `object` in the parenthesis of the class.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 48 行**：你引入了 `Dqn` 类。你不需要从其他类继承，因此只需在类的括号中输入 `object`。'
- en: '**Line 50**: As always, you start with the `__init__()` method, which this
    time takes four arguments:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 50 行**：和往常一样，你从 `__init__()` 方法开始，这次它有四个参数：'
- en: '`self`: The object'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self`：该对象'
- en: '`input_size`: The number of inputs in the input state vector (that is, 4)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`input_size`：输入状态向量中的输入数目（即，4）'
- en: '`nb_action`: The number of actions (that is, 3)'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nb_action`：动作数目（即，3）'
- en: '`gamma`: The discount factor in the temporal difference formula'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gamma`：时序差分公式中的折扣因子'
- en: '**Line 51**: You introduce the first object variable, `self.gamma`, set equal
    to the argument `gamma` (which will be entered later when you create an object
    of the `Dqn` class).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 51 行**：你引入了第一个对象变量 `self.gamma`，并将其设置为 `gamma` 参数的值（该值将在稍后创建 `Dqn` 类对象时输入）。'
- en: '**Line 52**: You introduce the second object variable, `self.model`, an object
    of the `Network` class you built before. This object is your neural network; in
    other words, the brain of our AI. When creating this object, you input the two
    arguments of the `__init__()` method in the `Network` class, which are `input_size`
    and `nb_action`. You''ll enter their real values (respectively `4` and `3`) later,
    when creating an object of the `Dqn` class.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 52 行**：你引入了第二个对象变量 `self.model`，它是你之前构建的 `Network` 类的一个对象。这个对象就是你的神经网络；换句话说，就是我们
    AI 的“大脑”。在创建这个对象时，你需要输入 `Network` 类中的 `__init__()` 方法的两个参数，分别是 `input_size` 和
    `nb_action`。稍后，你会在创建 `Dqn` 类的对象时输入它们的实际值（分别是 `4` 和 `3`）。'
- en: '**Line 53**: You introduce the third object variable, `self.memory`, as an
    object of the `ReplayMemory` class you built before. This object is the experience
    replay memory. Since the `__init__` method of the `ReplayMemory` class only expects
    one argument, the `capacity`, that''s exactly what you input here as `100,000`.
    In other words, you''re creating a memory of size 100,000, which means that instead
    of remembering just the last transition, the AI will remember the last 100,000
    transitions.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 53 行**：你引入了第三个对象变量 `self.memory`，它是你之前构建的 `ReplayMemory` 类的一个对象。这个对象是经验回放内存。由于
    `ReplayMemory` 类的 `__init__` 方法只需要一个参数 `capacity`，所以你在这里输入了 `100,000`。换句话说，你创建了一个大小为
    100,000 的内存，这意味着 AI 将记住最近的 100,000 次过渡，而不仅仅是最后一个。'
- en: '**Line 54**: You introduce the fourth object variable, `self.optimizer`, as
    an object of the `Adam` class, which is an existing class built in the `torch.optim`
    module. This object is the optimizer, which updates the weights through mini-batch
    gradient descent during backpropagation. In the arguments, keep most of the default
    parameter values (you can check them in the PyTorch documentation) and only enter the
    model parameters (the `params` argument), which you access with `self.model.parameters`,
    one of the attributes of the `nn.Module` class from which the `Network` class
    inherits.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**第54行**：你引入了第四个对象变量`self.optimizer`，它是`Adam`类的一个对象，`Adam`类是`torch.optim`模块中现有的类。这个对象是优化器，通过小批量梯度下降在反向传播过程中更新权重。在参数中，保持大部分默认值（你可以在PyTorch文档中查看），并且只输入模型参数（`params`参数），这些参数通过`self.model.parameters`访问，这是`nn.Module`类的一个属性，而`Network`类继承了这个类。'
- en: '**Line 55**: You introduce the fifth object variable, `self.last_state`, which
    will be the last state in each (last state, action, reward, next state) transition.
    This last state is initialized as an object of the `Tensor` class from the torch
    library, into which you only have to enter the `input_size` argument. Then `.unsqueeze(0)`
    is used to create an additional dimension at index 0, which will correspond to
    the batch. This allows us to do something like this, matching each last state
    to the appropriate batch:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**第55行**：你引入了第五个对象变量`self.last_state`，它将成为每次（最后状态，动作，奖励，下一个状态）转变中的最后一个状态。这个最后状态被初始化为`Tensor`类的一个对象，该类来自`torch`库，初始化时只需要输入`input_size`参数。然后，使用`.unsqueeze(0)`在索引0处创建一个额外的维度，这个维度将对应于批次。这样我们可以像下面这样做，将每个最后状态与相应的批次匹配：'
- en: '![](img/B14110_10_12.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_12.png)'
- en: 'Figure 12: Adding a dimension for the batch'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：为批次添加一个维度
- en: '**Line 56**: You introduce the sixth object variable, `self.last_action`, initialized
    as `0`, which is the last action played at each iteration.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**第56行**：你引入了第六个对象变量`self.last_action`，其初始值为`0`，表示每次迭代时执行的最后一个动作。'
- en: '**Line 57**: We introduce the last object variable, `self.last_reward`, initialized
    as `0`, which is the last reward received after playing the last action `self.last_action`,
    in the last state `self.last_state`.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**第57行**：我们引入了最后一个对象变量`self.last_reward`，其初始值为`0`，表示在上一次执行动作`self.last_action`后获得的最后奖励，该奖励发生在最后一个状态`self.last_state`中。'
- en: 'Now, you''re all good for the `__init__` method. Let''s move on to the next
    code section with the next method: the `select_action` method, which selects the
    action to play at each iteration using Softmax.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好`__init__`方法了。让我们继续进入下一个代码部分，并讨论下一个方法：`select_action`方法，它使用Softmax选择每次迭代时要执行的动作。
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Line 59**: You start defining the `select_action` method, which takes as
    input an input state vector (orientation, signal 1, signal 2, signal 3), and returns
    as output the selected action to play.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**第59行**：你开始定义`select_action`方法，它接收一个输入状态向量（方向、信号1、信号2、信号3）作为输入，并返回作为输出的选择动作。'
- en: '**Line 60**: You get the probabilities of the three actions thanks to the Softmax
    function taken from the `torch.nn.functional` module. This Softmax function takes
    the Q-values as input, which are exactly returned by `self.model(Variable(state))`.
    Remember, `self.model` is an object of the `Network` class, which has the `forward`
    method, which takes as input an input state tensor wrapped into a `torch` `Variable`,
    and returns as output the Q-values for the three actions.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**第60行**：你通过`torch.nn.functional`模块中的Softmax函数获得三个动作的概率。这个Softmax函数以Q值作为输入，这些Q值正是通过`self.model(Variable(state))`返回的。记住，`self.model`是`Network`类的一个对象，而该类具有`forward`方法，`forward`方法接收一个输入状态张量，该张量被封装在`torch`的`Variable`中，并返回三个动作的Q值作为输出。'
- en: '**Geek note**: Usually we would specify that we call the `forward` method this
    way – `self.model.forward(Variable(state))` – but since `forward` is the only
    method of the `Network` class, it is sufficient to just call `self.model`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**极客笔记**：通常我们会指定以这种方式调用`forward`方法——`self.model.forward(Variable(state))`——但由于`forward`是`Network`类的唯一方法，直接调用`self.model`就足够了。'
- en: 'Multiplying the Q-values by a number (here `100`) inside `softmax` is a good
    trick to remember: it allows you to regulate the Exploration versus Exploitation.
    The lower that number is, the more you''ll explore, and therefore the longer it
    will take to get optimized actions. Here, the problem''s not too complex, so choose
    a large number (`100`) in order to have confident actions and a smooth trajectory
    to the goal. You''ll clearly see the difference if you remove `*100` from the
    code. Simply put, with the `*100`, you''ll see a car sure of itself; without the
    `*100`, you''ll see a car fidgeting.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在`softmax`内将Q值乘以一个数（这里是`100`）是一个值得记住的技巧：它可以调节探索与开发之间的平衡。这个数字越小，你的探索就越多，因此优化动作的时间也会更长。在这里，问题并不复杂，所以选择一个较大的数字（`100`），以便做出更自信的动作，且路径更加平滑。若你移除`*100`，你会明显看到不同。简单来说，带上`*100`，你会看到一辆车很有自信；没有`*100`，你会看到一辆车在焦躁不安。
- en: '**Line 61**: You take a random draw from the distribution of actions created
    by the `softmax` function at line 60, by calling the `multinomial()` function
    from your probabilities `probs`.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**第61行**：你从第60行的`softmax`函数创建的动作分布中随机抽取，通过调用`multinomial()`函数从概率`probs`中获取。'
- en: '**Line 62**: You return the selected action to perform, which you access in
    `action.data[0,0]`. The returned `action` has an advanced tensor structure, and
    the action index (0, 1, or 2) that you''re interested in is located in the `data`
    attribute of the action tensor at the first cell of indexes [0,0].'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**第62行**：你返回要执行的选定操作，你可以通过`action.data[0,0]`来访问该操作。返回的`action`具有高级的张量结构，操作索引（0,
    1或2）位于操作张量的`data`属性中，索引[0,0]的第一个单元格。'
- en: 'Let''s move on to the next code section, the `learn` method. This one is pretty
    interesting because it''s where the heart of deep Q-learning beats. It''s in this
    method that we compute the temporal difference, and accordingly the loss, and
    update the weights with our optimizer in order to reduce that loss. That''s why
    this method is called `learn`, because it is here that the AI learns to perform
    better and better actions that increase the accumulated reward. Let''s continue:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进入下一个代码部分，`learn`方法。这个方法非常有趣，因为它是深度Q学习的核心所在。正是在这个方法中，我们计算时序差分（temporal
    difference），进而计算损失，并使用优化器更新权重，以减少损失。这就是为什么这个方法叫做`learn`，因为正是在这里，AI学会了执行越来越好的动作，进而提高累计的奖励。我们继续：
- en: '[PRE11]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Line 64**: You start by defining the `learn()` method, which takes as inputs
    the batches of the four elements composing a transition (input state, action,
    reward, next state):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**第64行**：你首先定义了`learn()`方法，该方法接受四个元素组成的转移批次作为输入（输入状态、动作、奖励、下一个状态）：'
- en: '`batch_states`: A batch of input states.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_states`：输入状态的批次。'
- en: '`batch_actions`: A batch of actions played.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_actions`：一批执行的动作。'
- en: '`batch_rewards`: A batch of the rewards received.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_rewards`：收到的一批奖励。'
- en: '`batch_next_states`: A batch of the next states reached.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_next_states`：到达的下一个状态批次。'
- en: 'Before I explain Lines 65, 66, and 67, let''s take a step back and see what
    you''ll have to do. As you know, the goal of this `learn` method is to update
    the weights in directions that reduce the back-propagated loss at each iteration
    of the training. First let''s remind ourselves of the formula for the loss:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我解释第65、66和67行之前，让我们先回顾一下你需要做的事情。如你所知，`learn`方法的目标是通过每次训练迭代来更新权重，以减少反向传播的损失。首先，让我们提醒一下损失的公式：
- en: '![](img/B14110_10_014.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_014.png)'
- en: 'Inside the formula for the loss, we clearly recognize the outputs (predicted
    Q-values) and the targets:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失公式中，我们清晰地识别出了输出（预测的Q值）和目标：
- en: '![](img/B14110_10_015.png)![](img/B14110_10_016.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_015.png)![](img/B14110_10_016.png)'
- en: 'Therefore, to compute the loss, you proceed this way over the next four lines
    of code:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算损失，你将按如下方式处理接下来的四行代码：
- en: '**Line 65**: You collect the batch of outputs, ![](img/B14110_10_017.png).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**第65行**：你收集输出批次，![](img/B14110_10_017.png)。'
- en: '**Line 66**: You compute the ![](img/B14110_10_018.png) part of the targets,
    which you call `batch_next_outputs`.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**第66行**：你计算目标的![](img/B14110_10_018.png)部分，称其为`batch_next_outputs`。'
- en: '**Line 67**: You get the batch of targets.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**第67行**：你获取目标的批次。'
- en: '**Line 68**: Since you have the outputs and targets, you''re ready to get the
    loss.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**第68行**：因为你已经有了输出和目标，现在可以计算损失了。'
- en: Now let's do this in detail.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细讨论一下。
- en: '**Line 65**: You collect the batch of outputs ![](img/B14110_10_019.png), meaning
    the predicted Q-values of the input states and the actions played in the batch.
    Getting them takes several steps. First, you call `self.model(batch_states)`,
    which, as seen in Line 60, returns the Q-values of each input state in `batch_states`
    and for all the three actions 0, 1, and 2\. To help you visualize it better, it
    returns something like this:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**第65行**：你收集了输出的批次 ![](img/B14110_10_019.png)，即输入状态和在批次中执行的动作的预测Q值。获取它们需要几个步骤。首先，你调用`self.model(batch_states)`，如第60行所见，它返回每个输入状态在`batch_states`中的Q值，以及三个动作0、1和2的Q值。为了帮助你更好地可视化，它返回类似这样的结果：'
- en: '![](img/B14110_10_13.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_13.png)'
- en: 'Figure 13: What is returned by self.model(batch_states)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：self.model(batch_states)返回的内容
- en: 'You only want the predicted Q-values for the selected actions from the batch
    of outputs, which are found in the batch of actions `batch_actions`. That''s exactly
    what the `.gather(1, batch_actions.unsqueeze(1)).squeeze(1)` trick does: for each
    input state of the batch, it picks the Q-value that corresponds to the action
    that was selected in the batch of actions. To help visualize this better, let''s
    suppose the batch of actions is the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你只想要来自输出批次中选择动作的预测Q值，这些Q值位于动作批次`batch_actions`中。这正是`.gather(1, batch_actions.unsqueeze(1)).squeeze(1)`技巧所做的：对于批次中的每个输入状态，它会选择与批次中的所选动作相对应的Q值。为了帮助更好地可视化，假设动作批次如下：
- en: '![](img/B14110_10_14.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_14.png)'
- en: 'Figure 14: Batch of actions'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：动作批次
- en: 'Then you would get the following batch of outputs composed of the red Q-values:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将得到以下由红色Q值组成的输出批次：
- en: '![](img/B14110_10_15.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_15.png)'
- en: 'Figure 15: Batch of outputs'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：输出批次
- en: I hope this is clear; I'm doing my best not to lose you along the way.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这很清楚；我会尽力避免让你迷失。
- en: '**Line 66**: Now you get the ![](img/B14110_10_020.png) part of the target.
    Call this `batch_next_outputs`; you get it in two steps. First, call `self.model(batch_next_states)`
    to get the predicted Q-values for each next state of the batch of next states
    and for each of the three actions. Then, for each next state of the batch, take
    the maximum of the three Q-values using `.detach().max(1)[0]`. That gives you
    the batch of the ![](img/B14110_10_021.png) values part of the targets.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**第66行**：现在你得到了目标的 ![](img/B14110_10_020.png)部分。称之为`batch_next_outputs`；你可以通过两个步骤获得它。首先，调用`self.model(batch_next_states)`来获取批次下一个状态的每个Q值的预测值，以及三个动作的Q值。然后，对于批次中的每个下一个状态，使用`.detach().max(1)[0]`取三个Q值中的最大值。这就给你目标的
    ![](img/B14110_10_021.png)值部分。'
- en: '**Line 67**: Since you have the batch of rewards ![](img/B14110_10_022.png)
    (it''s part of the arguments), and since you just got the batch of the ![](img/B14110_10_023.png)
    values part of the targets at Line 66, then you''re ready to get the batch of
    targets:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**第67行**：由于你已经有了奖励的批次 ![](img/B14110_10_022.png)（它是参数的一部分），并且由于你刚刚在第66行得到了目标的
    ![](img/B14110_10_023.png)值部分，那么你就准备好获取目标的批次：'
- en: '![](img/B14110_10_024.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_024.png)'
- en: That's exactly what you do at Line 67, by summing `batch_rewards` and `batch_next_outputs`
    multiplied by `self.gamma`, one of the object variables in the `Dqn` class. Now
    you have both the batch of outputs and the batch of targets, so you're ready to
    get the loss.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是你在第67行所做的，通过将`batch_rewards`与`batch_next_outputs`乘以`self.gamma`（这是`Dqn`类中的一个对象变量）相加。现在，你已经有了输出批次和目标批次，因此你可以准备好计算损失了。
- en: '**Line 68**: Let''s remind ourselves of the formula for the loss:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**第68行**：让我们回顾一下损失的公式：'
- en: '![](img/B14110_10_025.png)![](img/B14110_10_026.png)![](img/B14110_10_027.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_025.png)![](img/B14110_10_026.png)![](img/B14110_10_027.png)'
- en: Therefore, in order to get the loss, you just have to get the sum of the squared
    differences between our targets and outputs in the batch. That's exactly what
    the `smooth_l1_loss` function will do. Taken from the `torch.nn.functional` module,
    it takes as inputs the two batches of outputs and targets and returns the loss
    as given in the preceding formula. In the code, call this loss `td_loss` as in
    **temporal difference loss**.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算损失，你只需要计算目标和输出批次之间平方差的总和。这正是`smooth_l1_loss`函数所做的。它来自`torch.nn.functional`模块，接受输出批次和目标批次作为输入，并根据前面的公式返回损失。在代码中，将此损失称为`td_loss`，即**时序差分损失**。
- en: Excellent progress! Now that you have the loss, representing the error between
    the predictions and the targets, you're ready to backpropagate this loss into
    the neural network and update our weights to reduce this loss through mini-batch
    gradient descent. That's why the next step to take here is to use your optimizer,
    which is the tool that will perform the updates to the weights.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 出色的进展！现在你有了损失，表示预测与目标之间的误差，你准备通过反向传播这个损失到神经网络，并通过小批量梯度下降来更新权重，从而减少这个损失。接下来的步骤是使用你的优化器，它将执行权重更新。
- en: '**Line 69**: You first initialize the gradients, by calling the `zero_grad()`
    method from your `self.optimizer` object (`zero_grad` is a method of the `Adam`
    class), which will basically set all the gradients of the weights to zero.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**第69行**：你首先初始化梯度，通过调用`self.optimizer`对象的`zero_grad()`方法（`zero_grad`是`Adam`类的方法），它基本上会将所有权重的梯度设置为零。'
- en: '**Line 70**: You backpropagate the loss error `td_loss` into the neural network
    by calling the `backward()` function from `td_loss`.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**第70行**：你通过调用`td_loss`的`backward()`函数将损失误差`td_loss`反向传播到神经网络中。'
- en: '**Line 71**: You perform the weights updates by calling the `step()` method
    from your `self.optimizer` object (`step` is a method of the `Adam` class).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**第71行**：你通过调用`self.optimizer`对象的`step()`方法来执行权重更新（`step`是`Adam`类的方法）。'
- en: Congratulations! You've built yourself a tool in the `Dqn` class that will train
    your car to drive better. You've done the toughest part. Now all you have left
    to do is to wrap things up into a last method, called `update`, which will simply
    update the weights after reaching a new state.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经在`Dqn`类中构建了一个工具，可以训练你的汽车开得更好。你已经完成了最难的部分。现在你剩下的工作就是将这些工作整合到一个最终的方法中，叫做`update`，它将在达到新状态后更新权重。
- en: Now, in case you are thinking, "but isn't what I've already done with the `learn`
    method?," well, you're right; but you need to make an extra function that will
    update the weights at the right time. The right time to update the weights is
    right after our AI reaches a new state. Simply put, this final `update` method
    you're about to implement will connect the dots between the `learn` method and
    the dynamic environment.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想，“那不就是我已经在`learn`方法中做的事吗？”，嗯，你是对的；但你需要创建一个额外的函数，在适当的时机更新权重。更新权重的适当时机是在我们的AI到达新状态后。简而言之，接下来你要实现的`update`方法将把`learn`方法和动态环境联系起来。
- en: 'That''s the finish line! Are you ready? Here''s the code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是终点！你准备好了吗？下面是代码：
- en: '[PRE12]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Line 73**: You introduce the `update()` method, which takes as input the
    new state reached and the new reward received right after playing an action. This
    new state entered here will be the `state` variable you can see in Line 129 of
    the `map.py` file and this new reward will be the `reward` variable you can see
    in Lines 138 to 145 of the `map.py` file. This `update` method performs some operations
    including the weights updates and, in the end, returns the new action to perform.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**第73行**：你引入了`update()`方法，该方法的输入是刚执行一个动作后所到达的新状态以及新获得的奖励。此处输入的新状态将是你在`map.py`文件第129行看到的`state`变量，而新奖励将是你在`map.py`文件第138到145行看到的`reward`变量。这个`update`方法执行一些操作，包括权重更新，最后返回需要执行的新动作。'
- en: '**Line 74**: You first convert the new state into a torch tensor and unsqueeze
    it to create an additional dimension (placed first in index 0) corresponding to
    the batch. To ease future operations, you also make sure that all the elements
    of the new state (orientation plus the three signals) are converted into floats
    by adding `.float()`.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**第74行**：你首先将新状态转换为torch张量，并通过unsqueeze操作为其创建一个额外的维度（放在索引0的位置），该维度对应于批次。为了方便以后的操作，你还确保新状态的所有元素（方向和三个信号）都被转换为浮动数值，通过添加`.float()`。'
- en: '**Line 75**: Using the `push()` method from your memory object, add a new transition
    to the memory. This new transition is composed of:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**第75行**：使用你内存对象的`push()`方法，向内存中添加一个新的过渡。这个新的过渡由以下部分组成：'
- en: '`self.last_state`: The last state reached before reaching that new state'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.last_state`：到达新状态之前的最后一个状态'
- en: '`self.last_action`: The last action played that led to that new state'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.last_action`：导致到达新状态的最后一个动作'
- en: '`self.last_reward`: The last reward received after performing that last action'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.last_reward`：执行最后一个动作后获得的最后一个奖励'
- en: '`new_state`: The new state that was just reached'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`new_state`：刚刚到达的新状态'
- en: All the elements of this new transition are converted into torch tensors.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新过渡的所有元素都被转换为torch张量。
- en: '**Line 76**: Using the `select_action()` method from your `Dqn` class, perform
    a new action from the new state just reached.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**第76行**：使用`Dqn`类中的`select_action()`方法，从刚到达的新状态执行一个新动作。'
- en: '**Line 77**: Check if the size of the memory is larger than 100\. In `self.memory.memory`,
    the first `memory` is the object created at Line 53 and the second `memory` is
    the variable object introduced at Line 35.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**第77行**：检查内存大小是否大于100。在`self.memory.memory`中，第一个`memory`是第53行创建的对象，第二个`memory`是第35行引入的变量对象。'
- en: '**Line 78**: If that''s the case, sample 100 transitions from the memory, using
    the `sample()` method from your `self.memory` object. This returns four batches
    of size 100:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**第78行**：如果是这种情况，从内存中采样100个过渡，使用`self.memory`对象中的`sample()`方法。这将返回四个大小为100的批次：'
- en: '`batch_states`: The batch of current states (current at the time of the transition).'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_states`：当前状态的批次（在过渡时刻的状态）。'
- en: '`batch_actions`: The batch of actions performed in the current states.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_actions`：在当前状态下执行的动作批次。'
- en: '`batch_rewards`: The batch of rewards received after playing the actions of `batch_actions`
    in the current states of `batch_states`.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_rewards`：在`batch_states`的当前状态下执行`batch_actions`动作后获得的奖励批次。'
- en: '`batch_next_states`: The batch of next states reached after playing the actions
    of `batch_actions` in the current states of `batch_states`.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_next_states`：执行`batch_actions`动作后，在`batch_states`的当前状态下到达的下一个状态批次。'
- en: '**Line 79**: Still in the `if` condition, proceed to the weights updates using
    the `learn()` method called from the same `Dqn` class, with the four previous
    batches as inputs.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**第79行**：仍在`if`条件中，使用`learn()`方法更新权重，该方法从同一个`Dqn`类中调用，并以四个先前的批次作为输入。'
- en: '**Line 80**: Update the last state reached, `self.last_state`, which becomes
    `new_state`.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**第80行**：更新最后到达的状态`self.last_state`，并将其设为`new_state`。'
- en: '**Line 81**: Update the last action performed, `self.last_action`, which becomes
    `new_action`.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**第81行**：更新最后执行的动作`self.last_action`，并将其设为`new_action`。'
- en: '**Line 82**: Update the last reward received, `self.last_reward`, which becomes
    `new_reward`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**第82行**：更新最后获得的奖励`self.last_reward`，并将其设为`new_reward`。'
- en: '**Line 83**: Return the new action performed.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**第83行**：返回执行的新动作。'
- en: That's it for the `update()` method! I hope you can see how we connected the
    dots. Now, to connect the dots even better, let's see where and how you call that
    `update` method in the `map.py` file.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`update()`方法的全部！希望你能看到我们是如何将各个部分连接起来的。现在，为了更好地连接这些点，让我们看看你在`map.py`文件中如何以及在哪里调用`update`方法。
- en: First, before calling that `update()` method, you have to create an object of
    the `Dqn` class, which here is called `brain`. That's exactly what you do in Line
    33 of the `map.py` file.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在调用`update()`方法之前，你必须创建一个`Dqn`类的对象，这里称之为`brain`。这正是你在`map.py`文件第33行做的事情。
- en: '[PRE13]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The arguments entered here are the three arguments we see in the `__init__()`
    method of the `Dqn` class:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这里输入的参数是我们在`Dqn`类的`__init__()`方法中看到的三个参数：
- en: '`4` is the number of elements in the input state (`input_size`).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4`是输入状态中的元素数量（`input_size`）。'
- en: '`3` is the number of possible actions (`nb_action`).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`是可能的动作数量（`nb_action`）。'
- en: '`0.9` is the discount factor (`gamma`).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0.9`是折扣因子（`gamma`）。'
- en: 'Then, from this `brain` object, you call on the `update()` method in Line 130
    of the `map.py` file, right after reaching a new state, called `state` in the
    code:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从这个`brain`对象中，你在`map.py`文件第130行调用`update()`方法，紧接着到达一个新的状态，该状态在代码中被称为`state`：
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Going back to your `Dqn` class, you need two extra methods:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 回到你的`Dqn`类，你需要两个额外的方法：
- en: The `save()` method, which saves the weights of the AI's network after their
    last updates. This method will be called as soon as you click the **save** button
    while running the map. The weights of your AI will be then saved and put into
    a file named `last_brain.pth`, which will automatically be populated in the folder
    that contains your Python files. That's what allows you to have a pre-trained
    AI.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save()`方法，保存AI网络权重的方法，保存的是它们最后一次更新后的权重。每当你在运行地图时点击**保存**按钮时，都会调用这个方法。然后，你的AI权重将被保存并放入一个名为`last_brain.pth`的文件中，该文件将自动存储在包含你的Python文件的文件夹中。这就让你能够拥有一个预训练的AI。'
- en: The `load()` method, which loads the saved weights in the `last_brain.pth` file.
    This method will be called as soon as you click the **load** button while running
    the map. It allows you to start the map with a pre-trained self-driving car, without
    having to wait for it to train.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load()`方法，加载保存的`last_brain.pth`文件中的权重。当你在运行地图时点击**加载**按钮时，它会调用这个方法。它使你能够以一个预训练的自动驾驶汽车开始地图，而无需等待训练。'
- en: These last two methods aren't AI-related, so we won't spend time explaining
    each line of their code. Still, it's good for you to be able to recognize these
    two tools in case you want to use them for another AI model that you build with
    PyTorch.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后两种方法与 AI 无关，因此我们不会花时间解释每一行代码。不过，如果你以后想用它们来构建其他 AI 模型，能认识这两种工具还是挺有用的。
- en: 'Here''s how they''re implemented:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是这样实现的：
- en: '[PRE15]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Congratulations!
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！
- en: That's right! You've finished this 100 lines of code implementation of the AI
    inside our self-driving car. That's quite an accomplishment, especially when coding
    deep Q-learning for the first time. You really can be proud to have gone this
    far.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 没错！你已经完成了我们自驾车中的 AI 代码实现，总共 100 行代码。这是一个相当了不起的成就，尤其是在第一次编写深度 Q 学习时。你真的可以为自己走到这一步感到自豪。
- en: After all this hard work, you definitely deserve to have some fun, and I think
    it'll be the most fun to watch the result of your hard work. In other words, you're
    about to see your self-driving car in action! I remember I was so excited the
    first time I ran this. You'll feel it too; it's pretty cool!
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了这一切的辛勤工作后，你完全值得享受一些乐趣，而我认为最有趣的就是看到你辛勤工作的成果。换句话说，你即将看到你的自驾车开始运行！我记得第一次运行这个程序时，我是多么兴奋。你也会有这种感觉，真的很酷！
- en: The demo
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示
- en: I have some good news and some bad news.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一些好消息和一些坏消息。
- en: 'I''ll start with the bad news: we can''t run the `map.py` file with a simple
    plug and play on Google Colab. The reason for that is that Kivy is very tricky
    to install through Colab. So, we''ll go for the classic method of running a Python
    file: through the terminal.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我先说坏消息：我们不能通过简单的即插即用方式在 Google Colab 上运行 `map.py` 文件。原因是 Kivy 在 Colab 上的安装非常棘手。所以，我们将采用经典的方式运行
    Python 文件：通过终端。
- en: The good news is that once we install Kivy and PyTorch through the terminal,
    you'll have a fantastic demo!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，一旦通过终端安装了 Kivy 和 PyTorch，你将拥有一个精彩的演示！
- en: 'Let''s install everything we need to run our self-driving car. Here''s what
    we have to install, in the following order:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装运行自驾车所需的一切。以下是我们需要按顺序安装的内容：
- en: '**Anaconda**: A free and open source distribution of Python that offers an
    easy way to install packages thanks to the `conda` command. This is what we''ll
    use to install PyTorch and Kivy.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Anaconda**：一个免费的开源 Python 发行版，通过 `conda` 命令提供了一种简便的方式来安装包。我们将用它来安装 PyTorch
    和 Kivy。'
- en: '**Virtual environment with Python 3.6**: Anaconda is installed with Python
    3.7 or higher; however, that 3.7 version is not compatible with Kivy. We''ll create
    a virtual environment in which we install Python 3.6, a version compatible with
    both Kivy and our implementation. Don''t worry if that sounds intimidating, I''ll
    give you all the details you need to set this up.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Python 3.6 的虚拟环境**：Anaconda 默认安装 Python 3.7 或更高版本；然而，3.7 版本与 Kivy 不兼容。我们将创建一个虚拟环境，在其中安装
    Python 3.6，这是与 Kivy 以及我们的实现兼容的版本。如果这听起来有点吓人，不用担心，我会提供所有需要的细节，帮助你完成设置。'
- en: '**PyTorch**: Then, inside the virtual environment, we''ll install PyTorch,
    the AI framework used to build our deep Q-network. We''ll install a specific version
    of PyTorch that''s compatible with our implementation, so that everyone can be
    on the same page and run it with no issues. PyTorch upgrades sometimes include
    changes in the names of the modules, which can make an old implementation incompatible
    with the newest PyTorch versions. Here, we know we have the right PyTorch version
    for our implementation.'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PyTorch**：然后，在虚拟环境中，我们将安装 PyTorch，这个用于构建深度 Q 网络的 AI 框架。我们将安装一个与我们的实现兼容的 PyTorch
    特定版本，以确保每个人都能顺利运行，不会出现问题。PyTorch 升级有时会更改模块的名称，这可能会导致旧的实现与最新版本的 PyTorch 不兼容。在这里，我们确保安装了与我们的实现兼容的正确版本。'
- en: '**Kivy**: To finish, still inside the virtual environment, we''ll install Kivy,
    the open source Python framework on which we will run our map.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kivy**：最后，仍然在虚拟环境中，我们将安装 Kivy，这个开源 Python 框架，我们将在其上运行我们的地图。'
- en: Let's start with Anaconda.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从 Anaconda 开始。
- en: Installing Anaconda
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Anaconda
- en: 'On Google, or your favorite browser, go to [www.anaconda.com](https://www.anaconda.com/).
    On the Anaconda website, click **Download** on the upper right corner of the screen.
    Scroll down and you''ll find the Python versions to download:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google 或你喜欢的浏览器中，访问 [www.anaconda.com](https://www.anaconda.com/)。在 Anaconda
    网站上，点击屏幕右上角的 **Download** 按钮。向下滚动，你会看到可供下载的 Python 版本：
- en: '![](img/B14110_10_16.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_16.png)'
- en: 'Figure 16: Installing Anaconda – Step 2'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：安装 Anaconda – 第 2 步
- en: At the top, make sure that your system (Windows, macOS, or Linux) is correctly
    selected. If it is, click the **Download** button in the Python 3.7 version box.
    This will download Anaconda with Python 3.7.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，确保你的系统（Windows、macOS 或 Linux）已正确选择。如果是，点击 Python 3.7 版本框中的 **Download**
    按钮。这将下载带有 Python 3.7 的 Anaconda。
- en: Then double-click the downloaded file and keep clicking **Continue** and **Agree**
    to install, until the end. If you're prompted to choose who or how to install
    it for, choose **install for me only**.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然后双击下载的文件，并不断点击 **Continue** 和 **Agree** 进行安装，直到安装完成。如果提示你选择安装对象，选择 **仅为我安装**。
- en: Creating a virtual environment with Python 3.6
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 3.6 创建虚拟环境
- en: 'Now that Anaconda''s installed, you can create a virtual environment, named
    `selfdrivingcar`, with Python 3.6 installed. To do this you need to open a terminal and
    enter some commands. Here''s how to open it for the three systems:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Anaconda 已安装，你可以创建一个名为 `selfdrivingcar` 的虚拟环境，并安装 Python 3.6。为此，你需要打开终端并输入一些命令。以下是三种系统的打开方法：
- en: For Linux users, just press `Ctrl` + `Alt` + `T`.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 Linux 用户，只需按 `Ctrl` + `Alt` + `T`。
- en: For Mac users, press `Cmd` + `Space`, and then in the Spotlight Search enter
    `Terminal`.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 Mac 用户，按 `Cmd` + `Space`，然后在聚焦搜索中输入 `Terminal`。
- en: For Windows users, click the Windows button at the lower left corner of your screen,
    find `anaconda` in the list of programs, and click to open Anaconda prompt. A
    black window will open; that's the terminal you'll use to install the packages.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 Windows 用户，点击屏幕左下角的 Windows 按钮，在程序列表中找到 `anaconda`，点击打开 Anaconda 提示符。一个黑色窗口会打开；这就是你用来安装包的终端。
- en: 'Inside the terminal, enter the following command:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中输入以下命令：
- en: '[PRE16]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Just like so:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样：
- en: '![](img/B14110_10_17.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_17.png)'
- en: This command creates a virtual environment called `selfdrivingcar` with Python
    3.6 and other packages installed.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令创建了一个名为 `selfdrivingcar` 的虚拟环境，并安装了 Python 3.6 和其他包。
- en: 'After pressing `Enter`, you''ll get this in a few seconds:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 按下 `Enter` 后，几秒钟内你会看到这个：
- en: '![](img/B14110_10_18.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_18.png)'
- en: 'Press `y` to proceed. This will download and extract the packages. After a
    few seconds, you''ll get this, which marks the end of the installation:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 按 `y` 继续。这将下载并解压包。几秒钟后，你会看到这个，这标志着安装完成：
- en: '![](img/B14110_10_19.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_19.png)'
- en: Then we're going to activate the `selfdrivingcar` virtual environment, meaning
    we're going to get inside it in order to install PyTorch and Kivy within the `selfdrivingcar`
    virtual environment.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将激活 `selfdrivingcar` 虚拟环境，意味着我们将进入该环境，以便在 `selfdrivingcar` 虚拟环境中安装 PyTorch
    和 Kivy。
- en: 'As you can see just preceding, to activate the environment, we will enter the
    following command:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，之前为了激活环境，我们将输入以下命令：
- en: '[PRE17]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Enter that command, and then you''ll get inside the virtual environment:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输入该命令后，你将进入虚拟环境：
- en: '![](img/B14110_10_20.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_20.png)'
- en: Now we can see `(selfdrivingcar)` before my computer's name, `hadelins-macbook-pro`,
    which means we are inside the `selfdrivingcar` virtual environment.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到在我的电脑名称 `hadelins-macbook-pro` 前面出现 `(selfdrivingcar)`，这意味着我们已经进入了 `selfdrivingcar`
    虚拟环境。
- en: We're ready for the next steps, which are the installation of PyTorch and Kivy
    inside this virtual environment. Don't close your terminal, or when you open it
    again you'll be back in the main environment.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好进行下一步，即在这个虚拟环境中安装 PyTorch 和 Kivy。不要关闭你的终端，否则当你重新打开时，你将回到主环境。
- en: Installing PyTorch
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: 'Now we''re going to install PyTorch inside the virtual environment by entering
    the following command:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过输入以下命令在虚拟环境中安装 PyTorch：
- en: '[PRE18]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Just like so:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样：
- en: '![](img/B14110_10_21.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_21.png)'
- en: 'After a few seconds, we get this:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，我们得到了这个：
- en: '![](img/B14110_10_22.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_22.png)'
- en: Press `y` again, and then press `Enter`.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 再次按下 `y`，然后按 `Enter` 键。
- en: 'After a few seconds, PyTorch is installed:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，PyTorch 已安装：
- en: '![](img/B14110_10_23.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_23.png)'
- en: Installing Kivy
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Kivy
- en: 'Now let''s proceed to Kivy. In the same virtual environment, we''re going to
    install Kivy by entering the following command:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续安装 Kivy。在同一个虚拟环境中，我们将通过输入以下命令来安装 Kivy：
- en: '[PRE19]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/B14110_10_24.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_24.png)'
- en: 'Again, we get this:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们得到了这个：
- en: '![](img/B14110_10_25.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_25.png)'
- en: Enter `y` again, and after a few seconds more, Kivy is installed.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 再次输入 `y`，然后再等几秒钟，Kivy 就安装好了。
- en: '![](img/B14110_10_26.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_26.png)'
- en: 'Now I have some terrific news for you: you''re ready to run the self-driving
    car! To do that, we need to run our code in the terminal, still inside our virtual
    environment.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我有个好消息要告诉你：你已经准备好运行自动驾驶汽车了！为了实现这一点，我们需要在终端中运行我们的代码，仍然是在我们的虚拟环境中。
- en: If you already closed your terminal, then when you open it again enter the `conda
    activate selfdrivingcar` command in order to get back inside the virtual environment.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经关闭了终端，那么重新打开终端后，输入`conda activate selfdrivingcar`命令，以便重新进入虚拟环境。
- en: 'So, let''s run the code! If you haven''t already, download the whole repository
    by clicking the **Clone or download** button on the GitHub page:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们来运行代码吧！如果你还没有这样做，先通过点击GitHub页面上的**Clone or download**按钮下载整个仓库：
- en: ([https://github.com/PacktPublishing/AI-Crash-Course](https://github.com/PacktPublishing/AI-Crash-Course))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://github.com/PacktPublishing/AI-Crash-Course](https://github.com/PacktPublishing/AI-Crash-Course))
- en: '![](img/B14110_10_27.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_27.png)'
- en: 'Figure 17: The GitHub repository'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：GitHub 仓库
- en: 'Then unzip it and move the unzipped folder to your desktop, just like so:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 然后解压缩文件，并将解压后的文件夹移动到桌面上，像这样：
- en: '![](img/B14110_10_28.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_28.png)'
- en: 'Now go into `Chapter 10` and select and copy all the files inside:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，进入`Chapter 10`并选择并复制其中的所有文件：
- en: '![](img/B14110_10_29.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_29.png)'
- en: 'Then, because we''re only interested in these files right now, and to simplify
    the command lines in the terminal, paste these files inside the main `AI-Crash-Course-master`
    folder and remove all the rest, which we don''t need, so that you eventually end
    up with this:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由于我们现在只对这些文件感兴趣，并且为了简化终端中的命令行，将这些文件粘贴到主`AI-Crash-Course-master`文件夹内，并删除其他不需要的文件，最终你会得到如下结果：
- en: '![](img/B14110_10_30.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_30.png)'
- en: 'Now we''re going to access this folder from the terminal. Since we put the
    repository folder in the desktop, we will find it in a flash. Back into the terminal,
    enter `ls` (l as in lion) to see in which folder you are in your machine:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过终端访问这个文件夹。由于我们已经将仓库文件夹放在桌面上，所以很容易找到它。再次回到终端，输入`ls`（l代表lion）查看当前所在的文件夹：
- en: '![](img/B14110_10_31.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_31.png)'
- en: 'I can see that I''m in my main root folder, which contains the `Desktop` folder.
    It should usually be the case for you too. So now we''re going to go into the
    `Desktop` folder by entering the following command:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以看到我在我的主根文件夹中，这个文件夹包含了`Desktop`文件夹。通常你也应该能看到这个。所以现在我们进入`Desktop`文件夹，输入以下命令：
- en: '[PRE20]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/B14110_10_32.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_32.png)'
- en: 'Enter `ls` again and check that you indeed see the `AI-Crash-Course-master`
    folder:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 再次输入`ls`命令，确认你确实看到了`AI-Crash-Course-master`文件夹：
- en: '![](img/B14110_10_33.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_33.png)'
- en: 'Then go into the `AI-Crash-Course-master` folder by entering the following
    command:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过输入以下命令进入`AI-Crash-Course-master`文件夹：
- en: '[PRE21]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/B14110_10_34.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_34.png)'
- en: Perfect! Now we're in the right spot! By entering `ls` again, you can see all
    the files of the repo, including the `map.py` file, which is the one we have to
    run to see our self-driving car in action!
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！现在我们已经到了正确的位置！再次输入`ls`命令，你可以看到仓库中的所有文件，包括`map.py`文件，这是我们要运行的文件，用来查看我们的自动驾驶汽车实际运行！
- en: '![](img/B14110_10_35.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_35.png)'
- en: If by any chance you had trouble getting to this point, that may be because
    your main root folder doesn't contain your `Desktop` folder. If that's the case,
    just put the `AI-Crash-Course-master` repo folder inside one of the folders that
    you see when entering the `ls` command in the terminal, and redo the same process.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到任何困难，无法达到这一点，可能是因为你的主根目录中没有包含`Desktop`文件夹。如果是这种情况，只需将`AI-Crash-Course-master`仓库文件夹放入你在终端中输入`ls`命令时看到的文件夹中，然后重新执行相同的步骤。
- en: 'What you have to do is just find and enter the `AI-Crash-Course-master` folder
    with the `cd` commands. That''s it! Don''t forget to make sure your `AI-Crash-Course-master`
    folder only contains the self-driving car files:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的就是找到并进入`AI-Crash-Course-master`文件夹，使用`cd`命令。就这样！不要忘记确保你的`AI-Crash-Course-master`文件夹只包含自动驾驶汽车相关的文件：
- en: '![](img/B14110_10_36.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_36.png)'
- en: Now you're only one command line away from running your self-driving car. I
    hope you're excited to see the results of your hard work; I know exactly how you
    feel, I was in your shoes not so long ago!
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你只差一条命令就能运行你的自动驾驶汽车了。我希望你已经迫不及待地想看到自己努力的成果了；我完全能理解你的心情，毕竟不久前我也是这个样子！
- en: 'So, without further ado, let''s enter the final command, right now. It''s this:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，废话少说，我们现在就输入最终命令。这是：
- en: '[PRE22]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/B14110_10_37.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_37.png)'
- en: 'As soon as you enter it, the map with the car will pop up just like so:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 一进入后，带有汽车的地图就会像这样弹出：
- en: '![](img/B14110_10_38.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_38.png)'
- en: 'Figure 18: The map'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：地图
- en: 'For the first minute or so, your self-driving car will explore its actions
    by performing nonsense movements; you might see it spinning around. After each
    100 movements, the weights inside the neural network of the AI get updated, and
    the car improves its actions to get higher rewards. And suddenly, maybe after
    another 30 seconds or so, you should see your car making round trips between the
    Airport and Downtown, which I highlighted here again:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的一分钟左右，你的自动驾驶汽车会通过执行一些无意义的动作来探索自己的行为；你可能会看到它在旋转。每进行100次动作，AI神经网络中的权重会更新，汽车会改进其动作以获得更高的奖励。突然，也许在再过30秒左右，你应该能看到汽车在机场和市区之间进行往返，这里我再次标出了：
- en: '![](img/B14110_10_39.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_39.png)'
- en: 'Figure 19: The destinations'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：目的地
- en: Now have some fun! Draw some obstacles on the map to see if the car avoids them.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来点乐趣吧！在地图上画些障碍物，看看汽车是否能避开它们。
- en: 'On my side I have just drawn this, and after a few more minutes of training,
    I can clearly see the car avoiding the obstacles:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我这一侧刚画了这个，经过几分钟的训练后，我能清晰地看到汽车避开了障碍物：
- en: '![](img/B14110_10_40.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_40.png)'
- en: 'Figure 20: Road with obstacles'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：有障碍物的道路
- en: 'And you can have even more fun! By, for example, drawing a road like so:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 你还能玩得更开心！例如，可以像这样画一条路：
- en: '![](img/B14110_10_41.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_41.png)'
- en: 'Figure 21: The road of the demo'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：演示的道路
- en: After a few minutes of training, the car becomes able to self-drive along that
    road, while making many road trips between the Airport and Downtown.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟的训练后，汽车能够沿着那条路自驾行驶，并且在机场和市区之间进行多次往返。
- en: 'Quick question for you: how did you program the car to travel between the destinations?'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 快问你个问题：你是如何编程让汽车在目的地之间行驶的？
- en: 'You did it by giving a small positive reward to the AI when the car gets closer
    to the goal. That''s programmed in rows 144 and 145 inside the `map.py` file:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 你是通过在汽车接近目标时给予AI一个小的正奖励来实现的。这个程序代码写在`map.py`文件的第144和145行：
- en: '[PRE23]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Congratulations to you for completing this massive chapter on this not-so-basic
    self-driving car application! I hope you had fun, and that you feel proud to have
    mastered such an advanced model in deep reinforcement learning.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了这一章关于这个不那么基础的自动驾驶应用！希望你玩得开心，也为能够掌握深度强化学习中的这样一个先进模型而感到自豪。
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to build a deep Q-learning model to drive a
    self-driving car. As inputs it took the information from the three sensors and
    its current orientation. As outputs it decided the Q-values for each of the actions
    of going straight, turning left, or turning right. As for the rewards, we punished
    it badly for hitting the sand, punished it slightly for going in the wrong direction,
    and rewarded it slightly for going in the right direction. We made the AI implementation
    in PyTorch and used Kivy for the graphics. To run all of this we used the Anaconda
    environment.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何构建一个深度Q学习模型来驾驶自动驾驶汽车。它的输入是来自三个传感器的信息以及当前的方向。输出是决定直行、左转或右转的Q值。至于奖励，我们对撞到沙地的情况给予严重惩罚，对走错方向的情况轻微惩罚，对走正确方向的情况略微奖励。我们使用PyTorch实现了这个AI，并使用Kivy进行图形展示。为了运行这一切，我们使用了Anaconda环境。
- en: Now take a long break, you deserve it! I'll see you in the next chapter for
    our next AI challenge, where this time we will solve a real-world business problem
    with cost implications running into the millions.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，休息一下吧，你应该值得拥有！我们将在下一章继续我们的AI挑战，这一次我们将解决一个实际的商业问题，涉及到数百万的成本。
