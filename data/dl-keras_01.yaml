- en: Neural Networks Foundations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络基础
- en: 'Artificial neural networks (briefly, *nets*) represent a class of machine learning
    models, loosely inspired by studies about the central nervous systems of mammals.
    Each net is made up of several interconnected *neurons*, organized in *layers*,
    which exchange messages (they *fire*, in jargon) when certain conditions happen.
    Initial studies were started in the late 1950s with the introduction of the perceptron
    (for more information, refer to the article: *The Perceptron: A Probabilistic
    Model for Information Storage and Organization in the Brain*, by F. Rosenblatt, Psychological
    Review, vol. 65, pp. 386 - 408, 1958), a two-layer network used for simple operations,
    and further expanded in the late 1960s with the introduction of the *backpropagation
    algorithm*, used for efficient multilayer networks training (according to the
    articles: *Backpropagation through Time: What It Does and How to Do It*, by P.
    J. Werbos, Proceedings of the IEEE, vol. 78, pp. 1550 - 1560, 1990, and *A Fast
    Learning Algorithm for Deep Belief Nets*, by G. E. Hinton, S. Osindero, and Y.
    W. Teh, Neural Computing, vol. 18, pp. 1527 - 1554, 2006). Some studies argue
    that these techniques have roots dating further back than normally cited (for
    more information, refer to the article: *Deep Learning in Neural Networks: An
    Overview*, by J. Schmidhuber, vol. 61, pp. 85 - 117, 2015). Neural networks were
    a topic of intensive academic studies until the 1980s, when other simpler approaches
    became more relevant. However, there has been a resurrection of interest starting
    from the mid-2000s, thanks to both a breakthrough fast-learning algorithm proposed
    by G. Hinton (for more information, refer to the articles: *The Roots of Backpropagation:
    From Ordered Derivatives to Neural Networks and Political Forecasting*, *Neural
    Networks*, by S. Leven, vol. 9, 1996 and *Learning Representations by Backpropagating
    Errors*, by D. E. Rumelhart, G. E. Hinton, and R. J. Williams, vol. 323, 1986)
    and the introduction of GPUs, roughly in 2011, for massive numeric computation.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（简称*神经网络*）代表了一类机器学习模型，灵感来源于对哺乳动物中枢神经系统的研究。每个神经网络由多个互联的*神经元*组成，这些神经元按*层*组织，当某些条件发生时，它们会交换信息（在术语中称为*激活*）。最初的研究始于20世纪50年代末，伴随感知机的引入（更多信息请参见文章：*感知机：一种用于大脑信息存储和组织的概率模型*，作者：F.
    罗斯布拉特，心理学评论，卷65，第386 - 408页，1958年），这是一个用于简单操作的两层网络，随后在20世纪60年代末，通过引入*反向传播算法*，对多层网络的高效训练进行了扩展（参考文章：*反向传播：它的作用与如何实现*，作者：P.
    J. 韦尔博斯，IEEE会议录，卷78，第1550 - 1560页，1990年，及*深度信念网络的快速学习算法*，作者：G. E. 亨顿、S. 奥辛德罗和Y.
    W. 特，神经计算，卷18，第1527 - 1554页，2006年）。有些研究认为这些技术的起源比通常引用的时间更久远（更多信息请参见文章：*神经网络中的深度学习：概述*，作者：J.
    施米德胡伯，卷61，第85 - 117页，2015年）。神经网络曾是20世纪80年代以前的学术研究重点，直到其他更简单的方法变得更为相关。然而，随着2000年代中期的到来，神经网络重新受到关注，这要归功于G.
    亨顿提出的突破性快速学习算法（更多信息请参见文章：*反向传播的根源：从有序导数到神经网络和政治预测*，*神经网络*，作者：S. 莱文，卷9，1996年，及*通过反向传播误差学习表示*，作者：D.
    E. 鲁梅哈特、G. E. 亨顿和R. J. 威廉姆斯，卷323，1986年），以及2011年左右引入的用于大规模数值计算的GPU。
- en: These improvements opened the route for modern *deep learning*, a class of neural
    networks characterized by a significant number of layers of neurons, which are
    able to learn rather sophisticated models based on progressive levels of abstraction.
    People called it *deep* with 3-5 layers a few years ago, and now it has gone up
    to 100-200.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些改进为现代*深度学习*开辟了道路，深度学习是一类神经网络，其特点是包含大量神经元层，能够基于逐级抽象的方式学习复杂的模型。几年前，人们称其为*深度*，通常只有3到5层，而现在这一层数已上升到100到200层。
- en: 'This learning via progressive abstraction resembles vision models that have
    evolved over millions of years in the human brain. The human visual system is
    indeed organized into different layers. Our eyes are connected to an area of the
    brain called the **visual cortex V1**, which is located in the lower posterior
    part of our brain. This area is common to many mammals and has the role of discriminating
    basic properties and small changes in visual orientation, spatial frequencies,
    and colors. It has been estimated that V1 consists of about 140 million neurons,
    with 10 billion connections between them. V1 is then connected with other areas
    V2, V3, V4, V5, and V6, doing progressively more complex image processing and
    recognition of more sophisticated concepts, such as shapes, faces, animals, and
    many more. This organization in layers is the result of a huge number of attempts
    tuned over several 100 million years. It has been estimated that there are ~16
    billion human cortical neurons, and about 10%-25% of the human cortex is devoted
    to vision (for more information, refer to the article: *The Human Brain in Numbers:
    A Linearly Scaled-up Primate Brain*, by S. Herculano-Houzel, vol. 3, 2009). Deep
    learning has taken some inspiration from this layer-based organization of the
    human visual system: early artificial neuron layers learn basic properties of
    images, while deeper layers learn more sophisticated concepts.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '这种通过渐进抽象进行的学习类似于人类大脑中经过数百万年进化的视觉模型。人类视觉系统的确被组织成不同的层级。我们的眼睛连接到大脑的一个区域，称为**视觉皮层V1**，它位于大脑的后下部。这个区域对许多哺乳动物都是共有的，负责区分基本特性以及视觉方向、空间频率和颜色的微小变化。据估计，V1包含约1.4亿个神经元，它们之间有100亿个连接。V1随后与其他区域V2、V3、V4、V5和V6连接，进行更复杂的图像处理和对更复杂概念的识别，比如形状、面孔、动物等。这种分层组织是通过几亿年的多次尝试调优的结果。据估计，人类大脑皮层约有160亿个神经元，约10%-25%的皮层区域专门负责视觉处理（更多信息请参见文章：*The
    Human Brain in Numbers: A Linearly Scaled-up Primate Brain*，S. Herculano-Houzel，第三卷，2009年）。深度学习从人类视觉系统的这种分层组织中获得了一些灵感：早期的人工神经元层学习图像的基本属性，而较深层的神经元则学习更复杂的概念。'
- en: This book covers several major aspects of neural networks by providing working
    nets coded in Keras, a minimalist and efficient Python library for deep learning
    computations running on the top of either Google's TensorFlow (for more information,
    refer to [https://www.tensorflow.org/](https://www.tensorflow.org/)) or University
    of Montreal's Theano (for more information, refer to [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/))
    backend. So, let's start.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书通过提供在Keras中编写的工作网络，涵盖了神经网络的几个主要方面。Keras是一个简约高效的Python库，用于深度学习计算，支持运行在Google的TensorFlow（更多信息请参见[https://www.tensorflow.org/](https://www.tensorflow.org/)）或蒙特利尔大学的Theano（更多信息请参见[http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)）后台。因此，让我们开始吧。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Perceptron
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器
- en: Multilayer perceptron
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器
- en: Activation functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Gradient descent
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Stochastic gradient descent
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Backpropagation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Perceptron
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: 'The perceptron is a simple algorithm which, given an input vector *x* of *m*
    values (*x[1]*, *x[2]*, ..., *x[n]*) often called input features or simply features,
    outputs either *1* (yes) or *0* (no). Mathematically, we define a function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一种简单的算法，它给定一个输入向量 *x*，包含 *m* 个值（*x[1]*，*x[2]*，...，*x[n]*），通常称为输入特征或简写为特征，输出为
    *1*（是）或 *0*（否）。从数学上讲，我们定义一个函数：
- en: '![](img/image_01_005.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_005.png)'
- en: Here, *w* is a vector of weights, *wx* is the dot product ![](img/image_01_008.jpg), and
    *b* is a bias. If you remember elementary geometry, *wx + b* defines a boundary
    hyperplane that changes position according to the values assigned to *w* and *b*.
    If *x* lies above the straight line, then the answer is positive, otherwise it
    is negative. Very simple algorithm! The perception cannot express a *maybe* answer.
    It can answer *yes* (*1*) or *no* (*0*) if we understand how to define *w* and
    *b*, that is the training process that will be discussed in the following paragraphs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w* 是一个权重向量，*wx* 是点积 ![](img/image_01_008.jpg)，*b* 是偏置。如果你记得初等几何学，*wx + b*
    定义了一个边界超平面，该超平面的位置会根据赋给 *w* 和 *b* 的值变化。如果 *x* 位于直线以上，则答案为正，否则为负。这个算法非常简单！感知器无法表达
    *也许* 的答案。它只能回答 *是*（*1*）或 *否*（*0*），前提是我们理解如何定义 *w* 和 *b*，这就是训练过程，接下来我们将讨论。
- en: The first example of Keras code
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras代码的第一个示例
- en: 'The initial building block of Keras is a model, and the simplest model is called
    **sequential**. A sequential Keras model is a linear pipeline (a stack) of neural
    networks layers. This code fragment defines a single layer with `12` artificial
    neurons, and it expects `8` input variables (also known as features):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的初始构建模块是模型，而最简单的模型称为**顺序模型**。一个顺序的 Keras 模型是一个线性管道（堆叠）的神经网络层。以下代码片段定义了一个包含
    `12` 个人工神经元的单层，并且它期望 `8` 个输入变量（也称为特征）：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each neuron can be initialized with specific weights. Keras provides a few
    choices, the most common of which are listed as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元可以使用特定的权重进行初始化。Keras 提供了几种选择，最常见的几种列举如下：
- en: '`random_uniform`: Weights are initialized to uniformly random small values
    in (*-0.05*, *0.05*). In other words, any value within the given interval is equally
    likely to be drawn.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_uniform`：权重初始化为均匀随机的小值，范围在 (*-0.05*, *0.05*) 之间。换句话说，给定区间内的任何值都有相同的概率被选中。'
- en: '`random_normal`: Weights are initialized according to a Gaussian, with a zero
    mean and small standard deviation of *0.05*. For those of you who are not familiar
    with a Gaussian, think about a symmetric *bell curve* shape.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_normal`：权重根据高斯分布进行初始化，均值为零，标准差为 *0.05*。对于不熟悉高斯分布的朋友，可以将其想象为一个对称的*钟形曲线*。'
- en: '`zero`: All weights are initialized to zero.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero`：所有权重初始化为零。'
- en: A full list is available at [https://keras.io/initializations/](https://keras.io/initializations/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 完整列表可以在 [https://keras.io/initializations/](https://keras.io/initializations/)
    找到。
- en: Multilayer perceptron — the first example of a network
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器 —— 网络的第一个示例
- en: In this chapter, we define the first example of a network with multiple linear
    layers. Historically, perceptron was the name given to a model having one single
    linear layer, and as a consequence, if it has multiple layers, you would call
    it **multilayer perceptron** (**MLP**). The following image represents a generic
    neural network with one input layer, one intermediate layer and one output layer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们定义了一个具有多个线性层的网络的第一个示例。从历史上看，感知器是指具有单一线性层的模型，因此如果它具有多个层，你可以称之为**多层感知器**（**MLP**）。下图表示一个典型的神经网络，包含一个输入层、一个中间层和一个输出层。
- en: '![](img/B06258_01_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_02.png)'
- en: In the preceding diagram, each node in the first layer receives an input and
    fires according to the predefined local decision boundaries. Then the output of
    the first layer is passed to the second layer, the results of which are passed
    to the final output layer consisting of one single neuron. It is interesting to
    note that this layered organization vaguely resembles the patterns of human vision
    we discussed earlier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，第一层中的每个节点接收输入，并根据预定义的局部决策边界进行激活。然后，第一层的输出传递到第二层，第二层的结果传递到最终的输出层，输出层由一个单一的神经元组成。有趣的是，这种分层组织与我们早前讨论的人类视觉模式有些相似。
- en: The *net* is dense, meaning that each neuron in a layer is connected to all
    neurons located in the previous layer and to all the neurons in the following
    layer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*net* 是稠密的，意味着每个层中的神经元都与前一层中的所有神经元及后一层中的所有神经元相连接。'
- en: Problems in training the perceptron and a solution
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练感知器中的问题及其解决方案
- en: Let's consider a single neuron; what are the best choices for the weight *w*
    and the bias *b*? Ideally, we would like to provide a set of training examples
    and let the computer adjust the weight and the bias in such a way that the errors
    produced in the output are minimized. In order to make this a bit more concrete,
    let's suppose we have a set of images of cats and another separate set of images
    not containing cats. For the sake of simplicity, assume that each neuron looks
    at a single input pixel value. While the computer processes these images, we would
    like our neuron to adjust its weights and bias so that we have fewer and fewer
    images wrongly recognized as non-cats. This approach seems very intuitive, but
    it requires that a small change in weights (and/or bias) causes only a small change
    in outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个单一的神经元；对于权重 *w* 和偏置 *b*，最佳选择是什么？理想情况下，我们希望提供一组训练示例，让计算机调整权重和偏置，使输出中产生的误差最小化。为了让这个问题更加具体，假设我们有一组猫的图片和另一组不包含猫的图片。为了简化，假设每个神经元只查看一个输入像素值。在计算机处理这些图片时，我们希望神经元调整它的权重和偏置，使得错误识别为非猫的图片越来越少。这种方法看起来非常直观，但它要求权重（和/或偏置）的小变化只会导致输出的小变化。
- en: 'If we have a big output jump, we cannot *progressively* learn (rather than
    trying things in all possible directions—a process known as exhaustive search—without
    knowing if we are improving). After all, kids learn little by little. Unfortunately,
    the perceptron does not show this little-by-little behavior. A perceptron is either
    *0* or *1* and that is a big jump and it will not help it to learn, as shown in
    the following graph:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的输出跳跃过大，就无法*逐步*学习（而不是像进行穷尽搜索那样尝试所有可能的方向——这是一个在不确定是否改进的情况下进行的过程）。毕竟，孩子们是逐步学习的。不幸的是，感知器没有表现出这种逐步行为。感知器的输出要么是*0*，要么是*1*，这是一种很大的跳跃，这对学习没有帮助，正如下面的图所示：
- en: '![](img/B06258_01_03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_03.png)'
- en: We need something different, smoother. We need a function that progressively
    changes from *0* to *1* with no discontinuity. Mathematically, this means that
    we need a continuous function that allows us to compute the derivative.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些不同的、更平滑的东西。我们需要一个从 *0* 逐渐变化到 *1* 的函数，且没有间断。数学上，这意味着我们需要一个连续的函数，使我们能够计算其导数。
- en: Activation function — sigmoid
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数 — sigmoid
- en: 'The sigmoid function is defined as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数定义如下：
- en: '![](img/image_01_068.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_068.jpg)'
- en: 'As represented in the following graph, it has small output changes in *(0,
    1)* when the input varies in ![](img/image_01_020.jpg). Mathematically, the function
    is continuous. A typical sigmoid function is represented in the following graph:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，当输入在 ![](img/image_01_020.jpg) 中变化时，输出在 *(0, 1)* 范围内变化较小。数学上，这个函数是连续的。一个典型的
    sigmoid 函数在下图中表示：
- en: '![](img/B06258_01_05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_05.png)'
- en: A neuron can use the sigmoid for computing the nonlinear function ![](img/image_01_022.png).
    Note that, if ![](img/image_01_023.png) is very large and positive, then ![](img/image_01_024.jpg),
    so ![](img/image_01_025.jpg), while if ![](img/image_01_026.jpg) is very large
    and negative ![](img/image_01_027.jpg) so ![](img/image_01_028.jpg). In other
    words, a neuron with sigmoid activation has a behavior similar to the perceptron,
    but the changes are gradual and output values, such as *0.5539* or *0.123191*,
    are perfectly legitimate. In this sense, a sigmoid neuron can answer *maybe*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元可以使用 sigmoid 来计算非线性函数 ![](img/image_01_022.png)。注意，如果 ![](img/image_01_023.png)
    非常大且为正，那么 ![](img/image_01_024.jpg)，因此 ![](img/image_01_025.jpg)，而如果 ![](img/image_01_026.jpg)
    非常大且为负，![](img/image_01_027.jpg)，则 ![](img/image_01_028.jpg)。换句话说，具有 sigmoid 激活的神经元表现得类似于感知器，但其变化是渐进的，输出值，如
    *0.5539* 或 *0.123191*，都是完全合法的。从这个意义上说，sigmoid 神经元可以回答*也许*。
- en: Activation function — ReLU
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数 — ReLU
- en: 'The sigmoid is not the only kind of smooth activation function used for neural
    networks. Recently, a very simple function called **rectified linear unit** (**ReLU**)
    became very popular because it generates very good experimental results. A ReLU
    is simply defined as ![](img/image_01_029.jpg), and the nonlinear function is
    represented in the following graph. As you can see in the following graph, the
    function is zero for negative values, and it grows linearly for positive values:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 并不是唯一用于神经网络的平滑激活函数。最近，一种叫做**修正线性单元**（**ReLU**）的简单函数变得非常流行，因为它能够产生非常好的实验结果。ReLU
    函数简单地定义为 ![](img/image_01_029.jpg)，其非线性函数在下图中表示。正如你在下图中看到的，负值时该函数为零，正值时则呈线性增长：
- en: '![](img/B06258_01_06.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_06.png)'
- en: Activation functions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'Sigmoid and ReLU are generally called *activation functions* in neural network
    jargon. In the *Testing different optimizers in Keras* section, we will see that
    those gradual changes, typical of sigmoid and ReLU functions, are the basic building
    blocks to developing a learning algorithm which adapts little by little, by progressively
    reducing the mistakes made by our nets. An example of using the activation function
    σ with the (*x[1]*, *x[2]*, ..., *x[m]*) input vector, (*w[1]*, *w[2]*, *...*,
    *w[m]*) weight vector, *b* bias, and Σ summation is given in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 和 ReLU 通常被称为神经网络术语中的*激活函数*。在*Keras中测试不同优化器*一节中，我们将看到这些逐步变化，典型的 Sigmoid
    和 ReLU 函数，是发展学习算法的基本构建块，这些算法逐渐减少网络所犯的错误，一点点地适应。以下是使用激活函数 σ 与输入向量 (*x[1]*, *x[2]*,
    ..., *x[m]*)、权重向量 (*w[1]*, *w[2]*, *...*, *w[m]*)、偏置 *b* 和求和 Σ 的示例：
- en: '![](img/B06258_01_07.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_07.png)'
- en: Keras supports a number of activation functions, and a full list is available
    at [https://keras.io/activations/](https://keras.io/activations/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 支持多种激活函数，完整列表请参见 [https://keras.io/activations/](https://keras.io/activations/)。
- en: A real example — recognizing handwritten digits
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际示例 — 识别手写数字
- en: In this section, we will build a network that can recognize handwritten numbers.
    For achieving this goal, we use MNIST (for more information, refer to [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)),
    a database of handwritten digits made up of a training set of 60,000 examples
    and a test set of 10,000 examples. The training examples are annotated by humans
    with the correct answer. For instance, if the handwritten digit is the number
    three, then three is simply the label associated with that example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将构建一个可以识别手写数字的网络。为了实现这个目标，我们使用MNIST（有关更多信息，请参见[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)），这是一个由60,000个训练样本和10,000个测试样本组成的手写数字数据库。训练样本由人类标注，包含正确答案。例如，如果手写数字是数字3，那么3就是与该样本相关联的标签。
- en: In machine learning, when a dataset with correct answers is available, we say
    that we can perform a form of *supervised learning*. In this case, we can use
    training examples for tuning up our net. Testing examples also have the correct
    answer associated with each digit. In this case, however, the idea is to pretend
    that the label is unknown, let the network do the prediction, and then later on,
    reconsider the label to evaluate how well our neural network has learned to recognize
    digits. So, not unsurprisingly, testing examples are just used to test our net.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，当有正确答案的数据集时，我们称之为可以进行一种*有监督学习*。在这种情况下，我们可以使用训练样本来调整我们的网络。测试样本也会与每个数字相关联正确的答案。然而，在这种情况下，想法是装作标签是未知的，让网络进行预测，然后稍后重新考虑标签，以评估我们的神经网络在识别数字方面的学习效果。因此，毫不奇怪，测试样本仅用于测试我们的网络。
- en: 'Each MNIST image is in gray scale, and it consists of 28 x 28 pixels. A subset
    of these numbers is represented in the following diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个MNIST图像是灰度图，并由28 x 28个像素组成。这些数字的一部分在下面的图示中表示：
- en: '![](img/B06258_01_08.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_08.png)'
- en: One-hot encoding — OHE
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一热编码 — OHE
- en: In many applications, it is convenient to transform categorical (non-numerical)
    features into numerical variables. For instance, the categorical feature digit
    with the value *d* in *[0-9]* can be encoded into a binary vector with *10* positions,
    which always has *0* value, except the *d*-th position where a *1* is present.
    This type of representation is called **one-hot encoding** (**OHE**) and is very
    common in data mining when the learning algorithm is specialized for dealing with
    numerical functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，将分类（非数值）特征转换为数值变量是很方便的。例如，具有值*d*的分类特征数字* [0-9] *可以编码为一个具有*10*个位置的二进制向量，该向量始终在除*d*位置外的所有位置上为*0*，在*d*位置上为*1*。这种表示方式称为**一热编码**（**OHE**），在数据挖掘中非常常见，尤其是当学习算法专门处理数值函数时。
- en: Defining a simple neural net in Keras
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中定义一个简单的神经网络
- en: Here, we use Keras to define a network that recognizes MNIST handwritten digits.
    We start with a very simple neural network and then progressively improve it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Keras定义一个识别MNIST手写数字的网络。我们从一个非常简单的神经网络开始，然后逐步改进它。
- en: 'Keras provides suitable libraries to load the dataset and split it into training
    sets `X_train`*,* used for fine-tuning our net, and tests set *`X_test`,* used
    for assessing the performance. Data is converted into `float32` for supporting
    GPU computation and normalized to *[0, 1]*. In addition, we load the true labels
    into `Y_train` and `Y_test` respectively and perform a one-hot encoding on them.
    Let''s see the code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了适合的库来加载数据集，并将其划分为训练集`X_train`，用于对网络进行微调，以及测试集* `X_test`，* 用于评估性能。数据被转换为`float32`以支持GPU计算，并归一化为*
    [0, 1] *。此外，我们将真实标签分别加载到`Y_train`和`Y_test`中，并对它们进行一热编码。我们来看一下代码：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The input layer has a neuron associated with each pixel in the image for a total
    of *28 x 28 = 784* neurons, one for each pixel in the MNIST images.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层有一个与图像中每个像素关联的神经元，总共有*28 x 28 = 784*个神经元，每个像素对应MNIST图像中的一个像素。
- en: Typically, the values associated with each pixel are normalized in the range
    *[0, 1]* (which means that the intensity of each pixel is divided by 255, the
    maximum intensity value). The output is 10 classes, one for each digit.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个像素的值会在范围* [0, 1] *内进行归一化（这意味着每个像素的强度会除以255，255是最大强度值）。输出是10个类别，每个类别对应一个数字。
- en: 'The final layer is a single neuron with activation function softmax, which
    is a generalization of the sigmoid function*.* Softmax *squashes* a k-dimensional
    vector of arbitrary real values into a k-dimensional vector of real values in
    the range *(0, 1)*. In our case, it aggregates 10 answers provided by the previous
    layer with 10 neurons:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层是一个具有 softmax 激活函数的单神经元，softmax 是 sigmoid 函数的推广。Softmax *压缩* 一个具有 k 个维度的任意实数向量，将其映射到范围
    *(0, 1)* 的 k 维实数向量。在我们的例子中，它将前一层提供的 10 个答案与 10 个神经元的结果进行聚合：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once we define the model, we have to compile it so that it can be executed
    by the Keras backend (either Theano or TensorFlow). There are a few choices to
    be made during compilation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了模型，就必须对其进行编译，以便它可以被 Keras 后端（无论是 Theano 还是 TensorFlow）执行。在编译过程中需要做出一些选择：
- en: We need to select the *optimizer* that is the specific algorithm used to update
    weights while we train our model
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要选择用于更新权重的 *优化器*，它是训练模型时使用的特定算法。
- en: We need to select the *objective function* that is used by the optimizer to
    navigate the space of weights (frequently, objective functions are called *loss
    function*, and the process of optimization is defined as a process of loss *minimization*)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要选择用于优化器的 *目标函数*，该目标函数用于引导权重空间（通常，目标函数也被称为 *损失函数*，优化过程被定义为损失的 *最小化* 过程）。
- en: We need to evaluate the trained model
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要评估训练好的模型
- en: 'Some common choices for the objective function (a complete list of Keras objective
    functions is at [https://keras.io/objectives/](https://keras.io/objectives/))
    are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的目标函数选择（Keras 目标函数的完整列表可以参考 [https://keras.io/objectives/](https://keras.io/objectives/)）如下：
- en: '**MSE**: This is the mean squared error between the predictions and the true
    values. Mathematically, if ![](img/image_01_042.jpg) is a vector of *n* predictions,
    and *Y* is the vector of *n* observed values, then they satisfy the following
    equation:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差（MSE）**：这是预测值和真实值之间的均方误差。从数学上讲，如果 ![](img/image_01_042.jpg) 是包含 *n* 个预测值的向量，而
    *Y* 是包含 *n* 个观察值的向量，那么它们满足以下方程：'
- en: '![](img/image_01_043.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_043.png)'
- en: These objective functions average all the mistakes made for each prediction,
    and if the prediction is far from the true value, then this distance is made more
    evident by the squaring operation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些目标函数对每个预测所犯的错误进行平均，如果预测值与真实值之间的距离较远，那么通过平方操作，这个距离会更加显著。
- en: '**Binary cross-entropy**: This is the binary logarithmic loss. Suppose that
    our model predicts *p* while the target is *t*, then the binary cross-entropy
    is defined as follows:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类交叉熵**：这是二元对数损失。假设我们的模型预测值为 *p*，而目标为 *t*，那么二分类交叉熵定义如下：'
- en: '![](img/image_01_044.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_044.png)'
- en: This objective function is suitable for binary labels prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标函数适用于二分类标签预测。
- en: '**Categorical cross-entropy**: This is the multiclass logarithmic loss. If
    the target is *t[i,j]* and the prediction is *p[i,j]*, then the categorical cross-entropy
    is this:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别交叉熵**：这是多类的对数损失。如果目标是 *t[i,j]*，预测是 *p[i,j]*，那么类别交叉熵为：'
- en: '![](img/image_01_047.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_047.png)'
- en: This objective function is suitable for multiclass labels predictions. It is
    also the default choice in association with softmax activation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标函数适用于多类标签预测。在与 softmax 激活函数一起使用时，它也是默认选择。
- en: 'Some common choices for metrics (a complete list of Keras metrics is at [https://keras.io/metrics/](https://keras.io/metrics/))
    are as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的度量标准选择（Keras 度量标准的完整列表可以参考 [https://keras.io/metrics/](https://keras.io/metrics/)）如下：
- en: '**Accuracy**: This is the proportion of correct predictions with respect to
    the targets'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：这是预测正确的比例，与目标相比。'
- en: '**Precision**: This denotes how many selected items are relevant for a multilabel
    classification'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：表示在多标签分类中，有多少选定项是相关的。'
- en: '**Recall**: This denotes how many selected items are relevant for a multilabel
    classification'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：表示在多标签分类中，有多少选定项是相关的。'
- en: 'Metrics are similar to objective functions, with the only difference that they
    are not used for training a model but only for evaluating a model. Compiling a
    model in Keras is easy:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 度量标准与目标函数类似，唯一的区别是它们不是用于训练模型，而仅用于评估模型。在 Keras 中编译模型是非常简单的：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the model is compiled, it can be then trained with the `fit()` function,
    which specifies a few parameters:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被编译，就可以使用 `fit()` 函数进行训练，该函数指定了一些参数：
- en: '`epochs`: This is the number of times the model is exposed to the training
    set. At each iteration, the optimizer tries to adjust the weights so that the
    objective function is minimized.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：这是模型暴露于训练集的次数。在每次迭代中，优化器会尝试调整权重，以最小化目标函数。'
- en: '`batch_size`: This is the number of training instances observed before the
    optimizer performs a weight update.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：这是优化器执行权重更新之前，观察到的训练实例的数量。'
- en: 'Training a model in Keras is very simple. Suppose we want to iterate for `NB_EPOCH`
    steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中训练模型非常简单。假设我们想要迭代 `NB_EPOCH` 步：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We reserved part of the training set for validation. The key idea is that we
    reserve a part of the training data for measuring the performance on the validation
    while training. This is a good practice to follow for any machine learning task,
    which we will adopt in all our examples.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部分训练集保留用于验证。关键思想是，我们保留部分训练数据用于在训练过程中评估验证集的表现。这是进行任何机器学习任务时的良好实践，我们将在所有示例中采纳这一做法。
- en: Once the model is trained, we can evaluate it on the test set that contains
    new unseen examples. In this way, we can get the minimal value reached by the objective
    function and best value reached by the evaluation metric.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以在包含新未见示例的测试集上进行评估。通过这种方式，我们可以获得目标函数的最小值和评估指标的最佳值。
- en: 'Note that the training set and the test set are, of course, rigorously separated.
    There is no point in evaluating a model on an example that has already been used
    for training. Learning is essentially a process intended to generalize unseen
    observations and not to memorize what is already known:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练集和测试集当然是严格分开的。在已经用于训练的示例上评估模型是没有意义的。学习本质上是一个旨在概括未见观察结果的过程，而不是去记忆已知的内容：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, congratulations, you have just defined your first neural network in Keras.
    A few lines of code, and your computer is able to recognize handwritten numbers.
    Let's run the code and see what the performance is.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你刚刚在 Keras 中定义了你的第一个神经网络。只需几行代码，你的计算机就能够识别手写数字。让我们运行代码并看看性能如何。
- en: Running a simple Keras net and establishing a baseline
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个简单的 Keras 网络并建立基准
- en: 'So let''s see what will happen when we run the code in the following screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来看一下当我们运行下面截图中的代码时会发生什么：
- en: '![](img/B06258_01_12.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_12.png)'
- en: First, the net architecture is dumped, and we can see the different types of
    layers used, their output shape, how many parameters they need to optimize, and
    how they are connected. Then, the network is trained on 48,000 samples, and 12,000
    are reserved for validation. Once the neural model is built, it is then tested
    on 10,000 samples. As you can see, Keras is internally using TensorFlow as a backend
    system for computation. For now, we don't go into the internals on how the training
    happens, but we can notice that the program runs for 200 iterations, and each
    time, the accuracy improves. When the training ends, we test our model on the
    test set and achieve about 92.36% accuracy on training, 92.27% on validation,
    and 92.22% on the test.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，网络架构被转储，我们可以看到使用的不同类型的层、它们的输出形状、需要优化的参数数量以及它们是如何连接的。然后，网络在 48,000 个样本上进行训练，12,000
    个样本保留用于验证。一旦神经网络模型构建完成，它就会在 10,000 个样本上进行测试。正如你所看到的，Keras 内部使用 TensorFlow 作为计算的后端系统。现在，我们不会深入探讨训练是如何进行的，但我们可以注意到，程序运行了
    200 次迭代，每次迭代时，准确率都有所提高。当训练结束后，我们在测试集上测试模型，得到了约 92.36% 的训练准确率、92.27% 的验证准确率和 92.22%
    的测试准确率。
- en: 'This means that a bit less than one handwritten character out of ten is not
    correctly recognized. We can certainly do better than that. In the following screenshot,
    we can see the test accuracy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大约每十个手写字符中就有一个没有被正确识别。我们当然可以做得更好。在下方的截图中，我们可以看到测试集上的准确率：
- en: '![](img/B06258_01_12a.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_12a.png)'
- en: Improving the simple net in Keras with hidden layers
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用隐藏层改进 Keras 中的简单网络
- en: We have a baseline accuracy of 92.36% on training, 92.27% on validation, and
    92.22% on the test. This is a good starting point, but we can certainly improve
    it. Let's see how.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上的基准准确率为 92.36%，在验证集上的准确率为 92.27%，在测试集上的准确率为 92.22%。这是一个很好的起点，但我们肯定可以做得更好。让我们看看如何改进。
- en: 'A first improvement is to add additional layers to our network. So, after the
    input layer, we have a first dense layer with the `N_HIDDEN` neurons and an activation
    function `relu`. This additional layer is considered *hidden* because it is not
    directly connected to either the input or the output. After the first hidden layer,
    we have a second hidden layer, again with the `N_HIDDEN` neurons, followed by
    an output layer with 10 neurons, each of which will fire when the relative digit
    is recognized. The following code defines this new network:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个改进是向网络中添加额外的层。因此，在输入层之后，我们有一个第一个密集层，包含`N_HIDDEN`个神经元，并使用激活函数`relu`。这个附加层被视为*隐藏层*，因为它与输入和输出都没有直接连接。在第一个隐藏层之后，我们有第二个隐藏层，仍然包含`N_HIDDEN`个神经元，之后是一个输出层，包含10个神经元，每个神经元将在相应的数字被识别时激活。以下代码定义了这个新网络：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's run the code and see which result we get with this multilayer network.
    Not bad. By adding two hidden layers, we reached 94.50% on the training set, 94.63%
    on validation, and 94.41% on the test. This means that we gained an additional
    2.2% accuracy on the test with respect to the previous network. However, we dramatically
    reduced the number of iterations from 200 to 20\. That's good, but we want more.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行代码，看看这个多层网络的结果如何。还不错。通过添加两个隐藏层，我们在训练集上达到了94.50%，在验证集上94.63%，在测试集上94.41%。这意味着，相比于之前的网络，我们在测试集上的准确率提高了2.2%。然而，我们大幅减少了从200次迭代到20次迭代的训练周期。这是好事，但我们还想要更多。
- en: 'If you want, you can play by yourself and see what happens if you add only
    one hidden layer instead of two, or if you add more than two layers. I leave this
    experiment as an exercise. The following screenshot shows the output of the preceding
    example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你可以自己试试，看看如果只添加一个隐藏层而不是两个，或者如果添加两个以上的层会发生什么。我将这个实验留给你做。以下截图显示了前面示例的输出：
- en: '![](img/B06258_01_13.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_13.png)'
- en: Further improving the simple net in Keras with dropout
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中通过 dropout 进一步改进简单的网络
- en: 'Now our baseline is 94.50% on the training set, 94.63% on validation, and 94.41%
    on the test. A second improvement is very simple. We decide to randomly drop with
    the dropout probability some of the values propagated inside our internal dense
    network of hidden layers. In machine learning, this is a well-known form of regularization.
    Surprisingly enough, this idea of randomly dropping a few values can improve our
    performance:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的基准准确率为：训练集94.50%，验证集94.63%，测试集94.41%。第二个改进非常简单。我们决定通过dropout概率随机丢弃一些在内部密集隐藏层网络中传播的值。在机器学习中，这是一种众所周知的正则化方法。令人惊讶的是，随机丢弃一些值的想法竟然能提升我们的表现：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s run the code for 20 iterations as previously done, and we will see that
    this net achieves an accuracy of 91.54% on the training, 94.48% on validation,
    and 94.25% on the test:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像之前那样运行20次迭代，看看这个网络在训练集上达到91.54%，在验证集上94.48%，在测试集上94.25%的准确率：
- en: '![](img/B06258_01_14.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_14.png)'
- en: 'Note that training accuracy should still be above the test accuracy, otherwise
    we are not training long enough. So let''s try to increase significantly the number
    of epochs up to 250, and we get 98.1% accuracy on training, 97.73% on validation,
    and 97.7% on the test:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练集的准确率仍然应该高于测试集准确率，否则我们就没有训练足够长的时间。因此，让我们尝试显著增加epoch数量，直到250次，我们将获得98.1%的训练准确率，97.73%的验证准确率，和97.7%的测试准确率：
- en: '![](img/B06258_01_15.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_15.png)'
- en: 'It is useful to observe how accuracy increases on training and test sets when
    the number of epochs increases. As you can see in the following graph, these two
    curves touch at about 250 epochs, and therefore, there is no need to train further
    after that point:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 观察随着 epoch 数量增加，训练集和测试集上的准确率是非常有用的。正如你在下面的图表中看到的，这两条曲线大约在 250 个 epoch 时交汇，因此，在这一点之后就无需继续训练：
- en: '| ![](img/B06258_01_16.png) | ![](img/B06258_01_17.png) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_01_16.png) | ![](img/B06258_01_17.png) |'
- en: Note that it has been frequently observed that networks with random dropout
    in internal hidden layers can generalize better on unseen examples contained in
    test sets. Intuitively, one can think of this as each neuron becoming more capable
    because it knows it cannot depend on its neighbors. During testing, there is no
    dropout, so we are now using all our highly tuned neurons. In short, it is generally
    a good approach to test how a net performs when some dropout function is adopted.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，常常观察到，在内部隐藏层进行随机丢弃（dropout）的网络在测试集中的未知样本上表现更好。直观地看，可以将其理解为每个神经元变得更强大，因为它知道不能依赖于邻近的神经元。在测试时，不会有丢弃，所以此时我们使用的是所有已调优的神经元。简而言之，采用某种丢弃函数进行测试通常是检验网络性能的一个好方法。
- en: Testing different optimizers in Keras
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中测试不同的优化器
- en: 'We have defined and used a network; it is useful to start giving an intuition
    about how networks are trained. Let''s focus on one popular training technique
    known as **gradient descent** (**GD**). Imagine a generic cost function *C(w)*
    in one single variable *w* like in the following graph:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义并使用了一个网络；接下来可以开始介绍网络训练的直观理解。我们先聚焦于一种流行的训练技术——**梯度下降**（**GD**）。假设一个通用的代价函数
    *C(w)*，它是一个关于单一变量 *w* 的函数，如下图所示：
- en: '![](img/B06258_01_18.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_18.png)'
- en: The gradient descent can be seen as a hiker who aims at climbing down a mountain
    into a valley. The mountain represents the function *C*, while the valley represents
    the minimum *C[min]*. The hiker has a starting point *w[0]*. The hiker moves little
    by little. At each step *r*, the gradient is the direction of maximum increase.
    Mathematically, this direction is the value of the partial derivative ![](img/image_01_062.jpg) evaluated
    at point *w[r]* reached at step *r*. Therefore by taking the opposite direction, ![](img/image_01_064.jpg),
    the hiker can move towards the valley. At each step, the hiker can decide what
    the leg length is before the next step. This is the *learning rate* ![](img/image_01_065.jpg) in
    gradient descent jargon. Note that if ![](img/image_01_066.jpg) is too small,
    then the hiker will move slowly. However, if ![](img/image_01_066.jpg) is too
    high, then the hiker will possibly miss the valley.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降可以看作是一个登山者，目标是从山顶走到山谷。山代表代价函数 *C*，而山谷代表最小值 *C[min]*。登山者从起点 *w[0]* 开始，逐步向前移动。在每一步
    *r* 中，梯度指向最大增加的方向。从数学上讲，这个方向是偏导数的值 ![](img/image_01_062.jpg)，其在步数 *r* 达到的点 *w[r]*
    处被评估。因此，通过朝着相反方向移动 ![](img/image_01_064.jpg)，登山者可以朝着山谷前进。在每一步中，登山者可以决定步长。这就是梯度下降中的
    *学习率* ![](img/image_01_065.jpg)。注意，如果 ![](img/image_01_066.jpg) 太小，登山者会移动得很慢。然而，如果
    ![](img/image_01_066.jpg) 太大，登山者则可能会错过山谷。
- en: 'Now you should remember that a sigmoid is a continuous function, and it is
    possible to compute the derivative. It can be proven that the sigmoid is shown
    as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该记得，sigmoid 是一个连续函数，并且可以计算其导数。可以证明，sigmoid 如下所示：
- en: '![](img/image_01_068.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_068.png)'
- en: 'It has the following derivative:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它的导数为：
- en: '![](img/image_01_069.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_069.png)'
- en: 'ReLU is not differentiable in *0*. We can, however, extend the first derivative
    in *0* to a function over the whole domain by choosing it to be either *0* or
    *1*. The point-wise derivative of ReLU ![](img/image_01_070.png) is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 在 *0* 处不可导。然而，我们可以通过选择将 *0* 或 *1* 作为 *0* 处的导数，从而将其扩展为一个定义在整个领域上的函数。ReLU
    的逐点导数 ![](img/image_01_070.png) 如下所示：
- en: '![](img/B06258_01_070.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_070.png)'
- en: Once we have the derivative, it is possible to optimize the nets with a gradient
    descent technique. Keras uses its backend (either TensorFlow or Theano) for computing
    the derivative on our behalf so we don't need to worry about implementing or computing
    it. We just choose the activation function, and Keras computes its derivative
    on our behalf.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了导数，就可以使用梯度下降技术来优化网络。Keras 使用其后台（无论是 TensorFlow 还是 Theano）来为我们计算导数，所以我们无需担心实现或计算它。我们只需选择激活函数，Keras
    就会为我们计算其导数。
- en: A neural network is essentially a composition of multiple functions with thousands,
    and sometimes millions, of parameters. Each network layer computes a function
    whose error should be minimized in order to improve the accuracy observed during
    the learning phase. When we discuss backpropagation, we will discover that the
    minimization game is a bit more complex than our toy example. However, it is still
    based on the same intuition of descending a valley.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络本质上是多个函数的组合，包含成千上万，有时甚至数百万个参数。每一层网络都会计算一个函数，其误差应当被最小化，以提高在学习阶段观察到的准确率。当我们讨论反向传播时，我们会发现最小化的过程比我们的小示例要复杂一些。然而，它仍然基于通过下降谷底的直观理解。
- en: 'Keras implements a fast variant of gradient descent known as **stochastic gradient
    descent** (**SGD**) and two more advanced optimization techniques known as **RMSprop**
    and **Adam**. RMSprop and Adam include the concept of momentum (a velocity component)
    in addition to the acceleration component that SGD has. This allows faster convergence
    at the cost of more computation. A full list of Keras-supported optimizers is
    at [https://keras.io/optimizers/](https://keras.io/optimizers/). SGD was our default
    choice so far. So now let''s try the other two. It is very simple, we just need
    to change few lines:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Keras实现了一个快速的梯度下降变种，称为**随机梯度下降**（**SGD**），以及另外两种更先进的优化技术，分别是**RMSprop**和**Adam**。RMSprop和Adam除了包含SGD的加速成分外，还引入了动量（一个速度分量）。这使得它们在加速收敛的同时，也需要更多的计算。Keras支持的优化器完整列表可以查看[https://keras.io/optimizers/](https://keras.io/optimizers/)。到目前为止，SGD是我们的默认选择。那么现在我们来试试另外两种优化器。非常简单，我们只需要修改几行代码：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That''s it. Let''s test it as shown in the following screenshot:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。让我们按以下截图所示进行测试：
- en: '![](img/B06258_01_19.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_19.png)'
- en: 'As you can see in the preceding screenshot, RMSprop is faster than SDG since
    we are able to achieve an accuracy of 97.97% on training, 97.59% on validation,
    and 97.84% on the test improving SDG with only 20 iterations. For the sake of
    completeness, let''s see how the accuracy and loss change with the number of epochs,
    as shown in the following graphs:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在之前的截图中看到的，RMSprop比SDG更快，因为我们能够在训练集上获得97.97%的准确率，在验证集上为97.59%，在测试集上为97.84%，仅用20次迭代就超越了SDG。为了完整起见，让我们看看随着周期数的增加，准确率和损失的变化，如以下图表所示：
- en: '| ![](img/B06258_01_20.png) | ![](img/B06258_01_21.png) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_01_20.png) | ![](img/B06258_01_21.png) |'
- en: 'OK, let''s try the other optimizer, `Adam()`. It is pretty simple, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们试试另一个优化器，`Adam()`。它非常简单，如下所示：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we have seen, Adam is slightly better. With Adam, we achieve 98.28% accuracy
    on training, 98.03% on validation, and 97.93% on the test with 20 iterations,
    as shown in the following graphs:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，Adam稍微好一点。使用Adam时，在20次迭代后，我们在训练集上的准确率为98.28%，验证集为98.03%，测试集为97.93%，如以下图表所示：
- en: '| ![](img/B06258_01_22.png) | ![](img/B06258_01_23.png) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_01_22.png) | ![](img/B06258_01_23.png) |'
- en: This is our fifth variant, and remember that our initial baseline was at 92.36%.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的第五个变种，请记住，我们的初始基准是92.36%。
- en: 'So far, we made progressive improvements; however, the gains are now more and
    more difficult. Note that we are optimizing with a dropout of 30%. For the sake
    of completeness, it could be useful to report the accuracy on the test only for
    other dropout values with `Adam()` chosen as optimizer, as shown in the following
    graph:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们取得了逐步的进展；然而，现在的提升变得越来越困难。请注意，我们正在以30%的丢弃率进行优化。为了完整起见，报告不同丢弃率下的测试集准确率可能会很有帮助，且选择`Adam()`作为优化器，如以下图所示：
- en: '![](img/B06258_01_24.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_24.png)'
- en: Increasing the number of epochs
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加训练的周期数
- en: 'Let''s make another attempt and increase the number of epochs used for training
    from 20 to 200\. Unfortunately, this choice increases our computation time by
    10, but it gives us no gain. The experiment is unsuccessful, but we have learned
    that if we spend more time learning, we will not necessarily improve. Learning
    is more about adopting smart techniques and not necessarily about the time spent
    in computations. Let''s keep track of our sixth variant in the following graph:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次，将训练周期数从20增加到200。不幸的是，这样的选择让我们的计算时间增加了10倍，但并没有带来任何提升。实验失败了，但我们学到了一个重要的经验：如果我们花更多时间进行学习，并不一定会有所改善。学习更多的是采用聪明的技巧，而不一定是花费的计算时间。让我们在以下图表中跟踪我们第六个变种的表现：
- en: '![](img/B06258_01_25.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_25.png)'
- en: Controlling the optimizer learning rate
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制优化器学习率
- en: 'There is another attempt we can make, which is changing the learning parameter
    for our optimizer. As you can see in the following graph, the optimal value is
    somewhere close to *0.001*, which is the default learning rate for the optimer.
    Good! Adam works well out of the box:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个尝试是改变优化器的学习参数。正如下图所示，最佳值接近*0.001*，这是优化器的默认学习率。很好！Adam优化器开箱即用效果不错：
- en: '![](img/B06258_01_26.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_26.png)'
- en: Increasing the number of internal hidden neurons
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加内部隐藏神经元的数量
- en: 'We can make yet another attempt, that is, changing the number of internal hidden
    neurons. We report the results of the experiments with an increasing number of
    hidden neurons. We can see in the following graph that by increasing the complexity
    of the model, the run time increases significantly because there are more and
    more parameters to optimize. However, the gains that we are getting by increasing
    the size of the network decrease more and more as the network grows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试另一种方法，即改变内部隐藏神经元的数量。我们报告了随着隐藏神经元数量增加而进行的实验结果。从下图中可以看到，随着模型复杂度的增加，运行时间显著增加，因为需要优化的参数越来越多。然而，随着网络的扩展，通过增加网络规模获得的收益越来越小：
- en: '![](img/B06258_01_27.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_27.png)'
- en: 'In the following graph, we show the time needed for each iteration as the number
    of hidden neurons grow:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了随着隐藏神经元数量的增加，每次迭代所需的时间：
- en: '![](img/B06258_01_28.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_28.png)'
- en: 'The following graph shows the accuracy as the number of hidden neurons grow:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了随着隐藏神经元数量的增加，准确率的变化：
- en: '![](img/B06258_01_29.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_29.png)'
- en: Increasing the size of batch computation
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加批处理计算的大小
- en: 'Gradient descent tries to minimize the cost function on all the examples provided
    in the training sets and, at the same time, for all the features provided in the
    input. Stochastic gradient descent is a much less expensive variant, which considers
    only `BATCH_SIZE` examples. So, let''s see what the behavior is by changing this
    parameter. As you can see, the optimal accuracy value is reached for `BATCH_SIZE=128`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法试图最小化训练集提供的所有示例上的成本函数，同时也针对输入中提供的所有特征。随机梯度下降是一个更便宜的变种，它只考虑`BATCH_SIZE`个示例。因此，让我们通过改变这个参数来观察它的行为。正如你所看到的，最佳准确率出现在`BATCH_SIZE=128`时：
- en: '![](img/B06258_01_30.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_30.png)'
- en: Summarizing the experiments run for recognizing handwritten charts
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结识别手写图表的实验
- en: 'So, let''s summarize: with five different variants, we were able to improve
    our performance from 92.36% to 97.93%. First, we defined a simple layer network
    in Keras. Then, we improved the performance by adding some hidden layers. After
    that, we improved the performance on the test set by adding a few random dropouts
    to our network and by experimenting with different types of optimizers. Current
    results are summarized in the following table:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，总结一下：通过五种不同的变体，我们将性能从92.36%提高到了97.93%。首先，我们在Keras中定义了一个简单的网络层。然后，我们通过添加一些隐藏层来提高性能。接着，我们通过为网络添加随机丢弃层并实验不同类型的优化器来提高测试集上的性能。当前的结果总结在以下表格中：
- en: '| **Model/Accuracy** | **Training** | **Validation** | **Test** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| **模型/准确率** | **训练** | **验证** | **测试** |'
- en: '| **Simple** | 92.36% | 92.37% | 92.22% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| **简单模型** | 92.36% | 92.37% | 92.22% |'
- en: '| **Two hidden (128)** | 94.50% | 94.63% | 94.41% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **两个隐藏层（128）** | 94.50% | 94.63% | 94.41% |'
- en: '| **Dropout (30%)** | 98.10% | 97.73% | 97.7% (200 epochs) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| **Dropout（30%）** | 98.10% | 97.73% | 97.7%（200次迭代） |'
- en: '| **RMSprop** | 97.97% | 97.59% | 97.84% (20 epochs) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| **RMSprop** | 97.97% | 97.59% | 97.84%（20次迭代） |'
- en: '| **Adam** | 98.28% | 98.03% | 97.93% (20 epochs) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **Adam** | 98.28% | 98.03% | 97.93%（20次迭代） |'
- en: However, the next two experiments did not provide significant improvements.
    Increasing the number of internal neurons creates more complex models and requires
    more expensive computations, but it provides only marginal gains. We get the same
    experience if we increase the number of training epochs. A final experiment consisted
    in changing the `BATCH_SIZE` for our optimizer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，接下来的两个实验并没有提供显著的改进。增加内部神经元的数量会创建更复杂的模型，并需要更昂贵的计算，但它仅提供了边际性的提升。如果我们增加训练轮次，也会得到相同的结果。最后一个实验是改变优化器的`BATCH_SIZE`。
- en: Adopting regularization for avoiding overfitting
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用正则化来避免过拟合
- en: 'Intuitively, a good machine learning model should achieve low error on training
    data. Mathematically, this is equivalent to minimizing the loss function on the
    training data given the machine learning model built. This is expressed by the
    following formula.:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，一个好的机器学习模型应该在训练数据上达到较低的误差。从数学上讲，这等同于最小化给定训练数据上的损失函数，这是由以下公式表示的：
- en: '![](img/image_01_084.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_084.png)'
- en: 'However, this might not be enough. A model can become excessively complex in
    order to capture all the relations inherently expressed by the training data.
    This increase of complexity might have two negative consequences. First, a complex
    model might require a significant amount of time to be executed. Second, a complex
    model can achieve very good performance on training data—because all the inherent
    relations in trained data are memorized, but not so good performance on validation
    data—as the model is not able to generalize on fresh unseen data. Again, learning
    is more about generalization than memorization. The following graph represents
    a typical loss function decreasing on both validation and training sets. However,
    a certain point the loss on validation starts to increase because of overfitting:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能还不足以解决问题。模型可能会变得过于复杂，以捕捉训练数据中固有的所有关系。复杂度的增加可能会带来两个负面后果。首先，复杂模型可能需要大量的时间来执行。其次，复杂模型可能在训练数据上表现非常好——因为它记住了训练数据中固有的所有关系，但在验证数据上表现不佳——因为模型无法在新的未见过的数据上进行泛化。再强调一次，学习更多的是关于泛化，而不是记忆。下图表示了一个典型的损失函数，在验证集和训练集上都逐渐下降。然而，在某一点，验证集上的损失开始增加，这是由于过拟合造成的：
- en: '![](img/B06258_01_31.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_31.png)'
- en: As a rule of thumb, if during the training we see that the loss increases on
    validation, after an initial decrease, then we have a problem of model complexity
    that overfits training. Indeed, overfitting is the word used in machine learning
    for concisely describing this phenomenon.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果在训练过程中，我们发现损失在初期下降后，在验证集上反而开始增加，那么我们就遇到了过拟合问题，即模型复杂度过高导致过拟合训练数据。事实上，过拟合是机器学习中用来简洁描述这一现象的术语。
- en: 'In order to solve the overfitting problem, we need a way to capture the complexity
    of a model, that is, how complex a model can be. What could be the solution? Well,
    a model is nothing more than a vector of weights. Therefore the complexity of
    a model can be conveniently represented as the number of nonzero weights. In other
    words, if we have two models, *M1* and *M2*, achieving pretty much the same performance
    in terms of loss function, then we should choose the simplest model that has the
    minimum number of nonzero weights. We can use a hyperparameter *⅄>=0* for controlling
    what the importance of having a simple model is, as in this formula:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决过拟合问题，我们需要一种方法来捕捉模型的复杂性，也就是说，模型的复杂度有多大。解决方案是什么呢？其实，模型不过是一个权重向量。因此，模型的复杂度可以方便地通过非零权重的数量来表示。换句话说，如果我们有两个模型，*M1*
    和 *M2*，它们在损失函数上的表现几乎相同，那么我们应该选择权重非零数量最少的最简单模型。我们可以使用一个超参数 *⅄>=0* 来控制拥有简单模型的重要性，如下公式所示：
- en: '![](img/image_01_086.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_086.png)'
- en: 'There are three different types of regularizations used in machine learning:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中使用的正则化方法有三种不同类型：
- en: '**L1 regularization** (also known as **lasso**): The complexity of the model
    is expressed as the sum of the absolute values of the weights'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1 正则化**（也称为**套索回归**）：模型的复杂度表现为权重绝对值的总和'
- en: '**L2 regularization** (also known as **ridge**): The complexity of the model
    is expressed as the sum of the squares of the weights'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 正则化**（也称为**岭回归**）：模型的复杂度表现为权重平方和的总和'
- en: '**Elastic net regularization**: The complexity of the model is captured by
    a combination of the two preceding techniques'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性网正则化**：模型的复杂度通过前两种方法的组合来捕捉'
- en: Note that the same idea of regularization can be applied independently to the
    weights, to the model, and to the activation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正则化的相同理念可以独立应用于权重、模型和激活函数。
- en: Therefore, playing with regularization can be a good way to increase the performance
    of a network, in particular when there is an evident situation of overfitting.
    This set of experiments is left as an exercise for the interested reader.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，调整正则化可以是提升网络性能的好方法，特别是在出现明显过拟合的情况下。这一组实验留给有兴趣的读者自行完成。
- en: 'Note that Keras supports both l1, l2, and elastic net regularizations. Adding
    regularization is easy; for instance, here we have a `l2` regularizer for kernel
    (the weight *W*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Keras支持l1、l2和弹性网络正则化。添加正则化非常简单；例如，在这里，我们为核（权重*W*）添加了一个`l2`正则化器：
- en: '[PRE10]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A full description of the available parameters is available at: [https://keras.io/regularizers/](https://keras.io/regularizers/).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 可用参数的完整描述可见于：[https://keras.io/regularizers/](https://keras.io/regularizers/)。
- en: Hyperparameters tuning
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优
- en: The preceding experiments gave a sense of what the opportunities for fine-tuning
    a net are. However, what is working for this example is not necessarily working
    for other examples. For a given net, there are indeed multiple parameters that
    can be optimized (such as the number of `hidden neurons`, `BATCH_SIZE`, number
    of `epochs`, and many more according to the complexity of the net itself).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 前述实验让我们了解了调整网络的机会。然而，适用于此示例的内容不一定适用于其他示例。对于给定的网络，确实存在多个可以优化的参数（例如`hidden neurons`的数量、`BATCH_SIZE`、`epochs`的数量，以及根据网络复杂性调整的更多参数）。
- en: Hyperparameter tuning is the process of finding the optimal combination of those
    parameters that minimize cost functions. The key idea is that if we have *n* parameters,
    then we can imagine that they define a space with *n* dimensions, and the goal
    is to find the point in this space which corresponds to an optimal value for the
    cost function. One way to achieve this goal is to create a grid in this space
    and systematically check for each grid vertex what the value assumed by the cost
    function is. In other words, the parameters are divided into buckets, and different
    combinations of values are checked via a brute force approach.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是寻找那些最小化成本函数的最佳参数组合的过程。关键思想是，如果我们有*n*个参数，那么我们可以想象这些参数定义了一个*n*维的空间，目标是找到该空间中对应于成本函数最优值的点。一种实现此目标的方法是，在该空间中创建一个网格，并系统地检查每个网格顶点对应的成本函数值。换句话说，参数被分成不同的桶，然后通过暴力搜索方法检查不同的值组合。
- en: Predicting output
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测输出
- en: 'When a net is trained, it can be course be used for predictions. In Keras,
    this is very simple. We can use the following method:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络被训练完成后，它自然可以用于预测。在Keras中，这非常简单。我们可以使用以下方法：
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For a given input, several types of output can be computed, including a method:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入，可以计算出多种类型的输出，包括一种方法：
- en: '`model.evaluate()`: This is used to compute the loss values'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.evaluate()`：用于计算损失值'
- en: '`model.predict_classes()`: This is used to compute category outputs'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.predict_classes()`：用于计算类别输出'
- en: '`model.predict_proba()`: This is used to compute class probabilities'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.predict_proba()`：用于计算类别概率'
- en: A practical overview of backpropagation
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播的实用概述
- en: Multilayer perceptrons learn from training data through a process called backpropagation.
    The process can be described as a way of progressively correcting mistakes as
    soon as they are detected. Let's see how this works.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机通过一种叫做反向传播的过程从训练数据中学习。这个过程可以描述为一种在错误被检测到时逐步进行修正的方式。让我们来看看这是如何运作的。
- en: Remember that each neural network layer has an associated set of weights that
    determines the output values for a given set of inputs. In addition to that, remember
    that a neural network can have multiple hidden layers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每个神经网络层都有一组关联的权重，用于确定给定输入集的输出值。此外，记住一个神经网络可以有多个隐藏层。
- en: 'In the beginning, all the weights have some random assignment. Then the net
    is activated for each input in the training set: values are propagated *forward*
    from the input stage through the hidden stages to the output stage where a prediction
    is made (note that we have kept the following diagram simple by only representing
    a few values with green dotted lines, but in reality, all the values are propagated
    forward through the network):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，所有的权重都被随机分配。然后，网络会对训练集中的每个输入进行激活：值从输入阶段通过隐藏层传播*向前*，最终到达输出阶段进行预测（注意，我们通过用绿色虚线表示一些值来简化了下图，但实际上所有的值都会通过网络向前传播）：
- en: '![](img/B06258_01_32.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_32.png)'
- en: 'Since we know the true observed value in the training set, it is possible to
    calculate the error made in prediction. The key intuition for backtracking is
    to propagate the error back and use an appropriate optimizer algorithm, such as
    a gradient descent, to adjust the neural network weights with the goal of reducing
    the error (again for the sake of simplicity, only a few error values are represented):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道训练集中真实的观测值，因此可以计算预测中所犯的错误。反向传播的关键直觉是将误差反向传播，并使用适当的优化算法，如梯度下降，来调整神经网络权重，以减少误差（为了简单起见，这里仅表示一些误差值）：
- en: '![](img/B06258_01_33.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_33.png)'
- en: 'The process of forward propagation from input to output and backward propagation
    of errors is repeated several times until the error gets below a predefined threshold.
    The whole process is represented in the following diagram:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入到输出的前向传播过程以及误差的反向传播会重复多次，直到误差降到预定的阈值以下。整个过程在以下图示中表示：
- en: '![](img/B06258_01_34.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_01_34.png)'
- en: The features represent the input and the labels are here used to drive the learning
    process. The model is updated in such a way that the loss function is progressively
    minimized. In a neural network, what really matters is not the output of a single
    neuron but the collective weights adjusted in each layer. Therefore, the network
    progressively adjusts its internal weights in such a way that the prediction increases
    the number of labels correctly forecasted. Of course, using the right set features
    and having a quality labeled data is fundamental to minimizing the bias during
    the learning process.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 特征表示输入，标签在这里用于驱动学习过程。模型以这样一种方式更新，使得损失函数逐步最小化。在神经网络中，真正重要的不是单个神经元的输出，而是每一层中调整的集体权重。因此，网络逐渐调整其内部权重，以便增加正确预测的标签数量。当然，使用正确的特征集和拥有高质量的标签数据对于最小化学习过程中的偏差至关重要。
- en: Towards a deep learning approach
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝着深度学习方法迈进
- en: While playing with handwritten digit recognition, we came to the conclusion
    that the closer we get to the accuracy of 99%, the more difficult it is to improve.
    If we want to have more improvements, we definitely need a new idea. What are
    we missing? Think about it.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行手写数字识别时，我们得出结论：当我们接近99%的准确率时，进一步提升变得越来越困难。如果我们想要更多的改进，显然需要一个新思路。我们缺少了什么呢？思考一下。
- en: 'The fundamental intuition is that, so far, we lost all the information related
    to the local spatiality of the images. In particular, this piece of code transforms
    the bitmap, representing each written digit into a flat vector where the spatial
    locality is gone:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的直觉是，到目前为止，我们丢失了与图像的局部空间性相关的所有信息。特别是，这段代码将位图（表示每个手写数字）转换为一个平坦的向量，其中空间局部性已经丢失：
- en: '[PRE12]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: However, this is not how our brain works. Remember that our vision is based
    on multiple cortex levels, each one recognizing more and more structured information,
    still preserving the locality. First we see single pixels, then from that, we
    recognize simple geometric forms and then more and more sophisticated elements
    such as objects, faces, human bodies, animals and so on.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是我们大脑的工作方式。记住，我们的视觉是基于多个皮层层次的，每个层次识别越来越多的结构化信息，同时仍然保留局部性。首先，我们看到单个像素，然后从中识别简单的几何形状，再然后识别更多复杂的元素，如物体、面孔、人类身体、动物等等。
- en: 'In [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml), *Deep Learning
    with ConvNets*, we will see that a particular type of deep learning network known
    as **convolutional neural network** (**CNN**) has been developed by taking into
    account both the idea of preserving the spatial locality in images (and, more
    generally, in any type of information) and the idea of learning via progressive
    levels of abstraction: with one layer, you can only learn simple patterns; with
    more than one layer, you can learn multiple patterns. Before discussing CNN, we
    need to discuss some aspects of Keras architecture and have a practical introduction
    to a few additional machine learning concepts. This will be the topic of the next
    chapters.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)，*使用卷积神经网络进行深度学习*，我们将看到一种特定类型的深度学习网络，称为**卷积神经网络**（**CNN**），它通过考虑保留图像中的空间局部性（更一般地说，任何类型的信息）以及通过逐级抽象学习的思想发展而来：通过一层，你只能学习简单的模式；而通过多层，你可以学习多种模式。在讨论CNN之前，我们需要讨论Keras架构的一些方面，并对一些额外的机器学习概念做一个实际的介绍。接下来的章节将讨论这些内容。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the basics of neural networks, more specifically,
    what a perceptron is, what a multilayer perceptron is, how to define neural networks
    in Keras, how to progressively improve metrics once a good baseline is established,
    and how to fine-tune the hyperparameter's space. In addition to that, you now
    also have an intuitive idea of what some useful activation functions (sigmoid
    and ReLU) are, and how to train a network with backpropagation algorithms based
    on either gradient descent, on stochastic gradient descent, or on more sophisticated
    approaches, such as Adam and RMSprop.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了神经网络的基础知识，更具体地说，了解了感知机是什么、多层感知机是什么、如何在 Keras 中定义神经网络、如何在建立良好的基线后逐步改进指标，以及如何微调超参数空间。此外，您现在还对一些有用的激活函数（sigmoid
    和 ReLU）有了直观的理解，并了解了如何通过基于梯度下降、随机梯度下降或更复杂方法（如 Adam 和 RMSprop）的反向传播算法训练网络。
- en: In the next chapter, we will see how to install Keras on AWS, Microsoft Azure,
    Google Cloud, and on your own machine. In addition to that, we will provide an
    overview of Keras APIs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何在 AWS、微软 Azure、谷歌云以及您自己的机器上安装 Keras。此外，我们还将概述 Keras 的 API。
