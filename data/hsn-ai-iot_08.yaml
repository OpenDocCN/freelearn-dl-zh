- en: Distributed AI for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的分布式AI
- en: 'The advances in distributed computing environments and an easy availability
    of internet worldwide has resulted in the emergence of **Distributed Artificial
    Intelligence** (**DAI**). In this chapter, we will learn about two frameworks,
    one by Apache the **machine learning library **(**MLlib**), and another H2O.ai,
    both provide distributed and scalable **machine learning** (**ML**) for large,
    streaming data. The chapter will start with an introduction to Apache''s Spark,
    the de facto distributed data processing system. This chapter will cover the following
    topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算环境的进步和互联网的全球普及导致了**分布式人工智能**（**DAI**）的出现。在本章中，我们将了解两个框架，一个是Apache的**机器学习库**（**MLlib**），另一个是H2O.ai，它们都为大规模、流式数据提供分布式和可扩展的**机器学习**（**ML**）。本章将以Apache的Spark介绍开始，它是事实上的分布式数据处理系统。本章将涵盖以下主题：
- en: Spark and its importance in distributed data processing
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark及其在分布式数据处理中的重要性
- en: Understanding the Spark architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Spark架构
- en: Learning about MLlib
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习MLlib
- en: Using MLlib in your deep learning pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习管道中使用MLlib
- en: Delving deep into the H2O.ai platform
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解H2O.ai平台
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: IoT systems generate a lot of data; while in many cases it is possible to analyze
    the data at leisure, for certain tasks such as security, fraud detection, and
    so on, this latency is not acceptable. What we need in such a situation is a way
    to handle large data within a specified time—the solution—DAI, many machines in
    the cluster processing the big data (data parallelism) and/or training the deep
    learning models (model parallelism) in a distributed manner. There are many ways
    to perform DAI, and most of the approaches are built upon or around Apache Spark. Released
    in the year 2010 under the BSD licence, Apache Spark today is the largest open
    source project in big data. It helps the user to create a fast and general purpose
    cluster computing system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网（IoT）系统会生成大量数据；虽然在许多情况下可以从容分析数据，但对于某些任务（如安全、欺诈检测等），这种延迟是不可接受的。在这种情况下，我们需要的是一种在指定时间内处理大量数据的方法——解决方案是DAI，多个集群中的机器以分布式方式处理大数据（数据并行）和/或训练深度学习模型（模型并行）。有许多方式可以执行DAI，大多数方法是建立在或围绕Apache
    Spark的基础上。Apache Spark于2010年发布，并采用BSD许可协议，如今它是大数据领域最大的开源项目。它帮助用户创建一个快速且通用的集群计算系统。
- en: Spark runs on a Java virtual machine, making it possible to run it on any machine
    with Java installed, be it a laptop or a cluster. It supports a variety of programming
    languages including Python, Scala, and R. A large number of deep learning frameworks
    and APIs are built around Spark and TensorFlow to make the task of DAI easier,
    for example, **TensorFlowOnSpark** (**TFoS**), Spark MLlib, SparkDl, and Hydrogen
    Sparkling (a combination of H2O.ai and Spark).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark运行在Java虚拟机上，使得它可以在任何安装了Java的机器上运行，无论是笔记本电脑还是集群。它支持多种编程语言，包括Python、Scala和R。许多深度学习框架和API围绕Spark和TensorFlow构建，旨在简化分布式人工智能（DAI）任务，例如**TensorFlowOnSpark**（**TFoS**）、Spark
    MLlib、SparkDl和Hydrogen Sparkling（结合了H2O.ai和Spark）。
- en: Spark components
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark组件
- en: 'Spark uses master-slave architecture, with one central coordinator (called
    the **Spark driver**) and many distributed workers (called **Spark executors**).
    The driver process creates a `SparkContext` object and divides the user application
    into smaller execution units (tasks). These tasks are executed by the workers.
    The resources among the workers are managed by a **Cluster** **Manager**. The
    following diagram shows the workings of Spark:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用主从架构，其中一个中央协调器（称为**Spark驱动程序**）和多个分布式工作节点（称为**Spark执行器**）。驱动程序进程创建一个`SparkContext`对象，并将用户应用程序分解成更小的执行单元（任务）。这些任务由工作节点执行。工作节点之间的资源由**集群**
    **管理器**管理。下图展示了Spark的工作原理：
- en: '![](img/2b18990e-1d8c-4cb0-a01b-7e2c24272ba8.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b18990e-1d8c-4cb0-a01b-7e2c24272ba8.png)'
- en: Working of Spark
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的工作原理
- en: 'Let''s now go through the different components of Spark. The following diagram
    shows the basic components that constitute Spark:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐一了解Spark的不同组件。下图展示了构成Spark的基本组件：
- en: '![](img/ab3d925a-6e1b-426e-a3ba-9dbabe167138.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab3d925a-6e1b-426e-a3ba-9dbabe167138.png)'
- en: Components that constitute Spark
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 构成Spark的组件
- en: 'Let''s see, in brief, some of the components that we will be using in this
    chapter, as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要看看在本章中我们将使用的一些组件，如下所示：
- en: '**Resilient Distributed Datasets**: **Resilient Distributed Datasets** (**RDDs**)
    are the primary API in Spark. They represent an immutable, partitioned collection
    of data that can be operated in parallel. The higher APIs DataFrames and DataSets
    are built on top of RDDs.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**：**弹性分布式数据集**（**RDDs**）是Spark中的主要API。它们表示一个不可变的、分区的数据集合，可以并行操作。更高层的API如DataFrames和DataSets是建立在RDD之上的。'
- en: '**Distributed Variables**: Spark has two types of distributed variables: broadcast
    variables and accumulators. They are used by user-defined functions. Accumulators
    are used for aggregating the information from all the executors into a shared
    result. The broadcast variables, alternatively, are the variables that are shared
    throughout the cluster.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式变量**：Spark有两种类型的分布式变量：广播变量和累加器。它们由用户定义的函数使用。累加器用于将所有执行器中的信息聚合成共享结果。广播变量则是集群中共享的变量。'
- en: '**DataFrames**: It is a distributed collection of data, very much like the
    pandas DataFrame. They can read from various file formats and perform the operation on
    the entire DataFrame using a single command. They are distributed across the cluster.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataFrames**：它是一个分布式的数据集合，非常类似于pandas中的DataFrame。它们可以从各种文件格式中读取，并使用单个命令对整个DataFrame执行操作。它们分布在集群中。'
- en: '**Libraries**: Spark has built-in libraries for MLlib, and for working with
    graphs (GraphX). In this chapter, we will use MLlib and SparkDl that uses Spark
    framework. We will learn how to apply them to make ML predictions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库**：Spark内置了用于MLlib和图形处理（GraphX）的库。在本章中，我们将使用基于Spark框架的MLlib和SparkDl。我们将学习如何应用它们来进行机器学习预测。'
- en: Spark is a big topic, and it is beyond the scope of this book to give further
    details on Spark. We recommend the interested reader refer to the Spark documentation: [http://spark.apache.org/docs/latest/index.html](http://spark.apache.org/docs/latest/index.html).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个大话题，本书无法提供有关Spark的更多细节。我们建议感兴趣的读者参考Spark文档：[http://spark.apache.org/docs/latest/index.html](http://spark.apache.org/docs/latest/index.html)。
- en: Apache MLlib
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache MLlib
- en: 'Apache Spark MLlib provides a powerful computational environment for ML. It
    provides a distributed architecture on a large-scale basis, allowing one to run
    ML models more quickly and efficiently. That''s not all; it is open source with
    a growing and active community continuously working to improve and provide the
    latest features. It provides a scalable implementation of the popular ML algorithms.
    It includes algorithms for the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark MLlib为机器学习提供了强大的计算环境。它提供了一个大规模分布式架构，使得运行机器学习模型更加快速高效。这还不是全部；它是开源的，并且拥有一个不断壮大的活跃社区，持续改进并提供最新的功能。它提供了流行的机器学习算法的可扩展实现，涵盖以下算法：
- en: '**Classification**: Logistic regression, linear support vector machine, Naive
    Bayes'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：逻辑回归、线性支持向量机、朴素贝叶斯'
- en: '**Regression**: Generalized linear regression'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：广义线性回归'
- en: '**Collaborative filtering**: Alternating least square'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**：交替最小二乘法'
- en: '**Clustering**: K-means'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：K均值'
- en: '**Decomposition**: Singular value decomposition and principal component analysis'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分解**：奇异值分解和主成分分析'
- en: It has proved to be faster than Hadoop MapReduce. We can write applications in
    Java, Scala, R, or Python. It can also be easily integrated with TensorFlow.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它已被证明比Hadoop MapReduce更快。我们可以使用Java、Scala、R或Python编写应用程序。它还可以轻松与TensorFlow集成。
- en: Regression in MLlib
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib中的回归
- en: 'Spark MLlib has built-in methods for regression. To be able to use the built-in
    methods of Spark, you will have to install `pyspark` on your cluster (standalone
    or distributed cluster). The installation can be done using the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib内置了回归的方法。为了能够使用Spark的内置方法，您需要在您的集群（独立集群或分布式集群）上安装`pyspark`。可以使用以下方式进行安装：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The MLlib library has the following regression methods:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib库具有以下回归方法：
- en: '**Linear regression**: We already learned about linear regression in earlier
    chapters; we can use this method using the `LinearRegression`  class defined at
    `pyspark.ml.regression`. By default, it uses minimized squared error with regularization.
    It supports L1 and L2 regularization, and a combination of them.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性回归**：我们在前面的章节中已经学习了线性回归；我们可以使用在`pyspark.ml.regression`中定义的`LinearRegression`类来使用这种方法。默认情况下，它使用最小化平方误差和正则化。它支持L1和L2正则化以及它们的组合。'
- en: '**Generalized linear regression**: The Spark MLlib has a subset of exponential
    family distributions like Gaussian, Poissons, and so on. The regression is instantiated
    using the class `GeneralizedLinearRegression`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广义线性回归**：Spark MLlib提供了指数族分布的子集，如高斯分布、泊松分布等。回归是通过`GeneralizedLinearRegression`类实例化的。'
- en: '**Decision tree regression**: The `DecisionTreeRegressor` class can be used
    to make a prediction using decision tree regression.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树回归**：可以使用`DecisionTreeRegressor`类进行决策树回归预测。'
- en: '**Random forest regression**: One of the popular ML methods, they are defined
    in the `RandomForestRegressor` class.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林回归**：作为一种流行的机器学习方法，它们在`RandomForestRegressor`类中定义。'
- en: '**Gradient boosted tree regression**: We can use an ensemble of decision trees
    using the `GBTRegressor` class.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升树回归**：我们可以使用`GBTRegressor`类来使用决策树的集成方法。'
- en: Besides, the MLlib also has support for survival regression and isotonic regression
    using the `AFTSurvivalRegression` and `IsotonicRegression` classes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MLlib还支持使用`AFTSurvivalRegression`和`IsotonicRegression`类进行生存回归和等温回归。
- en: 'With the help of these classes, we can build a ML model for regression (or
    classification as you will see in next section) in as little as 10 lines of code.
    The basic steps are outlined as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 借助这些类，我们可以在不到10行代码的情况下构建回归（或如下一节所示的分类）机器学习模型。基本步骤如下：
- en: Build a Spark session
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建Spark会话。
- en: 'Implement the data-loading pipeline: load the data file, specify the format,
    and read it into Spark DataFrames'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现数据加载管道：加载数据文件，指定格式，并将其读取到Spark DataFrame中。
- en: Identify the features to be used as input and as the target (optionally split
    dataset in train/test)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定要用作输入的特征以及目标（可选择将数据集拆分为训练集/测试集）。
- en: Instantiate the desired class object
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化所需的类对象。
- en: Use the `fit()` method with training dataset as an argument
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法并将训练数据集作为参数。
- en: Depending upon the regressor chosen, you can see the learned parameters and
    evaluate the fitted model
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据所选择的回归器，你可以查看学习到的参数并评估拟合的模型。
- en: 'Let''s use linear regression for the Boston house price prediction dataset
    ([https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)),
    where we have the dataset in `csv` format:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用线性回归模型来预测波士顿房价数据集（[https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)），数据集为`csv`格式：
- en: 'Import the necessary modules. We will be using `LinearRegressor` for defining
    the linear regression class, `RegressionEvaluator` to evaluate the model after
    training, `VectorAssembler` to combine features as one input vector, and `SparkSession` to
    start the Spark session:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块。我们将使用`LinearRegressor`来定义线性回归类，使用`RegressionEvaluator`来评估训练后的模型，使用`VectorAssembler`来将特征合并为一个输入向量，使用`SparkSession`来启动Spark会话：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, start a Spark session using `SparkSession` class as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`SparkSession`类启动一个Spark会话，代码如下：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s now read the data; we first load the data from the given path, define
    the format we want to use, and finally, read it into Spark DataFrames, as follows:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来读取数据；首先从给定路径加载数据，定义我们要使用的格式，最后将其读取到Spark DataFrame中，具体如下：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can see the DataFrame now loaded in the memory, and its structure, shown
    in the following screenshot:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到现在内存中加载的DataFrame及其结构，见下图：
- en: '![](img/6ec3ba28-bb23-4336-a1fc-d38953ce1723.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ec3ba28-bb23-4336-a1fc-d38953ce1723.png)'
- en: 'Like pandas DataFrames, Spark DataFrames can also be processed with a single
    command. Let''s gain a little more insight into our dataset as seen in the following
    screenshot:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于pandas DataFrame，Spark DataFrame也可以通过单一命令进行处理。让我们通过以下截图来进一步了解我们的数据集：
- en: '![](img/316109bd-4504-4877-b9b8-09d4c053b456.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/316109bd-4504-4877-b9b8-09d4c053b456.png)'
- en: 'Next, we define the features we want to use for training; to do this, we make
    use of the `VectorAssembler` class. We define the columns from the `house_df` DataFrame
    to be combined together as an input feature vector and corresponding output prediction
    (similar to defining `X_train`, `Y_train`), and then perform the corresponding
    transformation, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义要用于训练的特征；为此，我们使用`VectorAssembler`类。我们定义来自`house_df` DataFrame的列，将其合并为输入特征向量和相应的输出预测（类似于定义`X_train`、`Y_train`），然后执行相应的转换，具体如下：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/0c1de199-e1b3-4e5f-9f54-4a3b79e57c5d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c1de199-e1b3-4e5f-9f54-4a3b79e57c5d.png)'
- en: 'The dataset is then split into train/test datasets, shown in the following
    code:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集随后被分割为训练/测试数据集，代码如下所示：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have our dataset ready, we instantiate the `LinearRegression` class
    and fit it for the training dataset, as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据集已经准备好，我们实例化`LinearRegression`类，并将其拟合到训练数据集上，如下所示：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can obtain the result coefficients of linear regression, as follows:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以获得线性回归的结果系数，如下所示：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/1446b664-709e-4413-883e-4337b7bf15f5.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1446b664-709e-4413-883e-4337b7bf15f5.png)'
- en: 'The model provides an RMSE value of `4.73` and an `r2` value of `0.71` on the
    training dataset in `21` iterations:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型在训练数据集上提供了RMSE值为`4.73`，`r2`值为`0.71`，共进行了`21`次迭代：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we evaluate our model on the test dataset; we obtain an RMSE of `5.55`
    and R2 value of `0.68`:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在测试数据集上评估我们的模型；我们得到的RMSE为`5.55`，R2值为`0.68`：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once the work is done, you should stop the Spark session using the `stop()`
    method. The complete code is available in `Chapter08/Boston_Price_MLlib.ipynb`.
    The reason for a low `r2` value and high RMSE is that we have considered all the
    features in the training dataset as an input feature vector, and many of them
    play no significant role in determining the house price. Try reducing the features,
    keeping the ones that have a high correlation with the price.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦工作完成，你应该使用`stop()`方法停止Spark会话。完整代码可以在`Chapter08/Boston_Price_MLlib.ipynb`中找到。`r2`值较低和RMSE较高的原因是我们考虑了训练数据集中的所有特征作为输入特征向量，而其中许多特征对房价的预测没有显著作用。尝试减少特征，保留与价格高度相关的特征。
- en: Classification in MLlib
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib中的分类
- en: MLlib also offers a wide range of classifiers; it provides both binomial and
    multinomial logistic regressor. The decision tree classifier, random forest classifier,
    gradient-boosted tree classifier, multilayered perceptron classifier, linear support
    vector machine classifier, and Naive Bayes classifier are supported. Each of them
    is defined in its class; for details, refer to [https://spark.apache.org/docs/2.2.0/ml-classification-regression.html](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#naive-bayes).
    The basic steps remain the same as we learned in the case of regression; the only
    difference is now, instead of RMSE or r2 metrics, the models are evaluated on
    accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib还提供了多种分类器；它提供了二项和多项逻辑回归分类器。决策树分类器、随机森林分类器、梯度提升树分类器、多层感知器分类器、线性支持向量机分类器和朴素贝叶斯分类器都得到了支持。每个分类器都在其各自的类中定义；有关详细信息，请参阅[https://spark.apache.org/docs/2.2.0/ml-classification-regression.html](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#naive-bayes)。基本步骤与我们在回归案例中学到的相同；唯一的区别是，现在，模型评估的标准是准确率，而不是RMSE或r2。
- en: 'This section will treat you to the wine quality classification problem implemented
    using Spark MLlib logistic regression classifier:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将为你展示如何使用Spark MLlib的逻辑回归分类器实现葡萄酒质量分类问题：
- en: For this classification problem, we will use logistic regression available through
    the `LogisticRegressor` class. The `VectorAssembler`, like in the previous example,
    will be used to combine the input features as one vector. In the wine quality
    dataset we have seen ([Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml), *Principles
    and Foundations of IoT and AI*), the quality was an integer number given between
    0–10, and we needed to process it. Here, we will process using `StringIndexer`.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个分类问题，我们将使用通过`LogisticRegressor`类提供的逻辑回归。像之前的例子一样，`VectorAssembler`将用于将输入特征合并为一个向量。在我们之前看到的葡萄酒质量数据集中（[第1章](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml)，*物联网与人工智能的原理与基础*），质量是一个介于0到10之间的整数，我们需要对其进行处理。在这里，我们将使用`StringIndexer`来处理。
- en: 'One of the great features of Spark is that we can define all the preprocessing
    steps as a pipeline. This becomes very useful when there are a large number of
    preprocessing steps. Here, we have only two preprocessing steps, but just to showcase
    how pipelines are formed, we will make use of the `Pipeline` class. We import
    all these modules as our first step and create a Spark session, shown in the following
    code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的一个伟大特点是我们可以将所有的预处理步骤定义为一个管道。当有大量的预处理步骤时，这非常有用。这里我们只有两个预处理步骤，但为了展示如何形成管道，我们将使用`Pipeline`类。我们将所有这些模块导入作为第一步，并创建一个Spark会话，代码如下所示：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will load and read the `winequality-red.csv` data file, as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载并读取`winequality-red.csv`数据文件，如下所示：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We process the `quality` label in the given dataset, and split it into three
    different classes, and add it to the existing Spark DataFrame as a new `quality_new` column,
    shown in the following code:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们处理给定数据集中的`quality`标签，将其拆分为三个不同的类别，并将其作为新的`quality_new`列添加到现有的Spark DataFrame中，代码如下所示：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Though the modified quality, `quality_new` is an integer already, and we can
    use it directly as our label. In this example, we have added `StringIndexer` to
    convert it into numeric indices for the purpose of illustration. One can use `StringIndexer`
    to convert string labels to numeric indices. We also use `VectorAssembler` to
    combine the columns into one feature vector. The two stages are combined together
    using `Pipeline`, as follows:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管修改后的质量`quality_new`已经是一个整数，我们可以直接将其用作标签。在这个例子中，我们添加了`StringIndexer`将其转换为数字索引，目的是为了说明。你可以使用`StringIndexer`将字符串标签转换为数字索引。我们还使用`VectorAssembler`将各列组合成一个特征向量。两个阶段通过`Pipeline`结合在一起，如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The data obtained after the pipeline is then split into training and testing
    datasets, shown in the following code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流水线处理之后得到的数据随后被拆分成训练数据集和测试数据集，如以下代码所示：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we instantiate the `LogisticRegressor` class and train it on the training
    dataset using the `fit` method, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实例化`LogisticRegressor`类，并使用`fit`方法在训练数据集上进行训练，如下所示：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the following screenshot, we can see the model parameters learned:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下截图中，我们可以看到学习到的模型参数：
- en: '![](img/7e2b0f57-da14-409a-ac9a-6eca299087e6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e2b0f57-da14-409a-ac9a-6eca299087e6.png)'
- en: 'The accuracy of the model is 94.75%. We can also see other evaluation metrics
    like `precision` and `recall`, F measure, true positive rate, and false positive
    rate in the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的准确率为 94.75%。我们还可以在以下代码中看到其他评估指标，如`precision`（精确度）、`recall`（召回率）、F 值、真阳性率和假阳性率：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see that the performance of the wine quality classifier using MLlib is
    comparable to our earlier approaches. The complete code is available in the GitHub
    repository under `Chapter08/Wine_Classification_MLlib.pynb`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用 MLlib 的葡萄酒质量分类器的性能与我们之前的方法相当。完整代码可以在 GitHub 仓库中的`Chapter08/Wine_Classification_MLlib.pynb`文件找到。
- en: Transfer learning using SparkDL
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SparkDL 进行迁移学习
- en: The previous sections elaborated how you can use the Spark framework with its
    MLlib for ML problems. In most complex tasks, however, deep learning models provide
    better performance. Spark supports SparkDL, a higher-level API working over MLlib.
    It uses TensorFlow at its backend, and it also requires TensorFrames, Keras, and
    TFoS modules.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面几节讲解了如何使用 Spark 框架和其 MLlib 处理机器学习问题。然而，在大多数复杂任务中，深度学习模型提供了更好的性能。Spark 支持 SparkDL，这是一个在
    MLlib 上运行的更高级 API。它在后台使用 TensorFlow，并且还需要 TensorFrames、Keras 和 TFoS 模块。
- en: In this section, we will make use of SparkDL for classifying images. This will
    allow you to get acquainted with the Spark support for the images. For images,
    as we learned in [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep
    Learning for IoT*, **Convolutional Neural Networks** (**CNNs**) are the de facto
    choice. In [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep Learning
    for IoT*,we built CNNs from scratch, and also learned about some popular CNN architectures.
    A very interesting property of CNNs is that each convolutional layer learns to
    identify different features from the image, which is they act as feature extractors.
    The lower convolutional layers filter out basic shapes like lines and circles,
    while higher layers filter more abstract shapes. This property can be used to
    employ a CNN trained on one set of images to classify another set of similar domain
    images by just changing the top fully connected layers. This technique is called
    **transfer learning**. Depending upon the availability of new dataset images and
    similarity between the two domains, transfer learning can significantly help in
    reducing the training time and need for large datasets.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 SparkDL 进行图像分类。这将帮助你熟悉 Spark 对图像的支持。对于图像，正如我们在[第 4 章](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml)《物联网的深度学习》中学到的，**卷积神经网络**（**CNNs**）是事实上的选择。在[第
    4 章](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml)《物联网的深度学习》中，我们从头开始构建了 CNN，并且还了解了一些流行的
    CNN 架构。CNN 的一个非常有趣的特点是，每个卷积层都学习识别图像中的不同特征，起到特征提取器的作用。较低的卷积层会过滤出基本形状，比如线条和圆形，而较高的层则过滤出更抽象的形状。这个特点可以用来利用在一组图像上训练的
    CNN 来分类另一组类似领域的图像，只需要更改顶部的全连接层。这种技术被称为**迁移学习**。根据新数据集图像的可用性以及两个领域之间的相似性，迁移学习可以显著帮助减少训练时间，并且减少对大规模数据集的需求。
- en: In the NIPS 2016 tutorial, Andrew Ng, one of the key figures in the AI field,
    said that *transfer learning will be the next driver for commercial success*.
    In the image domain, great success in transfer learning has been achieved using
    CNNs trained in ImageNet data for classifying images on other domains. A lot of
    research is being carried out in applying transfer learning to other data domains.
    You can get a primer on *Transfer Learning* from this blog post by Sebastian Ruder: [http://ruder.io/transfer-learning/](http://ruder.io/transfer-learning/).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年NIPS教程中，AI领域的关键人物之一Andrew Ng提到，*迁移学习将是下一个商业成功的推动力*。在图像领域，通过使用在ImageNet数据上训练的CNN进行迁移学习，已经在其他领域的图像分类中取得了巨大的成功。目前，许多研究正在致力于将迁移学习应用于其他数据领域。您可以通过Sebastian
    Ruder的这篇博客文章了解*迁移学习*的基础：[http://ruder.io/transfer-learning/](http://ruder.io/transfer-learning/)。
- en: We will employ InceptionV3, a CNN architecture proposed by Google ([https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)),
    trained on the ImageNet dataset ([http://www.image-net.org](http://www.image-net.org))
    to identify vehicles on roads (at present we restrict ourselves to only buses
    and cars).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用Google提出的CNN架构InceptionV3（[https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)），并使用在ImageNet数据集（[http://www.image-net.org](http://www.image-net.org)）上训练的模型，来识别道路上的车辆（目前我们仅限于公交车和小汽车）。
- en: 'Before we can start, ensure that the following modules are installed in your
    working environment:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保以下模块已安装在您的工作环境中：
- en: PySpark
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark
- en: TensorFlow
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Keras
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras
- en: TFoS
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFoS
- en: TensorFrames
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFrames
- en: Wrapt
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wrapt
- en: Pillow
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pillow
- en: pandas
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: Py4J
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Py4J
- en: SparkDL
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkDL
- en: Kafka
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka
- en: Jieba
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jieba
- en: These can be installed using the `pip install` command on your standalone machine
    or machines in the cluster.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以通过`pip install`命令在您的独立机器或集群中的机器上安装。
- en: 'Next you will learn how to use Spark and SparkDL for image classification.
    We have taken screenshots of two different flowers, daisies and tulips, using
    Google image search; there are 42 images of daisies and 65 images of tulips. In
    the following screenshot, you can see the sample screenshots of the daisies:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习如何使用Spark和SparkDL进行图像分类。我们通过Google图像搜索截图了两种不同的花朵，雏菊和郁金香；雏菊有42张图片，郁金香有65张图片。在下面的截图中，您可以看到雏菊的样本截图：
- en: '![](img/47e5e4e6-3443-4225-a3e7-823cc396503e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47e5e4e6-3443-4225-a3e7-823cc396503e.png)'
- en: 'The following screenshot shows the sample images of tulips:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了郁金香的样本图片：
- en: '![](img/99141d8d-f0cd-46b7-9bf0-f20d83a22267.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99141d8d-f0cd-46b7-9bf0-f20d83a22267.png)'
- en: 'Our dataset is too small, and hence if we make a CNN from scratch, it will
    not be able to give any useful performance. In cases like these, we can make use
    of transfer learning. The SparkDL module provides an easy and convenient way to
    use pre-trained models with the help of the class `DeepImageFeaturizer`. It supports
    the following CNN models (pre-trained on the ImageNet dataset ([http://www.image-net.org](http://www.image-net.org)):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集太小，因此如果从头开始构建CNN，它无法提供任何有用的性能。在这种情况下，我们可以利用迁移学习。SparkDL模块提供了一个简单方便的方式，通过`DeepImageFeaturizer`类使用预训练模型。它支持以下CNN模型（在ImageNet数据集（[http://www.image-net.org](http://www.image-net.org)）上预训练）：
- en: InceptionV3
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InceptionV3
- en: Xception
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception
- en: ResNet50
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet50
- en: VGG16
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG16
- en: VGG19
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG19
- en: 'We will use Google''s InceptionV3 as our base model. The complete code can
    be accessed from the GitHub repository under `Chapter08/Transfer_Learning_Sparkdl.ipynb`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Google的InceptionV3作为我们的基础模型。完整的代码可以从GitHub仓库中的`Chapter08/Transfer_Learning_Sparkdl.ipynb`访问：
- en: 'In the first step, we will need to specify the environment for the SparkDL
    library. It is an important step; without it, the kernel will not know from where
    the SparkDL packages are to be loaded:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步中，我们需要为SparkDL库指定环境。这是一个重要的步骤；如果没有它，内核将无法知道从哪里加载SparkDL包：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Even when you install SparkDL using `pip` on some OSes, it is required that
    you specify the OS environment or SparkDL.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在某些操作系统上通过`pip`安装SparkDL时，仍需要指定操作系统环境或SparkDL。
- en: 'Next, let''s initiate a `SparkSession`, shown in the following code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们启动一个`SparkSession`，如下所示的代码：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We import the necessary modules and read the data images. Along with reading
    the image paths, we also assign the labels to each image in the Spark DataFrame,
    as follows:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的模块并读取数据图像。除了读取图像路径，我们还将标签分配给Spark DataFrame中的每个图像，如下所示：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, you can see the top five rows of the two DataFrames. The first column
    contains the path of each image, and the column shows its label (whether it belongs
    to daisy (label 1) or it belongs to tulips (label 0)):'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以看到两个数据框的前五行。第一列包含每张图像的路径，第二列显示其标签（是否属于雏菊（标签1）或属于郁金香（标签0））：
- en: '![](img/99b1dae8-6d59-4f48-92e9-e94b553da9a2.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99b1dae8-6d59-4f48-92e9-e94b553da9a2.png)'
- en: 'We split the two image dataset into training and testing set (it is always
    a good practice), using the `randomSplit` function. Conventionally, people choose
    a test-train split of 60%—40%, 70%—30%, or 80%—20%. We have chosen a 70%—30% split
    here. For the purpose of training, we then combine the training images of both
    flowers in the `trainDF` DataFrame and test dataset images in the `testDF` DataFrame,
    as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`randomSplit`函数将两个图像数据集划分为训练集和测试集（这始终是一种良好的做法）。通常，人们选择60%—40%、70%—30%或80%—20%的训练-测试数据集比例。我们在这里选择了70%—30%的划分。为了训练，我们将两种花的训练图像合并到`trainDF`数据框中，将测试数据集图像合并到`testDF`数据框中，如下所示：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we build the pipeline with `InceptionV3` as the feature extractor followed
    by a logistic regressor classifier. We use the `trainDF` DataFrame to train the
    model:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`InceptionV3`作为特征提取器，后接逻辑回归分类器来构建管道。我们使用`trainDF`数据框来训练模型：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s now evaluate our trained model on the test dataset. We can see that,
    on the test dataset, we get an accuracy of 90.32% using the following code:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们在测试数据集上评估训练好的模型。我们可以看到，在测试数据集上，使用以下代码我们得到了90.32%的准确率：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is the confusion matrix for the two classes:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是两个类别的混淆矩阵：
- en: '![](img/884fed21-8f17-44bc-8c0d-a1c3b39a3bcd.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/884fed21-8f17-44bc-8c0d-a1c3b39a3bcd.png)'
- en: In fewer than 20 lines of code, we were able to train the model and obtain a
    good 90.32% accuracy. Remember, here the dataset used is raw; by increasing the
    dataset images, and filtering out low-quality images, you can improve the performance
    of your model. You can learn more about the deep learning library SparkDL from
    the official GitHub repository: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到20行代码的情况下，我们成功训练了模型并获得了90.32%的良好准确率。请记住，这里使用的数据集是原始数据；通过增加数据集中的图像数量，并过滤掉低质量的图像，你可以提高模型的表现。你可以从官方GitHub仓库了解更多关于深度学习库SparkDL的内容：[https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)。
- en: Introducing H2O.ai
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍H2O.ai
- en: H2O is a fast, scalable ML and deep learning framework developed by H2O.ai,
    released under the open source Apache license. According to the company-provided
    details, more than 9,000 organizations and 80,000+ data scientists use H2O for
    their ML/deep learning needs. It uses in-memory compression, which allows it to
    handle a large amount of data in memory, even with a small cluster of machines.
    It has an interface for R, Python, Java, Scala, and JavaScript, and even has a
    built-in web interface. H2O can run in standalone mode, and on Hadoop or Spark
    cluster.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: H2O是由H2O.ai开发的一个快速、可扩展的机器学习和深度学习框架，采用开源Apache许可证发布。根据公司提供的详细信息，超过9000个组织和80,000+名数据科学家使用H2O来满足他们的机器学习/深度学习需求。它使用内存压缩技术，即使在一个小型的机器集群中，也能处理大量数据。它支持R、Python、Java、Scala和JavaScript接口，甚至还内置了一个Web界面。H2O可以在独立模式下运行，也可以在Hadoop或Spark集群中运行。
- en: H2O includes a large number of ML algorithms like generalized linear modeling,
    Naive Bayes, random forest, gradient boosting, and deep learning algorithms. The
    best part of H2O is that one can build thousands of models, compare the results,
    and even do hyperparameter tuning with a few lines of codes. H2O also has better
    data preprocessing tools.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: H2O包括大量的机器学习算法，如广义线性模型、朴素贝叶斯、随机森林、梯度提升和深度学习算法。H2O的最佳特点是，用户可以在几行代码中构建成千上万个模型，比较结果，甚至进行超参数调优。H2O还拥有更好的数据预处理工具。
- en: 'H2O requires Java, so, ensure that Java is installed on your system. You can
    install H2O to work in Python using `PyPi`, shown in the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: H2O需要Java，因此，请确保你的系统上已安装Java。你可以使用`PyPi`安装H2O以在Python中使用，以下是安装代码：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: H2O AutoML
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O AutoML
- en: One of the most exciting features of H2O is **AutoML**, the automatic ML. It
    is an attempt to develop a user-friendly ML interface that can be used by non-experts.
    H2O AutoML automates the process of training and tuning a large selection of candidate
    models. Its interface is designed so that users just need to specify their dataset,
    input and output features, and any constraints they want on the number of total
    models trained, or time constraint. The rest of the work is done by AutoML itself;
    in the specified time constraint, it identifies the best performing models, and
    provides a leaderboard. It has been observed that, usually, the Stacked Ensemble
    model, the ensemble of all the previously trained models, occupies the top position
    on the leaderboard. There is a large number of options that advanced users can
    use; details of these options and their various features are available at[ http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: H2O最令人兴奋的功能之一是**AutoML**，自动化机器学习。它旨在开发一个易于使用的机器学习接口，可以供非专业人士使用。H2O AutoML自动化了训练和调优大量候选模型的过程。其界面设计如此简便，用户只需要指定数据集、输入输出特征以及他们希望在训练的总模型数量或时间限制方面设置的任何约束条件。其余的工作由AutoML本身完成；在指定的时间限制内，它会识别出表现最好的模型，并提供一个排行榜。通常，已训练的所有模型的集成模型——堆叠集成模型，会占据排行榜的顶端。对于高级用户，有大量选项可供使用；这些选项的详细信息以及它们的各种功能可以在[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)上找到。
- en: 'To know more about H2O you can visit their website: [http://h2o.ai](http://h2o.ai).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于H2O的信息，您可以访问他们的网站：[http://h2o.ai](http://h2o.ai)。
- en: Regression in H2O
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O中的回归
- en: 'We will first show how regression can be done in H2O. We will use the same
    dataset as we used earlier with MLlib, the Boston house prices, and predict the
    cost of the houses. The complete code can be found at GitHub: `Chapter08/boston_price_h2o.ipynb`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示如何在H2O中进行回归。我们将使用与之前在MLlib中使用的相同数据集，即波士顿房价数据集，来预测房屋的价格。完整的代码可以在GitHub上找到：`Chapter08/boston_price_h2o.ipynb`：
- en: 'The necessary modules for the task are as follows:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成该任务所需的模块如下：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After importing the necessary modules, the first step is starting an `h2o`
    server. We do this using the `h2o.init()` command. It checks for any existing
    `h20` instances first, and if none are available, it will start one. There is
    also the possibility of connecting to an existing cluster by specifying the IP
    address and the port number as arguments to the `init()` function. In the following
    screenshot, you can see the result of `init()` on the standalone system:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在导入必要的模块后，第一步是启动一个`h2o`服务器。我们通过`h2o.init()`命令来完成这一操作。它首先检查是否已有现有的`h2o`实例，如果没有，它将启动一个新的实例。还可以通过指定IP地址和端口号作为`init()`函数的参数来连接到现有集群。在下面的截图中，您可以看到在独立系统上执行`init()`的结果：
- en: '![](img/53f4cd2a-ca1f-4c61-b9f7-3b6252b263fd.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53f4cd2a-ca1f-4c61-b9f7-3b6252b263fd.png)'
- en: 'Next, we read the data file using the `h20` `import_file` function. It loads
    it into an H2O DataFrame, which can be processed just as easily as the panda''s
    DataFrame. We can find the correlation among the different input features in the `h20`
    DataFrame very easily using the `cor()` method:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`h2o`的`import_file`函数读取数据文件。它将文件加载到H2O数据框中，可以像使用pandas数据框一样轻松处理。我们可以很容易地使用`cor()`方法找到`h2o`数据框中不同输入特征之间的相关性：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the output of correlation map among different features of
    the Boston house price dataset:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是波士顿房价数据集中不同特征之间相关性图的输出：
- en: '![](img/4034fa0e-2e34-41ca-abd7-67bd6550ba9a.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4034fa0e-2e34-41ca-abd7-67bd6550ba9a.png)'
- en: 'Now, as usual, we split the dataset into training, validation, and test datasets.
    Define the features to be used as input features (`x`):'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，和往常一样，我们将数据集分为训练集、验证集和测试集。定义要作为输入特征使用的特征（`x`）：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once this work is done, the process is very simple. We just instantiate the
    regression model class available from the H2O library, and use `train()` with
    the training and validation datasets as arguments. In the `train` function, we
    also specify what are the input features (`x`) and the output features (`y`).
    In the present case, we are taking all the features available to us as input features
    and the house price `medv` as the output feature. We can see the features of the
    trained model by just using a print statement. Next, you can see the model declaration
    for a generalized linear regression model, and its result after training on both
    training and validation datasets:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成这项工作后，过程非常简单。我们只需实例化来自H2O库的回归模型类，并使用`train()`函数，传入训练集和验证集数据作为参数。在`train`函数中，我们还需要指定输入特征（`x`）和输出特征（`y`）。在本例中，我们将所有可用的特征作为输入特征，房价`medv`作为输出特征。通过简单的打印语句，我们可以查看训练后模型的特征。接下来，你可以看到一个广义线性回归模型的声明及其在训练集和验证集上的训练结果：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/05ca2730-9046-4226-9f2a-0a3fde5fc9c4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05ca2730-9046-4226-9f2a-0a3fde5fc9c4.png)'
- en: 'After training, the next step is checking the performance on the test dataset,
    which can be easily done using the `model_performance()` function. We can also
    pass it to any of the datasets: the train, validation, test, or some new similar
    dataset:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后，下一步是检查在测试数据集上的表现，这可以通过`model_performance()`函数轻松完成。我们也可以将其应用于任何数据集：训练集、验证集、测试集或某些新的类似数据集：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/37608bc5-7c4d-45e8-891e-00fbcf07a17d.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37608bc5-7c4d-45e8-891e-00fbcf07a17d.png)'
- en: 'If we want to use gradient boost estimator regression, or random forest regression,
    we will instantiate the respective class object; the following steps will remain
    the same. What will vary is the output parameters; in the case of gradient boost
    estimator and random forest, we will also learn the relative importance of the
    different input features:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想使用梯度提升估计回归模型或随机森林回归模型，我们将实例化相应的类对象；接下来的步骤保持不变。不同的是输出参数；在梯度提升估计器和随机森林回归的情况下，我们还会学习不同输入特征的相对重要性：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The most difficult part of machine and deep learning is choosing the right
    hyperparameters. In H2O, the task becomes quite easy with the help of its `H2OGridSearch`
    class. The following code snippet performs the grid search on the hyperparameter
    depth for the gradient boost estimator defined previously:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习和深度学习中最难的部分是选择正确的超参数。在H2O中，通过其`H2OGridSearch`类，任务变得相当简单。以下代码片段在之前定义的梯度提升估计器上执行网格搜索，搜索超参数深度：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The best part of H2O is using AutoML to find the best models automatically.
     Let''s ask it to search for us among the 10 models, with the constraint on time
    being 100 seconds. AutoML will, with these parameters, build 10 different models,
    excluding the Stacked Ensembles. It will run, at the most, for 100 seconds before
    training the final Stacked Ensemble models:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H2O的最佳部分是使用AutoML自动寻找最佳模型。让我们要求它在10个模型中为我们搜索，并且将时间限制为100秒。AutoML将在这些参数下构建10个不同的模型，不包括堆叠集成模型。它将在最多100秒内运行，最终训练出堆叠集成模型：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The leaderboard for our regression task is as follows:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们回归任务的排行榜如下：
- en: '![](img/63da4e07-dad8-4b77-b3a0-a6935ec13e70.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63da4e07-dad8-4b77-b3a0-a6935ec13e70.png)'
- en: Different models in the leaderboard can be accessed using their respective `model_id`.
    The best model is accessed with the leader parameter. In our case, `aml.leader`
    represents the best model, the Stacked Ensemble of all the models. We can save
    the best model using the `h2o.save_model` function in either binary or MOJO format.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 排行榜中的不同模型可以通过各自的`model_id`进行访问。最佳模型通过leader参数进行访问。在我们的例子中，`aml.leader`代表最佳模型，即所有模型的堆叠集成模型。我们可以使用`h2o.save_model`函数将最佳模型保存为二进制格式或MOJO格式。
- en: Classification in H20
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O中的分类
- en: 'The same models can be used for classification in H2O, with only one change;
    we will need to change the output features from numeric values to categorical
    values using the `asfactor()` function. We will perform the classification on
    the quality of red wine, and use our old red wine database ([Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml),
    *Machine Learning for IoT*). We will need to import the same modules and initiate
    the H2O server. The full code is available at in the `Chapter08/wine_classification_h2o.ipynb` file:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的模型可以用于H2O中的分类，只需做一个改动；我们需要使用`asfactor()`函数将输出特征从数值型转为类别型。我们将对红酒的质量进行分类，并使用我们之前的红酒数据库（[第3章](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml)，*物联网中的机器学习*）。我们需要导入相同的模块并启动H2O服务器。完整代码可在`Chapter08/wine_classification_h2o.ipynb`文件中找到：
- en: 'Here is the code to import the necessary modules and initiate the H2O server:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是导入必要模块并启动H2O服务器的代码：
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The next step is to read the data file. We modify the output feature first
    to account for two classes (good wine and bad wine) and then convert it to a categorical
    variable using the `asfactor()` function. This is an important step in H2O; since
    we are using the same class objects for both regression and classification, they
    require the output label to be numeric in the case of regression, and categorical
    in the case of classification, as seen in the code block:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是读取数据文件。我们首先修改输出特征以适应两个类别（好酒和坏酒），然后使用`asfactor()`函数将其转换为类别变量。这是H2O中的一个重要步骤；因为我们在回归和分类中使用相同的类对象，它们要求回归时输出标签为数值型，分类时输出标签为类别型，如代码块所示：
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, split the data into training, validation, and testing datasets. We feed
    the training and validation datasets to the generalized linear estimator, with
    one change; we specify the `family=binomial` argument because here, we have only
    two categorical classes, good wine or bad wine. If you have more than two classes
    use `family=multinomial`. Remember, specifying the argument is optional; H2O automatically
    detects the output feature:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将数据集分为训练集、验证集和测试集。我们将训练集和验证集输入到广义线性估计器中，唯一的改动是；我们需要指定`family=binomial`参数，因为这里我们只有两类分类：好酒和坏酒。如果你有超过两个类别，使用`family=multinomial`。记住，指定这个参数是可选的；H2O会自动检测输出特征：
- en: '[PRE34]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After being trained, you can see the model performance on all the performance
    metrics: accuracy, precision, recall, F1 measure, and AUC, even the confusion
    metrics. You can get them for all the three datasets (training, validation, and
    testing). The following are the metrics obtained for the test dataset from the
    generalized linear estimator:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过训练后，你可以查看模型在所有性能指标上的表现：准确率、精确度、召回率、F1值和AUC，甚至是混淆矩阵。你可以获取三种数据集（训练集、验证集和测试集）的所有指标。以下是从广义线性估计器获取的测试数据集的指标：
- en: '![](img/485b7663-f7da-4308-aacc-2c9eef924c7f.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/485b7663-f7da-4308-aacc-2c9eef924c7f.png)'
- en: '![](img/9c6bc796-6352-41ab-bf64-9259a084c731.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c6bc796-6352-41ab-bf64-9259a084c731.png)'
- en: 'Without changing anything else in the previous code, we can perform hyper tuning
    and use H2O''s AutoML to get the better model:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不改变前面的代码的情况下，我们可以进行超参数调优并使用H2O的AutoML来获得更好的模型：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/62356947-6833-41e5-9c11-5a6a2947e263.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62356947-6833-41e5-9c11-5a6a2947e263.png)'
- en: We see that, for wine quality classification, the best model is XGBoost.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在红酒质量分类中，最佳模型是XGBoost。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: With the ubiquitous status of IoT, the data being generated is growing at an
    exponential rate. This data, mostly unstructured and available in vast quantities,
    is often referred to as big data. A large number of frameworks and solutions have
    been proposed to deal with the large set of data. One of the promising solutions
    is DAI, distributing the model or data among the cluster of machines. We can use
    distributed TensorFlow, or TFoS frameworks to perform distributed model training.
    In recent years, some easy-to-use open source solutions have been proposed. Two
    of the most popular and successful solutions are Apache Spark's MLlib and H2O.ai's
    H2O. In this chapter, we showed how to train ML models for both regression and
    classification in MLlib and H2O. The Apache Spark MLlib supports SparkDL, which
    provides excellent support for image classification and detection tasks. The chapter
    used SparkDL to classify flower images using the pre-trained InceptionV3\. The
    H2O.ai's H2O, on the other hand, works well with numeric and tabular data. It
    provides an interesting and useful AutoML feature, which allows even non-experts
    to tune and search through a large number of ML/deep learning models, with very
    little details from the user. The chapter covered an example of how to use AutoML
    for both regression and classification tasks.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 随着物联网的普及，生成的数据正在以指数速度增长。这些数据通常是非结构化的，且数量庞大，常被称为大数据。为应对庞大的数据集，已经提出了大量的框架和解决方案。一个有前景的解决方案是DAI，将模型或数据分布在一组机器上。我们可以使用分布式TensorFlow或TFoS框架来进行分布式模型训练。近年来，出现了一些易于使用的开源解决方案。两个最受欢迎且成功的解决方案是Apache
    Spark的MLlib和H2O.ai的H2O。在本章中，我们展示了如何在MLlib和H2O中为回归和分类任务训练机器学习模型。Apache Spark的MLlib支持SparkDL，后者为图像分类和检测任务提供了出色的支持。本章使用SparkDL通过预训练的InceptionV3对花卉图像进行分类。另一方面，H2O.ai的H2O在处理数值型和表格数据时表现良好。它提供了一个有趣且实用的AutoML功能，甚至允许非专家用户在很少的细节输入下调优并搜索大量的机器学习/深度学习模型。本章涵盖了如何使用AutoML进行回归和分类任务的示例。
- en: One can take the best advantage of these distributed platforms when working
    on a cluster of machines. With computing and data shifting to the cloud at affordable
    rates, it makes sense to shift the task of ML to the cloud. Thus follows the next
    chapter, where you will learn about different cloud platforms, and how you can
    use them to analyze the data generated by your IoT devices.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当在一组机器上工作时，充分利用这些分布式平台是非常有利的。随着计算和数据以可承受的价格转移到云端，将机器学习任务迁移到云端是有意义的。接下来的一章将介绍不同的云平台，以及如何使用它们来分析由您的物联网设备生成的数据。
