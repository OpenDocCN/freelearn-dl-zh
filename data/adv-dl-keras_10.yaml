- en: Chapter 10. Policy Gradient Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 策略梯度方法
- en: In the final chapter of this book, we're going to introduce algorithms that
    directly optimize the policy network in reinforcement learning. These algorithms
    are collectively referred to as *policy gradient methods*. Since the policy network
    is directly optimized during training, the policy gradient methods belong to the
    family of *on-policy* reinforcement learning algorithms. Like value-based methods
    that we discussed in [Chapter 9](ch09.html "Chapter 9. Deep Reinforcement Learning"),
    *Deep Reinforcement Learning*, policy gradient methods can also be implemented
    as deep reinforcement learning algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章，我们将介绍直接优化策略网络的强化学习算法。这些算法统称为*策略梯度方法*。由于策略网络在训练过程中是直接优化的，因此策略梯度方法属于*在策略（on-policy）*强化学习算法家族。像我们在[第9章](ch09.html
    "第9章 深度强化学习")讨论的基于值的方法一样，*深度强化学习*，策略梯度方法也可以作为深度强化学习算法来实现。
- en: A fundamental motivation in studying the policy gradient methods is addressing
    the limitations of Q-Learning. We'll recall that Q-Learning is about selecting
    the action that maximizes the value of the state. With Q function, we're able
    to determine the policy that enables the agent to decide on which action to take
    for a given state. The chosen action is simply the one that gives the agent the
    maximum value. In this respect, Q-Learning is limited to a finite number of discrete
    actions. It's not able to deal with continuous action space environments. Furthermore,
    Q-Learning is not directly optimizing the policy. In the end, reinforcement learning
    is about finding that optimal policy that the agent will be able to use to decide
    on which action it should take in order to maximize the return.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 研究策略梯度方法的一个基本动机是解决Q-Learning的局限性。我们回顾一下，Q-Learning是通过选择能够最大化状态值的动作来进行学习的。通过Q函数，我们能够确定一种策略，使得代理能够根据给定的状态决定采取哪种动作。所选择的动作就是给代理带来最大值的那个动作。在这方面，Q-Learning只适用于有限数量的离散动作。它无法处理连续动作空间环境。此外，Q-Learning并不是在直接优化策略。最终，强化学习的目标是找到那种最优策略，代理可以利用它来决定采取哪种行动以最大化回报。
- en: In contrast, policy gradient methods are applicable to environments with discrete
    or continuous action spaces. In addition, the four policy gradient methods that
    we will be presenting in this chapter are directly optimizing the performance
    measure of the policy network. This results in a trained policy network that the
    agent can use to act in its environment optimally.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，策略梯度方法适用于具有离散或连续动作空间的环境。此外，本章将介绍的四种策略梯度方法是直接优化策略网络的性能度量。这导致了一个训练好的策略网络，代理可以使用该网络在其环境中进行最优行动。
- en: 'In summary, the goal of this chapter is to present:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章的目标是呈现：
- en: The policy gradient theorem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度定理
- en: 'Four policy gradient methods: **REINFORCE**, **REINFORCE with baseline**, **Actor-Critic**,
    and **Advantage Actor-Critic**(**A2C**)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四种策略梯度方法：**REINFORCE**，**带基线的REINFORCE**，**演员-评论家（Actor-Critic）**，以及**优势演员-评论家（Advantage
    Actor-Critic，A2C）**
- en: A guide on how to implement the policy gradient methods in Keras in a continuous
    action space environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何在Keras中实现策略梯度方法的指南，适用于连续动作空间环境
- en: Policy gradient theorem
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度定理
- en: 'As discussed in [Chapter 9](ch09.html "Chapter 9. Deep Reinforcement Learning"),
    *Deep Reinforcement Learning*, in Reinforcement Learning the agent is situated
    in an environment that is in state *s*[t''], an element of state space ![Policy
    gradient theorem](img/B08956_10_001.jpg). The state space ![Policy gradient theorem](img/B08956_10_001.jpg)
    may be discrete or continuous. The agent takes an action *a*[t] from the action
    space ![Policy gradient theorem](img/B08956_10_002.jpg) by obeying the policy,
    ![Policy gradient theorem](img/B08956_10_003.jpg). ![Policy gradient theorem](img/B08956_10_002.jpg)
    may be discrete or continuous. Because of executing the action *a*[t], the agent
    receives a reward *r* [t+1] and the environment transitions to a new state *s*
    [t+1]. The new state is dependent only on the current state and action. The goal
    of the agent is to learn an optimal policy ![Policy gradient theorem](img/B08956_10_004.jpg)
    that maximizes the *return* from all the states:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第9章](ch09.html "第9章. 深度强化学习")中所讨论的，*深度强化学习*，在强化学习中，智能体位于一个处于状态*s*[t']的环境中，这是状态空间![策略梯度定理](img/B08956_10_001.jpg)的一个元素。状态空间![策略梯度定理](img/B08956_10_001.jpg)可以是离散的，也可以是连续的。智能体从动作空间![策略梯度定理](img/B08956_10_002.jpg)中采取动作*a*[t]，遵循策略![策略梯度定理](img/B08956_10_003.jpg)。![策略梯度定理](img/B08956_10_002.jpg)可以是离散的，也可以是连续的。由于执行动作*a*[t]，智能体获得奖励*r*[t+1]，并且环境转移到一个新的状态*s*[t+1]。新状态仅依赖于当前状态和动作。智能体的目标是学习一个最优策略![策略梯度定理](img/B08956_10_004.jpg)，以最大化所有状态的*回报*：
- en: '![Policy gradient theorem](img/B08956_10_17.jpg) (Equation 9.1.1)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_17.jpg)（方程 9.1.1）'
- en: 'The return, ![Policy gradient theorem](img/B08956_10_095.jpg), is defined as
    the discounted cumulative reward from time *t* until the end of the episode or
    when the terminal state is reached:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 回报，![策略梯度定理](img/B08956_10_095.jpg)，被定义为从时间 *t* 到剧集结束或达到终止状态时的折扣累计奖励：
- en: '![Policy gradient theorem](img/B08956_10_19.jpg) (Equation 9.1.2)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_19.jpg)（方程 9.1.2）'
- en: From *Equation 9.1.2*, the return can also be interpreted as a value of a given
    state by following the policy ![Policy gradient theorem](img/B08956_10_005.jpg).
    It can be observed from *Equation 9.1.1* that future rewards have lower weights
    compared to immediate rewards since generally ![Policy gradient theorem](img/B08956_10_006.jpg)
    where ![Policy gradient theorem](img/B08956_10_007.jpg).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从*方程 9.1.2*可以看出，回报也可以解释为通过遵循策略![策略梯度定理](img/B08956_10_005.jpg)得到的给定状态的值。从*方程
    9.1.1*可以观察到，与即时奖励相比，未来奖励的权重较低，因为通常![策略梯度定理](img/B08956_10_006.jpg)，其中![策略梯度定理](img/B08956_10_007.jpg)。
- en: So far, we have only considered learning the policy by optimizing a value-based
    function, *Q(s,a)*. Our goal in this chapter is to directly learn the policy by
    parameterizing ![Policy gradient theorem](img/B08956_10_008.jpg). By parameterization,
    we can use a neural network to learn the policy function. Learning the policy
    means that we are going to maximize a certain objective function, ![Policy gradient
    theorem](img/B08956_10_009.jpg) which is a performance measure with respect to
    parameter ![Policy gradient theorem](img/B08956_10_010.jpg). In episodic reinforcement
    learning, the performance measure is the value of the start state. In the continuous
    case, the objective function is the average reward rate.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了通过优化基于值的函数*Q(s,a)*来学习策略。本章的目标是通过对![策略梯度定理](img/B08956_10_008.jpg)进行参数化来直接学习策略。通过参数化，我们可以使用神经网络来学习策略函数。学习策略意味着我们要最大化一个特定的目标函数![策略梯度定理](img/B08956_10_009.jpg)，该目标函数是相对于参数![策略梯度定理](img/B08956_10_010.jpg)的性能度量。在情节强化学习中，性能度量是起始状态的值。在连续情况下，目标函数是平均奖励率。
- en: Maximizing the objective function ![Policy gradient theorem](img/B08956_10_009.jpg)
    is achieved by performing *gradient ascent*. In gradient ascent, the gradient
    update is in the direction of the derivative of the function being optimized.
    So far, all our loss functions are optimized by minimization or by performing
    *gradient descent*. Later, in the Keras implementation, we're able to see that
    the gradient ascent can be performed by simply negating the objective function
    and performing gradient descent.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行*梯度上升*，可以最大化目标函数![策略梯度定理](img/B08956_10_009.jpg)。在梯度上升中，梯度更新是朝着被优化函数的导数方向进行的。到目前为止，我们所有的损失函数都是通过最小化或执行*梯度下降*来优化的。稍后，在Keras实现中，我们可以看到梯度上升通过简单地将目标函数取负并执行梯度下降来完成。
- en: 'The advantage of learning the policy directly is that it can be applied to
    both discrete and continuous action spaces. For discrete action spaces:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 直接学习策略的优势在于它可以应用于离散和连续的动作空间。对于离散动作空间：
- en: '![Policy gradient theorem](img/B08956_10_026.jpg) (Equation 10.1.1)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_026.jpg) (方程 10.1.1)'
- en: 'In that formula, *a*[*i*] is the *i*-th action. *a*[*i*] can be the prediction
    of a neural network or a linear function of state-action features:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在该公式中，*a*[*i*] 是第 *i* 个动作。*a*[*i*] 可以是神经网络的预测或状态-动作特征的线性函数：
- en: '![Policy gradient theorem](img/B08956_10_011.jpg) (Equation 10.1.2)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_011.jpg) (方程 10.1.2)'
- en: '![Policy gradient theorem](img/B08956_10_012.jpg) is any function such as an
    encoder that converts the state-action to features.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![策略梯度定理](img/B08956_10_012.jpg) 是任何将状态-动作转换为特征的函数，例如编码器。'
- en: '![Policy gradient theorem](img/B08956_10_013.jpg) determines the probability
    of each *a*[*i*]. For example, in the cartpole balancing problem in the previous
    chapter, the goal is to keep the pole upright by moving the cart along the 2D
    axis to the left or to the right. In this case, *a*[*0*] and *a*[*1*] are the
    probabilities of the left and right movements respectively. In general, the agent
    takes the action with the highest probability, ![Policy gradient theorem](img/B08956_10_014.jpg).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![策略梯度定理](img/B08956_10_013.jpg) 确定每个 *a*[*i*] 的概率。例如，在上一章中的平衡摆杆问题中，目标是通过沿二维轴向左或向右移动小车来保持摆杆竖直。在这种情况下，*a*[*0*]
    和 *a*[*1*] 分别是向左和向右移动的概率。一般来说，代理选择具有最高概率的动作，![策略梯度定理](img/B08956_10_014.jpg)。'
- en: For continuous action spaces, ![Policy gradient theorem](img/B08956_10_015.jpg)
    samples an action from a probability distribution given the state. For example,
    if the continuous action space is the range ![Policy gradient theorem](img/B08956_10_016.jpg),
    then ![Policy gradient theorem](img/B08956_10_17.jpg) is usually a Gaussian distribution
    whose mean and standard deviation are predicted by the policy network. The predicted
    action is a sample from this Gaussian distribution. To ensure that no invalid
    prediction is generated, the action is clipped between *-1.0* and *1.0*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续动作空间，![策略梯度定理](img/B08956_10_015.jpg) 会根据状态从概率分布中采样一个动作。例如，如果连续动作空间是范围 ![策略梯度定理](img/B08956_10_016.jpg)，那么
    ![策略梯度定理](img/B08956_10_17.jpg) 通常是一个高斯分布，其均值和标准差由策略网络预测。预测的动作是从这个高斯分布中采样得到的。为了确保不生成无效的预测，动作会在
    *-1.0* 和 *1.0* 之间截断。
- en: 'Formally, for continuous action spaces, the policy is a sample from a Gaussian
    distribution:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，对于连续动作空间，策略是从高斯分布中采样：
- en: '![Policy gradient theorem](img/B08956_10_018.jpg) (Equation 10.1.3)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_018.jpg) (方程 10.1.3)'
- en: 'The mean, ![Policy gradient theorem](img/B08956_10_019.jpg), and standard deviation,
    ![Policy gradient theorem](img/B08956_10_020.jpg), are both functions of the state
    features:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 均值，![策略梯度定理](img/B08956_10_019.jpg)，和标准差，![策略梯度定理](img/B08956_10_020.jpg)，都是状态特征的函数：
- en: '![Policy gradient theorem](img/B08956_10_021.jpg) (Equation 10.1.4)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_021.jpg) (方程 10.1.4)'
- en: '![Policy gradient theorem](img/B08956_10_022.jpg) (Equation 10.1.5)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_022.jpg) (方程 10.1.5)'
- en: '![Policy gradient theorem](img/B08956_10_023.jpg) is any function that converts
    the state to its features. ![Policy gradient theorem](img/B08956_10_024.jpg) is
    the *softplus* function that ensures positive values of standard deviation. One
    way of implementing the state feature function, ![Policy gradient theorem](img/B08956_10_023.jpg),
    is using the encoder of an autoencoder network. At the end of this chapter, we
    will train an autoencoder and use the encoder part as the state feature function.
    Training a policy network is therefore a matter of optimizing the parameters ![Policy
    gradient theorem](img/B08956_10_025.jpg).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![策略梯度定理](img/B08956_10_023.jpg) 是任何将状态转换为其特征的函数。![策略梯度定理](img/B08956_10_024.jpg)
    是 *softplus* 函数，它确保标准差为正值。实现状态特征函数的一个方法是使用自编码器网络的编码器。在本章结束时，我们将训练一个自编码器，并使用编码器部分作为状态特征函数。因此，训练策略网络就是优化参数
    ![策略梯度定理](img/B08956_10_025.jpg) 的问题。'
- en: 'Given a continuously differentiable policy function, ![Policy gradient theorem](img/B08956_10_015.jpg),
    the policy gradient can be computed as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个连续可微的策略函数，![策略梯度定理](img/B08956_10_015.jpg)，可以计算策略梯度：
- en: '![Policy gradient theorem](img/B08956_10_027.jpg) (Equation 10.1.6)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度定理](img/B08956_10_027.jpg) (方程 10.1.6)'
- en: '*Equation 10.1.6* is also known as the *policy gradient theorem*. It is applicable
    to both discrete and continuous action spaces. The gradient with respect to the
    parameter ![Policy gradient theorem](img/B08956_10_010.jpg) is computed from the
    natural logarithm of the policy action sampling scaled by the Q value. *Equation
    10.1.6* takes advantage of the property of the natural logarithm, ![Policy gradient
    theorem](img/B08956_10_33.jpg).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*方程 10.1.6* 也称为 *策略梯度定理*。它适用于离散和连续的动作空间。相对于参数的梯度![策略梯度定理](img/B08956_10_010.jpg)是通过策略动作采样的自然对数并按Q值缩放计算得到的。*方程
    10.1.6* 利用了自然对数的性质，![策略梯度定理](img/B08956_10_33.jpg)。'
- en: Policy gradient theorem is intuitive in the sense that the performance gradient
    is estimated from the target policy samples and proportional to the policy gradient.
    The policy gradient is scaled by the Q value to encourage actions that positively
    contribute to the state value. The gradient is also inversely proportional to
    the action probability to penalize frequently occurring actions that do not contribute
    to the increase of performance measure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度定理在直观上是合理的，因为性能梯度是从目标策略样本中估计的，并且与策略梯度成正比。策略梯度通过Q值进行缩放，以鼓励那些有助于状态价值提升的动作。梯度还与动作概率成反比，以惩罚那些频繁发生但对性能度量的提升没有贡献的动作。
- en: In the next section, we will demonstrate the different methods of estimating
    the policy gradient.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示估计策略梯度的不同方法。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For the proof of policy gradient theorem, please see [2] and lecture notes from
    David Silver on Reinforcement Learning, [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于策略梯度定理的证明，请参见[2]和David Silver关于强化学习的讲义，[http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)
- en: There are subtle advantages of policy gradient methods. For example, in some
    card-based games, value-based methods have no straightforward procedure in handling
    stochasticity, unlike policy-based methods. In policy-based methods, the action
    probability changes smoothly with the parameters. Meanwhile, value-based actions
    may suffer from drastic changes with respect to small changes in parameters. Lastly,
    the dependence of policy-based methods on parameters leads us to different formulations
    on how to perform gradient ascent on the performance measure. These are the four
    policy gradient methods to be presented in the succeeding sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法具有一些微妙的优势。例如，在某些基于卡片的游戏中，基于价值的方法在处理随机性时没有直接的程序，而策略方法则不同。在策略方法中，随着参数的变化，动作的概率会平滑变化。与此同时，基于价值的动作可能会因参数的微小变化而遭遇剧烈的波动。最后，策略方法对参数的依赖促使我们采用不同的方式来执行性能度量上的梯度上升。这些就是接下来章节中将介绍的四种策略梯度方法。
- en: Policy-based methods have their own disadvantages as well. They are generally
    harder to train because of the tendency to converge on a local optimum instead
    of the global optimum. In the experiments to be presented at the end of this chapter,
    it is easy for an agent to become comfortable and to choose actions that do not
    necessarily give the highest value. Policy gradient is also characterized by high
    variance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 策略方法也有其自身的缺点。它们通常更难训练，因为有趋向局部最优解的倾向，而不是全局最优解。在本章末尾将介绍的实验中，代理容易变得舒适，并选择那些不一定给出最高价值的动作。策略梯度还具有高方差的特点。
- en: The gradient updates are frequently overestimated. Furthermore, training policy-based
    methods are time-consuming. The training requires thousands of episodes (that
    is, not sample efficient). Each episode only provides a small number of samples.
    Typical training in the implementation provided at the end of the chapter would
    take about an hour for 1,000 episodes on a GTX 1060 GPU.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度更新经常被高估。此外，训练策略方法是费时的。训练通常需要成千上万的回合（即样本效率低）。每个回合只提供少量样本。在本章末尾提供的实现中，典型的训练需要在GTX
    1060 GPU上大约一个小时来进行1,000回合。
- en: In the following sections, we discuss the four policy gradient methods. While
    the discussion focuses on continuous action spaces, the concept is generally applicable
    to discrete action spaces. Due to similarities in the implementation of the policy
    and value networks of the four policy gradient methods, we will wait until the
    end of this chapter to illustrate the implementation into Keras.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论四种策略梯度方法。虽然讨论主要集中在连续动作空间上，但这一概念通常也适用于离散动作空间。由于四种策略梯度方法中策略网络和价值网络的实现方式相似，我们将在本章结束时演示如何将其实现到
    Keras 中。
- en: Monte Carlo policy gradient (REINFORCE) method
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛策略梯度（REINFORCE）方法
- en: 'The simplest policy gradient method is called REINFORCE [5], this is a Monte
    Carlo policy gradient method:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的策略梯度方法叫做 REINFORCE [5]，它是一种蒙特卡洛策略梯度方法：
- en: '![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_028.jpg) (Equation
    10.2.1)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_028.jpg)（方程 10.2.1）'
- en: where *R*[*t*] is the return as defined in *Equation 9.1.2*. *R*[*t*] is an
    unbiased sample of ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_029.jpg)
    in the policy gradient theorem.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *R*[*t*] 是在 *方程 9.1.2* 中定义的回报。*R*[*t*] 是策略梯度定理中![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_029.jpg)的无偏样本。
- en: '*Algorithm* *10.2.1* summarizes the REINFORCE algorithm [2]. REINFORCE is a Monte Carlo
    algorithm. It does not require knowledge of the dynamics of the environment (that
    is, model-free). Only experience samples, ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_030.jpg), are needed to optimally tune the parameters of
    the policy network, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_015.jpg).
    The discount factor, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_031.jpg),
    takes into consideration that rewards decrease in value as the number of steps
    increases. The gradient is discounted by ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_032.jpg). Gradients taken at later steps have smaller contributions.
    The learning rate, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_033.jpg),
    is a scaling factor of the gradient update.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法* *10.2.1* 总结了 REINFORCE 算法 [2]。REINFORCE 是一种蒙特卡洛算法。它不需要环境动态的知识（即无模型）。只需要经验样本，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_030.jpg)，就能优化地调整策略网络的参数，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_015.jpg)。折扣因子，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_031.jpg)，考虑到奖励随着步数增加而减少的价值。梯度被折扣，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_032.jpg)。在后期步骤中计算的梯度贡献较小。学习率，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_033.jpg)，是梯度更新的缩放因子。'
- en: The parameters are updated by performing gradient ascent using the discounted
    gradient and learning rate. As a Monte Carlo algorithm, REINFORCE requires that
    the agent completes an episode before processing the gradient updates. Due to
    its Monte Carlo nature, the gradient update of REINFORCE is characterized by high
    variance. At the end of this chapter, we will implement the REINFORCE algorithm
    into Keras.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 参数通过执行使用折扣梯度和学习率的梯度上升法进行更新。作为一种蒙特卡洛算法，REINFORCE 要求智能体完成一个回合后才会处理梯度更新。由于其蒙特卡洛性质，REINFORCE
    的梯度更新具有高方差的特点。在本章结束时，我们将把 REINFORCE 算法实现到 Keras 中。
- en: '**Algorithm 10.2.1 REINFORCE**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法 10.2.1 REINFORCE**'
- en: '*Require*: A differentiable parameterized target policy network, ![Monte Carlo
    policy gradient (REINFORCE) method](img/B08956_10_015.jpg).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：一个可微分的参数化目标策略网络，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_015.jpg)。'
- en: '*Require*: Discount factor, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_035.jpg)
    and learning rate ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_033.jpg).
    For example, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_036.jpg)
    and ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_037.jpg).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：折扣因子，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_035.jpg) 和学习率 ![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_033.jpg)。例如，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_036.jpg)
    和 ![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_037.jpg)。'
- en: '*Require*: ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_038.jpg),
    initial policy network parameters (for example, ![Monte Carlo policy gradient
    (REINFORCE) method](img/B08956_10_039.jpg)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_038.jpg)，初始策略网络参数（例如，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_039.jpg)）。'
- en: Repeat
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: Generate an episode ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_36.jpg)
    by following ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_041.jpg)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个 episode，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_36.jpg)，通过遵循 ![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_041.jpg)
- en: for steps ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_042.jpg)
    do
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于步骤 ![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_042.jpg)，执行
- en: Compute return, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_37.jpg)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_37.jpg)
- en: Compute discounted performance gradient, ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_38.jpg)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算折扣性能梯度，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_38.jpg)
- en: Perform gradient ascent, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_39.jpg)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度上升，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_39.jpg)
- en: '![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_01.jpg)'
- en: 'Figure 10.2.1: Policy network'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2.1：策略网络
- en: In REINFORCE, the parameterized policy can be modeled by a neural network as
    shown in *Figure 10.2.1*. As discussed in the previous section, for the case of continuous
    action spaces, the state input is converted into features. The state features
    are the inputs of the policy network. The Gaussian distribution representing the
    policy function has a mean and standard deviation that are both functions of the
    state features. The policy network, ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_043.jpg), could be an MLP, CNN, or an RNN depending on the
    nature of the state inputs. The predicted action is simply a sample from the policy
    function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 REINFORCE 中，参数化的策略可以通过神经网络建模，如*图 10.2.1*所示。如前一节所讨论，对于连续动作空间的情况，状态输入会被转换为特征。状态特征是策略网络的输入。表示策略函数的高斯分布具有均值和标准差，二者都是状态特征的函数。策略网络，![蒙特卡洛策略梯度（REINFORCE）方法](img/B08956_10_043.jpg)，可以是
    MLP、CNN 或 RNN，具体取决于状态输入的性质。预测的动作只是从策略函数中采样得到的。
- en: REINFORCE with baseline method
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带基准的 REINFORCE 方法
- en: 'The REINFORCE algorithm can be generalized by subtracting a baseline from the
    return, ![REINFORCE with baseline method](img/B08956_10_044.jpg). The baseline
    function, *B(s* *t* *)* can be any function as long as it does not depend on *a*[t]
    The baseline does not alter the expectation of the performance gradient:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 算法可以通过从回报中减去一个基准来进行泛化，![带基准的 REINFORCE 方法](img/B08956_10_044.jpg)。基准函数，*B(s*
    *t* *)* 可以是任何函数，只要它不依赖于 *a*[t]。基准不会改变性能梯度的期望值：
- en: '![REINFORCE with baseline method](img/B08956_10_046.jpg) (Equation 10.3.1)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![带基准的 REINFORCE 方法](img/B08956_10_046.jpg)（公式 10.3.1）'
- en: '*Equation 10.3.1* implies that ![REINFORCE with baseline method](img/B08956_10_047.jpg)
    since ![REINFORCE with baseline method](img/B08956_10_048.jpg) is not a function
    of ![REINFORCE with baseline method](img/B08956_10_049.jpg).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*公式 10.3.1* 表明，![带基准的 REINFORCE 方法](img/B08956_10_047.jpg)，因为![带基准的 REINFORCE
    方法](img/B08956_10_048.jpg) 不是![带基准的 REINFORCE 方法](img/B08956_10_049.jpg) 的函数。'
- en: 'While the introduction of baseline does not change the expectation, it reduces
    the variance of the gradient updates. The reduction in variance generally accelerates
    learning. In most cases, we use the value function, ![REINFORCE with baseline
    method](img/B08956_10_44.jpg) as the baseline. If the return is overestimated,
    the scaling factor is proportionally reduced by the value function resulting to
    a lower variance. The value function is also parameterized, ![REINFORCE with baseline
    method](img/B08956_10_050.jpg) and is jointly trained with the policy network.
    In continuous action spaces, the state value can be a linear function of state
    features:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然引入基准不会改变期望值，但它降低了梯度更新的方差。方差的减少通常会加速学习。在大多数情况下，我们使用价值函数，![带基准的 REINFORCE 方法](img/B08956_10_44.jpg)
    作为基准。如果回报被高估，缩放因子将按比例通过价值函数减小，从而降低方差。价值函数也是参数化的，![带基准的 REINFORCE 方法](img/B08956_10_050.jpg)，并与策略网络共同训练。在连续动作空间中，状态值可以是状态特征的线性函数：
- en: '![REINFORCE with baseline method](img/B08956_10_051.jpg) (Equation 10.3.2)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![带基准的 REINFORCE 方法](img/B08956_10_051.jpg)（公式 10.3.2）'
- en: '*Algorithm* *10.3.1* summarizes the REINFORCE with baseline method [1]. This
    is similar to REINFORCE except that the return is replaced by ![REINFORCE with
    baseline method](img/B08956_10_001.jpg). The difference is we are now training
    two neural networks. As shown in *Figure 10.3.1*, in addition to the policy network,
    ![REINFORCE with baseline method](img/B08956_10_034.jpg), the value network, ![REINFORCE
    with baseline method](img/B08956_10_052.jpg), is also trained at the same time.
    The policy network parameters are updated by the performance gradient, ![REINFORCE
    with baseline method](img/B08956_10_053.jpg), while the value network parameters
    are adjusted by the value gradient, ![REINFORCE with baseline method](img/B08956_10_054.jpg).
    Since REINFORCE is a Monte Carlo algorithm, it follows that the value function
    training is also a Monte Carlo algorithm.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法* *10.3.1* 总结了带基线的REINFORCE方法[1]。这与REINFORCE相似，唯一的不同是返回值被![带基线的REINFORCE方法](img/B08956_10_001.jpg)替代。不同之处在于我们现在训练两个神经网络。如*图10.3.1*所示，除了策略网络![带基线的REINFORCE方法](img/B08956_10_034.jpg)，还同时训练价值网络![带基线的REINFORCE方法](img/B08956_10_052.jpg)。策略网络参数通过性能梯度![带基线的REINFORCE方法](img/B08956_10_053.jpg)更新，而价值网络参数则通过价值梯度![带基线的REINFORCE方法](img/B08956_10_054.jpg)调整。由于REINFORCE是一种蒙特卡洛算法，因此可以推测，价值函数的训练也是蒙特卡洛算法。'
- en: The learning rates are not necessarily the same. Note that the value network
    is also performing gradient ascent. We illustrate how to implement REINFORCE with baseline
    using Keras at the end of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率不一定相同。请注意，价值网络也在执行梯度上升。我们将在本章末尾展示如何使用Keras实现带基线的REINFORCE方法。
- en: '**Algorithm 10.3.1 REINFORCE with baseline**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法 10.3.1 带基线的REINFORCE**'
- en: '*Require*: A differentiable parameterized target policy network, ![REINFORCE
    with baseline method](img/B08956_10_015.jpg).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：一个可微分的参数化目标策略网络![带基线的REINFORCE方法](img/B08956_10_015.jpg)。'
- en: '*Require*: A differentiable parameterized value network, ![REINFORCE with baseline
    method](img/B08956_10_055.jpg).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：一个可微分的参数化价值网络![带基线的REINFORCE方法](img/B08956_10_055.jpg)。'
- en: '*Require*: Discount factor, ![REINFORCE with baseline method](img/B08956_10_035.jpg),
    the learning rate ![REINFORCE with baseline method](img/B08956_10_033.jpg) for
    the performance gradient and learning rate for the value gradient, ![REINFORCE
    with baseline method](img/B08956_10_056.jpg).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：折扣因子![带基线的REINFORCE方法](img/B08956_10_035.jpg)，性能梯度的学习率![带基线的REINFORCE方法](img/B08956_10_033.jpg)和价值梯度的学习率![带基线的REINFORCE方法](img/B08956_10_056.jpg)。'
- en: '*Require*: ![REINFORCE with baseline method](img/B08956_10_057.jpg), initial
    policy network parameters (for example, ![REINFORCE with baseline method](img/B08956_10_058.jpg)).
    ![REINFORCE with baseline method](img/B08956_10_059.jpg), initial value network
    parameters (for example, ![REINFORCE with baseline method](img/B08956_10_060.jpg)).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：![带基线的REINFORCE方法](img/B08956_10_057.jpg)，初始策略网络参数（例如，![带基线的REINFORCE方法](img/B08956_10_058.jpg)）。![带基线的REINFORCE方法](img/B08956_10_059.jpg)，初始价值网络参数（例如，![带基线的REINFORCE方法](img/B08956_10_060.jpg)）。'
- en: Repeat
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: Generate an episode ![REINFORCE with baseline method](img/B08956_10_061.jpg)
    by following ![REINFORCE with baseline method](img/B08956_10_015.jpg)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过跟随![带基线的REINFORCE方法](img/B08956_10_015.jpg)生成一个回合![带基线的REINFORCE方法](img/B08956_10_061.jpg)
- en: for steps ![REINFORCE with baseline method](img/B08956_10_042.jpg) do
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于步骤![带基线的REINFORCE方法](img/B08956_10_042.jpg)，执行
- en: Compute return, ![REINFORCE with baseline method](img/B08956_10_063.jpg)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报![带基线的REINFORCE方法](img/B08956_10_063.jpg)
- en: Subtract baseline, ![REINFORCE with baseline method](img/B08956_10_064.jpg)
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减去基线![带基线的REINFORCE方法](img/B08956_10_064.jpg)
- en: Compute discounted value gradient, ![REINFORCE with baseline method](img/B08956_10_065.jpg)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算折扣后的价值梯度![带基线的REINFORCE方法](img/B08956_10_065.jpg)
- en: Perform gradient ascent, ![REINFORCE with baseline method](img/B08956_10_066.jpg)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度上升![带基线的REINFORCE方法](img/B08956_10_066.jpg)
- en: Compute discounted performance gradient, ![REINFORCE with baseline method](img/B08956_10_067.jpg)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算折扣后的性能梯度![带基线的REINFORCE方法](img/B08956_10_067.jpg)
- en: Perform gradient ascent, ![REINFORCE with baseline method](img/B08956_10_068.jpg)
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度上升![带基线的REINFORCE方法](img/B08956_10_068.jpg)
- en: '![REINFORCE with baseline method](img/B08956_10_02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![带基线的REINFORCE方法](img/B08956_10_02.jpg)'
- en: 'Figure 10.3.1: Policy and value networks'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3.1：策略和价值网络
- en: Actor-Critic method
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演员-评论员方法
- en: 'In REINFORCE with baseline method, the value is used as a baseline. It is not
    used to train the value function. In this section, we''ll introduce a variation
    of REINFORCE with baseline called the Actor-Critic method. The policy and value
    networks played the roles of actor and critic networks. The policy network is
    the actor deciding which action to take given the state. Meanwhile, the value
    network evaluates the decision made by the actor or the policy network. The value
    network acts as a critic which quantifies how good or bad the chosen action made
    by the actor is. The value network evaluates the state value, ![Actor-Critic method](img/B08956_10_069.jpg),
    by comparing it with the sum of the received reward, ![Actor-Critic method](img/B08956_10_070.jpg),
    and the discounted value of the observed next state, ![Actor-Critic method](img/B08956_10_071.jpg).
    The difference, ![Actor-Critic method](img/B08956_10_072.jpg), is expressed as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在带基线的REINFORCE方法中，值被用作基线。它不是用来训练值函数的。在本节中，我们将介绍一种带基线的REINFORCE变种，称为**演员-评论家方法**。策略网络和值网络分别扮演演员和评论家的角色。策略网络是演员，负责在给定状态下决定采取何种行动。与此同时，值网络则评估演员或策略网络做出的决策。值网络作为评论家，量化演员所做的选择的好坏。值网络通过将当前状态值与收到的奖励的总和以及观察到的下一个状态的折现值进行比较，来评估状态值，![演员-评论家方法](img/B08956_10_069.jpg)。这种差异，![演员-评论家方法](img/B08956_10_072.jpg)，表示为：
- en: '![Actor-Critic method](img/B08956_10_073.jpg) (Equation 10.4.1)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![演员-评论家方法](img/B08956_10_073.jpg)（方程式 10.4.1）'
- en: where we dropped the subscripts of ![Actor-Critic method](img/B08956_10_070.jpg)
    and ![Actor-Critic method](img/B08956_10_074.jpg) for simplicity. *Equation 10.4.1*
    is similar to the temporal differencing in Q-Learning discussed in [Chapter 9](ch09.html
    "Chapter 9. Deep Reinforcement Learning"), *Deep Reinforcement Learning*. The
    next state value is discounted by ![Actor-Critic method](img/B08956_10_075.jpg)
    Estimating distant future rewards is difficult. Therefore, our estimate is based
    only on the immediate future, ![Actor-Critic method](img/B08956_10_076.jpg). This
    has been known as *bootstrapping* technique. The bootstrapping technique and the
    dependence on state representation in *Equation 10.4.1* often accelerates learning
    and reduces variance. From *Equation 10.4.1*, we notice that the value network
    evaluates the current state, ![Actor-Critic method](img/B08956_10_077.jpg), which
    is due to the previous action, ![Actor-Critic method](img/B08956_10_078.jpg),
    of the policy network. Meanwhile, the policy gradient is based on the current
    action, ![Actor-Critic method](img/B08956_10_079.jpg). In a sense, the evaluation
    is delayed by one step.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中为了简便起见，我们省略了![演员-评论家方法](img/B08956_10_070.jpg)和![演员-评论家方法](img/B08956_10_074.jpg)的下标。*方程式
    10.4.1* 类似于[第9章](ch09.html "第9章 深度强化学习")中讨论的Q-Learning中的时序差分方法，*深度强化学习*。下一个状态值被折现了![演员-评论家方法](img/B08956_10_075.jpg)。估计远期奖励是困难的。因此，我们的估计仅基于近期的未来，![演员-评论家方法](img/B08956_10_076.jpg)。这被称为*自举*技术。自举技术和在*方程式
    10.4.1* 中对状态表示的依赖通常能够加速学习并减少方差。从*方程式 10.4.1*中我们可以看到，值网络评估的是当前状态，![演员-评论家方法](img/B08956_10_077.jpg)，这是由于策略网络的上一个动作，![演员-评论家方法](img/B08956_10_078.jpg)。与此同时，策略梯度是基于当前动作，![演员-评论家方法](img/B08956_10_079.jpg)。从某种意义上说，评估延迟了一个步骤。
- en: '*Algorithm* *10.4.1* summarizes the Actor-Critic method [2]. Apart from the
    evaluation of the state value which is used to train both the policy and value
    networks, the training is done online. At every step, both networks are trained.
    This is unlike REINFORCE and REINFORCE with baseline where the agent completes
    an episode before the training is performed. The value network is consulted twice.
    Firstly, during the value estimate of the current state and secondly for the value
    of the next state. Both values are used in the computation of gradients. *Figure
    10.4.1* shows the Actor-Critic network. We will implement the Actor-Critic method
    in Keras at the end of this chapter.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法* *10.4.1* 总结了演员-评论家方法[2]。除了用于训练策略和值网络的状态值评估外，训练是在线进行的。在每一步中，两个网络都会进行训练。这与REINFORCE和带基线的REINFORCE不同，后者在训练之前需要完成一个回合。值网络被调用了两次。首先是在当前状态的值估计过程中，其次是在下一个状态的值估计过程中。两个值都用于梯度计算。*图10.4.1*展示了演员-评论家网络。我们将在本章末尾使用Keras实现演员-评论家方法。'
- en: '**Algorithm 10.4.1 Actor-Critic**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法 10.4.1 演员-评论家**'
- en: '*Require*: A differentiable parameterized target policy network, ![Actor-Critic
    method](img/B08956_10_080.jpg).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*要求*：一个可微分的参数化目标策略网络，![演员-评论家方法](img/B08956_10_080.jpg)。'
- en: '*Require*: A differentiable parameterized value network, ![Actor-Critic method](img/B08956_10_081.jpg).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：一个可微分的参数化价值网络，![演员-评论员方法](img/B08956_10_081.jpg)。'
- en: '*Require*: Discount factor, ![Actor-Critic method](img/B08956_10_007.jpg),
    the learning rate ![Actor-Critic method](img/B08956_10_033.jpg) for the performance
    gradient, and the learning rate for the value gradient, ![Actor-Critic method](img/B08956_10_056.jpg).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：折扣因子，![演员-评论员方法](img/B08956_10_007.jpg)，性能梯度的学习率，![演员-评论员方法](img/B08956_10_033.jpg)，以及价值梯度的学习率，![演员-评论员方法](img/B08956_10_056.jpg)。'
- en: '*Require*: ![Actor-Critic method](img/B08956_10_038.jpg), initial policy network
    parameters (for example, ![Actor-Critic method](img/B08956_10_039.jpg)). ![Actor-Critic
    method](img/B08956_10_082.jpg), initial value network parameters (for example,
    ![Actor-Critic method](img/B08956_10_083.jpg)).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*：![演员-评论员方法](img/B08956_10_038.jpg)，初始策略网络参数（例如，![演员-评论员方法](img/B08956_10_039.jpg)）。![演员-评论员方法](img/B08956_10_082.jpg)，初始价值网络参数（例如，![演员-评论员方法](img/B08956_10_083.jpg)）。'
- en: Repeat
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: for steps ![Actor-Critic method](img/B08956_10_042.jpg) do
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于步骤 ![演员-评论员方法](img/B08956_10_042.jpg) 执行
- en: Sample an action ![Actor-Critic method](img/B08956_10_087.jpg)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样一个动作 ![演员-评论员方法](img/B08956_10_087.jpg)
- en: Execute the action and observe reward ![Actor-Critic method](img/B08956_10_070.jpg)
    and next state ![Actor-Critic method](img/B08956_10_088.jpg)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作并观察奖励 ![演员-评论员方法](img/B08956_10_070.jpg) 和下一个状态 ![演员-评论员方法](img/B08956_10_088.jpg)
- en: Evaluate state value estimate, ![Actor-Critic method](img/B08956_10_089.jpg)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估状态值估计，![演员-评论员方法](img/B08956_10_089.jpg)
- en: Compute discounted value gradient, ![Actor-Critic method](img/B08956_10_090.jpg)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算折扣价值梯度，![演员-评论员方法](img/B08956_10_090.jpg)
- en: Perform gradient ascent, ![Actor-Critic method](img/B08956_10_091.jpg)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度上升，![演员-评论员方法](img/B08956_10_091.jpg)
- en: Compute discounted performance gradient, ![Actor-Critic method](img/B08956_10_092.jpg)
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算折扣性能梯度，![演员-评论员方法](img/B08956_10_092.jpg)
- en: Perform gradient ascent, ![Actor-Critic method](img/B08956_10_093.jpg)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度上升，![演员-评论员方法](img/B08956_10_093.jpg)
- en: '![Actor-Critic method](img/B08956_10_094.jpg)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![演员-评论员方法](img/B08956_10_094.jpg)'
- en: '![Actor-Critic method](img/B08956_10_03.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![演员-评论员方法](img/B08956_10_03.jpg)'
- en: 'Figure 10.4.1: Actor-critic network'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4.1：演员-评论员网络
- en: Advantage Actor-Critic (A2C) method
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势演员-评论员 (A2C) 方法
- en: 'In the Actor-Critic method from the previous section, the objective is for
    the value function to evaluate the state value correctly. There are other techniques
    to train the value network. One obvious method is to use **MSE** (**mean squared
    error**) in the value function optimization, similar to the algorithm in Q-Learning.
    The new value gradient is equal to the partial derivative of the MSE between the
    return, ![Advantage Actor-Critic (A2C) method](img/B08956_10_095.jpg), and the
    state value:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中的演员-评论员方法中，目标是让价值函数正确评估状态值。还有其他技术可以训练价值网络。一种显而易见的方法是使用 **均方误差** (**MSE**)
    在价值函数优化中，类似于 Q-Learning 中的算法。新的价值梯度等于回报的 MSE 的偏导数，![优势演员-评论员 (A2C) 方法](img/B08956_10_095.jpg)，和状态值之间：
- en: '![Advantage Actor-Critic (A2C) method](img/B08956_10_61.jpg) (Equation 10.5.1)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![优势演员-评论员 (A2C) 方法](img/B08956_10_61.jpg)（方程 10.5.1）'
- en: As ![Advantage Actor-Critic (A2C) method](img/B08956_10_62.jpg), the value network
    prediction gets more accurate. We call this variation of the Actor-Critic algorithm
    as A2C. A2C is a single threaded or synchronous version of the **Asynchronous
    Advantage Actor-Critic** (**A3C**) by [3]. The quantity ![Advantage Actor-Critic
    (A2C) method](img/B08956_10_63.jpg) is called *Advantage*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随着![优势演员-评论员 (A2C) 方法](img/B08956_10_62.jpg)，价值网络的预测变得更加准确。我们将这种变种的演员-评论员算法称为
    A2C。A2C 是 **异步优势演员-评论员**（**A3C**）的单线程或同步版本，[3]提出。这个量![优势演员-评论员 (A2C) 方法](img/B08956_10_63.jpg)
    被称为 *优势*。
- en: '*Algorithm* *10.5.1* summarizes the A2C method. There are some differences
    between A2C and Actor-Critic. Actor-Critic is online or is trained on per experience
    sample. A2C is similar to Monte Carlo algorithms REINFORCE and REINFORCE with
    baseline. It is trained after one episode has been completed. Actor-Critic is
    trained from the first state to the last state. A2C training starts from the last
    state and ends on the first state. In addition, the A2C policy and value gradients
    are no longer discounted by ![Advantage Actor-Critic (A2C) method](img/B08956_10_032.jpg).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法* *10.5.1* 概述了A2C方法。A2C与行动者-评论员有一些区别。行动者-评论员是在线的，或者在每个经验样本上进行训练。A2C类似于蒙特卡洛算法REINFORCE和带基线的REINFORCE。它是在一个回合完成后进行训练的。行动者-评论员从第一个状态训练到最后一个状态，而A2C训练从最后一个状态开始，最终到达第一个状态。此外，A2C的策略和价值梯度不再被折扣因子![优势行动者-评论员(A2C)方法](img/B08956_10_032.jpg)折扣。'
- en: The corresponding network for A2C is similar to *Figure 10.4.1* since we only
    changed the method of gradient computation. To encourage agent exploration during
    training, A3C algorithm [3] suggests that the gradient of the weighted entropy
    value of the policy function is added to the gradient function, ![Advantage Actor-Critic
    (A2C) method](img/B08956_10_69.jpg). Recall that entropy is a measure of information
    or uncertainty of an event.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: A2C的对应网络类似于*图10.4.1*，因为我们只改变了梯度计算的方法。为了在训练过程中鼓励智能体探索，A3C算法[3]建议将策略函数加权熵值的梯度添加到梯度函数中，![优势行动者-评论员(A2C)方法](img/B08956_10_69.jpg)。回忆一下，熵是事件的信息量或不确定性的度量。
- en: '**Algorithm 10.5.1 Advantage Actor-Critic (A2C)**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法10.5.1 优势行动者-评论员(A2C)**'
- en: '*Require*: A differentiable parameterized target policy network, ![Advantage
    Actor-Critic (A2C) method](img/B08956_10_015.jpg).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*: 一个可微的参数化目标策略网络，![优势行动者-评论员(A2C)方法](img/B08956_10_015.jpg)。'
- en: '*Require*: A differentiable parameterized value network, ![Advantage Actor-Critic
    (A2C) method](img/B08956_10_096.jpg).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*: 一个可微的参数化值网络，![优势行动者-评论员(A2C)方法](img/B08956_10_096.jpg)。'
- en: '*Require*: Discount factor, ![Advantage Actor-Critic (A2C) method](img/B08956_10_035.jpg),
    the learning rate ![Advantage Actor-Critic (A2C) method](img/B08956_10_033.jpg)
    for the performance gradient, the learning rate for the value gradient, ![Advantage
    Actor-Critic (A2C) method](img/B08956_10_056.jpg) and entropy weight, ![Advantage
    Actor-Critic (A2C) method](img/B08956_10_097.jpg).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*: 折扣因子，![优势行动者-评论员(A2C)方法](img/B08956_10_035.jpg)，性能梯度的学习率，![优势行动者-评论员(A2C)方法](img/B08956_10_033.jpg)，值梯度的学习率，![优势行动者-评论员(A2C)方法](img/B08956_10_056.jpg)
    和熵权重，![优势行动者-评论员(A2C)方法](img/B08956_10_097.jpg)。'
- en: '*Require*: ![Advantage Actor-Critic (A2C) method](img/B08956_10_038.jpg), initial
    policy network parameters (for example, ![Advantage Actor-Critic (A2C) method](img/B08956_10_039.jpg)).
    ![Advantage Actor-Critic (A2C) method](img/B08956_10_098.jpg), initial value network
    parameters (for example, ![Advantage Actor-Critic (A2C) method](img/B08956_10_099.jpg)).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要*: ![优势行动者-评论员(A2C)方法](img/B08956_10_038.jpg)，初始策略网络参数（例如，![优势行动者-评论员(A2C)方法](img/B08956_10_039.jpg)）。![优势行动者-评论员(A2C)方法](img/B08956_10_098.jpg)，初始值网络参数（例如，![优势行动者-评论员(A2C)方法](img/B08956_10_099.jpg)）。'
- en: Repeat
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: Generate an episode ![Advantage Actor-Critic (A2C) method](img/B08956_10_061.jpg)
    by following ![Advantage Actor-Critic (A2C) method](img/B08956_10_015.jpg)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过遵循 ![优势行动者-评论员(A2C)方法](img/B08956_10_015.jpg)生成一个回合 ![优势行动者-评论员(A2C)方法](img/B08956_10_061.jpg)
- en: '![Advantage Actor-Critic (A2C) method](img/B08956_10_100.jpg)'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![优势行动者-评论员(A2C)方法](img/B08956_10_100.jpg)'
- en: for steps ![Advantage Actor-Critic (A2C) method](img/B08956_10_101.jpg) do
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于步骤 ![优势行动者-评论员(A2C)方法](img/B08956_10_101.jpg)，执行
- en: Compute return, ![Advantage Actor-Critic (A2C) method](img/B08956_10_66.jpg)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报，![优势行动者-评论员(A2C)方法](img/B08956_10_66.jpg)
- en: Compute value gradient, ![Advantage Actor-Critic (A2C) method](img/B08956_10_102.jpg)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值梯度，![优势行动者-评论员(A2C)方法](img/B08956_10_102.jpg)
- en: Accumulate gradient, ![Advantage Actor-Critic (A2C) method](img/B08956_10_103.jpg)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 累积梯度，![优势行动者-评论员(A2C)方法](img/B08956_10_103.jpg)
- en: Compute performance gradient, ![Advantage Actor-Critic (A2C) method](img/B08956_10_104.jpg)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算性能梯度，![优势行动者-评论员(A2C)方法](img/B08956_10_104.jpg)
- en: Perform gradient ascent, ![Advantage Actor-Critic (A2C) method](img/B08956_10_105.jpg)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度上升，![优势行动者-评论员(A2C)方法](img/B08956_10_105.jpg)
- en: Policy Gradient methods with Keras
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras的策略梯度方法
- en: 'The four policy gradient methods (*Algorithms* *10.2.1* to *10.5.1*) discussed
    in the previous sections use identical policy and value network models. The policy
    and value networks in *Figures 10.2.1* to *10.4.1* have the same configurations.
    The four policy gradient methods differ only in:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 前面讨论的四种策略梯度方法（*算法* *10.2.1* 到 *10.5.1*）使用相同的策略和价值网络模型。*图 10.2.1* 到 *10.4.1*
    中的策略和价值网络配置相同。四种策略梯度方法的区别仅在于：
- en: Performance and value gradients formula
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能和价值梯度公式
- en: Training strategy
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练策略
- en: In this section, we discuss the implementation in Keras of *Algorithms* *10.2.1*
    to *10.5.1* in one code, since they share many common routines.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在 Keras 中实现 *算法* *10.2.1* 到 *10.5.1* 的代码，因为它们共享许多共同的例程。
- en: Note
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The complete code can be found on [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在 [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)
    找到。
- en: But before discussing the implementation, let's briefly explore the training
    environment.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但在讨论实现之前，先简要了解一下训练环境。
- en: '![Policy Gradient methods with Keras](img/B08956_10_04.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 的策略梯度方法](img/B08956_10_04.jpg)'
- en: Figure 10.6.1 MountainCarContinuous-v0 OpenAI gym environment
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6.1 MountainCarContinuous-v0 OpenAI Gym 环境
- en: Unlike Q-Learning, policy gradient methods are applicable to both discrete and
    continuous action spaces. In our example, we'll demonstrate the four policy gradient
    methods on a continuous action space case example, `MountainCarContinuous-v0`
    of OpenAI gym, [https://gym.openai.com](https://gym.openai.com). In case you are
    not familiar with OpenAI gym, please see [Chapter 9](ch09.html "Chapter 9. Deep
    Reinforcement Learning"), *Deep Reinforcement Learning*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Q-Learning 不同，策略梯度方法适用于离散和连续的动作空间。在我们的示例中，我们将在一个连续动作空间的案例中演示四种策略梯度方法，`MountainCarContinuous-v0`
    环境来自 OpenAI Gym，[https://gym.openai.com](https://gym.openai.com)。如果你不熟悉 OpenAI
    Gym，请参考[第9章](ch09.html "第9章 深度强化学习")，*深度强化学习*。
- en: 'A snapshot of `MountainCarContinuous-v0` 2D environment is shown in *Figure
    10.6.1*. In this 2D environment, a car with a not too powerful engine is between
    two mountains. In order to reach the yellow flag on top of the mountain on the
    right, it must drive back and forth to gain enough momentum. The more energy (that
    is, the greater the absolute value of action) that is applied to the car, the
    smaller (or, the more negative) is the reward. The reward is always negative,
    and it is only positive upon reaching the flag. In that case, the car receives
    a reward of +100\. However, every action is penalized by the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`MountainCarContinuous-v0` 二维环境的快照如 *图 10.6.1* 所示。在这个二维环境中，一辆引擎不太强大的汽车位于两座山之间。为了到达右侧山顶的黄色旗帜，它必须前后行驶以获得足够的动能。施加到汽车上的能量（即，动作的绝对值）越大，奖励就越小（或者说，奖励变得更负）。奖励始终为负，只有到达旗帜时才为正。在那时，汽车会获得
    +100 的奖励。然而，每个动作都会受到以下代码的惩罚：'
- en: '[PRE0]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The continuous range of valid action values is [-1.0, 1.0]. Beyond the range,
    the action is clipped to its minimum or maximum value. Therefore, it makes no
    sense to apply an action value that is greater than 1.0 or less than -1.0\. The `MountainCarContinuous-v0`
    environment state has two elements:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有效动作值的连续范围是[-1.0, 1.0]。超出该范围时，动作会被截断为其最小值或最大值。因此，应用大于1.0或小于-1.0的动作值是没有意义的。`MountainCarContinuous-v0`
    环境的状态包含两个元素：
- en: Car position
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车位置
- en: Car velocity
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车速度
- en: 'The state is converted to state features by an encoder. The predicted action
    is the output of the policy model given the state. The output of the value function
    is the predicted value of the state:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 状态通过编码器转换为状态特征。预测的动作是策略模型在给定状态下的输出。价值函数的输出是状态的预测值：
- en: '![Policy Gradient methods with Keras](img/B08956_10_05.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 的策略梯度方法](img/B08956_10_05.jpg)'
- en: Figure 10.6.2 Autoencoder model
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6.2 自动编码器模型
- en: '![Policy Gradient methods with Keras](img/B08956_10_06.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 的策略梯度方法](img/B08956_10_06.jpg)'
- en: Figure 10.6.3 Encoder model
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6.3 编码器模型
- en: '![Policy Gradient methods with Keras](img/B08956_10_07.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 的策略梯度方法](img/B08956_10_07.jpg)'
- en: Figure 10.6.4 Decoder model
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6.4 解码器模型
- en: As shown in *Figures 10.2.1* to *10.4.1*, before building the policy and value
    networks, we must first create a function that converts the state to features.
    This function is implemented by an encoder of an autoencoder similar to the ones
    implemented in [Chapter 3](ch03.html "Chapter 3. Autoencoders"), *Autoencoders*.
    *Figure 10.6.2* shows an autoencoder made of an encoder and a decoder. In *Figure
    10.6.3*, the encoder is an MLP made of `Input(2)-Dense(256, activation='relu')-Dense(128,
    activation='relu')-Dense(32)`. Every state is converted into a 32-dim feature
    vector. In *Figure* *10.6.4*, the decoder is also an MLP but made of `Input(32)-Dense(128,
    activation='relu')-Dense(256, activation='relu')-Dense(2)`. The autoencoder is
    trained for 10 epochs with an **MSE**, loss function, and Keras default Adam optimizer.
    We sampled 220,000 random states for the train and test dataset and applied 200k/20k
    train-test split. After training, the encoder weights are saved for future use
    in the policy and value networks training. *Listing 10.6.1* shows the methods
    for building and training the autoencoder.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.2.1*到*图10.4.1*所示，在构建策略和价值网络之前，我们必须首先创建一个将状态转换为特征的函数。这个函数是通过类似于[第3章](ch03.html
    "Chapter 3. Autoencoders")中实现的自编码器的编码器来实现的，*自编码器*。*图10.6.2*展示了由编码器和解码器组成的自编码器。在*图10.6.3*中，编码器是一个MLP，由`Input(2)-Dense(256,
    activation='relu')-Dense(128, activation='relu')-Dense(32)`组成。每个状态都被转换为一个32维的特征向量。在*图10.6.4*中，解码器也是一个MLP，但由`Input(32)-Dense(128,
    activation='relu')-Dense(256, activation='relu')-Dense(2)`组成。自编码器经过10个epoch的训练，使用**MSE**损失函数和Keras默认的Adam优化器。我们为训练和测试数据集随机采样了220,000个状态，并应用了200k/20k的训练-测试集划分。训练后，编码器权重被保存以便在未来的策略和价值网络训练中使用。*列表10.6.1*展示了构建和训练自编码器的方法。
- en: 'Listing 10.6.1, `policygradient-car-10.1.1.py` shows us the methods for building
    and training the autoencoder:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.6.1，`policygradient-car-10.1.1.py`展示了我们构建和训练自编码器的方法：
- en: '[PRE1]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Policy Gradient methods with Keras](img/B08956_10_08.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![Policy Gradient methods with Keras](img/B08956_10_08.jpg)'
- en: 'Figure 10.6.5: Policy model (actor model)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6.5：策略模型（行为者模型）
- en: 'Given the `MountainCarContinuous-v0` environment, the policy (or actor) model
    predicts the action that must be applied on the car. As discussed in the first
    section of this chapter on policy gradient methods, for continuous action spaces
    the policy model samples an action from a Gaussian distribution, ![Policy Gradient
    methods with Keras](img/B08956_10_106.jpg). In Keras, this is implemented as:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 给定`MountainCarContinuous-v0`环境，策略（或行为者）模型预测必须施加在汽车上的动作。正如本章第一部分关于策略梯度方法的讨论，对于连续动作空间，策略模型从高斯分布中采样一个动作，![Policy
    Gradient methods with Keras](img/B08956_10_106.jpg)。在Keras中，这是通过以下方式实现的：
- en: '[PRE2]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The action is clipped between its minimum and maximum possible values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 动作在其最小值和最大值之间被裁剪。
- en: The role of the policy network is to predict the mean and standard deviation
    of the Gaussian distribution. *Figure 10.6.5* shows the policy network to model
    ![Policy Gradient methods with Keras](img/B08956_10_015.jpg). It's worth noting
    that the encoder model has pretrained weights that are frozen. Only the mean and
    standard deviation weights receive the performance gradient updates.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 策略网络的作用是预测高斯分布的均值和标准差。*图10.6.5*展示了策略网络建模的过程![Policy Gradient methods with Keras](img/B08956_10_015.jpg)。值得注意的是，编码器模型具有冻结的预训练权重。只有均值和标准差的权重会接收性能梯度更新。
- en: 'The policy network is basically the implementation of *Equations* *10.1.4*
    and *10.1.5* that are repeated here for convenience:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 策略网络基本上是*公式*10.1.4和*10.1.5*的实现，这些公式为了方便起见在此重复：
- en: '![Policy Gradient methods with Keras](img/B08956_10_108.jpg) (Equation 10.1.4)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![Policy Gradient methods with Keras](img/B08956_10_108.jpg)（公式10.1.4）'
- en: '![Policy Gradient methods with Keras](img/B08956_10_109.jpg) (Equation 10.1.5)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![Policy Gradient methods with Keras](img/B08956_10_109.jpg)（公式10.1.5）'
- en: 'where ![Policy Gradient methods with Keras](img/B08956_10_023.jpg) is the encoder,
    ![Policy Gradient methods with Keras](img/B08956_10_110.jpg) are the weights of
    the mean''s `Dense(1)` layer, and ![Policy Gradient methods with Keras](img/B08956_10_111.jpg)
    are the weights of the standard deviation''s `Dense(1)` layer. We used a modified
    *softplus* function, ![Policy Gradient methods with Keras](img/B08956_10_112.jpg),
    to avoid zero standard deviation:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![Policy Gradient methods with Keras](img/B08956_10_023.jpg)是编码器，![Policy
    Gradient methods with Keras](img/B08956_10_110.jpg)是均值`Dense(1)`层的权重，![Policy
    Gradient methods with Keras](img/B08956_10_111.jpg)是标准差`Dense(1)`层的权重。我们使用了修改过的*softplus*函数，![Policy
    Gradient methods with Keras](img/B08956_10_112.jpg)，以避免标准差为零：
- en: '[PRE3]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The policy model builder is shown in the following listing. Also included in
    this listing are the log probability, entropy, and value models which we will
    discuss next.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 政策模型生成器如下列表所示。此列表还包括我们接下来将讨论的对数概率、熵和值模型。
- en: 'Listing 10.6.2, `policygradient-car-10.1.1.py` shows us the method for building
    the policy (actor), `logp`, entropy, and value models from the encoded state features:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.6.2，`policygradient-car-10.1.1.py`向我们展示了从编码状态特征构建策略（演员）、`logp`、熵和值模型的方法：
- en: '[PRE4]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Policy Gradient methods with Keras](img/B08956_10_09.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras的策略梯度方法](img/B08956_10_09.jpg)'
- en: 'Figure 10.6.6: Gaussian log probability model of the policy'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6.6：策略的高斯对数概率模型
- en: '![Policy Gradient methods with Keras](img/B08956_10_10.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras的策略梯度方法](img/B08956_10_10.jpg)'
- en: 'Figure 10.6.7: Entropy model'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6.7：熵模型
- en: 'Apart from the policy network, ![Policy Gradient methods with Keras](img/B08956_10_015.jpg),
    we must also have the action log probability (`logp`) network ![Policy Gradient
    methods with Keras](img/B08956_10_70.jpg) since this is actually what calculates
    the gradient. As shown in *Figure 10.6.6*, the `logp` network is simply the policy
    network where an additional `Lambda(1)` layer computes the log probability of
    the Gaussian distribution given action, mean, and standard deviation. The `logp`
    network and actor (policy) model share the same set of parameters. The `Lambda`
    layer does not have any parameter. It is implemented by the following function:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 除了策略网络，![使用Keras的策略梯度方法](img/B08956_10_015.jpg)，我们还必须有动作对数概率(`logp`)网络![使用Keras的策略梯度方法](img/B08956_10_70.jpg)，因为这实际上计算梯度。如*图10.6.6*所示，`logp`网络只是策略网络，其中额外的`Lambda(1)`层计算给定动作、均值和标准差的高斯分布的对数概率。`logp`网络和演员（策略）模型共享相同的参数集。`Lambda`层没有任何参数。它由以下函数实现：
- en: '[PRE5]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training the `logp` network trains the actor model as well. In the training
    methods that are discussed in this section, only the `logp` network is trained.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练`logp`网络也训练了演员模型。在本节讨论的训练方法中，只训练了`logp`网络。
- en: 'As shown in *Figure 10.6.7*, the entropy model also shares parameters with
    the policy network. The output `Lambda(1)` layer computes the entropy of the Gaussian
    distribution given the mean and standard deviation using the following function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.6.7*所示，熵模型还与策略网络共享参数。输出`Lambda(1)`层使用以下函数计算高斯分布的熵，给定均值和标准差：
- en: '[PRE6]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The entropy model is only used by the A2C method:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 熵模型仅由A2C方法使用：
- en: '![Policy Gradient methods with Keras](img/B08956_10_11.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras的策略梯度方法](img/B08956_10_11.jpg)'
- en: 'Figure 10.6.8: A value model'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6.8：值模型
- en: 'Preceding figure shows the value model. The model also uses the pre-trained
    encoder with frozen weights to implement following equation which is repeated
    here for convenience:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了值模型。该模型还使用冻结权重的预训练编码器来实现以下方程，这里为了方便重复列出：
- en: '![Policy Gradient methods with Keras](img/B08956_10_113.jpg) (Equation 10.3.2)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras的策略梯度方法](img/B08956_10_113.jpg)（方程10.3.2）'
- en: '![Policy Gradient methods with Keras](img/B08956_10_114.jpg) are the weights
    of the `Dense(1)` layer, the only layer that receives value gradient updates.
    *Figure 10.6.8* represents ![Policy Gradient methods with Keras](img/B08956_10_096.jpg)
    in *Algorithms 10.3.1* to *10.5.1*. The value model can be built in a few lines:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![使用Keras的策略梯度方法](img/B08956_10_114.jpg)是`Dense(1)`层的权重，唯一接收值梯度更新的层。*图10.6.8*代表了![使用Keras的策略梯度方法](img/B08956_10_096.jpg)在*算法10.3.1*到*10.5.1*中。值模型可以用几行代码构建：'
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These lines are also implemented in method `build_actor_critic()`, which is
    shown in *Listing 10.6.2*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行也在`build_actor_critic()`方法中实现，如*列表10.6.2*所示。
- en: After building the network models, the next step is training. In *Algorithms*
    *10.2.1* to *10.5.1*, we perform objective function maximization by gradient ascent.
    In Keras, we perform loss function minimization by gradient descent. The loss
    function is simply the negative of the objective function being maximized. The
    gradient descent is the negative of gradient ascent. *Listing 10.6.3* shows the
    `logp` and value loss functions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 构建网络模型后，下一步是训练。在*算法* *10.2.1* 到 *10.5.1*中，我们通过梯度上升执行目标函数的最大化。在Keras中，我们通过梯度下降执行损失函数的最小化。损失函数简单地是要最大化的目标函数的负值。梯度下降是梯度上升的负数。*列表10.6.3*显示了`logp`和值损失函数。
- en: We can take advantage of the common structure of the loss functions to unify
    the loss functions in *Algorithms* *10.2.1* to *10.5.1*. The performance and value
    gradients differ only in their constant factors. All performance gradients have
    the common term, ![Policy Gradient methods with Keras](img/B08956_10_74.jpg).
    This is represented by `y_pred` in the policy log probability loss function, `logp_loss()`.
    The factor to the common term, ![Policy Gradient methods with Keras](img/B08956_10_74.jpg),
    depends on which algorithm and is implemented as `y_true`. Table *10.6.1* shows
    the values of `y_true`. The remaining term is the weighted gradient of entropy,
    ![Policy Gradient methods with Keras](img/B08956_10_69.jpg). It is implemented
    as the product of `beta` and `entropy` in the `logp_loss()` function. Only A2C
    uses this term, so by default, `beta=0.0`. For A2C, `beta=0.9`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用损失函数的共同结构，将 *算法* *10.2.1* 到 *10.5.1* 的损失函数统一起来。性能和值的梯度仅在常数因子上有所不同。所有的性能梯度都有共同项
    ![Policy Gradient methods with Keras](img/B08956_10_74.jpg)。这在策略日志概率损失函数 `logp_loss()`
    中由 `y_pred` 表示。共同项的因子 ![Policy Gradient methods with Keras](img/B08956_10_74.jpg)
    取决于所使用的算法，并通过 `y_true` 实现。*表 10.6.1* 显示了 `y_true` 的值。剩余项是熵的加权梯度 ![Policy Gradient
    methods with Keras](img/B08956_10_69.jpg)。它在 `logp_loss()` 函数中实现为 `beta` 和 `entropy`
    的乘积。只有 A2C 使用此项，因此默认情况下，`beta=0.0`。对于 A2C，`beta=0.9`。
- en: 'Listing 10.6.3, `policygradient-car-10.1.1.py`: The loss functions of `logp`
    and value networks.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6.3，`policygradient-car-10.1.1.py`：`logp` 和值网络的损失函数。
- en: '[PRE8]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '| Algorithm | `y_true of logp_loss` | `y_true of value_loss` |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | `logp_loss 的 y_true` | `value_loss 的 y_true` |'
- en: '| --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 10.2.1 REINFORCE | ![Policy Gradient methods with Keras](img/B08956_10_115.jpg)
    | Not applicable |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 10.2.1 REINFORCE | ![Policy Gradient methods with Keras](img/B08956_10_115.jpg)
    | 不适用 |'
- en: '| 10.3.1 REINFORCE with baseline | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 10.3.1 带基准的 REINFORCE | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg) |'
- en: '| 10.4.1 Actor-Critic | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 10.4.1 演员-评论员 | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg) |'
- en: '| 10.5.1 A2C | ![Policy Gradient methods with Keras](img/B08956_10_63.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_095.jpg) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 10.5.1 A2C | ![Policy Gradient methods with Keras](img/B08956_10_63.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_095.jpg) |'
- en: 'Table 10.6.1: y_true value of logp_loss and value_loss'
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表 10.6.1：`logp_loss` 和 `value_loss` 的 `y_true` 值。
- en: Similarly, the value loss functions of *Algorithms* *10.3.1* and *10.4.1* have
    the same structure. The value loss functions are implemented in Keras as `value_loss()`
    as shown in *Listing 10.6.3*. The common gradient factor ![Policy Gradient methods
    with Keras](img/B08956_10_75.jpg) is represented by the tensor `y_pred`. The remaining
    factor is represented by `y_true`. The `y_true` values are also shown in *Table
    10.6.1*. REINFORCE does not use a value function. A2C uses the MSE loss function
    to learn the value function. In A2C, `y_true` represents the target value or ground
    truth.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，*算法* *10.3.1* 和 *10.4.1* 的值损失函数具有相同的结构。这些值损失函数在 Keras 中实现为 `value_loss()`，如
    *列表 10.6.3* 所示。共同的梯度因子 ![Policy Gradient methods with Keras](img/B08956_10_75.jpg)
    由张量 `y_pred` 表示。剩余的因子由 `y_true` 表示。`y_true` 的值也显示在 *表 10.6.1* 中。REINFORCE 不使用值函数。A2C
    使用 MSE 损失函数来学习值函数。在 A2C 中，`y_true` 表示目标值或真实值。
- en: 'Listing 10.6.4, `policygradient-car-10.1.1.py` shows us, REINFORCE, REINFORCE
    with baseline, and A2C are trained by episode. The appropriate return is computed
    first before calling the main train routine in *Listing 10.6.5*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6.4，`policygradient-car-10.1.1.py` 向我们展示了，REINFORCE、带基准的 REINFORCE 和 A2C
    都是通过每个回合进行训练的。计算合适的回报后，再调用 *列表 10.6.5* 中的主要训练例程：
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 10.6.5, `policygradient-car-10.1.1.py` shows us the main `train` routine
    used by all the policy gradient algorithms. Actor-critic calls this every experience
    sample while the rest call this during train per episode routine in *Listing 10.6.4*:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6.5，`policygradient-car-10.1.1.py` 向我们展示了所有策略梯度算法使用的主要 `train` 例程。演员-评论员在每个经验样本时调用此例程，而其他算法则在每个回合的训练例程中调用此例程，见
    *列表 10.6.4*：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With all network models and loss functions in place, the last part is the training
    strategy, which is different for each algorithm. Two train functions are used
    as shown in *Listings 10.6.4* and *10.6.5*. *Algorithms* *10.2.1*, *10.3.1*, and
    *10.5.1* wait for a complete episode to finish before training, so it runs both
    `train_by_episode()` and `train()`. The complete episode is saved in `self.memory`.
    Actor-Critic *Algorithm* *10.4.1* trains per step and only runs `train()`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络模型和损失函数就绪后，最后一部分是训练策略，每个算法的训练策略不同。如 *列表 10.6.4* 和 *10.6.5* 中所示，使用了两个训练函数。*算法*
    *10.2.1*、*10.3.1* 和 *10.5.1* 在训练前等待完整剧集完成，因此同时运行 `train_by_episode()` 和 `train()`。完整的剧集保存在
    `self.memory` 中。Actor-Critic *算法* *10.4.1* 每步训练，仅运行 `train()`。
- en: Each algorithm processes its episode trajectory in a different way.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 每个算法处理其剧集轨迹的方式不同。
- en: '| Algorithm | `y_true` formula | `y_true` in Keras |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | `y_true` 公式 | Keras中的 `y_true` |'
- en: '| --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 10.2.1 REINFORCE | ![Policy Gradient methods with Keras](img/B08956_10_115.jpg)
    | `reward * discount_factor` |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 10.2.1 REINFORCE | ![带Keras的策略梯度方法](img/B08956_10_115.jpg) | `reward * discount_factor`
    |'
- en: '| 10.3.1 REINFORCE with baseline | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | `(reward - self.value(state)[0]) * discount_factor` |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 10.3.1 带基线的 REINFORCE | ![带Keras的策略梯度方法](img/B08956_10_117.jpg) | `(reward
    - self.value(state)[0]) * discount_factor` |'
- en: '| 10.4.1 Actor-Critic | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | `(reward - self.value(state)[0] + gamma*next_value) * discount_factor` |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 10.4.1 Actor-Critic | ![带Keras的策略梯度方法](img/B08956_10_117.jpg) | `(reward
    - self.value(state)[0] + gamma*next_value) * discount_factor` |'
- en: '| 10.5.1 A2C | ![Policy Gradient methods with Keras](img/B08956_10_63.jpg)and
    ![Policy Gradient methods with Keras](img/B08956_10_095.jpg) | `(reward - self.value(state)[0])`and`reward`
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 10.5.1 A2C | ![带Keras的策略梯度方法](img/B08956_10_63.jpg) 和 ![带Keras的策略梯度方法](img/B08956_10_095.jpg)
    | `(reward - self.value(state)[0])` 和 `reward` |'
- en: 'Table 10.6.2: y_true value in Table 10.6.1'
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表格 10.6.2：表格 10.6.1 中的 y_true 值
- en: For REINFORCE methods and A2C, the `reward` is actually the return as computed
    in `train_by_episode()`. `discount_factor = gamma**step`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 REINFORCE 方法和 A2C，`reward` 实际上是 `train_by_episode()` 中计算的返回值。`discount_factor
    = gamma**step`。
- en: 'Both REINFORCE methods compute the return, ![Policy Gradient methods with Keras](img/B08956_10_37.jpg),
    by replacing the reward value in the memory as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 两种 REINFORCE 方法通过替换内存中的奖励值来计算返回值 ![带Keras的策略梯度方法](img/B08956_10_37.jpg)：
- en: '[PRE11]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This then trains the policy (actor) and value models (with baseline only) for
    each step beginning with the first step.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，训练策略（演员）和价值模型（仅带基线）从第一步开始，对每一步进行训练。
- en: 'The training strategy of A2C is different in the sense that it computes gradients
    from the last step to the first step. Hence, the return accumulates beginning
    from the last step reward or the last next state value:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: A2C 的训练策略不同，它从最后一步到第一步计算梯度。因此，返回值从最后一步的奖励或下一个状态值开始累积：
- en: '[PRE12]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `reward` variable in the list is also replaced by return. It is initialized
    by `reward` if the terminal state is reached (that is, the car touches the flag)
    or the next state value for non-terminal states:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的 `reward` 变量也被返回值替代。如果到达终止状态（即汽车触及旗帜）或非终止状态的下一个状态值，则初始化为 `reward`：
- en: '[PRE13]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the Keras implementation, all the routines that we mentioned are implemented
    as methods in the `PolicyAgent` class. The role of the `PolicyAgent` is to represent
    the agent implementing policy gradient methods including building and training
    the network models and predicting the action, log probability, entropy, and state
    value.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 实现中，我们提到的所有程序都作为方法在 `PolicyAgent` 类中实现。`PolicyAgent` 的作用是表示实施策略梯度方法的智能体，包括构建和训练网络模型以及预测动作、对数概率、熵和状态值。
- en: Following listing shows how one episode unfolds when the agent executes and
    trains the policy and value models. The `for` loop is executed for 1000 episodes.
    An episode terminates upon reaching 1000 steps or when the car touches the flag.
    The agent executes the action predicted by the policy at every step. After each
    episode or step, the training routine is called.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了智能体执行并训练策略与价值模型时一个剧集的展开方式。`for` 循环执行1000个剧集。一个剧集在达到1000步或汽车触及旗帜时终止。智能体在每一步执行策略预测的动作。在每个剧集或步骤后，训练程序被调用。
- en: 'Listing 10.6.6, `policygradient-car-10.1.1.py`: The agent runs for 1000 episodes
    to execute the action predicted by the policy at every step and perform training:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6.6，`policygradient-car-10.1.1.py`：智能体运行1000个剧集，每一步都执行策略预测的动作并进行训练：
- en: '[PRE14]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Performance evaluation of policy gradient methods
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度方法的性能评估
- en: The four policy gradients methods were evaluated by training the agent for 1,000
    episodes. We define 1 training session as 1,000 episodes of training. The first
    performance metric is measured by accumulating the number of times the car reached
    the flag in 1,000 episodes. *Figures 10.7.1* to *10.7.4* shows five training sessions
    per method.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这四种策略梯度方法通过训练智能体1,000回合来进行评估。我们将1次训练定义为1,000回合的训练。第一个性能度量标准是通过累计智能体在1,000回合中到达旗帜的次数来衡量的。*图10.7.1*
    到 *10.7.4* 显示了每种方法的五次训练会话。
- en: In this metric, A2C reached the flag with the greatest number of times followed
    by REINFORCE with baseline, Actor-Critic, and REINFORCE. The use of baseline or
    critic accelerates the learning. Note that these are training sessions with the
    agent continuously improving its performance. There were cases in the experiments
    where the agent's performance did not improve with time.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个度量标准中，A2C以最大的次数到达旗帜，其次是带基线的REINFORCE、演员-评论家方法和REINFORCE方法。使用基线或评论家加速了学习。请注意，这些是智能体不断改进其性能的训练会话。在实验中，确实有一些情况，智能体的表现没有随时间提升。
- en: The second performance metric is based on the requirement that the `MountainCarContinuous-v0`
    is considered solved if the total reward per episode is at least 90.0\. From the
    five training sessions per method, we selected one training session with the highest
    total reward for the last 100 episodes (episodes 900 to 999). *Figures 10.7.5*
    to *10.7.8* show the results of the four policy gradient methods. REINFORCE with
    baseline is the only method that was able to consistently achieve a total reward
    of about 90 after 1,000 episodes of training. A2C has the second-best performance
    but could not consistently reach at least 90 for the total rewards.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个性能度量标准是基于以下要求：如果每个回合的总奖励至少为90.0，那么`MountainCarContinuous-v0`被认为是已解决的。通过每种方法的五次训练，我们选择了最后100个回合（第900到999回合）中总奖励最高的一次训练。*图10.7.5*
    到 *10.7.8* 显示了四种策略梯度方法的结果。带基线的REINFORCE是唯一能够在1,000回合训练后持续获得约90总奖励的方法。A2C表现第二好，但无法持续达到至少90的总奖励。
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_12.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_12.jpg)'
- en: 'Figure 10.7.1: The number of times the mountain car reached the flag using
    REINFORCE method'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.1：使用REINFORCE方法山地车到达旗帜的次数
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_13.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_13.jpg)'
- en: 'Figure 10.7.2: The number of times the mountain car reached the flag using
    REINFORCE with baseline method'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.2：使用带基线的REINFORCE方法山地车到达旗帜的次数
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_14.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_14.jpg)'
- en: 'Figure 10.7.3: The number of times the mountain car reached the flag using
    the Actor-Critic method'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.3：使用演员-评论家方法山地车到达旗帜的次数
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_15.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_15.jpg)'
- en: 'Figure 10.7.4: The number of times the mountain car reached the flag using
    the A2C method'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.4：使用A2C方法山地车到达旗帜的次数
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_16.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_16.jpg)'
- en: 'Figure 10.7.5: Total rewards received per episode using REINFORCE method'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.5：使用REINFORCE方法每回合获得的总奖励
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_17_a.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_17_a.jpg)'
- en: 'Figure 10.7.6: Total rewards received per episode using REINFORCE with baseline
    method.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.6：使用带基线的REINFORCE方法每回合获得的总奖励。
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_18.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_18.jpg)'
- en: 'Figure 10.7.7: Total rewards received per episode using the Actor-Critic method'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.7：使用演员-评论家方法每回合获得的总奖励
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_19.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度方法的性能评估](img/B08956_10_19.jpg)'
- en: 'Figure 10.7.8: The total rewards received per episode using the A2C method'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7.8：使用A2C方法每回合获得的总奖励
- en: In the experiments conducted, we used the same learning rate, `1e-3`, for log
    probability and value networks optimization. The discount factor is set to 0.99,
    except for A2C which is easier to train at a 0.95 discount factor.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们对对数概率和价值网络的优化使用相同的学习率`1e-3`。折扣因子设置为0.99，除了A2C方法，它在0.95的折扣因子下更容易训练。
- en: 'The reader is encouraged to run the trained network by executing:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者通过执行以下命令来运行训练好的网络：
- en: '[PRE15]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Following table shows other modes of running `policygradient-car-10.1.1.py`.
    The weights file (that is, `*.h5`) can be replaced by your own pre-trained weights
    file. Please consult the code to see the other potential options:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了运行 `policygradient-car-10.1.1.py` 的其他模式。权重文件（即 `*.h5`）可以用你自己的预训练权重文件替换。请参考代码查看其他潜在选项：
- en: '| Purpose | Run |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 运行 |'
- en: '| --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Train REINFORCE from scratch |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始训练 REINFORCE |'
- en: '[PRE16]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train REINFORCE with baseline from scratch |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始训练带基线的 REINFORCE |'
- en: '[PRE17]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train Actor-Critic from scratch |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始训练 Actor-Critic |'
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train A2C from scratch |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 从头开始训练 A2C |'
- en: '[PRE19]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train REINFORCE from previously saved weights |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 从之前保存的权重训练 REINFORCE |'
- en: '[PRE20]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train REINFORCE with baseline from previously saved weights |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 从之前保存的权重训练带基线的 REINFORCE |'
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train Actor-Critic from previously saved weights |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 从之前保存的权重训练 Actor-Critic |'
- en: '[PRE22]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Train A2C from previously saved weights |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 从之前保存的权重训练 A2C |'
- en: '[PRE23]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 10.7.1: Different options in running policygradient-car-10.1.1.py'
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表 10.7.1：运行 policygradient-car-10.1.1.py 时的不同选项
- en: As a final note, the implementation of the policy gradient methods in Keras
    has some limitations. For example, training the actor model requires resampling
    the action. The action is first sampled and applied to the environment to observe
    the reward and next state. Then, another sample is taken for training the log
    probability model. The second sample is not necessarily the same as the first
    one, but the reward that is used for training comes from the first sampled action,
    which can introduce stochastic error in the computation of gradients.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要注意的是，在 Keras 中实现策略梯度方法存在一些局限性。例如，训练 actor 模型需要重新采样动作。动作首先被采样并应用到环境中以观察奖励和下一个状态。然后，再采样一次用于训练对数概率模型。第二次采样不一定与第一次相同，但用于训练的奖励来自第一次采样的动作，这可能在梯度计算中引入随机误差。
- en: The good news is Keras is gaining a lot of support from TensorFlow in the form
    of `tf.keras`. Transitioning from Keras to a more flexible and powerful machine
    learning library, like TensorFlow, has been made a lot easier. If you started
    with Keras and wanted to build low-level custom machine learning routines, the
    APIs of Keras and `tf.keras` share strong similarities.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，Keras 在 `tf.keras` 中得到了 TensorFlow 大力支持。从 Keras 过渡到更灵活、更强大的机器学习库（如 TensorFlow）变得更加容易。如果你是从
    Keras 开始，并且想要构建低级自定义机器学习例程，那么 Keras 和 `tf.keras` 的 API 有很强的相似性。
- en: There is a small learning curve in using Keras in TensorFlow. Furthermore, in `tf.keras`,
    you're able to take advantage of the new easy to use Dataset and Estimators APIs
    of TensorFlow. This simplifies a lot of the code and model reuse that ends up with
    a clean pipeline. With the new eager execution mode of TensorFlow, it becomes
    even easier to implement and debug Python codes in `tf.keras` and TensorFlow.
    Eager execution allows the execution of codes without building a computational
    graph as we did in this book. It also allows code structures similar to a typical
    Python program.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中使用 Keras 有一定的学习曲线。此外，在 `tf.keras` 中，你可以利用 TensorFlow 新的易用 Dataset
    和 Estimators API。这简化了大量的代码和模型重用，最终形成一个干净的管道。随着 TensorFlow 新的急切执行模式的出现，实现和调试 Python
    代码在 `tf.keras` 和 TensorFlow 中变得更加容易。急切执行允许代码在不构建计算图的情况下执行，这与本书中所做的方式不同。它还允许代码结构类似于典型的
    Python 程序。
- en: Conclusion
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we've covered the policy gradient methods. Starting with the
    policy gradient theorem, we formulated four methods to train the policy network.
    The four methods, REINFORCE, REINFORCE with baseline, Actor-Critic, and A2C algorithms
    were discussed in detail. We explored how the four methods could be implemented
    in Keras. We then validated the algorithms by examining the number of times the
    agent successfully reached its goal and in terms of the total rewards received
    per episode.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了策略梯度方法。从策略梯度定理开始，我们制定了四种方法来训练策略网络。我们详细讨论了四种方法：REINFORCE、带基线的 REINFORCE、Actor-Critic
    和 A2C 算法。我们探索了这四种方法如何在 Keras 中实现。接着，我们通过检查代理成功达到目标的次数以及每个回合收到的总奖励来验证这些算法。
- en: Similar to Deep Q-Network [3] that we discussed in the previous chapter, there
    are several improvements that can be done on the fundamental policy gradient algorithms.
    For example, the most prominent one is the A3C [4] which is a multi-threaded version
    of A2C. This enables the agent to get exposed to different experiences simultaneously
    and to optimize the policy and value networks asynchronously. However, in the
    experiments conducted by OpenAI, [https://blog.openai.com/baselines-acktr-a2c/](https://blog.openai.com/baselines-acktr-a2c/),
    there is no strong advantage of A3C over A2C since the former could not take advantage
    of the strong GPUs available nowadays.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章讨论的 Deep Q-Network [3] 类似，基本的策略梯度算法可以进行一些改进。例如，最显著的一种是 A3C [4]，它是 A2C
    的多线程版本。这使得智能体能够同时接触到不同的经验，并异步地优化策略和值网络。然而，在 OpenAI 进行的实验中，[https://blog.openai.com/baselines-acktr-a2c/](https://blog.openai.com/baselines-acktr-a2c/)，A3C
    并没有比 A2C 强大的优势，因为前者无法充分利用如今强大的 GPU。
- en: Given that this is the end of the book, it's worth noting that the area of deep
    learning is huge, and to cover all the advances in one book like this is impossible.
    What we've done is carefully selected the advanced topics that I believe will
    be useful in a wide range of applications and that you, the reader will be able
    to easily build on. The implementations in Keras that have been illustrated throughout
    this book will allow you to carry on and apply the techniques in your own work
    and research.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这是本书的结尾，值得注意的是，深度学习领域非常广阔，要在一本书中涵盖所有的进展几乎是不可能的。我们所做的是精心挑选了那些我认为在广泛应用中会有用的高级话题，并且这些话题是你，读者，可以轻松构建的。本书中展示的
    Keras 实现将允许你继续进行，并将这些技术应用到你自己的工作和研究中。
- en: References
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sutton and Barto. *Reinforcement Learning: An Introduction*, [http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf),
    (2017).'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto。*强化学习：导论*，[http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)，（2017）。
- en: 'Mnih, Volodymyr, and others. *Human-level control through deep reinforcement
    learning*, *Nature* 518.7540 (2015): 529.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mnih, Volodymyr，及其他人。*通过深度强化学习实现人类水平的控制*，*自然* 518.7540 (2015): 529。'
- en: Mnih, Volodymyr, and others. *Asynchronous methods for deep reinforcement learning*, *International
    conference on machine learning*, 2016.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mnih, Volodymyr，及其他人。*深度强化学习的异步方法*，*国际机器学习会议*，2016。
- en: 'Williams and Ronald J. *Simple statistical gradient-following algorithms for
    connectionist reinforcement learning*, *Machine learning* 8.3-4 (1992): 229-256.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Williams 和 Ronald J. *简单统计梯度跟踪算法用于连接主义强化学习*，*机器学习* 8.3-4 (1992): 229-256。'
