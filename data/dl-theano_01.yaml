- en: Chapter 1. Theano Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。Theano基础
- en: This chapter presents Theano as a compute engine and the basics for symbolic
    computing with Theano. Symbolic computing consists of building graphs of operations
    that will be optimized later on for a specific architecture, using the computation
    libraries available for this architecture.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将Theano作为计算引擎介绍，并为符号计算打下基础。符号计算由构建操作图组成，这些操作图将在以后为特定架构进行优化，使用适用于该架构的计算库。
- en: Although this chapter might appear to be a long way from practical applications,
    it is essential to have an understanding of the technology for the following chapters;
    what is it capable of and what value does it bring? All the following chapters
    address the applications of Theano when building all possible deep learning architectures.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一章看起来与实际应用相距甚远，但了解这些技术对于后续章节至关重要；它能做什么，它带来了什么价值？接下来的章节将讨论在构建各种深度学习架构时，Theano的应用。
- en: 'Theano may be defined as a library for scientific computing; it has been available
    since 2007 and is particularly suited to deep learning. Two important features
    are at the core of any deep learning library: tensor operations, and the capability
    to run the code on CPU or **Graphical Computation Unit** (**GPU**). These two
    features enable us to work with a massive amount of multi-dimensional data. Moreover,
    Theano proposes automatic differentiation, a very useful feature that can solve
    a wider range of numeric optimizations than deep learning problems.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Theano可以定义为一个科学计算库；自2007年起可用，特别适用于深度学习。两个核心特性是任何深度学习库的基础：张量操作和将代码运行在CPU或**图形计算单元**（**GPU**）上的能力。这两个特性使我们能够处理大量的多维数据。此外，Theano还提供自动微分，这是一个非常有用的特性，可以解决比深度学习问题更广泛的数值优化问题。
- en: 'The chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Theano installation and loading
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theano安装与加载
- en: Tensors and algebra
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量与代数
- en: Symbolic programming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号编程
- en: Graphs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图
- en: Automatic differentiation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动微分
- en: GPU programming
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU编程
- en: Profiling
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能分析
- en: Configuration
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置
- en: The need for tensors
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量的需求
- en: 'Usually, input data is represented with multi-dimensional arrays:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，输入数据通过多维数组表示：
- en: '**Images have three dimensions**: The number of channels, the width, and the
    height of the image'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像有三个维度**：通道数、宽度和高度'
- en: '**Sounds and times series have one dimension**: The duration'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声音和时间序列具有一个维度**：持续时间'
- en: '**Natural language sequences can be represented by two-dimensional arrays**:
    The duration and the alphabet length or the vocabulary length'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言序列可以通过二维数组表示**：持续时间和字母表长度或词汇表长度'
- en: We'll see more examples of input data arrays in the future chapters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续章节中看到更多关于输入数据数组的例子。
- en: In Theano, multi-dimensional arrays are implemented with an abstraction class,
    named **tensor**, with many more transformations available than traditional arrays
    in a computer language such as Python.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Theano中，多维数组通过一个名为**张量**的抽象类实现，比Python等计算机语言中的传统数组有更多的变换功能。
- en: At each stage of a neural net, computations such as matrix multiplications involve
    multiple operations on these multi-dimensional arrays.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的每个阶段，诸如矩阵乘法等计算涉及对这些多维数组的多个操作。
- en: Classical arrays in programming languages do not have enough built-in functionalities
    to quickly and adequately address multi-dimensional computations and manipulations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 编程语言中的经典数组没有足够的内建功能，无法快速有效地处理多维计算和操作。
- en: Computations on multi-dimensional arrays have a long history of optimizations,
    with tons of libraries and hardware. One of the most important gains in speed
    has been permitted by the massive parallel architecture of the GPU, with computation
    ability on a large number of cores, from a few hundred to a few thousand.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对多维数组的计算有着悠久的优化历史，伴随着大量的库和硬件。速度提升的一个重要因素是GPU的大规模并行架构，利用数百到数千个核心的计算能力。
- en: Compared to the traditional CPU, for example, a quadricore, 12-core, or 32-core
    engine, the gains with GPU can range from 5x to 100x, even if part of the code
    is still being executed on the CPU (data loading, GPU piloting, and result outputting).
    The main bottleneck with the use of GPU is usually the transfer of data between
    the memory of the CPU and the memory of the GPU, but still, when well programmed,
    the use of GPU helps bring a significant increase in speed of an order of magnitude.
    Getting results in days rather than months, or hours rather than days, is an undeniable
    benefit for experimentation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的 CPU（例如四核、12 核或 32 核处理器）相比，GPU 的加速效果可以从 5 倍到 100 倍不等，即使部分代码仍然在 CPU 上执行（数据加载、GPU
    控制和结果输出）。使用 GPU 的主要瓶颈通常是 CPU 内存与 GPU 内存之间的数据传输，但如果编程得当，GPU 的使用能够显著提高计算速度，缩短实验时间，从几个月缩短到几天，甚至几天缩短到几个小时，这是实验中的一项不可忽视的优势。
- en: The Theano engine has been designed to address the challenges of multi-dimensional
    arrays and architecture abstraction from the beginning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 引擎从一开始就设计用于解决多维数组和架构抽象的挑战。
- en: 'There is another undeniable benefit of Theano for scientific computation: the
    automatic differentiation of functions of multi-dimensional arrays, a well-suited
    feature for model parameter inference via objective function minimization. Such
    a feature facilitates experimentation by releasing the pain to compute derivatives,
    which might not be very complicated, but are prone to many errors.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 对科学计算的另一个不可忽视的好处是：自动对多维数组的函数进行微分，这是通过目标函数最小化进行模型参数推断的一个非常合适的功能。这样的特性可以通过免去计算导数的麻烦来促进实验，尽管导数计算本身可能并不复杂，但容易出错。
- en: Installing and loading Theano
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和加载 Theano
- en: In this section, we'll install Theano, run it on the CPU and GPU devices, and
    save the configuration.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将安装 Theano，分别在 CPU 和 GPU 设备上运行它，并保存配置。
- en: Conda package and environment manager
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Conda 包管理和环境管理工具
- en: The easiest way to install Theano is to use `conda`, a cross-platform package
    and environment manager.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Theano 的最简单方法是使用 `conda`，一个跨平台的包和环境管理工具。
- en: 'If `conda` is not already installed on your operating system, the fastest way
    to install `conda` is to download the `miniconda` installer from [https://conda.io/miniconda.html](https://conda.io/miniconda.html).
    For example, for `conda under Linux 64 bit and Python 2.7`, use this command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的操作系统上尚未安装 `conda`，安装 `conda` 的最快方法是从 [https://conda.io/miniconda.html](https://conda.io/miniconda.html)
    下载 `miniconda` 安装程序。例如，对于 `Linux 64 位和 Python 2.7` 下的 `conda`，使用以下命令：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Conda enables us to create new environments in which versions of Python (2 or
    3) and the installed packages may differ. The `conda` root environment uses the
    same version of Python as the version installed on the system on which you installed
    `conda`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Conda 使我们能够创建新的环境，其中 Python（2 或 3）的版本以及安装的包可能会有所不同。`conda` 根环境使用与安装 `conda`
    的系统上相同版本的 Python。
- en: Installing and running Theano on CPU
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 CPU 上安装和运行 Theano
- en: 'Let''s install Theano:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来安装 Theano：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run a Python session and try the following commands to check your configuration:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个 Python 会话并尝试以下命令来检查您的配置：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The last command prints all the configuration of Theano. The `theano.config`
    object contains keys to many configuration options.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个命令打印出 Theano 的所有配置信息。`theano.config` 对象包含许多配置选项的键。
- en: 'To infer the configuration options, Theano looks first at the `~/.theanorc`
    file, then at any environment variables that are available, which override the
    former options, and lastly at the variable set in the code that are first in order
    of precedence:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推断配置选项，Theano 会首先查看 `~/.theanorc` 文件，然后查看任何可用的环境变量，这些环境变量会覆盖前面的选项，最后查看代码中设置的变量，这些变量按优先级顺序排列：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Some of the properties might be read-only and cannot be changed in the code,
    but `floatX`, which sets the default floating point precision for floats, is among
    the properties that can be changed directly in the code.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有些属性可能是只读的，无法在代码中更改，但 `floatX` 属性可以直接在代码中更改，它设置了浮点数的默认精度。
- en: Note
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It is advised to use `float32` since GPU has a long history without `float64`.
    `float64` execution speed on GPU is slower, sometimes much slower (2x to 32x on
    latest generation Pascal hardware), and `float32` precision is enough in practice.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用 `float32`，因为 GPU 在历史上并没有广泛支持 `float64`。在 GPU 上运行 `float64` 的速度较慢，有时甚至非常慢（在最新的
    Pascal 硬件上，可能慢 2 倍到 32 倍），而 `float32` 精度在实际应用中已经足够。
- en: GPU drivers and libraries
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 驱动和库
- en: Theano enables the use of GPU, units that are usually used to compute the graphics
    to display on the computer screen.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Theano启用GPU的使用，GPU单元通常用于计算显示在计算机屏幕上的图形。
- en: To have Theano work on the GPU as well, a GPU backend library is required on
    your system.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让Theano也能在GPU上工作，你的系统需要一个GPU后端库。
- en: The CUDA library (for NVIDIA GPU cards only) is the main choice for GPU computations.
    There is also the OpenCL standard, which is open source but far less developed,
    and much more experimental and rudimentary on Theano.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA库（仅适用于NVIDIA GPU卡）是进行GPU计算的主要选择。还有OpenCL标准，它是开源的，但远不如CUDA成熟，而且在Theano上的实现更为实验性和初步。
- en: Most scientific computations still occur on NVIDIA cards at the moment. If you
    have an NVIDIA GPU card, download CUDA from the NVIDIA website, [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads),
    and install it. The installer will install the latest version of the GPU drivers
    first, if they are not already installed. It will install the CUDA library in
    the `/usr/local/cuda` directory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数科学计算仍然发生在NVIDIA显卡上。如果你拥有NVIDIA GPU显卡，可以从NVIDIA官网[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)下载CUDA并安装。安装程序会首先安装最新版本的GPU驱动程序（如果尚未安装）。然后，它会将CUDA库安装到`/usr/local/cuda`目录。
- en: Install the cuDNN library, a library by NVIDIA, that offers faster implementations
    of some operations for the GPU. To install it, I usually copy the `/usr/local/cuda`
    directory to a new directory, `/usr/local/cuda-{CUDA_VERSION}-cudnn-{CUDNN_VERSION}`,
    so that I can choose the version of CUDA and cuDNN, depending on the deep learning
    technology I use and its compatibility.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 安装cuDNN库，这是NVIDIA提供的一个库，它为GPU提供更快的某些操作实现。为了安装它，我通常会将`/usr/local/cuda`目录复制到一个新目录`/usr/local/cuda-{CUDA_VERSION}-cudnn-{CUDNN_VERSION}`，这样我可以根据所使用的深度学习技术及其兼容性来选择CUDA和cuDNN的版本。
- en: 'In your .`bashrc` profile, add the following line to set the `$PATH` and `$LD_LIBRARY_PATH`
    variables:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的`.bashrc`配置文件中，添加以下行以设置`$PATH`和`$LD_LIBRARY_PATH`变量：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Installing and running Theano on GPU
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在GPU上安装并运行Theano
- en: N-dimensional GPU arrays have been implemented in Python in six different GPU
    libraries (`Theano/CudaNdarray,PyCUDA`/ `GPUArray,CUDAMAT`/ `CUDAMatrix`, `PYOPENCL`/`GPUArray`,
    `Clyther`, `Copperhead`), are a subset of `NumPy.ndarray`. `Libgpuarray` is a
    backend library to have them in a common interface with the same property.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: N维GPU数组已经在Python中通过六种不同的GPU库实现（`Theano/CudaNdarray,PyCUDA`/ `GPUArray,CUDAMAT`/
    `CUDAMatrix`, `PYOPENCL`/`GPUArray`, `Clyther`, `Copperhead`），它们是`NumPy.ndarray`的一个子集。`Libgpuarray`是一个后端库，它提供了一个共同的接口，具有相同的属性。
- en: 'To install `libgpuarray` with `conda`, use this command:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过`conda`安装`libgpuarray`，使用以下命令：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To run Theano in GPU mode, you need to configure the `config.device` variable
    before execution since it is a read-only variable once the code is run. Run this
    command with the `THEANO_FLAGS` environment variable:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要在GPU模式下运行Theano，需要在执行前配置`config.device`变量，因为该变量一旦代码运行后就为只读。可以通过设置`THEANO_FLAGS`环境变量来运行此命令：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first return shows that GPU device has been correctly detected, and specifies
    which GPU it uses.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个返回值表明GPU设备已被正确检测，并指定了使用的GPU。
- en: 'By default, Theano activates CNMeM, a faster CUDA memory allocator. An initial
    pre-allocation can be specified with the `gpuarra.preallocate` option. At the
    end, my launch command will be as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Theano会激活CNMeM，这是一个更快的CUDA内存分配器。可以使用`gpuarra.preallocate`选项指定初始预分配。最后，我的启动命令将如下所示：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first line confirms that cuDNN is active, the second confirms memory pre-allocation.
    The third line gives the default **context name** (that is, `None` when `flag
    device=cuda` is set) and the model of GPU used, while the default context name
    for the CPU will always be `cpu`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行确认cuDNN已激活，第二行确认内存预分配。第三行显示默认的**上下文名称**（当`flag device=cuda`被设置时为`None`），以及使用的GPU型号，而CPU的默认上下文名称始终为`cpu`。
- en: 'It is possible to specify a different GPU than the first one, setting the device
    to `cuda0`, `cuda1`,... for multi-GPU computers. It is also possible to run a
    program on multiple GPU in parallel or in sequence (when the memory of one GPU
    is not sufficient), in particular when training very deep neural nets, as for
    classification of full images as described in [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks*. In this case, the `contexts=dev0->cuda0;dev1->cuda1;dev2->cuda2;dev3->cuda3`
    flag activates multiple GPUs instead of one, and designates the context name to
    each GPU device to be used in the code. Here is an example on a 4-GPU instance:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指定与第一个 GPU 不同的 GPU，将设备设置为 `cuda0`、`cuda1` 等，用于多 GPU 计算机。在多个 GPU 上并行或顺序运行程序也是可能的（当一个
    GPU 的内存不足时），尤其是在训练非常深的神经网络时，比如在[第 7 章](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "第 7 章. 使用残差网络分类图像")，*使用残差网络分类图像*中描述的完整图像分类的场景中。在这种情况下，`contexts=dev0->cuda0;dev1->cuda1;dev2->cuda2;dev3->cuda3`
    标志会激活多个 GPU，而不是仅使用一个，并为代码中使用的每个 GPU 设备指定上下文名称。以下是一个 4-GPU 实例的示例：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To assign computations to a specific GPU in this multi-GPU setting, the names
    we choose, `dev0`, `dev1`, `dev2`, and `dev3`, have been mapped to each device
    (`cuda0`, `cuda1`, `cuda2`, `cuda3`).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种多 GPU 设置中，为了将计算分配到特定的 GPU，我们选择的名称 `dev0`、`dev1`、`dev2` 和 `dev3` 已映射到每个设备（`cuda0`、`cuda1`、`cuda2`、`cuda3`）。
- en: This name mapping enables to write codes that are independent of the underlying
    GPU assignments and libraries (CUDA or others).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种名称映射使得我们可以编写与底层 GPU 分配和库（CUDA 或其他）无关的代码。
- en: 'To keep the current configuration flags active at every Python session or execution
    without using environment variables, save your configuration in the `~/.theanorc`
    file as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 若要在每个 Python 会话或执行过程中保持当前配置标志处于激活状态，而不使用环境变量，请将配置保存在`~/.theanorc`文件中，如下所示：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now you can simply run `python` command. You are now all set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以简单地运行 `python` 命令。你现在一切就绪。
- en: Tensors
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: In Python, some scientific libraries such as NumPy provide multi-dimensional
    arrays. Theano doesn't replace Numpy, but it works in concert with it. NumPy is
    used for the initialization of tensors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，一些科学库如 NumPy 提供了多维数组。Theano 并不取代 NumPy，而是与它协同工作。NumPy 被用于张量的初始化。
- en: 'To perform the same computation on CPU and GPU, variables are symbolic and
    represented by the tensor class, an abstraction, and writing numerical expressions
    consists of building a computation graph of variable nodes and apply nodes. Depending
    on the platform on which the computation graph will be compiled, tensors are replaced
    by either of the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 CPU 和 GPU 上执行相同的计算，变量是符号的，并由张量类表示，张量类是一种抽象，编写数值表达式的过程包括构建一个包含变量节点和应用节点的计算图。根据将要编译计算图的平台，张量将被替换为以下任意一种：
- en: A `TensorType` variable, which has to be on CPU
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个必须位于 CPU 上的 `TensorType` 变量
- en: A `GpuArrayType` variable, which has to be on GPU
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个必须位于 GPU 上的 `GpuArrayType` 变量
- en: That way, the code can be written indifferently of the platform where it will
    be executed.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一来，代码就可以在不考虑平台的情况下编写，无论其将在哪个平台上执行。
- en: 'Here are a few tensor objects:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是几个张量对象：
- en: '| Object class | Number of dimensions | Example |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 对象类别 | 维度数 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `theano.tensor.scalar` | 0-dimensional array | 1, 2.5 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `theano.tensor.scalar` | 零维数组 | 1, 2.5 |'
- en: '| `theano.tensor.vector` | 1-dimensional array | [0,3,20] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `theano.tensor.vector` | 一维数组 | [0,3,20] |'
- en: '| `theano.tensor.matrix` | 2-dimensional array | [[2,3][1,5]] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `theano.tensor.matrix` | 二维数组 | [[2,3][1,5]] |'
- en: '| `theano.tensor.tensor3` | 3-dimensional array | [[[2,3][1,5]],[[1,2],[3,4]]]
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `theano.tensor.tensor3` | 三维数组 | [[[2,3][1,5]],[[1,2],[3,4]]] |'
- en: 'Playing with these Theano objects in the Python shell gives us a better idea:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python shell 中操作这些 Theano 对象，可以帮助我们更好地理解：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With `i`, `l`, `f`, or `d` in front of the object name, you initiate a tensor
    of a given type, `integer32`, `integer64`, `float32`, or `float64`. For real-valued
    (floating point) data, it is advised to use the direct form `T.scalar()` instead
    of the `f` or `d` variants since the direct form will use your current configuration
    for floats:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在对象名称前加上 `i`、`l`、`f` 或 `d`，你可以初始化一个给定类型的张量，如 `integer32`、`integer64`、`float32`
    或 `float64`。对于实值（浮动点）数据，建议使用直接形式 `T.scalar()`，而不是 `f` 或 `d` 的变体，因为直接形式会使用当前配置的浮动点数据：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Symbolic variables do either of the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 符号变量执行以下任一操作：
- en: 'Play the role of placeholders, as a starting point to build your graph of numerical
    operations (such as addition, multiplication): they receive the flow of the incoming
    data during the evaluation once the graph has been compiled'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扮演占位符的角色，作为构建数字运算图（例如加法、乘法）的起点：它们在图形编译后评估时接收输入数据的流。
- en: Represent intermediate or output results
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示中间或输出结果
- en: 'Symbolic variables and operations are both part of a computation graph that
    will be compiled either on CPU or GPU for fast execution. Let''s write our first
    computation graph consisting of a simple addition:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 符号变量和操作都是计算图的一部分，该图将在CPU或GPU上编译，以实现快速执行。让我们编写第一个计算图，内容是一个简单的加法操作：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: First, two symbolic variables, or *variable nodes*, are created, with the names
    `x` and `y`, and an addition operation, an *apply node*, is applied between both
    of them to create a new symbolic variable, `z`, in the computation graph.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建两个符号变量或*变量节点*，分别命名为`x`和`y`，并在它们之间应用加法操作，即一个*应用节点*，从而在计算图中创建一个新的符号变量`z`。
- en: The pretty print function, `pp`, prints the expression represented by Theano
    symbolic variables. `Eval` evaluates the value of the output variable, `z`, when
    the first two variables, `x` and `y`, are initialized with two numerical 2-dimensional
    arrays.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 美化打印函数`pp`打印由Theano符号变量表示的表达式。`Eval`在初始化`x`和`y`这两个变量并给它们赋值为两个二维数组时，计算输出变量`z`的值。
- en: 'The following example shows the difference between the variables `x` and `y`,
    and their names `x` and `y`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了变量`x`和`y`以及它们的名称`x`和`y`之间的区别：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Without names, it is more complicated to trace the nodes in a large graph.
    When printing the computation graph, names significantly help diagnose problems,
    while variables are only used to handle the objects in the graph:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 没有名称时，在大型图形中追踪节点会更加复杂。当打印计算图时，名称可以显著帮助诊断问题，而变量仅用于处理图中的对象：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the original symbolic variable, named `x`, does not change and stays part
    of the computation graph. `x + x` creates a new symbolic variable we assign to
    the Python variable `x`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，原始的符号变量`x`没有改变，并且仍然是计算图的一部分。`x + x`创建了一个新的符号变量，我们将其赋值给Python变量`x`。
- en: 'Note also that with the names, the plural form initializes multiple tensors
    at the same time:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，使用名称时，复数形式会同时初始化多个张量：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, let's have a look at the different functions to display the graph.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看不同的函数来显示图形。
- en: Graphs and symbolic computing
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形与符号计算
- en: 'Let''s take back the simple addition example and present different ways to
    display the same information:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到简单加法的例子，并展示不同的方式来显示相同的信息：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, the `debugprint` function prints the pre-compilation graph, the unoptimized
    graph. In this case, it is composed of two variable nodes, `x` and `y`, and an
    apply node, the elementwise addition, with the `no_inplace` option. The `inplace`
    option will be used in the optimized graph to save memory and re-use the memory
    of the input to store the result of the operation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`debugprint`函数打印的是预编译图形，即未优化的图形。在这种情况下，它由两个变量节点`x`和`y`组成，以及一个应用节点，即按元素加法，使用`no_inplace`选项。`inplace`选项将在优化后的图形中使用，以节省内存并重用输入的内存来存储操作结果。
- en: 'If the `graphviz` and `pydot` libraries have been installed, the `pydotprint`
    command outputs a PNG image of the graph:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经安装了`graphviz`和`pydot`库，`pydotprint`命令将输出图形的PNG图像：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Graphs and symbolic computing](img/00002.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图形与符号计算](img/00002.jpeg)'
- en: You might have noticed that the `z.eval` command takes while to execute the
    first time. The reason for this delay is the time required to optimize the mathematical
    expression and compile the code for the CPU or GPU before being evaluated.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，第一次执行`z.eval`命令时需要一些时间。这种延迟的原因是优化数学表达式并为CPU或GPU编译代码所需的时间，然后才会进行求值。
- en: 'The compiled expression can be obtained explicitly and used as a function that
    behaves as a traditional Python function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 编译后的表达式可以显式获得并作为函数使用，行为与传统的Python函数类似：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The first argument in the function creation is a list of variables representing
    the input nodes of the graph. The second argument is the array of output variables.
    To print the post compilation graph, use this command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 函数创建中的第一个参数是一个表示图形输入节点的变量列表。第二个参数是输出变量的数组。要打印编译后的图形，可以使用此命令：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Graphs and symbolic computing](img/00003.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图形与符号计算](img/00003.jpeg)'
- en: 'This case has been printed while using the GPU. During compilation, each operation
    has chosen the available GPU implementation. The main program still runs on CPU,
    where the data resides, but a `GpuFromHost` instruction performs a data transfer
    from the CPU to the GPU for input, while the opposite operation, `HostFromGpu`,
    fetches the result for the main program to display it:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 GPU 时打印了这个案例。在编译过程中，每个操作都选择了可用的 GPU 实现。主程序仍然运行在 CPU 上，数据驻留在 CPU，但 `GpuFromHost`
    指令会将数据从 CPU 传输到 GPU 作为输入，而相反的操作 `HostFromGpu` 会将结果取回供主程序显示：
- en: '![Graphs and symbolic computing](img/00004.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图表与符号计算](img/00004.jpeg)'
- en: 'Theano performs some mathematical optimizations, such as grouping elementwise
    operations, adding a new value to the previous addition:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 进行了一些数学优化，比如将逐元素操作进行分组，将新的值添加到之前的加法中：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The number of nodes in the graph has not increased: two additions have been
    merged into one node. Such optimizations make it more tricky to debug, so we''ll
    show you at the end of this chapter how to disable optimizations for debugging.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的节点数量没有增加：两个加法操作被合并成了一个节点。这样的优化使得调试变得更加复杂，因此我们将在本章的最后部分向你展示如何禁用优化以便调试。
- en: 'Lastly, let''s see a bit more about setting the initial value with NumPy:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们再看一下如何用 NumPy 设置初始值：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Executing the function on the NumPy arrays throws an error related to loss
    of precision, since the NumPy arrays here have `float64` and `int64` `dtypes`,
    but `x` and `y` are `float32`. There are multiple solutions to this; the first
    is to create the NumPy arrays with the right `dtype`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 数组上执行函数时，抛出了与精度丧失相关的错误，因为这里的 NumPy 数组有 `float64` 和 `int64` 的 `dtype`，而
    `x` 和 `y` 是 `float32`。对此有多种解决方案；第一种是使用正确的 `dtype` 创建 NumPy 数组：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Alternatively, cast the NumPy arrays (in particular for `numpy.diag`, which
    does not allow us to choose the `dtype` directly):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，转换 NumPy 数组（特别是对于 `numpy.diag`，它不允许我们直接选择 `dtype`）：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Or we could allow downcasting:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以允许类型下转：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Operations on tensors
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量上的操作
- en: We have seen how to create a computation graph composed of symbolic variables
    and operations, and compile the resulting expression for an evaluation or as a
    function, either on GPU or on CPU.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何创建一个由符号变量和操作组成的计算图，并将结果表达式编译为评估或作为一个函数，既可以在 GPU 上也可以在 CPU 上执行。
- en: As tensors are very important to deep learning, Theano provides lots of operators
    to work with tensors. Most operators that exist in scientific computing libraries
    such as NumPy for numerical arrays have their equivalent in Theano and have a
    similar name, in order to be more familiar to NumPy's users. But contrary to NumPy,
    expressions written with Theano can be compiled either on CPU or GPU.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于张量在深度学习中非常重要，Theano 提供了许多操作符来操作张量。大多数科学计算库中的操作符，例如 NumPy 中的数值数组操作符，在 Theano
    中都有对应的操作符，并且命名类似，以便让 NumPy 用户更熟悉。但与 NumPy 不同，使用 Theano 编写的表达式可以在 CPU 或 GPU 上编译执行。
- en: 'This, for example, is the case for tensor creation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这就是张量创建的情况：
- en: '`T.zeros()`, `T.ones()`, `T.eye()` operators take a shape tuple as input'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T.zeros()`、`T.ones()`、`T.eye()` 操作符接受一个形状元组作为输入'
- en: '`T.zeros_like()`, `T.one_like()`, `T.identity_like()` use the shape of the
    tensor argument'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T.zeros_like()`、`T.one_like()`、`T.identity_like()` 使用张量参数的形状'
- en: '`T.arange()`, `T.mgrid()`, `T.ogrid()` are used for range and mesh grid arrays'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T.arange()`、`T.mgrid()`、`T.ogrid()` 用于范围和网格数组'
- en: 'Let''s have a look in the Python shell:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Python shell 中看看：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Information such as the number of dimensions, `ndim`, and the type, `dtype`,
    are defined at tensor creation and cannot be modified later:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 信息如维度数量，`ndim`，和类型，`dtype`，在张量创建时定义，且不能在之后修改：
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Some other information, such as shape, is evaluated by the computation graph:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其他信息，如形状，通过计算图进行评估：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Dimension manipulation operators
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度操作符
- en: 'The first type of operator on tensor is for **dimension manipulation**. This
    type of operator takes a tensor as input and returns a new tensor:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的第一个操作符是用于**维度操作**的。这类操作符以张量作为输入并返回一个新的张量：
- en: '| Operator | Description |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 操作符 | 描述 |'
- en: '| --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `T.reshape` | Reshape the dimension of the tensor |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `T.reshape` | 重新调整张量的维度 |'
- en: '| `T.fill` | Fill the array with the same value |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `T.fill` | 用相同的值填充数组 |'
- en: '| `T.flatten` | Return all elements in a 1-dimensional tensor (vector) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `T.flatten` | 返回一个一维张量（向量）中的所有元素 |'
- en: '| `T.dimshuffle` | Change the order of the dimension, more or less like NumPy''s
    transpose method – the main difference is that it can be used to add or remove
    broadcastable dimensions (of length 1). |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `T.dimshuffle` | 改变维度的顺序，类似于 NumPy 的转置方法，主要区别在于可以添加或删除广播维度（长度为1的维度）。 |'
- en: '| `T.squeeze` | Reshape by removing dimensions equal to 1 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `T.squeeze` | 通过删除等于1的维度进行重塑 |'
- en: '| `T.transpose` | Transpose |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `T.transpose` | 转置 |'
- en: '| `T.swapaxes` | Swap dimensions |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `T.swapaxes` | 交换维度 |'
- en: '| `T.sort, T.argsort` | Sort tensor, or indices of the order |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `T.sort, T.argsort` | 对张量或排序索引进行排序 |'
- en: 'For example, the reshape operation''s output represents a new tensor, containing
    the same elements in the same order but in a different shape:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，重塑操作的输出表示一个新的张量，包含相同顺序的相同元素但具有不同形状：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The operators can be chained:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符可以链式连接：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Notice the use of traditional `[::-1]` array access by indices in Python and
    the `.T` for `T.transpose`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 Python 中传统的 `[::-1]` 数组索引访问和 `.T` 用于 `T.transpose`。
- en: Elementwise operators
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐元素操作
- en: The second type of operations on multi-dimensional arrays is elementwise operators.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 多维数组的第二类操作是逐元素操作。
- en: 'The first category of elementwise operations takes two input tensors of the
    same dimensions and applies a function, `f`, elementwise, which means on all pairs
    of elements with the same coordinates in the respective tensors `f([a,b],[c,d])
    = [ f(a,c), f(b,d)]`. For example, here''s multiplication:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类逐元素操作接受两个相同维度的输入张量，并逐元素应用函数 `f`，这意味着在各自张量中具有相同坐标的所有元素对上执行操作 `f([a,b],[c,d])
    = [ f(a,c), f(b,d)]`。
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The same multiplication can be written as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的乘法可以写成：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`T.add` and `T.mul` accept an arbitrary number of inputs:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`T.add` 和 `T.mul` 接受任意数量的输入：'
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Some elementwise operators accept only one input tensor `f([a,b]) = [f(a),f(b)])`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一些逐元素操作仅接受一个输入张量 `f([a,b]) = [f(a),f(b)])`：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Lastly, I would like to introduce the mechanism of **broadcasting**. When the
    input tensors do not have the same number of dimensions, the missing dimension
    will be broadcasted, meaning the tensor will be repeated along that dimension
    to match the dimension of the other tensor. For example, taking one multi-dimensional
    tensor and a scalar (0-dimensional) tensor, the scalar will be repeated in an
    array of the same shape as the multi-dimensional tensor so that the final shapes
    will match and the elementwise operation will be applied, `f([a,b], c) = [ f(a,c),
    f(b,c) ]`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想介绍**广播机制**。当输入张量的维度不相同时，缺失的维度将被广播，这意味着张量将沿该维度重复以匹配另一个张量的维度。例如，取一个多维张量和一个标量（0维）张量，标量将在一个与多维张量形状相同的数组中重复，从而最终形状将匹配并且逐元素操作将被应用，`f([a,b],
    c) = [ f(a,c), f(b,c) ]`：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is a list of elementwise operations:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些逐元素操作的列表：
- en: '| Operator | Other form | Description |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 操作符 | 其他形式 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `T.add, T.sub, T.mul, T.truediv` | `+, -, *, /` | Add, subtract, multiply,
    divide |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `T.add, T.sub, T.mul, T.truediv` | `+, -, *, /` | 加法、减法、乘法、除法 |'
- en: '| `T.pow, T.sqrt` | `**, T.sqrt` | Power, square root |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `T.pow, T.sqrt` | `**, T.sqrt` | 幂、平方根 |'
- en: '| `T.exp, T.log` |   | Exponential, logarithm |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `T.exp, T.log` |   | 指数、对数 |'
- en: '| `T.cos, T.sin, T.tan` |   | Cosine, sine, tangent |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `T.cos, T.sin, T.tan` |   | 余弦、正弦、正切 |'
- en: '| `T.cosh, T.sinh, T.tanh` |   | Hyperbolic trigonometric functions |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `T.cosh, T.sinh, T.tanh` |   | 双曲三角函数 |'
- en: '| `T.intdiv, T.mod` | `//, %` | Int div, modulus |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `T.intdiv, T.mod` | `//, %` | 整数除法、求余 |'
- en: '| `T.floor, T.ceil, T.round` |   | Rounding operators |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `T.floor, T.ceil, T.round` |   | 舍入操作符 |'
- en: '| `T.sgn` |   | Sign |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `T.sgn` |   | 符号 |'
- en: '| `T.and_, T.xor, T.or_, T.invert` | `&,^,&#124;,~` | Bitwise operators |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `T.and_, T.xor, T.or_, T.invert` | `&,^,&#124;,~` | 按位操作符 |'
- en: '| `T.gt, T.lt, T.ge, T.le` | `>, <, >=, <=` | Comparison operators |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `T.gt, T.lt, T.ge, T.le` | `>, <, >=, <=` | 比较操作符 |'
- en: '| `T.eq, T.neq, T.isclose` |   | Equality, inequality, or close with tolerance
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `T.eq, T.neq, T.isclose` |   | 等于、不等于或接近于某个值（带容差） |'
- en: '| `T.isnan` |   | Comparison with NaN (not a number) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `T.isnan` |   | 与 NaN（不是一个数字）的比较 |'
- en: '| `T.abs_` |   | Absolute value |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| `T.abs_` |   | 绝对值 |'
- en: '| `T.minimum, T.maximum` |   | Minimum and maximum elementwise |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| `T.minimum, T.maximum` |   | 逐元素的最小值和最大值 |'
- en: '| `T.clip` |   | Clip the values between a maximum and a minimum |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `T.clip` |   | 将值限制在最大值和最小值之间 |'
- en: '| `T.switch` |   | Switch |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `T.switch` |   | 开关 |'
- en: '| `T.cast` |   | Tensor type casting |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `T.cast` |   | 张量类型转换 |'
- en: The elementwise operators always return an array with the same size as the input
    array. `T.switch` and `T.clip` accept three inputs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 元素级别的运算符总是返回与输入数组相同大小的数组。`T.switch`和`T.clip`接受三个输入。
- en: 'In particular, `T.switch` will perform the traditional `switch` operator elementwise:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，`T.switch`会逐元素地执行传统的`switch`运算符：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: At the same position where `cond` tensor is true, the result has the `x` value;
    otherwise, if it is false, it has the `y` value.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当`cond`张量为真时，结果为`x`值；否则，如果为假，则为`y`值。
- en: 'For the `T.switch` operator, there is a specific equivalent, `ifelse`, that
    takes a scalar condition instead of a tensor condition. It is not an elementwise
    operation though, and supports lazy evaluation (not all elements are computed
    if the answer is known before it finishes):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`T.switch`运算符，有一个特定的等效运算符`ifelse`，它接受标量条件而不是张量条件。尽管如此，它并不是元素级的操作，且支持惰性求值（如果答案在完成之前已知，则不会计算所有元素）：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Reduction operators
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维运算符
- en: 'Another type of operation on tensors is reductions, reducing all elements to
    a scalar value in most cases, and for that purpose, it is required to scan all
    the elements of the tensor to compute the output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种对张量的操作是降维，将所有元素缩减为标量值，在大多数情况下，计算输出时需要扫描所有张量元素：
- en: '| Operator | Description |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 运算符 | 描述 |'
- en: '| --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `T.max, T.argmax, T.max_and_argmax` | Maximum, index of the maximum |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| `T.max, T.argmax, T.max_and_argmax` | 最大值，最大值的索引 |'
- en: '| `T.min, T.argmin` | Minimum, index of the minimum |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| `T.min, T.argmin` | 最小值，最小值的索引 |'
- en: '| `T.sum, T.prod` | Sum or product of elements |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| `T.sum, T.prod` | 元素的和或积 |'
- en: '| `T.mean, T.var, T.std` | Mean, variance, and standard deviation |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `T.mean, T.var, T.std` | 均值、方差和标准差 |'
- en: '| `T.all, T.any` | AND and OR operations with all elements |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `T.all, T.any` | 对所有元素进行AND和OR操作 |'
- en: '| `T.ptp` | Range of elements (minimum, maximum) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `T.ptp` | 元素范围（最小值，最大值） |'
- en: 'These operations are also available row-wise or column-wise by specifying an
    axis and the dimension along which the reduction is performed:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作也可以按行或按列进行，通过指定轴和执行降维的维度。
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Linear algebra operators
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性代数运算符
- en: 'A third category of operations are the linear algebra operators, such as matrix
    multiplication:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类运算是线性代数运算符，如矩阵乘法：
- en: '![Linear algebra operators](img/00005.jpeg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![线性代数运算符](img/00005.jpeg)'
- en: 'Also called inner product for vectors:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为向量的内积：
- en: '![Linear algebra operators](img/00006.jpeg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![线性代数运算符](img/00006.jpeg)'
- en: '| Operator | Description |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 运算符 | 描述 |'
- en: '| --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `T.dot` | Matrix multiplication/inner product |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `T.dot` | 矩阵乘法/内积 |'
- en: '| `T.outer` | Outer product |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| `T.outer` | 外积 |'
- en: There are some generalized (`T.tensordot` to specify the axis), or batched (`batched_dot,
    batched_tensordot`) versions of the operators.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些广义版本（`T.tensordot`用来指定轴），或批量版本（`batched_dot, batched_tensordot`）的运算符。
- en: 'Lastly, a few operators remain and can be very useful, but they do not belong
    to any of the previous categories: `T.concatenate` concatenates the tensors along
    the specified dimension, `T.stack` creates a new dimension to stack the input
    tensors, and `T.stacklist` creates new patterns to stack tensors together:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，仍有一些运算符非常有用，但它们不属于任何前面的类别：`T.concatenate`沿指定维度连接张量，`T.stack`创建一个新维度来堆叠输入张量，`T.stacklist`创建新的模式将张量堆叠在一起：
- en: '[PRE38]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'An equivalent of the NumPy expressions `a[5:] = 5` and `a[5:] += 5` exists
    as two functions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy表达式`a[5:] = 5`和`a[5:] += 5`的等效运算是两个函数：
- en: '[PRE39]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Unlike NumPy's syntax, the original tensor is not modified; instead, a new variable
    is created that represents the result of that modification. Therefore, the original
    variable `a` still refers to the original value, and the returned variable (here
    unassigned) represents the updated one, and the user should use that new variable
    in the rest of their computation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与NumPy的语法不同，原始张量不会被修改；相反，会创建一个新的变量，表示该修改的结果。因此，原始变量`a`仍然指向原始值，返回的变量（此处未赋值）表示更新后的值，用户应在其余计算中使用该新变量。
- en: Memory and variables
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存和变量
- en: 'It is good practice to always cast float arrays to the `theano.config.floatX`
    type:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 始终将浮点数组转换为`theano.config.floatX`类型是一个好习惯：
- en: Either at the array creation with `numpy.array(array, dtype=theano.config.floatX)`
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过`numpy.array(array, dtype=theano.config.floatX)`在创建数组时指定类型
- en: Or by casting the array as `array.as_type(theano.config.floatX)` so that when
    compiling on the GPU, the correct type is used
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或通过将数组转换为`array.as_type(theano.config.floatX)`，以便在GPU上编译时使用正确的类型。
- en: 'For example, let''s transfer the data manually to the GPU (for which the default
    context is None), and for that purpose, we need to use `float32` values:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，手动将数据转移到GPU（默认上下文为None），为此我们需要使用`float32`值：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `transfer(device)` functions, such as `transfer(''cpu'')`, enable us to
    move the data from one device to another one. It is particularly useful when parts
    of the graph have to be executed on different devices. Otherwise, Theano adds
    the transfer functions automatically to the GPU in the optimization phase:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`transfer(device)`函数，例如`transfer(''cpu'')`，使我们能够将数据从一个设备移动到另一个设备。当图的某些部分需要在不同设备上执行时，这尤其有用。否则，Theano会在优化阶段自动向GPU添加转移函数：'
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using the transfer function explicitly, Theano removes the transfer back to
    CPU. Leaving the output tensor on the GPU saves a costly transfer:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 显式使用转移函数，Theano去除了转移回CPU的操作。将输出张量保留在GPU上可以节省昂贵的传输：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The default context for the CPU is `cpu`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: CPU的默认上下文是`cpu`：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'A hybrid concept between numerical values and symbolic variables is the shared
    variables. They can also lead to better performance on the GPU by avoiding transfers.
    Initializing a shared variable with the scalar zero:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 数值与符号变量之间的混合概念是共享变量。它们还可以通过避免传输来提高GPU的性能。用标量零初始化共享变量：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Shared values are designed to be shared between functions. They can also be
    seen as an internal state. They can be used indifferently from the GPU or the
    CPU compile code. By default, shared variables are created on the default device
    (here, `cuda`), except for scalar integer values (as is the case in the previous
    example).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 共享值设计用于在函数之间共享。它们也可以看作是内部状态。它们可以无差别地用于GPU或CPU编译代码。默认情况下，共享变量是在默认设备（此处为`cuda`）上创建的，除非是标量整数值（如前面的例子所示）。
- en: 'It is possible to specify another context, such as `cpu`. In the case of multiple
    GPU instances, you''ll define your contexts in the Python command line, and decide
    on which context to create the shared variables:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指定另一个上下文，例如`cpu`。在多个GPU实例的情况下，您需要在Python命令行中定义上下文，并决定在哪个上下文中创建共享变量：
- en: '[PRE45]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Functions and automatic differentiation
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数和自动微分
- en: 'The previous section introduced the `function` instruction to compile the expression.
    In this section, we develop some of the following arguments in its signature:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了`function`指令来编译表达式。在这一节中，我们将展开其签名中的一些参数：
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We've already used the `allow_input_downcast` feature to convert data from `float64`
    to `float32`, `int64` to `int32` and so on. The `mode` and `profile` features
    are also displayed because they'll be presented in the optimization and debugging
    section.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了`allow_input_downcast`功能将数据从`float64`转换为`float32`，`int64`转换为`int32`，以此类推。`mode`和`profile`功能也会显示，因为它们将在优化和调试部分展示。
- en: Input variables of a Theano function should be contained in a list, even when
    there is a single input.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Theano函数的输入变量应该包含在列表中，即使只有一个输入。
- en: 'For outputs, it is possible to use a list in the case of multiple outputs to
    be computed in parallel:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出，在多个输出需要并行计算的情况下，可以使用列表：
- en: '[PRE48]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The second useful attribute is the `updates` attribute, used to set new values
    to shared variables once the expression has been evaluated:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个有用的属性是`updates`属性，用于在表达式求值后设置共享变量的新值：
- en: '[PRE49]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Such a mechanism can be used as an internal state. The shared variable `w` has
    been defined outside the function.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制可以作为内部状态使用。共享变量`w`已在函数外定义。
- en: With the `givens` parameter, it is possible to change the value of any symbolic
    variable in the graph, without changing the graph. The new value will then be
    used by all the other expressions that were pointing to it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`givens`参数，可以更改图中任何符号变量的值，而无需更改图。新值将由所有指向它的其他表达式使用。
- en: 'The last and most important feature in Theano is the automatic differentiation,
    which means that Theano computes the derivatives of all previous tensor operators.
    Such a differentiation is performed via the `theano.grad` operator:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Theano中最后也是最重要的特性是自动微分，这意味着Theano会计算所有先前张量操作符的导数。这样的微分通过`theano.grad`操作符完成：
- en: '[PRE50]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![Functions and automatic differentiation](img/00007.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![函数和自动微分](img/00007.jpeg)'
- en: In the optimization graph, `theano.grad` has computed the gradient of ![Functions
    and automatic differentiation](img/00008.jpeg) with respect to `a`, which is a
    symbolic expression equivalent to *2 * a*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化图中，`theano.grad`已经计算了关于`a`的![函数和自动微分](img/00008.jpeg)的梯度，这是一个等价于*2 * a*的符号表达式。
- en: Note that it is only possible to take the gradient of a scalar, but the *wrt*
    variables can be arbitrary tensors.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有标量的梯度可以被求取，但*相对于*（wrt）变量可以是任意张量。
- en: Loops in symbolic computing
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 符号计算中的循环
- en: The Python `for` loop can be used outside the symbolic graph, as in a normal
    Python program. But outside the graph, a traditional Python `for` loop isn't compiled,
    so it will not be optimized with parallel and algebra libraries, cannot be automatically
    differentiated, and introduces costly data transfers if the computation subgraph
    has been optimized for GPU.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`for`循环可以在符号图外部使用，就像在普通的Python程序中一样。但在图外，传统的Python `for`循环不会被编译，因此它不会使用并行和代数库进行优化，也不能自动微分，并且如果计算子图已经为GPU优化，可能会引入昂贵的数据传输。
- en: 'That''s why a symbolic operator, `T.scan`, is designed to create a `for` loop
    as an operator inside the graph. Theano will unroll the loop into the graph structure
    and the whole unrolled loop is going to be compiled on the target architecture
    as the rest of the computation graph. Its signature is as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么符号操作符`T.scan`设计为在图中创建`for`循环作为操作符的原因。Theano会将这个循环展开到图结构中，整个展开后的循环会与其他计算图一起在目标架构上进行编译。其签名如下：
- en: '[PRE51]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The `scan` operator is very useful to implement array loops, reductions, maps,
    multi-dimensional derivatives such as Jacobian or Hessian, and recurrences.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`scan`操作符非常有用，可以实现数组循环、归约、映射、多维导数（如Jacobian或Hessian）以及递归。'
- en: 'The `scan` operator is running the `fn` function repeatedly for `n_steps`.
    If `n_steps` is `None`, the operator will find out by the length of the sequences:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`scan`操作符会重复运行`fn`函数，直到`n_steps`。如果`n_steps`是`None`，操作符将根据序列的长度来确定：'
- en: Note
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The step `fn` function is a function that builds a symbolic graph, and that
    function will only get called once. However, that graph will then be compiled
    into another Theano function that will be called repeatedly. Some users try to
    pass a compile Theano function as `fn`, which is not possible.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤`fn`函数是构建符号图的函数，该函数只会被调用一次。然而，该图会被编译成另一个Theano函数，然后会被反复调用。一些用户尝试将已编译的Theano函数传递给`fn`，这是不可能的。
- en: 'Sequences are the lists of input variables to loop over. The number of steps
    will correspond to the shortest sequence in the list. Let''s have a look:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 序列是循环中输入变量的列表。步数将对应于列表中最短的序列。让我们来看一下：
- en: '[PRE52]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The `scan` operator has been running the function against all elements in the
    input tensor, `a`, and kept the same shape as the input tensor, `(2,3)`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`scan`操作符已经在输入张量`a`的所有元素上运行该函数，并保持与输入张量相同的形状，`(2,3)`。'
- en: Note
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It is a good practice to add the updates returned by `theano.scan` in the `theano.function`,
    even if these updates are empty.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些更新为空，最好还是将`theano.scan`返回的更新添加到`theano.function`中。
- en: 'The arguments given to the `fn` function can be much more complicated. `T.scan`
    will call the `fn` function at each step with the following argument list, in
    the following order:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`fn`函数的参数可以更加复杂。`T.scan`会在每一步调用`fn`函数，并按以下顺序传入参数列表：
- en: '[PRE53]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'As shown in the following figure, three arrows are directed towards the `fn`
    step function and represent the three types of possible input at each time step
    in the loop:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，三个箭头指向`fn`步骤函数，代表循环中每个时间步的三种可能输入类型：
- en: '![Loops in symbolic computing](img/00009.jpeg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![符号计算中的循环](img/00009.jpeg)'
- en: If specified, the `outputs_info` parameter is the initial state to use to start
    recurrence from. The parameter name does not sound very good, but the initial
    state also gives the shape information of the last state, as well as all other
    states. The initial state can be seen as the first output. The final output will
    be an array of states.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定，`outputs_info`参数是用于开始递归的初始状态。该参数名听起来不太好，但初始状态也提供了最后一个状态的形状信息，以及所有其他状态。初始状态可以看作是第一个输出。最终输出将是一个状态数组。
- en: 'For example, to compute the cumulative sum in a vector, with an initial state
    of the sum at `0`, use this code:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要计算一个向量的累积和，初始状态为`0`，可以使用以下代码：
- en: '[PRE54]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: When `outputs_info` is set, the first dimension of the `outputs_info` and sequence
    variables is the time step. The second dimension is the dimensionality of data
    at each time step.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置 `outputs_info` 时，`outputs_info` 和序列变量的第一个维度是时间步。第二个维度是每个时间步的数据维度。
- en: In particular, `outputs_info` has the number of previous time-steps required
    to compute the first step.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，`outputs_info` 包含计算第一步所需的先前时间步数。
- en: 'Here is the same example, but with a vector at each time step instead of a
    scalar for the input data:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相同的示例，但每个时间步使用一个向量，而不是标量作为输入数据：
- en: '[PRE55]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Twenty steps along the rows (times) have accumulated the sum of all elements.
    Note that initial state (here `0`) given by the `outputs_info` argument is not
    part of the output sequence.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着行（时间步）走了二十步，累计了所有元素的和。注意，由 `outputs_info` 参数给出的初始状态（此处为 `0`）不属于输出序列的一部分。
- en: 'The recurrent function, `fn`, may be provided with some fixed data, independent
    of the step in the loop, thanks to the `non_sequences` scan parameter:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `non_sequences` 扫描参数，递归函数 `fn` 可以在每次循环步骤中提供一些固定数据，独立于当前步骤：
- en: '[PRE56]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: It is multiplying the prior value by `5` and adding the new element.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 它将先前的值乘以 `5` 并添加新元素。
- en: Note that `T.scan` in the optimized graph on GPU does not execute different
    iterations of the loop in parallel, even in the absence of recurrence.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，优化后的图在 GPU 上的 `T.scan` 不会并行执行循环的不同迭代，即使没有递归。
- en: Configuration, profiling and debugging
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置、分析和调试
- en: 'For debugging purpose, Theano can print more verbose information and offers
    different optimization modes:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调试目的，Theano 可以打印更详细的信息，并提供不同的优化模式：
- en: '[PRE57]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In order for Theano to use the `config.optimizer` value, the mode has to be
    set to `Mode`, otherwise the value in `config.mode` will be used:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 Theano 使用 `config.optimizer` 值，必须将模式设置为 `Mode`，否则将使用 `config.mode` 中的值：
- en: '| config.mode / function mode | config.optimizer (*) | Description |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| config.mode / 函数模式 | config.optimizer (*) | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `FAST_RUN` | `fast_run` | Default; best run performance, slow compilation
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| `FAST_RUN` | `fast_run` | 默认；最佳运行性能，编译较慢 |'
- en: '| `FAST_RUN` | `None` | Disable optimizations |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| `FAST_RUN` | `None` | 禁用优化 |'
- en: '| `FAST_COMPILE` | `fast_compile` | Reduce the number of optimizations, compiles
    faster |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| `FAST_COMPILE` | `fast_compile` | 减少优化次数，编译更快 |'
- en: '| `None` |   | Use the default mode, equivalent to `FAST_RUN`; `optimizer=None`
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| `None` |   | 使用默认模式，相当于 `FAST_RUN`；`optimizer=None` |'
- en: '| `NanGuardMode` |   | NaNs, Infs, and abnormally big value will raise errors
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `NanGuardMode` |   | NaN、Inf 和异常大的值将引发错误 |'
- en: '| `DebugMode` |   | Self-checks and assertions during compilation |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `DebugMode` |   | 编译过程中进行自检和断言 |'
- en: 'The same parameter as in `config.mode` can be used in the `Mode` parameter
    in the function compile:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数编译中，`config.mode` 中相同的参数可以用于 `Mode` 参数：
- en: '[PRE58]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Disabling optimization and choosing high verbosity will help finding errors
    in the computation graph.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用优化并选择高详细度输出有助于找到计算图中的错误。
- en: 'For debugging on the GPU, you need to set a synchronous execution with the
    environment variable `CUDA_LAUNCH_BLOCKING`, since GPU execution is by default,
    fully asynchronous:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPU 调试，您需要通过环境变量 `CUDA_LAUNCH_BLOCKING` 设置同步执行，因为 GPU 执行默认是完全异步的：
- en: '[PRE59]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: To find out the origin of the latencies in your computation graph, Theano provides
    a profiling mode.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出计算图中延迟的来源，Theano 提供了分析模式。
- en: 'Activate profiling:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 激活分析：
- en: '[PRE60]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Activate memory profiling:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 激活内存分析：
- en: '[PRE61]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Activate profiling of optimization phase:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 激活优化阶段的分析：
- en: '[PRE62]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Or directly during compilation:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 或者直接在编译期间：
- en: '[PRE63]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The first concept is symbolic computing, which consists in building graph, that
    can be compiled and then executed wherever we decide in the Python code. A compiled
    graph is acting as a function that can be called anywhere in the code. The purpose
    of symbolic computing is to have an abstraction of the architecture on which the
    graph will be executed, and which libraries to compile it with. As presented,
    symbolic variables are typed for the target architecture during compilation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个概念是符号计算，它是构建可以编译并在 Python 代码中任何地方执行的图。一个编译后的图就像一个函数，可以在代码中的任何地方调用。符号计算的目的是对图将执行的架构进行抽象，以及选择哪些库来进行编译。如前所述，符号变量在编译期间会根据目标架构进行类型化。
- en: The second concept is the tensor, and the operators provided to manipulate tensors.
    Most of these were already available in CPU-based computation libraries, such
    as NumPy or SciPy. They have simply been ported to symbolic computing, requiring
    their equivalents on GPU. They use underlying acceleration libraries, such as
    BLAS, Nvidia Cuda, and cuDNN.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个概念是张量，以及用于操作张量的运算符。这些运算符大部分已经在基于 CPU 的计算库中提供，例如 NumPy 或 SciPy。它们只是被移植到符号计算中，需要在
    GPU 上提供其等价物。它们使用底层加速库，如 BLAS、Nvidia Cuda 和 cuDNN。
- en: The last concept introduced by Theano is automatic differentiation—a very useful
    feature in deep learning to backpropagate errors and adjust the weights following
    the gradients, a process known as *gradient descent*. Also, the `scan` operator
    enables us to program loops (`while...`, `for...`,) on the GPU, and, as other
    operators, available through backpropagation as well, simplifying the training
    of models a lot.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 引入的最后一个概念是自动微分——这是深度学习中非常有用的特性，用于反向传播误差并根据梯度调整权重，这一过程被称为*梯度下降*。另外，`scan`
    运算符使我们能够在 GPU 上编程循环（`while...`、`for...`），并且与其他运算符一样，也通过反向传播提供，极大简化了模型的训练。
- en: We are now ready to apply this to deep learning in the next few chapters and
    have a look at this knowledge in practice.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备在接下来的几章中将这些应用到深度学习中，看看这些知识在实际中的应用。
