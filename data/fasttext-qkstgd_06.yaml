- en: Sentence Classification in FastText
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FastText 中的句子分类
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Sentence classification
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子分类
- en: 'fastText supervised learning:'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastText 监督学习：
- en: Architecture
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构
- en: Hierarchical softmax architecture
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次化软最大架构
- en: 'N-grams features and the hashing trick:'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-gram 特征与哈希技巧：
- en: The **Fowler**-**Noll**-**Vo** (**FNV**) hash
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fowler**-**Noll**-**Vo**（**FNV**）哈希'
- en: Word embeddings and their use in sentence classification
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入及其在句子分类中的应用
- en: 'fastText model quantization:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastText 模型离散化：
- en: 'Compression:'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩：
- en: Quantization
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散化
- en: 'Vector quantization:'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量离散化：
- en: Finding the codebook for high-dimensional spaces
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找高维空间的代码书
- en: Product quantization
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品离散化
- en: Additional steps
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外步骤
- en: Sentence classification
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子分类
- en: Sentence classification deals with understanding text found in natural languages
    and determining the classes that it may belong to. In the text classification
    set of problems, you will have a set of documents *d* that belongs to the corpus
    *X* (which contains all the documents). You will also have a set of finite classes
    *C* = *{c[1] , c[2], ..., c[n]}*. Classes are also called categories or labels.
    To train a model, you would need a classifier, which is generally a well-tested
    algorithm (not necessary but in this case we will be talking about a well-tested
    algorithm that is used in fastText) and you will need a corpus with documents
    and associated labeling identifying the classes that each document belongs to.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分类涉及理解自然语言中的文本，并确定其可能属于的类别。在文本分类问题集中，你将拥有一组文档 *d*，它属于语料库 *X*（包含所有文档）。你还将拥有一组有限的类别
    *C* = *{c[1], c[2], ..., c[n]}*。类别也称为类别或标签。为了训练一个模型，你需要一个分类器，通常是一个经过充分测试的算法（虽然不一定，但在这里我们讨论的是一个在
    fastText 中使用的经过充分测试的算法），你还需要一个包含文档和关联标签的语料库，用来标识每个文档属于哪个类别。
- en: 'Text classification has many practical uses, such as the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类有许多实际应用，如下所示：
- en: Creating spam classifiers in email
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建电子邮件垃圾邮件分类器
- en: Page ranking and indexing in search engines
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎中的页面排名和索引
- en: Sentiment detection in reviews that will give an idea whether customers are
    happy with the product or not
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论中的情感检测，可以了解客户是否对产品满意
- en: Text classification is generally a way to augment manual classification. Labeling
    a document is largely subjective and depends on the corpus. A sample document
    "I like traveling to Hawaii" may be regarded as falling under the class "Travel"
    by a librarian but may be regarded as "Irrelevant" by a doctor. So the idea is
    that a set of documents will be labeled by a domain expert, the labeled data will
    be used to train a text classifier, and then the text classifier can be used to
    predict new incoming text, saving the time and resources of the domain expert
    (maybe the domain expert can periodically check and audit the performance of the
    classifier against incoming text). Also, the proposed idea is for general people
    and does not apply to crowd-sourced labeling as done by stack overflow when it
    asks for users to label; most business problems do not have the luxury of such *auto*
    labeling and hence you will have to spend some time manually labeling the documents.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类通常是对人工分类的一种补充。标注文档在很大程度上是主观的，取决于语料库。举例来说，一个示例文档“我喜欢去夏威夷旅行”，可能会被图书馆员视为属于“旅行”类，但医生可能会认为它属于“无关”类。因此，想法是：一组文档将由领域专家进行标注，标注数据将用于训练文本分类器，然后该文本分类器可以用于预测新的输入文本，从而节省领域专家的时间和资源（也许领域专家可以定期检查并审核分类器对输入文本的表现）。此外，所提议的思路是针对一般人群的，并不适用于像
    Stack Overflow 这类通过众包进行标注的方式；大多数商业问题没有这种*自动*标注的奢侈，因此你必须花费一些时间手动标注文档。
- en: To evaluate the performance of a classification model, we divide the training
    corpus into test and train sets. Only the train set is used for model training.
    Once done, we classify the test set and compare the predictions with the actual
    ones and measure the performance. The portion of correctly classified documents
    to the portion of actual documents is called accuracy. There are two more parameters
    that we can look at that will give a measure for the model performance. One is
    the *recall*, which means the percentage of all the correct labels that we recalled
    as opposed to the labels that actually existed. We can also look at the *precision*
    of the model, which means that we look at all the predicted labels and say which
    portions of them are the actual labels in the first place.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估分类模型的表现，我们将训练语料库分为测试集和训练集。只有训练集用于模型训练。完成训练后，我们对测试集进行分类，将预测结果与实际结果进行比较，并衡量性能。正确分类文档的比例与实际文档的比例称为准确率。我们还可以查看另外两个参数，它们可以衡量模型的表现。一个是
    *召回率*，它表示我们回忆出的所有正确标签的百分比，而不是实际存在的标签。我们还可以查看模型的 *精准度*，即我们查看所有预测标签，并判断其中哪些是最初的实际标签。
- en: fastText supervised learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fastText 有监督学习
- en: A fastText classifier is built on top of a linear classifier, specifically a
    BoW classifier. In this section, you will get to know the architecture of the
    fastText classifier and how it works.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 分类器是建立在线性分类器之上的，特别是 BoW 分类器。在这一节中，你将了解 fastText 分类器的架构以及它是如何工作的。
- en: Architecture
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'You can consider that each piece of text and each label is actually a vector
    in space and the coordinates of that vector are what we are actually trying to
    tweak and train so that the vector for a text and associated label are really
    close in space:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以认为每一段文本和每一个标签实际上都是空间中的一个向量，而这个向量的坐标是我们实际上试图调整和训练的目标，目的是使文本向量和其相关标签的向量在空间中非常接近：
- en: '![](img/00055.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: Vector representation of the text
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的向量表示
- en: So, in this example, which is an example shown in 2D space, you have texts that
    are saying things such as "Nigerian Tommy Thompson is also a relative newcomer
    to the wrestling scene" and "James scored 20 of his 46 points in the opening quarter"
    are closer to the "sports" label and not the "travel" label.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，这是一个二维空间中的示例，你会看到类似于“Nigerian Tommy Thompson 也是摔跤界的一个新面孔”以及“James 在开场的第一节得到了46分中的20分”的文本，它们更接近“体育”标签而非“旅游”标签。
- en: The way we can do this is we can take the vector representing the text and the
    vector representing the label and input it into the scoring function. We then
    take the score and then we normalize across sum of all the scores between the
    vector representing the text and the vector representations for every other possible
    label. And that provides us with the type of probability that the given text above
    will have the label. The scores are the individual values and we convert them
    to probabilities using the softmax function which was also discussed in the previous
    chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过这样的方式来实现：我们将代表文本的向量和代表标签的向量输入评分函数。然后我们得到一个分数，并对所有文本向量与每个可能标签的向量表示之间的分数进行归一化。这样就可以得出给定文本与该标签的概率。分数是单独的值，我们使用之前章节中讨论过的
    softmax 函数将它们转换为概率。
- en: The fastText uses similar vector space models for text classification, where
    the words are reduced to low-dimensional vectors called embeddings. The aim is
    to train and arrive at a vector space such that the sentence vectors and the label
    vectors are really close to each other. To apply vector space models to sentences
    or documents, one must first select an appropriate function, which is a mathematical
    process for combining multiple words.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 使用类似的向量空间模型进行文本分类，其中单词被压缩成称为嵌入的低维向量。其目的是训练并得到一个向量空间，使得句子向量和标签向量彼此非常接近。要将向量空间模型应用于句子或文档，首先需要选择一个适当的函数，这是一个将多个单词结合起来的数学过程。
- en: 'Composition functions fall into two classes: unordered or syntactic. Unordered
    functions treat input texts as **bag of words** (**BoW**) embeddings, while syntactic
    representations take word order and sentence structure into account. The fastText
    is mostly an unordered approach since it takes the BoW approach but has a little
    bit of syntactic representations using the n-grams, as we will see later.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 组合函数分为两类：无序和句法。无序函数将输入文本视为**词袋模型**（**BoW**）嵌入，而句法表示则考虑单词顺序和句子结构。fastText大多采用无序方法，因为它采用词袋方法，但也通过使用n-gram有少量的句法表示，正如我们稍后将看到的那样。
- en: What you can do next is take the representations and then train a linear classifier
    on top of them. Good linear classifiers that can be used are logistic regression
    and **support vector machines** (**SVM**). Linear classifiers, though, have a
    little caveat. They do not share parameters between features and classes. As a
    result of this, there is a possibility that this limits the generalization capabilities
    to those types of classes which do not have many examples to train on. The solution
    to this problem is to use multilayered neural networks or to factorize the linear
    classifier into low rank matrices and then run a neural network on top of them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以做的是获取表示并在其上训练一个线性分类器。可以使用的优秀线性分类器有逻辑回归和**支持向量机**（**SVM**）。然而，线性分类器有一个小问题。它们在特征和类别之间不共享参数。因此，这可能会限制其对训练样本较少的类别的泛化能力。解决这个问题的方法是使用多层神经网络，或者将线性分类器分解为低秩矩阵，然后在其上运行一个神经网络。
- en: Syntactic representations are more sparse than BoW approach and hence require
    more training time. This makes them computationally very expensive in case of
    huge datasets or when you have limited computational resources. For example, if
    you build a recursive neural network for training on syntactic word representations
    that again computes costly tensor products, and furthermore there will be non-linearity
    in every node of a syntactic parse tree, then your training time may stretch to
    days, which is prohibitive for fast feedback cycles.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 句法表示比BoW方法更加稀疏，因此需要更多的训练时间。这使得它们在处理大数据集或计算资源有限时非常昂贵。例如，如果你构建一个递归神经网络来训练句法词表示，这会再次计算昂贵的张量积，并且每个句法分析树的节点中都会有非线性，那么你的训练时间可能会延长至数天，这对于快速反馈周期来说是无法接受的。
- en: 'So you can use an averaging network, which is an unordered model that can be
    explained in three simple steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以使用一个平均网络，它是一个无序模型，可以通过三个简单的步骤来解释：
- en: Take the vector average of the embeddings associated with an input sequence
    of tokens.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对与输入序列相关联的嵌入向量取平均。
- en: Pass the average through one or more feed-forward layers.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将平均值通过一个或多个前馈层。
- en: Perform linear classification on the final layer's representation.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最终层的表示上执行线性分类。
- en: The model can be improved by applying a novel dropout-inspired regularizer.
    In this case, for each training instance, some of the token embeddings will be
    randomly dropped before computing the average. In fastText, this is done by subsampling
    frequent words, which was also discussed in the previous chapter and is used in
    the classifier as well.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用一种新的受dropout启发的正则化器，可以改进模型。在这种情况下，对于每个训练实例，在计算平均值之前，部分词嵌入将被随机丢弃。在fastText中，这是通过子采样频繁词汇来实现的，这在上一章也有讨论，并且同样用于分类器中。
- en: 'A general form of the architecture is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的一般形式如下：
- en: '![](img/00056.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: In this case, we want to map an input sequence of tokens to k labels. We first
    apply a composition function g to the sequence of word embeddings ν[ω] for ω ∈
    X. The output of this composition function is a vector z that serves as input
    to the logistic regression function. The architecture of the classification is
    similar to the cbow model that was discussed in the previous chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们希望将输入的词元序列映射到k个标签。我们首先对词嵌入序列ν[ω]（其中ω ∈ X）应用组合函数g。这个组合函数的输出是一个向量z，作为逻辑回归函数的输入。分类架构与上一章讨论的cbow模型相似。
- en: 'First, a weighted average of the word embeddings is taken:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，取词嵌入的加权平均值：
- en: '![](img/00057.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)'
- en: 'Feeding *z* to a softmax layer induces estimated probabilities for each output
    label:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将*z*输入到softmax层中会为每个输出标签产生估计的概率：
- en: '![](img/00058.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: 'Here, the softmax function is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，softmax函数如下所示：
- en: '![](img/00059.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00059.jpeg)'
- en: Here, `W_s` is a *k* x *d* matrix for the dataset with *k* output labels and
    *b* is a bias term. fastText uses a generic bias term in the form of an end of
    sentence character `<s>` that gets added to all input examples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`W_s` 是一个 *k* x *d* 的矩阵，表示具有 *k* 个输出标签的数据集，*b* 是一个偏置项。fastText 使用一个通用的偏置项，形式是一个句子结束字符`<s>`，该字符会被添加到所有输入样本中。
- en: 'This model can then be trained to minimize cross-entropy error, which for a
    single training instance with ground truth label *y* is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以通过最小化交叉熵误差来训练，对于具有真实标签 *y* 的单个训练实例，误差计算如下：
- en: '![](img/00060.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: 'In fastText, this gets translated to computing the probability distribution
    over the predefined classes. For the set of *N* documents, this leads to minimizing
    the negative log likelihood over the classes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 fastText 中，这被转换为计算预定义类别的概率分布。对于 *N* 个文档的集合，这将导致最小化类别上的负对数似然：
- en: '![](img/00061.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.jpeg)'
- en: Here, *x[n]* is the normalized bag of features of the nth document, *y[n]* is
    the label, and A and B are the weight matrices. A is just a lookup table over
    the words in fastText.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x[n]* 是第 *n* 个文档的标准化特征袋，*y[n]* 是标签，A 和 B 是权重矩阵。A 只是 fastText 中单词的查找表。
- en: Then we can use stochastic gradient descent to keep tweaking those coordinates
    until we maximize the probability of the correct label for every piece of text.
    The fastText trains this model asynchronously on multiple CPUs using stochastic
    gradient descent and a linearly decaying learning rate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用随机梯度下降不断调整这些坐标，直到我们最大化每个文本正确标签的概率。fastText 通过使用随机梯度下降和线性衰减的学习率，在多个
    CPU 上异步地训练这个模型。
- en: 'The architecture that is used in fastText is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 中使用的架构如下：
- en: '![](img/00062.gif)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.gif)'
- en: Model architecture of fastText for a sentence with N gram word features *x[1]*,
    *x[2]*, ..., *x[N]*. The features are embedded and averaged to form the hidden
    variable.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 的模型架构对于具有 N 元词特征 *x[1]*、*x[2]*、...、*x[N]* 的句子。特征会被嵌入并平均，形成隐藏变量。
- en: 'Now let''s summarize the architecture:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们总结一下架构：
- en: It starts with word representations, which are averaged into text representations,
    which are fed into a linear classifier
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从单词表示开始，单词表示被平均成文本表示，然后输入到线性分类器中。
- en: The classifier is essentially a linear model with a rank constraint and fast
    loss approximation
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该分类器本质上是一个具有秩约束和快速损失近似的线性模型。
- en: The text representation is a hidden state that can be shared among features
    and classes
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本表示是一个隐藏状态，可以在特征和类别之间共享。
- en: A softmax layer is used to obtain a probability distribution over predefined
    classes
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 softmax 层来获得预定义类别的概率分布。
- en: High computational complexity *O(kh)*, where *k* is the number of classes and
    *h* is the dimension of the text representation
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高计算复杂度 *O(kh)*，其中 *k* 是类别数，*h* 是文本表示的维度。
- en: Hierarchical softmax architecture
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化 softmax 架构。
- en: Finding the softmax is computationally quite expensive and prohibitive for a
    large corpus as this means that we not only have to find the score for the text
    with the label but the scores for the text with all the labels. So for *n* text
    and *m* labels, this scales with worst case performance of *O(n²)*, which you
    know is not good.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 softmax 是计算上非常昂贵的，并且对于大规模语料库来说是不可行的，因为这意味着我们不仅要为带有标签的文本找到分数，还要为所有标签的文本找到分数。因此，对于
    *n* 个文本和 *m* 个标签，这种情况的最坏性能为 *O(n²)*，显然这不是一个好的选择。
- en: Also, softmax and other similar methods for finding the probabilities do not
    take into account the semantically meaningful organization of classes. A classifier
    should know that classifying a dog as a submarine should have a higher penalty
    than a dog as a wolf. An intuition that we may have is that the target labels
    are probably not flat but rather a tree.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，softmax 及其他类似的方法在计算概率时并没有考虑类别的语义组织结构。分类器应该知道，将狗分类为潜艇的惩罚应大于将狗分类为狼的惩罚。我们可能有的直觉是，目标标签可能并不是平面的，而是类似树状结构的。
- en: 'So now we have *k* classes that we want each input to be classified into. So
    let''s consider these classes are the leaves of a tree. This tree is organized
    in such as way that the hierarchy is semantically meaningful. Consider further
    that our classifier maps an input to an output probability distribution over the
    leaves. Hopefully, this leaf will be the correct class of the corresponding input.
    The probability of any node in the tree is the probability of the path from the
    root to that node. If the node is at depth *l + 1* with parents *n[1]*, *n[2]*,
    ..., *n[l]* then the probability of the node is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们有 *k* 个类别，我们希望每个输入都能被分类到其中一个类别。我们可以认为这些类别是树的叶节点。此树的结构是有意义的层次结构。进一步假设我们的分类器将输入映射到叶节点上的输出概率分布。希望这个叶节点能代表对应输入的正确类别。树中任何节点的概率就是从根节点到该节点路径的概率。如果该节点位于深度
    *l + 1*，其父节点为 *n[1]*，*n[2]*，...，*n[l]*，则该节点的概率为：
- en: '![](img/00063.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: A leaf falls under the node if the node is on the path from the root to the
    leaf. We then define the amount of the "win" or the "winnings" to be the weighted
    sum of the probabilities of the nodes along its path from the root to the leaf
    corresponding to the correct classes. During the optimization or the training
    process, we want to maximize this "winnings" for our model, and conversely minimize
    the "loss". Loss in this case is considered the negative of the win.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个叶节点位于从根节点到叶节点的路径上，则该叶节点属于该路径上的节点。我们将“胜利”或“赢得”的金额定义为沿从根节点到叶节点路径上对应正确类别节点的概率的加权和。在优化或训练过程中，我们希望最大化这种“赢得”，相反，最小化“损失”。在这种情况下，损失被认为是胜利的负值。
- en: So in fastText, what is used is the hierarchical classifier, which is similar
    to the hierarchical softmax that you saw in the earlier chapter. In this method,
    it represents the labels in a binary tree and so every node in the binary tree
    is represented as a probability and so a label is represented by the probability
    along the path to that given label. In this case, the correct label is generated
    using the **breadth first search** (**BFS**) algorithm. BFS is quite fast for
    searching and hence you bring down the complexity to *log[2]n*. Now we just need
    to compute the probabilities of the path to the correct label. So when we have
    a lot of labels, this really increases the speed of computation for all the labels
    and hence the model training. And as you have seen in the previous chapter, the
    hierarchical probability representation asymptotes to the softmax probabilities
    and hence this approximations actually give the same kind of model performance
    and are vastly faster to train.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 fastText 中，使用的是层级分类器，它类似于你在前面章节中看到的层级 softmax。此方法将标签表示为二叉树中的节点，每个节点表示一个概率，因此标签通过从根节点到给定标签路径上的概率来表示。在这种方法中，正确的标签是使用**广度优先搜索**（**BFS**）算法生成的。BFS
    搜索非常快速，因此可以将复杂度降低到 *log[2]n*。现在我们只需要计算到正确标签路径的概率。当标签数量很多时，这会显著提高所有标签的计算速度，从而加速模型训练。正如你在上一章节中看到的，层级概率表示趋近于
    softmax 概率，因此这种近似方法实际上可以提供相同的模型性能，而且训练速度要快得多。
- en: As you have seen in the previous chapter, in this case the output of the hierarchical
    classifier is the label. Similar to training word embeddings, in this case a Huffman
    tree is formed. Since we have already discussed the internals of the Huffman tree
    in the previous chapter, in this case we will tinker at little bit with the code
    and try to see the exact tree that is formed and find the probabilities associated
    with it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前一章节中看到的，在这种情况下，层级分类器的输出是标签。类似于训练词向量，在此情况下会形成一个 Huffman 树。由于我们已经在上一章讨论了
    Huffman 树的内部结构，在此我们将稍微修改代码，尝试查看形成的确切树，并找到与之相关的概率。
- en: 'To keep things simple, we will take a very small dataset with very small number
    of labels. In this example, the following set of sentences along with the labels
    are taken and saved in a file named `labeledtextfile.txt`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将使用一个非常小的数据集，标签数量也很少。在这个例子中，以下是一组带有标签的句子，已保存在名为 `labeledtextfile.txt`
    的文件中：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Since fastText is written in C++, for performance reasons it does not manipulate
    and work with the direct label strings. To get the Huffman codes of the label,
    you can change the `hierarchicalSoftmax` function on line `81` of `model.cc` to
    the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 fastText 是用 C++ 编写的，出于性能考虑，它并不直接操作和处理标签字符串。要获取标签的 Huffman 码，你可以将 `model.cc`
    文件中第 `81` 行的 `hierarchicalSoftmax` 函数修改为以下内容：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see, I am listing for multiple labels. But you can choose the label
    that you want. You will get output similar to this. You will need to get the last
    occurrence of the target value to get the vector:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我正在列举多个标签。但是你可以选择你想要的标签。你将得到类似于这样的输出。你需要获取目标值的最后出现以获得向量：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So the similar vector, corresponding to target 2, is 101.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对应于目标2的相似向量是101。
- en: The n-gram features and the hashing trick
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: n-gram特征和哈希技巧
- en: As you have seen, the BoW of the vocabulary is taken to arrive at the word representation
    to be used later in the classification process. But the BoW is unordered and does
    not have any syntactic information. Hence, the bag of n-grams are used as additional
    features to capture some of the syntactic information.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，词汇的词袋（Bag of Words，BoW）用于生成后续分类过程中使用的单词表示。但是词袋是无序的，并且没有任何语法信息。因此，n-gram词袋被用作额外的特征，以捕捉部分语法信息。
- en: As we have already discussed, large-scale NLP problems almost always involve
    using a large corpus. This corpus will always have *unbounded* number of unique
    words, as we have seen from the Zipf's law. Words are generally defined as a string
    of characters separated with a delimiter, such as a space in English. Hence, taking
    word n-grams is simply not scalable to large corpora, which is essential to come
    to accurate classifications.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的那样，大规模的自然语言处理问题几乎总是涉及使用大型语料库。这个语料库总是具有*无限*数量的唯一单词，正如我们从Zipf定律中所看到的那样。单词通常被定义为由分隔符分隔的字符串，例如英语中的空格。因此，采用单词
    n-gram 对于大型语料库来说是*不可扩展的*，这对于准确分类至关重要。
- en: Because of these two factors, the matrices that are formed naively are always
    sparse and high-dimensional. You can try to reduce the dimensions of the matrices
    using techniques such as PCA but that would still involve doing matrix manipulations
    that require such a high amount of memory that it would make the whole computation
    infeasible.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这两个因素，天真地形成的矩阵总是稀疏且高维的。你可以尝试使用PCA等技术来减少矩阵的维数，但这仍然需要进行矩阵操作，需要如此大量的内存，以至于使整个计算变得不可行。
- en: 'What if you can do something so that you are able to circumvent the creation
    of the dictionary? A similar problem is tackled with what is known as the kernel
    trick. The kernel trick enables us to use linear classifiers on non-linear data.
    In this method, the input data is transformed to a high-dimensional feature space.
    Interestingly, you just need to specify the kernel for this step, no need to transform
    all the data to the feature space, and it will work. In other words, when you
    compute the distance and apply the kernel, you get a number. The number is the
    same as what you would have got if you expanded your initial points into the higher-order
    space that your kernel points to and computed their inner product:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能做些什么以避免创建字典？类似的问题可以通过所谓的核技巧来解决。核技巧使我们能够在非线性数据上使用线性分类器。在这种方法中，输入数据被转换为高维特征空间。有趣的是，你只需要为这一步指定核函数，而不需要将所有数据转换为特征空间，它就会起作用。换句话说，当你计算距离并应用核函数时，你会得到一个数字。这个数字与如果你将初始点扩展到你的核心指向的更高阶空间并计算它们的内积，你会得到相同的数字：
- en: '![](img/00064.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00064.jpeg)'
- en: 'Source: https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78
- en: It’s a lot easier to get the inner product in a higher-dimensional space than
    the actual points in the higher dimensional space.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中获得内积比在实际点中获得更容易。
- en: The challenges of text classification are complementary. The original input
    space is generally linearly separable (because generally humans decide on the
    features based on tagging) but the training set is prohibitively large in size
    and very high-dimensional. For this common scenario, a complementary variation
    to the kernel trick is used. This method is called the hashing trick. Here, the
    high-dimensional vectors in *ℜ^d* are mapped into a lower-dimensional feature
    space *ℜ^m* such that *m << d*. We will train the classifier in *ℜ^m* space.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的挑战是互补的。原始输入空间通常是线性可分的（因为通常是人类根据标记选择特征），但训练集的大小和高维度非常大。针对这种常见情况，核技巧的一种互补变体被使用。这种方法被称为哈希技巧。在这里，高维向量在*ℜ^d*空间中被映射到一个低维特征空间*ℜ^m*，使得*m
    << d*。我们将在*ℜ^m*空间中训练分类器。
- en: The core idea in the hashing trick is the hash functions. So in text classification
    and similar NLP tasks, we take a non-cryptographic hash such as murmur or FNV
    (more on this later in the chapter) and map the work into a finite integer (usually
    32-bit or 64-bit integers which are modulo of a prime number).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希技巧的核心思想就是哈希函数。因此，在文本分类和类似的NLP任务中，我们会使用一种非加密的哈希，如murmur或FNV（后面一章会详细介绍），并将工作映射到一个有限的整数（通常是32位或64位整数，通常是一个质数的模数）。
- en: 'The following are some of the characteristics that define a hash function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是定义哈希函数的一些特征：
- en: The most important one—if you feed the same input to a hash function, it will
    always give the same output.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的一点是——如果你将相同的输入传递给哈希函数，它始终会返回相同的输出。
- en: The choice of the hash function determines the range of the possible outputs.
    The range is generally fixed. For example, modern websites use SHA256 hashes that
    are truncated to 128 bits.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希函数的选择决定了输出可能的范围。这个范围通常是固定的。例如，现代网站使用SHA256哈希，它们会被截断为128位。
- en: Hash functions are meant to be one way. Given a hash, the input should not be
    computable.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希函数是单向的。给定一个哈希值，输入是无法计算出来的。
- en: 'Due to the fact that they have a fixed range, a pleasant side effect of using
    hash functions is that there are fixed memory requirements. Another advantage
    is that we get benefits on the **out-of-vocabulary** (**OOV**) front as well.
    This part is not that obvious so let me explain it. The first step on that note
    is that we don''t have to deal with the vocabulary at all. Instead, when starting
    with our BoW representations, we will start with a big column vector (2 million in
    our case) with a lot of elements for each of our training samples:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于哈希函数具有固定范围，使用哈希函数的一个好处是固定的内存需求。另一个优势是它同样在**词汇表外**（**OOV**）方面有所帮助。这一点可能不太明显，让我解释一下。首先，我们不需要处理词汇表。当我们使用词袋表示法（BoW）时，我们将使用一个大的列向量（在我们的例子中是200万维），为每个训练样本分配许多元素：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we will choose a hash function *f* that takes in strings as inputs and outputs
    values. In other words, we are making sure that our hash function will never address
    an index outside our feature's dimensions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将选择一个哈希函数*f*，它接受字符串作为输入并输出值。换句话说，我们确保我们的哈希函数不会访问超出特征维度的索引。
- en: There is the advantage in terms of OOV words as well, as compared to maintaining
    a large vocabulary in a *naive* BoW approach. Because the vector representation
    is created using a hash function, any string, even OOV words, will have a vector
    in the hash space. New words will worsen the accuracy of our classifier, true,
    but it will still work. No need to throw away the new words when predicting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与维持一个庞大的词汇表的*朴素*的词袋（BoW）方法相比，哈希函数在处理OOV（Out-Of-Vocabulary，词汇表外）单词时具有优势。由于向量表示是通过哈希函数创建的，任何字符串，即使是OOV单词，也会在哈希空间中有一个对应的向量。确实，新词会降低我们分类器的准确性，但它依然能正常工作。预测时不需要丢弃这些新词。
- en: One reason why this should work comes from the same Zipf's law that we are time
    and again referring to. Hash collisions that may happen (if any), would probably
    be between infrequent words or between frequent word and an infrequent word. This
    is because frequent words by definition will occur earlier and hence tend to occupy
    the spaces first. Thus the collided feature used for classification will be either
    unlikely to be selected for feature selection or represent the word that led the
    classifier to select it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以这样有效的原因来自我们不断提到的Zipf定律。可能发生的哈希碰撞（如果有的话），很可能发生在不常见的单词之间，或者是常见单词与不常见单词之间。这是因为常见单词按照定义会更早出现，因此倾向于首先占据位置。因此，碰撞的特征用于分类时，要么不太可能被选为特征选择的对象，要么代表着引导分类器选择它的那个词。
- en: Now that we have established the benefits of the hashing trick, we need to focus
    on the hashing function. There are many hash functions that are used for implementing
    the hashing trick, for example, Vowpal Wabbit and scikit-learn murmurhash v3\.
    A good list of possible non-cryptographic hashes can be found at the following
    Wiki link:[ https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions](https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions).
    FastText uses the FNV-1a hash, which will be discussed below.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经阐明了哈希技巧的好处，现在我们需要集中讨论哈希函数。有许多哈希函数可用于实现哈希技巧，例如，Vowpal Wabbit和scikit-learn的murmurhash
    v3。可以在以下维基链接中找到一个很好的非加密哈希函数列表：[https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions](https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions)。FastText使用FNV-1a哈希，下面将进行详细讨论。
- en: The FNV hash
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FNV哈希
- en: 'fastText uses the FNV-1a hash, which is the derivative of the FNV hash. To
    implement this algorithm, start with an initial hash value of `FNV_offset_basis`.
    For each byte in the input, take the XOR of the hash and the byte. Now multiply
    the result with the FNV prime. In terms of pseudo code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: fastText使用FNV-1a哈希算法，它是FNV哈希的衍生算法。实现该算法时，首先从初始哈希值`FNV_offset_basis`开始。对于输入中的每个字节，将哈希值与该字节进行XOR运算。然后，将结果与FNV质数相乘。伪代码如下：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Source: Wikipedia, [https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function](https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：维基百科，[https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function](https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function)
- en: 'In fastText, this is implemented in the `dictionary.cc` hash function ([https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143)):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在fastText中，这在`dictionary.cc`哈希函数中实现（[https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143)）：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the offset basis considered is `2166136261` and corresponding
    prime number is `16777619`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，考虑的偏移量基础值是`2166136261`，对应的质数是`16777619`。
- en: FNV is not cryptographic yet it has a high dispersion quality and a variable
    size hash result that can be any power of 2 from 32 to 1024 bits. The formula
    to generate it is among the simplest hash functions ever invented that actually
    achieve good dispersion. It also has good collision and bias resistance. One of
    the best properties of the FNV is that although it is not considered cryptographic,
    subverting bit sizes above 64 bits is mostly unsolvable. As the computational
    overhead needs to be kept to a minimum, fastText uses 32 bit, which has an FNV
    offset bias of 2166136261.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: FNV并非加密哈希，但它具有很高的分散性质量，并且其哈希结果的大小可变，从32位到1024位之间的任何2的幂。生成它的公式是有史以来最简单的哈希函数之一，且能实现良好的分散性。它还具有较好的碰撞和偏差抵抗能力。FNV的一个最佳特性是，尽管它不被视为加密哈希，但对超过64位的位数进行反向工程几乎是不可解决的。由于需要将计算开销保持在最小，fastText使用32位，这具有FNV偏移量基础值为2166136261。
- en: Take a look at the Python implementation in the file `fnv1a.py` under the `chapter4`
    folder in the repo.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 查看仓库中`chapter4`文件夹下的`fnv1a.py`文件中的Python实现。
- en: In fastText, word- and character-level n-grams are hashed into a fixed number
    of buckets. This prevents the memory requirements when training the model. You
    can change the number of buckets using the `-buckets` parameter. By default, the
    number of buckets is fixed at 2M (2 million).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在fastText中，单词级和字符级的n-gram被哈希到一个固定数量的桶中。这可以防止在训练模型时占用过多的内存。你可以通过`-buckets`参数来更改桶的数量。默认情况下，桶的数量固定为2M（200万）。
- en: Word embeddings and their use in sentence classification
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入及其在句子分类中的应用
- en: As you have seen in the previous chapter, word embeddings are the numerical
    representation of words in the shape of a vector in *ℜ^d*. They are unsupervised
    learned word representation vectors where there should be a correlation on semantic
    similarity. We also discussed what distributional representations and distributed
    representations are in [Chapter 7](part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755),
    *Deploying Models to Web and Mobile*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上一章中看到的，词嵌入是将单词表示为**ℜ^d**形状的向量的数值表示。它们是无监督学习的词表示向量，其中应存在语义相似性的关联。我们还在[第七章](part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755)中讨论了什么是分布式表示和分布式表示，*部署模型到Web和移动端*。
- en: When performing sentence classification, there is the hypothesis that one can
    take an existing, near-state-of-the-art, supervised NLP system and improve it
    using word embeddings. You can either create your own unsupervised word embeddings
    using the fastText cbow/skipgram approach as shown in [Chapter 2](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755), *Creating
    Models Using FastText Command Line*, or you can download them from the `fasttext.cc`
    website.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行句子分类时，有一个假设是可以利用现有的、接近最新技术的监督式NLP系统，并通过使用词嵌入来改进它。你可以使用[第2章](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755)中展示的fastText的cbow/skipgram方法来创建你自己的无监督词嵌入，*使用FastText命令行创建模型*，或者你可以从`fasttext.cc`网站下载现成的词嵌入。
- en: A question that may arise is whether certain word representations are better
    for certain tasks. Present research on some specific areas shows that word representations
    that work well in some tasks do not work well in others. An example that is generally
    given is that word representations that work in named entity recognition tasks
    do not work well when the problem domain is search query classification and vice
    versa.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能会出现的问题是，某些词向量表示是否对某些任务更有效。当前对一些特定领域的研究表明，在某些任务中效果较好的词向量表示在其他任务中可能表现不佳。一个常见的例子是，适用于命名实体识别任务的词向量表示，在搜索查询分类等问题领域中可能无法发挥良好效果，反之亦然。
- en: fastText model quantization
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fastText模型量化
- en: Due to the efforts of the Facebook AI Research team, there is a way to get vastly
    smaller models (in terms of the size that they take up in the hard drive), as
    you have seen in the *M**odel quantization* section in [Chapter 2](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755),* Creating
    Models Using FastText Command Line*. Models which take up hundreds of MBs can
    be quantized to only a couple of MBs. For example, if you see the DBpedia model
    released by Facebook, which can be accessed at the web page[ https://fasttext.cc/docs/en/supervised-models.html](https://fasttext.cc/docs/en/supervised-models.html),
    notice that the regular model (this is the BIN file) is of 427 MB while the smaller
    model (the FTZ file) is only 1.7 MB.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Facebook AI研究团队的努力，现在有了一种方法，可以得到极小的模型（就硬盘占用空间而言），正如你在[第2章](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755)的*M**odel量化*部分中看到的那样，*使用FastText命令行创建模型*。占用数百MB的模型可以被量化到仅为几MB。例如，你可以查看Facebook发布的DBpedia模型，网址是[https://fasttext.cc/docs/en/supervised-models.html](https://fasttext.cc/docs/en/supervised-models.html)，你会注意到，常规模型（这是BIN文件）的大小为427MB，而较小的模型（FTZ文件）仅为1.7MB。
- en: This reduction in size is achieved by throwing out some of the information that
    is encoded in the BIN files (or the bigger model). The problem that needs to be
    solved here is how to keep information that is important and how to identify information
    that is not that important so that the overall accuracy and performance of the
    model is not compromised by a significant margin. In this section, we will take
    a look at the considerations for that.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种体积的减小是通过舍弃一些在BIN文件（或更大模型）中编码的信息来实现的。这里需要解决的问题是如何保留重要信息，并识别不那么重要的信息，从而确保模型的整体准确性和性能不会受到显著影响。在本节中，我们将探讨一些相关的考虑因素。
- en: Compression techniques
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩技术
- en: Since we are interested in understanding how to go about compressing the big
    fastText model files, let's take a look at some of the compression techniques
    that can be used.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对如何压缩大型fastText模型文件感兴趣，让我们来看看一些可以使用的压缩技术。
- en: 'Different compression techniques can be categorized as the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的压缩技术可以分为以下几类：
- en: '**Lossless compression**: As the name suggests, lossless compression techniques
    are those techniques that will reproduce the same information structure when you
    compress, then uncompress. There is no loss in information. This type of compression
    is mostly done using statistical modeling. You have already encountered an algorithm
    that is used in this type of compression, namely Huffman coding.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无损压缩**：顾名思义，无损压缩技术是指那些在压缩然后解压后能恢复相同信息结构的技术，信息没有丢失。此类压缩主要使用统计建模方法。你已经遇到过一种用于这种压缩类型的算法，即霍夫曼编码。'
- en: '**Lossy compression**: Lossy compression is about discarding as much data as
    possible without discarding the benefits or usefulness of the data as much as
    possible. This is a good technique when we are not interested in recreating the
    original data, but more on what the original data represents. As you can correctly
    infer, you will generally be able to get a higher level of compression using lossy
    compression.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有损压缩**：有损压缩是尽可能丢弃数据而不丢失数据的利益或有用性的过程。这是一种好的技术，适用于我们不关心重新创建原始数据，而更关注原始数据所代表的意义的情况。正如你可以正确推测的那样，通常使用有损压缩时，你将能够获得更高程度的压缩。'
- en: FastText employs a type of lossy compression known as product quantization.
    In the following sections, we will try to understand what quantization is, then
    how the idea of vector quantization comes from that and how it implements compression.
    Then we will take a look at how product quantization is a better variant of vector
    quantization for this task.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: FastText采用了一种称为产品量化的有损压缩方法。在接下来的部分中，我们将尝试理解什么是量化，然后如何从中得出向量量化的概念，以及它如何实现压缩。接着，我们将看看产品量化是如何成为向量量化在这一任务中的更好变体。
- en: Quantization
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: The concept of quantization has been taken from the world of **digital signal
    processing** (**DSP**). In DSP, when converting an analog signal (for example,
    sound waves) into a digital signal, the signal is broken down to a series of individual
    samples based on a bit depth that determines the number of levels that the quantized
    signal will have. In the case of 8-bit quantization, you will have 2⁸=256 possible
    combinations for the amplitude signal, and similarly, in 16-bit quantization,
    you will have 2^(16)=65536 possible combinations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的概念来自**数字信号处理**（**DSP**）领域。在DSP中，当将模拟信号（例如声音波）转换为数字信号时，信号会根据位深度被分解为一系列单独的样本，位深度决定了量化信号将具有的级别数。在8比特量化的情况下，你将拥有2⁸=256种可能的振幅信号组合，类似地，在16比特量化的情况下，你将拥有2^(16)=65536种可能的组合。
- en: 'In the following example, a sine wave is quantized to 3-bit levels and hence
    it will support 8 values for the continuous sine wave. You can see the code to
    get the following image in the `product quantization.ipynb` notebook:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，一个正弦波被量化为3比特级别，因此它将支持8个值来表示连续的正弦波。你可以查看以下代码，获取`product quantization.ipynb`笔记本中的图像：
- en: '![](img/00065.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: Vector quantization
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量量化
- en: 'Vector quantization is a technique to apply the ideas of quantization to vector
    spaces. Let''s say that you have a vector space *ℜ[k]* and a training set consisting
    of *N* samples of *k*-dimensional vectors on the target vector space. Vector quantization
    is the process where you map your vectors into a finite set of vectors Y, which
    are part of the target vector space. Each vector *y[i]* is called a **code vector**
    or a **codeword** and the set of all *codewords* is called a **codebook**:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 向量量化是一种将量化思想应用于向量空间的技术。假设你有一个向量空间*ℜ[k]*，并且一个包含*N*个*k*维向量的训练集，目标向量空间为*ℜ[k]*。向量量化的过程是将你的向量映射到一个有限的向量集合Y，这些向量属于目标向量空间。每个向量*y[i]*被称为**代码向量**或**代码字**，所有**代码字**的集合称为**代码本**：
- en: '![](img/00066.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: The amount of compression that is achieved is dependent on the size of the codebook.
    If we have a codebook of size *k* and the input vector is of dimension *L*, we
    need to specify ![](img/00067.jpeg) bits to specify which of the codewords are
    selected from the codebook. Hence, the rate for an L-dimensional vector quantizer
    with a code book of size k is ![](img/00068.jpeg).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的压缩量取决于代码本的大小。如果我们有一个大小为*k*的代码本，而输入向量的维度是*L*，我们需要指定 ![](img/00067.jpeg) 比特来确定从代码本中选择了哪些代码字。因此，对于一个具有大小*k*的代码本的L维向量量化器，其速率是
    ![](img/00068.jpeg)。
- en: 'Let''s understand how that happens in more detail. If you have a vector of
    *L* dimension where *L=8*, it is represented as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这一过程。如果你有一个*L*维的向量，其中*L=8*，它表示如下：
- en: '![](img/00069.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: 'Now there are 8 numbers and hence you need 3 bits to put it in memory in case
    you choose to encode in binary. Converting the array into binary you get the following
    vector:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有8个数字，因此你需要3个比特来将其存储在内存中，前提是你选择使用二进制编码。将数组转换为二进制，你将得到以下向量：
- en: '![](img/00070.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpeg)'
- en: 'So you need 24 bits to save the previous array in memory if you want to save
    each number as 3 bits. What if you want to reduce the amount of memory consumption
    of each number by 1 bit? If you are able to achieve that, you will be able to
    use only 16 bits to save the previous array in memory and thus save 8 bits, achieving
    compression. To do that, you can consider the numbers 0, 2, 4, 6 as your codebook
    which will maps to the vectors 00, 01, 10, 11:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你想将每个数字存储为 3 位，那么你需要 24 位来存储之前的数组。如果你想将每个数字的内存消耗减少 1 位呢？如果你能够实现这一点，你就可以只用
    16 位来存储之前的数组，从而节省 8 位，达到压缩的效果。为此，你可以考虑将 0、2、4、6 作为你的代码书，这些将映射到向量 00、01、10、11：
- en: '![](img/00071.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.jpeg)'
- en: 'During the transformation, all numbers between 0 and 2 are mapped to 00, everything
    from 2 to 4 is mapped to 01, and so on. Hence your original vector gets changed
    to the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换过程中，所有介于 0 和 2 之间的数字都会映射到 00，所有从 2 到 4 的数字映射到 01，以此类推。因此，你的原始向量将被更改为以下内容：
- en: '![](img/00072.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpeg)'
- en: The amount of space that this vector occupies in memory is only 8 bits.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该向量在内存中占用的空间只有 8 位。
- en: Note that we can also target to encode our representation to 1 bit. In that
    case, only 2 bits of overall memory would be used for the array. But we lose more
    information about the original distribution. Hence, generalizing this understanding,
    decreasing the size of the codebook increases the compression ratio, but distortion
    of the original data increases.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还可以将目标设置为将表示编码为 1 位。在这种情况下，数组只会使用 2 位的总内存。但我们会丧失更多关于原始分布的信息。因此，推广这个理解，减小代码书的大小会增加压缩比，但原始数据的失真也会增加。
- en: Finding the codebook for high-dimensional spaces
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为高维空间找到代码书
- en: The principal goal while designing for vector quantizers is to find a codebook,
    specifying a decoder, and a rule for specifying the encoder, such that the overall
    performance of the vector space is optimal.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 设计向量量化器的主要目标是找到一个代码书，指定解码器，以及指定编码器的规则，使得向量空间的整体性能最优。
- en: 'An important class of quantizers is the Voronoi or nearest-neighbor quantizers.
    Given a set of L codevectors ![](img/00073.jpeg)  of size N, along with a distance
    measure ![](img/00074.jpeg), the R^k space is partitioned into L disjoint regions,
    known as Voronoi regions, with each codevector associated with each region. A
    particular Voronoi region Ω[j] associated with the codevector *v[j]* contains
    all the points in *R^k* space nearer to *v[j]* than any other codevector and is
    the nearest-neighbor locus region of *v[j]*. The following is an example Voronoi
    diagram for a given space with the associated codevectors denoted by points:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一类重要的量化器是 Voronoi 或最近邻量化器。给定一组大小为 N 的 L 个代码向量 ![](img/00073.jpeg)，以及一个距离度量 ![](img/00074.jpeg)，R^k
    空间被划分为 L 个不相交的区域，称为 Voronoi 区域，每个代码向量与每个区域相关联。与代码向量 *v[j]* 相关的特定 Voronoi 区域 Ω[j]
    包含所有在 *R^k* 空间中比任何其他代码向量更接近 *v[j]* 的点，并且是 *v[j]* 的最近邻域。以下是一个给定空间的 Voronoi 图示例，相关的代码向量用点表示：
- en: '![](img/00075.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.jpeg)'
- en: The code for getting the above diagram can also be found in the Jupyter notebook
    for the repo: [https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb](https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 获取上述图表的代码也可以在该仓库的 Jupyter notebook 中找到：[https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb](https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb)。
- en: 'So, essentially, you can see that there are two steps to the process:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本质上，你可以看到这个过程分为两步：
- en: '**Constructing the Voronoi space**: A preprocessing phase of constructing the
    Voronoi space, representing it in the form of a graph data structure and associating
    with each facet (for instance, vertex, edge, and region), the set of closest codevectors.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建 Voronoi 空间**：构建 Voronoi 空间的预处理阶段，将其表示为图数据结构，并将每个面（例如，顶点、边和区域）与最接近的代码向量集合关联。'
- en: '**Finding the codevector**: Given the Voronoi subdivisions, determine the facet
    of the subdivision which contains the query vector, and the associate code vector
    is the desired nearest neighbor.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**寻找代码向量**：给定 Voronoi 划分，确定包含查询向量的划分面，关联的代码向量即为所需的最近邻。'
- en: Product quantization
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 产品量化
- en: Till now, you might have understood that in vector quantization, you cluster
    the search space into a number of bins based on the distance to the cluster centroid.
    If a query vector is quantized to that bin, all other vectors in the bin are good
    candidates.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经理解了在矢量量化中，如何根据与聚类中心的距离将搜索空间划分为多个区域。如果查询向量被量化到某个区域，那么该区域内的所有其他向量都是潜在的好候选。
- en: Unfortunately, if a query lies at the edge, one has to consider all the neighboring
    bins as well. This might not seem a big deal until you realize that number of
    neighbors adjacent to each Voronoi cell increases exponentially with respect to
    the dimension *N* of the space. Note that when creating fastText vectors, we routinely
    deal in high dimensions such as 100, 300, and so on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果查询位于边缘位置，那么还必须考虑所有相邻的区域。这看起来可能没什么大不了，直到你意识到，每个Voronoi单元格的相邻邻居数会随着空间维度
    *N* 的增加而呈指数增长。请注意，在创建fastText向量时，我们通常处理的是高维数据，如100维、300维等。
- en: 'The default vectors in fastText are 100-dimensional vectors. A quantizer working
    on 50-bit codes, which means we only have a half (0.5) bit per component, contains ![](img/00076.jpeg)
    centroids (around 150 TB). Product quantization is an efficient solution to address
    this issue. The input vector *x[i]* is divided into m distinct subvectors j ![](img/00077.jpeg)
    of dimension *D** = *D/m*, where *D* is a multiple of *m*. The subvectors are
    quantized separately using m distinct quantizers. A given vector ν is therefore
    mapped as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: fastText中的默认向量是100维的向量。一个处理50位编码的量化器意味着我们每个分量只有0.5位，它包含！[](img/00076.jpeg)个质心（大约150
    TB）。产品量化是一种高效的解决方案来解决这个问题。输入向量 *x[i]* 被分成m个不同的子向量j ![](img/00077.jpeg)，每个子向量的维度为
    *D** = *D/m*，其中*D*是*m*的倍数。这些子向量被使用m个不同的量化器分别进行量化。给定向量ν因此按以下方式映射：
- en: _![](img/00078.jpeg)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: _![](img/00078.jpeg)
- en: Here, *q[j]* is a low-complexity quantizer associated with the *j*th subvector.
    With the subquantizer *q[j]*, we associate the index *Ι[j]*, the codebook *C[j]*,
    and the corresponding reproduction values *c[j,i]*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*q[j]*是与第*j*个子向量相关的低复杂度量化器。通过子量化器*q[j]*，我们关联索引*Ι[j]*、码本*C[j]*和相应的重构值*c[j,i]*。
- en: 'A reproduction value of the **product quantizer** (**PQ**) is identified by
    an element of the product index set ![](img/00079.jpeg). The codebook is therefore
    defined as the Cartesian product:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**产品量化器**（**PQ**）的重构值通过产品索引集中的一个元素来识别！[](img/00079.jpeg)。因此，码本被定义为笛卡尔积：'
- en: '![](img/00080.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpeg)'
- en: 'A centroid of this set is the concatenation of the centroids of the m sub-quantizers.
    From now on, we assume that all sub-quantizers have the same finite number *k**
    of reproduction values. In that case, the total number of centroids is given by
    the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该集合的质心是m个子量化器的质心的连接。从现在开始，我们假设所有子量化器都有相同的有限个重构值 *k**。在这种情况下，总质心数由以下公式给出：
- en: '![](img/00081.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.jpeg)'
- en: In fastText, the two parameters involved in product quantization, namely the
    number of sub-quantizers m and the number of bits b per quantization index, are
    typically set to ![](img/00082.jpeg), and *b*=8.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在fastText中，涉及产品量化的两个参数，即子量化器的数量m和每个量化索引的位数b，通常设置为！[](img/00082.jpeg)，且*b*=8。
- en: Thus, a PQ can generate an exponentially large codebook at very low memory/time
    cost. The essence of a PQ is to decompose the high-dimensional vector space into
    the Cartesian product of the subspaces and then quantize these subspaces separately.
    The optimal space decomposition is important for good product quantization implementation,
    and as per current knowledge, this is generally done by minimizing quantization
    distortions with respect to the space decomposition and quantization codebooks.
    There are two known ways to solve this optimization problem. One is using iterative
    quantization. A simple example of iteratively finding the codevectors is shown
    in the notebook, which can be considered a specific subquantizer. If you are interested,
    you can take a look at *Optimised Product Quantization* by Kaiming He et al.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PQ可以以非常低的内存/时间成本生成一个指数级大的码本。PQ的本质是将高维向量空间分解为子空间的笛卡尔积，然后分别对这些子空间进行量化。最佳的空间分解对于良好的产品量化实现至关重要，根据目前的知识，这通常是通过最小化与空间分解和量化码本相关的量化失真来完成的。解决此优化问题有两种已知方法。其中一种是使用迭代量化。一个简单的迭代找到码向量的例子在笔记本中有展示，可以认为它是一个特定的子量化器。如果你有兴趣，可以查看Kaiming
    He等人的《优化产品量化》。
- en: The other method, and one which fastText adopts, is having a Gaussian assumption
    for the input vectors and finding the k-means through expectation maximization.
    Take a look at the algorithm in this k-means function: [src/productquantizer.cc#L115.](https://github.com/facebookresearch/fastText/blob/d647be03243d2b83d0b4659a9dbfb01e1d1e1bf7/src/productquantizer.cc#L115)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，也是fastText采用的方法，是对输入向量假设高斯分布，并通过期望最大化来寻找k-means。请查看此k-means函数中的算法：[src/productquantizer.cc#L115.](https://github.com/facebookresearch/fastText/blob/d647be03243d2b83d0b4659a9dbfb01e1d1e1bf7/src/productquantizer.cc#L115)
- en: After quantizing on the input matrix, retraining is done on the output matrix,
    keeping the input matrix the same. This is done so that the network readjusts
    to the quantization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入矩阵进行量化后，在输出矩阵上进行再训练，同时保持输入矩阵不变。这样做是为了让网络重新调整以适应量化。
- en: Additional steps
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加步骤
- en: 'Following are the additional steps that can be taken:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以采取的附加步骤：
- en: '**Feature selection and pruning**: Pruning is done on those features that do
    not have a big influence on the decision of the classifier. During the classification
    step, only a limited number of *K* words and n-grams are selected. So for each
    document, first verification is done if it is covered by a retrained feature and,
    if not, we add the feature with the highest norm to the set of retrained features.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择与剪枝**：剪枝会对那些对分类器决策影响较小的特征进行处理。在分类步骤中，只选择有限数量的*K*词汇和n-gram。因此，对于每个文档，首先验证它是否被再训练的特征所覆盖，如果没有，则将具有最高范数的特征添加到再训练特征的集合中。'
- en: '**Hashing**: Both the words and n-grams are also hashed to further save on
    the memory.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈希化**：词汇和n-gram也会进行哈希处理，以进一步节省内存。'
- en: 'If you decide to implement these techniques discussed in your own model compression
    methods, there are various ideas here that you can tweak and see if you get better
    performance in your particular domain:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定将这些在模型压缩方法中讨论的技术应用于自己的模型，可以调整其中的各种思路，看看在你的具体领域中是否能获得更好的性能。
- en: You can explore whether some other distance metrics makes sense for finding
    the k-means.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以探索是否有其他距离度量适合用于找到k-means。
- en: You can change the pruning strategy based on neighborhood, entropy, and so on.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以根据邻域、熵等因素来改变剪枝策略。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: With this chapter, you have completed a deep dive into the theory behind how
    the fastText model is designed and implemented, the benefits, and the things that
    you need to consider while implementing it in your ML pipeline.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容，你已经深入了解了fastText模型设计和实现背后的理论，模型的优势，以及在将其应用到你的机器学习管道时需要考虑的事项。
- en: The next part of the book is about implementation and deployment and we start
    with how to use fastText in a Python environment in the next chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的下一部分将讲解实现与部署，我们将在下一章开始介绍如何在Python环境中使用fastText。
