- en: Go Live and Go Big
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时启用并扩展
- en: In this chapter, we are going to learn more about **Amazon Web Services** (**AWS**)
    and how to create a deep neural network to solve a video action recognition problem.
    We will show you how to use multiple GPUs for faster training. At the end of the
    chapter, we will give you a quick overview of Amazon Mechanical Turk Service,
    which allows us to collect labels and correct the model's results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入了解 **Amazon Web Services**（**AWS**）以及如何创建深度神经网络来解决视频动作识别问题。我们将展示如何使用多个
    GPU 加速训练。章末，我们将简要介绍 Amazon Mechanical Turk 服务，它允许我们收集标签并纠正模型的结果。
- en: Quick look at Amazon Web Services
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速浏览 Amazon Web Services
- en: '**Amazon Web Services** (**AWS**) is one of the most popular cloud platforms,
    and was made by Amazon.com. It provides many services, including cloud computing,
    storage, database services, content delivery, and other functionalities. In this
    section, we will only focus on virtual server services found on Amazon EC2\. Amazon
    EC2 allows us to create multiple servers that can support the serving of our model
    and even the training routine. When it comes to serving the model for end users,
    you can read [Chapter 9](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml), *Cruise
    Control - Automation*, to learn about TensorFlow Serving. In training, Amazon
    EC2 has many instance types that we can use. We can use their CPU servers to run
    our web bot to collect data from the internet. There are several instance types
    that have multiple NVIDIA GPUs.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Web Services**（**AWS**）是最受欢迎的云平台之一，由 Amazon.com 开发。它提供许多服务，包括云计算、存储、数据库服务、内容分发以及其他功能。在本节中，我们将重点关注亚马逊
    EC2 上的虚拟服务器服务。Amazon EC2 允许我们创建多个服务器，这些服务器可以支持模型的服务并进行训练。当涉及到为最终用户提供模型服务时，你可以阅读[第
    9 章](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml)，*巡航控制 - 自动化*，以了解 TensorFlow Serving。在训练中，Amazon
    EC2 提供了许多实例类型供我们使用。我们可以使用它们的 CPU 服务器来运行我们的网络机器人，从互联网上收集数据。还有几种实例类型配备了多个 NVIDIA
    GPU。'
- en: 'Amazon EC2 provides a wide selection of instance types to fit different use
    cases. The instance types are divided into five categories, as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 EC2 提供了多种实例类型，以适应不同的使用场景。这些实例类型分为五类，具体如下：
- en: General Purpose
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用型
- en: Compute Optimized
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算优化
- en: Memory Optimized
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存优化
- en: Storage Optimized
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储优化
- en: Accelerated Computing Instances
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速计算实例
- en: The first four categories are best suited to running backend servers. The accelerated
    computing instances have multiple NVIDIA GPUs that can be used to serve models
    and train new models with high-end GPUs. There are three types of instances—P2,
    G2, and F1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 前四类最适合运行后端服务器。加速计算实例配备了多个 NVIDIA GPU，可用于为模型提供服务并用高端 GPU 训练新模型。共有三种实例类型——P2、G2
    和 F1。
- en: P2 instances
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P2 实例
- en: 'P2 instances contain high-performance NVIDIA K80 GPUs, each with 2,496 CUDA
    cores and 12 GB of GPU memory. There are three models of P2, as described in the
    following table:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: P2 实例配备高性能的 NVIDIA K80 GPU，每个 GPU 拥有 2,496 个 CUDA 核心和 12 GB 的 GPU 内存。P2 实例有三种型号，具体见下表：
- en: '| **Model** | **GPUs** | **vCPU** | **Memory (GB)** | **GPU Memory (GB)** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **型号** | **GPU 数量** | **vCPU** | **内存 (GB)** | **GPU 内存 (GB)** |'
- en: '| p2.xlarge | 1 | 4 | 61 | 12 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| p2.xlarge | 1 | 4 | 61 | 12 |'
- en: '| p2.8xlarge | 8 | 32 | 488 | 96 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| p2.8xlarge | 8 | 32 | 488 | 96 |'
- en: '| p2.16xlarge | 16 | 64 | 732 | 192 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| p2.16xlarge | 16 | 64 | 732 | 192 |'
- en: These models with large GPU memory are best suited for training models. With
    more GPU memory, we can train the model with a larger batch size and a neural
    network with lots of parameters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配备大容量 GPU 内存的型号最适合用于训练模型。更多的 GPU 内存可以让我们使用更大的批量大小和更多参数的神经网络来训练模型。
- en: G2 instances
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: G2 实例
- en: 'G2 instances contain high-performance NVIDIA GPUs, each with 1,536 CUDA cores
    and 4 GB of GPU memory. There are two models of G2, as described in the following
    table:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: G2 实例配备高性能的 NVIDIA GPU，每个 GPU 拥有 1,536 个 CUDA 核心和 4 GB 的 GPU 内存。G2 实例有两种型号，具体见下表：
- en: '| **Model** | **GPUs** | **vCPU** | **Memory(GB)** | **SSD Storage (GB)** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **型号** | **GPU 数量** | **vCPU** | **内存 (GB)** | **SSD 存储 (GB)** |'
- en: '| g2.2xlarge | 1 | 8 | 15 | 1 x 60 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| g2.2xlarge | 1 | 8 | 15 | 1 x 60 |'
- en: '| g2.8xlarge | 4 | 32 | 60 | 2 x 120 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| g2.8xlarge | 4 | 32 | 60 | 2 x 120 |'
- en: These models have only 4 GB of GPU memory, so they are limited in training.
    However, 4 GB of GPU memory is generally enough for serving the model to end users.
    One of the most important factors is that G2 instances are much cheaper than P2
    instances, which allows us to deploy multiple servers under a load balancer for
    high scalability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型只有4 GB的GPU内存，因此在训练上有所限制。然而，4 GB的GPU内存通常足够将模型服务于最终用户。一个重要因素是，G2实例比P2实例便宜得多，这使得我们可以在负载均衡器下部署多个服务器，以实现高可扩展性。
- en: F1 instances
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1实例
- en: 'F1 instances support **field programmable gate arrays** (**FPGAs**). There
    are two models of F1, as described in the following table:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: F1实例支持**现场可编程门阵列**（**FPGAs**）。F1有两种型号，如下表所示：
- en: '| **Model** | **GPUs** | **vCPU** | **Memory(GB)** | **SSD Storage (GB)** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **型号** | **GPU** | **vCPU** | **内存(GB)** | **SSD存储 (GB)** |'
- en: '| f1.2xlarge | 1 | 8 | 122 | 470 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| f1.2xlarge | 1 | 8 | 122 | 470 |'
- en: '| f1.16xlarge | 8 | 64 | 976 | 4 x 940 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| f1.16xlarge | 8 | 64 | 976 | 4 x 940 |'
- en: FPGAs with high memory and computing power are very promising in the field of
    deep learning. However, TensorFlow and other popular deep learning libraries don't
    support FPGAs. Therefore, in the next section, we will only cover the prices of
    P2 and G2 instances.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有高内存和计算能力的FPGA在深度学习领域前景广阔。然而，TensorFlow和其他流行的深度学习库并不支持FPGA。因此，在下一节中，我们将只介绍P2和G2实例的价格。
- en: Pricing
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定价
- en: Let's explore the pricing of these instances at [https://aws.amazon.com/emr/pricing/](https://aws.amazon.com/emr/pricing/).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在[https://aws.amazon.com/emr/pricing/](https://aws.amazon.com/emr/pricing/)探索这些实例的定价。
- en: 'Amazon EC2 offers three pricing options for instances--On-Demand Instance,
    Reserved Instance, and Spot Instance:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊EC2提供三种实例定价选项——按需实例、预留实例和Spot实例：
- en: On-Demand instance gives you the ability to run the server without disruption.
    It is suitable if you only want to use the instance for a few days or weeks.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按需实例使你能够不受干扰地运行服务器。如果你只打算使用实例几天或几周，它非常合适。
- en: Reserved instance gives you the option to reserve the instance for a one- or
    three-year term with a significant discount compared to the On-Demand Instance.
    It is suitable if you want to run the server for production.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预留实例允许你以显著的折扣预定一个一或三年的实例，相比按需实例更具成本优势。如果你想将服务器用于生产环境，它非常合适。
- en: Spot instance gives you the option to bid for the server. You can choose the
    maximum price you are willing to pay per instance hour. This can save you a lot
    of money. However, these instances can be terminated at any time if someone bids
    higher than you. It is suitable if your system can handle interruption or if you
    just want to explore services.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spot实例允许你对服务器进行竞标。你可以选择每小时实例的最大支付价格。这可以帮助你节省大量费用。然而，如果有人出价高于你的价格，这些实例可能会随时被终止。如果你的系统能够处理中断或你只是想探索服务，那么这种类型适合你。
- en: Amazon has provided a website to calculate the monthly bill. You can see it
    at [http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊提供了一个网站来计算每月账单。你可以在[http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html)查看。
- en: You can click the Add New Row button and select an instance type.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以点击“添加新行”按钮并选择一个实例类型。
- en: 'In the following image, we have selected a p2.xlarge server. The price for
    a month is $658.80 at the time of writing:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们选择了一个p2.xlarge服务器。写作时该服务器一个月的价格为$658.80：
- en: '![](img/c034555a-aa11-4d0c-839e-89085779adfd.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c034555a-aa11-4d0c-839e-89085779adfd.png)'
- en: 'Now click on the Billing Option column. You will see the price of a reserved
    instance for a p2.xlarge server:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在点击“Billing Option”列。你将看到一个p2.xlarge服务器的预留实例价格：
- en: '![](img/2936c039-f527-4aa6-ae23-724bfbe68789.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2936c039-f527-4aa6-ae23-724bfbe68789.png)'
- en: There are many other instance types. We suggest that you take a look at the
    other types and select the server that is best suited to your requirements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他实例类型。我们建议你查看其他类型并选择最适合你需求的服务器。
- en: In the next section, we will create a new model that can perform video action
    recognition with TensorFlow. We will also leverage the training performance using
    multiple GPUs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建一个新的模型，使用TensorFlow执行视频动作识别。我们还将利用多GPU提高训练性能。
- en: Overview of the application
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序概览
- en: Human action recognition is a very interesting problem in computer vision and
    machine learning. There are two popular approaches to this problem,that is,**still
    image action recognition** and **video action recognition**. In still image action
    recognition, we can fine-tune a pre-trained model from ImageNet and perform a
    classification of the actions based on the static image. You can review the previous
    chapters for more information. In this chapter, we will create a model that can
    recognize human action from videos. At the end of the chapter, we will show you
    how to use multiple GPUs to speed up the training process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 人体动作识别是计算机视觉和机器学习中一个非常有趣的问题。解决这一问题有两种流行的方式，即**静态图像动作识别**和**视频动作识别**。在静态图像动作识别中，我们可以微调一个从ImageNet预训练的模型，并基于静态图像对动作进行分类。你可以回顾前几章以获得更多信息。在本章中，我们将创建一个能够从视频中识别人体动作的模型。在本章结束时，我们将展示如何使用多GPU来加速训练过程。
- en: Datasets
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: 'There are many available datasets that we can use in the training process,
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的数据集，我们可以在训练过程中使用，具体如下：
- en: UCF101 ([http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php))
    is an action recognition dataset of realistic action videos with 101 action categories.
    There are 13,320 videos in total for the 101 action categories, which makes this
    dataset a great choice for many research papers.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCF101 ([http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php))是一个包含101个动作类别的真实动作视频数据集。该数据集共有13,320个视频，涵盖了101个动作类别，因此它成为许多研究论文的理想选择。
- en: ActivityNet ([http://activity-net.org/](http://activity-net.org/)) is a large-scale
    dataset for human activity understanding. There are 200 categories with over 648
    hours of video. Each category has about 100 videos.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ActivityNet ([http://activity-net.org/](http://activity-net.org/))是一个用于理解人体活动的大型数据集。该数据集包含200个类别，总计648小时以上的视频，每个类别大约有100个视频。
- en: Sports-1M ([http://cs.stanford.edu/people/karpathy/deepvideo/](http://cs.stanford.edu/people/karpathy/deepvideo/))
    is another large-scale dataset for sports recognition. There are 1,133,158 videos
    in total, annotated with 487 sports labels.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sports-1M ([http://cs.stanford.edu/people/karpathy/deepvideo/](http://cs.stanford.edu/people/karpathy/deepvideo/))是另一个用于体育识别的大型数据集。该数据集总共有1,133,158个视频，并标注了487个体育标签。
- en: In this chapter, we will use UCF101 to perform the training process. We also
    recommend that you try to apply the techniques discussed in this chapter to a
    large-scale dataset to take full advantage of multiple-GPU training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用UCF101进行训练过程。我们还建议你尝试将本章讨论的技术应用于一个大型数据集，以充分利用多GPU训练。
- en: Preparing the dataset and input pipeline
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集和输入管道
- en: The UCF101 dataset contains 101 action categories, such as Basketball shooting,
    playing guitar, and Surfing. We can download the dataset from [http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: UCF101数据集包含101个动作类别，如篮球投篮、弹吉他和冲浪。我们可以从[http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php)下载该数据集。
- en: On the website, you need to download the UCF101 dataset in the file named `UCF101.rar`,
    and the train/test splits for action recognition in the file named `UCF101TrainTestSplits-RecognitionTask.zip`.
    You need to extract the dataset before moving to the next section, where we will
    perform a pre-processing technique on videos before training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在网站上，你需要下载名为`UCF101.rar`的UCF101数据集，并下载名为`UCF101TrainTestSplits-RecognitionTask.zip`的动作识别训练/测试数据集划分。在进入下一节之前，你需要解压数据集，接下来我们将对视频进行预处理，然后进行训练。
- en: Pre-processing the video for training
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对视频进行预处理以用于训练
- en: UCF101 contains 13,320 video clips with a fixed frame rate and resolution of
    25 FPS and 320 x 240 respectively. All video clips are stored in AVI format, so
    it is not convenient to use them in TensorFlow. Therefore, in this section, we
    will extract video frames from all the videos into JPEG files. We will only extract
    video frames at the fixed frame rate of 4 FPS so that we can reduce the input
    size of the network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: UCF101包含13,320个视频片段，固定帧率为25 FPS，分辨率为320 x 240。所有视频片段均以AVI格式存储，因此在TensorFlow中使用起来不太方便。因此，在本节中，我们将从所有视频中提取视频帧并保存为JPEG文件。我们只会以固定帧率4
    FPS提取视频帧，这样可以减少网络的输入大小。
- en: Before we start implementing the code, we need to install the av library from
    [https://mikeboers.github.io/PyAV/installation.html](https://mikeboers.github.io/PyAV/installation.html).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始实现代码之前，需要从[https://mikeboers.github.io/PyAV/installation.html](https://mikeboers.github.io/PyAV/installation.html)安装av库。
- en: 'First, create a Python package named `scripts` in the `root` folder. Then,
    create a new Python file at `scripts/convert_ucf101.py`. In the newly created
    file, add the first code to import and define some parameters, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在 `root` 文件夹中创建一个名为 `scripts` 的 Python 包。然后，在 `scripts/convert_ucf101.py`
    路径下创建一个新的 Python 文件。在新创建的文件中，添加第一个代码来导入并定义一些参数，如下所示：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, `dataset_dir` and `train_test_list_dir` are the locations
    of the folders containing the extracted content of `UCF101.rar` and `UCF101TrainTestSplits-RecognitionTask.zip`
    respectively. `target_dir` is the folder that all the training images will be
    stored in. `ensure_folder_exists` is a `utility` function that creates a folder
    if it doesn't exist.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`dataset_dir` 和 `train_test_list_dir` 分别是包含 `UCF101.rar` 和 `UCF101TrainTestSplits-RecognitionTask.zip`
    提取内容的文件夹的位置。`target_dir` 是存储所有训练图像的文件夹。`ensure_folder_exists` 是一个 `utility` 函数，用于在文件夹不存在时创建该文件夹。
- en: 'Next, let''s define the `main` function of the Python code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义 Python 代码的 `main` 函数：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the `main` function, we create the `target_dir` folder and call the `convert_data`
    function which we will create shortly. The `convert_data` function takes a list
    of train/test text files in the dataset and a Boolean called training that indicates
    whether the text files are for the training process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `main` 函数中，我们创建了 `target_dir` 文件夹，并调用了我们稍后将创建的 `convert_data` 函数。`convert_data`
    函数接受数据集中的训练/测试文本文件列表和一个布尔值 `training`，指示文本文件是否用于训练过程。
- en: 'Here are some lines from one of the text files:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是某个文本文件中的一些行：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each line of the text file contains the path to the `video` file and the correct
    label. In this case, we have three video paths from the `ApplyEyeMakeup` category,
    which is the first category in the dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行文本文件包含 `video` 文件的路径和正确的标签。在这种情况下，我们有三个来自 `ApplyEyeMakeup` 类别的视频路径，这是数据集中第一个类别。
- en: 'The main idea here is that we read each line of the text files, extract video
    frames in a JPEG format, and save the location of the extracted files with the
    corresponding label for further training. Here is the code for the `convert_data`
    function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要思想是我们读取每一行文本文件，提取视频帧并将其保存为 JPEG 格式，并保存提取文件的路径和对应的标签，供后续训练使用。以下是 `convert_data`
    函数的代码：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code is straightforward. We load the video path from the text
    files and use the `av` library to open the AVI files. Then, we use `FLAGS.fps`
    to control how many frames per second need to be extracted. You can run the `scripts/convert_ucf101.py`
    file using the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码很简单。我们从文本文件中加载视频路径，并使用 `av` 库打开 AVI 文件。然后，我们使用 `FLAGS.fps` 控制每秒需要提取多少帧。你可以通过以下命令运行
    `scripts/convert_ucf101.py` 文件：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The total process needs about 30 minutes to convert all the video clips. At
    the end, the `target_dir` folder will contain the following files:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程大约需要 30 分钟来转换所有视频片段。最后，`target_dir` 文件夹将包含以下文件：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the `train.txt` file, the lines will look like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `train.txt` 文件中，行的格式如下：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This format can be understood as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个格式可以理解如下：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There is one thing that you must remember, which is that the labels in `train.txt`
    and `test.txt` go from 0 to 100\. However, the labels in the UCF101 go from 1
    to 101\. This is because the `sparse_softmax_cross_entropy` function in TensorFlow
    needs class labels to start from 0.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有一件事你必须记住，那就是 `train.txt` 和 `test.txt` 中的标签是从 0 到 100 的。然而，UCF101 中的标签是从 1 到
    101 的。这是因为 TensorFlow 中的 `sparse_softmax_cross_entropy` 函数要求类别标签从 0 开始。
- en: Input pipeline with RandomShuffleQueue
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RandomShuffleQueue 的输入管道
- en: If you have read [Chapter 9](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml), *Cruise
    Control - Automation*, you will know that we can use TextLineReader in TensorFlow
    to simply read the text files line by line and use the line to read the image
    directly in TensorFlow. However, things get more complex as the data only contains
    the folder location and the label. Moreover, we only want a subset of frames in
    one folder. For example, if the number of frames is 30 and we only want 10 frames
    to train, we will randomize from 0 to 20 and select 10 frames from that point.
    Therefore, in this chapter, we will use another mechanism to sample the video
    frames in pure Python and put the selected frame paths into `RandomShuffleQueue`
    for training. We also use `tf.train.batch_join` to leverage the training with
    multiple pre-processing threads.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读过[第9章](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml)，*定速巡航 - 自动化*，你会知道我们可以使用TensorFlow中的TextLineReader来简单地逐行读取文本文件，并利用这一行直接在TensorFlow中读取图像。然而，当数据仅包含文件夹位置和标签时，事情变得更加复杂。而且，我们只想从一个文件夹中选择一部分帧。例如，如果帧数是30，我们只想选择10帧用于训练，我们会从0到20随机选择一个起始帧，然后从那里选择10帧。因此，在本章中，我们将使用另一种机制，在纯Python中对视频帧进行采样，并将选择的帧路径放入`RandomShuffleQueue`中进行训练。我们还使用`tf.train.batch_join`来利用多个预处理线程进行训练。
- en: 'First, create a new Python file named `utils.py` in the `root` folder and add
    the following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在`root`文件夹中创建一个新的Python文件，命名为`utils.py`，并添加以下代码：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this code, we create a `generator` function named `lines_from_file` to read
    the text files line by line. We also add a `repeat` parameter so that the `generator`
    function can read the text from the beginning when it reaches the end of the file.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们创建了一个名为`lines_from_file`的`generator`函数，用于逐行读取文本文件。我们还添加了一个`repeat`参数，以便当`generator`函数读取到文件末尾时，可以从头开始重新读取文本。
- en: 'We have added a main section so you can try to run it to see how the `generator`
    works:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个主程序部分，你可以尝试运行它，看看`generator`是如何工作的：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, create a new Python file named `datasets.py` in the `root` folder and
    add the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`root`文件夹中创建一个新的Python文件，命名为`datasets.py`，并添加以下代码：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `sample_videos` function is easy to understand. It will receive the `generator`
    object from `lines_from_file` function and use the `next` function to get the
    required samples. You can see that we use a `random.randint` method to randomize
    the starting frame position.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_videos`函数易于理解。它将接收来自`lines_from_file`函数的`generator`对象，并使用`next`函数获取所需的样本。你可以看到，我们使用了`random.randint`方法来随机化起始帧的位置。'
- en: 'You can run the main section to see how the `sample_videos` work with the following
    command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行主程序部分，使用以下命令查看`sample_videos`如何工作：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Up to this point, we have read the dataset text file into the `image_paths`
    and `labels` variables, which are Python lists. In the later training routine,
    we will use a built-in `RandomShuffleQueue` in TensorFlow to enqueue `image_paths`
    and `labels` into that queue.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将数据集文本文件读取到`image_paths`和`labels`变量中，它们是Python列表。在后续的训练过程中，我们将使用TensorFlow中的内建`RandomShuffleQueue`来将`image_paths`和`labels`入队。
- en: Now, we need to create a method that will be used in the training routine to
    get data from `RandomShuffleQueue`, perform pre-processing in multiple threads,
    and send the data to the `batch_join` function to create a mini-batch for training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个方法，在训练过程中使用它从`RandomShuffleQueue`获取数据，在多个线程中进行预处理，并将数据传送到`batch_join`函数中，以创建一个用于训练的迷你批次。
- en: 'In the `dataset.py` file, add the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dataset.py`文件中，添加以下代码：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this code, we prepare an array named `frames_and_labels` and use a for loop
    with a `num_threads` iteration. This is a very convenient way of adding multi-threading
    support to the pre-processing process. In each thread, we will call the method
    `dequeue` from the `input_queue` to get a `frame_paths` and `label`. From the
    `sample_video` function in the previous section, we know that `frame_paths` is
    a list of selected video frames. Therefore, we use another for loop to loop through
    each frame. In each frame, we read, resize, and perform image standardization.
    This part is similar to the code in [Chapter 9](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml),
    *Cruise Control - Automation*. At the end of the input pipeline, we add `frames_and_labels`
    with `batch_size` parameters. The returned `frames_batch` and `labels_batch` will
    be used for a later training routine.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们准备了一个名为`frames_and_labels`的数组，并使用`num_threads`迭代的for循环。这是一种非常方便的方式，能够在预处理过程中添加多线程支持。在每个线程中，我们将调用`input_queue`的`dequeue`方法来获取`frame_paths`和`label`。从前一节的`sample_video`函数中，我们知道`frame_paths`是一个选定视频帧的列表。因此，我们使用另一个for循环来遍历每一帧。在每一帧中，我们读取、调整大小并进行图像标准化。这部分与[第9章](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml)的代码类似，*巡航控制
    - 自动化*。在输入管道的末尾，我们添加了带有`batch_size`参数的`frames_and_labels`。返回的`frames_batch`和`labels_batch`将用于后续的训练流程。
- en: 'Finally, you should add the following code, which contains the `_aspect_preserving_resize`
    function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要添加以下代码，其中包含`_aspect_preserving_resize`函数：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code is the same as what you used in [Chapter 9](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml),
    *Cruise Control - Automation*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与[第9章](b38dd75a-b632-4e7b-b581-202500f4e001.xhtml)中的代码相同，*巡航控制 - 自动化*。
- en: In the next section, we will create the deep neural network architecture that
    we will use to perform video action recognitions with 101 categories.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建一个深度神经网络架构，使用该架构执行101个类别的视频动作识别。
- en: Neural network architecture
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: In this chapter, we will create a neural network that will take an input of
    10 video frames and output the probability over 101 action categories. We will
    create a neural network based on the conv3d operation in TensorFlow. This network
    is inspired on the work of D. Tran et al., Learning Spatiotemporal Features with
    3D Convolutional Networks. However, we have simplified the model so it is easier
    to explain in a chapter. We have also used some techniques that are not mentioned
    by Tran et al., such as batch normalization and dropout.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将创建一个神经网络，该网络将接受10帧视频作为输入，并输出101个动作类别的概率。我们将基于TensorFlow中的conv3d操作来创建该神经网络。这个网络的灵感来源于D.
    Tran等人的研究《使用3D卷积网络学习时空特征》。然而，我们简化了模型，使其在本章中更容易解释。我们还使用了一些Tran等人未提及的技术，例如批量归一化和dropout。
- en: 'Now, create a new Python file named `nets.py` and add the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个名为`nets.py`的新Python文件，并添加以下代码：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the `inference` function, we `call _conv3d`, `_max_pool3d`, and `_fully_connected`
    to create the network. It is not that different to the CNN network for images
    in previous chapters. At the end of the function, we also create a dictionary
    named `endpoints`, which will be used in the main section to visualize the network
    architecture.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在`inference`函数中，我们`调用 _conv3d`、`_max_pool3d`和`_fully_connected`来创建网络。这与之前章节中用于图像的CNN网络没有太大区别。在函数的末尾，我们还创建了一个名为`endpoints`的字典，它将在主部分中用于可视化网络架构。
- en: 'Next, let''s add the code of the `_conv3d` and `_max_pool3d` functions:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加`_conv3d`和`_max_pool3d`函数的代码：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code is similar to the previous chapters. However, we use the built-in
    `tf.nn.conv3d` and `tf.nn.max_pool3d` functions instead of `tf.nn.conv2d` and
    `tf.nn.max_pool3d` for images. Therefore, we need to add the `k_d` and `s_d` parameters
    to give information about the depth of the filters. Moreover, we will need to
    train this network from scratch without any pre-trained models. So, we need to
    use the `batch_norm` function to add the batch normalization to each layer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与之前的章节相似。然而，我们使用了内置的`tf.nn.conv3d`和`tf.nn.max_pool3d`函数，而不是针对图像的`tf.nn.conv2d`和`tf.nn.max_pool3d`函数。因此，我们需要添加`k_d`和`s_d`参数，以提供关于滤波器深度的信息。此外，我们将从头开始训练这个网络，而不是使用任何预训练模型。因此，我们需要使用`batch_norm`函数将批量归一化添加到每一层。
- en: 'Let''s add the code for the fully connected layer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加全连接层的代码：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function is a bit different to what we used with images. First, we check
    that the `input_shape.ndims` is equal to 5 instead of 4\. Secondly, we add the
    batch normalization to the output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数与我们使用图像时的有所不同。首先，我们检查 `input_shape.ndims` 是否等于 5，而不是 4。其次，我们将批量归一化添加到输出中。
- en: 'Finally, let''s open the `utils.py` file and add the following `utility` functions:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们打开 `utils.py` 文件并添加以下 `utility` 函数：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can run `nets.py` to have a better understanding of the network''s architecture:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行 `nets.py` 来更好地理解网络的架构：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the first part of the console result, you will see a table like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台结果的第一部分，你将看到如下表格：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These are the shapes of `variables` in the network. As you can see, three `variables`
    that have the text `BatchNorm` are added to each layer. These `variables` increase
    the total parameters that the network needs to learn. However, since we will train
    from scratch, it will be much for harder to train the network without batch normalization.
    Batch normalization also increases the ability of the network to regularize unseen
    data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是网络中 `variables` 的形状。如你所见，每一层都有三个带有 `BatchNorm` 文本的 `variables` 被添加进去。这些 `variables`
    增加了网络需要学习的总参数。然而，由于我们将从头开始训练，没有批量归一化的情况下，训练网络将更加困难。批量归一化还提高了网络对未见数据的正则化能力。
- en: 'In the second table of the console, you will see the following table:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台的第二个表格中，你将看到如下表格：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These are the shapes of the input and output of the network. As you can see,
    the input contains 10 video frames of size (112, 112, 3), and the output contains
    a vector of 101 elements.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是网络的输入和输出的形状。如你所见，输入包含 10 帧大小为 (112, 112, 3) 的视频帧，输出包含一个 101 个元素的向量。
- en: 'In the last table, you will see how the shape of the output at each layer has
    changed through the network:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的表格中，你将看到输出在每一层经过网络时形状的变化：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding table, we can see that the output of the `conv1` layer has
    the same size as the input, and the output of the `conv2` layer has changed due
    to the effect of max pooling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，我们可以看到 `conv1` 层的输出与输入的大小相同，而 `conv2` 层的输出由于最大池化的作用发生了变化。
- en: 'Now, let''s create a new Python file named `models.py` and add the following
    code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个新的 Python 文件，命名为 `models.py`，并添加以下代码：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These functions create the operation to calculate `loss`, `accuracy`, `learning
    rate`, and perform the train process. This is the same as the previous chapter,
    so we won't explain these functions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数创建计算 `loss`、`accuracy`、`learning rate` 的操作，并执行训练过程。这与前一章相同，因此我们不再解释这些函数。
- en: Now, we have all the functions required to train the network to recognize video
    actions. In the next section, we will start the training routine on a single GPU
    and visualize the results on TensorBoard.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了训练网络以识别视频动作所需的所有函数。在下一部分中，我们将开始在单个 GPU 上的训练过程，并在 TensorBoard 上可视化结果。
- en: Training routine with single GPU
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单 GPU 训练过程
- en: 'In the scripts package, create a new Python file named `train.py`. We will
    start by defining some parameters as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本包中，创建一个名为 `train.py` 的新 Python 文件。我们将首先定义一些参数，如下所示：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'These parameters are self-explanatory. Now, we will define some operations
    for training:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数不言自明。现在，我们将定义一些用于训练的操作：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this code, we get a `generator` object from the text file. Then, we create
    two placeholders for `image_paths` and `labels`, which will be enqueued to `RandomShuffleQueue`.
    The `input_pipeline` function that we created in `datasets.py` will receive `RandomShuffleQueue`
    and return a batch of `frames` and labels. Finally, we create operations to compute
    loss, accuracy, and the training operation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们从文本文件中获取一个 `generator` 对象。然后，我们创建两个占位符用于 `image_paths` 和 `labels`，它们将被加入到
    `RandomShuffleQueue` 中。我们在 `datasets.py` 中创建的 `input_pipeline` 函数将接收 `RandomShuffleQueue`
    并返回一批 `frames` 和标签。最后，我们创建操作来计算损失、准确率，并执行训练操作。
- en: 'We also want to log the training process and visualize it in TensorBoard. So,
    we will create some summaries:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望记录训练过程并在 TensorBoard 中可视化它。所以，我们将创建一些摘要：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`saver` and `train_writer` will be responsible for saving checkpoints and summaries
    respectively. Now, let''s finish the training process by creating the `session`
    and performing the training loop:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`saver` 和 `train_writer` 分别负责保存检查点和摘要。现在，让我们通过创建 `session` 并执行训练循环来完成训练过程：'
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This code is very straightforward. We will use the `sample_videos` function
    to get a list of image paths and labels. Then, we will call the `train_enqueue_op`
    operation to add these image paths and labels to `RandomShuffleQueue`. After that,
    the training process can be run by using `train_op` without the `feed_dict` mechanism.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常直接。我们将使用 `sample_videos` 函数获取图像路径和标签的列表。然后，我们将调用 `train_enqueue_op` 操作将这些图像路径和标签添加到
    `RandomShuffleQueue` 中。之后，训练过程可以通过使用 `train_op` 来运行，而无需使用 `feed_dict` 机制。
- en: 'Now, we can run the training process by calling the following command in the
    `root` folder:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过在 `root` 文件夹中运行以下命令来启动训练过程：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You may see the `OUT_OF_MEMORY` error if your GPU memory isn't big enough for
    a batch size of 32\. In the training process, we created a session with `gpu_options.allow_growth`
    so you can try to change the `batch_size` to use your GPU memory effectively.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的 GPU 内存不足以处理批大小为 32 时，您可能会看到 `OUT_OF_MEMORY` 错误。在训练过程中，我们创建了一个会话并设置了 `gpu_options.allow_growth`，因此您可以尝试更改
    `batch_size` 来有效使用您的 GPU 内存。
- en: The training process takes a few hours before it converges. We will take a look
    at the training process on TensorBoard.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程需要几个小时才能收敛。我们将通过 TensorBoard 查看训练过程。
- en: 'In the directory that you have chosen to save the checkpoints, run the following
    command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在您选择保存检查点的目录中，运行以下命令：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, open your web browser and navigate to `http://localhost:6006`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打开您的 web 浏览器并导航至 `http://localhost:6006`：
- en: '![](img/8bef41ea-1d59-44ee-9e19-ff375d4e6768.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bef41ea-1d59-44ee-9e19-ff375d4e6768.png)'
- en: 'The regularization loss and total loss with one GPU are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个 GPU 的正则化损失和总损失如下：
- en: '![](img/2a9a5bdf-2a3d-4a94-b80f-5baf06586d7f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a9a5bdf-2a3d-4a94-b80f-5baf06586d7f.png)'
- en: As you can see in these images, the training accuracy took about 10,000 steps
    to reach 100% accuracy on training data. These 10,000 steps took 6 hours on our
    machine. It may be different on your configuration.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在这些图像中所见，训练准确率大约经过 10,000 步达到了训练数据的 100% 准确率。这 10,000 步在我们的机器上花费了 6 小时，您的配置可能会有所不同。
- en: The training loss is decreasing, and it may reduce if we train longer. However,
    the training accuracy is almost unchanged after 10,000 steps.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失在下降，如果我们训练更长时间，它可能会进一步减少。然而，训练准确率在 10,000 步后几乎没有变化。
- en: Now, let's move on to the most interesting part of this chapter. We will use
    multiple GPUs to train and see how that helps.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入本章最有趣的部分。我们将使用多个 GPU 进行训练，并观察这对训练的帮助。
- en: Training routine with multiple GPU
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多 GPU 训练流程
- en: In our experiment, we will use our custom machine instead of Amazon EC2\. However,
    you can achieve the same result on any server with GPUs. In this section, we will
    use two Titan X GPUs with a batch size of 32 on each GPU. That way, we can compute
    up to 64 videos in one step, instead of 32 videos in a single GPU configuration.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将使用我们定制的机器，而不是 Amazon EC2。但是，您可以在任何有 GPU 的服务器上获得相同的结果。在本节中，我们将使用两块
    Titan X GPU，每块 GPU 的批大小为 32。这样，我们可以在一个步骤中处理最多 64 个视频，而不是单 GPU 配置下的 32 个视频。
- en: 'Now, let''s create a new Python file named `train_multi.py` in the `scripts`
    package. In this file, add the following code to define some parameters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 `scripts` 包中创建一个名为 `train_multi.py` 的新 Python 文件。在这个文件中，添加以下代码以定义一些参数：
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: These parameters are the same as in the previous `train.py` file, except `batch_size`.
    In this experiment, we will use the data parallelism strategy to train with multiple
    GPUs. Therefore, instead of using 32 for the batch size, we will use a batch size
    of 64\. Then, we will split the batch into two parts; each will be processed by
    a GPU. After that, we will combine the gradients from the two GPUs to update the
    weights and biases of the network.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数与之前的 `train.py` 文件相同，唯一不同的是 `batch_size`。在本实验中，我们将使用数据并行策略，通过多个 GPU 进行训练。因此，批大小不再使用
    32，而是使用 64。然后，我们将批次拆分成两部分，每部分由一块 GPU 处理。之后，我们将结合两块 GPU 的梯度来更新网络的权重和偏差。
- en: 'Next, we will use the same operations as before, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用与之前相同的操作，具体如下：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now, instead of creating a training operation with `models.train`,
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，不再使用 `models.train` 创建训练操作，
- en: we will create a optimizer and compute gradients in each GPU.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个优化器，并在每个 GPU 上计算梯度。
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The gradients will be computed on each GPU and added to a list named `total_gradients`.
    The final gradients will be computed on the CPU using `average_gradients`, which
    we will create shortly. Then, the training operation will be created by calling
    `apply_gradients` on the optimizer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度将在每个 GPU 上计算，并添加到名为 `total_gradients` 的列表中。最终的梯度将在 CPU 上使用 `average_gradients`
    计算，我们将很快创建这个函数。然后，通过调用优化器的 `apply_gradients` 来创建训练操作。
- en: 'Now, let''s add the following function to the `models.py` file in the `root`
    folder to compute the `average_gradient`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 `root` 文件夹中的 `models.py` 文件中添加以下函数来计算 `average_gradient`：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, back in the `train_multi.py` file, we will create the `saver` and `summaries`
    operation to save the `checkpoints` and `summaries`, like before:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 `train_multi.py` 文件中，我们将创建 `saver` 和 `summaries` 操作，用来保存 `checkpoints`
    和 `summaries`，就像以前一样：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, let''s add the training loop to train the network:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们添加训练循环来训练网络：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The training loop is similar to the previous, except that we have added the
    `allow_soft_placement=True` option to the session configuration. This option will
    allow TensorFlow to change the placement of `variables`, if necessary.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环与之前类似，不同之处在于我们为会话配置添加了 `allow_soft_placement=True` 选项。这个选项将允许 TensorFlow
    在必要时更改 `variables` 的位置。
- en: 'Now we can run the training scripts like before:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像之前一样运行训练脚本：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After a few hours of training, we can look at the TensorBoard to compare the
    results:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几小时的训练后，我们可以查看 TensorBoard 来比较结果：
- en: '![](img/be48229b-8546-428e-b38f-c7663b088447.png)![](img/fcb1ec27-d394-49db-b93b-c918b587d0dc.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be48229b-8546-428e-b38f-c7663b088447.png)![](img/fcb1ec27-d394-49db-b93b-c918b587d0dc.png)'
- en: Figure 04—Plot on Tensorboard of multiple GPUs training process
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 04—多个 GPU 训练过程在 Tensorboard 上的绘图
- en: As you can see, the training on multiple GPUs achieves 100% accuracy after about
    6,000 steps in about four hours on our computer. This almost reduces the training
    time by half.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在我们的计算机上，多个 GPU 训练大约经过 6000 步后，约四小时内达到了 100% 的准确率。这几乎将训练时间缩短了一半。
- en: 'Now, let''s see how the two training strategies compare:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这两种训练策略的对比：
- en: '![](img/c7041563-2279-42b8-b48d-3c5aa41d3289.png)![](img/aa1e5d15-8f4f-4c60-9a87-7d4e003dbd43.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7041563-2279-42b8-b48d-3c5aa41d3289.png)![](img/aa1e5d15-8f4f-4c60-9a87-7d4e003dbd43.png)'
- en: Figure 05—Plot on TensorBoard with single and multiple GPUs compared side by
    side
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05—在 TensorBoard 上，单个 GPU 和多个 GPU 结果并排对比的绘图
- en: The orange line is the multiple GPUs result and the blue line is the single
    GPU result. We can see that the multiple GPUs setup can achieve better results
    sooner than the single GPU. The different is not very large. However, we can achieve
    faster training with more and more GPUs. On the P1 instance on Amazon EC2, there
    are even eight and 16 GPUs. However, the benefit of training on multiple GPUs
    will be better if we train on large-scale datasets such as ActivityNet or Sports
    1M, as the single GPU will take a very long time to converge.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 橙色线表示多个 GPU 的结果，蓝色线表示单个 GPU 的结果。我们可以看到，多个 GPU 设置比单个 GPU 更早地实现了更好的结果，虽然差异不大。然而，随着
    GPU 数量的增加，我们可以实现更快的训练。在 Amazon EC2 的 P1 实例上，甚至有 8 个和 16 个 GPU。如果我们在大规模数据集上进行训练，例如
    ActivityNet 或 Sports 1M，多个 GPU 的训练优势将更加明显，因为单个 GPU 需要很长时间才能收敛。
- en: In the next section, we will take a quick look at another Amazon Service, Mechanical
    Turk.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将快速了解另一个 Amazon 服务——机械土耳其。
- en: Overview of Mechanical Turk
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机械土耳其概览
- en: Mechanical Turk is a service that allows us to create and manage online human
    intelligence tasks that will be completed by human workers. There are lots of
    tasks that humans can do better than computers. Therefore, we can take advantage
    of this service to support our machine learning system.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 机械土耳其是一项服务，允许我们创建和管理在线人类智能任务，这些任务将由人类工作者完成。很多任务是人类比计算机更擅长的。因此，我们可以利用这项服务来支持我们的机器学习系统。
- en: 'You can view this system at [https://www.mturk.com](https://www.mturk.com).
    Here is the website of the service:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://www.mturk.com](https://www.mturk.com) 查看这个系统。这里是该服务的网站：
- en: '![](img/623738e4-e367-4e9e-92a0-333b8fe78185.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/623738e4-e367-4e9e-92a0-333b8fe78185.png)'
- en: 'Here are a couple of examples of tasks that you can use to support your machine
    learning system:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些你可以用来支持机器学习系统的任务示例：
- en: '**Dataset labeling**: You usually have a lot of unlabeled data, and you can
    use Mechanical Turk to help you build a consistent ground truth for your machine
    learning workflow.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集标注**：你通常会有很多未标记的数据，可以利用机械土耳其帮助你为机器学习工作流构建一致的真实标签。'
- en: '**Generate dataset**: You can ask the workers to build a large amount of training
    data. For example, we can ask workers to create text translations or chat sentences
    for a natural language system. You can ask them to annotate the sentiments of
    the comments.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成数据集**：你可以让工作人员构建大量的训练数据。例如，我们可以要求工作人员为自然语言系统创建文本翻译或聊天句子。你还可以要求他们为评论标注情感。'
- en: Beyond labeling, Mechanical Turk can also clean up your messy datasets ready
    for training, data categorization, and metadata tagging. You can even use this
    service to have them judge your system output.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标注，Mechanical Turk还可以清理你凌乱的数据集，使其准备好进行训练、数据分类和元数据标注。你甚至可以使用此服务让他们评判你的系统输出。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have taken a look at the Amazon EC2 services to see how many server types
    we can use. Then, we created a neural network to perform human video action recognition
    on a single GPU. After that, we applied the data parallelism strategy to speed
    up the training process. Finally, we had a quick look at the Mechanical Turk service.
    We hope that you can take advantage of these services to bring your machine learning
    system to a higher level.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经查看了Amazon EC2服务，了解我们可以使用多少种服务器类型。然后，我们创建了一个神经网络，在单个GPU上进行人类视频动作识别。之后，我们应用了数据并行策略来加速训练过程。最后，我们简要了解了Mechanical
    Turk服务。我们希望你能够利用这些服务将你的机器学习系统提升到一个更高的水平。
