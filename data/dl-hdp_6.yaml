- en: Chapter 6.  Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章 自编码器
- en: '|   | *"People worry that computers will get too smart and take over the world,
    but the real problem is that they''re too stupid and they''ve already taken over
    the world."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“人们担心计算机会变得太聪明，接管世界，但真正的问题是它们太愚蠢，已经接管了世界。”* |   |'
- en: '|   | --*Pedro Domingos* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*Pedro Domingos* |'
- en: In the last chapter, we discussed a generative model called Restricted Boltzmann
    machine. In this chapter, we will introduce one more generative model called **autoencoder**.
    Autoencoder, a type of artificial neural network, is generally used for dimensionality
    reduction, feature learning, or extraction.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了一个生成模型——限制玻尔兹曼机。在本章中，我们将介绍另一个生成模型——**自编码器**。自编码器是一种人工神经网络，通常用于降维、特征学习或特征提取。
- en: As we move on with this chapter, we will discuss the concept of autoencoder
    and its various forms in detail. We will also explain the terms *regularized autoencoder*
    and *sparse autoencoder*. The concept of sparse coding, and selection criteria
    of the sparse factor in a sparse autoencoder will be taken up. Later, we will
    talk about the deep learning model, deep autoencoder, and its implementation using
    Deeplearning4j. Denoising autoencoder is one more form of a traditional autoencoder,
    which will be discussed in the end part of the chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章接下来的部分，我们将详细讨论自编码器的概念及其各种形式。我们还将解释 *正则化自编码器* 和 *稀疏自编码器* 这两个术语。将会介绍稀疏编码的概念，并探讨在稀疏自编码器中选择稀疏因子的标准。随后，我们将讨论深度学习模型——深度自编码器，并展示如何使用
    Deeplearning4j 实现它。去噪自编码器是传统自编码器的另一种形式，将在本章的后部分讨论。
- en: 'Overall, this chapter is broken into a few subsections, which are listed as
    follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，本章分为几个小节，具体内容如下：
- en: Autoencoder
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器
- en: Sparse autoencoder
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Deep autoencoder
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: Denoising autoencoder
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Applications of autoencoders
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的应用
- en: Autoencoder
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: An autoencoder is a neural network with one hidden layer, which is trained to
    learn an identity function that attempts to reconstruct its input to its output.
    In other words, the autoencoder tries to copy the input data by projecting onto
    a lower dimensional subspace defined by the hidden nodes. The hidden layer, *h*,
    describes a code, which is used to represent the input data and its structure.
    This hidden layer is thus forced to learn the structure from its input training
    dataset so that it can copy the input at the output layer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一个具有一个隐藏层的神经网络，经过训练来学习一种身份函数，尝试将其输入重建为输出。换句话说，自编码器试图通过将输入数据投影到由隐藏节点定义的低维子空间来复制输入数据。隐藏层
    *h* 描述了一种代码，用于表示输入数据及其结构。因此，这个隐藏层被迫从输入训练数据集中学习结构，以便在输出层能够复制输入。
- en: 'The network of an autoencoder can be split into two parts: encoder and decoder.
    The encoder is described by the function *h=f (k)*, and a decoder that tries to
    reconstruct or copy is defined by *r = g (h)*. The basic idea of autoencoder should
    be to copy only those aspects of the inputs which are prioritized, and not to
    create an exact replica of the input. They are designed in such a way so as to
    restrict the hidden layer to copy only approximately, and not everything from
    the input data. Therefore, an autoencoder will not be termed as useful if it learns
    to completely set *g(f(k) = k* for all the values of *k*. *Figure 6.1* represents
    the general structure of an autoencoder, mapping an input *k* to an output *r*
    through an internal hidden layer of code *h*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的网络可以分为两个部分：编码器和解码器。编码器通过函数 *h=f (k)* 描述，解码器则通过 *r = g (h)* 尝试重建或复制。自编码器的基本思想应该是只复制输入中优先的那些方面，而不是创建输入的精确副本。它们的设计方式是限制隐藏层只做大致的复制，而不是从输入数据中复制所有内容。因此，如果自编码器学习到完全设置
    *g(f(k) = k* 对所有 *k* 的值都成立，则它不能被称为有用的。*图 6.1* 表示自编码器的通用结构，通过内部隐藏层代码 *h* 将输入 *k*
    映射到输出 *r*：
- en: '![Autoencoder](img/image_06_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器](img/image_06_001.jpg)'
- en: 'Figure 6.1: General block diagram of an autoencoder. Here, input k is mapped
    to an output r through a hidden state or internal representation h. An encoder
    f maps the input k to the hidden state h, and decoder g performs the mapping of
    h to the output r.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：自编码器的通用框图。这里，输入 *k* 通过隐藏状态或内部表示 *h* 映射到输出 *r*。编码器 *f* 将输入 *k* 映射到隐藏状态
    *h*，解码器 *g* 执行 *h* 到输出 *r* 的映射。
- en: To provide one more example, let us consider *Figure 6.2*. The figure shows
    a practical representation of an autoencoder for input image patches *k*, which
    learns the hidden layer *h* to output *r*. The input layer *k* is a combination
    of intensity values from the image patches. The hidden layer nodes help to project
    the high-dimensional input layer into lower-dimensional activation values of the
    hidden nodes. These activation values of the hidden node are merged together to
    generate the output layer *r*, which is an approximation of the input pixel. In
    ideal cases, hidden layers generally have a smaller number of nodes as compared
    to the input layer nodes. For this reason, they are forced to diminish the information
    in such a way that the output layer can still be generated.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 再举一个例子，考虑 *图 6.2*。该图展示了自编码器对输入图像块 *k* 的实际表示，它学习隐藏层 *h* 以输出 *r*。输入层 *k* 是图像块强度值的组合。隐藏层节点有助于将高维输入层投影到隐藏节点的低维激活值。这些隐藏节点的激活值合并在一起生成输出层
    *r*，它是输入像素的近似。在理想情况下，隐藏层的节点数通常比输入层节点数少。因此，它们被迫以某种方式减少信息，以便仍能生成输出层。
- en: '![Autoencoder](img/B05883_06_02-1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Autoencoder](img/B05883_06_02-1.jpg)'
- en: 'Figure 6.2: Figure shows a practical example of how an autoencoder learns output
    structure from the approximation of the input pixels.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：该图展示了一个实际示例，说明了自编码器如何从输入像素的近似中学习输出结构。
- en: Replicating the structure of the input to the output might sound inefficacious,
    however, practically, the final result of an autoencoder is not exactly dependent
    on the output of the decoder. Instead, the main idea behind training an autoencoder
    is to copy the useful properties of the input task, which will reflect in the
    hidden layer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入的结构复制到输出可能听起来无效，但实际上，自编码器的最终结果并不完全依赖于解码器的输出。相反，训练自编码器的主要思想是复制输入任务的有用特性，这些特性将在隐藏层中体现。
- en: One of the common ways to extract desired features or information from the autoencoder
    is to limit the hidden layer, *h*, to have smaller dimension *(d^/)* than the
    input *k* with a dimension *d,* that is *d^/<d*. This resulting smaller dimensional
    layer can thus be called a loss compressed representation of the input *k*. An
    autoencoder whose hidden layer's dimension is less than the input's dimension
    is termed as *undercomplete*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从自编码器中提取所需特征或信息的常见方法之一是将隐藏层 *h* 的维度限制为比输入 *k* 的维度 *d* 小的维度 *d^/*，即 *d^/ < d*。这种结果较小的维度层可以称为输入
    *k* 的损失压缩表示。隐藏层维度小于输入维度的自编码器被称为*欠完备*。
- en: 'The learning process described can be mathematically represented as minimizing
    the loss function *L*, which is given as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的学习过程可以通过最小化损失函数 *L* 来数学表示，公式如下：
- en: '![Autoencoder](img/image_06_003.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![Autoencoder](img/image_06_003.jpg)'
- en: In simple words, *L* can be defined as a loss function that penalized *g (f
    (k))* for being different from the input *k*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，*L* 可以定义为一个损失函数，它惩罚 *g (f (k))* 与输入 *k* 的差异。
- en: With a linear decoder function, an autoencoder learns to form the basis for
    space as similar to the **Principal component analysis** (**PCA**) procedure.
    Upon convergence, the hidden layer will form a basis for the space spanned by
    the principal subspace of the training dataset given as the input. However, unlike
    PCA, these procedures need not necessarily generate orthogonal vectors. For this
    reason, autoencoders with non-linear encoder functions *f* and non-linear decoder
    function *g* can learn more powerful non-linear generalization of the PCA. This
    will eventually increase the capacity of the encoder and decoder to a large extent.
    With this increase in capacity, however, the autoencoder starts showing unwanted
    behavior.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性解码器函数，自编码器学习形成与**主成分分析**（**PCA**）程序相似的空间基础。收敛后，隐藏层将形成由训练数据集的主子空间所生成的空间基础。然而，与
    PCA 不同，这些过程不一定会生成正交向量。因此，具有非线性编码器函数 *f* 和非线性解码器函数 *g* 的自编码器可以学习 PCA 更强大的非线性泛化。这最终会大大增加编码器和解码器的容量。然而，随着容量的增加，自编码器开始表现出不希望出现的行为。
- en: It can then learn to undergo copying the whole input without giving attention
    to extract the desired information. In a theoretical sense, an autoencoder might
    be a one-dimensional code, but practically, a very powerful nonlinear encoder
    can learn to represent each training example *k(i)* with code *i*. The decoder
    then maps those integers *(i)* to the values of specific training examples. Hence,
    copying of only the useful features from the input dataset fails completely with
    an autoencoder with higher capacity.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它接着学习将整个输入复制，而不注意提取所需的信息。从理论上讲，一个自编码器可能是一个一维编码，但实际上，一个非常强大的非线性编码器可以学习表示每个训练示例*k(i)*，并用代码*i*来表示。然后，解码器将这些整数*(i)*映射到特定训练示例的值。因此，使用具有更高容量的自编码器完全无法从输入数据集中复制仅有的有用特征。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: PCA is a statistical method which applies orthogonal transformation to convert
    a set of possibly correlated observed variables into a set of linearly correlated
    set of variables termed as principal components. The number of principal components
    in the PCA method is less than or equal to the number of original input variables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种统计方法，通过正交变换将一组可能相关的观测变量转化为一组线性相关的变量，称为主成分。在PCA方法中，主成分的数量小于或等于原始输入变量的数量。
- en: Similar to the edge case problem mentioned for an undercomplete autoencoder,
    where the dimension of the hidden layer is less than that of the input, autoencoder,
    where the hidden layer or code is allowed to have an equal dimension of input,
    often faces the same problem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于为欠完备自编码器提到的边缘情况问题，在该问题中，隐藏层的维度小于输入的维度，允许隐藏层或编码器的维度与输入相等的自编码器通常面临相同的问题。
- en: An autoencoder, where the hidden code has greater dimension than the dimension
    of the input, is termed as an overcomplete autoencoder. This type of autoencoder
    is even more vulnerable to the aforementioned problems. Even a linear encoder
    and decoder can perform learning a copy of input to output without learning any
    desired attributes of the input dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个隐藏代码维度大于输入维度的自编码器称为完备自编码器。这种类型的自编码器更加容易受到上述问题的影响。即使是线性编码器和解码器也能在不学习任何输入数据集的期望属性的情况下，完成将输入复制到输出的学习。
- en: Regularized autoencoders
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化自编码器
- en: By choosing a proper dimension for the hidden layer, and the capacity of the
    encoder and decoder in accordance with the complexity of the model distribution,
    autoencoders of any kind of architecture can be built successfully. The autoencoder
    which has the ability to provide the same is termed as a regularized autoencoder.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为隐藏层选择合适的维度，并根据模型分布的复杂性调整编码器和解码器的容量，任何类型架构的自编码器都可以成功构建。能够提供这种能力的自编码器被称为正则化自编码器。
- en: Besides the ability to copy the input to output, a regularized autoencoder has
    a loss function, which helps the model to possess other properties too. These
    include robustness to missing inputs, sparsity of the representation of data,
    smallness of the derivative of the representation, and so on. Even a nonlinear
    and *overcomplete* regularized autoencoder is able to learn at least something
    about the data distribution, irrespective of the capacity of the model. Regularized
    autoencoders [131] are able to capture the structure of the training distribution
    with the help of productive opposition between a restructuring error and a regularizer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能够将输入复制到输出的功能外，正则化自编码器还有一个损失函数，帮助模型具备其他特性。这些特性包括对缺失输入的鲁棒性、数据表示的稀疏性、表示的导数小等。即使是非线性和*过完备*的正则化自编码器，也能学习到关于数据分布的某些信息，而不考虑模型的容量。正则化自编码器[131]能够通过重构误差与正则化项之间的有效对立，捕捉训练分布的结构。
- en: Sparse autoencoders
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Distributed sparse representation is one of the primary keys to learn useful
    features in deep learning algorithms. Not only is it a coherent mode of data representation,
    but it also helps to capture the generation process of most of the real world
    dataset. In this section, we will explain how autoencoders encourage sparsity
    of data. We will start with introducing sparse coding. A code is termed as sparse
    when an input provokes the activation of a relatively small number of nodes of
    a neural network, which combine to represent it in a sparse way. In deep learning
    technology, a similar constraint is used to generate the sparse code models to
    implement regular autoencoders, which are trained with sparsity constants called
    sparse autoencoders.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式稀疏表示是深度学习算法中学习有用特征的关键之一。它不仅是数据表示的一种连贯模式，还帮助捕捉到大多数现实世界数据集的生成过程。在这一部分，我们将解释自编码器如何鼓励数据的稀疏性。我们将从介绍稀疏编码开始。当输入引发神经网络中相对少量的节点激活时，结合这些节点可以以稀疏的方式表示输入，那么这种编码就被称为稀疏编码。在深度学习技术中，类似的约束被用来生成稀疏编码模型，以实现常规自编码器，这些自编码器是通过名为稀疏自编码器的稀疏常数训练出来的。
- en: Sparse coding
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏编码
- en: Sparse coding is a type of unsupervised method to learn sets of *overcomplete*
    bases in order to represent the data in a coherent and efficient way. The primary
    goal of sparse coding is to determine a set of vectors *(n) v[i] *such that the
    input vector *k* can be represented as a linear combination of these vectors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏编码是一种无监督学习方法，用于学习*过完备*的基集合，以便以连贯高效的方式表示数据。稀疏编码的主要目标是确定一组向量*(n) v[i]*，使得输入向量*k*可以表示为这些向量的线性组合。
- en: 'Mathematically, this can be represented as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，这可以表示为如下形式：
- en: '![Sparse coding](img/image_06_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏编码](img/image_06_004.jpg)'
- en: Here *a[i]* is the coefficient associated with each vector *v[i]* [.]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*a[i]*是与每个向量*v[i]*相关联的系数[.]
- en: With the help of PCA, we can learn a complete set of basis vectors in a coherent
    way; however, we want to learn an *overcomplete* set of basis vectors to represent
    the input vector *k* ![Sparse coding](img/equation.jpg) where *n>m*. The reason
    to have the *overcomplete* basis is that the basis vectors are generally able
    to catch the pattern and structure that are inherent to the input data. However,
    overcompleteness sometime raises a degeneracy that, with its basis, the coefficient
    *a[i ]*cannot uniquely identify the input vector *k*. For this reason, an additional
    criterion called sparsity is introduced in sparse coding.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA的帮助下，我们可以以一种连贯的方式学习一组完整的基向量；然而，我们希望学习一组*过完备*的基向量来表示输入向量*k* ![稀疏编码](img/equation.jpg)，其中*n>m*。拥有*过完备*基的原因在于，基向量通常能够捕捉到输入数据固有的模式和结构。然而，过完备性有时会引发退化问题，即通过其基，系数*a[i]*无法唯一标识输入向量*k*。因此，引入了一种额外的标准，称为稀疏性，这在稀疏编码中起到了作用。
- en: In a simple way, sparsity can be defined as having few non-zero components or
    having few components that are not close to zero. The set of coefficients *a[i]*
    is termed as sparse if, for a given input vector, the number of non-zero coefficients,
    or the number of coefficients that are way far from zero, should be a few.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，稀疏性可以定义为具有少量非零组件，或者具有少量不接近零的组件。如果对于给定的输入向量，非零系数的数量或远离零的系数数量较少，则称系数集合*a[i]*为稀疏。
- en: With this basic understanding of sparse coding, we can now move to the next
    part to discover how the sparse coding concept is used for autoencoders to generate
    sparse autoencoders.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有了对稀疏编码的基本理解后，我们可以进入下一部分，探讨稀疏编码概念如何用于自编码器，从而生成稀疏自编码器。
- en: Sparse autoencoders
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: When the input dataset maintains some structure, and if the input features are
    correlated, then even a simple autoencoder algorithm can discover those correlations.
    Moreover, in such cases, a simple autoencoder will end up learning a low-dimensional
    representation, which is similar to PCA.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入数据集保持一定结构，并且输入特征之间存在相关性时，即便是一个简单的自编码器算法也能够发现这些相关性。此外，在这种情况下，一个简单的自编码器最终会学习到一个低维表示，类似于主成分分析（PCA）。
- en: This perception is based on the fact that the number of hidden layers is relatively
    small. However, by imposing other constraints on the network, even with a large
    number of hidden layers, the network can still discover desired features from
    the input vectors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种看法基于隐藏层数量相对较少的事实。然而，通过对网络施加其他约束条件，即便是大量隐藏层的网络，依然能够从输入向量中发现所需的特征。
- en: Sparse autoencoders are generally used to learn features to perform other tasks
    such as classification. Autoencoders for which the sparsity constraints have been
    added must respond to the unique statistical features of the input dataset with
    which it is training on, rather than simply acting as an identity function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自编码器通常用于学习特征，以执行其他任务，如分类。对于添加了稀疏性约束的自编码器，它必须响应其训练数据集的独特统计特征，而不仅仅是充当一个恒等函数。
- en: '![Sparse autoencoders](img/B05883_06_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/B05883_06_03.jpg)'
- en: 'Figure 6.3: Figure shows a typical example of a sparse autoencoder'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：图示展示了一个典型的稀疏自编码器示例
- en: Sparse autoencoders are a type of autoencoder with a sparsity enforcer, which
    helps to direct a single layer network to learn the hidden layer code. This approach
    minimizes the reconstruction errors along with restricting the number of code
    words needed to restructure the output. This kind of sparsifying algorithm can
    be considered as a classification problem that restricts the input to a single
    class value, which helps to reduce the prediction errors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自编码器是一种具有稀疏性约束的自编码器，它有助于引导单层网络学习隐藏层代码。该方法通过限制重构所需的代码词数来最小化重构误差。这种稀疏化算法可以视为一个分类问题，它将输入限制为单一类别值，从而帮助减少预测误差。
- en: 'In this part, we will explain sparse autoencoder with a simple architecture.
    *Figure 6.3* shows the simplest form of a sparse autoencoder, consisting of a
    single hidden layer *h*. The hidden layer *h*, is connected to the input vector
    *K* by a weight matrix, *W*, which forms the encoding step. In the decoder step,
    the hidden layer h outputs to a reconstruction vector *K` *with the help of the
    tied weight matrix *W^T*. In the network, the activation function is denoted as
    *f* and the bias term as *b*. The activation function could be anything: linear,
    sigmoidal, or ReLU.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将通过一个简单的架构来解释稀疏自编码器。*图6.3*展示了稀疏自编码器的最简单形式，包含一个隐藏层*h*。隐藏层*h*通过权重矩阵*W*与输入向量*K*相连，形成编码步骤。在解码步骤中，隐藏层*h*通过绑定权重矩阵*W^T*输出到重构向量*K`。在网络中，激活函数用*f*表示，偏置项用*b*表示。激活函数可以是任何类型：线性、Sigmoid或ReLU。
- en: 'The equation to compute the sparse representation of the hidden code *l* is
    written as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算隐藏代码*l*的稀疏表示的方程式如下所示：
- en: '![Sparse autoencoders](img/Capture-10.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/Capture-10.jpg)'
- en: 'The reconstructed output is the hidden representation, mapped linearly to the
    output using this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重构的输出是隐藏表示，通过以下方式线性映射到输出：
- en: '![Sparse autoencoders](img/Capture-11.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/Capture-11.jpg)'
- en: 'Learning occurs via backpropagation on the reconstruction error. All the parameters
    are optimized to minimize the mean square error, given as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 学习通过反向传播重构误差进行。所有参数都被优化以最小化均方误差，公式如下：
- en: '![Sparse autoencoders](img/image_06_008.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/image_06_008.jpg)'
- en: As we have the network setup now, we can add the sparsifying component, which
    drives the vector *L* towards a sparse representation. Here, we will use k-Sparse
    autoencoders to implement the sparse representation of the layer. (Don't get confused
    between the *k* of k-Sparse representation and *K* input vector. To distinguish
    between both of them, we have denoted these two with a small *k* and capital *K*
    respectively.)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了网络，可以加入稀疏化组件，它将驱动向量*L*朝着稀疏表示方向发展。在这里，我们将使用k-Sparse自编码器来实现这一层的稀疏表示。（不要混淆k-Sparse表示中的*k*和*K*输入向量。为了区分它们，我们将这两者分别用小*k*和大*K*表示。）
- en: The k-Sparse autoencoder
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-Sparse自编码器
- en: The k-Sparse autoencoder [132] is based on an autoencoder with tied weights
    and linear activation functions. The basic idea of a k-Sparse autoencoder is very
    simple. In the feed-forward phase of the autoencoder, once we compute the hidden
    code *l = WK + b*, rather than reconstructing the input from all the hidden units,
    the method searches for the *k* largest hidden units and sets the remaining hidden
    units' values as zero.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: k-Sparse自编码器[132]是基于具有绑定权重和线性激活函数的自编码器。k-Sparse自编码器的基本思想非常简单。在自编码器的前馈阶段，一旦我们计算出隐藏代码*l
    = WK + b*，方法就不会从所有隐藏单元重构输入，而是寻找*k*个最大的隐藏单元，并将其余隐藏单元的值设置为零。
- en: There are alternative methods to determine the *k* largest hidden units. By
    sorting the activities of the hidden units or using ReLU, hidden units with thresholds
    are adjusted until we determine the *k* largest activities. This selection step
    to find the *k* largest activities is non-linear. The selection step behaves like
    a regularizer, which helps to prevent the use of large numbers of hidden units
    while building the output by reconstructing the input.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法可以确定 *k* 个最大的隐藏单元。通过排序隐藏单元的活动或使用 ReLU，隐藏单元的阈值会被调整，直到我们确定 *k* 个最大的活动。这个选择步骤用于找到
    *k* 个最大的活动是非线性的。这个选择步骤像一个正则化器，帮助防止在通过重构输入生成输出时使用过多的隐藏单元。
- en: How to select the sparsity level k
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何选择稀疏性水平 k
- en: An issue might arise during the training of a k-Sparse autoencoder if we enforce
    a low sparsity level, say *k=10*. One common problem is that in the first few
    epochs, the algorithm will aggressively start assigning the individual hidden
    units to groups of training cases. The phenomena can be compared with the k-means
    clustering approach. In the successive epochs, these hidden units will be selected
    and re-enforced, but the other hidden units would not be adjusted.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们强制使用较低的稀疏性水平，例如 *k=10*，在训练 k-Sparse 自编码器时可能会出现一个问题。一个常见的问题是，在前几个 epoch 中，算法会积极地开始将各个隐藏单元分配到训练样本的组中。这种现象可以与
    k-means 聚类方法进行比较。在接下来的 epochs 中，这些隐藏单元会被选中并重新强化，但其他隐藏单元则不会被调整。
- en: This issue can be addressed by scheduling the sparsity level in a proper way.
    Let us assume we are aiming for a sparsity level of 10\. In such cases, we can
    start with a large sparsity level of say *k=100* or *k=200*. Hence, the k-Sparse
    autoencoder can train all the hidden units present. Gradually, over half of the
    epoch, we can linearly decrease the sparsity level of *k=100* to *k=10*. This
    greatly increases the chances of all the hidden units being picked. Then, we will
    keep *k=10* for the next half of the epoch. In this way, this kind of scheduling
    will guarantee that even with a low sparsity level, all of the filters will be
    trained.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过适当地安排稀疏性水平来解决。假设我们的目标是稀疏性水平为 10。在这种情况下，我们可以从较大的稀疏性水平开始，比如 *k=100* 或
    *k=200*。因此，k-Sparse 自编码器可以训练所有的隐藏单元。逐渐地，在一个 epoch 的前半部分，我们可以将稀疏性水平从 *k=100* 线性地减少到
    *k=10*。这样大大增加了选中所有隐藏单元的机会。然后，在该 epoch 的后半部分，我们将保持 *k=10*。通过这种方式，这种调度方法可以保证即使在低稀疏性水平下，所有的过滤器都能得到训练。
- en: Effect of sparsity level
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏性水平的影响
- en: The choice of value of *k* is very much crucial while designing or implementing
    a k-Sparse autoencoder. The value of *k* determines the desirable sparsity level,
    which helps to make the algorithm ideal for a wide variety of datasets. For example,
    one application could be used to pre-train a deep discriminative neural network
    or a shallow network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计或实现 k-Sparse 自编码器时，*k* 值的选择至关重要。*k* 的值决定了期望的稀疏性水平，这有助于使算法适用于各种数据集。例如，一个应用可以用于预训练一个深度判别神经网络或一个浅层网络。
- en: If we take a large value for *k* (say *k=200* on an MNIST dataset), the algorithm
    will tend to identify and learn very local features of the dataset. These features
    sometimes behave too prematurely to be used for the classification of a shallow
    architecture. A shallow architecture generally has a naive linear classifier,
    which does not really have enough architectural strength to merge all of these
    features and achieve a substantial classification rate. However, similar local
    features are very much desirable to pre-train a deep neural network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择一个较大的 *k* 值（例如在 MNIST 数据集上 *k=200*），算法将倾向于识别和学习数据集的非常局部的特征。这些特征有时表现得过于初期，无法用于浅层架构的分类。浅层架构通常有一个简单的线性分类器，它实际上没有足够的架构能力来合并所有这些特征并实现显著的分类率。然而，类似的局部特征在预训练深度神经网络时非常有用。
- en: For a smaller value of the sparsity level (say *k=10* on an MNIST dataset),
    the output is reconstructed from the input using a smaller set of hidden units.
    This eventually results in detecting of global features from the datasets, instead
    of local features as in the earlier case. These less local features are suitable
    for shallow architecture for the classification tasks. On the contrary, these
    types of situations are not ideal for deep neural networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的稀疏性水平（例如在 MNIST 数据集上 *k=10*），输出是通过使用较少的隐藏单元从输入重构的。这最终会导致检测到数据集中的全局特征，而不是像前面那样检测到局部特征。这些较少的局部特征适用于浅层架构的分类任务。相反，这种情况对于深度神经网络来说并不理想。
- en: Deep autoencoders
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: So far, we have talked only about single-layer encoders and single-layer decoders
    for a simple autoencoder. However, a deep autoencoder with more than one encoder
    and decoder brings more advantages.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的只是简单自编码器的单层编码器和单层解码器。然而，具有多个编码器和解码器的深度自编码器带来了更多的优势。
- en: Feed-forward networks perform better when they are deep. Autoencoders are basically
    feed-forward networks; hence, the advantages of a basic feed-forward network can
    also be applied to autoencoders. The encoders and decoders are autoencoders, which
    also work like a feed-forward network. Hence, we can deploy the advantages of
    the depth of a feed-forward network in these components also.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络在深度增加时表现更好。自编码器本质上是前馈网络，因此，基础前馈网络的优势同样适用于自编码器。编码器和解码器是自编码器，它们也像前馈网络一样工作。因此，我们可以在这些组件中也应用前馈网络深度的优势。
- en: In this context, we can also talk about the universal approximator theorem,
    which ensures that a feed-forward neural network with at least one hidden layer,
    and with enough hidden units, can produce an approximation of any arbitrary function
    to any degree of accuracy. Following this concept, a deep autoencoder having at
    least one hidden layer, and containing sufficient hidden units, can approximate
    any mapping from input to code arbitrarily well.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们还可以讨论普适逼近定理，该定理保证了一个至少包含一层隐藏层，并且隐藏单元足够多的前馈神经网络，可以逼近任何任意函数，并且可以达到任意精度。根据这一概念，深度自编码器只要有至少一层隐藏层，并且包含足够的隐藏单元，就能逼近输入到编码的任意映射。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: One can approximate any continuous function to any degree of accuracy with a
    two-layer network. In the mathematical theory of artificial neural networks, the
    universal approximation function states that a feed-forward network can approximate
    any continuous function of a compact subset of *R^n*, if it has at least one hidden
    layer with a finite number of neurons.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个两层网络可以逼近任何连续函数，且可以达到任意精度。在人工神经网络的数学理论中，普适逼近定理指出，如果前馈网络至少有一层隐藏层，并且隐藏单元数量是有限的，那么它可以逼近*R^n*的任何连续函数。
- en: Deep autoencoder provides many advantages as compared to shallow architecture.
    The non-trivial depth of an autoencoder suppresses the computation of representing
    a few functions. Also, the depth of autoencoders drastically reduces the amount
    of training data required to learn the functions. Even experimentally, it has
    been found that deep autoencoders provide better compression when compared to
    shallow autoencoders.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器相比浅层架构提供了许多优势。自编码器的非平凡深度抑制了表示一些函数的计算。同时，自编码器的深度大大减少了学习这些函数所需的训练数据量。甚至通过实验发现，深度自编码器在压缩性能上优于浅层自编码器。
- en: To train a deep autoencoder, the common practice is to train a stack of shallow
    autoencoders. Therefore, to train a deep autoencoder, a series of shallow autoencoders
    are encountered frequently. In the next subsections, we will discuss the concept
    of deep autoencoders in depth.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度自编码器的常见做法是训练一系列浅层自编码器。因此，在训练深度自编码器时，常常会遇到一系列浅层自编码器。在接下来的小节中，我们将深入讨论深度自编码器的概念。
- en: Training of deep autoencoders
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度自编码器的训练
- en: 'The design of a deep autoencoder explained here is based on MNIST handwritten
    digit databases. In the paper [133],a well-structured procedure of building and
    training of a deep autoencoder is explained. The fundamentals of training a deep
    autoencoder is through three phases, that is: Pre-training, Unrolling, and Fine-tuning.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此处所解释的深度自编码器设计基于MNIST手写数字数据库。在论文[133]中，详细解释了构建和训练深度自编码器的结构化流程。训练深度自编码器的基本过程分为三个阶段：预训练、展开和微调。
- en: '**Pre-training**: The first phase of training a deep autoencoder is ''pre-training''.
    The main purpose of this phase is to work on binary data, generalize in to a real-valued
    data, and then to conclude that it works well for various datasets.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练**：训练深度自编码器的第一阶段是“预训练”。这一阶段的主要目的是处理二进制数据，将其推广到实值数据，并最终得出结论，表明它在各种数据集上都能良好工作。'
- en: 'We already have enough insights that a single layer of hidden units is not
    the proper way to model the structure in a large set of images. A deep autoencoder
    is composed of multiple layers of a Restricted Boltzmann machine. In [Chapter
    5](ch05.html "Chapter 5.  Restricted Boltzmann Machines"), *Restricted Boltzmann
    Machines* we gave enough information on how a Restricted Boltzmann machine works.
    Using the same concept, we can proceed to build the structure for a deep autoencoder:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经有足够的见解，认为单层隐藏单元并不是建模大量图像结构的合适方式。深度自编码器由多个限制玻尔兹曼机层组成。在[第5章](ch05.html "第5章
    限制玻尔兹曼机")，*限制玻尔兹曼机*中，我们提供了足够的信息来说明限制玻尔兹曼机是如何工作的。使用相同的概念，我们可以继续构建深度自编码器的结构：
- en: '![Training of deep autoencoders](img/image_06_009.jpg)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![深度自编码器的训练](img/image_06_009.jpg)'
- en: 'Figure 6.4: Pre-training a deep autoencoder involves learning a stack of Restricted
    Boltzmann machines (RBMs) where each RBM possesses a single layer of feature detectors.
    The learned features of one Restricted Boltzmann machine is used as the ''input
    data'' to train the next RBM of the stack. After the pre-training phase, all the
    RBMs are unfolded or unrolled to build a deep autoencoder. This deep autoencoder
    is then fine-tuned using the backpropagation approach of error derivatives.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.4：预训练深度自编码器涉及学习一堆限制玻尔兹曼机（RBM），每个RBM具有一层特征检测器。一个限制玻尔兹曼机学习到的特征作为“输入数据”用于训练堆叠中的下一个RBM。经过预训练阶段，所有RBM都会被展开或展开以构建深度自编码器。然后，使用误差导数的反向传播方法对深度自编码器进行微调。
- en: When the first layer of the RBM is driven by a stream of data, the layer starts
    to learn the feature detectors. This learning can be treated as input data for
    learning for the next layer. In this way, feature detectors of the first layer
    become the visible units for learning the next layer of the Restricted Boltzmann
    machine. This procedure of learning layer-by-layer can be iterated as many times
    as desired. This procedure is indeed very much effectual in pre-training the weights
    of a deep autoencoder. The features captured after each layer have a string of
    high-order correlations between the activities of the hidden units below. The
    first part of *Figure 6.4* gives a flow diagram of this procedure. Processing
    the benchmark dataset MNIST, a deep autoencoder would use binary transformations
    after each RBM. To process real-valued data, deep autoencoders use Gaussian rectified
    transformations after each Restricted Boltzmann machine layer.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当RBM的第一层通过数据流驱动时，该层开始学习特征检测器。这种学习可以作为下一层学习的输入数据。通过这种方式，第一层的特征检测器成为下一层限制玻尔兹曼机学习的可见单元。这个逐层学习的过程可以根据需要反复进行。这个过程在预训练深度自编码器的权重时确实非常有效。每一层捕捉到的特征具有与下方隐藏单元活动之间的高阶相关性。*图6.4*的第一部分给出了这一过程的流程图。在处理基准数据集MNIST时，深度自编码器在每个RBM之后会使用二值变换。为了处理实值数据，深度自编码器在每个限制玻尔兹曼机层之后使用高斯修正变换。
- en: '![Training of deep autoencoders](img/B05883_06_05-1.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![深度自编码器的训练](img/B05883_06_05-1.jpg)'
- en: 'Figure 6.5: Pictorial representation of how the number or vectors of encoder
    and decoder varies during the phases.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.5：图示表示编码器和解码器的向量数量如何在各个阶段变化。
- en: '**Unrolling**: Once the multiple layers of feature detectors of the deep autoencoders
    are pre-trained, the whole model is unrolled to generate the encoder and decoder
    networks, which at first use the same weights. We will explain each of the designs
    of each part given in the second part of the image separately to have a better
    understanding of this phase.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**展开**：一旦深度自编码器的多层特征检测器经过预训练，整个模型将被展开以生成编码器和解码器网络，最初它们使用相同的权重。我们将在图像第二部分中分别解释每个部分的设计，以便更好地理解这一阶段。'
- en: '**Encoder**: For an MNIST dataset of *28x28* pixel images, the input that the
    network will get is that of 784 pixels. As per the rule of thumb, the number of
    parameters of the first layer of the deep autoencoder should be slightly larger.
    As shown in *Figure 6.4*, **2000** parameters are taken for the first layer of
    the network. This might sound unreasonable, as taking more parameters as inputs
    increase the chance of overfitting the network. However, in this case, increasing
    the number of parameters will eventually increase the features of the input, which,
    in turn, make the decoding of the autoencoder data possible.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：对于一个 *28x28* 像素的 MNIST 数据集，网络输入的图像将是 784 个像素。根据经验法则，深度自编码器第一层的参数数量应该略大。如
    *图 6.4* 所示，网络第一层使用了 **2000** 个参数。这可能听起来不太合理，因为增加输入的参数数量会增加过拟合的风险。然而，在这种情况下，增加参数数量最终会增加输入的特征，从而使得自编码器数据的解码成为可能。'
- en: As shown in *Figure 6.4*, the layers would be **2000**, **1000**, **500**, and
    **30**-nodes wide respectively. A snapshot of this phenomenon is depicted in *Figure
    6.5*. In the end, the encoder will produce a vector **30** numbers long. This
    **30** number vector is the last layer of the encoder of the deep autoencoder.
    A rough outline for this encoder will be as follows:![Training of deep autoencoders](img/image_06_011.jpg)
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如 *图 6.4* 所示，层的宽度分别为 **2000**、**1000**、**500** 和 **30** 个节点。这个现象的快照在 *图 6.5*
    中得以展示。最终，编码器将生成一个 **30** 长度的向量。这个 **30** 个数字的向量是深度自编码器编码器的最后一层。这个编码器的大致框架如下：![深度自编码器训练](img/image_06_011.jpg)
- en: '**Decoder**: The **30** number vectors found at the end of the encoding phase
    are the encoded version of the 28x28 pixel images. The second part of the deep
    autoencoder is the decoder phase, where it basically learns how to decode the
    condensed vector. Hence, the output of the encoder phase (**30**-number vectors)
    becomes the input of the decoder phase. This half of the deep autoencoder is a
    feed-forward network, where the encoded condensed vector proceeds towards the
    reconstructed input after each layer. The layers shown in *Figure 6.4* are **30**,
    **500**, **1000**, and **2000**. The layers initially possess the same weights
    as their counterparts in the pre-training network; it is just that the weights
    are transposed as shown in the figure. A rough outline for this encoder will be
    as follows:![Training of deep autoencoders](img/image_06_012.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：在编码阶段结束时，得到的 **30** 个向量是 28x28 像素图像的编码版本。深度自编码器的第二部分是解码器阶段，它基本上学习如何解码压缩向量。因此，编码器阶段的输出（**30**
    个向量）成为解码器阶段的输入。深度自编码器的这一部分是一个前馈网络，经过每一层后，编码的压缩向量逐渐传递到重建的输入。*图 6.4* 中显示的各层是 **30**、**500**、**1000**
    和 **2000**。这些层最初的权重与预训练网络中的对应层相同，唯一不同的是，权重被转置，如图所示。这个编码器的大致框架如下：![深度自编码器训练](img/image_06_012.jpg)'
- en: So, the main purpose of decoding half of a deep autoencoder is to learn how
    to reconstruct the image. The operation is carried out in the second feed-forward
    network that also performs back propagation, which happens through reconstruction
    entropy.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，解码深度自编码器的一半的主要目的是学习如何重建图像。这个操作是在第二个前馈网络中进行的，且该网络也执行反向传播，这通过重建熵来实现。
- en: '**Fine-tuning**: In the fine-tuning phase, the stochastic activities are replaced
    by the deterministic, real-valued probabilities. The weights associated with each
    layer of the whole deep autoencoder are fine-tuned for optimal reconstruction
    by using the backpropagation method.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**：在微调阶段，随机活动被替换为确定性、实值概率。整个深度自编码器每一层的权重通过反向传播方法被微调，以实现最优重建。'
- en: Implementation of deep autoencoders using Deeplearning4j
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Deeplearning4j 实现深度自编码器
- en: So, you now have sufficient idea of how to build a deep autoencoder using a
    number of Restricted Boltzmann machines. In this section, we will explain how
    to design a deep autoencoder with the help of Deeplearning4j.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经对如何使用多个限制玻尔兹曼机构建深度自编码器有了足够的了解。在本节中，我们将解释如何利用 Deeplearning4j 设计深度自编码器。
- en: We will use the same MNIST dataset as in the previous section, and keep the
    design of the deep autoencoder similar to what we explained earlier.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一节相同的 MNIST 数据集，并保持深度自编码器的设计与之前解释的相似。
- en: As already explained in earlier examples, a small batch size of 1024 number
    of examples is used from the raw MNIST datasets, which can be split into *N* multiple
    blocks of Hadoop. These *N* multiple blocks will run on the Hadoop Distributed
    File System by each worker in parallel. The flow of code to implement the deep
    autoencoder is simple and straightforward.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面例子所述，使用从原始 MNIST 数据集中的小批次 `1024` 个样本，可以将其拆分成 *N* 个 Hadoop 块。这些 *N* 个块将由每个工作节点并行地在
    Hadoop 分布式文件系统上运行。实现深度自编码器的代码流程简单直接。
- en: 'The steps are shown as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下所示：
- en: Batch-wise loading of the MNIST dataset in HDFS. Each batch will contain `1024`
    number of examples.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 HDFS 中按批次加载 MNIST 数据集。每个批次将包含 `1024` 个样本。
- en: Start building the model.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始构建模型。
- en: Perform the encoding operation.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行编码操作。
- en: Perform the decoding operation.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行解码操作。
- en: Train the model by calling the `fit()` method.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `fit()` 方法训练模型。
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Initial configuration needed to set the Hadoop environment. The `batchsize`
    is set to `1024`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Hadoop 环境所需的初始配置。`batchsize` 被设置为 `1024`。
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Load the data into the HDFS:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到 HDFS 中：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are now all set to build the model to add the number of layers of the Restricted
    Boltzmann machine to build the deep autoencoder:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好构建模型，加入受限玻尔兹曼机的层数以构建深度自编码器：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To a create a ListBuilder with the specified layers (here it is eight), we
    call the .`list()` method:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个具有指定层数（这里是八层）的 ListBuilder，我们调用 `.list()` 方法：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step now is to build the encoding phase of the model. This can be
    done by the subsequent addition of the Restricted Boltzmann machine into the model.
    The encoding phase has four layers of the restricted Boltzmann machine in which
    each layer would have `2000`, `1000`, `500`, and `30` nodes respectively:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建模型的编码阶段。可以通过将受限玻尔兹曼机（Restricted Boltzmann machine）逐步加入到模型中来完成。编码阶段包含四层受限玻尔兹曼机，每层分别有
    `2000`、`1000`、`500` 和 `30` 个节点：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next phase after encoder is the decoder phase, where we will use four more
    Restricted Boltzmann machines in a similar manner:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器后的下一阶段是解码器阶段，我们将在此阶段使用四个受限玻尔兹曼机，方法与之前相似：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As all the intermediate layers are now built, we can build the model by calling
    the `build()` method:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有中间层已构建完毕，现在可以通过调用 `build()` 方法来构建模型：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last phase of the implementation is to train the deep autoencoder. It can
    be done by calling the `fit ()` method:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的最后阶段是训练深度自编码器。可以通过调用 `fit()` 方法来完成：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Denoising autoencoder
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: The reconstruction of output from input does not always guarantee the desired
    output, and can sometimes end up in simply copying the input. To prevent such
    a situation, in [134], a different strategy has been proposed. In that proposed
    architecture, rather than putting some constraints in the representation of the
    input data, the reconstruction criteria is built, based on cleaning the partially
    corrupted input.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入重构输出并不总是能保证得到期望的输出，有时可能只是简单地复制输入。为避免这种情况，[134] 提出了一个不同的策略。在该提议的架构中，重构标准是基于清理部分损坏的输入来构建的，而不是在输入数据的表示上施加一些约束。
- en: '*"A good representation is one that can be obtained robustly from a corrupted
    input and that will be useful for recovering the corresponding clean input."*'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"一个好的表示是能够从损坏的输入中稳健地获得的，并且对恢复相应的干净输入是有用的。"*'
- en: A denoising autoencoder is a type of autoencoder which takes corrupted data
    as input, and the model is trained to predict the original, clean, and uncorrupted
    data as its output. In this section, we will explain the basic idea behind designing
    a denoising autoencoder.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器是一种自编码器，它以损坏的数据作为输入，模型被训练以预测原始的、干净的和未损坏的数据作为输出。在本节中，我们将解释设计去噪自编码器的基本思想。
- en: Architecture of a Denoising autoencoder
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪自编码器的架构
- en: The primary idea behind a denoising autoencoder is to introduce a corruption
    process, *Q (k^/ | k)*, and reconstruct the output *r* from the corrupted input
    *k^/*. *Figure 6.6* shows the overall representation of a denoising autoencoder.
    In a denoising autoencoder, for every minibatch of training data *k*, the corresponding
    corrupted *k^/* should be generated using *Q (k^/ | k)*. From there, if we consider
    the initial input as the corrupted input *k^/*, then the whole model can be considered
    as a form of a basic encoder. The corrupted input *k^(/ )*is mapped to generate
    the hidden representation *h*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器的主要思想是引入一个损坏过程，*Q (k^/ | k)*，并从损坏的输入*k^/*重构输出*r*。*图 6.6*展示了去噪自编码器的整体表示。在去噪自编码器中，对于每个小批量的训练数据*k*，应该使用*Q
    (k^/ | k)*生成相应的损坏*k^/*。从这里来看，如果我们将初始输入视为损坏输入*k^/*，那么整个模型可以看作是一种基本编码器的形式。损坏的输入*k^/*被映射生成隐藏表示*h*。
- en: 'Therefore, we get the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到以下内容：
- en: '![Architecture of a Denoising autoencoder](img/image_06_013.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器的结构](img/image_06_013.jpg)'
- en: From this hidden representation, the reconstructed output, *r*, can be derived
    using *r = g (h)*. Denoising autoencoder reorganizes the data, and then tries
    to learn about the data for the reconstruction of the output. This reorganization
    of the data or shuffling of the data generates the noise, and the model learns
    the features from the noise, which allows categorizing the input. During training
    of the network, it produces a model, which computes the distance between that
    model and the benchmark through a loss function. The idea is to minimize the average
    reconstruction error over a training set to make the output r as close as possible
    to the original uncorrupted input *k*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个隐藏表示中，可以使用*r = g (h)*推导出重构的输出*r*。去噪自编码器重新组织数据，然后尝试学习这些数据，以便重构输出。这种数据的重新组织或数据的洗牌生成了噪声，模型通过噪声学习特征，从而实现对输入的分类。在网络训练过程中，它生成一个模型，该模型通过损失函数计算该模型与基准之间的距离。其思想是最小化训练集上的平均重构误差，使输出r尽可能接近原始未损坏的输入*k*。
- en: '![Architecture of a Denoising autoencoder](img/B05883_06_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器的结构](img/B05883_06_06.jpg)'
- en: 'Figure 6.6: The steps involved in designing a denoising autoencoder. The original
    input is k; corrupted input derived from k is denoted as k^/. The final output
    is denoted as r.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：设计去噪自编码器的步骤。原始输入为k；从k衍生的损坏输入表示为k^/。最终输出表示为r。
- en: Stacked denoising autoencoders
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠去噪自编码器
- en: The basic concept of building a stacked denoising autoencoder to initialize
    a deep neural network is similar to stacking a number of Restricted Boltzmann
    machines to build a Deep Belief network or a traditional deep autoencoder. The
    generation of corrupted input is only needed for the initial denoising training
    of each of the individual layers to help in learning the useful features extraction.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 构建堆叠去噪自编码器以初始化深度神经网络的基本概念类似于堆叠多个限制玻尔兹曼机来构建深度置信网络或传统深度自编码器。生成损坏输入仅在每个单独层的初始去噪训练中需要，以帮助学习有用的特征提取。
- en: Once we know the encoding function *f* to reach the hidden state, it is used
    on the original, uncorrupted data to reach the next level. In general, no corruption
    or noise is put to generate the representation, which will act as an uncorrupted
    input for training the next layer. A key function of a stacked denoising autoencoder
    is its layer-by-layer unsupervised pre-training as the input is fed through. Once
    a layer is pre-trained to perform the feature selection and extraction on the
    input from the preceding layer, the next stages of supervised fine tuning can
    follow, just as in case of the traditional deep autoencoders.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道编码函数*f*来达到隐藏状态，它就可以应用于原始的、未损坏的数据，以到达下一层。通常情况下，不会施加任何损坏或噪声来生成表示，作为训练下一层的未损坏输入。堆叠去噪自编码器的一个关键功能是其逐层的无监督预训练，当输入被馈送通过时。一旦某一层经过预训练，能够对来自前一层的输入进行特征选择和提取，就可以进入下一个阶段的监督微调，类似于传统深度自编码器的情况。
- en: '*Figure 6.7* shows the detailed representation to design a stacked denoising
    autoencoder. The overall procedure for learning and stacking multiple layers of
    a denoising autoencoder is shown in the following figure:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.7*展示了设计堆叠去噪自编码器的详细表示。学习和堆叠多个去噪自编码器层的整体过程如下图所示：'
- en: '![Stacked denoising autoencoders](img/B05883_06_07-1.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠去噪自编码器](img/B05883_06_07-1.jpg)'
- en: 'Figure 6.7: The representation of a stacked denoising autoencoder'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：堆叠去噪自编码器的表示
- en: Implementation of a stacked denoising autoencoder using Deeplearning4j
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Deeplearning4j实现堆叠去噪自编码器
- en: Stacked denoising autoencoders can be built using Deeplearning4j by creating
    a `MultiLayerNetwork` that possesses autoencoders as its hidden layers. The autoencoders
    have some `corruptionLevel`, which is denoted as noise.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用Deeplearning4j创建一个包含自编码器作为隐藏层的`MultiLayerNetwork`来构建堆叠去噪自编码器。自编码器具有一定的`corruptionLevel`，即噪声。
- en: Here we set the initial configuration needed to set up the model. For illustration
    purposes, a `batchSize` of `1024` numbers of examples is taken. The input number
    and output number is taken as `1000` and `2` respectively.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了建立模型所需的初始配置。为了演示的目的，选择了`batchSize`为`1024`个示例。输入数和输出数分别设置为`1000`和`2`。
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The loading of the input dataset is the same as explained in the deep autoencoder
    section. Therefore, we will directly jump to how to build the stack denoising
    autoencoder. We have taken a five-hidden-layer deep model to illustrate the method:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集的加载与在深度自编码器部分中解释的一样。因此，我们将直接跳到如何构建堆叠去噪自编码器。为了说明方法，我们采用了一个五层隐藏层的深度模型：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following code denotes how much input data is to be corrupted:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码表示输入数据需要被破坏的程度：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once the model is built, it is trained by calling the `fit()` method:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型建立完成，就通过调用`fit()`方法进行训练：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Applications of autoencoders
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器的应用
- en: 'Autoencoders can be successfully applied in many use cases, and hence, have
    gained much popularity in the world of deep learning. In this section, we will
    discuss the important applications and uses of autoencoders:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以成功应用于许多场景，因此，在深度学习领域获得了广泛的关注。在本节中，我们将讨论自编码器的重要应用和用途：
- en: '**Dimensionality reduction**: If you remember, in [Chapter 1](ch01.html "Chapter 1. Introduction
    to Deep Learning"), *Introduction to Deep Learning*, we introduced the concept
    of the ''curse of dimensionality''. Dimensionality reduction was one of the first
    applications of deep learning. Autoencoders were initially studied to overcome
    the issues with the curse of dimensionality. We have already got a fair idea from
    this chapter how deep autoencoders work on higher-dimensional data to reduce the
    dimensionality in the final output.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维度降维**：如果你记得，在[第1章](ch01.html "第1章. 深度学习简介")《深度学习简介》中，我们介绍了“维度灾难”这一概念。维度降维是深度学习的最初应用之一。自编码器最初的研究就是为了克服维度灾难的问题。从本章内容，我们已经对深度自编码器如何在高维数据上工作并在最终输出中减少维度有了大致的了解。'
- en: '**Information Retrieval**: One more important application of autoencoders is
    in information retrieval. Information retrieval basically means to search for
    some entries, which match with an entered query, in a database. Searching in high-dimensional
    data is generally a cumbersome task; however, with reduced dimensionality of a
    dataset, the search can become extremely efficient in certain kinds of lower dimensional
    data. The dimensionality reduction obtained from the autoencoder can generate
    codes that are low dimensional and binary in nature. These can be stored in a
    key values stored data structure, where keys are binary code vectors and values
    are the corresponding entries. Such key value stores help us to perform information
    retrieval by returning all the database entries that match some binary code with
    the query. This approach to retrieving information through dimensionality reduction
    and binary code is called semantic hashing [135].'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索**：自编码器的另一个重要应用是在信息检索中。信息检索基本上是指在数据库中搜索与输入查询匹配的条目。在高维数据中进行搜索通常是一项繁琐的任务；然而，通过降低数据集的维度，某些低维数据的搜索可以变得非常高效。自编码器所获得的维度降维可以生成低维且具有二进制性质的编码。这些编码可以存储在键值存储数据结构中，其中键是二进制代码向量，值是相应的条目。这样的键值存储帮助我们通过返回与查询匹配的二进制代码的所有数据库条目来执行信息检索。通过降维和二进制编码来检索信息的这种方法被称为语义哈希[135]。'
- en: '**Image Search**: As explained in the deep autoencoder section, deep autoencoders
    are capable of compressing image datasets of higher dimensions to a very small
    number of vectors, say 30\. Therefore, this has made image searching easier for
    high-dimensional images. Once an image is uploaded, the search engine will compress
    it into small vectors, and then compare that vector to all the others in its index.
    For a search query, the vectors that contain similar numbers will be returned
    and translated into the mapped image.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像搜索**：正如在深度自编码器部分所解释的，深度自编码器能够将高维图像数据集压缩成非常小的向量，例如30个。因此，这使得对高维图像的图像搜索变得更加容易。一旦上传图像，搜索引擎会将其压缩成小向量，然后将该向量与索引中的其他所有向量进行比较。对于搜索查询，将返回包含相似数字的向量，并转换为映射的图像。'
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Autoencoders, one of the most popular and widely applicable generative models,
    have been discussed in this chapter. Autoencoders basically help two phases: one
    is the encoder phase and the other is the decoder phase. In this chapter, we elaborated
    on both of these phases with suitable mathematical explanations. Going forward,
    we explained a special kind of autoencoder called the sparse autoencoder. We also
    discussed how autoencoders can be used in the world of deep neural networks by
    explaining deep autoencoders. Deep autoencoders consist of layers of Restricted
    Boltzmann machines, which take part in the encoder and decoder phases of the network.
    We explained how to deploy deep autoencoders using Deeplearning4j, by loading
    chunks of the input dataset into a Hadoop Distributed File System. Later in this
    chapter, we introduced the most popular form of autoencoder called the denoising
    autoencoder and its deep network version known as the stacked denoising autoencoder.
    The implementation of a stacked denoising autoencoder using Deeplearning4j was
    also shown. We concluded this chapter by outlining the common applications of
    autoencoders.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是最受欢迎且广泛应用的生成模型之一，本章对此进行了讨论。自编码器基本上帮助完成两个阶段：一个是编码阶段，另一个是解码阶段。在本章中，我们对这两个阶段进行了详细的数学解释。接下来，我们介绍了一种特殊类型的自编码器，称为稀疏自编码器。我们还讨论了自编码器如何在深度神经网络的世界中使用，通过解释深度自编码器来说明。深度自编码器由限制玻尔兹曼机的层组成，它们参与了网络的编码和解码阶段。我们解释了如何使用Deeplearning4j部署深度自编码器，通过将输入数据集的块加载到Hadoop分布式文件系统中。本章后面，我们介绍了最流行的自编码器形式——去噪自编码器及其深度网络版本，称为堆叠去噪自编码器。还展示了如何使用Deeplearning4j实现堆叠去噪自编码器。我们通过概述自编码器的常见应用来总结本章内容。
- en: In the next chapter, we will discuss some common useful applications of deep
    learning with the help of Hadoop.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将借助Hadoop讨论一些常见的深度学习实用应用。
