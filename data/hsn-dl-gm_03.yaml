- en: Convolutional and Recurrent Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积与递归网络
- en: The human brain is often the main inspiration and comparison we make when building
    AI and is something deep learning researchers often look to for inspiration or
    reassurance. By studying the brain and its parts in more detail, we often discover
    neural sub-processes. An example of a neural sub-process would be our visual cortex,
    the area or region of our brain responsible for vision. We now understand that
    this area of our brain is wired differently and responds differently to input.
    This just so happens to be analogous to analog what we have found in our previous
    attempts at using neural networks to classify images. Now, the human brain has
    many sub-processes all with specific mapped areas in the brain (sight, hearing,
    smell, speech, taste, touch, and memory/temporal), but in this chapter, we will
    look at how we model just sight and memory by using advanced forms of deep learning
    called **convolutional and recurrent networks**. The two-core sub-processes of
    sight and memory are used extensively by us for many tasks including gaming and
    form the focus of research of many deep learners.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑通常是我们在构建 AI 时的主要灵感来源和比较对象，深度学习研究人员经常从大脑中寻找灵感或获得确认。通过更详细地研究大脑及其各个部分，我们经常发现神经子过程。一个神经子过程的例子是我们的视觉皮层，这是大脑中负责视觉的区域或部分。我们现在了解到，大脑的这个区域的连接方式与反应输入的方式是不同的。这正好与我们之前在使用神经网络进行图像分类时的发现相似。现在，人脑有许多子过程，每个子过程在大脑中都有特定的映射区域（视觉、听觉、嗅觉、语言、味觉、触觉以及记忆/时间性），但在本章中，我们将探讨如何通过使用高级形式的深度学习来模拟视觉和记忆，这些高级形式被称为**卷积神经网络和递归网络**。视觉和记忆这两个核心子过程在许多任务中被广泛应用，包括游戏，它们也成为了许多深度学习研究的重点。
- en: Researchers often look to the brain for inspiration, but the computer models
    they build often don't entirely resemble their biological counterpart. However,
    researchers have begun to identify almost perfect analogs to neural networks inside
    our brains. One example of this is the ReLU activation function. It was recently
    found that the excitement level in our brains' neurons, when plotted, perfectly
    matched a ReLU graph.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员常常从大脑中寻找灵感，但他们构建的计算机模型通常与生物大脑的对应结构并不完全相似。然而，研究人员已经开始识别出大脑内几乎完美对应于神经网络的类比。例如，ReLU
    激活函数就是其中之一。最近发现，我们大脑中神经元的兴奋程度绘制出来后，与 ReLU 图形完全匹配。
- en: 'In this chapter, we will explore, in some detail, convolutional and recurrent
    neural networks. We will look at how they solve the problem of replicating accurate
    vision and memory in deep learning. These two new network or layer types are a
    fairly recent discovery but have been responsible in part for many advances in
    deep learning. This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细探讨卷积神经网络和递归神经网络。我们将研究它们如何解决在深度学习中复制准确视觉和记忆的问题。这两种新的网络或层类型是相对较新的发现，但它们在某些方面促进了深度学习的诸多进展。本章将涵盖以下主题：
- en: Convolutional neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Understanding convolution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积
- en: Building a self-driving CNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自驾 CNN
- en: Memory and recurrent networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆和递归网络
- en: Playing rock, paper, scissors with LSTMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 LSTM 玩石头剪子布
- en: Be sure you understand the fundamentals outlined in the previous chapter reasonably
    well before proceeding. This includes running the code samples, which install
    this chapter's required dependencies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，确保你已经较好地理解了前一章中概述的基本内容。这包括运行代码示例，安装本章所需的依赖项。
- en: Convolutional neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Sight is hands-down the most-used sub-process. You are using it right now! Of
    course, it was something researchers attempted to mimic with neural networks early
    on, except that nothing really worked well until the concept of convolution was
    applied and used to classify images. The concept of convolution is the idea behind
    detecting, sometimes grouping, and isolating common features in an image. For
    instance, if you cover up 3/4 of a picture of a familiar object and show it to
    someone, they will almost certainly recognize the image by recognizing just the
    partial features. Convolution works the same way, by blowing up an image and then
    isolating the features for later recognition.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉是最常用的子过程。你现在就正在使用它！当然，研究人员早期尝试通过神经网络来模拟这一过程，然而直到引入卷积的概念并用于图像分类，才真正有效。卷积的概念是检测、分组和隔离图像中常见特征的想法。例如，如果你遮住了一张熟悉物体的图片的四分之三，然后展示给某人，他们几乎肯定能通过识别部分特征来认出这张图。卷积也以同样的方式工作，它会放大图像然后隔离特征，供之后识别使用。
- en: 'Convolution works by dissecting an image into its feature parts, which makes
    it easier to train a network. Let''s jump into a code sample that extends from
    where we left off in the previous chapter but that now introduces convolution.
    Open up the `Chapter_2_1.py` listing and follow these steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积通过将图像分解成其特征部分来工作，这使得训练网络变得更加容易。让我们进入一个代码示例，该示例基于上一章的内容，并且现在引入了卷积。打开`Chapter_2_1.py`文件并按照以下步骤操作：
- en: 'Take a look at the first couple of lines doing the import:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下导入部分的前几行：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we import new layer types: `Conv2D`, `MaxPooling2D`, and `UpSampling2D`.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例中，我们导入了新的层类型：`Conv2D`、`MaxPooling2D`和`UpSampling2D`。
- en: 'Then we set the `Input` and build up the encoded and decoded network sections
    using the following code:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们设置`Input`并使用以下代码构建编码和解码网络部分：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first thing to note is that we are now preserving the dimensions of the
    image, in this case, 28 x 28 pixels wide and 1 layer or channel. This example
    uses an image that is in grayscale, so there is only a single color channel. This
    is vastly different from before, when we just unraveled the image into a single
    784-dimension vector.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先需要注意的是，我们现在保持图像的维度，在这个例子中是28 x 28像素宽，且只有1层或1个通道。这个示例使用的是灰度图像，所以只有一个颜色通道。这与之前完全不同，以前我们只是将图像展开成一个784维的向量。
- en: The second thing to note is the use of the `Conv2D` layer or two-dimensional
    convolutional layer and the following `MaxPooling2D` or `UpSampling2D` layers.
    Pooling or sampling layers are used to gather or conversely unravel features.
    Note how we use pooling or down-sampling layers after convolution when the image
    is encoded and then up-sampling layers when decoding the image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点需要注意的是使用了`Conv2D`层（即二维卷积层）和随后的`MaxPooling2D`或`UpSampling2D`层。池化层或采样层用于收集或反过来解开特征。注意我们如何在图像编码后使用池化或下采样层，在解码图像时使用上采样层。
- en: 'Next, we build and train the model with the following block of code:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码块构建并训练模型：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The training of the model in the preceding code mirrors what we did at the end
    of the previous chapter, but note the selection of training and testing sets now.
    We no longer squish the image but rather preserve its spatial properties as inputs
    into the convolutional layer.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上面代码中的模型训练与上一章末尾我们所做的相似，但现在请注意训练集和测试集的选择。我们不再压缩图像，而是保持其空间属性作为卷积层的输入。
- en: 'Finally, we output the results with the following code:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过以下代码输出结果：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Run the code, as you have before, and you'll immediately notice that it is about
    100 times slower to train. This may or may not require you to wait, depending
    on your machine; if it does, go get a beverage or three and perhaps a meal.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你之前所做，运行代码，你会立刻注意到训练速度大约慢了100倍。这可能需要你等待，具体取决于你的机器；如果需要等待，去拿一杯饮料或三杯，或者来一顿饭吧。
- en: Training our simple sample now takes a large amount of time, which may be quite
    noticeable on older hardware. In the next section, we look at how we can start
    to monitor the training sessions, in great detail.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练我们的简单示例需要大量时间，这在旧硬件上可能会非常明显。在下一节中，我们将详细介绍如何开始监控训练过程。
- en: Monitoring training with TensorBoard
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard监控训练过程
- en: TensorBoard is essentially a mathematical graph or calculation engine that performs
    very well at crunching numbers, hence our use of it in deep learning. The tool
    itself is still quite immature, but there are some very useful features for monitoring
    training exercises.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 本质上是一个数学图形或计算引擎，在处理数字时表现非常出色，因此我们在深度学习中使用它。该工具本身仍然相当不成熟，但它具有一些非常有用的功能，可以用于监控训练过程。
- en: 'Follow these steps to start monitoring training on our sample:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤开始监控我们的样本训练：
- en: 'You can monitor the training session by entering the following command into
    a new **Anaconda** or command window from the same directory/folder that you are
    running the sample from:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过在与运行样本相同的目录/文件夹中，在新的 **Anaconda** 或命令窗口中输入以下命令来监控训练会话：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will launch a TensorBoard server, and you can view the output by navigating
    your browser to the URL in italics, as shown in the window you are running `TensorBoard`
    from. It will typically look something like the following:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动一个 TensorBoard 服务器，您可以通过将浏览器导航到以斜体显示的 URL 来查看输出，正如您运行 `TensorBoard` 所在的窗口中显示的那样。它通常会像下面这样：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note, the URL should use your machine name, but if that doesn't work, try the
    second form. Be sure to allow ports `6000`, and `6006` and/or the **TensorBoard**
    application through your firewall if prompted.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，URL 应使用您的机器名称，但如果无法工作，可以尝试第二种形式。如果提示，请确保允许端口 `6000` 和 `6006` 以及 **TensorBoard**
    应用程序通过您的防火墙。
- en: 'When the sample is done running, you should see the following:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当样本运行完毕时，您应该会看到以下内容：
- en: '![](img/d666bad9-02b7-43c3-ae41-bee3a36635e2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d666bad9-02b7-43c3-ae41-bee3a36635e2.png)'
- en: Auto-encoding digits using convolution
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积进行自动编码数字
- en: Go back and compare the results from this example and the last example from
    [Chapter 1](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml),* Deep Learning for Games*.
    Note the improvement in performance.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回并对比本示例和[第1章](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml)《*深度学习游戏*》中的最后一个示例的结果。请注意性能的提升。
- en: Your immediate thought may be, "*Is the increased training time we experienced
    worth the effort?*" After all, the decoded images look quite similar in the previous
    example, and it trained much faster, except, remember we are training the network
    weights slowly by adjusting each weight over each iteration, which we can then
    save as a model. That model or brain can then be used to perform the same task
    again later, without training. Works scarily enough! Keep this concept in mind
    as we work through this chapter. In [Chapter 3](cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml),
    *GAN for Games*, we will start saving and moving our brain models around.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会立即想到，“*我们经历的训练时间是否值得付出这么多努力？*”毕竟，在前一个示例中，解码后的图像看起来非常相似，而且训练速度要快得多，除了请记住，我们是通过在每次迭代中调整每个权重来缓慢训练网络权重，这些权重然后可以保存为模型。这个模型或“大脑”可以用来以后再次执行相同的任务，而无需重新训练。效果惊人地有效！在我们学习本章时，请始终牢记这个概念。在[第3章](cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml)《*游戏中的GAN*》中，我们将开始保存并移动我们的“大脑”模型。
- en: In the next section, we take a more in-depth look at how convolution works.
    Convolution can be tricky to understand when you first encounter it, so take your
    time. It is important to understand how it works, as we will use it extensively
    later.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将更深入地探讨卷积的工作原理。当您第一次接触卷积时，它可能比较难以理解，所以请耐心些。理解它的工作原理非常重要，因为我们稍后会广泛使用它。
- en: Understanding convolution
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积
- en: '**Convolution** is a way of extracting features from an image that may allow
    us to more easily classify it based on known features. Before we get into convolution,
    let''s first take a step back and understand why networks, and our vision for
    that matter, need to isolate features in an image. Take a look at the following;
    it''s a sample image of a dog, called Sadie, with various image filters applied:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积** 是从图像中提取特征的一种方式，它可能使我们根据已知特征更容易地对其进行分类。在深入探讨卷积之前，让我们先退一步，理解一下为什么网络以及我们的视觉系统需要在图像中孤立出特征。请看下面的内容：这是一张名为
    Sadie 的狗的样本图像，应用了各种图像滤镜：'
- en: '![](img/fb6c51c0-04d3-40ec-9f20-5a342cefff6f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb6c51c0-04d3-40ec-9f20-5a342cefff6f.png)'
- en: Example of an image with different filters applied
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 应用不同滤镜的图像示例
- en: The preceding shows four different versions with no filter, edge detection,
    pixelate, and glowing edges filters applied. In all cases, though, you as a human
    can clearly recognize it is a picture of a dog, regardless of the filter applied,
    except note that in the edge detection case, we have eliminated the extra image
    data that is unnecessary to recognize a dog. By using a filter, we can extract
    just the required features our NN needs to recognize a dog. This is all a convolution
    filter does, and in some cases, one of those filters could be just a simple edge
    detection.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上面展示了四种不同的版本，分别应用了没有滤波器、边缘检测、像素化和发光边缘滤波器。然而，在所有情况下，作为人类的你都能清晰地识别出这是一张狗的图片，不论应用了什么滤波器，除了在边缘检测的情况下，我们去除了那些对于识别狗不必要的额外图像数据。通过使用滤波器，我们只提取了神经网络识别狗所需要的特征。这就是卷积滤波器的全部功能，在某些情况下，这些滤波器中的一个可能只是一个简单的边缘检测。
- en: 'A convolution filter is a matrix or kernel of numbers that defines a single
    math operation. The process starts by being multiplied by the upper-left corner
    pixel value, with the results of the matrix operation summed and set as the output.
    The kernel is slid across the image in a step size called a **stride**, and this
    operation is demonstrated:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积滤波器是一个由数字组成的矩阵或核，定义了一个单一的数学操作。这个过程从将其与左上角的像素值相乘开始，然后将矩阵操作的结果求和并作为输出。该核沿着图像滑动，步幅称为**步幅**，并演示了此操作：
- en: '![](img/114cee40-0fd4-4978-b46d-c8d5f8a5882d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/114cee40-0fd4-4978-b46d-c8d5f8a5882d.png)'
- en: Applying a convolution filter
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 应用卷积滤波器
- en: 'In the preceding diagram, a stride of 1 is being used. The filter being applied
    in the convolution operation is essentially an edge detection filter. If you look
    at the result of the final operation, you can see the middle section is now filled
    with OS, greatly simplifying any classification task. The less information our
    networks need to learn, the quicker they will learn and with less data. Now, the
    interesting part of this is that the convolution learns the filter, the numbers,or
    the weights it needs to apply in order to extract the relevant features. This
    is not so obvious and may be confusing, so let''s go over it again. Go back to
    our previous example and look at how we define the first convolution layer:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，使用了步幅为1的卷积。应用于卷积操作的滤波器实际上是一个边缘检测滤波器。如果你观察最终操作的结果，你会看到中间部分现在被填充了OS，这大大简化了任何分类任务。我们的网络需要学习的信息越少，学习速度越快，所需的数据也越少。现在，有趣的部分是，卷积学习滤波器、数字或权重，它需要应用这些权重以提取相关特征。这一点可能不太明显，可能会让人困惑，所以我们再来讲解一遍。回到我们之前的例子，看看我们如何定义第一个卷积层：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In that line of code, we define the first convolution layer as having `16` output
    filters, meaning our output from this layer is actually 16 filters. We then set
    the kernel size to `(3,3)`, which represents a `3x3` matrix , just as in our example.
    Note how we don't specify the values of the various kernel filter weights, as
    that is after all what the network is training to do.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在那行代码中，我们将第一个卷积层定义为具有`16`个输出滤波器，意味着这一层的输出实际上是16个滤波器。然后我们将核大小设置为`(3,3)`，这表示一个`3x3`矩阵，就像我们在例子中看到的那样。请注意，我们没有指定各种核滤波器的权重值，因为毕竟这就是网络正在训练去做的事情。
- en: 'Let''s see how this looks when everything is put together in the following
    diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当所有内容组合在一起时，这在以下图示中是怎样的：
- en: '![](img/7a13bb31-cf32-4ec2-838a-e066ef86f047.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a13bb31-cf32-4ec2-838a-e066ef86f047.png)'
- en: Full convolution operation
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的卷积操作
- en: The output from the first step in convolution is the feature map. One feature
    map represents a single convolution filter being applied and is generated by applying
    the learned filter/kernel. In our example, the first layer produces **16 kernels**,
    which in turn produce **16 feature maps**; remember that the value of `16` is
    for the number of filters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的第一步输出是特征图。一个特征图表示应用了单个卷积滤波器，并通过应用学习到的滤波器/核生成。在我们的例子中，第一层生成**16个核**，从而生成**16个特征图**；请记住，`16`是指滤波器的数量。
- en: 'After convolution, we then apply pooling or subsampling in order to collect
    or gather features into sets. This subsampling further creates new concentrated
    feature maps that highlight the image''s important features we are training for.
    Take a look back at how we defined the first pooling layer in our previous example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积后，我们应用池化或子采样操作，以便将特征收集或聚集到一起。这种子采样进一步创建了新的集中的特征图，突出显示了我们正在训练的图像中的重要特征。回顾一下我们在之前的例子中如何定义第一个池化层：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the code, we are subsampling using a `pool_size` of `(2,2)`. The size indicates
    the factor by which to down-sample the image by width and height. So a 2 x 2 pool
    size will create four feature maps at half the size in width and height. This
    results in a total of 64 feature maps after our first layer of convolution and
    pooling. We get this by multiplying 16 (convolution feature maps) x 4 (pooling
    feature maps) = 64 feature maps. Consider how many total feature maps we build
    in our simple example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们使用 `pool_size` 为 `(2,2)` 进行子采样。该大小表示图像在宽度和高度方向下采样的因子。所以一个 2 x 2 的池化大小将创建四个特征图，其宽度和高度各减半。这会导致我们的第一层卷积和池化后总共生成
    64 个特征图。我们通过将 16（卷积特征图）x 4（池化特征图） = 64 特征图来得到这个结果。考虑一下我们在这个简单示例中构建的特征图总数：
- en: '![](img/2061d9ed-16c1-475a-a519-bb47eff32601.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2061d9ed-16c1-475a-a519-bb47eff32601.png)'
- en: '![](img/d0f65917-9560-4f1b-8aae-c62ff29a9071.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0f65917-9560-4f1b-8aae-c62ff29a9071.png)'
- en: '![](img/56c4dfa5-f217-4d4a-8f46-e0cd5d904258.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56c4dfa5-f217-4d4a-8f46-e0cd5d904258.png)'
- en: That is 65,536 feature maps of 4 x 4 images. This means we now train our network
    on 65,536 smaller images; for each image, we attempt to encode or classify. This
    is obviously the cause for the increased training time, but also consider the
    amount of extra data we are now using to classify our images. Now our network
    is learning how to identify parts or features of our image, just as we humans
    identify objects.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是 65,536 个 4 x 4 图像的特征图。这意味着我们现在在 65,536 张更小的图像上训练我们的网络；对于每张图像，我们尝试对其进行编码或分类。这显然是训练时间增加的原因，但也要考虑到我们现在用于分类图像的额外数据量。现在，我们的网络正在学习如何识别图像的部分或特征，就像我们人类识别物体一样。
- en: For instance, if you were just shown the nose of a dog, you could likely recognize
    that as a dog. Consequently, our sample network now is identifying parts of the
    handwritten digits, which as we know now, dramatically improves performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你仅仅看到了狗的鼻子，你很可能就能认出那是一只狗。因此，我们的样本网络现在正在识别手写数字的各个部分，正如我们现在所知道的，这大大提高了性能。
- en: As we have seen, convolution works well for identifying images, but the process
    of pooling can have disruptive consequences to preserving spatial relationships.
    Therefore, when it comes to games or learning requiring some form of spatial understanding,
    we prefer to limit pooling or eliminate altogether. Since it is important to understand
    when to use and not to use pooling, we will cover that in more detail in the next
    section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，卷积非常适合识别图像，但池化过程可能会破坏空间关系的保持。因此，当涉及到需要某种形式的空间理解的游戏或学习时，我们倾向于限制池化或完全消除池化。由于理解何时使用池化以及何时不使用池化非常重要，我们将在下一节中详细讨论这一点。
- en: Building a self-driving CNN
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自驾车卷积神经网络（CNN）
- en: 'Nvidia created a multi-layer CNN called **PilotNet**, in 2017, that was able
    to steer a vehicle by just showing it a series of images or video. This was a
    compelling demonstration of the power of neural networks, and in particular the
    power of convolution. A diagram showing the neural architecture of PilotNet is
    shown here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 在 2017 年创建了一个名为**PilotNet**的多层卷积神经网络（CNN），它通过仅仅展示一系列图像或视频，就能控制车辆的方向。这是神经网络，特别是卷积网络强大功能的一个引人注目的演示。下图展示了
    PilotNet 的神经网络架构：
- en: '![](img/dc9e1eee-fc4c-4305-9b0e-489e93ff06d6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc9e1eee-fc4c-4305-9b0e-489e93ff06d6.png)'
- en: PilotNet neural architecture
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PilotNet 神经网络架构
- en: 'The diagram shows the input of the network moving up from the bottom where
    the results of a single input image output to a single neuron represent the steering
    direction. Since this is such a great example, several individuals have posted
    blog posts showing an example of PilotNet, and some actually work. We will examine
    the code from one of these blog posts to see how a similar architecture is constructed
    with Keras. Next is an image from the original PilotNet blog, showing a few of
    the types of images our self-driving network will use to train:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示了网络的输入从底部开始，上升到单个输入图像的结果输出到一个神经元，表示转向方向。由于这是一个很好的示例，许多人已在博客中发布了 PilotNet
    的示例，其中一些实际上是有效的。我们将查看这些博客中的一个代码示例，看看如何用 Keras 构建类似的架构。接下来是来自原始 PilotNet 博客的一张图，展示了我们的自驾网络将用于训练的一些图像类型：
- en: '![](img/4cf9c76e-f157-44bb-a455-14ffb5240090.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cf9c76e-f157-44bb-a455-14ffb5240090.png)'
- en: Example of PilotNet training images
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PilotNet 训练图像示例
- en: 'The goal of training in this example is to output the degree to which the steering
    wheel should be turned in order to keep the vehicle on the road. Open up the code
    listing in `Chapter_2_2.py` and follow these steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的训练目标是输出方向盘应该转动的角度，以保持车辆行驶在道路上。打开`Chapter_2_2.py`中的代码列表，并按照以下步骤操作：
- en: 'We will now switch to using Keras for a few samples. While the TensorFlow embedded
    version of Keras has served us well, there are a couple of features we need that
    are only found in the full version. To install Keras and other dependencies, open
    a shell or Anaconda window and run the following commands:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将转而使用Keras进行一些样本处理。虽然TensorFlow内嵌版的Keras一直表现良好，但有一些功能我们需要的仅在完整版Keras中才有。要安装Keras和其他依赖项，打开Shell或Anaconda窗口，并运行以下命令：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At the start of the code file (`Chapter_2_2.py`), we begin with some imports
    and load the sample data using the following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码文件（`Chapter_2_2.py`）的开始部分，我们首先进行一些导入操作，并使用以下代码加载示例数据：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code just does some imports and then downloads the sample driving frames
    from the author's source data. The original source of this blog was written in
    a notebook by **Roscoe's Notebooks** and can be found at [https://wroscoe.github.io/keras-lane-following-autopilot.html](https://wroscoe.github.io/keras-lane-following-autopilot.html).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码只是做一些导入操作，然后从作者的源数据中下载示例驾驶帧。这篇博客的原文是由**Roscoe's Notebooks**编写的，链接可以在[https://wroscoe.github.io/keras-lane-following-autopilot.html](https://wroscoe.github.io/keras-lane-following-autopilot.html)找到。
- en: '`pickle` is a decompression library that unpacks the data in datasets `X` and
    `Y` at the bottom of the previous listing.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`pickle`是一个解压库，用于解压前面列表底部数据集`X`和`Y`中的数据。'
- en: 'Then we shuffle the order of the frames around or essentially randomize the
    data. We often randomize data this way to make our training stronger. By randomizing
    the data order, the network needs to learn an absolute steering value for an image,
    rather than a possible relative or incremental value. The following code does
    this shuffle:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们会将帧的顺序打乱，或者说本质上是对数据进行随机化。我们通常这样随机化数据以增强训练效果。通过随机化数据顺序，网络需要学习图像的绝对转向值，而不是可能的相对或增量值。以下代码完成了这个打乱过程：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: All this code does is use `numpy` to randomly shuffle the image frames. Then
    it prints out the length of the first shuffled set `shuffled_X` so we can confirm
    the training data is not getting lost.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的作用仅仅是使用`numpy`随机打乱图像帧的顺序。然后它会输出第一个打乱数据集`shuffled_X`的长度，以便我们确认训练数据没有丢失。
- en: 'Next, we need to create a training and test set of data. The training set is
    used to train the network (weights), and the test, or validation, set is used
    to confirm the accuracy on new or raw data. As we have seen before, this is a
    common theme when using supervised training or labeled data. We often break the
    data into 80% training and 20% test. The following code is what does this:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建训练集和测试集数据。训练集用于训练网络（权重），而测试集或验证集用于验证在新数据或原始数据上的准确性。正如我们之前所看到的，这在使用监督式训练或标注数据时是一个常见的主题。我们通常将数据划分为80%的训练数据和20%的测试数据。以下代码执行了这一操作：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After creating the training and test sets, we now want to augment or expand
    the training data. In this particular case, the author augmented the data just
    by flipping the original images and adding those to the dataset. There are many
    other ways of augmenting data that we will discover in later chapters, but this
    simple and effective method of flipping is something to add to your belt of machine
    learning tools. The code to do this flip is shown here:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建了训练集和测试集后，我们现在想要增强或扩展训练数据。在这个特定的案例中，作者通过翻转原始图像并将其添加到数据集中来增强数据。我们将在后续章节中发现许多其他增强数据的方法，但这种简单有效的翻转方法是你可以加入到机器学习工具库中的一个技巧。执行这个翻转的代码如下：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now comes the heavy lifting part. The data is prepped, and it is time to build
    the model as shown in the code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在进入了重要部分。数据已经准备好，现在是构建模型的时候，如下代码所示：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code to build the model at this point should be fairly self-explanatory.
    Take note of the variation in the architecture and how the code is written from
    our previous examples. Also note the two highlighted lines. The first one uses
    a new layer type called `Flatten`. All this layer type does is flatten the 2 x
    2 image into a vector that is then input into a standard `Dense` hidden fully
    connected layer. The second highlighted line introduces another new layer type
    called `Dropout`. This layer type needs a bit more explanation and will be covered
    in more detail at the end of this section.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前构建模型的代码应该比较容易理解。注意架构的变化以及代码是如何与我们之前的示例不同的。还要注意两个高亮的行。第一行使用了一种新的层类型，叫做`Flatten`。这个层的作用就是将2
    x 2的图像展平为一个向量，然后输入到一个标准的`Dense`全连接隐藏层。第二行高亮的代码引入了另一种新的层类型，叫做`Dropout`。这个层类型需要更多的解释，将在本节末尾进行更详细的讲解。
- en: 'Finally comes the training part, which this code sets up:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后是训练部分，这段代码进行如下设置：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This last piece of code sets up a set of `callbacks` to update and control the
    training. We have already used callbacks to update the TensorBoard server with
    logs. In this case, we use the callbacks to resave the model after every checkpoint
    (epoch) and check for an early exit. Note the form in which we are saving the
    model – an `hdf5` file. This file format represents a hierarchical data structure.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码设置了一组`callbacks`来更新和控制训练。我们已经使用过callbacks来更新TensorBoard服务器的日志。在这种情况下，我们使用callbacks在每个检查点（epoch）后重新保存模型并检查是否提前退出。请注意我们保存模型的形式——一个`hdf5`文件。这个文件格式表示的是一种层次化的数据结构。
- en: Run the code as you have already been doing. This sample can take a while, so
    again be patient. When you are done, there will be no output, but pay special
    attention to the minimized loss value.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像你之前一样运行代码。这个示例可能需要一些时间，因此再次请保持耐心。当你完成后，将不会有输出，但请特别注意最小化的损失值。
- en: At this point in your deep learning career, you may be realizing that you need
    much more patience or a better computer or perhaps a TensorFlow-supported GPU.
    If you want to try the latter, feel free to download and install the TensorFlow
    GPU library and the other required libraries for your OS, as this will vary. Plenty
    of documentation can be found online. After you have the GPU version of TensorFlow
    installed, Keras will automatically try to use that. If you have a supported GPU,
    you should notice a performance increase, and if not, then consider buying one.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在你深度学习的这段职业生涯中，你可能意识到你需要更多的耐心，或者更好的电脑，或者也许是一个支持TensorFlow的GPU。如果你想尝试后者，可以下载并安装TensorFlow
    GPU库以及与你的操作系统相对应的其他必需库，这些会有所不同。网上可以找到大量文档。在安装了TensorFlow的GPU版本后，Keras将自动尝试使用它。如果你有支持的GPU，你应该会注意到性能的提升，如果没有，考虑购买一个。
- en: While there is no output for this example, in order to keep it simple, try to
    appreciate what is happening. After all, this could just as easily be set up as
    a driving game, where the network drives the vehicle by just looking at screenshots.
    We have omitted the results from the author's original blog post, but if you want
    to see how this performs further, go back and check out the [source link](https://wroscoe.github.io/keras-lane-following-autopilot.html).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个示例没有输出，为了简化，试着理解正在发生的事情。毕竟，这同样可以设置为一款驾驶游戏，网络仅通过查看截图来控制车辆。我们省略了作者原始博客文章中的结果，但如果你想进一步查看其表现，请返回并查看[源链接](https://wroscoe.github.io/keras-lane-following-autopilot.html)。
- en: One thing the author did in his blog post was to use pooling layers, which,
    as we have seen, is quite standard when working with convolution. However, when
    and how to use pooling layers is a bit contentious right now and requires further
    detailed discussion, which is provided in the next section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在他的博客文章中做的一件事是使用了池化层，正如我们所见，当处理卷积时，这是相当标准的做法。然而，池化层的使用时机和方式现在有些争议，需要进一步详细讨论，这将在下一节中提供。
- en: Spatial convolution and pooling
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 空间卷积和池化
- en: Geoffrey Hinton and his team have recently strongly suggested that using pooling
    with convolution removes spatial relationships in the image. Hinton instead suggests
    the use of **CapsNet**, or **Capsule Networks**. Capsule Networks are a method
    of pooling that preserves the spatial integrity of the data. Now, this may not
    be a problem in all cases. For handwritten digits, spatial relationships don't
    matter that much. However, self-driving cars or networks tasked with spatial tasks,
    a prime example of which is games, often don't perform as well when using pooling.
    In fact, the team at Unity do not use pooling layers after convolution; let's
    understand why.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey Hinton及其团队最近强烈建议，使用池化与卷积会去除图像中的空间关系。Hinton则建议使用**CapsNet**，或称为**胶囊网络**。胶囊网络是一种保留数据空间完整性的池化方法。现在，这并非在所有情况下都是问题。对于手写数字，空间关系并不那么重要。然而，对于自动驾驶汽车或从事空间任务的网络——比如游戏——使用池化时，性能往往不如预期。事实上，Unity团队在卷积后并不使用池化层；让我们来了解原因。
- en: 'Pooling or down-sampling is a way of augmenting data by collecting its common
    features together. The problem with this is that any relationship in the data
    often gets lost entirely. The following diagram demonstrates **MaxPooling(2,2)**
    over a convolution map:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 池化或下采样是通过将数据的共同特征聚集在一起的方式来增强数据。这样做的问题是，数据中的任何关系通常会完全丢失。下图演示了在卷积图上进行**MaxPooling(2,2)**的效果：
- en: '![](img/1fa08f01-0b68-450b-9f82-d8cca463a995.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fa08f01-0b68-450b-9f82-d8cca463a995.png)'
- en: Max pooling at work
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化的工作原理
- en: Even in the simple preceding diagram, you can quickly appreciate that pooling
    loses the spatial relationship of the corner (upper-left, bottom-left, lower-right
    and upper-right) the pooled value started in. Note that, after a couple layers
    of pooling, any sense of spatial relation will be completely gone.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是在简单的前图中，你也能迅速理解池化操作会丢失角落（上左、下左、下右和上右）的空间关系。需要注意的是，经过几层池化后，任何空间关系将完全消失。
- en: 'We can test the effect of removing pooling layers from the model and test this
    again by following these steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤测试从模型中移除池化层的效果，并再次进行测试：
- en: 'Open the `Chapter_2_3.py` file and note how we commented out a couple of pooling
    layers, or you can just delete the lines as well, like so:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_2_3.py`文件，并注意我们注释掉了几个池化层，或者你也可以像下面这样删除这些行：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note how we didn't comment out (or delete) all the pooling layers and left one
    in. In some cases, you may still want to leave a couple of pooling layers in,
    perhaps to identify features that are not spatially important. For example, when
    recognizing digits, space is less important with respect to the overall shape.
    However, if we consider recognizing a face, then the distance between a person's
    eyes, mouth, and so on, is what distinguishes a face from another face. However,
    if you just wanted to identify a face, with eyes, mouth, and so on, then just
    applying pooling could be quite acceptable.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们没有注释掉（或删除）所有的池化层，而是保留了一个。在某些情况下，你可能仍然希望保留一些池化层，可能是为了识别那些空间上不重要的特征。例如，在识别数字时，空间关系对于整体形状的影响较小。然而，如果我们考虑识别面孔，那么人的眼睛、嘴巴等之间的距离，就是区分面孔的关键特征。不过，如果你只想识别一个面孔，包含眼睛、嘴巴等，单纯使用池化层也完全可以接受。
- en: 'Next, we also increase the dropout rate on our `Dropout` layer like so:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们还会在`Dropout`层上增加丢弃率，代码如下：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will explore dropout in some detail in the next section. For now, though,
    just realize that this change will have a more positive effect on our model.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在下一节中详细探讨丢弃层。现在，只需明白这个更改将对我们的模型产生更积极的影响。
- en: 'Lastly, we bump up the number of epochs to `10` with the following code:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将`epochs`的数量增加到`10`，代码如下：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In our previous run, if you were watching the loss rate when training, you would
    realize the last example more or less started to converge at four epochs. Since
    dropping the pooling layers also reduces the training data, we need to also bump
    up the number of epochs. Remember, pooling or down-sampling increases the number
    of feature maps, and fewer maps means the network needs more training runs. If
    you are not training on a GPU, this model will take a while, so be patient.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们之前的运行中，如果你在训练时观察损失率，你会发现最后一个例子大约在四个epoch时开始收敛。由于去掉了池化层也减少了训练数据，我们还需要增加epoch的数量。记住，池化或下采样增加了特征图的数量，特征图更少意味着网络需要更多的训练轮次。如果你不是在GPU上训练，这个模型将需要一段时间，所以请耐心等待。
- en: Finally, run the example, again with those minor changes. One of the first things
    you will notice is that the training time shoots up dramatically. Remember, this
    is because our pooling layers do facilitate quicker training, but at a cost. This
    is one of the reasons we allow for a single pooling layer.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，再次运行这个示例，应用那些小的修改。你会注意到的第一件事是训练时间剧烈增加。记住，这是因为我们的池化层确实加速了训练，但代价也不小。这也是我们允许只有单个池化层的原因之一。
- en: When the sample is finished running, compare the results for the `Chapter_2_2.py`
    sample we ran earlier. Did it do what you expected it to?
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当示例运行完毕后，比较一下我们之前运行的`Chapter_2_2.py`示例的结果。它达到了你预期的效果吗？
- en: We only focus on this particular blog post because it is extremely well presented
    and well written. The author obviously knew his stuff, but this example just shows
    how important it is to understand the fundamentals of these concepts in as much
    detail as you can handle. This is not such an easy task with the flood of information,
    but this also reinforces the fact that developing working deep learning models
    is not a trivial task, at least not yet.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以专注于这篇博客文章，是因为它展示得非常好，写得也很出色。作者显然非常懂行，但这个示例也展示了在尽可能详细的情况下理解这些概念基础的重要性。面对信息的泛滥，这不是一件容易的事，但这也再次强调了开发有效的深度学习模型并非一项简单的任务，至少目前还不是。
- en: Now that we understand the cost/penalty of pooling layers, we can move on to the
    next section, where we jump back to understanding `Dropout`. It is an excellent
    tool you will use over and over again.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了池化层的成本/惩罚，我们可以进入下一部分，回到理解`Dropout`的内容。它是一个非常有效的工具，你将一次又一次地使用它。
- en: The need for Dropout
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout的必要性
- en: 'Now, let''s go back to our much-needed discussion about `Dropout`. We use dropout
    in deep learning as a way of randomly cutting network connections between layers
    during each iteration. An example showing an iteration of dropout being applied
    to three network layers is shown in the following diagram:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们非常需要讨论的`Dropout`。在深度学习中，我们使用Dropout作为在每次迭代过程中随机切断层之间网络连接的一种方式。下面的示意图展示了Dropout在三层网络中应用的一次迭代：
- en: '![](img/0588924a-6516-4bc0-8f6f-e217d96e8af1.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0588924a-6516-4bc0-8f6f-e217d96e8af1.png)'
- en: Before and after dropout
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout前后的变化
- en: The important thing to understand is that the same connections are not always
    cut. This is done to allow the network to become less specialized and more generalized.
    Generalizing a model is a common theme in deep learning, and we often do this
    so our models can learn a broader set of problems, more quickly. Of course, there
    may be times where generalizing a network limits a network's ability to learn.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的重要一点是，并非所有连接都会被切断。这样做是为了让网络变得不那么专注于特定任务，而是更加通用。使模型具备通用性是深度学习中的一个常见主题，我们通常这么做是为了让模型能更快地学习更广泛的问题。当然，有时将网络通用化也可能限制了网络的学习能力。
- en: 'If we go back to the previous sample now and look at the code, we see a `Dropout`
    layer being used like so:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在回到之前的示例，并查看代码，我们可以看到这样使用了`Dropout`层：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: That simple line of code tells the network to drop out or disconnect 50% of
    the connections randomly after every iteration. Dropout only works for fully connected
    layers (**Input** -> **Dense** -> **Dense**) but is very useful as a way of improving
    performance or accuracy. This may or may not account for some of the improved
    performance from the previous example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行简单的代码告诉网络在每次迭代后随机丢弃或断开50%的连接。Dropout仅对全连接层（**Input** -> **Dense** -> **Dense**）有效，但作为提高性能或准确性的一种方式非常有用。这可能在某种程度上解释了之前示例中性能提升的原因。
- en: In the next section, we will look at how deep learning mimics the memory sub-process
    or temporal scent.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将探讨深度学习如何模仿记忆子过程或时间感知。
- en: Memory and recurrent networks
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆和递归网络
- en: 'Memory is often associated with **Recurrent Neural Network** (**RNN**), but
    that is not entirely an accurate association. An RNN is really only useful for
    storing a sequence of events or what you may refer to as a **temporal sense**,
    a sense of time if you will. RNNs do this by persisting state back onto itself
    in a recursive or recurrent loop. An example of how this looks is shown here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆通常与**递归神经网络**（**RNN**）相关联，但这并不完全准确。RNN实际上只是用来存储事件序列或你可能称之为**时间感知**的东西，如果你愿意的话，它是“时间的感觉”。RNN通过在递归或循环中将状态保存回自身来实现这一点。下面是这种方式的一个示例：
- en: '![](img/e37e1983-cc7b-4826-818a-aa84968820fb.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e37e1983-cc7b-4826-818a-aa84968820fb.png)'
- en: Unfolded recurrent neural network
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 展开式递归神经网络
- en: What the diagram shows is the internal representation of a recurrent neuron
    that is set to track a number of time steps or iterations where **x** represents
    the input at a time step and **h** denotes the state. The network weights of **W**,
    **U**, and **V** remain the same for all time steps and are trained using a technique
    called **Backpropagation Through Time** (**BPTT**). We won't go into the math
    of BPTT and leave that up the reader to discover on their own, but just realize
    that the network weights in a recurrent network use a cost gradient method to
    optimize them.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了一个循环神经元的内部表示，该神经元被设置为跟踪若干时间步或迭代，其中**x**表示某一时间步的输入，**h**表示状态。**W**、**U**和**V**的网络权重在所有时间步中保持不变，并使用一种叫做**时间反向传播（BPTT）**的技术进行训练。我们不会深入讨论BPTT的数学原理，留给读者自己去发现，但要明白，循环神经网络中的网络权重使用一种成本梯度方法来进行优化。
- en: A recurrent network allows a neural network to identify sequences of elements
    and predict what elements typically come next. This has huge applications in predicting
    text, stocks, and of course games. Pretty much any activity that can benefit from
    some grasp of time or sequence of events will benefit from using RNN, except standard
    RNN, the type shown previously, which fails to predict longer sequences due to
    a problem with gradients. We will get further into this problem and the solution
    in the next section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）允许神经网络识别元素序列并预测通常接下来会出现的元素。这在预测文本、股票和当然是游戏中有巨大的应用。几乎任何能够从对时间或事件序列的理解中受益的活动，都可以通过使用RNN来获益，除了标准的RNN，前面展示的类型，由于梯度问题，无法预测更长的序列。我们将在下一节中进一步探讨这个问题及其解决方案。
- en: Vanishing and exploding gradients rescued by LSTM
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM拯救了梯度消失和爆炸问题
- en: 'The problem the RNN suffers from is either vanishing or exploding gradients.
    This happens because, over time, the gradient we try to minimize or reduce becomes
    so small or big that any additional training has no effect. This limits the usefulness
    of the RNN, but fortunately this problem was corrected with **Long Short-Term
    Memory*** (***LSTM**) blocks, as shown in this diagram:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: RNN所面临的问题是梯度消失或爆炸。这是因为，随着时间的推移，我们尝试最小化或减少的梯度变得非常小或非常大，以至于任何额外的训练都不会产生影响。这限制了RNN的实用性，但幸运的是，这个问题已经通过**长短期记忆（LSTM）**块得到解决，如下图所示：
- en: '![](img/c2cfd7cf-6e48-4993-8751-2d580fc8901f.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2cfd7cf-6e48-4993-8751-2d580fc8901f.png)'
- en: Example of an LSTM block
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM块示例
- en: LSTM blocks overcome the vanishing gradient problem using a few techniques.
    Internally, in the diagram where you see a **x** inside a circle, it denotes a
    gate controlled by an activation function. In the diagram, the activation functions
    are **σ** and **tanh**. These activation functions work much like a step or ReLU
    do, and we may use either function for activation in a regular network layer.
    For the most part, we will treat an LSTM as a black box, and all you need to remember
    is that LSTMs overcome the gradient problem of RNN and can remember long-term
    sequences.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM块使用一些技术克服了梯度消失问题。在图示中，您会看到一个圈内有一个**x**，它表示由激活函数控制的门控。在图示中，激活函数是**σ**和**tanh**。这些激活函数的工作方式类似于步长函数或ReLU，我们可能会在常规网络层中使用任一函数作为激活。大多数情况下，我们会将LSTM视为一个黑箱，您只需要记住，LSTM克服了RNN的梯度问题，并能够记住长期序列。
- en: 'Let''s take a look at a working example to see how this comes together. Open
    up `Chapter_2_4.py` and follow the these steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个实际的例子，看看如何将这些内容组合在一起。打开`Chapter_2_4.py`并按照以下步骤操作：
- en: 'We begin as per usual by importing the various Keras pieces we need, as shown:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像往常一样，首先导入我们需要的各种Keras组件，如下所示：
- en: This example was pulled from [https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/).
    This is a site hosted by **Dr. Jason Brownlee**, who has plenty more excellent
    examples explaining the use of LSTM and recurrent networks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子取自[https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)。这个网站由**Jason
    Brownlee博士**主办，他有许多出色的例子，解释了LSTM和循环神经网络的使用。
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This time we are importing two new classes, `Sequential` and `LSTM`. Of course
    we know what `LSTM` is for, but what about `Sequential`? `Sequential` is a form
    of model that defines the layers in a sequence one after another. We were less
    worried about this detail before, since our previous models were all sequential.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次我们引入了两个新的类，`Sequential` 和 `LSTM`。当然我们知道`LSTM`的作用，那`Sequential`呢？`Sequential`是一种模型形式，按顺序定义层级，一个接一个。我们之前对这个细节不太关注，因为我们之前的模型都是顺序的。
- en: 'Next, we set the random seed to a known value. We do this so that our example
    can replicate itself. You may have noticed in previous examples that not all runs
    perform the same. In many cases, we want our training to be consistent, and hence
    we set a known seed value by using this code:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将随机种子设置为一个已知值。这样做是为了使我们的示例能够自我复制。你可能在之前的示例中注意到，并非每次运行的结果都相同。在许多情况下，我们希望训练的一致性，因此我们通过以下代码设置一个已知的种子值：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It is important to realize that this just sets the `numpy` random seed value.
    Other libraries may use different random number generators and require different
    seed settings. We will try to identify these inconsistencies in the future when
    possible.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要意识到的是，这只是设置了`numpy`的随机种子值。其他库可能使用不同的随机数生成器，并需要不同的种子设置。我们将在未来尽可能地识别这些不一致之处。
- en: 'Next, we need to identify a sequence we will train to; in this case, we will
    just use the `alphabet` as shown in this code:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要确定一个训练的序列；在此示例中，我们将使用如下代码中的`alphabet`：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The preceding code builds our sequence of characters as integers and builds
    a map of each character sequence. It builds a `seq_in` and `seq_out` showing the
    forward and reverse positions. Since the length of a sequence is defined by `seq_length
    = 1`, then we are only concerned about a letter of the alphabet and the character
    that comes after it. You could, of course, do longer sequences.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码构建了我们的字符序列，并构建了每个字符序列的映射。它构建了`seq_in`和`seq_out`，展示了正向和反向的位置。由于序列长度由`seq_length
    = 1`定义，因此我们只关心字母表中的一个字母及其后面的字符。当然，你也可以使用更长的序列。
- en: 'With the sequence data built, it is time to shape the data and normalize it
    with this code:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建好序列数据后，接下来是使用以下代码对数据进行形状调整和归一化：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first line in the preceding code reshapes the data into a tensor with a
    size length of `dataX`, the number of steps or sequences, and the number of features
    to identify. We then normalize the data. Normalizing the data comes in many forms,
    but in this case we are normalizing values from 0 to 1\. Then we one hot encode
    the output for easier training.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码的第一行将数据重塑为一个张量，其大小长度为`dataX`，即步骤数或序列数，以及要识别的特征数。然后我们对数据进行归一化。归一化数据的方式有很多种，但在此我们将值归一化到0到1之间。接着，我们对输出进行独热编码，以便于训练。
- en: 'One hot encoding is where we you set the value to 1 where you have data or
    a response, and to zero everywhere else. In the example, our model output is 26
    neurons, which could also be represented by 26 zeros, one zero for each neuron,
    like so:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是将数据或响应的位置值设置为1，其它位置设置为0。在此示例中，我们的模型输出是26个神经元，它也可以用26个零表示，每个神经元对应一个零，像这样：
- en: '**00000000000000000000000000**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**00000000000000000000000000**'
- en: 'Where each zero represents the matching character position in the alphabet.
    If we wanted to denote a character **A**, we would output the one hot encoded
    value as this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个零代表字母表中匹配的字符位置。如果我们想表示字符**A**，我们会输出如下的独热编码值：
- en: '**10000000000000000000000000**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**10000000000000000000000000**'
- en: 'Then we construct the model, using a slightly different form of code than we
    have seen before and as shown here:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们构建模型，使用与之前略有不同的代码形式，如下所示：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The critical piece to the preceding code is the highlighted line that shows
    the construction of the `LSTM` layer. We construct an `LSTM` layer by setting
    the number of units, in this case `32`, since our sequence is 26 characters long
    and we want our units disable by `2`. Then we set the `input_shape` to match the
    previous tensor, `X`, that we created to hold our training data. In this case,
    we are just setting the shape to match all the characters (26) and the sequence
    length, in this case `1`.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面代码中的关键部分是高亮显示的那一行，展示了`LSTM`层的构建。我们通过设置单元数来构建`LSTM`层，在这个例子中是`32`，因为我们的序列长度为26个字符，我们希望通过`2`来禁用单元。然后我们将`input_shape`设置为与之前创建的张量`X`相匹配，`X`用于保存我们的训练数据。在这种情况下，我们只是设置形状以匹配所有字符（26个）和序列长度，在这种情况下是`1`。
- en: 'Finally, we output the model with the following code:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们用以下代码输出模型：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Run the code as you normally would and examine the output. You will notice that
    the accuracy is around 80%. See whether you can improve the accuracy of the model
    for predicting the next sequence in the alphabet.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像平常一样运行代码并检查输出。你会注意到准确率大约为80%。看看你能否提高模型预测字母表下一个序列的准确率。
- en: 'This simple example demonstrated the basic use of an LSTM block for recognizing
    a simple sequence. In the next section, we look at a more complex example: using
    LSTM to play Rock, Paper, Scissors.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的示例展示了使用LSTM块识别简单序列的基本方法。在下一部分，我们将看一个更复杂的例子：使用LSTM来玩石头、剪刀、布。
- en: Playing Rock, Paper, Scissors with LSTMs
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM玩石头、剪刀、布
- en: Remembering sequences of data have huge applications in many areas, not the
    least of which includes gaming. Of course, producing a simple, clean example is
    another matter. Fortunately, examples abound on the internet and `Chapter_2_5.py`
    shows an example of using an LSTM to play Rock, Paper, Scissors.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，数据序列的记忆在许多领域有着广泛的应用，尤其是在游戏中。当然，制作一个简单、清晰的示例是另一回事。幸运的是，互联网上有很多示例，`Chapter_2_5.py`展示了一个使用LSTM来玩石头、剪刀、布的例子。
- en: 'Open up that sample file and follow these steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 打开那个示例文件并按照以下步骤进行操作：
- en: This example was pulled from [https://github.com/hjpulkki/RPS](https://github.com/hjpulkki/RPS),
    but the code needed to be tweaked in several places to get it to work for us.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例来自[https://github.com/hjpulkki/RPS](https://github.com/hjpulkki/RPS)，但是代码需要在多个地方进行调整才能适应我们的需求。
- en: 'Let''s start as we normally do with the imports. For this sample, be sure to
    have Keras installed as we did for the last set of exercises:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们像往常一样开始导入。在这个示例中，确保像上次练习那样安装Keras：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we set some constants as shown:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们设置一些常量，如下所示：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we build the model, this time with three LSTM layers, one for each element
    in our sequence (rock, paper and scissors), like so:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们构建模型，这次有三个LSTM层，分别对应于序列中的每个元素（石头、剪刀和布），如下所示：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we create a function to extract our data from the `data.txt` file. This
    file holds the sequences of training data using the following code:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个函数，从`data.txt`文件中提取数据。该文件使用以下代码保存了训练数据的序列：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this example, we are training each block of training through 100 epochs in
    the same order as they are in the file. A better method would be to train each
    training sequence in a random order.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将每个训练块通过100次epoch进行训练，顺序与文件中的顺序一致。更好的方法是以随机顺序训练每个训练序列。
- en: 'Then we create the model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建模型：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Train the data using a loop, with each iteration pulling a batch from the `data.txt`
    file:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用循环训练数据，每次迭代从`data.txt`文件中获取一个批次：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we evaluate the results with a validation sequence as shown in this
    code:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用验证序列评估结果，如以下代码所示：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Run the sample as you normally would. Check the results at the end and note
    how accurate the model gets at predicting the sequence.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像平常一样运行示例。查看最后的结果，并注意模型在预测序列时的准确性。
- en: Be sure to run through this simple example a few times and understand how the
    LSTM layers are set up. Pay special attention to the parameters and how they are
    set.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要多次运行这个简单示例，理解LSTM层是如何设置的。特别注意参数及其设置方式。
- en: That concludes our quick look at understanding how to use recurrent aka LSTM
    blocks for recognizing and predicting sequences of data. We will of course use
    this versatile layer type many more times throughout the course of this book.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们快速了解如何使用递归，也就是LSTM块，来识别和预测数据序列。我们当然会在本书的其他章节中多次使用这一多功能的层类型。
- en: In the final section of this chapter, we again showcase a number of exercises
    you are encouraged to undertake for your own benefit.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一部分，我们再次展示了一些练习，鼓励你们为自己的利益进行尝试。
- en: Exercises
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Complete the following exercises in your own time and to improve your own learning
    experience. Improving your understanding of the material will make you a more
    successful deep learner, and you will likely enjoy this book better as well:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下练习，以便在自己的时间里提高学习体验。加深你对材料的理解会让你成为一个更成功的深度学习者，也能让你更享受本书的内容：
- en: In the `Chapter_2_1.py` example, change the `Conv2D` layers to use a different
    filter size. Run the sample again, and see what effect this has on training performance
    and accuracy.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chapter_2_1.py`示例中，将`Conv2D`层的过滤器大小更改为不同的值。再次运行示例，看看这对训练性能和准确度有何影响。
- en: Comment out or delete a couple of the `MaxPooling` layers and corresponding
    `UpSampling` layers in the `Chapter_2_1.py` example. Remember, if you remove a
    pooling layer between layers 2 and 3, you likewise need to remove the up-sampling
    to remain consistent. Run the sample again, and see what effect this has on training
    time, accuracy, and performance.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注释掉或删除 `Chapter_2_1.py` 示例中的几个**MaxPooling**层和相应的**UpSampling**层。记住，如果你删除了第2层和第3层之间的池化层，你也需要删除上采样层以保持一致性。重新运行示例，观察这对训练时间、准确度和性能的影响。
- en: Alter the **Conv2D** layers in the `Chapter_2_2.py` example using a different
    filter size. See what effect this has on training.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `Chapter_2_2.py` 示例中的**Conv2D**层，使用不同的滤波器大小。观察这对训练的影响。
- en: Alter the **Conv2D** layers in the `Chapter_2_2.py` example by using a stride
    value of **2**. You may need to consult the **Keras** docs in order to do this.
    See what effect this has on training.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `Chapter_2_2.py` 示例中的**Conv2D**层，使用步幅值为**2**。你可能需要参考**Keras**文档来完成此操作。观察这对训练的影响。
- en: Alter the **MaxPooling** layers in the `Chapter_2_2.py` example by altering
    the pooling dimensions. See what effect this has on training.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `Chapter_2_2.py` 示例中的**MaxPooling**层，改变池化的维度。观察这对训练的影响。
- en: Remove all or comment out different **MaxPooling** layers used in the `Chapter_2_3.py`
    example. What happens if all the pooling layers are commented out? Do you need
    to increase the training epochs now?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除或注释掉 `Chapter_2_3.py` 示例中使用的所有**MaxPooling**层。如果所有池化层都被注释掉，会发生什么？现在需要增加训练周期数吗？
- en: Alter the use of **Dropout** in the various examples used throughout this chapter.
    This includes adding dropout. Test the effects of using different levels of dropout.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改本章中使用的各个示例中的**Dropout**使用方式。这包括添加 dropout。测试使用不同 dropout 比例的效果。
- en: Modify the sample in `Chapter_2_4.py` so that the model produces better accuracy.
    What do you need to do in order to improve training performance?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `Chapter_2_4.py` 示例，使模型提高准确率。你需要做什么来提高训练性能？
- en: Modify the sample in `Chapter_2_4.py` to predict more than one character in
    the sequence. If you need help, go back and review the original blog post for
    more information.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `Chapter_2_4.py` 示例，以便预测序列中的多个字符。如果需要帮助，请回顾原始博客文章，获取更多信息。
- en: What happens if you change the number of units that the three **LSTM** layers
    use in the `Chapter_2_5.py` example? What if you increase the value to 128, 32,
    or 16? Try these values to understand the effect they have.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你改变 `Chapter_2_5.py` 示例中三个**LSTM**层使用的单元数，会发生什么？如果将其增加到128、32或16会怎样？尝试这些值，了解它们的影响。
- en: Feel free to expand on these exercises on your own. Try to write a new example
    on your own as well, even if it is just a simple one. There really is no better
    way to learn to code than to write your own.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 可以自行扩展这些练习。尝试自己写一个新的示例，即使只是一个简单的例子。没有什么比写代码更能帮助你学习编程了。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: For this chapter and the last, we took a deep dive into the core elements of
    deep learning and neural networks. While our review in the last couple chapters
    was not extensive, it should give you a good base for continuing through the rest
    of the book. If you had troubles with any of the material in the first two chapters,
    turn back now and spend more time reviewing the previous material. It is important
    that you understand the basics of neural network architecture and the use of various
    specialized layers, as we covered in this chapter (CNN and RNN). Be sure you understand
    the basics of CNN and how to use it effectively in picking features and what the
    trade—offs are when using pooling or sub sampling. Also understand the concept
    of RNN and how and when to use LSTM blocks for predicting or detecting temporal
    events. Convolutional layers and LSTM blocks are now fundamental components of
    deep learning, and we will use them in several networks we build going forward.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章以及上一章中，我们深入探讨了深度学习和神经网络的核心元素。尽管我们在过去几章中的回顾不够全面，但它应该为你继续阅读本书的其他部分打下良好的基础。如果你在前两章的任何内容上遇到困难，现在就回去复习这些内容，花更多时间进行复习。理解神经网络架构的基本概念和各种专用层的使用非常重要，如本章所讨论的（CNN和RNN）。确保你理解CNN的基础知识以及如何有效地使用它来选择特征，并了解使用池化或子采样时的权衡。同时，理解RNN的概念，以及在预测或检测时何时使用LSTM块。卷积层和LSTM块现在是深度学习的基础组件，接下来我们将在构建的多个网络中使用它们。
- en: In the next chapter, we start to build out our sample game for this book and
    introduce GANs, or generative adversarial networks. We will explore GANs and how
    they can be used to generate game content.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始为本书构建我们的示例游戏，并介绍GANs，即生成对抗网络。我们将探讨GANs以及它们如何用于生成游戏内容。
