- en: Optimization for Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的优化
- en: A number of applications in deep learning require optimization problems to be
    solved. Optimization refers to bringing whatever we are dealing with towards its
    ultimate state. The problem solved through the use of an optimization process
    must be supplied with data, providing model constants and parameters in functions,
    describing the overall objective function along with some constraints.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的许多应用需要解决优化问题。优化是指将我们所处理的对象带向其最终状态。通过优化过程解决的问题必须提供数据，提供模型常量和参数，函数中描述总体目标函数及一些约束条件。
- en: 'In this chapter, we will look at the TensorFlow pipeline and various optimization
    models provided by the TensorFlow library. The list of topics covered are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 TensorFlow 流水线和 TensorFlow 库提供的各种优化模型。涵盖的主题列表如下：
- en: Optimization basics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化基础
- en: Types of optimizers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器的类型
- en: Gradient descent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Choosing the correct optimizer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的优化器
- en: What is optimization?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是优化？
- en: The process to find maxima or minima is based on constraints. The choice of
    optimization algorithm for your deep learning model can mean the difference between
    good results in minutes, hours, and days.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找极大值或极小值的过程基于约束条件。选择优化算法对深度学习模型的影响可能意味着在几分钟、几小时或几天内产生不同的结果。
- en: Optimization sits at the center of deep learning. Most learning problems reduce
    to optimization problems. Let's imagine we are solving a problem for some set
    of data. Using this pre-processed data, we train a model by solving an optimization
    problem, which optimizes the weights of the model with regards to the chosen loss
    function and some regularization function.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 优化是深度学习的核心。大多数学习问题都可以简化为优化问题。让我们设想，我们正在为一组数据解决一个问题。使用这些预处理过的数据，我们通过解决一个优化问题来训练模型，优化模型的权重，以使其符合所选择的损失函数和某些正则化函数。
- en: Hyper parameters of a model play a significant role in the efficient training
    of a model. Therefore, it is essential to use the different optimization strategies
    and algorithms to measure appropriate and optimum values of model's hyper parameters,
    which affect our Model's learning process, and finally the output of a model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的超参数在模型的高效训练中起着重要作用。因此，使用不同的优化策略和算法来测量模型超参数的合适和值是至关重要的，这些超参数会影响模型的学习过程，并最终影响模型的输出。
- en: Types of optimizers
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器的类型
- en: First, we look at the high-level categories of optimization algorithms and then
    dive deep into the individual optimizers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看优化算法的高级类别，然后深入研究各个优化器。
- en: '**First order optimization** algorithms minimize or maximize a loss function
    using its gradient values concerning the parameters. The popularly used First
    order optimization algorithm is gradient descent. Here, the first order derivative
    tells us whether the function is decreasing or increasing at a particular point.
    The first order derivative gives us a line which is tangential to a point on its
    error surface.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**一阶优化**算法使用梯度值相对于参数来最小化或最大化损失函数。常用的一阶优化算法是梯度下降。在这里，一阶导数告诉我们函数在某个特定点是增加还是减少。一阶导数给出了一条切线，与误差曲面上的某点相切。'
- en: The derivative for a function depends on single variables, whereas a gradient
    for a function depends on multiple variables.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的导数依赖于单一变量，而函数的梯度依赖于多个变量。
- en: '**Second order optimization** algorithms use the second order derivative, which
    is also known as **Hessian**, to minimize or maximize the given loss function.
    Here, the Hessian is a matrix of second order partial derivatives. The second
    derivative is costly to compute. Hence, it''s not used much. The second order
    derivative indicates to us whether the first derivative is increasing or decreasing,
    giving an idea of functions curvature. The second order derivative gives us with
    a quadratic surface which touches the shape of the error surface.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**二阶优化**算法使用二阶导数，也称为**海森矩阵**，来最小化或最大化给定的损失函数。这里，海森矩阵是二阶偏导数的矩阵。二阶导数的计算代价较高，因此不常使用。二阶导数告诉我们一阶导数是增加还是减少，从而反映了函数的曲率。二阶导数给出了一个二次曲面，它与误差曲面的形状相切。'
- en: The second order derivative is costly to compute, but the advantage of a second
    order optimization method is that it does not neglect or ignore the curvature
    of a surface. Also, the stepwise performance is better. The key thing to note
    while choosing the optimization method is, first-order optimization methods are
    simple to compute and less time consuming, converging rather fast on large data
    sets. Second order methods are faster only when the second order derivative is
    known, and these methods are slower and expensive to compute in terms of both
    time and memory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数计算成本较高，但二阶优化方法的优势在于它不会忽略或忽视曲面的弯曲性。同时，逐步性能也更好。在选择优化方法时，关键要注意，一级优化方法计算简单且时间消耗较少，在大数据集上收敛速度较快。而二阶方法仅在已知二阶导数时更快，且在时间和内存上的计算开销较大，速度较慢且计算代价较高。
- en: The second order optimization method can, at times, work better than first-order
    gradient descent methods because second-order methods will never get stuck around
    paths of slow convergence, that is, around saddle points, whereas gradient descent
    at times gets stuck and does not converge.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶优化方法有时比一阶梯度下降方法表现得更好，因为二阶方法永远不会在收敛缓慢的路径上卡住，即不会卡在鞍点附近，而梯度下降有时会卡住并无法收敛。
- en: Gradient descent
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Gradient descent is an algorithm which minimizes functions. A set of parameters
    defines a function, and the gradient descent algorithm starts with the initial
    set of param values and iteratively moves toward a set of param values that minimizes
    the function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种最小化函数的算法。一组参数定义了一个函数，梯度下降算法从初始参数值集开始，逐步逼近最小化该函数的参数值集。
- en: 'This iterative minimization is achieved using calculus, taking steps in the
    negative direction of the function gradient, as can be seen in the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种迭代最小化通过微积分实现，沿函数梯度的负方向一步步推进，如下图所示：
- en: '![](img/e3e8215c-9a54-438f-926c-e840fb8dd465.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3e8215c-9a54-438f-926c-e840fb8dd465.jpeg)'
- en: Gradient descent is the most successful optimization algorithm. As mentioned
    earlier, it is used to do weights updates in a neural network so that we minimize
    the loss function. Let's now talk about an important neural network method called
    backpropagation, in which we firstly propagate forward and calculate the dot product
    of inputs with their corresponding weights, and then apply an activation function
    to the sum of products which transforms the input to an output and adds non linearities
    to the model, which enables the model to learn almost any arbitrary functional
    mappings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是最成功的优化算法。如前所述，它用于在神经网络中更新权重，从而最小化损失函数。接下来，我们将讨论一个重要的神经网络方法——反向传播。在这个过程中，我们首先进行前向传播，计算输入与相应权重的点积，然后将激活函数应用于加权和，将输入转化为输出，并向模型中添加非线性因素，使得模型能够学习几乎任何任意的功能映射。
- en: 'Later, we back propagate in the neural network, carrying error terms and updating
    weights values using gradient descent, as shown in the following graph:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在神经网络中进行反向传播，传递误差项并使用梯度下降更新权重值，如下图所示：
- en: '![](img/6408090e-36de-4968-8d52-5868895befb7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6408090e-36de-4968-8d52-5868895befb7.png)'
- en: Different variants of gradient descent
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降的不同变种
- en: '**Standard gradient descent**, also known as **batch gradient descent**, will
    calculate the gradient of the whole dataset but will perform only one update.
    Therefore, it can be quite slow and tough to control for datasets which are extremely
    large and don''t fit in the memory. Let''s now look at algorithms that can solve
    this problem.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准梯度下降**，也称为**批量梯度下降**，将计算整个数据集的梯度，但只进行一次更新。因此，对于那些极大且无法完全加载到内存中的数据集，它可能会非常缓慢且难以控制。接下来，让我们看看能解决这个问题的算法。'
- en: '**Stochastic gradient descent** (**SGD**) performs parameter updates on each
    training example, whereas mini batch performs an update with *n* number of training
    examples in each batch. The issue with SGD is that, due to the frequent updates
    and fluctuations, it eventually complicates the convergence to the accurate minimum
    and will keep exceeding due to regular fluctuations. Mini-batch gradient descent
    comes to the rescue here, which reduces the variance in the parameter update,
    leading to a much better and stable convergence. SGD and mini-batch are used interchangeably.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）在每个训练样本上执行参数更新，而小批量梯度下降则在每个批次的 *n* 个训练样本上执行更新。SGD 的问题在于，由于频繁的更新和波动，最终会使收敛到准确最小值变得复杂，并且由于持续的波动，可能会不断超出目标。而小批量梯度下降则解决了这个问题，它通过减少参数更新中的方差，从而实现更好且稳定的收敛。SGD
    和小批量梯度下降可以互换使用。'
- en: Overall problems with gradient descent include choosing a proper learning rate
    so that we avoid slow convergence at small values, or divergence at larger values and
    applying the same learning rate to all parameter updates wherein if the data is
    sparse we might not want to update all of them to the same extent. Lastly, is
    dealing with saddle points.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的总体问题包括选择合适的学习率，以避免在较小值时出现收敛缓慢，或者在较大值时出现发散，并且对所有参数更新应用相同的学习率。如果数据稀疏，我们可能不希望对所有参数进行相同程度的更新。最后，还需要处理鞍点问题。
- en: Algorithms to optimize gradient descent
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化梯度下降的算法
- en: We will now be looking at various methods for optimizing gradient descent in
    order to calculate different learning rates for each parameter, calculate momentum,
    and prevent decaying learning rates.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨优化梯度下降的各种方法，以便为每个参数计算不同的学习率、计算动量并防止学习率衰减。
- en: To solve the problem of high variance oscillation of the SGD, a method called
    **momentum** was discovered; this accelerates the SGD by navigating along the
    appropriate direction and softening the oscillations in irrelevant directions.
    Basically, it adds a fraction of the update vector of the past step to the current
    update vector. Momentum value is usually set to .9\. Momentum leads to a faster
    and stable convergence with reduced oscillations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 SGD 高方差振荡的问题，发现了一种叫做 **动量** 的方法；它通过沿着正确的方向进行加速，并在无关方向上减缓振荡，来加速 SGD。基本上，它会将上一步更新向量的一部分加到当前更新向量中。动量值通常设置为
    0.9。动量可以加速收敛并减少振荡，使收敛更稳定。
- en: '**Nesterov accelerated gradient** explains that as we reach the minima, that
    is, the lowest point on the curve, momentum is quite high and it doesn''t know
    to slow down at that point due to the large momentum which could cause it to miss
    the minima entirely and continue moving up. Nesterov proposed that we first make
    a long jump based on the previous momentum, then calculate the gradient and then
    make a correction which results in a parameter update. Now, this update prevents
    us to go too fast and not miss the minima, and makes it more responsive to changes.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nesterov 加速梯度** 解释了当我们达到最小值时，也就是曲线的最低点，动量相当大，且由于动量过大，它并不会在该点减速，这可能会导致错过最小值并继续上升。Nesterov
    提出了我们首先基于之前的动量进行一次较大的跳跃，然后计算梯度，接着进行修正，从而更新参数。这个更新可以防止我们过快地前进，从而错过最小值，并且使模型对变化更敏感。'
- en: '**Adagrad** allows the learning rate to adapt based on the parameters. Therefore,
    it performs large updates for infrequent parameters and small updates for frequent
    parameters. Therefore, it is very well-suited for dealing with sparse data. The
    main flaw is that its learning rate is always decreasing and decaying. Problems
    with decaying learning rates are solved using AdaDelta.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adagrad** 允许学习率根据参数自适应调整。因此，对于不常见的参数，它执行较大的更新，而对于常见的参数，它执行较小的更新。因此，Adagrad
    非常适合处理稀疏数据。其主要缺点是学习率总是在逐渐减小和衰减。使用 AdaDelta 可以解决学习率衰减的问题。'
- en: '**AdaDelta** solves the problem of decreasing learning rate in AdaGrad. In
    AdaGrad, the learning rate is computed as one divided by the sum of square roots.
    At each stage, we add another square root to the sum, which causes the denominator
    to decrease constantly. Now, instead of summing all prior square roots, it uses
    a sliding window which allows the sum to decrease.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaDelta** 解决了 AdaGrad 中学习率逐渐减小的问题。在 AdaGrad 中，学习率是通过将 1 除以平方根和计算的。在每个阶段，我们都会将另一个平方根加入总和，这会导致分母不断减小。现在，AdaDelta
    不再对所有以前的平方根求和，而是使用滑动窗口，这允许总和减小。'
- en: '**Adaptive Moment Estimation** (**Adam**) computes adaptive learning rates
    for each parameter. Like AdaDelta, Adam not only stores the decaying average of
    past squared gradients but additionally stores the momentum change for each parameter.
    Adam works well in practice and is one of the most used optimization methods today.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应矩估计**（**Adam**）为每个参数计算自适应学习率。与AdaDelta类似，Adam不仅存储过去平方梯度的衰减平均值，还存储每个参数的动量变化。Adam在实践中表现良好，是目前最常用的优化方法之一。'
- en: 'The following two images (image credit: Alec Radford) show the optimization
    behavior of optimization algorithms described earlier. We see their behavior on
    the contours of a loss surface over time. Adagrad, RMsprop, and Adadelta almost
    quickly head off in the right direction and converge fast, whereas momentum and
    NAG are headed off-track. NAG is soon able to correct its course due to its improved
    responsiveness by looking ahead and going to the minimum.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两张图（图片来源：Alec Radford）展示了前面描述的优化算法的优化行为。我们看到它们在损失曲面的等高线上的行为随时间变化。Adagrad、RMsprop
    和 Adadelta几乎迅速朝着正确的方向前进并快速收敛，而momentum和NAG则偏离轨道。由于NAG通过提前查看并向最小值移动，从而提高响应能力，很快就能修正其方向。
- en: '![](img/21078e47-ab9b-4dbd-95a9-aaf1f8dfa1eb.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21078e47-ab9b-4dbd-95a9-aaf1f8dfa1eb.png)'
- en: 'The second image displays the behavior of the algorithms at a saddle point.
    **SGD**, **Momentum**, and **NAG** find it challenging to break symmetry, but
    slowly they manage to escape the saddle point, whereas **Adagrad**, **Adadelta**,
    and **RMsprop** head down the negative slope, as can seen from the following image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张图展示了算法在鞍点的行为。**SGD**、**Momentum** 和 **NAG** 在打破对称性方面遇到挑战，但它们最终能够慢慢摆脱鞍点，而
    **Adagrad**、**Adadelta** 和 **RMsprop** 则沿着负斜率下坡，从下图可以看到：
- en: '![](img/04660bd2-3f81-4c3a-b5bf-94447cf0b8c5.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04660bd2-3f81-4c3a-b5bf-94447cf0b8c5.png)'
- en: Which optimizer to choose
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择哪个优化器
- en: In the case that the input data is sparse or if we want fast convergence while
    training complex neural networks, we get the best results using adaptive learning
    rate methods. We also don't need to tune the learning rate. For most cases, Adam
    is usually a good choice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据稀疏，或者我们希望在训练复杂的神经网络时实现快速收敛，使用自适应学习率方法能得到最佳结果。我们还不需要调整学习率。对于大多数情况，Adam通常是一个不错的选择。
- en: Optimization with an example
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例进行优化
- en: Let's take an example of linear regression, where we try to find the best fit
    for a straight line through a number of data points by minimizing the squares
    of the distance from the line to each data point. This is why we call it least
    squares regression. Essentially, we are formulating the problem as an optimization
    problem, where we are trying to minimize a loss function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以线性回归为例，我们通过最小化从直线到每个数据点的距离的平方，来找到最佳拟合的直线。这就是我们称其为最小二乘回归的原因。本质上，我们将问题表述为一个优化问题，其中我们尝试最小化损失函数。
- en: 'Let''s set up input data and look at the scatter plot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置输入数据并查看散点图：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/c03f289d-0fed-49f7-b77b-519f3e7ed240.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c03f289d-0fed-49f7-b77b-519f3e7ed240.png)'
- en: 'Define the data size and batch size:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 定义数据大小和批次大小：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will need to resize the data to meet the TensorFlow input format, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要调整数据大小以符合TensorFlow输入格式，如下所示：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following scope initializes the `weights` and `bias`, and describes the
    linear model and loss function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下范围初始化了`weights`和`bias`，并描述了线性模型和损失函数：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then set optimizers for minimizing the loss:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们设置优化器来最小化损失：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/e5d26fc6-cbfa-4d72-bbb5-a894c7135c0f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5d26fc6-cbfa-4d72-bbb5-a894c7135c0f.png)'
- en: 'We also get a sliding curve, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还得到一个滑动曲线，如下所示：
- en: '![](img/ba681a85-288f-4c46-90ba-092a214c96f1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba681a85-288f-4c46-90ba-092a214c96f1.png)'
- en: Summary
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned the fundamentals of optimization techniques and
    various types. Optimization is a complicated subject and a lot depends on the
    nature and size of our data. Also, optimization depends on weight matrices. A
    lot of these optimizers are trained and tuned for tasks like image classification
    or predictions. However, for custom or new use cases, we need to perform trial
    and error to determine the best solution.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了优化技术的基本原理和不同类型。优化是一个复杂的主题，很多因素取决于我们数据的性质和大小。此外，优化还取决于权重矩阵。这些优化器大多针对图像分类或预测任务进行了训练和调整。然而，对于定制或新的使用案例，我们需要通过反复试验来确定最佳解决方案。
