- en: Representation Learning - Implementing Word Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示学习 - 实现词嵌入
- en: Machine learning is a science that is mainly based on statistics and linear
    algebra. Applying matrix operations is very common among most machine learning
    or deep learning architectures because of backpropagation. This is the main reason
    deep learning, or machine learning in general, accepts only real-valued quantities
    as input. This fact contradicts many applications, such as machine translation,
    sentiment analysis, and so on; they have text as an input. So, in order to use
    deep learning for this application, we need to have it in the form that deep learning
    accepts!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一门主要基于统计学和线性代数的科学。矩阵运算在大多数机器学习或深度学习架构中非常常见，因为反向传播的原因。这也是深度学习或机器学习通常只接受实值输入的主要原因。这个事实与许多应用相矛盾，比如机器翻译、情感分析等，它们的输入是文本。因此，为了将深度学习应用于这些场景，我们需要将文本转化为深度学习能够接受的形式！
- en: In this chapter, we are going to introduce the field of representation learning,
    which is a way to learn a real-valued representation from text while preserving
    the semantics of the actual text. For example, the representation of love should
    be very close to the representation of adore because they are used in very similar
    contexts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍表示学习领域，这是从文本中学习实值表示的一种方法，同时保持实际文本的语义。例如，love的表示应该与adore的表示非常接近，因为它们在非常相似的上下文中使用。
- en: 'So, the following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，本章将涵盖以下主题：
- en: Introduction to representation learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示学习简介
- en: Word2Vec
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: A practical example of the skip-gram architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram架构的实际示例
- en: Skip-gram Word2Vec implementation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram Word2Vec实现
- en: Introduction to representation learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示学习简介
- en: All the machine learning algorithms or architectures that we have used so far
    require the input to be real-valued or matrices of real-valued quantities, and
    that's a common theme in machine learning. For example, in the convolution neural
    network, we had to feed raw pixel values of images as model inputs. In this part,
    we are dealing with text, so we need to encode our text somehow and produce real-valued
    quantities that can be fed to a machine learning algorithm. In order to encode
    input text as real-valued quantities, we need to use an intermediate science called
    **Natural Language Processing** (**NLP**).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用的所有机器学习算法或架构都要求输入为实值或实值矩阵，这是机器学习中的一个共同主题。例如，在卷积神经网络中，我们必须将图像的原始像素值作为模型输入。在这一部分，我们处理的是文本，因此我们需要以某种方式对文本进行编码，并生成可以输入到机器学习算法中的实值数据。为了将输入文本编码为实值数据，我们需要使用一种名为**自然语言处理**（**NLP**）的中介技术。
- en: We mentioned that in this kind of pipeline, where we feed text to a machine
    learning model such as sentiment analysis, this will be problematic and won't
    work because we won't be able to apply backpropagation or any other operations
    such as dot product on the input, which is a string. So, we need to use a mechanism
    of NLP that will enable us to build an intermediate representation of the text
    that can carry the same information as the text and also be fed to the machine
    learning models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，在这种管道中，当我们将文本输入到机器学习模型中进行情感分析时，这将是一个问题，并且无法工作，因为我们无法在输入（字符串）上应用反向传播或其他操作（如点积）。因此，我们需要使用NLP的机制，构建一个文本的中间表示，该表示能够携带与文本相同的信息，并且可以被输入到机器学习模型中。
- en: 'We need to convert each word or token in the input text to a real-valued vector.
    These vectors will be useless if they don''t carry the patterns, information,
    meaning, and semantics of the original input. For example, as in real text, the
    two words love and adore are very similar to each other and carry the same meaning.
    We need the resultant real-valued vectors that will represent them to be close
    to each other and be in the same vector space. So, the vector representation of
    these two words along with another word that isn''t similar to them will be like
    this diagram:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将输入文本中的每个单词或标记转换为实值向量。如果这些向量不携带原始输入的模式、信息、意义和语义，那么它们将毫无用处。例如，像真实文本中的两个单词love和adore非常相似，并且有相同的含义。我们需要将它们表示为的实值向量接近彼此，并处于相同的向量空间中。因此，这两个单词的向量表示与另一个不相似的单词一起，将呈现如下图所示的形态：
- en: '![](img/b7e0177f-a510-4ef9-9034-667ec7828f6b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7e0177f-a510-4ef9-9034-667ec7828f6b.png)'
- en: 'Figure 15.1: Vector representation of words'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：词的向量表示
- en: There are many techniques that can be used for this task. This family of techniques
    is called **embeddings**, where you're embedding text into another real-valued
    vector space.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多技术可以用于这个任务。这些技术统称为**嵌入（embeddings）**，它将文本嵌入到另一个实值向量空间中。
- en: As we'll see later on, this vector space is very interesting actually, because
    you will find out that you can drive a word's vectors from other words that are
    similar to it or even do some geography in this space.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后所见，这个向量空间实际上非常有趣，因为你会发现你可以通过其他与之相似的单词来推导一个单词的向量，甚至可以在这个空间里进行一些“地理”操作。
- en: Word2Vec
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec
- en: Word2Vec is one of the widely used embedding techniques in the area of NLP.
    This model creates real-valued vectors from input text by looking at the contextual
    information the input word appears in. So, you will find out that similar words
    will be mentioned in very similar contexts, and hence the model will learn that
    those two words should be placed close to each other in the particular embedding
    space.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是自然语言处理（NLP）领域中广泛使用的嵌入技术之一。该模型通过观察输入单词出现的上下文信息，将输入文本转换为实值向量。因此，你会发现相似的单词通常会出现在非常相似的上下文中，从而模型会学到这些单词应该被放置在嵌入空间中的彼此相近位置。
- en: 'From the statements in the following diagram, the model will learn that the
    words **love** and **adore** share very similar contexts and should be placed
    very close to each other in the resulting vector space. The context of like could
    be a bit similar as well to the word love, but it won''t be as close to love as
    the word adore:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面图示中的陈述来看，模型将学到 **love** 和 **adore** 共享非常相似的上下文，并且应该被放置在最终的向量空间中非常接近的位置。单词
    like 的上下文可能也与 love 稍有相似，但它不会像 adore 那样接近 love：
- en: '![](img/f0ad5b13-4057-486b-ae7d-90aac51560df.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0ad5b13-4057-486b-ae7d-90aac51560df.png)'
- en: 'Figure 15.2: Sample of sentiment sentences'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2：情感句子示例
- en: The Word2Vec model also relies on semantic features of input sentences; for
    example, the two words adore and love are mainly used in a positive context and
    usually precede noun phrases or nouns. Again, the model will learn that these
    two words have something in common and it will be more likely to put the vector
    representation of these two vectors in a similar context. So, the structure of
    the sentence will tell the Word2Vec model a lot about similar words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 模型还依赖于输入句子的语义特征；例如，单词 adore 和 love 主要在积极的语境中使用，通常会出现在名词短语或名词前面。同样，模型会学习到这两个词有共同之处，因此更可能将这两个词的向量表示放在相似的语境中。因此，句子的结构会告诉
    Word2Vec 模型很多关于相似词的信息。
- en: In practice, people feed a large corpus of text to the Word2Vec model. The model
    will learn to produce similar vectors for similar words, and it will do so for
    each unique word in the input text.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，人们将一个大规模的文本语料库输入到 Word2Vec 模型中。该模型将学习为相似的单词生成相似的向量，并为输入文本中的每个唯一单词执行此操作。
- en: All of these words' vectors will be combined and the final output will be an
    embedding matrix where each row represents the real-valued vector representation
    of a specific unique word.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些单词的向量将被结合起来，最终的输出将是一个嵌入矩阵，其中每一行代表特定唯一单词的实值向量表示。
- en: '![](img/312e4a86-0742-47ae-9db6-ab700a6d4376.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/312e4a86-0742-47ae-9db6-ab700a6d4376.png)'
- en: 'Figure 15.3: Example of Word2Vec model pipeline'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3：Word2Vec 模型流程示例
- en: So, the final output of the model will be an embedding matrix for all the unique
    words in the training corpus. Usually, good embedding matrices could contain millions
    of real-valued vectors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型的最终输出将是一个针对训练语料库中所有唯一单词的嵌入矩阵。通常，好的嵌入矩阵可以包含数百万个实值向量。
- en: Word2Vec modeling uses a window to scan the sentence and then tries to predict
    the vector of the middle word of that window based on its contextual information;
    the Word2Vec model will scan a sentence at a time. Similar to any machine learning
    technique, we need to define a cost function for the Word2Vec model and its corresponding
    optimization criteria that will make the model capable of generating real-valued
    vectors for each unique image and also relate the vectors to each other based
    on their contextual information
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 建模使用窗口扫描句子，然后根据上下文信息预测窗口中间单词的向量；Word2Vec 模型一次扫描一个句子。与任何机器学习技术类似，我们需要为
    Word2Vec 模型定义一个成本函数以及相应的优化标准，使得该模型能够为每个唯一的单词生成实值向量，并根据上下文信息将这些向量彼此关联。
- en: Building Word2Vec model
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 Word2Vec 模型
- en: In this section, we will go through some deeper details of how can we build
    a Word2Vec model. As we mentioned previously, our final goal is to have a trained
    model that will able to generate real-valued vector representation for the input
    textual data which is also called word embeddings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入讨论如何构建一个 Word2Vec 模型。如前所述，我们的最终目标是拥有一个训练好的模型，能够为输入的文本数据生成实值向量表示，这也叫做词嵌入。
- en: During the training of the model, we will use the maximum likelihood method
    ([https://en.wikipedia.org/wiki/Maximum_likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood)),
    which can be used to maximize the probability of the next word *w[t]* in the input
    sentence given the previous words that the model has seen, which we can call *h*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，我们将使用最大似然法 ([https://en.wikipedia.org/wiki/Maximum_likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood))，这个方法可以用来最大化给定模型已经看到的前一个词的条件下，下一个词
    *w[t]* 在输入句子中的概率，我们可以称之为 *h*。
- en: 'This maximum likelihood method will be expressed in terms of the softmax function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最大似然法将用软最大函数来表示：
- en: '![](img/8312e4a7-cfe0-47dd-9b24-42670c3aca92.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8312e4a7-cfe0-47dd-9b24-42670c3aca92.png)'
- en: 'Here, the *score* function computes a value to represent the compatibility
    of the target word *w[t]* with respect to the context *h*. This model will be
    trained on the input sequences while training to maximize the likelihood on the
    training input data (log likelihood is used for mathematical simplicity and derivation
    with the log):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*score* 函数计算一个值，用来表示目标词 *w[t]* 与上下文 *h* 的兼容性。该模型将在输入序列上进行训练，旨在最大化训练数据的似然性（为简化数学推导，使用对数似然）。
- en: '![](img/ad309d16-c7ef-4cef-9e0e-f3b791538f1d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad309d16-c7ef-4cef-9e0e-f3b791538f1d.png)'
- en: So, the *ML* method will try to maximize the above equation which, will result
    in a probabilistic language model. But the calculation of this is very computationally
    expensive, as we need to compute each probability using the score function for
    all the words in the
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*ML* 方法将尝试最大化上述方程，从而得到一个概率语言模型。但由于需要使用评分函数计算所有词的每个概率，这一计算非常耗费计算资源。
- en: vocabulary *V* words *w'*, in the corresponding current context *h* of this
    model. This will happen at every training step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表 *V* 中的单词 *w'*，在该模型的当前上下文 *h* 中。这将在每个训练步骤中发生。
- en: '![](img/202fe239-8362-46dc-ab60-3a54281cf6cd.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/202fe239-8362-46dc-ab60-3a54281cf6cd.png)'
- en: 'Figure 15.4: General architecture of a probabilistic language model'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4：概率语言模型的一般架构
- en: Because of the computational expensiveness of building the probabilistic language
    model, people tend to use different techniques that are less computationally expensive,
    such as **Continuous Bag-of-Words** (**CBOW**) and skip-gram models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于构建概率语言模型的计算开销较大，人们倾向于使用一些计算上更为高效的技术，比如 **连续词袋模型** (**CBOW**) 和跳字模型。
- en: 'These models are trained to build a binary classification with logistic regression
    to separate between the real target words *w[t]* and *h* noise or imaginary words
    ![](img/91f9ba2d-f6dc-4587-b482-b348d4b4560b.png)**,** which is in the same context.
    The following diagram simplifies this idea using the CBOW technique:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型经过训练，用逻辑回归构建一个二元分类模型，以区分真实目标词 *w[t]* 和 *h* 噪声或虚构词 ![](img/91f9ba2d-f6dc-4587-b482-b348d4b4560b.png)**,**
    它们处在相同的上下文中。下面的图表简化了这个概念，采用了 CBOW 技术：
- en: '![](img/4663156d-d15c-4994-8fa0-9942283b6244.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4663156d-d15c-4994-8fa0-9942283b6244.png)'
- en: 'Figure 15.5: General architecture of skip-gram model'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5：跳字模型的一般架构
- en: 'The next diagram, shows the two architectures that you can use for building
    the Word2Vec model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一张图展示了你可以用来构建 Word2Vec 模型的两种架构：
- en: '![](img/6ec47f15-c055-4e80-8173-a86d0670dc70.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ec47f15-c055-4e80-8173-a86d0670dc70.png)'
- en: 'Figure 15.6: different architectures for the Word2Vec model'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6：Word2Vec 模型的不同架构
- en: 'To be more formal, the objective function of these techniques maximizes the
    following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，这些技术的目标函数最大化如下：
- en: '![](img/bbd635d1-b9bd-42b3-915e-d6fcdd476168.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbd635d1-b9bd-42b3-915e-d6fcdd476168.png)'
- en: 'Where:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '**![](img/e3e0bbb6-3b7d-4a79-bcfa-1311153492a7.png)** is the probability of
    the binary logistic regression based on the model seeing the word *w* in the context
    *h* in the dataset *D***,** which is calculated in terms of the θ vector. This
    vector represents the learned embeddings.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**![](img/e3e0bbb6-3b7d-4a79-bcfa-1311153492a7.png)** 是基于模型在数据集 *D* 中看到词 *w*
    在上下文 *h* 中的二元逻辑回归概率，这个概率是通过 θ 向量计算的。这个向量表示已学习的嵌入。'
- en: '![](img/9653f073-6172-4710-800d-692f0ca28fbf.png)is the imaginary or noisy
    words that we can generate from a noisy probabilistic distribution, such as the
    unigram of the training input examples.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/9653f073-6172-4710-800d-692f0ca28fbf.png) 是我们可以从一个噪声概率分布中生成的虚拟或噪声词汇，例如训练输入样本的
    unigram。'
- en: To sum up, the objective of these models is to discriminate between real and
    imaginary inputs, and hence assign higher probability to real words and less probability
    for the case of imaginary or noisy words.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些模型的目标是区分真实和虚拟的输入，因此需要给真实词汇分配较高的概率，而给虚拟或噪声词汇分配较低的概率。
- en: This objective is maximized when the model assigns high probabilities to real
    words and low probabilities to noise words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型将高概率分配给真实词汇，低概率分配给噪声词汇时，该目标得到了最大化。
- en: 'Technically, the process of assigning high probability to real words is is
    called **negative sampling** ([https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)),
    and there is good mathematical motivation for using this loss function: the updates
    it proposes approximate the updates of the softmax function in the limit. But
    computationally, it is especially appealing because computing the loss function
    now scales only with the number of noise words that we select (*k*), and not all
    words in the vocabulary (*V*). This makes it much faster to train. We will actually
    make use of the very similar **noise-contrastive estimation** (**NCE**) ([https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf))
    loss, for which TensorFlow has a handy helper function, `tf.nn.nce_loss()`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，将高概率分配给真实词汇的过程称为**负采样**（[https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)），并且使用这种损失函数有很好的数学依据：它提出的更新近似了软最大（softmax）函数在极限情况下的更新。但从计算角度来看，它尤其具有吸引力，因为现在计算损失函数的复杂度仅与我们选择的噪声词数量（*k*）相关，而与词汇表中的所有词汇（*V*）无关。这使得训练变得更加高效。实际上，我们将使用非常类似的**噪声对比估计**（**NCE**）（[https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)）损失函数，TensorFlow
    提供了一个便捷的辅助函数 `tf.nn.nce_loss()`。
- en: A practical example of the skip-gram architecture
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: skip-gram 架构的一个实际示例
- en: 'Let''s go through a practical example and see how skip-gram models will work
    in this situation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际例子，看看在这种情况下 skip-gram 模型是如何工作的：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First off, we need to make a dataset of words and their corresponding context.
    Defining the context is up to us, but it has to make sense. So, we'll take a window
    around the target word and take a word from the right and another from the left.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要构建一个包含词语及其对应上下文的数据集。上下文的定义取决于我们，但必须合理。因此，我们会围绕目标词设置一个窗口，并从右边取一个词，再从左边取一个词。
- en: 'By following this contextual technique, we will end up with the following set
    of words and their corresponding context:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这种上下文技术，我们最终会得到以下一组词语及其对应的上下文：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The generated words and their corresponding context will be represented as
    pairs of `(context, target)`. The idea of skip-gram models is the inverse of CBOW
    ones. In the skip- gram model, we will try to predict the context of the word
    based on its target word. For example, considering the first pair, the skip-gram
    model will try to predict `the` and `brown` from the target word `quick`, and
    so on. So, we can rewrite our dataset as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的词语及其对应的上下文将以 `(context, target)` 的形式表示。skip-gram 模型的思想与 CBOW 模型正好相反。在 skip-gram
    模型中，我们会尝试根据目标词来预测该词的上下文。例如，考虑第一个词对，skip-gram 模型会尝试从目标词 `quick` 预测出 `the` 和 `brown`
    等词，依此类推。所以，我们可以将数据集重写如下：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, we have a set of input and output pairs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一组输入和输出的词对。
- en: Let's try to mimic the training process at specific step *t*. So, the skip-gram
    model will take the first training sample where the input is the word `quick`
    and the target output is the word `the`. Next, we need to construct the noisy
    input as well, so we are going to randomly select from the unigrams of the input
    data. For simplicity, the size of the noisy vector will be only one. For example,
    we can select the word `sheep` as a noisy example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试模仿在特定步骤 *t* 处的训练过程。那么，skip-gram 模型将以第一个训练样本为输入，其中输入词为 `quick`，目标输出词为 `the`。接下来，我们需要构造噪声输入，因此我们将从输入数据的单词集中随机选择。为了简化，噪声向量的大小仅为
    1。例如，我们可以选择 `sheep` 作为噪声样本。
- en: 'Now, we can go ahead and compute the loss between the real pair and the noisy
    one as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续计算真实对和噪声对之间的损失，公式如下：
- en: '![](img/01d022e1-f1ed-4382-9b53-b3ecb87473e9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01d022e1-f1ed-4382-9b53-b3ecb87473e9.png)'
- en: The goal in this case is to update the θ parameter to improve the previous objective
    function. Typically, we can use gradient for this. So, we will try to calculate
    the gradient of the loss with respect to the objective function parameter θ, which
    will be represented by ![](img/534a4675-7ec2-4f49-8ff8-7e28502eaabf.png).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，目标是更新 θ 参数，以改进之前的目标函数。通常，我们可以使用梯度来进行这个操作。因此，我们将尝试计算损失相对于目标函数参数 θ 的梯度，其表示为
    ![](img/534a4675-7ec2-4f49-8ff8-7e28502eaabf.png)。
- en: After the training process, we can visualize some results based on their reduced
    dimensions of the real-valued vector representation. You will find that this vector
    space is very interesting because you can do lots of interesting stuff with it.
    For example, you can learn Analogy in this space by saying that king is to queen
    as man is to woman. We can even derive the woman vector by subtracting the king
    vector from the queen one and adding the man; the result of this will be very
    close to the actual learned vector of the woman. You can also learn geography
    in this space.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程之后，我们可以基于实值向量表示的降维结果可视化一些结果。你会发现这个向量空间非常有趣，因为你可以用它做很多有趣的事情。例如，你可以在这个空间中学习类比，通过说“国王对王后就像男人对女人”。我们甚至可以通过从王后向量中减去国王向量并加上男人向量来推导出女人的向量；这个结果将非常接近实际学习到的女人向量。你也可以在这个空间中学习地理。
- en: '![](img/0a72b1e7-003f-43b2-9360-c26ff8e4fc94.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a72b1e7-003f-43b2-9360-c26ff8e4fc94.png)'
- en: 'Figure 15.7: Projection of the learned vectors to two dimensions using t-distributed
    stochastic neighbor embedding (t-SNE) dimensionality reduction technique'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：使用 t-分布随机邻域嵌入（t-SNE）降维技术将学习到的向量投影到二维空间
- en: The preceding example gives very good intuition behind these vectors and how
    they'll be useful for most NLP applications such as machine translation or **part-of-speech**
    (**POS**) tagging.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子为这些向量提供了很好的直觉，并且展示了它们如何对大多数自然语言处理应用（如机器翻译或 **词性**（**POS**）标注）非常有用。
- en: Skip-gram Word2Vec implementation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Skip-gram Word2Vec 实现
- en: After understanding the mathematical details of how skip-gram models work, we
    are going to implement skip-gram, which encodes words into real-valued vectors
    that have certain properties (hence the name Word2Vec). By implementing this architecture,
    you will get a clue of how the process of learning another representation works.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了 skip-gram 模型如何工作的数学细节后，我们将实现 skip-gram，该模型将单词编码为具有某些属性的实值向量（因此得名 Word2Vec）。通过实现这一架构，你将了解学习另一种表示方式的过程是如何进行的。
- en: Text is the main input for a lot of natural language processing applications
    such as machine translation, sentiment analysis, and text to speech systems. So,
    learning a real-valued representation for the text will help us use different
    deep learning techniques for these tasks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是许多自然语言处理应用的主要输入，例如机器翻译、情感分析和语音合成系统。因此，为文本学习实值表示将帮助我们使用不同的深度学习技术来处理这些任务。
- en: In the early chapters of this book, we introduced something called one-hot encoding,
    which produces a vector of zeros except for the index of the word that this vector
    represents. So, you may wonder why we are not using it here. This method is very
    inefficient because usually you have a big set of distinct words, maybe something
    like 50,000 words, and using one-hot encoding for this will produce a vector of
    49,999 entries set to zero and only one entry set to one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的早期章节中，我们介绍了叫做独热编码（one-hot encoding）的方法，它会生成一个零向量，除了表示该词的索引外其他都为零。那么，你可能会想，为什么这里不使用它呢？这种方法非常低效，因为通常你会有一个很大的独特单词集，可能有
    50,000 个单词，使用独热编码时，将会生成一个包含 49,999 个零的向量，并且只有一个位置是 1。
- en: Having a very sparse input like this will result in a huge waste of computation
    because of the matrix multiplications that we'd do in the hidden layers of the
    neural network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入非常稀疏，会导致大量计算浪费，特别是在神经网络的隐藏层进行矩阵乘法时。
- en: '![](img/af1fa91e-a31e-42f8-bbce-edde8e8bb703.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af1fa91e-a31e-42f8-bbce-edde8e8bb703.png)'
- en: 'Figure 15.8: One-hot encoding which will result in huge waste of computation'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8：一热编码将导致大量计算浪费
- en: As we mentioned previously, the outcome of using one-hot encoding will be a
    very sparse vector, especially when you have a huge amount of distinct words that
    you want to encode.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用一热编码的结果将是一个非常稀疏的向量，特别是当你有大量不同的词汇需要编码时。
- en: 'The following figure shows that when we multiply this sparse vector of all
    zeros except for one entry by a matrix of weights, the output will be only the
    row of the matrix that corresponds to the one value of the sparse vector:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图所示，当我们将这个除了一个条目之外全为零的稀疏向量与一个权重矩阵相乘时，输出将仅为矩阵中与稀疏向量中唯一非零值对应的行：
- en: '![](img/a3c6bad7-0f91-483c-aa1f-74c6c20a6a2f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3c6bad7-0f91-483c-aa1f-74c6c20a6a2f.png)'
- en: 'Figure 15.9: The effect of multiplying a one-hot vector with almost all zeros
    by hidden layer weight matrix'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9：将一个几乎全为零的一热向量与隐藏层权重矩阵相乘的效果
- en: To avoid this huge waste of computation, we will be using embeddings, which
    is just a fully-connected layer with some embedding weights. In this layer, we
    skip this inefficient multiplication and look up the embedding weights of the
    embedding layer from something called **weight matrix**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种巨大的计算浪费，我们将使用嵌入技术，它仅仅是一个带有嵌入权重的全连接层。在这一层中，我们跳过了低效的乘法操作，而是通过所谓的**权重矩阵**来查找嵌入层的嵌入权重。
- en: So, instead of the waste that results from the computation, we are going to
    use this weight lookup this weight matrix to find the embedding weights. First,
    need to build this lookup take. To do this, we are going to encode all the input
    words as integers, as shown in the following figure, and then to get the corresponding
    values for this word, we are going to use its integer representation as the row
    number in this weight matrix. The process of finding the corresponding embedding
    values of a specific word is called **embedding lookup.** As mentioned previously,
    the embedding layer will be just a fully connected layer, where the number of
    units represents the embedding dimension.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为了避免计算时产生的浪费，我们将使用这个权重查找矩阵来查找嵌入权重。首先，需要构建这个查找表。为此，我们将所有输入词编码为整数，如下图所示，然后为了获取该词的对应值，我们将使用其整数表示作为该权重矩阵中的行号。找到特定词汇对应嵌入值的过程称为**嵌入查找**。如前所述，嵌入层只是一个全连接层，其中单元的数量代表嵌入维度。
- en: '![](img/489f39af-fc62-473d-9b21-b34bbb90ba1d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/489f39af-fc62-473d-9b21-b34bbb90ba1d.png)'
- en: 'Figure 15.10: Tokenized lookup table'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：标记化的查找表
- en: 'You can see that this process is very intuitive and straightforward; we just
    need to follow these steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这个过程非常直观且简单；我们只需要按照这些步骤操作：
- en: Define the lookup table that will be considered as a weight matrix
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将被视为权重矩阵的查找表
- en: Define the embedding layer as a fully connected hidden layer with specific number
    of units (embedding dimensions)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将嵌入层定义为具有特定数量单元（嵌入维度）的全连接隐藏层
- en: Use the weight matrix lookup as an alternative for the computationally unnecessary
    matrix multiplication
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用权重矩阵查找作为避免不必要的矩阵乘法的替代方案
- en: Finally, train the lookup table as any weight matrix
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将查找表作为任何权重矩阵进行训练
- en: As we mentioned earlier, we are going to build a skip-gram Word2Vec model in
    this section, which is an efficient way of learning a representation for words
    while preserving the semantic information that the words have.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将在本节中构建一个跳字模型的Word2Vec，这是学习词语表示的一种高效方式，同时保持词语的语义信息。
- en: So, let's go ahead and build a Word2Vec model using the skip-gram architecture, which
    is proven to better than others.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续构建一个使用跳字架构的Word2Vec模型，它已被证明优于其他模型。
- en: Data analysis and pre-processing
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析与预处理
- en: In this section, we are going to define some helper functions that will enable
    us to build a good Word2Vec model. For this implementation, we are going to use
    a cleaned version of Wikipedia ([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将定义一些辅助函数，以帮助我们构建一个良好的Word2Vec模型。为了实现这一目标，我们将使用清理过的维基百科版本（[http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)）。
- en: 'So, let''s start off by importing the required packages for this implementation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们从导入实现所需的包开始：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next up, we are going to define a class that will be used to download the dataset
    if it was not downloaded before:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个类，用于在数据集未下载时进行下载：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can have a look at the first 100 characters of this dataset:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看该数据集的前100个字符：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next up, we are going to preprocess the text, so we are going to define a helper
    function that will help us to replace special characters such as punctuation ones
    into a know token. Also, to reduce the amount of noise in the input text, you
    might want to remove words that don''t appear frequently in the text:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对文本进行预处理，因此我们将定义一个辅助函数，帮助我们将标点等特殊字符替换为已知的标记。此外，为了减少输入文本中的噪音，您可能还想去除那些在文本中出现频率较低的单词：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s call this function on the input text and have a look at the output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在输入文本上调用这个函数，并查看输出：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s see how many words and distinct words we have for the pre-processed
    version of the text:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在处理过的文本中有多少个单词和不同的单词：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And here, I'm creating dictionaries to covert words to integers and backwards,
    that is, integers to words. The integers are assigned in descending frequency
    order, so the most frequent word (`the`) is given the integer `0`, the next most
    frequent gets `1`, and so on. The words are converted to integers and stored in
    the list `int_words`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我正在创建字典，将单词转换为整数并反向转换，即将整数转换为单词。这些整数按频率降序排列，因此出现频率最高的单词（`the`）被赋予整数`0`，接下来频率次高的得到`1`，以此类推。单词被转换为整数并存储在列表`int_words`中。
- en: As mentioned earlier in this section, we need to use the integer indexes of
    the words to look up their values in the weight matrix, so we are going to words
    to integers and integers to words. This will help us to look up the words and
    also get the actual word of a specific index. For example, the most repeated word
    in the input text will be indexed at position 0, followed by the second most repeated
    one, and so on.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节前面提到的，我们需要使用单词的整数索引来查找它们在权重矩阵中的值，因此我们将单词转换为整数，并将整数转换为单词。这将帮助我们查找单词，并且获取特定索引的实际单词。例如，输入文本中最常出现的单词将被索引为位置0，接下来是第二常出现的单词，以此类推。
- en: 'So, let''s define a function to create this lookup table:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们定义一个函数来创建这个查找表：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s call the defined function to create the lookup table:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们调用已定义的函数来创建查找表：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To build a more accurate model, we can remove words that don''t change the
    context much  as `of`, `for`, `the`, and so on. So, it is practically proven that
    we can build more accurate models while discarding these kinds of words. The process
    of removing context-irrelevant words from the context is called **subsampling**.
    In order to define a general mechanism for word discarding, Mikolov introduced
    a function for calculating the discard probability of a certain word, which is
    given by:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建更精确的模型，我们可以去除那些对上下文变化不大的单词，如`of`、`for`、`the`等。因此，实际上已经证明，在丢弃这些单词的情况下，我们可以构建更精确的模型。从上下文中去除与上下文无关的单词的过程被称为**子抽样**。为了定义一种通用的丢弃机制，Mikolov提出了一个函数，用于计算某个单词的丢弃概率，该概率由以下公式给出：
- en: '![](img/6e9b5ce4-d28c-4a71-aa15-1527b32e8d9b.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e9b5ce4-d28c-4a71-aa15-1527b32e8d9b.png)'
- en: 'Where:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*t* is a threshold parameter for word discarding'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t* 是单词丢弃的阈值参数'
- en: '*f(w[i])* is the frequency of a specific target word *w[i]* in the input dataset'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(w[i])* 是输入数据集中目标单词 *w[i]* 的频率'
- en: 'So, we are going to implement a helper function that will calculate the discarding
    probability of each word in the dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个辅助函数，用于计算数据集中每个单词的丢弃概率：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, we have a more refined and clean version of the input text.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个更精炼、更清晰的输入文本版本。
- en: We mentioned that the skip-gram architecture considers the context of the target
    word while producing its real-valued representation, so it defines a window around
    the target word that has size *C*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，skip-gram架构在生成目标单词的实值表示时，会考虑目标单词的上下文，因此它在目标单词周围定义了一个大小为 *C* 的窗口。
- en: Instead of treating all contextual words equally, we are going to assign less
    weight for words that are a bit far from the target word. For example, if we choose
    the size of the window to be *C = 4*, then we are going to select a random number
    *L* from the range of 1 to *C*, and then sample *L* words from the history and
    the future of the current word. For more details about this, refer to the Mikolov
    et al paper at: [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再平等地对待所有上下文单词，而是为那些距离目标单词较远的单词分配较小的权重。例如，如果我们选择窗口大小为 *C = 4*，那么我们将从 1 到
    *C* 的范围内随机选择一个数字 *L*，然后从当前单词的历史和未来中采样 *L* 个单词。关于这一点的更多细节，请参见 Mikolov 等人的论文：[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)。
- en: 'So, let''s go ahead and define this function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续定义这个函数：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Also, let''s define a generator function to generate a random batch from the
    training samples and get the contextual word for each word in that batch:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们定义一个生成器函数，从训练样本中生成一个随机批次，并为该批次中的每个单词获取上下文词：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Building the model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Next up, we are going to use the following structure to build the computational
    graph:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下结构来构建计算图：
- en: '![](img/f0d6926a-bce1-4013-82a6-f3f5ab664f81.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0d6926a-bce1-4013-82a6-f3f5ab664f81.png)'
- en: 'Figure 15.11: Model architecture'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：模型架构
- en: So, as mentioned previously, we are going to use an embedding layer that will
    try to learn a special real-valued representation for these words. Thus, the words
    will be fed as one-hot vectors. The idea is to train this network to build up
    the weight matrix.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所提到的，我们将使用一个嵌入层，尝试为这些词学习一个特殊的实数表示。因此，单词将作为 one-hot 向量输入。我们的想法是训练这个网络来构建权重矩阵。
- en: 'So, let''s start off by creating the input to our model:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们从创建模型输入开始：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The weight or embedding matrix that we are trying to build will have the following
    shape:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要构建的权重或嵌入矩阵将具有以下形状：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Also, we don't have to implement the lookup function ourselves because it's
    already available in Tensorflow: `tf.nn.embedding_lookup()`. So, it will use the
    integer encoding of the words and locate their corresponding rows in the weight
    matrix.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们不需要自己实现查找函数，因为它在 Tensorflow 中已经可用：`tf.nn.embedding_lookup()`。因此，它将使用单词的整数编码，并找到它们在权重矩阵中的对应行。
- en: 'The weight matrix will be randomly initialized from a uniform distribution:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵将从均匀分布中随机初始化：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's very inefficient to update all the embedding weights of the embedding layer
    at once. Instead of this, we will use the negative sampling technique which will
    only update the weight of the correct word with a small subset of the incorrect
    ones.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 更新嵌入层的所有权重是非常低效的。我们将采用负采样技术，它只会更新正确单词的权重，并且只涉及一个小的错误单词子集。
- en: Also, we don't have to implement this function ourselves as it's already there
    in TensorFlow **`tf.nn.sampled_softmax_loss`:**
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们不必自己实现这个函数，因为在 TensorFlow 中已经有了 **`tf.nn.sampled_softmax_loss`**：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To validate our trained model, we are going to sample some frequent or common
    words and some uncommon words and try to print our their closest set of words
    based on the learned representation of the skip-gram architecture:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们训练的模型，我们将采样一些常见的词和一些不常见的词，并尝试基于跳字模型的学习表示打印它们的最近词集：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, we have all the bits and pieces for our model and we are ready to kick
    off the training process.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经拥有了模型的所有组成部分，准备开始训练过程。
- en: Training
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'Let''s go ahead and kick off the training process:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续启动训练过程：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After running the preceding code snippet for 10 epochs, you will get the following
    output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码片段 10 个周期后，您将得到以下输出：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see from the output, the network somehow learned some semantically
    useful representation of the input words. To help us get a clearer picture of
    the embedding matrix, we are going to use a dimensionality reduction technique
    such as t-SNE to reduce the real-valued vectors to two dimensions, and then we''ll
    visualize them and label each point with its corresponding word:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，网络在某种程度上学习到了输入单词的一些语义有用的表示。为了帮助我们更清楚地看到嵌入矩阵，我们将使用降维技术，如 t-SNE，将实数值向量降至二维，然后我们将对它们进行可视化，并用相应的单词标记每个点：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/e7cd06e7-0156-4b62-84f1-7650c2c067d7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7cd06e7-0156-4b62-84f1-7650c2c067d7.png)'
- en: 'Figure 15.12: A visualization of word vectors'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：词向量的可视化
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through the idea of representation learning and why
    it's useful for doing deep learning or machine learning in general on input that's
    not in a real-valued form. Also, we covered one of the adopted techniques for
    converting words into real-valued vectors—Word2Vec—which has very interesting
    properties. Finally, we implemented the Word2Vec model using the skip-gram architecture.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了表示学习的概念以及它为什么对深度学习或机器学习（尤其是对非实数形式的输入）非常有用。此外，我们还讲解了将单词转换为实数向量的一种常用技术——Word2Vec，它具有非常有趣的特性。最后，我们使用skip-gram架构实现了Word2Vec模型。
- en: Next up, you will see the practical use of these learned representations in
    a sentiment analysis example, where we need to convert the input text to real-valued
    vectors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将看到这些学习到的表示在情感分析示例中的实际应用，在该示例中，我们需要将输入文本转换为实数向量。
