- en: Generative Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: In the previous two chapters ([Chapter 4](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),*Advanced
    Convolutional Networks*, and [Chapter 5](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),
    *Object Detection and Image Segmentation*), we focused on supervised computer
    vision problems, such as classification and object detection. In this chapter,
    we'll discuss how to create new images with the help of unsupervised neural networks. After
    all, it's a lot better knowing that you don't need labeled data. More specifically,
    we'll talk about generative models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章（[第4章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)，*高级卷积网络*，和[第5章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)，*物体检测与图像分割*）中，我们专注于监督学习的计算机视觉问题，如分类和物体检测。在本章中，我们将讨论如何借助无监督神经网络来生成新图像。毕竟，知道不需要标注数据是非常有优势的。更具体地说，我们将讨论生成模型。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Intuition and justification of generative models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型的直觉与理论依据
- en: Introduction to **Variational Autoencoders** (**VAEs**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAEs**）介绍'
- en: Introduction to **Generative Adversarial Networks** (**GANs**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）介绍'
- en: Types of GAN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的类型
- en: Introducing to artistic style transfer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艺术风格迁移介绍
- en: Intuition and justification of generative models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型的直觉与理论依据
- en: 'So far, we''ve used neural networks as **discriminative models**. This simply means that,
    given input data, a discriminative model will map it to a certain label (in other
    words, a classification). A typical example is the classification of MNIST images
    in 1 of 10 digit classes, where the neural network maps input data features (pixel
    intensities) to the digit label. We can also say this in another way: a discriminative model gives
    us the probability of [![](img/b238f670-d7e7-44c2-8206-3eef71a5182a.png)] (class),
    given [![](img/d7261168-d220-4861-80e7-8703f5e681a0.png)] (input). In the case
    of MNIST, this is the probability of the digit when given the pixel intensities
    of the image.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将神经网络作为**判别模型**使用。这意味着，给定输入数据，判别模型将其映射到某个标签（换句话说，就是分类）。一个典型的例子是将MNIST图像分类到10个数字类别中，神经网络将输入数据特征（像素强度）映射到数字标签。我们也可以换一种方式来说：判别模型给我们的是[![](img/b238f670-d7e7-44c2-8206-3eef71a5182a.png)]（类别）的概率，给定[![](img/d7261168-d220-4861-80e7-8703f5e681a0.png)]（输入）。以MNIST为例，这就是给定图像的像素强度时，数字的概率。
- en: On the other hand, a generative model learns how classes are distributed. You
    can think of it as the opposite of what the discriminative model does. Instead
    of predicting the class probability, [![](img/2a4f73c5-6ad5-4785-b1aa-0d2305853f43.png)],
    given certain input features, it tries to predict the probability of the input
    features when given a class, [![](img/0f43e650-3720-4ac8-a1a3-f729a3edd573.png) ]- [![](img/74f689dd-1cca-47e1-9348-0b06933bde17.png)].
    For example, a generative model will be able to create an image of a handwritten
    digit when given the digit class. Since we only have 10 classes, it will be able
    to generate just 10 images. However, we've only used this example to illustrate
    this concept. In reality, the [![](img/6e740484-c601-46eb-9648-9c3ca3d93f76.png)] *class*
    could be an arbitrary tensor of values, and the model would be able to generate
    an unlimited number of images with different features. If you don't understand
    this now, don't worry; we'll look at many examples throughout this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成模型学习的是类别如何分布。你可以把它看作是与判别模型所做的事情相反的过程。生成模型不是预测类别概率，而是给定某些输入特征时，尝试预测在给定类别下输入特征的概率， [![](img/2a4f73c5-6ad5-4785-b1aa-0d2305853f43.png)]， [![](img/0f43e650-3720-4ac8-a1a3-f729a3edd573.png)] - [![](img/74f689dd-1cca-47e1-9348-0b06933bde17.png)]。例如，当给定数字类别时，生成模型能够生成手写数字的图像。由于我们只有10个类别，它将只能生成10张图像。然而，我们仅用这个例子来说明这一概念。实际上， [![](img/6e740484-c601-46eb-9648-9c3ca3d93f76.png)] *类别*可以是任意值的张量，模型将能够生成具有不同特征的无限数量的图像。如果你现在不理解这点，不用担心；我们将在本章中查看许多示例。
- en: Throughout this chapter, we'll denote probability distribution with a lower-case
    *p*, rather than the usual upper-case *P* that we used in the previous chapters.
    We are doing this to follow the convention that has been established in the context
    of VAEs and GANs. While writing this book, I couldn't find a definitive reason
    to use lower-case, but one possible explanation is that *P* denotes the probability
    of events, while *p* denotes the probability of the mass (or density) functions
    of a random variable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将用小写的*p*表示概率分布，而不是之前章节中使用的通常的大写*P*。这样做是为了遵循在变分自编码器（VAE）和生成对抗网络（GANs）中已建立的惯例。写这本书时，我没有找到明确的理由使用小写字母，但一个可能的解释是，*P*表示事件的概率，而*p*表示随机变量的质量（或密度）函数的概率。
- en: Two of the most popular ways to use neural networks in a generative way is via
    VAEs and GANs. In the next section, we'll introduce VAEs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络进行生成的最流行方法之一是通过VAE和GAN。接下来，我们将介绍VAE。
- en: Introduction to VAEs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VAE简介
- en: To understand VAEs, we need to talk about regular autoencoders. An autoencoder
    is a feed-forward neural network that tries to reproduce its input. In other words,
    the target value (label) of an autoencoder is equal to the input data, **y***^i =*
    **x***^i*, where *i* is the sample index. We can formally say that it tries to
    learn an identity function, [![](img/1f499b17-4a29-411c-b7cc-4102c4f87f1c.png)] (a
    function that repeats its input). Since our labels are just input data, the autoencoder
    is an unsupervised algorithm.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解VAE，我们需要谈谈常规的自编码器。自编码器是一个前馈神经网络，试图重建其输入。换句话说，自编码器的目标值（标签）等于输入数据，**y***^i =*
    **x***^i*，其中*i*是样本索引。我们可以正式地说，它试图学习一个恒等函数，[![](img/1f499b17-4a29-411c-b7cc-4102c4f87f1c.png)]（一个重复其输入的函数）。由于我们的标签只是输入数据，自编码器是一种无监督算法。
- en: 'The following diagram represents an autoencoder:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下图表示了一个自编码器：
- en: '![](img/7ab06d46-2566-4471-ad60-891eeedf7a21.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ab06d46-2566-4471-ad60-891eeedf7a21.png)'
- en: An autoencoder
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'An autoencoder consists of input, hidden (or bottleneck), and output layers.
    Similar to U-Net ([Chapter 4](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)*, Object
    Detection and Image Segmentation*), we can think of the autoencoder as a virtual
    composition of two components:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器由输入层、隐藏（或瓶颈）层和输出层组成。类似于U-Net（[第4章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)*，目标检测和图像分割*），我们可以将自编码器视为两个组件的虚拟组合：
- en: '**Encoder**: Maps the input data to the network''s internal representation.
    For the sake of simplicity, in this example the encoder is a single, fully connected
    hidden bottleneck layer. The internal state is just its activation vector. In
    general, the encoder can have multiple hidden layers, including convolutional
    ones.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：将输入数据映射到网络的内部表示。为了简单起见，在这个例子中，编码器是一个单一的、全连接的瓶颈隐藏层。内部状态就是它的激活向量。一般来说，编码器可以有多个隐藏层，包括卷积层。'
- en: '**Decoder**: Tries to reconstruct the input from the network''s internal data
    representation. The decoder can also have a complex structure that typically mirrors
    the encoder. While U-Net tries to translate the input image into a target image
    of some other domain (for example, a segmentation map), the autoencoder simply
    tries to reconstruct its input.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：试图从网络的内部数据表示中重建输入。解码器也可以有一个复杂的结构，通常与编码器相对称。虽然U-Net试图将输入图像转换为另一个领域的目标图像（例如，分割图），但自编码器只是简单地试图重建其输入。'
- en: We can train the autoencoder by minimizing a loss function, which is known as
    the **reconstruction** **error**,** [![](img/59cf2229-1f46-4abc-9e7f-12cf96347b03.png)]**.
    It measures the distance between the original input and its reconstruction. We
    can minimize it in the usual way, that is, with gradient descent and backpropagation.
    Depending on the approach we use, we can use either use **mean square error**
    (**MSE**) or binary cross-entropy (such as cross-entropy, but with two classes)
    as reconstruction errors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过最小化损失函数来训练自编码器，这个损失函数称为**重建** **误差**，[![](img/59cf2229-1f46-4abc-9e7f-12cf96347b03.png)]。它衡量原始输入和其重建之间的距离。我们可以像通常那样，通过梯度下降和反向传播来最小化它。根据我们使用的方法，既可以使用**均方误差**（**MSE**），也可以使用二元交叉熵（比如交叉熵，但只有两个类别）作为重建误差。
- en: 'At this point, you may be wondering what the point of the autoencoder is since
    it just repeats its input. However, we aren''t interested in the network output,
    but in its internal data representation (which is also known as representation
    in the **latent space**). The latent space contains hidden data features that
    are not directly observed but are inferred by the algorithm instead. The key is
    that the bottleneck layer has fewer neurons than the input/output ones. There
    are two main reasons for this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能会想，既然自编码器只是重复它的输入，那么它的意义何在？然而，我们并不关心网络的输出，而是它的内部数据表示（也被称为**潜在空间**中的表示）。潜在空间包含那些不是直接观察到的隐藏数据特征，而是由算法推断出来的。关键在于瓶颈层的神经元数量少于输入/输出层的神经元数量。这样做有两个主要原因：
- en: Because the network tries to reconstruct its input from a smaller feature space,
    it learns a compact representation of the data. You can think of this as compression
    (but not lossless).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为网络尝试从较小的特征空间中重建输入数据，它学习到了数据的紧凑表示。你可以将其视为压缩（但不是无损的）。
- en: 'By using fewer neurons, the network is forced to learn only the most important
    features of the data. To illustrate this concept, let''s look at denoising autoencoders,
    where we intentionally use corrupted input data, but non-corrupted target data
    during training. For example, if we train a denoising autoencoder to reconstruct
    MNIST images, we can introduce noise by setting the max intensity (white) to random
    pixels of the image (as shown in the following screenshot). To minimize the loss
    with the noiseless target, the autoencoder is forced to look beyond the noise
    in the input and learn only the important features of the data. However, if the
    network had more hidden neurons than input, it might overfit on the noise. With
    the additional constraint of fewer hidden neurons, it can only try to ignore the
    noise. Once trained, we can use a denoising autoencoder to remove the noise from
    real images:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用更少的神经元，网络被迫仅学习数据中最重要的特征。为了说明这一概念，我们来看去噪自编码器（denoising autoencoder），在训练过程中，我们故意使用损坏的输入数据，但目标数据保持不受损坏。例如，如果我们训练一个去噪自编码器来重建MNIST图像，我们可以通过将最大强度（白色）设置为图像中的随机像素来引入噪声（如下图所示）。为了最小化与无噪声目标之间的损失，去噪自编码器被迫超越输入中的噪声，只学习数据的关键特征。然而，如果网络的隐藏神经元比输入神经元多，它可能会过拟合噪声。通过减少隐藏神经元的数量这一额外约束，网络只能尝试忽略噪声。训练完成后，我们可以使用去噪自编码器从真实图像中去除噪声：
- en: '![](img/7e476cb7-0952-4ef5-a08c-0778be517c82.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e476cb7-0952-4ef5-a08c-0778be517c82.png)'
- en: Denoising autoencoder input and target
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器输入和目标
- en: The encoder maps each input sample to the latent space, where each attribute
    of the latent representation has a discrete value. This means that an input sample
    can have only one latent representation. Therefore, the decoder can reconstruct
    the input in only one possible way. In other words, we can generate a single reconstruction
    of one input sample. But we don't want this. Instead, we want to generate new
    images that are different from the original ones. VAEs are one possible solution
    to this task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器将每个输入样本映射到潜在空间，在那里潜在表示的每个属性都有一个离散的值。这意味着一个输入样本只能拥有一个潜在表示。因此，解码器只能以一种可能的方式重建输入。换句话说，我们只能生成一个输入样本的单一重建。但我们并不想要这样。相反，我们希望生成与原始图像不同的新图像。变分自编码器（VAE）是实现这一任务的一个可能解决方案。
- en: A VAE can describe a latent representation in probabilistic terms. That is,
    instead of discrete values, we'll have a probability distribution for each latent
    attribute, making the latent space continuous. This makes it easier for random
    sampling and interpolation. Let's illustrate this with an example. Imagine that
    we are trying to encode an image of a vehicle and our latent representation is
    a vector, **z**, with *n* elements (*n* neurons in the bottleneck layer). Each
    element represents one vehicle property, such as length, height, and width (as
    shown in the following diagram).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: VAE可以用概率术语描述潜在表示。也就是说，我们将为每个潜在属性生成一个概率分布，而不是离散值，从而使潜在空间变得连续。这使得随机采样和插值变得更加容易。我们通过一个例子来说明这一点。假设我们正在尝试编码一辆车辆的图像，而我们的潜在表示是一个向量**z**，它包含*n*个元素（瓶颈层中的*n*个神经元）。每个元素表示车辆的一个属性，例如长度、高度和宽度（如下图所示）。
- en: 'Say that the average vehicle length is four meters. Instead of the fixed value,
    the VAE can decode this property as a normal distribution with a mean of 4 (the
    same applies for the others). Then, the decoder can choose to sample a latent
    variable from the range of its distribution. For example, it can reconstruct a
    longer and lower vehicle compared to the input. By doing this, the VAE can generate
    an unlimited number of modified versions of the input:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设平均车辆长度是四米。VAE并不是用固定值表示，而是将这个属性解码为均值为4的正态分布（其他属性也适用）。然后，解码器可以选择从该分布的范围内采样一个潜在变量。例如，它可以重建一个比输入更长、更低的车辆。通过这种方式，VAE可以生成输入的无限多个修改版本：
- en: '![](img/ca7963b1-63c9-4dfe-acec-3412d9b389b4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca7963b1-63c9-4dfe-acec-3412d9b389b4.png)'
- en: An example of a variational encoder sampling different values from the distribution
    ranges of the latent variables
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个变分编码器从潜在变量的分布范围中采样不同值的示例
- en: 'Let''s formalize this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式化这个过程：
- en: The goal of the encoder is to approximate the real probability distribution, ![](img/1a3c9132-2603-4a19-a100-3b9ffb3f270a.png),
    where **z** is the latent space representation. However, it does so indirectly
    by inferring ![](img/731217cc-21d3-498b-a99e-66c5d60d5ddb.png) from the conditional
    probability distribution of various samples, ![](img/f9ae7213-2ff8-4851-b65a-ac6c51503a03.png),
    where **x** is the input data. In other words, the encoder tries to learn the
    probability distribution of **z**, given the input data, **x**. We'll denote the
    encoder's approximation of ![](img/447524c7-6c0e-4a74-9e66-2351ba3595f7.png) with
    ![](img/987d3cac-ffcf-4c5c-8763-91cc83c75624.png), where *φ* are the weights of
    the network. The encoder output is a probability distribution (for example, Gaussian)
    over the possible values of **z**, which could have been generated by **x**. During
    training, we continuously update the weights, *φ*, to bring ![](img/73ef9a85-0440-413a-a935-5a8a41a5e66d.png)closer
    to the real ![](img/c80bd4e2-4ee1-4571-93ed-63d288c45a17.png)*.*
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的目标是近似真实的概率分布，![](img/1a3c9132-2603-4a19-a100-3b9ffb3f270a.png)，其中**z**是潜在空间的表示。然而，编码器通过从不同样本的条件概率分布推断![](img/731217cc-21d3-498b-a99e-66c5d60d5ddb.png)间接地实现这一目标，![](img/f9ae7213-2ff8-4851-b65a-ac6c51503a03.png)，其中**x**是输入数据。换句话说，编码器试图学习在给定输入数据**x**的情况下，**z**的概率分布。我们将编码器对![](img/447524c7-6c0e-4a74-9e66-2351ba3595f7.png)的近似表示为![](img/987d3cac-ffcf-4c5c-8763-91cc83c75624.png)，其中*φ*是网络的权重。编码器的输出是一个概率分布（例如，高斯分布），表示可能由**x**生成的**z**的所有值。在训练过程中，我们不断更新权重*φ*，使![](img/73ef9a85-0440-413a-a935-5a8a41a5e66d.png)更接近真实的![](img/c80bd4e2-4ee1-4571-93ed-63d288c45a17.png)*。
- en: The goal of the decoder is to approximate the real probability distribution, ![](img/e6ee0e0b-6516-405c-8cd4-d91e35893f01.png). In
    other words, the decoder tries to learn the conditional probability distribution
    of the data, *x*, given the latent representation, **z**. We'll denote the decoder's
    approximation of the real probability distribution with ![](img/3087226a-ff1d-475b-b8a9-645b704cb330.png) ,
    where *θ* is the decoder weights. The process starts by sampling **z** stochastically
    (randomly) from the probability distribution (for example, Gaussian). Then, **z**
    is sent through the decoder, whose output is a probability distribution over the
    possible corresponding values of *x*. During training, we continuously update
    the weights, *θ*, to bring ![](img/063bdfe0-14c3-4117-96d6-858379046b64.png)closer
    to the real ![](img/bdb62cc6-5642-48ba-80cf-367f36e30fdd.png)*.*
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的目标是近似真实的概率分布，![](img/e6ee0e0b-6516-405c-8cd4-d91e35893f01.png)。换句话说，解码器试图学习在给定潜在表示**z**的情况下，数据*x*的条件概率分布。我们将解码器对真实概率分布的近似表示为![](img/3087226a-ff1d-475b-b8a9-645b704cb330.png)，其中*θ*是解码器的权重。这个过程从随机（或称为随机采样）地从概率分布（例如高斯分布）中采样**z**开始。然后，**z**会传递到解码器，通过解码器的输出生成可能的对应*x*值的概率分布。在训练过程中，我们不断更新权重*θ*，使![](img/063bdfe0-14c3-4117-96d6-858379046b64.png)更接近真实的![](img/bdb62cc6-5642-48ba-80cf-367f36e30fdd.png)*。
- en: 'The VAE uses a special type of loss function with two terms:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: VAE使用一种特殊类型的损失函数，该函数包含两个项：
- en: '![](img/375738b8-6acf-4617-b579-8eb9ff731667.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/375738b8-6acf-4617-b579-8eb9ff731667.png)'
- en: The first is the Kullback-Leibler divergence ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*) between the probability distribution, ![](img/0a69f896-e2f3-4a73-a147-e7e14a41870f.png), and
    the expected probability distribution, ![](img/feeaa04b-54f9-4ce2-b225-29dbcaf4d4a8.png).
    In this context, it measures how much information is lost when we use ![](img/0141d506-ca77-486d-9793-10b0cf619839.png) to
    represent ![](img/03768c4c-840a-46b2-a849-0b935a8c36ea.png) (in other words, how
    close the two distributions are). It encourages the autoencoder to explore different
    reconstructions. The second is the reconstruction loss, which measures the difference
    between the original input and its reconstruction. The more they differ, the more
    it increases. Therefore, it encourages the autoencoder to reconstruct data in
    a better way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是Kullback-Leibler散度（[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*）在概率分布之间，![](img/0a69f896-e2f3-4a73-a147-e7e14a41870f.png)，和预期的概率分布，![](img/feeaa04b-54f9-4ce2-b225-29dbcaf4d4a8.png)之间的差异。在这个背景下，它衡量了当我们使用![](img/0141d506-ca77-486d-9793-10b0cf619839.png)表示![](img/03768c4c-840a-46b2-a849-0b935a8c36ea.png)时丢失了多少信息（换句话说，两者分布的接近程度）。它鼓励自编码器探索不同的重建方式。第二个是重建损失，衡量原始输入和其重建之间的差异。差异越大，损失越大。因此，它鼓励自编码器以更好的方式重建数据。
- en: 'To implement this, the bottleneck layer won''t directly output latent state
    variables. Instead, it will output two vectors, which describe the **mean** and **variance** of
    the distribution of each latent variable:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，瓶颈层不会直接输出潜在状态变量。相反，它将输出两个向量，这两个向量描述了每个潜在变量的分布的**均值**和**方差**：
- en: '![](img/0d2e5729-4a36-4ee0-b2e3-4bd93d3a5c2b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d2e5729-4a36-4ee0-b2e3-4bd93d3a5c2b.png)'
- en: Variational encoder sampling
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 变分编码器采样
- en: 'Once we have the mean and variance distributions, we can sample a state, **z**,
    from the latent variable distributions and pass it through the decoder for reconstruction.
    But we can''t celebrate yet. This presents us with another problem: backpropagation
    doesn''t work over random processes such as the one we have here. Fortunately,
    we can solve this with the so-called **reparameterization trick**. First, we''ll sample a
    random vector, ε, with the same dimensions as **z** from a Gaussian distribution
    (the ε circle in the preceding diagram). Then, we''ll shift it by the latent distribution''s
    mean, μ, and scale it by the latent distribution''s variance, σ:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得均值和方差分布，就可以从潜在变量分布中采样一个状态**z**，并将其传递通过解码器进行重建。但我们还不能庆祝，这给我们带来了另一个问题：反向传播无法应用于像我们这里这种随机过程。幸运的是，我们可以通过所谓的**重参数化技巧**来解决这个问题。首先，我们将从一个高斯分布中采样一个与**z**维度相同的随机向量**ε**（前面图中的**ε**圆）。然后，我们将其偏移潜在分布的均值**μ**，并按潜在分布的方差**σ**进行缩放：
- en: '![](img/966588f8-306a-4935-b6e9-9ddfa2b931e5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/966588f8-306a-4935-b6e9-9ddfa2b931e5.png)'
- en: In this way, we'll be able to optimize the mean and variance (red arrows) and
    we'll omit the random generator from the backward pass. At the same time, the
    sampled data will have the properties of the original distribution. Now that we've
    introduced VAEs, we'll learn how to implement one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们将能够优化均值和方差（红色箭头），并且我们将在反向传播中省略随机生成器。同时，采样数据将具有原始分布的属性。现在我们已经介绍了VAE，接下来我们将学习如何实现一个VAE。
- en: Generating new MNIST digits with VAE
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VAE生成新的MNIST数字
- en: In this section, we'll learn how a VAE can generate new digits for the MNIST dataset.
    We'll use Keras under TF 2.0.0 to do so. We chose MNIST because it will illustrate
    VAE's generative capabilities well.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将学习如何使用VAE生成MNIST数据集的新数字。我们将使用TF 2.0.0下的Keras来实现。我们选择MNIST是因为它能够很好地展示VAE的生成能力。
- en: The code in this section is partially based on [https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码部分基于[https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py)。
- en: 'Let''s go through the implementation, step by step:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地走过实现过程：
- en: 'Let''s start with the imports. We''ll use the Keras module, which is integrated
    in TF:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从导入开始。我们将使用集成在TF中的Keras模块：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we will instantiate the MNIST dataset. Recall that in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)*,
    Understanding Convolutional Networks*, we implemented a transfer learning example
    with TF/Keras, where we used the `tensorflow_datasets` module to load the CIFAR-10
    dataset. In this example, we''ll use the `keras.datasets` module to load MNIST,
    which also works:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实例化 MNIST 数据集。回想一下，在 [第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)
    中的 *理解卷积网络* 部分，我们使用 TF/Keras 实现了一个迁移学习的示例，并使用 `tensorflow_datasets` 模块加载了 CIFAR-10
    数据集。在这个例子中，我们将使用 `keras.datasets` 模块加载 MNIST，这同样适用：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we''ll implement the `build_vae` function, which will build the VAE:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `build_vae` 函数，该函数将构建 VAE：
- en: We'll have separate access to the encoder, decoder, and the full network. The
    function will return them as a tuple.
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将分别访问编码器、解码器和完整的网络。该函数将它们作为元组返回。
- en: The bottleneck layer will have only `2` neurons (that is, we'll have only `2` latent
    variables). In this way, we'll be able to display the latent distribution as a
    2D plot.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓶颈层将只有 `2` 个神经元（即，我们将只有 `2` 个潜在变量）。这样，我们就能够将潜在分布显示为二维图。
- en: The encoder/decoder will contain a single intermediate (hidden) fully-connected
    layer with `512` neurons. This is not a convolutional network.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器/解码器将包含一个具有 `512` 个神经元的单个中间（隐藏）全连接层。这不是一个卷积网络。
- en: We'll use cross-entropy reconstruction loss and KL divergence.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用交叉熵重建损失和 KL 散度。
- en: 'The following shows how this is implemented globally:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了这一全球实现方式：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Immediately tied to the network definition is the `sampling` function, which
    implements a random sampling of latent vectors `z` from the Gaussian unit (this
    is the reparameterization trick we introduced in the *Introduction to VAEs* section):'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与网络定义直接相关的是 `sampling` 函数，它实现了从高斯单位随机采样潜在向量 `z`（这是我们在 *VAE 简介* 部分介绍的重参数化技巧）：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we need to implement the `plot_latent_distribution` function. It collects
    the latent representations of all the images in the test set and displays them
    over a 2D plot. We can do this because our network has only two latent variables
    (for the two axes of the plot). Note that to implement this we only need the `encoder`:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要实现 `plot_latent_distribution` 函数。它收集所有测试集图像的潜在表示，并将其显示在二维图上。我们之所以能够这样做，是因为我们的网络只有两个潜在变量（对应图的两个轴）。请注意，为了实现这一点，我们只需要
    `encoder`：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will implement the `plot_generated_images` function. It will sample
    `n*n` vectors, `z`, in a `[-4, 4]` range for each of the two latent variables.
    Next, it will generate images based on the sampled vectors and display them in
    a 2D grid. Note that to do this we only need the `decoder`:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `plot_generated_images` 函数。它将在 `[-4, 4]` 范围内为每个潜在变量 `z` 采样 `n*n` 个向量。然后，它将基于这些采样的向量生成图像，并在二维网格中显示。请注意，为了做到这一点，我们只需要
    `decoder`：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, run the entirety of the code. We''ll use the Adam optimizer (introduced
    in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The Nuts and Bolts
    of Neural Networks*) to train the network for 50 epochs:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，运行整个代码。我们将使用 Adam 优化器（在 [第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中介绍的，*神经网络的基础*）训练网络
    50 个周期：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If everything goes to plan, once the training is over, we''ll see the latent
    distribution for each digit class for all the test images. The left and bottom
    axes represent the `z[1]` and `z[2]` latent variables. Different marker shapes
    represent different digit classes:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一切顺利，训练完成后，我们将看到每个数字类别的潜在分布图，适用于所有测试图像。左轴和底轴表示 `z[1]` 和 `z[2]` 潜在变量。不同的标记形状代表不同的数字类别：
- en: '![](img/f973f0aa-9c8b-4034-ada5-6c70f9d98baa.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f973f0aa-9c8b-4034-ada5-6c70f9d98baa.png)'
- en: The latent distributions of the MNIST test images
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 测试图像的潜在分布
- en: 'Next, we''ll look at the images that were generated by `plot_generated_images`.
    The axes represent the particular latent distribution, `z`, that was used for
    each image:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看由 `plot_generated_images` 生成的图像。坐标轴代表用于每张图像的特定潜在分布 `z`：
- en: '![](img/9b6f48b1-6347-4cc5-a5d2-ce4e5f088f34.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b6f48b1-6347-4cc5-a5d2-ce4e5f088f34.png)'
- en: Images generated by the VAE
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 生成的图像
- en: This concludes our description of VAEs. In the next section, we'll discuss GANs—arguably
    the most popular family of generative models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对 VAE 的描述的结束。接下来，我们将讨论 GANs——可以说是最流行的生成模型家族。
- en: Introduction to GANs
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs 简介
- en: 'In this section, we''ll talk about arguably the most popular generative model
    today: the GAN framework. It was first introduced in 2014 in the landmark paper *Generative
    Adversarial Nets *([http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)).
    The GAN framework can work with any type of data, but its most popular application
    by far is to generate images, and we''ll discuss them in this context only. Let''s
    see how it works:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论今天最受欢迎的生成模型之一：GAN框架。它首次出现在2014年的标志性论文《*生成对抗网络*》中([http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf))。GAN框架可以处理任何类型的数据，但它最受欢迎的应用无疑是生成图像，我们也将在这个背景下讨论它的应用。让我们看看它是如何工作的：
- en: '![](img/7c414189-7fd7-4213-8d98-b6d11719c1f8.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c414189-7fd7-4213-8d98-b6d11719c1f8.png)'
- en: A GAN system
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个GAN系统
- en: 'A GAN is a system of two components (neural networks):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是由两个组件（神经网络）组成的系统：
- en: '**Generator**: This is the generative model itself. It takes a probability distribution
    (random noise) as input and tries to generate a realistic output image. Its purpose
    is similar to the decoder part of the VAE.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**：这就是生成模型本身。它以一个概率分布（随机噪声）作为输入，并尝试生成一个逼真的输出图像。它的目的类似于VAE的解码器部分。'
- en: '**Discriminator**: This takes two alternating inputs: real images of the training
    dataset or generated fake samples from the generator. It tries to determine whether
    the input image comes from the real images or the generated ones.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器**：它接受两个交替的输入：训练数据集中的真实图像或生成器生成的假样本。它试图判断输入图像是来自真实图像还是生成的图像。'
- en: The two networks are trained together as a system. On the one hand, the discriminator
    tries to get better at distinguishing between real and fake images. On the other
    hand, the generator tries to output more realistic images so that it can *deceive*
    the discriminator into thinking that the generated images are real. To use the
    analogy in the original paper, you can think of the generator as a team of counterfeiters,
    trying to produce fake currency. Conversely, the discriminator acts as a police
    officer, trying to capture the fake money, and the two are constantly trying to
    deceive each other (hence the name adversarial). The ultimate goal of the system
    is to make the generator so good that the discriminator can't distinguish between
    real and fake images. Even though the discriminator performs classification, a
    GAN is still unsupervised, since we don't need labels for the images. In the next
    section, we'll discuss the process of training in the context of the GAN framework.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络是作为一个系统一起训练的。一方面，判别器尝试更好地分辨真实和虚假的图像。另一方面，生成器尝试输出更逼真的图像，以便能够*欺骗*判别器，让判别器认为生成的图像是真的。用原论文中的类比，你可以将生成器想象成一群伪造货币的人，试图制造假币。相反，判别器就像一名警察，试图抓住假币，而这两者在不断地互相欺骗（因此有了“对抗”这个名字）。系统的最终目标是使生成器变得如此出色，以至于判别器无法区分真实和虚假的图像。即使判别器执行分类任务，GAN仍然是无监督的，因为我们不需要为图像提供标签。在下一节中，我们将讨论在GAN框架下的训练过程。
- en: Training GANs
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练GAN
- en: Our main goal is for the generator to produce realistic images, and the GAN
    framework is a vehicle for that goal. We'll train the generator and the discriminator separately
    and sequentially (one after the other) and alternate between the two phases multiple
    times.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是让生成器生成逼真的图像，GAN框架是实现这个目标的工具。我们将分别和顺序地训练生成器和判别器（一个接一个），并多次交替进行这两个阶段。
- en: 'Before going into more detail, let''s use the following diagram to introduce
    some notations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍之前，让我们使用以下图示来介绍一些符号：
- en: We'll denote the generator with ![](img/bcbbfa02-24dc-4482-b07e-8dd60e1fd713.png) ,
    where ![](img/22bb18a4-d9b9-4556-a55a-c818717dac5a.png) is the network weights
    and **z** is the latent vector, which serves as an input to the generator. Think
    of it as a random seed value to kickstart the image-generation process. It is
    similar to the latent vector in VAEs. **z** has a probability distribution, ![](img/e32bcba0-3230-458b-9a75-ba2001ad4ab5.png),
    which is usually random normal or random uniform. The generator outputs fake samples,
    **x**, with a probability distribution of ![](img/3d4a4e6e-4f50-4a75-bc4b-61e2a9191eea.png).
    You can think of ![](img/7c9715b8-cb69-47a8-bcf0-b40d631cb7d4.png) as the probability
    distribution of the real data according to the generator.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用![](img/bcbbfa02-24dc-4482-b07e-8dd60e1fd713.png)来表示生成器，其中![](img/22bb18a4-d9b9-4556-a55a-c818717dac5a.png)是网络权重，**z**是潜在向量，它作为生成器的输入。可以把它看作是启动图像生成过程的随机种子值。它与变分自编码器（VAE）中的潜在向量相似。**z**具有一个概率分布![](img/e32bcba0-3230-458b-9a75-ba2001ad4ab5.png)，通常是随机正态分布或随机均匀分布。生成器输出虚假样本**x**，其概率分布为![](img/3d4a4e6e-4f50-4a75-bc4b-61e2a9191eea.png)。你可以把![](img/7c9715b8-cb69-47a8-bcf0-b40d631cb7d4.png)看作是生成器根据真实数据的概率分布。
- en: We'll denote the discriminator with ![](img/d2aeadec-72da-4c67-9ce4-3cdf4b4c2168.png),
    where ![](img/3e467c31-98b4-4b30-a4db-baec41b4d099.png) is the network weights.
    It takes either real data with the ![](img/d381bc4e-4576-48cd-8f6c-825cc69e46c4.png) distribution or
    generated samples, ![](img/e1542f41-298c-44aa-ae40-6e0fcea895de.png), as input.
    The discriminator is a binary classifier that outputs whether the input image
    is part of the real (network output 1) or the generated data (network output 0).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用![](img/d2aeadec-72da-4c67-9ce4-3cdf4b4c2168.png)来表示判别器，其中![](img/3e467c31-98b4-4b30-a4db-baec41b4d099.png)是网络权重。它的输入可以是真实数据，具有![](img/d381bc4e-4576-48cd-8f6c-825cc69e46c4.png)分布，或是生成的样本![](img/e1542f41-298c-44aa-ae40-6e0fcea895de.png)。判别器是一个二分类器，输出输入图像是否属于真实数据（网络输出1）或生成的数据（网络输出0）。
- en: During training, we'll denote the discriminator and generator loss functions
    with![](img/b9f92315-ed74-4577-a08b-781e7c1a4ddb.png) and ![](img/39905801-6028-4091-90d6-6d318e06f2ad.png) ,
    respectively.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，我们分别用![](img/b9f92315-ed74-4577-a08b-781e7c1a4ddb.png)和![](img/39905801-6028-4091-90d6-6d318e06f2ad.png)表示判别器和生成器的损失函数。
- en: 'The following is a more detailed diagram of a GAN framework:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是GAN框架的更详细的图示：
- en: '![](img/3e65a493-05b1-43fa-8f9d-4b24b32fd6e1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e65a493-05b1-43fa-8f9d-4b24b32fd6e1.png)'
- en: A detailed example of a GAN
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个详细的GAN示例
- en: 'GAN training is different compared to training a regular DNN because we have
    two networks. We can think of it as a sequential minimax zero-sum game of two
    players (generator and discriminator):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GAN训练与训练常规的深度神经网络（DNN）不同，因为我们有两个网络。我们可以把它看作是一个顺序的极小极大零和游戏，涉及两名玩家（生成器和判别器）：
- en: '**Sequential**: This means that the players take turns after one another, similar
    to chess or tic-tac-toe (as opposed to simultaneously). First, the discriminator
    tries to minimize ![](img/2c43645e-8a20-4d89-a7e4-9d0464cb1aa8.png), but it can
    only do so by adjusting the weights, ![](img/13edf7a3-eec0-46e6-8180-7dfac3b1fae0.png).
    Next, the generator tries to minimize ![](img/d7aa480b-9b21-4494-9186-b50a577d937b.png),
    but it can only adjust the weights, ![](img/7a5ffc5f-0c61-4633-9950-bbf71baff3cd.png). We
    repeat this process multiple times.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序**：这意味着玩家们依次进行轮流，就像象棋或井字游戏一样（与同时进行相对）。首先，判别器尝试最小化![](img/2c43645e-8a20-4d89-a7e4-9d0464cb1aa8.png)，但它只能通过调整权重![](img/13edf7a3-eec0-46e6-8180-7dfac3b1fae0.png)来实现这一点。接下来，生成器尝试最小化![](img/d7aa480b-9b21-4494-9186-b50a577d937b.png)，但它只能通过调整权重![](img/7a5ffc5f-0c61-4633-9950-bbf71baff3cd.png)来实现。我们会多次重复这个过程。'
- en: '**Zero-sum**: This means that the gains or losses of one player are balanced
    by the gains or losses of the opposite player. That is, the sum of the generator''s
    loss and the discriminator''s loss is always 0:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零和**：这意味着一个玩家的收益或损失由对方玩家的收益或损失来平衡。也就是说，生成器的损失和判别器的损失之和始终为0：'
- en: '![](img/9d13c209-7820-4a67-8eb5-47fab1eda918.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d13c209-7820-4a67-8eb5-47fab1eda918.png)'
- en: '**Minimax**: This means that the strategy of the first player (generator) is
    to **minimize** the opponent''s (discriminator) **maximum** score (hence the name).
    When we train the discriminator, it becomes better at distinguishing between real
    and fake samples (minimizing ![](img/3642e1bc-6d13-438e-9b8c-a6ff3f7d5eec.png)).
    Next, when we train the generator, it tries to step up to the level of the new
    and improved discriminator (we minimize ![](img/b7c64658-7d82-43a6-a7b4-68478bc67d01.png) ,
    which is equivalent to maximizing ![](img/935102ba-8da5-4087-819f-bacdebb515a6.png)).
    The two networks are in constant competition. We''ll denote the minimax game with
    the following formula, where ![](img/909d5f59-d3b0-4185-ab8f-1aa5e7eefa6e.png) is
    the loss function:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化最大化**：这意味着第一方（生成器）的策略是**最小化**对手（判别器）的**最大**分数（因此得名）。当我们训练判别器时，它变得更擅长区分真实样本和虚假样本（最小化！[](img/3642e1bc-6d13-438e-9b8c-a6ff3f7d5eec.png)）。接下来，当我们训练生成器时，它试图达到新改进的判别器的水平（我们最小化！[](img/b7c64658-7d82-43a6-a7b4-68478bc67d01.png)，这等同于最大化！[](img/935102ba-8da5-4087-819f-bacdebb515a6.png)）。这两个网络在不断竞争。我们用以下公式表示最小化最大化游戏，其中！[](img/909d5f59-d3b0-4185-ab8f-1aa5e7eefa6e.png)是损失函数：'
- en: '![](img/820971ea-bce7-4624-b374-b4c5722a920c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/820971ea-bce7-4624-b374-b4c5722a920c.png)'
- en: Let's assume that, after a number of training steps, both ![](img/47781deb-e5d3-49dd-be70-af2cf4d31eb1.png) and ![](img/35578969-ec91-43d8-902c-c900a29b99a6.png) will
    be at some local minimum. Here, the solution to the minimax game is called the
    Nash equilibrium. A Nash equilibrium happens when one of the actors doesn't change
    its action, regardless of what the other actor may do. A Nash equilibrium in a
    GAN framework happens when the generator becomes so good that the discriminator is
    no longer able to distinguish between generated and real samples. That is, the
    discriminator output will always be half, regardless of the presented input.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在经过一系列训练步骤后，![](img/47781deb-e5d3-49dd-be70-af2cf4d31eb1.png) 和 ![](img/35578969-ec91-43d8-902c-c900a29b99a6.png)
    都会达到某个局部最小值。这里，**最小化最大化**游戏的解被称为纳什均衡。纳什均衡发生在其中一个参与者的行为不再变化，无论另一个参与者如何行动。在生成对抗网络（GAN）框架中，当生成器变得足够优秀，以至于判别器无法再区分生成的样本和真实样本时，就会发生纳什均衡。也就是说，判别器的输出将始终为一半，无论输入是什么。
- en: Now that we have had an overview of GANs, let's discuss how to train them. We'll
    start with the discriminator and then we'll continue with the generator.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对GAN有了一个概览，接下来讨论如何训练它们。我们将从判别器开始，然后继续讨论生成器。
- en: Training the discriminator
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练判别器
- en: 'The discriminator is a classification neural network and we can train it in
    the usual way, that is, using gradient descent and backpropagation. However, the
    training set is composed of real and generated samples. Let''s learn how to incorporate
    that in the training process:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器是一个分类神经网络，我们可以像往常一样训练它，即使用梯度下降和反向传播。然而，训练集由真实样本和生成样本组成。让我们学习如何将这一点融入训练过程中：
- en: 'Depending on the input sample (real or fake), we have two paths:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据输入样本（真实或虚假），我们有两条路径：
- en: Select the sample from the real data, ![](img/5d97e04f-dfdf-4558-a5f9-bb2a5660b9dc.png),
    and use it to produce ![](img/3d5522f8-1f33-450b-b459-0c47b862181b.png).
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从真实数据中选择样本，![](img/5d97e04f-dfdf-4558-a5f9-bb2a5660b9dc.png)，并用它来生成![](img/3d5522f8-1f33-450b-b459-0c47b862181b.png)。
- en: Generate a fake sample, ![](img/d6b3f32c-cbcf-4eca-a9c8-b5fd2e5459d3.png). Here,
    the generator and discriminator work as a single network. We start with a random
    vector, **z**, which we use to produce the generated sample,![](img/7e3fa3c3-4644-4293-9205-05b28d7d22b3.png).
    Then, we use it as input to the discriminator to produce the final output, ![](img/7bb391a2-3cb9-46cb-bdaa-3c3897a2067c.png).
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个虚假样本，![](img/d6b3f32c-cbcf-4eca-a9c8-b5fd2e5459d3.png)。在这里，生成器和判别器作为一个单一网络工作。我们从一个随机向量**z**开始，利用它生成生成样本，![](img/7e3fa3c3-4644-4293-9205-05b28d7d22b3.png)。然后，我们将其作为输入传递给判别器，生成最终输出，![](img/7bb391a2-3cb9-46cb-bdaa-3c3897a2067c.png)。
- en: Next, we compute the loss function, which reflects the duality of the training
    data (more on that later).
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算损失函数，反映了训练数据的二重性（稍后会详细介绍）。
- en: Finally, we backpropagate the error gradient and update the weights. Although
    the two networks work together, the generator weights, ![](img/557fb571-01a9-4f5f-8ad0-df9f2ed4e42f.png),
    will be locked and we'll only update the discriminator weights, ![](img/1f4c5681-dfb1-4e14-bad0-a050832ee537.png).
    This ensures that we'll improve the discriminatory performance by making it better,
    as opposed to making the generator worse.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们反向传播误差梯度并更新权重。尽管两个网络是一起工作的，但生成器的权重 ![](img/557fb571-01a9-4f5f-8ad0-df9f2ed4e42f.png)将被锁定，我们只会更新判别器的权重 ![](img/1f4c5681-dfb1-4e14-bad0-a050832ee537.png)。这样可以确保我们通过改善判别器的性能来提高其效果，而不是使生成器变得更差。
- en: 'To understand discriminator loss, let''s recall the formula for cross-entropy
    loss:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解判别器损失，让我们回顾一下交叉熵损失的公式：
- en: '![](img/53a958ee-004a-4cb1-906a-d213fe2ea6be.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53a958ee-004a-4cb1-906a-d213fe2ea6be.png)'
- en: 'Here, ![](img/a44a36e6-b6ab-4220-9773-d625d3437bc7.png) is the estimated probability
    of the output belonging to the *i*-th class (out of *n* total classes) and ![](img/77505bad-5b15-4195-8331-f92ca37df667.png) is
    the actual probability. For the sake of simplicity, we''ll assume that we apply
    the formula over a single training sample. In the case of binary classification,
    this formula can be simplified, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/a44a36e6-b6ab-4220-9773-d625d3437bc7.png) 是输出属于第 *i* 类（在 *n* 个总类中）的估计概率，![](img/77505bad-5b15-4195-8331-f92ca37df667.png)
    是实际概率。为了简化起见，我们假设我们在单个训练样本上应用该公式。在二分类的情况下，公式可以简化如下：
- en: '![](img/a5101d66-76fb-4c3c-8aec-feb244019a92.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5101d66-76fb-4c3c-8aec-feb244019a92.png)'
- en: When the target probabilities are [![](img/976fe21a-8bf0-488f-8a35-5c6d987e44ef.png)] (one-hot-encoding),
    one of the loss terms is always *0*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标概率是[![](img/976fe21a-8bf0-488f-8a35-5c6d987e44ef.png)]（独热编码）时，损失项总是 *0*。
- en: 'We can expand the formula for a mini-batch of *m* samples:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以扩展该公式以适应一个包含 *m* 个样本的小批量：
- en: '![](img/adf53eaf-b140-4666-8e44-8d0aa532bfb1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adf53eaf-b140-4666-8e44-8d0aa532bfb1.png)'
- en: 'Knowing all this, let''s define the discriminator loss:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些之后，让我们定义判别器损失：
- en: '![](img/e2b14266-f49c-4547-bcf8-0c08f77f04e5.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2b14266-f49c-4547-bcf8-0c08f77f04e5.png)'
- en: 'Although it seems complex, this is just cross-entropy loss for a binary classifier
    with some GAN-specific bells and whistles. Let''s discuss them:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来很复杂，但这实际上只是一个二分类器的交叉熵损失，并且加上了一些GAN特定的调整项。让我们来讨论一下：
- en: The two components of the loss reflect the two possible classes (real or fake),
    which are equal in number in the training set.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失的两个组成部分反映了两种可能的类别（真实或伪造），这两种类别在训练集中数量相等。
- en: '![](img/774af71b-1b7e-4819-a8d7-029827c60db7.png)is the loss when the input
    is sampled from real data. Ideally, in such cases, we''ll have ![](img/e5b468c6-1f4b-4095-b86e-7fb2dadcac50.png).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/774af71b-1b7e-4819-a8d7-029827c60db7.png) 是当输入来自真实数据时的损失。理想情况下，在这种情况下，我们会有 ![](img/e5b468c6-1f4b-4095-b86e-7fb2dadcac50.png)。'
- en: In this context, the expectation term, ![](img/aaf721e7-7573-481c-92f9-7493b78a36c8.png), implies
    that **x** is sampled from ![](img/8cfa008d-275e-4f8a-9505-f162889529d1.png). In
    essence, this part of the loss means that, when we sample **x** from ![](img/a610dfc5-80da-410c-b7d3-307d39038478.png),
    we expect the discriminator output,![](img/14cd182d-42d2-4146-9fff-0623d9941f5c.png).
    Finally, 0.5 is the cumulative class probability of the real data, ![](img/802ad000-4795-4002-9ab3-464a238be1cb.png),
    since it comprises exactly half of the whole set.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，期望项 ![](img/aaf721e7-7573-481c-92f9-7493b78a36c8.png) 意味着 **x** 是从 ![](img/8cfa008d-275e-4f8a-9505-f162889529d1.png)中采样的。本质上，这部分损失意味着，当我们从 ![](img/a610dfc5-80da-410c-b7d3-307d39038478.png)中采样**x**时，我们期望判别器输出 ![](img/14cd182d-42d2-4146-9fff-0623d9941f5c.png)。最后，0.5是实际数据的累积类概率 ![](img/802ad000-4795-4002-9ab3-464a238be1cb.png)，因为它正好占整个数据集的一半。
- en: '![](img/f5860c78-954b-4743-ba11-b2fc0031a8d7.png)is the loss when the input
    is sampled from generated data. Here, we can make the same observations that we
    made with the real data component. However, this term is maximized when ![](img/8a323555-d661-467a-9932-7ddbe32ccc1a.png).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/f5860c78-954b-4743-ba11-b2fc0031a8d7.png) 是当输入来自生成数据时的损失。在这种情况下，我们可以做出与真实数据部分相同的观察。然而，当
    ![](img/8a323555-d661-467a-9932-7ddbe32ccc1a.png) 时，这个项是最大化的。'
- en: To summarize, the discriminator loss will be zero when ![](img/e4936295-2648-4560-b427-e2bb43070f70.png) for
    all ![](img/d3842269-6022-4eef-8a98-63ce3536015e.png) and ![](img/6d5ad66b-f88b-4f2e-bce5-4904241f8664.png) for
    all generated ![](img/515c08e1-d0bb-4b8e-9d68-1451ab7698c1.png) (or ![](img/9e0d2db1-d438-43d9-a894-9d2065079a42.png)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当![](img/e4936295-2648-4560-b427-e2bb43070f70.png)对于所有![](img/d3842269-6022-4eef-8a98-63ce3536015e.png)以及![](img/6d5ad66b-f88b-4f2e-bce5-4904241f8664.png)对于所有生成的![](img/515c08e1-d0bb-4b8e-9d68-1451ab7698c1.png)（或![](img/9e0d2db1-d438-43d9-a894-9d2065079a42.png)）时，判别器的损失将为零。
- en: Training the generator
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练生成器
- en: 'We''ll train the generator by making it better at deceiving the discriminator.
    To do this, we''ll need both networks, similar to the way we trained the discriminator
    with fake samples:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过提高生成器欺骗判别器的能力来训练生成器。为了实现这一点，我们需要同时使用两个网络，类似于我们用假样本训练判别器的方式：
- en: We start with a random latent vector, **z**, and feed it through both the generator
    and discriminator to produce the output, ![](img/a51a9fe6-0b01-4404-a7f3-a15015fa4528.png).
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从一个随机的潜在向量**z**开始，并将其传递给生成器和判别器，以生成输出![](img/a51a9fe6-0b01-4404-a7f3-a15015fa4528.png)。
- en: The loss function is the same as the discriminator loss. However, our goal here
    is to maximize rather than minimize it, since we want to deceive the discriminator.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数与判别器的损失函数相同。然而，我们在这里的目标是最大化它，而不是最小化它，因为我们希望欺骗判别器。
- en: In the backward pass, the discriminator weights, ![](img/04ebd62f-9d63-4b2c-ac18-15b1a60677c5.png),
    are locked and we can only adjust ![](img/dd4cbf8b-b4d8-4e6e-8cbd-506c388294f0.png).
    This forces us to maximize the discriminator loss by making the generator better,
    instead of making the discriminator worse.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播中，判别器的权重![](img/04ebd62f-9d63-4b2c-ac18-15b1a60677c5.png)被锁定，我们只能调整![](img/dd4cbf8b-b4d8-4e6e-8cbd-506c388294f0.png)。这迫使我们通过改善生成器来最大化判别器的损失，而不是让判别器变得更差。
- en: 'You may have noticed that, in this phase, we only use generated data. Since
    the discriminator weights are locked, we can ignore the part of the loss function
    that deals with real data. Therefore, we can simplify it to the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，在这个阶段，我们只使用生成的数据。由于判别器的权重被锁定，我们可以忽略处理真实数据的损失函数部分。因此，我们可以将其简化为以下形式：
- en: '![](img/b29f63c3-b5b4-4280-87dd-5a723926da2e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b29f63c3-b5b4-4280-87dd-5a723926da2e.png)'
- en: 'The derivative (gradient) of this formula is ![](img/8f81c792-2138-4c3f-9231-384a6e0d4d2d.png),
    which can be seen in the following diagram as an uninterrupted line. This imposes
    a limitation on the training. Early on, when the discriminator can easily distinguish
    between real and fake samples (![](img/f4f3790d-88d5-4cc1-abfb-72979964c0f3.png)),
    the gradient will be close to zero. This will result in little learning of the
    weights, ![](img/d9fa8f2a-dacd-40bf-96df-4ff3a5556821.png) (another manifestation
    of the vanishing gradient problem):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式的导数（梯度）是![](img/8f81c792-2138-4c3f-9231-384a6e0d4d2d.png)，在下图中可以看到它是连续不断的线。这对训练施加了限制。在初期，当判别器能够轻松区分真假样本（![](img/f4f3790d-88d5-4cc1-abfb-72979964c0f3.png)）时，梯度将接近零。这将导致权重![](img/d9fa8f2a-dacd-40bf-96df-4ff3a5556821.png)的学习几乎没有（这是消失梯度问题的另一种表现）：
- en: '![](img/ce5af7d3-6d16-499d-9e0f-1bbb38116cff.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce5af7d3-6d16-499d-9e0f-1bbb38116cff.png)'
- en: Gradients of the two generator loss functions
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 两个生成器损失函数的梯度
- en: 'We can solve this issue by using a different loss function:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用不同的损失函数来解决这个问题：
- en: '![](img/b0de4813-b13a-44f5-9051-6e99ef0395d3.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0de4813-b13a-44f5-9051-6e99ef0395d3.png)'
- en: The derivative of this function is displayed in the preceding diagram with a
    dashed line. This loss is still minimized when ![](img/fa89da70-a3a3-4e4d-9971-2e91e28cb23a.png) and
    when the gradient is large; that is, when the generator underperforms. With this
    loss, the game is no longer zero-sum, but this won't have a practical effect on
    the GAN framework. Now, we have all the ingredients we need to define the GAN
    training algorithm. We'll do this in the next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的导数在前面的图中用虚线表示。当![](img/fa89da70-a3a3-4e4d-9971-2e91e28cb23a.png)且梯度较大时，损失仍然会最小化；也就是说，当生成器表现不佳时。在这种损失下，博弈不再是零和博弈，但这对GAN框架不会产生实际影响。现在，我们拥有定义GAN训练算法所需的所有要素。我们将在下一节中进行定义。
- en: Putting it all together
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'With our newfound knowledge, we can define the minimax objective in full:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们新获得的知识，我们可以完整地定义最小最大目标：
- en: '![](img/aece5d87-490b-4419-9fd4-795396f47338.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aece5d87-490b-4419-9fd4-795396f47338.png)'
- en: In short, the generator tries to minimize the objective, while the discriminator
    tries to maximize it. Note that, while the discriminator should minimize its loss,
    the minimax objective is a negative of the discriminator loss, and therefore the
    discriminator has to maximize it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，生成器试图最小化目标，而判别器则试图最大化它。请注意，虽然判别器应该最小化其损失，但极小极大目标是判别器损失的负值，因此判别器必须最大化它。
- en: The following step-by-step training algorithm was introduced by the authors
    of the GAN framework.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下逐步训练算法由GAN框架的作者介绍。
- en: 'Repeat this for a number of iterations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对此进行多次迭代：
- en: 'Repeat for *k* steps, where *k* is a hyperparameter:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复进行*k*步，其中*k*是一个超参数：
- en: Sample a mini-batch of *m* random samples from the latent space,![](img/ccb1ce7f-ea61-44fd-a73b-5e4e74e2a3f5.png)
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从潜在空间中采样一个包含*m*个随机样本的小批量，![](img/ccb1ce7f-ea61-44fd-a73b-5e4e74e2a3f5.png)
- en: Sample a mini-batch of *m* samples from the real data,![](img/63b32472-2b4f-4365-a961-ddb74222380a.png)
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从真实数据中采样一个包含*m*个样本的小批量，![](img/63b32472-2b4f-4365-a961-ddb74222380a.png)
- en: 'Update the discriminator weights, ![](img/35d83794-8b1e-4ded-a95d-705dbaa266ab.png),
    by ascending the stochastic gradient of its cost:'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过上升其成本的随机梯度来更新判别器权重，![](img/35d83794-8b1e-4ded-a95d-705dbaa266ab.png)：
- en: '![](img/85093089-8bab-4a0c-a6b4-25c55e90ab02.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85093089-8bab-4a0c-a6b4-25c55e90ab02.png)'
- en: Sample a mini-batch of *m* random samples from the latent space,![](img/76bef6d4-3429-4816-ab7d-42970437865f.png).
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从潜在空间中采样一个包含*m*个随机样本的小批量，![](img/76bef6d4-3429-4816-ab7d-42970437865f.png)。
- en: 'Update the generator by descending the stochastic gradient of its cost:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过下降其成本的随机梯度来更新生成器：
- en: '![](img/92b4f00d-9be0-4105-9221-7a7ce2ec0eda.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92b4f00d-9be0-4105-9221-7a7ce2ec0eda.png)'
- en: 'Alternatively, we can use the updated cost function we introduced in the *Training
    the generator* section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用在*训练生成器*部分介绍的更新后的成本函数：
- en: '![](img/3de6492b-30bf-4e87-bc3f-1093600c06ca.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3de6492b-30bf-4e87-bc3f-1093600c06ca.png)'
- en: Now that we know how to train GANs, let's discuss some of the problems we may
    face while training them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何训练GANs了，让我们来讨论一些在训练过程中可能遇到的问题。
- en: Problems with training GANs
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练GAN时的问题
- en: 'Training GAN models has some major pitfalls:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN模型时有一些主要的陷阱：
- en: The gradient descent algorithm is designed to find the minimum of the loss function,
    rather than the Nash equilibrium, which is not the same thing. As a result, sometimes
    the training may fail to converge and could oscillate instead.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降算法旨在找到损失函数的最小值，而不是纳什均衡，两者并不相同。因此，有时训练可能无法收敛，反而会发生震荡。
- en: Recall that the discriminator output is a sigmoid function that represents the
    probability of the example being real or fake. If the discriminator becomes too
    good at this task, the probability output will converge to either 0 or 1 at every
    training sample. This would mean that the error gradient will always be 0, which
    will prevent the generator from learning anything. On the other hand, if the discriminator
    is bad at recognizing fakes from real images, it will backpropagate the wrong
    information to the generator. Therefore, the discriminator shouldn't be either
    too good or too bad for the training to succeed. In practice, this means that
    we cannot train it until convergence.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，判别器的输出是一个Sigmoid函数，表示示例为真实或伪造的概率。如果判别器在这项任务上表现得太好，那么每个训练样本的概率输出将会收敛到0或1。这意味着错误梯度将始终为0，从而阻止生成器学习任何东西。另一方面，如果判别器在识别真实和伪造图像方面表现不佳，它会将错误的信息反向传播给生成器。因此，判别器不能太强也不能太弱，才能保证训练成功。实际上，这意味着我们不能在训练时让其收敛。
- en: '**Mode collapse** is a problem where the generator can generate a limited number
    of images (or even just one), regardless of the latent input vector value. To
    understand why this happens, let''s focus on a single generator training episode
    that tries to minimize [![](img/5719dc8c-46a6-42d6-a170-76372b3fc78c.png)] while
    the weights of the discriminator are fixed. In other words, the generator tries
    to generate a fake image, **x**^*, so that [![](img/3c3cd4f4-708a-4f2c-b986-8ffd203afe57.png)].
    However, the loss function does not force the generator to create a unique image, **x**^*,
    for different values of the input latent vector. That is, the training can modify
    the generator in a way where it completely decouples the generated image, **x**^*,
    from the latent vector value and, at the same time, still minimize the loss function.
    For example, a GAN for generating new MNIST images could only generate the number
    4, regardless of the input. Once we update the discriminator, the previous value, **x**^*,
    may not be optimal anymore, which would force the generator to generate new and
    different images. Nevertheless, mode collapse may recur in different stages of
    the training process.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式崩溃**是一个问题，其中生成器无论输入的潜在向量值如何，都只能生成有限数量的图像（甚至可能只有一张）。为了理解为什么会发生这种情况，我们可以专注于一个单一的生成器训练过程，该过程试图最小化[![](img/5719dc8c-46a6-42d6-a170-76372b3fc78c.png)]，同时保持判别器的权重不变。换句话说，生成器试图生成一个假图像，**x**^*，使得[![](img/3c3cd4f4-708a-4f2c-b986-8ffd203afe57.png)]。然而，损失函数并没有强迫生成器为不同的潜在输入向量值创建独特的图像，**x**^*。也就是说，训练过程可能会改变生成器，使其完全脱离潜在向量值来生成图像，同时仍然最小化损失函数。例如，一个生成新MNIST图像的GAN可能只会生成数字4，而不管输入是什么。一旦我们更新了判别器，之前的图像**x**^*可能不再是最优的，这将迫使生成器生成新的不同图像。然而，模式崩溃可能会在训练过程的不同阶段再次出现。'
- en: Now that we are familiar with the GAN framework, we'll discuss several different
    types of GAN.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了GAN框架，接下来我们将讨论几种不同类型的GAN。
- en: Types of GAN
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN的类型
- en: Since the GAN framework was first introduced, a lot of new variations have emerged.
    In fact, there are so many new GANs now that, in order to stand out, the authors
    have come up with creative GAN names, such as BicycleGAN, DiscoGAN, GANs for LIFE,
    and ELEGANT. In the next few sections, we'll discuss some of them. All of the
    examples have been implemented with TensorFlow 2.0 and Keras.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自从GAN框架首次被提出以来，已经出现了许多新的变种。事实上，现在有很多新的GAN，为了突出特色，作者们提出了一些富有创意的GAN名称，例如BicycleGAN、DiscoGAN、GANs
    for LIFE和ELEGANT。在接下来的几个部分，我们将讨论其中的一些。所有示例都已经使用TensorFlow 2.0和Keras实现。
- en: The code for DCGAN, CGAN, WGAN, and CycleGAN is partially inspired by [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN).
    You can find the full implementations of all the examples in this chapter at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN、CGAN、WGAN和CycleGAN的代码部分灵感来自于 [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)。你可以在 [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05)
    找到本章所有示例的完整实现。
- en: Deep Convolutional GAN
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络
- en: 'In this section, we''ll implement the **Deep Convolutional GAN** (**DCGAN**, *Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks*,** [https://arxiv.rg/abs/1511.06434](https://arxiv.org/abs/1511.06434)**). In
    the original GAN framework proposal, the authors only used fully-connected networks.
    In contrast, in DCGANs both the generator and the discriminator are CNNs. They have some
    constraints that help stabilize the training process. You can think of these as
    general guidelines for GAN training and not just for DCGANs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现**深度卷积生成对抗网络**（**DCGAN**，*基于深度卷积生成对抗网络的无监督表示学习*，**[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)**）。在原始的GAN框架提案中，作者仅使用了全连接网络。相比之下，在DCGAN中，生成器和判别器都是卷积神经网络（CNN）。它们有一些约束，有助于稳定训练过程。你可以将这些约束视为GAN训练的一般准则，而不仅仅是针对DCGAN的：
- en: The discriminator uses strided convolutions instead of pooling layers.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器使用步幅卷积（strided convolutions）代替池化层（pooling layers）。
- en: The generator uses transpose convolutions to upsample the latent vector, ![](img/a6f3fc7b-2965-4074-8e82-6daa68f0a312.png), to
    the size of the generated image.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器使用转置卷积（transpose convolutions）将潜在向量 ![](img/a6f3fc7b-2965-4074-8e82-6daa68f0a312.png) 上采样到生成图像的大小。
- en: Both networks use batch normalization.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个网络都使用批归一化（batch normalization）。
- en: No fully-connected layers, with the exception of the last layer of the discriminator.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了判别器的最后一层外，不使用全连接层。
- en: LeakyReLU activations for all the layers of the generator and discriminator,
    except their outputs. The generator output layer uses Tanh activation (which has
    a range of (-1, 1)) to mimic the properties of real-world data. The discriminator
    has a single sigmoid output (recall that it's in the range of (0, 1)) because
    it measures the probability of the sample being real or fake.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成器和判别器的所有层使用LeakyReLU激活函数，除了它们的输出层。生成器的输出层使用Tanh激活函数（其范围为(-1, 1)），以模拟真实数据的特性。判别器的输出层只有一个sigmoid输出（记住，它的范围是(0,
    1)），因为它衡量样本是真实的还是伪造的概率。
- en: 'In the following diagram, we can see a sample generator network in the DCGAN
    framework:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图示中，我们可以看到DCGAN框架中的一个示例生成器网络：
- en: '![](img/018fc328-a765-4b96-a164-b39d2ae28c76.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/018fc328-a765-4b96-a164-b39d2ae28c76.png)'
- en: Generator network with transpose convolutions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用反卷积的生成器网络
- en: Implementing DCGAN
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DCGAN
- en: 'In this section, we''ll implement DCGAN, which generates new MNIST images.
    This example will serve as a blueprint for all GAN implementations in upcoming
    sections. Let''s get started:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现DCGAN，它生成新的MNIST图像。这个例子将作为后续所有GAN实现的模板。让我们开始吧：
- en: 'Let''s start by importing the necessary modules and classes:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入必要的模块和类开始：
- en: '[PRE7]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Implement the `build_generator` function. We''ll follow the guidelines that
    were outlined at the beginning of this section—upsampling with transpose convolutions,
    batch normalization, and LeakyReLU activations. The model starts with a fully-connected
    layer to upsample the 1D latent vector. Then, the vector is upsampled with a series
    of `Conv2DTranspose`. The final `Conv2DTranspose` has a `tanh` activation and
    the generated image has only 1 channel:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`build_generator`函数。我们将遵循本节开头概述的指导原则——使用反卷积进行上采样、批量归一化和LeakyReLU激活。模型从一个全连接层开始，用于上采样1D潜在向量。然后，向量通过一系列`Conv2DTranspose`进行上采样。最后一个`Conv2DTranspose`使用`tanh`激活，生成的图像只有1个通道：
- en: '[PRE8]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Build the discriminator. Again, it''s a simple CNN with stride convolutions:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建判别器。再次说明，这是一个简单的CNN，使用步幅卷积：
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Implement the `train` function with the actual GAN training. This function
    implements the procedure that was outlined in the *Putting it all together* subsection
    in the *Training GANs* section. We''ll start with the function declaration and
    the initialization of the variables:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`train`函数，进行实际的GAN训练。这个函数实现了在*Training GANs*部分的*Putting it all together*小节中概述的过程。我们将从函数声明和变量初始化开始：
- en: '[PRE10]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We''ll continue with the training loop, where we alternate one discriminator
    training episode with one generator training episode. First, we train the `discriminator`
    on 1 batch of `real_images` and one batch of `generated_images`. Then, we train
    the generator (which includes the `discriminator` as well) on the same batch of `generated_images`.
    Note that we label these images as real because we want to maximize the `discriminator`
    loss. The following is the implementation (please note the indentation; this is
    still part of the `train` function):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续训练循环，其中我们交替进行一次判别器训练和一次生成器训练。首先，我们在一批`real_images`和一批`generated_images`上训练`discriminator`。然后，我们在同一批`generated_images`上训练生成器（其中也包括`discriminator`）。注意，我们将这些图像标记为真实图像，因为我们希望最大化`discriminator`的损失。以下是实现代码（请注意缩进，这仍然是`train`函数的一部分）：
- en: '[PRE11]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Implement a boilerplate function, `plot_generated_images`, to display some
    generated images after the training is finished:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个模板函数`plot_generated_images`，在训练完成后显示一些生成的图像：
- en: Create an `nxn` grid (the `figure` variable).
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`nxn`网格（`figure`变量）。
- en: Create `nxn` random latent vectors (the `noise` variable)—one for each generated
    image.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`nxn`随机潜在向量（`noise`变量）——每个生成的图像对应一个潜在向量。
- en: Generate the images and place them in the grid cells.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成图像并将它们放置在网格单元中。
- en: Display the result.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果。
- en: 'The following is the implementation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现代码：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Build the full GAN model by including the `generator`, `discriminator`, and
    the `combined` network. We''ll use the latent vector that''s 64 in size (the `latent_dim`
    variable) and we''ll run the training for 50,000 batches using the Adam optimizer
    (this may take a while). Then, we''ll plot the results:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过包含`generator`、`discriminator`和`combined`网络来构建完整的GAN模型。我们将使用大小为64的潜在向量（`latent_dim`变量），并使用Adam优化器运行50,000个批次的训练（这可能需要一段时间）。然后，我们将绘制结果：
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If everything goes as planned, we should see something similar to the following:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们应该会看到类似以下的结果：
- en: '![](img/a2f2de4f-2637-4c56-b787-ac831df73d0e.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2f2de4f-2637-4c56-b787-ac831df73d0e.png)'
- en: Newly generated MNIST images
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 新生成的MNIST图像
- en: This concludes our discussion of DCGANs. In the next section, we'll discuss
    another type of GAN model called the Conditional GAN.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是我们对DCGAN的讨论的结束。在下一部分，我们将讨论另一种GAN模型，称为条件GAN。
- en: Conditional GAN
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件GAN
- en: 'The conditional GAN (CGAN, *Conditional Generative Adversarial Nets*, [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784))
    is an extension of the GAN model where both the generator and discriminator receive
    some additional conditioning input information. This could be the class of the
    current image or some other property:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 条件生成对抗网络（CGAN，*Conditional Generative Adversarial Nets*， [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)）是GAN模型的扩展，其中生成器和判别器都接收一些额外的条件输入信息。这些信息可以是当前图像的类别或其他某些特征：
- en: '![](img/8109b440-49ad-491c-b72a-3f0768a6256b.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8109b440-49ad-491c-b72a-3f0768a6256b.png)'
- en: Conditional GAN. *Y* represents the conditional input for the generator and
    discriminator
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 条件GAN。*Y*代表生成器和判别器的条件输入。
- en: 'For example, if we train a GAN to generate new MNIST images, we could add an
    additional input layer with values of one-hot encoded image labels. CGANs have
    the disadvantage that they are not strictly unsupervised and we need some kind
    of label for them to work. However, they have some other advantages:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们训练一个GAN来生成新的MNIST图像，我们可以添加一个额外的输入层，输入的是独热编码的图像标签。CGAN的缺点是它们不是严格意义上的无监督学习，我们需要某种标签才能使其工作。然而，它们也有其他一些优点：
- en: By using more well-structured information for training, the model can learn
    better data representations and generate better samples.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用更结构化的信息进行训练，模型可以学习更好的数据表示，并生成更好的样本。
- en: 'In regular GANs, all the image information is stored in the latent vector,
    **z**. This poses a problem: since ![](img/92040d51-daef-42af-9e62-7843f787a669.png) can
    be complex, we don''t have much control over the properties of the generated image.
    For example, suppose that we want our MNIST GAN to generate a certain digit; say,
    7\. We would have to experiment with different latent vectors until we reach the
    desired output. But with CGAN, we could simply combine the one-hot vector of 7
    with some random **z** and the network will generate the correct digit. We could
    still try different values for **z** and the model would generate different versions
    of the digit, that is, 7\. In short, CGAN provides a way for us to control (condition)
    the generator output.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在常规GAN中，所有的图像信息都存储在潜在向量**z**中。这就带来了一个问题：由于 ![](img/92040d51-daef-42af-9e62-7843f787a669.png) 可能是复杂的，我们无法很好地控制生成图像的属性。例如，假设我们希望我们的MNIST
    GAN生成某个特定的数字，比如7。我们必须尝试不同的潜在向量，直到达到想要的输出。但在CGAN中，我们只需将7的独热向量与一些随机的**z**结合，网络就会生成正确的数字。我们仍然可以尝试不同的**z**值，模型会生成不同版本的数字，也就是7。简而言之，CGAN为我们提供了一种控制（条件）生成器输出的方法。
- en: 'Because of the conditional input, we''ll modify the minimax objective to include
    the condition, *y*, as well:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于条件输入，我们将修改最小最大目标函数，使其也包含条件 *y*：
- en: '![](img/e603f48e-8b7d-4bbe-b73c-3d86c0025ed3.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e603f48e-8b7d-4bbe-b73c-3d86c0025ed3.png)'
- en: Implementing CGAN
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现CGAN
- en: The blueprint for the CGAN implementation is very similar to the DCGAN example
    in the *Implementing DCGAN* section. That is, we'll implement CGAN in order to
    generate new images of the MNIST dataset. For the sake of simplicity (and diversity),
    we'll use fully connected generators and discriminators. To avoid repetition,
    we'll only show modified sections of the code compared to DCGAN. You can find
    the full example in this book's GitHub repository.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN的实现蓝图与*实现DCGAN*部分中的DCGAN示例非常相似。也就是说，我们将实现CGAN，以生成MNIST数据集的新图像。为了简化（和多样化），我们将使用全连接的生成器和判别器。为了避免重复，我们将仅展示与DCGAN相比修改过的部分代码。完整的示例可以在本书的GitHub仓库中找到。
- en: 'The first significant difference is the definition of the generator:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个显著的区别是生成器的定义：
- en: '[PRE14]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Although it''s a fully-connected network, we still follow the GAN network design
    guidelines that were defined in the *Deep Convolutional GANs* section. Let''s
    discuss the way we combine the latent vector, `z_input`, with the conditional
    label, `label_input` (an integer with values from 0 to 9). We can see that `label_input`
    is transformed with an `Embedding` layer. This layer does two things:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个全连接网络，但我们仍然遵循在*深度卷积生成对抗网络（DCGAN）*部分中定义的GAN网络设计准则。让我们来讨论如何将潜在向量`z_input`与条件标签`label_input`（一个值从0到9的整数）结合。我们可以看到，`label_input`通过`Embedding`层进行转换。该层执行两项操作：
- en: Converts the integer value, `label_input`, into a one-hot representation with
    a length of `input_dim`
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将整数值`label_input`转换为长度为`input_dim`的独热编码表示。
- en: Uses the one-hot representation as an input for a fully-connected layer with
    the size of `output_dim`
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将独热编码表示作为输入传递给大小为`output_dim`的全连接层。
- en: The embedding layer allows us to obtain unique vector representations for each
    possible input value. In this case, the output of `label_embedding` has the same
    dimensions as the size of the latent vector and `z_input`. `label_embedding` is
    combined with the latent vector, `z_input`, with the help of element-wise multiplication
    in the `model_input` variable, which serves as an input for the rest of the network.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层允许我们为每个可能的输入值获取独特的向量表示。在这种情况下，`label_embedding`的输出与潜在向量和`z_input`的大小相同。`label_embedding`与潜在向量`z_input`通过`model_input`变量进行逐元素相乘，后者作为网络其余部分的输入。
- en: 'Next, we''ll focus on the discriminator, which is also a fully-connected network
    and uses the same embedding mechanism as the generator. This time, the embedding
    output size is `np.prod((28, 28, 1))`, which is equal to 784 (the size of the
    MNIST images):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点讨论判别器，它也是一个全连接网络，并使用与生成器相同的嵌入机制。这次，嵌入输出的大小是`np.prod((28, 28, 1))`，等于784（MNIST图像的大小）：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The rest of the example code is very similar to the DCGAN example. The only
    other differences are trivial—they account for the multiple inputs (latent vector
    and embedding) for the networks. The `plot_generated_images` function has an additional
    parameter, which allows it to generate images for random latent vectors and a
    specific conditional label (in this case, a digit). In the following, we can see
    the newly generated images for conditional labels 3, 8, and 9:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的示例代码与DCGAN的示例非常相似。唯一的其他区别是微不足道的——它们考虑了网络的多个输入（潜在向量和嵌入）。`plot_generated_images`函数有一个额外的参数，允许它为随机潜在向量和特定的条件标签（在本例中为数字）生成图像。以下是条件标签3、8和9的最新生成图像：
- en: '![](img/7e9fa126-d99e-4f28-8a98-5e9ffad73815.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e9fa126-d99e-4f28-8a98-5e9ffad73815.png)'
- en: CGAN for conditional labels 3, 8, and 9
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 条件标签3、8和9的CGAN
- en: This concludes our discussion of CGANs. In the next section, we'll discuss another
    type of GAN model called the Wasserstein GAN.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于CGAN的讨论。接下来的部分，我们将讨论另一种类型的GAN模型——Wasserstein GAN。
- en: Wasserstein GAN
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wasserstein GAN
- en: 'To understand the Wasserstein GAN (WGAN, [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)),
    let''s recall that, in the *Training GANs* section, we denoted the probability
    distribution of the generator with ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)and
    the probability distribution of the real data with ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png). In
    the process of training the GAN model, we update the generator weights and so
    we change ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png). The goal of the GAN
    framework is to converge ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) to ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)(this
    is also valid for other types of generative model, such as VAE), that is, the
    probability distribution of the generated images should be the same as the real
    ones, which would result in realistic images. WGAN uses a new way to measure the
    distance between the two distributions called the Wasserstein distance (or the
    **Earth mover''s distance** (**EMD**)). To understand it, let''s start with the
    following diagram:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Wasserstein GAN（WGAN，[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)），我们回顾一下，在*训练GANs*部分，我们将生成器的概率分布表示为![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)，真实数据的概率分布表示为![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)。在训练GAN模型的过程中，我们更新生成器的权重，从而改变![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)。GAN框架的目标是将![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)收敛到![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)（这对于其他类型的生成模型，如VAE也是有效的），即生成的图像的概率分布应与真实图像相同，从而得到逼真的图像。WGAN使用一种新的方法来度量两个分布之间的距离，称为Wasserstein距离（或**地球搬运工距离**（**EMD**））。为了理解它，让我们从以下图表开始：
- en: '![](img/c4118fda-f908-4d33-983d-4756d840b288.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4118fda-f908-4d33-983d-4756d840b288.png)'
- en: 'An example of EMD. Left: Initial and target distributions; Right: Two different
    ways to transform ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) into ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: EMD的一个示例。左：初始分布和目标分布；右：两种不同的方法将![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)转换为![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)
- en: For the sake of simplicity, we'll assume that the ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) and ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png) are
    distributions discrete (the same rule applies for continuous distributions). We
    can transform ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) into ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png) by
    moving the columns (a, b, c, d, e) left or right along the *x* axis. Each transfer
    of 1 position has a cost of 1. For example, the cost to move column *a* from its
    initial position, 2, to position 6 is 4. The right-hand side of the preceding
    diagram shows two ways of doing this. In the first case, we have *total cost =
    cost(a:2->6) + cost(e:6->3) + cost(b:3->2) = 4 +3 + 1 = 8*. In the second case,
    we have *total cost = cost(a:2->3) + cost(b:2->1) = 1 + 1 = 2*. EMD is the minimal
    total cost it takes to transform one distribution into the other. Therefore, in
    this example, we have EMD = 2.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们假设![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)和![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)是离散分布（对于连续分布也适用相同的规则）。我们可以通过沿*
    x *轴将列（a、b、c、d、e）向左或向右移动，将![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)转换为![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)。每次移动1个位置的成本是1。例如，将列*a*从其初始位置2移动到位置6的成本是4。前面图表的右侧显示了两种不同的做法。在第一种情况下，我们的*总成本
    = 成本(a:2->6) + 成本(e:6->3) + 成本(b:3->2) = 4 + 3 + 1 = 8*。在第二种情况下，我们的*总成本 = 成本(a:2->3)
    + 成本(b:2->1) = 1 + 1 = 2*。EMD是将一个分布转换为另一个分布所需的最小总成本。因此，在这个例子中，我们有EMD = 2。
- en: We now have a basic idea of what EMD is, but we still don't know why it's necessary
    to use this metric in the GAN model. The WGAN paper provides an elaborate but
    somewhat complex answer to this question. In this section, we'll try to explain
    it. To start, let's note that the generator starts with a low-dimensional latent
    vector, ![](img/7a361be8-949f-4b71-a9ba-641c76c67d3b.png), and then transforms
    it into a high-dimensional generated image (for example, 784, in the case of MNIST).
    The output size of the image also implies a high-dimensional distribution of the
    generated data, ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png). However, its
    intrinsic dimensions (the latent vector, ![](img/7a361be8-949f-4b71-a9ba-641c76c67d3b.png))
    are much lower. Because of this ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)
    will be excluded from big sections of the high-dimensional feature space. On the
    other hand, ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png) is truly high dimensional
    because it doesn't start from a latent vector; instead, it represents the real
    data with its full richness. Therefore, it's very likely that ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)
    and ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png) don't intersect anywhere
    in the feature space.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对EMD有了基本的了解，但我们仍然不知道为什么在GAN模型中使用这个度量是必要的。WGAN论文提供了一个详细但有些复杂的答案。在这一节中，我们将尝试解释它。首先，注意到生成器从一个低维的潜在向量开始，
    ![](img/7a361be8-949f-4b71-a9ba-641c76c67d3b.png)，然后将其转换为一个高维的生成图像（例如，在MNIST的情况下是784）。图像的输出大小也暗示了生成数据的高维分布，
    ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)。然而，它的内在维度（潜在向量， ![](img/7a361be8-949f-4b71-a9ba-641c76c67d3b.png)）要低得多。正因为如此，
    ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) 将被排除在高维特征空间的大部分区域之外。另一方面， ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)
    是实际的高维的，因为它不是从潜在向量开始的；相反，它通过其完整的丰富性表示了真实数据。因此，很可能 ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)
    和 ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png) 在特征空间中没有交集。
- en: 'To understand why this matters, let''s note that we can transform the generator
    and discriminator cost functions (see the *Training GANs* section) into functions
    of the KL and the **Jensen–Shannon** (**JS**,[https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence))
    divergence. The problem with these metrics is that they provide a zero gradient
    when the two distributions don''t intersect. That is, no matter what the distance
    between the two distributions is (small or large), if they don''t intersect, the
    metrics won''t provide any information about the actual difference between them.
    However, as we just explained, it''s very likely that the distributions won''t
    intersect. Contrary to this, the Wasserstein distance works regardless of whether
    the distributions intersect or not, which makes it a better candidate for the
    GAN model. We can illustrate this issue visually with the following diagram:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个问题为什么重要，我们注意到可以将生成器和判别器的代价函数（见 *训练GAN* 部分）转化为KL散度和 **Jensen–Shannon**
    (**JS**, [https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence))
    的函数。这些度量的问题在于，当两个分布不交集时，它们提供零梯度。也就是说，无论两个分布之间的距离是小还是大，如果它们没有交集，度量将无法提供关于它们实际差异的任何信息。然而，正如我们刚才解释的，很可能这些分布是不会交集的。与此相反，Wasserstein距离无论分布是否交集都能正常工作，这使得它成为GAN模型的更好选择。我们可以用以下图示直观地说明这个问题：
- en: '![](img/178670ad-2cb5-4f1c-9d7e-12c8738f532d.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/178670ad-2cb5-4f1c-9d7e-12c8738f532d.png)'
- en: The advantage of the Wasserstein distance over the regular GAN discriminator.
    Source: https://arxiv.org/abs/1701.07875
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein距离相较于常规GAN判别器的优势。来源：[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
- en: Here, we can see two non-intersecting Gaussian distributions, ![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) and ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png) (to
    the left and to the right, respectively). The regular GAN discriminator output
    is the sigmoid function (with a range of (0, 1)), which tells us the probability
    of the input being fake or not. In this case, the sigmoid output is meaningful
    in a very narrow range (centered around 0) and converges toward 0 or 1 in all
    other areas. This is a manifestation of the same problem we outlined in the *Problems
    with training GANs* section. It leads to vanishing gradients, which prevents error
    backpropagation to the generator. In contrast, the WGAN doesn't give us binary
    feedback on whether an image is real or fake and instead provides an actual distance
    measurement between the two distributions (also displayed in the preceding diagram).
    This distance is more useful than binary classification because it will provide
    a better indication of how to update the generator. To reflect this, the authors
    of the paper have renamed the discriminator and called it **critic**.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到两个不相交的高斯分布，![](img/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png) 和 ![](img/e043e680-d6bc-4abe-949a-441df0f3a05d.png)（分别位于左侧和右侧）。常规GAN判别器的输出是sigmoid函数（范围为(0,
    1)），它告诉我们输入是真还是假。在这种情况下，sigmoid输出在非常狭窄的范围内（围绕0居中）有意义，并且在其他区域收敛到0或1。这是我们在*GAN训练中的问题*部分中所描述问题的表现，导致梯度消失，从而阻止了错误的反向传播到生成器。相比之下，WGAN不会给出图像是真还是假的二元反馈，而是提供了两个分布之间的实际距离度量（也可以在前面的图中看到）。这个距离比二分类更有用，因为它能更好地指示如何更新生成器。为此，论文的作者将判别器重命名为**critic**。
- en: 'The following screenshot shows the WGAN algorithm as it''s described in the
    paper:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了论文中描述的WGAN算法：
- en: '![](img/8099c5fd-4ded-489a-af4b-1c7e85622148.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8099c5fd-4ded-489a-af4b-1c7e85622148.png)'
- en: 'Here, *f[w]* denotes the critic, *g[w]* is the critic weight update, and *g[θ]*
    is the generator weight update. Although the theory behind WGAN is sophisticated,
    in practice we can implement it by making relatively few changes to the regular
    GAN model:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f[w]* 表示critic，*g[w]* 是critic权重更新，*g[θ]* 是生成器权重更新。尽管WGAN背后的理论很复杂，但在实践中我们可以通过对常规GAN模型进行相对少量的修改来实现它：
- en: Remove the output sigmoid activation of the discriminator.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除判别器的输出sigmoid激活函数。
- en: Replace the log generator/discriminator loss functions with an EMD-derived loss.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用EMD衍生的损失函数替换对数生成器/判别器损失函数。
- en: Clip the critic weights after each mini-batch so that their absolute values
    are smaller than a constant, *c*. This requirement enforces the so-called Lipschitz
    constraint on the critic, which makes it possible to use the Wasserstein distance
    (more on this in the paper itself). Without getting into the details, we'll just
    mention that weight clipping can lead to undesired behavior. One successful solution
    to these issues has been the gradient penalty (WGAN-GP, *Improved Training of
    Wasserstein GANs*, [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)),
    which does not suffer from the same problems.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次处理完一个小批量后，裁剪判别器的权重，使其绝对值小于一个常数，*c*。这一要求强制执行了所谓的**Lipschitz**约束，这使得我们可以使用Wasserstein距离（关于这一点，论文中会有更多解释）。不深入细节，我们仅提到，权重裁剪可能会导致一些不良行为。解决这些问题的一个成功方案是梯度惩罚（WGAN-GP，*改进的Wasserstein
    GAN训练*， [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)），它没有出现相同的问题。
- en: The authors of the paper reported that optimization methods without momentum
    (SGD, RMSProp) work better than those with momentum.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文的作者报告称，未使用动量的优化方法（如SGD，RMSProp）比带有动量的方法效果更好。
- en: Implementing WGAN
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现WGAN
- en: 'Now that we have a basic idea of how the Wasserstein GAN works, let''s implement
    it. Once again, we''ll use the DCGAN blueprint and omit the repetitive code snippets
    so that we can focus on the differences. The `build_generator` and `build_critic` functions
    instantiate the generator and the critic, respectively. For the sake of simplicity,
    the two networks contain only fully connected layers. All the hidden layers have
    LeakyReLU activations. Following the paper''s guidelines, the generator has Tanh
    output activation and the critic has a single scalar output (no sigmoid activation,
    though). Next, let''s implement the `train` method since it contains some WGAN
    specifics. We''ll start with the method''s declaration and the initialization
    of the training process:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Wasserstein GAN的基本工作原理有了初步了解，让我们来实现它。我们将再次使用DCGAN的框架，并省略重复的代码段，以便我们可以专注于不同之处。`build_generator`和`build_critic`函数分别实例化生成器和判别器。为了简化起见，这两个网络只包含全连接层。所有隐藏层都使用LeakyReLU激活函数。根据论文的指导，生成器使用Tanh输出激活函数，而判别器则有一个单一的标量输出（没有sigmoid激活）。接下来，让我们实现`train`方法，因为它包含一些WGAN的特定内容。我们将从方法的声明和训练过程的初始化开始：
- en: '[PRE16]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we''ll continue with the training loop, which follows the steps of the
    WGAN algorithm we described earlier in this section. The inner loop trains the
    `critic` `n_critic` steps for each training step of the `generator`. In fact,
    this is the main difference between training the `critic` and training the `discriminator`
    in the train function of the *Implementing DCGAN* section, where the discriminator
    and the generator alternate at each step*.* Additionally, the `weights` critic
    is clipped after each mini-batch. The following is the implementation (please
    note the indentation; this code is part of the `train` function):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将继续训练循环，按照我们在本节中前面描述的WGAN算法步骤进行。内循环每训练`generator`的每个训练步骤时，训练`critic`的`n_critic`步骤。事实上，这是训练`critic`和在*实现DCGAN*部分的训练函数中训练`discriminator`的主要区别，后者在每一步都交替进行生成器和判别器的训练。此外，每个小批次后，`critic`的`weights`都会被裁剪。以下是实现（请注意缩进；这段代码是`train`函数的一部分）：
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we''ll implement the derivative of the Wasserstein loss itself. It is
    a TF operation that represents the mean value of the product of the network output
    and the labels (real or fake):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现Wasserstein损失本身的导数。这是一个TensorFlow操作，表示网络输出与标签（真实或伪造）乘积的均值：
- en: '[PRE18]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can build the full GAN model. This step is similar to the other GAN
    models:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建完整的GAN模型。这个步骤类似于其他GAN模型：
- en: '[PRE19]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, let''s initiate training and evaluation:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们开始训练和评估：
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once we run this example, WGAN will produce the following images after training
    40,000 mini-batches (this may take a while):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行这个示例，WGAN将在训练40,000个小批次后生成以下图像（这可能需要一些时间）：
- en: '![](img/118f62d9-b265-4744-a119-eb5a3fe348f6.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/118f62d9-b265-4744-a119-eb5a3fe348f6.png)'
- en: WGAN MNIST generator results
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN MNIST生成器结果
- en: This concludes our discussion of WGANs. In the next section, we'll discuss how
    to implement image-to-image translation with CycleGAN.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分结束了我们对WGAN的讨论。在下一节中，我们将讨论如何使用CycleGAN实现图像到图像的转换。
- en: Image-to-image translation with CycleGAN
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CycleGAN进行图像到图像转换
- en: In this section, we'll discuss **Cycle-Consistent Adversarial Networks** (**CycleGAN**, *Unpaired
    Image-to-Image Translation using Cycle-Consistent Adversarial Networks*, [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593))
    and their application for image-to-image translation. To quote the paper itself, image-to-image
    translation is a class of vision and graphics problems where the goal is to learn
    the mapping between an input image and an output image using a training set of
    aligned image pairs. For example, if we have grayscale and RGB versions of the
    same image, we can train an ML algorithm to colorize grayscale images or vice
    versa.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论**循环一致性对抗网络**（**CycleGAN**，*使用循环一致性对抗网络进行无配对图像到图像的转换*，[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)）及其在图像到图像转换中的应用。引用论文中的话，图像到图像转换是一类视觉与图形学问题，其目标是利用对齐的图像对训练集，学习输入图像和输出图像之间的映射。例如，如果我们有同一图像的灰度图和RGB版本，我们可以训练一个机器学习算法来为灰度图上色，或反之亦然。
- en: 'Another example is image segmentation ([Chapter 3](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),* Object
    Detection and Image Segmentation*), where the input image is translated into a
    segmentation map of the same image. In the latter case, we train the model (U-Net,
    Mask R-CNN) with image/segmentation map pairs. However, paired training data may
    not be available for many tasks. CycleGAN presents a way for us to transform an
    image from the source domain, *X*, into the target domain, *Y*, in the absence
    of paired samples. The following image shows some examples of paired and unpaired
    images:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是图像分割（[第3章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)，*物体检测与图像分割*），其中输入图像被转换成与该图像相同的分割图。在后一种情况下，我们使用图像/分割图配对训练模型（U-Net，Mask
    R-CNN）。然而，许多任务可能无法获得配对的训练数据。CycleGAN为我们提供了一种方法，能够在没有配对样本的情况下，将源领域*X*中的图像转换为目标领域*Y*中的图像。下图展示了一些配对和未配对图像的例子：
- en: '![](img/7f45296e-5e6d-40af-b836-d2cce1a0c2d8.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f45296e-5e6d-40af-b836-d2cce1a0c2d8.png)'
- en: 'Left: Paired training samples with the corresponding source and target images;
    Right: Unpaired training samples, where the source and target images don''t correspond.
    Source: https://arxiv.org/abs/1703.10593'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 左：配对的训练样本，包含相应的源图像和目标图像；右：未配对的训练样本，其中源图像和目标图像不对应。来源： https://arxiv.org/abs/1703.10593
- en: The *Image-to-Image Translation with Conditional Adversarial Networks* (known
    as Pix2Pix, [https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))
    paper from the same team also does image-to-image translation for paired training
    data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个团队的*图像到图像的转换与条件对抗网络*（即Pix2Pix，[https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004)）论文也针对配对训练数据进行图像到图像的转换。
- en: But how does CycleGAN do this? First, the algorithm assumes that, although there
    are no direct pairs in the two sets, there is still some relationship between
    the two domains. For example, these could be photographs of the same scene but
    from different angles. CycleGAN aims to learn this set-level relationship, rather
    than the relationships between distinct pairs. In theory, the GAN model lends
    itself to this task well. We can train a generator that maps ![](img/cc8fc839-1087-4065-b914-46ecf623989b.png),
    which produces an image, ![](img/f006826f-ea20-4518-b662-fd68d2e1c38d.png), that
    a discriminator cannot distinguish from the target images, ![](img/5a05feb6-7899-4a80-8011-091b7aebd4d3.png).
    More specifically, the optimal *G* should translate the domain, X, into a domain, ![](img/fc044c73-e153-4d7c-a777-1b23fa8b114f.png), with
    an identical distribution to domain *Y*. In practice, the authors of the paper
    discovered that such a translation does not guarantee that an individual input, *x*,
    and output, *y*, are paired up in a meaningful way—there are infinitely many mappings, *G*,
    that will create the same distribution over ![](img/0b892c90-4404-4327-8bbe-32148ff90681.png).
    They also found that this GAN model suffers from the familiar mode collapse problem.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，CycleGAN是如何做到这一点的呢？首先，算法假设，尽管在这两个集合中没有直接的配对，但这两个领域之间仍然存在某种关系。例如，这些图像可能是同一场景的不同角度拍摄的照片。CycleGAN的目标是学习这种集合级别的关系，而不是学习不同配对之间的关系。从理论上讲，GAN模型非常适合这个任务。我们可以训练一个生成器，将 ![](img/cc8fc839-1087-4065-b914-46ecf623989b.png)映射为生成图像，![](img/f006826f-ea20-4518-b662-fd68d2e1c38d.png)，而判别器无法将其与目标图像区分开，![](img/5a05feb6-7899-4a80-8011-091b7aebd4d3.png)。更具体地说，最佳的*G*应该将领域X转换为一个与领域*Y*具有相同分布的领域 ![](img/fc044c73-e153-4d7c-a777-1b23fa8b114f.png)。在实践中，论文的作者发现，尽管如此样的转换并不能保证一个特定的输入*x*和输出*y*在有意义的方式上配对——有无数种映射*G*，可以产生相同的分布。它们还发现，这个GAN模型会遭遇熟悉的模式崩溃问题。
- en: CycleGAN tries to solve these issues with the so-called **cycle consistency**.
    To understand what this is, let's say that we translate a sentence from English
    into German. The translation will be cycle-consistent if we translate the sentence
    back from German into English and we arrive at the original sentence we started
    with. In a mathematical context, if we have a translator, ![](img/0c4fefb7-620d-4f2d-9106-739d207e735b.png), and
    another translator, ![](img/06c75834-066b-49ee-be8f-d15acea94fc5.png), the two
    should be inverses of each other.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN尝试通过所谓的**循环一致性**来解决这些问题。为了理解这是什么意思，假设我们将一句话从英语翻译成德语。如果我们从德语翻译回英语，并且回到了我们最初的原始句子，那么这次翻译就被认为是循环一致的。在数学上下文中，如果我们有一个翻译器，![](img/0c4fefb7-620d-4f2d-9106-739d207e735b.png)，还有另一个翻译器，![](img/06c75834-066b-49ee-be8f-d15acea94fc5.png)，这两个翻译器应该是彼此的逆。
- en: 'To explain how CycleGAN implements cycle consistency, let''s start with the
    following diagram:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 CycleGAN 如何实现循环一致性，让我们从下图开始：
- en: '![](img/83c40c4d-e712-4b0f-8b07-8486589d8b64.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83c40c4d-e712-4b0f-8b07-8486589d8b64.png)'
- en: 'Left: Overall CycleGAN schema; Middle: Forward cycle-consistency loss; Right:
    Backward cycle-consistency loss. Source: https://arxiv.org/abs/1703.10593'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 左：整体 CycleGAN 结构图；中：前向循环一致性损失；右：后向循环一致性损失。来源：[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)
- en: 'The model has two generators, [![](img/0c4fefb7-620d-4f2d-9106-739d207e735b.png)]
    and [![](img/06c75834-066b-49ee-be8f-d15acea94fc5.png)], and two associated discriminators, *D[x]*
    and *D[y]*, respectively (left in the preceding diagram). Let''s take a look at *G*
    first*.* It takes an input image, ![](img/204079ed-a5c5-4cf1-b474-ba34f7a7de2f.png), and
    generates ![](img/fe49c7be-9444-4ede-b029-b9586188e0f3.png), which look similar
    to the images from domain *Y*. *D[y]* aims to discriminate between real images, ![](img/a242aec9-02be-4228-ab67-c1fb68d37a08.png), and
    the generated ![](img/98d91a20-d6b6-4ba5-ba80-acf9b5851539.png). This part of
    the model functions like a regular GAN and uses the regular minimax GAN adversarial
    loss:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有两个生成器，[![](img/0c4fefb7-620d-4f2d-9106-739d207e735b.png)] 和 [![](img/06c75834-066b-49ee-be8f-d15acea94fc5.png)]，以及两个相关的判别器，分别是
    *D[x]* 和 *D[y]*（在前面的图中位于左侧）。首先让我们来看一下 *G*。它接收一张输入图像，![](img/204079ed-a5c5-4cf1-b474-ba34f7a7de2f.png)，并生成
    ![](img/fe49c7be-9444-4ede-b029-b9586188e0f3.png)，这些图像与域 *Y* 中的图像相似。*D[y]* 旨在区分真实图像，![](img/a242aec9-02be-4228-ab67-c1fb68d37a08.png)，和生成的
    ![](img/98d91a20-d6b6-4ba5-ba80-acf9b5851539.png)。这部分模型的功能类似于常规的 GAN，使用常规的极小极大
    GAN 对抗损失：
- en: '![](img/a11267c7-2a3e-496c-a180-c46a94d2f204.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a11267c7-2a3e-496c-a180-c46a94d2f204.png)'
- en: The first term represents the original images, *y*, and the second represents
    the images that were generated by *G*. The same formula is valid for the generator, *F*.
    As we mentioned previously, this loss only ensures that ![](img/ca6d7e8d-990f-49b2-96a7-bb18d26f4af0.png) will
    have the same distribution as the images from *Y*, but doesn't create a meaningful
    pair of **x** and **y**. To quote the paper: with a large enough capacity, a network
    can map the same set of input images to any random permutation of images in the
    target domain, where any of the learned mappings can induce an output distribution
    that matches the target distribution. Thus, adversarial losses alone cannot guarantee
    that the learned function can map an individual input, **x**[*i*, ]to the desired
    output, **y***[i]*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项表示原始图像，*y*，第二个项表示由 *G* 生成的图像。相同的公式对生成器 *F* 也有效。正如我们之前提到的，这个损失仅确保 ![](img/ca6d7e8d-990f-49b2-96a7-bb18d26f4af0.png)
    将具有与 *Y* 中图像相同的分布，但并未创建一个有意义的 **x** 和 **y** 配对。引用论文中的话：具有足够大容量的网络可以将同一组输入图像映射到目标域中任何随机排列的图像，任何学习到的映射都能引起一个与目标分布匹配的输出分布。因此，仅靠对抗损失无法保证学习到的函数能够将单个输入
    **x**[*i*] 映射到期望的输出 **y**[*i*]。
- en: 'The authors of the paper argue that the learned mapping functions should be
    cycle-consistent (preceding diagram, middle). For each image, ![](img/ff116f2a-7091-4b4d-b4a3-86dfcee3af5f.png),
    the image translation cycle should be able to bring **x** back to the original
    image (this is called forward cycle consistency). *G* generates a new image, ![](img/36649dd2-269f-4f3f-a035-ea23bfc54e00.png),
    which serves as an input to *F*, which in turn generates a new image, ![](img/9d6869b8-ecf9-410f-a32a-29a9e9037cc3.png),
    where ![](img/af5abbe3-616b-4a4b-8e26-2b9842551933.png): ![](img/72a089ab-26c4-47f9-9d78-6a638e887967.png). *G*
    and *F* should also satisfy backward cycle consistency (preceding diagram, right):
    ![](img/6e24c434-598b-4a8c-87a4-77c7ffb6b636.png).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者认为，学习到的映射函数应该是循环一致的（前面的图，中）。对于每张图像，![](img/ff116f2a-7091-4b4d-b4a3-86dfcee3af5f.png)，图像翻译的循环应该能够将
    **x** 恢复到原始图像（这叫做前向循环一致性）。*G* 生成一张新图像，![](img/36649dd2-269f-4f3f-a035-ea23bfc54e00.png)，它作为
    *F* 的输入，*F* 进一步生成一张新图像，![](img/9d6869b8-ecf9-410f-a32a-29a9e9037cc3.png)，其中 ![](img/af5abbe3-616b-4a4b-8e26-2b9842551933.png)：![](img/72a089ab-26c4-47f9-9d78-6a638e887967.png)。*G*
    和 *F* 还应满足后向循环一致性（前面的图，右）：![](img/6e24c434-598b-4a8c-87a4-77c7ffb6b636.png)。
- en: 'This new path creates an additional cycle-consistency loss term:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这条新路径创建了额外的循环一致性损失项：
- en: '![](img/06594f64-84da-453e-9adf-20357b3d00ca.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06594f64-84da-453e-9adf-20357b3d00ca.png)'
- en: 'This measures the absolute difference between the original images, that is, *x*
    and *y*, and their generated counterparts, ![](img/ce8ce93b-93ba-426b-8d18-1f6e6f116879.png) and
    ![](img/f39352cb-a3a2-48e2-a1dd-35da7c23a89f.png). Note that these paths can be
    viewed as jointly training two autoencoders, ![](img/cfdf4e9a-1ea4-4749-a426-21b3fa1f6d55.png) and ![](img/987e6078-8c09-4169-968a-f0eb1dcf7bfe.png). Each
    autoencoder has a special internal structure: it maps an image to itself with
    the help of an intermediate representation – the translation of the image into
    another domain.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量的是原始图像（即*x*和*y*）与其生成对比图像之间的绝对差异，分别为![](img/ce8ce93b-93ba-426b-8d18-1f6e6f116879.png)和![](img/f39352cb-a3a2-48e2-a1dd-35da7c23a89f.png)。请注意，这些路径可以视为联合训练两个自编码器，![](img/cfdf4e9a-1ea4-4749-a426-21b3fa1f6d55.png)和![](img/987e6078-8c09-4169-968a-f0eb1dcf7bfe.png)。每个自编码器都有一个特殊的内部结构：它借助中间表示将图像映射到自身——即将图像转化为另一个领域。
- en: 'The full CycleGAN objective is a combination of the cycle consistency loss
    and the adversarial losses of *F* and *G*:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的CycleGAN目标是循环一致性损失和*F*与*G*的对抗损失的结合：
- en: '![](img/99d5230b-cef3-4d67-9ae4-624bd1176b8f.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99d5230b-cef3-4d67-9ae4-624bd1176b8f.png)'
- en: 'Here, the coefficient, λ, controls the relative importance between the two
    losses. CycleGAN aims to solve the following minimax objective:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，系数λ控制两个损失之间的相对重要性。CycleGAN旨在解决以下最小最大目标：
- en: '![](img/2d6daef8-178c-4fda-884e-faf0f245ecd0.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d6daef8-178c-4fda-884e-faf0f245ecd0.png)'
- en: Implementing CycleGAN
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现CycleGAN
- en: This example contains several source files located at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05/cyclegan](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05/cyclegan).
    Besides TF, the code also depends on `tensorflow_addons` and `imageio` packages.
    You can install them with the `pip` package installer. We'll implement CycleGAN
    for multiple training datasets, all of which were provided by the authors of the
    paper. Before you run the example, you have to download the relevant dataset with
    the help of the `download_dataset.sh` executable script, which uses the dataset
    name as an argument. The list of available datasets is included in the file. Once
    you've downloaded this, you can access the images with the help of the `DataLoader`
    class, which is located in the `data_loader.py` module (we won't include its source
    code here). Suffice to say that the class can load mini-batches and whole datasets
    of normalized images as `numpy` arrays. We'll also omit the usual imports.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子包含了几个源文件，位于[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05/cyclegan](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05/cyclegan)。除了
    TensorFlow，代码还依赖于`tensorflow_addons`和`imageio`包。你可以使用`pip`包管理器安装它们。我们将为多个训练数据集实现CycleGAN，这些数据集均由论文的作者提供。在运行示例之前，你需要通过`download_dataset.sh`可执行脚本下载相关数据集，该脚本使用数据集名称作为参数。可用数据集的列表已包含在文件中。下载完成后，你可以借助`DataLoader`类访问图像，该类位于`data_loader.py`模块中（我们不会在此包含其源代码）。简单来说，`DataLoader`类可以加载`numpy`数组形式的mini-batches和整个归一化图像数据集。我们还将省略常见的导入语句。
- en: Building the generator and discriminator
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建生成器和判别器
- en: 'First, we''ll implement the `build_generator` function. The GAN models we''ve
    looked at so far started with some sort of latent vector. But here, the generator
    input is an image from one of the domains and the output is an image from the
    opposite domain. Following the paper''s guidelines, the generator is a U-Net style
    network. It has a downsampling encoder, an upsampling decoder, and shortcut connections
    between the corresponding encoder/decoder blocks. We''ll start with the `build_generator`
    definition:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现`build_generator`函数。到目前为止，我们看到的GAN模型都是从某种潜在向量开始的。但在这里，生成器的输入是来自其中一个领域的图像，输出是来自另一个领域的图像。按照论文中的指南，生成器采用U-Net风格的网络。它有一个下采样的编码器，一个上采样的解码器，以及在相应的编码器/解码器块之间的快捷连接。我们将从`build_generator`的定义开始：
- en: '[PRE21]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The U-Net downsampling encoder consists of a number of convolutional layers
    with `LeakyReLU` activations, followed by `InstanceNormalization`. The difference
    between batch and instance normalization is that batch normalization computes
    its parameters across the whole mini-batch, while instance normalization computes
    them separately for each image of the mini-batch. For clarity, we''ll implement
    a separate subroutine called `downsampling2d`, which defines one such layer. We''ll
    use this function to build the necessary number of layers when we build the network
    encoder (please note the indentation here; `downsampling2d` is a subroutine defined
    within `build_generator`):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 的下采样编码器由多个卷积层和 `LeakyReLU` 激活组成，后跟 `InstanceNormalization`。批量归一化和实例归一化的区别在于，批量归一化计算它的参数是跨整个小批量的，而实例归一化是单独为小批量中的每个图像计算参数。为了更清晰，我们将实现一个单独的子程序，名为
    `downsampling2d`，它定义了这样一层。我们将使用此函数在构建网络编码器时构建所需数量的层（请注意这里的缩进；`downsampling2d`
    是在 `build_generator` 中定义的子程序）：
- en: '[PRE22]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, let''s focus on the decoder, which isn''t implemented with transpose
    convolutions. Instead, the input data is upsampled with the `UpSampling2D` operation,
    which simply duplicates each input pixel as a 2×2 patch. This is followed by a
    regular convolution to smooth out the patches. This smoothed output is concatenated
    with the shortcut (or `skip_input`) connection from the corresponding encoder
    block. The decoder consists of a number of such upsampling blocks. For clarity,
    we''ll implement a separate subroutine called `upsampling2d`, which defines one
    such block. We''ll use it to build the necessary number of blocks for the network
    decoder (please note the indentation here; `upsampling2d` is a subroutine defined
    within `build_generator`):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注解码器，它不是通过转置卷积来实现的。相反，输入数据通过 `UpSampling2D` 操作进行上采样，这只是将每个输入像素复制为一个
    2×2 的块。接着是一个常规卷积操作，以平滑这些块。这个平滑后的输出会与来自相应编码器块的捷径（或 `skip_input`）连接进行拼接。解码器由多个这样的上采样块组成。为了更清晰，我们将实现一个单独的子程序，名为
    `upsampling2d`，它定义了这样一个块。我们将使用它来构建网络解码器所需的多个块（请注意这里的缩进；`upsampling2d` 是在 `build_generator`
    中定义的子程序）：
- en: '[PRE23]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we''ll implement the full definition of the U-Net using the subroutines
    we just defined (please note the indentation here; the code is part of `build_generator`):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用刚刚定义的子程序实现 U-Net 的完整定义（请注意这里的缩进；代码是 `build_generator` 的一部分）：
- en: '[PRE24]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, we should implement the `build_discriminator` function. We'll omit the
    implementation here because it is a fairly straightforward CNN, similar to those
    shown in the previous examples (you can find this in the book's GitHub repository).
    The only difference is that, instead of using batch normalization, it uses instance
    normalization.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们应该实现 `build_discriminator` 函数。我们在这里省略实现，因为它是一个相当直接的 CNN，类似于前面示例中展示的（你可以在本书的
    GitHub 仓库中找到此实现）。唯一的不同是，它使用实例归一化，而不是批量归一化。
- en: Putting it all together
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合起来
- en: 'At this point, we usually implement the `train` method, but because CycleGAN
    has more components, we''ll show you how to build the entire model. First, we
    instantiate the `data_loader` object, where you can specify the name of the training
    set (feel free to experiment with the different datasets). All the images will
    be resized to `img_res=(IMG_SIZE, IMG_SIZE)` for the network input, where `IMG_SIZE
    = 256` (you can also try `128` to speed up the training process):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们通常会实现 `train` 方法，但由于 CycleGAN 有更多的组件，我们将展示如何构建整个模型。首先，我们实例化 `data_loader`
    对象，你可以指定训练集的名称（可以随意尝试不同的数据集）。所有图像将被调整为 `img_res=(IMG_SIZE, IMG_SIZE)`，作为网络输入，其中
    `IMG_SIZE = 256`（你也可以尝试 `128` 以加速训练过程）：
- en: '[PRE25]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we''ll define the optimizer and the loss weights:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将定义优化器和损失权重：
- en: '[PRE26]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we''ll create the two generators, `g_XY` and `g_YX`, and their corresponding
    discriminators, `d_Y` and `d_X`. We''ll also create the `combined` model to train
    both generators simultaneously. Then, we''ll create the composite loss function,
    which contains an additional identity mapping term. You can read more about it
    in the respective paper, but in short, it helps preserve color composition between
    the input and the output when translating images from the painting domain to the
    photo domain:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建两个生成器，`g_XY` 和 `g_YX`，以及它们各自的判别器，`d_Y` 和 `d_X`。我们还将创建 `combined` 模型，以同时训练这两个生成器。然后，我们将创建复合损失函数，其中包含一个额外的身份映射项。你可以在相关论文中了解更多内容，但简而言之，它有助于在将图像从绘画领域转换到照片领域时，保持输入和输出之间的颜色组合：
- en: '[PRE27]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, let''s configure the `combined` model for training:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们配置用于训练的`combined`模型：
- en: '[PRE28]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once the model is ready, we initiate the training process with the `train`
    function. In line with the paper''s guidelines, we will use a mini-batch of size
    1:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型准备好，我们便通过`train`函数启动训练过程。根据论文的指导方针，我们将使用大小为1的小批量：
- en: '[PRE29]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we''ll implement the `train` function. It is somewhat similar to the
    previous GAN models, but it also takes the two pairs of generators and discriminators
    into account:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将实现`train`函数。它与之前的GAN模型有些相似，但也考虑到了两对生成器和判别器：
- en: '[PRE30]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The training may take a while to finish, but the process will generate images
    after each `sample_interval` batch. The following shows some examples of the images
    that were generated by the Center for Machine Perception facade database ([http://cmp.felk.cvut.cz/~tylecr1/facade/](http://cmp.felk.cvut.cz/~tylecr1/facade/)).
    It contains building facades, where each pixel is labeled as one of multiple facade-related
    categories, such as windows, doors, balconies, and so on:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要一些时间才能完成，但该过程将在每个`sample_interval`批次后生成图像。以下是通过机器感知中心外立面数据库生成的图像示例（[http://cmp.felk.cvut.cz/~tylecr1/facade/](http://cmp.felk.cvut.cz/~tylecr1/facade/)）。该数据库包含建筑物的外立面，其中每个像素都标记为与外立面相关的多个类别之一，如窗户、门、阳台等：
- en: '![](img/2a6ef916-a9a3-4107-bc54-86806b502521.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a6ef916-a9a3-4107-bc54-86806b502521.png)'
- en: An example of CycleGAN image-to-image translation
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN图像到图像的转换示例
- en: This concludes our discussion of GANs. Next, we'll focus on a different type
    of generative model called artistic style transfer.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了我们对GAN的讨论。接下来，我们将重点介绍另一种生成模型——艺术风格迁移。
- en: Introducing artistic style transfer
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入艺术风格迁移
- en: In this final section, we'll discuss artistic style transfer. Similar to one
    of the applications of CycleGAN, it allows us to use the style (or texture) of
    one image to reproduce the semantic content of another. Although it can be implemented
    with different algorithms, the most popular way was introduced in 2015 in the *A
    Neural Algorithm of Artistic* *Style* paper ([https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)).
    It's also known as neural style transfer and it uses (you guessed it!) CNNs. The
    basic algorithm has been improved and tweaked over the past few years, but in
    this section we'll explore its original form as this will give us a good foundation
    for understanding the latest versions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一最终部分，我们将讨论艺术风格迁移。类似于CycleGAN的一种应用，它允许我们使用一张图像的风格（或纹理）来再现另一张图像的语义内容。尽管它可以通过不同的算法来实现，但最流行的方法是在2015年通过论文《*艺术风格的神经算法*》中提出的（[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)）。它也被称为神经风格迁移，并且使用了（你猜对了！）卷积神经网络（CNN）。基本算法在过去几年中得到了改进和调整，但在本节中，我们将探讨其原始形式，因为这将为理解最新版本奠定良好的基础。
- en: 'The algorithm takes two images as input:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法接受两张图像作为输入：
- en: The content image (*C*) we would like to redraw
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要重新绘制的内容图像（*C*）
- en: The style image (I) whose style (texture) we'll use to redraw *C*
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用来重新绘制*C*的风格图像（I），其风格（纹理）来自此图像
- en: 'The result of the algorithm is a new image: *G = C + S*. The following is an
    example of neural style transfer:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的结果是一张新图像：*G = C + S*。以下是神经风格迁移的示例：
- en: '![](img/5881e581-891e-4120-bdd1-38e5aec76e40.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5881e581-891e-4120-bdd1-38e5aec76e40.png)'
- en: An example of neural style transfer
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移的示例
- en: 'To understand how neural style transfer works, let''s recall that CNNs learn a
    hierarchical representation of their features. We know that initial convolutional
    layers learn basic features, such as edges and lines. Conversely, deeper layers
    learn more complex features, such as faces, cars, and trees. Knowing this, let''s
    look at the algorithm itself:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解神经风格迁移是如何工作的，让我们回顾一下CNN如何学习其特征的层次化表示。我们知道，初始的卷积层学习基本特征，如边缘和线条。而较深的层则学习更复杂的特征，如面孔、汽车和树木。了解这一点后，让我们来看一下算法本身：
- en: Like many other tasks (for example, [Chapter 3](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)*, Object
    Detection and Image Segmentation*), this algorithm starts with a pretrained VGG
    network.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像许多其他任务一样（例如，[第3章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)*，物体检测与图像分割*），该算法从一个预训练的VGG网络开始。
- en: Feed the network with the content image, *C*. Extract and store the output activations
    (or feature maps or slices) of one or more of the hidden convolutional layers
    in the middle of the network. Let's denote these activations with *A[c]^l*, where *l* is
    the index of the layer. We're interested in the middle layers because the level
    of feature abstraction encoded in them is best suited for this task.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将内容图像*C*输入网络。提取并存储网络中间层一个或多个隐藏卷积层的输出激活（或特征图或切片）。我们用*A[c]^l*表示这些激活，其中*l*是层的索引。我们关注中间层，因为它们编码的特征抽象层次最适合这个任务。
- en: Do the same with the style image, *S*. This time, denote the style activations
    of the *l* layer with *A[s]^l*. The layers we choose for the content and style
    are not necessarily the same.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对风格图像*S*做同样的处理。这一次，用*A[s]^l*表示风格图像在*l*层的激活。我们选择的内容和风格层不一定相同。
- en: 'Generate a single random image (white noise), *G*. This random image will gradually
    turn into the end result of the algorithm. We''ll repeat this for a number of
    iterations:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一张单一的随机图像（白噪声），*G*。这张随机图像将逐渐转变为算法的最终结果。我们将对其进行多次迭代：
- en: Propagate *G* through the network. This is the only image we'll use throughout
    the whole process. Like we did previously, we'll store the activations for all
    the *l* layers (here, *l* is a combination of all layers we used for the content
    and style images). Let's denote these activations with *A[g]^l*.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*G*传播通过网络。这是我们在整个过程中唯一使用的图像。像之前一样，我们将存储所有*l*层的激活（这里，*l*是我们用于内容和风格图像的所有层的组合）。我们用*A[g]^l*表示这些激活。
- en: 'Compute the difference between the random noise activations, *A[g]^l*, on one
    hand and *A[c]^l* and *A[s]^l* on the other. These will be the two components of
    our loss function:'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算随机噪声激活*A[g]^l*与*A[c]^l*和*A[s]^l*之间的差异。这些将是我们损失函数的两个组成部分：
- en: '![](img/40dba1df-c29d-4daf-b191-cf57362f210e.png), known as the **content loss**:
    This is just the MSE over the element-wise difference between the two activations
    of all *l* layers.'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/40dba1df-c29d-4daf-b191-cf57362f210e.png)，称为**内容损失**：这只是所有*l*层两个激活之间逐元素差异的均方误差（MSE）。'
- en: '![](img/5e6ed128-6148-46ff-82e9-10b6a9f5363c.png), known as the **style loss**:
    This is similar to the content loss, but instead of raw activations we''ll compare
    their **gram matrices** (we won''t go into  this in any detail).'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/5e6ed128-6148-46ff-82e9-10b6a9f5363c.png)，称为**风格损失**：这与内容损失相似，但我们不会直接比较原始激活，而是比较它们的**格拉姆矩阵**（我们不会深入讨论这一点）。'
- en: Use the content and style losses to compute the total loss, ![](img/31512847-eade-4abd-9f49-e81cbcd56250.png),
    which is just a weighted sum of the two. The α and β coefficients determine which
    of the components will carry more weight.
  id: totrans-318
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内容和风格损失计算总损失，![](img/31512847-eade-4abd-9f49-e81cbcd56250.png)，这只是两者的加权和。α
    和 β 系数决定了哪个组件将占据更大权重。
- en: Backpropagate the gradients to the start of the network and update the generated
    image, ![](img/b0971a06-2691-47a8-b6d4-ca441b8b02e9.png). In this way, we make *G* more
    similar to both the content and style images since the loss function is a combination
    of both.
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播梯度至网络的起始位置，并更新生成的图像，![](img/b0971a06-2691-47a8-b6d4-ca441b8b02e9.png)。通过这种方式，我们使*G*更接近内容和风格图像，因为损失函数是两者的组合。
- en: This algorithm makes it possible for us to harness the powerful representational
    power of CNNs for artistic style transfer. It does this with a novel loss function
    and the smart use of backpropagation.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使我们能够利用卷积神经网络（CNN）强大的表达能力进行艺术风格迁移。它通过一种新颖的损失函数和智能的反向传播方法实现这一点。
- en: If you are interested in implementing neural style transfer, check out the official
    PyTorch tutorial at [https://pytorch.org/tutorials/advanced/neural_style_tutorial.html](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html).
    Alternatively, go to [https://www.tensorflow.org/beta/tutorials/generative/style_transfer](https://www.tensorflow.org/beta/tutorials/generative/style_transfer)
    for the TF 2.0 implementation.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣实现神经风格迁移，请查看官方的 PyTorch 教程：[https://pytorch.org/tutorials/advanced/neural_style_tutorial.html](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)。或者，访问[https://www.tensorflow.org/beta/tutorials/generative/style_transfer](https://www.tensorflow.org/beta/tutorials/generative/style_transfer)查看
    TF 2.0 实现。
- en: One shortcoming of this algorithm is that it's relatively slow. Typically, we
    have to repeat this pseudo-training procedure for a couple of hundred iterations
    to produce a visually appealing result. Fortunately, the paper *Perceptual Losses
    for Real-Time Style Transfer and Super-Resolution* ([https://arxiv.org/abs/1603.08155](https://arxiv.org/abs/1603.08155))
    builds on top of the original algorithm to provide a solution, which is three
    orders of magnitude faster.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的一个缺点是相对较慢。通常，我们需要重复几百次伪训练过程才能得到一个视觉上令人满意的结果。幸运的是，论文《*实时风格迁移与超分辨率的感知损失*》([https://arxiv.org/abs/1603.08155](https://arxiv.org/abs/1603.08155))
    基于原始算法提出了一个解决方案，使其速度提高了三个数量级。
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to create new images with generative models,
    which is one of the most exciting deep learning areas at the moment. We learned
    about the theoretical foundations of VAEs and then we implemented a simple VAE to
    generate new MNIST digits. Then, we described the GAN framework and we discussed
    and implemented multiple types of GAN, including DCGAN, CGAN, WGAN, and CycleGAN.
    Finally, we mentioned the neural style transfer algorithm. This chapter concludes
    a series of four chapters dedicated to computer vision and I really hope you've
    enjoyed them.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用生成模型创建新图像，这是目前深度学习领域最令人兴奋的方向之一。我们了解了变分自编码器（VAE）的理论基础，然后实现了一个简单的VAE来生成新的MNIST数字。接着，我们介绍了GAN框架，并讨论并实现了多种类型的GAN，包括DCGAN、CGAN、WGAN和CycleGAN。最后，我们提到了神经风格迁移算法。本章是专门讲解计算机视觉的四章系列中的最后一章，希望你喜欢这些内容。
- en: In the next few chapters, we'll talk about Natural Language Processing and recurrent
    networks.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将讨论自然语言处理和递归网络。
