- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Analyzing Adversarial Performance
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析对抗性能
- en: An adversary, in the context of machine learning models, refers to an entity
    or system that actively seeks to exploit or undermine the performance, integrity,
    or security of these models. They can be malicious actors, algorithms, or systems
    designed to target vulnerabilities within machine learning models. Adversaries
    perform adversarial attacks, where they intentionally input misleading or carefully
    crafted data to deceive the model and cause it to make incorrect or unintended
    predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型的背景下，对抗者指的是一个主动寻求利用或破坏这些模型的性能、完整性或安全性的实体或系统。他们可以是恶意行为者、算法或系统，旨在攻击机器学习模型中的漏洞。对抗者进行对抗攻击，故意输入误导性或精心设计的数据，欺骗模型并使其做出不正确或意外的预测。
- en: Adversarial attacks can range from subtle perturbations of input data to sophisticated
    methods that exploit the vulnerabilities of specific algorithms. The objectives
    of adversaries can vary depending on the context. They may attempt to bypass security
    measures, gain unauthorized access, steal sensitive information, or cause disruption
    in the model’s intended functionality. Adversaries can also target the fairness
    and ethics of machine learning models, attempting to exploit biases or discrimination
    present in the training data or model design. One example of adversaries targeting
    fairness and ethics in machine learning models is in the context of facial recognition
    systems. Consider that a facial recognition system has a bias and it performs
    better for men than for women. Adversaries can exploit this bias by deliberately
    manipulating their appearance to mislead the system. They may use makeup, hairstyles,
    or accessories to confuse the facial recognition algorithms and make it harder
    for the system to accurately identify them. By doing so, adversaries can exploit
    the system’s weaknesses and potentially evade detection or misdirect law enforcement
    efforts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击可以从对输入数据的微小扰动到利用特定算法漏洞的复杂方法不等。对抗者的目标可能会根据不同的上下文有所变化。他们可能试图绕过安全措施、获得未经授权的访问权限、窃取敏感信息，或者破坏模型的预期功能。对抗者也可能瞄准机器学习模型的公平性和伦理，试图利用训练数据或模型设计中存在的偏见或歧视。一个例子是对抗者针对机器学习模型中的公平性和伦理问题进行攻击，尤其是在面部识别系统中。假设一个面部识别系统存在偏见，对男性的识别效果优于女性。对抗者可以通过故意改变外貌来利用这一偏见，误导系统。他们可能使用化妆、发型或配饰来混淆面部识别算法，使得系统更难准确识别他们。通过这样做，对抗者可以利用系统的弱点，可能逃避检测或误导执法部门的努力。
- en: To counter adversaries and adversarial attacks, the best first step is to analyze
    the adversarial performance of the trained machine learning models. This analysis
    allows for a better understanding of potential vulnerabilities and weaknesses
    in the models, enabling the development of targeted mitigation methods. Additionally,
    evaluating the adversarial performance can provide insights into the effectiveness
    of existing mitigation strategies and guide the improvement of future model designs.
    As an added benefit, it helps ensure that your model is well-equipped to handle
    any possible natural changes that may occur in its deployed environment, even
    in the absence of specific adversaries targeting the system.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对对抗者和对抗攻击，最好的第一步是分析训练好的机器学习模型的对抗性能。此分析有助于更好地理解模型中潜在的漏洞和弱点，从而能够开发出有针对性的缓解方法。此外，评估对抗性能还可以提供现有缓解策略有效性的洞察，并指导未来模型设计的改进。作为额外的好处，它有助于确保你的模型能够应对其部署环境中可能出现的任何自然变化，即使没有特定的对抗者针对该系统进行攻击。
- en: 'In this chapter, we will go through the adversarial performance evaluation
    of image, text, and audio data-based models separately. Specifically, the following
    topics will be discussed:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将分别介绍基于图像、文本和音频数据的模型的对抗性能评估。具体而言，以下主题将被讨论：
- en: Using data augmentations for adversarial analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强进行对抗分析
- en: Analyzing adversarial performance for audio-based models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析基于音频模型的对抗性能
- en: Analyzing adversarial performance for image-based models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析基于图像模型的对抗性能
- en: Exploring adversarial analysis for text-based models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索基于文本模型的对抗分析
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含一些在 Python 编程语言中的实际实现。要完成这些内容，您需要在计算机上安装以下库：
- en: '`matplotlib`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: '`scikit-learn`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: '`numpy`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`pytorch`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`accelerate==0.15.0`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accelerate==0.15.0`'
- en: '`captum`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`captum`'
- en: '`catalyst`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`catalyst`'
- en: '`adversarial-robustness-toolbox`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adversarial-robustness-toolbox`'
- en: '`torchvision`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchvision`'
- en: '`pandas`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: 'The code files for this chapter are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_14](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_14).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_14](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_14)。
- en: Using data augmentations for adversarial analysis
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据增强进行对抗性分析
- en: The core of the adversarial performance analysis method focuses on utilizing
    data augmentations. Data augmentation refers to the process of introducing realistic
    variations to existing data programmatically. Data augmentations are commonly
    employed during the model training process to enhance the validation performance
    and generalizability of deep learning models. However, we can also leverage augmentations
    as an evaluation method to ensure the robustness of performance under various
    conditions. By applying augmentations during evaluation, practitioners can obtain
    a more detailed and comprehensive estimation of the model’s performance when deployed
    in production.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性性能分析方法的核心是利用数据增强。数据增强是指通过编程方式为现有数据引入现实的变化。数据增强通常在模型训练过程中使用，以提高验证性能和深度学习模型的泛化能力。然而，我们也可以将数据增强作为一种评估方法，以确保在不同条件下的性能鲁棒性。通过在评估过程中应用数据增强，实践者可以更全面、详细地估计模型在生产环境中部署后的表现。
- en: Adversarial performance analysis offers two main advantages. Firstly, it assists
    in building a more generalizable model by enabling better model selection during
    validation in training and after training between multiple trained models. This
    is achieved through the use of augmentation prerequisite metrics. Certain use
    cases might have special conditions that are not necessarily representative of
    what is available in the raw validation or holdout partition. Augmentations can
    help change the representation of the evaluation dataset to mimic conditions in
    production. Secondly, adversarial performance analysis can be used to establish
    targeted guardrails when the model is deployed in production. By thoroughly assessing
    the model’s performance under different adversarial conditions, practitioners
    can set up specific thresholds, actions, and operating guidelines to ensure the
    model’s behavior aligns with their requirements.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性性能分析提供了两个主要优点。首先，它有助于通过在训练过程中和训练后，在多个训练模型之间进行更好的模型选择，从而构建一个更具泛化能力的模型。这是通过使用增强前提指标实现的。某些使用场景可能具有特殊条件，这些条件不一定能代表原始验证集或保持分区中的数据。数据增强可以帮助改变评估数据集的表示，以模拟生产中的条件。其次，对抗性性能分析可以用于在模型部署到生产环境时建立有针对性的保障措施。通过彻底评估模型在不同对抗性条件下的表现，实践者可以设置具体的阈值、行动和操作指南，以确保模型的行为符合其要求。
- en: 'Choosing all possible augmentations that you can think of will undoubtedly
    help align performance expectations in different conditions. However, thoughtfully
    chosen augmentations for adversarial analysis can help you more effectively extract
    value from the process instead of doing it merely for an understanding. Here are
    a few recommendations when performing adversarial performance analysis using augmentations:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 选择所有可能的增强方式无疑有助于在不同条件下对性能进行预期对齐。然而，为对抗性分析精心选择的增强方式可以帮助您更有效地从这个过程中提取价值，而不仅仅是为了理解。以下是在使用增强进行对抗性性能分析时的一些建议：
- en: '**Consider choosing augmentations that you can detect, measure, and control**:
    Do you have a system or a machine learning model that can already detect the component
    that an augmentation can change? Having a measurable component that you want to
    perform adversarial analysis associated with a chosen augmentation can help set
    up actual guardrails in production. Guardrails can range from rejecting automated
    prediction-based decisions from a model and delegating the decision to a human
    reviewer, to requiring the user or participant in the system that utilizes the
    machine learning model to resubmit input data that follows the requirements of
    the system. An example of this is having a guardrail that makes sure that the
    face is straight without any tilting in a face verification system.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑选择可以检测、测量和控制的增强方法**：你是否拥有一个系统或机器学习模型，可以已经检测到增强方法可能改变的组件？拥有一个可测量的组件，并将其与所选增强方法进行对抗性分析，可以帮助在生产环境中设置实际的保护措施。保护措施可以从拒绝模型基于自动预测做出的决策，并将决策委托给人工审阅者，到要求使用机器学习模型的系统中的用户或参与者重新提交符合系统要求的输入数据。例如，在面部验证系统中设置一个保护措施，确保面部没有任何倾斜，保持正对。'
- en: '**Think of conditions that are more likely to happen in real-life deployments**:
    By focusing on augmentations that mimic realistic conditions, practitioners can
    assess the model’s robustness and performance in situations that are relevant
    to its intended deployment.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑更可能在现实部署中发生的条件**：通过专注于模仿现实条件的增强方法，实践者可以评估模型在与其预期部署相关的情况下的稳健性和表现。'
- en: '**Evaluate the performance of the model at various degrees of strength of a
    chosen augmentation to understand performance more comprehensively**: Knowing
    the range of values where performance is at its peak and its bottom will help
    you make the proper actions. However, some augmentation methods only have a binary
    parameter such as added or not added augmentation and it’s okay to just compare
    the difference in performance from applying and not applying. For example, whether
    a horizontal flip has been applied or not.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在不同强度的增强方法下评估模型性能，以便更全面地理解性能**：了解性能达到峰值和最低点的范围，将帮助你采取正确的行动。然而，某些增强方法仅有二元参数，如是否添加了增强效果，完全可以通过比较应用和不应用时的性能差异来进行评估。例如，是否应用了水平翻转。'
- en: '**Consider evaluating performance jointly with multiple augmentations**: Real-world
    situations often involve a combination of factors that can affect the performance
    of a model. By testing the model’s performance with multiple augmentations applied
    simultaneously, you can better understand its ability to handle complex scenarios
    and identify potential weaknesses that may not be apparent when evaluating single
    augmentations individually.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑联合评估多种增强方法的性能**：现实世界中的情况通常涉及多种因素，这些因素可能会影响模型的表现。通过同时应用多种增强方法来测试模型的表现，可以更好地了解模型处理复杂场景的能力，并识别出在单独评估单一增强方法时可能未显现的潜在弱点。'
- en: '**Consider using popular adversarial examples or methods that generate adversarial
    examples**: Utilizing well-known adversarial examples or techniques can help you
    identify common vulnerabilities in your model that may have been overlooked. By
    identifying and mitigating them, you would have already defended against a significant
    portion of potential attacks, as these popular methods are more likely to be employed
    by adversaries.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑使用流行的对抗性示例或生成对抗性示例的方法**：利用知名的对抗性示例或技术，可以帮助你识别模型中可能被忽视的常见漏洞。通过识别并缓解这些漏洞，你已经防御了潜在攻击的相当一部分，因为这些流行方法更有可能被对手采用。'
- en: '**Grouping real data with targeted traits for assessment can be more effective
    for adversarial performance analysis instead of using augmentations**: Sometimes,
    augmentations can’t replicate real-life situations properly, so collecting and
    analyzing against real data samples with specific adversarial characteristics
    can provide a more accurate assessment of the model’s performance in real-world
    scenarios.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将具有目标特征的真实数据进行分组进行评估，比使用增强方法对对抗性性能分析更有效**：有时候，增强方法无法正确复制现实生活中的情况，因此，收集并分析具有特定对抗特征的真实数据样本，可以提供对模型在现实世界场景中表现的更准确评估。'
- en: In short, evaluate augmentations that are actionable. In general, valuable model
    insights are the ones that are actionable by any means. Next, we will go through
    our first practical example of adversarial analysis using audio-based models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，评估可执行的增强方法。一般来说，具有价值的模型洞察是任何方式都能执行的。接下来，我们将通过第一个实际示例来进行基于音频的模型对抗分析。
- en: Analyzing adversarial performance for audio-based models
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析基于音频的模型的对抗性能
- en: 'Adversarial analysis for audio-based models requires audio augmentations. In
    this section, we will be leveraging the open source `audiomentations` library
    to apply audio augmentation methods. We will analyze the adversarial accuracy-based
    performance of a speech recognition model practically. The accuracy metric we’ll
    use is the **Word Error Rate** (**WER**), which is a commonly used metric in automatic
    speech recognition and machine translation systems. It measures the dissimilarity
    between a system’s output and the reference transcription or translation by calculating
    the sum of word substitutions, insertions, and deletions divided by the total
    number of reference words, resulting in a percentage value. The formula for WER
    is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于音频的模型的对抗分析需要音频增强方法。在这一部分，我们将利用开源的`audiomentations`库来应用音频增强方法。我们将实际分析语音识别模型的对抗准确度表现。我们将使用的准确度指标是**词错率**（**WER**），这是自动语音识别和机器翻译系统中常用的指标。它通过计算单词替换、插入和删除的总和，再除以参考单词的总数，得出一个百分比值，从而衡量系统输出与参考转录或翻译之间的差异。WER的公式如下：
- en: WER = (S + I + D) / N
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: WER = (S + I + D) / N
- en: 'Here, we have the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: S represents the number of word substitutions
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S代表单词替换的数量
- en: I represents the number of word insertions
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I代表单词插入的数量
- en: D represents the number of word deletions
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D代表单词删除的数量
- en: N is the total number of words in the reference transcription or translation
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N是参考转录或翻译中的总单词数
- en: 'The following augmentations are considered for the analysis:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下增强方法将被考虑用于分析：
- en: '**Pronunciation speed augmentation**: Altering the speed of word pronunciation
    can have a significant impact on WER. Increasing the speed (time compression)
    may lead to more errors due to compressed phonetic information, while decreasing
    the speed (time expansion) may result in more accurate transcriptions. As words
    can be long and short, syllables per minute will be a good estimator of this without
    any special machine learning model.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发音速度增强**：改变单词发音的速度可以对WER产生显著影响。提高发音速度（时间压缩）可能导致更多错误，因为语音信息被压缩，而降低发音速度（时间扩展）可能会产生更准确的转录。由于单词的长短不同，每分钟音节数将是一个很好的估算指标，无需任何特殊的机器学习模型。'
- en: '**Speech pitch**: Changing the pitch of speech can affect the perception and
    recognition of spoken words. Augmentations such as pitch shifting can introduce
    variations in pitch, which can influence WER performance. Women and men generally
    have different pitch ranges, which can work as a proxy for measuring this, so
    we will not directly analyze pitch in this topic. Pitch can be measured either
    by machine learning models or rule-based scientific methods.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音音调**：改变语音音调可以影响口语单词的感知和识别。音调变化等增强方法可以引入音调的变化，进而影响WER性能。女性和男性的音调范围通常不同，这可以作为衡量音调的代理，因此我们不会在本主题中直接分析音调。音调可以通过机器学习模型或基于规则的科学方法来测量。'
- en: '**Background noise**: The presence of background noise can negatively impact
    speech recognition systems. Background noise can be created algorithmically, such
    as Gaussian noise, or it can be strategically chosen types of real-world background
    noise that can exist in real environments, such as car or motorbike sounds. However,
    its presence can’t be detected simply and has to depend on a machine learning
    model or manual environment controls.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景噪声**：背景噪声的存在可能会对语音识别系统产生负面影响。背景噪声可以通过算法生成，如高斯噪声，也可以是通过有策略地选择现实环境中可能存在的背景噪声类型，如汽车或摩托车声音。然而，其存在不能简单地被检测，必须依赖于机器学习模型或手动环境控制。'
- en: '**Speech loudness/magnitude**: The loudness or volume of speech can play a
    crucial role in speech recognition. Increasing or decreasing the loudness of the
    speech can introduce variability that reflects real-world conditions. Common speech
    datasets are collected in a closed environment without any external noise. This
    makes it easy to control the loudness of the speech by using simple mathematical
    methods.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音响度/强度**：语音的响度或音量在语音识别中起着至关重要的作用。增加或减少语音的响度可能引入反映现实条件的变化。常见的语音数据集是在没有任何外部噪声的封闭环境中收集的。这使得通过简单的数学方法就能轻松控制语音的响度。'
- en: '*Figure 14**.1* shows the adversarial performance analysis graph plots of four
    components: pronunciation speed performance analysis in graph *(a)*, Gaussian
    noise performance analysis in graph *(b)*, speech loudness performance analysis
    in graph *(c)*, and real-life background noise performance analysis (motorbikes
    noise) in graph *(d)*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.1*展示了四个组成部分的对抗性能分析图：语音速度性能分析图*(a)*，高斯噪音性能分析图*(b)*，语音响度性能分析图*(c)*，以及现实生活背景噪音（摩托车噪音）性能分析图*(d)*：'
- en: '![Figure 14.1 – Adversarial analysis results for a speech recognition model
    with WER](img/B18187_14_1.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.1 – 针对语音识别模型的对抗分析结果（WER）](img/B18187_14_1.jpg)'
- en: Figure 14.1 – Adversarial analysis results for a speech recognition model with
    WER
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – 针对语音识别模型的对抗分析结果（WER）
- en: In *Figure 14**.1(a)*, the performance seems to be the best at between 2.5 to
    4 syllables spoken per second across all categories, and males seem to be more
    susceptible to performance degradations going out of this range. In *Figure 14**.1(b)*,
    the models perform at an optimum level at every SNR value after 30 dB. Therefore,
    a simple guardrail would be to ensure speech is always at least 30 dB louder than
    any background noise, which can be measured by hardware. In *Figure 14**.1(c)*,
    it’s obvious that the performance for females doesn’t specifically degrade no
    matter how loud or soft the speech sounds made are. However, for males, the best
    performance can be obtained when the absolute magnitude of the voice is at around
    the 20 dB range and not over it. This shows some form of bias of the model toward
    gender. *Figure 14**.1(d)* shows that there isn’t any particular special behavior
    of motorbike noises versus Gaussian noise. Consider evaluating more real-life
    background noises you can think of at the end of the practical steps!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 14.1(a)*中，性能在每秒2.5到4个音节之间似乎是最佳的，并且男性似乎更容易在超出此范围时出现性能下降。在*图 14.1(b)*中，模型在每个SNR值大于30
    dB时表现最佳。因此，一个简单的护栏就是确保语音始终比任何背景噪声大至少30 dB，这可以通过硬件进行测量。在*图 14.1(c)*中，很明显，女性的表现无论语音的响度如何，都不会特别下降。然而，对于男性来说，最佳的表现可以在语音强度大约在20
    dB范围内时获得，而不是超过这一范围。这显示了模型在性别上的某种偏差。*图 14.1(d)*显示了摩托车噪音与高斯噪音没有任何特殊的行为差异。考虑在实践步骤结束时评估更多你能想到的现实生活中的背景噪音！
- en: Executing adversarial performance analysis for speech recognition models
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行语音识别模型的对抗性能分析
- en: 'Let’s start a step-by-step practical example that will show how to obtain the
    adversarial performance analysis results of the speech recognition model presented
    in *Figure 13**.1*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个逐步的实际例子开始，展示如何获得*图 13.1*所示语音识别模型的对抗性能分析结果：
- en: 'We will be using the Speech2Text model from the Hugging Face open source platform,
    which is an English speech-trained transformer-based speech recognition model.
    Let’s start by importing the necessary libraries, the highlights of which are
    `matplotlib` for graph plotting, `numpy` for array handling, `pytorch` for handling
    the PyTorch Speech2Text model, `audiomentations` for augmentations, the Speech2Text
    model, and the preprocessor from Hugging Face:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用来自Hugging Face开源平台的Speech2Text模型，这是一个基于变换器的英语语音训练语音识别模型。首先，导入必要的库，重点是`matplotlib`用于绘图，`numpy`用于数组处理，`pytorch`用于处理PyTorch
    Speech2Text模型，`audiomentations`用于数据增强，Speech2Text模型和Hugging Face的预处理器：
- en: '[PRE0]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will load the trained Speech2Text model and preprocessor, and assign
    the model to a GPU device:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载已训练好的Speech2Text模型和预处理器，并将模型分配到GPU设备上：
- en: '[PRE1]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The dataset we will use as a base to evaluate the model is the English speech
    recognition `fleurs` dataset from Google, which conveniently contains gender information
    that allows us to indirectly evaluate pitch performance differences and also perform
    bias analysis. Let’s download and load the dataset:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将用于评估模型的数据集是谷歌的英语语音识别`fleurs`数据集，该数据集恰好包含了性别信息，允许我们间接评估音高表现差异，并进行偏差分析。让我们下载并加载该数据集：
- en: '[PRE2]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will load the WER evaluation method from the Hugging Face `evaluate`
    library and define the helper method to extract the WER scores from the Speech2Text
    model:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将从Hugging Face `evaluate`库加载WER评估方法，并定义帮助方法从Speech2Text模型中提取WER分数：
- en: '[PRE3]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Three scores will be returned here, which are the male-specific score, the female-specific
    score, and the overall score.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里将返回三个分数，分别是男性特定分数、女性特定分数和总体分数。
- en: 'As a follow-up, we will define the main method that will apply the augmentation
    to all the baseline samples and obtain a WER score:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义主要方法，应用增强到所有基准样本，并获取WER分数：
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'First, we will analyze the adversarial performance for the pronunciation speed
    component using the time stretch method from `audiomentation`. The dataset contains
    audio data with different numbers of syllables spoken per second, so we have to
    make sure all of the audio data has the same number of syllables per second before
    starting the analysis. Let’s start by finding the mean number of syllables spoken
    per second:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用`audiomentation`的时间拉伸方法分析发音速度组件的对抗性表现。数据集包含了每秒发音音节数不同的音频数据，因此在开始分析之前，我们必须确保所有音频数据每秒发音的音节数相同。我们先来计算每秒发音的平均音节数：
- en: '[PRE5]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can obtain the initial set of baseline audio samples for pronunciation-based
    analysis:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以获取用于发音分析的初始基准音频样本集：
- en: '[PRE6]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we will perform the adversarial WER analysis using a range of different
    speed-up and speed-down rates:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用不同的加速和减速率进行对抗性WER分析：
- en: '[PRE7]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'By running the following plotting code, you will obtain the graph shown in
    *Figure 14**.1(a)*:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下绘图代码，你将得到如*图14.1(a)*所示的图表：
- en: '[PRE8]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we will move forward to the next augmentation component that algorithmically
    generates Gaussian background noise. In this example, we will be controlling the
    **signal-to-noise ratio** (**SNR**) of the speech signal and the Gaussian noise.
    In this case, the original data can be used as the baseline without any equalization:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将进入下一个增强组件，该组件算法生成高斯背景噪声。在这个例子中，我们将控制**信噪比**（**SNR**）在语音信号和高斯噪声之间的关系。在这种情况下，原始数据可以作为基准，且无需进行均衡：
- en: '[PRE9]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'By running the following plotting code, you will obtain the graph shown in
    *Figure 14**.1(b)*:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下绘图代码，你将得到如*图14.1(b)*所示的图表：
- en: '[PRE10]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we will analyze the WER performance at different speech loudness. The
    dataset from [https://huggingface.co/datasets/google/fleurs](https://huggingface.co/datasets/google/fleurs)
    was made in a closed environment without background noise, which makes it straightforward
    to compute magnitude with the raw speech audio data array with the following code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将分析不同语音响度下的WER表现。来自[https://huggingface.co/datasets/google/fleurs](https://huggingface.co/datasets/google/fleurs)的数据集是在没有背景噪音的封闭环境中制作的，因此可以通过以下代码直接计算原始语音音频数据数组的幅度：
- en: '[PRE11]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'By running the following plotting code, you will obtain the graph shown in
    *Figure 14**.1(c)*:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下绘图代码，你将得到如*图14.1(c)*所示的图表：
- en: '[PRE12]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we will be analyzing the adversarial performance of real background
    noise from the real world. We will use motorbike sounds from the `Freesound50k`
    dataset and mix them into the original audio data at different SNRs with the following
    code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将分析来自现实世界的真实背景噪音的对抗性表现。我们将使用来自`Freesound50k`数据集的摩托车声音，并将其与原始音频数据在不同SNR下混合，使用以下代码：
- en: '[PRE13]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'By running the following plotting code, you will obtain the graph shown in
    *Figure 14**.1(d)*:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下绘图代码，你将得到如*图14.1(d)*所示的图表：
- en: '[PRE14]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With that, we have attempted to analyze the adversarial performance of a speech
    recognition model. Consider extending the analysis on your end and try to evaluate
    multiple augmentations jointly. For example, consider scenarios where multiple
    background voices are present. Augmenting audio with different magnitudes of background
    voices can simulate scenarios with varying levels of interference.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们尝试分析了语音识别模型的对抗性性能。可以考虑在你的端进行扩展分析，尝试联合评估多种增强方法。例如，考虑多个背景声音同时存在的场景。通过不同程度的背景声音增强音频，可以模拟不同干扰程度的场景。
- en: After performing the analysis, apart from using these augmentations during training
    to mitigate poor performance, you can add guardrails in the production environment.
    As an example, consider the situation in which speech recognition outputs are
    used as input to an LLM model such as ChatGPT to prevent wrong results under the
    hood. An example guardrail would be to reroute the system to ask for manual verification
    of the speech recognized before submitting it to ChatGPT when specific background
    noises are detected in the audio background. Making subsequent actionable processes
    is crucial to unlocking value from insights. Next, let’s discover adversarial
    analysis for image-based models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行分析后，除了在训练过程中使用这些增强方法来缓解不良性能外，还可以在生产环境中添加保护措施。例如，考虑将语音识别输出作为输入传递给像ChatGPT这样的LLM模型，以防止内部错误结果的情况。一个保护措施的例子是，当检测到音频中的特定背景噪声时，重新路由系统，要求在将语音识别结果提交给ChatGPT之前进行人工验证。制定后续的可执行流程对于从洞察中释放价值至关重要。接下来，让我们探讨基于图像的模型的对抗性分析。
- en: Analyzing adversarial performance for image-based models
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图像的模型的对抗性性能分析
- en: 'Augmentations-based adversarial analysis can also be applied to image-based
    models. The key here is to discover possible degradations of accuracy-based performance
    in original non-existent conditions in the validation dataset. Here are some examples
    of components that could be evaluated by augmentations for the image domain:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基于增强的对抗性分析也可以应用于基于图像的模型。关键在于发现原本不存在的条件下，在验证数据集中可能导致准确度下降的因素。以下是一些可以通过增强方法评估的图像领域组件示例：
- en: '**Object of interest size**: In use cases that use CCTV camera image input,
    adversarial analysis can help us set up the camera with an appropriate distance
    so that optimal performance can be achieved. The original image can be iteratively
    resized into various sizes and overlayed on top of a base black image to perform
    analysis.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标物体的大小**：在使用闭路电视摄像头图像输入的应用场景中，对抗性分析可以帮助我们设置摄像头的适当距离，从而实现最佳性能。原始图像可以反复调整大小并覆盖在基础黑色图像上进行分析。'
- en: '**The roll orientation of the object of interest**: Pitch and yaw orientation
    is not straightforward to augment. However, rotation augmentation can help stress
    test roll orientation performance. Optimal performance can be enforced by any
    pose orientation detection model or system.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标物体的滚动方向**：俯仰和偏航方向不易增强。然而，旋转增强可以帮助对滚动方向的性能进行压力测试。任何姿态方向检测模型或系统都可以强制实现最佳性能。'
- en: '**Level of blurriness**: Images can be blurred and there are off-the-shelf
    image blurriness detectors from the OpenCV library for this. Blur augmentation
    can help stress test this component.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模糊程度**：图像可以被模糊化，OpenCV库中有现成的图像模糊检测器可以使用。模糊增强可以帮助对该组件进行压力测试。'
- en: '`albumentations` library provides rain, snow, fog, and sun augmentations!'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`albumentations`库提供了雨、雪、雾和阳光增强方法！'
- en: In addition to the method of using augmentation for adversarial analysis to
    assess performance under different conditions, numerous other widely recognized
    and extensively researched approaches exist for conducting adversarial attacks
    against image-based models. The term “popular” here also means that the techniques
    are easily accessible to potential attackers, allowing them to readily experiment
    with these methods. Consequently, it becomes crucial to thoroughly analyze such
    attacks due to their increased likelihood and potential impact.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用增强方法进行对抗性分析来评估不同条件下的性能外，还有许多其他广泛认可和广泛研究的方法，可以用于对基于图像的模型进行对抗性攻击。这里所说的“流行”也意味着这些技术对潜在攻击者容易获取，使他们能够轻松尝试这些方法。因此，必须彻底分析这些攻击，因为它们的发生概率和潜在影响增加了。
- en: 'These attacks try to obtain an adversarial image to fool the model through
    the optimization of one of the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击试图通过优化以下之一来获得一个对抗性图像，从而欺骗模型：
- en: An image perturbation matrix that acts as a noise mixer to the original image
    while maintaining a high perceived visual similarity to the original image
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种图像扰动矩阵，它作为噪声混合器作用于原始图像，同时保持与原始图像的高度视觉相似性
- en: An image patch that can be digitally overlayed on the original image and printed
    in the real world to evade detection or confuse the model
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种可以数字化叠加在原始图像上的图像补丁，并能在现实世界中打印出来以规避检测或迷惑模型
- en: '*Figure 14**.2* shows examples of these two optimization approaches for adversarial
    attacks:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.2*展示了两种针对对抗性攻击的优化方法示例：'
- en: '![Figure 14.2 – Examples of adversarial image patches and adversarial image
    noise mixers](img/B18187_14_2.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 对抗性图像补丁和对抗性图像噪声混合器示例](img/B18187_14_2.jpg)'
- en: Figure 14.2 – Examples of adversarial image patches and adversarial image noise
    mixers
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 对抗性图像补丁和对抗性图像噪声混合器示例
- en: The top left adversarial image patch is targeted on the **YOLOv2** image detection
    model on the class that represents a traffic stop sign and is capable of fooling
    the model to predict some other random class when printed and patched on the stop
    sign physically in the real world. The patch can also transfer its adversarial
    properties to the **Faster-RCNN** image object detection model. The bottom left
    adversarial image patch is an image that’s been optimized to fool the YOLOv2 detector
    model into not detecting a human when the printed patch is anywhere on a person.
    The right adversarial image noise mixer example shows the original image when
    added with the noise on the right produces an image that is visually indistinguishable
    from the original image. The ResNet50 model, which was used to build the model
    in the previous chapter, accurately predicts the right person on the original
    image but fails to predict the right person after adding the noise shown in the
    top right of the figure. Image patches are relevant attacks for CCTV based on
    real-time computer vision applications. For example, thieves want to prevent facial
    object detection. An adversarial image noise mixer is relevant for use cases where
    a user can provide their own data. For example, social media platforms and media-sharing
    platforms such as Instagram and YouTube want to filter and control media that
    can be uploaded using machine learning, and users would want to bypass the machine
    learning guardrails.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 左上方的对抗性图像补丁是针对**YOLOv2**图像检测模型的，它作用于代表交通停止标志的类别，并能在现实世界中打印出来并贴在停车标志上时，使模型预测出其他随机类别。该补丁还能够将其对抗性特性转移到**Faster-RCNN**图像物体检测模型。左下方的对抗性图像补丁是通过优化来欺骗YOLOv2检测模型，使其在打印补丁贴在一个人身上时无法检测到人。右侧的对抗性图像噪声混合器示例展示了，当噪声添加到原始图像右侧时，生成的图像与原始图像在视觉上无法区分。ResNet50模型（用于上一章构建模型时）准确预测了原始图像中的正确人物，但在添加了右上方图示中的噪声后，未能预测出正确的人物。图像补丁是基于实时计算机视觉应用的CCTV相关攻击。例如，窃贼希望阻止面部物体检测。对抗性图像噪声混合器适用于用户可以提供自己数据的用例。例如，社交媒体平台和媒体共享平台（如Instagram和YouTube）希望通过机器学习来筛选和控制可以上传的媒体，而用户则希望绕过这些机器学习防护措施。
- en: Audio noise mixers and audio patches can also be employed in adversarial attacks
    against audio-based models, following a similar approach as with image-based models.
    Audio noise mixers introduce carefully crafted noise to the original audio signal
    to create an adversarial example that maintains a high perceived auditory similarity
    to the original audio while fooling the model. This can be particularly relevant
    in applications such as voice recognition systems and audio content filtering,
    where adversaries might attempt to bypass security measures or manipulate system
    outputs. Audio patches, on the other hand, involve creating and overlaying adversarial
    audio segments onto the original audio signal. These patches can be designed to
    either mask certain elements in the audio or introduce new elements that deceive
    the model. For instance, adversarial audio can be played naturally in the environment
    with any audio speaker device to evade voice identification systems or to trick
    speech recognition models into misinterpreting specific words or phrases.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 音频噪声混合器和音频补丁也可以用于对抗性攻击，针对基于音频的模型，方法类似于基于图像的模型。音频噪声混合器通过精心设计的噪声注入到原始音频信号中，生成一个对抗性示例，该示例在视觉上与原始音频保持高度相似，但却能欺骗模型。这在语音识别系统和音频内容过滤等应用中尤为相关，在这些应用中，对手可能会试图绕过安全措施或操控系统输出。另一方面，音频补丁涉及创建并将对抗性音频片段叠加到原始音频信号上。这些补丁可以设计为遮掩音频中的某些元素，或加入新的元素以欺骗模型。例如，对抗性音频可以通过任何音频设备自然播放在环境中，从而避开语音识别系统，或使语音识别模型误解特定的单词或短语。
- en: These techniques are engineered forms of adversarial attacks that are categorized
    into either techniques that require access to the neural network gradients and
    the model itself, or techniques that only require the prediction probabilities
    or logits of the model to optimize an image. Since it is highly unlikely that
    an attacker would have access to the neural network gradients and the model itself,
    it is more practical to evaluate adversarial image generation methods that only
    require the prediction probabilities or logits.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术是经过工程设计的对抗性攻击方法，可以分为两类：一种是需要访问神经网络梯度和模型本身的技术，另一种则只需要访问模型的预测概率或logits来优化图像。由于攻击者不太可能访问到神经网络梯度和模型本身，因此评估那些只需要预测概率或logits的对抗性图像生成方法更为实际。
- en: 'An attacker can opt to randomly generate the noise perturbation matrix and
    hope that it’ll work to fool the model. However, some algorithms can automatically
    optimize the generation of a useful noise perturbation matrix, so long as you
    have access to the probabilities or logits. One such algorithm that requires only
    the prediction logits to produce the noise needed to fool a model is called the
    **HopSkipJump** algorithm. This algorithm aims to minimize the number of queries
    that are required to generate effective adversarial examples. Here is a summary
    of the algorithm:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可以选择随机生成噪声扰动矩阵，并希望它能成功欺骗模型。然而，一些算法可以自动优化生成有用的噪声扰动矩阵，只要你能访问到概率或logits。其中一种只需预测logits就能生成欺骗模型所需噪声的算法叫做**HopSkipJump**算法。该算法旨在最小化生成有效对抗性示例所需的查询次数。以下是该算法的概述：
- en: '**Initialization**: The algorithm starts by initializing the target class and
    the initial adversarial example.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：算法首先通过初始化目标类别和初始对抗性示例开始。'
- en: '**Main loop**: The algorithm iteratively performs a series of steps until it
    successfully generates an adversarial example or reaches a predefined query limit:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主循环**：算法会反复执行一系列步骤，直到成功生成对抗性示例或达到预定义的查询限制：'
- en: '**Hop**: In this step, the algorithm performs a local search to find a perturbation
    that moves the initial adversarial example closer to the target class while ensuring
    that the perturbed example remains adversarial.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**跳跃**：在这个步骤中，算法进行局部搜索，寻找一个扰动，使得初始对抗性示例更接近目标类别，同时确保扰动后的示例仍然是对抗性的。'
- en: '**Skip**: If the *hop* step fails to find a suitable perturbation, the algorithm
    attempts a more global search by skipping some pixels. This step is designed to
    explore a larger search space efficiently.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**跳过**：如果*跳跃*步骤未能找到合适的扰动，算法将通过跳过一些像素来进行更广泛的搜索。这个步骤的目的是高效地探索更大的搜索空间。'
- en: '**Jump**: If both the *hop* and *skip* steps fail, the algorithm resorts to
    a jump operation, which performs a random perturbation on a small subset of pixels
    to encourage exploration of different directions.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**跳跃**：如果*跳跃*和*跳过*步骤都失败，算法将进行跳跃操作，对一小部分像素进行随机扰动，以鼓励探索不同的方向。'
- en: '**Decision**: After each perturbation step (*hop*, *skip*, or *jump*), the
    algorithm queries the target model to obtain its predicted class label for the
    perturbed example.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**决策**：在每个扰动步骤后（*跳跃*、*跳过*或*跳跃*），算法查询目标模型，以获取其对扰动示例的预测类别标签。'
- en: '**Stopping criteria**: The algorithm terminates if it successfully generates
    an adversarial example – that is, the target model predicts the target class for
    the perturbed example. It can also stop if the number of queries exceeds a predefined
    threshold.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**停止准则**：如果算法成功生成对抗示例——即目标模型为扰动示例预测了目标类别——则算法终止。若查询次数超过预设阈值，算法也可以停止。'
- en: The HopSkipJump algorithm combines both local and global search strategies to
    efficiently explore the space of adversarial examples while minimizing the number
    of queries to the target model. This is one of the many attacks readily available
    in an open source adversarial attack toolkit at [https://github.com/Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox).
    We will go through practical steps that allow us to get the adversarial image
    noise mixer results seen in *Figure 14**.2* and more next.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: HopSkipJump算法结合了局部和全局搜索策略，以高效地探索对抗示例的空间，同时最小化对目标模型的查询次数。这是一个可以在开源对抗攻击工具包中找到的多种攻击之一，地址为[https://github.com/Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)。接下来，我们将逐步介绍如何获得在*图14.2*中看到的对抗图像噪声混合结果等。
- en: Executing adversarial performance analysis for a face recognition model
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行面部识别模型的对抗性能分析
- en: 'Before we start, please put yourselves in the shoes and mindset of an attacker!
    The steps are as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请把自己置身于攻击者的角度和心态！步骤如下：
- en: 'First, we will load the necessary libraries. The highlights are `albumentations`
    for augmentation, `torch` for the neural network model, and `art` for the adversarial
    example generation algorithm:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载必要的库。重点是`albumentations`用于数据增强，`torch`用于神经网络模型，`art`用于对抗示例生成算法：
- en: '[PRE15]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we will reuse the trained face classification model we built in [*Chapter
    13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*, which is
    a ResNet50 backbone model with ArcFace:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将重用我们在[*第13章*](B18187_13.xhtml#_idTextAnchor196)中构建的训练好的面部分类模型，*探索偏差与公平性*，它是一个以ResNet50为骨干的模型，结合了ArcFace：
- en: '[PRE16]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will also reuse the same dataset:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将重用相同的数据集：
- en: '[PRE17]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To start using the `HopSkipJump` class from the `art` library, we need to define
    an `art` library black box classifier instance. The class is a wrapper that sets
    the expected generated image noise matrix shape, and has access to only a fully
    isolated prediction method of any image-based model. First, let’s prepare the
    inference prediction method so that the art classifier can use it:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始使用来自`art`库的`HopSkipJump`类，我们需要定义一个`art`库的黑箱分类器实例。该类是一个包装器，设置预期的生成图像噪声矩阵形状，并且仅能访问任何基于图像的模型的完全孤立预测方法。首先，我们准备好推理预测方法，以便art分类器可以使用它：
- en: '[PRE18]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, let’s take an example image from the dataset and create the black box
    classifier with the isolated prediction method, the target image shape, the number
    of classes, and the range of accepted pixel values:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们从数据集中选择一个示例图像，并创建具有孤立预测方法、目标图像形状、类别数量和接受的像素值范围的黑箱分类器：
- en: '[PRE19]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we will initialize the `HopSkipJump` attack class with the classifier
    and set the max iteration per evaluation to `10`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用分类器初始化`HopSkipJump`攻击类，并将每次评估的最大迭代次数设置为`10`：
- en: '[PRE20]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will be running the algorithm for just 30 iterations. At every multiple
    of the tenth iteration, we will evaluate the algorithm by printing the minimum
    and maximum pixel difference to the original image, plotting the generated adversarial
    image, and predicting the generated image:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将运行该算法30次迭代。在每十次迭代的倍数时，我们将通过打印与原始图像的最小和最大像素差异、绘制生成的对抗图像并预测生成的图像来评估算法：
- en: '[PRE21]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we will plot the original image, obtain the model’s prediction on
    the original image, and print the actual label:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将绘制原始图像，获取模型在原始图像上的预测，并打印实际标签：
- en: '[PRE22]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The results are shown in *Figure 14**.3*. The more steps you take, the more
    visually similar the generated adversarial is to the original image while still
    fooling the model! The minimum and maximum pixel difference here provides a sense
    of how similar the generated image is to the original image, which will transfer
    to visual similarity:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示在*图 14.3*中。你采取的步骤越多，生成的对抗图像与原始图像的视觉相似度就越高，同时仍能欺骗模型！这里的最小和最大像素差异提供了生成图像与原始图像相似度的一个参考，这将转化为视觉上的相似性：
- en: '![Figure 14.3 – The adversarial performance of a single face example](img/B18187_14_3.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3 – 单个面孔样本的对抗性能](img/B18187_14_3.jpg)'
- en: Figure 14.3 – The adversarial performance of a single face example
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – 单个面孔样本的对抗性能
- en: 'With just the predictions of the model, we successfully fooled the model into
    misidentifying a facial identity! To advance, it would be beneficial to gain a
    deeper understanding of the model’s vulnerability to perturbation attacks by examining
    additional examples. To benchmark the method more widely, we need a way to ensure
    that the adversarial image that was generated is visually similar to the original
    image so that it makes sense from a likelihood standpoint. From the result shown
    in *Figure 14**.3*, it’s safe to assume that anything lower than the absolute
    minimum and maximum difference of 10 pixels should retrain a high enough visual
    similarity to fool a human evaluator into believing nothing is wrong while being
    able to fool the model. We will use the same HopSkipJump algorithm and a total
    of 30 iterations but will add a condition that the generated adversarial image
    needs to be under 10 pixels maximum and have a minimum absolute difference from
    the original image. If it is not, we will use the prediction on the original image
    treating it as a failure to generate a meaningful adversarial attack:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭模型的预测，我们成功地让模型错误地识别了一个面部身份！为了进一步推进，理解模型在扰动攻击中的脆弱性会很有帮助，可以通过检查更多的例子来深入了解。为了更广泛地基准化该方法，我们需要确保生成的对抗图像与原始图像在视觉上相似，这样从可能性角度来看才有意义。从*图
    14.3*中的结果来看，可以合理推测，任何低于绝对最小和最大 10 像素差异的图像，都应保持足够的视觉相似性，能够让人类评估者相信图像没有问题，同时还能欺骗模型。我们将使用相同的
    HopSkipJump 算法并进行 30 次迭代，但会添加一个条件：生成的对抗图像最大像素差异不超过 10 像素，并且与原始图像的最小绝对差异。如果不符合该条件，我们将使用原始图像的预测，视为未能生成有效的对抗攻击：
- en: 'Let’s start by taking 1,000 random stratified samples so that the evaluation
    can be done more quickly:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先随机选择 1,000 个分层样本，这样可以更快地进行评估：
- en: '[PRE23]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we will compute the predictions using the mentioned workflow and use
    the original images on the 1,000 images:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用前述的工作流程计算预测，并对这 1,000 张图片使用原始图像：
- en: '[PRE24]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let’s compare the accuracy performance of the model on the original images
    and on the adversarial images workflow we went through previously in *s**tep 7*.
    We will be using the Hugging Face `evaluate` library’s accuracy metric method:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们比较模型在原始图像和我们之前在*s**tep 7*中使用的对抗图像工作流程上的准确性表现。我们将使用 Hugging Face `evaluate`
    库的准确性评估方法：
- en: '[PRE25]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This will result in an accuracy of 56.9 on the original images and 30.9 accuracy
    with the adversarial image workflow!
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果是在原始图像上获得 56.9 的准确率，在对抗图像工作流程上则为 30.9 的准确率！
- en: In this practical example, we managed to use an automated algorithm to identify
    adversarial noise mixers that can successfully fool the model we built in the
    previous chapter!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实际示例中，我们成功使用了一个自动化算法来识别能成功欺骗我们在上一章构建的模型的对抗噪声混合器！
- en: Image models are highly susceptible to adversarial noise mixer types and adversarial
    image patch types due to their divergence from natural occurrences in the real
    world, which models may struggle to properly digest. While machine learning models
    excel at learning patterns and features from real-world data, they often struggle
    to handle synthetic perturbations that deviate significantly from the typical
    environmental conditions. These adversarial techniques exploit the vulnerabilities
    of image models, causing them to misclassify or produce erroneous outputs. Consequently,
    understanding and addressing these vulnerabilities becomes crucial in building
    robust defenses against such attacks. So, consider diving into the adversarial
    robustness toolbox repository to explore more adversarial examples and attack
    algorithms! As a recommendation to mitigate adversarial attacks in your image
    model, consider using adversarial targeted augmentation such as Gaussian noise
    and random pixel perturbator during training. By incorporating these augmentations
    during training, models can learn to be more resilient to synthetic perturbations,
    thereby increasing their overall robustness.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图像模型对对抗性噪声混合器类型和对抗性图像补丁类型极为敏感，因为这些情况与现实世界中的自然现象有很大偏离，模型可能难以正确处理。尽管机器学习模型擅长从真实世界数据中学习模式和特征，但它们通常难以应对显著偏离典型环境条件的合成扰动。这些对抗性技术利用图像模型的漏洞，导致它们误分类或产生错误的输出。因此，理解和解决这些漏洞在构建强大防御机制以应对此类攻击时至关重要。因此，考虑深入探索对抗性鲁棒性工具箱库，了解更多对抗性示例和攻击算法！为了减轻图像模型中的对抗性攻击，建议在训练过程中使用如高斯噪声和随机像素扰动器等对抗性目标增强。通过在训练过程中加入这些增强，模型可以学会更具抗扰动性，从而提高整体鲁棒性。
- en: Next, we will delve into adversarial attacks that target text-based models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨针对基于文本模型的对抗性攻击。
- en: Exploring adversarial analysis for text-based models
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索基于文本的模型的对抗性分析
- en: 'Text-based models can sometimes have performance vulnerabilities toward the
    usage of certain words, a specific inflection of a word stem, or a different form
    of the same word. Here’s an example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的模型有时在处理特定单词、单词词干的特定词形变化，或相同单词的不同形式时，可能会存在性能漏洞。以下是一个例子：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: So, adversarial analysis can be done by benchmarking performance on when you
    add important words to a sentence versus without. To mitigate such attacks, similar
    word replacement augmentation can be applied during training.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对抗性分析可以通过对比在句子中添加重要单词与不添加重要单词时的性能来进行。为了减轻此类攻击，可以在训练过程中应用类似的单词替换增强。
- en: However, when it comes to text-based models in the modern day, most widely adopted
    models now rely on a pre-trained language modeling foundation. This allows them
    to be capable of understanding natural language even after domain fine-tuning,
    and as a result, a more complex adversarial attack that utilizes natural language
    deception can be used. Consequently, it is crucial to thoroughly analyze and develop
    robust defense mechanisms against these sophisticated adversarial attacks that
    exploit natural language deception, to ensure the reliability and security of
    modern text-based models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现代基于文本的模型中，大多数广泛采用的模型现在依赖于预训练的语言模型基础。这使得它们即使经过领域微调后，仍然能够理解自然语言，因此，可以使用更复杂的对抗性攻击，利用自然语言欺骗。结果，彻底分析并开发出针对这些利用自然语言欺骗的复杂对抗性攻击的强大防御机制，对于确保现代基于文本的模型的可靠性和安全性至关重要。
- en: 'Think of natural language deception to be similar to how humans try to deceive
    each other through natural language speech. Just as people may employ various
    tactics to mislead or manipulate others, such as social engineering, context poisoning,
    and linguistic exploitation, these same methods can be used to trick a text model.
    Here is an example of a natural language-based deception on a spam/malicious email
    detection machine learning use case:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可以把自然语言欺骗看作类似于人类通过自然语言进行欺骗的方式。正如人们可能会采取各种策略来误导或操控他人，比如社会工程学、上下文毒化和语言剖析，这些相同的方法也可以用来欺骗文本模型。以下是一个基于自然语言欺骗的例子，针对垃圾邮件/恶意邮件检测的机器学习使用场景：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Social engineering involves using psychological manipulation to deceive others
    into divulging sensitive information or performing specific actions. Context poisoning
    refers to the deliberate introduction of misleading or irrelevant information
    to confuse the recipient. Meanwhile, linguistic exploitation takes advantage of
    the nuances and ambiguities of language to create confusion or misinterpretation.
    By understanding and addressing these deceptive techniques between humans and
    applying them to adversarial text analysis, we can enhance the resilience of text-based
    models against adversarial attacks and maintain their accuracy and reliability.
    Unfortunately, there isn’t an algorithmic way to benchmark such natural language
    deceptions that completely reform sentences. We will need to rely on collecting
    real-world deception data to perform adversarial analysis on this component.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 社会工程学涉及使用心理操控来欺骗他人泄露敏感信息或执行特定操作。上下文污染是指故意引入误导性或无关的信息以混淆接收者。与此同时，语言剥削则利用语言的细微差别和模糊性来制造混乱或误解。通过理解和应对这些在人与人之间的欺骗技术，并将其应用于对抗性文本分析，我们可以增强文本模型对抗对抗性攻击的韧性，并保持其准确性和可靠性。不幸的是，目前并没有算法可以完全基准化这种完全改变句子的自然语言欺骗。我们需要依赖收集真实世界的欺骗数据来对这一部分进行对抗性分析。
- en: However, there is another form of natural language adversarial attack worth
    mentioning that is becoming more widely used today to attack general questions
    and answer LLMs such as ChatGPT. Instead of reformatting the entire original text
    data, an additional malicious context is used as a pretext for the original text
    data. LLM API providers usually have built-in guardrails to prevent explicit,
    offensive, objectionable, discriminative, and personal attacks, as well as harassment,
    violence, illegal activities, misinformation, propaganda, self-harm, and any inappropriate
    content. This is to ensure the responsible use of AI and to prevent anything that
    can negatively impact individuals and society. However, a popular adversarial
    attack called “jailbreak” can remove all content generation restrictions ChatGPT
    has enforced. The jailbreak attack method is an engineered prompt that’s shared
    publicly by people and can be used as a pretext before any actual user prompt.
    Many versions of such engineered prompts are shared publicly and can be found
    easily through Google searches, allowing everybody in the world to attack ChatGPT.
    Fortunately, OpenAI has been diligently mitigating such jailbreak adversarial
    attacks as soon as they get their hands on the prompts. New versions of the jailbreak
    prompt get introduced pretty frequently and OpenAI has gotten stuck in a continuous
    loop trying to mitigate newly engineered jailbreak attacks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有另一种值得一提的自然语言对抗性攻击形式，当前在攻击一般问题与答案的LLM（如ChatGPT）时越来越广泛使用。与重新格式化整个原始文本数据不同，额外的恶意上下文被用作原始文本数据的借口。LLM
    API提供商通常会内置保护措施，以防止明显的、冒犯性的、令人反感的、歧视性的、个人攻击以及骚扰、暴力、非法活动、虚假信息、宣传、自残或任何不当内容。这是为了确保AI的负责任使用，并防止任何可能对个人和社会产生负面影响的行为。然而，一种名为“越狱”的流行对抗性攻击可以去除ChatGPT所实施的所有内容生成限制。越狱攻击方法是一种经过工程设计的提示，已经公开分享，并可以作为任何实际用户提示的前置条件。许多版本的这种工程化提示已被公开分享，并且可以通过谷歌搜索轻松找到，从而使世界上的每个人都能攻击ChatGPT。幸运的是，OpenAI一直在尽力减少这种越狱对抗性攻击，每当他们获得这些提示时都会及时应对。新的越狱提示版本更新非常频繁，OpenAI也因此陷入了一个不断循环的过程，试图应对新的工程化越狱攻击。
- en: In conclusion, it’s crucial to perform adversarial analysis on text models to
    enhance their resilience against deceptive techniques and attacks. By understanding
    human deception and staying vigilant against evolving threats, we can improve
    the reliability and security of these text models beyond mere accuracy on crafted
    testing data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，进行对抗性分析对于增强文本模型对欺骗技术和攻击的韧性至关重要。通过理解人类欺骗行为并保持警觉以应对不断变化的威胁，我们可以在超越单纯的在精心设计的测试数据上取得准确性的基础上，提升这些文本模型的可靠性和安全性。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, the concept of adversarial performance analysis for machine
    learning models was introduced. Adversarial attacks aim to deceive models by intentionally
    inputting misleading or carefully crafted data to cause incorrect predictions.
    This chapter highlighted the importance of analyzing adversarial performance to
    identify potential vulnerabilities and weaknesses in machine learning models and
    to develop targeted mitigation methods. Adversarial attacks can target various
    aspects of machine learning models, which include their bias and fairness behavior,
    and their accuracy-based performance. For instance, facial recognition systems
    may be targeted by adversaries who exploit biases or discrimination present in
    the training data or model design.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习模型的对抗性性能分析的概念。对抗性攻击通过故意输入误导性或精心设计的数据来欺骗模型，导致错误的预测。本章强调了分析对抗性性能的重要性，以识别机器学习模型中的潜在脆弱性和弱点，并开发针对性的缓解方法。对抗性攻击可以针对机器学习模型的各个方面，包括它们的偏见和公平性行为，以及基于准确度的性能。例如，面部识别系统可能会成为对手的攻击目标，攻击者利用训练数据或模型设计中的偏见或歧视。
- en: We also explored practical examples and techniques for analyzing adversarial
    performance in image, text, and audio data-based models. For image-based models,
    various approaches such as object size, orientation, blurriness, and environmental
    conditions were discussed. We also practically explored an algorithmic approach
    that’s used to generate a noise matrix so that it can mix and perturb the original
    image and generate an adversarial image capable of fooling a trained face classifier
    model; this was taken from [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring
    Bias and Fairness*. For audio-based models, augmentations such as pronunciation
    speed, speech pitch, background noise, and speech loudness were analyzed, while
    for text-based models, word variation-based attacks, natural language deception,
    and jailbreak attacks were explored.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了分析图像、文本和音频数据模型对抗性性能的实际例子和技术。对于基于图像的模型，讨论了对象大小、方向、模糊度和环境条件等各种方法。我们还实际探索了一种算法方法，该方法用于生成噪声矩阵，以便混合并扰乱原始图像，并生成能够欺骗训练好的面部分类器模型的对抗性图像；这一方法来自于[*第13章*](B18187_13.xhtml#_idTextAnchor196)，*探索偏见与公平性*。对于基于音频的模型，分析了如发音速度、语音音调、背景噪音和语音响度等增强方法，而对于基于文本的模型，探索了基于单词变化的攻击、自然语言欺骗和越狱攻击。
- en: In conclusion, adversarial analysis is essential for enhancing the resilience
    of machine learning models against deceptive techniques and attacks. By understanding
    human deception and staying vigilant against evolving adversarial threats, we
    can improve the reliability and security of our neural network models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，对抗性分析对于增强机器学习模型抵御欺骗技术和攻击的韧性至关重要。通过理解人类欺骗行为，并保持对不断发展的对抗性威胁的警觉，我们可以提高神经网络模型的可靠性和安全性。
- en: In the next chapter, we will be moving on to the next stage in the deep learning
    life cycle and explore the world of deep learning models in production.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将进入深度学习生命周期的下一阶段，探索深度学习模型在生产中的应用世界。
- en: Part 3 – DLOps
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分 – DLOps
- en: In this part of the book, you will dive into the exciting realm of deploying,
    monitoring, and governing deep learning models in production, drawing parallels
    with MLOps and DevOps. This part will provide you with a comprehensive understanding
    of the essential components required to ensure the success and impact of your
    deep learning models in production with real-world utilization.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，你将深入了解在生产环境中部署、监控和治理深度学习模型的激动人心领域，并与MLOps和DevOps进行类比。本部分将为你提供关于确保深度学习模型在生产中成功应用和实际影响所需的核心组件的全面理解。
- en: Throughout the chapters in this part, we’ll explore the various aspects of deploying
    deep learning models in production, touching upon important considerations such
    as hardware infrastructure, model packaging, and user interfaces. We’ll also delve
    into the three fundamental pillars of model governance, which are model utilization,
    model monitoring, and model maintenance. You’ll learn about the concept of drift
    and its impact on the performance of deployed deep learning models over time,
    as well as strategies to handle drift effectively. We’ll also discuss the benefits
    of AI platforms such as DataRobot, which streamline the complex stages of the
    machine learning life cycle and accelerate the creation, training, deployment,
    and governance of intricate deep learning models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分的各章中，我们将探讨将深度学习模型部署到生产环境中的各个方面，涵盖诸如硬件基础设施、模型打包和用户界面等重要考虑因素。我们还将深入讨论模型治理的三大支柱：模型利用、模型监控和模型维护。你将了解漂移的概念以及它对已部署深度学习模型性能的长期影响，并学习如何有效应对漂移。我们还将讨论像DataRobot这样的AI平台的好处，它们简化了机器学习生命周期中的复杂阶段，加速了复杂深度学习模型的创建、训练、部署和治理。
- en: As a bonus, building upon the foundational transformer method from [*Chapter
    6*](B18187_06.xhtml#_idTextAnchor092)*, Understanding Neural Network Transformers*,
    we’ll delve into **Large Language Model (LLM**) solutions, which are revolutionizing
    various domains. You’ll learn about architecting LLM solutions and building autonomous
    agents, equipping you with the knowledge to harness their potential.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外内容，基于[*第6章*](B18187_06.xhtml#_idTextAnchor092)《理解神经网络变换器》中的基础变换器方法，我们将深入探讨**大型语言模型（LLM）**解决方案，这些解决方案正在革新各个领域。你将学习如何架构LLM解决方案并构建自主智能体，掌握利用其潜力所需的知识。
- en: 'This part contains the following chapters:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 15*](B18187_15.xhtml#_idTextAnchor217)*, Deploying Deep Learning
    Models in Production*'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第15章*](B18187_15.xhtml#_idTextAnchor217)《在生产环境中部署深度学习模型》'
- en: '[*Chapter 16*](B18187_16.xhtml#_idTextAnchor238)*, Governing Deep Learning
    Models*'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第16章*](B18187_16.xhtml#_idTextAnchor238)《深度学习模型治理》'
- en: '[*Chapter 17*](B18187_17.xhtml#_idTextAnchor247)*, Managing Drift Effectively
    in a Dynamic Environment*'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第17章*](B18187_17.xhtml#_idTextAnchor247)《在动态环境中有效管理漂移效应》'
- en: '[*Chapter 18*](B18187_18.xhtml#_idTextAnchor265)*, Exploring the DataRobot
    AI Platform*'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第18章*](B18187_18.xhtml#_idTextAnchor265)《探索DataRobot AI平台》'
- en: '[*Chapter 19*](B18187_19.xhtml#_idTextAnchor286)*, Architecting LLM Solutions*'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第19章*](B18187_19.xhtml#_idTextAnchor286)《架构LLM解决方案》'
