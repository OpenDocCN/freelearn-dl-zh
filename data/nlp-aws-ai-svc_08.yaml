- en: '*Chapter 6*: Using NLP to Improve Customer Service Efficiency'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：利用NLP提升客户服务效率'
- en: So far, we have seen a couple of interesting real-world NLP use cases with intelligent
    document processing solutions for loan applications in [*Chapter 4*](B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063)*,*
    *Automating Document Processing Workflows*, and built smart search indexes in
    [*Chapter 5*](B17528_05_Final_SB_ePub.xhtml#_idTextAnchor074)*, Creating NLP Search*.
    NLP-based indexing for content search is becoming very popular because it bridges
    the gap between traditional keyword-based searches, which can be frustrating unless
    you know exactly what keyword to use, and natural language, to quickly search
    for what you are interested in. We also saw how we can use Amazon Textract and
    Amazon Comprehend with services such as Amazon Elasticsearch ([https://aws.amazon.com/elasticsearch-service/](https://aws.amazon.com/elasticsearch-service/)),
    a service that's fully managed by AWS and provides search and analytics capabilities
    offered by the open source Elasticsearch, but without the need for infrastructure
    heavy lifting, installation, or maintenance associated with setting up an Elasticsearch
    cluster, and Amazon Kendra ([https://aws.amazon.com/kendra/](https://aws.amazon.com/kendra/)),
    a fully managed enterprise search engine powered by ML that provides NLP-based
    search capabilities, to create an end-to-end smart search solution. In this chapter,
    we will address a ubiquitous use case that has been around for decades, if not
    centuries, and yet remains highly important for any business; that is, customer
    service improvement.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一些有趣的现实世界NLP应用案例，比如在[*第4章*](B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063)中用于贷款申请的智能文档处理解决方案，*自动化文档处理工作流*，以及在[*第5章*](B17528_05_Final_SB_ePub.xhtml#_idTextAnchor074)中构建的智能搜索索引，*创建NLP搜索*。基于NLP的内容搜索索引正变得越来越流行，因为它弥补了传统基于关键词的搜索与自然语言搜索之间的差距。传统的关键词搜索可能让人感到沮丧，除非你知道要使用的确切关键词，而自然语言搜索则可以帮助快速找到你感兴趣的内容。我们还看到，如何利用Amazon
    Textract和Amazon Comprehend与如Amazon Elasticsearch([https://aws.amazon.com/elasticsearch-service/](https://aws.amazon.com/elasticsearch-service/))这样的服务进行集成，Amazon
    Elasticsearch是由AWS完全管理的服务，提供开源Elasticsearch提供的搜索和分析功能，但无需处理设置Elasticsearch集群时的基础设施负担、安装或维护问题；以及Amazon
    Kendra([https://aws.amazon.com/kendra/](https://aws.amazon.com/kendra/))，这是一个由机器学习驱动的完全托管企业搜索引擎，提供基于NLP的搜索功能，可以用来创建端到端的智能搜索解决方案。在本章中，我们将讨论一个普遍存在的用例，它已经存在了几十年，甚至几个世纪，但对于任何企业来说依然非常重要——那就是客户服务的改进。
- en: Businesses cannot thrive without customers, and customer satisfaction is a key
    metric that has a direct correlation to the profitability of an organization.
    While the touchpoints that organizations have with customers during the sales
    cycle are important, what is even more important is the effectiveness of their
    customer service process. Organizations need to respond quickly to customer feedback,
    understand the emotional undercurrent of a customer conversation, and resolve
    their issues in the shortest possible time. Happy customers are loyal customers
    and, of course, this means that the customer churn will be low, which will help
    keep costs low and improve profitability.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 企业如果没有客户，就无法发展，而客户满意度是一个关键指标，直接与组织的盈利能力相关。虽然组织在销售周期中与客户接触的各个环节非常重要，但更为关键的是其客户服务流程的有效性。组织需要迅速响应客户反馈，理解客户对话中的情感潜流，并在最短时间内解决问题。满意的客户是忠诚的客户，当然，这意味着客户流失率低，从而有助于降低成本并提高盈利能力。
- en: To see improving customer service in action, we will build an AI solution that
    uses the AWS NLP service known as Amazon Comprehend to analyze historical customer
    service records to derive key topics using Amazon Comprehend Topic Modeling, train
    a custom classification model that will predict routing topics for call routing
    using Amazon Comprehend Custom Classification, and use Amazon Comprehend Detect
    Sentiments to understand the emotional sentiment of the customer feedback. We
    will be hands-on throughout this chapter, but we have all the code samples we
    need to get going.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示客户服务改进的实际应用，我们将构建一个使用AWS NLP服务——Amazon Comprehend的AI解决方案，分析历史客户服务记录，通过Amazon
    Comprehend主题建模提取关键主题，训练一个自定义分类模型，使用Amazon Comprehend自定义分类来预测呼叫路由的主题，并使用Amazon
    Comprehend情感检测来理解客户反馈的情感。我们将在本章中进行动手实践，但我们已经准备好所有需要的代码样本，以便开始。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the customer service use case
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入客户服务用例
- en: Building an NLP solution to improve customer service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自然语言处理解决方案以改进客户服务
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you will need access to an AWS account. Please make sure that
    you follow the instructions specified in the *Technical requirements* section
    of [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing
    Amazon Textract*, to create your AWS account. You will also need to log into the
    AWS Management Console before trying the steps in the *Building an NLP solution
    to improve customer service* section.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您将需要访问AWS账户。请确保您按照[*第2章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)中*技术要求*部分的说明来创建您的AWS账户。在尝试*构建自然语言处理解决方案以改进客户服务*部分的步骤之前，您还需要登录AWS管理控制台。
- en: The Python code and sample datasets for our solution can be found at [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2006](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2006).
    Please use the instructions in the following sections, along with the code in
    this repository, to build the solution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决方案的Python代码和示例数据集可以在[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2006](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2006)找到。请按照以下部分的说明和本存储库中的代码构建解决方案。
- en: Check out the following video to see the Code in Action at [https://bit.ly/2ZpWveN](https://bit.ly/2ZpWveN).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码在[https://bit.ly/2ZpWveN](https://bit.ly/2ZpWveN)的实际应用。
- en: Introducing the customer service use case
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍客户服务使用案例
- en: So, how can NLP help us improve customer service? To illustrate our example,
    let's go back to our fictitious banking corporation, **LiveRight Holdings private
    limited**. **LiveRight** has contact centers in many states of the US, and they
    receive more than 100,000 calls every day from customers with queries and issues
    on various topics, such as credit, accounts, debt, and more. While they have a
    competent team of agents who are highly experienced in handling customer requests,
    their first-tier triage teams often struggle with interpreting the nature of the
    customer's request within the first minute of conversation, which is an important
    SLA for them. This is required to determine which agents to route the request
    to. They have a team of specialized agents based on product type and experience
    levels. Junior agents handle customers who are happy with the products, while
    the challenge of dealing with irate customers is often the task of more experienced
    agents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，自然语言处理如何帮助我们改进客户服务呢？为了说明我们的例子，让我们回到我们的虚构银行公司**LiveRight Holdings private
    limited**。**LiveRight**在美国的许多州设有联系中心，他们每天收到超过100,000通来自客户的查询和问题，涉及信用、账户、债务等各种主题。虽然他们有一支经验丰富的团队处理客户请求，但他们的一线分流团队经常在第一分钟内解读客户请求的性质时遇到困难，这对他们来说是重要的服务水平协议。这是为了确定将请求路由给哪些代理人。他们有一支根据产品类型和经验水平专门的代理团队。初级代理处理对产品满意的客户，而处理恼怒客户的挑战通常是更有经验的代理人的任务。
- en: '**LiveRight''s** senior management is unhappy with the first-tier team''s performance
    as they are constantly failing to meet the 1-minute SLA. This is further exacerbated
    by the fact that in the last 3 months, the first-tier team has been incorrectly
    routing unhappy customers to junior agents, resulting in an increased customer
    churn. Therefore, senior management wants to automate the first-tier triage process,
    which will enable their teams to address these issues. **LiveRight** has hired
    you to design a solution architecture that can automatically determine the routing
    option and the sentiment of the customer conversation. As the enterprise architect
    for the project, you have decided to use Amazon Comprehend to leverage its pre-trained
    ML model for sentiment detection, Comprehend''s built-in Topic Modeling feature
    to determine common themes in a training dataset to determine routing option labels,
    and the Custom Classifier feature of Amazon Comprehend to incrementally create
    your own classifier for customer request routing, without the need to build complex
    NLP algorithms. The components of the solution we will build are shown in the
    following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**LiveRight**的高级管理层对一线团队的表现不满，因为他们总是未能满足1分钟SLA的要求。更糟糕的是，在过去的3个月里，一线团队错误地将不满的客户转接给了初级代理，导致了客户流失的增加。因此，高级管理层希望自动化一线筛选流程，以便他们的团队能够解决这些问题。**LiveRight**聘请你设计一个解决方案架构，能够自动确定路由选项和客户对话的情绪。作为该项目的企业架构师，你决定使用Amazon
    Comprehend，利用其预训练的机器学习模型进行情感分析，使用Comprehend内置的主题建模功能来确定训练数据集中常见的主题，从而确定路由选项标签，并利用Amazon
    Comprehend的自定义分类器功能逐步创建属于自己的客户请求路由分类器，而无需构建复杂的自然语言处理算法。我们将构建的解决方案组件如下图所示：'
- en: '![Figure 6.1 – NLP solution build for customer service](img/B17528_06_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 客户服务的自然语言处理解决方案构建](img/B17528_06_01.jpg)'
- en: Figure 6.1 – NLP solution build for customer service
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 客户服务的自然语言处理解决方案构建
- en: 'We will be walking through this solution using an Amazon SageMaker Jupyter
    notebook, which will allow us to review the code and results as we execute it
    step by step. For code samples on how to build this solution as a real-time workflow
    using AWS Lambda (a serverless, event-driven compute service for running code),
    please refer to the *Further reading* section:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Amazon SageMaker Jupyter笔记本演示这个解决方案，这将使我们能够逐步查看代码和结果。有关如何使用AWS Lambda（一个无服务器、事件驱动的计算服务来运行代码）将此解决方案构建为实时工作流的代码示例，请参考*进一步阅读*部分：
- en: As a first step, we will preprocess our input dataset, which contains consumer
    complaints available in this book's GitHub repository, load this into an S3 bucket,
    and run an Amazon Comprehend Topic Modeling job to determine routing option labels.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一阶段，我们将对输入数据集进行预处理，数据集中包含本书GitHub仓库中的消费者投诉内容，将其加载到S3桶中，并运行Amazon Comprehend的主题建模作业，以确定路由选项标签。
- en: We will then create the training dataset with the routing option labels that
    have been assigned to the consumer complaints from our input dataset, and then
    upload this into an S3 bucket.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建包含已分配给消费者投诉的路由选项标签的训练数据集，并将其上传到S3桶中。
- en: We will use Amazon Comprehend Custom Classification to train a classifier model
    using the training dataset we created previously.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Amazon Comprehend自定义分类功能，利用我们先前创建的训练数据集训练分类器模型。
- en: Finally, we will create an Amazon Comprehend real-time endpoint to deploy the
    trained model and show you how to predict the routing option. We will then show
    you how to use the Amazon Comprehend Detect Sentiment API to determine the sentiment
    of the customer conversation in real time.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个Amazon Comprehend实时端点，部署训练好的模型，并演示如何预测路由选项。然后，我们将演示如何使用Amazon Comprehend的情感分析API实时确定客户对话的情绪。
- en: In this section, we introduced the customer service problem we are trying to
    solve with our NLP solution, reviewed the challenges faced by **LiveRight**, and
    looked at an overview of the solution we will build. In the next section, we will
    walk through the build of the solution step by step.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了通过我们的自然语言处理解决方案试图解决的客户服务问题，回顾了**LiveRight**面临的挑战，并概述了我们将要构建的解决方案。在下一节中，我们将逐步展示解决方案的构建过程。
- en: Building an NLP solution to improve customer service
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自然语言处理解决方案以改善客户服务
- en: In the previous section, we introduced the contact center use case for customer
    service, covered the architecture of the solution we will be building, and briefly
    walked through the solution components and workflow steps. In this section, we
    will start executing the tasks to build our solution. But first, there are some
    prerequisites that we must take care of.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们介绍了客户服务的联络中心用例，概述了我们将要构建的解决方案架构，并简要回顾了解决方案组件和工作流程步骤。在本节中，我们将开始执行构建解决方案的任务。但首先，我们必须处理一些前提条件。
- en: Setting up to solve the use case
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置以解决用例
- en: If you have not done so already in the previous chapters, you will have to create
    an Amazon SageMaker Jupyter notebook, and then set up `Chapter 06` folder, open
    the `chapter6-nlp-in-customer-service-github.ipynb` notebook, and provide the
    bucket name in the notebook to start execution.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在之前的章节中还未完成此操作，您需要创建一个 Amazon SageMaker Jupyter notebook，然后设置 `Chapter 06`
    文件夹，打开 `chapter6-nlp-in-customer-service-github.ipynb` notebook，并在 notebook 中提供存储桶名称以开始执行。
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please ensure you have completed the tasks mentioned in the *Technical requirements*
    section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您已完成*技术要求*部分中提到的任务。
- en: 'If you have already completed the following steps in one of the previous chapters,
    please go to the *Preprocessing the customer service history data* section:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经在之前的章节中完成了以下步骤，请直接跳到*预处理客户服务历史数据*部分：
- en: 'Please refer to the Amazon SageMaker documentation to create a notebook instance:
    [https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html).
    To follow these steps, please sign into **AWS Management Console** and type in
    and select **Amazon SageMaker** from the search window. Then, navigate to the
    **Amazon SageMaker** console.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请参考 Amazon SageMaker 文档创建一个 notebook 实例：[https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)。要按照这些步骤操作，请登录**AWS
    管理控制台**，然后在搜索框中输入并选择**Amazon SageMaker**。接着，导航到**Amazon SageMaker**控制台。
- en: Select **Notebook instances** and create a Notebook instance by specifying an
    instance type, storage, and an IAM role.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**Notebook 实例**，通过指定实例类型、存储和 IAM 角色来创建一个 Notebook 实例。
- en: IAM role permissions while creating Amazon SageMaker Jupyter notebooks
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建 Amazon SageMaker Jupyter notebooks 时的 IAM 角色权限
- en: Accept the default for the IAM role at notebook creation time to allow access
    to any S3 bucket. Select **ComprehendFullAccess** as a permission policy by clicking
    on the IAM role and navigating to the Identity and Access Management console for
    the role being created. You can always go back to the IAM role for your notebook
    instances and attach other permissions policies as required.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 notebook 创建时接受 IAM 角色的默认设置，以允许访问任何 S3 存储桶。通过点击 IAM 角色并导航到身份和访问管理控制台，为正在创建的角色选择**ComprehendFullAccess**作为权限策略。您随时可以返回到您的
    notebook 实例的 IAM 角色，并根据需要附加其他权限策略。
- en: Once you've created the notebook instance and its status is **InService**, click
    on **Open Jupyter** in the **Actions** menu heading for the notebook instance:![Figure
    6.2 – Opening the Jupyter notebook
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了 notebook 实例并且其状态为**InService**后，点击 notebook 实例的**操作**菜单中的**打开 Jupyter**：![图
    6.2 – 打开 Jupyter notebook
- en: '](img/B17528_06_02.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_02.jpg)'
- en: Figure 6.2 – Opening the Jupyter notebook
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.2 – 打开 Jupyter notebook
- en: This will take you to the home folder of your notebook instance.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将把您带到 notebook 实例的主文件夹。
- en: Click on **New** and select **Terminal**, as shown in the following screenshot:![Figure
    6.3 – Opening a Terminal in a Jupyter notebook
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**新建**，然后选择**终端**，如下面的截图所示：![图 6.3 – 在 Jupyter notebook 中打开终端
- en: '](img/B17528_06_03.jpg)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_03.jpg)'
- en: Figure 6.3 – Opening a Terminal in a Jupyter notebook
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.3 – 在 Jupyter notebook 中打开终端
- en: In the Terminal window, type `cd SageMaker` and then `git clone` [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services),
    as shown in the following screenshot:![Figure 6.4 – git clone command
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端窗口中，输入 `cd SageMaker`，然后 `git clone` [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services)，如下面的截图所示：![图
    6.4 – git clone 命令
- en: '](img/B17528_06_04.jpg)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_04.jpg)'
- en: Figure 6.4 – git clone command
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.4 – git clone 命令
- en: Now, exit the Terminal window and go back to the home folder. You will see a
    folder called `Natural-Language-Processing-with-AWS-AI-Services`. Click it; you
    will see a folder called `Chapter 06`. Click this folder; you should see a notebook
    called `chapter6-nlp-in-customer-service-github`.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，退出终端窗口并返回主文件夹。您将看到一个名为`Natural-Language-Processing-with-AWS-AI-Services`的文件夹。点击它；您将看到一个名为`Chapter
    06`的文件夹。点击该文件夹；您应该能看到一个名为`chapter6-nlp-in-customer-service-github`的Notebook。
- en: Open this notebook by clicking it.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击此Notebook以打开它。
- en: Follow the steps in this notebook that correspond to the next few subheadings
    in this section by executing one cell at a time. Please read the descriptions
    provided above each notebook cell.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照本Notebook中与本节接下来几个小标题对应的步骤操作，一次执行一个单元格。请阅读每个Notebook单元格上方提供的描述。
- en: Now that we have set up our notebook and cloned the repository, let's add the
    permissions policies we need to successfully run our code sample.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好Notebook并克隆了代码库，让我们添加所需的权限策略，以便成功运行我们的代码示例。
- en: Additional IAM prerequisites
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他IAM先决条件
- en: 'To train the Comprehend custom entity recognizer and set up real-time endpoints,
    we have to enable additional policies and also update the Trust Relationships
    for our SageMaker notebook role. Please complete the following steps to do this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练Comprehend自定义实体识别器并设置实时端点，我们需要启用额外的策略，并更新SageMaker notebook角色的信任关系。请完成以下步骤：
- en: Please attach the `ComprehendFullAccess` policies to your Amazon SageMaker Notebook
    IAM role. To execute this step, please refer to the *Changing IAM permissions
    and Trust relationships for the Amazon SageMaker notebook execution role* subsection
    in the *Setting up your AWS environment* section of [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)*,*
    *Introducing Amazon Textract*.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请将`ComprehendFullAccess`策略附加到您的Amazon SageMaker Notebook IAM角色上。要执行此步骤，请参阅[*设置您的AWS环境*]章节中*更改IAM权限和信任关系以执行Amazon
    SageMaker notebook角色*小节，[*第2章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)*，*
    *介绍Amazon Textract*。
- en: 'Your SageMaker Execution Role should have access to S3 already. If not, add
    the following JSON statement as an inline policy. For instructions, please refer
    to the *Changing IAM permissions and Trust relationships for the Amazon SageMaker
    notebook execution role* subsection in the *Setting up your AWS environment* section
    of [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)*,* *Introducing
    Amazon Textract*:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的SageMaker执行角色应已具有访问S3的权限。如果没有，请将以下JSON语句作为内联策略添加。有关说明，请参阅[*设置您的AWS环境*]章节中*更改IAM权限和信任关系以执行Amazon
    SageMaker notebook角色*小节，[*第2章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)*，*
    *介绍Amazon Textract*：
- en: '[PRE0]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Finally, update the Trust relationships for your SageMaker Notebook execution
    role. For instructions, please refer to the *Changing IAM permissions and Trust
    relationships for the Amazon SageMaker notebook execution role* subsection in
    the *Setting up your AWS environment* section of [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)*,
    Introducing Amazon Textract*:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，更新您的SageMaker Notebook执行角色的信任关系。有关说明，请参阅[*设置您的AWS环境*]章节中*更改IAM权限和信任关系以执行Amazon
    SageMaker notebook角色*小节，[*第2章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)*，*
    介绍Amazon Textract*：
- en: '[PRE1]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have set up our Notebook and set up an IAM role to run the walkthrough
    notebook, in the next section, we will start processing the data for topic modeling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了Notebook并创建了IAM角色来运行演示Notebook，在下一节中，我们将开始处理主题建模的数据。
- en: Preprocessing the customer service history data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理客户服务历史数据
- en: 'Let''s begin by downloading and reviewing the customer service records we will
    use for this chapter. We will use the Consumer Complaints data for the State of
    Ohio from the Consumer Financial Protection Bureau for our solution: [https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&dateRange=1y&date_received_max=2021-05-17&date_received_min=2020-05-17&searchField=all&state=OH&tab=Map](https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&dateRange=1y&date_received_max=2021-05-17&date_received_min=2020-05-17&searchField=all&state=OH&tab=Map).
    You can try other datasets from this site, or your own unique customer service
    data. For your convenience, the complaints data is included as a CSV file in the
    GitHub repository: [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv).
    This should be available to you when you clone the repository. You can click on
    the CSV file by going to the folder it is present in inside the notebook to review
    its contents. Alternatively, you can view it using the code provided in the `chapter6-nlp-in-customer-service-github.ipynb`
    notebook.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载并查看我们将用于本章的客户服务记录。我们将使用来自消费者金融保护局（Consumer Financial Protection Bureau）的俄亥俄州消费者投诉数据作为我们的解决方案：[https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&dateRange=1y&date_received_max=2021-05-17&date_received_min=2020-05-17&searchField=all&state=OH&tab=Map](https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&dateRange=1y&date_received_max=2021-05-17&date_received_min=2020-05-17&searchField=all&state=OH&tab=Map)。你可以尝试该网站上的其他数据集，或者使用你自己的客户服务数据。为了方便起见，投诉数据已作为CSV文件包含在GitHub存储库中：[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv)。当你克隆该存储库时，这些数据应该是可以使用的。你可以通过点击存储在笔记本中的CSV文件来查看其内容，或者使用`chapter6-nlp-in-customer-service-github.ipynb`笔记本中的代码来查看它。
- en: 'Open the notebook and perform the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 打开笔记本并执行以下步骤：
- en: 'Execute the cells under **Prerequisites** to ensure we have the libraries we
    need for the notebook. Note that in this cell, you are getting the Amazon SageMaker
    Execution Role for the notebook. Please ensure that you create an Amazon S3 bucket
    ([https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html))
    and provide the bucket name in the line. Type in a prefix of your choice or accept
    what is already provided in the notebook:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行**先决条件**下的单元格，以确保我们有笔记本所需的库。请注意，在此单元格中，你将获取Amazon SageMaker执行角色。请确保你创建了一个Amazon
    S3桶（[https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html)），并在行中提供桶的名称。输入你选择的前缀，或者接受笔记本中已经提供的前缀：
- en: '[PRE2]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Execute the cells under **Preprocess the Text data**.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行**预处理文本数据**下的单元格。
- en: 'First, we will load the CSV file containing the consumer complaints data (this
    is already provided to you in this book''s GitHub repository at ([https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv))
    into a pandas DataFrame object for easy manipulation:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们将加载包含消费者投诉数据的CSV文件（该文件已在本书的GitHub存储库中提供，链接为：[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv)），并将其加载到pandas
    DataFrame对象中，便于操作：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When we execute the preceding cell, we will see that the notebook returns a
    shape of (11485, 18), which means there are 11,485 rows and 18 columns. We are
    only interested in the **Consumer complaint narrative** field, so we will drop
    the rest of the fields from the dataset. After we execute this cell, the shape
    should change to (5152, 1):'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们执行前面的单元格时，我们将看到笔记本返回一个形状为(11485, 18)的数据，这意味着有11,485行和18列。我们只关心**消费者投诉叙述**字段，因此我们将从数据集中删除其余字段。执行此单元格后，形状应更改为(5152,
    1)：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s convert this back into an updated CSV file:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们将其转换回更新后的CSV文件：
- en: '[PRE5]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Execute the cells in the notebook to clean up the textual content in our CSV
    file, including restructuring the text into individual sentences so that each
    consumer complaint is a separate line. For the source of this code block and a
    very good discussion on how to use the Python regex function with sentences, please
    refer to [https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences](https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences).
    Continue executing the cells to remove unnecessary spaces or punctuation, create
    a new CSV file with these changes, and upload it to an S3 bucket. We will also
    create a new pandas DataFrame object with the formatted content so that we can
    use it in the subsequent steps. Please execute all the remaining cells in the
    notebook from *Preprocess the Text data*:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行笔记本中的单元格，以清理CSV文件中的文本内容，包括将文本重构为单独的句子，使得每个消费者投诉都成为单独的一行。有关此代码块的来源以及如何在句子中使用Python正则表达式函数的详细讨论，请参见[https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences](https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences)。继续执行单元格，以删除不必要的空格或标点符号，使用这些更改创建一个新的CSV文件，并将其上传到S3存储桶。我们还将创建一个新的pandas
    DataFrame对象，使用格式化后的内容，以便在后续步骤中使用。请执行笔记本中从*预处理文本数据*开始的所有剩余单元格：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we will run an Amazon Comprehend Topic Modeling job on this formatted
    CSV file to extract a set of topics that can be applied to our list of consumer
    complaints. These topics represent and help us identify the subject area or the
    theme for the related text, as well as represent the common set of words with
    the same contextual reference throughout the document. For more details, please
    refer to Amazon Comprehend Topic Modeling at [https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html](https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在格式化后的CSV文件上运行Amazon Comprehend主题建模任务，以提取一组可以应用于消费者投诉列表的主题。这些主题代表并帮助我们识别相关文本的主题领域或主题，并且表示文档中具有相同上下文参考的常用词汇集。有关更多详细信息，请参见Amazon
    Comprehend主题建模，[https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html](https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html)。
- en: To get started, go to the AWS Console (please refer to the *Technical requirements*
    section if you don't have access to the AWS Console) and type Amazon Comprehend
    in the services search window at the top of the console. Then, navigate to the
    Amazon Comprehend Console.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要开始，请进入AWS控制台（如果您没有访问AWS控制台的权限，请参阅*技术要求*部分），并在控制台顶部的服务搜索窗口中输入Amazon Comprehend。然后，导航到Amazon
    Comprehend控制台。
- en: 'Click the `8`, as shown in the following screenshot:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击`8`，如以下截图所示：
- en: '![Figure 6.6 – Creating topic modeling job inputs – part1'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.6 – 创建主题建模任务输入 – 第1部分](img/B17528_06_06.jpg)'
- en: '](img/B17528_06_06.jpg)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_06.jpg)'
- en: Figure 6.6 – Creating topic modeling job inputs – part1
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.6 – 创建主题建模任务输入 – 第1部分
- en: 'Provide the details for the rest of the fields and click on **Create job**,
    as shown in the following screenshot:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供其余字段的详细信息并点击**创建任务**，如以下截图所示：
- en: '![Figure 6.7 – Creating topic modeling job inputs – part 2'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.7 – 创建主题建模任务输入 – 第2部分](img/B17528_06_07.jpg)'
- en: '](img/B17528_06_07.jpg)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_07.jpg)'
- en: Figure 6.7 – Creating topic modeling job inputs – part 2
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.7 – 创建主题建模任务输入 – 第2部分
- en: 'You should see a job submitted status after the IAM role propagation is completed,
    as shown in the following screenshot. The job should take about 30 minutes to
    complete, which gives you time to have a quick snack or a coffee/tea. Now, click
    on the job''s name, copy the S3 link provided in the **Output data location**
    field, and go back to your notebook. We will continue the steps in the notebook:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在IAM角色传播完成后，您应该看到任务提交状态，如以下截图所示。任务大约需要30分钟完成，这为您提供了快速吃点小吃或喝杯咖啡/茶的时间。现在，点击任务的名称，复制**输出数据位置**字段中提供的S3链接，然后返回到您的笔记本。我们将继续在笔记本中的步骤：
- en: '![Figure 6.8 – Topic modeling job submitted'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.8 – 提交主题建模任务](img/B17528_06_08.jpg)'
- en: '](img/B17528_06_08.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_08.jpg)'
- en: Figure 6.8 – Topic modeling job submitted
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.8 – 提交主题建模任务
- en: We will now execute the cells in the Process Topic Modeling Results section.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将执行“处理主题建模结果”部分中的单元格。
- en: 'To download the results of the Topic Modeling job, we need the `tpprefix` variable
    – specifically `output.tar.gz` file locally and extract it:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要下载主题建模任务的结果，我们需要`tpprefix`变量——特别是本地的`output.tar.gz`文件，并将其解压：
- en: '[PRE7]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: directory = "custom-classification"
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: directory = "custom-classification"
- en: parent_dir = os.getcwd()
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: parent_dir = os.getcwd()
- en: path = os.path.join(parent_dir, directory)
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: path = os.path.join(parent_dir, directory)
- en: os.makedirs(path, exist_ok = True)
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: os.makedirs(path, exist_ok = True)
- en: print("Directory '%s' created successfully" %directory)
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print("目录 '%s' 创建成功" %directory)
- en: directory = "train"
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: directory = "train"
- en: parent_dir = os.getcwd()+'/custom-classification'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: parent_dir = os.getcwd()+'/custom-classification'
- en: path = os.path.join(parent_dir, directory)
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: path = os.path.join(parent_dir, directory)
- en: os.makedirs(path, exist_ok = True)
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: os.makedirs(path, exist_ok = True)
- en: print("Directory '%s' created successfully" %directory)
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print("目录 '%s' 创建成功" %directory)
- en: '[PRE8]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: form_df = form_df[['Label','Text']]
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: form_df = form_df[['标签', '文本']]
- en: form_df.to_csv('custom-classification/train/train.csv', header=None, index=False)
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: form_df.to_csv('custom-classification/train/train.csv', header=None, index=False)
- en: s3.upload_file('custom-classification/train/train.csv', bucket, prefix+'/custom_classification/train/train.csv')
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: s3.upload_file('custom-classification/train/train.csv', bucket, prefix+'/custom_classification/train/train.csv')
- en: '[PRE9]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we will go back to the Amazon Comprehend AWS Console to train our Custom
    Classification model, which can predict a label for a given text. These labels
    are the topics we modeled in the previous section. With Amazon Comprehend Custom,
    you can train models that are unique to your business incrementally on top of
    the pre-trained, highly powerful Comprehend models. So, these custom models leverage
    what the default Comprehend model already knows, thereby training quickly, They
    are also more accurate than if you were to build a custom classification model
    from the ground up. You can run this training process without any ML skills with
    just a few clicks in the Amazon Comprehend console. For more details, please refer
    to [https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html).
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将返回到 Amazon Comprehend AWS 控制台，训练我们的自定义分类模型，它可以为给定的文本预测标签。这些标签是我们在上一节中建模的主题。通过
    Amazon Comprehend Custom，您可以在预训练的、功能强大的 Comprehend 模型基础上逐步训练出专属于您业务的模型。因此，这些自定义模型充分利用了默认
    Comprehend 模型的知识，从而实现快速训练。与从头开始构建自定义分类模型相比，它们的准确性更高。您可以在 Amazon Comprehend 控制台中仅需几次点击即可运行此训练过程，而无需任何机器学习技能。更多详细信息，请参考[https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html)。
- en: To get started, go to the AWS Console (please refer to the *Technical requirements*
    section at the beginning of this chapter if you don't have access to the AWS Console)
    and type Amazon Comprehend in the services search window at the top of the console.
    Then, navigate to the Amazon Comprehend Console.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要开始，请进入 AWS 控制台（如果您没有访问权限，请参考本章开头的*技术要求*部分），然后在控制台顶部的服务搜索窗口中输入 Amazon Comprehend。接着，导航到
    Amazon Comprehend 控制台。
- en: Click the **Launch Amazon Comprehend** button.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击**启动 Amazon Comprehend**按钮。
- en: Click on **Custom classification** under the **Customization** title in the
    left pane.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击左侧窗格中**自定义分类**下的**自定义**标题。
- en: 'Click on **Train classifier**, as shown in the following screenshot:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击**训练分类器**按钮，如下图所示：
- en: '![Figure 6.9 – Train classifier button'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.9 – 训练分类器按钮](img/B17528_06_09.jpg)'
- en: '](img/B17528_06_09.jpg)'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_09.jpg)'
- en: Figure 6.9 – Train classifier button
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.9 – 训练分类器按钮
- en: 'Enter a name for your classifier, leave the language as English, and set **Classifier
    mode** to **Multi-class**. (For our solution, we predict one label per document.
    If you need to predict multiple labels per document, you can use the **Multi-label**
    mode.) Select **CSV file** under **Training data format**, as shown in the following
    screenshot:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入分类器的名称，语言保持为英语，设置**分类器模式**为**多类别**。（对于我们的解决方案，我们为每个文档预测一个标签。如果您需要为每个文档预测多个标签，您可以使用**多标签**模式。）在**训练数据格式**下选择**CSV
    文件**，如下图所示：
- en: '![Figure 6.10 – Custom classifier inputs – part 1](img/B17528_06_10.jpg)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.10 – 自定义分类器输入 – 第 1 部分](img/B17528_06_10.jpg)'
- en: Figure 6.10 – Custom classifier inputs – part 1
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.10 – 自定义分类器输入 – 第 1 部分
- en: 'Provide our training dataset''s **S3 location**; that is, the one we created
    in the previous section. For **IAM role**, if you created an **AmazonComprehendServiceRole**
    in the previous chapters, use that, or select **Create an IAM role** and choose
    **Any S3 Bucket** from the list. Click the **Train classifier** button, as shown
    in the following screenshot:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供我们的训练数据集的**S3 位置**；即我们在前一节中创建的那个位置。对于**IAM 角色**，如果您在前几章中创建了**AmazonComprehendServiceRole**，请使用该角色，或者选择**创建一个
    IAM 角色**并从列表中选择**任何 S3 桶**。点击**训练分类器**按钮，如下图所示：
- en: '![Figure 6.11 – Custom classifier inputs – part 2](img/B17528_06_11.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.11 – 自定义分类器输入 – 第 2 部分](img/B17528_06_11.jpg)'
- en: Figure 6.11 – Custom classifier inputs – part 2
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.11 – 自定义分类器输入 – 第 2 部分
- en: 'The training job will be submitted. Shortly after, the training job''s status
    will change to **Training**, as shown in the following screenshot:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练作业将被提交。稍后，训练作业的状态将变为 **Training**，如下图所示：
- en: '![Figure 6.12 – Custom classifier training](img/B17528_06_12.jpg)'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.12 – 自定义分类器训练](img/B17528_06_12.jpg)'
- en: Figure 6.12 – Custom classifier training
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.12 – 自定义分类器训练
- en: 'Training will take approximately 1 hour to complete. The status will change
    to **Trained** when the job completes, as shown in the following screenshot:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练大约需要 1 小时才能完成。当作业完成时，状态将变为 **Trained**，如下图所示：
- en: '![Figure 6.13 – Custom classifier training complete](img/B17528_06_13.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.13 – 自定义分类器训练完成](img/B17528_06_13.jpg)'
- en: Figure 6.13 – Custom classifier training complete
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.13 – 自定义分类器训练完成
- en: Now that we have finished training our classifier, we will create a real-time
    endpoint to deploy the model. We will use this endpoint in our solution to run
    predictions for routing requests.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了分类器的训练，我们将创建一个实时端点来部署模型。我们将在解决方案中使用此端点来执行请求路由预测。
- en: 'Click on the name of your classifier in the Amazon Comprehend console. Then,
    scroll down to the **Endpoints** section and click **Create endpoint**, as shown
    in the following screenshot:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击你在 Amazon Comprehend 控制台中的分类器名称。然后，向下滚动至 **Endpoints** 部分并点击 **Create endpoint**，如下图所示：
- en: '![Figure 6.14 – Creating a Comprehend endpoint'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.14 – 创建 Comprehend 端点](img/B17528_06_14.jpg)'
- en: '](img/B17528_06_14.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_14.jpg)'
- en: Figure 6.14 – Creating a Comprehend endpoint
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.14 – 创建 Comprehend 端点
- en: 'Type in a name for your endpoint, provide an inference unit value of 1, and
    click on **Create endpoint**, as shown in the following screenshot. Inference
    units determine the price and capacity of the provisioned endpoint. An inference
    unit provides a prediction throughput of 100 characters every second. For more
    details, please refer to Amazon Comprehend''s pricing guide at [https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/):'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入端点名称，提供 1 的推理单元值，然后点击 **Create endpoint**，如下图所示。推理单元决定了端点的价格和容量。一个推理单元每秒提供
    100 个字符的预测吞吐量。有关更多详情，请参阅 Amazon Comprehend 的定价指南：[https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/)：
- en: '![Figure 6.15 – Creating Comprehend endpoint inputs'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.15 – 创建 Comprehend 端点输入](img/B17528_06_15.jpg)'
- en: '](img/B17528_06_15.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_06_15.jpg)'
- en: Figure 6.15 – Creating Comprehend endpoint inputs
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.15 – 创建 Comprehend 端点输入
- en: 'Once the endpoint has been created, please make a note of the endpoint''s ARN
    by clicking on the name of the endpoint, as shown in the following screenshot.
    This will be required for running inference in the notebook:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建完端点后，请通过点击端点名称记录端点的 ARN，如下图所示。这将在笔记本中进行推理时需要用到：
- en: '![Figure 6.16 – Comprehend endpoint ARN](img/B17528_06_16.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.16 – Comprehend 端点 ARN](img/B17528_06_16.jpg)'
- en: Figure 6.16 – Comprehend endpoint ARN
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.16 – Comprehend 端点 ARN
- en: As a next step, we will navigate back to our notebook and execute the steps
    in the *Automate Request Routing* section.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，我们将返回到笔记本并执行 *自动化请求路由* 部分中的步骤。
- en: 'Provide the endpoint ARN you took note of in the previous step in the notebook
    cell:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在笔记本单元格中提供你在前一步骤中记下的端点 ARN：
- en: '[PRE10]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s execute the next cell, which shows us how to run the real-time
    analysis with our endpoint. For input, we will use a sample text message that''s
    been assigned to the `test_text` variable, as shown in the following code:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们执行下一个单元，它展示了如何使用我们的端点进行实时分析。输入内容将使用一个已经分配给 `test_text` 变量的示例文本消息，如下方代码所示：
- en: '[PRE11]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our custom classifier returns a response, as shown in the following code block:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的自定义分类器返回一个响应，如下方代码块所示：
- en: '[PRE12]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the code given in the following code block to select the `Name` property
    with the highest confidence score from the response. This will be the department
    or the option that the customer request will be routed to in the contact center:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行以下代码块中的代码，从响应中选择具有最高置信度分数的 `Name` 属性。这将是客户请求在呼叫中心路由的部门或选项：
- en: '[PRE13]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code will return the following response:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码将返回以下响应：
- en: '[PRE14]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As a next step, we will execute the steps in the *Automate Feedback Analysis*
    section.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，我们将执行 *自动化反馈分析* 部分中的步骤。
- en: 'To analyze the sentiment of the customer conversation, we will use the **Amazon
    Comprehend Detect Sentiment API**. This is a built-in feature in Amazon Comprehend
    and does not require us to train any models. We can directly call the API with
    input. It will return the sentiment of the text, as follows:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了分析客户对话的情感，我们将使用**Amazon Comprehend Detect Sentiment API**。这是Amazon Comprehend中的内置功能，不需要我们训练任何模型。我们可以直接调用该API并提供输入，它将返回文本的情感，如下所示：
- en: '[PRE15]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'That concludes the solution build for this chapter. Please refer to the *Further
    reading* section for examples that are similar to this use case. In the case of
    **LiveRight**, you can integrate this build into the existing contact center workflow
    and scale the solution using **Amazon Transcribe**, **AWS StepFunctions**, and
    **AWS Lambda**. An example of how to do this is shown in the following diagram:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本章解决方案构建的总结。请参阅*进一步阅读*部分，了解与此用例类似的示例。在**LiveRight**的案例中，您可以将此构建集成到现有的联系中心工作流中，并使用**Amazon
    Transcribe**、**AWS Step Functions**和**AWS Lambda**扩展该解决方案。如何实现这一点的示例如下图所示：
- en: '![Figure 6.17 – NLP in customer service with a real-time transcription](img/B17528_06_17.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 客户服务中的NLP与实时转录](img/B17528_06_17.jpg)'
- en: Figure 6.17 – NLP in customer service with a real-time transcription
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 客户服务中的NLP与实时转录
- en: Amazon Transcribe provides real-time streaming transcription capabilities to
    convert a customer call from speech into text. Please refer to [https://aws.amazon.com/transcribe/](https://aws.amazon.com/transcribe/)
    for more details. An AWS Step Functions ([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/))
    workflow that enables orchestration of a complete process flow with AWS Lambda
    (a fully managed serverless compute service that can run code without the need
    to provision servers ([https://aws.amazon.com/lambda/](https://aws.amazon.com/lambda/)))
    and multiple AWS services can be set up to be triggered on receipt of a transcription
    of a specified length. The Step Functions workflow will call an AWS Lambda function
    to detect the routing option for the customer request, and the call can be automatically
    routed to that option, or/and the customer request/feedback sentiment can be analyzed
    by calling the Detect Sentiment API, as we saw in the *Automate feedback analysis*
    section. The outcome is that while the call is in progress, the contact center
    agent will have an automated response with a predicted routing option and sentiment,
    which, in turn, helps resolve the customer's request quickly and efficiently.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Transcribe提供实时流式转录功能，将客户电话的语音转为文本。详情请参考[https://aws.amazon.com/transcribe/](https://aws.amazon.com/transcribe/)。一个AWS
    Step Functions([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/))工作流可以实现对完整过程流的编排，结合AWS
    Lambda（一个完全托管的无服务器计算服务，可以在无需配置服务器的情况下运行代码，[https://aws.amazon.com/lambda/](https://aws.amazon.com/lambda/)）和多个AWS服务，设置为在接收到指定长度的转录时触发。Step
    Functions工作流将调用AWS Lambda函数，检测客户请求的路由选项，且电话可以自动被路由到该选项，或/和通过调用Detect Sentiment
    API分析客户请求/反馈的情感，正如我们在*自动化反馈分析*部分看到的那样。结果是在通话进行时，联系中心代理将获得一个自动响应，包含预测的路由选项和情感，从而帮助快速且高效地解决客户的请求。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned how to build an NLP solution to accelerate customer
    service efficiencies using Amazon Comprehend's Topic Modeling feature, Detect
    Sentiment feature, and by training our own custom classifier to predict routing
    options using Comprehend Custom Classification before hosting it using Comprehend
    real-time endpoints. We also saw how we can leverage the flexibility of powerful
    and accurate NLP models without the need for any ML skills. For your enterprise
    needs, Amazon Comprehend scales seamlessly to process millions of documents, provides
    usage-based pricing, supports batch inference, and with autoscaling support for
    real-time endpoints, you can manage your inference request volumes and control
    your inference costs effectively.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何构建一个NLP解决方案，通过使用Amazon Comprehend的主题建模功能、情感检测功能，并通过训练我们自己的自定义分类器来预测路由选项，使用Comprehend自定义分类后，通过Comprehend实时端点托管该解决方案。我们还看到如何在无需任何机器学习技能的情况下，利用强大且准确的NLP模型的灵活性。对于企业需求，Amazon
    Comprehend能够无缝扩展以处理数百万份文档，提供基于使用量的定价，支持批量推断，并且通过实时端点的自动扩展支持，您可以有效地管理推断请求量并控制推断成本。
- en: For our solution, we started by introducing the customer service use case, the
    inherent challenges with the way things are set up currently, and the need to
    perform automated routing and sentiment detection to control the high customer
    churn caused by current inefficient processes. We then designed an architecture
    to use Amazon Comprehend to identify common themes or topics, create a training
    dataset, train a custom classifier to predict routing options, and to run sentiment
    analysis on the customer request. We assumed that you were the solution architect
    that had been assigned to this project, and we provided an overview of the solution
    components, along with a diagram of the architecture in *Figure 6.1*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的解决方案，我们首先介绍了客户服务的使用案例、目前设置方式所固有的挑战，以及需要执行自动化路由和情感分析来控制由于当前低效流程导致的高客户流失率。随后，我们设计了一个架构，利用
    Amazon Comprehend 来识别常见的主题或话题，创建训练数据集，训练一个自定义分类器来预测路由选项，并对客户请求进行情感分析。我们假设你是被分配到这个项目的解决方案架构师，并提供了解决方案组件的概述，以及架构图，在*图
    6.1*中展示。
- en: We then went through the prerequisites for the solution build, set up an Amazon
    SageMaker notebook instance, cloned our GitHub repository, and started executing
    the code in the notebook based on the instructions provided in this chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们介绍了解决方案构建的前提条件，设置了一个 Amazon SageMaker 笔记本实例，克隆了我们的 GitHub 仓库，并根据本章提供的说明开始在笔记本中执行代码。
- en: In the next chapter, we will look at a slightly related use case on using NLP
    to run the voice of the customer analytics process. We will introduce the use
    case, discuss how to design the architecture, establish the prerequisites, and
    walk through the various steps required to build the solution.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一个稍微相关的使用案例，使用 NLP 来执行客户声音分析过程。我们将介绍该使用案例，讨论如何设计架构，建立前提条件，并详细讲解构建解决方案所需的各个步骤。
- en: Further reading
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Announcing the launch of Amazon Comprehend custom entity recognition real-time
    endpoints*, by Mona Mona and Prem Ranga ([https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-custom-entity-recognition-real-time-endpoints/](https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-custom-entity-recognition-real-time-endpoints/)).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*宣布 Amazon Comprehend 自定义实体识别实时端点的发布*，作者：Mona Mona 和 Prem Ranga（[https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-custom-entity-recognition-real-time-endpoints/](https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-custom-entity-recognition-real-time-endpoints/)）。'
- en: '*Active learning workflow for Amazon Comprehend custom classification models
    –Part 2*, by Shanthan Kesharaju, Joyson Neville Lewis, and Mona Mona ([https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/](https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/)).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Amazon Comprehend 自定义分类模型的主动学习工作流 - 第二部分*，作者：Shanthan Kesharaju, Joyson Neville
    Lewis 和 Mona Mona（[https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/](https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/)）。'
