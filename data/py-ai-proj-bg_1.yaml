- en: Building Your Own Prediction Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您自己的预测模型
- en: Our society is more technologically advanced than ever. **Artificial Intelligence**
    (**AI**) technology is already spreading throughout the world, replicating humankind.
    The intention of creating machines that could emulate aspects of human intelligence
    such as reasoning, learning, and problem solving gave birth to the development
    of AI technology. AI truly rivals human nature. In other words, AI makes a machine
    think and behave like a human. An example that can best demonstrate the power
    of this technology would be the tag suggestions or face-recognition feature of
    Facebook. Looking at the tremendous impact of this technology on today's world,
    AI will definitely become one of the greatest technologies out there in the coming
    years.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的社会比以往任何时候都更具技术先进性。**人工智能**（**AI**）技术已经在全球范围内传播，正在复制人类。创造能够模拟人类智能各个方面（如推理、学习和解决问题）的机器的初衷，促成了AI技术的发展。AI真正与人类本性相抗衡。换句话说，AI使机器像人类一样思考和行动。最能展示这种技术威力的例子就是Facebook的标签建议或人脸识别功能。考虑到这种技术对当今世界的巨大影响，AI无疑将在未来几年成为最伟大的技术之一。
- en: We are going to be experimenting with a project based on AI technology, exploring
    classification using machine learning algorithms along with the Python programming
    language. We will also explore a few examples for a better understanding.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个基于AI技术的项目进行实验，探索使用机器学习算法进行分类，并结合Python编程语言。我们还将通过一些示例来帮助更好地理解。
- en: 'In this chapter, we are going to explore the following interesting topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下几个有趣的主题：
- en: An overview of the classification technique
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类技术概述
- en: The Python scikit library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python scikit库
- en: Classification overview and evaluation techniques
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类概述和评估技术
- en: 'AI provides us with various classification techniques, but machine learning
    classification would be the best to start with as it is the most common and easiest
    classification to understand for the beginner. In our daily life, our eyes captures
    millions of pictures: be they in a book, on a particular screen, or maybe something
    that you caught in your surroundings. These images captured by our eyes help us
    to recognize and classify objects. Our application is based on the same logic.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI为我们提供了各种分类技术，但机器学习分类是最适合入门的技术，因为它是最常见且最容易理解的分类方法。在我们的日常生活中，眼睛捕捉到成千上万的图像：无论是在书本中，还是在某个屏幕上，或者是你周围环境中看到的东西。这些被我们眼睛捕捉到的图像帮助我们识别和分类物体。我们的应用程序基于相同的逻辑。
- en: Here, we are creating an application that will identify images using machine
    learning algorithms. Imagine that we have images of both apples and oranges, looking
    at which our application would help identify whether the image is of an apple
    or an orange. This type of classification can be termed as **binary classification**,
    which means classifying the objects of a given set into two groups, but techniques
    do exist for multiclass classification as well. We would require a large number
    of images of apples and oranges, and a machine learning algorithm that would be
    set in such a way that the application would be able to classify both image types.
    In other words, we make these algorithms learn the difference between the two
    objects to help classify all the examples correctly. This is known as **supervised
    learning**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建一个应用程序，通过机器学习算法来识别图像。假设我们有苹果和橙子的图像，通过观察这些图像，我们的应用程序将帮助识别图像是苹果还是橙子。这种类型的分类可以称为**二元分类**，意味着将给定集合中的物体分类为两个组，但也存在多类别分类的技术。我们需要大量的苹果和橙子图像，并且需要设置一个机器学习算法，使得应用程序能够分类这两种图像。换句话说，我们让这些算法学习两者之间的区别，从而帮助正确分类所有示例。这被称为**监督学习**。
- en: 'Now let''s compare supervised learning with unsupervised learning. Let''s assume
    that we are not aware of the actual data labels (which means we do not know whether
    the images are examples of apples or oranges). In such cases, classification won''t
    be of much help. The **clustering** method can always ease such scenarios. The
    result would be a model that can be deployed in an application, and it would function
    as seen in the following diagram. The application would memorize facts about the
    distinction between apples and oranges and recognize actual images using a machine
    learning algorithm. If we took a new input, the model would tell us about its
    decision as to whether the input is an apple or orange. In this example, the application
    that we created is able to identify an image of an apple with a 75% degree of
    confidence:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较监督学习与无监督学习。假设我们不知道实际的数据标签（即我们不知道这些图片是苹果还是橙子的例子）。在这种情况下，分类方法可能帮助不大。**聚类**方法总能缓解这种情况。结果会是一个可以部署在应用中的模型，并且它会像下面的图所示那样工作。该应用会记住苹果和橙子之间的区别，并使用机器学习算法识别实际的图像。如果我们输入一个新的数据，模型会告诉我们它的决策，判断该输入是苹果还是橙子。在这个例子中，我们创建的应用能够以75%的置信度识别苹果图像：
- en: '![](img/00005.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.jpeg)'
- en: Sometimes, we want to know the level of confidence, and other times we just
    want the final answer, that is, the choice in which the model has the most confidence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们希望知道置信度的程度，其他时候我们只需要最终的答案，也就是模型最有信心的选择。
- en: Evaluation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'We can evaluate how well the model is working by measuring its accuracy. Accuracy
    would be defined as the percentage of cases that are classified correctly. We
    can analyze the mistakes made by the model, or its level of confusion, using a
    confusion matrix. The confusion matrix refers to the confusion in the model, but
    these confusion matrices can become a little difficult to understand when they
    become very large. Let''s take a look at the following binary classification example,
    which shows the number of times that the model has made the correct predictions
    of the object:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过衡量模型的准确性来评估其性能。准确性定义为分类正确的案例所占的百分比。我们可以使用混淆矩阵分析模型的错误，或者说分析它的混淆程度。混淆矩阵是指模型中的混淆情况，但当混淆矩阵变得非常大时，理解起来可能会有些困难。让我们来看一下下面的二分类例子，它展示了模型在正确预测对象时的次数：
- en: '![](img/00006.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: In the preceding table, the rows of True apple and True orange refers to cases
    where the object was actually an apple or actually an orange. The columns refer
    to the prediction made by the model. We see that in our example, there are 20
    apples that were predicted correctly, while there were 5 apples that were wrongly
    identified as oranges.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，真正的苹果（True apple）和真正的橙子（True orange）所在的行表示该对象实际上是苹果或橙子的情况。列则表示模型做出的预测。我们看到，在这个例子中，有20个苹果被正确预测，而有5个苹果被错误地识别为橙子。
- en: 'Ideally, a confusion matrix should have all zeros, except for the diagonal.
    Here we can calculate the accuracy by adding the figures diagonally, so that these
    are all the correctly classified examples, and dividing that sum by the sum of
    all the numbers in the matrix:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，混淆矩阵应该只有对角线上的数字非零。这里我们可以通过将对角线上的数字相加来计算准确率，这些数字代表所有正确分类的例子，然后将这个和除以矩阵中所有数字的总和：
- en: '![](img/00007.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00007.jpeg)'
- en: 'Here we got the accuracy as 84%. To know more about confusion matrices, let''s go
    through another example, which involves three classes, as seen in the following
    diagram:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们得到了84%的准确率。为了更好地了解混淆矩阵，让我们通过另一个例子来学习，这个例子涉及三个类别，如下图所示：
- en: '![](img/00008.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.jpeg)'
- en: 'Source: scikit-learn docs'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：scikit-learn文档
- en: 'There are three different species of iris flowers. The matrix gives raw accounts
    of correct and incorrect predictions. So, **setosa** was correctly predicted 13
    times out of all the examples of setosa images from the dataset. On the other
    hand, **versicolor** was predicted correctly on 10 occasions, and there were 6
    occasions where versicolor was predicted as **virginica.** Now let''s normalize
    our confusion matrix and show the percentage of the cases that predicted image
    corrected or incorrectly. In our example we saw that the setosa species was predicted
    correctly throughout:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同种类的鸢尾花。矩阵给出了正确和错误预测的原始记录。因此，**setosa**被正确预测了13次，所有setosa图像的样本中。另一方面，**versicolor**被正确预测了10次，并且有6次versicolor被预测为**virginica**。现在，让我们对混淆矩阵进行标准化，并显示预测正确或错误的图像所占的百分比。在我们的例子中，我们看到setosa种类的图像被始终正确预测：
- en: '![](img/00009.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00009.jpeg)'
- en: 'Source: scikit-learn docs'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：scikit-learn文档
- en: 'During evaluation of the confusion matrix, we also saw that the system got
    confused between two species: versicolor and virginica. This also gives us the
    conclusion that the system is not able to identify species of virginica all the
    time.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估混淆矩阵时，我们还注意到系统在两个物种之间产生了混淆：versicolor和virginica。这也让我们得出结论，系统并不总能识别virginica种类的物种。
- en: For further instances, we need to be more aware that we cannot have really high
    accuracy since the system will be trained and tested on the same data. This will
    lead to memorizing the training set and overfitting of the model. Therefore, we
    should try to split the data into training and testing sets, first in either 90/10%
    or 80/20%. Then we should use the training set for developing the model and the
    test set for performing and calculating the accuracy of the confusion matrix.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于进一步的实例，我们需要更注意，因为系统将在同一数据上进行训练和测试，所以不能期望非常高的准确性。这将导致记忆训练集并使模型过度拟合。因此，我们应该尝试将数据拆分为训练集和测试集，首先按照90/10%或80/20%的比例进行拆分。然后，我们使用训练集开发模型，使用测试集执行和计算混淆矩阵的准确性。
- en: 'We need to be careful not to choose a really good testing set or a really bad
    testing set to get the accuracy. Hence to be sure we use a validation known as
    **K-fold cross validation**. To understand it a bit better, imagine 5-fold cross
    validation, where we move the testing set by 20 since there are 5 rows. Then we
    move the remaining set with the dataset and find the average of all the folds:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要小心，不要选择一个非常好的测试集或非常差的测试集来获取准确性。因此，为了确保这一点，我们使用一种验证方法，称为**K折交叉验证**。为了更好地理解它，想象一下5折交叉验证，其中我们将测试集移动20个数据点，因为总共有5个数据集。然后，我们将剩余的集合作为数据集，计算所有折叠的平均值：
- en: '![](img/00010.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00010.jpeg)'
- en: Quite confusing, right? But scikit-learn has built-in support for cross validation.
    This feature will be a good way to make sure that we are not overfitting our model
    and we are not running our model on a bad testing set.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点让人困惑，对吧？但是scikit-learn内置了交叉验证的支持。这个功能是确保我们不会过度拟合模型，且不会在不合格的测试集上运行模型的好方法。
- en: Decision trees
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: In this section, we will be using decision trees and student performance data
    to predict whether a child will do well in school. We will use the previous techniques
    with some scikit-learn code. Before starting with the prediction, let's just learn
    a bit about what decision trees are.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用决策树和学生表现数据来预测一个孩子是否能够在学校表现良好。我们将结合之前的技术和一些scikit-learn代码来实现预测。在开始预测之前，我们先了解一下什么是决策树。
- en: Decision trees are one of the simplest techniques for classification. They can
    be compared with a game of **20 questions**, where each node in the tree is either
    a leaf node or a question node. Consider the case of Titanic survivability, which
    was built from a dataset that includes data on the survival outcome of each passenger
    of the Titanic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是最简单的分类技术之一。它们可以与**20个问题**的游戏相比较，其中树中的每个节点要么是叶节点，要么是问题节点。以泰坦尼克号生还率为例，这个数据集包含了泰坦尼克号每位乘客的生还结果。
- en: 'Consider our first node as a question: *Is the passenger a male?* If not, then
    the passenger most likely survived. Otherwise, we would have another question
    to ask about the male passengers: *Was the male over the age of 9.5?* (where 9.5
    was chosen by the decision tree learning procedure as an ideal split of the data).
    If the answer is **Yes**, then the passenger most likely did not survive. If the
    answer is **No**, then it will raise another question: *Is the passenger a sibling?* The
    following diagram will give you a brief explanation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的第一个节点视为一个问题：*乘客是男性吗？* 如果不是，那么乘客很可能存活。否则，我们将继续对男性乘客提出另一个问题：*男性是否超过9.5岁？*（其中9.5岁是决策树学习过程选择的理想分裂点）。如果答案是**是**，那么该乘客很可能没有存活。如果答案是**否**，则会提出另一个问题：*该乘客是否有兄弟姐妹？*
    下图将为您提供简要说明：
- en: '![](img/00011.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: Understanding the decision trees does not require you to be an expert in the
    decision tree learning process. As seen in the previous diagram, the process makes
    understanding data very simple. Not all machine learning models are as easy to
    understand as decision trees.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 理解决策树并不要求你成为决策树学习过程的专家。如前面的图示所见，这个过程使得理解数据变得非常简单。并不是所有机器学习模型像决策树一样容易理解。
- en: 'Let us now dive deep into decision tree by knowing more about decision tree
    learning process. Considering the same titanic dataset we used earlier, we will
    find the best attribute to split on according to information gain, which is also
    known as **entropy**:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更深入了解决策树，通过了解决策树的学习过程。考虑我们之前使用的泰坦尼克号数据集，我们将根据信息增益来找到最佳的分裂属性，信息增益也称为**熵**：
- en: '![](img/00012.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: Information gain is highest only when the outcome is more predictable after
    knowing the value in a certain column. In other words, if we know whether the
    passenger is **male** or **female**, we will know whether he or she survived,
    hence the information gain is highest for the sex column. We do not consider age
    column best for our first split since we do not know much about the passengers
    ages, and is not the best first split because we will know less about the outcome
    if all we know is a passenger's age.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在知道某一列的值后，结果更可预测时，信息增益才最高。换句话说，如果我们知道乘客是**男性**还是**女性**，我们就能知道他或她是否存活，因此性别这一列的信息增益最高。我们不会将年龄列作为第一次分裂的最佳属性，因为我们对乘客的年龄知之甚少，这也不是最佳的第一次分裂，因为如果我们只知道乘客的年龄，我们对结果的了解将会更少。
- en: 'After splitting on the **sex** column according to the information gain, what
    we have now is **female** and **male** subsets, as seen in the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据信息增益对**性别**这一列进行分裂后，我们现在得到了**女性**和**男性**两个子集，如下图所示：
- en: '![](img/00013.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: 'After the split, we have one internode and one question node, as seen in the
    previous screenshot, and two paths that can be taken depending on the answer to
    the question. Now we need to find the best attribute again in both of the subsets.
    The left subset, in which all passengers are female, does not have a good attribute
    to split on because many passengers survived. Hence, the left subset just turns
    into a leaf node that predicts survival. On the right-hand side, the `age` attribute
    is chosen as the best split, considering the value **9.5** years of age as the
    split. We gain two more subsets: age greater than **9.5** and age lower than **9.5**:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在分裂之后，我们有一个分支节点和一个问题节点，如前面的截图所示，且根据问题的答案，可以选择两条路径。现在，我们需要再次在这两个子集内找到最佳属性进行分裂。左边的子集，其中所有乘客都是女性，并没有一个很好的属性来进行分裂，因为很多乘客都存活了。因此，左边的子集最终会变成一个叶节点，预测存活情况。在右边，`age`属性被选为最佳分裂点，考虑到**9.5**岁的年龄作为分裂点。我们得到了两个新的子集：年龄大于**9.5**岁和年龄小于**9.5**岁：
- en: '![](img/00014.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)'
- en: Repeat the process of splitting the data into two new subsets until there are
    no good splits, or no remaining attributes, and leaf nodes are formed instead
    of question nodes. Before we start with our prediction model, let us know a little
    more about the scikit-learn package.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重复这个过程，不断将数据分裂为两个新子集，直到没有好的分裂点，或者没有剩余的属性，最终形成叶节点而不是问题节点。在我们开始建立预测模型之前，让我们先了解一下scikit-learn包。
- en: Common APIs for scikit-learn classifiers
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn分类器的常用API
- en: 'In this section, we will be learn how to create code using the scikit-learn
    package to build and test decision trees. Scikit-learn contains many simple sets
    of functions. In fact, except for the second line of code that you can see in
    the following screenshot, which is specifically about decision trees, we will
    use the same functions for other classifiers as well, such as random forests:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 scikit-learn 包创建代码，以构建和测试决策树。Scikit-learn 包含许多简单的函数集。实际上，除了下面截图中第二行代码，这一行代码特别与决策树有关外，我们还将使用相同的函数来处理其他分类器，例如随机森林：
- en: '![](img/00015.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.jpeg)'
- en: Before we jump further into technical part, let's try to understand what the
    lines of code mean. The first two lines of code are used to set a decision tree,
    but we can consider this as not yet built as we have not pointed the tree to any
    trained set. The third line builds the tree using the `fit` function. Next, we
    score a list of examples and obtain an accuracy number. These two lines of code
    will be used to build the decision tree. After which, we predict function with
    a single example, which means we will take a row of data to train the model and
    predict the output with the survived column. Finally, we runs cross-validation,
    splitting the data and building an entry for each training split and evaluating
    the tree for each testing split. On running these code the result we have are
    the scores and the we average the scores.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入技术部分之前，让我们先理解这些代码行的含义。前两行代码用于设置决策树，但我们可以认为树尚未构建，因为我们还没有指向任何已训练的数据集。第三行通过
    `fit` 函数构建树。接下来，我们对一系列示例进行评分并获得准确性数值。这两行代码将用于构建决策树。之后，我们用一个示例进行预测，这意味着我们将用一行数据来训练模型，并根据“生还”列预测输出。最后，我们执行交叉验证，将数据分割并为每个训练集构建一个条目，评估每个测试集的决策树。在运行这些代码时，得到的结果是评分，随后我们会对这些评分进行平均。
- en: Here you will have a question: *When should we use decision trees?* The answer
    to this can be quite simple as decision trees are simple and easy to interpret
    and require little data preparation, though you cannot consider them as the most
    accurate techniques. You can show the result of a decision tree to any subject
    matter expert, such as a Titanic historian (for our example). Even experts who
    know very little about machine learning would presumably be able to follow the
    tree's questions and gauge whether the tree is accurate.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会有一个问题：*我们什么时候应该使用决策树？* 这个问题的答案其实很简单，因为决策树简单且易于解释，并且需要较少的数据准备，尽管你不能把它们视为最准确的技术。你可以将决策树的结果展示给任何领域专家，比如一位泰坦尼克号历史学家（以我们的例子为例）。即使是对机器学习知之甚少的专家，通常也能够理解决策树的提问并判断树的准确性。
- en: Decision trees can perform better when the data has few attributes, but may
    perform poorly when the data has many attributes. This is because the tree may
    grow too large to be understandable and could easily overfit the training data
    by introducing branches that are too specific to the training data and don't really
    bear any relation to the test data created, this can reduce the chance of getting
    an accurate result. As, by now, you are aware of the basics of the decision tree,
    we are now ready to achieve our goal of creating a prediction model using student
    performance data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据特征较少时，决策树的表现可能更好，但当数据具有较多特征时，它们的表现可能较差。这是因为树可能会变得过大，难以理解，并且可能会通过引入过于特定于训练数据的分支来轻易地过拟合训练数据，而这些分支与创建的测试数据并无太大关系，从而降低了获得准确结果的机会。现在，既然你已经了解了决策树的基本原理，我们准备好实现使用学生表现数据创建预测模型的目标了。
- en: Prediction involving decision trees and student performance data
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 涉及决策树和学生表现数据的预测
- en: In this section, we're going to use decision trees to predict student performance
    using the students, past performance data. We'll use the student performance dataset,
    which is available on the UC Irvine machine learning repository at [https://archive.ics.uci.edu/ml/datasets/student+performance](https://archive.ics.uci.edu/ml/datasets/student+performance).
    Our final goal is to predict whether the student has passed or failed. The dataset
    contains the data of about 649 students, with and 30 attributes for each student.
    The attributes formed are mixed categorically – word and phrase, and numeric attributes.
    These mixed attributes cause a small problem that needs to be fixed. We will need
    to convert those word and phrase attributes into numbers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将使用决策树来预测学生表现，基于学生的历史表现数据。我们将使用UC Irvine机器学习库中的学生表现数据集，[https://archive.ics.uci.edu/ml/datasets/student+performance](https://archive.ics.uci.edu/ml/datasets/student+performance)。我们的最终目标是预测学生是否通过。数据集包含约649名学生的数据，每个学生有30个属性。这些属性是混合型的——包括词汇和短语类型的分类属性，以及数值型属性。这些混合属性造成了一个小问题，需要进行修正。我们将需要将这些词汇和短语类型的属性转换成数字。
- en: 'The following screenshot shows the  first half of the attributes from the data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了数据中的前半部分属性：
- en: '![](img/00016.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: 'You must have noticed how some of the attributes are categorical, such as the
    name of the school; **sex**; **Mjob**, which is the mother''s occupation; **Fjob**,
    which is the father''s occupation; reason; and guardian. Others, such as **age** and **traveltime**,
    are numeric. The following screenshot shows the second half of the attributes
    from the data:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定注意到，有些属性是分类的，比如学校名称；**性别**；**母亲职业**（**Mjob**）；**父亲职业**（**Fjob**）；原因；以及监护人。其他的，如**年龄**（**age**）和**通学时间**（**traveltime**），是数值型的。以下截图显示了数据中的后半部分属性：
- en: '![](img/00017.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpeg)'
- en: It is clear that some of the attributes are better predictors, such as absences
    and the number of past failures, while others attributes are probably less predictive,
    such as whether or not the student is in a romantic relationship or whether the
    student's guardian is the mother, father, or someone else. The decision tree will
    attempt to identify the most important or predictive attributes using this information
    gain provided. We'll be able to look at the resulting tree and identify the most
    predictive attributes because the most predictive attributes will be the earliest
    questions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，某些属性是更好的预测因子，比如缺勤和过去的失败次数，而其他属性可能预测性较差，比如学生是否有恋爱关系，或者学生的监护人是母亲、父亲还是其他人。决策树将尝试通过提供的信息增益来识别最重要或最具预测性的属性。我们可以通过查看生成的树来识别最具预测性的属性，因为最具预测性的属性会出现在最前面的提问中。
- en: 'The original dataset had three test scores: `G1`, `G2`, and `G3`. Where `G1`
    would be first grade, `G2` being the second grade, and `G3` being the final grade.
    We will simplify the problem by just providing pass or fail. This can be done
    by adding these three scores and checking whether the sum is sufficiently large
    enough which is 35\. That brings us to about a 50% split of students passing and
    failing, giving us a balanced dataset. Now let''s look at the code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集中有三项考试成绩：`G1`、`G2`和`G3`。其中，`G1`是第一学期成绩，`G2`是第二学期成绩，`G3`是最终成绩。我们将通过只提供通过或失败来简化问题。这可以通过将三项成绩相加，检查其和是否足够大（即35）来完成。这样，我们大约能得到50%的学生通过，50%的学生失败，从而得到一个平衡的数据集。现在我们来看一下代码：
- en: '![](img/00018.gif)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00018.gif)'
- en: We import the dataset (`student-por.csv`), which comes with semicolons instead
    of commas; hence, we mention the separators as semicolons. To cross verify, we
    will find the number of rows in the dataset. Using the length variable, we can
    see that there are `649` rows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了数据集（`student-por.csv`），该数据集使用分号而不是逗号作为分隔符，因此我们将分隔符指定为分号。为了进行交叉验证，我们将找到数据集中的行数。使用长度变量，我们可以看到数据集有`649`行。
- en: Next we add columns for pass and fail. The data in these columns would contain
    1 or 0, where 1 means pass and 0 means fail. We are going to do that by computing
    with every row what the sum of the test scores would be. This will be calculated
    as if the sum of three score is greater than or equal to 35, 1 is given to the
    student and failing to that rule 0 is given to the student.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加了“通过”和“失败”列。这些列中的数据将包含1或0，其中1表示通过，0表示失败。我们将通过计算每一行的考试成绩之和来实现这一点。如果三项成绩之和大于或等于35，学生将被标记为1，否则标记为0。
- en: 'We need to `apply` this rule on every row of the dataset, and this will be
    done using the `apply` function, which is a feature of Pandas. Here `axis=1` means
    use apply per row and `axis=0` would mean apply per column. The next line means
    that a variable needs to be dropped: either `G1`, `G2`, `G3`. The following screenshot
    of the code will provide you with an idea of what we just learned:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对数据集的每一行应用这个规则，这将通过`apply`函数来实现，这是Pandas的一个功能。这里的`axis=1`表示按行使用apply，`axis=0`则表示按列应用。下一行表示需要删除一个变量：`G1`、`G2`
    或 `G3`。以下的代码截图将帮助你理解我们刚才学到的内容：
- en: '![](img/00019.gif)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.gif)'
- en: 'The following screenshot shows the first 5 rows of the dataset and 31 columns. There
    are 31 columns because we have all the attributes plus our pass and fail columns:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了数据集的前5行和31列。之所以有31列，是因为我们有所有的属性列，以及我们的"通过"和"未通过"列：
- en: '![](img/00020.gif)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00020.gif)'
- en: As mentioned before, some of these columns are words or phrases, such as **Mjob**,
    **Fjob**, **internet**, and **romantic**. These columns need to be converted into
    numbers, which can be done using the `get_dummies` function, which is a Pandas
    feature, and we need to mention which columns are the ones that we want to turn
    into numeric form.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些列中的一些是单词或短语，例如**Mjob**、**Fjob**、**internet** 和 **romantic**。这些列需要转换为数字，可以使用`get_dummies`函数来实现，这是Pandas的一个功能，我们需要指定要转换为数字形式的列。
- en: In the case of **Mjob**, for example, the function it is going to look at all
    the different possible answers or the values in that column and it's going to
    give each value a column name. These columns will receive names such as rename
    the columns to **Mjob** **at_home**, **Mjob** **health**, or **Mjob**. These new
    columns, for example, the **Mjob at_home** column will have value **1** and the
    rest will have **0**. This means only one of the new columns generated will have
    one.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以**Mjob**为例，函数会查看该列中所有可能的答案或值，并为每个值赋予一个列名。这些列将被命名为**Mjob** **at_home**、**Mjob**
    **health** 或 **Mjob**。例如，**Mjob at_home**列的值将为**1**，其余列的值为**0**。这意味着生成的新列中只有一个列的值为1。
- en: This is know as **one-hot encoding**. The reason this name was given is for
    example, imagine some wires going into a circuit. Suppose in the circuit there
    are five wires, and you want use one-hot encoding method, you need to activate
    only one of these wires while keeping the rest of wires off.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**独热编码**。之所以给这个方法起这个名字，是因为例如，想象有一些电线进入电路。假设电路中有五根电线，而你想使用独热编码方法，你需要激活其中一根电线，同时保持其余电线关闭。
- en: 'On performing `get_dummies` function on our dataset, You can notice for example
    **activities_no** and **activities_yes** columns. The originally associated columns
    that said no had 1 as value under **activies_no** column followed by 0\. The same
    as for **activities_yes** had yes it would have a value 0 followed by 1 for others.
    This led to creation of many more new columns around 57 in total but this made
    our dataset full of numeric data. The following screenshot shows the columns **activities_yes**
    and **activities_no** columns:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集执行`get_dummies`函数后，你可以注意到例如**activities_no**和**activities_yes**列。原先关联的列中，表示"no"的在**activities_no**列下的值为1，后面是0。对于**activities_yes**列，如果值为"yes"，则该列为0，其他列为1。这导致创建了许多新的列，总数约为57个，但这使我们的数据集充满了数字数据。以下截图显示了**activities_yes**和**activities_no**列：
- en: '![](img/00021.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.jpeg)'
- en: Here we need to shuffle the rows and produce a training set with first 500 rows
    and rest 149 rows for test set and then we just need to get attributes form the
    training set which means we will get rid of the pass column and save the pass
    column separately. The same is repeated for the testing set. We will apply the
    attributes to the entire dataset and save the pass column separately for the entire
    dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们需要打乱行并生成一个包含前500行的训练集，剩余的149行作为测试集，然后我们只需要从训练集中获取属性，也就是说我们将去掉"通过"列，并将"通过"列单独保存。对于测试集，也进行相同操作。我们将对整个数据集应用这些属性，并将"通过"列单独保存。
- en: 'Now we will find how many passed and failed from the entire dataset. This can
    be done by computing the percentage number of passed and failed which will give
    us a result of 328 out of 649\. This being the pass percentage which is roughly
    around 50% of the dataset. This constitutes a well-balanced dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将计算整个数据集中有多少人通过和未通过。这可以通过计算通过和未通过的百分比来完成，结果是649个数据中有328个是通过的。这表示通过率大约是50%，也就是一个均衡的数据集。
- en: '![](img/00022.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: 'Next, we start building the decision tree using the `DecisionTreeClassifier`
    function from the scikit-learn package, which is a class capable of performing
    multi-class classification on a dataset. Here we will use the entropy or information
    gain metric to decide when to split. We will split at a depth of five questions,
    by using `max_depth=5` as an initial tree depth to get a feel for how the tree
    is fitting the data:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始使用来自 scikit-learn 包的 `DecisionTreeClassifier` 函数构建决策树，这是一个能够对数据集进行多类分类的类。在这里，我们将使用熵或信息增益度量来决定何时进行拆分。我们将在深度为五个问题时进行拆分，初始树深度设置为
    `max_depth=5`，以便我们了解树是如何适应数据的：
- en: '![](img/00023.gif)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.gif)'
- en: 'To get an overview of our dataset, we need to create a visual representation
    of the tree. This can be achieved by using one more function of the scikit-learn
    package: `expoert_graphviz`. The following screenshot shows the representation
    of the tree in a Jupyter Notebook:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得数据集的概览，我们需要创建树的可视化表示。这可以通过使用 scikit-learn 包的另一个函数：`expoert_graphviz` 来实现。以下截图展示了在
    Jupyter Notebook 中树的表示：
- en: '![](img/00024.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: This is for representation, more can be seen on scrolling in Jupyter output
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个表示，可以通过在 Jupyter 输出中滚动查看更多内容
- en: It is pretty much easy to understand the previous representation that the dataset
    is divided into two parts. Let's try to interpret the tree from the top. In this
    case if failure is greater than or equal to 0.5, that means it is true and it
    placed on left-hand side of the tree. Consider tree is always true on left side
    and false on right side, which means there are no prior failures. In the representation
    we can see left side of the tree is mostly in blue which means it is predicting
    a pass even though there are few questions as compared to the failure maximum
    of 5 questions. The tree is o n right side if failure is less than 0.5, this makes
    the student fail, which means the first question is false. Prediction is failure
    if in orange color but as it proceeds further to more questions since we have
    used `max_depth = 5`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的表示可以很容易理解，数据集被分为两部分。让我们尝试从顶部解读树。在这种情况下，如果失败大于或等于 0.5，这意味着它为真，并且它被放置在树的左侧。考虑到树总是在左侧为真，右侧为假，这意味着没有先前的失败。在表示中，我们可以看到树的左侧大多数是蓝色，这意味着它预测通过，即使与最多
    5 个问题的失败相比，问题的数量较少。树的右侧如果失败小于 0.5，这将导致学生失败，这意味着第一个问题是错误的。如果预测为失败，它会以橙色显示，但随着问题数量的增加，由于我们使用了
    `max_depth = 5`，预测会进一步调整。
- en: 'The following code block shows a method to export the visual representation
    which by clicking on Export and save to PDF or any format if you want to visualize
    later:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了一种方法，用于导出可视化表示，您可以通过点击导出并保存为 PDF 或其他格式，便于后续查看：
- en: '![](img/00025.gif)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.gif)'
- en: 'Next we check the score of the tree using the testing set that we created earlier:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前创建的测试集检查树的评分：
- en: '![](img/00026.gif)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.gif)'
- en: 'The result we had was approximately 60%. Now let''s cross verify the result
    to be assured that the dataset is trained perfectly:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果大约是 60%。现在让我们交叉验证一下结果，以确保数据集已经被完美地训练过：
- en: '![](img/00027.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: 'Performing cross-validation on the entire dataset which will split the data
    on a of 20/80 basis, where 20% is the on testing set and 80% is on the training
    set. The average result is 67%. This shows that we have a well-balanced dataset.
    Here we have various choices to make regarding `max_depth`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对整个数据集执行交叉验证，它会按照 20/80 的比例拆分数据，其中 20% 用作测试集，80% 用作训练集。平均结果为 67%。这表明我们有一个良好平衡的数据集。在这里，我们有多个关于
    `max_depth` 的选择：
- en: '![](img/00028.gif)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.gif)'
- en: We use various `max_depth` values from 1 to 20, Considering we make a tree with
    one question or with 20 questions having depth value of 20 which will give us
    questions more than 20 which is you will have to go 20 steps down to reach a leaf
    node. Here we again perform cross- validation and save and print our answer. This
    will give different accuracy and calculations. On analyzing it was found that
    on have depth of 2 and 3 the accuracy is the best which was compared accuracy
    from the average we found earlier.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从 1 到 20 的各种 `max_depth` 值。考虑到我们构建的是一个深度为 20 的树，其中每个问题都具有 20 的深度值，这样我们将会得到超过
    20 个问题，也就是说你需要向下走 20 步才能到达叶节点。在这里，我们再次执行交叉验证，并保存并打印我们的答案。这将给出不同的准确性和计算结果。通过分析发现，当深度为
    2 和 3 时，准确性最佳，且与之前得到的平均准确性进行了比较。
- en: 'The following screenshot shows the data that we will be using to the create
    graph:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了我们将用于创建图表的数据：
- en: '![](img/00029.gif)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.gif)'
- en: 'The error bars shown in the following screenshot are the standard deviations
    in the score, which concludes that a depth of 2 or 3 is ideal for this dataset,
    and that our assumption of 5 was incorrect:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图中的误差条表示分数的标准差，结果表明，对于这个数据集，深度为 2 或 3 是理想的，而我们假设的深度为 5 是不正确的：
- en: '![](img/00030.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.jpeg)'
- en: More depth doesn't give any more power, and just having one question, which
    would be *did you fail previously?*, isn't going to provide you with the same
    amount of information as two or three questions would.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更深的层次并不会带来更多的能力，仅仅问一个问题，*你之前失败过吗？*，并不能提供与两个或三个问题相同的信息量。
- en: Our model shows that having more depth does not necessarily help, nor does having
    a single question of *did you fail previously?* provide us with the same amount
    of information as two or three questions would give us.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型表明，增加深度并不一定有帮助，单纯询问一个问题，*你之前失败过吗？*，也无法提供与两个或三个问题相同的信息量。
- en: Summary
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we learned about classification and techniques for evaluation,
    and learned in depth about decision trees. We also created a model to predict
    student performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们学习了分类和评估技术，并深入了解了决策树。我们还创建了一个模型来预测学生的表现。
- en: In the next chapter, we will learn more about random forests and use machine
    learning and random forests to predict bird species.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入了解随机森林，并利用机器学习和随机森林来预测鸟类物种。
