- en: Sentiment Analysis Using Word2Vec and LSTM Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Word2Vec和LSTM网络进行情感分析
- en: Sentiment analysis is a systematic way to identify, extract, quantify, and study
    effective states and subjective information. This is widely used in **natural
    language processing** (**NLP**), text analytics, and computational linguistics.
    This chapter demonstrates how to implement and deploy a hands-on deep learning
    project that classifies review texts as either positive or negative based on the
    words they contain. A large-scale movie review dataset that contains 50k reviews
    (training plus testing) will be used.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一种系统化的方式，用于识别、提取、量化并研究情感状态和主观信息。这在**自然语言处理**（**NLP**）、文本分析和计算语言学中广泛应用。本章演示了如何实现并部署一个实践性的深度学习项目，该项目基于文本中的词汇将评论文本分类为积极或消极。将使用一个包含50k评论（训练加测试）的电影评论大数据集。
- en: 'A combined approach using Word2Vec (that is, a widely used word embedding technique
    in NLP) and the **Long Short-Term Memory** (**LSTM**) network for modeling will
    be applied: the pre-trained Google news vector model will be used as the neural
    word embeddings. Then, the training vectors, along with the labels, will be fed
    into the LSTM network to classify them as negative or positive sentiments. Finally,
    it evaluates the trained model on the test set.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用结合使用Word2Vec（即NLP中广泛使用的词嵌入技术）和**长短期记忆**（**LSTM**）网络的建模方法：将使用预训练的Google新闻向量模型作为神经词嵌入。然后，将训练向量与标签一起输入LSTM网络，分类为消极或积极情感。最后，对测试集进行已训练模型的评估。
- en: Additionally, it shows how to apply text preprocessing techniques such as tokenizer,
    stop words removal, and **term frequency-inverse document frequency** (**TF-IDF**),
    and word-embedding operations in **Deeplearning4j** (**DL4J**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还展示了如何应用文本预处理技术，如分词器、停用词移除和**词频-逆文档频率**（**TF-IDF**）以及**Deeplearning4j**（**DL4J**）中的词嵌入操作。
- en: Nevertheless, it also shows how to save the trained DL4J model. Later on, the
    saved model will be restored from disk to make sentiment prediction on other small-scale
    review texts from Amazon cell, Yelp, and IMDb. Finally, it has answers to some
    frequently asked questions related to the projects and possible outlook.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它还展示了如何保存已训练的DL4J模型。之后，保存的模型将从磁盘恢复，并对来自Amazon Cell、Yelp和IMDb的其他小规模评论文本进行情感预测。最后，它还解答了与项目相关的一些常见问题及可能的前景。
- en: 'The following topics will be covered throughout this end-to-end project:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下主题将在本端到端项目中覆盖：
- en: Sentiment analysis in NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP中的情感分析
- en: Using Word2Vec for neural word embeddings
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec进行神经词嵌入
- en: Dataset collection and description
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集收集与描述
- en: Saving and restoring pre-trained models with DL4J
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DL4J保存和恢复预训练模型
- en: Developing a sentiment-analyzing model using Word2Vec and LSTM
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec和LSTM开发情感分析模型
- en: Frequently asked questions (FAQs)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQ）
- en: Sentiment analysis is a challenging task
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析是一项具有挑战性的任务
- en: Text analytics in NLP is all about processing and analyzing large-scale structured
    and unstructured text to discover hidden patterns and themes and derive contextual
    meaning and relationships. Text analytics has so many potential use cases, such
    as sentiment analysis, topic modeling, TF-IDF, named entity recognition, and event
    extraction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中的文本分析旨在处理和分析大规模的结构化和非结构化文本，以发现隐藏的模式和主题，并推导出上下文的意义和关系。文本分析有很多潜在的应用场景，如情感分析、主题建模、TF-IDF、命名实体识别和事件提取。
- en: Sentiment analysis includes many example use cases, such as analyzing the political
    opinions of people on Facebook, Twitter, and other social media. Similarly, analyzing
    the reviews of restaurants on Yelp is also another great example of Sentiment
    Analysis. NLP frameworks and libraries such as OpenNLP and Stanford NLP are typically
    used to implement sentiment analysis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析包括许多示例应用场景，如分析人们在Facebook、Twitter等社交媒体上的政治观点。同样，分析Yelp上的餐厅评论也是情感分析的另一个优秀例子。通常使用像OpenNLP和Stanford
    NLP这样的NLP框架和库来实现情感分析。
- en: However, for analyzing sentiments using text, particularly unstructured texts,
    we must find a robust and efficient way of feature engineering to convert the
    text into numbers. However, several stages of transformation of data are possible
    before a model is trained, and then subsequently deployed and finally performing
    predictive analytics. Moreover, we should expect the refinement of the features
    and model attributes. We could even explore a completely different algorithm,
    repeating the entire sequence of tasks as part of a new workflow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在使用文本分析情感时，尤其是分析非结构化文本时，我们必须找到一种强大且高效的特征工程方法，将文本转换为数字。然而，在模型训练之前，数据的转换可能经历多个阶段，然后再进行部署并最终执行预测分析。此外，我们应当预期对特征和模型属性进行进一步优化。我们甚至可以探索一种完全不同的算法，作为新工作流的一部分，重复整个任务流程。
- en: When you look at a line of text, we see sentences, phrases, words, nouns, verbs,
    punctuation, and so on, which, when put together, have a meaning and purpose.
    Humans are very good at understanding sentences, words, slang, annotations, and
    context extremely well. This comes from years of practice and learning how to
    read/write proper grammar, punctuation, exclamations, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看一行文本时，我们会看到句子、短语、单词、名词、动词、标点符号等等，这些东西组合在一起具有一定的意义和目的。人类在理解句子、单词、俚语、注解和上下文方面非常擅长。这是经过多年的练习和学习如何阅读/书写规范的语法、标点、感叹词等的结果。
- en: 'For example, the two sentences: *DL4J makes predictive analytics easy*, and
    *Predictive analytics makes DL4J easy*, might result in the same sentence vector
    having the same length equal to the size of our vocabulary that we pick. The second
    drawback is that the words "is" and "DL4J" have the same numerical index value
    of one, but our intuition says that the word "is" isn''t important compared to
    "DL4J". Let''s take a look at the second example: when your search string is *hotels
    in Berlin* in Google, we want results pertaining to *bnb*, *motel*, *lodging*,
    and *accommodation* in Berlin, too.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，两个句子：*DL4J使预测分析变得简单* 和 *预测分析使DL4J变得简单*，可能导致相同的句子向量具有相同的长度，这个长度等于我们选择的词汇的大小。第二个问题是，“is”和“DL4J”这两个词的数值索引值都是1，但我们的直觉告诉我们，“is”与“DL4J”相比并不重要。再来看第二个例子：当你在Google上搜索字符串*hotels
    in Berlin*时，我们希望获得与*bnb*、*motel*、*lodging*、*accommodation*等相关的柏林的结果。
- en: 'When layman terms come to the party, natural language learning becomes more
    complicated. Take the word bank as an example. This has several connections with
    a financial institution and land alongside a body of water. Now, if a natural
    sentence contains the term "bank" in conjunction with words such as finance, money,
    treasury, and interest rates, we can understand that its intended meaning is the
    former. However, if the neighboring words are water, shore, river, and lake, and
    so on, the case is the latter. Now, the question would be: can we exploit this
    concept to deal with polysemy and synonyms and make our model learn better?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当普通词汇出现时，自然语言学习变得更加复杂。以“银行”（bank）这个词为例。它既与金融机构相关，也与水边的陆地相关。现在，如果一个自然句子中包含“bank”一词，并且与金融、金钱、国库和利率等词语一同出现，我们可以理解它的含义是前者。然而，如果它周围的词语是水、海岸、河流、湖泊等，那么它指的就是后者。那么，问题来了：我们是否可以利用这一概念来处理歧义和同义词，并使我们的模型学习得更好？
- en: '![](img/f620eb24-b4e5-4a8d-9a95-ef5685415cd2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f620eb24-b4e5-4a8d-9a95-ef5685415cd2.png)'
- en: Classical machine learning versus deep learning-based NPL
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习与基于深度学习的自然语言处理（NLP）
- en: Nevertheless, natural language sentences also contain vague words, slang, trivial
    words, and special characters, and all of these make the overall understanding
    and machine learning troublesome.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自然语言句子也包含模糊词汇、俚语、琐碎的词语以及特殊字符，这些都使得整体理解和机器学习变得复杂。
- en: We have already seen how to use one-hot encoding or StringIndexer techniques
    to convert categorical variables (or even words) into numeric form. However, this
    kind of program often fails to interpret the semantics in a complex sentence,
    especially for lengthy sentences or even words. Consequently, human words have
    no natural notion of similarity. Thus, we naturally won't try to replicate this
    kind of capability, right?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用独热编码（one-hot encoding）或字符串索引器（StringIndexer）技术将分类变量（甚至单词）转换为数字形式。然而，这类程序通常无法理解复杂句子中的语义，特别是对于长句子或甚至单个单词。因此，人类的词汇并没有天然的相似性概念。因此，我们自然不会尝试复制这种能力，对吧？
- en: How can we build a simple, scalable, faster way to deal with the regular texts
    or sentences and derive relations between a word and its contextual words, and
    then embed them in billions of words that will produce exceedingly good word representations
    into numeric vector space so that the machine learning models can consume them?
    Let's look at the Word2Vec model to find the answer to this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何构建一个简单、可扩展、更快的方法来处理常规的文本或句子，并推导出一个词与其上下文词之间的关系，然后将它们嵌入到数十亿个词中，从而在数值向量空间中产生极好的词表示，以便机器学习模型可以使用它们呢？让我们通过Word2Vec模型来找到答案。
- en: Using Word2Vec for neural word embeddings
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Word2Vec进行神经网络词嵌入。
- en: 'Word2Vec is a two-layer neural network that processes texts and turns them
    into numerical features. This way, the output of the Word2Vec is a vocabulary
    in which each word is embedded in vector space. The resulting vector can then
    be fed into a neural network for better understanding of natural languages. The
    novelist EL Doctorow has expressed this idea quite poetically in his book *Billy
    Bathgate*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一个两层的神经网络，它处理文本并将其转化为数值特征。通过这种方式，Word2Vec的输出是一个词汇表，其中每个词都被嵌入到向量空间中。结果向量可以输入到神经网络中，以更好地理解自然语言。小说家EL
    Doctorow在他的书《*Billy Bathgate*》中以诗意的方式表达了这个思想：
- en: '"It''s like numbers are language, like all the letters in the language are
    turned into numbers, and so it''s something that everyone understands the same
    way. You lose the sounds of the letters and whether they click or pop or touch
    the palate, or go ooh or aah, and anything that can be misread or con you with
    its music or the pictures it puts in your mind, all of that is gone, along with
    the accent, and you have a new understanding entirely, a language of numbers,
    and everything becomes as clear to everyone as the writing on the wall. So as
    I say there comes a certain time for the reading of the numbers."'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: “这就像数字是语言，就像语言中的所有字母都被转化为数字，因此它是每个人都以相同方式理解的东西。你失去了字母的声音，无论它们是咔嚓声、啪嗒声、触碰上腭的声音，还是发出‘哦’或‘啊’的声音，任何可能被误读的东西，或是它通过音乐或图像欺骗你心智的东西，全都消失了，连同口音一同消失，你获得了一种完全新的理解，一种数字的语言，一切变得像墙上的文字一样清晰。所以，正如我所说，有一个特定的时刻，是时候去读这些数字了。”
- en: 'While using BOW and TF-IDF, all words are projected into the same position
    and their vectors are averaged: we address the word importance without considering
    the importance of word order in a collection of documents or in a single document.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用BOW和TF-IDF时，所有词都被投射到相同的位置，并且它们的向量被平均化：我们考虑了词的重要性，但没有考虑在文档集合或单个文档中词序的重要性。
- en: As the order of words in the history does not influence the projection, both
    BOW and TF-IDF have no such features that can take care of this issue. Word2Vec
    encodes each word into a vector by using either context to predict a target word
    using a **continuous bag-of-words** (**CBOW**) or using a word to predict a target
    context, which is called **continuous skip-gram**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于历史中词的顺序不会影响投影，BOW和TF-IDF都没有可以处理这个问题的特征。Word2Vec通过使用上下文预测目标词（使用**连续词袋法**（**CBOW**））或使用一个词来预测目标上下文（这就是所谓的**连续跳字法**）来将每个词编码成一个向量。
- en: '**N-gram versus skip-gram**: Words are read into vectors one at a time and
    scanned back and forth within a certain range'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N-gram与跳字法（skip-gram）**：词是一次一个地读入向量，并在一定范围内来回扫描。'
- en: '**CBOW**: The CBOW technique uses a continuously distributed representation
    of the context'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CBOW**：CBOW技术使用一个连续分布的上下文表示。'
- en: '**Continuous skip-gram**: Unlike CBOW, this method tries to maximize classification
    of a word based on another word in the same sentence'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续跳字法（Continuous skip-gram）**：与CBOW不同，这种方法尝试最大化基于同一句话中的另一个词来分类当前词。'
- en: I have experienced that an increase in the range improves the quality of the
    resulting word vectors, but it also increases the computational complexity. Since
    more distant words are usually less related to the current word than those close
    to it are, we give less weight to the distant words by sampling less from those
    words in our training examples. Because of the model building and prediction,
    times also increase.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经经历过，增加范围可以提高结果词向量的质量，但也会增加计算复杂度。由于距离较远的词通常与当前词的关系不如近距离的词密切，因此我们通过在训练样本中从这些远离的词中采样较少，来减少对它们的权重。由于模型构建和预测，所需的时间也会增加。
- en: 'A comparative analysis from the architecture''s point of view can be seen in
    the following diagram, where the architecture predicts the current word based
    on the context, and the skip-gram predicts the surrounding words given the current
    word:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构的角度来看，可以通过以下图示看到对比分析，其中架构根据上下文预测当前单词，而skip-gram根据当前单词预测周围的单词：
- en: '![](img/bba3319f-913c-4a4d-af80-de53f74b9cfd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bba3319f-913c-4a4d-af80-de53f74b9cfd.png)'
- en: 'CBOW versus skip-gram (source: Tomas Mikolov et al., Efficient Estimation of
    Word Representations in Vector Space, https://arxiv.org/pdf/1301.3781.pdf)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW与skip-gram（来源：Tomas Mikolov等人，《高效估计向量空间中的词表示》，https://arxiv.org/pdf/1301.3781.pdf）
- en: Datasets and pre-trained model description
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和预训练模型说明
- en: We are going to use the Large Movie Review dataset for training and testing
    the mode. Additionally, we will be using the Sentiment labeled Sentences dataset
    for making a single prediction on reviews on products, movies, and restaurants.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用大型电影评论数据集来训练和测试模型。此外，我们还将使用带有情感标签的句子数据集来对产品、电影和餐厅的评论进行单一预测。
- en: Large Movie Review dataset for training and testing
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于训练和测试的大型电影评论数据集
- en: The former one is a dataset for binary sentiment classification containing substantially
    more data than previous benchmark datasets. The dataset can be downloaded from
    [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).
    Alternatively, I have utilized a Java method that comes from DL4J examples that
    also downloads and extracts this dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前者是一个用于二元情感分类的数据集，包含的数据量远超过之前的基准数据集。该数据集可以从[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)下载。或者，我使用了来自DL4J示例的Java方法，该方法也可以下载并提取此数据集。
- en: 'I would like to acknowledge the following publications: Andrew L. Maas, Raymond
    E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011),
    *Learning Word Vectors for Sentiment Analysis*, The 49^(th) Annual Meeting of
    the Association for Computational Linguistics (ACL 2011).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我想特别感谢以下出版物：Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew
    Y. Ng, 和 Christopher Potts。（2011），*用于情感分析的词向量学习*，《第49届计算语言学协会年会（ACL 2011）》。
- en: This dataset contains 50,000 movie reviews along with their associated binary
    sentiment polarity labels. The reviews are split evenly into 25,000 for both train
    and test sets. The overall distribution of labels is balanced (25,000 positive
    and 25,000 negative). We also include an additional 50 thousand unlabeled documents
    for unsupervised learning. In the labeled train/test sets, if a reviews has a
    score <= 4 out of 10, it is treated as a negative review, but having a score >=
    7 out of 10 is treated as a positive review. Nevertheless, reviews with more neutral
    ratings are not included in the datasets.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含50,000条电影评论及其对应的二元情感极性标签。评论被平均分配为25,000条用于训练集和测试集。标签的总体分布是平衡的（25,000条正面评论和25,000条负面评论）。我们还包含了额外的50,000条未标记的文档，用于无监督学习。在标记的训练/测试集中，如果评论得分<=4分（满分10分），则视为负面评论，而得分>=7分则视为正面评论。然而，评分较为中立的评论未包含在数据集中。
- en: Folder structure of the dataset
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集的文件夹结构
- en: 'There are two folders, namely `train` and `test` for the training and test
    sets, respectively. Each folder has two separate subfolders called `pos` and `neg`,
    which contain reviews with binary labels (pos, neg). Reviews are stored in text
    files having the name `id_rating.txt`, where `id` is a unique ID and `rating`
    is the star rating on a 1-10 scale. Take a look at the following diagram to get
    a clearer view on the directory''s structure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有两个文件夹，分别是`train`和`test`，用于训练集和测试集。每个文件夹下有两个子文件夹，分别是`pos`和`neg`，其中包含带有二进制标签（pos,
    neg）的评论。评论以名为`id_rating.txt`的文本文件存储，其中`id`是唯一的ID，`rating`是1-10分的星级评分。查看以下图示可以更清楚地了解目录结构：
- en: '![](img/3c758053-b424-4acf-b7d1-76fa4dcc7225.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c758053-b424-4acf-b7d1-76fa4dcc7225.png)'
- en: Folder structure in Large Movie Review Dataset
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大型电影评论数据集中的文件夹结构
- en: 'For example, the `test/pos/200_8.txt` file is the text for a positive-labeled
    test set example with a unique ID of 200 and a star rating of 8/10 from IMDb.
    The `train/unsup/` directory has zero for all ratings because the ratings are
    omitted for this portion of the dataset. Let''s look at a sample positive review
    from IMDb:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`test/pos/200_8.txt`文件是一个正面标签的测试集示例，具有唯一ID 200和IMDb评分为8/10。`train/unsup/`目录中的所有评分均为零，因为该部分数据集省略了评分。让我们看一个来自IMDb的示例正面评论：
- en: '"Bromwell High is a cartoon comedy. It ran at the same time as some other programs
    about school life, such as "Teachers". My 35 years in the teaching profession
    lead me to believe that Bromwell High''s satire is much closer to reality than
    is "Teachers". The scramble to survive financially, the insightful students who
    can see right through their pathetic teachers'' pomp, the pettiness of the whole
    situation, all remind me of the schools I knew and their students. When I saw
    the episode in which a student repeatedly tried to burn down the school, I immediately
    recalled ......... at .......... High. A classic line: INSPECTOR: I''m here to
    sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many
    adults of my age think that Bromwell High is far-fetched. What a pity that it
    isn''t!"'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '"《布罗姆威尔高中》是一部卡通喜剧。它与其他一些关于学校生活的节目同时播放，比如《教师》。我在教育行业工作了35年，我认为《布罗姆威尔高中》的讽刺性比《教师》更贴近现实。为了生存而拼命挣扎的财务问题、那些能透彻看穿他们可悲老师虚伪的有洞察力的学生、整个情况的琐碎性，都让我想起了我所知道的学校和它们的学生。当我看到一集中有个学生一再试图烧掉学校时，我立刻回想起……在……高中。经典台词：督察：我来是为了开除你们的一位老师。学生：欢迎来到布罗姆威尔高中。我想我的很多同龄人可能觉得《布罗姆威尔高中》太夸张了。真遗憾，这并不夸张！"'
- en: Therefore, from the preceding review text, we can understand that the respective
    audience gave Bromwell High (a British-Canadian adult animated series about a
    British high school in South London, which you can see more of at [https://en.wikipedia.org/wiki/Bromwell_High](https://en.wikipedia.org/wiki/Bromwell_High))
    a positive review, that is, a positive sentiment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从前面的评论文本中，我们可以理解到，相应的观众给《布罗姆威尔高中》（一部关于位于伦敦南部的英国高中、英加合制的成人动画剧集，更多信息可以见 [https://en.wikipedia.org/wiki/Bromwell_High](https://en.wikipedia.org/wiki/Bromwell_High)）给出了积极的评价，即积极的情感。
- en: Description of the sentiment labeled dataset
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感标注数据集描述
- en: 'The sentiment labeled sentences dataset was downloaded from the UCI machine
    learning repository at [http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).
    This dataset was a research outcome by Kotzias and is used in the following publication:
    *From Group to Individual Labels using Deep Features*, Kotzias et. al, KDD'' 2015.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 情感标注句子数据集是从UCI机器学习库下载的，网址是 [http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)。这个数据集是Kotzias的研究成果，并且在以下出版物中使用：*从群体到个体标签使用深度特征*，Kotzias等，KDD'
    2015。
- en: 'The dataset contains sentences labeled with a positive or negative sentiment,
    extracted from reviews of products, movies, and restaurants. The review is a tab-delimited
    review having review sentences and a score of either 1 (for positive) or 0 (for
    negative). Let''s look at a sample review from Yelp with an associated label:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含标注为积极或消极情感的句子，这些句子来源于产品、电影和餐馆的评论。评论是一个制表符分隔的文件，其中包含评论句子和得分，得分为1（表示积极）或0（表示消极）。我们来看一个来自Yelp的示例评论及其标签：
- en: '*"I was disgusted because I was pretty sure that was human hair."*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*"我感到恶心，因为我几乎可以肯定那是人类的头发。"*'
- en: In the preceding review text, the score is 0, so it is a negative review and
    it expresses a negative sentiment on the part of the customer. On the other hand,
    there are 500 positive and 500 negative sentences.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的评论文本中，得分为0，因此它是一个负面评论，表达了客户的负面情感。另一方面，有500个积极句子和500个消极句子。
- en: 'Those were selected at random for larger datasets of reviews. The author has
    attempted to select sentences that have a clearly positive or negative connotation;
    the goal was for no neutral sentences to be selected. The review sentences are
    collected from three different websites/fields, which are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是从更大的评论数据集中随机选择的。作者试图选择那些有明确积极或消极含义的句子；目标是没有选择中立句子。这些评论句子来自三个不同的网站/领域，具体如下：
- en: '[https://www.imdb.com/](https://www.imdb.com/)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.imdb.com/](https://www.imdb.com/)'
- en: '[https://www.amazon.com/](https://www.amazon.com/)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.amazon.com/](https://www.amazon.com/)'
- en: '[https://www.yelp.com/](https://www.yelp.com/)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.yelp.com/](https://www.yelp.com/)'
- en: Word2Vec pre-trained model
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec 预训练模型
- en: Instead of generating a new Word2Vec model from scratch, Google's pre-trained
    news word vector model can be used, which provides an efficient implementation
    of the CBOW and skip-gram architectures for computing vector representations of
    words. These representations can subsequently be used in many NLP applications
    and further research.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与其从头开始生成一个新的 Word2Vec 模型，不如使用 Google 预训练的新闻词向量模型，它提供了 CBOW 和 skip-gram 架构的高效实现，用于计算单词的向量表示。这些表示随后可以用于许多
    NLP 应用和进一步的研究。
- en: The model can be downloaded from [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)
    manually. The Word2Vec model takes a text corpus as input and produces the word
    vectors as output. It first constructs a vocabulary from the training text data
    and then learns vector representation of words.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从 [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)
    手动下载模型。Word2Vec 模型以文本语料库为输入，输出词向量。它首先从训练文本数据构建词汇表，然后学习单词的向量表示。
- en: 'There are two ways to achieve a Word2Vec model: by using continuous bag-of-words
    and continuous skip-gram. Skip-gram is slower, but better for infrequent words,
    although CBOW is faster.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以实现 Word2Vec 模型：使用连续词袋模型（CBOW）和连续跳字模型（skip-gram）。Skip-gram 速度较慢，但对不常见的单词效果更好，而
    CBOW 速度较快。
- en: The resulting word vector file can be used as a feature in many natural language
    processing and machine learning applications.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的词向量文件可以作为许多自然语言处理和机器学习应用中的特征。
- en: Sentiment analysis using Word2Vec and LSTM
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Word2Vec 和 LSTM 进行情感分析
- en: 'First, let''s define the problem. Given a movie review (raw text), we have
    to classify that movie review as either positive or negative based on the words
    it contains, that is, sentiment. We do this by combining the Word2Vec model and
    LSTM: each word in a review is vectorized using the Word2Vec model and fed into
    an LSTM net. As stated earlier, we will train data in the Large Movie Review dataset.
    Now, here is the workflow of the overall project:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义问题。给定一条电影评论（原始文本），我们需要根据评论中的单词将其分类为正面或负面，即情感分析。我们通过结合 Word2Vec 模型和 LSTM
    来实现：评论中的每个单词都通过 Word2Vec 模型向量化，然后输入到 LSTM 网络中。如前所述，我们将在大型电影评论数据集中训练数据。现在，以下是整体项目的工作流程：
- en: First, we download the movie/product reviews dataset
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们下载电影/产品评论数据集
- en: Then we create or reuse an existing Word2Vec model (for example, Google News
    word vectors)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们创建或重用一个现有的 Word2Vec 模型（例如，Google News 词向量）
- en: Then we load each review text and convert words to vectors and reviews to sequences
    of vectors
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们加载每条评论文本，并将单词转换为向量，将评论转换为向量序列
- en: Then we create and train the LSTM network
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们创建并训练 LSTM 网络
- en: Then we save the trained model
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们保存训练好的模型
- en: Then we evaluate the model on the test set
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们在测试集上评估模型
- en: Then we restore the trained model and evaluate a sample review text from the
    sentiment labeled dataset
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们恢复训练好的模型，并评估情感标注数据集中的一条评论文本
- en: 'Now, let''s take a look at what the `main()` method would look like if we go
    with the preceding workflow:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如果我们遵循前面的工作流程，`main()` 方法会是什么样子：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's break the preceding steps down into smaller steps. We'll start with dataset
    preparation using the Word2Vec model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前面的步骤分解成更小的步骤。我们将从使用 Word2Vec 模型的 dataset 准备开始。
- en: Preparing the train and test set using the Word2Vec model
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Word2Vec 模型准备训练集和测试集
- en: 'Now, to prepare the dataset for training and testing, first, we have to download
    three files, which are outlined as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了准备训练和测试数据集，首先我们必须下载以下三个文件：
- en: A Google-trained Word2Vec model
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Google 训练的 Word2Vec 模型
- en: A large Movie Review dataset
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大型电影评论数据集
- en: A sentiment labeled dataset
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个情感标注数据集
- en: 'The pre-trained Word2Vec is downloaded from [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)
    and then we can set the location for the Google News vectors manually:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 Word2Vec 可从 [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)
    下载，然后我们可以手动设置 Google News 向量的位置：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we will download and extract the Large Movie Review dataset from the following
    URL.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将从以下 URL 下载并提取大型电影评论数据集。
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s set the location to save and extract the training/testing data:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置保存位置并提取训练/测试数据：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can either manually download or extract the dataset in our preferred
    location or, alternatively, the following method does it in an automated way.
    Note that I have slightly modified the original DL4J implementation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以手动下载或在我们喜欢的位置提取数据集，或者使用以下方法以自动化方式完成。请注意，我对原始的 DL4J 实现做了些许修改：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding method, download the dataset from the URL I mentioned using
    HTTP protocol. Then, extract the dataset to the location we mentioned. For this,
    I have used the `TarArchiveEntry`, `TarArchiveInputStream`, and `GzipCompressorInputStream`
    utilities from Apache Commons. Interested readers can find more details at [http://commons.apache.org/](http://commons.apache.org/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方法中，使用HTTP协议从我提到的URL下载数据集，然后将数据集解压到我们提到的位置。为此，我使用了Apache Commons的`TarArchiveEntry`、`TarArchiveInputStream`和`GzipCompressorInputStream`工具。感兴趣的读者可以在[http://commons.apache.org/](http://commons.apache.org/)查看更多细节。
- en: In short, I have provided a class named `DataUtilities.java` that has two methods,
    `downloadFile()` and `extractTarGz()`, that are used for downloading and extracting
    the dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我提供了一个名为`DataUtilities.java`的类，其中有两个方法，`downloadFile()`和`extractTarGz()`，用于下载和解压数据集。
- en: 'First, the `downloadFile()` method takes the remote URL (that is, the URL of
    the remote file) and the local path (that is, where to download the file) as parameters
    and downloads a remote file if it doesn''t exist. Now, let''s see what the signature
    looks like:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`downloadFile()`方法接受远程URL（即远程文件的URL）和本地路径（即下载文件的位置）作为参数，如果文件不存在，则下载远程文件。现在，让我们看看签名是怎样的：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Second, the `extractTarGz()` method takes an input path (the `ism` input file
    path) and the output path (that is, the output directory path) as parameters and
    extracts the `tar.gz` file to a local folder. Now, let''s see what the signature
    looks like:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，`extractTarGz()`方法接受输入路径（即`ism`输入文件路径）和输出路径（即输出目录路径）作为参数，并将`tar.gz`文件解压到本地文件夹。现在，让我们看看签名是怎样的：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, to utilize the preceding methods, you have to import the following packages:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要使用前述方法，您必须导入以下包：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By the way, Apache Commons is an Apache project focused on all aspects of reusable
    Java components. See more at [https://commons.apache.org/](https://commons.apache.org/).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，Apache Commons是一个专注于可重用Java组件各个方面的Apache项目。更多信息请见[https://commons.apache.org/](https://commons.apache.org/)。
- en: Finally, the sentiment labeled dataset can be downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/00331/](https://archive.ics.uci.edu/ml/machine-learning-databases/00331/).
    Once you have finished these steps, the next task will be to prepare the training
    and testing set. For this, I have written a class named `SentimentDatasetIterator`,
    which is a `DataSetIterator` that is specialized for the IMDb review dataset used
    in our project. However, this can also be applied to any text dataset for text
    analytics in NLP. This class is a slight extension of the `SentimentExampleIterator.java`
    class, which is provided by the DL4J example. Thanks to the DL4J folks for making
    our life easier.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以从[https://archive.ics.uci.edu/ml/machine-learning-databases/00331/](https://archive.ics.uci.edu/ml/machine-learning-databases/00331/)下载情感标签数据集。完成这些步骤后，接下来的任务是准备训练集和测试集。为此，我编写了一个名为`SentimentDatasetIterator`的类，它是一个专门为我们项目中使用的IMDb评论数据集定制的`DataSetIterator`。不过，它也可以应用于任何用于自然语言处理文本分析的文本数据集。这个类是`SentimentExampleIterator.java`类的一个小扩展，该类是DL4J示例提供的。感谢DL4J团队让我们的工作变得更轻松。
- en: 'The `SentimentDatasetIterator` class takes either the train or test set from
    the sentiment labeled dataset and Google pre-trained Word2Vec and generates training
    data sets. On the other hand, a single class (negative or positive) is used as
    the label to predict the final time step of each review. Additionally, since we
    are dealing with reviews of different lengths and only one output at the final
    time step, we use padding arrays. In short, our training dataset should contain
    the following items, that is, the 4D object:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`SentimentDatasetIterator`类从情感标签数据集的训练集或测试集中获取数据，并利用Google预训练的Word2Vec生成训练数据集。另一方面，使用一个单独的类别（负面或正面）作为标签，预测每个评论的最终时间步。除此之外，由于我们处理的是不同长度的评论，并且只有在最终时间步有一个输出，我们使用了填充数组。简而言之，我们的训练数据集应该包含以下项，即4D对象：'
- en: Features from each review text
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个评论文本中提取特征
- en: Labels in either 1 or 0 (that is, for positive and negative, respectively)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签为1或0（即，分别表示正面和负面）
- en: Feature masks
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征掩码
- en: Label masks
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签掩码
- en: 'So, let''s get started with the following constructor, which is used for the
    following purposes:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们从以下构造函数开始，它用于以下目的：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding signature of the constructor, we used the following purposes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的构造函数签名中，我们使用了以下目的：
- en: To keep track of the positive and negative review files in the directory of
    the IMDb review data set
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于跟踪IMDb评论数据集中正面和负面评论文件
- en: To tokenize the review texts to words with stop words and unknown words removed
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将评论文本分词，去除停用词和未知词
- en: If the longest review exceeds `truncateLength`, only take the first `truncateLength`
    words
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果最长的评论超过`truncateLength`，只取前`truncateLength`个词
- en: Word2Vec object
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec对象
- en: The batch size, which is the size of each minibatch for training
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小，即每个小批量的大小，用于训练
- en: 'Once initialization is complete, we load each review test as a string. Then,
    we alternate between positive and negative reviews:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化完成，我们将每个评论测试加载为字符串。然后，我们在正面和负面评论之间交替：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we tokenize the reviews and filter out unknown words (that is, words
    that are not included in the pre-trained Word2Vec model, for example, stop words):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将评论分词，并过滤掉未知词（即不包含在预训练的Word2Vec模型中的词，例如停用词）：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, if the longest review exceeds the threshold `truncateLength`, we only
    take the first `truncateLength` words:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果最长评论超过阈值`truncateLength`，我们只取前`truncateLength`个词：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we create data for training. Here, we have `reviews.size()` examples
    of varying lengths since we have two labels, positive or negative:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建用于训练的数据。在这里，由于我们有两个标签，正面或负面，因此我们有`reviews.size()`个不同长度的示例：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we are dealing with reviews of different lengths and only one output
    at the final time step, we use padding arrays, where the mask arrays contain 1
    if data is present at that time step for that example, or 0 if the data is just
    padding:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们处理的是不同长度的评论，并且在最终时间步只有一个输出，我们使用填充数组，其中掩码数组在该时间步对该示例的数据存在时为1，如果数据只是填充则为0：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It is to be noted that creating the mask arrays for the features and labels
    are optional and may be null too. Then, we get the truncated sequence length of
    the *i*^(th) document, obtain all the word vectors for the current document, and
    transpose them to fit the second and third feature shape.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，为特征和标签创建掩码数组是可选的，并且也可以为空。然后，我们获取第*i*^(th)文档的截断序列长度，获取当前文档的所有词向量，并将其转置以适应第二和第三个特征形状。
- en: Once we have the word vectors ready, we put them into the features array at
    three indices, which is equal to `NDArrayIndex.interval(0, vectorSize)` having
    all elements between 0 and the length of the current sequence. Then, we assign
    1 to each position where a feature is present, that is, at the interval of 0 and
    the sequence length.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好词向量，我们将它们放入特征数组的三个索引位置中，该位置等于`NDArrayIndex.interval(0, vectorSize)`，包括0和当前序列长度之间的所有元素。然后，我们为每个存在特征的位置分配1，也就是在0和序列长度之间的区间。
- en: 'Now, when it comes to label encoding, we set [0, 1] for a negative review text
    and [1, 0] for a positive review text. Finally, we specify that an output exists
    at the final time step for this example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，涉及标签编码时，我们将负面评论文本设置为[0, 1]，将正面评论文本设置为[1, 0]。最后，我们指定此示例在最终时间步有输出：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that the main problem hindering dropout in NLP has been that it could not
    be applied to recurrent connections, as the aggregating dropout masks would effectively
    zero out embeddings over time—hence, feature masking has been used in the preceding
    code block.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，限制NLP中dropout应用的主要问题是它不能应用于循环连接，因为聚合的dropout掩码会随着时间的推移有效地将嵌入值归零——因此，前面的代码块中使用了特征掩码。
- en: 'Now, up to this point, all the required elements are prepared so finally, we
    return the dataset as an `NDArray` (that is, 4D) containing the features, labels,
    `featuresMask`, and `labelsMask`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，所有必要的元素都已准备好，因此最后，我们返回包含特征、标签、`featuresMask`和`labelsMask`的`NDArray`（即4D）数据集：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: More elaborately, using `DataSet`, we will create a dataset with the specified
    input `INDArray` and labels (output) `INDArray`, and (optionally) mask arrays
    for the features and labels.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，使用`DataSet`，我们将创建一个具有指定输入`INDArray`和标签（输出）`INDArray`的数据集，并（可选地）为特征和标签创建掩码数组。
- en: 'Finally, our training set will be at hand using the following invocation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用以下调用方式获取训练集：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Fantastic! Now we can create our neural networks by specifying the layers and
    hyperparameters in the next step.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在我们可以在下一步中通过指定层和超参数来创建我们的神经网络。
- en: Network construction, training, and saving the model
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络构建、训练和保存模型
- en: As discussed in the Titanic survival prediction section, again, everything starts
    with `MultiLayerConfiguration`, which organizes those layers and their hyperparameters.
    Our LSTM network consists of five layers. The input layer is followed by three
    LSTM layers. Then, the last layer is an RNN layer, which is also the output layer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如《Titanic生存预测》部分所讨论的那样，一切从`MultiLayerConfiguration`开始，它组织这些层及其超参数。我们的LSTM网络由五层组成。输入层后跟三层LSTM层。然后，最后一层是RNN层，也是输出层。
- en: More technically, the first layer is the input layer, and then three layers
    are placed as LSTM layers. For the LSTM layers, we initialize the weights using
    Xavier, we use SGD as the optimization algorithm with the Adam updater, and we
    use Tanh as the activation function. Finally, the RNN output layer has a softmax
    activation function that gives us a probability distribution over classes (that
    is, it outputs the sum to 1.0) and MCXENT, which is the multiclass cross entropy
    loss function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地讲，第一层是输入层，接着三层作为LSTM层。对于LSTM层，我们使用Xavier初始化权重，使用SGD作为优化算法，并配合Adam更新器，我们使用Tanh作为激活函数。最后，RNN输出层具有Softmax激活函数，给出类别的概率分布（也就是说，它输出的总和为1.0）以及MCXENT，这是多类交叉熵损失函数。
- en: 'For creating LSTM layers, DL4J provides both LSTM and `GravesLSTM` classes.
    The latter is an LSTM recurrent net, which is based on Graves, but comes up without
    CUDA support: supervised sequence labelling with RNN (see more at [http://www.cs.toronto.edu/~graves/phd.pdf](http://www.cs.toronto.edu/~graves/phd.pdf)).
    Now, before we start creating the network, first let''s define the required hyperparameters
    such as the number of input/hidden/output nodes (that is, neurons):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建LSTM层，DL4J提供了LSTM和`GravesLSTM`类。后者是一个基于Graves的LSTM循环网络，但没有CUDA支持：使用RNN进行监督序列标注（详情请参见[http://www.cs.toronto.edu/~graves/phd.pdf](http://www.cs.toronto.edu/~graves/phd.pdf)）。现在，在开始创建网络之前，首先让我们定义所需的超参数，如输入/隐藏/输出节点的数量（即神经元）：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now create a network configuration and conduct network training. With
    DL4J, you add a layer by calling a layer on the `NeuralNetConfiguration.Builder()`,
    specifying its place in the order of layers (the zero-indexed layer in the following
    code is the input layer):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个网络配置并进行网络训练。使用DL4J，你通过调用`NeuralNetConfiguration.Builder()`上的`layer`方法来添加一层，并指定它在层中的顺序（下面代码中的零索引层是输入层）：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we also specify that we do not need to do any pre-training (which
    is typically needed in a deep belief network or stacked auto-encoders). Then,
    we initialize the network and start the training on the training set:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还指定不需要进行任何预训练（这通常在深度信念网络或堆叠自编码器中需要）。然后，我们初始化网络并开始在训练集上进行训练：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Typically, this type of network has lots of hyperparameters. Let''s print the
    number of parameters in the network (and for each layer):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种类型的网络有很多超参数。让我们打印出网络中的参数数量（以及每一层的参数）：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As I said, our network has 1 million parameters, which is huge. This also imposes
    a great challenge while tuning hyperparameters. However, we will see some tricks
    in the FAQ section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所说，我们的网络有100万参数，这是非常庞大的。这在调整超参数时也带来了很大的挑战。不过，我们将在常见问题解答部分看到一些技巧。
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once the training has been completed, we can save the trained model for model
    persistence and subsequent reuse. For that, DL4J provides support for the trained
    model sterilization using the `writeModel()` method from the `ModelSerializer`
    class. Additionally, it provides the functionality for restoring the saved model
    using the `restoreMultiLayerNetwork()` method.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以保存训练好的模型，以便模型持久化和后续重用。为此，DL4J通过`ModelSerializer`类的`writeModel()`方法提供对训练模型的序列化支持。此外，它还提供了通过`restoreMultiLayerNetwork()`方法恢复已保存模型的功能。
- en: 'We will see more in the following step. Nevertheless, we can also save the
    network updater too, that is, the state for momentum, RMSProp, Adagrad, and so
    on:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的步骤中看到更多内容。不过，我们也可以保存网络更新器，即动量、RMSProp、Adagrad等的状态：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Restoring the trained model and evaluating it on the test set
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复训练好的模型并在测试集上进行评估
- en: Once the training has been completed, the next task will be to evaluate the
    model. We will evaluate the model's performance on the test set. For the evaluation,
    we will be using `Evaluation()`, which creates an evaluation object with two possible
    classes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，下一步任务就是评估模型。我们将在测试集上评估模型的表现。为了评估，我们将使用`Evaluation()`，它创建一个评估对象，包含两个可能的类。
- en: 'First, let''s iterate the evaluation on every test sample and get the network''s
    prediction from the trained model. Finally, the `eval()` method checks the prediction
    against the true class:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们对每个测试样本进行迭代评估，并从训练好的模型中获得网络的预测结果。最后，`eval()`方法将预测结果与真实类别进行比对：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The predictive accuracy for the sentiment analysis using LSTM is about 87%,
    which is good considering that we have not focused on hyperparameter tuning! Now,
    let''s see how the classifier predicts across each class:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSTM进行情感分析的预测准确率约为87%，考虑到我们没有专注于超参数调优，这个结果还是不错的！现在，让我们看看分类器在每个类别上的预测情况：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similar to [Chapter 2](e27fb252-7892-4659-81e2-2289de8ce570.xhtml), *Cancer
    Types Prediction Using Recurrent Type Networks*, we will now compute another metric
    called Matthews''s correlation coefficient for this binary classification problem:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第2章](e27fb252-7892-4659-81e2-2289de8ce570.xhtml)，*使用递归类型网络预测癌症类型*，我们现在将计算一个称为马修斯相关系数的度量，用于这个二分类问题：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This shows a weakly positive relationship, showing that our model performs quite
    well. Up next, we will use the trained model for inferencing, that is, we will
    perform predictions on sample review texts.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了一个弱正相关，表明我们的模型表现相当不错。接下来，我们将使用训练好的模型进行推理，也就是对样本评论文本进行预测。
- en: Making predictions on sample review texts
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对样本评论文本进行预测
- en: 'Now, let''s take a look at how our trained model generalizes, that is, how
    it performs on unseen review texts from the sentiment labeled sentences dataset.
    First, we need to restore the trained model from the disk:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看我们的训练模型如何泛化，也就是说，它在来自情感标注句子数据集的未见过的评论文本上的表现如何。首先，我们需要从磁盘中恢复训练好的模型：
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can randomly extract two review texts from IMDb, Amazon, and Yelp,
    where the first one expresses a positive sentiment, and the second one expresses
    a negative sentiment (according to the known labels). Then, we can create a HashMap
    containing both the review strings and labels:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以随机提取两条来自IMDb、Amazon和Yelp的评论文本，其中第一条表示正面情感，第二条表示负面情感（根据已知标签）。然后，我们可以创建一个包含评论字符串和标签的HashMap：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we create an array of the preceding strings:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个包含前面字符串的数组：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we iterate over the map and do the sample evaluation using the pre-trained
    model as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历这个映射并使用预训练的模型进行样本评估，如下所示：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If you look at the preceding code block carefully, you can see that we converted
    each review text as a time series by extracting features. Then, we computed the
    network output (that is, probability). Then, we compare the probability, that
    is, if the probability is that of it being a positive sentiment, we set a flag
    as true, or false otherwise. This way, we then take a decision on the final class
    prediction.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细查看前面的代码块，你会发现我们通过提取特征将每条评论文本转化为时间序列。然后，我们计算了网络输出（即概率）。接着，我们比较概率，也就是说，如果概率是正面情感的概率，我们就设置标志为真，否则为假。这样，我们就做出了最终的类别预测决定。
- en: 'We have also utilized the `loadFeaturesFromString()` method in the preceding
    code block, which converts a review string to features in `INDArray` format. It
    takes two parameters, `reviewContents`, which is the content of the review to
    vectorize, and `maxLength`, which is the maximum length of the review text. Finally,
    it returns a `features` array for the given input string:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在前面的代码块中使用了`loadFeaturesFromString()`方法，它将评论字符串转换为`INDArray`格式的特征。它接受两个参数，`reviewContents`，即要向量化的评论内容，以及`maxLength`，即评论文本的最大长度。最后，它返回给定输入字符串的`features`数组：
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you don't want to truncate, simply use `Integer.MAX_VALUE`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想截断，只需使用`Integer.MAX_VALUE`。
- en: 'Now, let''s go back to our original discussion. Hilariously, we made it more
    human, that is, without utilizing an activation function. Finally, we print the
    result for each review text and its associated label:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到原来的讨论。令人捧腹的是，我们使其更具人性化，也就是说，没有使用激活函数。最后，我们打印每条评论文本及其相关标签的结果：
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: So, our trained model has made 50% incorrect predictions, especially since it
    always predicts a positive review as a negative. In short, it is not that good
    at generalizing to unknown texts, which can be seen with an accuracy of 50%.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的训练模型做出了50%的错误预测，尤其是它总是将正面评论预测为负面评论。简而言之，它在泛化到未知文本时表现得不好，这可以通过50%的准确率看出来。
- en: 'Now, a stupid question might come to mind. Did our network underfit? Is there
    any way to observe how the training went? In other words, the question would be:
    Why didn''t our LSTM net neural show higher accuracy? We will try to answer these
    questions in the next section. So stay with me!'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可能会有一个愚蠢的问题浮现出来。我们的网络是否出现了欠拟合？有没有办法观察训练过程？换句话说，问题是：为什么我们的LSTM神经网络没有显示出更高的准确性？我们将在下一节中尝试回答这些问题。所以请继续关注！
- en: Frequently asked questions (FAQs)
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题（FAQ）
- en: 'Now that we have solved the sentiment analysis problem with an acceptable level
    of accuracy, there are other practical aspects of this problem and overall deep
    learning phenomena that need to be considered too. In this section, we will see
    some frequently asked questions that might already be on your mind. Answers to
    these questions can be found in Appendix A:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过可接受的准确度解决了情感分析问题，但该问题以及整体深度学习现象中还有其他实际方面需要考虑。在本节中，我们将看到一些可能已经在你脑海中的常见问题。问题的答案可以在附录A中找到：
- en: I understand that the predictive accuracy of sentiment analysis using LSTM is
    still reasonable. However, it does not perform well on the Sentiment labeled dataset.
    Did our network overfit? Is there any way to observe how the training went?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我理解使用LSTM进行情感分析的预测准确度仍然是合理的。然而，它在情感标注数据集上的表现并不理想。我们的网络是否出现了过拟合？有没有办法观察训练过程？
- en: Considering a huge number of review texts, can we perform the training on the
    GPU?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到大量的评论文本，我们可以在GPU上进行训练吗？
- en: In relation to question 2, can we even undertake the whole process using Spark?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于问题2，我们是否可以完全使用Spark来执行整个过程？
- en: Where can I get more training datasets for sentiment analysis?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我在哪里可以获取更多的情感分析训练数据集？
- en: Instead of downloading the training data in `.zip` format manually, can we use
    the `extractTarGz()` method?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们是否可以使用`extractTarGz()`方法，而不是手动以`.zip`格式下载训练数据？
- en: My machine has limited memory. Give me a clue as to how memory management and
    garbage collection work in DL4J.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我的机器内存有限。能否给我一个关于DL4J中内存管理和垃圾回收工作的提示？
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have seen how to implement and deploy a hands-on deep learning
    project that classifies review texts as either positive or negative based on the
    words they contain. We have used a large-scale movie review dataset that contains
    50,000 reviews (training plus testing). A combined approach using Word2Vec (that
    is, a widely used word embedding technique in NLP) and the LSTM network for modeling
    was applied: the pre-trained Google news vector model was used as the neural word
    embeddings.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到如何实现并部署一个实际的深度学习项目，该项目基于评论文本的内容将其分类为正面或负面。我们使用了一个包含50,000条评论（训练和测试）的大规模电影评论数据集。应用了结合Word2Vec（即在NLP中广泛使用的词嵌入技术）和LSTM网络的建模方法：使用了预训练的Google新闻向量模型作为神经网络词嵌入。
- en: Then, the training vectors, along with the labels, were fed into the LSTM network,
    which successfully classified them as negative or positive sentiments. Then, it
    evaluated the trained model on the test set. Additionally, we have also seen how
    to apply text-based preprocessing techniques such as tokenizer, stop words removal
    and TF-IDF, as well as word-embedding operations in DL4J.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将训练向量与标签一起输入LSTM网络，该网络成功地将它们分类为负面或正面情感。接着，它在测试集上评估了训练好的模型。此外，我们还看到了如何在DL4J中应用基于文本的预处理技术，如分词器、停用词去除和TF-IDF，以及词嵌入操作。
- en: In the next chapter, we will see a complete example of how to develop a deep
    learning project to classify images using the DL4J transfer learning API. Through
    this application, users will be able to modify the architecture of an existing
    model, fine-tune learning configurations of an existing model, and hold parameters
    of a specified layer constantly during training, which is also referred to as
    frozen.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到一个完整的示例，展示如何使用DL4J迁移学习API开发一个深度学习项目来分类图像。通过这个应用，用户将能够修改现有模型的架构，微调现有模型的学习配置，并在训练过程中保持指定层的参数不变，这也被称为冻结。
- en: Answers to questions
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题的答案
- en: '**Answer to question 1**: We have seen that our trained model performs pretty
    well on the test set with an accuracy of 87%. Now, if we see the model versus
    iteration score and other parameters from the following graph, then we can see
    that our model was not overfitted:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题1的答案**：我们已经看到我们的训练模型在测试集上的表现相当不错，准确率为87%。现在，如果我们查看模型与迭代分数以及以下图表中的其他参数，我们可以看到我们的模型没有过拟合：'
- en: '![](img/45849f1d-1af8-415e-ba55-2557cb8acf40.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45849f1d-1af8-415e-ba55-2557cb8acf40.png)'
- en: Model versus iteration score and other parameters of the LSTM sentiment analyzer
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM情感分析器的模型与迭代得分及其他参数
- en: Now, for the sentiment labeled sentences, the trained model did not perform
    well. There could be several reasons for that. For example, our model is trained
    with only the movie review dataset, but here, we try to force our model to perform
    on different types of datasets too, for example, Amazon and Yelp. Nevertheless,
    we have not tuned the hyperparameters carefully.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于情感标注的句子，训练好的模型表现不佳。可能有几个原因。比如，我们的模型只用电影评论数据集进行训练，但在这里，我们尝试强迫模型在不同类型的数据集上进行表现，例如Amazon和Yelp。然而，我们没有仔细调整超参数。
- en: '**Answer to question 2**: Yes, in fact, this will be very helpful. For this,
    we have to make sure that our programming environment is ready. In other words,
    first, we have to configure CUDA and cuDNN on our machine.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题2的答案**：是的，实际上，这将非常有帮助。为此，我们必须确保我们的编程环境已经准备好。换句话说，首先，我们必须在机器上配置CUDA和cuDNN。'
- en: 'However, make sure that your machine has a NVIDIA GPU installed and configured
    with sufficient memory and CUDA compute capability. If you do not know how to
    configure such prerequisites, refer to this URL at [https://docs.nvidia.com/deeplearning/sdk/cudnn-install/](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/).
    Once your machine has CUDA/cuDNN installed, in the `pom.xml` file, you have to
    add two entries:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确保你的机器已安装并配置了具有足够内存和CUDA计算能力的NVIDIA GPU。如果你不知道如何配置这些前提条件，请参考此URL：[https://docs.nvidia.com/deeplearning/sdk/cudnn-install/](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/)。一旦你的机器安装了CUDA/cuDNN，在`pom.xml`文件中，你需要添加两个条目：
- en: Backend in the project properties
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目属性中的后端
- en: CUDA as the platform dependency
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA作为平台依赖
- en: 'For step 1, the properties should now look as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第1步，属性现在应如下所示：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, for step 2, add the following dependency in the `pop.xml` file (that is,
    inside the dependencies tag):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于第2步，在`pop.xml`文件中添加以下依赖项（即，在dependencies标签内）：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, update the Maven project, and the required dependencies will be downloaded
    automatically. Now, unless we perform the training on multiple GPUs, we do not
    need to make any changes. However, just run the same script again to perform the
    training. Then, you will experience the following logs on the console:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，更新Maven项目，所需的依赖项将自动下载。现在，除非我们在多个GPU上执行训练，否则不需要进行任何更改。然而，只需再次运行相同的脚本来执行训练。然后，你将在控制台上看到以下日志：
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Nevertheless, in [Chapter 8](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml), *Distributed
    Deep Learning – Video Classification Using Convolutional LSTM Networks*, we will
    see how to make everything faster and scalable overall on multiple GPUs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在[第8章](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml)，*分布式深度学习 - 使用卷积LSTM网络进行视频分类*，我们将看到如何在多个GPU上使一切变得更快并且可扩展。
- en: '**Aswer** **to question 3**: Yes, in fact, this will be very helpful. For this,
    we have to make sure that our programming environment is ready. In other words,
    first, we have to configure Spark on our machine. Once your machine has CUDA/cuDNN
    installed, we just want to configure Spark. In the `pom.xml` file, you have to
    add two entries:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3的答案**：是的，实际上，这将非常有帮助。为此，我们必须确保我们的编程环境已经准备好。换句话说，首先，我们必须在机器上配置Spark。一旦你的机器安装了CUDA/cuDNN，我们只需配置Spark。在`pom.xml`文件中，你需要添加两个条目：'
- en: Backend in the project properties
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目属性中的后端
- en: Spark dependency
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark依赖
- en: 'For step 1, the properties should now look as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第1步，属性现在应如下所示：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, for step 2, add the following dependency in the `pop.xml` file (that is,
    inside the dependencies tag):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于第2步，在`pop.xml`文件中添加以下依赖项（即，在dependencies标签内）：
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then, update the Maven project, and the required dependencies will be downloaded
    automatically. Now, unless we perform the training on multiple GPUs, we do not
    need to make any changes. However, we need to convert the training/testing dataset
    into a Spark-compatible JavaRDD.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，更新Maven项目，所需的依赖项将自动下载。现在，除非我们在多个GPU上执行训练，否则不需要进行任何更改。然而，我们需要将训练/测试数据集转换为Spark兼容的JavaRDD。
- en: 'I have written all of the steps in the `SentimentAnalyzerSparkGPU.java` file
    that can be used to see how the overall steps work. A general warning is that
    if you perform the training on Spark, the DL4J UI will not work because of cross-dependencies
    on the Jackson library. For that, we must first create the `JavaSparkContext`
    using the `sparkSession()` method as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我已在`SentimentAnalyzerSparkGPU.java`文件中编写了所有步骤，可以用于查看整体步骤如何工作。一般警告是，如果你在 Spark
    上执行训练，DL4J UI 将无法正常工作，因为与 Jackson 库存在交叉依赖。为此，我们必须首先通过调用`sparkSession()`方法创建`JavaSparkContext`，如下所示：
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we have to convert the sentiment training dataset iterator to JavaRDD
    of the dataset. First, we create a list of datasets and then add each training
    sample to the list as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将情感训练数据集迭代器转换为 JavaRDD 数据集。首先，我们创建一个数据集列表，然后按如下方式将每个训练样本添加到列表中：
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we create a `JavaSparkContext` by invoking the `sparkSession()` method
    as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过调用`sparkSession()`方法创建`JavaSparkContext`，如下所示：
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, we utilize the `parallelize()` method of Spark to create the JavaRDD
    of the dataset, which can then be used to perform the training using Spark:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们利用 Spark 的`parallelize()`方法创建数据集的 JavaRDD，随后可以用它通过 Spark 执行训练：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, the Spark `TrainingMaster` uses the `ParameterAveragingTrainingMaster`,
    which helps perform the training using Spark. Please refer to [Chapter 8](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml),
    *Distributed Deep Learning – Video Classification Using Convolutional LSTM Networks*,
    for more details:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Spark 的`TrainingMaster`使用`ParameterAveragingTrainingMaster`，它帮助通过 Spark 执行训练。更多细节请参阅[第8章](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml)，*分布式深度学习
    – 使用卷积 LSTM 网络进行视频分类*：
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we create the `SparkDl4jMultiLayer` instead of just the `MultilayerNetwork`
    as we did previously:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们创建`SparkDl4jMultiLayer`，而不是像之前那样仅创建`MultilayerNetwork`：
- en: '[PRE42]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, we create a training listener that records the score of each iteration
    as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个训练监听器，按如下方式记录每次迭代的分数：
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we start the training as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按如下方式开始训练：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'However, using this approach, there is a drawback, that is, we cannot save
    the trained model directly like this but first, we have to fit the network using
    the training data and collect the output as the `MultiLayerNetwork` as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这种方法有一个缺点，即我们不能像这样直接保存训练后的模型，而是必须首先使用训练数据来拟合网络，并将输出收集为`MultiLayerNetwork`，如下所示：
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Answer to question 4**: There are many sources where you can get a sentiment
    analysis dataset. A few of them are listed here:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4 的答案**：你可以从许多来源获得情感分析数据集。以下是其中的一些：'
- en: 'The huge n-grams dataset from Google: [storage.googleapis.com/books/ngrams/books/datasetsv2.html](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 Google 的庞大 n-gram 数据集：[storage.googleapis.com/books/ngrams/books/datasetsv2.html](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)
- en: 'Twitter sentiment: [http://www.sananalytics.com/lab/twitter-sentiment/](http://www.sananalytics.com/lab/twitter-sentiment/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter 情感：[http://www.sananalytics.com/lab/twitter-sentiment/](http://www.sananalytics.com/lab/twitter-sentiment/)
- en: 'UMICH SI650—sentiment classification dataset on Kaggle: [http://inclass.kaggle.com/c/si650winter11/data](http://inclass.kaggle.com/c/si650winter11/data)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UMICH SI650—Kaggle 上的情感分类数据集：[http://inclass.kaggle.com/c/si650winter11/data](http://inclass.kaggle.com/c/si650winter11/data)
- en: 'Multi-domain sentiment dataset: [http://www.cs.jhu.edu/~mdredze/datasets/sentiment/](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多领域情感数据集：[http://www.cs.jhu.edu/~mdredze/datasets/sentiment/](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
- en: '**Answer to question 5**: The answer is no, but with a little effort we can
    make it work. For that, we can use the `ZipArchiveInputStream` and `GzipCompressorInputStream`
    classes from Apache commons as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5 的答案**：答案是否定的，但稍加努力我们可以使其工作。为此，我们可以使用 Apache commons 中的`ZipArchiveInputStream`和`GzipCompressorInputStream`类，代码如下：'
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Answer to question 6**: Well, this is really only a concern if your machine
    doesn''t have enough memory. For this application, I did not face any OOP type
    issue while running this project as my laptop has 32 GB of RAM.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 6 的答案**：嗯，这个问题只有在你的机器内存不足时才需要关注。对于这个应用程序，当我在我的 32 GB 内存的笔记本上运行该项目时，我并未遇到任何面向对象的类型问题。'
- en: 'Apart from this step, we can also choose DL4J garbage collection, especially
    because memory is constrained on your end. DL4J provides a method called `getMemoryManager()`
    that returns a backend-specific `MemoryManager` implementation for low-level memory
    management. Additionally, we have to enable the periodic `System.gc()` calls with
    the windowsills minimal time in milliseconds between calls. Let''s see an example:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一步，我们还可以选择使用DL4J的垃圾回收，尤其是因为你的端口内存受限。DL4J提供了一种名为`getMemoryManager()`的方法，它返回一个特定于后端的`MemoryManager`实现，用于低级内存管理。此外，我们还必须启用周期性的`System.gc()`调用，并设置调用之间的最小时间（以毫秒为单位）。让我们来看一个例子：
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: However, simply set `windowMillis` to `0` to disable this option.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只需将`windowMillis`设置为`0`，即可禁用此选项。
