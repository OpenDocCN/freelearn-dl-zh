- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Quantization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: In this chapter, we’ll dive into **quantization** methods that can optimize
    LLMs for deployment on resource-constrained devices, such as mobile phones, embedded
    systems, or edge computing environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨**量化**方法，这些方法可以优化LLM以在资源受限的设备上部署，例如移动电话、嵌入式系统或边缘计算环境。
- en: Quantization is a technique that reduces the precision of numerical representations,
    thus shrinking the model’s size and improving its inference speed without heavily
    compromising its performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种降低数值表示精度的技术，从而缩小模型的大小并提高其推理速度，而不会严重损害其性能。
- en: 'Quantization is particularly beneficial in the following scenarios:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 量化在以下场景中特别有益：
- en: '**Resource-constrained deployment**: When deploying models on devices with
    limited memory, storage, or computational power, such as mobile phones, IoT devices,
    or edge computing platforms'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源受限的部署**：当在内存、存储或计算能力有限的设备上部署模型时，例如移动电话、物联网设备或边缘计算平台'
- en: '**Latency-sensitive applications**: When real-time or near-real-time responses
    are required, quantization can significantly reduce inference time'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对延迟敏感的应用**：当需要实时或近实时响应时，量化可以显著减少推理时间'
- en: '**Large-scale deployment**: When deploying models at scale, even modest reductions
    in model size and inference time can translate to substantial cost savings in
    infrastructure and energy consumption'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大规模部署**：当大规模部署模型时，即使模型大小和推理时间的适度减少也可以转化为基础设施和能源消耗的显著成本节约'
- en: '**Bandwidth-limited scenarios**: When models need to be downloaded to devices
    over limited bandwidth connections, smaller quantized models reduce transmission
    time and data usage'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带宽受限的场景**：当模型需要通过有限的带宽连接下载到设备时，较小的量化模型可以减少传输时间和数据使用量'
- en: '**Models with redundant precision**: When many LLMs are trained with higher
    precision than necessary for good performance, they become excellent candidates
    for quantization.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有冗余精度的模型**：当许多LLM被训练以比良好性能所需的精度更高的精度时，它们成为量化极佳的候选者。'
- en: 'However, quantization may not be suitable in some cases, such as the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，量化可能不适用，例如以下情况：
- en: '**Highly precision-sensitive tasks**: For applications where even minor degradation
    in accuracy is unacceptable, such as certain medical diagnostics or critical financial
    models'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对精度高度敏感的任务**：对于即使精度略有下降也不可接受的应用，例如某些医疗诊断或关键金融模型'
- en: '**Models already optimized for low precision**: If a model was specifically
    designed or trained to operate efficiently at lower precisions, further quantization
    may cause significant performance drops'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已针对低精度优化的模型**：如果一个模型被专门设计或训练以在较低精度下高效运行，进一步的量化可能会导致性能显著下降'
- en: '**Small models**: For already compact models, the overhead of quantization
    operations might outweigh the benefits in some hardware configurations'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小型模型**：对于已经紧凑的模型，量化操作的开销在某些硬件配置中可能超过其带来的好处'
- en: '**Development and fine-tuning phases**: During active development and experimentation,
    working with full-precision models is often preferable for maximum flexibility
    and to avoid masking potential issues'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发和微调阶段**：在积极开发和实验期间，使用全精度模型通常更可取，以获得最大灵活性并避免掩盖潜在问题'
- en: '**Hardware incompatibility**: Target hardware may lack efficient support for
    the specific quantized formats you’re planning to use (e.g., some devices may
    not have optimized INT8 or INT4 computation capabilities)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件兼容性**：目标硬件可能缺乏对您计划使用的特定量化格式的有效支持（例如，某些设备可能没有优化INT8或INT4计算能力）'
- en: '**Complex architectures with varying sensitivity**: Some parts of an LLM architecture
    (such as attention mechanisms) may be more sensitive to quantization than others,
    requiring more sophisticated mixed-precision approaches rather than naive quantization'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有不同敏感性的复杂架构**：LLM架构的某些部分（例如注意力机制）可能比其他部分对量化更敏感，需要更复杂的混合精度方法，而不是简单的量化'
- en: By understanding these considerations, you can make informed decisions about
    whether and how to apply quantization techniques to your LLM deployments, balancing
    performance requirements against resource constraints.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些考虑因素，您可以就是否以及如何将量化技术应用于您的LLM部署做出明智的决定，在性能需求和资源限制之间取得平衡。
- en: In this chapter, you will learn about different quantization strategies, and
    by the end of this chapter, you’ll be able to apply quantization methods to make
    your LLMs more efficient, while ensuring that any reduction in precision has minimal
    impact on the model’s performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解不同的量化策略，到本章结束时，你将能够应用量化方法使你的 LLM 更高效，同时确保任何精度降低对模型性能的影响最小。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the basics
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基础知识
- en: Mixed-precision quantization
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合精度量化
- en: Hardware-specific considerations
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件特定考虑
- en: Comparing quantization strategies
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较量化策略
- en: Combining quantization with other optimization techniques
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将量化与其他优化技术相结合
- en: Understanding the basics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基础知识
- en: Quantization refers to reducing the precision of the weights and activations
    of a model, typically from **32-bit floating point** (**FP32**) to lower precision
    formats such as **16-bit** (**FP16**) or even **8-bit integers** (**INT8**). The
    goal is to decrease memory usage, speed up computation, and make the model more
    deployable on hardware with limited computational capacity. While quantization
    can lead to performance degradation, carefully tuned quantization schemes usually
    result in only minor losses in accuracy, especially for LLMs with robust architectures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是指降低模型权重和激活的精度，通常从 **32 位浮点**（**FP32**）降低到更低的精度格式，如 **16 位**（**FP16**）或甚至
    **8 位整数**（**INT8**）。目标是减少内存使用，加快计算速度，并使模型在计算能力有限的硬件上更容易部署。虽然量化可能导致性能下降，但精心调整的量化方案通常只会导致精度损失很小，特别是对于具有稳健架构的
    LLM。
- en: 'There are two primary quantization methods: **dynamic quantization** and **static
    quantization**.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要的量化方法：**动态量化**和**静态量化**。
- en: '`torch.quantization.quantize_dynamic` to dynamically quantize the linear layers
    of a pre-trained LLM:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `torch.quantization.quantize_dynamic` 对预训练的 LLM 的线性层进行动态量化：
- en: '[PRE0]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This immediately reduces memory requirements and increases inference speed.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这立即降低了内存需求并提高了推理速度。
- en: '`torch.quantization.prepare` and `torch.quantization.convert`:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.quantization.prepare` 和 `torch.quantization.convert`:'
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This statically quantized model uses fixed scale and zero-point parameters for
    each quantized tensor, allowing hardware accelerators to achieve higher inference
    efficiency.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此静态量化模型使用每个量化张量固定的缩放和零点参数，允许硬件加速器实现更高的推理效率。
- en: In contrast to dynamic quantization, static quantization requires a calibration
    phase with representative data before inference. During this phase, the model
    is run in evaluation mode to collect activation statistics, which are then used
    to compute quantization parameters. The weights and activations are then quantized
    ahead of time and remain fixed during inference, allowing for faster execution
    and more predictable performance.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与动态量化不同，静态量化在推理之前需要一个具有代表性数据的校准阶段。在这个阶段，模型以评估模式运行以收集激活统计信息，然后使用这些统计信息来计算量化参数。权重和激活量在推理前进行量化并保持固定，从而实现更快的执行和更可预测的性能。
- en: 'There are also two main quantization approaches based on when quantization
    is applied:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据何时应用量化，也存在两种主要的量化方法：
- en: '**Post-training quantization (PTQ)**: Applies quantization after the model
    has been fully trained, with minimal or no additional training. Can be implemented
    as either static (with calibration) or dynamic.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后训练量化（PTQ）**：在模型完全训练后应用量化，最小化或无需额外训练。可以实施为静态（带有校准）或动态。'
- en: '**Quantization-aware training (QAT)**: Simulates quantization effects during
    training by adding fake quantization operations in the forward pass while keeping
    gradients in full precision. Typically results in static quantization for deployment.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化感知训练（QAT）**：通过在正向传递中添加模拟量化操作的假量化操作，在训练期间模拟量化效果，同时保持梯度以全精度。通常导致部署时的静态量化。'
- en: PTQ
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PTQ
- en: PTQ is the most straightforward form of quantization and is applied after a
    model has been fully trained. It doesn’t require model retraining and works by
    converting the high-precision weights and activations into lower-precision formats,
    typically INT8\. PTQ is ideal for models where retraining is expensive or impractical,
    and it works best for tasks that are not overly sensitive to precision loss.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ 是量化最直接的形式，在模型完全训练后应用。它不需要模型重新训练，通过将高精度权重和激活转换为低精度格式（通常是 INT8）来实现。PTQ 对于重新训练昂贵或不切实际的情况非常理想，并且对于对精度损失不太敏感的任务效果最佳。
- en: Keep in mind that some PTQ methods often require a calibration step on a representative
    dataset to determine optimal quantization parameters such as scaling factors and
    zero points, capture activation distributions during inference, and minimize the
    error between original and quantized model outputs. This calibration process helps
    the quantization algorithm understand the numerical range and distribution of
    weights and activations across the network, allowing more accurate mapping from
    higher precision formats (such as FP32) to lower precision formats (such as INT8
    or INT4), ultimately preserving model accuracy while reducing memory footprint
    and computational requirements for deployment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，一些PTQ方法通常需要在代表性数据集上执行校准步骤，以确定最佳量化参数，如缩放因子和零点，捕获推理期间的激活分布，并最小化原始模型输出和量化模型输出之间的误差。这个过程有助于量化算法理解网络中权重和激活的数值范围和分布，从而实现从更高精度格式（如FP32）到更低精度格式（如INT8或INT4）的更准确映射，最终在减少内存占用和部署的计算需求的同时保持模型精度。
- en: 'This example demonstrates static PTQ:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了静态PTQ：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model is first put in evaluation mode using `.eval()`, then prepared for
    quantization using the `.prepare()` method, and finally converted into a quantized
    model. This method provides an efficient means of deploying LLMs on low-power
    devices with minimal overhead.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 模型首先使用`.eval()`方法置于评估模式，然后使用`.prepare()`方法准备量化，最后转换为量化模型。这种方法为在低功耗设备上高效部署LLM提供了一种有效手段。
- en: QAT
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QAT
- en: QAT goes beyond simple PTQ by incorporating the effects of quantization into
    the training process itself. This allows the model to learn how to compensate
    for the quantization-induced noise, often resulting in better performance than
    PTQ, particularly for more complex tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: QAT通过将量化效果纳入训练过程本身，超越了简单的PTQ。这允许模型学习如何补偿量化引起的噪声，通常比PTQ有更好的性能，尤其是在更复杂的任务中。
- en: During QAT, both weights and activations are simulated at lower precision during
    training but are kept at higher precision for gradient calculations. This method
    is particularly useful when the application requires high performance with aggressive
    quantization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化加速训练（QAT）期间，训练过程中权重和激活都使用较低的精度进行模拟，但在梯度计算时保持较高的精度。这种方法在应用需要高性能且量化程度较大的情况下特别有用。
- en: 'In the following example, we configure the model for QAT using `get_default_qat_qconfig()`,
    which simulates the quantized behavior during the training phase:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用`get_default_qat_qconfig()`配置模型进行QAT，该配置在训练阶段模拟量化行为：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the model has been trained, it is converted to a quantized version suitable
    for deployment. QAT typically results in better model accuracy compared to PTQ,
    particularly for more complex or critical applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，将其转换为适合部署的量化版本。与PTQ相比，QAT通常能带来更好的模型精度，尤其是在更复杂或关键的应用中。
- en: Mixed-precision quantization
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合精度量化
- en: '**Mixed-precision quantization** is a more flexible approach that leverages
    multiple levels of numerical precision within a single model. For instance, less
    critical layers of the model can use INT8, while more sensitive layers remain
    in FP16 or FP32\. This allows greater control over the trade-off between performance
    and precision. Using mixed-precision quantization can significantly reduce model
    size and inference time while keeping critical aspects of the LLM intact.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合精度量化**是一种更灵活的方法，它利用单个模型内的多个数值精度级别。例如，模型中不那么关键的层可以使用INT8，而更敏感的层则保持在FP16或FP32。这允许在性能和精度之间有更大的控制权。使用混合精度量化可以显著减小模型大小和推理时间，同时保持LLM的关键特性。'
- en: 'The following code demonstrates an example of quantization to optimize memory
    usage and speed in LLM training or inference:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了量化示例，以优化LLM训练或推理中的内存使用和速度：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this example, we use the `autocast()` function from PyTorch’s **Automatic
    Mixed Precision** (**AMP**) library to enable FP16 computation in parts of the
    model where precision is less critical, while FP32 is retained for more sensitive
    layers. This method helps reduce memory usage and inference time without severely
    affecting performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们使用PyTorch的**自动混合精度**（**AMP**）库中的`autocast()`函数，在模型中精度不那么关键的部位启用FP16计算，而保留FP32用于更敏感的层。这种方法有助于减少内存使用和推理时间，同时不会严重影响性能。
- en: Hardware-specific considerations
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件特定考虑因素
- en: Different hardware platforms—such as GPUs, CPUs, or specialized accelerators
    such as TPUs—can have vastly different capabilities and performance characteristics
    when it comes to handling quantized models. For instance, some hardware may natively
    support INT8 operations, while others are optimized for FP16.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的硬件平台——例如GPU、CPU或专门的加速器如TPU——在处理量化模型时可能具有截然不同的能力和性能特征。例如，某些硬件可能原生支持INT8操作，而其他硬件则针对FP16进行了优化。
- en: Understanding the target deployment hardware is crucial for selecting the right
    quantization technique. For example, NVIDIA GPUs are well-suited to FP16 computations
    due to their support for mixed-precision training and inference, while CPUs often
    perform better with INT8 quantization because of hardware-accelerated integer
    operations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 理解目标部署硬件对于选择合适的量化技术至关重要。例如，NVIDIA GPU因其支持混合精度训练和推理而非常适合FP16计算，而CPU通常因为硬件加速的整数操作而在INT8量化方面表现更佳。
- en: When deploying LLMs in production, it is important to experiment with quantization
    strategies tailored to your specific hardware and ensure that your model leverages
    the strengths of the platform.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当在生产中部署LLM时，重要的是要尝试针对特定硬件量身定制的量化策略，并确保您的模型利用平台的优势。
- en: Comparing quantization strategies
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较量化策略
- en: When comparing different quantization strategies, each approach offers distinct
    advantages and challenges, which can be measured through factors such as implementation
    complexity, accuracy preservation, performance impact, and resource requirements.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较不同的量化策略时，每种方法都提供了独特的优势和挑战，这些可以通过实现复杂度、精度保持、性能影响和资源需求等因素来衡量。
- en: In terms of implementation complexity, PTQ is the simplest to execute, requiring
    minimal additional work beyond the training of the original model. Dynamic quantization
    is more complex, as it involves more runtime considerations due to the dynamic
    handling of activations. Mixed-precision quantization introduces more complexity
    since it requires a granular, layer-by-layer assessment of precision sensitivity
    and potentially custom kernel development for optimized execution. QAT ranks as
    the most complex, requiring the integration of fake quantization nodes into the
    training graph and extended training times to account for the noise introduced
    by quantization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现复杂度方面，PTQ是最简单的，只需在原始模型训练之外做最少的工作。动态量化更复杂，因为它涉及到更多的运行时考虑，因为需要动态处理激活。混合精度量化引入了更多的复杂性，因为它需要逐层评估精度敏感性，并可能需要为优化执行开发定制的内核。QAT被认为是最复杂的，需要将伪量化节点集成到训练图中，并延长训练时间以考虑量化引入的噪声。
- en: When it comes to accuracy preservation, QAT performs the best, maintaining accuracy
    within a small margin of floating-point performance, especially when targeting
    aggressive quantization (sub-8-bit). Mixed-precision quantization also ranks high
    in accuracy retention since it allows critical layers to remain in higher precision,
    balancing performance and accuracy well. PTQ generally maintains accuracy within
    acceptable limits, though more complex architectures may suffer higher losses
    in precision. Dynamic quantization typically retains better accuracy than PTQ
    in RNN-based models, but struggles in CNN architectures, particularly when activations
    are sensitive to input distribution changes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在精度保持方面，QAT表现最佳，将精度保持在浮点性能的小范围内，尤其是在针对激进的量化（小于8位）时。混合精度量化在精度保持方面也排名很高，因为它允许关键层保持更高的精度，从而在性能和精度之间取得良好的平衡。PTQ通常在可接受的精度范围内保持精度，尽管更复杂的架构可能会遭受更高的精度损失。动态量化通常在基于RNN的模型中比PTQ保持更好的精度，但在CNN架构中表现不佳，尤其是在激活对输入分布变化敏感时。
- en: In terms of resource requirements, PTQ demands the fewest resources, making
    it ideal for fast deployment scenarios with limited computational availability.
    Dynamic quantization ranks slightly higher in resource consumption because it
    handles activation quantization at runtime, though this is offset by the reduced
    burden on memory and storage. Mixed-precision quantization, while requiring more
    resources during implementation due to sensitivity analysis, can be efficient
    during inference, particularly on hardware that supports multiple precisions.
    QAT is the most resource-intensive, as it necessitates additional training time,
    higher memory usage during training, and more compute resources to adapt the model
    to quantization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源需求方面，PTQ（Post-Training Quantization，训练后量化）需要的资源最少，使其成为计算能力有限且需要快速部署的场景的理想选择。动态量化在资源消耗上略高，因为它在运行时处理激活量化，尽管这减少了内存和存储的负担。混合精度量化由于敏感性分析，在实现过程中需要更多资源，但在推理阶段可以更高效，尤其是在支持多种精度的硬件上。QAT（Quantization-Aware
    Training，量化感知训练）是资源消耗最多的，因为它需要额外的训练时间，训练期间的内存使用更高，以及更多的计算资源来适应量化。
- en: From a performance perspective, PTQ offers considerable improvements in memory
    savings and computational speedup, typically reducing storage by 75% and achieving
    2–4x acceleration on compatible hardware. However, QAT, while similar in compression
    ratio, adds overhead during training but compensates by producing models that
    can handle more aggressive quantization without significant performance loss.
    Dynamic quantization provides similar memory savings as PTQ, but its compute acceleration
    is generally lower due to runtime overhead. Mixed-precision quantization can offer
    near-floating-point performance, with speedups dependent on how efficiently the
    hardware can execute models with varying precision levels.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，PTQ在内存节省和计算加速方面提供了显著的改进，通常可以减少75%的存储，并在兼容硬件上实现2–4倍的加速。然而，QAT虽然压缩比相似，但在训练期间增加了开销，但通过产生可以处理更激进量化而不会造成显著性能损失的模式来补偿。动态量化提供了与PTQ相似的内存节省，但由于运行时开销，其计算加速通常较低。混合精度量化可以提供接近浮点性能，其加速取决于硬件执行具有不同精度级别的模型的有效性。
- en: The decision framework for choosing the optimal quantization strategy hinges
    on specific project requirements. PTQ is appropriate when fast deployment is a
    priority, the model architecture is relatively simple, and slight accuracy loss
    is acceptable. QAT is the best choice when accuracy is paramount, retraining resources
    are available, and aggressive quantization is needed. Dynamic quantization fits
    scenarios that require runtime flexibility and the handling of varying input distributions,
    especially in RNN-based architectures. Mixed-precision quantization is optimal
    for complex models with varying precision needs, where both high accuracy and
    performance are required, and where the hardware can efficiently manage multiple
    precision formats.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳量化策略的决策框架取决于具体的项目需求。当快速部署是首要任务，模型架构相对简单，且可以接受轻微的精度损失时，PTQ是合适的。当精度至关重要，有可用的重新训练资源，并且需要激进量化时，QAT是最佳选择。动态量化适合需要运行时灵活性和处理不同输入分布的场景，尤其是在基于RNN的架构中。混合精度量化对于需要不同精度需求的复杂模型是最优的，在这些模型中，需要高精度和性能，并且硬件可以有效地管理多种精度格式。
- en: Each quantization strategy serves a different purpose based on the trade-off
    between accuracy, complexity, performance, and resources, allowing users to tailor
    their approach to the specific needs of their deployment environment.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每种量化策略基于精度、复杂性、性能和资源之间的权衡，服务于不同的目的，使用户能够根据其部署环境的特定需求定制其方法。
- en: '*Table 13.1* compares each strategy.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*表13.1* 对每种策略进行了比较。'
- en: '| **Strategy** | **Accuracy** | **Complexity** | **Performance** | **Resources**
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **策略** | **精度** | **复杂性** | **性能** | **资源** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PTQ | Good for simple models; declines with complexity | Low; minimal setup
    | 75% storage reduction; 2–4x speedup | Low; minimal compute needed |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| PTQ | 适用于简单模型；随着复杂度的增加而下降 | 低；最小设置 | 75%的存储减少；2–4倍的加速 | 低；所需计算最小 |'
- en: '| QAT | Highest; best for sub-8-bit | High; requires extended training | High
    compression with the best accuracy | High; intensive training needs |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| QAT | 最高；适用于小于8位的子集 | 高；需要扩展训练 | 高压缩率，最佳精度 | 高；需要密集训练 |'
- en: '| Dynamic | Good for RNNs; weak for CNNs | Medium; runtime overhead | Good
    memory savings; slower compute | Medium; runtime processing |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 适合RNN；对CNN较弱 | 中；运行时开销 | 良好的内存节省；较慢的计算 | 中；运行时处理'
- en: '| Mixed-Precision | High; flexible precision options | Medium-high; layer-specific
    tuning | Hardware-dependent speedup | Medium-high during setup |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 混合精度 | 高；灵活的精度选项 | 中高；层特定调整 | 硬件依赖的速度提升 | 中高在设置期间'
- en: Table 13.1 – Comparison of quantization strategies
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.1 – 量化策略比较
- en: In practice, some scenarios may benefit from combining strategies. For example,
    you might initially apply PTQ to achieve quick deployment, then use QAT selectively
    on accuracy-sensitive layers. Another approach could involve using mixed-precision
    for specific layers while applying dynamic quantization for activations to balance
    runtime flexibility and performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，某些场景可能从结合策略中受益。例如，您可能最初应用PTQ以实现快速部署，然后有选择地在精度敏感层上使用QAT。另一种方法可能涉及对特定层使用混合精度，同时为激活应用动态量化以平衡运行时灵活性和性能。
- en: Combining quantization with other optimization techniques
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将量化与其他优化技术相结合
- en: Quantization can be combined with other optimization techniques, such as pruning
    and knowledge distillation, to create highly efficient models that are suitable
    for deployment on resource-constrained devices. By leveraging multiple methods,
    you can significantly reduce model size while maintaining or minimally impacting
    performance. This is especially useful when deploying LLMs on edge devices or
    mobile platforms where computational and memory resources are limited.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以与其他优化技术相结合，例如剪枝和知识蒸馏，以创建适用于在资源受限设备上部署的高度高效模型。通过利用多种方法，您可以显著减小模型大小，同时保持或最小化对性能的影响。这在将LLM部署在边缘设备或移动平台上特别有用，因为这些平台计算和内存资源有限。
- en: Pruning and quantization
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝和量化
- en: 'One of the most effective combinations is **pruning** followed by quantization.
    First, pruning removes redundant weights from the model, reducing the number of
    parameters. Quantization then reduces the precision of the remaining weights,
    which further decreases the model size and improves inference speed. Here’s an
    example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的组合之一是**剪枝**后跟量化。首先，剪枝从模型中移除冗余权重，减少参数数量。然后量化降低剩余权重的精度，这进一步减小模型大小并提高推理速度。以下是一个示例：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, pruning is applied to remove 50% of the weights in all linear
    layers, and dynamic quantization reduces the precision of the remaining weights
    to INT8 for further size reduction.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，剪枝应用于移除所有线性层中50%的权重，动态量化将剩余权重的精度降低到INT8以进一步减小尺寸。
- en: The result is a compact, highly optimized model that consumes fewer computational
    resources, making it suitable for deployment on devices with limited hardware
    capabilities.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个紧凑、高度优化的模型，消耗更少的计算资源，使其适合在硬件能力有限的设备上部署。
- en: Knowledge distillation and quantization
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏和量化
- en: Another powerful combination is knowledge distillation followed by quantization.
    In this scenario, a smaller student model is trained to replicate the behavior
    of a larger teacher model. Once the student model is trained, quantization is
    applied to further optimize the student model for deployment. This combination
    is particularly useful when you need to maintain high performance with minimal
    computational overhead.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种强大的组合是先知识蒸馏后量化。在这种情况下，一个较小的学生模型被训练以复制较大教师模型的行为。一旦学生模型训练完成，量化就被应用于进一步优化学生模型以进行部署。这种组合在您需要以最小的计算开销保持高性能时特别有用。
- en: 'Let’s look at an example step by step:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看看一个例子：
- en: 'Define teacher and student models:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义教师和学生模型：
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define a knowledge distillation loss function:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义知识蒸馏损失函数：
- en: '[PRE7]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Add a training loop for knowledge distillation:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为知识蒸馏添加训练循环：
- en: '[PRE8]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Forward pass through the teacher and student models:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过教师和学生模型进行前向传递：
- en: '[PRE9]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The forward pass through both the teacher and student models generates their
    respective output logits for the same input data. This parallel inference step
    is necessary to compute the distillation loss, which quantifies how closely the
    student replicates the teacher’s behavior. By comparing these outputs, the training
    process can guide the student to internalize the teacher’s knowledge without requiring
    the original labels.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过教师模型和学生模型对相同输入数据进行正向传播，生成它们各自的对数输出。这一并行推理步骤是计算蒸馏损失所必需的，该损失量化了学生模型复制教师模型行为的多接近程度。通过比较这些输出，训练过程可以引导学生内部化教师的知识，而无需原始标签。
- en: 'Compute the distillation loss:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算蒸馏损失：
- en: '[PRE10]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Computing the distillation loss allows the student model to learn from the teacher
    by minimizing the discrepancy between their output distributions. This guides
    the student to approximate the behavior of the larger, more accurate teacher model
    while maintaining its own compact structure. By backpropagating this loss and
    updating the model parameters through optimization, the student progressively
    aligns its predictions with the teacher, leading to improved performance with
    reduced model complexity.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算蒸馏损失允许学生模型通过最小化它们输出分布之间的差异来从教师模型中学习。这引导学生近似更大、更准确的教师模型的行为，同时保持其自身的紧凑结构。通过反向传播此损失并通过优化更新模型参数，学生模型逐渐使其预测与教师模型对齐，从而在降低模型复杂性的同时提高性能。
- en: 'Quantize the distilled student model:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 量化蒸馏的学生模型：
- en: '[PRE11]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Check size and efficiency improvements:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查大小和效率改进：
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Knowledge distillation is used to train a smaller student model that mimics
    the behavior of the larger teacher model, and quantization is applied to the student
    model, reducing the precision of its weights to further optimize it for deployment.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识蒸馏用于训练一个较小的学生模型，该模型模仿较大教师模型的行为，并将量化应用于学生模型，降低其权重的精度，以进一步优化其部署。
- en: This method helps maintain performance while drastically reducing the model’s
    size, making it well-suited for low-power or real-time applications.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法有助于在大幅减少模型尺寸的同时保持性能，使其非常适合低功耗或实时应用。
- en: By combining quantization with pruning and knowledge distillation, you can achieve
    highly optimized models that balance size, efficiency, and performance. These
    models are especially useful for deployment on edge devices or environments with
    stringent resource constraints.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将量化与剪枝和知识蒸馏相结合，你可以实现高度优化的模型，这些模型在大小、效率和性能之间取得平衡。这些模型特别适用于部署在边缘设备或资源受限的环境中。
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored different quantization techniques for optimizing
    LLMs, including PTQ, QAT, and mixed-precision quantization. We also covered hardware-specific
    considerations and methods for evaluating quantized models. By combining quantization
    with other optimization methods, such as pruning or knowledge distillation, LLMs
    can be made both efficient and powerful for real-world applications.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了优化LLMs的不同量化技术，包括PTQ、QAT和混合精度量化。我们还涵盖了针对特定硬件的考虑因素和评估量化模型的方法。通过将量化与其他优化方法相结合，如剪枝或知识蒸馏，LLMs可以变得既高效又强大，适用于现实世界的应用。
- en: In the next chapter, we will delve into the process of evaluating LLMs, focusing
    on metrics for text generation, language understanding, and dialogue systems.
    Understanding these evaluation methods is key to ensuring your optimized models
    perform as expected across diverse tasks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨评估LLMs的过程，重点关注文本生成、语言理解和对话系统的指标。理解这些评估方法是确保你的优化模型在多样化的任务中按预期表现的关键。
- en: 'Part 3: Evaluation and Interpretation of Large Language Models'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：大型语言模型的评估和解释
- en: In this part, we focus on methods for evaluating and interpreting LLMs to ensure
    that they meet performance expectations and align with the intended use cases.
    You will learn how to use evaluation metrics tailored to various NLP tasks and
    apply cross-validation techniques to reliably assess your models. We explore interpretability
    methods that allow you to understand the inner workings of LLMs, as well as techniques
    for identifying and addressing biases in their outputs. Adversarial robustness
    is another key area covered, helping you defend models against attacks. Additionally,
    we introduce Reinforcement Learning from Human Feedback (RLHF) as a powerful method
    for aligning LLMs with user preferences. By mastering these evaluation and interpretation
    techniques, you will be able to fine-tune your models to achieve transparency,
    fairness, and reliability.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，我们专注于评估和解释LLMs的方法，以确保它们满足性能预期并与预期用例保持一致。您将学习如何使用针对各种NLP任务的评估指标，并应用交叉验证技术来可靠地评估您的模型。我们探讨了允许您理解LLMs内部工作原理的解释方法，以及识别和解决其输出中偏差的技术。对抗鲁棒性是另一个关键领域，有助于您防御模型受到的攻击。此外，我们介绍了从人类反馈中进行强化学习（RLHF）作为一种将LLMs与用户偏好对齐的强大方法。通过掌握这些评估和解释技术，您将能够微调您的模型以实现透明度、公平性和可靠性。
- en: 'This part has the following chapters:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 14*](B31249_14.xhtml#_idTextAnchor230), *Evaluation Metrics*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第14章*](B31249_14.xhtml#_idTextAnchor230), *评估指标*'
- en: '[*Chapter 15*](B31249_15.xhtml#_idTextAnchor247), *Cross-Validation*'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第15章*](B31249_15.xhtml#_idTextAnchor247), *交叉验证*'
- en: '[*Chapter 16*](B31249_16.xhtml#_idTextAnchor265), *Interpretability*'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第16章*](B31249_16.xhtml#_idTextAnchor265), *可解释性*'
- en: '[*Chapter 17*](B31249_17.xhtml#_idTextAnchor276), *Fairness and Bias Detection*'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第17章*](B31249_17.xhtml#_idTextAnchor276), *公平性与偏差检测*'
- en: '[*Chapter 18*](B31249_18.xhtml#_idTextAnchor286), *Adversarial Robustness*'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第18章*](B31249_18.xhtml#_idTextAnchor286), *对抗鲁棒性*'
- en: '[*Chapter 19*](B31249_19.xhtml#_idTextAnchor295), *Reinforcement Learning from
    Human Feedback*'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第19章*](B31249_19.xhtml#_idTextAnchor295), *从人类反馈中进行强化学习*'
