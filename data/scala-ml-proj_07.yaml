- en: Options Trading Using Q-learning and Scala Play Framework
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Q学习和Scala Play框架进行期权交易
- en: As human beings, we learn from experiences. We have not become so charming by
    accident. Years of positive compliments as well as negative criticism, have all
    helped shape us into who we are today. We learn how to ride a bike by trying out
    different muscle movements until it just clicks. When you perform actions, you
    are sometimes rewarded immediately. This is all about **Reinforcement learning**
    (**RL**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们通过经验学习。我们并不是偶然变得如此迷人。多年的正面夸奖和负面批评，塑造了今天的我们。我们通过尝试不同的肌肉动作来学习骑自行车，直到掌握为止。当你执行某些动作时，往往会立即获得奖励。这就是**强化学习**（**RL**）的核心。
- en: This chapter is all about designing a machine learning system driven by criticisms
    and rewards. We will see how to apply RL algorithms for a predictive model on
    real-life datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于设计一个由批评和奖励驱动的机器学习系统。我们将展示如何将强化学习算法应用于实际数据集的预测模型中。
- en: From the trading point of view, an option is a contract that gives its owner
    the right to buy (call option) or sell (put option) a financial asset (underlying)
    at a fixed price (the strike price) at or before a fixed date (the expiry date).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从交易的角度来看，期权是一种合约，赋予持有者在固定价格（行权价）下，在固定日期（到期日）或之前买入（看涨期权）或卖出（看跌期权）金融资产（标的资产）的权利。
- en: We will see how to develop a real-life application for such options trading
    usingan RL algorithm called **QLearning**. To be more precise, we will solve the
    problem of computing the best strategy in options trading, and we want to trade
    certain types of options given some market conditions and trading data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何利用强化学习算法（称为**QLearning**）为期权交易开发一个实际应用。更具体地说，我们将解决计算期权交易中最佳策略的问题，并希望在某些市场条件和交易数据下进行特定类型的期权交易。
- en: The IBM stock datasets will be used to design a machine learning system driven
    by criticisms and rewards. We will start from RL and its theoretical background
    so that the concept is easier to grasp. Finally, we will wrap up the whole application
    as a web app using Scala Play Framework.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用IBM股票数据集来设计一个由批评和奖励驱动的机器学习系统。我们将从强化学习及其理论背景开始，以便更容易理解这个概念。最后，我们将通过使用Scala
    Play框架将整个应用程序封装为一个Web应用。
- en: 'Concisely, we will learn the following topics throughout this end-to-end project:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，在这个从头到尾的项目中，我们将学习以下内容：
- en: Using Q-learning—an RL algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Q学习——一种强化学习算法
- en: Options trading—what is it all about?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期权交易——它到底是怎么回事？
- en: Overview of technologies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术概述
- en: Implementing Q-learning for options trading
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为期权交易实现Q学习
- en: Wrapping up the application as a web app using Scala Play Framework
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scala Play框架将应用程序封装为Web应用
- en: Model deployment
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署
- en: Reinforcement versus supervised and unsupervised learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与监督学习和无监督学习的比较
- en: Whereas supervised and unsupervised learning appear at opposite ends of the
    spectrum, RL exists somewhere in the middle. It is not supervised learning because
    the training data comes from the algorithm deciding between exploration and exploitation.
    In addition, it is not unsupervised because the algorithm receives feedback from
    the environment. As long as you are in a situation where performing an action
    in a state produces a reward, you can use RL to discover a good sequence of actions
    to take the maximum expected rewards.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然监督学习和无监督学习处于光谱的两端，但强化学习位于其中间。它不是监督学习，因为训练数据来自算法在探索和利用之间的选择。此外，它也不是无监督学习，因为算法会从环境中获得反馈。只要你处于一个执行某个动作能带来奖励的状态，你就可以使用强化学习来发现一个能够获得最大预期奖励的良好动作序列。
- en: The goal of an RL agent will be to maximize the total reward that it receives
    in the end. The third main subelement is the `value` function. While rewards determine
    an immediate desirability of the states, values indicate the long-term desirability
    of states, taking into account the states that may follow and the available rewards
    in these states. The `value` function is specified with respect to the chosen
    policy. During the learning phase, an agent tries actions that determine the states
    with the highest value, because these actions will get the best number of rewards
    in the end.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）智能体的目标是最大化最终获得的总奖励。第三个主要子元素是`价值`函数。奖励决定了状态的即时吸引力，而价值则表示状态的长期吸引力，考虑到可能跟随的状态以及这些状态中的可用奖励。`价值`函数是根据所选择的策略来定义的。在学习阶段，智能体尝试那些能确定最高价值状态的动作，因为这些动作最终将获得最佳的奖励数量。
- en: Using RL
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强化学习（RL）
- en: '*Figure 1* shows a person making decisions to arrive at their destination.
    Moreover, suppose that on your drive from home to work, you always choose the
    same route. However, one day your curiosity takes over and you decide to try a
    different path, hoping for a shorter commute. This dilemma of trying out new routes
    or sticking to the best-known route is an example of exploration versus exploitation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1* 显示了一个人做出决策以到达目的地。此外，假设你从家到公司时，总是选择相同的路线。然而，有一天你的好奇心占了上风，你决定尝试一条不同的道路，希望能更快到达。这个尝试新路线与坚持最熟悉路线的困境，就是探索与利用之间的例子：'
- en: '![](img/e85f2dfa-b4f1-4f3c-bf90-bc66c9d4f1ae.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e85f2dfa-b4f1-4f3c-bf90-bc66c9d4f1ae.png)'
- en: 'Figure 1: An agent always tries to reach the destination by passing through
    the route'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个代理始终尝试通过特定路线到达目的地
- en: RL techniques are being used in many areas. A general idea that is being pursued
    right now is creating an algorithm that does not need anything apart from a description
    of its task. When this kind of performance is achieved, it will be applied virtually
    everywhere.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习技术正在许多领域中被应用。目前正在追求的一个普遍理念是创建一个算法，它只需要任务的描述而不需要其他任何东西。当这种性能实现时，它将被几乎应用于所有领域。
- en: Notation, policy, and utility in RL
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的符号、策略和效用
- en: 'You may notice that RL jargon involves incarnating the algorithm into taking
    actions in situations to receive rewards. In fact, the algorithm is often referred
    to as an agent that acts with the environment. You can just think of it is an
    intelligent hardware agent that is sensing with sensors and interacting with the
    environment using its actuators. Therefore, it should not be a surprise that much
    of RL theory is applied in robotics. Now, to extend our discussion further, we
    need to know a few terminologies:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，强化学习的术语涉及将算法化身为在情境中采取动作以获得奖励。事实上，算法通常被称为与环境互动的代理。你可以将它看作一个智能硬件代理，使用传感器感知环境，并利用执行器与环境互动。因此，强化学习理论在机器人学中的广泛应用并不令人惊讶。现在，为了进一步展开讨论，我们需要了解一些术语：
- en: '**Environment**: An environment is any system having states and mechanisms
    to transition between different states. For example, the environment for a robot
    is the landscape or facility it operates in.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：环境是任何拥有状态以及在不同状态之间转换机制的系统。例如，一个机器人的环境就是它所操作的景观或设施。'
- en: '**Agent**: An agent is an automated system that interacts with the environment.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：代理是一个与环境互动的自动化系统。'
- en: '**State**: The state of the environment or system is the set of variables or
    features that fully describe the environment.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：环境或系统的状态是完全描述环境的变量或特征的集合。'
- en: '**Goal**: A goal is a state that provides a higher discounted cumulative reward
    than any other state. A high cumulative reward prevents the best policy from being
    dependent on the initial state during training.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：目标是一个状态，它提供比任何其他状态更高的折扣累计奖励。高累计奖励能够防止最佳策略在训练过程中依赖于初始状态。'
- en: '**Action**: An action defines the transition between states, where an agent
    is responsible for performing, or at least recommending, an action. Upon execution
    of an action, the agent collects a reward (or punishment) from the environment.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：动作定义了状态之间的转变，代理负责执行或至少推荐某个动作。在执行动作后，代理会从环境中收获奖励（或惩罚）。'
- en: '**Policy**: The policy defines the action to be performed and executed for
    any state of the environment.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：策略定义了在环境的任何状态下需要执行的动作。'
- en: '**Reward**: A reward quantifies the positive or negative interaction of the
    agent with the environment. Rewards are essentially the training set for the learning
    engine.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：奖励量化了代理与环境之间的正向或负向互动。奖励本质上是学习引擎的训练集。'
- en: '**Episode *(***also known as **trials**): This defines the number of steps
    necessary to reach the goal state from an initial state.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回合** *(***也称为 **试验**)*：这定义了从初始状态到达目标状态所需的步骤数量。'
- en: 'We will discuss more on policy and utility later in this section. *Figure 2*
    demonstrates the interplay between **states**, **actions**, and **rewards**. If
    you start at state **s[1]**, you can perform action **a[1]** to obtain a reward
    **r (s[1], a[1])**. Arrows represent **actions**, and **states** are represented
    by circles:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节稍后讨论更多关于策略和效用的内容。*图2* 展示了**状态**、**动作**和**奖励**之间的相互作用。如果你从状态 **s[1]** 开始，你可以执行动作
    **a[1]** 来获得奖励 **r (s[1], a[1])**。箭头代表**动作**，**状态**由圆圈表示：
- en: '![](img/4e880ac5-2e4b-44a2-9c91-38786d5942b0.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e880ac5-2e4b-44a2-9c91-38786d5942b0.png)'
- en: 'Figure 2: An agent performing an action on a state produces a reward'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：代理在一个状态下执行一个动作会产生回报
- en: A robot performs actions to change between different states. But how does it
    decide which action to take? Well, it is all about using a different or concrete
    policy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人执行动作以在不同状态之间变化。但它如何决定采取哪种动作呢？嗯，这一切都与使用不同的或具体的策略有关。
- en: Policy
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: 'In RL lingo, we call a strategy **policy**. The goal of RL is to discover a
    good strategy. One of the most common ways to solve it is by observing the long-term
    consequences of actions in each state. The short-term consequence is easy to calculate:
    it''s just the reward. Although performing an action yields an immediate reward,
    it is not always a good idea to greedily choose the action with the best reward.
    That is a lesson in life too, because the most immediate best thing to do may
    not always be the most satisfying in the long run. The best possible policy is
    called the optimal policy, and it is often the holy grail of RL, as shown in *Figure
    3*, which shows the optimal action, given any state:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL术语中，我们称一个策略为**策略**。RL的目标是发现一个好的策略。解决RL问题的最常见方法之一是通过观察在每个状态下采取动作的长期后果。短期后果很容易计算：它就是回报。尽管执行某个动作会产生即时回报，但贪婪地选择回报最好的动作并不总是一个好主意。这也是生活中的一课，因为最直接的最佳选择可能不会在长远看来是最令人满足的。最好的策略被称为最优策略，它通常是RL中的“圣杯”，如*图
    3*所示，展示了在给定任何状态下的最优动作：
- en: '![](img/bca50b75-f45d-4aa0-ab22-714d10131b3a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bca50b75-f45d-4aa0-ab22-714d10131b3a.png)'
- en: 'Figure 3: A policy defines an action to be taken in a given state'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：策略定义了在给定状态下要采取的动作
- en: We have seen one type of policy where the agent always chooses the action with
    the greatest immediate reward, called **greedy policy**. Another simple example
    of a policy is arbitrarily choosing an action, called **random policy**. If you
    come up with a policy to solve a, RL problem, it is often a good idea to double-check
    that your learned policy performs better than both the random and the greedy policies.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到过一种类型的策略，其中代理始终选择具有最大即时回报的动作，称为**贪心策略**。另一个简单的策略例子是随意选择一个动作，称为**随机策略**。如果你想出一个策略来解决一个RL问题，通常一个好主意是重新检查你的学习策略是否优于随机策略和贪心策略。
- en: In addition, we will see how to develop another robust policy called **policy
    gradients**, where a neural network learns a policy for picking actions by adjusting
    its weights through gradient descent using feedback from the environment. We will
    see that, although both the approaches are used, policy gradient is more direct
    and optimistic.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将看到如何开发另一种强健的策略，称为**策略梯度**，在这种策略中，神经网络通过使用来自环境的反馈调整其权重，通过梯度下降学习选择动作的策略。我们将看到，尽管两种方法都被使用，策略梯度更为直接且充满乐观。
- en: Utility
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 效用
- en: 'The long-term reward is called a **utility**. It turns out that if we know
    the utility of performing an action upon a state, then it is easy to solve RL.
    For example, to decide which action to take, we simply select the action that
    produces the highest utility. However, uncovering these utility values is difficult.
    The utility of performing an action *a* at a state *s* is written as a function, *Q(s,
    a)*, called the **utility function**. This predicts the expected immediate reward,
    and rewards following an optimal policy given the state-action input, as shown
    in *Figure 4*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 长期回报被称为**效用**。事实证明，如果我们知道在某一状态下执行某个动作的效用，那么解决强化学习（RL）就变得容易。例如，为了决定采取哪种动作，我们只需选择产生最高效用的动作。然而，揭示这些效用值是困难的。在状态*s*下执行动作*a*的效用被写为一个函数，*Q(s,
    a)*，称为**效用函数**。它预测期望的即时回报，以及根据最优策略执行后续回报，如*图 4*所示：
- en: '![](img/86800cd6-c7ec-4a55-9148-2d4afb66e6b0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86800cd6-c7ec-4a55-9148-2d4afb66e6b0.png)'
- en: 'Figure 4: Using a utility function'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用效用函数
- en: 'Most RL algorithms boil down to just three main steps: infer, do, and learn.
    During the first step, the algorithm selects the best action (*a*) given a state
    (*s*) using the knowledge it has so far. Next, it perform the action to find out
    the reward (*r*) as well as the next state (*s''*). Then it improves its understanding
    of the world using the newly acquired knowledge *(s, r, a, s'')*. However,  as
    I think you will agree, this is just a naive way to calculate the utility.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RL算法归结为三个主要步骤：推断、执行和学习。在第一步中，算法根据当前所掌握的知识选择给定状态*s*下的最佳动作*a*。接下来，执行该动作以获得回报*r*以及下一个状态*s'*。然后，算法利用新获得的知识*(s,
    r, a, s')*来改进对世界的理解。然而，正如你可能同意的那样，这只是计算效用的一种朴素方法。
- en: 'Now, the question is: what could be a more robust way to compute it? We can
    calculate the utility of a particular state-action pair *(s, a)* by recursively
    considering the utilities of future actions. The utility of your current action
    is influenced not only by the immediate reward but also the next best action,
    as shown in the following formula:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是：有什么更稳健的方法来计算它呢？我们可以通过递归地考虑未来动作的效用来计算某个特定状态-动作对*(s, a)*的效用。当前动作的效用不仅受到即时奖励的影响，还受到下一最佳动作的影响，如下式所示：
- en: '![](img/5215bab5-b9d6-40f5-a5c9-12c83fa6cea8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5215bab5-b9d6-40f5-a5c9-12c83fa6cea8.png)'
- en: '*s''* denotes the next state, and *a''* denotes the next action. The reward
    of taking action *a* in state *s* is denoted by *r(s, a)*. Here, *γ* is a hyperparameter
    that you get to choose, called the discount factor. If *γ* is *0*, then the agent
    chooses the action that maximizes the immediate reward. Higher values of *γ* will
    make the agent give more importance to considering long-term consequences. In
    practice, we have more such hyperparameter to be considered. For example, if a
    vacuum cleaner robot is expected to learn to solve tasks quickly, but not necessarily
    optimally, we may want to set a faster learning rate.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*s''*表示下一个状态，*a''*表示下一个动作。在状态*s*下采取动作*a*的奖励用*r(s, a)*表示。这里，*γ*是一个超参数，你可以选择它，称为折扣因子。如果*γ*为*0*，那么智能体选择的是最大化即时奖励的动作。较高的*γ*值会使智能体更加重视长期后果。在实际应用中，我们还需要考虑更多这样的超参数。例如，如果一个吸尘器机器人需要快速学习解决任务，但不一定要求最优，我们可能会希望设置一个较快的学习率。'
- en: 'Alternatively, if a robot is allowed more time to explore and exploit, we may
    tune down the learning rate. Let us call the learning rate *α* and change our
    utility function as follows (note that when *α = 1*, both the equations are identical):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果允许机器人有更多时间来探索和利用，我们可以调低学习率。我们将学习率称为*α*，并将我们的效用函数修改如下（请注意，当*α = 1*时，两个方程是相同的）：
- en: '![](img/8ec4cadc-c023-415b-9ed9-e50bcc6078c1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ec4cadc-c023-415b-9ed9-e50bcc6078c1.png)'
- en: In summary, an RL problem can be solved if we know this *Q(s, a)* function.
    Here comes an algorithm called Q-learning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们知道这个*Q(s, a)*函数，就可以解决一个强化学习问题。接下来是一个叫做Q学习的算法。
- en: A simple Q-learning implementation
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的Q学习实现
- en: Q-learning is an algorithm that can be used in financial and market trading
    applications, such as options trading. One reason is that the best policy is generated
    through training. that is, RL defines the model in Q-learning over time and is
    constantly updated with any new episode. Q-learning is a method for optimizing
    (cumulated) discounted reward, making far-future rewards less prioritized than
    near-term rewards; Q-learning is a form of model-free RL. It can also be viewed
    as a method of asynchronous **dynamic programming** (**DP**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种可以用于金融和市场交易应用的算法，例如期权交易。一个原因是最佳策略是通过训练生成的。也就是说，强化学习通过在Q学习中定义模型，并随着每一个新的实验不断更新它。Q学习是一种优化（累计）折扣奖励的方法，使得远期奖励低于近期奖励；Q学习是一种无模型的强化学习方法。它也可以看作是异步**动态规划**（**DP**）的一种形式。
- en: It provides agents with the capability of learning to act optimally in Markovian
    domains by experiencing the consequences of actions, without requiring them to
    build maps of the domains. In short, Q-learning qualifies as an RL technique because
    it does not strictly require labeled data and training. Moreover, the Q-value
    does not have to be a continuous, differentiable function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它为智能体提供了通过体验行动的后果来学习在马尔科夫领域中最优行动的能力，而无需它们建立领域的映射。简而言之，Q学习被认为是一种强化学习技术，因为它不严格要求标签数据和训练。此外，Q值不一定是连续可微的函数。
- en: On the other hand, Markov decision processes provide a mathematical framework
    for modeling decision-making in situations where outcomes are partly random and
    partly under the control of a decision-maker. Therein, the probability of the
    random variables at a future point of time depends only on the information at
    the current point in time and not on any of the historical values. In other words,
    the probability is independent of historical states.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，马尔科夫决策过程提供了一个数学框架，用于在结果部分随机且部分受决策者控制的情况下建模决策过程。在这种框架中，随机变量在未来某一时刻的概率仅依赖于当前时刻的信息，而与任何历史值无关。换句话说，概率与历史状态无关。
- en: Components of the Q-learning algorithm
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习算法的组成部分
- en: This implementation is highly inspired by the Q-learning implementation from
    a book, written by Patrick R. Nicolas, *Scala for Machine Learning - Second Edition*,
    Packt Publishing Ltd., September 2017\. Thanks to the author and Packt Publishing
    Ltd. The source code is available at [https://github.com/PacktPublishing/Scala-for-Machine-Learning-Second-Edition/tree/master/src/main/scala/org/scalaml/reinforcement](https://github.com/PacktPublishing/Scala-for-Machine-Learning-Second-Edition/tree/master/src/main/scala/org/scalaml/reinforcement).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现深受Patrick R. Nicolas所著《*Scala for Machine Learning - Second Edition*》一书中的Q学习实现的启发，出版于Packt
    Publishing Ltd.，2017年9月。感谢作者和Packt Publishing Ltd.。源代码可以在[https://github.com/PacktPublishing/Scala-for-Machine-Learning-Second-Edition/tree/master/src/main/scala/org/scalaml/reinforcement](https://github.com/PacktPublishing/Scala-for-Machine-Learning-Second-Edition/tree/master/src/main/scala/org/scalaml/reinforcement)获取。
- en: 'Interested readers can take a look at the the original implementation at the
    extensed version of course can be downloaded from Packt repository or GitHub repo
    of this book. The key components of implementation of the Q-learning algorithm
    are a few classes—`QLearning`, `QLSpace`, `QLConfig`, `QLAction`, `QLState`, `QLIndexedState`,
    and `QLModel`—as described in the following points:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以查看原始实现，扩展版课程可以从Packt仓库或本书的GitHub仓库下载。Q学习算法实现的关键组件有几个类——`QLearning`、`QLSpace`、`QLConfig`、`QLAction`、`QLState`、`QLIndexedState`和`QLModel`——如以下几点所描述：
- en: '`QLearning`: Implements training and prediction methods. It defines a data
    transformation of type `ETransform` using a configuration of type `QLConfig`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLearning`：实现训练和预测方法。它使用类型为`QLConfig`的配置定义一个类型为`ETransform`的数据转换。'
- en: '`QLConfig`: This parameterized class defines the configuration parameters for
    the Q-learning. To be more specific, it is used to hold an explicit configuration
    from the user.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLConfig`：这个参数化的类定义了Q学习的配置参数。更具体地说，它用于保存用户的显式配置。'
- en: '`QLAction`**:** This is a class that defines actions between on source state
    and multiple destination states.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLAction`**：** 这是一个定义在源状态和多个目标状态之间执行的动作的类。'
- en: '`QLPolicy`: This is an enumerator used to define the type of parameters used
    to update the policy during the training of the Q-learning model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLPolicy`：这是一个枚举器，用于定义在Q学习模型训练过程中更新策略时使用的参数类型。'
- en: '`QLSpace`: This has two components: a sequence of states of type `QLState`
    and the identifier, `id`, of one or more goal states within the sequence.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLSpace`：它有两个组成部分：类型为`QLState`的状态序列和序列中一个或多个目标状态的标识符`id`。'
- en: '`QLState`: Contains a sequence of `QLAction` instances that help in the transition
    from one state to another. It is also used as a reference for the object or instance
    for which the state is to be evaluated and predicted.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLState`：包含一系列`QLAction`实例，帮助从一个状态过渡到另一个状态。它还用作要评估和预测状态的对象或实例的引用。'
- en: '`QLIndexedState`: This class returns an indexed state that indexes a state
    in the search toward the goal state.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLIndexedState`：这个类返回一个索引状态，用于在搜索目标状态的过程中索引一个状态。'
- en: '`QLModel`: This is used to generate a model through the training process. Eventually,
    it contains the best policy and the accuracy of a model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QLModel`：这个类用于通过训练过程生成一个模型。最终，它包含最佳策略和模型的准确性。'
- en: 'Note that, apart from the preceding components, an optional constraint function
    limits the scope of the search for the next most rewarding action from the current
    state. The following diagram shows the key components of the Q-learning algorithm
    and their interaction:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了前面的组件外，还有一个可选的约束函数，限制从当前状态搜索下一个最有回报的动作的范围。以下图示展示了Q学习算法的关键组件及其交互：
- en: '![](img/52b17c4f-9ee2-41f5-8530-ebf971a10444.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52b17c4f-9ee2-41f5-8530-ebf971a10444.png)'
- en: 'Figure 5: Components of the QLearning algorithm and their interaction'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：QLearning算法的组成部分及其交互
- en: States and actions in QLearning
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLearning中的状态和动作
- en: 'The `QLAction` class specifies the transition from one state to another state.
    It takes two parameters—that is, from and to. Both of them have their own integer
    identifiers that need to be greater than 0:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`QLAction`类指定了从一个状态到另一个状态的过渡。它接受两个参数——即从和到。它们各自有一个整数标识符，且需要大于0：'
- en: '`from`: A source of the action'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from`：动作的源'
- en: '`to`: Target of the action'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to`：动作的目标'
- en: 'The signature is given as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其签名如下所示：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `QLState` class defines the state in the Q-learning. It takes three parameters:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`QLState`类定义了Q学习中的状态。它接受三个参数：'
- en: '`id`: An identifier that uniquely identifies a state'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`：一个唯一标识状态的标识符'
- en: '`actions`: A list of actions for the transition from this state to other states,'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`actions`：从当前状态过渡到其他状态的动作列表，'
- en: '`instance`: A state may have properties of type `T`, independent from the state
    transition'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance`：状态可能具有`T`类型的属性，与状态转移无关'
- en: 'Here is the signature of the class:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是类的签名：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, the `toString()` method is used for textual representation
    of a state in Q-learning. The state is defined by its ID and the list of actions
    it may potentially trigger.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`toString()`方法用于表示Q-learning中状态的文本形式。状态由其ID和可能触发的动作列表定义。
- en: The state might not have any actions. This is usually the case with the goal
    or absorbing state. In this case, the list is empty. The parameterized instance
    is a reference to the object for which the state is computed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 状态可能没有任何动作。通常这种情况发生在目标状态或吸收状态中。在这种情况下，列表为空。参数化实例是指为其计算状态的对象。
- en: Now we know the state and action to perform. However, the `QLearning` agent
    needs to know the search space of the form (States *x* Actions). The next step
    consists of creating the graph or search space.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道要执行的状态和动作。然而，`QLearning`代理需要知道形如（状态 *x* 动作）的搜索空间。下一步是创建图形或搜索空间。
- en: The search space
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索空间
- en: 'The search space is the container responsible for any sequence of states. The
    `QLSpace` class defines the search space (States *x* Actions) for the Q-learning
    algorithm, as shown in the following figure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间是负责任何状态序列的容器。`QLSpace`类定义了Q-learning算法的搜索空间（状态 *x* 动作），如下图所示：
- en: '![](img/d7238d9c-0a36-4840-b783-0d97adf1b688.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7238d9c-0a36-4840-b783-0d97adf1b688.png)'
- en: 'Figure 6: State transition matrix with QLData (Q-value, reward, probability)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：带有 QLData（Q值、奖励、概率）的状态转移矩阵
- en: 'The search space can be provided by the end user with a list of states and
    actions, or automatically created by providing the number of states by taking
    the following parameters:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间可以通过最终用户提供状态和动作的列表来提供，或者通过提供以下参数来自动创建状态数量：
- en: '`States`: The sequence of all possible states defined in the Q-learning search
    space'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`States`：Q-learning 搜索空间中定义的所有可能状态的序列'
- en: '`goalIds`: A list of identifiers of states that are goals'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`goalIds`：目标状态的标识符列表'
- en: 'Now let us see the implementation of the class. It is rather a large code block.
    So let us start from the constructor that generates a map named `statesMap`. It
    retrieves the state using its `id` and the array of goals, `goalStates`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下这个类的实现。这是一个相当大的代码块。因此，我们从构造函数开始，它生成一个名为`statesMap`的映射。它通过`id`获取状态，并使用目标数组`goalStates`：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then it creates a map of the states as an immutable Map of state ID and state
    instance:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它创建一个不可变的状态映射，映射包含状态ID和状态实例：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have a policy and a state of an action, the next task is to compute
    the maximum value given a state and policy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了策略和动作状态，接下来的任务是根据状态和策略计算最大值：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Additionally, we need to know the number of states by accessing the number
    of states in the search space:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要通过访问搜索空间中的状态数来知道状态的数量：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then the `init` method selects an initial state for training episodes. The
    state is randomly selected if the `state0` argument is invalid:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`init`方法选择一个初始状态用于训练集。 如果`state0`参数无效，则随机选择该状态：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, the `nextStates` method retrieves the list of states resulting from
    the execution of all the actions associated with that state. The search space
    `QLSpace` is created by the factory method `apply` defined in the `QLSpace` companion
    object, as shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`nextStates`方法检索执行所有与该状态相关的动作后得到的状态列表。搜索空间`QLSpace`由在`QLSpace`伴生对象中定义的工厂方法`apply`创建，如下所示：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Additionally, how do you know whether the current state is a goal state? Well,
    the `isGoal()` method does the trick.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如何知道当前状态是否为目标状态？嗯，`isGoal()`方法可以解决这个问题。
- en: 'It accepts a parameter called `state`*,* which is *a* state that is tested
    against the goal and returns `Boolean: true` if this state is a goal state; otherwise, it
    returns false:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '它接受一个名为`state`*的参数，*它是*一个*被测试是否为目标状态的状态，并且如果该状态是目标状态，则返回`Boolean: true`；否则返回false：'
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The apply method creates a list of states using the instances set, the goals,
    and the constraining function `constraints` as input. Each state creates its list
    of actions. The actions are generated from this state to any other states:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: apply 方法使用实例集合、目标和约束函数`constraints`作为输入，创建一个状态列表。每个状态都会创建一个动作列表。动作是从这个状态到任何其他状态生成的：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The function constraints limit the scope of the actions that can be triggered
    from any given state, as shown in figure X.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 函数约束限制了从任何给定状态触发的操作范围，如图 X 所示。
- en: The policy and action-value
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略和行动值
- en: The `QLData `class encapsulates the attributes of the policy in the Q-learning
    algorithm by creating a `QLData` record or instance with a given reward, probability,
    and Q-value that is computed and updated during training. The probability variable
    is used to model the intervention condition for an action to be executed.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`QLData` 类通过创建一个具有给定奖励、概率和 Q 值的 `QLData` 记录或实例来封装 Q-learning 算法中策略的属性，这些值在训练过程中被计算和更新。概率变量用于建模执行操作的干预条件。'
- en: 'If the action does not have any external constraint, the probability is 1 (that
    is, the highest), and it is zero otherwise (that is, the action is not allowed
    anyhow). The signature is given as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作没有任何外部约束，则概率为 1（即最高概率），否则为零（即无论如何该操作都不被允许）。签名如下所示：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code block, the Q-Value updated during training using the Q-learning
    formula, but the overall value is computed for an action by using its reward,
    adjusted by its probability, and then returning the adjusted value. Then the `value()`
    method selects the attribute of an element of the Q-learning policy using its
    type. It takes the `varType` of the attribute (that is, `REWARD`, `PROBABILITY`,
    and `VALUE`) and returns the value of this attribute.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，Q 值通过 Q-learning 公式在训练过程中更新，但整体值是通过使用奖励调整其概率来计算的，然后返回调整后的值。然后，`value()`
    方法使用属性的类型选择 Q-learning 策略元素的属性。它接受属性的 `varType`（即 `REWARD`、`PROBABILITY` 和 `VALUE`），并返回该属性的值。
- en: 'Finally, the `toString()` method helps to represent the value, reward, and
    the probability. Now that we know how the data will be manipulated, the next task
    is to create a simple schema that initializes the reward and probability associated
    with each action. The following Scala case is a class named `QLInput`; it inputs
    to the Q-learning search space (`QLSpace`) and policy (`QLPolicy`):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`toString()` 方法有助于表示值、奖励和概率。现在我们知道数据将如何操作，接下来的任务是创建一个简单的模式，用于初始化与每个操作相关的奖励和概率。以下
    Scala 示例是一个名为 `QLInput` 的类；它输入到 Q-learning 搜索空间（`QLSpace`）和策略（`QLPolicy`）中：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding signature, the constructor creates an action input to Q-learning.
    It takes four parameters:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的签名中，构造函数创建了一个 Q-learning 的操作输入。它接受四个参数：
- en: '`from`, the identifier for the source state'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from`，源状态的标识符'
- en: '`to`, the identifier for the target or destination state'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to`，目标或目的地状态的标识符'
- en: '`reward`, which is the credit or penalty to transition from the state with
    id `from` to the state with id `to`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`，即从状态 `from` 转移到状态 `to` 的奖励或惩罚'
- en: prob, the probability of transition from the state `from` to the state `to`
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: prob，表示从状态 `from` 转移到状态 `to` 的概率
- en: In the preceding class, the `from` and `to` arguments are used for a specific
    action, but the last two arguments are the reward collected at the completion
    of the action and its probability, respectively. Both the actions have a reward
    and a probability of 1 by default. In short, we only need to create an input for
    actions that have either a higher reward or a lower probability.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的类中，`from` 和 `to` 参数用于特定操作，而最后两个参数分别是操作完成后收集的奖励和其概率。默认情况下，这两个操作的奖励和概率均为 1。简而言之，我们只需要为那些具有更高奖励或更低概率的操作创建输入。
- en: 'The number of states and the sequence of input define the policy of type `QLPolicy`,
    which is a data container. An action has a Q-value (also known as **action-value**),
    a reward, and a probability. The implementation defines these three values in
    three separate matrices—*Q* for the action values, *R* for rewards, and *P* for
    probabilities—in order to stay consistent with the mathematical formulation. Here
    is the workflow for this class:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 状态数和输入序列定义了 `QLPolicy` 类型的策略，这是一个数据容器。一个操作有一个 Q 值（也称为**行动值**）、一个奖励和一个概率。实现通过三个独立的矩阵定义这三个值——*Q*
    用于行动值，*R* 用于奖励，*P* 用于概率——以保持与数学公式的一致性。以下是此类的工作流程：
- en: Initialize the policy using the input probabilities and rewards (see the `qlData` variable).
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用输入的概率和奖励初始化策略（参见 `qlData` 变量）。
- en: Compute the number of states from the input size (see the `numStates` variable).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据输入大小计算状态数（参见`numStates`变量）。
- en: Set the Q value for an action from state `from` to state `to` (see the `setQ`
    method) and get the Q-value using the `get()` method.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置从状态`from`到状态`to`的动作的Q值（见`setQ`方法），并通过`get()`方法获取Q值。
- en: Retrieve the Q-value for a state transition action from state `from` to state
    `to` (see the Q method).
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取从状态`from`到状态`to`的状态转移动作的Q值（见Q方法）。
- en: Retrieve the estimate for a state transition action from state `from` to state
    `to` (see the `EQ` method), and return the value in a `double`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取从状态`from`到状态`to`的状态转移动作的估计值（见`EQ`方法），并以`double`类型返回该值。
- en: Retrieve the reward for a state transition action from state `from` to state `to` (see
    the R method).
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取从状态`from`到状态`to`的状态转移动作的奖励（见R方法）。
- en: Retrieve the probability for a state transition action from state `from` to
    state `to` (see the `P` method).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取从状态`from`到状态`to`的状态转移动作的概率（见`P`方法）。
- en: Compute the minimum and maximum value for `Q` (see the `minMaxQ` method).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`Q`的最小值和最大值（见`minMaxQ`方法）。
- en: 'Retrieve the pair (index source state, index destination state) whose transition
    is a positive value. The index of the state is converted to a Double (see the `EQ:
    Vector[DblPair]` method).'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '获取一对（源状态索引，目标状态索引），其转移值为正。状态的索引将转换为Double类型（见`EQ: Vector[DblPair]`方法）。'
- en: Get the textual description of the reward matrix for this policy using the first
    `toString()` method.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第一个`toString()`方法获取此策略的奖励矩阵的文本描述。
- en: 'Textual representation of any one of the following: Q-value, reward, or probability
    matrix using the second `toString()` method.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第二个`toString()`方法，文本表示以下任意一项：Q值、奖励或概率矩阵。
- en: Validate the `from` and `to` value using the `check()` method.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`check()`方法验证`from`和`to`的值。
- en: 'Now let us see the class definition consisting of the preceding workflow:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下包含前述工作流的类定义：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: QLearning model creation and training
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLearning模型的创建与训练
- en: The `QLearning` class encapsulates the Q-learning algorithm, more specifically
    the action-value updating equation. It is a data transformation of type `ETransform`
    (we will see this later on) with an explicit configuration of type `QLConfig`.
    This class is a generic parameterized class that implements the `QLearning` algorithm.
    The Q-learning model is initialized and trained during the instantiation of the
    class so it can be in the correct state for runtime prediction.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`QLearning`类封装了Q学习算法，更具体地说，是动作-值更新方程。它是`ETransform`类型的数据转换（我们稍后会讨论），并有一个明确的`QLConfig`类型配置。该类是一个泛型参数化类，实现了`QLearning`算法。Q学习模型在类实例化时进行初始化和训练，以便它能处于正确的状态，进行运行时预测。'
- en: 'Therefore, the class instances have only two states: successfully trained and
    failed training (we''ll see this later).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，类实例只有两种状态：成功训练和失败训练（我们稍后会看到这一点）。
- en: The implementation does not assume that every episode (or training cycle) will
    be successful. At the completion of training, the ratio of labels over the initial
    training set is computed. The client code is responsible for evaluating the quality
    of the model by testing the ratio (see the model evaluation section).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 实现不假设每个回合（或训练周期）都会成功。训练完成后，计算初始训练集上标签的比例。客户端代码负责通过测试该比例来评估模型的质量（见模型评估部分）。
- en: 'The constructor takes the configuration of the algorithm (that is, `config`),
    the search space (that is, `qlSpace`), and the policy (that is, `qlPolicy`) parameters
    and creates a Q-learning algorithm:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受算法的配置（即`config`）、搜索空间（即`qlSpace`）和策略（即`qlPolicy`）参数，并创建一个Q学习算法：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The model is automatically created effectively if the minimum coverage is reached
    (or trained) during instantiation of the class, which is essentially a Q-learning
    model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在类实例化过程中达到（或训练）最小覆盖率，模型会自动有效地创建，这本质上是一个Q学习模型。
- en: 'The following `train()` method is applied to each episode with randomly generated
    initial states. Then it computes the coverage (based on the `minCoverage` configuration
    value supplied by the `conf` object) as the number of episodes for each the goal
    state was reached:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的`train()`方法应用于每个回合，并随机生成初始状态。然后，它根据由`conf`对象提供的`minCoverage`配置值计算覆盖率，即每个目标状态达到的回合数：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code block, the `heavyLiftingTrain(state0: Int)` method does
    the heavy lifting at each episode (or epoch). It triggers the search by selecting
    either the initial state state 0 or a random generator *r* with a new seed, if
    `state0` is < 0.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '在上述代码块中，`heavyLiftingTrain(state0: Int)`方法在每个回合（或迭代）中执行繁重的工作。它通过选择初始状态state
    0或使用新种子生成的随机生成器*r*来触发搜索，如果`state0`小于0。'
- en: At first, it gets all the states adjacent to the current state, and then it
    selects the most rewarding of the list of adjacent states. If the next most rewarding
    state is a goal state, we are done. Otherwise, it recomputes the policy value
    for the state transition using the reward matrix (that is, `QLPolicy.R`).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它获取当前状态的所有相邻状态，然后从相邻状态列表中选择回报最高的状态。如果下一个回报最高的状态是目标状态，那么任务完成。否则，它将使用奖励矩阵（即`QLPolicy.R`）重新计算状态转移的策略值。
- en: 'For the recomputation, it applies the Q-learning updating formula by updating
    the Q-Value for the policy; then it invokes the search method with the new state
    and incremented iterator. Let''s see the body of this method:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于重新计算，它通过更新Q值来应用Q学习更新公式，然后使用新的状态和递增的迭代器调用搜索方法。让我们来看一下该方法的主体：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As a list of policies and training coverage is given, let us get the trained
    model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一组策略和训练覆盖率后，让我们获取训练后的模型：
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that the preceding model is trained using the input data (see the class
    `QLPolicy`) used for training the Q-learning algorithm using the inline `getInput()`
    method:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述模型是通过用于训练Q学习算法的输入数据（参见类`QLPolicy`）和内联方法`getInput()`进行训练的：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we need to do one of the most important steps that will be used in our
    options trading application. Therefore, we need to retrieve the model for Q-learning
    as an option:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要执行一个在期权交易应用中将会用到的重要步骤。因此，我们需要将Q学习的模型作为一个选项进行检索：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The overall application fails if the model is not defined (see the `validateConstraints()`
    method for validation):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型未定义，则整体应用程序会失败（参见`validateConstraints()`方法进行验证）：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Then, a recursive computation of the next most rewarding state is performed
    using Scala tail recursion. The idea is to search among all states and recursively
    select the state with the most awards given for the best policy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用Scala尾递归执行下一最有回报的状态的递归计算。其思路是在所有状态中搜索，并递归选择为最佳策略给予最多奖励的状态。
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code block, the `nextState()` method retrieves the eligible
    states that can be transitioned to from the current state. Then it extracts the
    state, `qState`, with the most rewarding policy by incrementing the iteration
    counter. Finally, it returns the states if there are no more states or if the
    method does not converge within the maximum number of allowed iterations supplied
    by the `config.episodeLength` parameter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码块中，`nextState()`方法检索可以从当前状态转移到的合适状态。然后，它通过递增迭代计数器来提取具有最高回报策略的状态`qState`。最后，如果没有更多状态或方法未在由`config.episodeLength`参数提供的最大迭代次数内收敛，它将返回状态。
- en: '**Tail recursion**: In Scala, tail recursion is a very effective construct
    used to apply an operation to every item of a collection. It optimizes the management
    of the function stack frame during recursion. The annotation triggers a validation
    of the condition necessary for the compiler to optimize function calls.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**尾递归**：在Scala中，尾递归是一种非常有效的结构，用于对集合中的每个项应用操作。它在递归过程中优化了函数栈帧的管理。注解触发了编译器优化函数调用所需的条件验证。'
- en: 'Finally, the configuration of the Q-learning algorithm, `QLConfig`, specifies:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Q学习算法的配置`QLConfig`指定：
- en: The learning rate, `alpha`
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率，`alpha`
- en: The discount rate, `gamma`
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣率，`gamma`
- en: The maximum number of states (or length) of an episode, `episodeLength`
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个回合的最大状态数（或长度），`episodeLength`
- en: The number of episodes (or epochs) used in training, `numEpisodes`
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练中使用的回合数（或迭代次数），`numEpisodes`
- en: The minimum coverage required to select the best policy, `minCoverage`
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳策略所需的最小覆盖率，`minCoverage`
- en: 'These are shown as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容如下所示：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we are almost done, except that the validation is not completed. However,
    let us first see the companion object for the configuration of the Q-learning
    algorithm. This singleton defines the constructor for the `QLConfig` class and
    validates its parameters:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎完成了，除了验证尚未完成。然而，让我们先看一下Q学习算法配置的伴生对象。此单例定义了`QLConfig`类的构造函数，并验证其参数：
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Excellent! We have seen how to implement the `QLearning` algorithm in Scala.
    However, as I said, the implementation is based on openly available sources, and
    the training may not always converge. One important consideration for such an
    online model is validation. A commercial application (or even a fancy Scala web
    app, which we will be covering in the next section) may require multiple types
    of validation mechanisms regarding the states transition, reward, probability,
    and Q-value matrices.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经看到了如何在 Scala 中实现 `QLearning` 算法。然而，正如我所说，实施是基于公开的来源，训练可能并不总是收敛。对于这种在线模型，一个重要的考虑因素是验证。商业应用（或甚至是我们将在下一节讨论的高大上的
    Scala Web 应用）可能需要多种验证机制，涉及状态转换、奖励、概率和 Q 值矩阵。
- en: QLearning model validation
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLearning 模型验证
- en: 'One critical validation is to verify that the user-defined constraints function
    does not create a dead-end in the search or training of Q-learning. The function
    constraints establish the list of states that can be accessed from a given state
    through actions. If the constraints are too tight, some of the possible search
    paths may not reach the goal state. Here is a simple validation of the constraints
    function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的验证是确保用户定义的约束函数不会在 Q-learning 的搜索或训练中产生死锁。约束函数确定了从给定状态通过行动可以访问的状态列表。如果约束过于严格，一些可能的搜索路径可能无法到达目标状态。下面是对约束函数的一个简单验证：
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Making predictions using the trained model
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练好的模型进行预测
- en: Now that we can select the state with the most awards given for the best policy
    recursively (see the `nextState` method in the following code), an online training
    method for the Q-learning algorithm can be performed for options trading, for
    example.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以递归地选择给定最佳策略的最多奖励的状态（参见下面代码中的`nextState`方法），例如，可以对期权交易执行 Q-learning 算法的在线训练。
- en: 'So, once the Q-learning model is trained using the supplied data, the next
    state can be predicted using the Q-learning model by overriding the data transformation
    method (`PipeOperator`, that is, `|`) with a transformation of a state to a predicted
    goal state:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦 Q-learning 模型使用提供的数据进行了训练，下一状态就可以通过覆盖数据转换方法（`PipeOperator`，即`|`）来使用 Q-learning
    模型进行预测，转换为预测的目标状态：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: I guess that's enough of a mouthful, though it would have been good to evaluate
    the model. But evaluating on a real-life dataset, it would be even better, because
    running and evaluating a model's performance on fake data is like buying a new
    car and never driving it. Therefore, I would like to wrap up the implementation
    part and move on to the options trading application using this Q-learning implementation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我想这已经够多了，虽然评估模型会很好。但是，在真实数据集上进行评估会更好，因为在假数据上运行和评估模型的表现，就像是买了辆新车却从未开过。因此，我想结束实现部分，继续进行基于这个
    Q-learning 实现的期权交易应用。
- en: Developing an options trading web app using Q-learning
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q-learning 开发期权交易 Web 应用
- en: The trading algorithm is the process of using computers programmed to follow
    a defined set of instructions for placing a trade in order to generate profits
    at a speed and frequency that is impossible for a human trader. The defined sets
    of rules are based on timing, price, quantity, or any mathematical model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 交易算法是利用计算机编程，按照定义的一组指令执行交易，以生成人类交易员无法匹敌的速度和频率的利润。定义的规则集基于时机、价格、数量或任何数学模型。
- en: Problem description
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题描述
- en: 'Through this project, we will predict the price of an option on a security
    for *N* days in the future according to the current set of observed features derived
    from the time of expiration, the price of the security, and volatility. The question
    would be: what model should we use for such an option pricing model? The answer
    is that there are actually many; Black-Scholes stochastic **partial differential
    equations** (**PDE**) is one of the most recognized.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个项目，我们将根据当前一组从到期时间、证券价格和波动性派生的观察特征，预测期权在未来*N*天的价格。问题是：我们应该使用什么模型来进行这种期权定价？答案是，实际上有很多模型；其中
    Black-Scholes 随机**偏微分方程**（**PDE**）是最为人熟知的。
- en: 'In mathematical finance, the Black-Scholes equation is necessarily a PDE overriding
    the price evolution of a European call or a European put under the Black-Scholes
    model. For a European call or put on an underlying stock paying no dividends,
    the equation is:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学金融学中，Black-Scholes 方程是必然的偏微分方程，它覆盖了欧式看涨期权或欧式看跌期权在 Black-Scholes 模型下的价格演变。对于不支付股息的标的股票的欧式看涨期权或看跌期权，方程为：
- en: '![](img/beeab1f3-57f6-47fa-9358-4a3b47f69bcc.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/beeab1f3-57f6-47fa-9358-4a3b47f69bcc.png)'
- en: Where *V* is the price of the option as a function of stock price *S* and time
    *t*, *r* is the risk-free interest rate, and *σ* *σ* (displaystyle sigma) is the
    volatility of the stock. One of the key financial insights behind the equation
    is that anyone can perfectly hedge the option by buying and selling the underlying
    asset in just the right way without any risk. This hedge implies that there is
    only one right price for the option, as returned by the Black-Scholes formula.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *V* 表示期权价格，是股票价格 *S* 和时间 *t* 的函数，*r* 是无风险利率，*σ* 是股票的波动率。方程背后的一个关键金融洞察是，任何人都可以通过正确的方式买卖标的资产来完美对冲期权而不承担任何风险。这种对冲意味着只有一个正确的期权价格，由
    Black-Scholes 公式返回。
- en: 'Consider a January maturity call option on an IBM with an exercise price of
    $95\. You write a January IBM put option with an exercise price of $85\. Let us
    consider and focus on the call options of a given security, IBM. The following
    chart plots the daily price of the IBM stock and its derivative call option for
    May 2014, with a strike price of $190:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种行使价格为 $95 的 IBM 一月到期期权。你写出一种行使价格为 $85 的 IBM 一月看跌期权。让我们考虑和关注给定安全性 IBM 的看涨期权。下图绘制了
    2014 年 5 月 IBM 股票及其衍生看涨期权的每日价格，行使价格为 $190：
- en: '![](img/7cce8043-97c9-41b7-a0e2-bd401127cfe0.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cce8043-97c9-41b7-a0e2-bd401127cfe0.png)'
- en: Figure 7: IBM stock and call $190 May 2014 pricing in May-Oct 2013
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：2013 年 5 月至 10 月期间 IBM 股票和行使价格为 $190 的看涨期权定价
- en: 'Now, what will be the profit and loss be for this position if IBM is selling
    at $87 on the option maturity date? Alternatively, what if IBM is selling at $100?
    Well, it is not easy to compute or predict the answer. However, in options trading,
    the price of an option depends on a few parameters, such as time decay, price,
    and volatility:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果 IBM 在期权到期日以 $87 出售，这个头寸的盈亏将是多少？或者，如果 IBM 以 $100 出售呢？嗯，计算或预测答案并不容易。然而，在期权交易中，期权价格取决于一些参数，如时间衰减、价格和波动率：
- en: Time to expiration of the option (time decay)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期权到期时间（时间衰减）
- en: The price of the underlying security
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标的证券的价格
- en: The volatility of returns of the underlying asset
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标的资产收益的波动率
- en: 'A pricing model usually does not consider the variation in trading volume in
    terms of the underlying security. Therefore, some researchers have included it
    in the option trading model. As we have described, any RL-based algorithm should
    have an explicit state (or states), so let us define the state of an option using
    the following four normalized features:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 定价模型通常不考虑标的证券的交易量变化。因此，一些研究人员将其纳入期权交易模型中。正如我们所描述的，任何基于强化学习的算法应该具有显式状态（或状态），因此让我们使用以下四个归一化特征定义期权的状态：
- en: '**Time decay** (`timeToExp`): This is the time to expiration once normalized
    in the range of (0, 1).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间衰减** (`timeToExp`)：这是归一化后的到期时间在 (0, 1) 范围内。'
- en: '**Relative volatility** (`volatility`): within a trading session, this is the
    relative variation of the price of the underlying security. It is different than
    the more complex volatility of returns defined in the Black-Scholes model, for
    example.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相对波动性** (`volatility`)：在一个交易会话内，这是标的证券价格相对变化的相对值。这与 Black-Scholes 模型中定义的更复杂收益波动性不同。'
- en: '**Volatility relative to volume** (`vltyByVol`): This is the relative volatility
    of the price of the security adjusted for its trading volume.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**波动性相对于成交量** (`vltyByVol`)：这是调整后的标的证券价格相对于其成交量的相对波动性。'
- en: '**Relative** **difference between the current price and the strike price**
    (`priceToStrike`): This measures the ratio of the difference between the price
    and the strike price to the strike price.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当前价格与行权价格之间的相对差异** (`priceToStrike`)：这衡量的是价格与行权价格之间差异与行权价格的比率。'
- en: 'The following graph shows the four normalized features that can be used for
    the IBM option strategy:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了可以用于 IBM 期权策略的四个归一化特征：
- en: '![](img/96deb7b5-fd00-41aa-8eed-a514786ef05b.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96deb7b5-fd00-41aa-8eed-a514786ef05b.png)'
- en: Figure 8: Normalized relative stock price volatility, volatility relative to
    trading volume, and price relative to strike price for the IBM stock
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：IBM股票的归一化相对股价波动性、相对于交易量的波动性以及相对于行权价格的股价
- en: 'Now let us look at the stock and the option price dataset. There are two files
    `IBM.csv` and `IBM_O.csv` contain the IBM stock prices and option prices, respectively.
    The stock price dataset has the date, the opening price, the high and low price,
    the closing price, the trade volume, and the adjusted closing price. A shot of
    the dataset is given in the following diagram:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看股票和期权价格的数据集。有两个文件，`IBM.csv`和`IBM_O.csv`，分别包含IBM股票价格和期权价格。股票价格数据集包含日期、开盘价、最高价、最低价、收盘价、交易量和调整后的收盘价。数据集的一部分如下图所示：
- en: '![](img/28f78eae-73f9-450c-a168-a30ec57f50f9.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28f78eae-73f9-450c-a168-a30ec57f50f9.png)'
- en: 'Figure 9: IBM stock data'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：IBM股票数据
- en: 'On the other hand, `IBM_O.csv` has 127 option prices for IBM Call 190 Oct 18,
    2014\. A few values are 1.41, 2.24, 2.42, 2.78, 3.46, 4.11, 4.51, 4.92, 5.41,
    6.01, and so on. Up to this point, can we develop a predictive model using a `QLearning`,
    algorithm that can help us answer the previously mentioned question: Can it tell
    us the how IBM can make maximum profit by utilizing all the available features?'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`IBM_O.csv`包含了127个IBM 190 Oct 18, 2014的期权价格。其中几个值为1.41、2.24、2.42、2.78、3.46、4.11、4.51、4.92、5.41、6.01等。到此为止，我们能否利用`QLearning`算法开发一个预测模型，帮助我们回答之前提到的问题：它能告诉我们如何通过利用所有可用特征帮助IBM实现最大利润吗？
- en: Well, we know how to implement the `QLearning`, and we know what option trading
    is. Another good thing is that the technologies that will be used for this project
    such as Scala, Akka, Scala Play Framework, and RESTful services are already discussed
    in [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml), *High-Frequency Bitcoin
    Price Prediction from Historical Data*. Therefore, it may be possible. Then we
    try it to develop a Scala web project that helps us maximize the profit.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们知道如何实现`QLearning`，也知道什么是期权交易。另一个好处是，本项目将使用的技术，如Scala、Akka、Scala Play框架和RESTful服务，已经在[第3章](51e66c26-e12b-4764-bbb7-444986c05870.xhtml)《从历史数据中进行高频比特币价格预测》中进行了讨论。*因此，可能是可行的*。接下来我们尝试开发一个Scala
    Web项目，帮助我们最大化利润。
- en: Implementating an options trading web application
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现期权交易Web应用程序
- en: 'The goal of this project is to create an options trading web application that
    creates a QLearning model from the IBM stock data. Then the app will extract the
    output from the model as a JSON object and show the result to the user. *Figure
    10*, shows the overall workflow:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目标是创建一个期权交易的Web应用程序，该程序从IBM股票数据中创建一个QLearning模型。然后，应用程序将从模型中提取输出作为JSON对象，并将结果显示给用户。*图10*显示了整体工作流程：
- en: '![](img/edad61e5-68f9-4f3a-bd72-b09377596bf1.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edad61e5-68f9-4f3a-bd72-b09377596bf1.png)'
- en: Figure 10: Workflow of the options trading Scala web
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：期权交易Scala Web的工作流程
- en: 'The compute API prepares the input for the Q-learning algorithm, and the algorithm
    starts by extracting the data from the files to build the option model. Then it
    performs operations on the data such as normalization and discretization. It passes
    all of this to the Q-learning algorithm to train the model. After that, the compute
    API gets the model from the algorithm, extracts the best policy data, and puts
    it onto JSON to be returned to the web browser. Well, the implementation of the
    options trading strategy using Q-learning consists of the following steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 计算API为Q-learning算法准备输入数据，算法通过从文件中提取数据来构建期权模型。然后，它对数据进行归一化和离散化等操作。所有这些数据都传递给Q-learning算法以训练模型。之后，计算API从算法中获取模型，提取最佳策略数据，并将其放入JSON中返回给Web浏览器。期权交易策略的实现，使用Q-learning包含以下几个步骤：
- en: Describing the property of an option
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述期权的属性
- en: Defining the function approximation
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义函数近似
- en: Specifying the constraints on the state transition
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定状态转换的约束条件
- en: Creating an option property
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个期权属性
- en: 'Considering the market volatility, we need to be a bit more realistic, because
    any longer-term prediction is quite unreliable. The reason is that it would fall
    outside the constraint of the discrete Markov model. So, suppose we want to predict
    the price for next two days—that is, *N= 2*. That means the price of the option
    two days in the future is the value of the reward profit or loss. So, let us encapsulate
    the following four parameters:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到市场波动性，我们需要更现实一点，因为任何长期预测都相当不可靠。原因是它将超出离散马尔科夫模型的约束。因此，假设我们想预测未来两天的价格——即 *N=
    2*。这意味着期权未来两天的价格是利润或损失的奖励值。那么，让我们封装以下四个参数：
- en: '`timeToExp`: Time left until expiration as a percentage of the overall duration
    of the option'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeToExp`：期权到期前剩余时间，占期权整体持续时间的百分比'
- en: Volatility normalized Relative volatility of the underlying security for a given
    trading session
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波动性标准化：给定交易时段内，基础证券的相对波动性
- en: '`vltyByVol`: Volatility of the underlying security for a given trading session
    relative to a trading volume for the session'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vltyByVol`：在给定交易时段内，相对于该时段交易量的基础证券波动性'
- en: '`priceToStrike`: Price of the underlying security relative to the Strike price
    for a given trading session'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`priceToStrike`：相对于行使价的基础证券价格，在给定交易时段内'
- en: 'The `OptionProperty` class defines the property of a traded option on a security.
    The constructor creates the property for an option:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`OptionProperty` 类定义了一个交易期权的属性。构造函数为期权创建属性：'
- en: '[PRE25]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Creating an option model
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个期权模型
- en: 'Now we need to create an `OptionModel` to act as the container and the factory
    for the properties of the option. It takes the following parameters and creates
    a list of option properties, `propsList`, by accessing the data source of the
    four features described earlier:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个 `OptionModel` 来充当期权属性的容器和工厂。它接受以下参数，并通过访问之前描述的四个特征的数据源，创建一个期权属性列表
    `propsList`：
- en: The symbol of the security.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证券的符号。
- en: The strike price for `option`, `strikePrice`.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option` 的行使价格，`strikePrice`。'
- en: The source of the `data`, `src`.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data` 的来源，`src`。'
- en: The minimum time decay or time to expiration, `minTDecay`. Out-of-the-money
    options expire worthlessly, and in-the-money options have a very different price
    behavior as they get closer to the expiration. Therefore, the last `minTDecay`
    trading sessions prior to the expiration date are not used in the training process.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小时间衰减或到期时间，`minTDecay`。期权价值低于行使价的期权会变得毫无价值，而价值高于行使价的期权在接近到期时价格行为差异很大。因此，期权到期日前的最后
    `minTDecay` 个交易时段不会参与训练过程。
- en: 'The number of steps (or buckets), `nSteps`, is used in approximating the values
    of each feature. For instance, an approximation of four steps creates four buckets:
    (0, 25), (25, 50), (50, 75), and (75, 100).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于近似每个特征值的步数（或桶数），`nSteps`。例如，四步近似会创建四个桶：（0，25），（25，50），（50，75），和（75，100）。
- en: 'Then it assembles `OptionProperties` and computes the normalized minimum time
    to the expiration of the option. Then it computes an approximation of the value
    of options by discretization of the actual value in multiple levels from an array
    of options prices; finally it returns a map of an array of levels for the option
    price and accuracy. Here is the constructor of the class:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它会组装 `OptionProperties` 并计算期权到期的最小标准化时间。接着，它通过将实际价值离散化为多个层次，从期权价格数组中近似计算期权的价值；最后，它返回一个包含期权价格和精度层次的映射。以下是该类的构造函数：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Inside this class implementation, at first, a validation is done using the
    `check()` method, by checking the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类的实现中，首先通过 `check()` 方法进行验证，检查以下内容：
- en: '`strikePrice`: A positive price is required'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strikePrice`：需要一个正的价格'
- en: '`minExpT`: This has to be between 2 and 16'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minExpT`：此值必须介于 2 和 16 之间'
- en: '`nSteps`: Requires a minimum of two steps'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nSteps`：至少需要两个步数'
- en: 'Here''s the invocation of this method:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是调用该方法的示例：
- en: '[PRE27]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The signature of the preceding method is shown in the following code:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法的签名如下所示：
- en: '[PRE28]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once the preceding constraint is satisfied, the list of option properties,
    named `propsList`, is created as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦满足前述约束条件，期权属性列表 `propsList` 被创建如下：
- en: '[PRE29]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code block, the factory uses the `zipWithIndex` Scala method
    to represent the index of the trading sessions. All feature values are normalized
    over the interval (0, 1), including the time decay (or time to expiration) of
    the `normDecay` option.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，工厂使用了`zipWithIndex`的Scala方法来表示交易会话的索引。所有的特征值都在区间(0, 1)内进行归一化，包括`normDecay`期权的时间衰减（或到期时间）。
- en: 'The `quantize()` method of the `OptionModel` class converts the normalized
    value of each option property of features into an array of bucket indices. It
    returns a map of profit and loss for each bucket keyed on the array of bucket
    indices:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`OptionModel`类的`quantize()`方法将每个期权属性的归一化值转换为一个桶索引数组。它返回一个以桶索引数组为键的盈亏映射表：'
- en: '[PRE30]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The method also creates a mapper instance to index the array of buckets. An
    accumulator, `acc`, of type `NumericAccumulator` extends the `Map[Int, (Int, Double)]`
    and computes this tuple *(number of occurrences of features on each bucket, sum
    of the increase or decrease of the option price)*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法还创建了一个映射器实例，用于索引桶数组。一个类型为`NumericAccumulator`的累加器`acc`扩展了`Map[Int, (Int,
    Double)]`，并计算这个元组*(每个桶中特征的出现次数，期权价格的增减总和)*。
- en: 'The `toArrayInt` method converts the value of each option property (`timeToExp`,
    `volatility`, and so on) into the index of the appropriate bucket. The array of
    indices is then encoded to generate the id or index of a state. The method updates
    the accumulator with the number of occurrences and the total profit and loss for
    a trading session for the option. It finally computes the reward on each action
    by averaging the profit and loss on each bucket. The signature of the `encode()`,
    `toArrayInt()` is given in the following code:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`toArrayInt`方法将每个期权属性（如`timeToExp`、`volatility`等）的值转换为相应桶的索引。然后，索引数组被编码以生成一个状态的id或索引。该方法更新累加器，记录每个交易会话的期权出现次数及其总盈亏。最后，它通过对每个桶的盈亏进行平均计算每个操作的奖励。`encode()`、`toArrayInt()`方法的签名如下所示：'
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finally, and most importantly, if the preceding constraints are satisfied (you
    can modify these constraints though) and once the instantiation of the `OptionModel`
    class generates a list of `OptionProperty` elements if the constructor succeeds; otherwise, it
    generates an empty list.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也是最重要的，如果前述约束条件得到满足（不过你可以修改这些约束），并且一旦`OptionModel`类的实例化成功生成`OptionProperty`元素的列表；否则，它将生成一个空列表。
- en: Putting it altogether
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将它们汇总在一起
- en: Because we have implemented the Q-learning algorithm, we can now develop the
    options trading application using Q-learning. However, at first, we need to load
    the data using the `DataSource` class (we will see its implementation later on).
    Then we can create an option model from the data for a given stock with default
    strike and minimum expiration time parameters, using `OptionModel`, which defines
    the model for a traded option, on a security. Then we have to create the model
    for the profit and loss on an option given the underlying security.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经实现了Q-learning算法，我们现在可以使用Q-learning开发期权交易应用程序。然而，首先，我们需要使用`DataSource`类加载数据（稍后我们将看到其实现）。然后，我们可以为给定股票创建一个期权模型，使用`OptionModel`，它定义了一个在证券上交易的期权模型，并设置默认的行权价和最短到期时间参数。然后，我们需要为期权的盈亏模型创建基础证券。
- en: The profit and loss are adjusted to produce positive values. It instantiates
    an instance of the Q-learning class, that is, a generic parameterized class that
    implements the Q-learning algorithm. The Q-learning model is initialized and trained
    during the instantiation of the class, so it can be in the correct state for the
    runtime prediction.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 盈亏被调整为产生正值。它实例化了一个Q-learning类的实例，即一个实现了Q-learning算法的通用参数化类。Q-learning模型在类实例化时被初始化和训练，因此它可以在运行时进行预测时处于正确的状态。
- en: 'Therefore, the class instances have only two states: successfully trained and
    failed training Q-learning value action. Then the model is returned to get processed
    and visualized.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，类的实例只有两种状态：成功训练和失败训练的Q-learning值操作。然后模型被返回并处理和可视化。
- en: 'So, let us create a Scala object and name it `QLearningMain`. Then, inside
    the `QLearningMain` object, define and initialize the following parameters:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们创建一个Scala对象并命名为`QLearningMain`。接着，在`QLearningMain`对象内部，定义并初始化以下参数：
- en: '`Name`: Used to indicate the reinforcement algorithm''s name (for our case,
    it''s Q-learning)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Name`：用于指示强化算法的名称（在我们的例子中是Q-learning）'
- en: '`STOCK_PRICES`: File that contains the stock data'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STOCK_PRICES`: 包含股票数据的文件'
- en: '`OPTION_PRICES`: File that contains the available option data'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OPTION_PRICES`: 包含可用期权数据的文件'
- en: '`STRIKE_PRICE`: Option strike price'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STRIKE_PRICE`: 期权行权价格'
- en: '`MIN_TIME_EXPIRATION`: Minimum expiration time for the option recorded'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MIN_TIME_EXPIRATION`: 记录的期权最小到期时间'
- en: '`QUANTIZATION_STEP`: Steps used in discretization or approximation of the value
    of the security'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QUANTIZATION_STEP`: 用于对证券值进行离散化或近似的步长'
- en: '`ALPHA`: Learning rate for the Q-learning algorithm'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ALPHA`: Q-learning算法的学习率'
- en: '`DISCOUNT` (gamma): Discount rate for the Q-learning algorithm'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DISCOUNT`（gamma）：Q-learning算法的折扣率'
- en: '`MAX_EPISODE_LEN`: Maximum number of states visited per episode'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MAX_EPISODE_LEN`: 每个回合访问的最大状态数'
- en: '`NUM_EPISODES`: Number of episodes used during training'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NUM_EPISODES`: 训练过程中使用的回合数'
- en: '`MIN_COVERAGE`: Minimum coverage allowed during the training of the Q-learning
    model'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MIN_COVERAGE`: Q-learning模型训练过程中允许的最小覆盖率'
- en: '`NUM_NEIGHBOR_STATES`: Number of states accessible from any other state'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NUM_NEIGHBOR_STATES`: 从任何其他状态可访问的状态数'
- en: '`REWARD_TYPE`: Maximum reward or Random'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`REWARD_TYPE`: 最大奖励或随机'
- en: 'Tentative initializations for each parameter are given in the following code:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数的初步初始化如下代码所示：
- en: '[PRE32]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now the `run()` method accepts as input the reward type (`Maximum reward` in
    our case), quantized step (in our case, `QUANTIZATION_STEP`), alpha (the learning
    rate, `ALPHA` in our case) and gamma (in our case, it''s `DISCOUNT`, the discount
    rate for the Q-learning algorithm). It displays the distribution of values in
    the model. Additionally, it displays the estimated Q-value for the best policy
    on a Scatter plot (we will see this later). Here is the workflow of the preceding
    method:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`run()`方法接受作为输入的奖励类型（在我们的例子中是`最大奖励`）、量化步长（在我们的例子中是`QUANTIZATION_STEP`）、alpha（学习率，在我们的例子中是`ALPHA`）和gamma（在我们的例子中是`DISCOUNT`，Q-learning算法的折扣率）。它显示了模型中的值分布。此外，它在散点图上显示了最佳策略的估计Q值（我们稍后会看到）。以下是前述方法的工作流程：
- en: First, it extracts the stock price from the `IBM.csv` file
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它从`IBM.csv`文件中提取股票价格
- en: Then it creates an option model `createOptionModel` using the stock prices and
    quantization, `quantizeR` (see the `quantize` method for more and the main method
    invocation later)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它使用股票价格和量化方法`quantizeR`创建一个选项模型`createOptionModel`（有关更多信息，请参见`quantize`方法和稍后的主方法调用）
- en: The option prices are extracted from the `IBM_o.csv` file
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 期权价格从`IBM_o.csv`文件中提取
- en: After that, another model, `model`, is created using the option model to evaluate
    it on the option prices, `oPrices`
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用期权模型创建另一个模型`model`，并使用期权价格`oPrices`对其进行评估
- en: Finally, the estimated Q-Value (that is, *Q-value = value * probability*) is
    displayed 0n a Scatter plot using the `display` method
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，估计的Q值（即，*Q值 = 值 * 概率*）在散点图上显示，使用`display`方法
- en: 'By amalgamating the preceding steps, here''s the signature of the `run()` method:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合前述步骤，以下是`run()`方法的签名：
- en: '[PRE33]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now here is the signature of the `createOptionModel()` method that creates
    an option model using (see the `OptionModel` class):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是`createOptionModel()`方法的签名，该方法使用（请参见`OptionModel`类）创建一个期权模型：
- en: '[PRE34]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Then the `createModel()` method creates a model for the profit and loss on an
    option given the underlying security. Note that the option prices are quantized
    using the `quantize()` method defined earlier. Then the constraining method is
    used to limit the number of actions available to any given state. This simple
    implementation computes the list of all the states within a radius of this state.
    Then it identifies the neighboring states within a predefined radius.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，`createModel()`方法创建一个期权的利润和亏损模型，给定基础证券。请注意，期权价格是使用之前定义的`quantize()`方法量化的。然后，使用约束方法限制给定状态下可用的动作数。这个简单的实现计算了该状态范围内的所有状态列表。然后，它确定了一个预定义半径内的邻接状态。
- en: 'Finally, it uses the input data to train the Q-learning model to compute the
    minimum value for the profit, a loss so the maximum loss is converted to a null
    profit. Note that the profit and loss are adjusted to produce positive values.
    Now let us see the signature of this method:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它使用输入数据训练Q-learning模型，计算最小的利润值和亏损值，以便最大亏损被转化为零利润。请注意，利润和亏损被调整为正值。现在让我们看看此方法的签名：
- en: '[PRE35]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note that if the preceding invocation cannot create an option model, the code
    fails to show a message that the model creation failed. Nonetheless, remember
    that the `minCoverage` used in the following line is important, considering the
    small dataset we used (because the algorithm will converge very quickly):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果前面的调用无法创建一个选项模型，代码不会显示模型创建失败的消息。尽管如此，请记住，考虑到我们使用的小数据集，接下来的这一行中使用的`minCoverage`非常重要（因为算法会非常快速地收敛）：
- en: '[PRE36]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Although we''ve already stated that it is not assured that the model creation
    and training will be successful, a Naïve clue would be using a very small `minCoverage`
    value between `0.0` and `0.22`. Now, if the preceding invocation is successful,
    then the model is trained and ready for making prediction. If so, then the display
    method is used to display the estimated *Q-value = value * probability* in a Scatter
    plot. Here is the signature of the method:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经说明模型的创建和训练未必成功，但一个简单的线索是使用一个非常小的`minCoverage`值，范围在`0.0`到`0.22`之间。如果前面的调用成功，那么模型已训练完成，可以进行预测。如果成功，那么就可以使用显示方法，在散点图中显示估算的*Q值
    = 值 * 概率*。方法的签名如下：
- en: '[PRE37]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Hang on and do not lose patience! We are finally ready to see a simple `rn`
    and inspect the result. So let us do it:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 稍等一下，不要急！我们终于准备好查看一个简单的`rn`并检查结果。让我们来看看：
- en: '[PRE38]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Evaluating the model
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: The preceding output shows the transition from one state to another, and for
    the **0.1** coverage, the `QLearning` model had 15,750 transitions for 126 states
    to reach goal state 37 with optimal rewards. Therefore, the training set is quite
    small and only a few buckets have actual values. So we can understand that the
    size of the training set has an impact on the number of states. `QLearning` will
    converge too fast for a small training set (like what we have for this example).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了从一个状态到另一个状态的转变，对于**0.1**的覆盖率，`QLearning`模型在126个状态中有15,750次转变，最终达到了目标状态37，且获得了最优奖励。因此，训练集相当小，只有少数桶包含实际值。所以我们可以理解，训练集的大小对状态的数量有影响。对于一个小的训练集（比如我们这个例子中的情况），`QLearning`会收敛得太快。
- en: However, for a larger training set, `QLearning` will take time to converge;
    it will provide at least one value for each bucket created by the approximation.
    Also, by seeing those values, it is difficult to understand the relation between
    Q-values and states.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于更大的训练集，`QLearning`需要一定的时间才能收敛；它会为每个由近似生成的桶提供至少一个值。同时，通过查看这些值，很难理解Q值和状态之间的关系。
- en: 'So what if we can see the Q-values per state? Why not! We can see them on a
    scatter plot:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们能看到每个状态的Q值呢？当然可以！我们可以在散点图中看到它们：
- en: '![](img/ca965320-2e1b-4071-8bcd-12deb0e638da.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca965320-2e1b-4071-8bcd-12deb0e638da.png)'
- en: 'Figure 11: Q-value per state'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：每个状态的Q值
- en: 'Now let us display the profile of the log of the Q-value (`QLData.value`) as
    the recursive search (or training) progress for different episodes or epochs.
    The test uses a learning rate *α = 0.1* and a discount rate *γ = 0.9* (see more
    in the deployment section):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们展示Q值（`QLData.value`）对数的曲线，作为递归搜索（或训练）过程中不同轮次或周期的进展。测试使用学习率*α = 0.1*和折扣率*γ
    = 0.9*（更多细节见部署部分）：
- en: '![](img/d881e918-f728-4520-826c-62a84b78c31a.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d881e918-f728-4520-826c-62a84b78c31a.png)'
- en: 'Figure 12: Profile of the logarithmic Q-Value for different epochs during Q-learning
    training'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Q-learning训练过程中，不同周期的对数Q值曲线
- en: The preceding chart illustrates the fact that the Q-value for each profile is
    independent of the order of the epochs during training. However, the number of
    iterations to reach the goal state depends on the initial state selected randomly
    in this example. To get more insights, inspect the output on your editor or access
    the API endpoint at `http://localhost:9000/api/compute` (see following). Now,
    what if we display the distribution of values in the model and display the estimated
    Q-value for the best policy on a Scatter plot for the given configuration parameters?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表说明了每个轮次中的Q值与训练的顺序无关。然而，达到目标状态所需的迭代次数则取决于在此示例中随机选择的初始状态。为了获得更多的见解，请检查编辑器中的输出，或者访问API端点`http://localhost:9000/api/compute`（见下文）。那么，如果我们在模型中显示值的分布，并在散点图中展示给定配置参数下最佳策略的估算Q值会怎么样呢？
- en: '*![](img/97ed3adc-210c-4522-bf60-22f75a28d47d.png)*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/97ed3adc-210c-4522-bf60-22f75a28d47d.png)*'
- en: 'Figure 13: Maximum reward with quantization 32 with the QLearning'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：在量化32的情况下，QLearning的最大奖励
- en: 'The final evaluation consists of evaluating the impact of the learning rate
    and discount rate on the coverage of the training:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最终评估包括评估学习率和折扣率对训练覆盖率的影响：
- en: '![](img/94fc5941-bbad-41f5-97ae-f9219bba71d0.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94fc5941-bbad-41f5-97ae-f9219bba71d0.png)'
- en: 'Figure 14: Impact of the learning rate and discount rate on the coverage of
    the training'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：学习率和折扣率对训练覆盖率的影响
- en: The coverage decreases as the learning rate increases. This result confirms
    the general rule of using *learning rate* *< 0.2*. A similar test to evaluate
    the impact of the discount rate on the coverage is inconclusive. We could have
    thousands of such configuration parameters with different choices and combinations.
    So, what if we can wrap the whole application as a Scala web app similar to what
    we did in [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml), *High-Frequency
    Bitcoin Price Prediction from Historical Data*? I guess it would not be that bad
    an idea. So let us dive into it.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 随着学习率的增加，覆盖率降低。这个结果验证了使用*学习率* *< 0.2*的普遍规律。为了评估折扣率对覆盖率的影响，进行的类似测试并没有得出明确结论。我们可能会有成千上万种这种配置参数的不同选择和组合。那么，如果我们能将整个应用程序包装成类似于我们在[第3章](51e66c26-e12b-4764-bbb7-444986c05870.xhtml)中做的那样的Scala
    Web应用程序——*基于历史数据的高频比特币价格预测*，会怎么样呢？我猜这应该不会是个坏主意。那么让我们深入研究一下吧。
- en: Wrapping up the options trading app as a Scala web app
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将期权交易应用程序封装为Scala Web应用程序
- en: The idea is to get the trained model and construct the best policy JSON output
    for the maximum reward case. `PlayML` is a web app that uses the options trading
    Q-learning algorithm to provide a compute API endpoint that takes the input dataset
    and some options to calculate the q-values and returns them in JSON format to
    be modeled in the frontend.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是获取训练好的模型并构建最佳策略的JSON输出，以便得到最大回报的情况。`PlayML`是一个Web应用程序，使用期权交易Q-learning算法，提供一个计算API端点，接收输入数据集和一些选项来计算q值，并以JSON格式返回这些值，以便在前端进行建模。
- en: 'The wrapped up Scala web ML app has the following directory structure:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 封装后的Scala Web ML应用程序具有以下目录结构：
- en: '![](img/b9c7cd08-9159-4dad-8f7b-d2a920c6fc22.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9c7cd08-9159-4dad-8f7b-d2a920c6fc22.png)'
- en: 'Figure 15: Scala ML web app directory structure'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Scala ML Web应用程序目录结构
- en: 'In the preceding structure, the app folder has both the original QLearning
    implementation (see the `ml` folder) and some additional backend code. The `controller` subfolder has
    a Scala class named `API.scala`, used as the Scala controller for controlling
    the model behavior from the frontend. Finally, `Filters.scala` acts as the `DefaultHttpFilters`:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的结构中，应用程序文件夹包含了原始的QLearning实现（见`ml`文件夹）以及一些额外的后端代码。`controller`子文件夹中有一个名为`API.scala`的Scala类，它作为Scala控制器，用于控制前端的模型行为。最后，`Filters.scala`作为`DefaultHttpFilters`起作用：
- en: '![](img/06994b75-70b4-434d-87e1-40d0b40a16c4.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06994b75-70b4-434d-87e1-40d0b40a16c4.png)'
- en: 'Figure 16: The ml directory structure'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：ml目录结构
- en: The `conf` folder has the Scala web app configuration file, `application.conf`,
    containing the necessary configurations. All the dependencies are defined in the
    `build.sbt` file, as shown in the following code*:*
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`conf`文件夹包含Scala Web应用程序的配置文件`application.conf`，其中包含必要的配置。所有的依赖项都在`build.sbt`文件中定义，如下所示：'
- en: '[PRE39]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `lib` folder has some `.jar` files used as external dependencies defined
    in the `build.sbt` file*.* The `public` folder has the static pages used in the
    UI. Additionally, the data files `IBM.csv` and `IBM_O.csv` are also there. Finally,
    the target folder holds the application as a packaged (if any).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`lib`文件夹包含一些作为外部依赖项的`.jar`文件，这些依赖项在`build.sbt`文件中定义。`public`文件夹包含UI中使用的静态页面。此外，数据文件`IBM.csv`和`IBM_O.csv`也存放在其中。最后，`target`文件夹保存打包后的应用程序（如果有的话）。'
- en: The backend
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后端
- en: 'In the backend, I encapsulated the preceding Q-learning implementation and
    additionally created a Scala controller that controls the model behavior from
    the frontend. The structure is given here:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在后端，我封装了前面提到的Q-learning实现，并额外创建了一个Scala控制器，来控制前端模型的行为。其结构如下：
- en: '[PRE40]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Look at the preceding code carefully; it has more or less the same structure
    as the `QLearningMain.scala` file. There are only two important things here, as
    follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看前面的代码，它与`QLearningMain.scala`文件的结构差不多。这里只有两件重要的事，如下所示：
- en: Compute is done as an Action that takes the input from the UI and computes the
    value
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算作为一个Action进行，该Action接收来自UI的输入并计算结果值
- en: Then the result is returned as a JSON object using the `JsObject()` method to
    be shown on the UI (see the following)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，结果作为JSON对象通过`JsObject()`方法返回，用于在UI上显示（见下文）
- en: The frontend
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前端
- en: 'The app consists of two main parts: the API endpoint, built with the play framework,
    and the frontend single-page application, built with `Angular.js`. The frontend
    app sends the data to the API to get computed and then shows the results using
    `chart.js`. Here are the steps that we need for this:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用由两个主要部分组成：API端点，使用Play框架构建，以及前端单页面应用，使用`Angular.js`构建。前端应用将数据发送到API进行计算，然后使用`chart.js`展示结果。我们需要的步骤如下：
- en: Initialize the form
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化表单
- en: Communicate with the API
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与API通信
- en: Populate the view with coverage data and charts
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用覆盖数据和图表填充视图
- en: 'The algorithm''s JSON output should be as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的JSON输出应如下所示：
- en: All the config parameters are returned
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的配置参数都会被返回
- en: '`GOAL_STATE_INDEX`, the maximum Profit Index'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GOAL_STATE_INDEX`，最大利润指数'
- en: '`COVERAGE`, the ratio of training trials or epochs that reach a predefined
    goal state'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COVERAGE`，达到预定义目标状态的训练试验或周期的比率'
- en: '`COVERAGE_STATES`, the size of the quantized option values'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COVERAGE_STATES`，量化期权值的大小'
- en: '`COVERAGE_TRANSITIONS`, the number of states squared'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COVERAGE_TRANSITIONS`，状态的平方数'
- en: '`Q_VALUE`, the q-value of all the states'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Q_VALUE`，所有状态的q值'
- en: '`OPTIMAL`, the states with the most reward returned if the reward type isn''t
    random'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OPTIMAL`，如果奖励类型不是随机的，返回最多奖励的状态'
- en: '**The frontend code** initiates the `Angular.js` app with the `chart.js` module
    as follows (see in the `PlayML/public/assets/js/main.js` file):'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**前端代码**使用如下代码初始化`Angular.js`应用，并集成`chart.js`模块（见`PlayML/public/assets/js/main.js`文件）：'
- en: '[PRE42]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then the run button action prepares the form data to be sent to the API and
    sends the data to the backend. Next, it passes the returned data to the result
    variable to be used in the frontend. Then, it clears the charts and recreates
    them; if an optimal is found, it initializes the optimal chart. Finally, if the
    Q-value is found initialize, the q-value chart is getting initialized:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行按钮的操作准备表单数据并将其发送到API，接着将返回的数据传递给结果变量，在前端使用。接下来，它会清除图表并重新创建；如果找到最优解，则初始化最优图表。最后，如果找到了Q值，则初始化Q值图表：
- en: '[PRE43]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The preceding frontend code is then embedded in the HTML (see `PlayML/public/index.html`)
    to get the UI to be accessed on the Web as a fancy app at `http://localhost:9000/`.
    Feel free to edit the content according to your requirement. We will see the details
    soon.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 上述前端代码随后嵌入到HTML中（见`PlayML/public/index.html`），使UI能够作为一个精美的应用通过Web在`http://localhost:9000/`访问。根据您的需求，您可以随意编辑内容。我们很快会看到详细信息。
- en: Running and Deployment Instructions
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行和部署说明
- en: 'As was already stated in [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml),
    *High-Frequency Bitcoin Price Prediction from Historical Data*, you need Java
    1.8+ and SBT as the dependencies. Then follow these instructions:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第3章](51e66c26-e12b-4764-bbb7-444986c05870.xhtml)《从历史数据中预测高频比特币价格》中已经提到的，您需要Java
    1.8+和SBT作为依赖。然后按照以下说明操作：
- en: Download the app. I named the code `PlayML.zip`.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载应用。我将代码命名为`PlayML.zip`。
- en: Unzip the file and you will get the folder `ScalaML`.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解压文件后，您将得到一个文件夹`ScalaML`。
- en: Go to the PlayML project folder.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转到PlayML项目文件夹。
- en: Run `$ sudo sbt run` to download all the dependencies and run the app.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`$ sudo sbt run`来下载所有依赖并运行应用。
- en: 'Then the application can be accessed at `http://localhost:9000/`, where we
    can upload the IBM stock and option prices and, of course, provide other config
    parameters:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以通过`http://localhost:9000/`访问该应用，在这里我们可以上传IBM的股票和期权价格，并且提供其他配置参数：
- en: '![](img/db3fee1d-a5a3-495e-99f3-1019e92ac422.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db3fee1d-a5a3-495e-99f3-1019e92ac422.png)'
- en: 'Figure 17: The UI of options trading using QLearning'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：使用QLearning进行期权交易的UI
- en: 'Now, if you upload the stock price and option price data and click on the run
    button, a graph will be generated as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您上传股票价格和期权价格数据并点击运行按钮，系统将生成如下图表：
- en: '![](img/9e0609eb-4652-44ce-84ef-b70bbc57c84a.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e0609eb-4652-44ce-84ef-b70bbc57c84a.png)'
- en: 'Figure 18: QLearning reaches goal state 81 with a coverage of 0.2 for 126 states
    and 15,750 transitions'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：QLearning以0.2的覆盖率在126个状态和15,750次转换中达到了目标状态81
- en: On the other hand, the API endpoint can be accessed at [http://localhost:9000/api/compute](http://localhost:9000/api/compute).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，API端点可以通过[http://localhost:9000/api/compute](http://localhost:9000/api/compute)进行访问。
- en: '![](img/bb375ce1-4039-41c1-97e4-7c6924a92171.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb375ce1-4039-41c1-97e4-7c6924a92171.png)'
- en: 'Figure 19: The API endpoint (abridged)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：API端点（简化版）
- en: Model deployment
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'You can easily deploy your application as a standalone server by setting the
    application HTTP port to 9000, for example:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将应用程序的 HTTP 端口设置为 9000 来轻松地将应用程序部署为独立服务器，例如：
- en: '[PRE44]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Note that you probably need root permissions to bind a process to this port.
    Here is a short workflow:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可能需要根权限才能将进程绑定到此端口。以下是一个简短的工作流程：
- en: Run `$ sudo sbt dist` to build application binary. The output can be found at
    `PlayML /target/universal/APP-NAME-SNAPSHOT.zip`. In our case, it's `playml-1.0.zip`.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`$ sudo sbt dist`来构建应用程序二进制文件。输出可以在`PlayML /target/universal/APP-NAME-SNAPSHOT.zip`找到。在我们的案例中，它是`playml-1.0.zip`。
- en: 'Now, to run the application, unzip the file and then run the script in the
    `bin` directory:'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，要运行该应用程序，解压文件，然后在`bin`目录中运行脚本：
- en: '[PRE45]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then you need to configure your web server to map to the app port configuration.
    Nevertheless, you can easily deploy your application as a standalone server by
    setting the application HTTP port to `9000`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要配置您的 web 服务器，以映射到应用程序的端口配置。不过，您可以通过将应用程序的 HTTP 端口设置为`9000`，轻松将应用程序部署为独立服务器：
- en: '[PRE46]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: However, if you plan to host several applications on the same server or load-balance
    several instances of your application for scalability or fault tolerance, you
    can use a frontend HTTP server. Note that using a frontend HTTP server will rarely
    give you better performance than using a Play server directly.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您打算在同一服务器上托管多个应用程序，或为了可扩展性或容错性而对多个应用程序实例进行负载均衡，您可以使用前端 HTTP 服务器。请注意，使用前端
    HTTP 服务器通常不会比直接使用 Play 服务器提供更好的性能。
- en: However, HTTP servers are very good at handling HTTPS, conditional GET requests,
    and static assets, and many services assume that a frontend HTTP server is part
    of your architecture. Additional information can be found at [https://www.playframework.com/documentation/2.6.x/HTTPServer](https://www.playframework.com/documentation/2.6.x/HTTPServer).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，HTTP 服务器非常擅长处理 HTTPS、条件 GET 请求和静态资源，许多服务假设前端 HTTP 服务器是您架构的一部分。更多信息可以在[https://www.playframework.com/documentation/2.6.x/HTTPServer](https://www.playframework.com/documentation/2.6.x/HTTPServer)中找到。
- en: Summary
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to develop a real-life application called options
    trading using a RL algorithm called Q-learning. The IBM stock datasets were used
    to design a machine learning system driven by criticisms and rewards. Additionally,
    we learned some theoretical background. Finally, we learned how to wrap up a Scala
    desktop application as a web app using Scala Play Framework and deploy it in production.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了如何使用 Q-learning 算法开发一个名为“期权交易”的真实应用程序。我们使用了 IBM 股票数据集来设计一个由批评和奖励驱动的机器学习系统。此外，我们还学习了一些理论背景。最后，我们学习了如何使用
    Scala Play Framework 将一个 Scala 桌面应用程序打包为 Web 应用，并部署到生产环境中。
- en: In the next chapter, we will see two examples of building very robust and accurate
    predictive models for predictive analytics using H2O on a bank marketing dataset.
    For this example, we will be using bank marketing datasets. The data is related
    to direct marketing campaigns of a Portuguese banking institution. The marketing
    campaigns were based on phone calls. The goal of this end-to-end project will
    be to predict that the client will subscribe a term deposit.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到使用 H2O 在银行营销数据集上构建非常稳健和准确的预测模型的两个示例。在这个例子中，我们将使用银行营销数据集。该数据与葡萄牙银行机构的电话营销活动相关。营销活动是通过电话进行的。该端到端项目的目标是预测客户是否会订阅定期存款。
