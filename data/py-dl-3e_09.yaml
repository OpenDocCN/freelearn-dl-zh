- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Advanced Applications of Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的高级应用
- en: In the previous two chapters, we introduced the transformer architecture and
    learned about its latest large-scale incarnations, known as **large language models**
    (**LLMs**). We discussed them in the context of **natural language processing**
    (**NLP**) tasks. NLP was the original transformer application and is still the
    field at the forefront of LLM development today. However, the success of the architecture
    has led the research community to explore the application of transformers in other
    areas, such as computer vision.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们介绍了变换器架构，并学习了其最新的大规模版本，被称为**大型语言模型**（**LLMs**）。我们在**自然语言处理**（**NLP**）任务中讨论了它们。NLP是变换器最初的应用领域，并且仍然是大型语言模型发展的前沿领域。然而，架构的成功使研究界开始探索变换器在其他领域的应用，如计算机视觉。
- en: In this chapter, we’ll focus on these areas. We’ll discuss transformers as replacements
    for convolutional networks (CNNs, [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107))
    for tasks such as image classification and object detection. We’ll also learn
    how to use them as generative models for images instead of text, as we have done
    until now. We’ll also implement a model fine-tuning example – something we failed
    to do in [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220). And finally, we’ll implement
    a novel LLM-driven application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点讨论以下内容。我们将讨论将变换器用作卷积网络（CNN，[*第4章*](B19627_04.xhtml#_idTextAnchor107)）的替代品，用于图像分类和目标检测等任务。我们还将学习如何将它们用作图像生成模型，而不是像之前那样只用于文本。我们还将实现一个模型微调的示例——这是我们在[*第8章*](B19627_08.xhtml#_idTextAnchor220)中未能完成的内容。最后，我们将实现一个基于大型语言模型驱动的新型应用。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要主题：
- en: Classifying images with Vision Transformer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用视觉变换器进行图像分类
- en: Detection transformer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测变换器
- en: Generating images with stable diffusion
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用稳定扩散生成图像
- en: Fine-tuning transformers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调变换器
- en: Harnessing the power of LLMs with LangChain
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用LangChain发挥大型语言模型的力量
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, the Hugging
    Face Transformers library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)),
    and the LangChain framework ([https://www.langchain.com/](https://www.langchain.com/),
    [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)).
    If you don’t have an environment with these tools, fret not – the example is available
    as a Jupyter Notebook on Google Colab. The code examples can be found in this
    book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter09).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch、Hugging Face的Transformers库（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）以及LangChain框架（[https://www.langchain.com/](https://www.langchain.com/)，[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)）来实现示例。如果你没有配置这些工具的环境，也不用担心——该示例可以在Google
    Colab上的Jupyter Notebook中找到。代码示例可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter09)。
- en: Classifying images with Vision Transformer
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用视觉变换器进行图像分类
- en: '**Vision Transformer** (**ViT**, *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale*, [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929))
    proves the adaptability of the attention mechanism by introducing a clever technique
    for processing images. One way to use transformers for image inputs is to encode
    each pixel with four variables – pixel intensity, row, column, and channel location.
    Each pixel encoding is an input to a simple **neural network** (**NN**), which
    outputs a ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/845.png)-dimensional
    embedding vector. We can represent the three-dimensional image as a one-dimensional
    sequence of these embedding vectors. It acts as an input to the model in the same
    way as a token embedding sequence does. Each pixel will attend to every other
    pixel in the attention blocks.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉变换器**（**ViT**，*一张图胜过16x16个词：用于大规模图像识别的变换器*，[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)）通过引入一种巧妙的图像处理技术，证明了注意力机制的适应性。使用变换器处理图像输入的一种方法是通过四个变量对每个像素进行编码——像素强度、行、列和通道位置。每个像素编码是一个简单的**神经网络**（**NN**）的输入，该网络输出一个![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/845.png)维度的嵌入向量。我们可以将三维图像表示为这些嵌入向量的一维序列。它作为模型的输入，方式与令牌嵌入序列相同。每个像素将在注意力块中关注到所有其他像素。'
- en: 'This approach has some disadvantages related to the length of the input sequence
    (context window). Unlike a one-dimensional text sequence, an image has a two-dimensional
    structure (the color channel doesn’t increase the number of pixels). Therefore,
    the input sequence length increases quadratically as the image size increases.
    Even a small 64×64 image would result in an input sequence with a length of 64*64=4,096\.
    On one hand, this makes the model computationally intensive. On the other hand,
    as each pixel attends to the entire long sequence, it will be hard for the model
    to learn the structure of the image. CNNs approach this problem by using filters,
    which restrict the input size of a unit only to its immediate surrounding area
    (receptive field). To understand how ViT solves this problem, let’s start with
    the following figure:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在输入序列长度（上下文窗口）方面存在一些缺点。与一维文本序列不同，图像具有二维结构（颜色通道不会增加像素数量）。因此，随着图像尺寸的增大，输入序列的长度呈二次方增长。即使是一个小的64×64图像，也会导致输入序列的长度为64*64=4,096。这样一来，一方面使得模型的计算量增加，另一方面，由于每个像素都要关注整个长序列，模型很难学习图像的结构。卷积神经网络（CNN）通过使用滤波器来解决这个问题，滤波器将单位的输入大小限制在其周围的区域内（感受野）。为了理解ViT是如何解决这个问题的，让我们从下面的图开始：
- en: '![Figure 9.1 – Vision Transformer. Inspired by https://arxiv.org/abs/2010.11929](img/B19627_09_1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 视觉变换器。灵感来源于 https://arxiv.org/abs/2010.11929](img/B19627_09_1.jpg)'
- en: Figure 9.1 – Vision Transformer. Inspired by [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 视觉变换器。灵感来源于 [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
- en: Let’s denote the input image resolution with *(H, W)* and the number of channels
    with *C*. Then, we can represent the input image as a tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math>](img/846.png).
    ViT splits the image into a sequence of two-dimension square patches, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math>](img/847.png)
    (*Figure 9**.1*). Here, *(P, P)* is the resolution of each image patch (*P=16*)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo><mml:mtext>/</mml:mtext><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/848.png)
    is the number of patches (which is also the input sequence length). The sequence
    of patches serves as input to the model, in the same way as a token sequence does.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 设输入图像的分辨率为*(H, W)*，通道数为*C*。然后，我们可以将输入图像表示为一个张量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math>](img/846.png)。ViT将图像分割成一系列二维方形图像块，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math>](img/847.png)
    (*图 9.1*)。这里，*(P, P)*是每个图像块的分辨率（*P=16*），而![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo><mml:mtext>/</mml:mtext><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/848.png)是图像块的数量（即输入序列的长度）。这些图像块的序列作为输入提供给模型，方式与token序列相同。
- en: 'Next, the input patches, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/849.png),
    serve as input to a linear projection, which outputs a ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png)-dimensional
    **patch embedding** vector for each patch. The patch embeddings form the input
    sequence, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/851.png).
    We can summarize the patch-to-embedding process with the following formula:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，输入图像块![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/849.png)将作为输入传递给线性投影，输出一个![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png)-维的**图像块嵌入**向量用于每个图像块。这些图像块嵌入形成输入序列![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/851.png)。我们可以用以下公式总结图像块到嵌入的过程：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi
    mathvariant="bold">E</mml:mi><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi
    mathvariant="bold">E</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/852.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi>E</mml:mi><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi
    mathvariant="bold">E</mml:mi><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi
    mathvariant="bold">E</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/852.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">E</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/853.png)
    is the linear projection and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/854.png)
    is the static positional encoding (the same as in the original transformer).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">E</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/853.png)
    是线性投影，和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/854.png)
    是静态位置编码（与原始变换器相同）。
- en: 'Once we have the embedding sequence, ViT processes it with a standard encoder-only
    pre-normalization transformer, similar to BERT ([*Chapter 7*](B19627_07.xhtml#_idTextAnchor202)).
    It comes in three variants, displayed as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得嵌入序列，ViT 会使用标准的仅编码器预归一化变换器进行处理，类似于 BERT（[ *第 7 章*](B19627_07.xhtml#_idTextAnchor202)）。它有三种变体，如下所示：
- en: '![Figure 9.2 – ViT variants. Based on https://arxiv.org/abs/2010.11929](img/B19627_09_2.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – ViT 变体。基于 https://arxiv.org/abs/2010.11929](img/B19627_09_2.jpg)'
- en: Figure 9.2 – ViT variants. Based on https://arxiv.org/abs/2010.11929
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – ViT 变体。基于 https://arxiv.org/abs/2010.11929
- en: The encoder architecture uses unmasked self-attention, which allows a token
    to attend to the full sequence rather than the preceding tokens only. This makes
    sense because the preceding or the next element doesn’t carry the same meaning
    in the relationship between pixels of an image as the order of the elements in
    a text sequence. The similarities between the two models don’t end here. Like
    BERT, the input sequence starts with a special `[CLS]` (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/855.png))
    token (for classification tasks). The model output for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/856.png)
    token is the output for the full image. In this way, the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/856.png)
    token attends to the entire input sequence (that is, the entire image). Alternatively,
    if we take the model output for any other patch, we will introduce an imbalance
    between the selected patch and the others of the sequence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器架构使用无掩码的自注意力，这允许一个标记关注整个序列，而不仅仅是前面的标记。这是有道理的，因为图像像素之间的关系不像文本序列中的元素顺序那样，前后元素并不携带相同的意义。两种模型的相似之处不仅仅止于此。与
    BERT 类似，输入序列以特殊的 `[CLS]` (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/855.png))
    标记开始（用于分类任务）。模型对 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/856.png)
    标记的输出是整个图像的输出。通过这种方式，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math>](img/856.png)
    标记关注整个输入序列（即整个图像）。或者，如果我们选择任何其他补丁的模型输出，我们将引入所选补丁与序列中其他部分之间的不平衡。
- en: Following the example of BERT, ViT has pre-training and fine-tuning phases.
    Pre-training uses large general-purpose image datasets (such as ImageNet) while
    fine-tuning trains the model on smaller task-specific datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿 BERT，ViT 也有预训练和微调阶段。预训练使用大规模通用图像数据集（例如 ImageNet），而微调则是在较小的任务特定数据集上训练模型。
- en: The model ends with a **classification head**, which contains one hidden layer
    during pre-training and no hidden layers during fine-tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以 **分类头** 结束，预训练阶段包含一个隐藏层，而微调阶段则没有隐藏层。
- en: One issue with ViT is that it performs best when pre-training with very large
    datasets, such as JFT-300M with 300M labeled images (*Revisiting Unreasonable
    Effectiveness of Data in Deep Learning Era*, [https://arxiv.org/abs/1707.02968](https://arxiv.org/abs/1707.02968)).
    This makes the training a lot more computationally intensive versus a comparable
    CNN. Many further variants of ViT try to solve this challenge and propose other
    improvements to the original model. You can find out more in *A Survey on Visual
    Transformer* ([https://arxiv.org/abs/2012.12556](https://arxiv.org/abs/2012.12556)),
    which is regularly updated with the latest advancements in the field.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 的一个问题是，当使用非常大的数据集进行预训练时，它的表现最好，例如使用 300M 标签图像的 JFT-300M 数据集（*Revisiting
    Unreasonable Effectiveness of Data in Deep Learning Era*，[https://arxiv.org/abs/1707.02968](https://arxiv.org/abs/1707.02968)）。这使得训练变得比可比的
    CNN 更加计算密集。许多 ViT 的变体尝试解决这一挑战，并对原始模型提出其他改进。你可以在 *A Survey on Visual Transformer*（[https://arxiv.org/abs/2012.12556](https://arxiv.org/abs/2012.12556)）中找到更多信息，该文献定期更新领域的最新进展。
- en: With that, let’s see how to use ViT in practice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们来看看如何在实际中使用 ViT。
- en: Using ViT with Hugging Face Transformers
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ViT 和 Hugging Face Transformers
- en: 'In this section, we’ll implement a basic example of ViT image classification
    with the help of Hugging Face Transformers and its `pipeline` abstraction, which
    we introduced in [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220). Let’s start:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将借助Hugging Face Transformers和其`pipeline`抽象实现一个ViT图像分类的基本示例，这一点我们在[第*8章*](B19627_08.xhtml#_idTextAnchor220)中有介绍。让我们开始：
- en: 'Import the `pipeline` abstraction:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pipeline`抽象：
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create an image classification pipeline instance. The pipeline uses the ViT-Base
    model:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个图像分类管道实例。该管道使用ViT-Base模型：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run the instance with an image of a bicycle from Wikipedia:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自维基百科的自行车图像运行实例：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code outputs the following top-5 class probability distribution (only
    the first class is displayed):'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码输出以下前五个类别的概率分布（这里只显示第一个类别）：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This example is simple enough, but let’s dive in and analyze the ViT model
    itself. We can do this with the `print(img_classification_pipeline.model)` command,
    which outputs the following:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个示例足够简单，但让我们深入分析ViT模型本身。我们可以使用`print(img_classification_pipeline.model)`命令来实现，输出如下：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The model works with 224×224 input images. Here, `in_f` and `out_f` are shortened
    for `in_features` and `out_features`, respectively. Unlike other models, ViT uses
    bias in all `Linear` layers (the `bias=True` input parameter is not displayed).
    Let’s discuss the components of the model in the order that they appear:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型处理224×224的输入图像。这里，`in_f`和`out_f`是`in_features`和`out_features`的缩写。与其他模型不同，ViT在所有`Linear`层中使用偏置（`bias=True`输入参数未显示）。让我们按出现顺序讨论模型的组成部分：
- en: '`ViTEmbeddings`: The patch embedding block. It contains a 2D convolution with
    a 16×16 filter size, stride of 16, three input channels (one for each color),
    and 768 output channels (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:math>](img/858.png)).
    Applying the convolutional filter at each location produces one 768-dimensional
    patch embedding per location of the input image. Since the patches form a two-dimensional
    grid (the same as the input), the output is flattened to a one-dimensional sequence.
    This block also adds positional encoding information, which is not reflected in
    its string representation. The dropout probability of all dropout instances is
    0 because the model runs in inference rather than training mode.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ViTEmbeddings`：补丁嵌入块。它包含一个大小为16×16的2D卷积滤波器，步幅为16，三个输入通道（每个颜色一个），以及768个输出通道
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>768</mml:mn></mml:math>](img/858.png))。在每个位置应用卷积滤波器，生成每个输入图像位置的一个768维的补丁嵌入。由于补丁形成了一个二维网格（与输入图像相同），所以输出会被展平为一维序列。该块还添加了位置编码信息，这些信息在其字符串表示中没有体现。所有dropout实例的丢弃概率为0，因为该模型在推理模式下运行，而非训练模式。'
- en: '`ViTEncoder`: The main encoder model contains 12 `ViTLayer` pre-ln (`LayerNorm`)
    encoder block instances. Each contains the following:'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ViTEncoder`：主编码器模型包含12个`ViTLayer`预归一化（`LayerNorm`）编码块实例。每个实例包含以下内容：'
- en: '`ViTAttention` attention block: `ViTSelfAttention` multi-head attention and
    its output linear projection, `ViTSelfOutput`. All `ViTIntermediate` plus `GELUActivation`
    and `ViTOutput`.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ViTAttention` 注意力块：`ViTSelfAttention` 多头注意力及其输出线性投影，`ViTSelfOutput`。所有的`ViTIntermediate`加上`GELUActivation`和`ViTOutput`。'
- en: 'Classification head (`classifier`): In inference mode, the classification head
    has only one `Linear` layer with 1,000 outputs (because the model was fine-tuned
    on the ImageNet dataset).'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类头（`classifier`）：在推理模式下，分类头只有一个`Linear`层，输出1,000个结果（因为该模型在ImageNet数据集上进行了微调）。
- en: Next, let’s see how object detection with transformers works.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看使用Transformer进行目标检测是如何工作的。
- en: Understanding the DEtection TRansformer
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DEtection TRansformer
- en: '**DEtection TRansformer** (**DETR**, *End-to-End Object Detection with Transformers*,
    [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872)) introduces
    a novel transformer-based object detection algorithm.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**DEtection TRansformer** (**DETR**, *端到端目标检测与Transformer*, [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872))引入了一种基于Transformer的创新目标检测算法。'
- en: A quick recap of the YOLO object detection algorithm
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾YOLO目标检测算法
- en: We first introduced YOLO in [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
    It has three main components. The first is the backbone – that is, a CNN model
    that extracts features from the input image. Next is the neck – an intermediate
    part of the model that connects the backbone to the head. Finally, the head outputs
    the detected objects using a multi-step algorithm. More specifically, it splits
    the image into a grid of cells. Each cell contains several pre-defined anchor
    boxes with different shapes. The model predicts whether any of the anchor boxes
    contains an object and the coordinates of the object’s bounding box. Many of the
    boxes will overlap and predict the same object. The model filters the overlapping
    objects with the help of intersection-over-union and non-maximum suppression.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第 5 章*](B19627_05.xhtml#_idTextAnchor146)中首次介绍了 YOLO。它有三个主要组件。第一个是骨干网络——即一个卷积神经网络（CNN）模型，用于从输入图像中提取特征。接下来是颈部——模型的中间部分，连接骨干网络和头部。最后，头部使用多步骤算法输出检测到的物体。更具体地，它将图像划分为一个网格，每个网格包含若干个具有不同形状的预定义锚框。模型预测这些锚框中是否包含物体以及物体边界框的坐标。许多框会重叠并预测相同的物体。模型通过交并比（IoU）和非极大值抑制（NMS）帮助筛选重叠的物体。
- en: 'Like YOLO, DetR starts with a CNN backbone. However, it replaces the neck and
    the head with a full post-normalization transformer encoder-decoder. This negates
    the need for hand-designed components such as the non-maximum suppression procedure
    or anchor boxes. Instead, the model outputs a set of bounding boxes and class
    labels for the detected objects. To understand how it works, we’ll start with
    the following figure, which displays the components of DetR:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 和 YOLO 一样，DetR 也从 CNN 骨干网络开始。然而，它用一个完整的后归一化变换器编码器-解码器替代了颈部和头部。这消除了需要手工设计组件（例如非极大值抑制过程或锚框）的需求。相反，模型输出一组边界框和类标签，用于表示检测到的物体。为了理解它是如何工作的，我们从下图开始，它展示了
    DetR 的组件：
- en: '![Figure 9.3 – DetR architecture. Inspired by https://arxiv.org/abs/2005.12872](img/B19627_09_3.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – DetR 架构。灵感来源于 https://arxiv.org/abs/2005.12872](img/B19627_09_3.jpg)'
- en: Figure 9.3 – DetR architecture. Inspired by https://arxiv.org/abs/2005.12872
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – DetR 架构。灵感来源于 https://arxiv.org/abs/2005.12872
- en: First, the backbone CNN extracts the features from the input image, the same
    as in YOLO. Its outputs are the feature maps of the last convolutional layer.
    The original input image is a tensor with a shape of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/861.png),
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/862.png)
    is the number of color channels and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/863.png)/![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/864.png)
    are the image dimensions. The last convolution output is a tensor with a shape
    of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math>](img/865.png).
    Typically, the number of output feature maps is *C=2048*, and their height and
    width are ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mn>32</mml:mn></mml:math>](img/866.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mn>32</mml:mn></mml:math>](img/867.png),
    respectively.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，主干CNN从输入图像中提取特征，和YOLO中的操作相同。其输出是最后一个卷积层的特征图。原始输入图像是一个形状为![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/861.png)的张量，其中![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/862.png)是颜色通道的数量，![
- en: However, the three-dimensional (excluding the batch dimension) backbone output
    is incompatible with the expected input tensor of the encoder, which should be
    a one-dimensional input sequence of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/868.png)-sized
    embedding tensors (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo><</mml:mo><mml:mi>C</mml:mi></mml:math>](img/869.png)).
    To solve this, the model applies 1×1 bottleneck convolution, which downsamples
    the number of channels from *C* to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/870.png),
    followed by a flattening operation. The transformed tensor becomes ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math>](img/871.png),
    which we can use as a transformer input sequence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，三维（排除批次维度）主干输出与编码器预期的输入张量不兼容，后者应该是一个一维的输入序列，大小为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/868.png)大小的嵌入张量
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo><</mml:mo><mml:mi>C</mml:mi></mml:math>](img/869.png))。为了解决这个问题，模型应用了1×1瓶颈卷积，将通道数从
    *C* 降采样到 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/870.png)，然后进行展平操作。变换后的张量变为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>⋅</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math>](img/871.png)，我们可以将其作为变换器的输入序列使用。
- en: 'Next, let’s focus on the actual transformer, which is displayed in detail in
    the following figure:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注实际的变换器，其详细内容显示在下图中：
- en: '![Figure 9.4 – DetR transformer in detail. Inspired by https://arxiv.org/abs/2005.12872](img/B19627_09_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – DetR 变换器的详细结构。灵感来源于 https://arxiv.org/abs/2005.12872](img/B19627_09_4.jpg)'
- en: Figure 9.4 – DetR transformer in detail. Inspired by https://arxiv.org/abs/2005.12872
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – DetR 变换器的详细结构。灵感来源于 [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872)
- en: The encoder maps the input sequence to a sequence of continuous representations,
    just like the original encoder ([*Chapter 7*](B19627_07.xhtml#_idTextAnchor202)).
    One difference is that the model adds fixed absolute positional encodings to each
    **Q**/**K** tensor of all attention layers of the encoder, as opposed to static
    positional encodings added only to the initial input tensor of the original transformer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器将输入序列映射到一系列连续的表示，就像原始编码器一样（[*第 7 章*](B19627_07.xhtml#_idTextAnchor202)）。不同之处在于，模型在每个**Q**/**K**张量的所有注意力层中添加了固定的绝对位置编码，而原始变换器仅在初始输入张量中添加了静态位置编码。
- en: The decoder is where it gets more interesting. First, let’s note that the fixed
    positional encodings also participate in the decoder’s encoder-decoder attention
    block. Since they participate in all self-attention blocks of the encoder, we
    propagate them to the encoder-decoder attention to level the playing field.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是更有趣的地方。首先，我们注意到，固定位置编码也参与了解码器的编码器-解码器注意力块。由于它们参与了编码器的所有自注意力块，我们将它们传递到编码器-解码器注意力层，以使得各部分公平竞争。
- en: Next, the encoder takes as input a sequence of *N* **object queries**, represented
    by tensors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/872.png).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，编码器将输入一个*N*个**物体查询**的序列，这些查询由张量表示，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/872.png)。
- en: We can think of them as slots, which the model uses to detect objects. The model
    output for each input object query represents the properties (bounding box and
    class) of one detected object. Having *N* object queries means that the model
    can detect *N* objects at most. Because of this, the paper’s authors propose to
    use *N*, which is significantly larger than the typical number of objects in an
    image. Unlike the original transformer, the decoder’s attention here isn’t masked,
    so it can detect all objects in parallel rather than sequentially.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将它们视为槽位，模型利用这些槽位来检测物体。每个输入物体查询的模型输出代表一个被检测物体的属性（边界框和类别）。拥有*N*个物体查询意味着模型最多可以检测*N*个物体。正因为如此，论文的作者提出使用*N*，其值显著大于图像中通常的物体数量。与原始的变换器不同，解码器的注意力在这里没有被掩盖，因此它可以并行检测所有物体，而不是按顺序进行检测。
- en: At the start of the training process, the object query tensors are initialized
    randomly. The training itself updates both the model weights and the query tensors
    – that is, the model learns the object queries alongside the model weights. They
    act as learned positional encodings of the detected objects and serve the same
    purpose as the initial fixed input positional encodings. Because of this, we add
    the object queries to the encoder-decoder attention and the self-attention layers
    of the decoder blocks in the same way we add the input positional encodings to
    the encoder. This architecture has a sort of *bug* – the very first self-attention
    layer of the first decoder block will take as input the same object query twice,
    making it useless. Empirical experiments show that this doesn’t degrade the model
    performance. For the sake of simplicity, the implementation doesn’t have a unique
    first decoder block without self-attention but uses the standard decoder block
    instead.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程开始时，物体查询张量是随机初始化的。训练过程中会更新模型权重和查询张量——也就是说，模型在学习权重的同时也在学习物体查询。它们作为检测物体的学习型位置编码，并起到了与初始固定输入位置编码相同的作用。因此，我们将物体查询添加到编码器-解码器注意力层以及解码器块的自注意力层中，方式与将输入位置编码添加到编码器时相同。这种架构存在一个*bug*——第一个解码器块的第一个自注意力层将重复两次接收相同的物体查询作为输入，这使得该查询变得无用。实验证明，这不会降低模型的性能。为了简化实现，模型没有设计一个没有自注意力的独立第一个解码器块，而是直接使用了标准解码器块。
- en: Encoding configurations
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 编码配置
- en: 'The model can work with multiple configurations of the fixed and learned encodings:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以处理固定编码和学习编码的多种配置：
- en: add both types of encodings only to input data;
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只将两种类型的编码添加到输入数据；
- en: add the fixed encodings to the input data and the learned encodings to the input
    and all decoder attention layers;
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将固定编码添加到输入数据，并将学习到的编码添加到输入和所有解码器注意力层；
- en: add the fixed encodings to the data and all encoder attention layers and the
    learned encodings only to the decoder input;
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将固定编码添加到数据和所有编码器注意力层，只将学习到的编码添加到解码器输入；
- en: add both types of encodings to the input data and every attention layer of the
    encoder and the decoder.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将两种类型的编码添加到输入数据以及编码器和解码器的每个注意力层中。
- en: The model works best in the fourth configuration, but for the sake of simplicity,
    it can be implemented in the first.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在第四种配置下表现最佳，但为了简化起见，可以在第一种配置中实现。
- en: The object queries make it possible to not impose prior geometric limitations
    such as grid cells and anchor boxes in YOLO. Instead, we specify only the maximum
    number of objects to detect and let the model do its magic. The learned queries
    tend to specialize over different regions of the image. However, this is a result
    of the training and the properties of the training dataset, as opposed to manually
    crafted features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 物体查询使得不再强制施加诸如网格单元和锚框这样的几何限制（如在 YOLO 中）。相反，我们只指定检测的最大物体数量，并让模型发挥其魔力。学习到的查询通常会专注于图像的不同区域。然而，这归因于训练和训练数据集的特性，而不是手动设计的特征。
- en: 'The model ends with a combination of two heads: a three-layer perceptron with
    ReLU activations and a separate FC layer. The perceptron is called an FFN, which
    differs from the FFNs in the transformer blocks. It predicts the detected object
    bounding box height, width, and normalized center coordinates concerning the input
    image. The FC has softmax activation and predicts the class of the object. Like
    YOLO, it includes an additional special background class, which indicates that
    no object is detected within the slot. Having this class is even more necessary
    because some slots will inevitably be empty, as *N* is much larger than the number
    of objects in the image.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以两个头部的组合结束：一个具有 ReLU 激活的三层感知机和一个独立的全连接（FC）层。该感知机被称为 FFN，它与变换器块中的 FFN 不同。它预测检测到的物体边界框的高度、宽度和相对于输入图像的标准化中心坐标。FC
    层采用 softmax 激活，预测物体的类别。像 YOLO 一样，它包含一个额外的特殊背景类，表示在该位置没有检测到任何物体。拥有这个类别尤为必要，因为某些位置不可避免地会为空，*N*
    远大于图像中的物体数量。
- en: Predicting a set of unrestricted bounding boxes poses a challenge to the training
    because it is not trivial to match the predicted boxes with the ground-truth ones.
    The first step is to pad the ground-truth boxes for each image with dummy entries,
    so the number of ground-truth boxes becomes equal to the number of predicted ones,
    *N*. Next, the training uses one-to-one **bipartite matching** between the predicted
    and ground-truth boxes. Finally, the algorithm supervises each predicted box to
    be closer to the ground-truth box it was matched to. You can check out the paper
    for more details on the training.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 预测一组无限制的边界框给训练带来了挑战，因为将预测的边界框与真实框匹配并非易事。第一步是为每张图片的真实框添加虚拟条目，使得真实框的数量等于预测框的数量，即
    *N*。接下来，训练使用预测框与真实框之间的 **二分匹配**。最后，算法监督每个预测框接近与其匹配的真实框。你可以查阅论文以获取有关训练的更多细节。
- en: DetR for image segmentation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像分割的 DetR
- en: The authors of DetR extend the model for image segmentation. The relationship
    between DetR for detection and segmentation is similar to the one between Faster
    R-CNN and Mask R-CNN ([*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)). DetR for
    segmentation adds a third head that’s implemented with upsampling convolutions.
    It produces binary segmentation masks for each detected object in parallel. The
    final result merges all masks using pixel-wise argmax.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DetR 的作者扩展了该模型以用于图像分割。DetR 在检测和分割之间的关系类似于 Faster R-CNN 和 Mask R-CNN 之间的关系（见
    [*第 5 章*](B19627_05.xhtml#_idTextAnchor146)）。用于分割的 DetR 添加了一个第三个头部，通过上采样卷积实现。它并行生成每个检测物体的二值分割掩码。最终结果通过像素级的
    argmax 合并所有掩码。
- en: Using DetR with Hugging Face Transformers
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hugging Face Transformers 运行 DetR
- en: 'In this section, we’ll implement a basic example of DetR object detection with
    the help of Hugging Face Transformers and its `pipeline` abstraction, which we
    introduced in [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220). This example follows
    the ViT pattern, so we’ll include the full code without any comments. Here it
    is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将借助 Hugging Face Transformers 和它的 `pipeline` 抽象，实施一个基本的 DetR 物体检测示例，我们在
    [*第 8 章*](B19627_08.xhtml#_idTextAnchor220) 中介绍了该抽象。这个示例遵循 ViT 模式，因此我们将包括完整的代码，而不做任何注释。代码如下：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The last call returns a list of detected objects in the following form:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一次调用返回以下形式的检测到的物体列表：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we can see the model definition with the `print(obj_detection_pipeline.model)`
    command. Here, `in_f` and `out_f` are shortened for `in_features` and `out_features`,
    respectively. DetR uses bias in all `Linear` layers (the `bias=True` input parameter
    is not displayed). We’ll omit the backbone definition.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`print(obj_detection_pipeline.model)`命令查看模型定义。这里，`in_f`和`out_f`分别是`in_features`和`out_features`的缩写。DetR在所有`Linear`层中使用偏置（`bias=True`输入参数未显示）。我们将省略主干网络的定义。
- en: 'Let’s discuss the model elements in the order that they appear, starting with
    the 1×1 bottleneck convolution (we have ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:math>](img/873.png)):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按出现顺序讨论模型元素，从1×1瓶颈卷积开始（我们有 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:math>](img/873.png))：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we have the object query embedding (*N=100*). As we mentioned, the object
    queries are learned alongside the weight updates during training:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有对象查询嵌入（*N=100*）。正如我们所提到的，对象查询会在训练过程中与权重更新一起学习：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the encoder with six post-ln encoder blocks, ReLU activation,
    and FFN with one 2,048-dimensional hidden layer. Note that the positional encodings
    are not displayed (the same applies to the decoder):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是带有六个后置-ln编码器块、ReLU激活函数和具有一个2,048维隐藏层的FFN的编码器。注意，位置编码未显示（解码器也同样适用）：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we have the decoder with six post-ln decoder blocks and the same properties
    as the encoder:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有了解码器，带有六个后置-ln解码器块，属性与编码器相同：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we have the output FFN and linear layer. The FFN outputs four values
    (the bounding box coordinates), and the linear layer can detect 91 classes and
    the background:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了输出FFN和线性层。FFN输出四个值（边界框坐标），而线性层可以检测91个类别和背景：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, let’s see how we can generate new images with transformers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用变换器生成新图像。
- en: Generating images with stable diffusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用稳定扩散生成图像
- en: 'In this section, we’ll introduce **stable diffusion** (**SD**, *High-Resolution
    Image Synthesis with Latent Diffusion Models*, [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752),
    [https://github.com/Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion)).
    This is a generative model that can synthesize images based on text prompts or
    other types of data (in this section, we’ll focus on the text-to-image scenario).
    To understand how it works, let’s start with the following figure:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍**稳定扩散**（**SD**，*高分辨率图像合成与潜在扩散模型*，[https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)，[https://github.com/Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion)）。这是一种生成模型，可以基于文本提示或其他类型的数据合成图像（在本节中，我们将重点讨论文本到图像的场景）。为了理解它的工作原理，让我们从以下图开始：
- en: '![Figure 9.5 – Stable diffusion model and training. Inspired by https://arxiv.org/abs/2112.10752](img/B19627_09_5.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 稳定扩散模型与训练。灵感来源于 https://arxiv.org/abs/2112.10752](img/B19627_09_5.jpg)'
- en: Figure 9.5 – Stable diffusion model and training. Inspired by https://arxiv.org/abs/2112.10752
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 稳定扩散模型与训练。灵感来源于 [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)
- en: SD combines an autoencoder (**AE**, the *Pixel space* section of *Figure 9**.5*),
    denoising diffusion probabilistic models (**DDPM** or simply **DM**, the *Latent
    distribution space* section of *Figure 9**.5* and [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)),
    and transformers (the *Conditioning* section of *Figure 9**.5*). Before we dive
    into each of these components, let’s outline their role in the training and inference
    pipelines of SD. Training involves all of them – AE encoder, forward diffusion,
    reverse diffusion (**U-Net**, [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)),
    AE decoder, and conditioning. Inference (generating images from text) only involves
    reverse diffusion, AE decoder, and conditioning. Don’t worry if you don’t understand
    everything you just read, as we’ll go into more detail in the following sections.
    We’ll start with the AE, continue with the conditioning transformer, and combine
    it when we discuss the diffusion process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SD结合了自编码器（**AE**，*图9.5中的像素空间*部分），去噪扩散概率模型（**DDPM**或简写为**DM**，*图9.5中的潜在分布空间*部分，以及[*第5章*](B19627_05.xhtml#_idTextAnchor146)），和变换器（*图9.5中的条件*部分）。在我们深入讨论这些组件之前，先概述它们在SD的训练和推理管道中的作用。训练涉及所有这些组件——AE编码器、前向扩散、反向扩散（**U-Net**，[*第5章*](B19627_05.xhtml#_idTextAnchor146)）、AE解码器和条件。推理（从文本生成图像）仅涉及反向扩散、AE解码器和条件。不要担心如果你没有完全理解刚才所读内容，我们将在接下来的部分详细讨论。我们将从AE开始，接着讨论条件变换器，并在讨论扩散过程时将它们结合起来。
- en: Autoencoder
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'Although we mentioned AEs briefly in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016),
    we’ll introduce this architecture in more detail here, starting with the following
    figure:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中简要提到了自编码器（AE），但在这里我们将更详细地介绍这一架构，从以下图示开始：
- en: "![Figure 9.6 – An \uFEFFAE\uFEFF](img/B19627_09_6.jpg)"
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 一个AE](img/B19627_09_6.jpg)'
- en: Figure 9.6 – An AE
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 一个AE
- en: An AE is a feed-forward neural network that tries to reproduce its input. In
    other words, an AE’s target value (label), **y**, equals the input data, **x**.
    We can formally say that it tries to learn an identity function, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/874.png) (a function that repeats
    its input). In its most basic form, an AE consists of hidden
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AE）是一个前馈神经网络，它试图重建其输入。换句话说，AE的目标值（标签）**y**等于输入数据**x**。我们可以正式地说，它试图学习一个恒等函数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/874.png)（一个重复其输入的函数）。在最基本的形式下，自编码器由隐藏层组成。
- en: '(or bottleneck) and output layers (**W** and **W**’ are the weight matrices
    of these layers). Like U-Net, we can think of the autoencoder as a virtual composition
    of two components:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （或瓶颈）和输出层（**W**和**W**’是这些层的权重矩阵）。像U-Net一样，我们可以将自编码器看作是两个组件的虚拟组合：
- en: '**Encoder**: This maps the input data to the network’s internal latent representation.
    For the sake of simplicity, in this example, the encoder is a single FC bottleneck
    layer. The internal state is just its activation tensor, **z**. The encoder can
    have multiple hidden layers, including convolutional ones (as in SD). In this
    case, **z** is the activation of the last layer.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：它将输入数据映射到网络的内部潜在表示。为了简化起见，在这个例子中，编码器是一个单一的全连接瓶颈层。内部状态就是其激活张量**z**。编码器可以有多个隐藏层，包括卷积层（如SD中的情况）。在这种情况下，**z**是最后一层的激活。'
- en: '**Decoder**: This tries to reconstruct the input from the network’s internal
    state, **z**. The decoder can also have a complex structure that typically mirrors
    the encoder. While U-Net tries to translate the input image into a target image
    of some other domain (for example, a segmentation map), the autoencoder simply
    tries to reconstruct its input.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：它试图从网络的内部状态**z**重建输入。解码器也可以具有复杂的结构，通常与编码器相似。虽然U-Net试图将输入图像转换为其他领域的目标图像（例如，分割图），但自编码器仅仅试图重建其输入。'
- en: We can train the autoencoder by minimizing a loss function, known as the **reconstruction
    error**. It measures the distance between the original input and its reconstruction.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过最小化一个损失函数来训练自编码器，这个损失函数被称为**重构误差**。它衡量原始输入与其重构之间的距离。
- en: The latent tensor, **z**, is the focus of the entire AE. The key is that the
    bottleneck layer has fewer units than the input/output ones. Because the model
    tries to reconstruct its input from a smaller feature space, we force it to learn
    only the most important features of the data. Think of the compact data representation
    as a form of compression (but not lossless). We can use only the encoder part
    of the model to generate latent tensors for downstream tasks. Alternatively, we
    can use only the decoder to synthesize new images from generated latent tensors.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在张量**z**是整个自编码器的核心。关键在于瓶颈层的单元数少于输入/输出层的单元数。因为模型试图从较小的特征空间重构输入，我们迫使它只学习数据中最重要的特征。可以将这种紧凑的数据表示看作一种压缩形式（但不是无损的）。我们可以仅使用模型的编码器部分来生成下游任务所需的潜在张量。或者，我们可以仅使用解码器从生成的潜在张量合成新的图像。
- en: During training, the encoder maps the input sample to the latent space, where
    each latent attribute has a discrete value. An input sample can have only one
    latent representation. Therefore, the decoder can reconstruct the input in only
    one possible way. In other words, we can generate a single reconstruction of one
    input sample. However, we want to generate new images conditioned on text prompts
    rather than recreating the original ones. One possible solution to this task is
    **variational autoencoders** (**VAEs**). A VAE can describe the latent representation
    in probabilistic terms. Instead of discrete values, we’ll have a probability distribution
    for each latent attribute, making the latent space continuous. We can modify the
    latent tensor to influence the probability distribution (that is, the properties)
    of the generated image. In SD, the DM component, combined with the conditioning
    text prompts, acts as this modifier.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，编码器将输入样本映射到潜在空间，在这里每个潜在属性都有一个离散的值。一个输入样本只能有一个潜在表示。因此，解码器只能用一种可能的方式来重构输入。换句话说，我们只能生成一个输入样本的单一重构。然而，我们希望生成基于文本提示的新图像，而不是重新创建原始图像。解决此任务的一种可能方法是**变分自编码器**（**VAE**）。VAE可以用概率术语来描述潜在表示。我们将不再使用离散值，而是为每个潜在属性提供一个概率分布，从而使潜在空间变为连续的。我们可以修改潜在张量以影响生成图像的概率分布（即属性）。在SD中，DM组件与条件文本提示相结合，充当这个修改器。
- en: With this short detour completed, let’s discuss the role of the convolutional
    encoder in SD (the *Pixel space* section of *Figure 9**.5*). During training,
    the AE encoder creates a compressed initial latent representation tensor, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math>](img/875.png),
    of the input image, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/876.png).
    More specifically, the encoder downsamples the image by a factor, *f = H/h = W/w*,
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math>](img/877.png)
    (*m* is an integer selected by empirical experiments). Then, the entire diffusion
    process (forward and reverse) works with the compressed **z** rather than the
    original image, **x**. Only when the reverse diffusion ends does the AE decoder
    upsample the newly generated representation, **z**, into the final generated image,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math>](img/878.png).
    In this way, the smaller **z** allows the use of a smaller and more computationally
    efficient U-Net, which benefits both the training and the inference. The paper’s
    authors refer to this combination of AEs and diffusion models as **latent** **diffusion
    models**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个简短的绕道后，我们来讨论卷积编码器在SD中的作用（*像素空间*部分，见*图9.5*）。在训练过程中，AE编码器创建了一个压缩的初始潜在表示张量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math>](img/875.png)，来自输入图像，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/876.png)。更具体地说，编码器将图像按因子进行下采样，*f
    = H/h = W/w*，其中 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math>](img/877.png)
    (*m*是通过经验实验选择的整数)。然后，整个扩散过程（前向和反向）使用压缩后的**z**，而不是原始图像**x**。只有当反向扩散结束时，AE解码器才会将新生成的表示**z**上采样成最终生成的图像，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math>](img/878.png)。通过这种方式，更小的**z**允许使用更小、更高效的计算U-Net，这对训练和推理都有好处。论文的作者将这种AE与扩散模型的结合称为**潜在扩散模型**。
- en: The AE training is separate from the U-Net training. Because of this, we can
    train the AE once and then use it for multiple downstream tasks with different
    U-Net configurations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: AE训练与U-Net训练是分开的。因此，我们可以先训练AE，然后用它在不同的U-Net配置下进行多个下游任务。
- en: Conditioning transformer
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件变换器
- en: 'The conditioning transformer, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/879.png)
    (*Figure 9**.5*), produces a latent representation of the text description of
    the desired image. SD provides this representation to the U-Net so that it can
    influence its output. For this to work, the text latent representation has to
    live in the same semantic (not just dimensional) space as the image latent representation
    of the U-Net. To achieve this, the latest version of SD, 2.1, uses the OpenCLIP
    open source model as a conditioning transformer (*Reproducible scaling laws for
    contrastive language-image learning*, https://arxiv.org/abs/2212.07143). **CLIP**
    stands for **contrastive language-image pre-training**. This technique was introduced
    by OpenAI (*Learning Transferable Visual Models From Natural Language Supervision*,
    https://arxiv.org/abs/2103.00020). Let’s discuss it in more detail, starting with
    the following figure:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 条件变换器，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/879.png)
    (*图 9.5*)，生成所需图像的文本描述的潜在表示。SD 将此表示提供给 U-Net，以便它可以影响其输出。为了使这一点生效，文本的潜在表示必须与 U-Net
    的图像潜在表示处于相同的语义（不仅仅是维度）空间中。为此，SD 的最新版本 2.1 使用 OpenCLIP 开源模型作为条件变换器（*可重复的对比语言-图像学习的规模定律*，https://arxiv.org/abs/2212.07143）。**CLIP**
    代表 **对比语言-图像预训练**。这一技术由 OpenAI 提出（*从自然语言监督中学习可迁移的视觉模型*，https://arxiv.org/abs/2103.00020）。让我们从下面的图开始更详细地讨论：
- en: '![Figure 9.7 – CLIP. Inspired by https://arxiv.org/abs/2103.00020](img/B19627_09_7.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – CLIP。灵感来源于 https://arxiv.org/abs/2103.00020](img/B19627_09_7.jpg)'
- en: Figure 9.7 – CLIP. Inspired by https://arxiv.org/abs/2103.00020
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – CLIP。灵感来源于 https://arxiv.org/abs/2103.00020
- en: 'It has two main components:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 它有两个主要组成部分：
- en: '`[EOS]` token. The model output at this token serves as an embedding vector
    of the entire sequence. In the context of SD, we’re only interested in the text
    encoder, and all other components of the CLIP system are only necessary for its
    training.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[EOS]` 标记。该标记处的模型输出作为整个序列的嵌入向量。在 SD 的背景下，我们只关注文本编码器，CLIP 系统的所有其他组件仅在其训练时才是必要的。'
- en: '**Image encoder**: This is either a ViT or a CNN (most often ResNet). It takes
    an image as input and outputs its embedding vector, **i**. Like the text encoder,
    this is the activation of the highest layer of the model and not a task-specific
    head.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像编码器**：这可以是 ViT 或 CNN（最常见的是 ResNet）。它以图像作为输入，输出其嵌入向量，**i**。与文本编码器类似，这也是模型最高层的激活值，而不是任务特定的头部。'
- en: For CLIP to work, the embedding vectors of the two encoders must have the same
    size, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/870.png).
    When necessary (for example, in the case of the CNN image encoder), the encoder’s
    output tensor is flattened to a one-dimensional vector. If the dimensions of the
    two encoders still differ, we can add linear projections (FC layers) to equalize
    them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 CLIP 起作用，两个编码器的嵌入向量必须具有相同的大小，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/870.png)。如果有必要（例如，在
    CNN 图像编码器的情况下），编码器的输出张量会被展平为一维向量。如果两个编码器的维度仍然不同，我们可以添加线性投影（FC 层）来使它们相等。
- en: Next, let’s focus on the actual pre-training algorithm. The training set contains
    *N* text-image pairs, where the text of each pair describes the content of its
    corresponding image. We feed all text representations to the text encoder and
    the images to the image encoder to produce the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math>](img/881.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math>](img/882.png)
    embeddings, respectively. Then, we compute a cosine similarity between every two
    embedding vectors (a total of *N×N* similarity measurements). Within these measurements,
    we have *N* correctly matching text-image pairs (the table diagonal of *Figure
    9**.5*) and *N×N-N* incorrect pairs (all pairs outside the table diagonal). The
    training updates the weights of the two encoders so that the similarity scores
    for the correct pairs are maximized and the incorrect ones are minimized. Should
    the training prove successful, we’ll have similar embeddings for text prompts
    that correctly describe what’s on the image and dissimilar embeddings in all other
    cases. During SD training, we optimize the text encoder alongside the U-Net (but
    not the full CLIP system).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们专注于实际的预训练算法。训练集包含*N*个文本-图像对，其中每对的文本描述对应图像的内容。我们将所有文本表示输入文本编码器，将图像输入图像编码器，分别产生![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math>](img/881.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math>](img/882.png)
    的嵌入。然后，我们计算每两个嵌入向量之间的余弦相似度（共*N×N*个相似度测量）。在这些测量中，我们有*N*个正确匹配的文本-图像对（*图9**.5*的表对角线）和*N×N-N*个不正确的对（表对角线之外的所有对）。训练更新两个编码器的权重，以使正确对的相似度分数最大化，不正确对的分数最小化。如果训练成功，我们将获得对于正确描述图像内容的文本提示的相似嵌入，并且在所有其他情况下具有不相似的嵌入。在SD训练期间，我们优化文本编码器以及U-Net（但不包括完整的CLIP系统）。
- en: Now that we know how to produce semantically correct text embeddings, we can
    proceed with the actual diffusion model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何生成语义上正确的文本嵌入，我们可以继续进行实际的扩散模型。
- en: Diffusion model
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型
- en: DM is a type of generative model that has forward and reverse phases. Forward
    diffusion starts with the latent vector, **z**, produced by the AE encoder (which
    takes an image, **x**, as input). Then, it gradually adds random Gaussian noise
    to **z** through a series of *T* steps until the final (latent) representation,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/883.png),
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: DM是一种具有正向和反向阶段的生成模型。前向扩散从由AE编码器产生的潜在向量**z**开始（接受图像**x**作为输入）。然后，通过一系列*T*步骤逐渐向**z**添加随机高斯噪声，直到最终的（潜在的）表示，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/883.png)。
- en: is pure noise. Forward diffusion uses an accelerated algorithm, which produces
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/884.png)
    in a single step instead of *T* steps ([*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 是纯噪声。前向扩散使用加速算法，在一个步骤中生成![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/884.png)，而不是*T*步骤（[*第5章*](B19627_05.xhtml#_idTextAnchor146)）。
- en: Reverse diffusion does the opposite and starts with pure noise. It gradually
    tries to restore the original latent tensor, **z**, by removing small amounts
    of noise in a series of *T* denoising steps. In practice, we’re interested in
    reverse diffusion to generate images based on latent representations (forward
    diffusion only participates in the training). It is usually implemented with a
    U-Net type of CNN (*Figure 9**.5*, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/885.png)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 反向扩散与此相反，从纯噪声开始。它逐渐通过在一系列*T*去噪步骤中去除少量噪声来恢复原始的潜在张量**z**。实际上，我们更关心反向扩散，它是基于潜在表示生成图像（前向扩散只参与训练）。它通常使用U-Net类型的CNN来实现（*图9.5*，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/885.png)）。
- en: It takes the noise tensor, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/540.png),
    at step *t* as input and outputs an approximation of the noise added to the original
    latent tensor, **z** (that is, only the noise and not the tensor itself). Then,
    we subtract the predicted noise from the current U-Net input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/887.png),
    and feed the result as new input to the U-Net, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/888.png).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它以噪声张量![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/540.png)作为输入，并输出对添加到原始潜在张量**z**中的噪声的近似值（即仅噪声，而不是张量本身）。然后，我们从当前的U-Net输入中减去预测的噪声，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/887.png)，并将结果作为新的输入传递给U-Net，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/888.png)。
- en: During training, the cost function measures the difference between the predicted
    and actual noise and accordingly updates the U-Net weights after each denoising
    step. This process continues until (hopefully) only the original tensor, **z**,
    remains. Then, the AE decoder uses it to produce the final image.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，代价函数衡量预测噪声与实际噪声之间的差异，并在每次去噪步骤后相应地更新U-Net的权重。这个过程持续进行，直到（希望）仅剩下原始张量**z**。然后，AE解码器使用它生成最终图像。
- en: 'The pure form of DM has no way to influence the properties of the generated
    image (this is known as conditioning) because we start with random noise, which
    results in random images. SD allows us to do just that – a way to condition the
    U-Net to generate images based on specific text prompts or other data types. To
    do this, we need to integrate the output embedding of the conditioning transformer
    with the denoising U-Net. Let’s assume that we have a text prompt, `[EOS]` token.
    Then, we map its output to the intermediate layers of the U-Net via a **cross-attention**
    layer. In this layer, the key and value tensors represent the conditioning transformer
    outputs, and the query tensors represent the intermediate U-Net layers (*Figure
    9**.5*):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DM的纯粹形式无法影响生成图像的属性（这被称为条件化），因为我们从随机噪声开始，导致生成的是随机图像。SD允许我们做到这一点——一种方法可以将U-Net条件化，以根据特定的文本提示或其他数据类型生成图像。为了实现这一点，我们需要将条件化变换器的输出嵌入与去噪U-Net结合。假设我们有一个文本提示`[EOS]`标记，那么我们通过**交叉注意力**层将其输出映射到U-Net的中间层。在这一层中，键和值张量表示条件化变换器的输出，而查询张量表示U-Net的中间层（*图9.5*）：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/892.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/892.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">K</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/893.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">K</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/893.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/894.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/894.png)'
- en: 'Here, *i* is the *i*-th intermediate U-Net layer, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/895.png)
    is the flattened activation of that layer, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:math>](img/896.png)
    is the flattened activation tensor size. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/897.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/898.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/899.png)
    are learnable projection matrices, where *d* is the chosen size of the actual
    cross-attention embedding. We have a unique set of three matrices for each of
    the *i* intermediate U-Net layers with cross-attention. In its simplest form,
    we can add one or more cross-attention blocks after the output of an intermediate
    U-Net layer. The blocks can have a residual connection, which preserves the unmodified
    intermediate layer output and augments it with the attention vector. Note that
    the output of the intermediate convolutional layers has four dimensions: `[batch,
    channel, height, width]`. However, the standard attention blocks use two-dimensional
    input: `[batch, dim]`. One solution is to flatten the convolutional output before
    feeding it to the attention block. Alternatively, we can preserve the channel
    dimension and only flatten the height and width: `[batch, channel, height*width]`.
    In this case, we can assign one attention head to the output of each convolutional
    channel.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*i* 是第 *i* 个中间 U-Net 层，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/895.png)
    是该层的展平激活值，且 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:math>](img/896.png)
    是展平激活张量的大小。![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/897.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/898.png)，以及
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/899.png)
    是可学习的投影矩阵，其中 *d* 是所选择的实际交叉注意嵌入的大小。对于每个带有交叉注意力的 *i* 中间 U-Net 层，我们有一组独特的三维矩阵。最简单的形式是，在中间
    U-Net 层的输出之后添加一个或多个交叉注意力模块。这些模块可以具有残差连接，保留未修改的中间层输出并通过注意力向量进行增强。请注意，中间卷积层的输出有四个维度：`[batch,
    channel, height, width]`。然而，标准的注意力模块使用二维输入：`[batch, dim]`。一种解决方案是，在将其输入到注意力模块之前，先展平卷积输出。或者，我们可以保留通道维度，只展平高度和宽度：`[batch,
    channel, height*width]`。在这种情况下，我们可以将每个卷积通道的输出分配给一个注意力头。
- en: Note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 9**.5* has a *switch* component, which allows us to concatenate the
    text prompt representation and the U-Net input rather than using cross-attention
    in the intermediate layers. This use case is for tasks other than text-to-image,
    which is the focus of this section.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.5*有一个*开关*组件，它允许我们将文本提示表示与U-Net输入连接，而不是在中间层使用交叉注意力。这个用例适用于文本到图像以外的任务，这是本节的重点。'
- en: With that, let’s see how to use SD in practice.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何实际使用SD。
- en: Using stable diffusion with Hugging Face Transformers
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face Transformers进行稳定扩散
- en: 'In this section, we’ll use SD to generate an image conditioned on a text prompt.
    In addition to the Transformers library, we’ll also need **Diffusers** (https://github.com/huggingface/diffusers)
    – a library for pre-trained diffusion models for generating images and audio.
    Please note that the diffusers SD implementation requires the presence of a GPU.
    You can run this example in the Google Colab notebook with GPU enabled. Let’s
    start:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用SD生成一个基于文本提示的图像。除了Transformers库外，我们还需要**Diffusers**（https://github.com/huggingface/diffusers）—一个用于生成图像和音频的预训练扩散模型的库。请注意，Diffusers的SD实现要求有GPU。您可以在启用GPU的Google
    Colab笔记本中运行此示例。让我们开始：
- en: 'Do the necessary imports:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行必要的导入：
- en: '[PRE12]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Instantiate an SD pipeline (`sd_pipe`) with SD version 2.1\. We don’t use the
    main transformers `pipeline` abstraction, which we used in the preceding examples.
    Instead, we use `StableDiffusionPipeline`, which comes from the `diffusers` library.
    We’ll also move the model to a `cuda` device (an NVIDIA GPU) if it is available:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SD版本2.1实例化SD管道（`sd_pipe`）。我们不使用前面例子中使用的主变换器`pipeline`抽象。相反，我们使用来自`diffusers`库的`StableDiffusionPipeline`。如果有可用的`cuda`设备（NVIDIA
    GPU），我们还将把模型移动到该设备：
- en: '[PRE13]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s run `sd_pipe` for 100 denoising steps with the following text prompt:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行`sd_pipe`进行100次去噪步骤，并使用以下文本提示：
- en: '[PRE14]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The generated `image` is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的`image`如下：
- en: '![Figure 9.8 – SD-generated image](img/B19627_09_8.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – SD生成的图像](img/B19627_09_8.jpg)'
- en: Figure 9.8 – SD-generated image
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – SD生成的图像
- en: 'Unfortunately, the AE, U-Net, and conditioning transformer descriptions are
    large, and it would be impractical to include them here. Still, they are available
    in the Jupyter Notebook. Nevertheless, we can see a shortened summary of the entire
    SD pipeline with the `print(sd_pipe)` command:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，AE、U-Net和条件变换器的描述很长，包含在此处不太实际。不过，它们可以在Jupyter Notebook中查看。尽管如此，我们仍然可以通过`print(sd_pipe)`命令看到整个SD管道的简要总结：
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, `transformers` and `diffusers` refer to the package of origin for the
    given component.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`transformers`和`diffusers`指的是给定组件的源包。
- en: The first component is an optional `safety_checker` (not initialized), which
    can identify **not-safe-for-work** (**NSFW**) images.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个组件是一个可选的`safety_checker`（未初始化），它可以识别**不适合工作环境**（**NSFW**）的图像。
- en: Next, we have a BPE-based `CLIPTokenizer` `tokenizer`, with a token vocabulary
    size of around 50,000 tokens. It tokenizes the text prompt and feeds it to `text_encoder`
    of `CLIPTextModel`. The Hugging Face `CLIPTextModel` duplicates the OpenAI CLIP
    transformer-decoder (the model card is available at https://huggingface.co/openai/clip-vit-large-patch14).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个基于BPE的`CLIPTokenizer` `tokenizer`，它的词汇表大小约为50,000个词汇。它将文本提示进行标记化并传递给`CLIPTextModel`的`text_encoder`。Hugging
    Face的`CLIPTextModel`复制了OpenAI CLIP变换器解码器（模型卡可以在 https://huggingface.co/openai/clip-vit-large-patch14
    查阅）。
- en: Then, we have `UNet2DConditionModel`. The convolutional portions of the U-Net
    use residual blocks ([*Chapter 4*](B19627_04.xhtml#_idTextAnchor107)). It has
    four downsampling blocks with a downsampling factor of 2 (implemented with convolutions
    with stride 2). The first three include `text_encoder` cross-attention layers.
    Then, we have a single mid-block, which preserves input size and contains one
    residual and one cross-attention sublayer. The model ends with four skip-connected
    upsampling blocks, symmetrical to the downsampling sequence. The last three blocks
    also include cross-attention layers. The model uses **sigmoid linear unit** (**SiLU**,
    [*Chapter* *3*](B19627_03.xhtml#_idTextAnchor079)) activations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有`UNet2DConditionModel`。U-Net的卷积部分使用残差块（见[*第4章*](B19627_04.xhtml#_idTextAnchor107)）。它有四个下采样块，下采样因子为2（通过步长为2的卷积实现）。前三个块包括`text_encoder`交叉注意力层。然后，我们有一个单一的中间块，它保持输入大小，并包含一个残差层和一个交叉注意力子层。模型以四个跳跃连接的上采样块结束，结构上与下采样序列对称。最后三个块也包括交叉注意力层。模型使用**sigmoid线性单元**（**SiLU**，见[*第3章*](B19627_03.xhtml#_idTextAnchor079)）激活函数。
- en: Next, we have the convolutional autoencoder, `AutoencoderKL`, with four downsampling
    residual blocks, one residual mid-block (the same as the one in U-Net), four upsampling
    residual blocks (symmetrical to the downsampling sequence), and SiLU activations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有卷积自编码器`AutoencoderKL`，包含四个下采样残差块，一个残差中间块（与U-Net中的相同），四个上采样残差块（与下采样序列对称），以及SiLU激活函数。
- en: Finally, let’s focus on `scheduler` of `DDIMScheduler`, which is part of the
    `diffusers` library. It is one of multiple available schedulers. During training,
    a scheduler adds noise to a sample to train the DM. It defines how to update the
    latent tensor based on the U-Net output during inference.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们聚焦于`DDIMScheduler`的`scheduler`，它是`diffusers`库的一部分。这是多个可用调度器之一。在训练过程中，调度器会向样本添加噪声，以训练DM。它定义了在推理期间如何根据U-Net的输出更新潜在张量。
- en: Stable Diffusion XL
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion XL
- en: 'Recently, Stability AI released Stable Diffusion XL (*SDXL: Improving Latent
    Diffusion Models for High-Resolution Image Synthesis*, [https://arxiv.org/abs/2307.01952](https://arxiv.org/abs/2307.01952)).
    SDXL uses a three times larger U-Net. The larger size is due to more attention
    blocks and a larger attention context (the new version uses the concatenated outputs
    of two different text encoders). It also utilizes an optional **refinement model**
    (**refiner**) – a second U-Net in the same latent space as the first, specializing
    in high-quality, high-resolution data. It takes the output latent representation,
    **z**, of the first U-Net as input and uses the same conditioning text prompt.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Stability AI发布了Stable Diffusion XL（*SDXL：改进高分辨率图像合成的潜在扩散模型*，[https://arxiv.org/abs/2307.01952](https://arxiv.org/abs/2307.01952)）。SDXL使用了三倍大的U-Net。更大的尺寸源于更多的注意力块和更大的注意力上下文（新版本使用了两个不同文本编码器的连接输出）。它还利用了一个可选的**精炼模型**（**refiner**）——第二个U-Net与第一个处于相同的潜在空间，专注于高质量、高分辨率的数据。它将第一个U-Net的输出潜在表示**z**作为输入，并使用相同的条件文本提示。
- en: With that, we’ve concluded our introduction to SD and the larger topic of transformers
    for computer vision. Next, let’s see how to fine-tune a transformer-based model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已完成对SD的介绍，以及计算机视觉中变压器的更大主题。接下来，让我们看看如何微调基于变压器的模型。
- en: Exploring fine-tuning transformers
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索微调变压器
- en: 'In this section, we’ll use PyTorch to fine-tune a pre-trained transformer.
    More specifically, we’ll fine-tune a `Trainer` class ([https://huggingface.co/docs/transformers/main_classes/trainer](https://huggingface.co/docs/transformers/main_classes/trainer)),
    which implements the basic training loop, model evaluation, distributed training
    on multiple GPUs/TPUs, mixed precision, and other training features. This is opposed
    to implementing the training from scratch, as we’ve been doing until now in our
    PyTorch examples. We’ll also need the **Datasets** ([https://github.com/huggingface/datasets](https://github.com/huggingface/datasets))
    and **Evaluate** ([https://github.com/huggingfahttps://github.com/huggingface/evaluate](https://github.com/huggingfahttps://github.com/huggingface/evaluate))
    packages. Let’s start:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用PyTorch微调一个预训练的变压器。更具体地，我们将微调一个`Trainer`类（[https://huggingface.co/docs/transformers/main_classes/trainer](https://huggingface.co/docs/transformers/main_classes/trainer)），它实现了基本的训练循环、模型评估、在多个GPU/TPU上的分布式训练、混合精度和其他训练特性。这与我们目前在PyTorch示例中所做的从头开始实现训练相对立。我们还需要**Datasets**（[https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)）和**Evaluate**（[https://github.com/huggingfahttps://github.com/huggingface/evaluate](https://github.com/huggingfahttps://github.com/huggingface/evaluate)）包。让我们开始：
- en: 'Load the dataset, which is split into `train`, `validation`, and `test` portions:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集，该数据集被划分为`train`、`validation`和`test`部分：
- en: '[PRE16]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Load the DistilBERT WordPiece subword `tokenizer`:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载DistilBERT WordPiece子词`tokenizer`：
- en: '[PRE17]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use `tokenizer` to tokenize the dataset. In addition, it’ll pad or truncate
    each sample to the maximum length accepted by the model. The `batched=True` mapping
    speeds up processing by combining the data in batches (as opposed to single samples).
    The `Tokenizers` library works faster with batches because it parallelizes the
    tokenization of all the examples in a batch:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tokenizer`对数据集进行分词。此外，它将对每个样本进行填充或截断，直到符合模型所接受的最大长度。`batched=True`映射通过将数据合并成批次（而不是单个样本）来加速处理。`Tokenizers`库在批量处理时运行更快，因为它并行化了批次中所有示例的分词过程：
- en: '[PRE18]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load the transformer `model`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载变压器`model`：
- en: '[PRE19]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `AutoModelForSequenceClassification` class loads DistilBERT configuration
    for binary classification – the model head has a hidden layer and an output layer
    with two units. This configuration works for our task because we have to classify
    the movie reviews into two categories.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`AutoModelForSequenceClassification`类加载DistilBERT配置，用于二分类任务——该模型头部有一个隐藏层和一个带有两个单元的输出层。此配置适用于我们的任务，因为我们需要将电影评论分类为两类。'
- en: 'Initialize `TrainingArguments` of the `Trainer` instance. We’ll specify `output_dir`
    for the location of the model predictions and checkpoints. We’ll also run the
    evaluation once per epoch:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`Trainer`实例的`TrainingArguments`。我们将指定`output_dir`，作为模型预测和检查点的存储位置。我们还将每个周期运行一次评估：
- en: '[PRE20]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Initialize the `accuracy` evaluation metric:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`accuracy`评估指标：
- en: '[PRE21]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Initialize `trainer` with all the necessary components for training and evaluation:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`trainer`，包括训练和评估所需的所有组件：
- en: '[PRE22]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It accepts the model, train, and evaluation datasets and the `training_args`
    instance. The `compute_metrics` function will compute the validation accuracy
    after each epoch. `preprocess_logits_for_metrics` will convert the one-hot encoded
    model output (`x[0]`) to indexed labels so that it can match the format of the
    ground-truth labels (`x[1]`) in the `compute_metrics` function.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它接受模型、训练和评估数据集，以及`training_args`实例。`compute_metrics`函数将在每个周期后计算验证准确率。`preprocess_logits_for_metrics`会将经过独热编码的模型输出（`x[0]`）转换为索引标签，以便与`compute_metrics`函数中的真实标签（`x[1]`）格式匹配。
- en: 'Finally, we can run the training:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以开始训练：
- en: '[PRE23]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The model will achieve around 85% accuracy in three epochs.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型将在三个训练周期内达到大约85%的准确率。
- en: Next, let’s see how to harness the power of LLMs with the LangChain framework.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用LangChain框架释放LLM的强大能力。
- en: Harnessing the power of LLMs with LangChain
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用LangChain释放LLM的强大能力
- en: 'LLMs are powerful tools, yet they have some limitations. One of them is the
    context window length. For example, the maximum input sequence of Llama 2 is 4,096
    tokens and even less in terms of words. As a reference, most of the chapters in
    this book hover around 10,000 words. Many tasks wouldn’t fit this length. Another
    LLM limitation is that its entire knowledge is stored within the model weights
    at training time. It has no direct way to interact with external data sources,
    such as databases or service APIs. Therefore, the knowledge can be outdated or
    insufficient. The **LangChain** framework can help us alleviate these issues.
    It does so with the following modules:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是强大的工具，但它们也有一些局限性。其中之一是上下文窗口的长度。例如，Llama 2的最大输入序列为4,096个令牌，按单词计数则更少。作为参考，本书大部分章节大约为10,000个单词。许多任务的长度会超出这个限制。另一个LLM的局限性是，它的所有知识都储存在训练时的模型权重中。它没有直接与外部数据源（如数据库或服务API）交互的方式。因此，知识可能会过时或不足。**LangChain**框架可以帮助我们缓解这些问题。它通过以下模块实现：
- en: '**Model I/O**: The framework differentiates between classic LLMs and chat models.
    In the first case, we can prompt the model with a single prompt, and it will generate
    a response. The second case is more interactive – it presumes a back-and-forth
    communication between the human and the model in a chat form. Internally, both
    are LLMs; the difference comes from using different APIs. Regardless of the model
    type, a token sequence is the only way to feed it with input data. The I/O module
    provides helper prompt templates for different use cases. For example, the chat
    template maintains an explicit list of all messages instead of concatenating them
    in a single sequence. We also have a few-shot template, which provides an interface
    to include one or more instructive input/output examples within the input query.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型输入/输出**：该框架区分经典的LLM和聊天模型。在第一种情况下，我们可以通过一个简单的提示来激活模型，模型会生成一个响应。第二种情况则更具互动性——它假设人类和模型之间会有来回的交流。内部来说，两者都是LLM，区别在于使用了不同的API。无论模型类型如何，令牌序列都是输入数据的唯一方式。I/O模块为不同的用例提供了辅助的提示模板。例如，聊天模板会维持一个明确的所有消息列表，而不是将它们合并为一个单一的序列。我们还提供了一个少量示例模板，它为输入查询提供了一个接口，可以在查询中包含一个或多个指令性输入/输出示例。'
- en: The module can also parse the model output (a token sequence converted into
    words). For example, if the output is a JSON string, a JSON parser can convert
    it into an actual JSON object.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模块还可以解析模型输出（将令牌序列转换为单词）。例如，如果输出是一个JSON字符串，JSON解析器可以将其转换为实际的JSON对象。
- en: '**Retrieval**: This retrieves external data and feeds it to the model input
    sequence. Its most basic function is to parse file formats such as CSV and JSON.
    It can also split larger documents into chunks if they don’t fit within the context
    window size.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**：这是用来获取外部数据并将其输入到模型序列中的功能。其最基本的功能是解析文件格式，例如 CSV 和 JSON。如果文档过大而无法适应上下文窗口大小，它还可以将文档拆分成更小的块。'
- en: Vector databases
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库
- en: The primary output of an LLM and other neural networks (before any task-specific
    heads) are embedding vectors, which we use for downstream tasks such as classification
    or text generation. The universal nature of this data format has led to the creation
    of vector-specific databases (or stores). As the name suggests, these stores only
    work with vectors and support fast vector operations, such as different similarity
    measures over the whole database. We can query an input embedding vector against
    all other database vectors and find the most similar ones. This concept is similar
    to the **Q**/**K**/**V** attention mechanism but in an external database form,
    which allows it to work with a larger dataset than in-memory attention.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 和其他神经网络的主要输出（在任何任务特定头部之前）是嵌入向量，我们将这些向量用于下游任务，如分类或文本生成。该数据格式的通用性促使了向量特定数据库（或存储）的创建。如其名称所示，这些存储仅与向量一起使用，并支持快速的向量操作，例如对整个数据库执行不同的相似度度量。我们可以查询输入的嵌入向量与数据库中所有其他向量进行比较，找到最相似的向量。这个概念类似于
    **Q**/**K**/**V** 注意力机制，但它是一种外部数据库形式，可以处理比内存注意力更多的数据集。
- en: This retrieval module has integrations with multiple vector databases. This
    way, we can use the LLM to generate and store document embeddings (the document
    acts as an input sequence). Later, we can query the LLM to generate a new embedding
    for a given query and compare this query against the database to find the nearest
    matches. In this scenario, the role of the LLM is limited to generating vector
    embeddings.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该检索模块与多个向量数据库集成。这样，我们就可以使用 LLM 生成并存储文档嵌入（文档作为输入序列）。随后，我们可以查询 LLM 为给定查询生成新的嵌入，并将此查询与数据库进行比较，找到最相似的匹配项。在这种情况下，LLM
    的作用仅限于生成向量嵌入。
- en: '**Chains**: These are mechanisms that combine multiple LangChain components
    to create a single application. For example, we can create a chain that takes
    user input, formats it with a special prompt template, feeds it to an LLM, and
    parses the LLM output to JSON. We can branch chains or combine multiple chains.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链**：链是将多个 LangChain 组件组合起来，创建单一应用程序的机制。例如，我们可以创建一个链，它接收用户输入，使用特殊的提示模板格式化输入，将其传递给
    LLM，并将 LLM 输出解析为 JSON。我们还可以分支链或组合多个链。'
- en: '**Memory**: This maintains the input token sequence throughout a chain of steps
    or model interactions with the outside world, which can modify and extend the
    sequence dynamically. It can also use the emerging LLM abilities to create a shortened
    summary of the current historical sequence. The shortened version replaces the
    original in the input token sequence for future inputs. This compression allows
    us to use the context window more efficiently and store more information.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：记忆保持输入的标记序列，贯穿整个步骤链或模型与外部世界的交互过程中，它可以动态地修改和扩展该序列。它还可以利用新兴的 LLM 能力来创建当前历史序列的简短摘要。这个简短版本会替代原始输入序列中的内容，用于未来的输入。这种压缩使我们能够更高效地利用上下文窗口，并存储更多信息。'
- en: '**Agents**: An agent is an entity that can take actions that interact with
    the environment. In the current context, an LLM acts as the agent’s reasoning
    engine to determine which actions the agent is to take and in which order. To
    help with this task, the agent/LLM can use special functions called **tools**.
    These can be generic utilities (for example, API calls), other chains, or even
    agents.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：代理是可以采取与环境交互的行动的实体。在当前上下文中，LLM 作为代理的推理引擎，决定代理应该采取哪些行动以及按什么顺序执行。为了帮助完成这一任务，代理/LLM
    可以使用特殊的功能，称为 **工具**。这些工具可以是通用实用程序（例如 API 调用）、其他链，甚至是代理。'
- en: '**Callbacks**: We can use callbacks to plug into various points of the LLM
    application, which is useful for logging, monitoring, or streaming.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回调**：我们可以使用回调函数来插入 LLM 应用程序的不同点，这对于日志记录、监控或流式处理非常有用。'
- en: Next, let’s solidify our understanding of LangChain with an example.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过一个示例加深对 LangChain 的理解。
- en: Using LangChain in practice
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际应用 LangChain
- en: 'In this section, we’ll use LangChain, LangChain Experimental ([https://github.com/langchain-ai/langchain/tree/master/libs/experimental](https://github.com/langchain-ai/langchain/tree/master/libs/experimental)),
    and OpenAI’s `gpt-3.5-turbo` model to answer the question: *What are the sum of
    the elevations of the deepest section of the ocean and the highest peak on Earth?
    Use metric units only*. To make things more interesting, we won’t let an LLM generate
    the output one word at a time. Instead, we’ll ask it to break up the solution
    into steps and use data lookup and calculations to find the right answer.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用LangChain、LangChain Experimental（[https://github.com/langchain-ai/langchain/tree/master/libs/experimental](https://github.com/langchain-ai/langchain/tree/master/libs/experimental)）以及OpenAI的`gpt-3.5-turbo`模型来回答这个问题：*海洋最深处和地球最高峰的海拔总和是多少？仅使用公制单位*。为了让事情更有趣，我们不会让LLM一次生成一个词。相反，我们将要求它将解决方案拆分为步骤，并使用数据查找和计算来找到正确答案。
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This example is partially based on https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute.
    It requires access to the OpenAI API ([https://platform.openai.com/](https://platform.openai.com/))
    and SerpAPI ([https://serpapi.com/](https://serpapi.com/)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例部分基于[https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute](https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute)。它需要访问OpenAI
    API（[https://platform.openai.com/](https://platform.openai.com/)）和SerpAPI（[https://serpapi.com/](https://serpapi.com/)）。
- en: 'Let’s start:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始：
- en: 'Initialize LangChain’s API wrapper for the `gpt-3.5-turbo` model. The `temperature`
    parameter (in the [0,1] range) determines how the model selects the next token.
    For example, if `temperature=0`, it will always output the highest probability
    token. The closer `temperature` is to 1, the more likely it is that the model
    selects a token with a lower probability:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化LangChain的API包装器以使用`gpt-3.5-turbo`模型。`temperature`参数（在[0,1]范围内）决定模型如何选择下一个token。例如，如果`temperature=0`，它将始终输出概率最高的token。`temperature`越接近1，模型选择低概率token的可能性就越大：
- en: '[PRE24]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the tools that will help us solve this task. First is the search tool,
    which uses SerpAPI to perform Google searches. This allows the LLM to query Google
    for the elevations of the deepest part of the ocean and the highest mountain in
    our question:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将帮助我们解决此任务的工具。首先是搜索工具，它使用SerpAPI执行Google搜索。这允许LLM查询Google，以获取我们问题中关于海洋最深处和最高山峰的海拔：
- en: '[PRE25]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next is the calculator tool, which will allow the LLM to compute the sum of
    the elevations. This tool uses a special few-shot learning LangChain `PromptTemplate`
    to query the LLM to calculate mathematical equations:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是计算器工具，它将允许LLM计算海拔的总和。该工具使用一个特殊的少样本学习LangChain `PromptTemplate`来查询LLM计算数学方程式：
- en: '[PRE26]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Initialize a special `PlanAndExecute` `agent`. It accepts the LLM, the tools
    we just defined, as well as `planner` and `executor` agents, as arguments:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个特殊的`PlanAndExecute` `agent`。它接受LLM、我们刚才定义的工具，以及`planner`和`executor`代理作为参数：
- en: '[PRE27]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`planner` uses a special LangChain text prompt template that queries the LLM
    model to break up the solution of the task into subtasks (steps). The model generates
    a list-friendly formatted string, which the planner parses and returns as the
    list of steps that `executor` (itself an `agent`) executes.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`planner`使用一个特殊的LangChain文本提示模板来查询LLM模型，将任务的解决方案拆分为子任务（步骤）。模型生成一个适合列表格式的字符串，`planner`解析并返回作为`executor`（本身是一个`agent`）执行的步骤列表。'
- en: 'Finally, we can run `agent`:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以运行`agent`：
- en: '[PRE28]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The final answer is `The depth of the deepest section of the ocean in metric
    units is 19,783 meters`. Although the text description is off the mark, the computation
    seems correct.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终答案是`海洋最深处的深度（公制单位）为19,783米`。尽管文本描述有些不准确，但计算结果似乎是正确的。
- en: 'Let’s analyze part of the steps `planner` and `executor` take to reach the
    result. First, `planner` takes our initial query and asks the LLM to break it
    up into the following list of steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析`planner`和`executor`在达到结果的过程中所采取的部分步骤。首先，`planner`接受我们的初始查询，并要求LLM将其拆分为以下步骤列表：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, `agent` iterates over each step and tasks `executor` to perform it. `executor`
    has an internal LLM planner, which can also break up the current step into subtasks.
    In addition to the step description, `executor` uses a special text prompt, instructing
    its LLM `model` to identify the list of `tools` (a tool has a name) it can use
    for each step. For example, `executor` returns the following result as output
    for the augmented version of the first step:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`agent`遍历每个步骤，并指派`executor`执行它。`executor`有一个内部LLM规划器，它也可以将当前步骤拆分为子任务。除了步骤描述外，`executor`还使用一个特殊的文本提示，指示其LLM`model`识别它可以用于每个步骤的`tools`（工具有名称）。例如，`executor`返回以下结果作为第一步增强版本的输出：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `Search` agent’s `action` represents a new intermediate step to be executed
    after the current one. It will use the search tool to query Google with `action_input`.
    In that sense, the chain is dynamic, as the output of one step can lead to additional
    steps added to the chain. We add the result of each step to the input sequence
    of the future steps, and the LLM, via different prompt templates, ultimately determines
    the next actions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`Search`代理的`action`表示在当前步骤之后执行的一个新的中间步骤。它将使用搜索工具通过`action_input`查询Google。从这个角度来看，链条是动态的，因为一个步骤的输出可以导致额外的步骤被添加到链条中。我们将每个步骤的结果加入到未来步骤的输入序列中，LLM通过不同的提示模板最终决定下一步的行动。'
- en: This concludes our introduction to LangChain – a glimpse of what is possible
    with LLMs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对LangChain的介绍——展示了LLM的可能性。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed a variety of topics. We started with LLMs in
    the computer vision domain: ViT for image classification, DetR for object detection,
    and SD for text-to-image generation. Next, we learned how to fine-tune an LLM
    with the Transformers library. Finally, we used LangChain to implement a novel
    LLM-driven application.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了多个话题。我们从计算机视觉领域的LLM开始：ViT用于图像分类，DetR用于目标检测，SD用于文本到图像的生成。接着，我们学习了如何使用Transformers库对LLM进行微调。最后，我们使用LangChain实现了一个新型的LLM驱动应用。
- en: In the next chapter, we’ll depart from our traditional topics and dive into
    the practical field of MLOps.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将脱离传统话题，深入探讨MLOps的实际应用。
- en: 'Part 4:'
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：
- en: Developing
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 开发
- en: and Deploying Deep Neural Networks
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 和深度神经网络的部署
- en: In this single-chapter part, we’ll discuss some techniques and tools that will
    help us develop and deploy neural network models.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一单章部分，我们将讨论一些有助于我们开发和部署神经网络模型的技术和工具。
- en: 'This part has the following chapter:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 10*](B19627_10.xhtml#_idTextAnchor253), *Machine Learning Operations
    (MLOps)*'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B19627_10.xhtml#_idTextAnchor253)，*机器学习运维（MLOps）*'
