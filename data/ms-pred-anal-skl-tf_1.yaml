- en: Ensemble Methods for Regression and Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归和分类的集成方法
- en: Advanced analytical tools are widely used by business enterprises in order to
    solve problems using data. The goal of analytical tools is to analyze data and
    extract relevant information that can be used to solve problems or increase performance
    of some aspect of the business. It also involves various machine learning algorithms
    with which we can create predictive models for better results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 高级分析工具在商业企业中被广泛应用，以利用数据解决问题。分析工具的目标是分析数据并提取相关信息，以解决问题或提高业务某些方面的绩效。它还涉及各种机器学习算法，通过这些算法我们可以创建预测模型，从而获得更好的结果。
- en: In this chapter, we are going to explore a simple idea that can drastically
    improve the performance of basic predictive models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一种可以显著提高基础预测模型性能的简单思想。
- en: 'We are going to cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Ensemble methods and their working
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法及其工作原理
- en: Ensemble methods for regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归的集成方法
- en: Ensemble methods for classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类的集成方法
- en: Ensemble methods and their working
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法及其工作原理
- en: 'Ensemble methods are based on a very simple idea: instead of using a single
    model to make a prediction, we use many models and then use some method to **aggregate**
    the predictions. Having different models is like having different points of view,
    and it has been demonstrated that by aggregating models that offer a different
    point of view; predictions can be more accurate. These methods further improve
    generalization over a single model because they reduce the risk of selecting a
    poorly performing classifier:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法基于一个非常简单的思想：与其使用单一模型进行预测，不如使用多个模型，然后采用某种方法来**聚合**这些预测。拥有不同的模型就像拥有不同的视角，已经证明，通过聚合提供不同视角的模型，预测可以更加准确。这些方法进一步提高了模型的泛化能力，因为它们减少了选择表现不佳的分类器的风险：
- en: '![](img/7ea084f3-b32d-4323-b493-cce263e07286.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ea084f3-b32d-4323-b493-cce263e07286.png)'
- en: 'In the preceding diagram, we can see that each object belongs to one of three
    classes: triangles, circles, and squares. In this simplified example, we have
    two features to separate or classify the objects into the different classes. As
    you can see here, we can use three different classifiers and all the three classifiers
    represent different approaches and have different kinds of decision boundaries.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到每个对象属于三种类别之一：三角形、圆形和正方形。在这个简化的例子中，我们有两个特征来将对象分离或分类到不同的类别中。正如你所见，我们可以使用三种不同的分类器，且这三种分类器代表不同的方法，并具有不同类型的决策边界。
- en: Ensemble learning combines all those individual predictions into a single one.
    The predictions made from combining the three boundaries usually have better properties
    than the ones produced by the individual models. This is the simple idea behind
    ensemble methods, also called **ensemble learning**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习将所有个体预测结合为一个单一的预测。通过结合三个边界做出的预测通常比单一模型产生的预测具有更好的性能。这就是集成方法背后的简单思想，也称为**集成学习**。
- en: 'The most commonly used ensemble methods are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的集成方法如下：
- en: Bootstrap sampling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助抽样
- en: Bagging
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助法（Bagging）
- en: Random forests
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Boosting
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升法（Boosting）
- en: Before giving a high-level explanation of these methods, we need to discuss
    a very important statistical technique known as **bootstrap sampling**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在对这些方法进行高层次解释之前，我们需要讨论一种非常重要的统计技术，称为**自助抽样**。
- en: Bootstrap sampling
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自助抽样
- en: Many ensemble learning methods use a statistical technique called bootstrap
    sampling. A bootstrap sample of a dataset is another dataset that's obtained by
    randomly sampling the observations from the original dataset *with replacement*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多集成学习方法使用一种统计技术，称为自助抽样（bootstrap sampling）。数据集的自助抽样是通过从原始数据集中随机抽样观测值*有放回*地生成的另一个数据集。
- en: This technique is heavily used in statistics, for example; it is used for estimating
    standard errors on sample statistics like mean or standard deviation of values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在统计学中广泛应用，例如，它用于估计样本统计量的标准误差，如均值或标准差。
- en: 'Let''s understand this technique more by taking a look at the following diagram:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看以下图示来更好地理解这一技术：
- en: '![](img/2fedfc25-71ae-4f2f-a64c-488eb2c170a9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fedfc25-71ae-4f2f-a64c-488eb2c170a9.png)'
- en: Let's assume that we have a population of 1 to 10, which can be considered original
    population data. To get a bootstrap sample, we need to draw 10 samples from the
    original data with replacement. Imagine you have the 10 numbers written in 10
    cards in a hat; for the first element of your sample, you take one card at random
    from the hat and write it down, then put the card back in the hat and this process
    goes on until you get 10 elements. This is your bootstrap sample. As you can see
    in the preceding example, **9** is repeated thrice in the bootstrap sample.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个从1到10的种群数据，可以视为原始种群数据。为了获得自助样本，我们需要从原始数据中以有放回的方式抽取10个样本。想象你有10张写着数字的卡片放在一顶帽子里；对于样本的第一个元素，你从帽子中随机抽取一张卡片并记录下来，然后将卡片放回帽子里，这个过程一直进行，直到你得到10个元素。这就是你的自助样本。如前面的例子所示，**9**在自助样本中被重复了三次。
- en: This resampling of numbers with replacement improves the accuracy of the true
    population data. It also helps in understanding various discrepancies and features
    involved in the resampling process, thereby increasing accuracy of the same.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种带放回的数字重抽样提高了对真实种群数据的准确性。它还有助于理解在重抽样过程中涉及的各种差异和特征，从而提高相同过程的准确性。
- en: Bagging
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 袋装法
- en: Bagging, also known as bootstrap aggregation, is a general purpose procedure
    for reducing variance in the machine learning model. It is based on the bootstrap
    sampling technique and is generally used with regression or classification trees,
    but in principle this bagging technique can be used with any model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法，也称为自助聚合法，是一种常用的减少机器学习模型方差的程序。它基于自助抽样技术，通常与回归或分类树一起使用，但原则上，这种袋装技术可以用于任何模型。
- en: 'The following steps are involved in the bagging process:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装过程包括以下步骤：
- en: We choose the number of estimators or individual models to use. Let's consider
    this as parameter B.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择使用的估计器或单独模型的数量。我们将其视为参数B。
- en: We take sample datasets from B with replacement using the bootstrap sampling
    from the training set.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从B中使用自助抽样法从训练集中抽取样本数据集。
- en: For every one of these training datasets, we fit the machine learning model
    in each of the bootstrap samples. This way, we get individual predictors for the
    B parameter.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一个训练数据集，我们在每个自助样本中拟合机器学习模型。这样，我们就得到了参数B的单独预测器。
- en: We get the ensemble prediction by aggregating all of the individual predictions.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过聚合所有单独的预测结果来获得集成预测。
- en: In the regression problem, the most common way to get the ensemble prediction
    would be to find the average of all of the individual predictions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，获取集成预测结果的最常见方法是求所有单独预测的平均值。
- en: In the classification problem, the most common way to get the aggregated predictions
    is by doing a majority vote. The majority vote can be explained by an example.
    Let's say that we have 100 individual predictors and 80 of them vote for one particular
    category. Then, we choose that category as our aggregated prediction. This is
    what a majority vote means.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，最常见的获取聚合预测结果的方法是通过多数投票。我们可以通过一个例子来解释多数投票。假设我们有100个单独的预测器，其中80个投票支持某一特定类别。那么，我们就选择该类别作为我们的聚合预测结果。这就是多数投票的含义。
- en: Random forests
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: This ensemble method is specifically created for regression or classification
    trees. It is very similar to bagging since, here, each individual tree is trained
    on a bootstrap sample of the training dataset. The difference with bagging is
    that it makes the model very powerful, and on splitting a node from the tree,
    the split that is picked is the best among a random subset of the features. So,
    every individual predictor considers a random subset of the features. This has
    the effect of making each individual predictor slightly worse and more biased
    but, due to the correlation of the individual predictors, the overall ensemble
    is generally better than the individual predictors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个集成方法是专门为回归或分类树创建的。它与袋装法非常相似，因为在这里，每棵单独的树都是在训练数据集的自助样本上训练的。与袋装法的不同之处在于，它使模型非常强大，并且在对树进行节点分裂时，选择的分裂是特征的随机子集中的最佳分裂。因此，每个单独的预测器都会考虑特征的随机子集。这会导致每个单独的预测器稍微变得较差且更具偏差，但由于单独预测器之间的相关性，整体集成预测通常比单个预测器要好。
- en: Boosting
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升法
- en: Boosting is another approach to ensemble learning. There are many methods for
    boosting, but one of the most successful and popular methods that people use for
    ensemble learning has been the **AdaBoost** algorithm. It is also called **adaptive
    boosting**. The core idea behind this algorithm is that, instead of fitting many
    individual predictors individually, we fit a sequence of weak learners. The next
    algorithm depends on the result of the previous one. In the AdaBoost algorithm,
    every iteration reweights all of these samples. The training data here reweights
    based on the result of the previous individual learners or individual models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提升（Boosting）是集成学习的另一种方法。提升有许多方法，但最成功和最受欢迎的一种方法是**AdaBoost**算法，也叫**自适应提升**。该算法的核心思想是，不是单独训练多个预测器，而是训练一系列弱学习器。下一个算法依赖于前一个算法的结果。在AdaBoost算法中，每次迭代都会重新加权所有样本。这里的训练数据会根据前一个学习器或模型的结果重新加权。
- en: For example, in classification, the basic idea is that the examples that are
    misclassified gain weight and the examples that are classified correctly lose
    weight. So, the next learner in the sequence or the next model in the sequence
    focuses more on misclassified examples.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在分类中，基本思想是错误分类的示例会增加权重，而正确分类的示例则减少权重。因此，序列中的下一个学习器或模型会更关注错误分类的示例。
- en: Ensemble methods for regression
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归的集成方法
- en: Regarding regression, we will train these different models and later compare
    their results. In order to test all of these models, we will need a sample dataset.
    We are going to use this in order to implement these methods on the given dataset
    and see how this helps us with the performance of our models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关于回归，我们将训练这些不同的模型，并随后比较它们的结果。为了测试所有这些模型，我们需要一个样本数据集。我们将使用这个数据集来实现这些方法，看看它如何帮助我们提高模型的性能。
- en: The diamond dataset
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 钻石数据集
- en: 'Let''s make actual predictions about diamond prices by using different ensemble
    learning models. We will use a diamonds dataset(which can be found here: [https://www.kaggle.com/shivam2503/diamonds](https://www.kaggle.com/shivam2503/diamonds)).
    This dataset has the prices, among other features, of almost 54,000 diamonds.
    The following are the features that we have in this dataset:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用不同的集成学习模型对钻石价格进行实际预测。我们将使用一个钻石数据集（可以在此处找到：[https://www.kaggle.com/shivam2503/diamonds](https://www.kaggle.com/shivam2503/diamonds)）。该数据集包含约54,000颗钻石的价格以及其他特征。以下是该数据集中的特征：
- en: '**Feature information**: A dataframe with 53,940 rows and 10 variables'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征信息**：一个包含53,940行和10个变量的数据框'
- en: '**Price**: Price in US dollars'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格**：以美元为单位的价格'
- en: 'The following are the nine predictive features:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是九个预测特征：
- en: '`carat`: This feature represents weight of the diamond (0.2-5.01)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`carat`：该特征表示钻石的重量（0.2-5.01）'
- en: '`cut`: This feature represents quality of the cut (`Fair`, `Good`, `Very Good`,
    `Premium`, and `Ideal`)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cut`：该特征表示切割的质量（`Fair`，`Good`，`Very Good`，`Premium`，`Ideal`）'
- en: '`color`: This feature represents diamond color, from `J` (worst) to `D` (best)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`color`：该特征表示钻石的颜色，从`J`（最差）到`D`（最好）'
- en: '`clarity`: This feature represents a measurement of how clear the diamond is
    (`I1` (worst), `SI2`, `SI1`, `VS2`, `VS1`, `VVS2`, `VVS1`, `IF` (best))'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clarity`：该特征表示钻石的清晰度测量（`I1`（最差），`SI2`，`SI1`，`VS2`，`VS1`，`VVS2`，`VVS1`，`IF`（最好））'
- en: '`x`: This feature represents length of diamond in mm (0-10.74)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`：该特征表示钻石的长度，单位为毫米（0-10.74）'
- en: '`y`: This feature represents width of diamond in mm (0-58.9)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`：该特征表示钻石的宽度，单位为毫米（0-58.9）'
- en: '`z`: This feature represents depth of diamond in mm (0-31.8)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`z`：该特征表示钻石的深度，单位为毫米（0-31.8）'
- en: '`depth`: This feature represents z/mean(x, y) = 2 * z/(x + y) (43-79)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth`：该特征表示 z/平均值(x, y) = 2 * z/(x + y)（43-79）'
- en: '`table`: This feature represents width of the top of the diamond relative to
    the widest point (43-95)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table`：该特征表示钻石顶部宽度与最宽点的比率（43-95）'
- en: The `x`, `y`, and `z` variables denote the size of the diamonds.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`、`y` 和 `z` 变量表示钻石的尺寸。'
- en: 'The libraries that we will use are `numpy`, `matplotlib`, and `pandas`. For
    importing these libraries, the following lines of code can be used:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的库有`numpy`、`matplotlib`和`pandas`。要导入这些库，可以使用以下代码：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the lines of code that we use to call the raw
    dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了我们用来调用原始数据集的代码行：
- en: '![](img/744cd5bf-030d-4c57-9858-8f8ea652dee5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/744cd5bf-030d-4c57-9858-8f8ea652dee5.png)'
- en: The preceding dataset has some numerical features and some categorical features.
    Here, 53,940 is the exact number of samples that we have in this dataset. Now,
    for encoding the information in these categorical features, we use the one-hot
    encoding technique to transform these categorical features into dummy features.
    The reason behind this is because `scikit-learn` only works with numbers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据集包含一些数值特征和一些类别特征。这里，53,940 是我们数据集中样本的确切数量。现在，为了编码这些类别特征中的信息，我们使用独热编码技术将这些类别特征转换为虚拟特征。这样做的原因是因为`scikit-learn`只能处理数字。
- en: 'The following screenshot shows the lines of code used for the transformation
    of the categorical features to numbers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了将类别特征转换为数字时使用的代码行：
- en: '![](img/3b9400c6-af95-46f0-9950-b791d06199ca.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b9400c6-af95-46f0-9950-b791d06199ca.png)'
- en: 'Here, we can see how we can do this with the `get_dummies` function from `pandas`.
    The final dataset looks similar to the one in the following screenshot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到如何使用来自`pandas`的`get_dummies`函数实现这一点。最终的数据集看起来类似于以下截图所示的样子：
- en: '![](img/7bb25e6e-cf8d-4f81-bca4-b6424662d894.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bb25e6e-cf8d-4f81-bca4-b6424662d894.png)'
- en: Here, for each of the categories in the categorical variable, we have dummy
    features. The value here is `1` when the category is present and `0` when the
    category is not present in the particular diamond.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于类别变量中的每个类别，我们都有虚拟特征。当类别在特定的钻石中出现时，值为`1`，否则为`0`。
- en: Now, for rescaling the data, we will use the `RobustScaler` method to transform
    all the features to a similar scale.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了重新缩放数据，我们将使用`RobustScaler`方法将所有特征转换到相似的尺度。
- en: 'The following screenshot shows the lines of code used for importing the `train_test_split`
    function and the `RobustScaler` method:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了导入`train_test_split`函数和`RobustScaler`方法时使用的代码行：
- en: '![](img/f0f42b97-e3c0-415a-b423-f8cf1299ecf6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0f42b97-e3c0-415a-b423-f8cf1299ecf6.png)'
- en: Here, we extract the features in the `X` matrix, mention the target, and then
    use the `train_test_split` function from `scikit-learn` to partition the data
    into two sets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提取了`X`矩阵中的特征，指定了目标，然后使用来自`scikit-learn`的`train_test_split`函数将数据分为两组。
- en: Training different regression models
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练不同的回归模型
- en: 'The following screenshot shows the dataframe that we will use to record the
    metrics and the performance metrics that we will use for these models. Since this
    is a regression task, we will use the mean squared error. Here, in the columns,
    we have the four models that we will use. We will be using the `KNN`, `Bagging`,
    `RandomForest`, and `Boosting` variables:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们将用来记录这些模型度量值的 dataframe，以及我们将使用的性能指标。由于这是一个回归任务，我们将使用均方误差。在这里，列中展示了我们将使用的四个模型。我们将使用`KNN`、`Bagging`、`RandomForest`和`Boosting`变量：
- en: '![](img/7372d44f-c895-45b9-8bce-e90ee96cd9a7.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7372d44f-c895-45b9-8bce-e90ee96cd9a7.png)'
- en: KNN model
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 模型
- en: 'The **K-Nearest Neighbours** (**KNN**) model is not an ensemble learning model,
    but it performs the best among the simple models:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-Nearest Neighbours**（**KNN**）模型不是集成学习模型，但它在所有简单模型中表现最好：'
- en: '![](img/49b9ce91-a006-4b09-b2e9-021e1ca3a76e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49b9ce91-a006-4b09-b2e9-021e1ca3a76e.png)'
- en: In the preceding model, we can see the process used while making a KNN. We will
    use 20 neighbors. We are using the `euclidean` metric to measure the distances
    between the points, and then we will train the model. Here, the performance metric
    is saved since the value is just `1`, which is the mean squared error.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述模型中，我们可以看到在做 KNN 时使用的过程。我们将使用 20 个邻居。我们使用`euclidean`度量来衡量点之间的距离，然后训练模型。在这里，性能度量已保存，因为其值为`1`，即均方误差。
- en: Bagging model
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging 模型
- en: 'Bagging is an ensemble learning model. Any estimator can be used with the bagging
    method. So, let''s take a case where we use KNN, as shown in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 是一种集成学习模型。任何估算器都可以与 Bagging 方法一起使用。所以，假设我们使用 KNN，如下图所示：
- en: '![](img/6e77b966-f186-47b0-a62d-df853ab1477c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e77b966-f186-47b0-a62d-df853ab1477c.png)'
- en: Using the `n_estimators` parameter, we can produce an ensemble of 15 individual
    estimators. As a result, this will produce 15 bootstrap samples of the training
    dataset, and then, in each of these samples, it will fit one of these KNN regressors
    with 20 neighbors. In the end, we will get the individual predictions by using
    the bagging method. The method that this algorithm uses for giving individual
    predictions is a majority vote.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`n_estimators`参数，我们可以生成15个独立的估计器。因此，这将生成15个训练数据集的自助抽样（bootstrap samples），然后在每个样本中，使用20个邻居来拟合KNN回归模型。最后，我们将使用袋装方法（bagging
    method）获得个别的预测结果。该算法生成个别预测的方式是通过多数投票。
- en: Random forests model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林模型
- en: 'Random forests is another ensemble learning model. Here, we get all the ensemble
    learning objects from the `ensemble` submodule in `scikit-learn`. For example,
    here, we use the `RandomForestRegressor` method. The following screenshot, shows
    the algorithm used for this model:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是另一种集成学习模型。在这里，我们从`scikit-learn`中的`ensemble`子模块获取所有集成学习对象。例如，在这里，我们使用`RandomForestRegressor`方法。以下截图展示了这个模型使用的算法：
- en: '![](img/e9a128fc-153c-45ba-baeb-6497cbb42d7b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9a128fc-153c-45ba-baeb-6497cbb42d7b.png)'
- en: So, in a case where we produce a forest of 50 individual predictors, this algorithm
    will produce 50 individual trees. Each tree will have `max_depth` of `16`, which
    will then produce the individual predictions again by majority vote.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在生成50个独立预测器的森林的情况下，这个算法会生成50棵独立的树。每棵树的`max_depth`为`16`，然后通过多数投票方法生成单独的预测结果。
- en: Boosting model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Boosting模型
- en: 'Boosting is also an ensemble learning model. Here, we are using the `AdaBoostRegressor`
    model, and we will again produce `50` estimators. The following screenshot shows
    the algorithm used for this model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting也是一种集成学习模型。在这里，我们使用`AdaBoostRegressor`模型，并且我们将再次生成`50`个估计器。以下截图展示了这个模型使用的算法：
- en: '![](img/a8b9a936-0158-41a2-a251-428a10e3d4e2.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8b9a936-0158-41a2-a251-428a10e3d4e2.png)'
- en: 'The following screenshot shows the `train_mse` and `test_mse` results that
    we get after training all these models:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了训练所有这些模型后得到的`train_mse`和`test_mse`结果：
- en: '![](img/c9526e11-4aca-41c5-b379-5886696eb13a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9526e11-4aca-41c5-b379-5886696eb13a.png)'
- en: 'The following screenshot shows the algorithm and gives the comparison of all
    of these models on the basis of the values of the test mean squared error. The
    result is shown with the help of a horizontal bar graph:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了算法，并基于测试集均方误差值对这些模型进行比较。结果通过水平条形图显示：
- en: '![](img/12d93d64-a55f-4309-b819-93c9523166cf.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12d93d64-a55f-4309-b819-93c9523166cf.png)'
- en: Now, when we compare the result of all of these models, we can see that the
    random forest model is the most successful. The bagging and KNN models come second
    and third, respectively. This is why we use the KNN model with the bagging model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们比较所有这些模型的结果时，可以看到随机森林模型是最成功的。袋装（bagging）和KNN模型分别排名第二和第三。这就是为什么我们将KNN模型与袋装模型结合使用的原因。
- en: 'The following screenshot shows the algorithm used to produce a graphical representation
    between the predicted prices and the observed prices while testing the dataset,
    and also shows the performance of the random forest model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了用于生成预测价格与实际价格之间图形表示的算法，同时还展示了随机森林模型的性能：
- en: '![](img/89828296-8384-4cf6-98be-298ad1722860.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89828296-8384-4cf6-98be-298ad1722860.png)'
- en: On using this model again with a `predict` API or with a `predict` method, we
    can get individual predictions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该模型时，通过`predict` API或`predict`方法，我们可以获得个别预测。
- en: 'For example, let''s predict the values for the first ten predictions that we
    get from the testing dataset. The following algorithm shows the prediction that
    is made by this random forest model, which in turns shows us the real price and
    the predicted price of the diamonds that we have from the testing dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们要预测从测试数据集获得的前十个预测值。以下算法展示了这个随机森林模型做出的预测结果，同时展示了测试数据集中钻石的真实价格和预测价格：
- en: '![](img/fe7c964b-d9ad-4610-8191-f77921c308d7.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe7c964b-d9ad-4610-8191-f77921c308d7.png)'
- en: From this screenshot, we can see that the values for `Real price` and `Predicted
    price` are very close, both for the expensive and inexpensive diamonds.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张截图中可以看到，`真实价格`和`预测价格`的值非常接近，无论是对于昂贵的钻石还是便宜的钻石。
- en: Using ensemble methods for classification
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成方法进行分类
- en: We are now familiar with the basic concept of ensemble learning and ensemble
    methods. Now, we will actually put these methods into use in building models using
    various machine learning algorithms and compare the results generated by them.
    To actually test all of these methods, we will need a sample dataset in order
    to implement these methods on the given dataset and see how this helps us with
    the performance of our models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经熟悉了集成学习和集成方法的基本概念。接下来，我们将实际应用这些方法，利用各种机器学习算法构建模型，并比较它们生成的结果。为了实际测试这些方法，我们需要一个样本数据集，以便将这些方法应用于给定数据集，并查看它如何帮助我们提升模型的性能。
- en: Predicting a credit card dataset
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测信用卡数据集
- en: 'Let''s take an example of a credit card dataset. This dataset comes from a
    financial institution in Taiwan and can be found here: [https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset).
    Take a look at the following screenshot, which shows you the dataset''s information
    and its features:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个信用卡数据集为例。这个数据集来自台湾的一个金融机构，可以在这里找到：[https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset)。看看下面的截图，它展示了数据集的信息和特征：
- en: '![](img/4843c2d6-cf4e-4e11-8f8f-8f0707ac43b8.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4843c2d6-cf4e-4e11-8f8f-8f0707ac43b8.png)'
- en: 'Here, we have the following detailed information about each customer:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有每个客户的以下详细信息：
- en: It contains the limit balance, that is, the credit limit provided to the customer
    that is using the credit card
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含了信用额度，即提供给使用信用卡的客户的信用限额。
- en: Then, we have a few features regarding personal information about each customer,
    such as gender, education, marital status, and age
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们有一些关于每个客户的个人信息特征，比如性别、教育、婚姻状况和年龄。
- en: We also have a history of past payments
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还拥有过去付款的历史记录。
- en: We also have the bill statement's amount
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还有账单金额。
- en: We have the history of the bill's amount and previous payment amounts from the
    previous month up to six months prior, which was done by the customer
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有从上个月到六个月前客户的账单金额和以前支付金额的历史记录。
- en: With this information, we are going to predict next month's payment status of
    the customer. We will first do a little transformation on these features to make
    them easier to interpret.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们将预测下个月客户的付款状态。我们将首先对这些特征做一点转换，以便更容易理解。
- en: In this case, the positive class will be the default, so the number 1 represents
    the customers that fall under the default status category and the number 0 represents
    the customers who have paid their credit card dues.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，正类将是违约，所以数字1代表属于违约状态的客户，数字0代表已支付信用卡账单的客户。
- en: 'Now, before we start, we need to import the required libraries by running a
    few commands, as shown in the following code snippet:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在开始之前，我们需要通过运行一些命令来导入所需的库，如下代码片段所示：
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following screenshot shows the line of code that was used to prepare the
    credit card dataset:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了用于准备信用卡数据集的代码行：
- en: '![](img/fe6dbac4-e195-481e-8831-add56f69a0de.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe6dbac4-e195-481e-8831-add56f69a0de.png)'
- en: 'Let''s produce the dummy feature for education in `grad _school`, `university`,
    and `high_school`. Instead of using the word sex, use the `male` dummy feature,
    and instead of using marriage, let''s use the `married` feature. This feature
    is given value of 1 when the person is married, and 0 otherwise. For the `pay_1`
    feature, we will do a little simplification process. If we see a positive number
    here, it means that the customer was late in his/her payments for `i` months.
    This means that this customer with an `ID` of 1 delayed the payment for the first
    two months. We can see that, 3 months ago, he/she was not delayed on his/her payments.
    This is what the dataset looks like:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为教育特征生成虚拟变量`grad_school`、`university`和`high_school`。用`male`虚拟变量代替性别一词，并且用`married`特征代替婚姻状况。这个特征在当一个人已婚时赋值为1，否则为0。对于`pay_1`特征，我们将进行一点简化处理。如果我们看到这里有一个正数，意味着客户在`i`个月内延迟了付款。这意味着ID为1的客户延迟了前两个月的付款。我们可以看到，3个月前他/她并没有延迟付款。数据集大致如下：
- en: '![](img/f4ab3d2e-f4b8-4deb-a55d-80d918758c2d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4ab3d2e-f4b8-4deb-a55d-80d918758c2d.png)'
- en: Before fitting our models, the last thing we will do is rescale all the features
    because, as we can see here, we have features that are in very different scales.
    For example, `limit_bal` is in a very different scale than `age`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合我们的模型之前，我们做的最后一件事是重新缩放所有特征，因为正如我们在这里看到的，我们有些特征的尺度差异非常大。例如，`limit_bal`的尺度与`age`有很大不同。
- en: 'This is why we will be using the `RobustScaler` method from `scikit-learn`—to
    try and transform all the features to a similar scale:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们将使用`scikit-learn`中的`RobustScaler`方法的原因——为了尝试将所有特征转换到类似的尺度：
- en: '![](img/a7a7c24d-f772-4668-8a3c-cfd8d7bb559c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7a7c24d-f772-4668-8a3c-cfd8d7bb559c.png)'
- en: 'As we can see in the preceding screenshot in the last line of code, we are
    partitioning our dataset into a training set and a testing set and below that,
    the `CMatrix` function is used to print the confusion matrix for each model. This
    function is explained in the following code snippet:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的截图中看到的，在最后一行代码中，我们将数据集划分为训练集和测试集，在此之后，`CMatrix`函数用于打印每个模型的混淆矩阵。这个函数在下面的代码片段中进行了说明：
- en: '[PRE2]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Training different regression models
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练不同的回归模型
- en: 'The following screenshot shows a dataframe where we are going to save performance.
    We are going to run four models, namely logistic regression, bagging, random forest,
    and boosting:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个数据框，我们将把性能保存到这个数据框中。我们将运行四个模型，分别是逻辑回归、装袋、随机森林和提升：
- en: '![](img/c745a454-b945-46c8-9d98-ab5ee3142f24.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c745a454-b945-46c8-9d98-ab5ee3142f24.png)'
- en: 'We are going to use the following evaluation metrics in this case:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将使用以下评估指标：
- en: '`accuracy`: This metric measures how often the model predicts defaulters and
    non-defaulters correctly'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accuracy`：该指标衡量模型预测违约者和非违约者的准确性'
- en: '`precision`: This metric will be when the model predicts the default and how
    often the model is correct'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precision`：该指标衡量模型预测违约的准确性以及模型的正确预测频率'
- en: '`recall`: This metric will be the proportion of actual defaulters that the
    model will correctly predict'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recall`：该指标表示模型正确预测的实际违约者的比例'
- en: The most important of these is the `recall` metric. The reason behind this is
    that we want to maximize the proportion of actual defaulters that the model identifies,
    and so the model with the best recall is selected.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标中最重要的是`recall`。原因在于我们希望最大化模型识别实际违约者的比例，因此选择具有最佳召回率的模型。
- en: Logistic regression model
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: 'As in `scikit-learn`, we just import the object and then instantiate the estimator,
    and then pass training set `X` and training set `Y` to the `fit()` method. First,
    we will predict the test dataset and then produce the accuracy, precision, and
    recall scores. The following screenshot shows the code and the confusion matrix
    as the output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在`scikit-learn`中，我们只需导入对象，然后实例化估算器，再将训练集`X`和训练集`Y`传递给`fit()`方法。首先，我们将预测测试数据集，然后生成准确率、精度和召回率的分数。以下截图显示了代码和混淆矩阵的输出：
- en: '![](img/6d069752-6b15-482d-9354-c6589c2ca984.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d069752-6b15-482d-9354-c6589c2ca984.png)'
- en: Later, we will save these into our `pandas` dataframe that we just created.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们将把这些数据保存到我们刚刚创建的`pandas`数据框中。
- en: Bagging model
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装袋模型
- en: Training the bagging model using methods from the ensemble learning techniques
    involves importing the bagging classifier with the logistic regression methods.
    For this, we will fit 10 of these logistic regression models and then we will
    combine the 10 individual predictions into a single prediction using bagging.
    After that, we will save this into our metrics dataframe.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成学习方法中的装袋模型进行训练时，需要导入带有逻辑回归方法的装袋分类器。为此，我们将拟合10个逻辑回归模型，然后通过装袋方法将这10个单独的预测结果合并为一个预测结果。之后，我们将把它保存在我们的指标数据框中。
- en: 'The following screenshot shows the code and the confusion matrix as the output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了代码和混淆矩阵的输出：
- en: '![](img/84e409fc-7e30-4321-8f88-57c0926c643c.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84e409fc-7e30-4321-8f88-57c0926c643c.png)'
- en: Random forest model
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林模型
- en: To perform classification with the random forest model, we have to import the
    `RandomForestClassifier` method. For example, let's take 35 individual trees with
    a `max_depth` of `20` for each tree. The `max_features` parameter tells `scikit-learn`
    that, when deciding upon the best split among possible features, we should use
    the square root of the total number of features that we have. These are all hyperparameters
    that we can tune.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用随机森林模型进行分类，我们必须导入`RandomForestClassifier`方法。例如，我们选择35棵单独的树，每棵树的`max_depth`为`20`。`max_features`参数告诉`scikit-learn`，在决定最佳拆分特征时，我们应该使用特征总数的平方根。这些都是我们可以调整的超参数。
- en: 'The following screenshot shows the code and the confusion matrix as the output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了代码和混淆矩阵作为输出：
- en: '![](img/667a44c8-3ce3-4e59-964b-1263b7b0b470.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/667a44c8-3ce3-4e59-964b-1263b7b0b470.png)'
- en: Boosting model
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升模型
- en: In classification with the boosting model, we'll use the `AdaBoostClassifier`
    object. Here, we'll also use `50` estimators to combine the individual predictions.
    The learning rate that we will use here is `0.1`, which is another hyperparameter
    for this model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用提升模型进行分类时，我们将使用`AdaBoostClassifier`对象。在这里，我们还将使用`50`个估计器来组合各个预测。我们将使用的学习率是`0.1`，这是该模型的另一个超参数。
- en: 'The following screenshot shows the code and the confusion matrix:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了代码和混淆矩阵：
- en: '![](img/4b51908b-ba6c-4b3a-adc8-0a8728badee8.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b51908b-ba6c-4b3a-adc8-0a8728badee8.png)'
- en: 'Now, we will compare the four models as shown in the following screenshot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将比较以下截图中显示的四个模型：
- en: '![](img/f880f3a8-3521-4ee4-a837-7277b036abb9.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f880f3a8-3521-4ee4-a837-7277b036abb9.png)'
- en: The preceding screenshot shows the similar accuracies for the four models, but
    the most important metric for this particular application is the `recall` metric.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了四个模型的准确率相似，但对于这个特定应用，最重要的指标是`recall`（召回率）。
- en: 'The following screenshot shows that the model with the best recall and accuracy
    is the random forest model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了最佳召回率和准确率的模型是随机森林模型：
- en: '![](img/7505e831-8e35-4fb1-b5a2-ba85059cb6e7.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7505e831-8e35-4fb1-b5a2-ba85059cb6e7.png)'
- en: The preceding screenshot proves that the random forest model is better than
    the other models overall.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图证明了随机森林模型总体上优于其他模型。
- en: To see the relationship between `precision`, `recall`, and `threshold`, we can
    use the `precision_recall_curve` function from `scikit-learn`. Here, pass the
    predictions and the real observed values, and the result we get consists of the
    objects that will allow us to produce the code for the `precision_recall_curve` function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`precision`（精准率）、`recall`（召回率）和`threshold`（阈值）之间的关系，我们可以使用`scikit-learn`中的`precision_recall_curve`函数。在这里，传入预测结果和实际观察值，得到的结果将是允许我们生成`precision_recall_curve`函数代码的对象。
- en: 'The following screenshot shows the code for the `precision_recall_curve` function
    from `scikit-learn`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`scikit-learn`中`precision_recall_curve`函数的代码：
- en: '![](img/e302b9c0-37ad-41fb-ba88-8fd8b9ebd89e.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e302b9c0-37ad-41fb-ba88-8fd8b9ebd89e.png)'
- en: 'The following screenshot will now visualize the relationship between precision
    and recall when using the random forest model and the logistic regression model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图将可视化使用随机森林模型和逻辑回归模型时，精准率和召回率之间的关系：
- en: '![](img/480e2a82-f60d-4756-8889-a7dacaf3e7f7.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/480e2a82-f60d-4756-8889-a7dacaf3e7f7.png)'
- en: The preceding screenshot shows that the random forest model is better because
    it is above the logistic regression curve. So, for a precision of `0.30`, we get
    more recall with the random forest model than the logistic regression model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了随机森林模型更好，因为它位于逻辑回归曲线之上。因此，在`0.30`的精准率下，随机森林模型的召回率比逻辑回归模型更高。
- en: To see the performance of the `RandomForestClassifier` method, we change the
    classification threshold. For example, we set a classification threshold of `0.12`,
    so we will get a precision of `30` and a recall of `84`. This model will correctly
    predict 84% of the possible defaulters, which will be very useful for a financial
    institution. This shows that the boosting model is better than the logistic regression
    model for this.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看`RandomForestClassifier`方法的性能，我们改变分类阈值。例如，我们设置分类阈值为`0.12`，这样我们将获得`30`的精准率和`84`的召回率。该模型将正确预测84%的可能违约者，对于金融机构来说非常有用。这表明，提升模型比逻辑回归模型更适合这种情况。
- en: 'The following screenshot shows the code and the confusion matrix:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了代码和混淆矩阵：
- en: '![](img/251c8b59-4160-4ffe-821f-185c7b13f6ae.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/251c8b59-4160-4ffe-821f-185c7b13f6ae.png)'
- en: Feature importance is something very important that we get while using a random
    forest model. The `scikit-learn` library calculates this metric of feature importance
    for each of the features that we use in our model. The internal calculation allows
    us to get a metric for the importance of each feature in the predictions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性是我们在使用随机森林模型时获得的非常重要的信息。`scikit-learn`库会计算我们在模型中使用的每个特征的特征重要性指标。内部计算使我们能够得到每个特征在预测中的重要性指标。
- en: 'The following screenshot shows the visualization of these features, hence highlighting
    the importance of using a `RandomForestClassifier` method:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了这些特征的可视化，从而突出了使用`RandomForestClassifier`方法的重要性：
- en: '![](img/731b8ad3-d783-4308-a3a7-a442087ee7ee.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/731b8ad3-d783-4308-a3a7-a442087ee7ee.png)'
- en: The most important feature for predicting whether the customer will default
    next month or whether the customer defaulted the month before is `pay_1`. Here,
    we just have to verify whether the customer paid last month or not. The other
    important features of this model are the bill amounts of two months, while the
    other feature in terms of importance is age.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测客户是否会在下个月违约或客户是否在上个月已违约，最重要的特征是`pay_1`。在这里，我们只需要验证客户是否在上个月支付过。该模型的其他重要特征是两个月的账单金额，而另一个重要的特征是年龄。
- en: The features that are not important for predicting the target are gender, marital
    status, and the education level of the customer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测目标不重要的特征包括客户的性别、婚姻状况和教育水平。
- en: Overall, the random forest model has proved to be better than the logistic regression
    model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，随机森林模型已经证明比逻辑回归模型更好。
- en: According to the no free lunch theorem, there is no single model that works
    best for every problem in every dataset. This means that ensemble learning cannot
    always outperform simpler methods because sometimes simpler methods perform better
    than complex methods. So, for every machine learning problem, we must use simple
    methods over complex methods and then evaluate the performance of both approaches
    to get the best results.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据“没有免费午餐”定理，没有一个模型能在所有问题和数据集上都表现最好。这意味着集成学习并不总是优于简单方法，因为有时简单方法的表现比复杂方法更好。因此，对于每一个机器学习问题，我们必须先使用简单方法，再评估两种方法的性能，以获得最佳结果。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced different ensemble methods such as bootstrap
    sampling, bagging, random forest, and boosting, and their working was explained
    with the help of some examples. We then used them for regression and classification.
    For regression, we took the example of a diamond dataset, and we also trained
    some KNN and other regression models. Later, their performance was compared. For
    classification, we took the example of a credit card dataset. Again, we trained
    all of the regression models. We compared their performance, and we found that
    the random forest model was the best performer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了不同的集成方法，如自助抽样、装袋法、随机森林和提升法，并通过一些例子解释了它们的工作原理。然后，我们将它们用于回归和分类问题。对于回归，我们以一个钻石数据集为例，同时也训练了一些KNN和其他回归模型。随后，我们比较了它们的性能。对于分类，我们以信用卡数据集为例，再次训练了所有回归模型。我们比较了它们的性能，发现随机森林模型表现最好。
- en: In the next chapter, we will study k-fold cross-validation and parameter tuning.
    We will compare different ensemble learning models with k-fold cross-validation
    and later, we'll use k-fold cross-validation for hyperparameter tuning.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习k折交叉验证和参数调优。我们将使用k折交叉验证比较不同的集成学习模型，随后，我们将利用k折交叉验证进行超参数调优。
