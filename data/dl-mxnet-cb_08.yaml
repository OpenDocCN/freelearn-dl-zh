- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Improving Training Performance with MXNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MXNet 改进训练性能
- en: 'In previous chapters, we have leveraged MXNet capabilities to solve computer
    vision and `GluonCV` and `GluonNLP`. We trained these models using different approaches:
    *from scratch*, *transfer learning*, and *fine-tuning*. In this chapter, we will
    focus on improving the performance of the training process itself and accelerating
    how we can obtain those results.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们利用 MXNet 的功能解决了计算机视觉和 `GluonCV`、`GluonNLP` 的问题。我们通过不同的方法训练了这些模型：*从头开始*、*迁移学习*和*微调*。在本章中，我们将重点关注如何提高训练过程的性能，并加速我们如何获得这些结果。
- en: To achieve the objective of optimizing the performance of our training loops,
    MXNet contains different features. We have already briefly used some of those
    features such as the concept of **lazy evaluation**, which was introduced in [*Chapter
    1*](B16591_01.xhtml#_idTextAnchor016). We will revisit it in this chapter, in
    combination with automatic parallelization. Moreover, we will optimize how to
    access data efficiently, leveraging Gluon DataLoaders in different contexts (CPU,
    GPU) to perform data transforms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现优化训练循环性能的目标，MXNet 提供了多种功能。我们已经简要使用了一些这些功能，例如 **延迟计算** 的概念，该概念在[*第1章*](B16591_01.xhtml#_idTextAnchor016)中介绍过。我们将在本章中再次探讨这一点，并结合自动并行化来使用。此外，我们还将优化如何高效访问数据，利用
    Gluon DataLoaders 在不同的环境（CPU、GPU）中执行数据转换。
- en: Moreover, we will explore how to combine multiple GPUs to accelerate training,
    making use of techniques such as data parallelization for optimal performance.
    We will also explore how we can use different data types with `MXNet` to dynamically
    optimize the different data formats.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将探索如何结合多个 GPU 加速训练，利用诸如数据并行化等技术来获得最佳性能。我们还将探讨如何使用不同的数据类型与 `MXNet` 配合，以动态优化不同的数据格式。
- en: Finally, using problems already explored in the book, we will apply all these
    techniques with examples. For our computer vision task, we will choose image segmentation,
    and for our NLP task, we will choose translating text from English to German.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，利用书中已探讨的问题，我们将通过示例应用所有这些技术。对于我们的计算机视觉任务，我们将选择图像分割，而对于 NLP 任务，我们将选择翻译英语到德语的文本。
- en: 'Specifically, this chapter is structured with the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章的结构包含以下食谱：
- en: Introducing training optimization features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍训练优化功能
- en: Optimizing training for image segmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为图像分割优化训练
- en: Optimizing training for translating text from English to German
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为英语到德语的文本翻译优化训练
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*前言*中指定的技术要求外，以下技术要求适用：
- en: Ensure that you have completed the *Installing MXNet, Gluon, GluonCV and GluonNLP*
    recipe from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*安装 MXNet、Gluon、GluonCV 和 GluonNLP*](B16591_01.xhtml#_idTextAnchor016)的食谱。
- en: Ensure that you have completed [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098)
    and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第5章*](B16591_05.xhtml#_idTextAnchor098)和[*第6章*](B16591_06.xhtml#_idTextAnchor121)。
- en: Ensure that you have completed [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第7章*](B16591_07.xhtml#_idTextAnchor148)。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub URL 找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08)。
- en: 'Furthermore, you can access each recipe directly from Google Colab. For example,
    the code for the first recipe of this chapter can be found here: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接从 Google Colab 访问每个食谱。例如，本章第一个食谱的代码可以在此找到：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb)。
- en: Introducing training optimization features
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍训练优化功能
- en: In the previous chapters, we saw how we could leverage *MXNet*, *GluonCV*, and
    *GluonNLP* to retrieve pre-trained models in certain datasets (such as **ImageNet**,
    **MS COCO**, or **IWSLT2015**) and use them for our specific tasks and datasets.
    Furthermore, we used transfer learning and fine-tuning techniques to improve the
    performance on those tasks/datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们展示了如何利用*MXNet*、*GluonCV*和*GluonNLP*来检索特定数据集（如**ImageNet**、**MS COCO**或**IWSLT2015**）中的预训练模型，并将其用于我们的特定任务和数据集。此外，我们还使用了迁移学习和微调技术来提高这些任务/数据集上的性能。
- en: In this recipe, we will introduce (and revisit) several concepts and features
    that will optimize our training loops, after which we will analyze the trade-offs
    involved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍（并重温）几个概念和特性，这些将优化我们的训练循环，之后我们将分析其中的权衡。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Similar to the previous chapters, in this recipe, we will be using some matrix
    operations and linear algebra, but it will not be hard at all, as you will find
    lots of examples and code snippets to facilitate your learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前几章，在本教程中，我们将使用一些矩阵操作和线性代数，但这不会很困难，因为你会发现许多示例和代码片段来帮助你学习。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will work through the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将通过以下步骤进行操作：
- en: Working with lazy evaluation and automatic parallelization
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用懒评估和自动并行化
- en: 'Optimizing DataLoaders: GPU preprocessing and CPU threads'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化DataLoader：GPU预处理和CPU线程
- en: Training with `Float32`, `Float16`, and Automatic Mixed Precision
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Float32`、`Float16`和自动混合精度进行训练
- en: Training with multiple GPUs and data parallelization
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多个GPU和数据并行化进行训练
- en: Let’s dive into each of these steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这些步骤。
- en: Working with lazy evaluation and automatic parallelization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用懒评估和自动并行化
- en: In the *NumPy and MXNet NDArrays* recipe of [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016),
    we introduced lazy evaluation, the strategy that MXNet follows when computing
    operations. This strategy is optimal for large compute loads, where the actual
    calculation is deferred until the values are actually needed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B16591_01.xhtml#_idTextAnchor016)的*NumPy和MXNet NDArrays*教程中，我们介绍了懒评估，MXNet在计算操作时采用的策略。这种策略对于大计算负载来说是最优的，因为实际的计算会被延迟，直到这些值真正需要时才会计算。
- en: Furthermore, by not executing the computation of the operations until they are
    needed, MXNet can also parallelize some of those computations, meaning the data
    involved is not sequentially processed. This process is done automatically and
    is very useful when sharing data across multiple hardware resources such as CPUs
    and GPUs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MXNet通过推迟操作计算，直到它们真正需要时，能够并行化一些计算，这意味着涉及的数据不会按顺序处理。这一过程是自动完成的，对于在多个硬件资源（如CPU和GPU）之间共享数据时非常有用。
- en: 'As a toy example, we can run some experiments with matrix multiplication. Our
    first experiment will run the generation of four matrices and then a combination
    of multiplications among them. After each computation, we will force the computation
    to be finalized (by adding calls to the `wait_to_read()` function). We will compute
    the results for two configurations. The initial configuration will be to force
    MXNet to work with one thread (`NaiveEngine`). With this configuration, the computation
    took this long:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个示例，我们可以进行一些矩阵乘法实验。我们的第一个实验将生成四个矩阵，然后进行它们之间的乘法组合。在每次计算后，我们将强制计算完成（通过添加`wait_to_read()`函数调用）。我们将计算两种配置下的结果。初始配置将强制MXNet使用一个线程（`NaiveEngine`）。在这种配置下，计算花费的时间是：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The second configuration tested will be MXNet in its usual, default configuration
    (`ThreadedEnginePerDevice`, with four CPU threads). With this configuration, the
    computation took this long:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种配置是测试MXNet的常规默认配置（`ThreadedEnginePerDevice`，四个CPU线程）。在这种配置下，计算花费的时间是：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, forcing each computation to be finalized before moving to the
    next one (`wait_to_read()` calls) is counter-productive when working in a multi-threading
    configuration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，强制每次计算在进入下一步之前完成（通过调用`wait_to_read()`）在多线程配置中是适得其反的。
- en: 'Our second experiment will be very similar; however, this time, we will remove
    all calls to the `wait_to_read()` function. We will only ensure that all calculations
    for the multiplication of the matrices are finalized before computing the time
    taken. For the initial configuration (*NaiveEngine*), the computation takes the
    following amount of time:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二次实验将非常相似；但是，这一次，我们将删除所有对`wait_to_read()`函数的调用。我们将只确保在计算时间之前，所有矩阵乘法的计算都已完成。对于初始配置（*NaiveEngine*），计算所需的时间如下：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As expected, this is a very similar duration to only working with one thread,
    as all computations are sequential.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，这个时长与仅使用一个线程时非常相似，因为所有计算都是按顺序进行的。
- en: 'With our second configuration (*ThreadedEnginePerDevice*, with four CPU threads),
    the computation for this second experiment took the following amount of time:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的第二种配置（*ThreadedEnginePerDevice*，具有四个CPU线程），第二次实验的计算时间如下：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The results show that when using multiple threads (the default automatic configuration
    for MXNet), we achieved a ~20% improvement (improvements can be even higher with
    different workloads more suited for multi-threading).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，使用多个线程（MXNet的默认自动配置）时，我们获得了约20%的提升（在更适合多线程的工作负载下，提升可能更高）。
- en: Important Note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note how in the code, we used the `mx.nd.waitall()` function to verify
    that all computations had been strictly completed before computing the time these
    operations took.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在代码中，我们使用了`mx.nd.waitall()`函数，以确保所有计算在计算操作所花费的时间之前都已严格完成。
- en: Optimizing DataLoaders – GPU preprocessing and CPU threads
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化DataLoader——GPU预处理与CPU线程
- en: In the *Understanding image datasets – loading, managing, and visualizing the
    fashion MNIST dataset* recipe of [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    we introduced **Gluon DataLoader**, an efficient mechanism to generate batch sizes
    to feed into our models for training and evaluation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B16591_02.xhtml#_idTextAnchor029)的*理解图像数据集——加载、管理和可视化时尚MNIST数据集*一节中，我们介绍了**Gluon
    DataLoader**，这是一种高效的机制，用于生成批次大小，供我们的模型用于训练和评估。
- en: DataLoader has two important roles to play in our data preprocessing. On the
    one hand, as we have explored in previous chapters, our models are optimized for
    parallel data processing, meaning that we can ingest several samples (for example,
    images for an image segmentation task) at the same time in the same *batch* and
    it will be processed in parallel by the *GPU*. This parameter is called *batch
    size*. On the other hand, samples typically need to be preprocessed in order to
    maximize the performance of the model (for example, images are resized and its
    values allocated to `[0,` `1]` from `[0,` `255]`). These operations are time-consuming
    and optimizing them can save large amounts of time and compute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader在我们的数据预处理过程中扮演着两个重要角色。首先，正如我们在前面的章节中探讨过的，我们的模型是为并行数据处理进行了优化，这意味着我们可以在同一时间处理多个样本（例如，图像分割任务中的图像），这些样本会在同一个*批次*中并行处理，且由*GPU*进行处理。这个参数称为*批次大小*。另一方面，样本通常需要进行预处理，以最大化模型的性能（例如，图像被调整大小，并将其值从`[0,`
    `255]`映射到`[0,` `1]`）。这些操作耗时，优化这些操作可以节省大量时间和计算资源。
- en: 'Let’s analyze the effect of the preprocessing of the data in the GPU, compared
    to the general, default behavior of using the CPU. As a baseline, let’s compute
    how long it takes to just load the dataset using only the CPU. We select the validation
    split of a segmentation dataset, and the result is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下将数据预处理放在GPU上与使用CPU的常规默认行为之间的效果。作为基准，我们计算仅使用CPU加载数据集所需的时间。我们选择分割数据集的验证集，结果如下：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'However, when loading the dataset, we typically apply certain `transform` operations
    that maximize our network performance. The usual transform operations including
    image resizing, cropping, transforming to tensors, and normalizing can be defined
    in MXNet with the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在加载数据集时，我们通常会应用某些`transform`操作，以最大化网络性能。常见的转换操作包括图像缩放、裁剪、转化为张量以及归一化，可以通过以下代码在MXNet中定义：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With these transform operations applied when processing the validation split
    of a segmentation dataset using only the CPU, the processing time is the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅使用CPU处理分割数据集的验证分割时，应用这些转换操作后的处理时间如下：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we can see, the processing time has increased by more than 50%, from ~24s
    to ~39s. However, when we leverage the GPU for the data preprocessing, the processing
    time is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，处理时间增加了超过50%，从大约24秒增加到大约39秒。然而，当我们利用GPU进行数据预处理时，处理时间如下：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, the GPU-based preprocessing operations have an overhead that
    is almost negligible (<5%).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，基于GPU的预处理操作几乎没有额外开销（<5%）。
- en: 'Furthermore, performing the preprocessing in the GPU has another advantage:
    the data can be kept stored in the GPU for our models to process, whereas when
    preprocessing with the CPU, we need to send a copy of the data to the GPU memory,
    which can take a significant amount of time. If we actually measure our end-to-end
    preprocessing pipeline, combining the data preprocessing with the copy operation
    to the GPU memory, these are the results. With the CPU only, the end-to-end processing
    time is the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在GPU上执行预处理还有另一个优势：数据可以保存在GPU中供我们的模型处理，而使用CPU进行预处理时，我们需要将数据复制到GPU内存中，这可能会占用大量时间。如果我们实际测量端到端的预处理流水线，将数据预处理与复制操作到GPU内存结合起来，得到的结果如下：仅使用CPU时，端到端处理时间如下：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we can see, the copy time is significant, taking the whole pipeline more
    than 1 minute. However, the result when using the GPU is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，复制时间非常长，整个流水线需要超过1分钟。然而，使用GPU时的结果如下：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This shows a significant improvement (<40%) in the time it took for the full
    preprocessing. In summary, this was due to two factors: the fact that the preprocessing
    operations are faster in the GPU, and secondly, that the data needs to be copied
    to the GPU at the end of the process so that our models (which are also stored
    in the GPU) process the data efficiently.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明完整预处理所需的时间有了显著改善（<40%）。总的来说，这是由于两个因素：首先，预处理操作在GPU上更快；其次，数据需要在过程结束时复制到GPU，这样我们的模型（也存储在GPU中）才能高效地处理数据。
- en: The most important drawback of this approach is the need to keep the full dataset
    in the GPU. Typically, GPU memory space is optimized for each batch that you use
    for training or inference, not the whole dataset. This is the reason why this
    approach typically finishes with the processed data being copied back out of the
    GPU memory space and into the CPU memory space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最主要的缺点是需要将整个数据集保存在GPU中。通常，GPU内存空间是为你在训练或推理中使用的每个批次进行优化的，而不是为整个数据集进行优化。这就是为什么这种方法通常会以将处理后的数据从GPU内存空间复制回CPU内存空间的方式结束。
- en: However, there are situations where keeping the data in the GPU memory space
    might be the right approach – for example, when you are experimenting with datasets,
    and maybe loading several datasets and testing different preprocessing pipelines.
    In these situations you want fast turn-around times for your experiments and,
    therefore, speed is the right variable to optimize for. Moreover, sometimes you
    are not working with the full training/validation/test splits of a dataset, but
    just a part of it (again, for example, for experiments). In these cases, optimizing
    for speed makes sense as well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些情况下，将数据保存在GPU内存空间可能是正确的做法——例如，当你在实验不同的数据集时，可能会加载多个数据集并测试不同的预处理流水线。在这种情况下，你希望实验能快速完成，因此速度是需要优化的变量。此外，有时你并不是在处理数据集的完整训练/验证/测试集，而只是其中的一部分（例如，为了实验）。在这种情况下，优化速度也是合理的。
- en: 'For other, more production-oriented environments, the right approach is to
    preprocess in GPU memory space but keep the data (copying back) in CPU memory
    space. In this scenario, the results vary slightly:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他更面向生产的环境，正确的方法是在GPU内存空间中进行预处理，但将数据（回复制）保留在CPU内存空间。在这种情况下，结果略有不同：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, the preprocessing step being done in the GPU is still a significant
    increase (~50%) in performance, even taking into account the necessary data movements
    from the CPU to the GPU and back.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，即使考虑到必要的数据移动（从CPU到GPU，再从GPU回到CPU），在GPU中进行预处理仍然能显著提高性能（约50%）。
- en: 'Now, we will take a deeper look at how we can leverage two important parameters
    that Gluon DataLoader takes as input: the number of workers and the batch size.
    The number of workers is the number of threads that DataLoader will launch in
    parallel (multi-threading) for data preprocessing. Batch size, as mentioned, is
    the number of samples that will be processed in parallel.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨如何利用Gluon DataLoader作为输入的两个重要参数：工作线程数和批量大小。工作线程数是DataLoader将并行启动的线程数量（多线程）用于数据预处理。批量大小，如前所述，是将并行处理的样本数量。
- en: 'These parameters are directly related to the number of cores the CPU has, and
    can be optimized to use the available HW for maximum performance. To find out
    how many cores our CPU has, Python provides a very simple API:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数与CPU的核心数量直接相关，并且可以通过优化使用可用的硬件来实现最大性能。为了了解CPU的核心数，Python提供了一个非常简单的API：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the environment selected, the number of cores available is shown as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在所选环境中，显示的可用核心数如下：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Combining the usage of the CPU and the GPU, we can compute the best performance
    taking into account different values for the number of workers and the batch size.
    The results for the selected environment are as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合使用CPU和GPU，我们可以计算最佳性能，考虑不同的工作线程数和批量大小值。为所选环境计算的结果如下：
- en: "![Figure 8.1 – Runtim\uFEFFe(s) versus Batch Size for different computation\
    \ regimes (CPU/GPU and number of workers)](img/B16591_08_1.jpg)"
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 不同计算模式下（CPU/GPU和工作线程数）运行时间与批量大小的关系](img/B16591_08_1.jpg)'
- en: Figure 8.1 – Runtime(s) versus Batch Size for different computation regimes
    (CPU/GPU and number of workers)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 不同计算模式下（CPU/GPU和工作线程数）运行时间与批量大小的关系
- en: 'From *Figure 8**.1*, we can conclude three important aspects:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图8.1*中，我们可以得出以下三个重要结论：
- en: A GPU preprocessing pipeline (data processing plus memory storage) is much faster
    (+50% runtime improvement), even when copying back the data.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU预处理管道（数据处理加内存存储）要快得多（+50% 的运行时提升），即使是将数据复制回CPU时也是如此。
- en: When combining both the GPU and CPU, as we are only working with one GPU in
    this environment, it bottlenecks when we copy the data back to the CPU, as it’s
    done per sample (not per batch).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当结合使用GPU和CPU时，由于在这个环境下我们只使用一个GPU，因此当将数据复制回CPU时会遇到瓶颈，因为数据是逐个样本复制的（而不是按批次）。
- en: If working only with a CPU, adding workers improves the processing time. However,
    the limit is the number of threads. Adding more workers than threads (four in
    our case) will give no improvement in performance. An increase in the batch size
    yields better performance until a given number (8 in our case) and won’t improve
    performance further than that.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果仅使用CPU，增加工作线程可以改善处理时间。然而，限制因素是线程的数量。添加的工作线程数超过线程数（在我们的例子中为四个）将不会提高性能。增加批量大小能够提升性能，直到达到某一数量（在我们的例子中为8），超过该数量后，性能不会进一步提高。
- en: Important Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When working with the GPU, the MXNet Gluon DataLoader only supports the value
    `0` (zero) for the number of workers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU时，MXNet Gluon DataLoader仅支持工作线程数为`0`（零）的值。
- en: Training with Float32, Float16, and automatic mixed precision
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Float32、Float16和自动混合精度进行训练
- en: In the previous recipes, we have seen how to optimize our training loops by
    using different approaches to optimize the CPU and the GPU for maximum performance
    for a given model. In this recipe, we will explore how our data inputs, our model
    parameters, and the different arithmetic calculations around them are computed,
    and how we can optimize them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们已经看到了如何通过不同的方式优化训练循环，以最大限度地提高给定模型的CPU和GPU性能。在本示例中，我们将探讨如何计算我们的数据输入、模型参数及其周围的各种算术运算，并了解如何优化它们。
- en: 'First of all, let’s understand how computations work. The default data type
    for the data inputs and the model parameters is `Float32`, as can be verified
    (see the recipe code), which yields the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解计算是如何进行的。数据输入和模型参数的默认数据类型是`Float32`，可以通过（参见示例代码）验证这一点，产生以下输出：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This output indicates, as expected, that the data type of our data inputs and
    our model parameters is `Float32` (single-precision). But what does this mean?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出结果如预期所示，表明我们的数据输入和模型参数的数据类型是`Float32`（单精度）。但这意味着什么呢？
- en: '`Float32` indicates two things: on the one hand, that it is a data type that
    supports decimal numbers using a floating radix point, and on the other hand,
    that 32 bits are used to store a single number in this format. The most important
    features of this format are the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`Float32` 表示两件事：一方面，它是一种支持使用浮动小数点表示十进制数的数据类型；另一方面，它使用 32 位来存储单个数字。此格式的最重要特性如下：'
- en: The ability to represent large numbers, from 10-45 to 10+38
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够表示大数值，从 10^-45 到 10^+38
- en: Variable precision
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可变精度
- en: 'Using `Float32` as the data type has numerous advantages, mostly connected
    to its variable precision. However, the training process is an iterative optimization
    process, for which many of the calculations involved do not require the precision
    of the `Float32` data type. We could afford, in a controlled way, to trade off
    some precision if it allowed us to speed up the training process. One of the ways
    we can achieve that balanced trade-off is with the `Float16` data type (half-precision).
    Similarly to `Float32`, the most important features of `Float16` are the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Float32` 作为数据类型有很多优点，主要与其可变精度有关。然而，训练过程是一个迭代的优化过程，其中许多计算并不需要 `Float32` 数据类型的精度。如果能以受控的方式牺牲一些精度来加速训练过程，那是可以接受的。我们可以通过
    `Float16` 数据类型（半精度）实现这种平衡的权衡。与 `Float32` 类似，`Float16` 的最重要特性如下：
- en: The ability to represent large numbers, from 2-24 to 2+16
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够表示大数值，从 2^-24 到 2^+16
- en: Variable precision
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可变精度
- en: 'As an example of the loss of precision, we can display the approximated value
    of 1/3 in both formats with this code excerpt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为精度丧失的示例，我们可以通过以下代码片段显示 1/3 的近似值，以两种格式呈现：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This yields the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As we can see, none of the representations are exact, with `Float32` yielding
    higher precision as expected, and `Float16` having more limited accuracy, but
    potentially enough for some use cases (such as model training, as we will prove
    shortly).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有表示都不是精确的，`Float32` 如预期一样提供了更高的精度，而 `Float16` 的精度更有限，但对于某些应用场景（如模型训练）可能足够，稍后我们会证明这一点。
- en: 'As mentioned, this loss of accuracy is a trade-off, where we obtain large speed
    gains in our training loops. To enable `Float16` (half-precision) for our training
    loops, we need to apply certain changes to our code. First of all, we need to
    update our model parameters to `Float16`, which we can do with one simple line
    of code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种精度丧失是一种权衡，我们在训练循环中获得了巨大的速度提升。为了在训练循环中启用 `Float16`（半精度），我们需要对代码进行一些更改。首先，我们需要将模型参数更新为
    `Float16`，这一操作只需一行简单的代码：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After this, when our model is going to process the data and ground truth, these
    need to be updated to `Float16` too, so in our training loop, we add the following
    lines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，当我们的模型处理数据和真实标签时，这些也需要更新为 `Float16`，因此在我们的训练循环中，我们加入了以下几行：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With these changes, we can now run an experiment to compare the performance
    of both training loops. For example, we are going to fine-tune a DeepLabv3 pre-trained
    model in an image segmentation task (see the *Improving performance for segmenting
    images* recipe of [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148)). For `Float32`,
    we obtain the following results:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些更改，我们现在可以运行一个实验，比较两种训练循环的性能。例如，我们将微调一个 DeepLabv3 预训练模型，进行图像分割任务（参见 [*第 7
    章*](B16591_07.xhtml#_idTextAnchor148) 中的 *改善图像分割性能* 配方）。对于 `Float32`，我们得到以下结果：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For `Float16`, these are the results we obtained:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `Float16`，我们获得了以下结果：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Unfortunately, for `Float16`, although our training time took ~1/3rd than the
    `Float32` training loop, it did not converge. This is due to several reasons:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于 `Float16`，尽管我们的训练时间比 `Float32` 的训练循环少了约三分之一，但它并未收敛。这是由于几个原因：
- en: Limited support for large numbers, as any integer larger than `65519` is represented
    as infinity
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对大数值的支持有限，因为任何大于 `65519` 的整数都表示为无穷大
- en: Limited support for small numbers, as any positive decimal number smaller than
    `1e-7` is represented as `0` (zero)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对小数值的支持有限，因为任何小于 `1e-7` 的正十进制数都表示为 `0`（零）
- en: 'Thankfully, MXNet offers a solution that automatically combines the best of
    both worlds:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，MXNet 提供了一个解决方案，能够自动结合两者的优点：
- en: Applying `Float32` (single-precision) where it is necessary
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在必要的地方应用 `Float32`（单精度）
- en: Applying `Float16` (half-precision) where it is not, for runtime optimization
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有使用的地方应用 `Float16`（半精度），以优化运行时
- en: 'This approach is called **Automatic Mixed Precision** (**AMP**), and in order
    to enable it, we just need to make a few changes in our code. First of all, before
    creating our model, we need to initialize the library:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**自动混合精度**（**AMP**），为了启用它，我们只需要在代码中进行一些更改。首先，在创建模型之前，我们需要初始化库：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, after initializing our trainer/optimizer, we need to link it with AMP:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在初始化训练器/优化器之后，我们需要将其与AMP链接：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And finally, in order to prevent underflow or overflow, we need to enable `Float16`
    data type. This is done quite conveniently in the training loop:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了防止下溢或溢出，我们需要启用`Float16`数据类型。这在训练循环中非常方便地实现：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we apply these changes and repeat the previous experiment for `Float16`
    (now with AMP enabled), we obtain the following results:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用这些变化并为`Float16`（现在启用了AMP）重复之前的实验时，得到了以下结果：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see, we obtained very similar results for the validation loss in a
    much shorter amount of time (~33%).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们在更短的时间内获得了非常相似的验证损失结果（约33%）。
- en: 'As the memory footprint of our training loop is now approximately half of what
    it was before, we can typically either double the size of our model (more layers
    and larger resolutions), or double our batch size, as the GPU memory consumed
    will be the same in this case compared to a full `Float32` training loop. Running
    the same experiment with a double batch size yields the following results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练循环的内存占用大约是之前的一半，我们通常可以将模型的大小翻倍（更多的层和更大的分辨率），或者将批量大小翻倍，因为GPU内存的消耗在这种情况下与完整的`Float32`训练循环相比是相同的。使用双倍批量大小运行相同的实验得到以下结果：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, increasing the batch size has an excellent effect on the performance
    of our training loop, with a much lower validation loss, and still benefiting
    from a significantly smaller amount of training time (~33%).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，增加批量大小对训练循环的性能有着非常好的影响，验证损失大大降低，并且训练时间也显著缩短（约33%）。
- en: 'However, typically, as a **Machine Learning Engineer** (**MLE**) or **Data
    Scientist** (**DS**), we will work with large amounts of data and large models,
    running training loops expected to last for hours or days. Therefore, it is very
    common for MLEs/DSs at work to start training loops just before the end of the
    working day, leaving the training running in the background, and coming back the
    next working day to analyze and evaluate the results. In such an environment,
    it is actually a better strategy to optimize performance given an expected training
    time. With MXNet, we can optimize our training parameters for this as well. For
    example, we could adjust the training time by doubling the number of epochs. In
    this case, the experiment yields the following results:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常作为**机器学习工程师**（**MLE**）或**数据科学家**（**DS**），我们将处理大量数据和大型模型，运行训练循环，预计需要持续数小时或数天。因此，在工作中，MLEs/DSs通常会在工作日结束前启动训练循环，留下训练在后台运行，并在下一个工作日回来分析和评估结果。在这种环境下，实际上优化预期训练时间以提升性能是一种更好的策略。使用MXNet，我们也可以为此优化训练参数。例如，我们可以通过将训练轮数翻倍来调整训练时间。在这种情况下，实验得到了以下结果：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Compared to a vanilla `Float32` training loop, these results are excellent.
    However, let’s not forget that the actual results depend on the specific task,
    datasets, model, hyperparameters, and so on. You are encouraged to try different
    options and hyperparameters with toy training loops to find the solution that
    works best for each case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准的`Float32`训练循环相比，这些结果非常好。然而，我们不要忘记，实际结果取决于特定任务、数据集、模型、超参数等。建议您在玩具训练循环中尝试不同的选项和超参数，以找到每种情况的最佳解决方案。
- en: Training with multiple GPUs and data parallelization
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个GPU和数据并行化训练
- en: In this recipe, we will leverage having multiple GPUs in our environment to
    optimize our training further. MXNet and Gluon allow us to update our training
    loops to include multiple GPUs very easily.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将利用环境中多个GPU进一步优化训练。MXNet和Gluon让我们可以非常轻松地更新训练循环以包含多个GPU。
- en: 'From a high-level perspective, there are two paradigms to leverage multiple
    GPUs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，利用多个GPU有两种范式：
- en: '**Model parallelization**: The model is split into parts and each part is deployed
    to a specific GPU. This paradigm is very useful when the model does not fit in
    a single GPU.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型并行化**：将模型分割成多个部分，并将每个部分部署到特定的GPU上。当模型无法适配单个GPU时，这种范式非常有用。'
- en: '**Data parallelization**: The data batches are split into parts and each part
    is deployed to a specific GPU that can perform a forward and a backward pass using
    that data fully.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据并行化**：数据批次被拆分成多个部分，每个部分会被分配到一个特定的 GPU 上，该 GPU 能够完全使用这些数据进行前向和反向传播。'
- en: We will work exclusively with data parallelization as it is the most common
    use case, yielding high speed-ups, and is also the most convenient given the simplicity
    of its approach.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只使用数据并行化，因为它是最常见的用例，能带来很高的加速，并且由于其方法的简单性，它也最为便捷。
- en: 'In order to apply data parallelization, we will need to make modifications
    to our training loop, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用数据并行化，我们需要对训练循环进行如下修改：
- en: '**Setting the context**: The context is now a list, where each element is a
    specific GPU context.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置上下文**：上下文现在是一个列表，每个元素是一个特定的 GPU 上下文。'
- en: '**Initializing our model in those contexts**: In data parallelization, each
    GPU will store a copy of all the model parameters.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在这些上下文中初始化我们的模型**：在数据并行化中，每个 GPU 都会存储一份所有模型参数的副本。'
- en: '**Adapting hyperparameters**: Batch size is typically set to the largest possible
    without filling up the GPU memory. When working with several GPUs in parallel,
    this number can typically be multiplied by the number of GPUs in the context.
    However, this also has a side effect on the learning rate, which must be multiplied
    by the same number to keep gradient updates in the same range.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整超参数**：批量大小通常设置为尽可能大的值，而不填满 GPU 内存。当多个 GPU 并行工作时，这个数字通常可以乘以上下文中 GPU 的数量。然而，这也会对学习率产生副作用，必须将学习率乘以相同的数字，以保持梯度更新在相同的范围内。'
- en: '**Distributing the data**: Each GPU must have a slice of each batch and run
    the forward and backward passes with it.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配数据**：每个 GPU 必须拥有每个批次的一部分，并使用它进行前向和反向传播。'
- en: '**Computing the losses and updating the gradients**: Each GPU will compute
    the losses associated with their slice of each batch. MXNet automatically combines
    the losses and computes the gradients that are distributed to each GPU to update
    their model copy.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算损失并更新梯度**：每个 GPU 会计算与其批次切片相关的损失。MXNet 会自动结合这些损失并计算梯度，然后将其分发到每个 GPU，以更新它们的模型副本。'
- en: '**Displaying results**: Statistics such as the training loss and the validation
    loss are typically computed and accumulated during each batch and visualized at
    the end of each epoch.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**显示结果**：训练损失和验证损失等统计信息通常会在每个批次中计算并积累，并在每个周期结束时进行可视化。'
- en: Let’s see some examples of how to apply each of these steps.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些如何应用这些步骤的例子。
- en: 'For example, to set the context in an environment with four GPUs is very easy
    with MXNet and just requires one line of code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个包含四个 GPU 的环境中设置上下文非常简单，使用 MXNet 只需要一行代码：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Initializing the model and custom layers is as easy as that. For our environment,
    this is how we can initialize a Deeplabv3 network with a `ResNet-101` backbone:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化模型和自定义层就这么简单。对于我们的环境，以下是如何初始化带有 `ResNet-101` 主干的 Deeplabv3 网络：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To update the hyperparameters, we just need to compute the number of GPUs in
    the context and update the previously computed batch size and learning rates.
    For our example, this simply means adding/modifying some lines of code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新超参数，我们只需要计算上下文中的 GPU 数量，并更新之前计算的批量大小和学习率。对于我们的示例，这意味着只需添加或修改几行代码：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In order to distribute the data evenly across every GPU, MXNet and Gluon have
    a very convenient function, `split_and_load()`, which automatically allocates
    the data according to the number of GPUs in the context. For our environment,
    this is done as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据均匀地分配到每个 GPU 上，MXNet 和 Gluon 提供了一个非常方便的函数 `split_and_load()`，它会根据上下文中的
    GPU 数量自动分配数据。在我们的环境中，操作如下：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To compute the losses and update the gradients, the data distributed in each
    GPU is processed in parallel using a loop. As MXNet provides automatic parallelization,
    the calls are not blocking and each GPU computes its outputs and losses independently.
    Furthermore, MXNet combines those losses to generate the full gradient updates,
    and redistributes this to each GPU, and all of this is done automatically. We
    can achieve all this with just a few lines of code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算损失并更新梯度，分布在每个 GPU 上的数据会通过循环并行处理。由于 MXNet 提供了自动并行化，这些调用是非阻塞的，每个 GPU 独立计算其输出和损失。此外，MXNet
    会将这些损失结合起来生成完整的梯度更新，并将其重新分配给每个 GPU，所有这些操作都是自动完成的。我们只需要几行代码即可完成这一切：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Lastly, in order to display the loss computations, each GPU loss needs to be
    processed and combined. Using automatic parallelization, this can be achieved
    with just one line of code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了显示损失计算，需要处理每个GPU的损失并将其组合。使用自动并行化，可以通过一行代码轻松实现这一点：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With these simple steps, we have been able to modify our training loop to support
    multiple GPUs, and we are now ready to measure the performance increase of these
    changes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些简单的步骤，我们已经能够修改我们的训练循环以支持多个GPU，并且现在可以测量这些变化带来的性能提升。
- en: 'As a reminder, using one GPU, we reached the following performance (with a
    batch size of four):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '作为提醒，使用一个GPU，我们达到了以下性能（批处理大小为四）:'
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In our environment, with 4 GPUs, we could increase the batch size to 16, the
    results of which would be as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的环境中，使用4个GPU，我们可以将批处理大小增加到16，其结果如下：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As expected, we have been able to reduce the time spent in training to ~25%
    (the expected reduction when going from 1 GPU to 4 GPUs, with some expected loss
    due to the data distribution) while maintaining our validation scores (even slightly
    improving them).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，我们已经能够将训练时间减少到约25%（从1个GPU到4个GPU时的预期减少量，由于数据分布的预期损失而稍微改善了验证分数）。
- en: How it works...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理如下…
- en: 'In this recipe, we take a deeper look at how MXNet and Gluon can help us optimize
    our training loops. We have leveraged our HW (CPUs and GPUs) to address each of
    the steps in the training loop:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们深入探讨了如何利用MXNet和Gluon优化我们的训练循环。我们利用我们的硬件（CPU和GPU）来处理训练循环中的每一个步骤：
- en: We revisited how lazy evaluation and automatic parallelization mechanisms work
    together to optimize all MXNet-based flows.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重新审视了惰性评估和自动并行化机制如何共同作用以优化所有基于MXNet的流程。
- en: We leveraged all our CPU threads to load data and optimized that process further
    via preprocessing in GPU. We also compared the trade-offs between speed and memory
    optimizations.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们利用所有的CPU线程来加载数据，并通过GPU中的预处理进一步优化该过程。我们还比较了速度和内存优化之间的权衡。
- en: We analyzed different data types and combined the accuracy and precision of
    `Float32` with the speed-ups of `Float16` where possible, using AMP.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们分析了不同的数据类型，并在可能的情况下将`Float32`的精度与`Float16`的加速结合起来，使用AMP。
- en: We increased the performance of our training loops by using multiple GPUs (assuming
    our HW has these devices available).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过使用多个GPU（假设我们的硬件有这些设备可用）提升了训练循环的性能。
- en: 'We compared each of these scenarios by running two experiments, comparing the
    performance before a specific optimization to the performance afterward, emphasizing
    potential trade-offs that have to be taken into account when using these optimizations.
    In the recipes that follow, we will apply all these optimization techniques concurrently
    to optimize two familiar tasks: **image segmentation** and **text translation**.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行两个实验比较了每种场景，在特定优化之前和之后的性能，并强调了在使用这些优化时需要考虑的潜在权衡。在接下来的配方中，我们将同时应用所有这些优化技术，优化两个熟悉的任务：**图像分割**和**文本翻译**。
- en: There’s more…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'All the optimization features shown in this recipe have been thoroughly described
    in the research literature. The following are some introductory links to start
    understanding each of the features in depth:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方中展示的所有优化特性都已在研究文献中进行了详细描述。以下是一些入门链接，以深入了解每个特性：
- en: '**Lazy evaluation and automatic** **parallelization:** [https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization](https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**惰性评估和自动** **并行化:** [https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization](https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization)'
- en: '**Gluon** **DataLoaders:** [**https:**//mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gluon** **DataLoaders:** [**https:**//mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html)'
- en: '**AMP:** [https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0](https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AMP:** [https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0](https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0)'
- en: '**Training with multiple** **GPUs:** [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用多个GPU进行训练:** [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html)'
- en: Optimizing training for image segmentation
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化图像分割的训练
- en: In the previous recipe, we saw how we could leverage MXNet and Gluon to optimize
    the training of our models with a variety of different techniques. We understood
    how we can jointly use lazy evaluation and automatic parallelization for parallel
    processing. We saw how to improve the performance of our DataLoaders by combining
    preprocessing in the CPU and GPU, and how using half-precision (`Float16`) in
    combination with AMP can halve our training times. Lastly, we explored how to
    take advantage of multiple GPUs to further reduce training times.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们展示了如何利用MXNet和Gluon通过各种技术来优化模型的训练。我们了解了如何联合使用懒惰求值和自动并行化来进行并行处理。我们看到如何通过结合在CPU和GPU上进行预处理来提高DataLoader的性能，以及如何使用半精度（`Float16`）与AMP结合来减少训练时间。最后，我们探索了如何利用多个GPU进一步减少训练时间。
- en: 'Now, we can revisit a problem we have been working with throughout the book:
    **image segmentation**. We have worked on this task in recipes from previous chapters.
    In the *Segmenting objects semantically with MXNet Model Zoo – PSPNet and DeepLabv3*
    recipe in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), we learned how to use
    pre-trained models from GluonCV Model Zoo, and introduced the task and the datasets
    that we will be using in this recipe: **MS COCO** and the **Penn-Fudan Pedestrian**
    dataset. Furthermore, in the *Improving performance for segmenting images* recipe
    in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148)*, Optimizing Models with Transfer
    Learning and Fine-Tuning* we compared the different approaches that we could take
    when dealing with a target dataset, training our models from scratch, or leveraging
    the existing knowledge of pre-trained models and adjusting it for our task using
    the different modalities of transfer learning and fine-tuning.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重新审视一个贯穿全书的课题：**图像分割**。我们在前几章的食谱中曾处理过这个任务。在[*第5章*](B16591_05.xhtml#_idTextAnchor098)中的*使用MXNet
    Model Zoo进行语义化物体分割——PSPNet和DeepLabv3*食谱中，我们学习了如何使用GluonCV Model Zoo中的预训练模型，并介绍了我们将在本食谱中使用的任务和数据集：**MS
    COCO**和**Penn-Fudan Pedestrian**数据集。此外，在[*第7章*](B16591_07.xhtml#_idTextAnchor148)中的*提高图像分割性能*食谱中，我们比较了处理目标数据集时可以采取的不同方法——是从头开始训练模型，还是利用预训练模型的现有知识并通过不同的迁移学习和微调方式进行调整。
- en: In this recipe, we will apply all these optimization techniques for the specific
    task of training an image segmentation model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将应用所有这些优化技术，以训练图像分割模型为具体任务。
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Similar to previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all, as you will find lots of examples
    and code snippets to facilitate your learning.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节类似，在本食谱中，我们将使用一些矩阵运算和线性代数，但这并不难，因为你会发现很多示例和代码片段来帮助你学习。
- en: How to do it...
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将探讨以下步骤：
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新审视我们当前的预处理和训练流程
- en: Applying training optimization techniques
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用训练优化技术
- en: Analyzing the results
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析结果
- en: Let’s dive into each of these steps.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个步骤。
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新审视我们当前的预处理和训练流程
- en: 'In the *Improving performance for segmenting images* recipe in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148),
    we processed the data with the following approach:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B16591_07.xhtml#_idTextAnchor148)中的*提高图像分割性能*食谱中，我们使用以下方法处理数据：
- en: Loaded the data from storage into the *CPU* *memory space*
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据从存储加载到*CPU* *内存空间*
- en: Preprocessed the data using the *CPU*
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*CPU*预处理数据
- en: Used the **default parameters** to process the data during training
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**默认参数**在训练过程中处理数据
- en: This was a valid approach to compare the different training alternatives available
    to us (training from scratch, pre-trained models, transfer learning, and fine-tuning)
    without adding complexity to the experiments. For example, this approach worked
    quite well to introduce and evaluate the technique of fine-tuning directly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是一个有效的途径，用来比较我们可用的不同训练方案（从头开始训练、预训练模型、迁移学习和微调），而无需为实验增加复杂性。例如，这种方法在直接引入并评估微调技术时效果很好。
- en: 'Following the aforementioned approach on the dataset selected for this recipe
    (*Penn-Fudan Pedestrian*), the CPU-based preprocessing took the following amount
    of time:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述方法，在为此食谱选择的数据集（*Penn-Fudan Pedestrian*）上，基于 CPU 的预处理花费了以下时间：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Furthermore, when combined with the necessary step of reloading the data in
    batches and copying it to the GPU, we obtain the following performance:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当与必要的步骤结合使用，如将数据批量重新加载并复制到 GPU 时，我们获得了以下性能：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations directly by using the
    technique of fine-tuning. In combination with this approach, we will use the following
    hyperparameters:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理之后，下一步是训练过程。如前所述，我们将通过直接使用微调技术来评估训练优化的效果。结合这种方法，我们将使用以下超参数：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In these conditions, the training process duration and performance achieved
    were as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些条件下，训练过程的持续时间和所达到的性能如下：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we can see, we got an excellent validation performance (~0.09) in a little
    over 10 minutes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在 10 分钟多一点的时间内，我们获得了优秀的验证性能（约为 0.09）。
- en: 'The evolution of the training loss and the validation loss across each epoch
    looks as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 epoch 中训练损失和验证损失的演变如下：
- en: '![Figure 8.2 – Revisiting training: training loss versus validation loss](img/B16591_08_2.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 回顾训练：训练损失与验证损失](img/B16591_08_2.jpg)'
- en: 'Figure 8.2 – Revisiting training: training loss versus validation loss'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 回顾训练：训练损失与验证损失
- en: From *Figure 8**.2*, we can see the evolution of the training and validation
    loss. As explored throughout the chapters, we select the model that provide the
    minimal validation loss (in this case, this was achieved in the last epoch, epoch
    10).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 8.2*中，我们可以看到训练损失和验证损失的演变。正如本书各章节所探讨的那样，我们选择提供最小验证损失的模型（在这种情况下，这是在最后一个 epoch，即
    epoch 10 中实现的）。
- en: 'After the training is completed, we can verify the overall performance in the
    test split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完成后，我们可以验证数据集测试集上的整体性能。从定量角度来看，以下是我们获得的结果：
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As expected, we got excellent results by training for just a limited number
    of epochs (10 in this case).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，通过仅训练有限的 epoch 数（此处为 10），我们获得了优异的结果。
- en: 'From a qualitative point of view, this is what we have:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，结果如下：
- en: '![Figure 8.3 – Revisiting training: GroundTruth example and Prediction post-training](img/B16591_08_3.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 回顾训练：GroundTruth 示例和训练后的预测](img/B16591_08_3.jpg)'
- en: 'Figure 8.3 – Revisiting training: GroundTruth example and Prediction post-training'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 回顾训练：GroundTruth 示例和训练后的预测
- en: As expected, the results show how the model has learned to focus on the people
    in the foreground, avoiding the ones in the background.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，结果展示了模型如何学会将焦点集中在前景中的人身上，避免背景中的人。
- en: Applying training optimization techniques
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用训练优化技术
- en: In the *Introducing training optimization features* recipe at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take when training a machine learning
    model, including preprocessing the data and training and evaluating the model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头的*引入训练优化功能*食谱中，我们展示了不同的优化技术如何提高训练机器学习模型过程中各个步骤的性能，包括数据预处理、模型训练和评估。
- en: In this section, we will show how, with MXNet and Gluon and just a few lines
    of code, we can easily apply all the techniques we’ve been introduced to.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何通过使用 MXNet 和 Gluon，仅用几行代码，我们可以轻松应用所有我们已经介绍过的技术。
- en: As shown in the first recipe of this chapter, MXNet applies by default the best
    policy (`ThreadedEnginePerDevice`) to optimize lazy evaluation and automatic parallelization,
    taking into account the number of CPU threads available, so there is no need for
    us to apply any changes here (please note that this technique is also applied
    automatically when working with multiple GPUs).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章第一个示例所示，MXNet默认应用最佳策略（`ThreadedEnginePerDevice`）来优化惰性求值和自动并行化，考虑到可用的CPU线程数，因此我们无需在此进行任何更改（请注意，当使用多个GPU时，这项技术也会自动应用）。
- en: 'We also showed how we could optimize our data preprocessing pipeline by combining
    the usage of CPU threads and GPUs, taking into account the number of devices available
    for each, and optimizing accordingly. For this experiment, specific HW was chosen
    with the following characteristics:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了如何通过结合使用CPU线程和GPU来优化数据预处理管道，考虑到每种设备的数量，并据此进行优化。为了进行此实验，选择了具有以下特征的特定硬件：
- en: '[PRE39]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In order to use this optimization technique, we had to apply some changes to
    our code. Specifically, we define the GPUs available for use:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这种优化技术，我们需要对代码做一些更改。具体来说，我们需要定义可供使用的GPU：
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Furthermore, in our preprocessing pipeline, we now need a specific step that
    takes the data from CPU memory space and copies it to GPU memory space:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们的预处理管道中，我们现在需要一个特定的步骤，将数据从CPU内存空间复制到GPU内存空间：
- en: '[PRE41]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As discussed in the first recipe of this chapter, in a typical production-oriented
    environment, we do not want to keep the data in the GPU, occupying precious GPU
    memory. It is usual to optimize the batch size for the GPU memory available, and
    to load the data from the CPU memory space into the GPU memory space in batches
    using *MXNet Gluon DataLoaders*. Therefore, for our GPU-based preprocessing pipeline
    to be complete, we need a final step to copy the data back into the CPU memory
    space:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章第一个示例所讨论的，在典型的面向生产的环境中，我们不希望将数据保留在GPU中，以免占用宝贵的GPU内存。通常会根据GPU可用内存优化批量大小，并使用*MXNet
    Gluon DataLoaders*将数据从CPU内存空间批量加载到GPU内存空间。因此，为了使我们的基于GPU的预处理管道完整，我们需要一个最终步骤，将数据复制回CPU内存空间：
- en: '[PRE42]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'With these code changes, our optimal preprocessing pipeline is ready, and we
    can continue with the next optimization technique: applying `Float16` optimizations,
    including AMP.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些代码更改，我们的最佳预处理管道已经准备就绪，可以继续进行下一个优化技术：应用`Float16`优化，包括AMP。
- en: 'As shown in the first recipe of this chapter, in order to enable this technique,
    we just need a few changes in our code. First of all, we initialize the library:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章第一个示例所示，为了启用这项技术，我们只需要对代码进行一些更改。首先，我们初始化库：
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Secondly, we attach the trainer/optimizer to the library:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将训练器/优化器附加到库中：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And lastly, due to the limitations of the `Float16` data type, there is a risk
    of gradients over/under-flowing; therefore, we need to adjust (scale) the loss
    accordingly, which can be done automatically with these lines of code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于`Float16`数据类型的局限性，存在梯度溢出/下溢的风险；因此，我们需要根据情况调整（缩放）损失，这可以通过以下几行代码自动完成：
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: With these three simple changes, we have updated our training loop to work efficiently
    with the `Float16` data type (when appropriate).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这三项简单的更改，我们已经更新了训练循环，使其能够有效地使用`Float16`数据类型（在适当的情况下）。
- en: 'Please note in the preceding code snippet how we are now working with a list
    of losses, instead of a single instance. This is due to our next and last training
    optimization technique: working with *multiple GPUs*.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在前面的代码片段中，我们现在正使用一个损失列表，而不是单一的实例。这是由于我们的下一个也是最后一个训练优化技术：使用*多个GPU*。
- en: As we will see, working with multiple GPUs optimally implies working with them
    in parallel, and therefore, computing losses and executing the training backward
    pass in parallel, yielding the losses list described in the previous paragraph.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，优化地使用多个GPU意味着将它们并行工作，因此，需要并行计算损失并执行训练的反向传播，从而生成前述段落中描述的损失列表。
- en: 'In order to work with multiple GPUs in parallel, we need to define the new
    context as a list (seen before for preprocessing, and shown here again for convenience):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行使用多个GPU，我们需要将新的上下文定义为一个列表（之前在预处理部分出现过，这里为了方便再次展示）：
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As we now have multiple GPUs, we can increase our batch size to optimally use
    the available GPU memory space:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现在我们有多个GPU，我们可以增加批量大小，以便最佳利用可用的GPU内存空间：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Furthermore, when reading from Gluon DataLoaders, we need to split the batches
    of data across the GPUs. Thankfully, Gluon also provides a function that simplifies
    that action. We just need the following lines of code to be added (for each training
    and validation batch):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在从Gluon DataLoader读取数据时，我们需要将数据批次分配到多个GPU上。幸运的是，Gluon还提供了一个简化该操作的功能。我们只需要添加以下几行代码（对于每个训练和验证批次）：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As mentioned, this split across GPUs allows us to compute in parallel the model
    outputs and the losses associated with those outputs (a measure of the difference
    between the actual outputs and the expected outputs). This can be achieved with
    the following lines of code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种跨GPU的划分使我们能够并行计算模型输出及与这些输出相关的损失（衡量实际输出与预期输出之间的差异）。这可以通过以下几行代码实现：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And lastly, we compute the backward pass used to update the weights of our
    model (combined with the scaled loss of AMP):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算用于更新模型权重的反向传播过程（结合AMP的缩放损失）：
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With these minimal code changes, we now have an optimal preprocessing and training
    pipeline and can run our experiments to analyze the performance changes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些最小的代码更改，我们现在拥有了一个最佳的预处理和训练管道，可以运行实验以分析性能变化。
- en: Analyzing the results
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析结果
- en: In the previous sections, we revisited the previous performance of our preprocessing
    and training pipelines, and we reviewed how we had to apply the necessary changes
    for our training optimization techniques, specifically for our image segmentation
    task.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们回顾了预处理和训练管道的先前性能，并回顾了我们如何应用必要的更改以实现训练优化技术，特别是针对我们的图像分割任务。
- en: 'Our preprocessing pipeline steps are now the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预处理管道步骤现在如下：
- en: Load the data from storage into CPU memory space.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从存储中加载数据到CPU内存空间。
- en: Preprocess the data using the GPU.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GPU预处理数据。
- en: Copy back the data to CPU memory space.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据复制回CPU内存空间。
- en: Use the optimized parameters to process the data during training.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用优化的参数在训练过程中处理数据。
- en: For our experiments, we are going to use the technique of fine-tuning directly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将直接使用微调技术。
- en: 'Applying the approach described earlier on the dataset selected for this recipe
    (*Penn-Fudan Pedestrian*), the preprocessing took the following amount of time:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 将之前描述的方法应用于为本方案选择的数据集（*Penn-Fudan Pedestrian*），预处理的时间如下：
- en: '[PRE51]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'An end-to-end preprocessing pipeline must take into account the process of
    batching using the *Gluon DataLoader* to load the data – in our case, into multiple
    GPUs as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端的预处理管道必须考虑使用*Gluon DataLoader*加载数据的批处理过程——在我们的情况下，将数据加载到多个GPU中，如下所示：
- en: '[PRE52]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Compared to the initial section of this recipe (where the preprocessing took
    `0.4` seconds), we can see how, even with the added overhead of copying back the
    data to the CPU memory space, we have improved the preprocessing performance by
    >2 times.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 与本方案的初始部分相比（当时预处理需要`0.4`秒），我们可以看到，即使在将数据复制回CPU内存空间的额外开销下，我们仍然将预处理性能提高了>2倍。
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations using the technique
    of fine-tuning directly. In combination with this approach, we use the following
    hyperparameters:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理之后，下一步是训练过程。正如前面所描述的，我们将直接使用微调技术来评估我们训练优化的效果。结合这种方法，我们使用以下超参数：
- en: '[PRE53]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Please note how, by adding multiple GPUs to the training process, we can increase
    the batch size (multiplied by the number of GPUs), and we can also increase the
    learning rate (from 0.1 to 0.5). In these conditions, the training process duration
    and performance achieved were as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过将多个GPU添加到训练过程中，我们可以增加批量大小（乘以GPU的数量），还可以增加学习率（从0.1增加到0.5）。在这些条件下，训练过程的持续时间和实现的性能如下：
- en: '[PRE54]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As can be seen, we got excellent validation performance (~0.09) in less than
    1 minute. When comparing with the results obtained in the recipe, we can see how
    there was a minimal decrease in the loss (a positive change that we will confirm
    with our performance analysis shortly), but the largest improvement by far was
    a >10x decrease in the training time. This improvement is due to all the training
    optimization techniques that we have applied. In a nutshell, each of the optimizations
    provided the following improvements:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们在不到1分钟的时间内就得到了优秀的验证表现（约0.09）。与配方中获得的结果相比，我们可以看到损失的减少非常小（这将通过我们的性能分析进一步确认），但迄今为止最大的一项改进是训练时间减少了>10倍。这个改进归功于我们应用的所有训练优化技术。简而言之，每项优化都提供了以下改进：
- en: '**Using 4 GPUs**: Provided a 4x decrease in time'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用4个GPU**：提供了4倍的时间缩短'
- en: '**Using Float16 and AMP**: Provided a 2x decrease (8x combined)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Float16和AMP**：提供了2倍的时间减少（合计8倍）'
- en: '**Preprocessing the datasets**: Provided a 1.25x decrease (>10x combined)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理数据集**：提供了1.25倍的时间减少（合计>10倍）'
- en: 'The evolution of the training loss and the validation loss across each epoch
    was the following:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 每个epoch中的训练损失和验证损失的变化如下：
- en: '![Figure 8.4 – Optimized training: training loss versus validation loss](img/B16591_08_4.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 优化训练：训练损失与验证损失](img/B16591_08_4.jpg)'
- en: 'Figure 8.4 – Optimized training: training loss versus validation loss'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 优化训练：训练损失与验证损失
- en: From *Figure 8**.4*, we can see the evolution of the training and validation
    losses. As explored throughout the chapters so far, we select the model that provided
    the minimal validation loss (in this case, achieved in the last epoch, epoch 10).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 8.4*中，我们可以看到训练损失和验证损失的变化。正如本章至今所探讨的，我们选择提供最小验证损失的模型（在这种情况下，在最后一个epoch，即第10个epoch中取得）。
- en: 'After training is completed, we can verify the overall performance in the test
    split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以在数据集的测试分割上验证整体性能。从定量的角度来看，我们获得的结果如下：
- en: '[PRE55]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As expected, we got excellent results just by training for a limited number
    of epochs (10 in this case). We can also confirm that the minimal improvement
    in the validation loss provided a minimal improvement in our test metrics (compared
    with 0.96/0.91 in our initial experiment).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，仅通过训练有限的epoch（本例中为10个epoch），我们就得到了优秀的结果。我们还可以确认，验证损失的最小改善带来了测试指标的微小改进（与我们初始实验中的0.96/0.91相比）。
- en: 'From a qualitative point of view, we have the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性的角度来看，我们得到了以下结果：
- en: '![Figure 8.5 – Optimized training: GroundTruth example and Prediction post-training](img/B16591_08_5.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 优化训练：GroundTruth 示例与训练后预测](img/B16591_08_5.jpg)'
- en: 'Figure 8.5 – Optimized training: GroundTruth example and Prediction post-training'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 优化训练：GroundTruth 示例与训练后预测
- en: As expected, the results show how the model has learned to focus on the different
    people in the foreground, avoiding the ones in the background.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，结果显示模型已经学会了将注意力集中在前景中的不同人物上，避免了背景中的人物。
- en: How it works...
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In this recipe, we applied the different training optimization techniques seen
    in the first recipe of this chapter, leveraging our HW (CPUs and GPUs) to address
    each of the steps in the training loop:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们应用了本章第一部分中的不同训练优化技术，利用我们的硬件（CPU和GPU）来解决训练循环中的每个步骤：
- en: We revisited how lazy evaluation and automatic parallelization mechanisms worked
    together to optimize all MXNet-based flows.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重新审视了惰性评估和自动并行化机制如何协同工作，以优化所有基于MXNet的流程。
- en: We leveraged all our CPU threads to load data and optimized that process further
    via preprocessing in the GPU. We also compared the trade-offs between speed and
    memory optimizations.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们利用所有CPU线程加载数据，并通过在GPU上进行预处理进一步优化了该过程。我们还比较了速度和内存优化之间的权衡。
- en: We analyzed different data types and combined the accuracy and precision of
    `Float32` with the speed-ups of `Float16` where possible, using AMP.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们分析了不同的数据类型，并结合了`Float32`的准确性与`Float16`的加速效果（在可能的情况下），并使用了AMP。
- en: We increased the performance of our training loops by using multiple GPUs (assuming
    our HW has these devices available).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过使用多个GPU提高了训练循环的性能（假设我们的硬件有这些设备）。
- en: We compared each of these scenarios applied specifically to the task of image
    segmentation, running two experiments. In the first experiment, we did not apply
    any of the training optimization techniques described in the previous recipe,
    following the approach seen in previous chapters of the book. In the second experiment,
    we applied all the techniques in parallel, trying to optimize as much as we could.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些场景分别应用于图像分割任务，并进行了两次实验。在第一次实验中，我们没有应用前面章节中描述的任何训练优化技术，而是遵循了书中前几章提到的方法。在第二次实验中，我们并行应用了所有技术，尽可能进行优化。
- en: This proved quite useful, delivering similar algorithmic performance, with 10x
    improvement in training time (from 10 minutes to 1 minute). This was mostly due
    to using multiple GPUs (4x decrease), leveraging `Float16` AMP (2x decrease),
    and the optimized preprocessing (1.25x decrease).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这一方法非常有用，提供了类似的算法性能，同时将训练时间提高了10倍（从10分钟缩短到1分钟）。这主要得益于使用了多个GPU（减少了4倍），利用`Float16`
    AMP（减少了2倍）以及优化的预处理（减少了1.25倍）。
- en: There’s more…
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: We have described, implemented, executed, and evaluated several training optimization
    techniques. However, there are even more advanced techniques that can be leveraged
    to achieve the optimal training loop.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经描述、实现、执行并评估了几种训练优化技术。然而，还有更多先进的技术可以用来实现最佳的训练循环。
- en: 'One such technique is **learning rate schedules**. Throughout the book, we
    have been working with constant learning rates. However, there are multiple advantages
    of using a dynamically adjusted learning rate. Some of them are as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种技术是**学习率调度**。在本书中，我们一直使用常数学习率。然而，使用动态调整的学习率有多个优点，其中一些如下：
- en: '**Warmup**: When working with pre-trained models, it’s not advisable to start
    with a large learning rate. The initial epochs must be used for the gradients
    to start adjusting. This can be thought of as a way of *adjusting the model from
    the source task to the target task*, retaining and leveraging the knowledge from
    the previous task, so smaller learning rates are recommended.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预热**：在使用预训练模型时，不建议从较大的学习率开始。初始的几个epoch必须用于梯度的调整。这可以看作是*将模型从源任务调整到目标任务*的方式，保留并利用来自前一个任务的知识，因此推荐使用较小的学习率。'
- en: '**Decay**: In optimal training loops, as the model learns the expected representation
    of inputs to outputs, the objective of the training is to produce finer and finer
    improvements. Smaller learning rates achieve better performance at these stages
    (smaller and more stable weight updates). Therefore, a decaying learning rate
    is preferred after a few epochs.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**衰减**：在最佳训练循环中，当模型学习到输入到输出的预期表示时，训练的目标是产生越来越精细的改进。较小的学习率在这些阶段能获得更好的性能（更小、更稳定的权重更新）。因此，经过几个epoch后，衰减学习率是首选。'
- en: '*Dive into Deep Learning* provides great insights on how to implement these
    techniques in MXNet: [https://d2l.ai/chapter_optimization/lr-scheduler.html.](https://d2l.ai/chapter_optimization/lr-scheduler.html)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dive into Deep Learning* 书中提供了关于如何在MXNet中实现这些技术的深入见解：[https://d2l.ai/chapter_optimization/lr-scheduler.html.](https://d2l.ai/chapter_optimization/lr-scheduler.html)'
- en: Optimizing training for translating text from English to German
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化训练以将文本从英语翻译为德语
- en: In the first recipe of this chapter, we saw how we could leverage MXNet and
    Gluon to optimize the training of our models, applying different techniques. We
    understood how to jointly use lazy evaluation and automatic parallelization for
    parallel processing and improved the performance of our DataLoaders by combining
    preprocessing in the CPU and GPU. We saw how using half-precision (`Float16`)
    in combination with AMP can halve our training times, and explored how to take
    advantage of multiple GPUs for further reduced training times.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个示例中，我们展示了如何利用MXNet和Gluon优化我们的模型训练，应用不同的技术。我们理解了如何联合使用惰性计算和自动并行化进行并行处理，并通过将预处理分配到CPU和GPU上提高了DataLoader的性能。我们还看到，结合使用半精度（`Float16`）和AMP可以将训练时间缩短一半，并探索了如何利用多个GPU进一步缩短训练时间。
- en: 'Now, we can revisit a problem we have been working with throughout the book,
    that of **translating text from English to German**. We have worked with translation
    tasks in recipes in previous chapters. In the *Translating text from Vietnamese
    to English* recipe from [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), we introduced
    the task of translating text, while also learning how to use pre-trained models
    from GluonCV Model Zoo. Furthermore, in the *Improving performance for translating
    English to German* recipe from [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148),
    we introduced the datasets that we will be using in this recipe: *WMT2014* and
    *WMT2016*, and compared the different approaches that we could take when dealing
    with a target dataset, training our models from scratch or leveraging past knowledge
    from pre-trained models and adjusting it for our task, using the different modalities
    of transfer learning and fine-tuning.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重新审视我们在整本书中一直在处理的问题，即**从英语到德语的翻译**。我们在之前的章节中已经处理了翻译任务。在[*第6章*](B16591_06.xhtml#_idTextAnchor121)中的*从越南语到英语的翻译*示例中，我们介绍了翻译任务，并学习了如何使用来自GluonCV模型库的预训练模型。此外，在[*第7章*](B16591_07.xhtml#_idTextAnchor148)中的*提高从英语到德语翻译性能*示例中，我们介绍了本示例中将要使用的数据集：*WMT2014*和*WMT2016*，并比较了我们在处理目标数据集时可以采取的不同方法：从头开始训练我们的模型，或利用预训练模型的过去知识并进行调整，采用不同的迁移学习和微调策略。
- en: Therefore, in this recipe, we will apply all these optimization techniques for
    the specific task of training an *English-to-German text* *translation model*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本示例中，我们将应用所有这些优化技术，专门用于训练一个*英语到德语的文本* *翻译模型*。
- en: Getting ready
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be difficult to understand at all.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节一样，在本示例中我们将使用一些矩阵运算和线性代数，但理解起来一点也不难。
- en: How to do it...
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will work through the following steps:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将按以下步骤进行操作：
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新审视我们当前的数据预处理和训练流程
- en: Applying training optimization techniques
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用训练优化技术
- en: Analyzing the results
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析结果
- en: Let’s dive into each of these steps.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每一步。
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新审视我们当前的数据预处理和训练流程
- en: 'In the *Improving performance for translating English to German* recipe from
    [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), we processed the data with the
    following approach:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B16591_07.xhtml#_idTextAnchor148)中的*提高从英语到德语翻译性能*的示例中，我们使用以下方法处理数据：
- en: Loaded the data from storage into CPU memory space
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据从存储加载到CPU内存中
- en: Preprocessed the data using CPU
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CPU对数据进行了预处理
- en: Used the default parameters to process the data during training
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中使用了默认参数来处理数据
- en: This was a valid approach to compare the different training alternatives available
    for us (training from scratch, pre-trained models, transfer learning, and fine-tuning)
    without adding complexity to the experiments. For example, this approach worked
    quite well to introduce and evaluate the technique of fine-tuning, which is the
    technique that we have selected to work with in this recipe.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有效的方法，可以比较我们可用的不同训练选择（从头开始训练、预训练模型、迁移学习和微调），而不会增加实验的复杂性。例如，这种方法非常适合介绍和评估微调技术，这是我们在本示例中选择的技术。
- en: 'Applying the approach described earlier on the dataset selected for this recipe
    (*WMT2016*), the CPU-based preprocessing took the following amount of time:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用前面描述的方法到本示例所选数据集（*WMT2016*）时，基于CPU的预处理需要以下时间：
- en: '[PRE56]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Furthermore, when combined with the necessary step of reloading the data in
    batches and copying it to the GPU, we obtain the following performance:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当与必要的批量重新加载数据并将其复制到GPU的步骤结合时，我们将获得以下性能：
- en: '[PRE57]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations using the technique
    of fine-tuning directly. In combination with this approach, we use the following
    hyperparameters:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理完成后，下一步是训练过程。如前所述，我们将直接评估使用微调技术对训练优化的影响。结合这种方法，我们使用以下超参数：
- en: '[PRE58]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In these conditions, the training process duration and performance achieved
    were as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些条件下，训练过程的持续时间和所取得的性能如下：
- en: '[PRE59]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As we can see, we got an excellent validation performance (~1.4) for a training
    time of ~3 hours.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在训练时间大约为3小时的情况下，我们获得了优秀的验证性能（~1.4）。
- en: 'The evolution of the training loss and the validation loss across each epoch
    looked as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 每轮训练损失和验证损失的变化情况如下所示：
- en: '![Figure 8.6 – Revisiting training: training loss versus validation loss](img/B16591_08_6.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 重新审视训练：训练损失与验证损失](img/B16591_08_6.jpg)'
- en: 'Figure 8.6 – Revisiting training: training loss versus validation loss'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 重新审视训练：训练损失与验证损失
- en: From *Figure 8**.6*, we can see the evolution of the training and validation
    loss. As explored throughout the chapters, we select the model that provide the
    minimal validation loss (in this case, it was achieved in the first epoch, epoch
    1).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图8.6*中，我们可以看到训练损失和验证损失的变化。正如在各章节中探讨的那样，我们选择提供最小验证损失的模型（在这个案例中，最小验证损失出现在第一轮训练，即第1轮）。
- en: 'After the training is completed, we can verify the overall performance in the
    test split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以在数据集的测试分割中验证整体性能。从定量角度来看，以下是我们获得的结果：
- en: '[PRE60]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As expected, we got excellent results just by training for a limited number
    of epochs (10 in this case).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，通过仅仅训练有限的时期（在此为10轮），我们就获得了优异的结果。
- en: 'From a qualitative point of view, we can also check how well our model is performing
    by testing it with an example sentence. In our case, we chose `I learn new things
    every day`, and the output obtained is as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们还可以通过测试一个示例句子来检查模型的表现。在我们的案例中，我们选择了`I learn new things every day`，并且得到的输出如下：
- en: '[PRE61]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The German sentence obtained in the output (`Immer wieder erfährt ich Neues`)
    means `I'm always learning new things`, and therefore, as can be seen from the
    results, the text has been almost perfectly translated from English to German.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果中的德语句子（`Immer wieder erfährt ich Neues`）的意思是`我总是学习新东西`，因此，从结果中可以看出，文本几乎已从英语完美翻译成德语。
- en: Applying training optimization techniques
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用训练优化技术
- en: In the *Introducing training optimization features* recipe at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take when training a machine learning
    model, including preprocessing the data and training and evaluating the model.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头的*引入训练优化特性*配方中，我们展示了不同的优化技术如何提高我们在训练机器学习模型时所采取的不同步骤的性能，包括数据预处理、训练和评估模型。
- en: In this section, we will show how, with MXNet and Gluon and just a few lines
    of code, we can easily apply all of the techniques we’ve been introduced to.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何通过使用MXNet和Gluon以及仅仅几行代码，轻松应用我们已介绍的所有技术。
- en: As shown in the first recipe of this chapter, MXNet applies by default the best
    policy (`ThreadedEnginePerDevice`) to optimize lazy evaluation and automatic parallelization,
    taking into account the number of CPU threads available, so there is no need for
    us to apply any changes here (please note that this technique is also applied
    automatically when working with multiple GPUs).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章的第一种配方所示，MXNet默认应用最佳策略（`ThreadedEnginePerDevice`）来优化懒评估和自动并行化，考虑到可用的CPU线程数，因此我们无需在此处进行任何修改（请注意，当使用多个GPU时，这项技术也会自动应用）。
- en: 'We have shown how we could optimize our data preprocessing pipeline by combining
    the usage of CPU threads and GPUs, taking into account the number of devices available
    for each and optimizing accordingly. For this experiment, specific HW was chosen
    with the following characteristics:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何通过结合使用CPU线程和GPU，优化我们的数据预处理管道，考虑到每个设备的可用数量并进行相应优化。对于这次实验，选择了具有以下特征的特定硬件：
- en: '[PRE62]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In order to apply this optimization technique, we had to apply some changes
    to our code. Specifically, we defined the GPUs available for use:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用这种优化技术，我们不得不对代码进行一些修改。具体来说，我们定义了可用的GPU：
- en: '[PRE63]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Furthermore, in our preprocessing pipeline, we now need a specific step that
    takes the data from the CPU memory space and copies it to the GPU memory space:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们的预处理管道中，我们现在需要一个特定的步骤，将数据从CPU内存空间复制到GPU内存空间：
- en: '[PRE64]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: As discussed in the first recipe of this chapter, in a typical production-oriented
    environment, we do not want to keep the data in the GPU as it occupies precious
    GPU memory. It is usual to optimize the batch size for the GPU memory available,
    and to load the data from the CPU memory space into the GPU memory space in batches
    using MXNet Gluon DataLoaders. Therefore, for our GPU-based preprocessing pipeline
    to be complete, we need a final step to copy the data back into the CPU memory
    space. As introduced in the *Improving performance for translating English to
    German* recipe from [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), we are using
    the `ShardedDataLoader` class from MXNet `GluonNLP` library. This class performs
    that data transfer back to the CPU memory space automatically.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章第一个例子所讨论的，在典型的生产环境中，我们并不希望将数据保留在 GPU 中，因为它会占用宝贵的 GPU 内存。通常会根据 GPU 可用内存优化批次大小，并通过
    MXNet Gluon DataLoaders 从 CPU 内存空间批量加载数据到 GPU 内存空间。因此，为了使我们的基于 GPU 的预处理管道完整，我们需要一个最终步骤，将数据复制回
    CPU 内存空间。正如在[*第七章*](B16591_07.xhtml#_idTextAnchor148)中的*提高英德翻译性能*一节中介绍的，我们使用的是来自
    MXNet `GluonNLP` 库的 `ShardedDataLoader` 类。这个类会自动执行数据的回传到 CPU 内存空间。
- en: However, as will be seen in our experiments, when working with multiple GPUs,
    performance is better when working directly with MXNet Gluon DataLoaders, as these
    are designed to be parallelized optimally afterward.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们在实验中将看到的，当使用多个 GPU 时，直接使用 MXNet Gluon DataLoaders 会更高效，因为它们设计上可以在后续进行最佳并行化。
- en: 'With these code changes, our optimal preprocessing pipeline is ready, and we
    can continue with the next optimization technique: applying `Float16` optimizations,
    including AMP.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些代码修改，我们的最佳预处理管道已经准备好，接下来可以继续进行下一个优化技术：应用 `Float16` 优化，包括 AMP。
- en: 'As shown in the first recipe of this chapter, in order to enable this technique,
    we just need a few changes in our code. First of all, we initialize the library:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章第一个例子所示，为了启用该技术，我们只需要在代码中做几个修改。首先，我们初始化库：
- en: '[PRE65]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Secondly, we attach the trainer/optimizer to the library:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将训练器/优化器附加到库中：
- en: '[PRE66]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In the previous recipe, when dealing with images, we described how, due to the
    risk of gradients over/under-flowing, there was a need to adjust (scale) the loss
    accordingly. This is not necessary for our use case; therefore, we do not apply
    **loss** **scaling** here.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，当处理图像时，我们描述了由于梯度可能出现过度/欠流动的问题，因此需要相应地调整（缩放）损失。这在我们的用例中并不必要，因此我们在这里不进行**损失**
    **缩放**。
- en: With these two simple changes, we have updated our training loop to work efficiently
    with the `Float16` data type (when appropriate).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这两个简单的修改，我们已更新训练循环，以便在适当时使用 `Float16` 数据类型高效工作。
- en: 'Finally, we can apply our next and last training optimization technique: working
    with multiple GPUs.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以应用下一个也是最后一个训练优化技术：使用多个 GPU。
- en: As we will see, working with multiple GPUs optimally implies working with them
    in parallel, and therefore, computing losses and executing the training backward
    pass in parallel, yielding the losses list described in the previous paragraph.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，优化地使用多个 GPU 意味着并行处理它们，因此并行计算损失并执行训练的反向传递，从而得到上一段描述的损失列表。
- en: 'In order to work with multiple GPUs in parallel, we need to define the new
    context as a list (seen before for preprocessing, and shown here again for convenience):'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在多个 GPU 上并行工作，我们需要将新上下文定义为一个列表（之前在预处理时见过，这里再次展示以便于参考）：
- en: '[PRE67]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'As we now have multiple GPUs, we can increase our batch size to optimally use
    the available GPU memory space:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在有了多个 GPU，我们可以增加批次大小，以最优化使用可用的 GPU 内存空间：
- en: '[PRE68]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Furthermore, when reading from Gluon DataLoaders, we need to split the batches
    of data across the GPUs. Thankfully, Gluon also provides a function that simplifies
    that action. We just need the following lines of code to be added (for each training
    and validation batch):'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当从 Gluon DataLoaders 中读取数据时，我们需要将数据批次分配到各个 GPU 上。幸运的是，Gluon 也提供了一个简化该操作的函数。我们只需为每个训练和验证批次添加以下几行代码：
- en: '[PRE69]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As mentioned, this split across the GPUs allows us to compute in parallel the
    model outputs and the losses associated with those outputs (a measure of the difference
    between the actual outputs and the expected outputs). This can be achieved with
    the following lines of code:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GPU 之间的分割使我们能够并行计算模型的输出及其相关的损失（即实际输出与预期输出之间的差异度量）。这可以通过以下几行代码实现：
- en: '[PRE70]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Typically, in order to finalize our updates to work with multiple GPUs in the
    training loop, we would need to apply further changes to our loss scaling. However,
    as discussed, for our use case, this is not necessary.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了使我们的更新能够在训练循环中与多个 GPU 一起工作，我们需要对损失缩放进行进一步修改。然而，正如前面所讨论的，对于我们的使用案例，这是不必要的。
- en: With these minimal code changes, we now have an optimal preprocessing and training
    pipeline, and we can run the required experiments to analyze the performance changes.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些最小的代码更改，我们现在拥有了一个最佳的预处理和训练管道，可以运行所需的实验来分析性能变化。
- en: Analyzing the results
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析结果
- en: In the previous sections, we revisited the previous performance of our preprocessing
    and training pipelines and reviewed how we had to apply the necessary changes
    for our training optimization techniques, specifically for our task of translating
    text from English to German.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们回顾了我们预处理和训练管道的先前性能，并回顾了如何为我们的训练优化技术应用必要的更改，特别是针对我们将英文翻译成德文的任务。
- en: 'Our preprocessing pipeline steps are now the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预处理管道步骤现在如下：
- en: Load the data from storage into the CPU memory space.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据从存储加载到 CPU 内存空间。
- en: Preprocess the data using the GPU (although as we will see, we will change this
    to the CPU).
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 GPU 预处理数据（尽管正如我们将看到的，我们会将其改为 CPU）。
- en: Copy back the data to the CPU memory space (won’t be necessary).
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据复制回 CPU 内存空间（此操作不必要）。
- en: Use the optimized parameters to process the data during training.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中使用优化的参数处理数据。
- en: For our experiments, we are going to use the technique of fine-tuning directly.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将直接使用微调技术。
- en: 'Following the aforementioned approach on the dataset selected for this recipe
    (*WMT2016*), the GPU-based preprocessing took the following amount of time:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前述方法，在为本食谱选择的数据集（*WMT2016*）上，基于 GPU 的预处理花费了以下时间：
- en: '[PRE71]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'An end-to-end preprocessing pipeline must take into account the process of
    batching using the Gluon DataLoader to load the data (in our case, into multiple
    GPUs), giving us the following performance:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端的预处理管道必须考虑使用 Gluon DataLoader 进行批处理的过程（在我们的案例中，将数据加载到多个 GPU 中），从而为我们提供以下性能：
- en: '[PRE72]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Compared to the initial section of this recipe (where the preprocessing took
    27 seconds), we can see how, in this case, preprocessing in the GPU has not been
    effective. This is due to the nature of the text data, which is not as straightforward
    to parallelize as it is with images, for example.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 与本食谱的初始部分（预处理花费了 27 秒）相比，我们可以看到，在这种情况下，GPU 上的预处理效果并不显著。这是由于文本数据的特性，它不像图像那样容易并行化。
- en: 'In this scenario, a CPU-based preprocessing pipeline is best, avoiding the
    Gluon NLP`ShardedDataLoader` class and using the `Gluon DataLoader` class instead
    (which is better suited for parallelizing). Applying this pipeline, we get the
    following results:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，基于 CPU 的预处理管道是最佳选择，避免使用 Gluon NLP`ShardedDataLoader` 类，而改用 `Gluon DataLoader`
    类（它更适合并行化）。应用此管道后，我们得到了以下结果：
- en: '[PRE73]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This gives us a minimal edge (2 seconds), but, as mentioned, this is the best
    we can get with the usage of Gluon DataLoader and its parallelization capabilities.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个最小的优势（2 秒），但如前所述，这是使用 Gluon DataLoader 及其并行化功能时我们能得到的最佳结果。
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations using the technique
    of fine-tuning directly. In combination with this approach, we use the following
    hyperparameters:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 经过预处理后，下一步是训练过程。如前所述，我们将使用微调技术直接评估我们训练优化的效果。结合这种方法，我们使用以下超参数：
- en: '[PRE74]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Please note how by adding multiple GPUs to the training process, we can increase
    the batch size (multiplied by the number of GPUs), and we can also increase the
    learning rate (from 0.00003 to 0.0001). In these conditions, the training process
    duration and achieved performance is as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过在训练过程中增加多个 GPU，我们可以增加批处理大小（乘以 GPU 数量），并且还可以增加学习率（从 0.00003 增加到 0.0001）。在这些条件下，训练过程的持续时间和达到的性能如下：
- en: '[PRE75]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As we can see, we got excellent validation performance (~1.4) with training
    that took ~3 hours. When compared to the results obtained in the initial section
    of this recipe, we can see how there was a minimal decrease in the loss (a positive
    change, which we will confirm with our performance analysis shortly), but the
    largest improvement by far has been a 5.5x decrease in the training time. This
    improvement is due to all the training optimization techniques that we have applied.
    In a nutshell, each of the optimizations provided the following improvements:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在训练时间约为3小时的情况下，我们获得了出色的验证表现（约1.4）。与本食谱初始部分获得的结果相比，我们可以看到损失有了最小的下降（这是一个积极的变化，我们将在接下来的性能分析中确认），但迄今为止最大的改进是训练时间减少了5.5倍。这个改进归功于我们应用的所有训练优化技术。简而言之，每个优化提供了以下改进：
- en: '**Using 4 GPUs**: Provided a 4x decrease (as expected).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用4个GPU**：提供了4倍的降低（如预期）。'
- en: '`Float16` without compromising algorithmic performance.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Float16` 在不影响算法性能的情况下使用。'
- en: '**Preprocessing the datasets**: In this case, there were negligible improvements.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理数据集**：在这种情况下，改进几乎可以忽略不计。'
- en: 'The evolution of the training loss and the validation loss across each epoch
    looked as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 每一轮训练中的训练损失和验证损失的演变如下所示：
- en: '![Figure 8.7 – Optimized training: training loss versus validation loss](img/B16591_08_7.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 优化训练：训练损失与验证损失](img/B16591_08_7.jpg)'
- en: 'Figure 8.7 – Optimized training: training loss versus validation loss'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 优化训练：训练损失与验证损失
- en: From *Figure 8**.7*, we can see the evolution of the training and validation
    losses. As explored throughout the chapters, we select the model that provided
    the minimal validation loss (in this case, achieved in the first epoch).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 8.7*中，我们可以看到训练损失和验证损失的变化过程。如本书各章节所述，我们选择了提供最小验证损失的模型（在这种情况下，这是在第一轮训练时实现的）。
- en: 'After training is completed, we can verify the overall performance in the test
    split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以在数据集的测试分区中验证整体性能。从量化角度来看，这些是我们获得的结果：
- en: '[PRE76]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: As expected, we got excellent results just by training for a limited number
    of epochs (5 in this case). We can also confirm how the minimal improvement in
    the validation loss provided a minimal improvement in our test metrics (compared
    to 27.05 as initially obtained).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，仅通过训练有限数量的轮次（在本例中为5次），我们就获得了优异的结果。我们还可以确认，验证损失的最小改进为我们的测试指标提供了最小的提升（与最初获得的27.05相比）。
- en: 'From a qualitative point of view, we can also check how well our model is performing
    by testing it with an example sentence. In our case, we chose `I learn new things
    every day`, and the output obtained is as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们也可以通过用一个示例句子测试模型来检查它的表现。在我们的例子中，我们选择了 `I learn new things every day`，得到的输出如下：
- en: '[PRE77]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The German sentence obtained in the output (`Ich lerne jedes Mal Neues`) means
    `I learn something new every time`, and therefore, as can be seen from the results,
    the text has been almost perfectly translated from English to German.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中得到的德语句子（`Ich lerne jedes Mal Neues`）的意思是 `I learn something new every time`，因此从结果来看，文本几乎已经被完美地从英语翻译成德语。
- en: How it works...
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we applied the different training optimization techniques seen
    in the first recipe of this chapter, leveraging our HW (CPUs and GPUs) to address
    each of the steps in the training loop:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们应用了本章第一个食谱中看到的不同训练优化技术，利用我们的硬件（CPU和GPU）来解决训练循环中的每个步骤：
- en: We revisited how lazy evaluation and automatic parallelization mechanisms work
    together to optimize all MXNet-based flows.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重新审视了懒惰求值和自动并行化机制如何协同工作，以优化所有基于MXNet的流程。
- en: We leveraged all our CPU threads to load data and tested to optimize that process
    further via preprocessing in the GPU. In this case, it was shown how a CPU-based
    preprocessing pipeline in combination with Gluon DataLoader was the optimal approach.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们利用了所有CPU线程来加载数据，并通过在GPU上进行预处理进一步优化了该过程。在这种情况下，展示了结合Gluon DataLoader的基于CPU的预处理管道是最优方案。
- en: We analyzed different data types and combined the accuracy and precision of
    `Float32` with the speed-ups of `Float16`, and where possible, AMP.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们分析了不同的数据类型，并结合了 `Float32` 的准确性和精度，以及 `Float16` 的加速效果，并在可能的情况下，使用了AMP。
- en: We increased the performance of our training loops by using multiple GPUs (assuming
    our HW has these devices available).
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过使用多个GPU（假设我们的硬件具备这些设备）提高了训练循环的性能。
- en: We compared each of these scenarios applied specifically to the task of *translating
    text from English to German*, running two experiments. In the first experiment,
    we did not apply any of the training optimization techniques described, following
    the approaches seen in previous chapters of the book. In the second experiment,
    we applied all the techniques in parallel, trying to optimize as much as we could.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每种具体应用于*将英文文本翻译成德文*的情景进行了比较，进行了两项实验。在第一项实验中，我们没有应用书中描述的任何训练优化技术，而是采用了之前章节中的方法。在第二项实验中，我们同时应用了所有技术，试图尽可能优化。
- en: This proved quite useful, delivering similar algorithmic performance, with a
    5.5x improvement in training time (from 3 hours to 30 minutes). This was mostly
    due to using multiple GPUs (4x decrease) and leveraging `Float16` and AMP (1.4x
    decrease), whereas the optimized preprocessing provided negligible improvements.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明非常有用，提供了类似的算法性能，训练时间缩短了5.5倍（从3小时缩短到30分钟）。这主要是由于使用了多个GPU（减少了4倍）和利用了`Float16`和AMP（减少了1.4倍），而优化的预处理提供了微不足道的改进。
- en: There’s more…
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: We have described, implemented, executed, and evaluated several training optimization
    techniques. However, there are even more advanced techniques that can be leveraged
    to achieve the optimal training loop.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述、实施、执行和评估了几种训练优化技术。然而，还有更高级的技术可以利用，以实现最佳的训练循环。
- en: One such technique is **Reinforcement Learning from Human Feedback** (**RLHF**),
    where a *human-in-the-loop* process is introduced. In this process, after a model
    has been trained, a person is presented with different options for output by the
    model (for example, different potential translations) and they rank those responses
    according to how they better represent the original sentence. These human inputs
    are then used to train a reward model that scores the output of the model and
    selects the one with the highest score. This technique has been proven to be extremely
    powerful. As an example, **OpenAI** developed **ChatGPT** on top of the **GPT-3**
    language model using RLHF.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种技术是**人类反馈强化学习**（**RLHF**），引入了*人在回路中*的过程。在这个过程中，模型训练完成后，会向人员展示模型的不同输出选项（例如，不同的潜在翻译），并根据这些人员对哪个最好地表达原始句子进行排序。这些人类输入然后用于训练一个评分模型，评分模型会对模型的输出进行评分，并选择分数最高的输出。这种技术已被证明非常强大。例如，**OpenAI**利用**RLHF**在**GPT-3**语言模型之上开发了**ChatGPT**。
- en: 'To learn more about *ChatGPT* and *RLHF*, the following article is recommended:
    [https://huyenchip.com/2023/05/02/rlhf.html](https://huyenchip.com/2023/05/02/rlhf.html).'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于*ChatGPT*和*RLHF*的信息，推荐阅读以下文章：[https://huyenchip.com/2023/05/02/rlhf.html](https://huyenchip.com/2023/05/02/rlhf.html)。
