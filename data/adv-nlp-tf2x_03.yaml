- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于BiLSTM、CRF和Viterbi解码的命名实体识别（NER）
- en: 'One of the fundamental building blocks of NLU is **Named Entity Recognition**
    (**NER**). The names of people, companies, products, and quantities can be tagged
    in a piece of text with NER, which is very useful in chatbot applications and
    many other use cases in information retrieval and extraction. NER will be the
    main focus of this chapter. Building and training a model capable of doing NER
    requires several techniques, such as **Conditional Random Fields** (**CRFs**)
    and **Bi-directional LSTMs** (**BiLSTMs**). Advanced TensorFlow techniques like
    custom layers, losses, and training loops are also used. We will build on the
    knowledge of BiLSTMs gained from the previous chapter. Specifically, the following
    will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言理解（NLU）的一个基础构建模块是**命名实体识别**（**NER**）。通过NER，可以在文本中标记出人名、公司名、产品名和数量等实体，这在聊天机器人应用以及信息检索和提取的许多其他用例中都非常有用。NER将在本章中作为主要内容。构建和训练一个能够进行NER的模型需要几种技术，例如**条件随机场**（**CRFs**）和**双向LSTM**（**BiLSTMs**）。还会使用一些高级TensorFlow技术，如自定义层、损失函数和训练循环。我们将基于上一章获得的BiLSTMs知识进行拓展。具体来说，将涵盖以下内容：
- en: Overview of NER
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NER概述
- en: Building an NER tagging model with BiLSTM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个基于BiLSTM的NER标注模型
- en: CRFs and Viterbi algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CRFs和Viterbi算法
- en: Building a custom Keras layer for CRFs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为CRFs构建自定义Keras层
- en: Building a custom loss function in Keras and TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras和TensorFlow中构建自定义损失函数
- en: Training a model with a custom training loop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义训练循环训练模型
- en: It all starts with understanding NER, which is the focus of the next section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都始于理解NER，这是下一节的重点。
- en: Named Entity Recognition
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Given a sentence or a piece of text, the objective of an NER model is to locate
    and classify text tokens as named entities in categories such as people''s names,
    organizations and companies, physical locations, quantities, monetary quantities,
    times, dates, and even protein or DNA sequences. NER should tag the following
    sentence:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一句话或一段文本，NER模型的目标是定位并将文本中的词语分类为命名实体，类别包括人名、组织与公司、地理位置、数量、货币数量、时间、日期，甚至蛋白质或DNA序列。NER应该标记以下句子：
- en: '*Ashish paid Uber $80 to go to the Twitter offices in San Francisco.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*阿什什支付了80美元，乘坐Uber去Twitter位于旧金山的办公室。*'
- en: 'as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示：
- en: '*[Ashish]*[PER] *paid [Uber]*[ORG] *[$80]*[MONEY] *to go the [Twitter]*[ORG]
    *offices in [San Francisco]*[LOC]*.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*[阿什什]*[PER] *支付了[Uber]*[ORG] *[$80]*[MONEY] *去[Twitter]*[ORG] *位于[旧金山]*[LOC]的办公室。*'
- en: 'Here is an example from the Google Cloud Natural Language API, with several
    additional classes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自Google Cloud自然语言API的一个示例，包含多个附加类别：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_03_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  描述自动生成](img/B16252_03_01.png)'
- en: 'Figure 3.1: An NER example from the Google Cloud Natural Language API'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：来自Google Cloud自然语言API的NER示例
- en: 'The most common tags are listed in the table below:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的标签列在下表中：
- en: '| Type | Example Tag | Example |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 示例标签 | 示例 |'
- en: '| Person | PER | *Gregory* went to the castle. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 人物 | PER | *格雷戈里*去了城堡。 |'
- en: '| Organization | ORG | *WHO* just issued an epidemic advisory. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 组织 | ORG | *世界卫生组织*刚刚发布了疫情警告。 |'
- en: '| Location | LOC | She lives in *Seattle*. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | LOC | 她住在*西雅图*。 |'
- en: '| Money | MONEY | You owe me *twenty dollars*. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 金钱 | MONEY | 你欠我*二十美元*。 |'
- en: '| Percentage | PERCENT | Stocks have risen *10%* today. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 百分比 | PERCENT | 股票今天上涨了*10%*。 |'
- en: '| Date | DATE | Let''s meet on *Wednesday*. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | DATE | 我们周三见。 |'
- en: '| Time | TIME | Is it *5 pm* already? |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | TIME | *已经是下午五点了吗？* |'
- en: 'There are different data sets and tagging schemes that can be used to train
    NER models. Different data sets will have different subsets of the tags listed
    above. In other domains, there may be additional tags specific to the domain.
    The Defence Science Technology Laboratory in the UK created a data set called
    **re3d** ([https://github.com/dstl/re3d](https://github.com/dstl/re3d)), which
    has entity types such as vehicle (Boeing 777), weapon (rifle), and military platform
    (tank). The availability of adequately sized labeled data sets in various languages
    is a significant challenge. Here is a link to a good collection of NER data sets:
    [https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets).
    In many use cases, you will need to spend a lot of time collecting and annotating
    data. For example, if you are building a chatbot for ordering pizza, the entities
    could be bases, sauces, sizes, and toppings.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的数据集和标注方案可以用来训练 NER 模型。不同的数据集会包含上述标注的不同子集。在其他领域，可能会有针对该领域的额外标签。英国的国防科学技术实验室（Defence
    Science Technology Laboratory）创建了一个名为**re3d**的数据集（[https://github.com/dstl/re3d](https://github.com/dstl/re3d)），其中包含诸如车辆（如波音
    777）、武器（如步枪）和军事平台（如坦克）等实体类型。各种语言中适当大小的标注数据集的可用性是一个重大挑战。以下是一个很好的 NER 数据集集合链接：[https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets)。在许多使用案例中，你需要花费大量时间收集和标注数据。例如，如果你正在为订购披萨构建一个聊天机器人，实体可能包括基础、酱料、尺寸和配料。
- en: There are a few different ways to build an NER model. If the sentence is considered
    a sequence, then this task can be modeled as a word-by-word labeling task. Hence,
    models similar to the models used for **Part of Speech** (**POS**) tagging are
    applicable. Features can be added to a model to improve labeling. The POS of a
    word and its neighboring words are the most straightforward features to add. Word
    shape features that model lowercase letters can add a lot of information, principally
    because a lot of the entity types deal with proper nouns, such as those for people
    and organizations. Organization names can be abbreviated. For example, the World
    Health Organization can be represented as WHO. Note that this feature will only
    work in languages that distinguish between lowercase and uppercase letters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 NER 模型有几种不同的方法。如果将句子视为一个序列，那么这个任务可以建模为逐字标注任务。因此，类似于**词性标注**（**POS**）的模型是适用的。可以向模型中添加特征以改进标注。一个词的词性及其相邻词汇是最直接可以添加的特征。用于建模小写字母的词形特征可以提供大量信息，主要是因为许多实体类型涉及专有名词，比如人名和组织名。组织名称可能会被缩写。例如，世界卫生组织可以表示为
    WHO。请注意，这一特征仅适用于区分大小写字母的语言。
- en: Another vital feature involves checking a word in a **gazetteer**. A gazetteer
    is like a database of important geographical entities. See [geonames.org](http://geonames.org)
    for an example of a data set licensed under Creative Commons. A set of people's
    names in the USA can be sourced from the US Social Security Administration at
    [https://www.ssa.gov/oact/babynames/state/namesbystate.zip](https://www.ssa.gov/oact/babynames/state/namesbystate.zip).
    The linked ZIP file has the names of people born in the United States since 1910,
    grouped by state. Similarly, Dunn and Bradstreet, popularly known as D&B, offers
    a data set of companies with over 200 million businesses across the world that
    can be licensed. The biggest challenge with this approach is the complexity of
    maintaining these lists over time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的特征是检查一个词是否在**地名辞典**中。地名辞典就像一个重要地理实体的数据库。可以参考[geonames.org](http://geonames.org)上的数据集，该数据集获得了创意共享（Creative
    Commons）许可。美国社会保障管理局（US Social Security Administration）提供了一份美国人名数据集，地址为[https://www.ssa.gov/oact/babynames/state/namesbystate.zip](https://www.ssa.gov/oact/babynames/state/namesbystate.zip)。该压缩文件包含自1910年以来出生在美国的人的名字，按州分组。类似地，知名的邓白氏公司（Dunn
    and Bradstreet，简称 D&B）提供了一份全球超过2亿家企业的数据集，用户可以申请许可。使用这种方法的最大挑战是随着时间的推移维护这些列表的复杂性。
- en: In this chapter, we will focus on a model that does not rely on additional external
    data on top of labelled data for training, like a gazetteer, and also has no dependence
    on hand-crafted features. We will try to get to as high a level of accuracy as
    possible using deep neural networks and some additional techniques. The model
    we will use will be a combination of BiLSTM and a CRF on top. This model is based
    on the paper titled *Neural Architectures for Named Entity Recognition*, written
    by Guillaume Lample et al. and presented at the NAACL-HTL conference in 2016\.
    This paper was state of the art in 2016 with an F1 score of 90.94\. Currently,
    the SOTA has an F1-score of 93.5, where the model uses extra training data. These
    numbers are measured on the CoNLL 2003 English data set. The GMB data set will
    be used in this chapter. The next section describes this data set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注一个不依赖额外外部数据（如地名词典）进行训练的模型，也不依赖人工特征。我们将尽可能使用深度神经网络和一些额外技术来提高准确度。我们将使用的模型将是
    BiLSTM 和 CRF 的组合。该模型基于 Guillaume Lample 等人撰写的论文《命名实体识别的神经网络架构》，并在2016年NAACL-HTL会议上发表。这篇论文在2016年处于前沿水平，F1
    分数为90.94。目前，SOTA（最先进技术）的F1分数为93.5，其中模型使用了额外的训练数据。这些数据是在 CoNLL 2003 英文数据集上测量的。本章将使用
    GMB 数据集。下一节将描述该数据集。
- en: The GMB data set
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GMB 数据集
- en: 'With all the basics in the bag, we are ready to build a model that classifies
    NERs. For this task, the **Groningen Meaning Bank** (**GMB**) data set will be
    used. This dataset is not considered a gold standard. This means that this data
    set is built using automatic tagging software, followed by human raters updating
    subsets of the data. However, this is a very large and rich data set. This data
    has a lot of useful annotations that make it quite suitable for training models.
    It is also constructed from public domain text, making it easy to use for training.
    The following named entities are tagged in this corpus:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识掌握后，我们准备构建一个用于分类命名实体识别（NER）的模型。对于这一任务，将使用**格罗宁根语义库**（**GMB**）数据集。这个数据集并不被认为是黄金标准。也就是说，该数据集是通过自动标注软件构建的，随后由人工评分员更新数据子集。然而，这是一个非常大且丰富的数据集，包含了大量有用的注释，非常适合用于模型训练。它也来源于公共领域的文本，因此很容易用于训练。这个语料库中标注了以下命名实体：
- en: geo = Geographical entity
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: geo = 地理实体
- en: org = Organization
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: org = 组织
- en: per = Person
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: per = 人物
- en: gpe = Geopolitical entity
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gpe = 地缘政治实体
- en: tim = Time indicator
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tim = 时间指示符
- en: art = Artifact
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: art = 人造物
- en: eve = Event
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: eve = 事件
- en: nat = Natural phenomenon
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nat = 自然现象
- en: In each of these categories, there can be subcategories. For example, *tim*
    may be further sub-divided and represented as *tim-dow* representing a time entity
    corresponding to a day of the week, or *tim-dat*, which represents a date. For
    this exercise, these sub-entities are going to be aggregated into the eight top-level
    entities listed above. The number of examples varies widely between the sub-entities.
    Consequently, the accuracy varies widely due to the lack of enough training data
    for some of these subcategories.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些类别中，可能存在子类别。例如，*tim* 可能进一步细分为 *tim-dow*，表示一周中的某一天，或者 *tim-dat*，表示一个日期。对于本次练习，这些子实体将被汇总成上述的八个顶级实体。这些子实体的样本数量差异很大，因此，由于某些子类别缺乏足够的训练数据，准确度差异也很大。
- en: The data set also provides the NER entity for each word. In many cases, an entity
    may comprise multiple words. If *Hyde Park* is a geographical entity, both words
    will be tagged as a *geo* entity. In terms of training models for NER, there is
    another way to represent this data that can have a significant impact on the accuracy
    of the model. This requires the usage of the BIO tagging scheme. In this scheme,
    the first word of an entity, single word or multi-word, is tagged with *B-{entity
    tag}*. If the entity is multi-word, each successive word would be tagged as *I-{entity
    tag}*. In the example above, *Hyde Park* would be tagged as *B-geo I-geo*. All
    these are steps of pre-processing that are required for a data set. All the code
    for this example can be found in the `NER with BiLSTM and CRF.ipynb` notebook
    in the `chapter3-ner-with-lstm-crf` folder of the GitHub repository.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还提供了每个单词的NER实体。在许多情况下，一个实体可能由多个单词组成。如果*Hyde Park*是一个地理实体，那么这两个单词都会被标记为*geo*实体。在训练NER模型时，还有另一种表示数据的方式，这对模型的准确性有显著影响。这需要使用BIO标注方案。在这种方案中，实体的第一个单词，无论是单一词还是多词，都标记为*B-{实体标签}*。如果实体是多词的，每个后续的单词将标记为*I-{实体标签}*。在上面的例子中，*Hyde
    Park*将被标记为*B-geo I-geo*。所有这些都是数据集预处理的步骤。本示例的所有代码可以在GitHub仓库的`chapter3-ner-with-lstm-crf`文件夹中的`NER
    with BiLSTM and CRF.ipynb`笔记本中找到。
- en: Let's get started by loading and processing the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从加载和处理数据开始。
- en: Loading the data
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'Data can be downloaded from the University of Groningen website as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以通过格罗宁根大学的网站下载，具体如下：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Please note that the data is quite large – over 800MB. If `wget` is not available
    on your system, you may use any other tool such as, `curl` or a browser to download
    the data set. This step may take some time to complete. If you have a challenge
    accessing the data set from the University server, you may download a copy from
    Kaggle: [https://www.kaggle.com/bradbolliger/gmb-v220](https://www.kaggle.com/bradbolliger/gmb-v220).
    Also note that since we are going to be working on large data sets, some of the
    following steps may take some time to execute. In the world of **Natural Language
    Processing** (**NLP**), more training data and training time is key to great results.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据非常大——超过800MB。如果您的系统中没有`wget`，可以使用任何其他工具，如`curl`或浏览器，来下载数据集。此步骤可能需要一些时间才能完成。如果您在从大学服务器访问数据集时遇到问题，可以从Kaggle下载一份副本：[https://www.kaggle.com/bradbolliger/gmb-v220](https://www.kaggle.com/bradbolliger/gmb-v220)。另外，由于我们将处理大数据集，接下来的步骤可能需要一些时间来执行。在**自然语言处理**（**NLP**）领域，更多的训练数据和训练时间是取得良好结果的关键。
- en: All the code for this example can be found in the `NER with BiLSTM and CRF.ipynb`
    notebook in the `chapter3-ner-with-lstm-crf` folder of the GitHub repository.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的所有代码可以在GitHub仓库的`chapter3-ner-with-lstm-crf`文件夹中的`NER with BiLSTM and CRF.ipynb`笔记本中找到。
- en: The data unzips into the `gmb-2.2.0` folder. The `data` subfolder has a number
    of subfolders with different files. `README` supplied with the data set provides
    details about the various files and their contents. For this example, we will
    be using only files named `en.tags` in various subdirectories. These files are
    tab-separated files with each word of a sentence in a row.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据解压后会进入`gmb-2.2.0`文件夹。`data`子文件夹中有多个子文件夹和不同的文件。数据集提供的`README`文件详细说明了各种文件及其内容。在此示例中，我们只使用命名为`en.tags`的文件，这些文件位于不同的子目录中。这些文件是制表符分隔的文件，每一行表示一个句子的一个单词。
- en: 'There are ten columns of information:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有十列信息：
- en: The token itself
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令牌本身
- en: A POS tag as used in the Penn Treebank ([ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz](ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz))
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penn Treebank中使用的词性标记（[ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz](ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz)）
- en: A lemma
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词条
- en: A named-entity tag, or 0 if none
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体标签，如果没有则为0
- en: A WordNet word sense number for the respective lemma-POS combinations, or 0
    if not applicable ([http://wordnet.princeton.edu](http://wordnet.princeton.edu))
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应词条-词性组合的WordNet词义编号，如果不适用则为0（[http://wordnet.princeton.edu](http://wordnet.princeton.edu)）
- en: For verbs and prepositions, a list of the VerbNet roles of the arguments in
    order of combination in the **Combinatory Categorial Grammar** (**CCG**) derivation,
    or `[]` if not applicable ([http://verbs.colorado.edu/~mpalmer/projects/verbnet.html](http://verbs.colorado.edu/~mpalmer/projects/verbnet.html))
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于动词和介词，列出**组合范畴语法**（**CCG**）推导中按组合顺序排列的VerbNet论元角色，如果不适用则为`[]`（[http://verbs.colorado.edu/~mpalmer/projects/verbnet.html](http://verbs.colorado.edu/~mpalmer/projects/verbnet.html)）
- en: Semantic relation in noun-noun compounds, possessive apostrophes, temporal modifiers,
    and so on. Indicated using a preposition, or 0 if not applicable
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名词-名词复合词中的语义关系、所有格撇号、时间修饰语等。通过介词表示，若不适用则为0
- en: An animacy tag as proposed by Zaenen et al. (2004), or 0 if not applicable ([http://dl.acm.org/citation.cfm?id=1608954](http://dl.acm.org/citation.cfm?id=1608954))
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据Zaenen等人（2004年）提出的建议，提供一个生动性标签，若不适用则为0（[http://dl.acm.org/citation.cfm?id=1608954](http://dl.acm.org/citation.cfm?id=1608954)）
- en: A supertag (lexical category of CCG)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个超级标签（CCG的词汇类别）
- en: The lambda-DRS representing the semantics of the token in Boxer's Prolog format
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用Boxer的Prolog格式表示的lambda-DRS，表示标记的语义
- en: 'Out of these fields, we are going to use only the token and the named entity
    tag. However, we will work through loading the POS tag for a future exercise.
    The following code gets all the paths for these tags files:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些字段中，我们只使用标记和命名实体标签。然而，我们将在未来的练习中加载POS标签。以下代码获取这些标签文件的所有路径：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few processing steps need to happen. Each file has a number of sentences,
    with each words in a row. The entire sentence as a sequence and the corresponding
    sequence of NER tags need to be fed in as inputs while training the model. As
    mentioned above, the NER tags also need to be simplified to the top-level entities
    only. Secondly, the NER tags need to be converted to the IOB format. **IOB** stands
    for **In-Other-Begin**. These letters are used as a prefix to the NER tag. The
    sentence fragment in the table below shows how this scheme works:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行一些处理步骤。每个文件包含多个句子，每个句子中的单词排列成行。整个句子作为一个序列，与之对应的NER标签序列在训练模型时需要一起输入。如前所述，NER标签也需要简化为仅包含顶级实体。其次，NER标签需要转换为IOB格式。**IOB**代表**In-Other-Begin**。这些字母作为前缀附加到NER标签上。下表中的句子片段展示了该方案的工作方式：
- en: '| Reverend | Terry | Jones | arrived | in | New | York |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Reverend | Terry | Jones | arrived | in | New | York |'
- en: '| B-per | I-per | I-per | O | O | B-geo | I-geo |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| B-per | I-per | I-per | O | O | B-geo | I-geo |'
- en: The table above shows this tagging scheme after processing. Note that New York
    is one location. As soon as *New* is encountered, it marks the start of the geo
    NER tag, hence it is assigned B-geo. The next word is *York*, which is a continuation
    of the same geographical entity. For any network, classifying the word *New* as
    the start of the geographical entity is going to be very challenging. However,
    a BiLSTM network would be able to see the succeeding words, which helps quite
    a bit with disambiguation. Furthermore, the advantage of IOB tags is that the
    accuracy of the model improves considerably in terms of detection. This happens
    because once the beginning of an NER tag is detected, the choices for the next
    tag become quite limited.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上表展示了处理后的标签方案。请注意，New York是一个地点。一旦遇到*New*，它标志着地理位置NER标签的开始，因此被标记为B-geo。下一个单词是*York*，它是同一地理实体的延续。对于任何网络来说，将单词*New*分类为地理实体的开始将是非常具有挑战性的。然而，BiLSTM网络能够看到随后的单词，这对消除歧义非常有帮助。此外，IOB标签的优势在于，在检测方面，模型的准确性显著提高。这是因为一旦检测到NER标签的开始，下一标签的选择就会大大受限。
- en: 'Let''s get to the code. First, create a directory to store all the processed
    files:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入代码部分。首先，创建一个目录来存储所有处理后的文件：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We want to process the tags so that we strip the subcategories of the NER tags
    out. It would also be nice to collect some stats on the types of tags in the documents:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望处理这些标签，以便去除NER标签中的子类别。还希望收集文档中标签类型的一些统计数据：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The NER tag and IOB tag counters are set up above. A method for stripping the
    subcategory out of the NER tags is defined. The next method takes a sequence of
    tags and converts them into IOB format:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上面已设置了NER标签和IOB标签计数器。定义了一个方法来去除NER标签中的子类别。下一个方法接受一系列标签并将其转换为IOB格式：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once these two convenience functions are ready, all the tags files need to
    be read and processed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这两个便捷函数准备好后，所有的标签文件都需要被读取并处理：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'First, a counter is set for the number of sentences. A list of files written
    with paths are also initialized. As processed files are written out, their paths
    are added to the `outfiles` variable. This list will be used later to load all
    the data and to train the model. Files are read and split into two empty newline
    characters. That is the marker for the end of a sentence in the file. Only the
    actual words, POS tokens, and NER tokens are used from the file. Once these are
    collected, a new CSV file is written with three columns: the sentence, a sequence
    of POS tags, and a sequence of NER tags. This step may take a little while to
    execute:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设置一个计数器来统计句子的数量。还初始化了一个包含路径的文件列表。随着处理文件的写入，它们的路径会被添加到`outfiles`变量中。这个列表将在稍后用于加载所有数据并训练模型。文件被读取并根据两个空行符进行分割。该符号表示文件中句子的结束。文件中仅使用实际的单词、POS标记和NER标记。收集完这些后，将写入一个新的CSV文件，包含三列：句子、POS标签序列和NER标签序列。这个步骤可能需要一点时间来执行：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To confirm the distribution of the NER tags before and after processing, we
    can use the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认NER标签在处理前后的分布，我们可以使用以下代码：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As is evident, some tags were very infrequent, like *tim-dom*. It would be next
    to impossible for a network to learn them. Aggregating up one level helps increase
    the signal for these tags. To check if the entire process completed properly,
    check that the `ner` folder has 10,000 files. Now, let us load the processed data
    to normalize, tokenize, and vectorize it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，有些标签非常不常见，比如*tim-dom*。网络几乎不可能学习到它们。将其聚合到一个层级有助于增加这些标签的信号。为了检查整个过程是否完成，可以检查`ner`文件夹是否有10,000个文件。现在，让我们加载处理后的数据以进行标准化、分词和向量化。
- en: Normalizing and vectorizing data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化和向量化数据
- en: 'For this section, `pandas` and `numpy` methods will be used. The first step
    is to load the contents of the processed files into one `DataFrame`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，将使用`pandas`和`numpy`方法。第一步是将处理过的文件内容加载到一个`DataFrame`中：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This step may take a while given that it is processing 10,000 files. Once the
    content is loaded, we can check the structure of the `DataFrame`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可能需要一些时间，因为它正在处理10,000个文件。一旦内容加载完成，我们可以检查`DataFrame`的结构：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Both the text and NER tags need to be tokenized and encoded into numbers for
    use in training. We are going to be using core methods provided by the `keras.preprocessing`
    package. First, the tokenizer will be used to tokenize the text. In this example,
    the text only needs to be tokenized by white spaces, as it has been broken up
    already:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和NER标签都需要被分词并编码成数字，以便用于训练。我们将使用`keras.preprocessing`包提供的核心方法。首先，将使用分词器来分词文本。在这个例子中，由于文本已经被空格分割开，所以只需要通过空格进行分词：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The default values for the tokenizer are quite reasonable. However, in this
    particular case, it is important to only tokenize on spaces and not clean the
    special characters out. Otherwise the data will become mis-formatted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器的默认值相当合理。然而，在这种特殊情况下，重要的是只按空格进行分词，而不是清理特殊字符。否则，数据会变得格式错误：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Even though we do not use the POS tags, the processing for them is included.
    Use of the POS tags can have an impact on the accuracy of an NER model. Many NER
    entities are nouns, for example. However, we will see how to process POS tags
    but not use them in the model as features. This is left as an exercise to the
    reader.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们不使用POS标签，处理它们的步骤仍然包括在内。POS标签的使用会对NER模型的准确性产生影响。例如，许多NER实体是名词。然而，我们将看到如何处理POS标签但不将其作为特征用于模型。这部分留给读者作为练习。
- en: 'This tokenizer has some useful features. It provides a way to restrict the
    size of the vocabulary by word counts, TF-IDF, and so on. If the `num_words` parameter
    is passed with a numeric value, the tokenizer will limit the number of tokens
    by word frequencies to that number. The `fit_on_texts` method takes in all the
    texts, tokenizes them, and constructs dictionaries with tokens that will be used
    later to tokenize and encode in one go. A convenience function, `get_config()`,
    can be called after the tokenizer has been fit on texts to provide information
    about the tokens:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器有一些有用的功能。它提供了一种通过词频、TF-IDF等方式限制词汇表大小的方法。如果传入`num_words`参数并指定一个数字，分词器将根据词频限制令牌的数量为该数字。`fit_on_texts`方法接受所有文本，将其分词，并构建一个字典，稍后将在一次操作中用于分词和编码。可以在分词器适配完文本后调用方便的`get_config()`函数，以提供有关令牌的信息：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `index_word` dictionary property in the config provides a mapping between
    IDs and tokens. There is a considerable amount of information in the config. The
    vocabularies can be obtained from the config:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 配置中的`index_word`字典属性提供了ID与标记之间的映射。配置中包含了大量信息。词汇表可以从配置中获取：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Tokenizing and encoding text and named entity labels is quite easy:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本和命名实体标签进行分词和编码是非常简单的：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since sequences are of different sizes, they will all be padded or truncated
    to a size of 50 tokens. A helper function is used for this task:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序列的大小不同，它们将被填充或截断为50个标记的大小。为此任务使用了一个辅助函数：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The last step above is to ensure that shapes are correct before moving to the
    next step. Verifying shapes is a very important part of developing code in TensorFlow.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上面最后一步是确保在进入下一步之前，形状是正确的。验证形状是开发TensorFlow代码中非常重要的一部分。
- en: 'There is an additional step that needs to be performed on the labels. Since
    there are multiple labels, each label token needs to be one-hot encoded like so:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对标签执行额外的步骤。由于有多个标签，每个标签标记需要进行独热编码，如下所示：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we are ready to build and train a model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好构建和训练模型了。
- en: A BiLSTM model
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个BiLSTM模型
- en: 'The first model we will try is a BiLSTM model. First, the basic constants need
    to be set up:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试的第一个模型是BiLSTM模型。首先，需要设置基本常量：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, a convenience function for instantiating models is defined:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个便捷函数来实例化模型：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We are going to train our own embeddings. The next chapter will talk about using
    pre-trained embeddings and using them in models. After the embedding layer, there
    is a BiLSTM layer, followed by a `TimeDistributed` dense layer. This last layer
    is different from the sentiment analysis model, where there was only a single
    unit for binary output. In this problem, for each word in the input sequence,
    an NER token needs to be predicted. So, the output has as many tokens as the input
    sequence. Consequently, output tokens correspond 1-to-1 with input tokens and
    are classified as one of the NER classes. The `TimeDistributed` layer provides
    this capability. The other thing to note in this model is the use of regularization.
    It is important that the model does not overfit the training data. Since LSTMs
    have high model capacity, using regularization is very important. Feel free to
    play with some of these hyperparameters to get a feel for how the model will react.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练我们自己的嵌入层。下一章将讨论如何使用预训练的嵌入层并将其应用于模型。在嵌入层之后是一个BiLSTM层，接着是一个`TimeDistributed`全连接层。这个最后的层与情感分析模型有所不同，情感分析模型只有一个用于二分类输出的单元。而在这个问题中，对于输入序列中的每个单词，都需要预测一个NER标记。因此，输出的标记与输入的标记一一对应，并被分类为其中一个NER类别。`TimeDistributed`层提供了这种能力。这个模型的另一个需要注意的地方是正则化的使用。确保模型不会过拟合训练数据非常重要。由于LSTM具有较高的模型容量，因此使用正则化非常重要。可以随意调整这些超参数，看看模型的反应。
- en: 'Now the model can be compiled:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以编译模型了：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This simplistic model has over 2.6 million parameters!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的模型有超过260万个参数！
- en: If you notice, the bulk of the parameters are coming from the size of the vocabulary.
    The vocabulary has 39,422 words. This increases the model training time and computational
    capacity required. One way to reduce this is to make the vocabulary size smaller.
    The easiest way to do this would be to only consider words that have more than
    a certain frequency of occurrence or to remove words smaller than a certain number
    of characters. The vocabulary can also be reduced by converting all characters
    to lower case. However, in NER, case is a very important feature.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，大部分参数来自词汇表的大小。词汇表包含39,422个单词。这增加了模型的训练时间和所需的计算能力。减少这个问题的一种方法是将词汇表的大小减小。最简单的方法是只考虑出现频率超过某个阈值的单词，或者去除小于某个字符数的单词。还可以通过将所有字符转换为小写来减少词汇表的大小。然而，在NER任务中，大小写是一个非常重要的特征。
- en: 'This model is ready for training. The last thing that is needed is to split
    the data into train and test sets:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型已经准备好进行训练了。最后需要做的事情是将数据拆分为训练集和测试集：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, the model is ready for training:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已准备好进行训练：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Over 15 epochs of training, the model is doing quite well with over 99% accuracy.
    Let''s see how the model performs on the test set and whether the regularization
    helped:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 经过15个epochs的训练，模型表现得相当不错，准确率超过99%。让我们看看模型在测试集上的表现，正则化是否有所帮助：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The model performs well on the test data set, with over 96.5% accuracy. The
    difference between the train and test accuracies is still there, implying that
    the model could use some additional regularization. You can play with the dropout
    variable or add additional dropout layers between the embedding and BiLSTM layers,
    and between the `TimeDistributed` layer and the final Dense layer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在测试数据集上的表现良好，准确率超过 96.5%。训练集与测试集的准确率仍有差距，意味着该模型可能需要更多的正则化。你可以调整丢弃率变量，或者在嵌入层和
    BiLSTM 层之间，`TimeDistributed` 层和最终的 Dense 层之间添加额外的丢弃层。
- en: 'Here is an example of a sentence fragment tagged by this model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该模型标记的一个句子片段的示例：
- en: '|  | Faure | Gnassingbe | said | in | a | speech | carried | by | state | media
    | Friday |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 法乌尔 | 格纳辛贝 | 在周五的讲话中表示 | 由国家媒体报道 |'
- en: '| Actual | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 实际 | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
- en: '| Model | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
- en: This model is not doing poorly at all. It was able to identify the person and
    time entities in the sentence.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型表现并不差。它能够识别句子中的人名和时间实体。
- en: As good as this model is, it does not use an important characteristic of named
    entity tags – a given tag is highly correlated with the tag coming after it. CRFs
    can take advantage of this information and further improve the accuracy of NER
    tasks. Let's understand how CRFs work and add them to the network above next.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该模型表现优秀，但它没有利用命名实体标签的一个重要特征——一个给定的标签与后续标签高度相关。CRFs 可以利用这一信息，进一步提高 NER 任务的准确性。接下来我们将理解
    CRFs 的工作原理，并将其添加到上述网络中。
- en: Conditional random fields (CRFs)
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件随机场（CRFs）
- en: 'BiLSTM models look at a sequence of input words and predict the label for the
    current word. In making this determination, only the information of previous inputs
    is considered. Previous predictions play no role in making this decision. However,
    there is information encoded in the sequence of labels that is being discounted.
    To illustrate this point, consider a subset of NER tags: **O**, **B-Per**, **I-Per**,
    **B-Geo**, and **I-Geo**. This represents two domains of person and geographical
    entities and an *Other* category for everything else. Based on the structure of
    IOB tags, we know that any **I**- tag must be preceded by a **B-I** from the same
    domain. This also implies that an **I**- tag cannot be preceded by an **O** tag.
    The following diagram shows the possible state transitions between these tags:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: BiLSTM 模型查看一系列输入词，并预测当前词的标签。在做出这一决定时，仅考虑之前输入的信息。之前的预测不在做出决策时起作用。然而，标签序列中编码的信息却被忽略了。为了说明这一点，考虑一组
    NER 标签：**O**、**B-Per**、**I-Per**、**B-Geo** 和 **I-Geo**。这代表了两种实体领域：人名和地理名，以及一个表示其他所有实体的
    *Other* 类别。基于 IOB 标签的结构，我们知道任何 **I** 标签必须由同一领域的 **B-I** 标签之前标记。这也意味着 **I** 标签不能由
    **O** 标签之前标记。下图展示了这些标签之间可能的状态转换：
- en: '![](img/B16252_03_02.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_03_02.png)'
- en: 'Figure 3.2: Possible NER tag transitions'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：可能的 NER 标签转换
- en: '*Figure 3.2* color codes similar types of transitions with the same color.
    An **O** tag can transition only to a **B** tag. A **B** tag can go to its corresponding
    **I** tag or back to the **O** tag. An **I** tag can transition back to itself,
    an **O** tag, or a **B** tag of a different domain (not represented in the diagram
    for simplicity). For a set of **N** tags, these transitions can be represented
    by a matrix of dimension *N x N*. *P*[i,j] denotes the possibility of tag *j*
    coming after tag *i*. Note that these transition weights can be learned based
    on the data. Such a learned transition weights matrix could be used during prediction
    to consider the entire sequence of predicted labels and make updates to the probabilities.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.2* 使用颜色编码相似类型的转换，以相同颜色表示。**O** 标签只能转换为 **B** 标签。**B** 标签可以转换为其对应的 **I**
    标签，或者回到 **O** 标签。**I** 标签可以转回自身、**O** 标签，或者不同领域的 **B** 标签（为简化图示，未在图中表示）。对于一组 **N**
    个标签，这些转换可以用一个 *N x N* 的矩阵表示。*P*[i,j] 表示标签 *j* 紧随标签 *i* 之后的可能性。请注意，这些转换权重可以基于数据进行学习。在预测过程中，可以使用这种学习到的转换权重矩阵，考虑整个预测标签序列并更新概率。'
- en: 'Here is an illustrative matrix with indicative transition weights:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示意矩阵，显示了指示性的转换权重：
- en: '| From > To | O | B-Geo | I-Geo | B-Org | I-Org |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 从 > 到 | O | B-Geo | I-Geo | B-Org | I-Org |'
- en: '| O | 3.28 | 2.20 | 0.00 | 3.66 | 0.00 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| O | 3.28 | 2.20 | 0.00 | 3.66 | 0.00 |'
- en: '| B-Geo | -0.25 | -0.10 | 4.06 | 0.00 | 0.00 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| B-Geo | -0.25 | -0.10 | 4.06 | 0.00 | 0.00 |'
- en: '| I-Geo | -0.17 | -0.61 | 3.51 | 0.00 | 0.00 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| I-Geo | -0.17 | -0.61 | 3.51 | 0.00 | 0.00 |'
- en: '| B-Org | -0.10 | -0.23 | 0.00 | -1.02 | 4.81 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| B-Org | -0.10 | -0.23 | 0.00 | -1.02 | 4.81 |'
- en: '| I-Org | -0.33 | -1.75 | 0.00 | -1.38 | 5.10 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| I-Org | -0.33 | -1.75 | 0.00 | -1.38 | 5.10 |'
- en: As per the table above, the weight of the edge connecting I-Org to B-Org has
    a weight of -1.38, implying that this transition is extremely unlikely to happen.
    Practically, implementing a CRF has three main steps. The first step is modifying
    the score generated by the BiLSTM layer and accounting for the transition weights,
    as shown above. A sequence of predictions
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上表，从I-Org到B-Org的边的权重为-1.38，意味着这种转移发生的可能性极低。实际上，实现CRF有三个主要步骤。第一步是修改BiLSTM层生成的得分，并考虑转移权重，如上所示。一个预测序列
- en: '![](img/B16252_03_001.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_03_001.png)'
- en: 'generated by the BiLSTM layer above for a sequence of *n* tags in the space
    of *k* unique tags is available, which operates on an input sequence *X*. *P*
    represents a matrix of dimensions *n × k*, where the element *P*[i,j] represents
    the probability of *j*^(th) tag for output at the position *y*[i]. Let *A* be
    a square matrix of transition probabilities as shown above, with a dimension of
    *(k + 2) × (k + 2)* where two additional tokens are added for start- and end-of-sentence
    markers. Element *A*[i,j] represents the transition probability from *i* to tag
    *j*. Using these values, a new score can be calculated like so:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由上述BiLSTM层为一系列*n*标签生成的得分，在*k*个唯一标签的空间中是可用的，它作用于输入序列*X*。*P*表示一个维度为*n × k*的矩阵，其中元素*P*[i,j]表示在位置*y*[i]处输出标签*j*的概率。让*A*成为如上所示的转移概率的方阵，维度为*(k
    + 2) × (k + 2)*，因为在句子的开始和结束标记处增加了两个额外的标记。元素*A*[i,j]表示从*i*到标签*j*的转移概率。使用这些值，可以计算出新的得分，方式如下：
- en: '![](img/B16252_03_002.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_03_002.png)'
- en: 'A softmax can be calculated over all possible tag sequences to get the probability
    for a given sequence *y*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可以对所有可能的标签序列计算softmax，以获取给定序列*y*的概率：
- en: '![](img/B16252_03_003.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_03_003.png)'
- en: '*Y*[X] represents all possible tag sequences, including those that may not
    conform to the IOB tag format. To train using this softmax, a log-likelihood can
    be calculated over this. Through clever use of dynamic programming, a combinatorial
    explosion can be avoided, and the denominator can be computed quite efficiently.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y*[X]表示所有可能的标签序列，包括那些可能不符合IOB标签格式的序列。为了使用这个softmax进行训练，可以计算其对数似然。通过巧妙地使用动态规划，可以避免组合爆炸，且分母可以高效地计算。'
- en: Only simplistic math is shown to help build an intuition of how this method
    works. The actual computations will become clear in the custom layer implementation
    below.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里只展示了简单的数学公式，以帮助建立这种方法如何工作的直觉。实际的计算将在下面的自定义层实现中更加明确。
- en: While decoding, the output sequence is the one that has the maximum score among
    these possible sequences, calculated conceptually using an `argmax` style function.
    The Viterbi algorithm is commonly used to implement a dynamic programming solution
    for decoding. First, let us code the model and the training for it before getting
    into decoding.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码时，输出序列是这些可能序列中得分最高的序列，这个得分通过概念上使用`argmax`风格的函数计算。维特比算法通常用于实现动态规划解码。首先，我们先编写模型并进行训练，然后再进入解码部分。
- en: NER with BiLSTM and CRFs
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有BiLSTM和CRF的命名实体识别（NER）
- en: 'Implementing a BiLSTM network with CRFs requires adding a CRF layer on top
    of the BiLSTM network developed above. However, a CRF is not a core part of the
    TensorFlow or Keras layers. It is available through the `tensorflow_addons` or
    `tfa` package. The first step is to install this package:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 实现带有CRF的BiLSTM网络需要在上述开发的BiLSTM网络上添加一个CRF层。然而，CRF并不是TensorFlow或Keras层的核心部分。它可以通过`tensorflow_addons`或`tfa`包来使用。第一步是安装这个包：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'There are many sub-packages, but the convenience functions for the CRF are
    in the `tfa.text` subpackage:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多子包，但CRF的便捷函数位于`tfa.text`子包中：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_03_03.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图，自动生成的描述](img/B16252_03_03.png)'
- en: 'Figure 3.3: tfa.text methods'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：tfa.text方法
- en: While low-level methods for implementing the CRF layer are provided, a high-level
    layer-like construct is not provided. The implementation of a CRF requires a custom
    layer, a loss function, and a training loop. Post training, we will look at how
    to implement a customized inference function that will use Viterbi decoding.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提供了实现 CRF 层的低级方法，但并没有提供类似高级层的构造。实现 CRF 需要一个自定义层、损失函数和训练循环。训练完成后，我们将查看如何实现一个自定义的推理函数，该函数将使用
    Viterbi 解码。
- en: Implementing the custom CRF layer, loss, and model
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自定义的 CRF 层、损失函数和模型
- en: 'Similar to the flow above, there will be an embedding layer and a BiLSTM layer.
    The output of the BiLSTM needs to be evaluated with the CRF log-likelihood loss
    described above. This is the loss that needs to be used to train the model. The
    first step in implementation is creating a custom layer. Implementing a custom
    layer in Keras requires subclassing `keras.layers.Layer`. The main method to be
    implemented is `call()`, which takes inputs to the layer, transforms them, and
    returns the result. Additionally, the constructor to the layer can also set up
    any parameters that are needed. Let''s start with the constructor:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 与上面的流程类似，模型中将包含一个嵌入层和一个 BiLSTM 层。BiLSTM 层的输出需要使用上文描述的 CRF 对数似然损失进行评估。这是训练模型时需要使用的损失函数。实现的第一步是创建一个自定义层。在
    Keras 中实现自定义层需要继承 `keras.layers.Layer` 类。需要实现的主要方法是 `call()`，该方法接收层的输入，对其进行转换，并返回结果。此外，层的构造函数还可以设置所需的任何参数。我们先从构造函数开始：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The main parameters that are needed are:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的主要参数有：
- en: '**The number of labels and the transition matrix**: As described in the section
    above, a transition matrix needs to be learned. The dimension of that square matrix
    is the number of labels. The transition matrix is initialized using the parameters.
    This transition parameters matrix is not trainable through gradient descent. It
    is calculated as a consequence of computing the log-likelihoods. The transition
    parameters matrix can also be passed into this layer if it has been learned in
    the past.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签数量和转移矩阵**：如上文所述，需要学习一个转移矩阵。这个方阵的维度等于标签的数量。转移矩阵使用参数初始化。该转移参数矩阵无法通过梯度下降进行训练。它是计算对数似然的结果。转移参数矩阵也可以在过去已学习的情况下传递给该层。'
- en: '**The mask id**: Since the sequences are padded, it is important to recover
    the original sequence lengths for computing transition scores. By convention,
    a value of 0 is used for the mask, and that is the default. This parameter is
    set up for future configurability.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩码 ID**：由于序列是填充的，因此在计算转移得分时，恢复原始序列长度非常重要。按照约定，掩码使用值 0，且这是默认值。这个参数为将来的可配置性做好了设置。'
- en: 'The second method is to compute the result of applying this layer. Note that
    as a layer, the CRF layer merely regurgitates the outputs during training time.
    The CRF layer is useful only during inference. At inference time, it uses the
    transition matrix and logic to correct the sequences'' output by the BiLSTM layers
    before returning them. For now, this method is quite simple:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个方法是计算应用该层后的结果。请注意，作为一层，CRF 层仅在训练时重复输出。CRF 层仅在推理时有用。在推理时，它使用转移矩阵和逻辑来纠正 BiLSTM
    层输出的序列，然后再返回它们。现在，这个方法相对简单：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This method takes the inputs as well as a parameter that helps specify if this
    method is called during training or during inference. If this variable is not
    passed, it is pulled from the Keras backend. When models are trained with the
    `fit()` method, `learning_phase()` returns `True`. When the `.predict()` method
    is called on a model, this flag is set to `false`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收输入以及一个参数，该参数帮助指定此方法是用于训练时调用还是用于推理时调用。如果未传递此变量，它将从 Keras 后端获取。当使用 `fit()`
    方法训练模型时，`learning_phase()` 返回 `True`。当调用模型的 `.predict()` 方法时，这个标志将被设置为 `false`。
- en: As sequences being passed are masked, this layer needs to know the real sequence
    lengths during inference time for decoding. A variable is passed for it but is
    unused at this time. Now that the basic CRF layer is ready, let's build the model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传递的序列是被掩码的，因此在推理时该层需要知道真实的序列长度以进行解码。为此，传递了一个变量，但当前未使用它。现在基本的 CRF 层已准备就绪，让我们开始构建模型。
- en: A custom CRF model
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个自定义的 CRF 模型
- en: 'Since the model builds on a number of preexisting layers in addition to the
    custom CRF layer above, explicit imports help the readability of the code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型在构建时除了自定义的 CRF 层外，还依赖于多个预先存在的层，因此显式的导入语句有助于提高代码的可读性：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The first step is to define a constructor that will create the various layers
    and store the appropriate dimensions:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义一个构造函数，该构造函数将创建各种层并存储适当的维度：
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This constructor takes in the number of hidden units for the BiLSTM later, the
    size of words in the vocabulary, the number of NER labels, and the size of the
    embeddings. Additionally, a default name is set by the constructor, which can
    be overridden at the time of instantiation. Any additional parameters supplied
    are passed along as keyword arguments.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 该构造函数接收 BiLSTM 后续层的隐藏单元数、词汇表中单词的大小、NER 标签的数量以及嵌入的大小。此外，构造函数会设置一个默认名称，可以在实例化时进行覆盖。任何额外的参数都会作为关键字参数传递。
- en: 'During training and prediction, the following method will be called:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和预测过程中，将调用以下方法：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So, in a few lines of code, we have implemented a customer model using the custom
    CRF layer developed above. The only thing that we need now to train this model
    is a loss function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过几行代码，我们已经使用上述自定义 CRF 层实现了一个自定义模型。现在唯一需要的就是一个损失函数来训练该模型。
- en: A custom loss function for NER using a CRF
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 CRF 的自定义损失函数进行命名实体识别（NER）
- en: 'Let''s implement the loss function as part of the CRF layer, encapsulated in
    a function of the same name. Note that when this function is called, it is usually
    passed the labels and predicted values. We will model our loss function on the
    custom loss functions in TensorFlow. Add this code to the CRF layer class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将损失函数作为 CRF 层的一部分来实现，并封装在一个同名的函数中。请注意，调用此函数时，通常会传递标签和预测值。我们将基于 TensorFlow
    中的自定义损失函数来建模我们的损失函数。将以下代码添加到 CRF 层类中：
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This function takes the true labels and predicted labels. Both of these tensors
    are usually of the shape (batch size, max sequence length, number of NER labels).
    However, the log-likelihood function in the `tfa` package expects the labels to
    be in a (batch size, max sequence length)-shaped tensor. So a convenience function,
    implemented as part of the CRF layer and shown below, is used to perform the conversion
    of label shapes:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接收真实标签和预测标签。这两个张量通常具有形状（批量大小，最大序列长度，NER 标签数量）。然而，`tfa` 包中的对数似然函数要求标签的形状为（批量大小，最大序列长度）张量。因此，使用一个便利函数，它作为
    CRF 层的一部分，如下所示，用于执行标签形状的转换：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The log-likelihood function also requires the actual sequence lengths for each
    example. These sequence lengths can be computed from the labels and the mask identifier
    that was set up in the constructor of this layer (see above). This process is
    encapsulated in another convenience function, also part of the CRF layer:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然函数还需要每个样本的实际序列长度。这些序列长度可以从标签和在该层构造函数中设置的掩码标识符中计算出来（见上文）。这个过程被封装在另一个便利函数中，也属于
    CRF 层的一部分：
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: First, a Boolean mask is generated from the labels by comparing the value of
    the label to the mask ID. Then, through casting the Boolean as an integer and
    summing across the row, the length of the sequence is regenerated. Now, the `tfa.text.crf_log_likelihood()`
    function is called to calculate and return the log-likelihoods and the transition
    matrix. The CRF layer's transition matrix is updated with the transition matrix
    returned from the function call. Finally, the loss is computed by summing up all
    the log-likelihoods returned.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过将标签的值与掩码 ID 进行比较，生成一个布尔掩码。然后，通过将布尔值转换为整数并对行进行求和，重新生成序列的长度。接着，调用 `tfa.text.crf_log_likelihood()`
    函数来计算并返回对数似然值和转移矩阵。CRF 层的转移矩阵将根据函数调用返回的转移矩阵进行更新。最后，通过对所有返回的对数似然值求和来计算损失。
- en: At this point, our coded custom model is ready to start training. We will need
    to set up the data and create a custom training loop.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们的自定义模型已经准备好开始训练。我们需要设置数据并创建自定义训练循环。
- en: Implementing custom training
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自定义训练
- en: 'The model needs to be instantiated and initialized for training:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模型需要实例化并初始化以进行训练：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As in past examples, an Adam optimizer will be used. Next, we will construct
    `tf.data.DataSet` from the DataFrames loaded in the BiLSTM section above:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例一样，将使用 Adam 优化器。接下来，我们将从上述 BiLSTM 部分加载的 DataFrame 构建 `tf.data.DataSet`：
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Roughly 20% of the data is reserved for testing. The rest is used for training.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 大约 20% 的数据将保留用于测试，其余部分用于训练。
- en: 'To implement a custom training loop, TensorFlow 2.0 exposes a gradient tape.
    This allows low-level management of the main steps required for training any model
    with gradient descent. These steps are:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现自定义训练循环，TensorFlow 2.0 提供了梯度带（gradient tape）。这允许对训练任何模型时所需的主要步骤进行低级别的管理，这些步骤包括：
- en: Computing the forward pass predictions
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算前向传播的预测结果
- en: Computing the loss when these predictions are compared with the labels
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些预测与标签比较时的损失
- en: Computing the gradients for the trainable parameters based on the loss and then
    using the optimizer to adjust the weights
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算基于损失函数的可训练参数的梯度，然后使用优化器来调整权重
- en: 'Let us train this model for 5 epochs and watch the loss as training progresses.
    Compare this to the 15 epochs of training for the previous model. The custom training
    loop is shown below:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练这个模型 5 个时期，并观察随着训练的进行，损失是如何变化的。与之前模型训练 15 个时期的情况进行对比。自定义训练循环如下所示：
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'A metric is created to keep track of the average loss over time. For 5 epochs,
    inputs and labels are pulled from the training data set, one batch at a time.
    Using `tf.GradientTape()` to keep track of the operations, the steps outlined
    in the bullets above are implemented. Note that we pass the trainable variable
    manually as this is a custom training loop. Finally, the loss metric is printed
    every 50^(th) step to show training progress. This yields the results below, which
    have been abbreviated:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个指标来跟踪平均损失随时间的变化。在 5 个时期内，输入和标签每次从训练数据集中提取一批。使用 `tf.GradientTape()` 跟踪操作，按照上面列出的步骤实现。请注意，由于这是自定义训练循环，我们手动传递了可训练变量。最后，每
    50 步打印一次损失指标，以显示训练进度。下面是略去部分内容后的结果：
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Given we implemented a custom training loop, without requiring a compilation
    of the model, we could not obtain a summary of the model parameters before. To
    get an idea of the size of the model, a summary can be obtained now:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们实现了自定义训练循环，并且不需要模型编译，因此之前无法获取模型参数的总结。现在，为了了解模型的规模，可以获取一个摘要：
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It is comparable in size to the previous model but has some untrainable parameters.
    These are coming from the transition matrix. The transition matrix is not learned
    through gradient descent. Thus, they are classified as non-trainable parameters.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它的大小与之前的模型相当，但有一些不可训练的参数。这些参数来自转移矩阵。转移矩阵并不是通过梯度下降学习的，因此它们被归类为不可训练参数。
- en: However, training loss is hard to interpret. To compute accuracy, we need to
    implement decoding, which is the focus of the next section. For the moment, let's
    assume that decoding is available and examine the results of training for 5 epochs.
    For illustration purposes, here is a sentence from the test set with the results
    pulled at the end of the first epoch and at the end of five epochs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练损失很难解释。为了计算准确率，我们需要实现解码，这是下一节的重点。暂时假设解码已经实现，并检查训练 5 个时期后的结果。为了便于说明，这里有一个来自测试集的句子，显示了在第一个时期结束时和
    5 个时期结束时的结果。
- en: 'The example sentence is:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 示例句子是：
- en: '[PRE48]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The corresponding true label is:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的真实标签是：
- en: '[PRE49]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This is a difficult example for NER with *The Washington Post* as a three-word
    organization, where the first word is very common and used in multiple contexts,
    and the second word is also the name of a geographical location. Also note the
    imperfect labels of the GMB data set, where the second tag of the name *Ushakov*
    is tagged as an organization. At the end of the first epoch of training, the model
    predicts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个对于命名实体识别（NER）来说很有挑战的例子，其中 *The Washington Post* 被标记为一个三词的组织名，第一个词非常常见，并在多个语境中使用，第二个词也是一个地理位置的名称。还需要注意
    GMB 数据集中的标签不完善，其中名字 *Ushakov* 的第二个标签被标记为一个组织。训练的第一个时期结束时，模型预测结果为：
- en: '[PRE50]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'It gets confused by the organization not being where it expects it to be. It
    also shows that it hasn''t learned the transition probabilities by putting an
    I-org tag after a B-geo tag. However, it does not make a mistake in the person
    portion. Unfortunately for the model, it will not get credit for this great prediction
    of the person tag, and due to imperfect labels, it will still count as a miss.
    The result after five epochs of training is better than the original:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 它在组织名没有出现在预期位置时会产生混淆。它还显示它没有学会转移概率，因为它在 B-geo 标签后面放了一个 I-org 标签。然而，它在处理人物部分时没有犯错。不幸的是，模型并不会因为准确预测人物标签而得到奖励，由于标签的不完善，这仍然会被算作一次漏检。经过五个时期的训练后，结果比最初要好：
- en: '[PRE51]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is a great result, given the limited amount of training we have done. Now,
    let's see how we can decode the sentence in the CRF layer to get these sequences.
    The algorithm used for decoding is called the Viterbi decoder.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的结果，考虑到我们所做的训练有限。现在，让我们看看如何在CRF层中解码句子以得到这些序列。用于解码的算法称为维特比解码器。
- en: Viterbi decoding
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维特比解码
- en: A straightforward way to predict the sequence of labels is to output the label
    that has the highest activation from the previous layers of the network. However,
    this could be sub-optimal as it assumes that each label prediction is independent
    of the previous or successive predictions. The Viterbi algorithm is used to take
    the predictions for each word in the sequence and apply a maximization algorithm
    so that the output sequence has the highest likelihood. In future chapters, we
    will see another way of accomplishing the same objective through beam search.
    Viterbi decoding involves maximizing over the entire sequence as opposed to optimizing
    at each word of the sequence. To illustrate this algorithm and way of thinking,
    let's take an example of a sentence of 5 words, and a set of 3 labels. These labels
    could be O, B-geo, and I-geo as an example.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接预测标签序列的方法是输出来自网络前一层激活值最高的标签。然而，这可能是次优的，因为它假设每个标签的预测与之前或之后的预测是独立的。维特比算法用于获取序列中每个单词的预测，并应用最大化算法，使得输出序列具有最高的可能性。在未来的章节中，我们将看到通过束搜索（beam
    search）实现相同目标的另一种方法。维特比解码涉及对整个序列进行最大化，而不是对序列中的每个单词进行优化。为了说明这个算法和思维方式，让我们以一个包含5个单词的句子和3个标签的集合为例。这些标签可以是O、B-geo和I-geo。
- en: 'This algorithm needs the transition matrix values between labels. Recall that
    this was generated and stored in the custom CRF layer above. Let''s say that the
    matrix looks like so:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法需要标签之间的转换矩阵值。回忆一下，这些值是通过上面的自定义CRF层生成并存储的。假设矩阵如下所示：
- en: '| From > To | Mask | O | B-geo | I-geo |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 从 > 到 | 掩码 | O | B-geo | I-geo |'
- en: '| Mask | 0.6 | 0.3 | 0.2 | 0.01 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 掩码 | 0.6 | 0.3 | 0.2 | 0.01 |'
- en: '| O | 0.8 | 0.5 | 0.6 | 0.01 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| O | 0.8 | 0.5 | 0.6 | 0.01 |'
- en: '| B-geo | 0.2 | 0.4 | 0.01 | 0.7 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| B-geo | 0.2 | 0.4 | 0.01 | 0.7 |'
- en: '| I-geo | 0.3 | 0.4 | 0.01 | 0.5 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| I-geo | 0.3 | 0.4 | 0.01 | 0.5 |'
- en: 'To explain how the algorithm works, the figure shown below will be used:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明算法是如何工作的，下面将使用所示的图形：
- en: '![](img/B16252_03_04.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_03_04.png)'
- en: 'Figure 3.4: Steps in the Viterbi decoder'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：维特比解码器的步骤
- en: The sentence starts from the left. Arrows from the start of the word to the
    first token represent the probability of the transition between the two tokens.
    The numbers on the arrows should match the values in the transition matrix above.
    Within the circles denoting labels, scores generated by the neural network, the
    BiLSTM model, in our case, are shown for the first word. These scores need to
    be added together to give the final score of the words. Note that we switched
    the terminology from probabilities to scores as normalization is not being performed
    for this particular example.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 句子从左侧开始。箭头从单词的起始位置指向第一个标记，表示两个标记之间转换的概率。箭头上的数字应该与上方转换矩阵中的值相匹配。在表示标签的圆圈内，显示了神经网络（在我们这个案例中是BiLSTM模型）为第一个单词生成的分数。这些分数需要加在一起，得到单词的最终分数。请注意，我们将术语从概率改为分数，因为在这个特定示例中没有执行归一化操作。
- en: The probability of the first word label
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个单词标签的概率
- en: 'Score of *O*: 0.3 (transition score) + 0.2 (activation score) = 0.5'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*O*的分数：0.3（转换分数）+ 0.2（激活分数）= 0.5'
- en: 'Score of *B-geo*: 0.2 (transition score) + 0.3 (activation score) = 0.5'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*B-geo*的分数：0.2（转换分数）+ 0.3（激活分数）= 0.5'
- en: 'Score of *I-geo*: 0.01 (transition score) + 0.01 (activation score) = 0.02'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*I-geo*的分数：0.01（转换分数）+ 0.01（激活分数）= 0.02'
- en: 'At this point, it is equally likely that an *O* or *B-geo* tag will be the
    starting tag. Let''s consider the next tag and calculate the scores using the
    same approach for the following sequences:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，*O*标签或*B-geo*标签作为起始标签的可能性是一样的。让我们考虑下一个标签，并使用相同的方法计算以下序列的分数：
- en: '| (*O*, *B-geo*) = 0.6 + 0.3 = 0.9 | (*B-geo*, *O*) = 0.4 + 0.3 = 0.7 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| (*O*, *B-geo*) = 0.6 + 0.3 = 0.9 | (*B-geo*, *O*) = 0.4 + 0.3 = 0.7 |'
- en: '| (*O*, *I-geo*) = 0.01+ 0.25 = 0.26 | (*B-geo*, *B-geo*) = 0.01 + 0.3 = 0.31
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| (*O*, *I-geo*) = 0.01 + 0.25 = 0.26 | (*B-geo*, *B-geo*) = 0.01 + 0.3 = 0.31
    |'
- en: '| (*O*, *O*) = 0.5 + 0.3 = 0.8 | (*B-geo*, *I-geo*) = 0.7 + 0.25 = 0.95 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| (*O*, *O*) = 0.5 + 0.3 = 0.8 | (*B-geo*, *I-geo*) = 0.7 + 0.25 = 0.95 |'
- en: 'This process is called the forward pass. It should also be noted, even though
    this is a contrived example, that activations at a given input may not be the
    best predictor of the right label for that word once the previous labels have
    been considered. If the sentence was only two words, then the scores for various
    sequences could be calculated by summing by each step:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为前向传播。还应注意，尽管这是一个人为构造的示例，但考虑到先前的标签时，给定输入的激活可能并不是最好的预测该词正确标签的依据。如果句子只有两个词，那么可以通过每一步相加来计算各种序列的得分：
- en: '| (*Start*, *O*, *B-Geo*) = 0.5 + 0.9 = 1.4 | (*Start*, *B-Geo*, *O*) = 0.5
    + 0.7 = 1.2 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| (*Start*, *O*, *B-Geo*) = 0.5 + 0.9 = 1.4 | (*Start*, *B-Geo*, *O*) = 0.5
    + 0.7 = 1.2 |'
- en: '| (*Start*, *O*, *O*) = 0.5 + 0.8 = 1.3 | (*Start*, *B-geo*, *B-geo*) = 0.5
    + 0.31 = 0.81 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| (*Start*, *O*, *O*) = 0.5 + 0.8 = 1.3 | (*Start*, *B-geo*, *B-geo*) = 0.5
    + 0.31 = 0.81 |'
- en: '| (*Start*, *O*, *I-Geo*) = 0.5 + 0.26 = 0.76 | (*Start*, *B-geo*, *I-geo*)
    = 0.5 + 0.95) = 1.45 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| (*Start*, *O*, *I-Geo*) = 0.5 + 0.26 = 0.76 | (*Start*, *B-geo*, *I-geo*)
    = 0.5 + 0.95 = 1.45 |'
- en: If only the activation scores were considered, the most probable sequences would
    be either (*Start*, *B-geo*, *O*) or (*Start*, *B-geo*, *B-geo*). However, using
    the transition scores along with the activations means that the sequence with
    the highest probability is (*Start*, *B-geo*, *I-geo*) in this example. While
    the forward pass gives the highest score of the entire sequence given the last
    token, the backward pass process would reconstruct the sequence that resulted
    in this highest score. This is essentially the Viterbi algorithm, which uses dynamic
    programming to perform these steps in an efficient manner.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仅考虑激活得分，那么最可能的序列将是 (*Start*, *B-geo*, *O*) 或 (*Start*, *B-geo*, *B-geo*)。然而，使用转换得分和激活值一起考虑时，得分最高的序列是
    (*Start*, *B-geo*, *I-geo*)。虽然前向传播给出了给定最后一个标记时整个序列的最高得分，反向传播过程将重构出导致该最高得分的序列。这实际上是维特比算法，它使用动态规划以高效的方式执行这些步骤。
- en: 'Implementing this algorithm is aided by the fact the core computation is provided
    as a method in the `tfa` package. This decoding step will be implemented in the
    `call()` method of the CRF layer implemented above. Modify this method to look
    like so:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这个算法的一个优势是，核心计算已作为 `tfa` 包中的方法提供。这个解码步骤将在上面实现的 CRF 层的 `call()` 方法中实现。修改此方法如下所示：
- en: '[PRE52]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The new lines added have been highlighted. The `viterbi_decode()` method takes
    the activations from the previous layers and the transition matrix along with
    the maximum sequence length to compute the path with the highest score. This score
    is also returned, but we ignore it for our purposes of inference. This process
    needs to be performed for each sequence in the batch. Note that this method returns
    sequences on different lengths. This makes it harder to convert into tensors,
    so a utility function is used to pad the returned sequences:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 新添加的行已被突出显示。`viterbi_decode()`方法将前一层的激活值和转换矩阵与最大序列长度一起使用，计算出得分最高的路径。此得分也会返回，但在推理过程中我们忽略它。这个过程需要对批次中的每个序列执行。请注意，此方法返回不同长度的序列。这使得转换为张量变得更加困难，因此使用了一个实用程序函数来填充返回的序列：
- en: '[PRE53]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: A dropout layer works completely opposite to the way this CRF layer works. A
    dropout layer modifies the inputs only during training time. During inference,
    it merely passes all the inputs through.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 层的工作方式完全与 CRF 层相反。Dropout 层只在训练时修改输入数据。在推理时，它只是将所有输入传递过去。
- en: Our CRF layer works in the exact opposite fashion. It passes the inputs through
    during training, but it transforms inputs using the Viterbi decoder during inference
    time. Note the use of the `training` parameter to control the behavior.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CRF 层的工作方式完全相反。它在训练时传递输入数据，但在推理时使用维特比解码器转换输入。注意 `training` 参数的使用来控制行为。
- en: 'Now that the layer is modified and ready, the model needs to be re-instantiated
    and trained. Post-training, inference can be performed like so:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，层已修改并准备好，模型需要重新实例化并进行训练。训练后，可以像这样进行推理：
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will run inference on a small batch of testing data. Let''s check the
    result for the example sentence:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这将对一小批测试数据进行推理。让我们查看示例句子的结果：
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As we can see in the highlighted output, the results are better than the actual
    data!
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在突出显示的输出中看到的，结果比实际数据更好！
- en: '[PRE57]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To get a sense of the accuracy of the training, a custom method needs to be
    implemented. This is shown below:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估训练的准确性，需要实现一个自定义方法。如下所示：
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Using `numpy`''s `MaskedArray` feature, the predictions and labels are compared
    and converted to an integer array, and the mean is calculated to compute the accuracy:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`numpy`的`MaskedArray`功能，比较预测结果和标签并将其转换为整数数组，然后计算均值以计算准确率：
- en: '[PRE60]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This is a pretty accurate model, just after 5 epochs of training and with very
    simple architecture, all while using embeddings that are trained from scratch.
    A recall metric can also be implemented in a similar fashion. A BiLSTM-only model,
    shown earlier, took 15 epochs of training to get to a similar accuracy!
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当准确的模型，仅经过5个周期的训练，并且架构非常简单，同时使用的是从零开始训练的词嵌入。召回率指标也可以以类似的方式实现。前面展示的仅使用BiLSTM的模型，经过15个周期的训练才达到了类似的准确度！
- en: This completes the implementation of an NER model using BiLSTMs and CRFs. If
    this is interesting and you would like to continue working on this, look for the
    CoNLL 2003 data set for NER. Even today, papers are being published that aim to
    improve the accuracy of the models based on that data set.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了使用BiLSTM和CRF实现NER模型。如果这对你有兴趣，并且你想继续研究这个领域，可以查找CoNLL 2003数据集用于NER。即使在今天，仍然有许多论文发布，旨在提高基于该数据集的模型的准确性。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have covered quite a lot of ground in this chapter. NER and its importance
    in the industry were explained. To build NER models, BiLSTMs and CRFs are needed.
    Using BiLSTMs, which we learned about in the previous chapter while building a
    sentiment classification model, we built a first version of a model that can label
    named entities. This model was further improved using CRFs. In the process of
    building these models, we covered the use of the TensorFlow DataSet API. We also
    built advanced models for CRF mode by building a custom Keras layer, a custom
    model, custom loss function, and a custom training loop.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已经涵盖了相当多的内容。我们解释了命名实体识别（NER）及其在行业中的重要性。要构建NER模型，需要BiLSTM和CRF。通过使用我们在上一章中构建情感分类模型时学到的BiLSTM，我们构建了一个可以标注命名实体的模型的第一个版本。这个模型后来通过使用CRF进行了改进。在构建这些模型的过程中，我们介绍了TensorFlow
    DataSet API的使用。我们还通过构建自定义Keras层、自定义模型、自定义损失函数和自定义训练循环，构建了用于CRF模式的高级模型。
- en: Thus far, we have trained embeddings for tokens in the models. A considerable
    amount of lift can be achieved by using pre-trained embeddings. In the next chapter,
    we'll focus on the concept of transfer learning and the use of pre-trained embeddings
    like BERT.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经为模型中的标记训练了词嵌入。通过使用预训练的词嵌入，可以获得相当大的提升。在下一章，我们将重点讨论迁移学习的概念以及使用像BERT这样的预训练嵌入。
