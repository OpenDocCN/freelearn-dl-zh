- en: '*Chapter 2*: Implementing Value-Based, Policy-Based, and Actor-Critic Deep
    RL Algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第2章*：实现基于值、基于策略和演员-评论员深度RL算法'
- en: This chapter provides a practical approach to building value-based, policy-based,
    and actor-critic algorithm-based **reinforcement learning** (**RL**) agents. It
    includes recipes for implementing value iteration-based learning agents and breaks
    down the implementation details of several foundational algorithms in RL into
    simple steps. The policy gradient-based agent and the actor-critic agent make
    use of the latest major version of **TensorFlow 2.x** to define the neural network
    policies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了一种实际的方法，用于构建基于值、基于策略和基于演员-评论员算法的**强化学习**（**RL**）智能体。它包括实现基于值迭代的学习智能体的食谱，并将RL中几个基础算法的实现细节分解为简单的步骤。基于策略梯度的智能体和演员-评论员智能体使用最新版本的**TensorFlow
    2.x**来定义神经网络策略。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下食谱：
- en: Building stochastic environments for training RL agents
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用于训练RL智能体的随机环境
- en: Building value-based (RL) agent algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基于值的（RL）智能体算法
- en: Implementing temporal difference learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现时序差分学习
- en: Building Monte Carlo prediction and control algorithms for RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为RL构建蒙特卡罗预测和控制算法
- en: Implementing the SARSA algorithm and an RL agent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现SARSA算法和RL智能体
- en: Building a Q-learning agent
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Q-learning智能体
- en: Implementing policy gradients
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现策略梯度
- en: Implementing actor-critic algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现演员-评论员算法
- en: Let's get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in this book has been tested extensively on Ubuntu 18.04 and Ubuntu
    20.04, and should work with later versions of Ubuntu if Python 3.6+ is available.
    With Python 3.6 installed, along with the necessary Python packages listed at
    the beginning of each recipe, the code should run fine on Windows and Mac OS X
    too. It is advised that you create and use a Python virtual environment named
    `tf2rl-cookbook` to install the packages and run the code in this book. Installing
    Miniconda or Anaconda for Python virtual environment management is recommended.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码已在Ubuntu 18.04和Ubuntu 20.04上进行了广泛测试，并应当能在更高版本的Ubuntu上运行，只要安装了Python 3.6+。在安装了Python
    3.6以及每个食谱开头列出的必要Python包后，代码也应该可以在Windows和Mac OS X上运行。建议你创建并使用名为`tf2rl-cookbook`的Python虚拟环境来安装包并运行本书中的代码。推荐安装Miniconda或Anaconda来进行Python虚拟环境管理。
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个章节中每个食谱的完整代码可以在这里找到：[https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook)。
- en: Building stochastic environments for training RL agents
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于训练RL智能体的随机环境
- en: 'To train RL agents for the real world, we need learning environments that are
    stochastic, since real-world problems are stochastic in nature. This recipe will
    walk you through the steps for building a **Maze** learning environment to train
    RL agents. The Maze is a simple, stochastic environment where the world is represented
    as a grid. Each location on the grid can be referred to as a cell. The goal of
    an agent in this environment is to find its way to the goal state. Consider the
    maze shown in the following diagram, where the black cells represent walls:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要为现实世界训练RL智能体，我们需要随机的学习环境，因为现实世界问题本质上是随机的。本食谱将带你一步步构建一个**迷宫**学习环境来训练RL智能体。迷宫是一个简单的随机环境，世界被表示为一个网格。网格上的每个位置可以称为一个单元格。这个环境中智能体的目标是找到通往目标状态的道路。考虑下图中的迷宫，其中黑色单元格表示墙壁：
- en: '![Figure 2.1 – The Maze environment ](img/B15074_02_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 迷宫环境](img/B15074_02_001.jpg)'
- en: Figure 2.1 – The Maze environment
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 迷宫环境
- en: The agent's location is initialized to be at the top-left cell in the Maze.
    The agent needs to find its way around the grid to reach the goal located at the
    top-right cell in the Maze, collecting a maximum number of coins along the way
    while avoiding walls. The location of the goal, coins, walls, and the agent's
    starting location can be modified in the environment's code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的位置初始化为位于迷宫的左上角单元格。智能体需要绕过网格，找到通往迷宫右上角目标单元格的路径，在此过程中收集最多数量的金币，同时避免碰到墙壁。目标位置、金币、墙壁和智能体的起始位置可以在环境代码中修改。
- en: 'The four-dimensional discrete actions that are supported in this environment
    are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该环境中支持的四维离散动作如下：
- en: '*0*: Move up'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*0*: 向上移动'
- en: '*1*: Move down'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1*: 向下移动'
- en: '*2*: Move left'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*2*: 向左移动'
- en: '*3*: Move right'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*3*: 向右移动'
- en: The reward is based on the number of coins that are collected by the agent before
    they reach the goal state. Because the environment is stochastic, the action that's
    taken by the environment has a slight (0.1) probability of "slipping" wherein
    the actual action that's executed will be altered stochastically. The slip action
    will be the clockwise directional action (LEFT -> UP, UP -> RIGHT, and so on).
    For example, with `slip_probability=0.2`, there is a 0.2 probability that a RIGHT
    action may result in DOWN.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励基于智能体在到达目标状态之前收集的硬币数量。由于环境具有随机性，环境执行的动作有0.1的概率发生“滑动”，即实际执行的动作会随机发生变化。滑动动作将是顺时针方向的动作（左
    -> 上，上 -> 右，依此类推）。例如，当`slip_probability=0.2`时，右移动作有0.2的概率会变成下移。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此任务，您需要激活`tf2rl-cookbook` Python/conda虚拟环境，并运行`pip install -r requirements.txt`。如果以下导入语句运行没有问题，那么您可以开始了：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we can begin.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始了。
- en: How to do it…
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: The learning environment is a simulator that provides observations for the RL
    agent, supports a set of actions that the RL agent can perform by executing the
    actions, and returns the resultant/new observation as a result of the agent taking
    the action.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 学习环境是一个模拟器，提供RL智能体的观察，支持RL智能体可以执行的一组动作，并返回执行动作后得到的新观察结果。
- en: 'Follow these steps to implement a stochastic Maze learning environment that
    represents a simple 2D map with cells representing the location of the agent,
    their goal, walls, coins, and empty space:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现一个随机迷宫学习环境，表示一个简单的二维地图，单元格代表智能体的位置、目标、墙壁、硬币和空白区域：
- en: 'We''ll start by defining the MazeEnv class and a map of the Maze environment:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先定义MazeEnv类和迷宫环境的地图：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, place the obstacles/walls on the environment map in the appropriate places:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将障碍物/墙壁放置在环境地图的适当位置：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s define the slip mapping action in clockwise order:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义顺时针方向的滑动映射动作：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let''s define a lookup table in the form of a dictionary to map indices
    to cells in the Maze environment:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个字典形式的查找表，将索引映射到迷宫环境中的单元格：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, let''s define the reverse lookup to find a cell, when given an index:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义反向查找，以便在给定索引时找到一个单元格：
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With that, we have finished initializing the environment!
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到此，我们已经完成了环境的初始化！
- en: 'Now, let''s define a method that will handle the coins and their statuses in
    the Maze, where 0 means that the coin wasn''t collected by the agent and 1 means
    that the coin was collected by the agent:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个方法来处理迷宫中的硬币及其状态，其中0表示硬币未被智能体收集，1表示硬币已被智能体收集：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s define a quick method that will do the inverse operation of finding
    the number status/value of a coin:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个快速方法来执行与查找硬币的数字状态/值相反的操作：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we will define a setter function to set the state of the environment.
    This is useful for algorithms such as value iteration, where each and every state
    needs to be visited in the environment for it to calculate values:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个设置函数来设置环境的状态。对于值迭代等算法，这非常有用，因为每个状态都需要在环境中被访问，以便算法计算值：
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, it is time to implement the `step` method. We''ll begin by implementing
    the `step` method and applying the `slip` action based on `slip_probability`:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候实现`step`方法了。我们将首先实现`step`方法，并根据`slip_probability`应用滑动动作：
- en: '[PRE9]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Continuing with our implementation of the `step` function, we''ll update the
    state of the maze based on the action that''s taken:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续实现`step`函数时，我们将根据执行的动作更新迷宫的状态：
- en: '[PRE10]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we will determine whether the agent has reached the goal:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将判断智能体是否已达到目标：
- en: '[PRE11]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we''ll handle cases when the action results in hitting an obstacle/wall:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将处理当动作导致碰到障碍物/墙壁的情况：
- en: '[PRE12]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The last case you need to handle is seeing whether the action leads to collecting
    a coin:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要处理的最后一个情况是判断动作是否导致收集硬币：
- en: '[PRE13]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To visualize the state of the Gridworld in a human-friendly manner, let''s
    implement a render function that will print out a text version of the current
    state of the Maze environment:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了以一种对人类友好的方式可视化Gridworld的状态，让我们实现一个渲染函数，该函数将打印出迷宫环境当前状态的文本版本：
- en: '[PRE14]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To test whether the environment is working as expected, let''s add a `__main__`
    function that gets executed if the environment script is run directly:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试环境是否按预期工作， let’s 添加一个`__main__`函数，当环境脚本直接运行时会执行：
- en: '[PRE15]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With that, we''re all set! The Maze environment is ready and we can quickly
    test it by running the script (`python envs/maze.py`). An output similar to the
    following will be displayed:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 至此，我们已经准备好了！迷宫环境已经准备好，我们可以通过运行脚本（`python envs/maze.py`）快速测试它。将显示类似于以下的输出：
- en: '![Figure 2.2 – Textual representation of the Maze environment highlighting
    and underlining the agent''s current state ](img/B15074_02_002.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 迷宫环境的文本表示，突出显示并下划线当前状态](img/B15074_02_002.jpg)'
- en: Figure 2.2 – Textual representation of the Maze environment highlighting and
    underlining the agent's current state
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 迷宫环境的文本表示，突出显示并下划线当前状态
- en: Let's see how it works.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的。
- en: How it works…
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Our `map`, as defined in *step 1* in the *How to do it…* section, represents
    the state of the learning environment. The Maze environment defines the observation
    space, the action space, and the rewarding mechanism for implementing a `env.render()`
    method converts the environment's internal grid representation into a simple text/string
    grid and prints it for easy visual understanding.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`map`，如*如何做…*部分中的*步骤 1*所定义，表示学习环境的状态。迷宫环境定义了观察空间、动作空间和奖励机制，实现`env.render()`方法将环境的内部网格表示转换为简单的文本/字符串网格，并打印出来以便于可视化理解。
- en: Building value-based reinforcement learning agent algorithms
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基于价值的强化学习代理算法
- en: Value-based reinforcement learning works by learning the state-value function
    or the action-value function in a given environment. This recipe will show you
    how to create and update the value function for the Maze environment to obtain
    an optimal policy. Learning value functions, especially in model-free RL problems
    where a model of the environment is not available, can prove to be quite effective,
    especially for RL problems with low-dimensional state space.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的强化学习通过学习给定环境中的状态值函数或动作值函数来工作。此配方将向您展示如何为迷宫环境创建和更新价值函数，以获得最优策略。在模型无法使用的无模型强化学习问题中，学习价值函数，尤其是在低维状态空间的强化学习问题中，可能非常有效。
- en: 'Upon completing this recipe, you will have an algorithm that can generate the
    following optimal action sequence based on value functions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此配方后，您将拥有一个算法，可以根据价值函数生成以下最优动作序列：
- en: '![Figure 2.3 – Optimal action sequence generated by a value-based RL algorithm
    with state values represented through a jet color map ](img/B15074_02_003.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 基于价值的强化学习算法生成的最优动作序列，状态值通过喷气色彩图表示](img/B15074_02_003.jpg)'
- en: Figure 2.3 – Optimal action sequence generated by a value-based RL algorithm
    with state values represented through a jet color map
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 基于价值的强化学习算法生成的最优动作序列，状态值通过喷气色彩图表示
- en: Let's get started.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install numpy gym`. If the following import statement
    runs without issues, you are ready to get started:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此配方，您需要激活`tf2rl-cookbook` Python/conda 虚拟环境并运行`pip install numpy gym`。如果以下导入语句运行没有问题，您就可以开始了：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, we can begin.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始了。
- en: How to do it…
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Let's implement a value function learning algorithm based on value iteration.
    We will use the Maze environment to implement and analyze the value iteration
    algorithm.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个基于价值迭代的价值函数学习算法。我们将使用迷宫环境来实现并分析价值迭代算法。
- en: 'Follow these steps to implement this recipe:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实施此配方：
- en: 'Import the Maze learning environment from `envs.maze`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`envs.maze`导入迷宫学习环境：
- en: '[PRE17]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create an instance of `MazeEnv` and print the observation space and action
    space:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`MazeEnv`实例并打印观察空间和动作空间：
- en: '[PRE18]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s define the state dimension in order to initialize `state-values`, `state-action
    values`, and our policy:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义状态维度，以便初始化`state-values`、`state-action values`和我们的策略：
- en: '[PRE19]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we are ready to implement a function that can calculate the state/action
    value when given a state in the environment and an action. We will begin by declaring
    the `calculate_values` function; we''ll complete the implementation in the following
    steps:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备实现一个函数，当给定环境中的状态和一个动作时，能够计算状态/动作值。我们将从声明`calculate_values`函数开始；我们将在接下来的步骤中完成实现：
- en: '[PRE20]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As the next step, we will generate `slip_action`, which is a stochastic action
    based on the stochasticity of the learning environment:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，我们将生成`slip_action`，这是一个基于学习环境随机性的随机动作：
- en: '[PRE21]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When calculating the values of a given state-action pair, it is important to
    be able to set the state of the environment before executing an action to observe
    the reward/result. The Maze environment provides a convenient `set_state` method
    for setting the current state of the environment. Let''s make use of it and step
    through the environment with the desired (input) action:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算给定状态-动作对的值时，能够在执行动作前设置环境状态，以便观察奖励/结果是很重要的。迷宫环境提供了一个方便的`set_state`方法来设置当前的环境状态。让我们利用它，按所需的（输入）动作一步步执行环境：
- en: '[PRE22]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We need a list of transitions in the environment to be able to calculate the
    rewards, as per the Bellman equations. Let''s create a `transitions` list and
    append the newly obtained environment transition information:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要一个环境中的转换列表，以便根据贝尔曼方程计算奖励。让我们创建一个`transitions`列表，并附加新获得的环境转换信息：
- en: '[PRE23]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s obtain another transition using the state and the action, this time
    without stochasticity. We can do this by not using `slip_action` and setting `slip=False`
    while stepping through the Maze environment:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过状态和动作获取另一个转换，这一次不考虑随机性。我们可以通过不使用`slip_action`并将`slip=False`来在迷宫环境中执行：
- en: '[PRE24]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'There is only one more step needed to complete the `calculate_values` function,
    which is to calculate the values:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需再执行一步，即可完成`calculate_values`函数，那就是计算值：
- en: '[PRE25]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can start implementing the state/action value learning. We will begin
    by defining the `max_iteration` hyperparameters:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始实现状态/动作值学习了。我们将从定义`max_iteration`超参数开始：
- en: '[PRE26]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s implement the `state-value` function learning loop using value iteration:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用价值迭代来实现`state-value`函数学习循环：
- en: '[PRE27]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now that we have the `state-value` function learning loop implemented, let''s
    move on and implement the `action-value` function:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经实现了`state-value`函数学习循环，接下来让我们继续实现`action-value`函数：
- en: '[PRE28]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With the `action-value` function computed, we are only one step away from obtaining
    the optimal policy. Let's go get it!
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算出`action-value`函数后，我们离获得最优策略只差一步了。让我们去实现它吧！
- en: '[PRE29]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can print the Q values (the `state-action` values) and the policy using
    the following lines of code:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码行打印Q值（`state-action` 值）和策略：
- en: '[PRE30]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As a final step, let''s visualize the value function''s learning and policy
    updates:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步，让我们可视化价值函数的学习和策略更新：
- en: '[PRE31]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code will generate the following diagrams, which show the progress
    of the value function while it''s learning and the policy updates:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将生成以下图示，显示在学习过程中价值函数的进展以及策略更新：
- en: '![Figure 2.4 – Progression (from left to right and from top to bottom) of the  learned
    value function and the policy ](img/B15074_02_004.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 学习到的价值函数和策略的进展（从左到右，从上到下）](img/B15074_02_004.jpg)'
- en: Figure 2.4 – Progression (from left to right and from top to bottom) of the
    learned value function and the policy
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 学习到的价值函数和策略的进展（从左到右，从上到下）
- en: How it works…
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The Maze environment contains a start cell, a goal cell, and a few cells containing
    coins, walls, and open spaces. There are 112 distinct states in the Maze environment
    due to the varying nature of the cells with coins. For illustration purposes,
    when an agent collects one of the coins, the environment is in a completely different
    state compared to the state when the agent collects a different coin. This is
    because the location of the coin also matters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 迷宫环境包含一个起始单元、一个目标单元，以及一些包含金币、墙壁和空地的单元。由于金币单元的不同性质，迷宫环境中有112个不同的状态。为了说明，当代理收集其中一个金币时，环境与代理收集另一个金币时的状态完全不同。这是因为金币的位置也很重要。
- en: '`q_values` (state-action values) is a big matrix of size 112 x 4, so it will
    print a long list of values. We will not show these here. The other two print
    statements in *step 14* should produce an output similar to the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`q_values`（状态-动作值）是一个112 x 4的大矩阵，因此它会打印出一长串值。我们在这里不展示这些。*第14步*中的其他两个打印语句应产生类似以下的输出：'
- en: '![Figure 2.5 – Textual representation of the optimal action sequence ](img/B15074_02_005.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 最优动作序列的文本表示](img/B15074_02_005.jpg)'
- en: Figure 2.5 – Textual representation of the optimal action sequence
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 最优动作序列的文本表示
- en: Value iteration-based value function learning follows Bellman equations, and
    the optimal policy is obtained from the Q-value function by simply choosing the
    action with the highest Q/action-value.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值迭代的值函数学习遵循贝尔曼方程，最优策略是通过从Q值函数中选择Q/动作值最高的动作来获得的。
- en: In *Figure 2.4*, the value function is represented using a jet color map, while
    the policy is represented using the green-arrows. Initially, the values for the
    states are almost even. As the learning progress, states with coins get more value
    than states without coins, and the state that leads to the goal gets a very high
    value that's only slightly less than the goal state itself. The black cells in
    the maze represents the walls. The arrows represent the directional action that
    the policy is prescribing from the given cell in the maze. As the learning converges,
    as shown in the bottom-right diagram, the policy is optimal, leading the agent
    to the goal after it's collected every coin.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图2.4*中，值函数使用喷射色图表示，而策略则通过绿色箭头表示。最初，状态的值几乎相等。随着学习进展，拥有硬币的状态比没有硬币的状态更有价值，指向目标的状态获得了一个非常高的值，只有略低于目标状态本身。迷宫中的黑色单元表示墙壁。箭头表示策略在给定迷宫单元格中的指导行动。随着学习的收敛，如右下方的图所示，策略达到了最优，指引代理在收集完所有硬币后到达目标。
- en: Important note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The color versions of the diagrams in this book are available to download. You
    can find the link to these diagrams in the *Preface* of this book.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的彩色版本图表可供下载。你可以在本书的*前言*部分找到这些图表的链接。
- en: Implementing temporal difference learning
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现时间差学习
- en: 'This recipe will walk you through how to implement the **temporal difference**
    (**TD**) learning algorithm. TD algorithms allow us to incrementally learn from
    incomplete episodes of agent experiences, which means they can be used for problems
    that require online learning capabilities. TD algorithms are useful in model-free
    RL settings as they do not depend on a model of the MDP transitions or rewards.
    To visually understand the learning progression of the TD algorithm, this recipe
    will also show you how to implement the GridworldV2 learning environment, which
    looks as follows when rendered:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将引导你实现**时间差（TD）**学习算法。TD算法使我们能够从代理体验的不完整回合中逐步学习，这意味着它们可以用于需要在线学习能力的问题。TD算法在无模型强化学习（RL）环境中非常有用，因为它们不依赖于MDP转换或奖励的模型。为了更直观地理解TD算法的学习进展，本食谱还将展示如何实现GridworldV2学习环境，该环境在渲染时如下所示：
- en: '![Figure 2.6 – The GridworldV2 learning environment 2D rendering with  state
    values and grid cell coordinates ](img/B15074_02_006.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图2.6 – 带有状态值和网格单元坐标的GridworldV2学习环境2D渲染](img/B15074_02_006.jpg)'
- en: Figure 2.6 – The GridworldV2 learning environment 2D rendering with state values
    and grid cell coordinates
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 带有状态值和网格单元坐标的GridworldV2学习环境2D渲染
- en: Getting ready
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install numpy gym`. If the following import statements
    run without issues, you are ready to get started:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本食谱，你需要激活`tf2rl-cookbook` Python/conda虚拟环境，并运行`pip install numpy gym`。如果以下导入语句没有问题，则可以开始了：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we can begin.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始了。
- en: How to do it…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'This recipe will contain two components that we will put together at the end.
    The first component is the GridworldV2 implementation, while the second component
    is the TD learning algorithm''s implementation. Let''s get started:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将包含两个组件，最后我们将把它们结合起来。第一个组件是GridworldV2的实现，第二个组件是TD学习算法的实现。让我们开始吧：
- en: 'We will start by implementing GridworldV2 and then by defining the `GridworldV2Eng`
    class:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先实现GridworldV2，然后定义`GridworldV2Eng`类：
- en: '[PRE33]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In this step, you will continue implementing the `__init__` method and define
    the necessary values that define the size of the Gridworld, the goal location,
    the wall location, and the location of the bomb, among other things:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，你将继续实现`__init__`方法，并定义必要的值，这些值将定义Gridworld的大小、目标位置、墙壁位置以及炸弹的位置等：
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we can move on to the definition of the `reset()` method, which will be
    called at the start of every episode, including the first one:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以继续定义`reset()`方法，该方法将在每个回合开始时调用，包括第一个回合：
- en: '[PRE35]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s implement a `get_next_state` method so that we can conveniently obtain
    the next state:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现一个`get_next_state`方法，这样我们就可以方便地获取下一个状态：
- en: '[PRE36]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'With that, we are ready to implement the main `step` method of the `GridworldV2`
    environment:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们就可以准备实现`GridworldV2`环境的主要`step`方法：
- en: '[PRE37]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we can move on and implement the temporal difference learning algorithm.
    Let''s begin by initializing the state values of the grid using a 2D `numpy` array
    and then set the value of the goal location and the bomb state:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以继续实现时序差分学习算法。我们首先通过初始化一个二维`numpy`数组来设置网格的状态值，然后设置目标位置和炸弹状态的值：
- en: '[PRE38]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, let''s define the discount factor, `gamma`, the learning rate, `alpha`,
    and initialize `done` to `False`:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义折扣因子`gamma`、学习率`alpha`，并将`done`初始化为`False`：
- en: '[PRE39]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can now define the main outer loop so that it runs `max_episodes` times,
    resetting the state of the environment to its initial state at the start of every
    episode:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义主要的外部循环，使其运行`max_episodes`次，在每个回合开始时重置环境的状态到初始状态：
- en: '[PRE40]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, it''s time to implement the inner loop with the temporal difference learning
    update one-liner:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候实现带有时序差分学习更新的内部循环一行代码了：
- en: '[PRE41]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once the learning has converged, we want to be able to visualize the state
    values for each state in the GridwordV2 environment. To do that, we can make use
    of the `visualize_grid_state_values` function from `value_function_utils`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦学习已经收敛，我们希望能够可视化GridwordV2环境中每个状态的状态值。为此，我们可以利用`value_function_utils`中的`visualize_grid_state_values`函数：
- en: '[PRE42]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We are now ready to run the `temporal_difference_learning` function from our
    main function:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备从主函数中运行`temporal_difference_learning`函数：
- en: '[PRE43]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The preceding code will take a few seconds to run temporal difference learning
    for `max_episodes`. It will then produce a diagram similar to the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码将花费几秒钟时间进行`max_episodes`的时序差分学习。然后，它将生成一个类似于以下的图示：
- en: '![Figure 2.7 – Rendering of the GridworldV2 environment, with the grid cell
    coordinates and state values colored according to the scale shown on the right  ](img/B15074_02_007.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 渲染GridworldV2环境，网格单元的坐标和状态值根据右侧显示的尺度进行着色](img/B15074_02_007.jpg)'
- en: Figure 2.7 – Rendering of the GridworldV2 environment, with the grid cell coordinates
    and state values colored according to the scale shown on the right
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 渲染GridworldV2环境，网格单元的坐标和状态值根据右侧显示的尺度进行着色
- en: How it works…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Based on our environment''s implementation, you may have noticed that `goal_state`
    is located at `(0, 3)` and that `bomb_state` is located at `(1, 3)`. This is based
    on the coordinates, colors, and values of the grid cells:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们环境的实现，你可能已经注意到`goal_state`位于`(0, 3)`，而`bomb_state`位于`(1, 3)`。这基于网格单元的坐标、颜色和数值：
- en: '![Figure 2.8 – Rendering of the GridWorldV2 environment with initial state
    values ](img/B15074_02_008.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 渲染GridWorldV2环境，带有初始状态值](img/B15074_02_008.jpg)'
- en: Figure 2.8 – Rendering of the GridWorldV2 environment with initial state values
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 渲染GridWorldV2环境，带有初始状态值
- en: 'The state is linearized and is represented using a single integer indicating
    each of the 12 distinct states in the GridWorldV2 environment. The following diagram
    shows a linearized rendering of the grid states to give you a better understanding
    of the state encoding:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 状态被线性化，并使用一个整数表示GridWorldV2环境中每个12个不同状态。以下图示展示了网格状态的线性化渲染，帮助你更好地理解状态编码：
- en: '![Figure 2.9 – Linearized representation of the states ](img/B15074_02_009.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – 状态的线性化表示](img/B15074_02_009.jpg)'
- en: Figure 2.9 – Linearized representation of the states
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 状态的线性化表示
- en: Now that we have seen how to implement temporal difference learning, let's move
    on to building Monte Carlo algorithms.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何实现时序差分学习，接下来让我们开始构建蒙特卡洛算法。
- en: Building Monte Carlo prediction and control algorithms for RL
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建蒙特卡洛预测和控制算法
- en: 'This recipe provides the ingredients for building a **Monte Carlo** prediction
    and control algorithm so that you can build your RL agents. Similar to the temporal
    difference learning algorithm, Monte Carlo learning methods can be used to learn
    both the state and the action value functions. Monte Carlo methods have zero bias
    since they learn from complete episodes with real experience, without approximate
    predictions. These methods are suitable for applications that require good convergence
    properties. The following diagram illustrates the value that''s learned by the
    Monte Carlo method for the GridworldV2 environment:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这份食谱提供了构建 **蒙特卡洛** 预测与控制算法的基本材料，帮助你构建 RL 智能体。与时序差分学习算法类似，蒙特卡洛学习方法可以用来学习状态和动作值函数。蒙特卡洛方法没有偏差，因为它们从完整的回合中学习真实的经验，而没有近似预测。这些方法适用于需要良好收敛性特征的应用。以下图示展示了蒙特卡洛方法在
    GridworldV2 环境中学习到的值：
- en: '![Figure 2.10 – Monte Carlo prediction of state values (left) and state-action
    values (right) ](img/B15074_02_010.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.10 – 蒙特卡洛预测的状态值（左）和状态-动作值（右）](img/B15074_02_010.jpg)'
- en: Figure 2.10 – Monte Carlo prediction of state values (left) and state-action
    values (right)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 – 蒙特卡洛预测的状态值（左）和状态-动作值（右）
- en: Getting ready
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statement runs without issues, you are ready to get started:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个步骤，你需要激活 `tf2rl-cookbook` Python/conda 虚拟环境，并运行 `pip install -r requirements.txt`。如果以下导入语句能够顺利运行，那么你就可以开始了：
- en: '[PRE44]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now, let's begin.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始。
- en: How to do it…
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: We will start by implementing the `monte_carlo_prediction` algorithm and visualizing
    the learned value function for each state in the `GridworldV2` environment. After
    that, we will implement an `monte_carlo_control` algorithm to construct an agent
    that will act in an RL environment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实现 `monte_carlo_prediction` 算法并可视化 `GridworldV2` 环境中每个状态的学习值函数开始。之后，我们将实现
    `monte_carlo_control` 算法，构建一个在 RL 环境中进行决策的智能体。
- en: 'Follow these steps:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行：
- en: 'Let''s start with the import statements and import the necessary Python modules:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入语句开始，并导入必要的 Python 模块：
- en: '[PRE45]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The next step is to define the `monte_carlo_prediction` function and initialize
    the necessary objects, as shown here:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义 `monte_carlo_prediction` 函数，并初始化所需的对象，如下所示：
- en: '[PRE46]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, let''s implement the outer loop. Outer loops are commonplace in all RL
    agent training code:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现外层循环。外层循环在所有强化学习智能体训练代码中都很常见：
- en: '[PRE47]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next up is the inner loop:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是内层循环：
- en: '[PRE48]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We now have all the information we need to compute the state values of the
    states in the grid:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在拥有计算网格中状态值所需的所有信息：
- en: '[PRE49]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, it''s time to run our Monte Carlo predictor:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候运行我们的蒙特卡洛预测器了：
- en: '[PRE50]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The preceding code should produce a diagram showing the rendering for the GridworldV2
    environment, along with state values:![Figure 2.11 – Rendering of GridworldV2
    with state values learned using the  Monte Carlo prediction algorithm ](img/B15074_02_011.jpg)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码应生成一个图示，显示 GridworldV2 环境的渲染结果，以及状态值：![图 2.11 – 使用蒙特卡洛预测算法学习的 GridworldV2
    状态值渲染图](img/B15074_02_011.jpg)
- en: Figure 2.11 – Rendering of GridworldV2 with state values learned using the Monte
    Carlo prediction algorithm
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.11 – 使用蒙特卡洛预测算法学习的 GridworldV2 状态值渲染图
- en: 'Let''s implement a function for the epsilon-greedy policy:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现一个 epsilon-贪婪策略的函数：
- en: '[PRE51]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, let''s move on to the implementation of the **Monte Carlo Control** algorithm
    for reinforcement learning. We will start by defining the function, along with
    the initial values for the state-action values:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们进入 **蒙特卡洛控制** 算法的实现，用于强化学习。我们将从定义函数并为状态-动作值初始化初始值开始：
- en: '[PRE52]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let''s continue with the implementation of the Monte Carlo Control function
    by initializing the returns for all the possible state and action pairs:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续实现蒙特卡洛控制函数，通过初始化所有可能的状态-动作对的回报值：
- en: '[PRE53]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'As the next step, let''s define the outer loop for each episode and then the
    inner loop for each step in an episode. By doing this, we can collect trajectories
    of experience until the end of an episode:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为下一步，让我们为每个回合定义外层循环，然后为回合中的每个步骤定义内层循环。通过这样做，我们可以收集经验轨迹，直到回合结束：
- en: '[PRE54]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now that we have a full trajectory for an episode in the inner loop, we can
    implement our Monte Carlo Control update to update the state-action values:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个完整的内循环轨迹，我们可以实施蒙特卡洛控制更新，以更新状态-行动值：
- en: '[PRE55]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Once the outer loop completes, we can visualize the state-action values using
    the `visualize_grid_action_values` helper function from the `value_function_utils`
    script:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外部循环完成后，我们可以使用`value_function_utils`脚本中的`visualize_grid_action_values`辅助函数来可视化状态-行动值：
- en: '[PRE56]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, let''s run our `monte_carlo_control` function to learn the `state-action`
    values in the GridworldV2 environment and display the learned values:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们运行`monte_carlo_control`函数来学习GridworldV2环境中的`状态-行动`值，并展示学习到的值：
- en: '[PRE57]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The preceding code will produce a rendering similar to the following:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将生成如下所示的渲染结果：
- en: '![Figure 2.12 – Rendering of the GridworldV2 environment with four action values
    per grid state shown using rectangles ](img/B15074_02_012.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – 显示每个网格状态下的四个行动值的GridworldV2环境渲染图，使用矩形表示](img/B15074_02_012.jpg)'
- en: Figure 2.12 – Rendering of the GridworldV2 environment with four action values
    per grid state shown using rectangles
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 显示每个网格状态下的四个行动值的GridworldV2环境渲染图
- en: That concludes this recipe!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本食谱的全部内容！
- en: How it works…
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Monte Carlo methods for episodic tasks learn directly from experience from
    full sample returns obtained in an episode. The Monte Carlo prediction algorithm
    for estimating the value function based on first visit averaging is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 用于周期性任务的蒙特卡洛方法直接从经验中学习，基于一个完整的样本回报在一个回合中获得的回报。基于第一次访问平均的蒙特卡洛预测算法估算价值函数如下：
- en: '![Figure 2.13 – Monte Carlo prediction algorithm ](img/B15074_02_013.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – 蒙特卡洛预测算法](img/B15074_02_013.jpg)'
- en: Figure 2.13 – Monte Carlo prediction algorithm
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 蒙特卡洛预测算法
- en: Once a series of trajectories have been collected by the agent, we can use the
    transition information in the Monte Carlo Control algorithm to learn the state-action
    value function. This can be used by an agent so that they can act in a given RL
    environment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦智能体收集到一系列轨迹，我们可以使用蒙特卡洛控制算法中的过渡信息来学习状态-行动值函数。这可以被智能体用来在给定的RL环境中进行决策。
- en: 'The Monte Carlo Control algorithm is shown in the following diagram:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了蒙特卡洛控制算法：
- en: '![Figure 2.14 – Monte-Carlo Control algorithm ](img/B15074_02_014.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – 蒙特卡洛控制算法](img/B15074_02_014.jpg)'
- en: Figure 2.14 – Monte-Carlo Control algorithm
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 蒙特卡洛控制算法
- en: The results of the learned state-action value function are shown in *Figure
    2.12*, where each triangle in a grid cell shows the state-action value of taking
    that directional action in that grid state. The base of the triangle lies in the
    direction of the action. For example, the triangle in the top-left corner of *Figure
    2.12* that has a value of 0.44 is the state-action value of taking the LEFT action
    in that grid state.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的状态-行动值函数的结果显示在*图2.12*中，其中网格单元中的每个三角形表示在该网格状态下采取该方向行动的状态-行动值。三角形的底部朝向行动的方向。例如，*图2.12*左上角的三角形，值为0.44，表示在该网格状态下采取“向左”行动的状态-行动值。
- en: Implementing the SARSA algorithm and an RL agent
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现SARSA算法和强化学习智能体
- en: This recipe will show you how to implement the **State-Action-Reward-State-Action**
    (**SARSA**) algorithm, as well as how to develop and train an agent using the
    SARSA algorithm so that it can act in a reinforcement learning environment. The
    SARSA algorithm can be applied to model-free control problems and allows us to
    optimize the value function of an unknown MDP.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将展示如何实现**状态-行动-奖励-状态-行动**（**SARSA**）算法，以及如何使用SARSA算法开发和训练一个智能体，使其能够在强化学习环境中执行任务。SARSA算法可以应用于无模型控制问题，并允许我们优化一个未知MDP的价值函数。
- en: 'Upon completing this recipe, you will have a working RL agent that, when acting
    in the GridworldV2 environment, will generate the following state-action value
    function using the SARSA algorithm:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个食谱后，你将得到一个工作中的强化学习（RL）智能体，当它在GridworldV2环境中运行时，将使用SARSA算法生成如下的状态-行动值函数：
- en: '![Figure 2.15 – Rendering of the GridworldV2 environment – each triangle represents
    the action value of taking that directional action in that grid state ](img/B15074_02_015.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图2.15 – GridworldV2环境的渲染 – 每个三角形表示在该网格状态下采取该方向行动的行动值](img/B15074_02_015.jpg)'
- en: Figure 2.15 – Rendering of the GridworldV2 environment – each triangle represents
    the action value of taking that directional action in that grid state
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 – GridworldV2 环境渲染 – 每个三角形表示在该网格状态下执行该方向动作的动作值
- en: Getting ready
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此实例，你需要激活 `tf2rl-cookbook` Python/conda 虚拟环境，并运行 `pip install -r requirements.txt`。如果以下导入语句没有问题，就可以开始了：
- en: '[PRE58]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Now, let's begin.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。
- en: How to do it…
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: Let's implement the SARSA learning update as a function and make use of an epsilon-greedy
    exploration policy. With these two pieces combined, we will have a complete agent
    to act in a given RL environment. In this recipe, we will train and test the agent
    in the GridworldV2 environment.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 SARSA 学习更新实现为一个函数，并利用 epsilon-greedy 探索策略。将这两部分结合后，我们将拥有一个完整的智能体，用于在给定的强化学习环境中进行动作。在本实例中，我们将在
    GridworldV2 环境中训练并测试智能体。
- en: 'Let''s start our implementation step by step:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们一步一步开始实现： '
- en: 'First, let''s define a function for implementing the SARSA algorithm and initialize
    the state-action values with zeros:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个函数来实现 SARSA 算法，并用零初始化状态-动作值：
- en: '[PRE59]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can now update the values for the goal state and the bomb state based on
    the environment''s configuration:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以根据环境的配置更新目标状态和炸弹状态的值：
- en: '[PRE60]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s define the discount factor, `gamma`, and the learning rate hyperparameter,
    `alpha`. Also, let''s create a convenient alias for `grid_action_values` by calling
    it `q`:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义折扣因子 `gamma` 和学习率超参数 `alpha`。同时，为了方便起见，我们将 `grid_action_values` 创建一个别名，命名为
    `q`：
- en: '[PRE61]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let''s begin to implement the outer loop:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始一步一步地实现外循环：
- en: '[PRE62]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, it''s time to implement the inner loop with the SARSA learning update
    step:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候实现 SARSA 学习更新步骤中的内循环了：
- en: '[PRE63]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As the final step in the `sarsa` function, let''s visualize the state-action
    value function:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为 `sarsa` 函数的最后一步，让我们可视化状态-动作值函数：
- en: '[PRE64]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we will implement the epsilon-greedy policy that the agent will use:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实现智能体将使用的 epsilon-greedy 策略：
- en: '[PRE65]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Finally, we must implement the main function and run the SARSA algorithm:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须实现主函数并运行 SARSA 算法：
- en: '[PRE66]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'When executed, a rendering of the GridworldV2 environment with the state-action
    values will appear, as shown in the following diagram:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，将出现 GridworldV2 环境的渲染图，状态-动作值将如以下图所示：
- en: '![Figure 2.16 – Output of the SARSA algorithm in the GridworldV2 environment
    ](img/B15074_02_016.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.16 – SARSA 算法在 GridworldV2 环境中的输出](img/B15074_02_016.jpg)'
- en: Figure 2.16 – Output of the SARSA algorithm in the GridworldV2 environment
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 – SARSA 算法在 GridworldV2 环境中的输出
- en: How it works…
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'SARSA is an on-policy temporal difference learning-based control algorithm.
    This recipe made uses of the SARSA algorithm to estimate the optimal state-action
    values. The SARSA algorithm can be summarized as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA 是一种基于策略的时序差分学习控制算法。本实例使用 SARSA 算法来估计最优的状态-动作值。SARSA 算法可以总结如下：
- en: '![Figure 2.17 – SARSA algorithm ](img/B15074_02_017.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.17 – SARSA 算法](img/B15074_02_017.jpg)'
- en: Figure 2.17 – SARSA algorithm
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 – SARSA 算法
- en: As you may be able to tell, this is very similar to the Q-learning algorithm.
    The similarities will become clear when we look at the next recipe in this chapter,
    *Building a Q-learning agent*.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这与 Q-learning 算法非常相似。当我们查看本章的下一个实例 *构建 Q-learning 智能体* 时，相似之处将更加清晰。
- en: Building a Q-learning agent
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个 Q-learning 智能体
- en: This recipe will show you how to build a **Q-learning** agent. Q-learning can
    be applied to model-free RL problems. It supports off-policy learning and therefore
    provides a practical solution to problems where available experiences were/are
    collected using some other policy or by some other agent (even humans).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本实例将展示如何构建一个**Q-learning**智能体。Q-learning 可以应用于无模型的强化学习问题。它支持离策略学习，因此为那些使用其他策略或其他智能体（甚至人类）收集的经验提供了实际解决方案。
- en: 'Upon completing this recipe, you will have a working RL agent that, when acting
    in the GridworldV2 environment, will generate the following state-action value
    function using the SARSA algorithm:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本实例后，你将拥有一个有效的强化学习智能体，该智能体在 GridworldV2 环境中执行时，将使用 SARSA 算法生成以下状态-动作值函数：
- en: '![Figure 2.18 – State-action values obtained using the Q-learning algorithm
    ](img/B15074_02_018.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.18 – 使用 Q-learning 算法获得的状态-动作值](img/B15074_02_018.jpg)'
- en: Figure 2.18 – State-action values obtained using the Q-learning algorithm
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18 – 使用 Q 学习算法获得的状态-动作值
- en: Getting ready
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此配方，您需要激活 `tf2rl-cookbook` Python/conda 虚拟环境，并运行 `pip install -r requirements.txt`。如果以下导入语句没有问题，您就可以开始了：
- en: '[PRE67]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Now, let's begin.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始。
- en: How to do it…
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: Let's implement the Q-learning algorithm as a function, as well as an epsilon-greedy
    policy to build our Q-learning agent.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 Q 学习算法作为一个函数实现，同时实现 epsilon-greedy 策略，以构建我们的 Q 学习代理。
- en: 'Let''s start our implementation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实现：
- en: 'First, let''s define a function for implementing the Q-learning algorithm and
    initialize the state-action values with zeros:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个函数来实现 Q 学习算法，并将状态-动作值初始化为零：
- en: '[PRE68]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We can now update the values for the goal state and the bomb state based on
    the environment''s configuration:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以根据环境配置更新目标状态和炸弹状态的值：
- en: '[PRE69]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s define the discount factor, `gamma`, and the learning rate hyperparameter,
    `alpha`. Also, let''s create a convenient alias for `grid_action_values` by calling
    it `q`:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义折扣因子 `gamma` 和学习率超参数 `alpha`。同时，让我们为 `grid_action_values` 创建一个方便的别名，称其为
    `q`：
- en: '[PRE70]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Let''s begin to implement the outer loop:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始实现外部循环：
- en: '[PRE71]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'As the next step, let''s implement the inner loop with the Q-learning update.
    We will also decay the epsilon used in the epsilon-greedy policy:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，让我们实现带有 Q 学习更新的内部循环。同时，我们还将衰减在 epsilon-greedy 策略中使用的 epsilon：
- en: '[PRE72]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'As the final step in the `q_learning` function, let''s visualize the state-action
    value function:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `q_learning` 函数的最后一步，让我们可视化状态-动作值函数：
- en: '[PRE73]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, we will implement the epsilon-greedy policy that the agent will use:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现代理将使用的 epsilon-greedy 策略：
- en: '[PRE74]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, we will implement the main function and run the SARSA algorithm:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将实现主函数并运行 SARSA 算法：
- en: '[PRE75]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'When executed, a rendering of the GridworldV2 environment with the state-action
    values will appear, as shown in the following diagram:'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行时，将显示带有状态-动作值的 GridworldV2 环境渲染，如下图所示：
- en: '![Figure 2.19 – Rendering of the GridworldV2 environment with the action values
    obtained using the Q-learning algorithm ](img/B15074_02_019.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.19 – 使用 Q 学习算法获得的动作值渲染的 GridworldV2 环境](img/B15074_02_019.jpg)'
- en: Figure 2.19 – Rendering of the GridworldV2 environment with the action values
    obtained using the Q-learning algorithm
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19 – 使用 Q 学习算法获得的动作值渲染的 GridworldV2 环境
- en: How it works…
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The Q-learning algorithm involves the Q value update, which can be summarized
    by the following equation:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习算法涉及 Q 值更新，可以通过以下方程式总结：
- en: '![](img/Formula_02_001.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_001.jpg)'
- en: 'Here, we have the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '![](img/Formula_02_002.png) is the value of the Q function for the current
    state, s, and action, a.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_002.png) 是当前状态 s 和动作 a 的 Q 函数值。'
- en: '![](img/Formula_02_003.png) is used for choosing the maximum value from the
    possible next steps.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_003.png) 用于从可能的下一步中选择最大值。'
- en: '![](img/Formula_02_004.png) is the current position of the agent.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_004.png) 是代理的当前位置。'
- en: '![](img/Formula_02_005.png) is the current action.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_005.png) 是当前动作。'
- en: '![](img/Formula_02_006.png) is the learning rate.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_006.png) 是学习率。'
- en: '![](img/Formula_02_007.png) is the reward that is received in the current position.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_007.png) 是在当前状态下获得的奖励。'
- en: '![](img/Formula_02_008.png) is the gamma (reward decay, discount factor).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_008.png) 是 gamma（奖励衰减，折扣因子）。'
- en: '![](img/Formula_02_009.png) is the next state.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_009.png) 是下一个状态。'
- en: '![](img/Formula_02_010.png) is the actions that are available in the next state,
    ![](img/Formula_02_011.png).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_010.png) 是下一个状态下可用的动作，![](img/Formula_02_011.png)。'
- en: As you may now be able to tell, the difference between Q-learning and SARSA
    is only in how the action-value/Q-value of the next state and action pair is calculated.
    In Q-learning, we use ![](img/Formula_02_012.png), the maximum value of the Q-function,
    whereas in the SARSA algorithm, we take the Q-value of the action that was chosen
    in the next state. This may sound subtle, but because the Q-learning algorithm
    infers the value by taking the max over all actions and doesn't just infer based
    on the current behavior policy, it can directly learn the optimal policy. On the
    other hand, the SARSA algorithm learns a near-optimal policy based on the behavior
    policy's exploration parameter (for example, the ε parameter in the ε-greedy policy).
    The SARSA algorithm has a better convergence property than the Q-learning algorithm,
    so it is more suited for cases where learning happens online and or on a real-world
    system, or even if there are real resources (time and/or money) being spent compared
    to training in a simulation or simulated worlds. Q-learning is more suited for
    training an "optimal" agent in simulation or when the resources (like time/money)
    are not too costly.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如你现在可能已经能看出，Q-learning 和 SARSA 之间的区别仅在于如何计算下一状态和动作对的动作值/Q值。在 Q-learning 中，我们使用
    ![](img/Formula_02_012.png)，即 Q 函数的最大值，而在 SARSA 算法中，我们选择的是下一状态中所选择动作的 Q 值。这听起来可能很微妙，但因为
    Q-learning 算法是通过对所有动作的最大值进行推断，而不仅仅是基于当前的行为策略进行推断，它可以直接学习最优策略。另一方面，SARSA 算法基于行为策略的探索参数（例如，ε-greedy
    策略中的 ε 参数）学习近似最优策略。SARSA 算法比 Q-learning 算法具有更好的收敛性，因此更适用于在线学习或现实世界系统中的情况，或者即使有真实资源（时间和/或金钱）被投入，也比在模拟或模拟世界中训练更为合适。而
    Q-learning 更适合在模拟中训练“最优”智能体，或者当资源（如时间/金钱）不那么昂贵时。
- en: Implementing policy gradients
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现策略梯度
- en: '**Policy gradient algorithms** are fundamental to reinforcement learning and
    serve as the basis for several advanced RL algorithms. These algorithms directly
    optimize for the best policy, which can lead to faster learning compared to value-based
    algorithms. Policy gradient algorithms are effective for problems/applications
    with high-dimensional or continuous action spaces. This recipe will show you how
    to implement policy gradient algorithms using TensorFlow 2.0\. Upon completing
    this recipe, you will be able to train an RL agent in any compatible OpenAI Gym
    environment.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略梯度算法**是强化学习的基础，并且是许多高级 RL 算法的基础。这些算法直接优化最佳策略，相较于基于价值的算法，它们可以更快地学习。策略梯度算法对具有高维或连续动作空间的问题/应用有效。本教程将向你展示如何使用
    TensorFlow 2.0 实现策略梯度算法。完成本教程后，你将能够在任何兼容的 OpenAI Gym 环境中训练 RL 智能体。'
- en: Getting ready
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个教程，你需要激活`tf2rl-cookbook` Python/conda 虚拟环境并运行`pip install -r requirements.txt`。如果以下导入语句没有问题，那么你就准备好开始了：
- en: '[PRE76]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Now, let's begin.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。
- en: How to do it…
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: There are three main parts to this recipe. The first one is applying the policy
    function, which is going to be represented using a neural network implemented
    in TensorFlow 2.x. The second part is applying the Agent class' implementation,
    while the final part will be to apply a trainer function, which is used to train
    the policy gradient-based agent in a given RL environment.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程有三个主要部分。第一个部分是应用策略函数，它将通过在 TensorFlow 2.x 中实现的神经网络来表示。第二部分是应用 Agent 类的实现，而最后一部分是应用训练函数，用于在给定的
    RL 环境中训练基于策略梯度的智能体。
- en: 'Let''s start implementing the parts one by one:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步实现各个部分：
- en: 'The first step is to define the `PolicyNet` class. We will define the model
    so that it has three fully connected or **dense** neural network layers:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是定义`PolicyNet`类。我们将定义模型，使其具有三层全连接或**密集**的神经网络层：
- en: '[PRE77]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we will implement the `call` function, which will be called to process
    inputs to the model:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现`call`函数，它将被调用来处理模型的输入：
- en: '[PRE78]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Let''s also define a `process` function that we can call with a batch of observations
    to be processed by the model:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们还定义一个`process`函数，我们可以使用它来处理一批观测数据，并由模型进行处理：
- en: '[PRE79]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'With the policy network defined, we can implement the `Agent` class, which
    utilizes the policy network, and an optimizer for training the model:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义好策略网络后，我们可以实现`Agent`类，它利用该策略网络，并使用优化器来训练模型：
- en: '[PRE80]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now, let''s define a policy helper function that takes an observation as input,
    has it processed by the policy network, and returns the action as the output:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个策略辅助函数，它接受一个观测作为输入，通过策略网络处理后返回动作作为输出：
- en: '[PRE81]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let''s define another helper function to get the action from the agent:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义另一个辅助函数来从代理那里获取动作：
- en: '[PRE82]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, it''s time to define the learning updates for the policy gradient algorithm.
    Let''s initialize the `learn` function with an empty list for discounted rewards:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候定义策略梯度算法的学习更新了。让我们初始化`learn`函数，并为折扣奖励创建一个空列表：
- en: '[PRE83]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'This is the right place to calculate the discounted rewards while using the
    episodic rewards as input:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是计算折扣奖励的正确位置，同时使用回合奖励作为输入：
- en: '[PRE84]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, let''s implement the crucial step of calculating the policy gradient and
    update the parameters of the neural network policy using an optimizer:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现计算策略梯度的关键步骤，并使用优化器更新神经网络策略的参数：
- en: '[PRE85]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let''s implement the loss function that we referred to in the previous step
    to calculate the policy parameter updates:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现前一步中提到的损失函数，以计算策略参数更新：
- en: '[PRE86]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'With the Agent class fully implemented, we can move on to implementing the
    agent training function. Let''s start with the function''s definition:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理类完全实现后，我们可以继续实现代理训练函数。让我们从函数定义开始：
- en: '[PRE87]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, let''s begin with the outer loop implementation of the agent training
    function:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始实现代理训练函数的外部循环：
- en: '[PRE88]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Let''s continue to implement the inner loop to finalize the `train` function:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续实现内部循环，完成 `train` 函数：
- en: '[PRE89]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Finally, we need to implement the main function:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要实现主函数：
- en: '[PRE90]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The preceding code will launch the training process for the agent in the `render=True`)
    and display what the agent is doing in the environment with respect to driving
    the car uphill. Once the agent has been trained for a sufficient number of episodes,
    you will see the agent driving the car all the way up hill, as shown in the following
    diagram:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将启动代理的训练过程（`render=True`），并展示代理在环境中进行的操作，即驾驶汽车向山上行驶。一旦代理经过足够多的训练轮次，你将看到代理成功地将汽车一路开上山顶，如下图所示：
- en: '![Figure 2.20 – Policy gradient agent completing the MountainCar task ](img/B15074_02_020.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.20 – 策略梯度代理完成 MountainCar 任务](img/B15074_02_020.jpg)'
- en: Figure 2.20 – Policy gradient agent completing the MountainCar task
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20 – 策略梯度代理完成 MountainCar 任务
- en: That concludes this recipe!
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本章节的全部内容！
- en: How it works…
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理…
- en: 'We used TensorFlow 2.x''s `MountainCar` RL environment. The policy gradient
    algorithm is shown in the following diagram:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 TensorFlow 2.x 的 `MountainCar` 强化学习环境。策略梯度算法如图所示：
- en: '![Figure 2.21 – Policy gradient algorithm ](img/B15074_02_021.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.21 – 策略梯度算法](img/B15074_02_021.jpg)'
- en: Figure 2.21 – Policy gradient algorithm
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21 – 策略梯度算法
- en: As you train the policy gradient-based agent, you will observe that while the
    agent can learn to drive the car up the mountain, this can take a long time or
    they may get stuck in local minima. This basic version of the policy gradient
    has some limitations. The policy gradient is an on-policy algorithm that can only
    use experiences/trajectories or episode transitions from the same policy that
    is being optimized. The basic version of the policy gradient algorithm does not
    provide a guarantee for monotonic improvements in performance as it can get stuck
    in local minima.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练基于策略梯度的代理时，你会发现，尽管代理能够学习如何将汽车开上山，但这个过程可能非常漫长，或者它们可能会陷入局部最小值。这个基本版本的策略梯度有一些局限性。策略梯度是一种基于策略的算法，只能使用来自同一策略的经验/轨迹或回合转换，这个策略正在被优化。基本版本的策略梯度算法不能保证性能的单调提升，因为它可能会陷入局部最小值。
- en: Implementing actor-critic RL algorithms
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现演员-评论员强化学习算法
- en: '**Actor-critic algorithms** allow us to combine value-based and policy-based
    reinforcement learning – an all-in-one agent. While policy gradient methods directly
    search and optimize the policy in the policy space, leading to smoother learning
    curves and improvement guarantees, they tend to get stuck at the local maxima
    (for a long-term reward optimization objective). Value-based methods do not get
    stuck at local optimum values, but they lack convergence guarantees, and algorithms
    such as Q-learning tend to have high variance and are not very sample-efficient.
    Actor-critic methods combine the good qualities of both value-based and policy
    gradient-based algorithms. Actor-critic methods are also more sample-efficient.
    This recipe will make it easy for you to implement an actor-critic-based RL agent
    using TensorFlow 2.x. Upon completing this recipe, you will be able to train the
    actor-critic agent in any OpenAI Gym-compatible reinforcement learning environment.
    As an example, we will train the agent in the CartPole-V0 environment.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**Actor-critic算法**使我们能够结合基于值的方法和基于策略的方法进行强化学习——一个全能型的智能体。虽然策略梯度方法直接搜索并优化策略空间中的策略，导致更平滑的学习曲线和改进保证，但它们往往会卡在局部最大值（针对长期奖励优化目标）。基于值的方法不会卡在局部最优值，但它们缺乏收敛保证，像Q-learning这样的算法往往有较大的方差，并且样本效率较低。Actor-critic方法结合了基于值的方法和基于策略梯度方法的优点。Actor-critic方法的样本效率也更高。本教程将帮助你轻松实现一个基于actor-critic的强化学习智能体，使用TensorFlow
    2.x。在完成本教程后，你将能够在任何OpenAI Gym兼容的强化学习环境中训练actor-critic智能体。作为示例，我们将在CartPole-V0环境中训练该智能体。'
- en: Getting ready
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个过程，你需要激活`tf2rl-cookbook`的Python/conda虚拟环境，并运行`pip install -r requirements.txt`。如果以下的导入语句没有问题，那么你就可以开始了：
- en: '[PRE91]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Now, let's begin.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始吧。
- en: How to do it…
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: There are three main parts to this recipe. The first is creating the actor-critic
    model, which is going to be represented using a neural network implemented in
    TensorFlow 2.x. The second part is creating the Agent class' implementation, while
    the final part is going to be about creating a trainer function that will train
    the policy gradient-based agent in a given RL environment.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程主要有三个部分。第一部分是创建actor-critic模型，这将通过一个在TensorFlow 2.x中实现的神经网络表示。第二部分是实现Agent类，而最后一部分则是创建一个训练器函数，用于在给定的RL环境中训练基于策略梯度的智能体。
- en: 'Let''s start implementing the parts one by one:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地开始实现各个部分：
- en: 'Let''s begin with our implementation of the `ActorCritic` class:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从实现`ActorCritic`类开始：
- en: '[PRE92]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The final thing we need to do in the `ActorCritic` class is implement the `call`
    function, which performs a forward pass through the neural network model:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`ActorCritic`类中，我们需要做的最后一件事是实现`call`函数，它执行神经网络模型的前向传递：
- en: '[PRE93]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'With the `ActorCritic` class defined, we can move on and implement the `Agent`
    class and initialize an `ActorCritic` model, along with an optimizer to update
    the parameters of the actor-critic model:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了`ActorCritic`类后，我们可以继续实现`Agent`类，并初始化一个`ActorCritic`模型，连同一个优化器，用来更新actor-critic模型的参数：
- en: '[PRE94]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, we must implement the agent''s `get_action` method:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须实现智能体的`get_action`方法：
- en: '[PRE95]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Now, let''s implement a function that will calculate the actor loss based on
    the actor-critic algorithm. This will drive the parameters of the actor-critic
    network and allow the agent to improve:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个函数，根据actor-critic算法计算actor的损失。这将推动actor-critic网络的参数更新，并使智能体不断改进：
- en: '[PRE96]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We are now ready to implement the learning function of the actor-critic agent:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备好实现actor-critic智能体的学习功能了：
- en: '[PRE97]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Now, let''s define the training function for training the agent in a given
    RL environment:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义训练函数，用于在给定的RL环境中训练智能体：
- en: '[PRE98]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The final step is to implement the main function, which will call the trainer
    to train the agent for the specified number of episodes:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是实现主函数，该函数将调用训练器来训练智能体，直到指定的训练轮次完成：
- en: '[PRE99]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Once the agent has been sufficiently trained, you will see that the agent is
    able to balance the pole on the cart pretty well, as shown in the following diagram:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦智能体得到了充分的训练，你将看到它能够很好地保持杆子在小车上平衡，如下图所示：
- en: '![Figure 2.22 – Actor-critic agent solving the CartPole task ](img/B15074_02_022.jpg)'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.22 – Actor-critic智能体解决CartPole任务](img/B15074_02_022.jpg)'
- en: Figure 2.22 – Actor-critic agent solving the CartPole task
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22 – 演员-评论员代理解决 CartPole 任务
- en: How it works…
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we defined a neural network-based actor-critic model using TensorFlow
    2.x's Keras API. In the neural network model, we defined two fully connected or
    dense neural network layers to extract features from the input. This produced
    two outputs corresponding to the output for an actor and an output for the critic.
    The critic's output is a single float value, whereas the actor's output represents
    the logits for each of the allowed actions in a given RL environment.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用 TensorFlow 2.x 的 Keras API 定义了一个基于神经网络的演员-评论员模型。在这个神经网络模型中，我们定义了两层全连接（密集）神经网络层，用于从输入中提取特征。这样产生了两个输出，分别对应演员和评论员的输出。评论员的输出是一个单一的浮动值，而演员的输出则表示给定强化学习环境中每个允许动作的
    logits。
