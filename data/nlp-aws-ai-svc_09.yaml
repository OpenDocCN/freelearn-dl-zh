- en: '*Chapter 7*: Understanding the Voice of Your Customer Analytics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：理解客户声音分析'
- en: In the previous chapters, to see improving customer service in action, we built
    an AI solution that uses the **AWS NLP** service **Amazon Comprehend** to first
    analyze historical customer service records to derive key topics using Amazon
    Comprehend Topic Modeling and train a custom classification model that will predict
    routing topics for call routing using **Amazon Comprehend Custom Classification**.
    Finally, we used **Amazon Comprehend detect sentiment** to understand the emotional
    aspect of the customer feedback.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，为了展示如何提升客户服务，我们构建了一个AI解决方案，利用**AWS NLP**服务**Amazon Comprehend**首先分析历史客户服务记录，使用Amazon
    Comprehend主题建模提取关键话题，并训练一个定制的分类模型，通过**Amazon Comprehend Custom Classification**预测呼叫路由的主题。最后，我们使用**Amazon
    Comprehend detect sentiment**来理解客户反馈的情感方面。
- en: In this chapter, we are going to focus more on the emotional aspect of the customer
    feedback, which could be an Instagrammer, Yelp reviewer, or your aunt posting
    comments about your business on Facebook, and so on and so forth.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论客户反馈的情感方面，客户可能是Instagram用户、Yelp评论员，或者是你阿姨在Facebook上发表评论关于你生意的等等。
- en: Twenty years back, it was extremely tough to find out as soon as possible what
    people felt about your products and get meaningful feedback to improve them. With
    globalization and the invention of social media, nowadays, everyone has social
    media apps and people express their emotions more freely than ever before. We
    all love tweeting about Twitter posts and expressing our opinions in happy, sad,
    angry, and neutral modes. If you are a very popular person such as a movie star
    or a business getting millions of such comments, the challenges become going through
    large volumes of tweets posted by your fans and then quickly finding out whether
    your movie did well or it was a flop. In the case of a business, a company, or
    a start-up, people quickly know by reading comments whether your customer service
    and product support are good or not.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 二十年前，想要尽快了解人们对你产品的感受并获取有意义的反馈以改进产品是非常困难的。随着全球化的进程和社交媒体的发明，今天，人人都有社交媒体应用程序，人们比以往任何时候都更自由地表达情感。我们都喜欢在Twitter上发表推文，表达我们的情感，无论是开心、悲伤、生气还是中立。如果你是一个非常受欢迎的人，比如电影明星，或者是一个有着成千上万评论的商业公司，那么挑战就变成了如何浏览粉丝发布的大量推文，并快速了解你的电影表现如何，是大获成功还是惨败。如果是公司、企业或初创公司，人们通过阅读评论迅速了解你的客户服务和产品支持是否好。
- en: Social media analytics is a really popular and challenging use case. In this
    chapter, we will focus on text analytics use cases. We will talk about some of
    the very common use cases where the power of NLP will be combined with analytics
    to analyze unstructured data from chats, social media comments, emails, or PDFs.
    We will show you how you can quickly set up powerful analytics for social media
    reviews.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体分析是一个非常受欢迎且充满挑战的应用场景。本章我们将专注于文本分析的应用场景。我们将讨论一些非常常见的应用案例，其中NLP的强大功能将与分析技术相结合，分析来自聊天、社交媒体评论、电子邮件或PDF等非结构化数据。我们将展示如何快速为社交媒体评论设置强大的分析功能。
- en: 'We will navigate through the following sections:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下几个部分进行讲解：
- en: Challenges of setting up a text analytics solution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置文本分析解决方案的挑战
- en: Setting up a Yelp review text analytics workflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Yelp评论文本分析工作流
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need access to an AWS account. Before getting started,
    we recommend that you create an AWS account, and if you have not created one,
    please refer to the previous chapter for sign-up details. You can skip signup
    if you already have an existing AWS account and are following instructions from
    past chapters for creating the AWS account. The Python code and sample datasets
    for Amazon Textract examples are at the repository link here: [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2007](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2007).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章你需要有一个AWS账户。在开始之前，我们建议你创建一个AWS账户，如果你还没有账户，请参考前一章的注册细节。如果你已经有AWS账户并且按照前几章的说明进行操作，可以跳过注册步骤。Amazon
    Textract示例的Python代码和示例数据集可以在此存储库链接中找到：[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2007](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2007)。
- en: We will walk you through the setup steps on how you can set up the preceding
    code repository on a Jupyter notebook with the correct IAM permissions in the
    *Setting up to solve the use case* section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 *设置解决方案用例* 部分，逐步带您完成如何在 Jupyter notebook 上设置上述代码库并配置正确的 IAM 权限的步骤。
- en: Check out the following video to see the Code in Action at [https://bit.ly/3mfioWX](https://bit.ly/3mfioWX).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，观看 [https://bit.ly/3mfioWX](https://bit.ly/3mfioWX) 中的代码演示。
- en: Challenges of setting up a text analytics solution
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置文本分析解决方案的挑战
- en: One of the challenges most organizations face is getting business insights from
    unstructured data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数组织面临的挑战之一是从非结构化数据中获取业务洞察。
- en: This data can be in various formats, such as chats, PDF documents, emails, tweets,
    and so on. Since this data does not have a structure, it's really challenging
    for the traditional data analytics tool to perform analytics on it. This is where
    Amazon Textract and Amazon Comprehend can help.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据可以是各种格式，例如聊天记录、PDF 文档、电子邮件、推文等。由于这些数据没有结构，传统的数据分析工具很难对其进行分析。这正是 Amazon Textract
    和 Amazon Comprehend 发挥作用的地方。
- en: Amazon Textract can make this unstructured data structured by extracting text
    and then Amazon Comprehend can extract insights. Once we have the data in text,
    you can perform serverless **Extract, Transform, Load (ETL)** by using **Amazon
    Glue** and convert it into a structured format. Moreover, you can use **Amazon
    Athena** to perform serverless ad hoc SQL analytics on the unstructured text you
    just extracted and transformed using Glue ETL. Amazon Glue can also crawl your
    unstructured data or text extracted from Amazon Textract in Amazon S3 and store
    it in a Hive metadata store.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Textract 可以通过提取文本将这些非结构化数据转化为结构化数据，随后 Amazon Comprehend 可以提取洞察信息。一旦我们将数据转化为文本，就可以使用
    **Amazon Glue** 执行无服务器的 **提取、转换、加载（ETL）**，并将其转换为结构化格式。此外，您可以使用 **Amazon Athena**
    对刚刚使用 Glue ETL 提取并转换的非结构化文本执行无服务器的临时 SQL 分析。Amazon Glue 还可以爬取您从 Amazon Textract
    提取的非结构化数据或文本，并将其存储在 Hive 元数据存储中。
- en: Lastly, you can also analyze and visualize this data using **Amazon QuickSight**
    to gain business insight.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您还可以使用 **Amazon QuickSight** 分析和可视化这些数据，以获得业务洞察。
- en: Amazon QuickSight helps you create quick visualizations and dashboards to be
    shared with your business or integrated into an application for your end users.
    For example, you are using Instagram for business and want to analyze the comments
    posted on your product pictures and create a real-time dashboard to know whether
    people are posting positive or negative comments about them. You can use the components
    mentioned to create a real-time social media analytics dashboard.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon QuickSight 帮助您创建快速的可视化图表和仪表板，供您与业务共享或集成到您的应用程序中，供最终用户使用。例如，您正在使用 Instagram
    做生意，想要分析您产品图片下的评论，并创建一个实时仪表板来了解人们是发布了关于这些评论的积极还是消极意见。您可以使用上述组件创建一个实时社交媒体分析仪表板。
- en: We all love eating out in a restaurant. The challenge is picking the best restaurant
    based just on reviews.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都喜欢在餐馆用餐。挑战是仅仅根据评论来挑选最佳餐馆。
- en: 'We are going to show you how you can analyze Yelp reviews by using the succeeding
    architecture:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向您展示如何使用以下架构分析 Yelp 评论：
- en: '![Figure 7.1 – Social media analytics serverless architecture'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – 社交媒体分析无服务器架构]'
- en: '](img/B17528_07_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_01.jpg)'
- en: Figure 7.1 – Social media analytics serverless architecture
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 社交媒体分析无服务器架构
- en: 'The architecture in *Figure 7.1* walks you through how you take raw data and
    transform it to provide new insights, optimize datasets in your data lake, and
    visualize the serverless results. We will start by doing the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.1* 中的架构向您展示了如何将原始数据转化为新的洞察信息，优化数据湖中的数据集，并可视化无服务器结果。我们将通过以下步骤开始：'
- en: Getting the Yelp review dataset and uploading it to an Amazon S3 bucket using
    the Jupyter notebook.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 Yelp 评论数据集，并使用 Jupyter notebook 上传到 Amazon S3 存储桶中。
- en: Registering the raw data in the AWS Glue Data Catalog by crawling it.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过爬取原始数据将其注册到 AWS Glue 数据目录中。
- en: Performing AWS Glue ETL on this raw data cataloged in the AWS Glue Data Catalog.
    Once registered in the AWS Glue Data Catalog, after crawling the Yelp reviews
    data in AWS S3, this ETL will transform the raw data into Parquet format and enrich
    it with Amazon Comprehend insights.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS Glue 数据目录中执行此原始数据目录的 AWS Glue ETL。一旦在 AWS Glue 数据目录中注册，在爬取 AWS S3 中的 Yelp
    评论数据后，ETL 将把原始数据转换为 Parquet 格式，并用 Amazon Comprehend 的洞察信息进行丰富。
- en: Now we can crawl and catalog this data again in the AWS Glue Data Catalog. Cataloging
    adds the tables and metadata for Amazon Athena for ad hoc SQL analytics.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以在 AWS Glue 数据目录中再次抓取并编目这些数据。编目操作会为 Amazon Athena 添加表格和元数据，以便进行临时的 SQL
    分析。
- en: Lastly, you can quickly create visualizations on this transformed Parquet data
    crawled in Amazon Athena using Amazon QuickSight. Amazon QuickSight integrates
    directly with Amazon Athena as well as Amazon S3 as a data source for visualization.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以使用 Amazon QuickSight 快速创建在 Amazon Athena 中抓取的转换后的 Parquet 数据的可视化。Amazon
    QuickSight 可以直接与 Amazon Athena 和 Amazon S3 集成，作为可视化的数据源。
- en: You can also convert this solution into a real-time streaming solution by using
    Amazon Kinesis Data Firehose, which will call these Yelp reviews and Twitter APIs,
    and directly store near real-time streaming data in Amazon S3, and from there
    you can use AWS Glue ETL and cataloging with Amazon Comprehend to transform and
    enrich with NLP. Then this transformed data can be directly visualized in QuickSight
    and Athena. Moreover, you can use AWS Lambda functions and step functions to set
    up a completely serverless architecture and automate these steps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过使用 Amazon Kinesis Data Firehose 将此解决方案转换为实时流式解决方案，调用这些 Yelp 评论和 Twitter
    API，并将近实时流数据直接存储在 Amazon S3 中，然后您可以使用 AWS Glue ETL 和编目功能与 Amazon Comprehend 结合进行转换和
    NLP 丰富处理。之后，这些转换后的数据可以直接在 QuickSight 和 Athena 中进行可视化。此外，您还可以使用 AWS Lambda 函数和
    Step Functions 来设置完全无服务器架构并自动化这些步骤。
- en: In this section, we covered the challenges of setting up a text analytics workflow
    with unstructured data and proposed architecture. In the next section, we will
    walk you through how you can build this out for a Yelp reviews dataset or any
    social media analytics dataset using a few lines of code through a Jupyter notebook.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了使用非结构化数据设置文本分析工作流的挑战，并提出了架构方案。在下一节中，我们将带您一步步构建 Yelp 评论数据集或任何社交媒体分析数据集的解决方案，只需通过
    Jupyter 笔记本编写几行代码即可完成。
- en: Setting up a Yelp review text analytics workflow
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Yelp 评论文本分析工作流
- en: 'In this section, we will walk you through how you can build this out for a
    Yelp reviews dataset or any social media analytics dataset by following the steps
    using a Jupyter notebook and Python APIs:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何通过遵循使用 Jupyter 笔记本和 Python API 的步骤，为 Yelp 评论数据集或任何社交媒体分析数据集构建解决方案：
- en: Setting up to solve the use case
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置以解决用例
- en: Walking through the solution using a Jupyter notebook
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Jupyter 笔记本走查解决方案
- en: The setup steps will involve the steps to configure **Identity and Access Management
    (IAM)** roles and the walkthrough notebook will walk you through the architecture.
    So, let's get started.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 设置步骤将包括配置 **身份与访问管理（IAM）** 角色，且演示笔记本会引导您完成架构设置。那么，我们开始吧。
- en: Setting up to solve the use case
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置以解决用例
- en: 'If you have not done so in the previous chapters, you will first have to create
    an Amazon SageMaker Jupyter notebook and set up `Chapter 07` folder, and open
    the `chapter07 social media text analytics.ipynb` notebook:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在前面的章节中没有这样做，您将需要先创建一个 Amazon SageMaker Jupyter 笔记本并设置 `Chapter 07` 文件夹，然后打开
    `chapter07 social media text analytics.ipynb` 笔记本：
- en: 'You can refer to the Amazon SageMaker documentation to create a notebook instance:
    [https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html).
    To follow these steps, please sign in to the `Amazon SageMaker` in the search
    window, select it, and navigate to the **Amazon SageMaker** console.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以参考 Amazon SageMaker 文档来创建笔记本实例：[https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)。为了按照这些步骤进行，请在搜索窗口中登录
    `Amazon SageMaker`，选择它，然后导航到 **Amazon SageMaker** 控制台。
- en: Select **Notebook instances** and create a notebook instance by specifying an
    instance type, storage, and an IAM role.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **Notebook 实例** 并通过指定实例类型、存储和 IAM 角色创建笔记本实例。
- en: 'When creating a SageMaker notebook for this setup, you will need IAM access
    to the services that follow:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为此设置创建 SageMaker 笔记本时，您需要对以下服务具有 IAM 访问权限：
- en: AWS Glue to run ETL jobs and crawl the data
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 AWS Glue 运行 ETL 任务并抓取数据
- en: AWS Athena to call Athena APIs through the notebook
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 AWS Athena 通过笔记本调用 Athena API
- en: AWS Comprehend to perform sentiment analysis
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 AWS Comprehend 执行情感分析
- en: AWS QuickSight for visualization
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS QuickSight 用于可视化
- en: AWS S3 access
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS S3 访问
- en: 'Make sure your IAM for the notebook has the following roles:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保您的笔记本 IAM 具有以下角色：
- en: '![Figure 7.2 – Important IAM roles to run the notebook'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 运行笔记本所需的重要 IAM 角色'
- en: '](img/B17528_07_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_02.jpg)'
- en: Figure 7.2 – Important IAM roles to run the notebook
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 运行笔记本的重要 IAM 角色
- en: You can modify an existing notebook role you are using to add these permissions.
    In *Figure 7.2* is the SageMaker IAM role that you used to create your notebook
    instance. You can navigate to that role and click **Attach policies** to make
    sure you have the necessary roles for your notebook to execute the service APIs
    we are going to use.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以修改正在使用的现有笔记本角色，添加这些权限。在*图 7.2*中是你用于创建笔记本实例的 SageMaker IAM 角色。你可以导航到该角色并点击**附加策略**，确保你拥有执行笔记本所需的权限，以调用我们将使用的服务
    API。
- en: Follow through the steps in this notebook that correspond to the next few subheadings
    in this section, by executing one cell at a time. Please do read the descriptions
    provided in each notebook cell.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 按照本笔记本中的步骤执行，逐个单元格地完成，每次执行时请阅读每个单元格中提供的描述。
- en: Additional IAM pre-requisites for invoking AWS Glue from this notebook
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从本笔记本调用 AWS Glue 的附加 IAM 先决条件
- en: In the previous section, we walked you through how to set up the notebook and
    important IAM roles to run this notebook. In this section, we assume that you
    are already in the notebook and we have added instructions in the notebook on
    how you can get the execution role of the notebook and enable it to invoke AWS
    Glue jobs from this notebook.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们引导你如何设置笔记本和重要的 IAM 角色以运行此笔记本。在本部分中，我们假设你已经进入笔记本，并且我们在笔记本中提供了有关如何获取笔记本执行角色并启用其从此笔记本调用
    AWS Glue 作业的说明。
- en: 'Go to the notebook and follow the *Finding out the current execution role of
    the notebook* section by running the following code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 转到笔记本并按照*查找当前执行角色的笔记本*部分运行以下代码：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will get the role associated with this notebook. Now, in the next section,
    we will walk you through how you can add AWS Glue as an additional trusted entity
    to this role.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你将获取与此笔记本关联的角色。接下来，在下一部分中，我们将引导你如何将 AWS Glue 添加为此角色的附加受信实体。
- en: Adding AWS Glue and Amazon Comprehend as an additional trusted entity to this
    role
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 AWS Glue 和 Amazon Comprehend 作为附加受信实体添加到该角色中
- en: 'This step is needed if you want to pass the execution role of this notebook
    while calling Glue APIs as well, without creating an additional role. If you have
    not used AWS Glue before, then this step is mandatory. If you have used AWS Glue
    previously, then you should have an already existing role that can be used to
    invoke Glue APIs. In that case, you can pass that role while calling Glue (later
    in this notebook) and skip this next step:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望在调用 Glue API 时传递此笔记本的执行角色，而无需创建额外的角色，则需要执行此步骤。如果你之前没有使用过 AWS Glue，则此步骤是必须的。如果你之前使用过
    AWS Glue，则应该已经有一个可以用来调用 Glue API 的现有角色。在这种情况下，你可以在调用 Glue 时传递该角色（稍后在本笔记本中），并跳过下一步：
- en: On the IAM dashboard, click on **Roles** on the left-side navigation and search
    for the **r****ole**. Once the role appears, click on **Role** to go to its **Summary**
    page.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 IAM 仪表板中，点击左侧导航栏中的**角色**，搜索**角色**。角色出现后，点击**角色**以进入其**概述**页面。
- en: Click on the **Trust relationships** tab on the **Summary** page to add AWS
    Glue as an additional trusted entity.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**概述**页面中的**信任关系**选项卡，以将 AWS Glue 添加为附加受信实体。
- en: 'Click on **Edit trust relationship** and replace the JSON with this JSON:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**编辑信任关系**，并将 JSON 替换为以下 JSON：
- en: '[PRE1]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once this is complete, click on **Update Trust Policy** and you are done.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，点击**更新信任策略**，即可完成设置。
- en: '![Figure 7.3 – Setting up trust with Glue in an IAM role'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – 在 IAM 角色中设置与 Glue 的信任'
- en: '](img/B17528_07_03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_03.jpg)'
- en: Figure 7.3 – Setting up trust with Glue in an IAM role
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 在 IAM 角色中设置与 Glue 的信任
- en: In this section, we covered how you can set up the Jupyter notebook and the
    necessary IAM permissions to run the Jupyter notebook. In the next section, we
    will walk you through the solution by the notebook setup you did.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们介绍了如何设置 Jupyter 笔记本和运行 Jupyter 笔记本所需的 IAM 权限。在下一部分中，我们将通过你所做的笔记本设置来演练解决方案。
- en: Walking through the solution using Jupyter Notebook
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Jupyter Notebook 演练解决方案
- en: 'In this section, we will walk you through how you can build this out for a
    Yelp review dataset or any social media analytics dataset by following the given
    steps using a Jupyter notebook and Python APIs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将引导你如何通过遵循给定的步骤，使用 Jupyter 笔记本和 Python API 来构建 Yelp 评论数据集或任何社交媒体分析数据集：
- en: Download the review dataset from Yelp Reviews NLP Fast.ai, which we have already
    done for you in the notebook.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Yelp Reviews NLP Fast.ai 下载评论数据集，我们已经在笔记本中为你完成了这一步。
- en: Register the raw dataset as a table with the AWS Glue Data Catalog.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始数据集注册为AWS Glue数据目录中的一个表。
- en: Run PySpark (AWS Glue job) to convert the dataset into Parquet and get the review
    sentiment with Amazon Comprehend.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行PySpark（AWS Glue作业）将数据集转换为Parquet格式，并使用Amazon Comprehend获取评论情感。
- en: Store the transformed results in a newly curated dataset.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转换后的结果存储在一个新创建的数据集中。
- en: Run a serverless query for the optimized dataset with Amazon Athena.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Amazon Athena对优化后的数据集执行无服务器查询。
- en: Provide visual insights of the results with Amazon QuickSight or Bokeh AWS Glue
    Data Catalog.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Amazon QuickSight或Bokeh AWS Glue数据目录提供结果的可视化洞察。
- en: 'Go to this notebook and run the following steps in the code block to set up
    the libraries:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到这个笔记本并运行代码块中的以下步骤来设置库：
- en: '[PRE2]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have imported the necessary libraries for notebook setup and defined the
    database name and table names for the AWS Glue Data Catalog. In the next step,
    we will download the Yelp reviews dataset by running the following code:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经导入了笔记本设置所需的库，并定义了AWS Glue数据目录的数据库名和表名。在下一步中，我们将通过运行以下代码下载Yelp评论数据集：
- en: '[PRE3]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will run the following code to untar or unzip this review dataset:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将运行以下代码来解压或解压这个评论数据集：
- en: '[PRE4]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There are two CSV files in the tarball. One is called `train.csv`, the other
    is `test.csv`. For those interested, the `readme.txt` file describes the dataset
    in more detail.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在tar包中有两个CSV文件，一个叫做`train.csv`，另一个是`test.csv`。有兴趣的人可以查看`readme.txt`文件，文件中对数据集有更详细的描述。
- en: 'We will use Python pandas to read the CSV files and view the dataset. You will
    notice the data contains two unnamed columns for the rating and review. The rating
    is between 1 and 5 and the review is a free-form text field:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Python的pandas来读取CSV文件并查看数据集。你会注意到数据中包含两列未命名的列，分别是评分和评论。评分介于1到5之间，评论则是自由格式的文本字段：
- en: '[PRE5]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You get the following output:![Figure 7.4 – Raw Yelp review data
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将得到如下输出：![图7.4 – 原始Yelp评论数据
- en: '](img/B17528_07_04.jpg)'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_07_04.jpg)'
- en: Figure 7.4 – Raw Yelp review data
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.4 – 原始Yelp评论数据
- en: 'Now, we will upload the file created previously to S3 to be used later by executing
    the following notebook code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将上传之前创建的文件到S3，以便稍后使用，通过执行以下笔记本代码：
- en: '[PRE6]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We downloaded the Yelp dataset and stored it in a raw S3 bucket. In the next
    section, we will create the AWS Glue Data Catalog database.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经下载了Yelp数据集并将其存储在一个原始S3存储桶中。在下一部分中，我们将创建AWS Glue数据目录数据库。
- en: Creating the AWS Glue Catalog database
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建AWS Glue数据目录数据库
- en: 'In this section, we will walk you through how you can define a table and add
    it to the Glue Data Catalog database. Glue crawlers automatically crawl your data
    from Amazon S3 or from any on-premises database as it supports multiple data stores.
    You can also bring your own Hive metadata store and get started with crawlers.
    Once you have created a crawler, you can perform Glue ETL jobs as these jobs use
    these Data Catalog tables as source data and target data. Moreover, the Glue ETL
    job will read and write to the data stores that are specified in the source and
    target Data Catalog tables in Glue crawlers. There is a central Glue Data Catalog
    for each AWS account:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将向你展示如何定义表并将其添加到Glue数据目录数据库中。Glue爬虫会自动从Amazon S3或任何本地数据库中爬取数据，因为它支持多种数据存储。你也可以使用自己的Hive元数据存储并开始使用爬虫。一旦创建了爬虫，你可以执行Glue
    ETL作业，因为这些作业会使用这些数据目录表作为源数据和目标数据。此外，Glue ETL作业将读取和写入Glue爬虫中指定的源数据和目标数据存储。每个AWS账户都有一个中央Glue数据目录：
- en: 'We are going to use the `glue.create_database` API ([https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database))
    to create a Glue database:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`glue.create_database` API（[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database)）来创建一个Glue数据库：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we will create the raw table in Glue ([https://docs.aws.amazon.com/glue/latest/dg/tables-described.html](https://docs.aws.amazon.com/glue/latest/dg/tables-described.html)).
    There is more than one way to create a table in Glue:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将在Glue中创建原始表（[https://docs.aws.amazon.com/glue/latest/dg/tables-described.html](https://docs.aws.amazon.com/glue/latest/dg/tables-described.html)）。在Glue中创建表有不止一种方式：
- en: '*Using an AWS Glue crawler*: We have classifiers that automatically determine
    the schema of the dataset while crawling using a built-in classifier. You can
    also use a custom classifier if your data schema has a complicated nested JSON
    structure.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用AWS Glue爬虫*：我们有分类器可以在爬取时自动确定数据集的架构，使用内置分类器。如果您的数据架构具有复杂的嵌套JSON结构，您还可以使用自定义分类器。'
- en: '*Creating a table manually or using the APIs*: You create a table manually
    in the console or by using an API. You specify the schema to be classified when
    you define the table.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*手动创建表或使用API*：您可以在控制台中手动创建表，也可以通过API创建表。在定义表时，您需要指定要分类的架构。'
- en: 'Note:'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：
- en: 'For more information about creating a table using the AWS Glue console, see
    *Working with Tables on the AWS Glue Console*: [https://docs.aws.amazon.com/glue/latest/dg/console-tables.html](https://docs.aws.amazon.com/glue/latest/dg/console-tables.html).'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欲了解有关使用AWS Glue控制台创建表的更多信息，请参阅*在AWS Glue控制台上处理表*：[https://docs.aws.amazon.com/glue/latest/dg/console-tables.html](https://docs.aws.amazon.com/glue/latest/dg/console-tables.html)。
- en: 'We are using the `glue.create_table` API to create tables:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`glue.create_table` API来创建表：
- en: '[PRE8]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code will create tables in the AWS Glue Data Catalog.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将创建AWS Glue数据目录中的表。
- en: 'Now we will visualize this data using the Amazon Athena `pyAthena` API. To
    see the raw Yelp reviews, we will be installing this Python library for querying
    the data in the Glue Data Catalog with Athena:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用Amazon Athena的`pyAthena` API可视化这些数据。为了查看原始Yelp评论，我们将安装此Python库，以便使用Athena查询Glue数据目录中的数据：
- en: '[PRE9]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code will help us visualize the database and tables in Glue for
    the raw dataset:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将帮助我们在Glue中可视化原始数据集的数据库和表：
- en: '[PRE10]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will see the following table in the output:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将在输出中看到以下表格：
- en: '![Figure 7.5 – Athena output for the raw dataset in Glue Data Catalog](img/B17528_07_05.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – Glue数据目录中原始数据集的Athena输出](img/B17528_07_05.jpg)'
- en: Figure 7.5 – Athena output for the raw dataset in Glue Data Catalog
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – Glue数据目录中原始数据集的Athena输出
- en: So, we covered how you can create a Glue Data Catalog and crawl the raw Yelp
    data we downloaded in Amazon S3\. Then we showed you how you can visualize this
    data in Amazon Athena using `pyAthena` libraries. In the next section, we will
    walk you through how you can transform this data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们介绍了如何创建Glue数据目录，并抓取我们在Amazon S3下载的原始Yelp数据。然后，我们展示了如何使用`pyAthena`库在Amazon
    Athena中可视化这些数据。在下一节中，我们将向您展示如何转换这些数据。
- en: Transforming raw data to provide insights and visualization
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换原始数据以提供见解和可视化
- en: Now we will walk you through how you can transform the raw data using PySpark
    in an AWS Glue job to call Amazon Comprehend APIs to get sentiment analysis on
    the review, convert the data into Parquet, and partition by sentiment. This will
    allow us to optimize analytics queries when viewing data by sentiment and return
    just the values we need, leveraging the columnar format of Parquet.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将向您展示如何使用PySpark在AWS Glue作业中转换原始数据，调用Amazon Comprehend API对评论进行情感分析，将数据转换为Parquet格式，并按情感进行分区。这将使我们在按情感查看数据时优化分析查询，并仅返回我们需要的值，利用Parquet的列式格式。
- en: We covered Comprehend's detect sentiment real-time API in [*Chapter 3*](B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049),
    *Introducing Amazon Comprehend*. In this job, we will use the real-time batch
    detect sentiment APIs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第3章*](B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049)中介绍了Comprehend的实时情感检测API，*《介绍Amazon
    Comprehend》*。在这个作业中，我们将使用实时批处理情感检测API。
- en: 'We will create a PySpark job to add a primary key and run a batch of reviews
    through Amazon Comprehend to get a sentiment analysis of the reviews. The job
    will limit the number of rows it converts, but this code could be modified to
    run the entire dataset:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个PySpark作业来添加主键，并通过Amazon Comprehend批量处理评论，以获取评论的情感分析。该作业将限制转换的行数，但这段代码可以修改为处理整个数据集：
- en: 'In order to run your code in AWS Glue, we will upload the code and dependencies
    directly to S3 and pass those locations while invoking the Glue job. We will write
    the ETL job using Jupyter Notebook''s cell magic `%%writefile`. We are using the
    AWS Glue script next to transform or perform ETL on the Yelp reviews dataset by
    using Glue transform and adding a sentiment column to the DataFrame by analyzing
    sentiment using Amazon Comprehend:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在AWS Glue中运行代码，我们将直接将代码和依赖项上传到S3，并在调用Glue作业时传递这些位置。我们将使用Jupyter Notebook的单元魔法`%%writefile`来编写ETL作业。接下来，我们使用AWS
    Glue脚本，通过Glue转换处理Yelp评论数据集，并通过使用Amazon Comprehend进行情感分析，将情感列添加到DataFrame中：
- en: '[PRE11]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We have imported the necessary APIs and now we will convert the Glue DynamicFrame
    to a Spark DataFrame to read the data from our Glue Data Catalog. We will select
    the review and rating columns from the database and the table we created in the
    Glue Data Catalog:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经导入了必要的 API，现在我们将把 Glue DynamicFrame 转换为 Spark DataFrame，以便从 Glue 数据目录中读取数据。我们将从数据库和我们在
    Glue 数据目录中创建的表中选择评论和评分列：
- en: '[PRE12]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are defining some limits such as how many characters are to be sent for
    batch detect sentiment – `MAX_SENTENCE_LENGTH_IN_CHARS` – `w`hat the batch size
    of reviews sent for getting sentiment should be – `COMPREHEND_BATCH_SIZE` – and
    how many batches are to be sent:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们正在定义一些限制，例如要发送多少个字符进行批量情感分析——`MAX_SENTENCE_LENGTH_IN_CHARS`——批量情感分析中每批评论的大小——`COMPREHEND_BATCH_SIZE`——以及要发送多少批次：
- en: '[PRE13]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Each task handles 5*10 records and we are calling the Comprehend batch detect
    sentiment API to get the sentiment and add that sentiment after the transformation
    with AWS Glue. Creating a function where we are passing the text list and calling
    the batch detect sentiment API was covered in [*Chapter 3*](B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049),
    *Introducing Amazon Comprehend*:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个任务处理 5*10 条记录，我们正在调用 Comprehend 批量情感分析 API 来获取情感，并在经过 AWS Glue 转换后添加该情感。创建一个函数，传递文本列表并调用批量情感分析
    API，已在 [*第 3 章*](B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049) 中介绍，*介绍 Amazon
    Comprehend*：
- en: '[PRE14]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following code will grab a sample set of records with a review size under
    the Comprehend limits:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将获取一组评论大小小于 Comprehend 限制的记录样本：
- en: '[PRE15]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We are using the Glue DataFrame to concatenate the submission ID and body tuples
    into arrays of a similar size and transforming the results:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们正在使用 Glue DataFrame 将提交 ID 和正文元组连接成大小相似的数组，并转换结果：
- en: '[PRE16]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We are converting the transformed DataFrame with sentiments into Parquet format
    and saving it in our transformed Amazon S3 bucket:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们正在将转换后的带有情感的 DataFrame 转换为 Parquet 格式，并将其保存到我们转换后的 Amazon S3 存储桶中：
- en: '[PRE17]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The key point in this code is how easy it is to get access to the AWS Glue
    Data Catalog leveraging the Glue libraries:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的关键点是利用 Glue 库，轻松访问 AWS Glue 数据目录：
- en: '[PRE18]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Write S3 using `glueContext.write_dynamic_frame. from_options` with the following
    options:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `glueContext.write_dynamic_frame.from_options` 写入 S3，并指定以下选项：
- en: 'Partition the data based on columns – `connection_options = {"path": parquet_output_path,
    "partitionKeys": ["sentiment"]}`.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '基于列对数据进行分区——`connection_options = {"path": parquet_output_path, "partitionKeys":
    ["sentiment"]}`。'
- en: Convert data to a columnar format – `format="parquet"`.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将数据转换为列式格式——`format="parquet"`。
- en: 'We will be uploading the `github_etl.py` script to S3 now so that Glue can
    use it to run the PySpark job. You can replace it with your own script if needed.
    If your code has multiple files, you need to zip those files and upload them to
    S3 instead of uploading a single file like is being done here:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将把 `github_etl.py` 脚本上传到 S3，以便 Glue 可以使用它来运行 PySpark 作业。如果需要，您可以将其替换为您自己的脚本。如果您的代码有多个文件，您需要将这些文件压缩成一个
    ZIP 文件并上传到 S3，而不是像这里所做的那样上传单个文件：
- en: '[PRE19]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we''ll be creating a Glue client via Boto3 so that we can invoke the
    `create_job` API of Glue. The `create_job` API will create a job definition that
    can be used to execute your jobs in Glue. The job definition created here is mutable.
    While creating the job, we are also passing the code location as well as the dependencies''
    locations to Glue. The `AllocatedCapacity` parameter controls the hardware resources
    that Glue will use to execute this job. It is measured in units of **DPU**. For
    more information on **DPU**, please see [https://docs.aws.amazon.com/glue/latest/dg/add-job.html](https://docs.aws.amazon.com/glue/latest/dg/add-job.html):'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过 Boto3 创建一个 Glue 客户端，以便调用 `create_job` API。`create_job` API 将创建一个作业定义，该定义可用于在
    Glue 中执行您的作业。这里创建的作业定义是可变的。在创建作业时，我们还将代码位置以及依赖项的位置传递给 Glue。`AllocatedCapacity`
    参数控制 Glue 用于执行此作业的硬件资源。该参数以 **DPU** 单位进行度量。有关 **DPU** 的更多信息，请参见 [https://docs.aws.amazon.com/glue/latest/dg/add-job.html](https://docs.aws.amazon.com/glue/latest/dg/add-job.html)：
- en: '[PRE20]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The aforementioned job will be executed now by calling the `start_job_run`
    API. This API creates an immutable run/execution corresponding to the job definition
    created previously. We will require the `job_run_id` value for the particular
    job execution to check the status. We''ll pass the data and model locations as
    part of the job execution parameters:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述作业现在将通过调用 `start_job_run` API 来执行。此 API 会创建一个不可变的运行/执行，对应于之前创建的作业定义。我们将需要该特定作业执行的
    `job_run_id` 值来检查其状态。我们将把数据和模型的位置作为作业执行参数传递：
- en: '[PRE21]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Note:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：
- en: This job will take approximately 2 minutes to run.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该作业大约需要 2 分钟才能完成。
- en: 'Now we will check the job status to see whether it is `SUCCEEDED`, `FAILED`,
    or `STOPPED`. Once the job has succeeded, we have the transformed data in S3 in
    Parquet format, which we will use to query with Athena and visualize with QuickSight.
    If the job fails, you can go to the AWS Glue console, click on the **Jobs** tab
    on the left, and from the page, click on this particular job and you will be able
    to find the CloudWatch Logs link (the link under **Logs**) for these jobs, which
    can help you to see what exactly went wrong in the job execution:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将检查作业状态，以查看其是否为`SUCCEEDED`、`FAILED`或`STOPPED`。一旦作业成功，我们将在 S3 中以 Parquet
    格式获得转换后的数据，我们将使用 Athena 查询并通过 QuickSight 可视化。如果作业失败，您可以进入 AWS Glue 控制台，点击左侧的**Jobs**选项卡，并从页面中点击该特定作业，您将能够找到这些作业的
    CloudWatch Logs 链接（**Logs**下的链接），这可以帮助您查看作业执行中究竟发生了什么错误：
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the next section, we will walk you through how to use a Glue crawler to discover
    the transformed data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将演示如何使用 Glue 爬虫发现已转换的数据。
- en: Using a Glue crawler to discover the transformed data
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Glue 爬虫发现已转换的数据
- en: 'Most AWS Glue users use a crawler to populate the AWS Glue Data Catalog with
    tables as a primary method. To do this, what you have to do is add a crawler within
    your Data Catalog to traverse your data stores. The output of the crawler consists
    of one or more metadata tables that are defined in your Data Catalog. **ETL**
    jobs that you define in AWS Glue use these metadata tables as sources and targets:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 AWS Glue 用户使用爬虫将 AWS Glue 数据目录中的表填充为主要方法。为此，您需要在数据目录中添加一个爬虫来遍历您的数据存储。爬虫的输出由一个或多个在数据目录中定义的元数据表组成。您在
    AWS Glue 中定义的**ETL** 作业使用这些元数据表作为数据源和目标：
- en: 'A crawler can crawl both file-based and table-based data stores. Crawlers can
    crawl the data from various types of data stores, such as Amazon S3, RDS, Redshift,
    DynamoDB, or on-premises databases:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬虫可以抓取基于文件和基于表的数据存储。爬虫可以从各种类型的数据存储中抓取数据，例如 Amazon S3、RDS、Redshift、DynamoDB 或本地数据库：
- en: '[PRE23]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Start the Glue Crawler**: You can use the Glue crawler to populate the AWS
    Glue Data Catalog with tables. The crawler will automatically crawl your data
    source, which can be an on-premises database or raw CSV files in Amazon S3, and
    create a metadata table in the Glue Data Catalog, as well as inferring the schema.
    Glue ETL jobs use these metadata tables in the Glue Data Catalog as sources and
    targets. You can also bring your existing Hive data catalog to run Glue ETL:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启动 Glue 爬虫**：您可以使用 Glue 爬虫将表格填充到 AWS Glue 数据目录中。爬虫将自动抓取您的数据源，这可以是本地数据库或 Amazon
    S3 中的原始 CSV 文件，并在 Glue 数据目录中创建元数据表，同时推断模式。Glue ETL 作业使用 Glue 数据目录中的这些元数据表作为源和目标。您还可以将现有的
    Hive 数据目录引入并运行 Glue ETL：'
- en: '[PRE24]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Go to the link in the output to visualize your crawler in Amazon Athena:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问输出中的链接以在 Amazon Athena 中可视化您的爬虫：
- en: '![Figure 7.6 – Yelp curated crawler'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – Yelp 策划的爬虫]'
- en: '](img/B17528_07_06.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_06.jpg)'
- en: Figure 7.6 – Yelp curated crawler
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – Yelp 策划的爬虫
- en: '`READY` state, meaning the crawler completed its crawl. You can also look at
    the CloudWatch logs for the crawler for more details:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`READY` 状态，意味着爬虫完成了抓取。您还可以查看爬虫的 CloudWatch 日志以获取更多详情：'
- en: '[PRE25]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Once you get an output of `READY`, move on to the next step. In this section,
    we showed you how you can crawl the transformed Parquet data for Yelp reviews,
    which has sentiment scores from Amazon Comprehend in the AWS Glue Data Catalog.
    In the next section, we will cover how you can visualize this data to gain meaningful
    insights into the voice of customer analytics.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您获得 `READY` 输出，继续下一步。在本节中，我们展示了如何爬取 Yelp 评论的转换后的 Parquet 数据，这些评论包含来自 Amazon
    Comprehend 的情感分数，并存储在 AWS Glue 数据目录中。在下一节中，我们将介绍如何可视化这些数据，以深入分析客户声音的见解。
- en: Viewing the transformed results
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看转换后的结果
- en: 'We will again use the PyAthena library to run queries against the newly created
    dataset with sentiment results and in the Parquet format. In the interest of time,
    we will be using the Bokeh AWS Glue Data Catalog within the notebook to visualize
    the results instead of Amazon QuickSight:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用 PyAthena 库，针对新创建的数据集运行查询，该数据集包含情感分析结果并以 Parquet 格式存储。为了节省时间，我们将使用笔记本中的
    Bokeh AWS Glue 数据目录来可视化结果，而不是使用 Amazon QuickSight：
- en: 'QuickSight is able to use the same Athena queries to visualize the results
    as well as numerous built-in connectors to many data sources:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QuickSight 也能够使用相同的 Athena 查询来可视化结果，并且具有多个内置连接器，可以连接到许多数据源：
- en: '[PRE26]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here''s the output of querying the Glue Data Catalog via Amazon Athena to get
    the rating, review, and sentiment from the Yelp reviews table:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是通过 Amazon Athena 查询 Glue 数据目录，以获取 Yelp 评论表中的评分、评论和情感的输出：
- en: '![Figure 7.7 – Athena query to select rating and sentiment from the Yelp transformed
    table in the Glue Data Catalog'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – 从 Glue 数据目录中的 Yelp 转换表中选择评分和情感的 Athena 查询'
- en: '](img/B17528_07_07.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_07.jpg)'
- en: Figure 7.7 – Athena query to select rating and sentiment from the Yelp transformed
    table in the Glue Data Catalog
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 从 Glue 数据目录中的 Yelp 转换表中选择评分和情感的 Athena 查询
- en: '`groupby` locally. Alternatively, we could have used the built-in SQL and aggregate
    functions in Athena to achieve the same result:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupby` 本地使用。或者，我们也可以使用 Athena 中的内置 SQL 和聚合函数来实现相同的结果：'
- en: '[PRE27]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, you will find the output of the `groupby` sentiment query:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，你将看到按情感分组的 `groupby` 查询输出：
- en: '![Figure 7.8 – Output of the groupby sentiment query'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – 按情感分组查询的输出'
- en: '](img/B17528_07_08.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_08.jpg)'
- en: Figure 7.8 – Output of the groupby sentiment query
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 按情感分组查询的输出
- en: We can see from the preceding output that we have more positive reviews than
    negative ones for the Yelp reviews dataset.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中我们可以看到，Yelp 评论数据集中正面评论的数量多于负面评论。
- en: The Bokeh framework has a number of built-in visualizations. We will use Bokeh
    to visualize reviews by sentiment and rating in subsequent code.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bokeh 框架有许多内置的可视化工具。我们将在随后的代码中使用 Bokeh 来可视化按情感和评分分组的评论。
- en: '*Visualize by rating*: We will now compare what the Comprehend API came up
    with compared to the user rating in the dataset. We are changing `groupby` in
    the DataFrame to change the dataset:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*按评分可视化*：我们现在将比较 Comprehend API 与数据集中的用户评分之间的差异。我们正在更改 DataFrame 中的 `groupby`
    来更改数据集：'
- en: '[PRE28]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will find the output of the query result grouped by rating in the following
    screenshot:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将在下一个屏幕截图中看到按评分分组的查询结果输出：
- en: '![Figure 7.9 – Output of the query result grouped by rating](img/B17528_07_09.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 按评分分组的查询结果输出](img/B17528_07_09.jpg)'
- en: Figure 7.9 – Output of the query result grouped by rating
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 按评分分组的查询结果输出
- en: 'Now, we will show you how you can plot this in Bokeh:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们将向你展示如何在 Bokeh 中绘制这些数据：
- en: '[PRE29]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You will find the bar graph showing the count of users with their ratings.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到显示用户评分计数的条形图。
- en: '![Figure 7.10 – User rating by count'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.10 – 按用户评分的计数'
- en: '](img/B17528_07_10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_07_10.jpg)'
- en: Figure 7.10 – User rating by count
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 按用户评分的计数
- en: We can see that the most popular rating given is 1 and the least popular is
    2\. We covered in this section how you can use simple queries in Amazon Athena
    and visualization tools such as Bokeh to perform quick visualization and SQL analytics
    on the Yelp reviews dataset enriched with the sentiment from Amazon Comprehend.
    We got some cool insights, such as most of the reviews are positive, and also
    the most popular rating given by users is 1\. You can further drill down and get
    very specific insights into particular comments by using simple SQL. This helps
    drive business outcomes very quickly. In the next section, we will walk you through
    how you can easily create a QuickSight dashboard to gain some cool insights for
    your business users.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，给出的最常见评分是 1，最不常见的是 2。本节中我们介绍了如何使用 Amazon Athena 中的简单查询和像 Bokeh 这样的可视化工具，对增强了
    Amazon Comprehend 情感分析的 Yelp 评论数据集进行快速可视化和 SQL 分析。我们得到了很酷的洞察，例如大多数评论都是正面的，用户给出的最常见评分是
    1。你可以通过使用简单的 SQL 进一步深入分析，获得特定评论的具体见解。这有助于迅速推动业务成果。在下一节中，我们将向你展示如何轻松创建一个 QuickSight
    仪表板，为你的业务用户提供一些酷的见解。
- en: Amazon QuickSight visualization
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon QuickSight 可视化
- en: 'In this section, we will walk you through how you can set up or get started
    with Amazon QuickSight and visualize the tables in Athena, which was transformed
    with sentiments using AWS Glue and Comprehend:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向你展示如何设置或开始使用 Amazon QuickSight，并可视化 Athena 中的表格，这些表格通过 AWS Glue 和 Comprehend
    进行了情感分析转换：
- en: 'You can follow along with the `Getting Started` guide for QuickSight at [https://docs.aws.amazon.com/quicksight/latest/user/getting-started.html](https://docs.aws.amazon.com/quicksight/latest/user/getting-started.html)
    to set up your account and then follow the code block to navigate to the QuickSight
    console:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以按照 [https://docs.aws.amazon.com/quicksight/latest/user/getting-started.html](https://docs.aws.amazon.com/quicksight/latest/user/getting-started.html)
    上的 `入门指南` 设置你的账户，然后按照代码块导航到 QuickSight 控制台：
- en: '[PRE30]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Manage S3 Access in QuickSight**: We need to do this mandatory step to make
    sure we do not get an access denied exception while accessing Amazon Athena with
    Amazon QuickSight.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**管理QuickSight中的S3访问**：我们需要完成这个必需的步骤，以确保在使用Amazon QuickSight访问Amazon Athena时不会遇到访问拒绝的异常。'
- en: Go to **Manage QuickSight | Security and permission | Add or remove | In S3\.
    C**lick on **details** | select the bucket you want to query | **update**.![Figure
    7.11 – Manage QuickSight access for the S3 data lake
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入**管理QuickSight | 安全和权限 | 添加或移除 | 在S3中**。点击**详细信息** | 选择您要查询的存储桶 | **更新**。![图
    7.11 – 管理QuickSight对S3数据湖的访问
- en: '](img/B17528_07_11.jpg)'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_07_11.jpg)'
- en: Figure 7.11 – Manage QuickSight access for the S3 data lake
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.11 – 管理QuickSight对S3数据湖的访问
- en: Click on the **Create Dataset** option and you will see the following options:![Figure
    7.12 – Amazon QuickSight setup with the Create a Dataset option
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建数据集**选项，您将看到以下选项：![图 7.12 – Amazon QuickSight设置及创建数据集选项
- en: '](img/B17528_07_12.jpg)'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_07_12.jpg)'
- en: Figure 7.12 – Amazon QuickSight setup with the Create a Dataset option
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.12 – Amazon QuickSight设置及创建数据集选项
- en: Choose **Athena** from the preceding options. Click the **New data set** button
    and select **Athena data source**. Name the data source and choose the Yelp **Glue
    database** and **reviews_parquet** table. Finish the creation by clicking the
    **Create data source** button. QuickSight supports a number of data connectors.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择前面选项中的**Athena**。点击**新建数据集**按钮，并选择**Athena数据源**。命名数据源并选择Yelp的**Glue数据库**和**reviews_parquet**表。点击**创建数据源**按钮完成创建。QuickSight支持多种数据连接器。
- en: In the `yelp_reviews` and click **Create data source**. Also, click on **validated**.![Figure
    7.13 – Creating an Athena data source in QuickSight
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`yelp_reviews`中点击**创建数据源**。同时，点击**已验证**。![图 7.13 – 在QuickSight中创建Athena数据源
- en: '](img/B17528_07_13.jpg)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_07_13.jpg)'
- en: Figure 7.13 – Creating an Athena data source in QuickSight
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.13 – 在QuickSight中创建Athena数据源
- en: Next, you will be selecting the **Yelp** database we created in the Glue Data
    Catalog and the **reviews_parquet** table.![Figure 7.14 – Select the table to
    visualize in QuickSight
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您将选择我们在Glue数据目录中创建的**Yelp**数据库和**reviews_parquet**表。![图 7.14 – 在QuickSight中选择要可视化的表
- en: '](img/B17528_07_14.jpg)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_07_14.jpg)'
- en: Figure 7.14 – Select the table to visualize in QuickSight
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.14 – 在QuickSight中选择要可视化的表
- en: Click on **Save and Visualize**.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存并可视化**。
- en: In this section, we covered how you can create a QuickSight dashboard to analyze
    Yelp reviews. In the next section, we will talk about deleting the resources you
    created during this exercise to avoid incurring charges.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何创建QuickSight仪表板以分析Yelp评论。在下一节中，我们将讨论如何删除您在此过程中创建的资源，以避免产生费用。
- en: Cleaning up
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理工作
- en: In this section, we will walk you through code samples in the notebook to clean
    up AWS resources created while going through the Yelp sentiment analysis solution
    to avoid incurring a cost.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将带您逐步了解如何在笔记本中通过代码示例清理AWS资源，避免在进行Yelp情感分析解决方案时产生费用。
- en: 'Run the cleanup steps to delete the resources you have created for this notebook
    by running the following code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 运行清理步骤，通过执行以下代码删除您为本笔记本创建的资源：
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We deleted the Glue crawlers, Glue ETL jobs, and the database we created using
    the notebook here. Let's move on to the next section to wrap it up.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们删除了Glue爬虫、Glue ETL作业以及使用笔记本创建的数据库。让我们继续下一部分，做个总结。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how you can set up a text analytics solution with
    your existing social media analytics workflow. We gave a specific example of using
    the Yelp reviews dataset and using serverless ETL with NLP using Amazon Comprehend
    to set up a quick visual dashboard using Amazon QuickSight. We also covered ad
    hoc SQL analytics using Amazon Athena to understand the voice or sentiment of
    the majority of your users using some easy SQL queries. This solution can be implemented
    with any social media integration, such as Twitter, Reddit, and Facebook, in batch
    or real-time mode.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何将文本分析解决方案与现有的社交媒体分析工作流相结合。我们给出了一个具体的例子，使用Yelp评论数据集并通过无服务器ETL结合NLP，使用Amazon
    Comprehend快速创建一个可视化仪表板，并使用Amazon QuickSight进行展示。我们还介绍了通过Amazon Athena进行临时SQL分析，利用一些简单的SQL查询来了解大多数用户的声音或情感。这个解决方案可以与任何社交媒体平台集成，如Twitter、Reddit和Facebook，支持批处理或实时模式。
- en: In the case of a real-time setup, you would integrate Kinesis Data Firehose
    to have near real-time streaming tweets or social media feeds in this proposed
    workflow or architecture. Check out the *Further reading* section for a really
    cool **AI-driven social media dashboard** to implement this architecture at scale.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时设置的情况下，你可以集成Kinesis Data Firehose，在这个提议的工作流或架构中实现接近实时的流式推文或社交媒体信息流。查看*进一步阅读*部分，了解一个非常酷的**AI驱动的社交媒体仪表盘**，可以在大规模实施该架构时使用。
- en: Another approach you can take in terms of document automation is to have Amazon
    Textract extract data from your PDFs in the case of RFPs or agreements, and this
    pipeline can be used to gather sentiment quickly paragraph-wise after performing
    Glue ETL on the extracted text.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你在文档自动化方面可以采取的另一种方法是使用Amazon Textract从你的PDF文件中提取数据，特别是在处理RFP（请求建议书）或协议时，之后在提取的文本上执行Glue
    ETL操作后，可以按段落快速收集情感分析。
- en: In the next chapter, we will talk about how you can use AI to automate media
    workflows to reduce costs and monetize content.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何使用AI自动化媒体工作流，以降低成本并实现内容变现。
- en: Further reading
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*How to scale sentiment analysis using Amazon Comprehend, AWS Glue and Amazon
    Athena* by Roy Hasson ([https://aws.amazon.com/blogs/machine-learning/how-to-scale-sentiment-analysis-using-amazon-comprehend-aws-glue-and-amazon-athena/](https://aws.amazon.com/blogs/machine-learning/how-to-scale-sentiment-analysis-using-amazon-comprehend-aws-glue-and-amazon-athena/))'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何使用Amazon Comprehend、AWS Glue和Amazon Athena扩展情感分析* 作者：Roy Hasson ([https://aws.amazon.com/blogs/machine-learning/how-to-scale-sentiment-analysis-using-amazon-comprehend-aws-glue-and-amazon-athena/](https://aws.amazon.com/blogs/machine-learning/how-to-scale-sentiment-analysis-using-amazon-comprehend-aws-glue-and-amazon-athena/))'
- en: '*AI-Driven Social Media Dashboard* ([https://aws.amazon.com/solutions/implementations/ai-driven-social-media-dashboard/](https://aws.amazon.com/solutions/implementations/ai-driven-social-media-dashboard/))'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AI驱动的社交媒体仪表盘* ([https://aws.amazon.com/solutions/implementations/ai-driven-social-media-dashboard/](https://aws.amazon.com/solutions/implementations/ai-driven-social-media-dashboard/))'
- en: '*Harmonize, Query, and Visualize Data from Various Providers using AWS Glue,
    Amazon Athena, and Amazon QuickSight* by Ben Snively ([https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/](https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/))'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用AWS Glue、Amazon Athena和Amazon QuickSight协调、查询和可视化来自不同供应商的数据* 作者：Ben Snively
    ([https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/](https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/))'
