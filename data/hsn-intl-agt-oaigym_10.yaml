- en: Exploring the Learning Algorithm Landscape - DDPG (Actor-Critic), PPO (Policy-Gradient),
    Rainbow (Value-Based)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索学习算法领域 - DDPG（演员-评论家），PPO（策略梯度），Rainbow（基于值的方法）
- en: In the previous chapter, we looked at several promising learning environments
    that you can use to train agents to solve a variety of different tasks. In [Chapter
    7](part0131.html#3STPM0-22c7fc7f93b64d07be225c00ead6ce12), *Creating Custom OpenAI
    Gym Environments – CARLA Driving Simulator*, we also saw how you can create your
    own environments to solve the task or problem that you may be interested in developing
    a solution for, using intelligent and autonomous software agents. That provides
    you with directions on where you can head after finishing in order to explore
    and play around with all the environments, tasks, and problems we discussed in
    this book. Along the same lines, in this chapter, we will discuss several promising
    learning algorithms that serve as future references for your intelligent agent
    development endeavors.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了几种有前景的学习环境，你可以用来训练智能体解决不同的任务。在[第7章](part0131.html#3STPM0-22c7fc7f93b64d07be225c00ead6ce12)，*创建自定义OpenAI
    Gym环境——CARLA驾驶模拟器*，我们还展示了如何创建自己的环境，以解决你可能感兴趣的任务或问题，借助智能和自主软件代理。这为你提供了完成后可以继续深入的方向，以探索和尝试本书中讨论的所有环境、任务和问题。按此思路，在本章中，我们将讨论几种有前景的学习算法，它们将成为你智能代理开发工作中的未来参考。
- en: 'So far in this book, we have gone through the step-by-step process of implementing
    intelligent agents that can learn to improve and solve discrete decision making/control
    problems ([Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12),
    *Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*)
    and continuous action/control problems ([Chapter 8](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12),
    *Implementing an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic
    algorithm*). They served as good starting points in the development of such learning
    agents. Hopefully, the previous chapters gave you a holistic picture of an autonomous
    intelligent software agent/system that can learn to improve given the task or
    problem at hand. We also looked at the overall pipeline with useful utilities
    and routines (such as logging, visualization, parameter management, and so on)
    that help when developing, training, and testing such complex systems. We saw
    two main classes of algorithms: deep Q-learning (and its extensions) and deep
    actor-critic (and their extensions)-based deep reinforcement learning algorithms.
    They are good baseline algorithms and in fact are still referenced in state-of-the
    art research papers in this area. This area of research has been under active
    development in recent years, and several new algorithms have been proposed. Some
    have better sample complexity, which is the number of samples the agent collects
    from the environment before it reaches a certain level of performance. Some other
    algorithms have stable learning characteristics and find optimal policies, given
    enough time, for most problems with little or no tuning. Several new architectures,
    such as IMPALA and Ape-X, have also been introduced and enable highly scaleable
    learning algorithm implementations.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经详细介绍了实现智能代理的逐步过程，这些代理能够学习改进并解决离散决策/控制问题（[第6章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度Q学习实现最优离散控制的智能代理*）和连续动作/控制问题（[第8章](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度演员-评论家算法实现智能自主汽车驾驶代理*）。这些内容为开发这样的学习代理提供了良好的起点。希望之前的章节为你展示了一个能够学习改进并应对手头任务或问题的自主智能软件代理/系统的整体框架。我们还审视了开发、训练和测试这些复杂系统时，有助于的整体管道及其有用的工具和常规（如日志记录、可视化、参数管理等）。我们看到两类主要算法：基于深度Q学习（及其扩展）和深度演员-评论家（及其扩展）的深度强化学习算法。它们是良好的基准算法，实际上，直到今天，它们仍然是该领域最新研究论文中的参考。这一研究领域近年来活跃发展，提出了若干新的算法。一些算法在样本复杂度上表现更好，即智能体在达到某一性能水平之前，从环境中收集的样本数。一些其他算法具有稳定的学习特性，并且在足够的时间内能够找到最优策略，适用于大多数问题且几乎不需要调参。还引入了几种新的架构，如IMPALA和Ape-X，它们使得高可扩展的学习算法实现成为可能。
- en: We will have a quick look at these promising algorithms, their advantages, and
    their potential application types. We will also look at code examples for the
    key components that these algorithms add to what we already know. Sample implementations
    of these algorithms are available in this book's code repository under the `ch10`
    folder at [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速了解这些有前景的算法、它们的优点以及潜在的应用类型。我们还将查看这些算法为我们已有的知识添加的关键组件的代码示例。这些算法的示例实现可在本书的代码库中找到，位于`ch10`文件夹，[https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym)。
- en: Deep Deterministic Policy Gradients
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: '**Deep Deterministic Policy Gradient** (**DDPG**) is an off-policy, model-free,
    actor-critic algorithm and is based on the **Deterministic Policy Gradient** (**DPG**)
    theorem ([proceedings.mlr.press/v32/silver14.pdf](http://proceedings.mlr.press/v32/silver14.pdf)). Unlike
    the deep Q-learning-based methods, actor-critic policy gradient-based methods
    are easily applicable to continuous action spaces, in addition to problems/tasks
    with discrete action spaces.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度确定性策略梯度**（**DDPG**）是一种离策略、无模型、演员-评论家算法，基于**确定性策略梯度**（**DPG**）定理（[proceedings.mlr.press/v32/silver14.pdf](http://proceedings.mlr.press/v32/silver14.pdf)）。与基于深度Q学习的方法不同，基于演员-评论家的策略梯度方法不仅适用于离散动作空间的问题/任务，也能轻松应用于连续动作空间。'
- en: Core concepts
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概念
- en: 'In [Chapter 8](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm*,
    we walked you through the derivation of the policy gradient theorem and reproduced
    the following for bringing in context:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度演员-评论家算法实现智能自动驾驶汽车代理*，我们带你走过了策略梯度定理的推导，并为引入上下文，复现了以下内容：
- en: '![](img/00315.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00315.jpeg)'
- en: 'You may recall that the policy we considered was a stochastic function that
    assigned a probability to each action given the **state** (**s**) and the parameters
    (![](img/00316.jpeg)). In deterministic policy gradients, the stochastic policy
    is replaced by a deterministic policy that prescribes a fixed policy for a given
    state and set of parameters ![](img/00317.jpeg). In short, DPG can be represented
    using the following two equations:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们考虑的策略是一个随机函数，给定**状态**（**s**）和参数（![](img/00316.jpeg)）为每个动作分配一个概率。在确定性策略梯度中，随机策略被一个确定性策略所替代，该策略为给定的状态和参数集指定了一个固定策略！[](img/00317.jpeg)。简而言之，DPG可以通过以下两个方程表示：
- en: 'This is the policy objective function:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是策略目标函数：
- en: '![](img/00318.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00318.jpeg)'
- en: Here, ![](img/00319.jpeg) is the deterministic policy parametrized by ![](img/00320.jpeg),
    r(s,a) is the reward function for taking action *a* in state s, and ![](img/00321.jpeg) is
    the discounted state distribution under the policy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/00319.jpeg)是由![](img/00320.jpeg)参数化的确定性策略，r(s,a)是执行动作*a*在状态s下的奖励函数，而![](img/00321.jpeg)是该策略下的折扣状态分布。
- en: 'The gradient of the deterministic policy objective function is proven (in the
    paper linked before) to be:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性策略目标函数的梯度已在之前链接的论文中证明为：
- en: '![](img/00322.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00322.jpeg)'
- en: We now see the familiar action-value function term, which we typically call
    the critic. DDPG builds on this result and uses a deep neural network to represent
    the action-value function, like we did in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*, along
    with a few other modifications to stabilize the training. Specifically, a Q-target
    network is used (like what we discussed in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12),
    *Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*),
    but now this target network is slowly updated rather than keeping it fixed for
    a few update steps and then updating it. DDPG also uses the experience replay
    buffer and uses a noisy version of ![](img/00323.jpeg), represented using the
    equation ![](img/00324.jpeg), to encourage exploration as the policy. ![](img/00325.jpeg) is
    deterministic.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在看到熟悉的动作-价值函数项，通常称为评论员。DDPG基于这个结果，并使用深度神经网络表示动作-价值函数，就像我们在[第6章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)中做的那样，*使用深度Q学习实现智能最优离散控制代理*，以及其他一些修改来稳定训练。具体而言，使用了Q目标网络（就像我们在[第6章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)中讨论的那样，*使用深度Q学习实现智能最优离散控制代理*），但是现在这个目标网络是缓慢更新的，而不是在几个更新步骤后保持固定并更新它。DDPG还使用经验重放缓冲区，并使用噪声版本的
    ![](img/00323.jpeg)，用方程式 ![](img/00324.jpeg) 表示，以鼓励探索作为策略。 ![](img/00325.jpeg)
    是确定性的。
- en: 'There is an extension to DDPG called D4PG, short for Distributed Distributional
    DDPG. I can guess what you might be thinking: DPG -> DDPG -> {Missing?}-> DDDDPG.
    Yes! The missing item is for you to implement.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG有一个扩展版本叫做D4PG，简称为分布式分布式DDPG。我猜你可能在想：DPG -> DDPG -> {缺失？} -> DDDDPG。没错！缺失的部分就是你需要实现的内容。
- en: 'The D4PG algorithm applies four main improvements to the DDPG algorithm, which
    are listed here briefly if you are interested:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG算法对DDPG算法做了四个主要改进，如果你有兴趣，可以简要列出如下：
- en: Distributional critic (the critic now estimates a distribution for Q-values
    rather than a single Q-value for a given state and action)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式评论员（评论员现在估计Q值的分布，而不是给定状态和动作的单一Q值）
- en: N-step returns (similar to what we used in [Chapter 8](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm*,
    n-step TD returns are used instead of the usual 1-step return)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N步回报（类似于我们在[第8章](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12)中使用的*实现智能自动驾驶代理，使用深度演员-评论员算法*，使用了n步TD回报，而不是通常的1步回报）
- en: Prioritized experience replay (this is used to sample experiences from the experience
    replay memory)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先经验重放（用于从经验重放记忆中抽样经验）
- en: Distributed parallel actors (utilizes K independent actors, gathering experience
    in parallel and populating the experience replay memory)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式并行演员（利用K个独立的演员并行收集经验并填充经验重放记忆）
- en: Proximal Policy Optimization
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邻近策略优化（Proximal Policy Optimization）
- en: '**Proximal Policy Optimization** (**PPO**) is a policy gradient-based method
    and is one of the algorithms that have been proven to be stable as well as scalable.
    In fact, PPO was the algorithm used by the OpenAI Five team of agents that played
    (and won) against several human DOTA II players, which we discussed in our previous
    chapter.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**邻近策略优化（PPO）** 是一种基于策略梯度的方法，是已经被证明既稳定又具有可扩展性的算法之一。事实上，PPO是OpenAI Five团队使用的算法，该团队的代理与多位人类DOTA
    II玩家对战并获胜，这一点我们在前一章中讨论过。'
- en: Core concept
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概念
- en: In policy gradient methods, the algorithm performs rollouts to collect samples
    of transitions and (potentially) rewards, and updates the parameters of the policy
    using gradient descent to minimize the objective function. The idea is to keep
    updating the parameters to improve the policy until a good policy is obtained.
    To improve the training stability, the **Trust Region Policy Optimization** (**TRPO**)
    algorithm enforces a **Kullback-Liebler** (**KL**) divergence constraint on the
    policy updates, so that the policy is not updated too much in one step when compared
    to the old policy. TRPO was the precursor to the PPO algorithm. Let's briefly
    discuss the objective function used in the TRPO algorithm in order to get a better
    understanding of PPO.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度方法中，算法通过执行回合（rollouts）来收集状态转移和（可能的）奖励样本，并使用梯度下降更新策略的参数，以最小化目标函数。其思想是不断更新参数以改进策略，直到获得一个较好的策略。为了提高训练的稳定性，**信任域策略优化**（**TRPO**）算法对策略更新施加了**Kullback-Liebler**（**KL**）散度约束，确保策略在与旧策略的比较中不会在一步中更新得过多。TRPO
    是 PPO 算法的前身。让我们简要讨论一下 TRPO 算法中使用的目标函数，以便更好地理解 PPO。
- en: Off-policy learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 脱策略学习
- en: 'As we know, in the case of off-policy learning, the agent follows a behavioral
    policy that is different from the policy that the agent is trying to optimize.
    Just to remind you, Q-learning, which we discussed in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*, along
    with several extensions, is also an off-policy algorithm. Let''s denote the behavior
    policy using ![](img/00326.jpeg). Then, we can write the objective function of
    the agent to be the total advantage over the state-visitation distribution and
    actions given by the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，在脱策略学习的情况下，代理遵循一个与其试图优化的策略不同的行为策略。提醒一下，我们在[第6章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)中讨论过的
    Q-learning，*使用深度 Q-learning 实现智能代理进行最优离散控制*，以及一些扩展，都是脱策略算法。我们用 ![](img/00326.jpeg)
    来表示行为策略。那么，我们可以将代理的目标函数写为状态访问分布和动作的总优势，如下所示：
- en: '![](img/00327.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00327.jpeg)'
- en: 'Here, ![](img/00328.jpeg) is the policy parameters before the update and ![](img/00329.jpeg) is
    the state visitation probability distribution under the old policy parameters. We
    can multiply and divide the terms in the inner summation by the behavior policy ![](img/00330.jpeg),
    with the idea being the use of importance sampling to account for the fact that
    the transitions are sampled using the behavior policy ![](img/00331.jpeg):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/00328.jpeg) 是更新前的策略参数，![](img/00329.jpeg) 是旧策略参数下的状态访问概率分布。我们可以在内层求和式中对项进行乘除操作，使用行为策略
    ![](img/00330.jpeg)，其目的是使用重要性采样来考虑到状态转移是通过行为策略 ![](img/00331.jpeg) 进行采样的：
- en: '![](img/00332.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00332.jpeg)'
- en: The changed terms in the preceding equation compared to the previous equation
    are shown in red.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一方程相比，前述方程中变化的项用红色表示。
- en: 'We can write the previous summations over a distribution as an expectation,
    like so:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将之前的求和表示为一个期望，如下所示：
- en: '![](img/00333.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00333.jpeg)'
- en: On-policy
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在策略学习中
- en: 'In the case of on-policy learning, the behavior policy and the target policy
    for the agent are one and the same. So, naturally the current policy (before the
    update) that the agent is using to collect samples is going to be ![](img/00334.jpeg),
    which is the behavior policy, and therefore the objective function becomes this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略学习的情况下，行为策略和目标策略是相同的。因此，自然地，当前策略（更新前）是代理用来收集样本的策略，表示为 ![](img/00334.jpeg)，这就是行为策略，因此目标函数变为：
- en: '![](img/00335.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00335.jpeg)'
- en: The changed terms in the preceding equation compared to the previous equation
    are shown in red.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一方程相比，前述方程中变化的项用红色表示。
- en: 'TRPO optimizes the previous object function with a *trust region* constraint,
    which using the KL divergence metric given by the following equation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO 通过*信任域*约束优化前述目标函数，使用以下方程给出的 KL 散度度量：
- en: '![](img/00336.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00336.jpeg)'
- en: 'This is the constraint that makes sure that the new update to the policy is
    not diverging too much from the current policy. Although the idea behind TRPO
    was neat and intuitively simple, the implementation and gradient updates involved
    complexities. PPO simplifies the approach using a clipped surrogate objective
    that was effective and simple as well. Let''s get a deeper understanding of the
    core concepts behind PPO using the math behind the algorithm. Let the probability
    ratio of taking action *a* given state *s* between the new policy ![](img/00337.jpeg) and
    the old policy ![](img/00338.jpeg) be defined as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是确保新策略更新不会与当前策略相差过大的约束条件。尽管 TRPO 背后的理念简洁且直观，但其实现和梯度更新涉及复杂性。PPO 通过使用一个裁剪的替代目标简化了这一方法，且既有效又简单。让我们通过算法背后的数学原理深入理解
    PPO 的核心概念。假设在给定状态 *s* 下采取动作 *a* 的新策略与旧策略的概率比定义如下：
- en: '![](img/00339.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00339.jpeg)'
- en: 'Substituting this into the on-policy objective function equation of TRPO that
    we discussed earlier results in the objective function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个代入我们之前讨论的 TRPO 的在线策略目标函数方程中，得到的目标函数如下：
- en: '![](img/00340.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00340.jpeg)'
- en: 'Simply removing the KL divergence constraint will result in instability, due
    to the large number of parameter updates that may result. PPO imposes the constraint
    by forcing ![](img/00341.jpeg) to lie within the interval ![](img/00342.jpeg),
    where ![](img/00343.jpeg) is a tunable hyperparameter. Effectively, the objective
    function used in PPO takes the minimum value between the original parameter values
    and the clipped version, which can mathematically be described as shown here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地移除 KL 散度约束将导致不稳定，因为可能会出现大量的参数更新。PPO 通过强制！[](img/00341.jpeg) 落在区间！[](img/00342.jpeg)内，施加了这个约束，其中！[](img/00343.jpeg)是一个可调超参数。实际上，PPO
    中使用的目标函数取原始参数值和裁剪版本中的最小值，其数学描述如下所示：
- en: '![](img/00344.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00344.jpeg)'
- en: This results in a stable learning objective with monotonically improving policy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个稳定的学习目标，并且策略单调地改善。
- en: Rainbow
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rainbow
- en: 'Rainbow ([https://arxiv.org/pdf/1710.02298.pdf](https://arxiv.org/abs/1710.02298))
    is an off-policy deep reinforcement learning algorithm based on DQN. We looked
    at and implemented deep Q-learning (DQN) and some of the extensions to DQN in
    [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*. There
    have been several more extensions and improvements to the DQN algorithm. Rainbow
    combines six of those extensions and shows that the combination works much better.
    Rainbow is a state-of-the art algorithm that currently holds the record for the
    highest score on all Atari games. If you are wondering why the algorithm is named
    *Rainbow*, it is most probably due to the fact that it combines seven (the number
    of colors in a rainbow) extensions to the Q-learning algorithm, namely:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow ([https://arxiv.org/pdf/1710.02298.pdf](https://arxiv.org/abs/1710.02298))
    是基于 DQN 的一种离策略深度强化学习算法。我们在[第六章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度
    Q 学习实现最优离散控制的智能代理*中研究并实现了深度 Q 学习（DQN）以及一些 DQN 的扩展。DQN 算法已经有了几个扩展和改进。Rainbow 结合了其中六个扩展，显示出它们的组合效果更佳。Rainbow
    是一个最先进的算法，目前在所有 Atari 游戏中保持着最高分记录。如果你想知道为什么这个算法叫做 *Rainbow*，很可能是因为它结合了七个（即彩虹的颜色数）Q
    学习算法的扩展，具体包括：
- en: DQN
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN
- en: Double Q-Learning
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双重 Q 学习
- en: Prioritized experience replay
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: Dueling networks
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗网络
- en: Multi-step learning/n-step learning
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多步学习/n 步学习
- en: Distributional RL
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式 RL
- en: Noisy nets
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声网络
- en: Core concept
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概念
- en: Rainbow combines DQN with six selected extensions that were shown to address
    the limitations of the original DQN algorithm. We will briefly look at the six
    extensions to understand how they contributed to the overall performance boost
    and landed Rainbow in the top spot on the Atari benchmark, and also how they proved
    to be successful in the OpenAI Retro contest.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow 结合了 DQN 和六个已被证明能解决原始 DQN 算法局限性的扩展。我们将简要回顾这六个扩展，了解它们如何为整体性能提升做出贡献，并使
    Rainbow 在 Atari 基准测试中占据了榜首，同时也证明了它们在 OpenAI Retro 大赛中的成功。
- en: DQN
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN
- en: 'By now, you should be very familiar with DQN, as we went through the step-by-step
    implementation of a deep Q-learning agent in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*, where
    we discussed DQN in detail and how it extends standard Q-learning with a deep
    neural network function approximation, replay memory, and a target network. Let''s
    recall the Q-learning loss that we used in the deep Q-learning agent in [Chapter
    6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing an Intelligent
    Agent for Optimal Discrete Control Using Deep Q-Learning*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经非常熟悉 DQN，因为我们在[第 6 章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度
    Q-Learning 实现最优离散控制的智能体*中逐步实现了深度 Q-learning 智能体，在那里我们详细讨论了 DQN 以及它如何通过深度神经网络函数逼近、回放记忆和目标网络扩展标准
    Q-learning。让我们回顾一下我们在[第 6 章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度
    Q-Learning 实现最优离散控制的智能体*中使用的 Q-learning 损失：
- en: '![](img/00345.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00345.jpeg)'
- en: This is basically the mean squared error between the TD target and DQN's Q-estimate,
    as we noted in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12),
    *Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*,
    where ![](img/00346.jpeg) is the slow-moving target network and ![](img/00347.jpeg) is
    the main Q network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是 TD 目标和 DQN 的 Q 估计之间的均方误差，正如我们在[第 6 章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度
    Q-Learning 实现最优离散控制的智能体*中所提到的，其中 ![](img/00346.jpeg) 是缓慢变化的目标网络，![](img/00347.jpeg)
    是主 Q 网络。
- en: Double Q-Learning
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重 Q-Learning
- en: 'In Double Q-Learning, there are two action-value/Q functions. Let''s call them
    Q1 and Q2\. The idea in double Q-learning is to *decouple action selection from
    the value estimation*. That is, when we want to update Q1, we select the best
    action according to Q1, but use Q2 to find the value of the selected action. Similarly,
    when Q2 is being updated, we select the action based on Q2, but use Q1 to determine
    the value of the selected action. In practice, we can use the main Q network ![](img/00348.jpeg) as
    Q1 and the slow-moving target network ![](img/00349.jpeg) as Q2, which gives us
    the following Double Q-Learning loss equation (the differences from the DQN equation
    are shown in red):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在双重 Q-Learning 中，有两个动作值/Q 函数。我们将它们称为 Q1 和 Q2。双重 Q-Learning 的理念是*将动作选择与价值估计解耦*。也就是说，当我们想更新
    Q1 时，我们根据 Q1 选择最佳动作，但使用 Q2 来找到选定动作的价值。类似地，当 Q2 被更新时，我们基于 Q2 选择动作，但使用 Q1 来确定选定动作的价值。实际上，我们可以使用主
    Q 网络 ![](img/00348.jpeg) 作为 Q1，使用缓慢变化的目标网络 ![](img/00349.jpeg) 作为 Q2，从而得到以下双重
    Q-Learning 损失方程（与 DQN 方程的差异以红色显示）：
- en: '![](img/00350.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00350.jpeg)'
- en: The motivation behind this change in the loss function is that Q-learning is
    affected by overestimation bias, and this can harm learning. The overestimation
    is due to the fact that the expectation of a maximum is greater than or equal
    to the maximum of the expectation (often the inequality is the one that holds)
    which arises due to the maximization step in the Q-learning algorithm and DQN.
    The change introduced by double Q-learning was shown to reduce overestimations
    that were harmful to the learning process, thereby improving performance over
    DQN.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更改损失函数的动机是，Q-learning 会受到过度估计偏差的影响，这可能会损害学习。过度估计的原因是最大值的期望大于或等于期望的最大值（通常是不等式成立），这源于
    Q-learning 算法和 DQN 中的最大化步骤。双重 Q-Learning 所引入的变化已被证明可以减少有害的过度估计，从而改善性能，相较于 DQN
    更具优势。
- en: Prioritized experience replay
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: 'When we implemented deep Q-learning in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*, we used
    an experience replay memory to store and retrieve sampled transition experience.
    In our implementation, and in the DQN algorithm, the experiences from the replay
    memory buffer are sampled uniformly. Intuitively, we would want to sample those
    experiences more frequently, as there is much to learn. Prioritized experience
    replay samples transition with probability ![](img/00351.jpeg) relative to the
    last encountered absolute TD error, given by the following equation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现深度 Q 学习的[第 6 章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度
    Q 学习实现智能代理进行最优离散控制*中，我们使用经验回放内存来存储和检索采样的过渡经验。在我们的实现中，以及在 DQN 算法中，来自回放内存缓冲区的经验是均匀采样的。直观地说，我们希望更频繁地采样这些经验，因为有很多内容需要学习。优先经验回放根据以下方程，以相对最后遇到的绝对
    TD 误差的概率 ![](img/00351.jpeg)进行过渡采样：
- en: '![](img/00352.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00352.jpeg)'
- en: Here, ![](img/00353.jpeg) is a hyperparameter that determines the shape of the
    distribution. This makes sure that we sample those transitions in which the predictions
    of the Q-values were more different from the correct values. In practice, new
    transitions are inserted into the replay memory with maximum priority to signify
    the importance of recent transition experiences.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/00353.jpeg) 是一个超参数，决定分布的形状。这确保我们采样那些 Q 值预测与正确值相差较大的过渡。实际上，新的过渡会以最高优先级插入回放内存，以表示最近过渡经验的重要性。
- en: Dueling networks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对战网络
- en: 'Dueling networks is a neural network architecture designed for value-based
    reinforcement learning. The name *dueling* stems from the main feature of this
    architecture, which is that there are two streams of computations, one for the
    value function and the other for the advantage. The following diagram, from a
    research paper ([https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)),
    shows the comparison of the dueling network architecture (the network shown at
    the bottom of the diagram) with the typical DQN architecture (shown at the top
    of the diagram):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对战网络（Dueling networks）是一种为基于值的强化学习设计的神经网络架构。名称*对战（dueling）*来源于该架构的主要特点，即它有两条计算流，一条用于值函数，另一条用于优势函数。下面的图示来自一篇研究论文（[https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)），展示了对战网络架构（图中底部的网络）与典型
    DQN 架构（图中顶部的网络）之间的对比：
- en: '![](img/00354.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00354.jpeg)'
- en: 'The convolutional layers that encode features are shared by both the value
    and advantage streams, and are merged by a special aggregation function, as discussed
    in the paper that corresponds to the following factorization of the action values:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 编码特征的卷积层是由值流和优势流共享的，并通过一个特殊的聚合函数进行合并，正如在论文中所讨论的，它对应于以下的动作值分解：
- en: '![](img/00355.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00355.jpeg)'
- en: '![](img/00356.jpeg), and ![](img/00357.jpeg) are, respectively, the parameters
    of the value stream, the shared convolutional encoder, and the advantage stream,
    and ![](img/00358.jpeg) is their concatenation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00356.jpeg)，和 ![](img/00357.jpeg) 分别是值流、共享卷积编码器和优势流的参数， ![](img/00358.jpeg) 是它们的连接。'
- en: Multi-step learning/n-step learning
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多步学习/n 步学习
- en: 'In [Chapter 8](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm*,
    we implemented the n-step return TD return method and discussed how forward-view
    multi-step targets can be used in place of a single/one-step TD target. We can
    use that n-step return with DQN, and that is essentially the idea behind this
    extension. Recall that the truncated n-step return from state ![](img/00359.jpeg) is
    given as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 8 章](part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12)，*使用深度演员-评论家算法实现智能自主汽车驾驶代理*中，我们实现了
    n 步返回 TD 方法，并讨论了如何使用前视多步目标替代单步 TD 目标。我们可以将这个 n 步返回与 DQN 一起使用，这本质上是该扩展的思想。回想一下，从状态
    ![](img/00359.jpeg) 的截断 n 步返回给出如下：
- en: '![](img/00360.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00360.jpeg)'
- en: 'Using this equation, a multi-step variant of DQN can be defined to minimize
    the following loss (the differences from the DQN equation are shown in red):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程，可以定义 DQN 的多步变体，以最小化以下损失（与 DQN 方程的差异以红色显示）：
- en: '![](img/00361.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00361.jpeg)'
- en: This equation shows the change introduced to DQN.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程展示了 DQN 中引入的变化。
- en: Distributional RL
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: 'The distributional RL method ([https://arxiv.org/abs/1707.06887](https://arxiv.org/abs/1707.06887))
    is about learning to approximate the distribution of returns rather than the expected
    (average) return. The distributional RL method proposes the use of probability
    masses placed on a discrete support to model such distributions. This, in essence,
    means that rather than trying to model one action-value given the state, a distribution
    of action-values for each action given the state is sought. Without going too
    much into the details (as that would require a lot of background information),
    we will look at one of the key contributions of this method to RL in general,
    which is the formulation of the Distributional Bellman equation. As you may recall
    from the previous chapters of this book, the action-value function, using a one-step
    Bellman backup for it, can be returned as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式强化学习方法（[https://arxiv.org/abs/1707.06887](https://arxiv.org/abs/1707.06887)）是学习如何近似回报的分布，而不是期望（平均）回报。分布式强化学习方法提出使用放置在离散支持集上的概率质量来建模这些分布。本质上，这意味着与其试图建模给定状态下的单一动作值，不如为每个动作给定状态下寻找一个动作值的分布。虽然不深入细节（因为那需要大量背景信息），我们将看看此方法对强化学习的一个关键贡献，即分布式贝尔曼方程的公式化。如你从本书前几章回忆的那样，使用一步贝尔曼备份的动作值函数可以通过如下方式返回：
- en: '![](img/00362.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00362.jpeg)'
- en: 'In the case of Distributional Bellman equations, the scalar quantity ![](img/00363.jpeg) is
    replaced by a random variable ![](img/00364.jpeg), which gives us the following
    equation:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式贝尔曼方程的情况下，标量量 ![](img/00363.jpeg) 被随机变量 ![](img/00364.jpeg) 所替代，从而得到如下方程：
- en: '![](img/00365.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00365.jpeg)'
- en: 'Because the quantity is no longer a scalar, the update equation needs to be
    dealt with more car than just adding the discounted value of the next state-action-value
    to the step-return. The update step of the distributional bellman equation can
    be understood easily with the help of the following diagram (stages from left
    to right):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该量不再是标量，因此更新方程需要比单纯将下一状态-动作值的折扣值加到步进回报中更加小心地处理。分布式贝尔曼方程的更新步骤可以通过以下图示轻松理解（从左到右的各个阶段）：
- en: '![](img/00366.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00366.jpeg)'
- en: In the previous illustration, the distribution of the next state action-value
    is depicted in red on the left, which is then scaled by the discount factor ![](img/00367.jpeg) (middle),
    and finally the distribution is shifted by ![](img/00368.jpeg) to yield the Distributional
    Bellman update. After the update, the target distribution ![](img/00369.jpeg)that
    results from the previous update operation is projected onto the supports of the
    current distribution ![](img/00370.jpeg) by minimizing the cross entropy loss
    between ![](img/00371.jpeg) and ![](img/00372.jpeg).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的插图中，下一状态的动作值分布用红色显示在左侧，接着通过折扣因子 ![](img/00367.jpeg) （中间）进行缩放，最终通过 ![](img/00368.jpeg) 平移该分布，从而得到分布贝尔曼更新。在更新之后，由前一更新操作得到的目标分布 ![](img/00369.jpeg) 通过最小化 ![](img/00371.jpeg) 和 ![](img/00372.jpeg) 之间的交叉熵损失，投影到当前分布的支持集 ![](img/00370.jpeg) 上。
- en: 'With this background, you can briefly glance through the pseudo code of the
    C51 algorithm from the Distributional RL paper, which is integrated into the Rainbow
    agent:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，你可以简要浏览分布式强化学习论文中的C51算法伪代码，它已经集成到Rainbow智能体中：
- en: '![](img/00373.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00373.jpeg)'
- en: Noisy nets
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声网络
- en: If you recall, we used an ![](img/00374.jpeg)-greedy policy for the deep Q-learning
    agent in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning*, to take
    action based on the action-values learned by the deep Q-network, which basically
    means taking the action with the highest action-value for a given state most of
    the time, except when, for some tiny fraction of the time (that is, with a very
    small probability ![](img/00375.jpeg)), the agent selects a random action. This
    may prevent the agent from exploring more reward states, especially if the action-values
    it has converged to are not the optimal action-values. The limitations of exploring
    using ![](img/00376.jpeg)-greedy policies were evident from the performance of
    the DQN variants and the value-based learning methods in the Atari game Montezuma's
    Revenge, where a long sequence of actions have to be executed in the right way
    to collect the first reward. To overcome this exploration limitation, Rainbow
    uses the idea of noisy nets—which are a simple but effective method proposed in
    2017.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们在[第6章](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)《*使用深度Q学习实现智能代理进行最优离散控制*》中使用了![](img/00374.jpeg)-贪心策略，基于深度Q网络学习到的动作值来选择动作，这基本上意味着大多数时候都会选择给定状态下的最高动作值对应的动作，除了在一小部分时间（即非常小的概率![](img/00375.jpeg)）内，代理会选择一个随机动作。这可能会阻止代理探索更多的奖励状态，特别是当它已经收敛的动作值不是最优的动作值时。使用![](img/00376.jpeg)-贪心策略进行探索的局限性，在Atari游戏《Montezuma's
    Revenge》的表现中显而易见，在该游戏中，必须以正确的方式执行一系列动作才能获得第一个奖励。为了克服这一探索限制，Rainbow使用了喧嚣网络的思想——这是一种2017年提出的简单但有效的方法。
- en: 'The main idea behind noisy nets is a noisy version of the linear neural network
    layer that combines a deterministic and a noisy stream, as exemplified in the
    following equation for the case of the linear neural network layer:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 喧嚣网络的主要思想是线性神经网络层的一个喧嚣版本，它结合了确定性流和喧嚣流，如以下方程所示，针对线性神经网络层的情况：
- en: '![](img/00377.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00377.jpeg)'
- en: Here, ![](img/00378.jpeg) and ![](img/00379.jpeg) are the parameters of the
    noisy layer, which are learned along with the other parameters of the DQN using
    gradient descent. The ![](img/00380.jpeg) represents the element-wise product
    operation and ![](img/00381.jpeg) and ![](img/00382.jpeg) are zero-mean random
    noise. We can use the noisy linear layer in place of the usual linear layer in
    our DQN implementation, which will have the added advantage of exploring better.
    Because ![](img/00383.jpeg) and ![](img/00384.jpeg) are learnable parameters,
    the network can learn to ignore the noisy stream. Because this happens over time
    for each of the neurons, the noisy stream decays at different rates in different
    parts of the state space, allowing better exploration with a form of self-annealing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/00378.jpeg) 和 ![](img/00379.jpeg) 是喧嚣层的参数，它们与DQN的其他参数一起通过梯度下降进行学习。![](img/00380.jpeg)
    代表元素级的乘积操作，![](img/00381.jpeg) 和 ![](img/00382.jpeg) 是均值为零的随机噪声。我们可以在DQN实现中用喧嚣线性层代替普通的线性层，这将具有更好的探索优势。由于![](img/00383.jpeg)
    和 ![](img/00384.jpeg) 是可学习的参数，网络可以学会忽略喧嚣流。随着时间的推移，对于每个神经元来说，喧嚣流会在状态空间的不同部分以不同的速率衰减，从而通过一种自我退火的形式实现更好的探索。
- en: 'The Rainbow agent implementation combines all of these methods to achieve state-of-the
    art results with better performance than any other method on the Atari suite of
    57 games. Overall, the performance of the Rainbow agent against the previous best-performing
    agent algorithms on the combined benchmark for Atari games is summarized in the
    following graph from the Rainbow paper:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow代理的实现将所有这些方法结合起来，达到了最先进的成果，并且在57款Atari游戏的测试中，比其他任何方法的表现都要优秀。总体来说，Rainbow代理与以前最佳表现的算法在Atari游戏综合基准测试中的表现，如下图所示：
- en: '![](img/00385.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00385.jpeg)'
- en: From the plot, it is clearly evident that the methods incorporated into the
    Rainbow agent lead to substantially improved performance across 57 different Atari
    games.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以清楚地看出，Rainbow代理所包含的方法在57款不同的Atari游戏中带来了显著的性能提升。
- en: Quick summary of advantages and applications
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势和应用的快速总结
- en: 'A few of the key advantages of the Rainbow agent are summarized here for your
    quick reference:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow代理的一些关键优势总结如下，供您快速参考：
- en: Combines several notable extensions to Q-learning developed over the past several
    years
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合了过去几年中Q-learning的多个显著扩展
- en: Achieves state-of-the art results in the Atari benchmarks
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Atari 基准测试中取得了最先进的结果。
- en: n-step targets with a suitably tuned value for *n* often leads to faster learning
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当调整的 *n* 值的 n 步目标通常能加速学习。
- en: Unlike other DQN variants, the Rainbow agent can start learning with 40% less
    frames collected in the experience replay memory
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他 DQN 变体不同，Rainbow 智能体可以在经验回放内存中收集的帧数减少 40% 的情况下开始学习。
- en: Matches the best performance of DQN in under 10 hours (7 million frames) on
    a single-GPU machine
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不到 10 小时（700 万帧）内，在单 GPU 机器上匹配了 DQN 的最佳表现。
- en: The Rainbow algorithm has become the most sought after agent for discrete control
    problems where the action space is small and discrete. It has been very successful
    with other game environments, such as Gym-Retro, and notably a tweaked version
    of the Rainbow agent placed second in the OpenAI Retro contest held in 2018, which
    is a transfer learning contest where the task is to learn to play the retro Genesis
    console games Sonic The Hedgehog, Sonic The Hedgehog II, and Sonic & Knuckles
    on some levels, and then be able to play well on other levels that the agent was
    not trained on. Considering the fact that in typical reinforcement learning settings,
    agents are trained and tested in the same environment, the retro contest measured
    the learning algorithm's ability to generalize its learning from previous experience.
    In general, the Rainbow agent is the best first bet to try on any RL problem/task
    in a discrete action space.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow 算法已成为离散控制问题中最受追捧的智能体，尤其适用于动作空间小且离散的场景。它在其他游戏环境中也非常成功，例如 Gym-Retro，特别是经过调优的
    Rainbow 智能体在 2018 年 OpenAI Retro 比赛中获得了第二名。该比赛是一个迁移学习竞赛，任务是学习在某些关卡中玩复古的 Genesis
    游戏《刺猬索尼克》、《刺猬索尼克 II》和《刺猬索尼克与纳克尔斯》，然后能够在未经过训练的其他关卡中表现良好。考虑到在典型的强化学习环境中，智能体通常在相同环境中训练和测试，Retro
    比赛衡量了学习算法从先前经验中泛化学习的能力。总的来说，Rainbow 智能体是任何具有离散动作空间的强化学习问题/任务的最佳初步选择。
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Being the final chapter of this book, this chapter provided summaries of key
    learning algorithms that are currently state of the art in this domain. We looked
    at the core concepts behind three different state-of-the-art algorithms, each
    with their own unique elements and their own categories (actor-critic/policy based/value-function
    based).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本书的最后一章，本章总结了当前在该领域内最先进的关键学习算法。我们了解了三种不同最先进算法的核心概念，每种算法都有其独特的元素和分类（演员-评论家/基于策略/基于价值函数）。
- en: Specifically, we discussed the deep deterministic policy gradient algorithm,
    which is an actor-critic architecture method that uses a deterministic policy
    rather than the usual stochastic policy, and achieves good performance on several
    continuous control tasks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们讨论了深度确定性策略梯度算法，这是一种演员-评论家架构方法，采用确定性策略，而非通常的随机策略，并在多个连续控制任务中取得了良好的表现。
- en: We then looked at the PPO algorithm, which is a policy gradient-based method
    that uses a clipped version of the TRPO objective and learns a monotonically better
    and stable policy, and has been successfully used in very high-dimensional environments
    such as DOTA II.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们研究了 PPO 算法，这是一种基于策略梯度的方法，采用了 TRPO 目标的剪辑版本，并学习到一个单调提升且稳定的策略，在像 DOTA II 这样的高维环境中取得了成功。
- en: Finally, we looked at the Rainbow algorithm, which is a value-based method and
    combines several extensions to the very popular Q-learning algorithm, namely DQN,
    double Q-learning, prioritized experience replay, dueling networks, multi-step
    learning/n-step learning, distributional reinforcement learning, and noisy-network
    layers. The Rainbow agent achieved significantly better performance in the Atari
    benchmark suite of 57 games and also performed very well in transfer learning
    tasks in the OpenAI Retro contest.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了 Rainbow 算法，这是一种基于价值的方法，结合了多个扩展到非常流行的 Q-learning 算法的技术，即 DQN、双重 Q-learning、优先经验回放、对抗网络、多步学习/n
    步学习、分布式强化学习和噪声网络层。Rainbow 智能体在 57 款 Atari 基准游戏中取得了显著更好的表现，并且在 OpenAI Retro 比赛中的迁移学习任务上表现也非常出色。
- en: With that, we are into the last paragraph of this book! I hope you enjoyed your
    journey through the book, learned a lot, and acquired a lot of hands-on skills
    to implement intelligent agent algorithms and the necessary building blocks to
    train and test the agents on the learning environment/problem of your choice.
    You can use the issue-tracking system in the book's code repository to report
    issues with the code, or if you would like to discuss a topic further, or need
    any additional references/pointers to move to the next level.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入了这本书的最后一段！希望您在阅读过程中享受了旅程，学到了很多，并获得了实现智能代理算法以及在您选择的学习环境/问题上训练和测试代理所需的许多实际技能的机会。您可以使用书籍代码库中的问题追踪系统报告代码问题，或者如果您想进一步讨论某个主题，或者需要任何额外的参考资料/指引来进入下一个阶段。
