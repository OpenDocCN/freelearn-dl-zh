- en: Chapter 13
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第十三章
- en: AI for Games – Become the Master at Snake
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏 AI – 成为 Snake 大师
- en: This is the last practical chapter; congratulations on finishing the previous
    ones! I hope you really enjoyed them. Now, let's leave aside business problems
    and self-driving cars. Let's have some fun by playing a popular game called Snake
    and making an AI that teaches itself to play this game!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最后一章实用章节；恭喜你完成了前面的章节！我希望你真的很享受这些内容。现在，让我们暂时放下商业问题和自动驾驶汽车。通过玩一个名为 Snake 的流行游戏来玩得开心，同时制作一个能够自学的
    AI 来玩这个游戏！
- en: That's exactly what we'll do in this chapter. The model we'll implement is called
    deep convolutional Q-learning, using a **Convolutional Neural Network** (**CNN**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在这一章要做的。我们将实现的模型称为深度卷积 Q 学习，使用 **卷积神经网络**（**CNN**）。
- en: Our AI won't be perfect, and it won't fill in the entire map, but after some
    training it will start playing at a level comparable with humans.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 AI 并不完美，也不会填满整个地图，但经过一些训练后，它将开始以与人类相当的水平进行游戏。
- en: Let's start tackling this problem by looking at what the game looks like and
    what the target is.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过先看看这个游戏是什么样子以及目标是什么，来解决这个问题。
- en: Problem to solve
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 待解决的问题
- en: 'First, let''s have a look at the game itself:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下游戏本身：
- en: '![](img/B14110_13_01.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_01.png)'
- en: 'Figure 1: The Snake game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：蛇游戏
- en: Does that look somewhat familiar to you?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有点眼熟吗？
- en: I'm pretty convinced that it will; everyone's played Snake at least once in
    their life.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我相当确信它会做到；每个人至少玩过一次 Snake。
- en: The game is pretty simple; it consists of a snake and an apple. We control the
    snake and our aim is to eat as many apples as possible.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏非常简单；它由一条蛇和一个苹果组成。我们控制蛇，目标是吃尽可能多的苹果。
- en: Sounds easy? Well, there's a small catch. Every time our snake eats an apple,
    our snake gets larger by one tile. This means that the game is unbelievably simple
    at the beginning, but it gets gradually harder, to the point where it becomes
    a strategic game.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来简单吗？嗯，其实有一个小陷阱。每当我们的蛇吃掉一个苹果时，蛇会长大一格。这意味着游戏在开始时异常简单，但它会逐渐变得更加困难，直到成为一个策略性游戏。
- en: Also, when controlling our snake, we can't hit ourselves, nor the borders of
    the board. This rather predictably results in us losing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当控制我们的蛇时，我们不能撞到自己，也不能撞到棋盘的边界。这会导致我们输掉游戏，这也是非常容易预见的。
- en: Now that we understand the problem, we can progress to the first step when creating
    an AI – building the environment!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了问题，我们可以进入创建 AI 时的第一步——构建环境！
- en: Building the environment
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建环境
- en: 'This time, as opposed to some of the other practical sections in this book,
    we don''t have to specify any variables or make any assumptions. We can just go
    straight to the three crucial steps present in every deep Q-learning project:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，与本书中的其他一些实用部分不同，我们不需要指定任何变量或做出任何假设。我们可以直接进入每个深度 Q 学习项目中的三个关键步骤：
- en: Defining the states
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义状态
- en: Defining the actions
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义动作
- en: Defining the rewards
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义奖励
- en: Let's begin!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Defining the states
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义状态
- en: In every previous example, our states were a 1D vector that represented some
    values that define the environment. For example, for our self-driving car we had
    the information gathered from the three sensors around the car and the car's position.
    All of these were put into a single 1D array.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的每个例子中，我们的状态都是一个 1D 向量，表示定义环境的某些值。例如，对于我们的自动驾驶汽车，我们有来自汽车周围三个传感器和汽车位置的信息。所有这些信息都被放入一个
    1D 数组中。
- en: But what if we want to make something slightly more realistic? What if we want
    the AI to see and gather information from the same source as we do? Well, that's
    what we'll do in this chapter. Our AI will see exactly the same board as we see
    when playing Snake!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们想让它看起来更真实一点呢？如果我们希望 AI 能像我们一样通过相同的来源来查看并收集信息呢？好吧，这就是我们在本章要做的。我们的 AI 将看到与我们玩
    Snake 时完全相同的棋盘！
- en: The state of the game should be a 2D array representing the board of the game,
    exactly the same thing that we can see.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的状态应该是一个 2D 数组，代表游戏的棋盘，完全与我们看到的情况相同。
- en: 'There''s just one problem with this solution. Take a look at the following
    image, and see if you can answer the question: which way is our snake moving right
    now?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案有一个问题。看一下以下图片，看看你是否能回答这个问题：现在我们的蛇正朝哪个方向移动？
- en: '![](img/B14110_13_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_02.png)'
- en: 'Figure 2: The Snake game'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：蛇游戏
- en: If you said "I don't know," then you're exactly right.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你说“我不知道”，那你完全正确。
- en: Based on a single frame, we can't tell which way our snake is going. Therefore,
    we'll need to stack multiple images, and then input all of them at once to a Convolutional
    Neural Network. This will result in us having 3D states rather than 2D ones.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单一的画面，我们无法判断我们的蛇在朝哪个方向移动。因此，我们需要堆叠多个图像，然后将它们一起输入到卷积神经网络中。这样，我们将得到三维状态，而不是二维状态。
- en: 'So, just to recap:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，简单回顾一下：
- en: '![](img/B14110_13_03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_03.png)'
- en: 'Figure 3: The AI vision'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：AI 视觉
- en: We'll have a 3D array, containing next game frames stacked on top of each other,
    where the top one is the latest frame obtained from our game. Now, we can clearly
    see which way our AI is moving; in this case it's going up, toward the apple.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将有一个三维数组，包含一个接一个堆叠的游戏画面，其中最上面的画面是我们游戏中获得的最新画面。现在，我们可以清楚地看到我们的 AI 在朝哪个方向移动；在这个例子中，它是向上走，朝着苹果走去。
- en: 'Now that we have defined states, we can go the next step: defining the actions!'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了状态，我们可以迈出下一步：定义动作！
- en: Defining the actions
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义动作
- en: 'When we play Snake on a phone or a website, there are four actions available
    for us to take:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在手机或网站上玩贪吃蛇时，有四个可供我们选择的动作：
- en: Go up
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向上走
- en: Go down
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下走
- en: Go right
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向右走
- en: Go left
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向左走
- en: However, if the action we take would require the snake to make a 180° turn directly
    back on itself, then the game blocks this action and the snake continues going
    in its current direction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们所采取的动作需要蛇做出 180° 的掉头，那么游戏将阻止这个动作，蛇将继续朝着当前的方向前进。
- en: In the preceding example, if we were to select action 2 – go down–our snake
    would still continue going up, because going down is impossible as the snake can't
    make a 180° turn directly back on itself.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，如果我们选择动作 2——向下走——我们的蛇仍然会继续向上走，因为向下走是不可能的，因为蛇无法直接做出 180° 的掉头。
- en: It's worth noting that all of these actions are relative to the board, not the
    snake; they're not affected by the current movement of the snake. Going up, down,
    right, or left always means going up, down, right, or left with respect to the
    board, not to the snake's current direction of movement.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，所有这些动作都是相对于棋盘的，而不是相对于蛇的；它们不会受到蛇当前运动方向的影响。向上、向下、向右或向左的动作，始终是相对于棋盘的，不是相对于蛇当前的运动方向。
- en: 'Alright, so right now you might be in one of these two groups when it comes
    to deciding what actions we model in our AI:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在你可能属于以下两种情况之一，关于我们在 AI 中建模的动作：
- en: We can use these four same actions for our AI.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将这四个相同的动作用于我们的 AI。
- en: We can't use these same actions, because blocking certain moves will be confusing
    for our AI. Instead, we should invent a way to tell the snake to go left, go right,
    or keep going.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能使用这些相同的动作，因为阻止某些动作会让我们的 AI 感到困惑。相反，我们应该发明一种方法，告诉蛇向左走、向右走，或者继续前进。
- en: We actually can use these same actions for our AI!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们确实可以将这些相同的动作用于我们的 AI！
- en: Why won't it be confusing for our agent? That's because as long as our AI agent
    gets rewards for the actions it chose, and not for the action ultimately performed
    by the snake, then deep Q-learning will work and our AI will understand that in
    the example above choosing either *go up* or *go down* results in the same outcome.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这不会让我们的智能体感到困惑呢？那是因为，只要我们的 AI 智能体根据它选择的动作获得奖励，而不是根据蛇最终执行的动作获得奖励，那么深度 Q 学习就能有效工作，我们的
    AI 将会理解，在上面的例子中，选择 *向上走* 或 *向下走* 会导致相同的结果。
- en: For example, let's say that the AI-controlled snake is currently going left.
    It chooses action 3, go right; and because that would cause the snake to make
    a 180° turn back on itself, instead the snake continues going left. Let's say
    that action means the snake crashes into the wall and, as a result, dies. In order
    for this not to be confusing for our agent, all we need to do is tell it that
    the action of *go right* caused it to crash, even though the snake kept moving
    left.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 AI 控制的蛇当前正向左移动。它选择了动作 3，向右走；但是因为这将导致蛇做出 180° 的掉头，所以蛇仍然继续向左走。假设该动作意味着蛇撞到了墙壁，并因此死亡。为了避免这对我们的智能体造成困扰，我们所需要做的就是告诉它，*向右走*的动作导致它撞墙，尽管蛇依然向左移动。
- en: Think of it as teaching an AI to play with the actual buttons on a phone. If
    you keep trying to make your snake double back on itself when it's moving left,
    by pressing the go right button over and over again, the game will keep ignoring
    the impossible move you keep telling it to do, keep going left, and eventually
    crash. That's all the AI needs to learn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可以把它看作是教AI用手机上的实际按钮来玩游戏。如果你不断地按右键，试图让蛇在向左移动时转回去，游戏会一直忽略你不断要求它执行的不可能的操作，继续向左走，最终崩溃。这就是AI需要学习的全部内容。
- en: This is because, remember, in deep Q-learning we only update the Q-values of
    the action that the AI takes. If our snake is going left, and the AI decides to
    go right and the snake dies, it needs to understand that the action of *go right*
    caused it to get the negative reward, not the fact that the snake moved left;
    even though choosing the action *go left* would cause the same outcome.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，请记住，在深度Q学习中，我们只更新AI执行的动作的Q值。如果蛇正在向左走，而AI决定向右走并且蛇死了，它需要明白“向右走”这个动作导致了负奖励，而不是蛇向左移动的事实；即使选择“向左走”也会导致相同的结果。
- en: I hope you understand that the AI can use the same actions as we use when we
    play. We can continue to the next, final step – defining the rewards!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你明白，AI可以使用我们在玩游戏时使用的相同动作。我们可以继续到下一个最终步骤——定义奖励！
- en: Defining the rewards
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义奖励
- en: 'This last step is pretty simple; we just need three rewards:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步很简单；我们只需要三个奖励：
- en: Reward for eating an apple
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 吃苹果的奖励
- en: Reward for dying
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 死亡奖励
- en: The living penalty
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生存惩罚
- en: 'The first two are hopefully easy to understand. After all, we want to encourage
    our agent to eat as many apples as possible and therefore we will set its reward
    to be positive. To be precise: **eating an apple = +2**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个应该容易理解。毕竟，我们希望鼓励我们的代理尽可能多地吃苹果，因此我们会设置它的奖励为正值。准确来说：**吃苹果 = +2**
- en: 'Meanwhile, we want to discourage our snake from dying. That''s why we set that
    reward to be a negative one. To be precise: **dying = -1**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们希望避免让我们的蛇死亡。这就是为什么我们将奖励设置为负值。准确来说：**死亡 = -1**
- en: 'Then comes the final reward: the living penalty.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是最终的奖励：生存惩罚。
- en: What is that, and why is it necessary? We have to convince our agent that collecting
    apples as quickly as possible, without dying, is a good idea. If we were to only
    have the two rewards we've already defined, our agent would simply travel around
    the entire map, hoping that at some point it finds an apple. It wouldn't understand
    that it needs to collect apples as quickly as it can.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那是什么，为什么它是必要的？我们必须让我们的代理相信，尽快收集苹果而不死掉是一个好主意。如果我们只定义已经有的这两个奖励，我们的代理将只是在整个地图上游荡，希望在某个时刻找到苹果。它不会明白它需要尽可能快地收集苹果。
- en: 'That''s why we introduce the living penalty. It will slightly punish our AI
    for every action it takes, unless this action leads to dying or collecting an
    apple. This will show our agent that it needs to collect apples quickly, as only
    moves that collect an apple lead to gaining a positive reward. So, how big this
    reward should be? Well, we don''t want to punish it too much. To be precise: **living
    penalty =-0.03**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们引入生存惩罚。除非某个动作导致死亡或收集到苹果，否则它会稍微惩罚我们的AI每一个动作。这将向我们的代理展示，它需要尽快收集苹果，因为只有收集苹果的动作才能获得正奖励。那么，这个奖励应该有多大呢？嗯，我们不想惩罚它太多。准确来说：**生存惩罚
    = -0.03**
- en: If you want to tinker with these rewards, the absolute value of this reward
    should always be relatively small compared to the other rewards, for dying (-1)
    and collecting an apple (+2).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想调整这些奖励，这个奖励的绝对值应该始终相对较小，和其他奖励（死亡（-1）和收集苹果（+2））相比。
- en: AI solution
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI解决方案
- en: 'As always, the AI solution for deep Q-learning consists of two parts:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，深度Q学习的AI解决方案由两部分组成：
- en: '**Brain** – the neural network that will learn and take actions'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**大脑** – 将学习并采取行动的神经网络'
- en: '**Experience replay memory** – the memory that will store our experience; the
    neural network will learn from this memory'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**经验重放记忆** – 存储我们经验的记忆；神经网络将从这些记忆中学习'
- en: Let's tackle those now!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来处理这些吧！
- en: The brain
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大脑
- en: This part of the AI solution will be responsible for teaching, storing, and
    evaluating our neural network. To build it, we're going to use a CNN!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分AI解决方案将负责教导、存储和评估我们的神经网络。为了构建它，我们将使用CNN！
- en: Why a CNN? When explaining the theory behind them, I mentioned that they're
    often used when "our environment as state returns images," and that's exactly
    what we're dealing with here. We've already established that the game state is
    going to be a stacked 3D array containing the last few game frames.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择CNN？在解释其背后的理论时，我提到它们通常在“我们的环境作为状态返回图像”的情况下使用，而这正是我们在这里处理的内容。我们已经确认，游戏状态将是一个堆叠的3D数组，包含了最后几帧游戏画面。
- en: In the previous chapter, we discussed that a CNN takes a 2D image as input,
    not a stacked 3D array of images; but do you remember this graphic?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了CNN如何将一个2D图像作为输入，而不是一个堆叠的3D图像数组；但是你还记得这个图形吗？
- en: '![https://lh5.googleusercontent.com/qjfDY_d7Dvn92gkZ2KDpPAoy-SM_7AO8RExLTjtj-FYCQcCDVIrfSjvgslPBBT5kAneqJMRbJKAOikeslS-1T5TQaPDDxX338ko4DWQxi5xPggLbosb-p3tR8y5DDGp-blxs1aqj](img/B14110_13_04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![https://lh5.googleusercontent.com/qjfDY_d7Dvn92gkZ2KDpPAoy-SM_7AO8RExLTjtj-FYCQcCDVIrfSjvgslPBBT5kAneqJMRbJKAOikeslS-1T5TQaPDDxX338ko4DWQxi5xPggLbosb-p3tR8y5DDGp-blxs1aqj](img/B14110_13_04.png)'
- en: 'Figure 4: RGB images'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：RGB图像
- en: Here, I informed you that the RGB images are represented by 3D arrays that contain
    every single 2D channel of this image. Does that sound familiar? We can use the
    very same method for our problem. Just like each color in the RGB structure, we'll
    simply input every game frame as a new channel, which will give us a 3D array,
    which we will be able to input into a CNN.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我告诉过你，RGB图像是通过3D数组表示的，这些数组包含了图像的每个2D通道。听起来像是很熟悉吗？我们可以用完全相同的方法来解决我们的这个问题。就像RGB结构中的每个颜色一样，我们只需将每一帧游戏画面作为一个新的通道输入，这将为我们提供一个3D数组，然后我们可以将它输入到CNN中。
- en: In reality, CNNs usually only support 3D arrays as inputs. In order to input
    a 2D array, you need to create a fake single channel that transforms a 2D array
    into a 3D one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，CNN通常只支持3D数组作为输入。为了输入2D数组，你需要创建一个虚拟的单通道，将2D数组转换为3D数组。
- en: When it comes to the CNN architecture, we'll have two convolution layers separated
    by a pooling layer. One convolution layer will have 32 3x3 filters, and the other
    one will have 64 2x2 filters. The pooling layer will shrink the size by 2, as
    the pooling window size will be 2x2\. Why such an architecture? It's a classic
    one, found in many research papers, which I arbitrarily chose as common practice
    and which turned out to work brilliantly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 关于CNN架构，我们将有两个卷积层，中间有一个池化层。一个卷积层将有32个3x3的滤波器，另一个卷积层将有64个2x2的滤波器。池化层将把尺寸缩小2倍，因为池化窗口的大小是2x2。为什么选这样的架构？这是一个经典架构，在许多研究论文中都能找到，我随便选择了一个常用的架构，结果证明它的效果非常好。
- en: Our neural network will have one hidden layer with 256 neurons, and an output
    layer with 4 neurons; one for each of our possible outcome actions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络将有一个包含256个神经元的隐藏层和一个包含4个神经元的输出层；每个神经元对应我们可能的一个结果动作。
- en: We also need to set two last parameters for our CNN – learning rate and input
    shape.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要为CNN设置两个最后的参数——学习率和输入形状。
- en: Learning rate, which was used in the previous examples, is a parameter that
    specifies by how much we update the weights in the neural network. Too small and
    it won't learn, too big and it won't learn for a different reason; the changes
    will be too big for any optimization. I found through experimentation that a good
    learning rate for this example is 0.0001.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率，在前面的例子中有使用，是一个指定我们在神经网络中更新权重幅度的参数。学习率太小，模型就学不起来；学习率太大，模型也无法学习，因为更新幅度太大，无法进行任何有效的优化。通过实验，我发现这个例子中合适的学习率是0.0001。
- en: We've already agreed that the input should be a 3D array containing last frames
    obtained from our game. To be exact, we will not be reading pixels from our screen.
    Instead, we'll read the direct 2D array that represents our game's screen at a particular
    time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经达成一致，输入应该是一个包含游戏中最后几帧的3D数组。更准确地说，我们并不会从屏幕上读取像素，而是会读取表示游戏屏幕在某一时刻的直接2D数组。
- en: As you've probably noticed, our game is built on a grid. In the example we are
    using, the grid is 10x10\. Then, inside the environment is an array with the same
    size (10x10), telling us mathematically what the board looks like. For example,
    if we have part of the snake in one cell, then we place the value 0.5 in the corresponding
    cell in our 2D array, which we will read. An apple is described as value 1 in
    this array.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，我们的游戏是基于一个网格构建的。在我们使用的示例中，网格的大小是10x10。然后，环境中有一个大小相同（10x10）的数组，数学上告诉我们棋盘的状态。例如，如果蛇的部分在一个格子里，那么我们就在相应的2D数组格子中放置值0.5，我们将从中读取它。一个苹果在这个数组中的值为1。
- en: Now that we know how we'll see one frame, we need to decide how many previous
    frames we'll use when we describe the current game state. 2 should be enough,
    since we can discern from that which way the snake is going, but to make sure,
    we'll have 4.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何查看一帧，我们需要决定在描述当前游戏状态时将使用多少前一帧。2帧应该足够，因为我们可以从中辨别蛇的方向，但为了确保，我们将使用4帧。
- en: Can you tell me exactly what shape our input to the CNN will be?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你能告诉我我们的CNN输入到底是什么形状吗？
- en: It'll be 10x10x4, which gives us a 3D array!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它将是10x10x4，这样我们就得到了一个3D数组！
- en: The experience replay memory
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经验回放内存
- en: As defined in the theoretical chapter of deep Q-learning, we need to have a
    memory that stores experience gathered during training.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在深度Q学习的理论章节中定义的那样，我们需要一个存储在训练过程中收集的经验的记忆。
- en: 'We''ll store the following data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将存储以下数据：
- en: Current state – The game state the AI was in when it performed an action (what
    we inputted to our CNN)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态 – AI执行动作时所在的游戏状态（即我们输入到CNN中的内容）
- en: Action – Which action was undertaken
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作 – 执行的动作
- en: Reward – The reward gained by performing this action on the current state
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励 – 通过在当前状态下执行该动作所获得的奖励
- en: Next state – What happened (how the state looked) after performing the action
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一状态 – 执行动作后发生了什么（状态是什么样的）
- en: Game over – Information about whether we have lost or not
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏结束 – 关于我们是否输了的信息
- en: 'Also, we always have to specify two parameters for every experience replay
    memory:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们始终需要为每个经验回放内存指定两个参数：
- en: Memory size – The maximum size of our memory
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存大小 – 我们的内存的最大大小
- en: Gamma – The discount factor, existent in the Bellman equation
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma – 折扣因子，存在于贝尔曼方程中
- en: We'll set the memory size to 60,000 and the gamma parameter to 0.9.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将内存大小设置为60,000，gamma参数设置为0.9。
- en: There's one last thing to specify here.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一件事需要指定。
- en: I told you that our AI will learn from this memory, and that's true; but the
    AI won't be learning from the entire memory. Rather, it will learn from a small
    batch taken from it. The parameter that specifies this size will be called batch
    size, and in this example, we'll set its value to 32\. That means that our AI
    will learn every iteration from a batch of this size taken from experience replay
    memory.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉过你，我们的AI将从这个记忆中学习，确实如此；但AI并不会从整个记忆中学习。而是从其中提取的小批量进行学习。指定这个批量大小的参数叫做批量大小，在这个例子中，我们将其设置为32。也就是说，我们的AI将从每次迭代中，从经验回放内存中提取一个大小为32的批量进行学习。
- en: Now that you understand everything you have to code, you can get started!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经理解了所有需要编码的内容，你可以开始动手了！
- en: Implementation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'You''ll implement the entire AI code and the Snake game in five files:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在五个文件中实现整个AI代码和贪吃蛇游戏：
- en: '`environment.py` file – The file containing the environment (Snake game)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`environment.py`文件 – 包含环境（贪吃蛇游戏）的文件'
- en: '`brain.py` file – The file in which we build our CNN'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`brain.py`文件 – 我们构建CNN的文件'
- en: '`DQN.py` – The file that builds the Experience Replay Memory'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DQN.py` – 构建经验回放内存的文件'
- en: '`train.py` – The file where we will train our AI to play Snake'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train.py` – 我们将训练AI玩贪吃蛇的文件'
- en: '`test.py` – The file where we will test our AI to see how well it performs'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test.py` – 我们将测试AI表现如何的文件'
- en: You can find all of them on the GitHub page along with a pre-trained model.
    To get there, select `Chapter 13` folder on the main page.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub页面上找到所有这些内容，并且还可以找到一个预训练的模型。要访问该页面，请在主页面选择`Chapter 13`文件夹。
- en: We'll go through each file in the same order. Let's start building the environment!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按相同的顺序逐个查看每个文件。让我们开始构建环境吧！
- en: Step 1 – Building the environment
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步 – 构建环境
- en: 'Start this first, important step by importing the libraries you''ll need. Like
    this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入所需的库，开始这一步骤。像这样：
- en: '[PRE0]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You''ll only use two libraries: NumPy and PyGame. The former is really useful
    when dealing with lists or arrays, and the latter will be used to build the entire
    game – to draw the snake and the apple, and update the screen.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要使用两个库：NumPy和PyGame。前者在处理列表或数组时非常有用，后者将用于构建整个游戏——绘制蛇和苹果，并更新屏幕。
- en: Now, let's create the `Environment` class which will contain all the information,
    variables and methods that you need for your game. Why a class? This is because
    it makes things easier for you later on. You'll be able to call specific methods
    or variables from the object of this class.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建`Environment`类，这个类将包含你需要的所有信息、变量和方法。为什么是类？因为这样后续会更方便。你可以从这个类的对象中调用特定的方法或变量。
- en: 'The first method that you always have to have is the `__init__` method, always
    called when a new object of this class is created in the main code. To create
    this class along with this `__init__` method, you need to write:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须总是有的第一个方法是`__init__`方法，每当这个类的一个新对象在主代码中被创建时，都会调用它。要创建这个类以及`__init__`方法，你需要编写：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You create a new class, the `Environment()` class, along with its `__init__`
    method. This method only takes one argument, which is `waitTime`. Then after defining
    the method, create a list of constants, each of which is explained in the inline
    comments. After that, you perform some initialization. You make sure the snake
    is half the length of the screen or less on lines 24 and 25, and set the screen
    up on line 27\. One important thing to note is that you create the `screenMap`
    array on line 32, which represents the board more mathematically. 0.5 in a cell
    means that this cell is taken by the snake, and 1 in a cell means that this cell
    is taken by the apple.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了一个新的类，即`Environment()`类，并且它有一个`__init__`方法。这个方法只需要一个参数，即`waitTime`。然后，在定义方法后，创建一个常量列表，每个常量都在内联注释中解释。之后，进行一些初始化。你确保蛇的长度在第24和第25行不超过屏幕的一半，并且在第27行设置屏幕。需要注意的一点是，在第32行你创建了`screenMap`数组，它更数学化地表示了游戏板。单元格中的0.5表示这个单元格被蛇占据，而单元格中的1表示这个单元格被苹果占据。
- en: On lines 34 to 36, you place the snake in the middle of the screen, facing upward,
    and then in the remaining lines you place an apple using the `placeapple()` method
    (which we are about to define), draw the screen, set that the apple hasn't been
    collected, and set that there's no last move.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在第34到第36行，你将蛇放置在屏幕中央，面朝上，然后在剩余的行中使用`placeapple()`方法（我们即将定义）放置一个苹果，绘制屏幕，设置苹果未被收集，并且设置没有上一次的移动。
- en: 'That''s the very first method completed. Now you can proceed to the next one:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个方法的完成。现在你可以继续下一个方法：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This short method places an apple in a new, random spot in your `screenMap`
    array. You'll need this method when our snake collects the apple and a new apple
    needs to be placed. It also returns the random position of the new apple.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简短的方法将苹果放置在`screenMap`数组中的一个新的随机位置。当我们的蛇收集到苹果时，需要使用这个方法来放置新的苹果。它还会返回新苹果的随机位置。
- en: 'Then, you''ll need a function that draws everything for you to see:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要一个绘制所有内容的函数，让你可以看到：
- en: '[PRE3]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the name of this method is `drawScreen` and it doesn't take
    any arguments. Here you simply empty the entire screen, then fill it in with white
    tiles where the snake is and with a red tile where the apple is. At the end, you
    update the screen with `pg.display.flip()`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个方法的名称是`drawScreen`，它不接受任何参数。在这里，你只是清空整个屏幕，然后用白色的瓦片填充蛇所在的位置，用红色瓦片填充苹果所在的位置。最后，你用`pg.display.flip()`更新屏幕。
- en: 'Now, you need a function that will update the snake''s position and not the
    entire environment:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要一个只更新蛇的位置，而不更新整个环境的函数：
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see that this new method takes two arguments: `nextPos` and `col`.
    The former tells you where the head of the snake will be after performing a certain
    action. The latter will inform you whether the snake has collected an apple by
    taking this action, or not. Remember that if the snake has collected an apple,
    then the length of the snake increases by 1\. If you go deep into this code, you
    can see that, but we won''t go into detail here since it''s not so relevant for
    the AI. You can also see that if the snake has collected an apple, a new one is
    spawned in a new spot.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这个新方法接受两个参数：`nextPos`和`col`。前者告诉你在执行某个动作后蛇头的位置。后者则告诉你蛇是否通过这个动作收集了苹果。记住，如果蛇收集了苹果，那么蛇的长度会增加1。如果深入这段代码，你可以看到这一点，但由于这对AI不太相关，我们在这里不做详细说明。你还可以看到，如果蛇收集了苹果，会在新位置生成一个新的苹果。
- en: 'Now, let''s move on to the most important part of this code. You define a function
    that will update the entire environment. It will move your snake, calculate the
    reward, check if you lost, and return a new game frame. This is how it starts:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入这段代码最重要的部分。你定义了一个更新整个环境的函数。它会移动你的蛇，计算奖励，检查你是否输了，并返回一个新的游戏帧。它的起始方式如下：
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see, this method is called `step` and it takes one argument: the
    action that tells you which way you want the snake to be going. Just beneath the
    method''s definition, in the comments, you can see which action means which direction.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个方法被称为`step`，它接受一个参数：告诉你蛇想朝哪个方向移动的动作。方法定义下方的注释中，你可以看到每个动作代表的方向。
- en: Then you reset some variables. You set `gameOver` to `False` as this bool variable
    will tell you if you lost after performing this action. You set `reward` to `defReward`,
    as this is the living penalty; it can change if we collect an apple or die later.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你重置一些变量。你将`gameOver`设置为`False`，因为这个布尔变量会告诉你在执行这个动作后是否输了。你将`reward`设置为`defReward`，这是生存惩罚；如果我们收集了苹果或稍后死掉，这个值可能会改变。
- en: Then there's a `for` loop. It's there to make sure the PyGame window doesn't
    freeze; this is a requirement of the PyGame library. It just has to be there.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一个`for`循环。它的作用是确保PyGame窗口不会冻结；这是PyGame库的要求。它必须存在。
- en: '`snakeX` and `snakeY` tell you what the head position of the snake is. It''ll
    be used by the algorithm later, to determine what happens after the head moves.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`snakeX`和`snakeY`告诉你蛇头的位置。稍后，算法会使用这个信息来确定蛇头移动后的状态。'
- en: In the last few lines, you can see the algorithm that blocks impossible actions.
    Just to recap, an impossible action is the one that requires the snake to make
    a 180° turn in place. `lastMove` tells you which way the snake is going right
    now, and is compared with `action`. If these lead to a contradiction, then `action`
    is set to `lastMove`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几行，你可以看到阻止不可能动作的算法。简单回顾一下，不可能的动作是指蛇在原地做180°转弯的动作。`lastMove`告诉你蛇现在的移动方向，并与`action`进行比较。如果这些导致了矛盾，`action`就会被设置为`lastMove`。
- en: 'Still inside this method, you update the snake position, check for game over,
    and calculate the reward, like so:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在这个方法中，你更新蛇的位置，检查是否结束游戏，并计算奖励，像这样：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here you check what happens if the snake goes up. If the head of the snake is
    already in the top row (row no. `0`) then you've obviously lost, since the snake
    hits the wall. So, `reward` is set to `negReward` and `gameOver` is set to `True`.
    Otherwise, you check what lies ahead of the snake.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你检查蛇头向上移动会发生什么。如果蛇头已经在最上面的一行（第`0`行），那么显然你已经输了，因为蛇撞到墙壁了。所以，`reward`被设置为`negReward`，`gameOver`被设置为`True`。否则，你检查蛇头前方的情况。
- en: If the cell ahead already contains part of the snake's body, then you've lost.
    You check that in the first `if` statement, then set `gameOver` to `True` and
    `reward` to `negReward`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前方的格子已经包含了蛇身的一部分，那么你就输了。你在第一个`if`语句中检查这个情况，然后将`gameOver`设置为`True`，并将`reward`设置为`negReward`。
- en: Else if the cell ahead is an apple, then you set `reward` to `posReward`. You
    also update the snake's position by calling the method you created just before
    this one.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前方的格子是一个苹果，那么你将`reward`设置为`posReward`。你还会通过调用你刚刚创建的那个方法来更新蛇的位置。
- en: 'Else if the cell ahead is empty, then you don''t update `reward` in any way.
    You call the same method again, but this time with the `col` argument set to `False`,
    since the snake hasn''t collected an apple. You go through the same process for
    every other action. I won''t go through every line, but have a look at the code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前方的格子是空的，那么你不会对`reward`做任何更新。你会再次调用相同的方法，但这次将`col`参数设置为`False`，因为蛇并没有吃到苹果。对于每个其他动作，你会执行相同的过程。我不会逐行讲解，但你可以看一下代码：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Simply handle every single action in the same way you did with the action of
    going up. Check if the snake didn't hit the walls, check what lies ahead of the
    snake and update the snake's position, `reward`, and `gameOver` accordingly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以和向上移动时相同的方式处理每个动作。检查蛇是否没有撞到墙壁，检查蛇前方的情况并更新蛇的位置、`reward`和`gameOver`。
- en: 'There are two more steps in this method; let''s jump straight into the first
    one:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法还有两个步骤，我们直接跳到第一个：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You update our screen by drawing the snake and the apple on it, then change
    `lastMove` to `action`, since your snake has already moved and now it's moving
    in the `action` direction.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过在屏幕上绘制蛇和苹果来更新我们的屏幕，然后将`lastMove`设置为`action`，因为蛇已经移动，现在正朝着`action`的方向移动。
- en: 'The last step in this method is to return what the game looks like now, what
    the reward is that was obtained, and whether you''ve lost, like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的最后一步是返回当前游戏的样子、获得的奖励以及你是否输了，像这样：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`screenMap` gives you the information you need about what the game looks like
    after performing an action, `reward` gives you the collected reward from taking
    this action, and `gameOver` tells you whether you lost or not.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`screenMap`提供了执行动作后游戏界面所需的信息，`reward`告诉你通过执行这个动作获得的奖励，`gameOver`告诉你是否输了。'
- en: 'That''s it for this method! To have a complete `Environment` class, you only
    need to make a function that will reset the environment, like this `reset` method:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这个方法的全部内容！为了拥有一个完整的`Environment`类，你只需要创建一个重置环境的函数，就像这样一个`reset`方法：
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It simply resets the game board (`screenMap`), as well as the snake's position,
    to the default, which is the middle of the board. It also sets the apple's position
    to the same as it was in the last round.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是简单地重置了游戏板（`screenMap`），以及蛇的位置，恢复到默认的中间位置。它还将苹果的位置设置为和上一轮相同。
- en: Congratulations! You've just finished building the environment. Now, we'll proceed
    to the second step, building the brain.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚完成了环境的构建。现在我们将进入第二步，构建大脑。
- en: Step 2 – Building the brain
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步 – 构建大脑
- en: This is where you'll build our brain with a Convolutional Neural Network. You'll also
    set some parameters for its training and define a method that loads a pre-trained
    model for testing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里你将用卷积神经网络（CNN）来构建我们的“大脑”。你还将为其训练设置一些参数，并定义一个加载预训练模型进行测试的方法。
- en: Let's begin!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: 'As always, you start by importing the libraries that you''ll use, like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往地，你首先导入你将使用的库，像这样：
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you''ve probably noticed, all of the classes are a part of the Keras library,
    which is the one you''re going to use in this chapter. Keras is actually the only
    library that you''ll use in this file. Let''s go through each of these classes
    and methods right now:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，所有这些类都是Keras库的一部分，而这正是你将在本章中使用的库。实际上，Keras是你在这个文件中唯一需要使用的库。现在让我们一起回顾一下这些类和方法：
- en: '`Sequential` – A class that allows you to initialize a neural network, and
    defines the general structure of this network.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Sequential` – 一个可以初始化神经网络的类，并定义网络的一般结构。'
- en: '`load_model` – A function that loads a model from a file.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_model` – 一个从文件中加载模型的函数。'
- en: '`Dense` – A class to create fully connected layers in an Artificial Neural
    Network (ANN).'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Dense` – 一个类，用来创建人工神经网络（ANN）中的全连接层。'
- en: '`Dropout` – A class that adds dropout to our network. You''ve seen it used
    already, in *Chapter 8*, *AI for Logistics – Robots in a Warehouse*.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Dropout` – 一个向我们的网络中添加Dropout的类。你已经在*第8章* *《物流中的AI – 仓库中的机器人》*中见过它的使用。'
- en: '`Conv2D` – A class that builds convolution layers.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Conv2D` – 一个用于构建卷积层的类。'
- en: '`MaxPooling2D` – A class that builds max pooling layers.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MaxPooling2D` – 一个用于构建最大池化层的类。'
- en: '`Flatten` – A class that performs flattening, so that you''ll have an input
    for a classic ANN.'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Flatten` – 一个执行扁平化操作的类，以便为经典的人工神经网络（ANN）提供输入。'
- en: '`Adam` – An optimizer, which will optimize your neural network. It''s used
    when training the CNN.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Adam` – 一个优化器，用来优化你的神经网络。在训练CNN时使用它。'
- en: 'Now you''ve imported your library, you can continue by creating a class called
    `Brain`, where all these classes and methods are used. Start by defining a class
    and the `__init__` method, like this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经导入了库，接下来可以创建一个叫做`Brain`的类，在这个类中使用所有这些类和方法。首先定义一个类和`__init__`方法，像这样：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can see that the `__init__` method takes two arguments: `iS` (input shape)
    and `lr` (learning rate). Then you define some variables that will be associated
    with this class: `learningRate`, `inputShape`, `numOutputs`. Set `numOutputs`
    to `4`, as this is how many actions our AI can take. Then, in the last line, create
    an empty model. To do this, use the `Sequential` class, which we imported earlier.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`__init__`方法接收两个参数：`iS`（输入形状）和`lr`（学习率）。然后你定义了一些与这个类相关的变量：`learningRate`，`inputShape`，`numOutputs`。将`numOutputs`设置为`4`，因为这是我们的AI可以采取的动作数量。最后，在最后一行创建一个空模型。为此，使用我们之前导入的`Sequential`类。
- en: 'Doing this will allow you to add all the layers that you need to the model.
    That''s exactly what you do with these lines:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做会让你将所需的所有层添加到模型中。这正是你通过这些代码行完成的：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s break this code down into lines:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这段代码分解成几行：
- en: '**Line 21**: You add a new convolution layer to your model. It has 32 3x3 filters
    with the ReLU activation function. You need to specify the input shape here as
    well. Remember that the input shape is one of the arguments of this function,
    and is saved under the `inputShape` variable.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**第21行**：你向模型中添加了一个新的卷积层。它有32个3x3的过滤器，使用ReLU激活函数。你还需要在这里指定输入形状。记住，输入形状是这个函数的参数之一，并且保存在`inputShape`变量中。'
- en: '**Line 23**: You add a max pooling layer. The window''s size is 2x2, which
    will shrink our feature maps in size by `2`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**第23行**：你添加了一个最大池化层。窗口的大小是2x2，这将使我们的特征图的大小缩小`2`倍。'
- en: '**Line 25**: You add the second convolution layer. This time it has 64 2x2
    filters, with the same ReLU activation function. Why ReLU this time? I tried some
    other activation functions experimentally, and it turned out that for this AI
    ReLU worked the best.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**第25行**：你添加了第二个卷积层。这次它有64个2x2的过滤器，使用相同的ReLU激活函数。为什么这次使用ReLU呢？我在实验中尝试了其他一些激活函数，结果发现对于这个AI，ReLU效果最好。'
- en: '**Line 27**: Having applied convolution, you receive new feature maps, which
    you flatten to a 1D vector. That''s exactly what this line does – it flattens
    2D images to a 1D vector, which you''ll then be able to use as the input to your
    neural network.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**第27行**：在应用了卷积之后，你获得了新的特征图，并将其展平为1D向量。这正是这一行所做的——它将2D图像展平为1D向量，之后你将能够将其作为神经网络的输入。'
- en: '**Line 29**: Now, you''re in the full connection step – you''re building the
    traditional ANN. This specific line adds a new hidden layer with `256` neurons
    and the ReLU activation function to our model.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**第29行**：现在，你进入了全连接步骤——你正在构建传统的人工神经网络（ANN）。这一行代码为我们的模型添加了一个新的隐藏层，包含`256`个神经元，并使用ReLU激活函数。'
- en: '**Line 31**: You create the last layer in your neural network – the output
    layer. How big is it? Well, it has to have as many neurons as there are actions
    that you can take. You put that value under the `numOutputs` variable earlier,
    and the value is equal to `4`. You don''t specify the activation function here,
    which means that the activation function will be linear as a default. It turns
    out that in this case, during training, using a linear output works better than
    a Softmax output; it makes the training more efficient.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**第31行**：你创建了神经网络中的最后一层——输出层。它有多大呢？它必须拥有和你能采取的动作数量一样多的神经元。你在之前的`numOutputs`变量中设置了这个值，它的值为`4`。这里没有指定激活函数，这意味着激活函数默认为线性。事实证明，在这种情况下，使用线性输出比Softmax输出效果更好；它使训练更加高效。'
- en: 'You also have to `compile` your model. This will tell your code how to calculate
    the error, and which optimizer to use when training your model. You can do it
    with this single line:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要`compile`你的模型。这将告诉你的代码如何计算误差，以及在训练模型时使用哪个优化器。你可以用这一行代码来实现：
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, you use a method that's a part of the `Sequential` class (that's why you
    can use your model to call it) to do just that. The method is called `compile`
    and, in this case, takes two arguments. `loss` is a function that tells the AI
    how to calculate the error of your neural network; you'll use `mean_squared_error`.
    The second parameter is the optimizer. You've already imported the `Adam` optimizer,
    and you use it here. The learning rate for this optimizer was one of the arguments
    of the `__init__` method of this class, and its value is represented by the `learningRate`
    variable.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你使用了一个属于`Sequential`类的方法（这就是你可以用模型调用它的原因）来完成这个操作。这个方法叫做`compile`，在本例中需要两个参数。`loss`是一个函数，告诉AI如何计算神经网络的误差；你将使用`mean_squared_error`。第二个参数是优化器。你已经导入了`Adam`优化器，并在这里使用它。这个优化器的学习率是该类`__init__`方法的参数之一，值由`learningRate`变量表示。
- en: 'There''s only one step left to do in this class – make a function that will
    load a model from a file. You do it with this code:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这节课中，只剩下一步了——编写一个从文件中加载模型的函数。你可以使用以下代码来实现：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can see that you've created a new function called `loadModel`, which takes
    one argument – `filepath`. This parameter is the file path to the pre-trained
    model. Once you've defined the function, you can actually load the model from
    this file path. To do so, you use the `load_model` method, which you imported
    earlier. This function takes the same argument – `filepath`. Then in the final
    line, you return the loaded model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，你创建了一个新的函数`loadModel`，它接收一个参数——`filepath`。这个参数是预训练模型的文件路径。定义完函数后，你就可以从这个文件路径加载模型。为此，你使用了之前导入的`load_model`方法。这个函数接收相同的参数——`filepath`。最后一行，你返回加载的模型。
- en: Congratulations! You've just finished building the brain.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你刚刚完成了大脑的构建。
- en: Let's advance on our path, and build the experience replay memory.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进，构建经验重放记忆。
- en: Step 3 – Building the experience replay memory
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步 – 构建经验重放记忆
- en: You'll build this memory now, and later, you'll train your model from small
    batches of this memory. The memory will contain information about the game state
    before taking the action, the action that was taken, the reward gained, and the
    game state after performing the action.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将构建这个内存，稍后，你将从这个内存的小批量中训练你的模型。内存将包含有关游戏状态（采取行动前）、所采取的行动、获得的奖励以及执行行动后游戏状态的信息。
- en: I have some excellent news for you – do you remember this code?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我有个好消息要告诉你——你还记得这段代码吗？
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You'll use almost the same code, with only two small changes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你将几乎使用相同的代码，只是做了两个小小的改变。
- en: 'First, you get rid of this line:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，去掉这一行：
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And then change this line:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，修改这一行：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To this one:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Why did you have to do this? Well, you got rid of the first line since you no
    longer have a 1D vector of inputs. Now you have a 3D array.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你需要这么做呢？好吧，你去掉了第一行，因为你不再拥有一个1D的输入向量。现在你有一个3D数组。
- en: Then, if you look closely, you'll see that you didn't actually change `inputs`.
    Before, you had a 2D array, one dimension of which was batch size and the other
    of which was number of inputs. Now, things are very similar; the first dimension
    is once again the batch size, and the last three correspond to the size of the
    input as well!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果你仔细观察，你会发现其实你并没有改变`inputs`。之前，你有一个二维数组，其中一个维度是批量大小，另一个维度是输入的数量。现在，事情非常相似；第一个维度仍然是批量大小，最后三个维度则对应输入的大小！
- en: Since our input is now a 3D array, you wrote `.shape[1]`, `.shape[2]`, and `.shape[3]`.
    What exactly are those shapes?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的输入现在是一个3D数组，所以你写了`.shape[1]`、`.shape[2]`和`.shape[3]`。这些形状到底是什么意思？
- en: '`.shape[1]` is the number of rows in the game (in your case 10). `.shape[2]`
    is the number of columns in the game (in your case 10). `.shape[3]` is the number
    of last frames stacked onto each other (in your case 4).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`.shape[1]`是游戏中的行数（在你的例子中是10）。`.shape[2]`是游戏中的列数（在你的例子中是10）。`.shape[3]`是最后几帧叠加在一起的数量（在你的例子中是4）。'
- en: As you can see, you didn't really change anything. You just made the code work
    for our 3D inputs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，你其实并没有真正改变什么。你只是让代码适用于我们的3D输入。
- en: I also renamed this `dqn.py` file to `DQN.py` and renamed the class `DQN` to
    `Dqn`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我还将这个`dqn.py`文件重命名为`DQN.py`，并将类`DQN`重命名为`Dqn`。
- en: That's that! That was probably much simpler than most of you expected it to
    be.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这可能比你们大多数人预期的要简单得多。
- en: You can finally start training your model. We'll do that in the next section
    – training the AI.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你终于可以开始训练你的模型了。我们将在下一节中进行训练AI。
- en: Step 4 – Training the AI
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4步 – 训练AI
- en: This is, by far, the most important step. Here we finally teach our AI to play
    Snake!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迄今为止最重要的一步。在这里，我们终于教会了AI玩蛇游戏！
- en: 'As always, start by importing the libraries you need:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，首先导入你需要的库：
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the first three lines you import the tools that you created earlier, including
    the `Brain`, the `Environment`, and the experience replay memory.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在前三行中，你导入了之前创建的工具，包括`Brain`、`Environment`和经验回放内存。
- en: Then, in the following two lines, you import the libraries that you'll use.
    These include NumPy and Matplotlib. You'll already recognize the former; the latter
    will be used to display your model's performance. To be specific, it will help
    you display a graph that, every 100 games, will show you the average number of
    apples collected.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在接下来的两行中，你导入了你将使用的库。这些包括NumPy和Matplotlib。你已经会识别前者；后者将用于展示你模型的表现。具体来说，它将帮助你展示一个图表，每100局游戏，会显示你收集到的苹果的平均数量。
- en: 'That''s all for this step. Now, define some hyperparameters for your code:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这一步的全部内容。现在，定义一些代码的超参数：
- en: '[PRE21]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'I''ll explain them in this list:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在这个列表中解释它们：
- en: '`memSize` – The maximum size of your experience replay memory.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`memSize` – 你的经验回放内存的最大大小。'
- en: '`batchSize` – The size of the batch of inputs and targets that you get at each
    iteration from your experience replay memory for your model to train on.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batchSize` – 每次迭代中，从经验回放内存中获取的输入和目标的批量大小，用于模型训练。'
- en: '`learningRate` – The learning rate for your `Adam` optimizer in the `Brain`.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learningRate` – 你在`Brain`中使用的`Adam`优化器的学习率。'
- en: '`gamma` – The discount factor for your experience replay memory.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gamma` – 你的经验回放内存的折扣因子。'
- en: '`nLastStates` – How many last frames you save as your current state of the
    game. Remember, you''ll input a 3D array of size `nRows` x `nColumns` x `nLastStates`
    to your CNN in the `Brain`.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nLastStates` – 你保存作为当前游戏状态的最后几帧。记住，你会将一个大小为`nRows` x `nColumns` x `nLastStates`的3D数组输入到你的CNN中，作为`Brain`的一部分。'
- en: '`epsilon` – The initial epsilon, the chance of taking a random action.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`epsilon` – 初始epsilon，即采取随机动作的机会。'
- en: '`epsilonDecayRate` – By how much you decrease `epsilon` after every single game/epoch.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`epsilonDecayRate` – 每场/轮游戏后你减少`epsilon`的比率。'
- en: '`minEpsilon` – The lowest possible epsilon, after which it can''t be adjusted
    any lower.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`minEpsilon` – 最低可能的epsilon值，之后不能再调低。'
- en: '`filepathToSave` – Where you want to save your model.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filepathToSave` – 你希望保存模型的位置。'
- en: 'There you go – you''ve defined the hyperparameters. You''ll use them later
    when you write the rest of the code. Now, you have to create an environment, a
    brain, and an experience replay memory:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样 - 你已经定义了超参数。稍后在编写其余代码时将使用它们。现在，你需要创建一个环境、一个大脑和一个经验重播内存：
- en: '[PRE22]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can see that in the first line you create an object of the `Environment`
    class. You need to specify one variable here, which is the slowdown of your environment
    (wait time between moves). You don't want any slowdown during the training, so
    you input `0` here.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在第一行中，你创建了`Environment`类的一个对象。你需要在这里指定一个变量，即你的环境减慢（移动之间的等待时间）。你在训练期间不想有任何减速，所以在这里输入`0`。
- en: In the next line you create an object of the `Brain` class. It takes two arguments
    – the input shape and the learning rate. As I've mentioned multiple times, the
    input shape will be a 3D array of size `nRows` x `nColumns` x `nLastStates`, so
    that's what you type in here. The second argument is the learning rate, and since
    you've created a variable for that, you simply input the name of this variable
    – `learningRate`. After this line you take the model of this `Brain` class and
    create an instance of this model in your code. Keep things simple, and call it
    `model`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你创建了`Brain`类的一个对象。它接受两个参数 - 输入形状和学习率。正如我多次提到的，输入形状将是一个大小为`nRows` x `nColumns`
    x `nLastStates`的3D数组，所以你在这里输入这些内容。第二个参数是学习率，由于你已经为此创建了一个变量，所以你简单地输入这个变量的名称 - `learningRate`。在这一行之后，你取得了这个`Brain`类的模型，并在你的代码中创建了这个模型的一个实例。保持简单，称其为`model`。
- en: In the last line you create an object of the `Dqn` class. It takes two arguments
    – the maximum size of the memory, and the discount factor for the memory. You've
    specified two variables, `memSize` and `gamma`, for just that, so you use them
    here.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一行中，你创建了`Dqn`类的一个对象。它接受两个参数 - 内存的最大大小和内存的折扣因子。你为此指定了两个变量，`memSize`和`gamma`，所以在这里使用它们。
- en: 'Now, you need to write a function that will reset the states for your AI. You
    need it because the states are quite complicated, and resetting them in the main
    code would mess it up a lot. Here''s what it looks like:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要编写一个函数，重置你的AI的状态。你需要它是因为状态相当复杂，在主代码中重置它们会造成很大的混乱。这是它的样子：
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s break it down into separate lines:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把它分成单独的行：
- en: '**Line 31**: You define a new function called `resetStates`. It doesn''t take
    any arguments.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**第31行**：你定义了一个名为`resetStates`的新函数。它不接受任何参数。'
- en: '**Line 32**: You create a new array called `currentState`. It''s full of zeros,
    but you may ask why it''s 4D; shouldn''t the input be 3D as we said? You''re absolutely
    right, and it will be. The first dimension is called batch size and simply says
    how many inputs you input to your neural network at once. You''ll only input one
    array at a time, so the first size is `1`. The next three sizes correspond to
    the size of the input.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**第32行**：你创建了一个名为`currentState`的新数组。它充满了零，但你可能会问为什么是4D的；我们不是说输入应该是3D吗？你完全正确，确实应该是。第一个维度称为批处理大小，简单地表示你一次向神经网络输入多少个输入。你每次只输入一个数组，因此第一个大小是`1`。接下来的三个大小对应输入的大小。'
- en: '**Lines 34-35**: In a `for` loop, which will be executed `nLastStates` times,
    you set the board for each layer in your 3D state to the current, initial look
    of the game board from your environment. Every frame in your state will look the
    same initially, the same way the board of the game looks when you start a game.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**第34-35行**：在一个`for`循环中，将执行`nLastStates`次，你将每个3D状态的层的板子设置为来自环境的当前游戏板的初始外观。每个状态中的每一帧最初看起来都一样，就像开始游戏时游戏板的样子一样。'
- en: '**Line 37**: This function will return two `currentStates`. Why? This is because
    you need two game state arrays. One to represent the board before you''ve taken
    an action, and one to represent the board after you''ve taken an action.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**第37行**：这个函数将返回两个`currentStates`。为什么？因为你需要两个游戏状态数组。一个表示在你执行动作之前的板子状态，另一个表示在你执行动作之后的板子状态。'
- en: 'Now you can start writing the code for the entire training. First, create a
    couple of useful variables, like this:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以开始编写整个训练的代码了。首先，创建几个有用的变量，如下所示：
- en: '[PRE24]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`epoch` will tell you which epoch/game you''re in right now. `scores` is a
    list in which you save the average scores per game after every 100 games/epochs.
    `maxNCollected` tells you the highest score obtained so far in the training, while
    `nCollected` is the score in each game/epoch. The last variable, `totNCollected`,
    tells you how many apples you''ve collected over 100 epochs/games.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`epoch`会告诉你现在处于哪个epoch/游戏。`scores`是一个列表，你在每100局游戏/epoch后保存每局游戏的平均分。`maxNCollected`告诉你训练过程中获得的最高分，而`nCollected`是每局游戏/epoch的分数。最后一个变量`totNCollected`告诉你在100个epoch/游戏中收集了多少苹果。'
- en: 'Now you start an important, infinite `while` loop, like this:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你开始一个重要的、无限的`while`循环，如下所示：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, you iterate through every game, every epoch. That's why you restart the
    environment in the first line, create new `currentState` and `nextState` in the
    next line, increase `epoch` by one, and set `gameOver` to `False` as you obviously
    haven't lost yet.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你遍历每一局游戏，每个epoch。这就是为什么你在第一行重置环境，在接下来的行中创建新的`currentState`和`nextState`，将`epoch`加一，并将`gameOver`设置为`False`，因为显然你还没有输掉游戏。
- en: Note that this loop doesn't end; therefore, the training never stops. We do
    it this way because we don't have a set goal for when to stop the training, since
    we haven't defined what a satisfactory result for our AI would be. We could calculate
    the average result, or a similar metric, but then training might take too long.
    I prefer to keep the training going and you can just stop the training whenever
    you want. A good time to stop is when the AI reaches an average of six apples
    per game, or you can even go up to 12 apples per game if you want better performance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个循环不会结束；因此，训练永远不会停止。我们这样做是因为我们没有设定何时停止训练的目标，因为我们还没有定义什么是我们AI的满意结果。我们可以计算平均结果或类似的指标，但那样训练可能会花费太长时间。我更倾向于让训练继续进行，你可以在任何时候停止训练。一个合适的停止时机是当AI每局游戏收集到平均六个苹果时，或者如果你想要更好的表现，你甚至可以设定每局游戏收集12个苹果。
- en: 'You''ve started the first loop that will iterate through every epoch. Now you
    need to create the second loop, where the AI performs actions, updates the environment,
    and trains your CNN. Start it with these lines:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经开始了第一个循环，它将遍历每个epoch。现在你需要创建第二个循环，在其中AI执行动作、更新环境并训练你的CNN。从以下几行代码开始：
- en: '[PRE26]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As I mentioned, this is the loop in which your AI makes decisions, moves, and
    updates the environment. You start off by initializing a `while` loop that will
    be executed as long as you haven't lost; that is, as long as `gameOver` is set
    to `False`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，这是你的AI做出决策、移动并更新环境的循环。你首先初始化一个`while`循环，只要你没有输掉游戏，也就是`gameOver`被设置为`False`，这个循环就会执行。
- en: Then, you can see `if` conditions. This is where your AI will make decisions.
    If a random value from range (0,1) is lower than the epsilon, then a random action
    will be performed. Otherwise, you predict the Q-values based on the current state
    of the game and from these Q-values you take the index with the highest Q-value.
    This will be the action performed by your AI.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以看到`if`条件。这是你的AI做出决策的地方。如果从范围（0,1）中随机选出的值小于epsilon，那么将执行一个随机动作。否则，你会根据当前游戏状态预测Q值，并从这些Q值中选择最大的Q值对应的索引。这将是你的AI执行的动作。
- en: 'Then, you have to update your environment:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要更新环境：
- en: '[PRE27]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You use the `step` method from your `Environment` class object. It takes one
    argument, which is the action that you perform. It also returns the new frame
    obtained from your game after performing this action along with the reward obtained
    and the game over information. You'll use these variables soon.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用的是`Environment`类对象的`step`方法。它接受一个参数，即你执行的动作。它还会返回执行这个动作后从游戏中获得的新帧，以及获得的奖励和游戏结束信息。你很快就会使用到这些变量。
- en: 'Keep in mind, that this method returns a single 2D frame from your game. This
    means that you have to add this new frame to your `nextState` and remove the last one.
    You do this with these lines:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个方法返回的是你游戏中的单一2D帧。这意味着你必须将这个新帧添加到`nextState`中，并删除最后一个帧。你可以用以下几行代码实现：
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you can see, first you reshape `state` because it is 2D, while both `currentState`
    and `nextState` are 4D. Then you add this new, reshaped frame to `nextState` along
    the 3rd axis. Why 3rd? That's because the 3rd index refers to the 4th dimension
    of this array, which keeps the 2D frames inside. In the last line you simply delete
    the first frame from `nextState`, which has index 0 (the oldest frames are kept
    on the lowest indexes).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，首先你重新塑形了`state`，因为它是二维的，而`currentState`和`nextState`是四维的。然后，你将这个新的、重新塑形后的帧添加到`nextState`的第3维。为什么是第3维呢？因为第3维对应于数组的第四个维度，它保存了二维帧。在最后一行，你简单地删除了`nextState`中的第一帧（索引为0），保留了最旧的帧在最低的索引位置。
- en: 'Now, you can `remember` this transition in your experience replay memory, and
    train your model from a random batch of this memory. You do that with these lines:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将这个转换`remember`到经验回放记忆中，并从这个记忆中随机抽取一批进行训练。你可以用这些代码行来实现：
- en: '[PRE29]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the first line, you append this transition to the memory. It contains information
    about the game state before taking the action (`currentState`), the action that
    was taken (`action`), the reward gained (`reward`), and the game state after taking
    this action (`nextState`). You also remember the `gameOver` status. In the following
    two lines, you take a random batch of inputs and targets from your memory, and
    train your model on them.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，你将这个转换添加到记忆中。它包含了执行动作前的游戏状态（`currentState`）、所采取的动作（`action`）、获得的奖励（`reward`）以及执行该动作后的游戏状态（`nextState`）。你还记住了`gameOver`状态。在接下来的两行中，你从记忆中随机选择一批输入和目标，并用它们来训练你的模型。
- en: 'Having done that, you can check if your snake has collected an apple and update
    `currentState`. You can do that with these lines:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一步后，你可以检查你的蛇是否收集到苹果，并更新`currentState`。你可以用以下代码行来实现：
- en: '[PRE30]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the first two lines, you check whether the snake has collected an apple and
    if it has, you increase `nCollected`. Then you update `currentState` by setting
    its values to the ones of `nextState`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两行中，你检查蛇是否收集到了苹果，如果收集到了，你就增加`nCollected`。然后你通过将`nextState`的值赋给`currentState`来更新`currentState`。
- en: 'Now, you can quit this loop. You still have a couple of things to do:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以退出这个循环了。你还需要做几件事：
- en: '[PRE31]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You check if you've beaten the record for the number of apples eaten in a round
    (this number has to be bigger than 2) and if you did, you update the record and
    save your current model to the file path you specified before. You also increase
    `totNCollected` and reset `nCollected` to 0 for the next game.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你检查自己是否打破了单局游戏吃苹果的记录（这个数字必须大于2），如果是，你就更新记录，并将当前模型保存到之前指定的文件路径中。你还增加了`totNCollected`并将`nCollected`重置为0，准备迎接下一局游戏。
- en: 'Then, after 100 games, you show the average score, like this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在100局游戏后，你展示平均分数，如下所示：
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You have a list called `scores`, where you store the average score after 100
    games. You append a new value to it and then reset this value. Then you show `scores`
    on a graph, using the Matplotlib library that you imported before. This graph
    is saved in `stats.png` every 100 games/epochs.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个名为`scores`的列表，用来存储100局游戏后的平均分数。你将一个新的值添加到列表中，然后重置这个值。接着，你用之前导入的Matplotlib库在图表中展示`scores`。这个图表每进行100局游戏/迭代后会保存在`stats.png`中。
- en: 'Then you lower the epsilon, like so:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你降低epsilon值，像这样：
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With the `if` condition, you make sure that the epsilon doesn't go lower than
    the minimum threshold.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`if`条件，你确保epsilon不会低于最小阈值。
- en: 'In the last line, you display some additional information about every single
    game, like this:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一行，你显示关于每一局游戏的额外信息，如下所示：
- en: '[PRE34]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You display the current epoch (game), the current record for the number of apples
    collected in one game, and the current epsilon.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你展示当前的轮次（游戏），当前单局游戏收集苹果的记录，以及当前的epsilon值。
- en: That's it! Congratulations! You've just built a function that will train your
    model. Remember that this training goes on infinitely until you decide it's finished.
    When you're satisfied with it, you'll want to test it. For that, you need a short
    file to test your model. Let's do it!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！恭喜你！你刚刚构建了一个训练你模型的函数。记住，这个训练会无限进行，直到你决定它完成为止。当你满意后，你就可以进行测试了。为了测试，你需要一个简单的文件来验证你的模型。让我们来做吧！
- en: Step 5 – Testing the AI
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5步 – 测试AI
- en: This will be a very short section, so don't worry. You'll be running this code
    in just a moment!
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个非常简短的部分，所以不用担心。你很快就会运行这段代码！
- en: 'As always, you start by importing the libraries you need:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，你首先导入所需的库：
- en: '[PRE35]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This time you won't be using the DQN memory nor the Matplotlib library, and therefore
    you don't import them.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这次你不会使用DQN记忆也不会使用Matplotlib库，因此不需要导入它们。
- en: 'You also need to specify some hyperparameters, like this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要指定一些超参数，像这样：
- en: '[PRE36]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You'll need `nLastStates` later in this code. You also created a file path to
    the model that you'll test. Finally, there's also a variable that you'll use to
    specify the wait time after every move, so that you can clearly see how your AI
    performs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你稍后在代码中会用到`nLastStates`。你还创建了一个文件路径来测试你的模型。最后，还有一个变量，用来指定每次移动后的等待时间，以便你能清晰地看到AI的表现。
- en: 'Once again, you create some useful objects, like an `Environment` and a `Brain`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你创建了一些有用的对象，比如`Environment`和`Brain`：
- en: '[PRE37]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Into the brackets of the `Environment`, you input the `slowdown`, because that's
    the argument that this class takes. You also create an object of the `Brain` class,
    but this time, you don't specify the learning rate, since you won't be training
    your model. In the final line you load a pre-trained model using the `loadModel`
    method from the `Brain` class. This method takes one argument, which is the file
    path from which you load the model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Environment`的括号里，你输入`slowdown`，因为这是这个类所需要的参数。你还创建了一个`Brain`类的对象，但这次你没有指定学习率，因为你不会训练模型。在最后一行，你使用`Brain`类的`loadModel`方法加载一个预训练模型。这个方法需要一个参数，也就是加载模型的文件路径。
- en: 'Once again, you need a function to reset states. You can use the same one as
    before, so just copy and paste these lines:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你需要一个函数来重置状态。你可以使用之前的那个，所以直接复制并粘贴这些行：
- en: '[PRE38]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, you can enter the main `while` loop like before. This time, however, you
    won''t define any variables, since you don''t need any:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以像以前一样进入主`while`循环。不过这次，你不需要定义任何变量，因为你不需要它们：
- en: '[PRE39]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you can see, you've started this infinite `while` loop. Once again, you have
    to restart the environment, the states, and the game over, every iteration.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，你已经开始了这个无限的`while`循环。再次提醒，你每次都需要重启环境、状态和游戏结束判定。
- en: 'Now, you can enter the game''s `while` loop, where you take actions, update
    the environment, and so on:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以进入游戏的`while`循环，执行动作、更新环境等等：
- en: '[PRE40]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This time, you don't need any `if` statements. After all, you're testing your
    AI, so you mustn't have any random actions here.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，你不需要任何`if`语句。毕竟，你是在测试AI，所以这里不能有任何随机行为。
- en: 'Once again, you update the environment:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你需要更新环境：
- en: '[PRE41]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You don't really care about the reward, so just place "`_`" instead of `reward`.
    The environment still returns the frame after taking an action, along with the
    information about game over.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你并不关心奖励，所以只需用"`_`"代替`reward`。环境在执行动作后仍然返回帧，并且提供关于游戏是否结束的信息。
- en: 'Due to this fact, you need to reshape your `state` and update `nextState` in
    the same way as before:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，你需要以与之前相同的方式重塑`state`并更新`nextState`：
- en: '[PRE42]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In the final line, you need to update `currentState` as you did in the other
    file:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一行，你需要像在另一个文件中一样更新`currentState`：
- en: '[PRE43]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: That's the end of coding for this section! This isn't, however, the end of this
    chapter. You still have to run the code.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分的编码到此为止！然而，这并不是本章的结束。你仍然需要运行代码。
- en: The demo
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示
- en: Unfortunately, due to PyGame not being supported by Google Colab, you'll need
    to use Anaconda.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于Google Colab不支持PyGame，你需要使用Anaconda。
- en: Thankfully, you should have it installed after *Chapter 10*, *AI for Autonomous
    Vehicles – Build a Self-Driving Car*, so it'll be easier to install the required
    packages and libraries.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你，你应该在*第10章*，*自动驾驶汽车的AI – 构建一辆自驾车*后完成安装，所以安装所需的包和库会更加容易。
- en: Installation
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: First, create a new virtual environment inside Anaconda. This time, I'll walk
    you through the installation on the Anaconda Prompt from a PC, so that you can
    all see how it's done from any system.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在Anaconda中创建一个新的虚拟环境。这次，我将通过PC上的Anaconda Prompt演示安装过程，这样你们可以在任何系统上看到如何操作。
- en: 'Windows users, please open the Anaconda Prompt on your PC, and Mac/Linux users,
    please open your Terminal on Mac/Linux. Then type:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Windows用户，请在PC上打开Anaconda Prompt，Mac/Linux用户，请在Mac/Linux上打开终端。然后输入：
- en: '[PRE44]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Just like so:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样：
- en: '![](img/B14110_13_05.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_05.png)'
- en: 'Then, hit `Enter` on your keyboard. You should get something more or less like
    this:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按下键盘上的`Enter`键。你应该会得到如下内容：
- en: '![](img/B14110_13_06.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_06.png)'
- en: 'Type `y` on your keyboard and hit `Enter` once again. After everything gets
    installed, type this in your Anaconda Prompt:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在键盘上输入`y`并再次按下`Enter`键。安装完成后，在你的Anaconda提示符中输入以下命令：
- en: '[PRE45]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/B14110_13_07.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_07.png)'
- en: And hit `Enter` once again. Now on the left, you should see **snake** written
    instead of **base**. This means that you're in the newly created Anaconda environment.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次按下`Enter`键。现在在左侧，你应该看到**snake**而不是**base**。这意味着你已经进入了新创建的Anaconda环境。
- en: 'Now you need to install the required libraries. The first one is Keras:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要安装所需的库。第一个是Keras：
- en: '[PRE46]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](img/B14110_13_08.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_08.png)'
- en: 'After writing that, hit `Enter`. When you get this:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 写完之后，按下`Enter`键。当你看到这个：
- en: '![](img/B14110_13_09.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_09.png)'
- en: Type `y` once again and hit `Enter` once again. Once you have it installed,
    you need to install PyGame and Matplotlib.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 再次输入`y`并按下`Enter`键。一旦安装完成，你需要安装PyGame和Matplotlib。
- en: The first one can be installed by entering `pip install pygame`, while the second
    one can be installed by entering `pip install matplotlib`. The installation follows
    the same procedure as you just took to install Keras.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个可以通过输入`pip install pygame`来安装，而第二个可以通过输入`pip install matplotlib`来安装。安装过程与安装Keras时的步骤相同。
- en: Ok, now you can run your code!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在你可以运行你的代码了！
- en: 'If you''ve accidentally closed your Anaconda Prompt/Terminal for any reason,
    re-open it and type in this to activate the `snake` environment that we have just
    created:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不小心关闭了你的Anaconda提示符/终端，重新打开它并输入以下命令来激活我们刚刚创建的`snake`环境：
- en: '[PRE47]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/B14110_13_10.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_10.png)'
- en: 'And then hit `Enter`. I got a bunch of warnings after doing this, and you may
    see similar warnings as well, but don''t worry about them:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按下`Enter`键。我在做这一步之后收到了很多警告，你也可能会看到类似的警告，但不用担心：
- en: '![](img/B14110_13_11.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_11.png)'
- en: Now, you need to navigate this console to the folder that contains the file
    you want to run, in this case `train.py`. I recommend that you put all the code
    of `Chapter 13` in one folder called `Snake` on your desktop. Then you'll be able
    to follow the exact instructions that I'll give you now. To navigate to this folder,
    you'll need to use `cd` commands.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要将此终端导航到包含你想运行的文件的文件夹，在本例中是`train.py`。我建议你将`第13章`的所有代码放在一个名为`Snake`的文件夹中，并放在桌面上。然后，你就能跟着我给出的确切指示来操作了。要导航到该文件夹，你需要使用`cd`命令。
- en: 'First, navigate to the desktop by running `cd Desktop`, like this:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过运行`cd Desktop`来导航到桌面，像这样：
- en: '![](img/B14110_13_12.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_12.png)'
- en: 'And then enter the `Snake` folder that you created. Just as with the previous
    command, run `cd Snake`, like this:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然后进入你创建的`Snake`文件夹。就像之前的命令一样，运行`cd Snake`，像这样：
- en: '![](img/B14110_13_13.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_13.png)'
- en: 'You''re getting super close. To train a new model, you need to type:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经快到了。要训练一个新的模型，你需要输入：
- en: '[PRE48]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/B14110_13_14.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_14.png)'
- en: 'And hit `Enter`. This is more or less what you should see:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按下`Enter`键。这就是你应该看到的内容：
- en: '![](img/B14110_13_15.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_15.png)'
- en: You have both a window on the left with the game, and one on the right with
    the terminal informing you about every game (every epoch).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个左侧显示游戏窗口，一个右侧显示终端，告诉你每场游戏的情况（每个epoch）。
- en: Congratulations! You just smashed the code of this chapter and built an AI for
    Snake. Be patient with it though! Training it may take up a couple of hours.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚完成了本章的代码并为Snake游戏创建了一个AI。尽管如此，请耐心等待！训练可能需要几个小时。
- en: So, what kind of results can you expect?
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你可以期待什么样的结果呢？
- en: The results
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果
- en: Firstly, make sure to follow the results also on your Anaconda Prompt/Terminal,
    epoch by epoch. An epoch is one game played. After thousands of games (epochs),
    you'll see the score increase, as well as the snake size increase.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保在Anaconda提示符/终端中逐epoch地跟踪结果。一个epoch就是玩一次游戏。经过成千上万的游戏（epoch）后，你会看到分数和蛇的体积增加。
- en: After thousands of epochs of training, while the snake doesn't fill in the entire
    map, your AI plays on a level comparable with humans. Here are some pictures after
    25,000 epochs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了数千个epoch的训练后，尽管蛇并没有填满整个地图，但你的AI在与人类相当的水平上进行游戏。这是25,000个epoch后的截图。
- en: '![](img/B14110_13_16.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_16.png)'
- en: 'Figure 5: Results example 1'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：结果示例1
- en: '![](img/B14110_13_17.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_17.png)'
- en: 'Figure 6: Results example 2'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：结果示例2
- en: 'You''ll also get a graph created in the folder (`stats.png`) showing the average
    score over the epochs. Here is the graph I got when training our AI over 25,000
    epochs:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会得到一个在文件夹中创建的图表（`stats.png`），显示了每个 epoch 的平均得分。这是我在训练我们的 AI 25,000 次后得到的图表：
- en: '![](img/B14110_13_18.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_13_18.png)'
- en: 'Figure 7: Average score over 25,000 epochs'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：超过 25,000 次训练的平均得分
- en: You can see that our AI reached an average score of 10-11 per game. This isn't
    bad considering that before training it knew absolutely nothing about the game.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们的 AI 达到了每局 10-11 的平均得分。考虑到在训练之前它对游戏一无所知，这个成绩还不错。
- en: 'You can also see the same results if you run the `test.py` file using the pre-trained
    model `model.h5` attached to this chapter in GitHub. To do this, you simply need
    to enter in your Anaconda Prompt/Terminal (still in the same `Snake` folder on
    your desktop that contains all the code of `Chapter 13`, and still inside the
    `snake` virtual environment):'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用本章附带的预训练模型 `model.h5` 运行 `test.py` 文件，你也能看到相同的结果。为了做到这一点，你只需要在 Anaconda
    Prompt/Terminal 中输入（仍然在桌面上包含 `第 13 章` 所有代码的同一个 `Snake` 文件夹内，并且仍然在 `snake` 虚拟环境中）：
- en: '[PRE49]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: If you want to test your model after training, you simply need to replace `model.h5`
    with `model2.h5` in the `test.py` file. That's because during the training the
    weights of your AI's neural network will be saved into a file named `model2.h5`.
    Then re-enter `python test.py` in your Anaconda Prompt/Terminal, and enjoy your
    own results.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在训练后测试你的模型，只需要在 `test.py` 文件中将 `model.h5` 替换为 `model2.h5`。这是因为在训练过程中，你的
    AI 神经网络的权重会保存到名为 `model2.h5` 的文件中。然后在你的 Anaconda Prompt/Terminal 中重新输入 `python
    test.py`，就可以欣赏到你自己的结果了。
- en: Summary
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this last practical chapter of the book, we built a deep convolutional Q-Learning
    model for Snake. Before we built anything, we had to define what our AI would
    see. We established that we needed to stack multiple frames, so that our AI would
    see the continuity of its moves. This was the input to our Convolutional Neural
    Network. The outputs were the Q-values corresponding to each of the four possible
    moves: going up, going down, going left, and going right. We rewarded our AI for
    eating an apple, punished it for losing, and punished it slightly for performing
    any action (the living penalty). Having run 25,000 games, we can see that our
    AI is able to eat 10-11 apples per game.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章实践部分，我们为贪吃蛇构建了一个深度卷积 Q 学习模型。在我们开始构建之前，必须先定义我们的 AI 能看到什么。我们确定需要堆叠多个帧，以便
    AI 能看到其动作的连续性。这是我们卷积神经网络的输入。输出是对应四个可能动作的 Q 值：向上、向下、向左和向右。我们奖励 AI 吃到苹果，惩罚它失败，稍微惩罚它执行任何动作（生存惩罚）。经过
    25,000 局游戏后，我们可以看到我们的 AI 每局能吃到 10-11 个苹果。
- en: I hope you enjoyed it!
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这个内容！
