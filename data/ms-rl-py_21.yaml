- en: '*Chapter 17*: Smart City and Cybersecurity'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第17章*：智能城市与网络安全'
- en: 'Smart cities are expected to be one of the defining experiences of the next
    decades. A smart city collects a lot of data using sensors located in various
    parts of the city, such as on the roads, utility infrastructures, and water resources.
    The data is then used to make data-driven and automated decisions, such as how
    to allocate the city''s resources, manage traffic in real time, and identify and
    mitigate infrastructure problems. This prospect comes with two challenges: how
    to program the automation and how to protect the highly connected city assets
    from cyberattacks. Fortunately, **reinforcement learning** (**RL**) can help with
    both.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 智能城市预计将在未来几十年成为定义性体验之一。智能城市通过位于城市各个区域的传感器收集大量数据，如道路、公共设施基础设施和水资源。然后，这些数据用于做出基于数据和自动化的决策，如如何分配城市资源、实时管理交通以及识别和减轻基础设施问题。这个前景带来了两个挑战：如何编程自动化以及如何保护高度互联的城市资产免受网络攻击。幸运的是，**强化学习**（**RL**）可以帮助解决这两个问题。
- en: In this chapter, we will cover three problems related to smart cities and cybersecurity
    and describe how to model them as RL problems. Along the way, we will introduce
    you to the Flow library, a framework that connects traffic simulation software
    with RL libraries, and use it to solve an example traffic light control problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨与智能城市和网络安全相关的三个问题，并描述如何将其建模为强化学习（RL）问题。在这个过程中，我们将向你介绍Flow库，这是一个将交通仿真软件与RL库连接的框架，并使用它解决一个交通信号灯控制问题。
- en: 'In particular, here are the problems we will address in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将在本章中解决以下问题：
- en: Traffic light control to optimize vehicle flow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过交通信号灯控制优化车辆流量
- en: Providing an ancillary service to a power grid
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为电网提供辅助服务
- en: Detecting cyberattacks in a smart grid
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测智能电网中的网络攻击
- en: This will be a fun ride, so let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一次有趣的旅程，让我们开始吧！
- en: Traffic light control to optimize vehicle flow
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过交通信号灯控制优化车辆流量
- en: 'One of the key challenges of a smart city is optimizing traffic flows on road
    networks. There are numerous benefits in reducing traffic congestions, such as
    the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 智能城市的一个关键挑战是优化道路网络上的交通流量。减少交通拥堵有许多好处，如下所示：
- en: Reducing the time and energy wasted in traffic
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少交通中浪费的时间和能源
- en: Saving on gas and resulting exhaust emissions
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节省燃油和减少排放
- en: Increasing vehicle and road lifetime
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延长车辆和道路使用寿命
- en: Decreasing the number of accidents
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低事故发生率
- en: There has already been a lot of research gone into this area; but recently,
    RL has emerged as a competitive alternative to traditional control approaches.
    So, in this section, we will optimize the traffic flow on a road network by controlling
    the traffic light behavior using multi-agent RL. To this end, we will use the
    Flow framework, which is an open source library for RL, and run experiments on
    realistic traffic microsimulations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域已经进行了大量研究；但最近，强化学习（RL）已成为与传统控制方法竞争的替代方案。因此，在本节中，我们将通过使用多智能体强化学习（RL）控制交通信号灯行为来优化道路网络上的交通流量。为此，我们将使用Flow框架，它是一个开源的强化学习（RL）库，并在现实交通微观仿真中进行实验。
- en: Introducing Flow
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Flow
- en: Transportation research significantly relies on simulation software, such as
    **SUMO** (**Simulation** **of** **Urban** **Mobility**) and Aimsun, for areas
    such as traffic light control, vehicle route choice, traffic surveillance, and
    traffic forecast, which involves the optimal control of these agents. On the other
    side, the rise of deep RL as an alternative to traditional control approaches
    has led to the creation of numerous libraries, such as RLlib and OpenAI Baselines.
    Flow is an open source framework that connects these two worlds of traffic simulators
    and RL libraries.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 交通研究在很大程度上依赖于仿真软件，如**SUMO**（**城市交通模拟**）和Aimsun，用于交通信号灯控制、车辆路线选择、交通监控和交通预测等领域，这些都涉及到这些智能体的最优控制。另一方面，深度强化学习作为传统控制方法的替代方案的兴起，催生了许多库，如RLlib和OpenAI
    Baselines。Flow是一个开源框架，连接了交通仿真器和强化学习库这两个领域。
- en: In this section, as in previous chapters, we will use RLlib as the RL backend.
    For traffic simulation, we will use SUMO, a powerful open source library that
    has been developed since the early 2000s.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，与之前的章节一样，我们将使用RLlib作为强化学习后端。对于交通仿真，我们将使用SUMO，这是一个强大的开源库，自2000年代初期以来一直在开发。
- en: Info
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'Here, we will give only a glimpse into applying RL to traffic problems. Detailed
    documentation and tutorials (which we closely follow here) are available on the
    Flow website: [https://flow-project.github.io/](https://flow-project.github.io/).
    SUMO documentation and libraries are available at [https://www.eclipse.org/sumo/](https://www.eclipse.org/sumo/).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将仅简单介绍如何将强化学习应用于交通问题。详细的文档和教程（我们在这里严格遵循的）可以在 Flow 网站上找到：[https://flow-project.github.io/](https://flow-project.github.io/)。SUMO
    的文档和库可以在 [https://www.eclipse.org/sumo/](https://www.eclipse.org/sumo/) 获得。
- en: Let's start by installing Flow and SUMO.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装 Flow 和 SUMO 开始。
- en: Installing Flow and SUMO
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Flow 和 SUMO
- en: 'In order to install Flow, we need to create a new virtual environment since
    it depends on library versions different than what we used in earlier chapters:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装 Flow，我们需要创建一个新的虚拟环境，因为它依赖的库版本与我们在前面章节中使用的不同：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To install Flow, we need to download the repo and run the following commands:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Flow，我们需要下载仓库并运行以下命令：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'These commands install the necessary dependencies, including TensorFlow and
    RLlib. The last two commands are needed to run Flow on Jupyter Notebook, which
    is what the Flow tutorials, as well as our example code, are on. To install SUMO
    on Ubuntu 18.04, use the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令安装了必要的依赖项，包括 TensorFlow 和 RLlib。最后两个命令用于在 Jupyter Notebook 上运行 Flow，这也是
    Flow 教程以及我们的示例代码所依赖的环境。要在 Ubuntu 18.04 上安装 SUMO，请使用以下命令：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Setup scripts for earlier Ubuntu versions and macOS are also available in the
    same folder.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 针对早期 Ubuntu 版本和 macOS 的设置脚本也可以在同一文件夹中找到。
- en: 'You can see Flow and SUMO in action by running the following (in the Flow folder
    and with your virtual environment activated):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来查看 Flow 和 SUMO 的实际操作（在 Flow 文件夹中，并且虚拟环境已激活）：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A window similar to the one shown in *Figure 17.1* should appear:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 应该会出现一个类似于*图17.1*中所示的窗口：
- en: '![Figure 17.1 – A sample SUMO window simulating the traffic on a ring road'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.1 – 一个示例的SUMO窗口，模拟环路上的交通情况'
- en: '](img/B14160_17_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_17_01.jpg)'
- en: Figure 17.1 – A sample SUMO window simulating the traffic on a ring road
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1 – 一个示例的SUMO窗口，模拟环路上的交通情况
- en: If you run into issues with the setup, the Flow documentation can help you with
    troubleshooting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在设置过程中遇到问题，Flow 文档可以帮助你进行故障排除。
- en: Now that we have set things up, let's dive into how to put together a traffic
    environment using Flow.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好环境，接下来让我们深入了解如何使用 Flow 构建交通环境。
- en: Creating an experiment in Flow
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Flow 中创建实验
- en: Now that we have the setup, we can create environments and experiments in Flow.
    We will then connect them to RLlib to train RL agents.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了设置，可以在 Flow 中创建环境和实验。接下来，我们将它们与 RLlib 连接，以训练 RL 代理。
- en: 'There are certain ingredients that go into a Flow experiment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Flow 实验中有几个关键元素：
- en: '**A road network**, such as a ring road (as in *Figure 17.1*), or a Manhattan-like
    grid network. Flow comes with a set of predefined networks. For advanced users,
    it also allows creating custom networks. Traffic lights are defined together with
    the road network.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个道路网络**，比如一个环形道路（如*图17.1*所示），或类似曼哈顿的网格网络。Flow 提供了一套预定义的网络。对于高级用户，它还允许创建自定义网络。交通信号灯与道路网络一起定义。'
- en: '**A simulation backend**, which is not our focus here. We will use the defaults
    for this.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个仿真后端**，这不是我们关注的重点，我们将使用默认设置。'
- en: '**An RL environment** that configures what is controlled, observed, and rewarded
    in the experiment, similar to a Gym environment.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个 RL 环境**，它配置了实验中控制、观察和奖励的内容，类似于 Gym 环境。'
- en: '**Vehicles** are essential to the entire experiment, and their behavior and
    characteristics are defined separately.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**车辆**是整个实验的核心，它们的行为和特征是单独定义的。'
- en: All of these components are parametrized and are passed to Flow separately.
    We then pack them all to create a Flow parameters object.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些组件都是参数化的，并且会单独传递给 Flow。然后，我们将它们打包在一起，创建一个 Flow 参数对象。
- en: It could be difficult to use a bottom-up approach here and start with individual
    parameters for each component to compose the big picture. In addition, these details
    are out of the scope of this chapter. Instead, it is much easier to unpack a prebuilt
    Flow parameters object. Let's do that next.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自下而上的方法可能会很困难，从每个组件的单独参数开始，来拼凑出整体的情况。此外，这些细节超出了本章的范围。相反，解包一个预构建的 Flow 参数对象会更容易。接下来我们就做这个。
- en: Analyzing a Flow parameters object
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析 Flow 参数对象
- en: 'Flow has some benchmark experiments defined for traffic light optimization
    on a grid network. Take a look at the Flow parameters object for the Grid-0 experiment:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Flow 定义了一些用于网格网络上交通信号灯优化的基准实验。请查看 Grid-0 实验的 Flow 参数对象：
- en: Chapter17/Traffic Lights on a Grid Network.ipynb
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter17/Traffic Lights on a Grid Network.ipynb
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can, for example, inspect what is inside the network parameters:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以举个例子，检查网络参数中的内容：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And, of course, a good way to understand what they do is to visually run the
    experiment:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，理解它们如何工作的一个好方法是通过视觉化实验来进行：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will pop up a SUMO screen similar to the one shown in *Figure 17.2*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将弹出一个与*图17.2*中类似的 SUMO 屏幕：
- en: '![Figure 17.2 – SUMO rendering of the grid network'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.2 – 网格网络的 SUMO 渲染'
- en: '](img/B14160_17_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_17_02.jpg)'
- en: Figure 17.2 – SUMO rendering of the grid network
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2 – 网格网络的 SUMO 渲染
- en: With that, now we have an example up and running. Before we go into RL modeling
    and training, let's discuss how to get a baseline reward for this experiment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们现在有了一个正在运行的示例。在进入 RL 建模和训练之前，让我们讨论一下如何为这个实验获取基准奖励。
- en: Getting a baseline reward
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取基准奖励
- en: The Jupyter notebook in our GitHub repo includes a code snippet taken from the
    Flow code base to get a baseline reward on this environment. It has some carefully
    optimized traffic light phase definitions that lead to a -204 reward on average.
    We will use this reward to benchmark the RL result. Also, feel free to modify
    the phases to see their impact on the traffic pattern on the network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 GitHub 仓库中的 Jupyter notebook 包括一个从 Flow 代码库中提取的代码片段，用于获取该环境的基准奖励。它包含一些经过精心优化的交通信号灯阶段定义，平均奖励为
    -204。我们将使用这个奖励来作为 RL 结果的基准。同样，随时可以修改阶段设置，看看它们对网络中交通模式的影响。
- en: With that, we are now ready to define the RL environment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们现在准备好定义 RL 环境了。
- en: Modeling the traffic light control problem
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模交通信号灯控制问题
- en: As always, we need to define the action, observation, and reward for the RL
    problem. We will do so in the following sections.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们需要为 RL 问题定义动作、观察和奖励。我们将在接下来的章节中进行定义。
- en: Defining the action
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义动作
- en: 'We would like to train a single controller for all of the lights at a given
    intersection that is illustrated in *Figure 17.2*, in example *b*. In the figure,
    the lights are in a green-red-green-red state. We define a binary action that
    tells us 0: continue and 1: switch. When instructed to switch, the state of the
    lights on the figure would become yellow-red-yellow-red, and then red-green-red-green
    after a few seconds.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望为给定交叉口的所有信号灯训练一个单一控制器，如*图17.2*中所示，例子*b*。在图中，信号灯处于绿-红-绿-红状态。我们定义一个二进制动作，0：继续，1：切换。当指示切换时，图中的信号灯状态将变为黄-红-黄-红，几秒钟后将变为红-绿-红-绿。
- en: The default environment accepts a continuous action for each intersection, ![](img/Formula_17_001.png),
    and rounds it up to discretize as we described previously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 默认环境接受每个交叉口的连续动作，![](img/Formula_17_001.png)，并且按照之前描述的方法将其四舍五入以离散化。
- en: Single versus multi-agent modeling
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一智能体与多智能体建模
- en: 'The next design decision we have to make is whether to control all of the traffic
    lights using a centralized agent or adapt a multi-agent approach. If we pick the
    latter, whether we train a single policy for all intersections or train multiple
    policies, we need to note the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的设计决策是，是否使用一个集中式智能体控制所有交通信号灯，或者采用多智能体方法。如果选择后者，无论是为所有交叉口训练一个单一策略，还是训练多个策略，我们需要注意以下几点：
- en: The advantage of the centralized approach is that, in theory, we can perfectly
    coordinate all of the intersections and achieve a better reward. On the other
    hand, a trained agent may not be easily applied to a different road network. In
    addition, for larger networks, the centralized approach won't scale easily.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中式方法的优势在于，理论上我们可以完美协调所有交叉口，从而获得更好的奖励。另一方面，训练好的智能体可能不容易应用于不同的道路网络。此外，对于更大的网络，集中式方法不容易扩展。
- en: If we decide to use a multi-agent approach, we don't have a lot of reasons to
    differentiate between the intersections and the policies they use. So, training
    a single policy makes more sense.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们决定使用多智能体方法，我们没有太多理由区分交叉口和它们使用的策略。因此，训练一个单一策略更有意义。
- en: Training a single generic policy for all intersections where the agents (the
    lights at the intersections) collaboratively try to maximize the reward is a scalable
    and efficient approach. Of course, this lacks the full coordination capabilities
    of a centralized, single-agent approach. In practice, this would be a trade-off
    you would have to evaluate.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为所有交叉口训练一个单一的通用策略，其中智能体（交叉口的信号灯）协同尝试最大化奖励，这是一个可扩展且高效的方法。当然，这缺乏集中的单智能体方法的完全协调能力。实际上，这是一个需要你评估的权衡。
- en: So, we will go with the multi-agent setting, in which the policy will be trained
    with the data coming from all of the agents. The agents will retrieve actions
    from the policy according to their local observations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将采用多智能体设置，其中策略将使用来自所有智能体的数据进行训练。智能体将根据其本地观察从策略中获取动作。
- en: With that, let's define the observation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，让我们定义观察。
- en: Defining the observation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义观察
- en: 'The default multi-agent grid environment uses the following as the observations:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的多智能体网格环境使用以下内容作为观察：
- en: Speeds of the ![](img/Formula_17_002.png) closest vehicles heading to the intersection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ！[](img/Formula_17_002.png) 接近交叉口的车辆的速度
- en: Distances of the ![](img/Formula_17_003.png) closest vehicles heading to the
    intersection
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ！[](img/Formula_17_003.png) 接近交叉口的车辆的距离
- en: The IDs of the road edges that these ­![](img/Formula_17_004.png) vehicles are
    on
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些车辆所在道路边缘的ID是 ­![](img/Formula_17_004.png)
- en: The traffic density, average velocity, and traffic direction on each of the
    local edges
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个本地边缘的交通密度、平均速度和交通方向
- en: Whether the lights are currently in a yellow state
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前信号灯是否处于黄色状态
- en: For more detailed information, you can check the `flow.envs.multiagent.traffic_light_grid`
    module in the Flow repo.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，您可以查看Flow仓库中的`flow.envs.multiagent.traffic_light_grid`模块。
- en: Finally, let's define the reward.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们定义奖励。
- en: Defining the reward
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义奖励
- en: 'The environment has a simple and intuitive cost definition for a given time
    step, which measures the average vehicle delay compared to the top speed allowed:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 环境对于给定时间步长有一个简单直观的成本定义，衡量的是与允许的最高速度相比，车辆的平均延迟：
- en: '![](img/Formula_17_005.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_17_005.jpg)'
- en: Here, ![](img/Formula_17_006.png) is the velocity of the ![](img/Formula_17_007.png)
    of ![](img/Formula_17_008.png) total vehicles, and ![](img/Formula_17_009.png)
    is the maximum allowed speed. The reward can then be defined as the negative of
    this cost term.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，！[](img/Formula_17_006.png) 是总车辆数为 ![](img/Formula_17_008.png) 的车辆的速度，而 ![](img/Formula_17_009.png)
    是允许的最高速度。然后，奖励可以定义为此成本项的负值。
- en: Now that we have all the formulations in place, it is time to solve the problem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的公式，是时候解决问题了。
- en: Solving the traffic control problem using RLlib
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RLlib解决交通控制问题
- en: 'Since we will use the multi-agent interface of RLlib, we need to do the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用RLlib的多智能体接口，我们需要做以下操作：
- en: Register the environment in RLlib with a name and environment creation function.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在RLlib中注册环境，并提供一个名称和环境创建函数。
- en: Define the names of the policies we will train, which we have only one of, `tlight`.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们将训练的策略的名称，我们只有一个策略，`tlight`。
- en: Define a function that generates the arguments needed for the RLlib trainer
    for the policy.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个生成RLlib训练器所需参数的函数，以便为策略提供支持。
- en: Define a function that maps agents to the policies, which again is simple to
    do in our case since all the agents map to the same policy.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将智能体映射到策略，在我们的情况下这很简单，因为所有智能体都映射到相同的策略。
- en: 'So, these can be achieved with the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些可以通过以下代码实现：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once defined, we need to pass these functions and lists to the RLlib config:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义完毕，我们需要将这些函数和列表传递给RLlib配置：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The rest is the regular RLlib training loop. We use the hyperparameters identified
    in the Flow benchmarks with PPO. The full code for all of this is available in
    `Chapter17/Traffic Lights on a Grid Network.ipynb`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分是常规的RLlib训练循环。我们使用在Flow基准测试中使用的超参数进行PPO训练。所有代码的完整内容可以在`Chapter17/Traffic
    Lights on a Grid Network.ipynb`中找到。
- en: Obtaining and observing the results
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取并观察结果
- en: 'After a couple million training steps, the reward converges around -243, which
    is a bit lower than the handcrafted benchmark. The training progress can be observed
    on TensorBoard:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过几百万次训练步骤后，奖励收敛到约-243，略低于手工制作的基准。训练进度可以在TensorBoard中观察到：
- en: '![Figure 17.3 – Training progress of the multi-agent traffic light environment
    in Flow'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.3 – Flow中多智能体交通信号环境的训练进度'
- en: '](img/B14160_17_03.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_17_03.jpg)'
- en: Figure 17.3 – Training progress of the multi-agent traffic light environment
    in Flow
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3 – Flow 中多智能体交通信号灯环境的训练进度
- en: 'You can also visualize how the trained agent is doing with a command on Jupyter
    Notebook in the following format:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过在 Jupyter Notebook 中运行以下格式的命令，来可视化训练后智能体的表现：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, the argument at the end refers to the checkpoint number, which is generated
    regularly during training.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，末尾的参数指的是检查点编号，该编号在训练过程中定期生成。
- en: Next, let's also discuss why the RL performance is falling a bit short of the
    handcrafted policy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还将讨论为什么强化学习的表现稍微逊色于手工设计的策略。
- en: Further improvements
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步改进
- en: 'There are several reasons why RL may not have reached the baseline performance:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因可能导致强化学习未能达到基准性能：
- en: Lack of more hyperparameter tuning and training. This factor is always there.
    There is no way to know whether the performance can be improved with more fiddling
    with the model architecture and training until you try, which we encourage you
    to do.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏更多的超参数调优和训练。这一因素始终存在。我们无法知道是否通过更多调整模型架构和训练可以提升性能，直到你尝试为止，我们鼓励你去尝试。
- en: The baseline policy is exercising a finer control over the yellow light durations,
    whereas the RL model did not have control over that.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准策略对黄灯持续时间进行了更精细的控制，而强化学习模型则没有对这一点进行控制。
- en: The baseline policy coordinates all the intersections on the network, whereas
    each RL agent makes local decisions. So, we might be running into the shortcomings
    of decentralized control here.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准策略协调网络中的所有交叉口，而每个强化学习智能体则做出局部决策。因此，我们可能会遇到去中心化控制的缺点。
- en: This can be mitigated by adding observations that will help coordinate the agents
    with their neighbors.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加一些观察，可以缓解这一问题，这些观察将有助于智能体与邻居之间的协调。
- en: It is not uncommon for RL algorithms to struggle with crossing the last mile
    in the optimization and reach the very peak of the reward curve. This may require
    fine control over the training procedure by reducing the learning rates and adjusting
    the batch sizes.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习（RL）算法在优化的最后阶段，尤其是在达到奖励曲线的顶峰时，遇到困难并不罕见。这可能需要通过减少学习率和调整批量大小来精细控制训练过程。
- en: So, although there is room for improvement, our agents have successfully learned
    how to control traffic lights, which is much more scalable than manually crafting
    policies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管仍有改进空间，我们的智能体已经成功学会如何控制交通信号灯，这比手动设计策略更具可扩展性。
- en: Before we wrap up this topic, let's discuss a few more resources to learn more
    about the problem and the libraries we used.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这个话题之前，来讨论一些资源，帮助更深入了解问题和我们使用的库。
- en: Further reading
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: We have already provided links to the Flow and SUMO documentation. The Flow
    library and the benchmarks obtained with it are explained in *Wu et al., 2019*,
    and *Vinitsky et al*., *2018*. In these resources, you will discover additional
    problems that you can model and solve using various RL libraries.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提供了 Flow 和 SUMO 文档的链接。Flow 库及其获得的基准测试在 *Wu et al., 2019* 和 *Vinitsky et
    al*., *2018* 中有所解释。在这些资源中，你将发现其他你可以用各种强化学习库建模和解决的问题。
- en: Congratulations! We have done a lot in such a short time and space to leverage
    RL for traffic control problems. Next, we will cover another interesting problem,
    which is to modulate the electricity demand to stabilize a power grid.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们在如此短的时间和篇幅内，成功利用强化学习解决了交通控制问题。接下来，我们将探讨另一个有趣的问题，那就是调节电力需求以稳定电网。
- en: Providing an ancillary service to a power grid
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为电网提供辅助服务
- en: In this section, we will describe how RL can help with integrating clean energy
    resources into a power grid by managing smart appliances in home and office buildings.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将描述强化学习如何通过管理家居和办公大楼中的智能设备，帮助将清洁能源资源集成到电网中。
- en: Power grid operations and ancillary services
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电网操作与辅助服务
- en: 'The transmission and distribution of electrical power from generators to consumers
    is a massive operation that requires continuous monitoring and control of the
    system. In particular, the generation and consumption should be nearly equal in
    a region to keep the electric current at the standard frequency (60 Hz in the
    United States) to prevent blackouts and damages. This is a challenging undertaking
    for various reasons:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从发电机到消费者的电力传输与分配是一项庞大的操作，需持续监控和控制系统。特别是，发电与消耗必须在一个地区几乎保持平衡，以使电流频率保持在标准频率（美国为60
    Hz），防止停电和损坏。这是一个具有挑战性的任务，原因如下：
- en: The power supply is planned ahead in energy markets with the generators in the
    region to match the demand.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电力供应在能源市场中提前规划，与该地区的发电机一起匹配需求。
- en: Despite this planning, the future power supply is uncertain, especially when
    obtained from renewable resources. The amount of wind and solar energy may be
    less or more than expected, causing under or oversupply.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管有这些规划，未来的电力供应仍然不确定，尤其是当电力来源于可再生能源时。风能和太阳能的供给可能低于或超过预期，导致供给过剩或不足。
- en: Future demand is uncertain too, as consumers are mostly free to decide when
    and how much to consume.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来的需求也不确定，因为消费者通常可以自由决定何时以及消费多少。
- en: Failures in the grid, such as at generators or transmission lines, can cause
    sudden changes in the supply or demand, putting the reliability of the system
    at risk.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电网故障，如发电机或输电线路出现问题，可能导致供需发生突然变化，从而危及系统的可靠性。
- en: 'The balance between the supply and demand is maintained by authorities called
    **Independent System Operators** (**ISOs**). Traditionally, ISOs ask generators
    to ramp up or down their supply based on the changes in the grid, which is an
    ancillary service provided by generators to ISOs for a price. On the other hand,
    there are several issues regarding generators providing this service:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 供需平衡由称为**独立系统操作员**（**ISO**）的主管部门维持。传统上，ISO根据电网的变化要求发电机增加或减少供应，这是发电机为ISO提供的附加服务，ISO会支付费用。然而，关于发电机提供这种服务存在一些问题：
- en: Generators are usually slow to respond to sudden changes in the grid balance.
    For example, it may take hours to bring in a new generation unit to address a
    supply deficit in the grid.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发电机通常对电网平衡的突变反应较慢。例如，可能需要数小时才能投入新的发电机组来应对电网中的供应不足。
- en: In recent years, there has been a significant increase in renewable energy supply,
    adding to the volatility in the grid.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来，可再生能源供应显著增加，进一步加剧了电网的不稳定性。
- en: For these reasons, a line of research has been initiated to enable consumers
    to provide these ancillary services to the grid. In other words, the goal is to
    modulate the demand in addition to the supply to better maintain the balance.
    This requires more sophisticated control mechanisms, which is what we bring in
    RL to help with.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已有一条研究路线开始，旨在使消费者也能向电网提供这些附加服务。换句话说，目标是调节需求，而不仅仅是供应，以更好地维持平衡。这需要更复杂的控制机制，这就是我们引入强化学习（RL）来协助的地方。
- en: After this introduction, let's now more concretely define the control problem
    here.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段介绍之后，让我们更具体地定义这里的控制问题。
- en: Describing the environment and the decision-making problem
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述环境与决策问题
- en: To reiterate, our goal is to dynamically increase or decrease the total electricity
    consumption in an area. Let's first describe the parties involved in this setting
    and their roles.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下，我们的目标是动态地增加或减少一个区域内的总电力消费。首先，我们来描述在这种情境下的各方及其角色。
- en: Independent system operator
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立系统操作员
- en: The ISO of the region continuously monitors the supply and demand balance and
    broadcasts an automated signal to all ancillary service providers in the region
    to adjust their demand. Let's call this signal ![](img/Formula_17_010.png), which
    is simply a number in the range ![](img/Formula_17_011.png). We will come back
    to what this number precisely means in a moment. For now, let's state that the
    ISO updates this signal every 4 seconds (which is a particular type of ancillary
    service called a regulation service).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该地区的ISO持续监控供需平衡，并向该地区的所有附加服务提供商广播自动信号，要求他们调整需求。我们将这个信号称为 ![](img/Formula_17_010.png)，它只是一个在
    ![](img/Formula_17_011.png) 范围内的数字。稍后我们会详细解释这个数字的确切含义。目前，我们只需要说明，ISO每4秒更新一次这个信号（这是一种叫做调节服务的附加服务）。
- en: Smart building operator
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智能建筑操作员
- en: 'We assume that there is a **smart building operator** (**SBO**) that is in
    charge of modulating the total demand in a (collection of) building(s) to follow
    the ISO signal. The SBO, which will be our RL agent, operates as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有一个**智能建筑操作员**（**SBO**），负责调节（一栋或多栋）建筑的总需求，以跟随ISO信号。SBO将作为我们的强化学习（RL）代理，按照以下方式操作：
- en: The SBO sells the regulation service to the ISO of the region. According to
    this obligation, the SBO promises to maintain the consumption at a rate of ![](img/Formula_17_012.png)
    kW and adjust it up or down up to ![](img/Formula_17_013.png) kW at the ISO's
    request. We assume that ![](img/Formula_17_014.png) and ![](img/Formula_17_015.png)
    are predetermined for our problem.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBO将调节服务出售给该区域的ISO（独立系统运营商）。根据这一义务，SBO承诺将消耗维持在![](img/Formula_17_012.png) kW的速率，并在ISO的要求下，上下调节最多![](img/Formula_17_013.png)
    kW。我们假设![](img/Formula_17_014.png)和![](img/Formula_17_015.png)是为我们的问题预先设定的。
- en: When ![](img/Formula_17_016.png), the SBO needs to quickly decrease the consumption
    in the neighborhood to ![](img/Formula_17_017.png) kW. When ![](img/Formula_17_018.png),
    the consumption rate needs to go up to ![](img/Formula_17_019.png) kW.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当![](img/Formula_17_016.png)，SBO需要快速将社区的消耗量降低到![](img/Formula_17_017.png) kW。当![](img/Formula_17_018.png)，消耗速率需要提升至![](img/Formula_17_019.png)
    kW。
- en: In general, the SBO needs to control the consumption to follow an ![](img/Formula_17_020.png)
    kW rate.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，SBO需要控制消耗，以遵循一个![](img/Formula_17_020.png) kW的速率。
- en: The SBO controls a population of smart appliances/units, such as **heating,
    ventilation, and air conditioning** (**HVAC**) units and **electric** **vehicles**
    (**EVs**), to abide by the signal. This is where we will leverage RL.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: SBO控制着一群智能家电/设备，例如**供暖、通风和空调**（**HVAC**）设备和**电动** **汽车**（**EVs**），以遵循信号。在这里我们将利用强化学习（RL）。
- en: 'We illustrate this setup in *Figure 17.4*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 17.4*中展示了这一设置：
- en: '![Figure 17.4 – Regulation service provision by a smart building operator'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.4 – 智能建筑运营商提供调节服务'
- en: '](img/B14160_17_04.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_17_04.jpg)'
- en: Figure 17.4 – Regulation service provision by a smart building operator
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.4 – 智能建筑运营商提供调节服务
- en: Next, let's go a bit more into the details of how smart appliances operate.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地了解一下智能家电是如何运作的。
- en: Smart appliances
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智能家电
- en: You may feel uncomfortable with the idea that some algorithm is interfering
    with your appliances and causing them to turn on or off. After all, who would
    want the TV to shut down to save power while watching the Super Bowl, or turn
    it on in the middle of the night just because there is excess electricity generation
    due to higher-than-anticipated winds outside? This certainly does not make sense.
    On the other hand, you would be more okay if the AC turned on a minute late or
    earlier than normal. Or you would not mind whether your EV reached full battery
    at 4 a.m. or 5 a.m. in the morning as long as it is ready for you before you leave
    home. So, the point is that some appliances have more room for flexibility in
    terms of when to operate, which is of interest to us in this case.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对某些算法干预你的家电并导致它们开关感到不舒服。毕竟，谁会希望在看超级碗的时候，电视为了省电而自动关掉，或者仅仅因为外面的风力比预期强，导致半夜电视突然打开呢？这显然没有意义。另一方面，如果空调比平时晚一分钟或早一分钟启动，你可能就会觉得没问题。或者你并不介意你的电动汽车在早上4点或5点充满电，只要它能在你离家前准备好。总之，重点是一些家电在运行时间上有更多的灵活性，这在本案例中是我们关注的焦点。
- en: 'We also assume that these appliances are smart and have the following capabilities:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设这些家电是智能的，并且具备以下能力：
- en: They can communicate with the SBO to receive the actions.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以与SBO进行通信，以接收指令。
- en: They can assess the "utility," which is a measure of how much need there is
    for the appliance to consume power at a given moment.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以评估“效用”，即衡量家电在某一时刻需要消耗电力的程度。
- en: Defining the utility
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义效用
- en: 'Let''s give two examples of how the utility changes in different situations.
    Consider an EV that needs to be fully charged by 7 a.m.:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举两个例子，说明不同情况下效用的变化。考虑一个需要在早上7点之前充满电的电动汽车（EV）：
- en: The utility would be high if it is 6 a.m. and the battery is still low.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是早上6点，且电池电量仍然较低，那么效用就会很高。
- en: Conversely, the utility would be low if there is still plenty of time until
    departure and/or the battery is close to full.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，如果离出发时间还很远和/或电池已经接近充满，那么效用就会很低。
- en: Similarly, an air conditioner would have high utility when the room temperature
    is about to exceed the user's comfort zone and low utility when it is close to
    the bottom.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，当房间温度即将超过用户的舒适区时，空调的效用会很高；当房间温度接近底部时，效用则很低。
- en: 'See *Figure 17.5* for an illustration of these situations:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见*图 17.5*以了解这些情况的示意图：
- en: '![Figure 17.5 – Utility levels under different conditions for an (a) EV and
    (b) AC'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.5 – 不同条件下的效用水平（a）电动汽车和（b）空调'
- en: '](img/B14160_17_05.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_17_05.jpg)'
- en: Figure 17.5 – Utility levels under different conditions for an (a) EV and (b)
    AC
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5 – 不同条件下电动汽车（a）和空调（b）的效用水平
- en: Next, let's discuss why this is a sequential decision-making problem.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论为什么这是一个顺序决策问题。
- en: Defining the sequential decision-making problem
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义顺序决策问题
- en: By now, you may have noticed how SBO actions taken now will have implications
    later. Fully charging the EVs in the system too early may limit how much the consumption
    may be ramped up later when needed. Conversely, keeping room temperatures too
    high for too many rooms for too long may cause all ACs to turn on later together
    to bring the room temperatures to normal levels.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经注意到，现在采取的 SBO 动作将会对以后产生影响。过早地为系统中的电动汽车（EV）充满电，可能会限制将来在需要时消费量的增加。相反，如果保持过多房间的室温过高，可能会导致所有空调在之后一起开启，以使房间温度恢复到正常水平。
- en: In the next section, let's cast this as an RL problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们将其视为一个强化学习问题。
- en: RL model
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习模型
- en: As always, we need to define the action, observation, and reward to create an
    RL model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们需要定义动作、观察和奖励来创建一个强化学习模型。
- en: Defining the action
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义动作
- en: 'There are different approaches to how we can define the SBO control:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何定义 SBO 控制，有不同的方法：
- en: 'First, and a more obvious approach, would be to directly control each appliance
    in the system by observing their utilities. On the other hand, this would make
    the model inflexible and potentially intractable: we would have to modify and
    retrain the agent when a new appliance is added. In addition, when there are many
    appliances, the action and observation space would be too big.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，更明显的方法是通过观察电器的效用来直接控制系统中的每个电器。另一方面，这会使模型变得不灵活，且可能无法处理：每当添加新电器时，我们必须修改并重新训练智能体。此外，当电器数量众多时，动作和观察空间会变得过于庞大。
- en: Another approach would be to train a policy for each appliance class (ACs, heating
    units, and EVs) in a multi-agent setting. This would bring in the inherent complexities
    of multi-agent RL. For example, we would have to design a mechanism for the coordination
    of individual appliances.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是为每个电器类别（空调、加热设备和电动汽车）在多智能体设置中训练一个策略。这将引入多智能体强化学习的固有复杂性。例如，我们必须设计一个机制来协调各个电器。
- en: A middle ground is to apply indirect control. In this approach, the SBO would
    broadcast its action and let each appliance decide on what to do for itself.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个折衷的方法是应用间接控制。在这种方法中，SBO 会广播其动作，让每个电器自行决定该怎么做。
- en: Let's describe what such an indirect control might look like in more detail.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地描述这种间接控制可能是什么样子的。
- en: Indirect control of the appliances
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 电器的间接控制
- en: 'Here is how we define the indirect control/action:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们如何定义间接控制/动作：
- en: Assume that there are ![](img/Formula_17_021.png) appliance types, such as ACs,
    EVs, and refrigerators.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设有 ![](img/Formula_17_021.png) 种电器类型，如空调、电动汽车和冰箱。
- en: At any given time, an appliance, ![](img/Formula_17_022.png), has a utility,
    ![](img/Formula_17_023.png), that takes a maximum value of ![](img/Formula_17_024.png).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何给定的时刻，一个电器，![](img/Formula_17_022.png)，具有效用，![](img/Formula_17_023.png)，其最大值为
    ![](img/Formula_17_024.png)。
- en: At every time step, the SBO broadcasts an action, ![](img/Formula_17_025.png),
    for each appliance type, ![](img/Formula_17_026.png). Therefore, the action is
    ![](img/Formula_17_027.png) and ![](img/Formula_17_028.png)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步，SBO 会广播一个动作，![](img/Formula_17_025.png)，针对每种电器类型，![](img/Formula_17_026.png)。因此，动作是
    ![](img/Formula_17_027.png) 和 ![](img/Formula_17_028.png)
- en: Each appliance, when off, checks the action for its class once in a while. This
    won't be at every time step and will depend on its type. For example, AC units
    might check the broadcasted action more frequently than EVs.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个电器在关闭时，会时不时地检查其类别的动作。这并不是在每个时间步都会发生，而是取决于电器的类型。例如，空调（AC）可能会比电动汽车（EV）更频繁地检查广播的动作。
- en: When an appliance, ![](img/Formula_17_029.png), of type ![](img/Formula_17_030.png)
    checks action ![](img/Formula_17_031.png), it turns on if and only if ![](img/Formula_17_032.png).
    Therefore, the action acts like the **price** for electricity. The appliance is
    willing to turn on only when its utility is greater than or equal to the **price**.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当电器，![](img/Formula_17_029.png)，类型为 ![](img/Formula_17_030.png)，检查动作 ![](img/Formula_17_031.png)
    时，只有在 ![](img/Formula_17_032.png) 的情况下才会开启。因此，动作就像是**电价**。电器只有在其效用大于或等于**电价**时才会开启。
- en: Once turned on, an appliance stays on for a certain time. Then, it turns off
    and starts periodically checking the action again.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦开启，电器会保持开启一段时间。然后，它会关闭并开始定期检查动作。
- en: With this mechanism, the SBO is able to influence the demand indirectly. It
    gives less precise control over the environment, but at a much-reduced complexity
    compared to a direct or multi-agent control.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种机制，SBO能够间接影响需求。它对环境的控制不如直接控制或多代理控制精确，但相比之下复杂度大大降低。
- en: Next, let's define the observation space.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义观察空间。
- en: Defining the observation
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义观察
- en: 'The SBO could use the following observations to make informed decisions:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: SBO可以利用以下观察来做出有根据的决策：
- en: The ISO signal at time ![](img/Formula_17_033.png), ![](img/Formula_17_034.png),
    as the SBO is obliged to track it by adjusting its demand.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间![](img/Formula_17_033.png)时的ISO信号，![](img/Formula_17_034.png)，因为SBO有义务通过调整其需求来跟踪该信号。
- en: The number of appliances that are on at time ![](img/Formula_17_035.png) for
    each type, ![](img/Formula_17_036.png). For simplicity, a fixed electricity consumption
    rate could be assumed for an appliance of type ![](img/Formula_17_037.png), ![](img/Formula_17_038.png).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间![](img/Formula_17_035.png)时，每种类型的开启家电数量，![](img/Formula_17_036.png)。为了简单起见，可以假设每种类型家电的电力消耗率为固定值，![](img/Formula_17_037.png)，![](img/Formula_17_038.png)。
- en: Time and date features, such as time of day, day of the week, holiday calendar,
    and more.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间和日期特征，例如一天中的时间、一周中的天数、节假日日历等。
- en: Auxiliary information, such as weather temperature.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助信息，如天气温度。
- en: In addition to making all these observations at each time step, what will also
    be needed is to keep a memory of the observations. This is a partially observable
    environment where the energy needs of the appliances and the state of the grid
    are hidden from the agent. So, keeping a memory will help the agent uncover these
    hidden states.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在每个时间步进行这些观察之外，还需要保持对观察的记忆。这是一个部分可观察的环境，其中家电的能量需求和电网状态对代理是隐藏的。因此，保持记忆将帮助代理揭示这些隐藏状态。
- en: Finally, let's describe the reward function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们描述奖励函数。
- en: Defining the reward function
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义奖励函数
- en: 'In this model, the reward function consists of two parts: the tracking cost
    and the utility.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模型中，奖励函数由两部分组成：跟踪成本和效用。
- en: We mentioned that the SBO is obliged to track the ISO signal as it is paid for
    this service.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，SBO有义务跟踪ISO信号，因为它为此项服务获得报酬。
- en: 'Therefore, we assign a penalty for deviating from the target implied by the
    signal at time ![](img/Formula_17_039.png):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们为偏离信号所暗示的目标时间![](img/Formula_17_039.png)分配了惩罚：
- en: '![](img/Formula_17_040.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_17_040.jpg)'
- en: Here, ![](img/Formula_17_041.png) is the target and ![](img/Formula_17_042.png)
    is the actual consumption rate at time ![](img/Formula_17_043.png).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_17_041.png)是目标，而![](img/Formula_17_042.png)是时间![](img/Formula_17_043.png)时的实际消费率。
- en: 'The second part of the reward function is the total utility realized by the
    appliances. We want the appliances to turn on and consume energy but to do so
    when they really need it. An example of why this is beneficial is the following:
    an AC would consume less energy when the average room temperature is kept closer
    to the top of the comfort zone (76°F in *Figure 17.3*) where the utility is the
    highest than when it is kept closer to the bottom while the outside temperature
    is above the comfort zone. So, the total utility realized at time ![](img/Formula_17_044.png)
    is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数的第二部分是由家电实现的总效用。我们希望家电能够开启并消耗能源，但要在它们真正需要的时候进行。以下是这一点为何有益的一个例子：当空调将室内温度保持在舒适区的上限（*图17.3*中的76°F）时，它的能耗要低于当温度保持在下限而外部温度高于舒适区时。因此，在时间![](img/Formula_17_044.png)时实现的总效用如下：
- en: '![](img/Formula_17_045.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_17_045.jpg)'
- en: 'Here, ![](img/Formula_17_046.png) is the set of appliances that turn on within
    discrete time step ![](img/Formula_17_047.png). Then, the RL objective becomes
    the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_17_046.png)是那些在离散时间步![](img/Formula_17_047.png)内开启的家电集合。然后，RL目标变为以下形式：
- en: '![](img/Formula_17_048.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_17_048.jpg)'
- en: Here, ![](img/Formula_17_049.png) is some coefficient to control the trade-off
    between utility and tracking cost, and ![](img/Formula_17_050.png) is the discount
    factor.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_17_049.png)是一个系数，用于控制效用和跟踪成本之间的权衡，![](img/Formula_17_050.png)是折扣因子。
- en: Terminal conditions
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终端条件
- en: Finally, let's talk about the terminal conditions for this problem. Normally,
    this is a continuous task without a natural terminal state. However, we can introduce
    terminal conditions, for example, to stop the episode if the tracking error is
    too large. Other than that, we can convert this into an episodic task by taking
    the episode length as a day.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来谈谈这个问题的终止条件。通常，这是一个没有自然终止状态的连续任务。然而，我们可以引入终止条件，例如，如果跟踪误差过大，则停止任务。除此之外，我们还可以将其转化为一个阶段性任务，将每个阶段的长度设定为一天。
- en: That's it! We've left the exact implementation of this model out, but you now
    have a solid idea about how to approach this problem. If you need more details,
    you can check out the references at the end of this chapter by Bilgin and Caramanis.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们没有详细介绍该模型的具体实现，但你现在已经对如何处理这个问题有了清晰的思路。如果需要更多细节，可以查看本章末尾由Bilgin和Caramanis提供的参考文献。
- en: Last but not least, let's switch gears to model the early detection of cyberattacks
    in a power grid.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，让我们转变思路，建模电网中网络攻击的早期检测。
- en: Detecting cyberattacks in a smart grid
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在智能电网中检测网络攻击
- en: Smart cities, by definition, run on intense digital communications between their
    assets. Despite its benefits, this makes smart cities prone to cyberattacks. As
    RL is finding its way into cybersecurity, in this section, we will describe how
    it can be applied to detecting attacks on a smart power grid infrastructure. Throughout
    the section, we will follow the model proposed in *Kurt et al., 2019*, leaving
    the details to the paper.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 智慧城市在定义上依赖于资产之间的强大数字通信。尽管这一点带来了好处，但也使得智慧城市容易受到网络攻击。随着强化学习（RL）逐步应用于网络安全，本节将描述它如何应用于检测智能电网基础设施中的攻击。在本节中，我们将遵循*Kurt
    et al., 2019*提出的模型，具体细节请参见该论文。
- en: Let's start by describing the power grid environment.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从描述电网环境开始。
- en: The problem of early detection of cyberattacks in a power grid
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电网中网络攻击的早期检测问题
- en: 'An electric power grid consists of nodes, called **buses**, which correspond
    to generation, demand, or power line intersection points. Grid authorities collect
    measurements from these buses to make certain decisions such as bringing in additional
    power generation units. To this end, a critical quantity measured is the **phase
    angle** at each bus (except the reference bus), which makes it a potential target
    for cyber-attackers, hence our interest in it:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 电力网由称为**总线**的节点组成，这些节点对应于发电、需求或电力线路的交叉点。电网管理者从这些总线收集测量数据，以做出某些决策，例如调入额外的发电单元。为此，关键的测量量是每个总线的**相位角**（参考总线除外），这使得它成为网络攻击者的潜在目标，因此我们对其感兴趣：
- en: Not surprisingly, the measurements from the meters are noisy and subject to
    errors.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不出所料，来自仪表的测量数据是嘈杂的，且容易出错。
- en: A cyberattack on these meters and their measurements has the potential to mislead
    the decisions made by grid authorities and cause the system to collapse.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对这些仪表及其测量结果进行网络攻击，可能会误导电网管理者的决策，并导致系统崩溃。
- en: Therefore, it is important to detect when there is an attack on the system.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，检测系统是否遭受攻击是非常重要的。
- en: However, it is not easy to differentiate the noise and real system changes from
    anomalies caused by an attack. Normally, waiting and collecting more measurements
    are helpful to this end.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，要区分噪声和由攻击引起的异常与真实系统变化并不容易。通常，等待并收集更多的测量数据有助于解决这个问题。
- en: On the other hand, being late in declaring an attack can lead to incorrect decisions
    in the meantime. Therefore, our goal is to identify these attacks as soon as possible,
    but without too many false alarms.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，迟迟不宣布攻击可能会导致此期间做出错误的决策。因此，我们的目标是尽快识别这些攻击，但又不能产生过多的误报。
- en: 'So, the set of possible actions that our cybersecurity agent can take is simple:
    declare an attack or not. A sample timeline of false and true (but delayed) alarms
    is illustrated in *Figure 17.6*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的网络安全代理可以采取的可能行动集合很简单：宣布攻击或不宣布攻击。一个关于错误和真实（但延迟的）警报的示例时间线如*图 17.6*所示：
- en: '![Figure 17.6 – A sample timeline of (a) false and (b) true but delayed alarms'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.6 – (a) 错误警报和 (b) 真实但延迟的警报的示例时间线'
- en: '](img/B14160_17_06.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_17_06.jpg)'
- en: Figure 17.6 – A sample timeline of (a) false and (b) true but delayed alarms
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.6 – (a) 错误警报和 (b) 真实但延迟的警报的示例时间线
- en: 'Here are the details of the episode life cycle and rewards:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是任务生命周期和奖励的详细信息：
- en: Once an attack is declared, the episode terminates.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦宣布攻击，任务终止。
- en: If it is a false alarm, a reward of ![](img/Formula_17_051.png) is incurred.
    If it is a true alarm, the reward is 0.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是误报，则会产生奖励![](img/Formula_17_051.png)。如果是实警，则奖励为0。
- en: If there is an attack but the action continues (and doesn't declare an attack),
    for each time step, a reward of ![](img/Formula_17_052.png), ![](img/Formula_17_053.png)
    is incurred.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果发生攻击但行动继续（且未声明攻击），则每个时间步会产生奖励！[](img/Formula_17_052.png)，![](img/Formula_17_053.png)。
- en: The reward is 0 in all other time steps.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有其他时间步中，奖励为0。
- en: 'With that, the goal of the agent is to minimize the following cost function
    (or maximize its negative):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，智能体的目标是最小化以下代价函数（或最大化其负值）：
- en: '![](img/Formula_17_054.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_17_054.jpg)'
- en: Here, the first term is the probability of a false alarm, the second term is
    the expected (positive) delay in declaring an attack, and ![](img/Formula_17_055.png)
    is the cost coefficient to manage the trade-off in between.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一个项是误报的概率，第二个项是宣布攻击的预期（正向）延迟，![](img/Formula_17_055.png)是用于管理两者之间折衷的成本系数。
- en: One missing piece is the observations, which we will discuss next.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缺失的部分是观测值，我们将在接下来的内容中讨论。
- en: Partial observability of the grid state
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电网状态的部分可观测性
- en: 'The true state of the system, which is whether there is an attack or not, is
    not observable to the agent. Instead, it collects measurements of phase angles,
    ![](img/Formula_17_056.png). A key contribution of *Kurt et al., 2019*, is to
    use the phase angle measurements in the following way:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的真实状态，即是否发生攻击，智能体无法直接观测到。相反，它收集相位角的测量值，![](img/Formula_17_056.png)。*Kurt 等人，2019*的一个关键贡献是以以下方式使用相位角的测量值：
- en: Use a Kalman filter to predict the true phase angles from the previous observations.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卡尔曼滤波器根据先前的观测值预测真实的相位角。
- en: Based on this prediction, estimate the expected measurements, ![](img/Formula_17_057.png).
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于此预测，估计预期的测量值，![](img/Formula_17_057.png)。
- en: Define ![](img/Formula_17_058.png) as a measure of the discrepancy between ![](img/Formula_17_059.png)
    and ![](img/Formula_17_060.png), which then becomes the observation used by the
    agent.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义![](img/Formula_17_058.png)作为![](img/Formula_17_059.png)和![](img/Formula_17_060.png)之间差异的度量，接着成为智能体使用的观测值。
- en: Observe ![](img/Formula_17_061.png) and carry a memory of past observations
    for the agent's use.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察![](img/Formula_17_061.png)并保留过去观测的记忆供智能体使用。
- en: The paper uses a tabular SARSA method to solve this problem by discretizing
    ![](img/Formula_17_062.png), and shows the effectiveness of the approach. An interesting
    extension would be to use deep RL methods without discretization and under varying
    grid topographies and attack characteristics.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用表格化的SARSA方法通过离散化![](img/Formula_17_062.png)来解决此问题，并展示了该方法的有效性。一个有趣的扩展是使用深度强化学习方法，在没有离散化的情况下，适应不同的电网拓扑和攻击特征。
- en: With that, we conclude our discussion on the topic and the chapter. Great job,
    we have done a lot! Let's summarize what we have covered in the chapter.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，我们结束了这一主题的讨论以及本章内容。做得好，我们已经完成了很多！让我们总结一下本章的内容。
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'RL is poised to play a significant role in automation. Smart cities are a great
    field to leverage the power of RL. In this chapter, we discussed three sample
    applications: traffic light control, ancillary service provision by electricity-consuming
    appliances, and detecting cyberattacks in a power grid. The first problem allowed
    us to showcase a multi-agent setting, we used a price-like indirect control mechanism
    for the second one, and the last one was a good example of advanced input preprocessing
    in partially observed environments.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习将在自动化领域发挥重要作用。智慧城市是利用强化学习力量的一个重要领域。在本章中，我们讨论了三个示例应用：交通信号控制、电力消耗设备提供辅助服务以及检测电网中的网络攻击。第一个问题使我们能够展示一个多智能体环境，我们使用了一种类似价格的间接控制机制来解决第二个问题，最后一个问题是部分观测环境中先进输入预处理的一个很好的例子。
- en: In the next and final chapter, we will wrap up the book with a discussion on
    the challenges of real-life RL and future directions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章也是最后一章，我们将总结本书内容，并讨论现实生活中强化学习的挑战及未来的方向。
- en: References
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Wu, C., et al. (2019). *Flow: A Modular Learning Framework for Autonomy in
    Traffic*. ArXiv:1710.05465 [Cs]. arXiv.org, [http://arxiv.org/abs/1710.05465](http://arxiv.org/abs/1710.05465)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu, C., et al. (2019). *流动：一种面向交通自主性的模块化学习框架*。ArXiv:1710.05465 [Cs]。arXiv.org，[http://arxiv.org/abs/1710.05465](http://arxiv.org/abs/1710.05465)
- en: Vinitsky, E., Kreidieh, A., Flem, L.L., Kheterpal, N., Jang, K., Wu, C., Wu,
    F., Liaw, R., Liang, E., & Bayen, A.M. (2018). *Benchmarks for reinforcement learning
    in mixed-autonomy traffic*. Proceedings of The 2nd Conference on Robot Learning,
    in PMLR 87:399-409, [http://proceedings.mlr.press/v87/vinitsky18a.html](http://proceedings.mlr.press/v87/vinitsky18a.html)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinitsky, E., Kreidieh, A., Flem, L.L., Kheterpal, N., Jang, K., Wu, C., Wu,
    F., Liaw, R., Liang, E., & Bayen, A.M. (2018). *混合自主交通中的强化学习基准*. 第二届机器人学习会议论文集，PMLR
    87:399-409，[http://proceedings.mlr.press/v87/vinitsky18a.html](http://proceedings.mlr.press/v87/vinitsky18a.html)
- en: 'Bilgin, E., Caramanis, M. C., Paschalidis, I. C., & Cassandras, C. G. (2016).
    *Provision of Regulation Service by Smart Buildings*. IEEE Transactions on Smart
    Grid, vol. 7, no. 3, pp. 1683-1693, DOI: 10.1109/TSG.2015.2501428'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bilgin, E., Caramanis, M. C., Paschalidis, I. C., & Cassandras, C. G. (2016).
    *智能建筑提供调节服务*. IEEE智能电网学报, 第7卷，第3期，1683-1693页，DOI: 10.1109/TSG.2015.2501428'
- en: 'Bilgin, E., Caramanis, M. C., & Paschalidis, I. C. (2013). *Smart building
    real time pricing for offering load-side Regulation Service reserves*. 52nd IEEE
    Conference on Decision and Control, Florence, pp. 4341-4348, DOI: 10.1109/CDC.2013.6760557'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bilgin, E., Caramanis, M. C., & Paschalidis, I. C. (2013). *智能建筑实时定价以提供负荷端调节服务储备*.
    第52届IEEE决策与控制会议，佛罗伦萨，4341-4348页，DOI: 10.1109/CDC.2013.6760557'
- en: 'Caramanis, M., Paschalidis, I. C., Cassandras, C., Bilgin, E., & Ntakou, E.
    (2012). *Provision of regulation service reserves by flexible distributed loads*.
    IEEE 51st IEEE Conference on Decision and Control (CDC), Maui, HI, pp. 3694-3700,
    DOI: 10.1109/CDC.2012.6426025'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caramanis, M., Paschalidis, I. C., Cassandras, C., Bilgin, E., & Ntakou, E.
    (2012). *通过灵活的分布式负荷提供调节服务储备*. 第51届IEEE决策与控制会议（CDC），毛伊岛，HI，3694-3700页，DOI: 10.1109/CDC.2012.6426025'
- en: Bilgin, E. (2014). *Participation of distributed loads in power markets that
    co-optimize energy and reserves*. Dissertation, Boston University
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bilgin, E. (2014). *分布式负荷参与同时优化能源和备用的电力市场*. 论文，波士顿大学
- en: 'Kurt, M. N., Ogundijo, O., Li C., & Wang, X. (2019). *Online Cyber-Attack Detection
    in Smart Grid: A Reinforcement Learning Approach*. IEEE Transactions on Smart
    Grid, vol. 10, no. 5, pp. 5174-5185, DOI: 10.1109/TSG.2018.2878570'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kurt, M. N., Ogundijo, O., Li C., & Wang, X. (2019). *智能电网中的在线网络攻击检测：一种强化学习方法*.
    IEEE智能电网学报, 第10卷，第5期，5174-5185页，DOI: 10.1109/TSG.2018.2878570'
