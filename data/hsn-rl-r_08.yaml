- en: Monte Carlo Methods for Predictions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法在预测中的应用
- en: The Monte Carlo methods for estimating the value function and discovering excellent
    policies do not require the presence of a model of the environment. You can learn
    through the use of the agent's experience alone, or from samples of state sequences,
    actions, and rewards obtained from the interactions between agent and environment.
    The experience can be acquired by the agent in line with the learning process,
    or emulated by a previously populated dataset. In this chapter, we will learn
    how to use Monte Carlo methods to predict an optimal strategy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法用于估计价值函数并发现优秀策略时，并不需要环境的模型。你可以仅通过智能体的经验，或通过从智能体与环境交互中获得的状态序列、动作和奖励样本来进行学习。经验可以通过智能体的学习过程获得，或者通过先前填充的数据集进行模拟。在本章中，我们将学习如何使用蒙特卡洛方法来预测最优策略。
- en: By the end of the chapter, you should be familiar with the basic concepts of
    forecasting techniques and should have learned how to apply Monte Carlo methods
    in order to forecast environment behavior. We will also learn the model-free approach
    to deal with reinforcement learning problems, how to estimate the action value,
    and how learning the value of the optimal policy regardless of the agent's actions
    constitutes an off-policy algorithm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该熟悉预测技术的基本概念，并学会如何应用蒙特卡洛方法来预测环境行为。我们还将学习无模型方法来处理强化学习问题，如何估计动作值，以及如何通过学习最优策略的价值来构成一个脱离策略的算法，而无需考虑智能体的动作。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: Overview of forecasting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测概述
- en: Understanding Monte Carlo methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解蒙特卡洛方法
- en: Approaching model-free algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近无模型算法
- en: Estimation of action values
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作值估计
- en: Blackjack strategy prediction using the Monte Carlo approach
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法预测黑杰克策略
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请观看以下视频，查看代码的实际应用：
- en: '[http://bit.ly/2qONScL](http://bit.ly/2qONScL)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2qONScL](http://bit.ly/2qONScL)'
- en: Overview of forecasting
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测概述
- en: An attempt to predict the future has ancient roots and has covered the entire
    history of humanity, adapting to the typical ways of different civilizations and
    to different religious settings. The need to foresee future events appears to
    be justifiable not only for purely speculative and cognitive purposes, but also
    for operational purposes. The aim is to choose the most appropriate behavior to
    address the problems that will arise and try to take full advantage of the future
    situation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 试图预测未来有着悠久的历史，贯穿了整个人类历史，适应了不同文明的典型方式和宗教背景。预测未来事件的需求似乎不仅仅是为了纯粹的推测和认知目的，还具有操作性目的。其目标是选择最合适的行为来应对将要出现的问题，并尽力充分利用未来的情况。
- en: '**Forecast **and **prediction** are often used as synonyms, but it''s always
    a good idea to make a distinction between the meanings of these two terms. Forecasting
    allows you to associate the probability of occurrence with future events, or to
    specify confidence intervals to estimate the size that will be observable and
    measurable in the future. Prediction, on the other hand, involves identifying
    the specific value that a measurable quantity will assume in the future. It is,
    therefore, easy to associate the corresponding forecasts with the predictions
    thus formulated, using the classical instruments of inferential statistics to
    derive the relative confidence intervals.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**和**预言**常常被作为同义词使用，但区分这两个术语的含义通常是个好主意。预测允许你将未来事件的发生概率与之关联，或者指定置信区间来估计将来可观察和可测量的大小。另一方面，预言涉及识别某一可测量量在未来将呈现的具体值。因此，使用推断统计的经典工具，可以轻松地将相应的预测与所制定的预言关联起来，从而得出相关的置信区间。'
- en: 'In the following diagram, we can see two examples to understand the differences
    between forecast and prediction:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到两个例子，用来理解预测与预言之间的区别：
- en: '![](img/4fdf7f91-892a-46b6-a69e-5594729965dc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fdf7f91-892a-46b6-a69e-5594729965dc.png)'
- en: The difficulties associated with forecasting derive from the uncertainty of
    the future, which, at the moment in which it is to be expected, is not yet determined.
    The strong relevance of the elements of uncertainty in the forecast means that
    the probability calculation tools are essential in the development of a good forecast.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 预测中遇到的困难源于未来的不确定性，而在预期的时刻，未来尚未确定。预测中不确定性元素的强大相关性意味着概率计算工具在制定良好预测时至关重要。
- en: 'In the elaboration of a forecast, some aspects must be observed, such as the
    following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定预测时，需要观察一些方面，如下所示：
- en: The nature of the forecasts can be qualitative or quantitative, but often, both
    aspects appear for complex phenomena.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的性质可以是定性的或定量的，但通常在复杂现象中，两者的方面都会出现。
- en: The object of the forecast can be the future value of a phenomenon that manifests
    itself with continuity, or the time in which a phenomenon occurs, or the modalities
    and characteristics of an event that will occur in the future.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测对象可以是以连续性表现出来的现象的未来值，或者是现象发生的时间，或者是未来将发生事件的方式和特征。
- en: The forecast horizon is usually classified in the short, medium and long term.
    The distinction is not clear and precise. In general, we talk about short-term
    forecasts when the structural conditions remain unchanged. This is because the
    event to be foreseen will be largely determined by actions and behaviors that
    have already been implemented at the time of forecasting. Instead, we talk about
    long-term forecasts if the fundamental conditions that determine the event to
    be forecast are still substantially uncertain.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的时间范围通常被划分为短期、中期和长期。这个区分并不清晰和精确。通常，我们谈论短期预测时，结构条件保持不变。这是因为要预测的事件将在很大程度上由在预测时已经实施的行为和行动决定。而如果决定要预测事件的基本条件仍然基本不确定时，我们则称之为长期预测。
- en: Finally, with regard to the dimension, the forecast can only relate to one phenomenon
    (univariate forecast), or, at the same time, more connected phenomena (multivariate
    forecast), and, in this case, it can be based on causal links through which the
    behavior of a phenomenon determines, possibly with a certain time lag, the trend
    of others (causal forecast).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，关于维度，预测可以仅涉及一个现象（单变量预测），或者同时涉及更多相关现象（多变量预测）。在这种情况下，它可以基于因果关系，通过这些关系，现象的行为可能会在一定的时间滞后后，决定其他现象的趋势（因果预测）。
- en: Risk and uncertainty are crucial in the elaboration of a forecast. In fact,
    it is good practice to indicate the degree of uncertainty related to the forecasts.
    In any case, the data must be updated so that the forecast is as accurate as possible.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 风险和不确定性在预测制定中至关重要。事实上，最好标明与预测相关的不确定性程度。无论如何，数据必须更新，以便预测尽可能准确。
- en: 'Three terms are linked to the concept of statistical forecasting: object, purpose,
    and method. Let''s try to understand what is meant by statistical forecasting.
    Statistical forecasting applies to conceptually defined phenomena in order to
    be objectively measured. The phenomenon and the measurement method must therefore
    be specified and defined, and must remain unchanged for the entire duration of
    the survey. The purpose of a forecast is to study the future manifestations of
    a phenomenon, determined by the persistence of a stability in the general structural
    properties that have occurred in the past. Finally, the forecasting method represents
    the mathematical model that uses the developments of the calculation of the probabilities
    related to stochastic processes on the one hand, and on the other, the paradigm
    of statistical inference and the principles of decision theory in conditions of
    uncertainty. Let''s now try to understand how to approach the different methods
    available by analyzing real examples.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与统计预测概念相关的三个术语是：对象、目的和方法。让我们尝试理解统计预测的含义。统计预测适用于概念上定义的现象，以便进行客观测量。因此，现象和测量方法必须明确和定义，并且在整个调查过程中保持不变。预测的目的是研究现象的未来表现，这些表现由过去所发生的结构性稳定性决定。最后，预测方法代表了使用与随机过程相关的概率计算发展的数学模型，一方面，另一方面是统计推理的范式和在不确定条件下的决策理论原则。现在，让我们通过分析实际例子来理解如何使用不同的方法。
- en: Forecasting methods
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测方法
- en: Forecasting methodologies differ mainly on the basis of the characteristics
    and objectives of the decisions for which they will be used. The length of the
    time horizon, the availability and the homogeneity of a wide historical database,
    and the characteristics of the product to which the forecasts refer, such as the
    life cycle stage, are some of the factors that influence the choice of a method.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方法的差异主要基于所使用决策的特征和目标。时间范围的长度、广泛历史数据库的可用性与均质性，以及预测所涉及产品的特征（如生命周期阶段）是影响方法选择的一些因素。
- en: 'Essentially, the forecasting methods are divided into two categories—**qualitative**
    and **quantitative**. In the following diagram, we can see examples of the two
    categories:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，预测方法分为两大类——**定性**和**定量**。在下图中，我们可以看到这两类方法的示例：
- en: '![](img/9586f16d-bc1e-4d58-b724-06a4cfa0f30c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9586f16d-bc1e-4d58-b724-06a4cfa0f30c.png)'
- en: In the following sections, we will deal with both types of methods. We will
    analyze concrete cases to understand on what basis this classification is carried
    out.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论这两种方法。我们将分析具体案例，理解这种分类的依据。
- en: Qualitative methods
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定性方法
- en: Qualitative forecasting methods are used to predict future data on the basis
    of past data. They are adopted when past numerical data is available, and when
    it is reasonable to assume that some of the models in the data should continue.
    These methods are generally applied to short- or medium-range decisions. So, qualitative
    methods are based primarily on judgements, and therefore depend on the opinion
    and judgement of consumers and experts. They are used when there is limited or
    no quantitative information, but sufficient qualitative information exists.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 定性预测方法用于根据过去的数据预测未来的数据。当过去的数值数据可用，并且合理假设数据中的某些模式应当延续时，便采用这些方法。这些方法通常应用于短期或中期决策。因此，定性方法主要依赖于判断，因此依赖于消费者和专家的意见与判断。当定量信息有限或不存在时，但有足够的定性信息时，便采用这些方法。
- en: 'The following bullet points include some examples of the application of qualitative
    methods:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下项目列出了一些定性方法的应用示例：
- en: '**Assessments of the sales department**: Each sales agent estimates the future
    demand for its territory for the next period. The hypothesis underlying this method
    is that the people closest to the client know their future needs better than anyone
    else. This information is then aggregated to arrive at global forecasts for each
    geographic area or product family.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**销售部门评估**：每位销售代表估计其所在区域下一个时期的未来需求。该方法的假设是，最接近客户的人比其他任何人更了解客户的未来需求。然后，这些信息会被汇总，得出每个地理区域或产品系列的全球预测。'
- en: '**Market surveys**: Companies often turn to firms who specialize in market
    surveys to make this type of forecast. Information is obtained directly from customers
    or, more often, from a representative sample of them. This type of investigation,
    however, is mainly used to look for new ideas, to like or dislike existing products,
    to find out which are the favorite brands of a given product, and so on.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场调查**：公司通常会寻求专门从事市场调查的公司来进行此类预测。信息通常直接来自客户，或者更常见的是来自他们的代表性样本。然而，这种调查主要用于寻找新想法、了解客户是否喜欢或不喜欢现有产品、查找某个产品的最受欢迎品牌等等。'
- en: Quantitative methods
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定量方法
- en: We talk about quantitative methods when quantitative information is adequately
    available. They are also used to predict future data based on past data. These
    methods are usually applied to short- or medium-term decisions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当定量信息充分可用时，我们谈论定量方法。它们也用于根据过去的数据预测未来的数据。这些方法通常应用于短期或中期决策。
- en: 'Examples of quantitative prediction methods include the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 定量预测方法的示例包括以下内容：
- en: '**Time series**: The phenomenon to be expected is treated like a black box,
    because it does not try to identify the phenomena that can influence it. The goal
    of this approach is to identify the past evolution of the phenomenon, and to extrapolate
    the past to obtain a prediction. In other words, the phenomenon to be predicted
    is modeled with respect to time, and not with respect to an explanatory variable
    (consider sales trends, gross domestic product (GDP) trends, and so on).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列**：待预测的现象被视为一个黑箱，因为它并不试图识别可能影响它的现象。该方法的目标是识别现象的过去演变，并通过外推过去的数据来做出预测。换句话说，待预测的现象是相对于时间进行建模的，而不是相对于某个解释性变量（考虑销售趋势、国内生产总值（GDP）趋势等）。'
- en: '**Explanatory methods**: This assumes that the variable to be predicted can
    be related to one or more independent or explanatory variables. For example, the
    demand for consumer goods of a family depends on the income received, and the
    age of the goods.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释性方法**：假设待预测的变量可以与一个或多个独立变量或解释性变量相关联。例如，一个家庭的消费品需求取决于其收入和商品的年龄。'
- en: Such forecasting techniques employ regression methods and, therefore, the main
    phase of the analysis entails specifying and estimating a model that relates the
    variable to be predicted (response) and the explanatory variables (for example,
    the effect on sales of advertising and/or a price promotion).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的预测技术采用回归方法，因此，分析的主要阶段是指定并估计一个模型，该模型将待预测的变量（响应变量）与解释变量（例如，广告和/或价格促销对销售的影响）关联起来。
- en: 'These methods can be used in the following hypotheses:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法可以用于以下假设：
- en: Sufficient information is available regarding the past evolution of the phenomenon.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于现象过去演变的足够信息是可用的。
- en: This information can be quantified.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些信息是可以量化的。
- en: It can be assumed that the characteristics of the past evolution continue to
    exist in the future in order to make the forecast.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以假设过去演变的特征在未来仍然存在，以便做出预测。
- en: Ultimately, quantitative methods are used when adequate quantitative information
    is available.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，定量方法在有足够的定量信息可用时被使用。
- en: In the following section, we will introduce Monte Carlo methods. In particular,
    we will focus on the use of these methods to solve reinforcement learning problems.
    We will also see what it means to use these methods for prediction and control.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍蒙特卡罗方法，特别是我们将重点介绍这些方法在解决强化学习问题中的应用。我们还将探讨使用这些方法进行预测和控制意味着什么。
- en: Understanding Monte Carlo methods
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解蒙特卡罗方法
- en: Monte Carlo methods for estimating the value function and discovering excellent
    policies do not require the presence of a model of the environment. These methods
    can learn using the agent's experience alone, or from samples of state sequences,
    actions, and rewards obtained from interactions between the agent and the environment.
    The experience can be acquired by the agent in line with the learning process,
    or it can be emulated by a previously populated dataset. The possibility of gaining
    experience during learning (online learning) is interesting, because it allows
    the acquisition of excellent behavior even in the absence of a priori knowledge
    of the dynamics of the environment. Even learning through an already-populated
    experience dataset can be interesting because, if combined with online learning,
    it makes automatic policy improvements induced by others' experiences possible.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法用于估计价值函数并发现优秀策略，不需要环境模型的存在。这些方法可以仅通过代理的经验来进行学习，或通过从代理与环境交互中获得的状态序列、动作和奖励样本来进行学习。经验可以通过代理在学习过程中的获取，也可以通过先前填充的数据集来模拟。在线学习中获得经验的可能性非常有趣，因为它使得即使在没有事先了解环境动态的情况下，也能获得优秀的行为。即使是通过已经填充的经验数据集进行学习，也可以很有趣，因为如果与在线学习相结合，它使得由他人经验所引发的自动策略改进成为可能。
- en: To solve reinforcement learning problems, Monte Carlo methods estimate the value
    function based on the total sum of rewards obtained, on average, in past episodes.
    This assumes that the experience is divided into episodes, and that all episodes
    are composed of a finite number of transitions. This is because, in Monte Carlo
    methods, the policy update and value function estimate take place after an episode
    is completed. Indeed, Monte Carlo methods iteratively estimate policy and value
    functions. In this case, however, each iteration cycle is equivalent to completing
    an episode. So, the policy update and value function estimate occur episode by
    episode, as we just said.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决强化学习问题，蒙特卡洛方法通过基于过去回合中获得的奖励总和（平均值）来估计价值函数。这假设经验被分为回合，并且每个回合由有限数量的过渡组成。这是因为在蒙特卡洛方法中，策略更新和价值函数估计发生在回合完成之后。事实上，蒙特卡洛方法是通过迭代估计策略和价值函数的。然而，在这种情况下，每次迭代周期相当于完成一个回合。因此，策略更新和价值函数估计是按回合进行的，正如我们刚才所说的。
- en: Monte Carlo methods try to get the best policy by using example returns. An
    environment model capable of generating these example transitions is therefore
    sufficient. Unlike dynamic programming, it is not necessary to know the probability
    of all possible transitions. In many cases, it is, in fact, easy to generate samples
    that satisfy the desired probability distributions, while it is impractical to
    express explicitly the totality of the probability distributions. These algorithms
    simulate an example sequence called an episode, and are based on observed values,
    ​​update values, and policy estimates. While iterating for a sufficient number
    of episodes, the results obtained show a satisfactory accuracy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法通过使用示例回报来获取最佳策略。因此，一个能够生成这些示例过渡的环境模型就足够了。与动态规划不同，蒙特卡洛方法不需要知道所有可能过渡的概率。在许多情况下，实际上很容易生成满足期望概率分布的样本，而显式表达所有概率分布是不可行的。这些算法模拟一个称为“回合”的示例序列，并基于观察值、更新值和策略估计进行运算。在经过足够多的回合迭代后，所得结果显示出令人满意的准确性。
- en: Compared to algorithms based on dynamic programming, Monte Carlo algorithms
    do not require the complete system model. However, they offer the possibility
    to update value and policy only at the end of each simulation, unlike dynamic
    programming algorithms, which update the estimates at each step.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于动态规划的算法相比，蒙特卡洛算法不需要完整的系统模型。然而，它们提供了在每次仿真结束时才更新价值和策略的可能性，而不像动态规划算法那样在每一步都更新估计。
- en: Now it is time to understand what it means to use Monte Carlo methods for prediction
    and control.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候理解使用蒙特卡洛方法进行预测和控制的意义了。
- en: Monte Carlo methods for prediction
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测方法
- en: 'Monte Carlo prediction is used to estimate a value function. In this case,
    the expected total reward from any given state is predicted, given the policy.
    The procedure follows this flow:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测用于估计价值函数。在这种情况下，给定策略下，从任何给定状态开始的预期总奖励将被预测。该过程遵循以下流程：
- en: Gives the policy
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给出策略
- en: Calculates the value function
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算价值函数
- en: You will recall that the policy defines the agent's way of acting based on the
    current state, thus representing the probability of action, in a specific way
    when in a specific state.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你会回忆到，策略定义了代理在当前状态下的行为方式，从而以特定方式表示在特定状态下采取某一动作的概率。
- en: A prediction task requires that the policy is provided with the aim of measuring
    its performance, that is, of forecasting the total reward provided by each given
    state, assuming that the policy is set a priori.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 预测任务要求提供策略，目的是衡量其表现，即预测在给定状态下由策略提供的总奖励，假设策略是预先设定的。
- en: Monte Carlo methods for control
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛控制方法
- en: Monte Carlo control is used to optimize the value function to make the value
    function more accurate than the estimation. In control, policy is not fixed, and
    the goal is to find the optimal policy. In this case, our aim is to find that
    policy that maximizes the total reward provided by each given state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛控制用于优化价值函数，以使价值函数比估计更准确。在控制中，策略并非固定，目标是找到最优策略。在这种情况下，我们的目标是找到能够最大化每个给定状态下总奖励的策略。
- en: A control algorithm also works for prediction, which predicts the values of
    the action in different ways and adjusts the policy to choose the best actions
    at each stage. Thus, the output of these algorithms provides an approximately
    optimal policy and the expected future rewards for following that policy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 控制算法也适用于预测，以不同的方式预测动作的值，并调整策略以在每个阶段选择最佳动作。因此，这些算法的输出提供了一个近似最优策略和遵循该策略的未来预期奖励。
- en: In the next section, we will understand how to differentiate between a model-free
    and a model-based algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将了解如何区分模型无关和模型基础的算法。
- en: Approaching model-free algorithms
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接近模型无关算法
- en: 'In the previous section, *Understanding Monte Carlo methods*, we said that
    Monte Carlo methods do not require the presence of a model of the environment
    to estimate the value function, or to discover excellent policies. This means
    that Monte Carlo is model-free: no knowledge of **Markov decision process** (**MDP**)
    transitions or rewards is required. So, we don''t need to have modeled the environment
    previously, but the necessary information will be collected during an interaction
    with the environment (online learning). Monte Carlo methods learn directly from
    episodes of experience, where an episode of experience is a series of tuples (state,
    action, reward, and next state).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，*理解蒙特卡罗方法*，我们说过蒙特卡罗方法不需要环境模型来估计值函数或发现优秀的策略。这意味着蒙特卡罗是模型无关的：不需要**马尔可夫决策过程**（**MDP**）转移或奖励的知识。因此，我们之前不需要对环境进行建模，但在与环境的交互中会收集必要的信息（在线学习）。蒙特卡罗方法直接从经验中学习，其中一段经验是一系列元组（状态、动作、奖励和下一个状态）。
- en: 'In the following screenshot, we can see a comparison between a model-based
    and a model-free approach:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们可以看到模型基础和模型无关方法的比较：
- en: '![](img/fa0b4143-8c09-4725-8beb-3e383a63b79b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa0b4143-8c09-4725-8beb-3e383a63b79b.png)'
- en: Model-free methods can be applied to many reinforcement learning problems that
    do not require any model of the environment. Many model-free approaches try to
    learn the value function and infer from it the optimal policy, or by searching
    for the optimal policy directly in the policy parameter space itself. These approaches
    can also be classified as on-policy approaches or off-policy approaches. On-policy
    methods use current policy to generate actions, and to update the policy itself,
    while off-policy methods use a different exploration policy to generate actions
    with respect to the policy that is updated. What are the differences between the
    two approaches—model-free and model-based? In the following section, we will try
    to highlight the differences in order to choose the right approach to solve a
    problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 模型无关方法可以应用于许多不需要任何环境模型的强化学习问题。许多模型无关方法尝试学习值函数并从中推断出最优策略，或者直接在策略参数空间中搜索最优策略。这些方法也可以分类为在策略方法或离策略方法。在策略方法使用当前策略生成动作，并更新策略本身，而离策略方法则使用不同的探索策略生成动作，并相对于更新的策略。两种方法——模型无关和模型基础的差异是什么？在接下来的部分中，我们将尝试突出这些差异，以便选择解决问题的正确方法。
- en: Model-free versus model-based
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型无关与模型基础
- en: In this section, we will try to clarify the difference between these two approaches
    with a view to solving a problem with reinforcement learning. In such problems,
    the agent does not know all the elements of the system, which prevents him from
    planning a solution. In particular, the agent does not know how the environment
    will change in response to his actions. This is because the transition function, *T*,
    is not known. Furthermore, he does not even know what immediate reward he will
    receive in response to his actions. This is because he has not yet noted the reward
    function. The agent will have to explore the environment by trying to act, observing
    the answers, and somehow finding a good policy in order to obtain the best possible
    final reward.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将尝试通过强化学习来澄清这两种方法的差异，以解决问题。在这些问题中，代理人不知道系统的所有元素，这使他无法计划解决方案。特别是，代理人不知道环境将如何响应他的行动而发生变化。这是因为转移函数*T*是未知的。此外，他甚至不知道他的行动将获得什么即时奖励。这是因为他还没有注意到奖励函数。代理人将不得不通过尝试行动、观察回答，并在某种程度上找到一个好的策略，以获得可能的最佳最终奖励。
- en: 'The following question arises: If the agent knows neither the transition function
    nor the reward function, how can he derive good policy? To do so, two approaches
    can be followed: model-based, and model-free.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来出现一个问题：如果代理既不知道转移函数，也不知道奖励函数，那么他如何推导出好的策略呢？为此，可以采取两种方法：基于模型的方法和无模型的方法。
- en: In the first approach (model-based), the agent learns a model from the observations
    of the functioning of the environment from his observations, and then derives
    a solution using that model. For example, if the agent is in the state s1, and
    performs an action a1, he can observe the environmental transition that takes
    him to the state s2, thereby obtaining a reward r2\. This information can be used
    to update the evaluation of the transition matrix T (s2 | s1, a1) and R (s1, a1).
    This update can be performed using the supervised learning paradigm. Once the
    agent has adequately modeled the environment, he can use that model to find a
    policy. The algorithms that adopt this approach are called model-based.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法（基于模型的方法）中，代理从其观察到的环境功能中学习一个模型，然后利用该模型推导出解决方案。例如，如果代理处于状态s1，并执行一个动作a1，他可以观察到环境转移将其带到状态s2，从而获得奖励r2。此信息可以用于更新转移矩阵T(s2
    | s1, a1)和R(s1, a1)的评估。可以使用监督学习范式执行此更新。一旦代理充分建模了环境，他就可以使用该模型来找到一个策略。采用这种方法的算法被称为基于模型的方法。
- en: The second approach does not involve learning an environment model to find a
    good policy. One of the most classic examples is Q-learning, which we will analyze
    in detail in [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml), *Temporal
    Difference Learning*. This algorithm directly estimates the optimal values ​​of
    the usefulness of each action in each state, from which a policy can be derived
    by choosing the action with the highest value in the current state. Because these
    approaches do not learn an environmental model, they are called **model-free**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法不涉及学习环境模型来找到一个好的策略。最经典的例子之一是Q学习，我们将在[第7章](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml)中详细分析，*时间差学习*。该算法直接估计每个状态下每个动作的最优值，从中可以通过选择当前状态下值最高的动作来推导出策略。由于这些方法不学习环境模型，它们被称为**无模型**方法。
- en: Ultimately, if, after learning, the agent can make predictions about what will
    be the next status, and reward, before taking any action, then our algorithm is
    model-based. Otherwise, it is a model-free algorithm.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，如果在学习之后，代理能够在采取任何行动之前预测下一个状态和奖励，那么我们的算法就是基于模型的。否则，它就是一个无模型算法。
- en: Now, let's see how the action values in the Monte Carlo methods are updated.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看蒙特卡洛方法中的动作值是如何更新的。
- en: Estimation of action values
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作值的估计
- en: In general, Monte Carlo methods depend on repeated random sampling to obtain
    numerical results. To do this, they use randomness to solve deterministic problems.
    In our case, we will use random sampling of states and action-state pairs, we
    will look at the rewards, and then we will review the policy in an iterative way.
    The iteration of the process will converge on optimal policy as we explore every
    possible action-state pair.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，蒙特卡洛方法依赖于重复的随机采样来获得数值结果。为此，它们使用随机性来解决确定性问题。在我们的例子中，我们将使用状态和动作-状态对的随机采样，查看奖励，然后以迭代的方式回顾策略。随着我们探索每个可能的动作-状态对，过程的迭代将收敛到最优策略。
- en: 'For example, we could use the following procedure:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下程序：
- en: We assign a reward of +1 to a correct action, -1 to an incorrect action, and
    0 to a draw.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将正确的动作赋予奖励+1，错误的动作赋予奖励-1，平局赋予奖励0。
- en: We establish a table in which each key corresponds to a particular state-action
    pair, and each value is the value of that pair. This represents the average reward
    received for that action in that state.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们建立一个表格，其中每个键对应于一个特定的状态-动作对，每个值是该对的值。这代表了在该状态下执行该动作所获得的平均奖励。
- en: 'To solve the reinforcement learning problem, Monte Carlo methods estimate the
    value function on the basis of the total sum of rewards, obtained on average in
    past episodes. This assumes that the experience is divided into episodes, and
    that all episodes are composed of a finite number of transitions. This is because,
    in Monte Carlo methods, the estimate of the new values and the modification of
    the policy takes place once an episode is completed. Monte Carlo methods iteratively
    estimate policy and value function. In this case, however, each iteration cycle
    is equivalent to completing an episode—the new estimates of policy and value function
    occur episode by episode, as shown in the following diagram:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决强化学习问题，蒙特卡罗方法通过估计基于过去回合中平均获得的总奖励的值函数。这个假设是经验被分为若干回合，并且每个回合包含有限数量的转换。因为在蒙特卡罗方法中，新的值估计和策略修改发生在每个回合结束后。蒙特卡罗方法通过迭代方式估计策略和值函数。然而，在这种情况下，每个迭代周期相当于完成一次回合——新的策略和值函数估计是在每个回合后逐步进行的，如下图所示：
- en: '![](img/cb5a3260-5fb6-454b-a839-f0e620188085.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb5a3260-5fb6-454b-a839-f0e620188085.png)'
- en: The workflow includes the sampling of experience episodes and the subsequent
    updating of estimates at the end of each episode. Because of the many random decisions
    within each episode, these methods have a high variance, although these are unbiased.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流包括对经验回合的采样以及在每个回合结束时更新估计值。由于每个回合中有许多随机决策，这些方法的方差很高，尽管它们是无偏的。
- en: 'You may recall two processes, called **policy evaluation** and **policy improvement**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会回忆起两个过程，分别叫做**策略评估**和**策略改进**：
- en: Policy evaluation algorithms consist of applying an iterative method to the
    resolution of the Bellman equation. Since convergence is guaranteed to us only
    for k → ∞, we must be content to have good approximations by imposing a stopping
    condition.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略评估算法通过应用迭代方法解决贝尔曼方程。由于我们只能在k → ∞时保证收敛性，因此我们必须通过设置停止条件来获得良好的近似值。
- en: Policy improvement algorithms improve policy based on current values.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略改进算法基于当前的值来改进策略。
- en: As we said, the new estimates of policy and value function occur episode by
    episode; hence, the policy is updated only at the end of an episode.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，新的策略和值函数估计是在每个回合后逐步进行的；因此，策略仅在回合结束时更新。
- en: 'The following code block shows the pseudocode for Monte Carlo policy evaluation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了蒙特卡罗策略评估的伪代码：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Usually, the term **Monte Carlo** is used for estimation methods, the operations
    of which involve random components. In this case, the term **Monte Carlo** refers
    to reinforcement learning methods based on total reward averages. Unlike dynamic
    programming methods, which calculate the values for each state, Monte Carlo methods
    calculate values for each state-action pair because, in the absence of a model,
    only state values are not sufficient to decide which action is best performed
    in a certain state.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，**蒙特卡罗**一词用于描述涉及随机组件的估计方法。在这里，**蒙特卡罗**指的是基于总奖励平均值的强化学习方法。与动态规划方法通过计算每个状态的值不同，蒙特卡罗方法计算每个状态-动作对的值，因为在没有模型的情况下，只有状态值不足以决定在某个状态下执行哪个动作最优。
- en: After analyzing in detail how the Monte Carlo methods approach problems based
    on reinforcement learning, the time has come to see a practical case. To do this,
    we will use a very popular game—blackjack. We will see how to forecast the best
    game strategy using the Monte Carlo methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细分析了蒙特卡罗方法如何基于强化学习解决问题之后，接下来我们将查看一个实际案例。为此，我们将使用一个非常流行的游戏——二十一点。我们将看到如何使用蒙特卡罗方法预测最佳游戏策略。
- en: Blackjack strategy prediction using the Monte Carlo approach
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗方法进行二十一点策略预测
- en: Blackjack is a card game that takes place between the dealer and the players.
    Players who achieve a higher score than the dealer and do not exceed 21 win, while
    those players who exceed 21 **bust** and lose the game. Blackjack is usually played
    with a sabot made up of 2 French card decks, for a total of 104 cards. In the
    game, the ace can be worth 11, or 1, the pictures are worth 10, while the other
    cards are worth their face value. Seeds have no influence or value. The sum of
    the points, for the purpose of calculating the score, takes place by simple arithmetic
    calculation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 二十一点是一种在庄家和玩家之间进行的纸牌游戏。玩家如果得分超过庄家且不超过21点，则获胜；而得分超过21点的玩家**爆牌**并输掉游戏。二十一点通常使用由两副法式扑克牌组成的牌组（104张牌）。在游戏中，A牌可以算作11点或1点，图牌算作10点，其它牌按照面值计算。种子牌不具备影响或价值。点数的计算通过简单的算术运算来完成。
- en: 'Once the players have made their bet, the dealer, proceeding from left to right,
    assigns each of the players an uncovered card in each location played, assigning
    the last one to himself. He then does a second round of uncovered cards, without
    attributing one to himself. Once the distribution has taken place, the dealer
    reads in order the score of each player inviting them to show their game: they
    can ask for cards (hit) or stay (stick), at their discretion. If a player exceeds
    21, he loses, and the dealer will take the bet. Once the players have defined
    their scores, the dealer develops his game on the basis of a simple rule; that
    is, he finds a card if he has a score lower than 17, and once he gets or passes
    17, he must stop. If he passes 21, the dealer **busts** and must pay all the bets
    left on the table. Once all the scores have been defined, the dealer compares
    his own score with that of the other players, pays the combinations higher than
    his, collects the lower ones, and leaves the ones in a draw. The payment of winning
    bets is at par.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦玩家下注，庄家从左到右依次为每位玩家发放一张未盖面的牌，在每个位置上发一张，最后一张发给自己。然后庄家进行第二轮发牌，仍然不发给自己。一旦发牌结束，庄家按顺序读取每位玩家的得分，并邀请他们展示自己的牌：他们可以选择要牌（hit）或停牌（stick），完全由他们决定。如果某个玩家的点数超过21点，他就会输，庄家将赢得该玩家的赌注。一旦玩家们确定了自己的得分，庄家根据一个简单的规则进行游戏；也就是说，如果庄家的点数低于17点，他必须继续要牌，一旦得分达到或超过17点，他就必须停牌。如果庄家的点数超过21点，庄家**爆牌**，并且必须支付桌面上所有剩余的赌注。一旦所有得分都确定，庄家会将自己的得分与其他玩家进行比较，支付比自己高的组合，收取低于自己的赌注，并对平局的赌注不做处理。赢钱的赌注按面值支付。
- en: Blackjack game as an MDP
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二十一点游戏作为一个MDP
- en: Blackjack can be treated as an MDP because the status of the players can be
    defined by the value of the cards in possession, regardless of the cards you have.
    The status of the dealer is defined entirely by the value of the individual card
    displayed, and therefore the finished state of the entire game is defined by the
    status of the player, and the status of the dealer. Finally, the next state of
    the game is stochastically defined in its entirety from the current state and
    the player's action.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 二十一点可以视为一个MDP（马尔可夫决策过程），因为玩家的状态可以通过他们手中的牌的点数来定义，而与玩家自己手中的牌无关。庄家的状态完全由他面前显示的单张牌的点数决定，因此整个游戏的最终状态由玩家和庄家的状态共同决定。最后，游戏的下一状态完全由当前状态和玩家的行动以随机方式定义。
- en: We recall that a problem can be defined as MDP if the next state is a stochastic
    function of the current state only, and of the applied action. Furthermore, the
    MDPs are applicable to situations in which the decision-making space is limited
    and discreet, the results are uncertain, and the terminal status and the relative
    prizes are well defined. The solution—an MDP provides us with the optimal action
    to perform based on a process that aims to maximize the reward for each possible
    state.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾一下，问题可以被定义为MDP，如果下一状态仅仅是当前状态和执行的动作的随机函数。此外，MDP适用于那些决策空间有限且离散，结果不确定，且终止状态和相对奖赏明确的情况。MDP的解决方案为我们提供了基于一个旨在最大化每个可能状态的奖励的过程，来执行的最优行动。
- en: 'In the following sections, we will explain the code line by line:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐行解释代码：
- en: 'We will start defining the actions available:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将开始定义可用的操作：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s look at the meaning of *hit* and *stick*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下*要牌*和*停牌*的含义：
- en: '`HIT`: Take another card from the dealer.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HIT`：从庄家那里再拿一张牌。'
- en: '`STICK`: Take no more cards.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STICK`：不再拿牌。'
- en: 'Now, we have to simulate the drafting of the cards from the deck, this operation
    being carried out by the dealer who, as anticipated, distributes two cards for
    each player:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要模拟从牌堆中发牌的过程，这个操作由庄家执行，庄家会给每个玩家发两张牌：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `sample()` function takes a sample of the specified size from the elements
    passed using either with (`1`) or without (`0`) replacement.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()`函数从传递的元素中提取指定大小的样本，可以选择有放回（`1`）或无放回（`0`）抽样。'
- en: 'Now, let''s start generating an initial state randomly:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始随机生成一个初始状态：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The initial state and, in general, every possible state, is therefore represented
    by a vector with the three elements specified here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，初始状态以及一般的所有可能状态，由具有这里指定的三个元素的向量表示：
- en: '**Dealer card** (sample(10, 1)): A value between 1 and 10'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**庄家牌**（sample(10, 1)）：一个介于1到10之间的值'
- en: '**Player hand value **(sample(10, 1)): The sum of the player''s card values'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家手牌值**（sample(10, 1)）：玩家卡牌值的总和'
- en: '**Terminal state** (0): A binary value (0-1) that tells us if the hand is over'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**终止状态**（0）：一个二进制值（0-1），告诉我们手牌是否结束'
- en: State and reward update
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态和奖励更新
- en: In the following code block that we will analyze, we will update the state and
    the reward returned from the environment.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码块中，我们将分析并更新状态以及从环境中返回的奖励。
- en: 'Now, we will create the function that allows us to execute a single step of
    the process (`StepFunc`), based on the state (s) and the action (a) passed that
    returns a new state and reward obtained:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个函数来执行过程的单步操作（`StepFunc`），根据传递的状态（s）和动作（a），返回新的状态和奖励：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first check entered verifies whether we are in the terminal state (hand
    over), in which case the cycle is exited and the current state is returned and
    a reward equal to zero.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个检查项是验证我们是否处于终止状态（手牌结束），如果是，则退出循环，返回当前状态和奖励为零。
- en: 'Let''s check the action passed:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查传递的动作：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If the action to be performed is `HIT`, then a new card is discovered and the
    status and reward are updated.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要执行的动作是`HIT`，则会发现一张新卡，并更新状态和奖励。
- en: 'Let''s see what happens if the past action is `STICK`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看如果过去的动作是`STICK`会发生什么：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, the hand passes to the dealer who executes his game. You start by updating
    the player's terminal status to 1\. Then, the dealer status (`DealerWork` <- `FALSE`)
    and its current score are updated (`DealerSum <- s [1]`). From this point on,
    using a `while` loop runs the dealer game. To start, a new card is discovered.
    At this point, a first `IF` value is used to check whether the dealer has busted
    (`DealerSum``> 21`). In this case, the game ends with the victory of the player.
    The status of the dealer game is updated (`DealerWork <- TRUE`), as well as the
    total `BJReward` reward <- 1\. Otherwise, if `DealerSum> = 17`, the dealer stops
    his game and checks the status of the player. If the scores are equal (`DealerSum
    == s [2]`), the game ends in a draw (`BJReward <- 0`), otherwise, if the dealer's
    score is greater than that of the player, then `BJReward` of the player = -1 and
    the player loses. If the dealer 's score is less than that of the player,then `BJReward` of
    the player = 1 and the player wins. Finally, as already mentioned, the function
    returns the updated status and the final reward of the step.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，手牌传给庄家，庄家执行他的游戏。你开始通过将玩家的终止状态更新为1来开始。然后，庄家状态（`DealerWork` <- `FALSE`）及其当前分数被更新（`DealerSum
    <- s [1]`）。从这一点开始，使用`while`循环运行庄家的游戏。首先，发一张新卡。这时，使用第一个`IF`值检查庄家是否爆掉（`DealerSum
    > 21`）。如果爆掉了，游戏以玩家胜利结束。庄家游戏的状态更新（`DealerWork <- TRUE`），并且总奖励`BJReward` <- 1。如果不是，若`DealerSum
    >= 17`，则庄家停止游戏并检查玩家状态。如果分数相等（`DealerSum == s [2]`），则游戏以平局结束（`BJReward <- 0`）；否则，如果庄家的分数大于玩家的分数，玩家的`BJReward`
    = -1，玩家失败。如果庄家的分数小于玩家的分数，则玩家的`BJReward` = 1，玩家胜利。最后，如前所述，函数返回更新后的状态和步骤的最终奖励。
- en: Strategy prediction
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略预测
- en: 'Now is the time to predict the best strategy to be successful in the game:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是预测成功游戏最佳策略的时候了：
- en: 'After defining the function that updates the status and the reward, it is time
    to define the policy:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义了更新状态和奖励的函数后，现在是定义策略的时候了：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To define the policy, an `ActionsEpsValGreedy()` function was created. This
    function accepts the following inputs:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义策略，创建了一个`ActionsEpsValGreedy()`函数。该函数接受以下输入：
- en: '`s`: The state'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s`: 状态'
- en: '`QFunc`: The action-value function'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QFunc`: 动作值函数'
- en: '`EpsVAl`: The numeric value for epsilon'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EpsVAl`: epsilon的数值'
- en: This returns the action to be followed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回要遵循的动作。
- en: As we said in [Chapter 4](80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml), *Multi-Armed
    Bandit Models*, in the ε-greedy approach, we assume that, with a probability ε,
    a different action is chosen. This action is chosen with uniform probability between
    the n possible actions available. In this way, we introduce an element of exploration
    that improves performance. However, if two actions only exhibit a very small difference
    between their Q values, this algorithm will also choose only that action that
    has a higher probability than the others.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml)中所说的，*多臂老虎机模型*，在ε-贪婪方法中，我们假设以ε的概率选择不同的行动。这个行动是在n个可能的行动中均匀选择的。通过这种方式，我们引入了一种探索的元素，从而提高了性能。然而，如果两个行动之间的Q值差异非常小，那么该算法也会选择Q值更高的那个行动。
- en: 'Now, let''s load the `foreach` library:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载`foreach`库：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This package handles the `foreach` loop construct. The `foreach` command allows
    you to scroll through items in a collection without using an explicit counter.
    We recommend using the package for its return value, rather than for its side
    effects. Used in this way, it is similar to the standard `lapply` function, but
    does not require the evaluation of a function. Thus, the use of `foreach` facilitates
    the execution of the cycle in parallel.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该包处理`foreach`循环结构。`foreach`命令允许你在不使用显式计数器的情况下遍历集合中的项。我们建议使用该包的返回值，而不是它的副作用。以这种方式使用时，它类似于标准的`lapply`函数，但不需要评估函数。因此，使用`foreach`可以方便地并行执行循环。
- en: 'Finally, we can define the `MontecarloFunc` function that will guide us in
    solving the problem. This function accepts the following variable as input:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以定义`MontecarloFunc`函数，它将指导我们解决问题。此函数接受以下变量作为输入：
- en: '`NumEpisode`: Number of episodes to play'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumEpisode`：要播放的回合数'
- en: 'The `MontecarloFunc()` function returns the following values:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`MontecarloFunc()`函数返回以下值：'
- en: '`QFunc`: Updated action-value function'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QFunc`：更新的行动值函数'
- en: '`N`: Updated numbers of state-action visits'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`：更新后的状态-行动访问次数'
- en: 'Let''s analyze this in detail:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们详细分析一下：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the inputs are passed, the following variables are initialized:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦传入输入，以下变量将被初始化：
- en: '`QFunc`: Action-value function as an array containing the following variables:
    all possible card values (dim=10), all possible sum values (dim=21), and all possible
    actions (dim=2)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QFunc`：作为数组的行动值函数，包含以下变量：所有可能的卡牌值（dim=10）、所有可能的和值（dim=21）以及所有可能的行动（dim=2）'
- en: '`N`: Number of state-action visits'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`：状态-行动访问次数'
- en: '`N0`: Offset for N'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N0`：N的偏移量'
- en: 'Then, we will pass to define a policy:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义一个策略：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now use a cycle to perform all the episodes necessary to calculate
    the best strategy:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将使用一个循环执行所有必要的回合，以计算最佳策略：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s play a game for each episode:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们为每个回合玩一场游戏：
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Until such time as the game''s terminal status is 0 (game in progress), it
    performs the following operations:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏的终止状态为0（游戏进行中）之前，它执行以下操作：
- en: Chooses an action to perform based on the defined policy
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据定义的策略选择一个行动来执行
- en: Increases the visits counter
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加访问计数器
- en: Performs a step by calling the `StepFunc()` function
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用`StepFunc()`函数执行一个步骤
- en: Updates your rewards
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新你的奖励
- en: 'After doing this, we move on to update Q and N:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 做完这些后，我们继续更新Q和N：
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding code block includes the key element of the entire process—the
    updating mode of the Q function. In this case, an incremental approach has been
    adopted.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块包括了整个过程的关键元素——Q函数的更新模式。在这种情况下，采用了增量方法。
- en: 'In fact, the function *q* is updated using the following function:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事实上，函数*q*是使用以下函数更新的：
- en: '![](img/5cc73415-ea95-4a4c-b092-a4e1dd0bb02d.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cc73415-ea95-4a4c-b092-a4e1dd0bb02d.png)'
- en: 'Here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '*G*: This is a sum of the rewards.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*G*：这是奖励的总和。'
- en: 'Q: This is the action-value function.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q：这是行动值函数。
- en: '*N*: This is the number of state-action visits.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*：这是状态-行动访问次数。'
- en: 'Finally, the following results are returned:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，返回以下结果：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After defining all the necessary functions, it is time to run the simulation:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了所有必要的函数后，到了运行模拟的时候：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We simply passed the number of episodes required to obtain a good forecast of
    the best strategy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需传递获得良好预期的最佳策略所需的回合数。
- en: 'At this point, to analyze the results, we can draw a graph. However, it is
    necessary to adequately format the action-value function:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，为了分析结果，我们可以绘制图形。然而，需要适当格式化行动值函数：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To do this, we used the `apply()` function, which returns a vector or array,
    or a list of values obtained by applying a function to margins of an array or
    matrix.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用了`apply()`函数，该函数返回一个向量或数组，或者是通过将函数应用于数组或矩阵的边界所得到的值列表。
- en: 'Now, we can draw a chart:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以绘制一张图表：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `persp()` function was used. This function draws perspective plots of a
    surface over the x-y plane.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了`persp()`函数。此函数绘制了一个在x-y平面上表面的透视图。
- en: 'The following chart is plotted:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制了以下图表：
- en: '![](img/ea1fae42-e238-4354-bce7-d4b32b34f99e.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea1fae42-e238-4354-bce7-d4b32b34f99e.png)'
- en: In this way, we have a good estimate of the value function.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们对价值函数有了良好的估计。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, the basic concepts of the Monte Carlo method were explored.
    The Monte Carlo method entails looking for the solution to a problem, representing
    it as a parameter of a hypothetical population, and estimating this parameter
    by examining a sample of the population obtained through sequences of random numbers.
    Later, we highlighted the differences between the different methods that this
    technology makes available to us. The Monte Carlo prediction is used to estimate
    the value function, while Monte Carlo control is used to optimize the value function
    to make the value function more accurate than the estimation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了蒙特卡罗方法的基本概念。蒙特卡罗方法通过将问题的解决方案表示为假设总体的一个参数，并通过随机数序列获取的总体样本来估算这个参数。之后，我们强调了这种技术为我们提供的不同方法之间的差异。蒙特卡罗预测用于估算价值函数，而蒙特卡罗控制用于优化价值函数，使价值函数比估算值更准确。
- en: We then moved on to analyze the differences between algorithms based on a model-free
    approach, and those based on a model-based approach. Furthermore, we analyzed
    step by step the procedure that allows us to carry out the Monte Carlo policy
    evaluation. Finally, as a practical case of the concepts learned, a Blackjack
    strategy prediction involving the Monte Carlo method was performed.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续分析基于无模型方法的算法与基于有模型方法的算法之间的区别。此外，我们还逐步分析了进行蒙特卡罗策略评估的过程。最后，作为学习到的概念的实际案例，进行了涉及蒙特卡罗方法的二十一点策略预测。
- en: In the next chapter, we will learn about the different types of **temporal difference**
    (**TD**) learning algorithms. You will discover how to use TD algorithms to predict
    the future behavior of a system, and learn the basic concepts of the Q-learning
    algorithm. You will also learn to use the current best policy estimate to generate
    system behavior through the Q-learning algorithm.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习不同类型的**时序差分**（**TD**）学习算法。你将了解如何使用TD算法预测系统的未来行为，并学习Q学习算法的基本概念。你还将学习如何使用当前最佳策略估计通过Q学习算法生成系统行为。
