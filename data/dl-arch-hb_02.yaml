- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Designing Deep Learning Architectures
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计深度学习架构
- en: In the previous chapter, we went through the entire deep learning life cycle
    and understood what it means to make a deep learning project successful from end
    to end. With that knowledge, we are now ready to dive further into the technicalities
    of deep learning models. In this chapter, we will dive into common deep learning
    architectures used in the industry and understand the reasons behind each architecture’s
    design. For intermediate and advanced readers, this will be a brief recap to ensure
    alignment in the definitions of terms. For beginner readers, architectures will
    be presented in a way that is easy to digest so that you can get up to speed on
    the useful neural architectures in the world of deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们回顾了整个深度学习生命周期，并理解了从头到尾使深度学习项目成功的含义。有了这些知识后，我们现在准备进一步深入探讨深度学习模型的技术细节。本章将深入探讨业界常用的深度学习架构，并理解每种架构设计背后的原因。对于中级和高级读者，这将是一个简要回顾，以确保术语的定义一致性。对于初学者，本章将以易于理解的方式呈现架构，帮助你快速掌握深度学习领域中有用的神经网络架构。
- en: Grasping the methodologies behind a wide variety of architectures allows you
    to innovate custom architectures specific to your use case and, most importantly,
    gain the skill to choose an appropriate foundational architecture based on the
    data input or problem type.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 理解多种架构背后的方法论可以让你创新适用于特定使用案例的自定义架构，最重要的是，能够根据数据输入或问题类型选择合适的基础架构。
- en: 'In this chapter, the focus will be on the **Multilayer Perceptron** (**MLP**)
    network architecture. The comprehensive coverage of MLPs, along with some key
    concepts in general related to neural network implementations, such as gradients,
    activation functions, and regularization methods, will set the stage for exploring
    other, more complex architecture types in later chapters. Specifically, the following
    topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍**多层感知机**（**MLP**）网络架构。全面讲解 MLP 及一些与神经网络实现相关的关键概念，如梯度、激活函数和正则化方法，为后续章节中更复杂架构类型的探索奠定基础。具体来说，本章将涵盖以下内容：
- en: Exploring the foundations of neural networks using an MLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MLP 探索神经网络的基础
- en: Understanding neural network gradients
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经网络的梯度
- en: Understanding gradient descent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解梯度下降
- en: Implementing an MLP from scratch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始实现 MLP
- en: Implementing an MLP using deep learning frameworks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习框架实现 MLP
- en: Designing an MLP
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计 MLP
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些使用**Python**编程语言的实践实现。为完成这些内容，你需要在计算机上安装以下库：
- en: '`pandas`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`Matplotlib`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Matplotlib`'
- en: '`Seaborn`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seaborn`'
- en: '`Scikit-learn`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Scikit-learn`'
- en: '`NumPy`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumPy`'
- en: '`Keras`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Keras`'
- en: '`PyTorch`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PyTorch`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可以在 GitHub 上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2)。
- en: Exploring the foundations of neural networks using an MLP
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MLP 探索神经网络的基础
- en: A deep learning architecture is created when at least three perceptron layers
    are used, excluding the input layer. A perceptron is a single-layer network consisting
    of neuron units. Neuron units hold a bias variable and act as nodes for vertices
    to be connected. These neurons will interact with other neurons in a separate
    layer with weights applied to the connections/vertices between neurons. A perceptron
    is also known as a **fully connected layer** or **dense layer**, and MLPs are
    also known as **feedforward neural networks** or **fully connected** **neural
    networks**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构是通过至少使用三个感知机层（不包括输入层）来创建的。感知机是由神经元单元组成的单层网络。神经元单元包含一个偏置变量，并作为与其他神经元连接的节点。这些神经元将与不同层中的其他神经元进行交互，连接/节点之间应用权重。感知机也称为**全连接层**或**密集层**，而
    MLP 也被称为**前馈神经网络**或**全连接****神经网络**。
- en: Let’s refer back to the MLP figure from the previous chapter to get a better
    idea.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾上一章的 MLP 图示，以便更好地理解。
- en: "![Figure 2.1 – Simple deep learning architecture, also called \uFEFFan MLP](img/B18187_02_001.jpg)"
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 简单的深度学习架构，也叫做 MLP](img/B18187_02_001.jpg)'
- en: Figure 2.1 – Simple deep learning architecture, also called an MLP
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 简单的深度学习架构，也叫做 MLP
- en: The figure shows how three data column inputs get passed into the input layer,
    then subsequently get propagated to the hidden layer, and finally, through the
    output layer. Although not depicted in the figure, an additional activation function
    is applied at the hidden and output layer outputs. The activation function at
    the hidden layers adds non-linearity to the model and allows the neural network
    to capture non-linear relationships between the input data and output data. The
    activation function used at the output layer depends on the problem type and will
    be discussed in more detail in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised* *Deep Learning*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了如何将三个数据列输入传递到输入层，然后逐步传播到隐藏层，最后通过输出层。尽管图中没有展示，但在隐藏层和输出层的输出上应用了额外的激活函数。隐藏层的激活函数为模型添加了非线性，使神经网络能够捕捉输入数据和输出数据之间的非线性关系。输出层使用的激活函数取决于问题类型，具体内容将在[*第八章*](B18187_08.xhtml#_idTextAnchor125)《探索监督式深度学习》中更详细地讨论。
- en: Before we dive into the relevant hidden activation methods, we need to first
    be aware of the vanishing gradient problem. The vanishing gradient problem is
    a challenge that arises when gradients of the loss function with respect to the
    model’s parameters become very small during backpropagation. This can lead to
    slow learning and poor convergence, as the weights update minimally or not at
    all. The vanishing gradient problem is particularly prominent when using activation
    functions that squash input values into a narrow range. To address this issue,
    the **Rectified Linear Unit** (**ReLU**) activation function has been widely adopted
    due to its ability to mitigate the vanishing gradient problem to a certain extent.
    ReLU maps negative values into zeros and maintains positive values, as depicted
    in *Figure 2**.2*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论相关的隐藏激活方法之前，我们首先需要了解梯度消失问题。梯度消失问题是指在反向传播过程中，损失函数相对于模型参数的梯度变得非常小。这可能导致学习速度缓慢和收敛效果差，因为权重更新极少甚至没有更新。梯度消失问题在使用将输入值压缩到狭窄范围的激活函数时尤为突出。为了解决这个问题，**Rectified
    Linear Unit** (**ReLU**) 激活函数得到了广泛应用，因为它能够在一定程度上缓解梯度消失问题。ReLU 将负值映射为零，保持正值，如*图
    2.2* 所示。
- en: "![Figure 2.2 – ReLU, Leaky ReLU, and PReLU inpu\uFEFFt/output graph plot](img/B18187_02_002.jpg)"
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – ReLU、Leaky ReLU 和 PReLU 输入/输出图](img/B18187_02_002.jpg)'
- en: Figure 2.2 – ReLU, Leaky ReLU, and PReLU input/output graph plot
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – ReLU、Leaky ReLU 和 PReLU 输入/输出图
- en: 'Apart from ReLU, there are other useful hidden layer activation functions that
    can help alleviate the vanishing gradient problem while offering various benefits.
    Some of these include the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 ReLU，还有其他一些有用的隐藏层激活函数，它们有助于缓解梯度消失问题，同时提供各种优点。以下是其中一些：
- en: '**Leaky ReLU**: Leaky ReLU is a variation of the ReLU function that allows
    a small, non-zero gradient for negative input values. This helps mitigate the
    “dying ReLU” problem, where neurons become inactive and stop learning if their
    input values are consistently negative. Leaky ReLU introduces a small slope for
    negative inputs, ensuring that gradients do not vanish entirely.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU**：Leaky ReLU 是 ReLU 函数的一种变体，它允许负输入值有一个小的、非零的梯度。这有助于缓解“死亡 ReLU”问题，当神经元的输入值持续为负时，神经元会变得不活跃并停止学习。Leaky
    ReLU 为负输入值引入了一个小的斜率，确保梯度不会完全消失。'
- en: '**Parametric ReLU** (**PReLU**): PReLU is another variation of the ReLU function,
    where the negative slope is learned during the training process, allowing the
    model to adapt its behavior. This flexibility can lead to better performance but
    at the cost of increased complexity and the risk of overfitting.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parametric ReLU** (**PReLU**)：PReLU 是 ReLU 函数的另一种变体，其中负斜率在训练过程中进行学习，使得模型能够自适应其行为。这种灵活性可能会提高性能，但代价是增加了复杂性并存在过拟合的风险。'
- en: Additionally, we will be exploring more hidden activation functions as we dive
    into different prominent architectures in this book. Each of these activation
    functions has its strengths and weaknesses, and the choice of activation function
    depends on the specific problem being addressed and the architecture being employed.
    Understanding, experimenting with, and assessing these activation functions is
    crucial for selecting the most suitable one for a given task within a neural network’s
    hidden layers. Furthermore, the recommended method to assess any model-building-related
    experiments will be explored in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised* *Deep Learning*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着我们在本书中深入探讨不同的主流架构，我们将探索更多的隐藏激活函数。这些激活函数各有优缺点，激活函数的选择取决于要解决的具体问题和所使用的架构。理解、实验和评估这些激活函数对于在神经网络的隐藏层中选择最适合的激活函数至关重要。此外，评估与模型构建相关的任何实验的推荐方法将在[*第8章*](B18187_08.xhtml#_idTextAnchor125)中进行探讨，*探索监督式深度学习*。
- en: 'Moving on, the process of propagating values from one layer to another is called
    a forward pass or forward propagation, where the formula can be generally defined
    as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从一层传播到另一层的过程称为前向传递或前向传播，其公式一般可以定义如下：
- en: a = g( ∑ n=0 neuronswx + b)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: a = g( ∑ n=0 神经元wx + b)
- en: Here, a represents the outputs of the neural network layer (called an **activation**),
    g represents the non-linear activation function, w represents the weights between
    neuron connections, x represents the input data or activation, and b represents
    the bias of the neuron. Different types of neural network layers consume and output
    data in different ways but generally still use this formula as a foundation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，a表示神经网络层的输出（称为**激活**），g表示非线性激活函数，w表示神经元连接之间的权重，x表示输入数据或激活值，b表示神经元的偏置。不同类型的神经网络层以不同方式消耗和输出数据，但通常仍以此公式为基础。
- en: Understanding neural network gradients
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络梯度
- en: The goal of machine learning for an MLP is to find the weights and biases that
    will effectively map the inputs to the desired outputs. The weights and biases
    generally get initialized randomly. In the training process, with a provided dataset,
    they get updated iteratively and objectively in batches to minimize the loss function,
    which uses gradients computed with a method called **backward propagation**, also
    known as **backpropagation**. A batch is a subset of the dataset used for training
    or evaluation, allowing the neural network to process the data in smaller groups
    rather than the entire dataset at once. The loss function is also known as the
    error function or the cost function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MLP（多层感知机）中的机器学习目标是找到能够有效地将输入映射到期望输出的权重和偏置。权重和偏置通常是随机初始化的。在训练过程中，使用提供的数据集，它们会以批次的方式进行迭代更新，以最小化损失函数，该函数使用一种叫做**反向传播**（backpropagation）的方法计算梯度。批次是用于训练或评估的数据集的一个子集，允许神经网络以较小的组处理数据，而不是一次性处理整个数据集。损失函数也被称为误差函数或代价函数。
- en: Backpropagation is a technique to find out how sensitive a change of weights
    and bias of every neuron is to the overall loss by using the partial derivative
    of the loss with respect to the weights and biases. Partial derivatives from calculus
    are a measure of the rate of change of a function with respect to a variable,
    which uses a technique called **differentiation**, and is effectively applied
    in neural networks. A convenient method called the **chain rule** allows you to
    obtain the derivative of neural networks by calculating the partial derivatives
    of each function, a forward pass in the case of neural networks, separately. To
    be clear, derivatives can be called the sensitivity of change, the gradients,
    and the rate of change. The idea is that when we know which model parameter affects
    the error the most, we can update its weights proportionally according to its
    magnitude and direction. Let’s take a simple case of a two-layer MLP with one
    neuron in each layer as an example to get an idea of this, as depicted in *Figure
    2**.3*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种技术，用来找出每个神经元的权重和偏置变化对总体损失的敏感度，方法是利用损失函数相对于权重和偏置的偏导数。来自微积分的偏导数是衡量函数相对于某一变量变化速率的工具，使用一种叫做**微分**的技术，在神经网络中得到有效应用。一种方便的方法叫做**链式法则**，它允许通过分别计算每个函数的偏导数，即神经网络中的前向传播，来得到神经网络的导数。明确来说，导数可以称为变化的敏感度、梯度或变化率。其基本思想是，当我们知道哪个模型参数对误差的影响最大时，可以根据其幅度和方向相应地更新其权重。让我们以一个简单的两层多层感知机（MLP）为例，每一层有一个神经元，如*图
    2.3*所示，来理解这一过程。
- en: '![Figure 2.3 – A diagram of a two-layer MLP](img/B18187_02_003.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 两层多层感知机的示意图](img/B18187_02_003.jpg)'
- en: Figure 2.3 – A diagram of a two-layer MLP
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 两层多层感知机的示意图
- en: 'For clarity, **w** indicates the weight of neuron connections, **b** indicates
    the bias of the neurons, and **L** indicates layers. Different problem types require
    a different loss function, but for explanation purposes, let’s assume this is
    an MLP for a regression problem, where we will use the mean squared error as a
    loss function to compute the loss component from the final layer activation and
    the numerical target value. The loss function can then be defined as the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰，**w**表示神经元连接的权重，**b**表示神经元的偏置，**L**表示层级。不同问题类型需要不同的损失函数，但为了说明问题，我们假设这是一个回归问题的MLP，在这里我们使用均方误差作为损失函数，从最终层激活值和数值目标值中计算损失组件。损失函数可以定义为以下形式：
- en: L =  1 _ n  ∑ i=1 n (a2 − y) 2
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: L =  1 _ n  ∑ i=1 n (a2 − y) 2
- en: 'Here, n is the total size of neurons. To obtain the rate of change of the loss
    function with respect to the output layer weight **w2**, which is  δL _ δw2, let’s
    define the formula based on the chain rule. Consider the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，n表示神经元的总数。为了获得损失函数相对于输出层权重**w2**的变化率δL _ δw2，让我们基于链式法则定义公式。考虑以下内容：
- en: z2 = w2 ∙ a1 + b2
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: z2 = w2 ∙ a1 + b2
- en: 'So, if a2 = g(z2), where g is a ReLU function, the gradients with respect to
    the output layer weight w2 will be defined as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果a2 = g(z2)，其中g是ReLU函数，那么输出层权重w2的梯度将定义如下：
- en: δL _ δw2  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ dw2
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δw2  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ dw2
- en: 'The rate of change of the loss function with respect to w2 can be computed
    by multiplying the three independent change components: namely, the change of
    the loss function with respect to the second-layer outputs, the change of the
    activation outputs with respect to a wrapped z2 that is a forward pass without
    the activation, and the change of the wrapped z2 with respect to w2\. Let’s define
    these components next. Now consider the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数相对于w2的变化率可以通过将三个独立的变化组件相乘来计算：即损失函数相对于第二层输出的变化，激活输出相对于没有激活的包装z2的变化，以及包装z2相对于w2的变化。接下来我们定义这些组件。现在考虑以下内容：
- en: e = a2 − y
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: e = a2 − y
- en: 'Based on the chain rule, the first change component will be defined as the
    following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于链式法则，第一个变化组件将被定义为以下内容：
- en: δL _ δa2  =  δL _ δe  ⋅  δe _ δa2
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δa2  =  δL _ δe  ⋅  δe _ δa2
- en: δL _ δe  = 2e
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δe  = 2e
- en: δe _ δa2  = 1
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: δe _ δa2  = 1
- en: 'Putting this in the simplified component representation will result in the
    following equation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个简化的组件表示放入公式中，得到以下方程：
- en: δL _ δa2  =  2 _ n (a2 − y)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δa2  =  2 _ n (a2 − y)
- en: 'For the second change component, it can be defined with the following formula:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个变化组件，它可以通过以下公式定义：
- en: δa2 _ δz2  = g ′ (z2) = 1
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: δa2 _ δz2  = g ′ (z2) = 1
- en: There is no activation function applied at the output layer in this case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，输出层没有应用激活函数。
- en: 'For the third and last change component, it can be defined with the following
    formula:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三个也是最后一个变更组件，它可以用以下公式来定义：
- en: δz2 _ δw2  = a1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: δz2 _ δw2  = a1
- en: 'Finally, placing the simplified representation of the three components into
    the formula to obtain the gradients of the output layer weights w2 will result
    in the following equation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将三个组件的简化表示代入公式中，以获得输出层权重 w2 的梯度，结果为以下方程：
- en: δL _ δw2 =  2 _ n(a2 − y) ∙ a1
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δw2 =  2 _ n(a2 − y) ∙ a1
- en: 'All you need to do now is to plug in the actual values to obtain the layer
    2 weight gradients. The same formula structure can be adapted similarly to the
    hidden layer’s weight w1 as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要做的就是将实际值代入公式，以获得第二层的权重梯度。相同的公式结构可以类似地应用于隐藏层的权重 w1，如下所示：
- en: a1 = g(z1)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: a1 = g(z1)
- en: z1 = w1 ∙ a0 + b1
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: z1 = w1 ∙ a0 + b1
- en: δL _ δw1  =  δL _ δa1  ∙  δa1 _ δz1  ∙  δz1 _ δw1
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δw1  =  δL _ δa1  ∙  δa1 _ δz1  ∙  δz1 _ δw1
- en: 'After expanding  δL _ δa1 using the chain rule, the formula can then be the
    following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用链式法则展开 δL _ δa1 后，公式可以变为以下形式：
- en: δL _ δw1  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ δa1  ∙  δa1 _ δz1  ∙  δz1 _ δw1
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δw1  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ δa1  ∙  δa1 _ δz1  ∙  δz1 _ δw1
- en: 'The additional individual components can be defined as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的各个组件可以定义如下：
- en: δz2 _ δa1  = w2
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: δz2 _ δa1  = w2
- en: δa1 _ δz1  = g ′ (z1) = 0 if a2 < 0, 1 if a2 > 0
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: δa1 _ δz1  = g ′ (z1) = 0 如果 a2 < 0，1 如果 a2 > 0
- en: δz1 _ δw1  = a0
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: δz1 _ δw1  = a0
- en: 'a0 here is the input data. Now, let’s define the gradient of the hidden layer
    weights w1 with the representation where we can plug in actual values to compute
    it, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 a0 是输入数据。现在，我们定义隐藏层权重 w1 的梯度，并用表示法来代入实际值进行计算，如下所示：
- en: δL _ δw1  =  2 _ n (a2 − y) ∙ w2 ∙ g ′ (z1) ∙ a0
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δw1  =  2 _ n (a2 − y) ∙ w2 ∙ g ′ (z1) ∙ a0
- en: 'The same process can be repeated for the bias term to obtain its gradients.
    Only the partial derivative of z with respect to the weights needs to be replaced
    with a partial derivative of z with respect to the biases, as shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的过程可以重复应用于偏置项，以获得其梯度。只需将 z 关于权重的偏导数替换为 z 关于偏置的偏导数，如下所示：
- en: δz2 _ δb2  =  δz1 _ δb1  = 1
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: δz2 _ δb2  =  δz1 _ δb1  = 1
- en: δL _ δb2  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ δb2
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δb2  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ δb2
- en: δL _ δb2  = 2(a2 − y)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δb2  = 2(a2 − y)
- en: δL _ δb1  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ δa1  ∙  δa1 _ δz1  ∙  δz1 _ δb1
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δb1  =  δL _ δa2  ∙  δa2 _ δz2  ∙  δz2 _ δa1  ∙  δa1 _ δz1  ∙  δz1 _ δb1
- en: 'Now, let’s define the gradient of the first bias term with the representation
    where we can plug in actual values to compute it, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义第一个偏置项的梯度，并用表示法来代入实际值进行计算，如下所示：
- en: δL _ δb1  =  2 _ n  ∙ w2 ∙ (a2 − y) ∙ g ′ (z1)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: δL _ δb1  =  2 _ n  ∙ w2 ∙ (a2 − y) ∙ g ′ (z1)
- en: The previously defined formulae were meant to be specific to the example neural
    network for layers with one neuron. In practical usage, these layers usually contain
    more than one neuron in each of the layers. To compute the loss and derivatives
    for layers with more than one neuron, and for more than one data sample, you simply
    need to obtain an average of all the values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的公式是特定于示例神经网络的，其中每层只有一个神经元。在实际应用中，这些层通常包含多个神经元。要计算包含多个神经元和多个数据样本的层的损失和导数，你只需对所有值进行平均。
- en: Once the gradients or derivatives are obtained, different strategies can be
    used to update the weights. The algorithm used to optimize the weights and biases
    of the neural network is called the optimizer. There are many optimizer options
    today and each has its own pros and cons. As gradients are used to optimize weights
    and biases, this process of optimization is called **gradient descent**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得了梯度或导数，就可以使用不同的策略来更新权重。用于优化神经网络权重和偏置的算法称为优化器。如今有许多优化器选项，每种都有自己的优缺点。由于梯度用于优化权重和偏置，因此这个优化过程被称为**梯度下降**。
- en: Understanding gradient descent
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度下降
- en: A good way to think about loss for a deep learning model is that it exists in
    a three-dimensional loss landscape that has many different hills and valleys,
    with valleys being more optimal, as shown in *Figure 2**.4*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 思考深度学习模型的损失，一种好的方式是将其视为一个三维损失景观，景观中有许多不同的山丘和谷底，谷底代表着更优的结果，如*图 2.4*所示。
- en: '![Figure 2.4 – An example loss landscape](img/B18187_02_004.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 一个示例损失景观](img/B18187_02_004.jpg)'
- en: Figure 2.4 – An example loss landscape
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 一个示例损失景观
- en: 'In reality, however, we can only approximate these loss landscapes as the parameter
    values of the neural networks can exist in an infinite number of ways. The most
    common way practitioners use to monitor the behavior of loss during each epoch
    of training and validation is to simply plot a two-dimensional line graph with
    the *x* axis being the epochs executed and the *y* axis being the loss performance.
    An epoch is a single iteration through the entire dataset during the training
    process of a neural network. The loss landscape in *Figure 2**.4* is an approximation
    of the loss landscape in three dimensions of a neural network. To visualize the
    three-dimensional loss landscape in *Figure 2**.4*, we can use two randomly initialized
    parameters and one fully trained parameter from the same neuron positions within
    the neural network. The loss can be calculated by performing a weighted summation
    of these three parameters. The weight of the fully trained parameter remains constant,
    while the weights of the two randomly initialized parameters are adjusted. This
    process allows us to approximate the 3D loss landscape shown in *Figure 2**.4*.
    In this figure, *x* axis and *y* axis are the weights of the two randomly initialized
    parameters of the same neural network and the *z* axis is the loss value. The
    goal of gradient descent is to attempt to find the *global* deepest valleys and
    not be stuck in *local* valleys or *local* minima. The gradients computed provide
    the suggested directions needed to nudge and update the weights and biases iteratively.
    One thing to note is that gradients provide the direction to increase the loss
    function in the fastest way, so for the descent, the parameters are subtracted
    from the gradients. Let’s go through a simple form of gradient descent that controls
    how the weights and biases should be updated:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上我们只能近似这些损失景观，因为神经网络的参数值可能存在无限多种组合。实践者通常用来监控每个训练和验证周期中损失表现的方式是简单地绘制一个二维折线图，其中
    *x* 轴表示执行的周期数，*y* 轴表示损失表现。一个周期是神经网络训练过程中对整个数据集的单次迭代。*图 2.4* 中的损失景观是神经网络三维损失景观的近似。为了可视化
    *图 2.4* 中的三维损失景观，我们可以使用两个随机初始化的参数和一个完全训练好的参数，这些参数来自神经网络中的相同神经元位置。损失可以通过对这三个参数进行加权求和来计算。完全训练好的参数的权重保持不变，而两个随机初始化的参数的权重则会被调整。这个过程使我们能够近似
    *图 2.4* 中所示的三维损失景观。在这个图中，*x* 轴和 *y* 轴分别是来自同一个神经网络的两个随机初始化参数的权重，而 *z* 轴则是损失值。梯度下降的目标是尝试找到
    *全局* 最深的谷底，而不是被困在 *局部* 谷底或 *局部* 最小值中。计算出来的梯度提供了建议的方向，用于逐步调整和更新权重与偏置。需要注意的是，梯度提供了最快增大损失函数的方向，因此在梯度下降时，参数会从梯度中被减去。让我们通过一个简单的梯度下降形式来了解如何更新权重和偏置：
- en: w = w − α ∙  δL _ δw
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: w = w − α ∙  δL _ δw
- en: b = b − α ∙  δL _ δb
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: b = b − α ∙  δL _ δb
- en: Here, α refers to the learning rate, which controls how aggressive you want
    the deep learning model to be. A learning rate is a hyperparameter that controls
    the speed at which a neural network learns and updates its weights and biases
    during the optimization process. The higher the learning rate, the bigger the
    steps taken by the deep learning model in the loss landscape. By iteratively applying
    this parameter update step, the neural network will slowly move downhill so that
    the learned set of parameters can allow the network to effectively map the input
    to the desired target values. The gradients are obtained for all the data samples
    and averaged together to obtain a single update direction for the weight and biases
    update.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α指的是学习率，它控制着你希望深度学习模型的训练进度。学习率是一个超参数，决定了神经网络在优化过程中学习和更新权重与偏置的速度。学习率越高，深度学习模型在损失函数中的步伐就越大。通过反复应用这个参数更新步骤，神经网络会慢慢地朝着下降的方向移动，从而使学习到的参数集能够有效地将输入映射到期望的目标值。所有数据样本的梯度会被计算并求平均，从而获得一个更新方向，用于更新权重和偏置。
- en: Datasets can sometimes be too big and cause a slow learning process from basic
    gradient descent due to the need to compute the gradients from every sample before
    an update can be done to the neural network parameters. **Stochastic gradient
    descent** (**SGD**) was created to tackle this problem. The idea is simply to
    learn from the dataset in batches and iteratively learn the entire dataset with
    a different data batch partition instead of waiting for the gradients to be obtained
    from the entire dataset before updating the parameters of the network. This way,
    the learning process can be efficient even for a big-sized dataset with the added
    benefit of seeing initial results quickly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有时可能过大，导致基础梯度下降算法的学习过程变慢，因为在更新神经网络参数之前，需要从每个样本计算梯度。**随机梯度下降法**（**SGD**）就是为了解决这个问题而创建的。其核心思想是通过批次学习数据集，并通过不同的数据批次划分来迭代学习整个数据集，而不是等待从整个数据集获取梯度再更新网络参数。这样，即使数据集非常大，学习过程也能保持高效，并且能够快速看到初步结果。
- en: 'There are a lot more variations of gradient descent that offer different advantages
    and are suited for specific situations. Here, we will list gradient descent algorithms
    that, on average across a variety of datasets, work well and are relevant:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法有很多变种，每种算法都有不同的优点，适用于特定的情况。在这里，我们将列出一些在各种数据集上表现良好且相关的梯度下降算法：
- en: '**Momentum**: A variation of SGD, Momentum incorporates a “momentum” term that
    helps the optimizer navigate through the loss landscape more effectively. This
    momentum term is a moving average of the gradients, which helps the optimizer
    overcome local minima and converge faster. The momentum term also adds some inertia
    to the optimizer, causing it to take bigger steps in directions that have consistent
    gradients, which can speed up convergence.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动量法**（**Momentum**）：动量法是SGD的一种变体，它引入了一个“动量”项，帮助优化器更有效地在损失函数的空间中导航。这个动量项是梯度的移动平均值，帮助优化器克服局部最小值并加速收敛。动量项还为优化器增加了一些惯性，使其在具有一致梯度的方向上采取更大的步伐，从而加速收敛。'
- en: '**Root Mean Square Propagation** (**RMSProp**): RMSProp is an adaptive learning
    rate optimization algorithm that adjusts the learning rate for each parameter
    individually. By dividing the learning rate by an exponentially decaying average
    of squared gradients, RMSProp helps to prevent the oscillations observed in the
    convergence of SGD. This results in a more stable and faster convergence toward
    the optimal solution.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根传播法**（**RMSProp**）：RMSProp是一种自适应学习率优化算法，它为每个参数单独调整学习率。通过将学习率除以平方梯度的指数衰减平均值，RMSProp有助于防止在SGD收敛过程中观察到的震荡现象。这使得收敛过程更加稳定且更快地趋向最优解。'
- en: '**Adaptive Moment Estimation** (**Adam**): Adam is another popular optimization
    algorithm that combines the advantages of both Momentum and RMSProp. It maintains
    separate adaptive learning rates for each parameter, as well as incorporating
    a momentum term. This combination allows Adam to converge quickly and find more
    accurate solutions, making it a popular choice for many deep learning tasks.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应矩估计**（**Adam**）：Adam是另一种流行的优化算法，它结合了Momentum和RMSProp的优点。它为每个参数维持独立的自适应学习率，并且还包含一个动量项。这种组合使得Adam能够快速收敛并找到更精确的解，因此它成为许多深度学习任务中的常见选择。'
- en: While there are many gradient descent algorithms available, choosing the right
    one depends on the specific problem and dataset at hand. In general, Adam is often
    recommended as a good starting point due to its adaptive nature and combination
    of Momentum and RMSProp features. To determine the best fit for your specific
    deep learning task, it is essential to experiment with different algorithms and
    their hyperparameters and validate their performance. Next, we will code up an
    MLP using Python.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多梯度下降算法可供选择，但选择合适的算法取决于具体问题和数据集。一般来说，由于Adam算法的自适应特性及其结合了Momentum和RMSProp的优点，它通常被推荐作为一个好的起点。为了确定最适合您特定深度学习任务的算法，必须尝试不同的算法及其超参数，并验证它们的性能。接下来，我们将使用Python编写一个多层感知机（MLP）模型。
- en: Implementing an MLP from scratch
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始实现多层感知机（MLP）
- en: 'Today, the process to create a neural network and its layers along with the
    backpropagation process has been encapsulated in deep learning frameworks. The
    differentiation process has been automated, where there is no actual need to define
    the derivative formulas manually. Removing the abstraction layer provided by the
    deep learning libraries will help to solidify your understanding of neural network
    internals. So, let’s create this neural network manually and explicitly with the
    logic to forward pass and backward pass instead of using the deep learning libraries:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，创建神经网络及其层并进行反向传播的过程已经被封装在深度学习框架中。微分过程已经被自动化，不再需要手动定义导数公式。移除深度学习库提供的抽象层将有助于巩固你对神经网络内部结构的理解。因此，让我们手动并显式地创建这个神经网络，并实现前向传播和反向传播的逻辑，而不是使用深度学习库：
- en: 'We’ll start by importing `numpy` and the methods from the scikit-learn library
    to load sample datasets and perform data partitioning:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入 `numpy` 以及来自 scikit-learn 库的方法，以加载示例数据集并执行数据分区：
- en: '[PRE0]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define ReLU, the method that makes an MLP non-linear:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义 ReLU，这是使 MLP 非线性的方法：
- en: '[PRE1]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s define partially the class needed to initialize an MLP model with
    a single hidden layer and an output layer that can perform a forward pass. The
    layers are represented by weights, where `w1` is the weight of the hidden layer,
    and `w2` is the weight of the output layer. Additionally, `b1` is the bias for
    the connection between the input layer and the hidden layer and `b2` is the bias
    for the connection between the hidden layer and the output layer:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们部分定义初始化 MLP 模型所需的类，该模型有一个隐藏层和一个输出层，能够进行前向传播。层由权重表示，其中 `w1` 是隐藏层的权重，`w2`
    是输出层的权重。此外，`b1` 是输入层和隐藏层之间连接的偏置，`b2` 是隐藏层和输出层之间连接的偏置：
- en: '[PRE2]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To allow the MLP to learn, we will now implement the backward pass method to
    generate the average gradients for the biases and weights:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了让 MLP 学习，我们将现在实现反向传播方法，生成偏置和权重的平均梯度：
- en: '[PRE3]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the derivative of the ReLU function is f′(x) = 1 if x > 0 and f′(x)
    = 0 if x <= 0.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，ReLU 函数的导数是 f′(x) = 1 如果 x > 0，f′(x) = 0 如果 x <= 0。
- en: 'For the last class method, we will implement the gradient descent step utilizing
    the average gradients from the backward pass, which is the process that allows
    the bias and weights to be updated:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于最后一个类方法，我们将实现梯度下降步骤，利用反向传播得到的平均梯度，这是更新偏置和权重的过程：
- en: '[PRE4]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have made a proper class for the MLP manually, let’s set up a dataset
    and attempt to learn from it. The structure of MLP only allows for tabular structured
    dataset types that are both one-dimensional and numerical, so we will be using
    a dataset called `diabetes`, which contains 10 numerical features, that is, age,
    sex, body mass index, average blood pressure, and 6 blood serum measurements,
    as inputs along with a quantitative measure of diabetes disease progression as
    the target data. The data is conveniently saved in the scikit-learn library, so
    let’s first load the input DataFrame:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经手动创建了一个正确的 MLP 类，让我们设置数据集并尝试从中学习。MLP 的结构只允许使用表格结构的数据集，这些数据集既是单维的又是数值型的，因此我们将使用一个名为
    `diabetes` 的数据集，它包含 10 个数值特征，即年龄、性别、体重指数、平均血压和 6 项血清测量数据，作为输入数据，同时有一个衡量糖尿病疾病进展的定量指标作为目标数据。该数据集已经方便地保存在
    scikit-learn 库中，因此让我们首先加载输入的 DataFrame：
- en: '[PRE5]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will convert the DataFrame into NumPy array values so that it is ready
    to be used by a neural network:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将把 DataFrame 转换为 NumPy 数组值，以便准备好供神经网络使用：
- en: '[PRE6]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The final step for loading the data is to load the target data from the diabetes
    data and make sure it has an additional outer dimension as the PyTorch model outputs
    its predictions in this way:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据的最后一步是从糖尿病数据中加载目标数据，并确保它有一个额外的外部维度，因为 PyTorch 模型以这种方式输出其预测结果：
- en: '[PRE7]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s partition the dataset into 80% for training and 20% for validation:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们将数据集分为 80% 用于训练，20% 用于验证：
- en: '[PRE8]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that the data is prepared with training and evaluation partitions, let’s
    initialize an MLP model from our defined class with a single 20-neuron hidden
    layer and a 1-neuron output layer:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，数据已经准备好，包括训练和评估分区，让我们从定义的类初始化一个 MLP 模型，包含一个20神经元的隐藏层和一个1神经元的输出层：
- en: '[PRE9]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With the data and model ready, it is time to train the model we built from
    scratch. Since the dataset is small enough, with 442 samples, there isn’t a runtime
    issue using gradient descent, so we will be using the full gradient descent here
    for 100 epochs. One epoch means a full round of training going through the entire
    training dataset once:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据和模型准备好后，到了训练我们从头开始构建的模型的时候。由于数据集足够小，有442个样本，所以使用梯度下降法时没有运行时问题，因此我们将在这里使用完整的梯度下降进行100个周期。一个周期意味着对整个训练数据集进行一轮完整的训练：
- en: '[PRE10]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s plot the collected mean squared error for both the validation and training
    partitions using `matplotlib`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`matplotlib`绘制训练集和验证集的均方误差：
- en: '[PRE11]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will produce the following plot:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 2.5 – Training and validation partition mean squared error versus
    epochs plots](img/B18187_02_005.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 训练和验证集的均方误差与周期数的图](img/B18187_02_005.jpg)'
- en: Figure 2.5 – Training and validation partition mean squared error versus epochs
    plots
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 训练和验证集的均方误差与周期数的图
- en: With that, you’ve implemented an MLP and trained it from scratch without depending
    on deep learning frameworks! But is our implementation correct and sound? Let’s
    verify this in the next topic.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你已经实现了一个MLP，并且从头开始训练了它，而无需依赖深度学习框架！但是，我们的实现是否正确和合理？让我们在下一节验证这一点。
- en: Implementing MLP using deep learning frameworks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度学习框架实现MLP
- en: Deep learning frameworks are made to ease and expedite the development of deep
    learning models. They provide a plethora of commonly used neural network layers,
    optimizers, and tools generally used to build neural network models, along with
    very easily extensible interfaces to implement new methods. Backpropagation itself
    is abstracted away from the users of the frameworks as the gradients are computed
    automatically in the background when needed. Most importantly, they allow the
    usage of the GPU for efficient model training and prediction.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架旨在简化并加速深度学习模型的开发。它们提供了大量常用的神经网络层、优化器和通常用于构建神经网络模型的工具，并且提供了非常容易扩展的接口以实现新方法。反向传播本身被框架的用户抽象化，因为梯度会在后台自动计算，并在需要时使用。最重要的是，它们允许使用GPU来高效地进行模型训练和预测。
- en: 'In this section, we will build the same MLP model as in the previous section,
    using a deep learning framework called PyTorch, and verify that both implementations
    produce the same results:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个名为PyTorch的深度学习框架构建与上一节相同的MLP模型，并验证这两种实现是否产生相同的结果：
- en: 'We’ll start by importing the necessary libraries:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入必要的库开始：
- en: '[PRE12]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, let’s define the MLP class with the two fully connected layers along
    with the forward propagation method with arguments that allow us to set the input
    layer size, hidden layer size, and output layer size:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义包含两个全连接层的MLP类，并定义一个前向传播方法，该方法带有可以设置输入层大小、隐藏层大小和输出层大小的参数：
- en: '[PRE13]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will notice that there isn’t a backward propagation function implemented,
    which reduces the amount of effort needed to define a neural network model. When
    you inherit from the `Pytorch` module class, the backward propagation functionality
    will already be provided out of the box with your defined `Pytorch` layers. Finally,
    let’s initialize the MLP using a hidden layer size of 10 along with input and
    output sizes according to the diabetes dataset:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会注意到没有实现反向传播函数，这减少了定义神经网络模型所需的工作量。当你从`Pytorch`模块类继承时，反向传播功能将自动提供，与您定义的`Pytorch`层一起使用。最后，让我们使用隐藏层大小为10以及根据糖尿病数据集的输入和输出大小来初始化MLP：
- en: '[PRE14]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s check the forward and backward propagation functionality with our
    `numpy` variant. First, let’s initialize the `Pytorch` MLP and copy the weights
    from the `numpy`-based MLP model:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下使用`numpy`变体的前向和反向传播功能。首先，让我们初始化`Pytorch` MLP，并从基于`numpy`的MLP模型复制权重：
- en: '[PRE15]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s prepare the dataset into Tensor objects suitable for PyTorch model
    consumption:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将数据集准备为适合PyTorch模型使用的Tensor对象：
- en: '[PRE16]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To obtain the same gradients, we have to use the same MSE loss and apply backward
    propagation:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获得相同的梯度，我们必须使用相同的MSE损失并应用反向传播：
- en: '[PRE17]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let’s verify the gradients for the two implementations:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们验证这两种实现的梯度：
- en: '[PRE18]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This solidifies your foundational neural network knowledge, along with knowledge
    of the MLP architecture, and prepares you for more advanced concepts in the realm
    of deep learning. Before we move on to look at a more advanced neural network,
    in the next section, we will explore the topic of regularization, and then finally,
    explore how to design an MLP with a practical use case.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这巩固了你对神经网络基础知识的理解，以及对多层感知器（MLP）架构的掌握，为你深入学习深度学习领域中的更高级概念做好准备。在我们进入下一节讨论更高级的神经网络之前，我们将先探讨正则化这一主题，最后再探讨如何设计一个具有实际应用案例的MLP。
- en: Regularization
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: '**Regularization** in deep learning has evolved into a state where it now means
    any addition or modification to the neural network, data, or training process
    that is used to increase the generalization of the build model to external data.
    All performant neural networks today have some form of regularization embedded
    into the architecture. Some of these regularization methods introduce some extra
    beneficial side effects, such as the speedup of training or the performance on
    the training dataset. But ultimately, the regularizer’s main goal is to improve
    generalization, which in other words is to improve performance metrics and reduce
    errors on external data. As a quick recap, the following list shows a summary
    of some of the more common regularization methods:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习中的正则化**已经发展到现在的阶段，指的是任何用于增加构建模型对外部数据的泛化能力的神经网络、数据或训练过程中的附加或修改。如今，所有高效能的神经网络都在架构中嵌入了某种形式的正则化方法。这些正则化方法中的一些会带来额外的有益副作用，比如加速训练过程或提升训练数据集上的表现。但最终，正则化的主要目标是提高泛化能力，换句话说，就是提升外部数据上的表现指标并减少错误。简要回顾一下，以下是一些常见的正则化方法：'
- en: '**Dropout layer**: During training, randomly remove information from all neural
    nodes according to a specified probability level by replacing neural node outputs
    with zeros, effectively nullifying information. This reduces over-reliance on
    any single node/information and increases the probability of generalization.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout层**：在训练过程中，按指定的概率随机删除所有神经节点的信息，通过将神经节点的输出替换为零，实质上使得信息无效。这减少了对任何单一节点/信息的过度依赖，并增加了泛化的概率。'
- en: '**L1/L2 regularization**: These methods add a penalty term to the loss function,
    which discourages the model from assigning high weights to the features. L1 regularization,
    also known as Lasso, uses the absolute value of the weights, while L2 regularization,
    also known as Ridge, uses the squared value of the weights. By controlling the
    magnitude of the weights, these methods help to prevent overfitting and improve
    generalization. Typically, this is applied to the input features.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1/L2正则化**：这些方法向损失函数添加一个惩罚项，旨在减少模型为特征分配高权重的情况。L1正则化，也称为Lasso，使用权重的绝对值，而L2正则化，也称为Ridge，使用权重的平方值。通过控制权重的大小，这些方法有助于防止过拟合并提高泛化能力。通常，这些方法应用于输入特征。'
- en: '**Batch normalization layer**: This method is standardizing the data in both
    training and inferencing on the external data stage by scaling the data to have
    a mean of zero and a standard deviation of one. This is done by removing the computed
    mean and dividing it by the computed standard deviation. The mean and standard
    deviation are computed and iteratively updated by mini-batch (based on the models
    determining the training batch size) during training. During inference, the final
    learned running mean and standard deviation calculated during training are applied.
    This has the side effect of improving the training time, training stability, and
    generalization. Note that each element has its own mean and standard deviation.
    Research has shown that batch normalization smooths out the loss landscape, making
    it way easier to reach an optimum value.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化层**：这种方法在训练和推理阶段对外部数据进行标准化，通过将数据缩放到均值为零、标准差为一来实现。这是通过去除计算得到的均值并将其除以计算得到的标准差来完成的。均值和标准差通过小批量（基于模型确定的训练批量大小）在训练过程中进行计算并迭代更新。在推理阶段，会应用在训练过程中计算得到的最终学习运行均值和标准差。这有助于提高训练时间、训练稳定性和泛化能力。请注意，每个元素都有自己的均值和标准差。研究表明，批量归一化可以平滑损失函数的曲面，使得更容易达到最优值。'
- en: '**Group normalization layer**: Instead of having an individual mean and standard
    deviation for each element across the batch size, group normalization standardizes
    the data by groups per sample, where each group has one mean and one standard
    deviation. The number of groups can be configured. Batch normalization degrades
    the performance when the number of samples in a batch is small due to hardware
    limitations. This layer is used over batch normalization when the amount of data
    per batch is a very small number as the mean and standardization updates do not
    depend on the batch. In large batches, however, batch normalization still triumphs.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组归一化层**：与每个元素在批次中的均值和标准差不同，组归一化按每个样本的组对数据进行标准化，每组有一个均值和一个标准差。可以配置组的数量。由于硬件限制，批量归一化在批次中样本数量较小时会降低性能。当每批的数据量非常小时，这一层优于批量归一化，因为均值和标准化的更新不依赖于批次。然而，在大批量数据下，批量归一化仍然优胜。'
- en: '**Weight standardization**: This applies the same standardization process to
    the weights of the neural networks. The weights of the neural network might grow
    to very large numbers after training, which would create large output values.
    The idea is that if we use the batch normalization layer, the output values will
    be standardized anyway, so why don’t we take a step back and apply the same process
    to the weights themselves, making sure the values are standardized in some form
    before becoming an output value? Some simple benchmarks have demonstrated that
    it works well when combined with a group normalization layer in low batch sizes,
    achieving a better performance than batch normalization with a high batch size
    setting.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重标准化**：这将相同的标准化过程应用于神经网络的权重。神经网络的权重在训练后可能会增长到非常大的数字，这将导致输出值过大。这个方法的想法是，如果我们使用批量归一化层，输出值反正会被标准化，那为什么不退一步，将同样的过程应用于权重本身，确保在变成输出值之前，权重的值已经以某种形式被标准化呢？一些简单的基准测试表明，当与组归一化层结合使用时，在低批量大小下，它能取得比批量归一化在高批量大小设置下更好的性能。'
- en: '**Stochastic depth**: Instead of conceptually making a neural network with
    narrower layers during the training stage with dropout, stochastic depth reduces
    the depth of the network during training. This regularizing method leverages the
    concept of skip connections from ResNets, which will be introduced later, where
    outputs from earlier layers are additionally forwarded to the later layers. During
    training, the layers in between the skip connections are completely bypassed to
    simulate a shallower network randomly. This regularizer has the effect of a faster
    training time along with an increased generalization performance.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机深度**：与其在训练阶段通过dropout概念上让神经网络使用更窄的层，随机深度则是在训练过程中减少网络的深度。这个正则化方法利用了ResNet中跳跃连接的概念，稍后会介绍，跳跃连接是指将前面层的输出额外传递到后面的层。在训练过程中，跳跃连接之间的层会被完全绕过，从而随机模拟一个更浅的网络。这个正则化器可以加速训练时间，并提高模型的泛化性能。'
- en: '`[0, 0, 0, 1]` and `[0.0001, 0.0001, 0.0001, 0.9999]`, respectively. The idea
    is that we shouldn’t train the model to be overconfident in its result, which
    will signal that it is overfitted to the training data and won’t be able to generalize
    to external data. This method encourages representations of the last layer outputs
    to be closer to each other for samples in the same class and encourages the same
    outputs to be equally distant among samples from different classes. Additionally,
    this helps to mitigate overconfidence in samples that have inaccurate labels.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[0, 0, 0, 1]` 和 `[0.0001, 0.0001, 0.0001, 0.9999]`，分别表示。这个方法的想法是，我们不应该训练模型让它对结果过于自信，这会表明它对训练数据过拟合，并且无法推广到外部数据。该方法鼓励同一类别样本的最后一层输出更接近，而鼓励不同类别样本之间的输出保持相等的距离。此外，这也有助于缓解在标签不准确的样本中产生的过度自信。'
- en: '**Data augmentation**: When the raw data does not adequately represent all
    the variations of the data of any label, data augmentation helps to computationally
    add variations in the data to be used for training. This effectively increases
    generalization simply due to the model being able to learn from more complete
    variations of the data. This can be applied to any data modality and will be introduced
    in more detail in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125), *Exploring*
    *Supervised Learning*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：当原始数据不足以充分代表任何标签的所有变异时，数据增强有助于在计算上增加数据的变异性，从而用于训练。这通过模型能够学习更完整的数据变异性，来有效地提高泛化能力。这可以应用于任何数据类型，并将在
    [*第八章*](B18187_08.xhtml#_idTextAnchor125) 中进行更详细的介绍，*探索* *监督学习*。'
- en: Regularization is an important component in any neural network architecture
    and will be seen in all of the architectures that will be introduced in the chapter.
    When choosing regularization techniques for a specific problem, you should first
    consider the nature of your dataset and the problem you are trying to solve. For
    instance, if you have a small batch size, group normalization or weight standardization
    might be more suitable than batch normalization. If your dataset has limited variations,
    data augmentation can be used to improve generalization. To choose between these
    techniques, start with a simple regularization method such as dropout or L1/L2
    regularization, and evaluate its performance. Then, you can experiment with other
    techniques, either individually or in combination, and compare their impact on
    the model’s performance. It’s essential to monitor the training and validation
    metrics to ensure that the chosen regularization methods are not causing overfitting
    or underfitting. Ultimately, the choice of regularization technique depends on
    a combination of experimentation and validation, domain knowledge, and understanding
    of the specific problem and dataset at hand.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是任何神经网络架构中的重要组成部分，在本章介绍的所有架构中都会看到它。当为特定问题选择正则化技术时，首先应考虑数据集的特性和你试图解决的问题。例如，如果你有一个小批量大小，群体归一化或权重标准化可能比批量归一化更适合。如果数据集的变化有限，数据增强可以用来提高泛化能力。选择这些技术时，可以从简单的正则化方法（如
    dropout 或 L1/L2 正则化）开始，并评估其性能。然后，你可以单独或组合尝试其他技术，并比较它们对模型性能的影响。监控训练和验证指标非常重要，以确保所选的正则化方法不会导致过拟合或欠拟合。最终，正则化技术的选择取决于实验与验证的结合、领域知识以及对特定问题和数据集的理解。
- en: Next, let’s dive into the design of an MLP.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨 MLP（多层感知器）的设计。
- en: Designing an MLP
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计 MLP
- en: Tabular data is not where neural networks shine most, and more often than not,
    boosted decision trees outperform MLPs in terms of metric performance. However,
    sometimes, in some datasets, neural networks can outperform boosted trees. Make
    sure to benchmark MLPs with other non-neural network models when dealing with
    tabular data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据并不是神经网络最擅长的领域，而且往往在性能指标上，提升决策树的表现优于 MLP。然而，有时在某些数据集上，神经网络可能会超越提升树模型。在处理表格数据时，务必将
    MLP 与其他非神经网络模型进行基准测试。
- en: MLPs are the simplest form of neural networks and can be modified at a high
    level in two dimensions similar to all neural networks, which are the width of
    the network and the depth of the network. A common strategy when building standard
    MLP architectures from scratch is to start small with a shallow depth and narrow
    width and gradually increase both dimensions once a small baseline is obtained.
    Usually, for MLPs on tabular data, the performance benefits of increasing the
    depth of the neural network stagnate at around the fourth layer. ReLU is a standard
    activation layer that is proven to allow stable gradients and optimal learning
    of any task. However, if you have time to achieve practical value, consider replacing
    the activation layer with more advanced activation layers. At this point, the
    space of activation layer research is just too nuanced and results are mostly
    not standardized, with mixed responses on different datasets, so there is no guarantee
    of better performance when you use any advanced activation layers.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: MLP是最简单的神经网络形式，可以在两个维度上进行高层次的修改，这两个维度与所有神经网络类似：网络的宽度和网络的深度。从头开始构建标准MLP架构时，一个常见策略是从浅层和狭窄的宽度开始，获得一个小的基准后，再逐渐增加这两个维度。通常，对于表格数据上的MLP，增加网络深度的性能收益在大约第四层时会趋于停滞。ReLU是一种标准的激活层，已证明可以允许稳定的梯度和最佳的任务学习。然而，如果你有时间追求实际价值，考虑将激活层替换为更高级的激活层。目前，激活层研究领域非常细致，结果大多没有标准化，在不同的数据集上反馈混合，因此使用任何高级激活层并不能保证获得更好的性能。
- en: One adaptation of MLPs is to use a type of neural network called **denoising
    autoencoders** to generate denoised features that can be used as input to MLPs.
    This advancement will be described in more detail later in [*Chapter 5*](B18187_05.xhtml#_idTextAnchor085),
    *Understanding Autoencoders*. Training methods go hand in hand with the architecture
    when trying to achieve good performance. The methods are mostly generic and don’t
    depend on any architecture specifically, so they will be covered in [*Chapter
    8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Supervised Deep Learning*, and
    [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149), *Exploring Unsupervised Deep*
    *Learning*, separately.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的一种改进方法是使用一种叫做**去噪自编码器**的神经网络，生成可以作为MLP输入的去噪特征。这个进展将在稍后的[*第5章*](B18187_05.xhtml#_idTextAnchor085)，《理解自编码器》中详细描述。训练方法与架构密切相关，旨在实现良好的性能。这些方法大多是通用的，不依赖于任何特定架构，因此将在[*第8章*](B18187_08.xhtml#_idTextAnchor125)，《探索监督式深度学习》和[*第9章*](B18187_09.xhtml#_idTextAnchor149)，《探索无监督深度学习》中分别介绍。
- en: Next, let’s summarize what we’ve learned from this chapter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们总结一下本章的内容。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: MLPs are the foundational piece of architecture in deep learning that transcends
    just processing tabular data and is more than an old architecture that got superseded.
    MLPs are very commonly utilized as a sub-component in many advanced neural network
    architectures today to either provide more automatic feature engineering, reduce
    the dimensionality of large features, or shape the features into the desired shapes
    for target predictions. Look out for MLPs or, more importantly, the fully connected
    layer, in the next few architectures that are going to be introduced in the next
    few chapters!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MLP是深度学习中基础性的架构组成部分，不仅限于处理表格数据，它也不再是一个被取代的旧架构。MLP在当今许多先进的神经网络架构中非常常见，作为子组件用于提供更自动化的特征工程、减少大特征的维度，或者将特征形状调整为目标预测所需的形状。请留意接下来几章中将介绍的MLP，或者更重要的是，全连接层！
- en: The automatic gradient computation provided by deep learning frameworks simplifies
    the implementation of backpropagation and allows us to focus on designing new
    neural networks. It is essential to ensure that the mathematical functions used
    in these networks are differentiable, although this is often taken care of when
    adopting successful research findings. And that’s the beauty of open source research
    coupled with powerful deep learning frameworks!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架提供的自动梯度计算简化了反向传播的实现，使我们能够专注于设计新的神经网络。确保这些网络中使用的数学函数是可微分的至关重要，尽管在采用成功的研究成果时通常会自动处理这一点。这就是开源研究与强大深度学习框架结合的魅力所在！
- en: Regularization is a crucial aspect of neural network design, and while we have
    discussed it in detail in this chapter, upcoming chapters will showcase its application
    in different architectures without delving into further explanations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是神经网络设计中的一个关键方面，虽然我们在本章中已详细讨论了它，但后续章节将展示它在不同架构中的应用，而无需进一步解释。
- en: In the next chapter, we will dive into a different kind of neural network, called
    the convolutional neural network, which is particularly suited for image-related
    tasks and has a wide range of applications.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨另一种类型的神经网络，称为卷积神经网络，它特别适用于与图像相关的任务，并且有着广泛的应用。
