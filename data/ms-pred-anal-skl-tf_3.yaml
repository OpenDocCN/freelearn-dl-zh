- en: Working with Features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'In this chapter, we are going to take a close look at how features play an
    important role in the feature engineering technique. We''ll learn some techniques
    that will allow us to improve our predictive analytics models in two ways: in
    terms of the performance metrics of our models and to understand the relationship
    between the features and the target variables that we are trying to predict.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细探讨特征在特征工程技术中的重要作用。我们将学习一些技术，这些技术将帮助我们从两个方面改进预测分析模型：一是提升模型的性能指标，二是理解特征与我们试图预测的目标变量之间的关系。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Feature selection methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择方法
- en: Dimensionality reduction and PCA
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维和PCA
- en: Creating new features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建新特征
- en: Improving models with feature engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征工程改进模型
- en: Feature selection methods
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择方法
- en: 'Feature selection methods are used for selecting features that are likely to
    help with predictions. The following are the three methods for feature selection:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择方法用于选择可能有助于预测的特征。以下是三种特征选择方法：
- en: Removing dummy features with low variance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除低方差的虚拟特征
- en: Identifying important features statistically
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计学上识别重要特征
- en: Recursive feature elimination
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归特征消除
- en: When building predictive analytics models, some features won't be related to
    the target and this will prove to be less helpful in prediction. Now, the problem
    is that including irrelevant features in the model can introduce noise and add
    bias to the model. So, feature selection techniques are a set of techniques used
    to select the most relevant and useful features that will help either with prediction
    or with understanding our model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建预测分析模型时，某些特征可能与目标无关，这将对预测帮助较小。问题在于，将无关特征包含在模型中可能会引入噪声并增加模型的偏差。因此，特征选择技术是一系列技术，用于选择最相关和有用的特征，以帮助预测或理解我们的模型。
- en: Removing dummy features with low variance
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去除低方差的虚拟特征
- en: 'The first technique of feature selection that we will learn about is removing
    dummy features with low variance. The only transformation that we have been applying
    so far to our features is to transform the categorical features using the encoding
    technique. If we take one categorical feature and use this encoding technique,
    we get a set of dummy features, which are to be examined to see whether they have
    variability or not. So, features with a very low variance are likely to have little
    impact on prediction. Now, why is that? Imagine that you have a dataset where
    you have a gender feature and that 98% of the observations correspond to just
    the female gender. This feature won''t have any impact on prediction because almost
    all of the cases are just of a single category, so there is not enough variability.
    These cases become candidates lined up for elimination and such features should
    be examined more carefully. Now, take a look at the following formula:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习的第一种特征选择技术是去除低方差的虚拟特征。到目前为止，我们对特征进行的唯一变换是使用编码技术转换类别特征。如果我们取一个类别特征并使用这种编码技术，我们会得到一组虚拟特征，需要检查这些特征是否具有变异性。因此，方差非常低的特征可能对预测几乎没有影响。那么，为什么会这样呢？假设你有一个数据集，其中有一个性别特征，且98%的样本都是女性。这个特征对预测没有任何影响，因为几乎所有的样本都属于同一类别，所以变异性不足。这些样本成为待淘汰的候选项，这类特征应该更仔细地进行检查。接下来，看看以下公式：
- en: '![](img/5fc96b69-e4fa-497c-9c96-25fc7005239d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5fc96b69-e4fa-497c-9c96-25fc7005239d.png)'
- en: You can remove all dummy features that are either 0 or 1 in more than x% of
    the samples, or what you can do is to establish a minimum threshold for the variance
    of such features. Now, the variance of such features can be obtained with the
    preceding formula, where **p** is the number or the proportion of **1** in your
    dummy features. We will see how this works in a Jupyter Notebook.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以去除在超过x%的样本中为0或1的所有虚拟特征，或者你可以为这些特征建立一个最小方差阈值。现在，这些特征的方差可以通过前面的公式计算，其中**p**是虚拟特征中**1**的数量或比例。我们将在Jupyter
    Notebook中看到这种方法的应用。
- en: Identifying important features statistically
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计学上识别重要特征
- en: This method will help you make use of some statistical tests for identifying
    and selecting relevant features. So, for example, for classification tasks we
    can use an ANOVA F-statistic to evaluate the relationship between numerical features
    and the target, which will be a categorical feature because this is an example
    of a classic task. Or, to evaluate the statistical relationship between a categorical
    feature and the target, we will use the chi-squared test to evaluate such a relationship.
    In `scikit-learn`, we can use the `SelectKBest` object and we will see how to
    use these objects in a Jupyter Notebook.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将帮助你利用一些统计检验来识别和选择相关特征。举个例子，对于分类任务，我们可以使用 ANOVA F 统计量来评估数值特征与目标之间的关系，目标将是一个分类特征，因为这是经典任务的例子。或者，评估分类特征与目标之间的统计关系时，我们将使用卡方检验来评估这种关系。在`scikit-learn`中，我们可以使用`SelectKBest`对象，接下来我们将展示如何在
    Jupyter Notebook 中使用这些对象。
- en: Recursive feature elimination
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归特征消除
- en: The process of identifying important features and removing the ones that we
    think are not important for our model is called **recursive feature elimination**
    (**RFE**). RFE can also be applied in `scikit-learn` and we can use this technique
    for calculating coefficients, such as linear, logistic regression, or with models
    to calculate something called **feature importance**. The random forests model
    provides us with those feature importance metrics. So, for models that don't calculate
    either coefficients or feature importance, these methods cannot be used; for example,
    for KNN models, you cannot apply the RFE technique because this begins by predefining
    the required features to use in your model. Using all features, this method fits
    the model and then, based on the coefficients or the feature importance, the least
    important features are eliminated. This procedure is recursively repeated on the
    selected set of features until the desired number of features to select is eventually
    reached.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 识别重要特征并去除我们认为不重要的特征的过程叫做**递归特征消除**（**RFE**）。RFE 也可以在`scikit-learn`中应用，我们可以使用这种技术来计算系数，比如线性回归、逻辑回归，或者通过模型计算所谓的**特征重要性**。随机森林模型为我们提供了这些特征重要性指标。因此，对于那些不计算系数或特征重要性的模型，无法使用这些方法；例如，对于
    KNN 模型，你不能应用 RFE 技术，因为这首先要求预定义模型中需要使用的特征。使用所有特征时，该方法先拟合模型，然后基于系数或特征重要性，最不重要的特征被消除。这个过程会在选定的特征集上递归重复，直到最终选择出所需数量的特征。
- en: 'There are the following few methods to select important features in your models:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型中重要特征的方法如下：
- en: L1 feature
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 特征
- en: Selection threshold methods
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择阈值方法
- en: Tree-based methods
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于树的方法
- en: 'Let''s go to our Jupyter Notebook to see how we actually apply these methods
    in `scikit-learn`. The following screenshot depicts the necessary libraries and
    modules to import:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入 Jupyter Notebook，看看我们如何在`scikit-learn`中实际应用这些方法。以下截图展示了需要导入的必要库和模块：
- en: '![](img/d000765a-0b4c-4f0a-8728-f5099e1253e0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d000765a-0b4c-4f0a-8728-f5099e1253e0.png)'
- en: 'In the following screenshot, we have first used the credit card default dataset
    and we are applying the traditional transformations that we do to the raw data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们首先使用了信用卡默认数据集，并对原始数据应用了传统的转换方法：
- en: '![](img/9f819808-eb10-415a-932d-7e3ea519fb41.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f819808-eb10-415a-932d-7e3ea519fb41.png)'
- en: 'The following screenshot shows the dummy features that we have in our dataset
    and the numerical features, depending on the type of feature:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了数据集中我们拥有的虚拟特征和数值特征，取决于特征的类型：
- en: '![](img/4060697e-b56b-42ff-a373-fba66f231307.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4060697e-b56b-42ff-a373-fba66f231307.png)'
- en: 'Here, we are applying the scaling operation for feature modeling:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用了特征建模的缩放操作：
- en: '![](img/89e9ea9d-aabf-4dc1-9df0-c8b36d6e2b98.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89e9ea9d-aabf-4dc1-9df0-c8b36d6e2b98.png)'
- en: 'The first method that we talked about in the presentation was removing dummy
    features with low variance to get the variances from our features using the `var()`
    method:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示中我们讨论的第一个方法是使用`var()`方法去除低方差的虚拟特征，从而得到特征的方差：
- en: '![](img/9f9a3dae-0abb-442a-a84e-06eee184e9e1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9a3dae-0abb-442a-a84e-06eee184e9e1.png)'
- en: 'Let''s see the variances only for the dummy features; for example, a threshold
    for the variance will consider only the dummy features with a variance over `0.1`.
    In that case, with such a threshold of 0.1, the two candidates for elimination, `pay_5` and `pay_6`,
    would be the first few unnecessary dummy features with low variance that will
    be removed. Take a look at the following screenshot, which depicts the candidates
    for elimination:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只查看虚拟特征的方差。例如，方差的阈值将仅考虑方差大于`0.1`的虚拟特征。在这种情况下，使用0.1作为阈值，`pay_5`和`pay_6`这两个候选特征将是第一个需要删除的方差较低的虚拟特征。请看下面的截图，展示了候选的删除特征：
- en: '![](img/36866649-2516-4eca-b8ec-7d2e8624cf8f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36866649-2516-4eca-b8ec-7d2e8624cf8f.png)'
- en: The second approach that we talked about is statistically selecting the features
    that are related to the target, and we have two cases, dummy features and numerical
    features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的第二种方法是通过统计方法选择与目标相关的特征，我们有两种情况，虚拟特征和数值特征。
- en: 'Let''s perform the statistical tests for the dummy features. We are going to
    import objects in the `chi2` object from the `feature_selection` module in the
    `scikit-learn` library. We will also use the `SelectKBest` object to perform the
    statistical tests in all of the dummy features as shown in the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对虚拟特征执行统计检验。我们将从`scikit-learn`库中的`feature_selection`模块导入`chi2`对象。我们还将使用`SelectKBest`对象，对所有虚拟特征执行统计检验，如下图所示：
- en: '![](img/22827f00-aeb2-41ce-8d4f-7fa9e1b74377.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22827f00-aeb2-41ce-8d4f-7fa9e1b74377.png)'
- en: 'Here, we instantiate an object called `dummy _selector` and pass the required
    statistical test to apply to it. Here, we are passing the `k ="all"` argument because
    this statistical test is to be applied to all of the dummy features. After instantiating
    this object, the `fit()` method is called. Take a look at the following screenshot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实例化了一个名为`dummy _selector`的对象，并传入了要应用的统计检验。这里，我们传入了`k ="all"`参数，因为这个统计检验是应用于所有的虚拟特征。实例化这个对象后，调用`fit()`方法。请看下面的截图：
- en: '![](img/fb58b92b-6578-4731-9691-e36707a4aca6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb58b92b-6578-4731-9691-e36707a4aca6.png)'
- en: 'In the following screenshot, we have the chi-squared scores. This isn''t a
    statistical test and, the larger the number, the stronger the relationship between
    the feature and the target:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了卡方得分。这不是一个统计检验，数字越大，特征与目标之间的关系越强：
- en: '![](img/9a4ab20a-c9c9-447b-99b6-e9127829ea6f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a4ab20a-c9c9-447b-99b6-e9127829ea6f.png)'
- en: 'Now, if you remember your statistics class, this is a hypothesis testing setting.
    So, we can also calculate the p values and we can say that the features where
    `pvalues_` is greater than `0.05` are not related to the target. Now, in this
    case, we get very small p values for all of the features, as shown in the following
    screenshot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你还记得你的统计学课，这其实是一个假设检验的设置。因此，我们也可以计算p值，并且可以说，`pvalues_`大于`0.05`的特征与目标没有关系。现在，在这种情况下，我们得到所有特征的非常小的p值，如下图所示：
- en: '![](img/a45bc992-e373-4029-a504-a3a1cab481d5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a45bc992-e373-4029-a504-a3a1cab481d5.png)'
- en: There is a relationship between the target and all of the dummy features, so
    under this methodology, we shouldn't eliminate any of these dummy features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 目标与所有虚拟特征之间都有关系，因此在这种方法下，我们不应该删除任何一个虚拟特征。
- en: 'Now, we can use another statistical test called `f_ classif` to evaluate the
    relationship between numerical features and the target, as shown in the following
    screenshot:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用另一个统计检验方法，叫做`f_ classif`，来评估数值特征与目标之间的关系，如下图所示：
- en: '![](img/a383b1d7-db46-402f-8685-561a59bb9efe.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a383b1d7-db46-402f-8685-561a59bb9efe.png)'
- en: 'Reusing this `f_classif` object, we will pass the required statistical tests
    and number of features. In this case, we want to apply the test to all numerical
    features and then use the `fit()` method again with the numerical features and
    the target:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重新使用这个`f_classif`对象，我们将传入所需的统计检验和特征数量。在这种情况下，我们希望将检验应用于所有数值特征，然后再次使用`fit()`方法，处理数值特征和目标：
- en: '![](img/7d176673-5b2a-4baf-bcca-c02a1d8d8d1c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d176673-5b2a-4baf-bcca-c02a1d8d8d1c.png)'
- en: 'The p values that we receive from the application of this statistical test
    are shown in the following screenshot:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个统计检验中得到的p值如下图所示：
- en: '![](img/1f7a09ea-105f-424e-8e4c-9c4887a7ced9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f7a09ea-105f-424e-8e4c-9c4887a7ced9.png)'
- en: 'We can pass the `f_classif` statistical test and then select the numerical
    features that have a p value greater than `0.05`, which is the usual threshold
    for statistical tests; the resulting features here are `bill_amt4`, `bill_amt5`,
    and `bill_amt6`, which are likely to be irrelevant, or not related to the target:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`f_classif`统计检验，然后选择那些p值大于`0.05`的数值特征，这通常是统计检验的阈值；这里得到的特征是`bill_amt4`、`bill_amt5`和`bill_amt6`，这些特征可能不相关或与目标无关：
- en: '![](img/1d128d68-8142-47e4-b6d0-95982e12dc43.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d128d68-8142-47e4-b6d0-95982e12dc43.png)'
- en: We have three candidates for elimination which can be eliminated or can be applied.
    We have used the second technique in the preceding steps and now we will use the
    third one in the following section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有三个待排除的候选特征，可以排除也可以保留。我们已经使用了前面步骤中的第二种技术，现在我们将在下一部分使用第三种技术。
- en: 'The RFE is the third technique in which we will use the `RandomForestClassifier`
    model, and remember that we have 25 features here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RFE是我们将使用`RandomForestClassifier`模型的第三种技术，记住这里有25个特征：
- en: '![](img/ccc37681-6a78-4244-ac36-8cef9d4360ee.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccc37681-6a78-4244-ac36-8cef9d4360ee.png)'
- en: 'So, let''s assume that we want to select only 12 features and we want a model
    that uses only 12 features. So, we are using about half of the features. We can
    use the `RFE` object present in `scikit-learn` from the `feature_selection` module.
    We can use this to actually select these 12 features using the RFE technique.
    So, we instantiate this object by passing the required estimator and the number
    of features to select:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，假设我们只想选择12个特征，并且我们希望使用仅包含12个特征的模型。也就是说，我们只使用了大约一半的特征。我们可以使用`scikit-learn`中的`RFE`对象，来自`feature_selection`模块。我们可以用它来实际选择这12个特征，使用RFE技术。所以，我们通过传递所需的估计器和要选择的特征数量来实例化这个对象：
- en: '![](img/515cf59d-8bc9-428d-99ee-c0693af054cc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/515cf59d-8bc9-428d-99ee-c0693af054cc.png)'
- en: 'Now, remember that random forest provides us with a metric of feature importance,
    which can be used with the RFE technique:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，记住随机森林为我们提供了特征重要性度量，这可以与RFE技术一起使用：
- en: '![](img/7c041ba0-ebbf-4f72-acb1-331a690b14fe.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c041ba0-ebbf-4f72-acb1-331a690b14fe.png)'
- en: 'After using the `fit()` method on the whole dataset, we get `recursive_selector.support_`
    and `True` for the features that are included in our model, the 12 that we wanted,
    and we get `False` for the ones that should be eliminated:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在对整个数据集使用`fit()`方法后，我们得到`recursive_selector.support_`，对于包含在模型中的特征（我们想要的12个特征）返回`True`，而对于应该排除的特征返回`False`：
- en: '![](img/316a9ba5-01e9-4406-b46e-7f3db0970ea8.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/316a9ba5-01e9-4406-b46e-7f3db0970ea8.png)'
- en: 'So, according to this object and method, we should include the 12 most important
    features in our random forest model in order to predict targets such as `limit_bal`,
    `age`, `pay`; all of the bill amounts; and `pay_amt1`, `pay_amt2`, and `pay_amt3`,
    as shown in the following screenshot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，根据这个对象和方法，我们应该在我们的随机森林模型中包含12个最重要的特征，以便预测目标，比如`limit_bal`、`age`、`pay`，所有账单金额，以及`pay_amt1`、`pay_amt2`和`pay_amt3`，如以下截图所示：
- en: '![](img/dc1ad6e3-8378-4971-b073-ec6ffe1631a1.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc1ad6e3-8378-4971-b073-ec6ffe1631a1.png)'
- en: 'These are the features that should be eliminated because they are not very
    relevant according to this method and this model for predicting the target:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是应该被排除的特征，因为根据这个方法和模型，它们与预测目标的相关性不大：
- en: '![](img/a290e198-14e2-4776-aa59-c31137a3b76c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a290e198-14e2-4776-aa59-c31137a3b76c.png)'
- en: 'Now we can evaluate the simpler model, the one with the 12 features against
    the full model that we have been using so far, after which we can calculate the
    metrics using cross-validation. So, in this example, we are using 10-fold cross-validation
    to get an estimation of the performance of these two models. Remember, this selector
    model is the full model according to the RFE technique and these are the results:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以评估更简单的模型，这个模型只有12个特征，与我们目前为止使用的完整模型进行比较，然后我们可以使用交叉验证来计算这些指标。所以，在这个例子中，我们使用了10折交叉验证来估算这两个模型的性能。记住，这个选择器模型是根据RFE技术得到的完整模型，以下是结果：
- en: '![](img/c19b384c-506d-4e42-85bc-8cfe711c1e3f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c19b384c-506d-4e42-85bc-8cfe711c1e3f.png)'
- en: 'The full model has a recall of `0.361365`, and the model that includes only
    12 features has a recall of `0.355791`. Since this model has less recall, the
    full model remains the best one. But if we use half of the features, the full
    model will also give us similar performance:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 完整模型的召回率是`0.361365`，而只包含12个特征的模型的召回率是`0.355791`。由于这个模型的召回率较低，完整模型依然是最好的。但是，如果我们使用一半的特征，完整模型也会给出类似的表现：
- en: '![](img/a2f3dcbb-fedd-4e46-b2e9-3a4d52135093.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2f3dcbb-fedd-4e46-b2e9-3a4d52135093.png)'
- en: 'As you can see in the following screenshot, the values are really close:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在下面的截图中看到的那样，这些值非常接近：
- en: '![](img/d85e222b-999c-40d8-bf81-05b389a3b7c6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d85e222b-999c-40d8-bf81-05b389a3b7c6.png)'
- en: 'Now you can decide whether you want to use the full model or you want to use
    the simpler model. This is up to you, but in terms of accuracy we get almost the
    same, although with still a little bit more accuracy for the full model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以决定是使用完整模型，还是使用更简单的模型。这个取决于你，但在准确性方面，我们几乎得到了相同的结果，尽管完整模型的准确性稍微高一些：
- en: '![](img/de3fc718-3686-4136-a8e3-b483cdfa8f62.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de3fc718-3686-4136-a8e3-b483cdfa8f62.png)'
- en: Now, you have a technique to decide whether you want to use a more complicated
    model that uses more features, or a simpler model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有了一种技术来决定是否使用一个更复杂的模型，它使用更多的特征，或者使用一个更简单的模型。
- en: Dimensionality reduction and PCA
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维和PCA
- en: The dimensionality reduction method is the process of reducing the number of
    features under consideration by obtaining a set of principal variables. The **Principal
    Component Analysis **(**PCA**) technique is the most important technique used
    for dimensionality reduction. Here, we will talk about why we need dimensionality
    reduction, and we will also see how to perform the PCA technique in `scikit-learn`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 降维方法是通过获取一组主变量来减少考虑的特征数量的过程。**主成分分析**（**PCA**）技术是用于降维的最重要技术。在这里，我们将讨论为什么需要降维，并且还将看到如何在`scikit-learn`中执行PCA技术。
- en: 'These are the reasons for having a high number of features while working on
    predictive analytics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在进行预测分析时拥有大量特征的原因：
- en: It enables the simplification of models, in order to make them easier to understand
    and to interpret. There might be some computational considerations if you are
    dealing with thousands of features. It might be a good idea to reduce the number
    of features in order to save computational resources.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使得模型简化，从而更容易理解和解释。如果你处理的是成千上万的特征，可能会涉及一些计算上的考虑。在这种情况下，减少特征数量有助于节省计算资源。
- en: Another reason is to avoid the "curse of dimensionality." Now, this is a technical
    term and a set of problems that arise when you are working with high-dimensional
    data.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个原因是避免“维度灾难”。这是一个技术术语，指的是当你处理高维数据时会遇到的一组问题。
- en: This also helps us to minimize overfitting because if you are including a lot
    of irrelevant features to predict the target, then your model can overfit to that
    noise. So, removing irrelevant features will help you with overfitting.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这也有助于我们减少过拟合，因为如果你包含了很多无关的特征来预测目标，那么你的模型可能会对这些噪声过拟合。因此，去除无关特征有助于避免过拟合。
- en: Feature selection, seen earlier in this chapter, can be considered a form of
    dimensionality reduction. When you have a set of features that are closely related
    or even redundant, PCA will be the preferred technique to encode the same information
    using less features. So, what is PCA? It's a statistical procedure that converts
    a set of observations of possibly correlated variables into a set of linearly
    uncorrelated variables called **principal components**. Let's not go into the
    mathematical details about what's going on with PCA.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本章之前看到的特征选择可以被认为是一种降维方式。当你拥有一组密切相关甚至冗余的特征时，PCA将是首选技术，用更少的特征编码相同的信息。那么，什么是PCA呢？它是一种统计过程，将一组可能相关的变量转换为一组线性不相关的变量，称为**主成分**。我们不深入讨论PCA背后的数学细节。
- en: Let's assume we have a dataset that is two-dimensional. PCA identifies a direction
    where the dataset varies the most and encodes the maximum amount of information
    on these two features into one single feature to reduce the dimensions from two
    to one. This method projects every point onto these axes or new dimensions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个二维的数据集。PCA识别出一个方向，在这个方向上数据集变化最大，并将这两个特征上的最大信息量编码成一个特征，从而将维度从二维降低到一维。此方法将每个点投影到这些轴或新的维度上。
- en: 'As you can see in the following screenshot, the first principal component of
    these two features would be the projections of the points onto the red line, which
    is the main mathematical intuition behind what''s going on in PCA:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在下面的截图中看到的那样，这两个特征的第一个主成分将是这些点投影到红线上的结果，这就是PCA中发生的主要数学直觉：
- en: '![](img/941fd408-2576-441e-b2eb-37e2ea9a84fa.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/941fd408-2576-441e-b2eb-37e2ea9a84fa.png)'
- en: 'Now, let''s go to the Jupyter Notebook to see how to implement the dimensionality
    reduction method and to apply PCA on the given dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入Jupyter Notebook，看看如何实现降维方法并在给定的数据集上应用PCA：
- en: '![](img/0029c5de-ae04-47d1-b311-e42d0d1bff40.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0029c5de-ae04-47d1-b311-e42d0d1bff40.png)'
- en: 'In this case, we will use the credit card default dataset. So, here we are
    doing the transformations that we have covered so far:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将使用信用卡违约数据集。所以，在这里我们做了到目前为止我们已覆盖的转换：
- en: '![](img/c5fd83e2-cab0-468d-af4d-1270ef9b3656.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5fd83e2-cab0-468d-af4d-1270ef9b3656.png)'
- en: 'Now, let''s take a look at the bill amount features. We have six of these features,
    the history of the bill amounts from one to six months ago, which are closely
    related, as you can see from the visualization generated from the following screenshot
    of code snippets:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看账单金额特征。我们有这六个特征，它们是来自一到六个月前的账单金额历史，正如你从以下截图中生成的可视化中看到的，它们密切相关：
- en: '![](img/360ba160-c0a2-457e-9435-abe501b4c447.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/360ba160-c0a2-457e-9435-abe501b4c447.png)'
- en: 'So, they represent the same information. If you see a customer with a very
    high bill amount two or three months ago, it is very likely that they also got
    a very high bill amount one month ago. So, these features, as you can see from
    the visualization shown in the following screenshot, are really closely related:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，它们表示相同的信息。如果你看到一个客户在两三个月前账单金额非常高，那很有可能他在一个月前也有非常高的账单金额。正如你从以下截图中的可视化中看到的，这些特征真的有很高的相关性：
- en: '![](img/11d8b84c-f9c7-4cb8-a6d2-8bea6e03ad7e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11d8b84c-f9c7-4cb8-a6d2-8bea6e03ad7e.png)'
- en: 'We confirm this with the calculation of the correlation coefficient. As you
    can see, they are really highly correlated:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算相关系数来确认这一点。正如你所看到的，它们的相关性非常高：
- en: '![](img/c8a4f728-6e65-43d6-887f-331d841c3298.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8a4f728-6e65-43d6-887f-331d841c3298.png)'
- en: 'The correlation between the bill amount one month ago and two months ago is
    `0.95`. We have very high correlations, which is a good opportunity to apply a
    dimensionality reduction technique, such as PCA in `scikit-learn`, for which we
    import it from `sklearn.decomposition` , as shown in the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 账单金额一个月前和两个月前之间的相关系数为`0.95`。我们有非常高的相关性，这是应用降维技术的一个好机会，比如在`scikit-learn`中的PCA，我们从`sklearn.decomposition`导入它，如下截图所示：
- en: '![](img/14f1e55c-9574-4e3f-a89e-902ba51eb556.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14f1e55c-9574-4e3f-a89e-902ba51eb556.png)'
- en: 'After that, we instantiate this `PCA` object. Then, we pass the columns or
    the features that we want to apply PCA decomposition to:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们实例化这个`PCA`对象。然后，我们传入我们想要应用PCA分解的列或特征：
- en: '![](img/f4998794-1b14-4646-83ca-ab9e871a4de8.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4998794-1b14-4646-83ca-ab9e871a4de8.png)'
- en: 'So after using the `fit()` method derived from this object, we receive one
    of the attributes, the explained variance ratio, as shown in the following screenshot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在使用从该对象派生的`fit()`方法后，我们获得了其中一个属性——解释方差比率，如下图所示：
- en: '![](img/b99b1080-769c-40c4-8c7f-23e63fe9adbd.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b99b1080-769c-40c4-8c7f-23e63fe9adbd.png)'
- en: 'Let''s plot this quantity to get a feel for what''s going on with these features.
    As you can see here, we get the explained variance of all six components:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这个量，以便了解这些特征的情况。正如你在这里看到的，我们得到了所有六个组件的解释方差：
- en: '![](img/e63ef069-1047-4913-b7d1-396f817af0df.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e63ef069-1047-4913-b7d1-396f817af0df.png)'
- en: The way to read this plot is that the first component of the PCA that we did
    on these six features encodes more than 90% of the total variance of all six features.
    The second one shows a very small variance, and the third, fourth, fifth, and
    sixth components also have minimal variance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这个图的方式是，我们对这六个特征做的PCA的第一个成分编码了所有六个特征总方差的90%以上。第二个成分展示了非常小的方差，第三、第四、第五和第六个成分也有最小的方差。
- en: 'Now, we can see this in the plot of cumulative explained variance shown in
    the following screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在以下截图中看到累积解释方差的图像：
- en: '![](img/ef93a131-19d8-4ce0-b10e-698b21ef1cfa.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef93a131-19d8-4ce0-b10e-698b21ef1cfa.png)'
- en: As you can see, the first component encodes more than 90% of the variance of
    the six features that we used. So, you are getting more than 90% of the information
    in just one feature. Therefore, instead of using six features, you can use just
    one single feature and still get more than 90% of the variance. Or, you can use
    the first two components and get more than 95% of the total information contained
    in the six features in just two features, the first and second components of this
    PCA. So, this is how this works in practice and we can use this as one technique
    for performing feature engineering.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，第一个主成分编码了我们使用的六个特征中超过90%的方差。因此，你只用一个特征就能获得超过90%的信息。因此，与你使用六个特征相比，你只需使用一个特征就能得到超过90%的方差。或者，你可以使用前两个主成分，通过这两个主成分就能获得六个特征中超过95%的信息。所以，这就是实际操作的方式，我们可以将其作为进行特征工程的一种技术。
- en: Feature engineering
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering plays a vital role in making machine learning algorithms
    work and, if carried out properly, it enhances the predictive ability of machine
    learning algorithms. In other words, feature engineering is the process of extracting
    existing features or creating new features from the raw data using domain knowledge,
    the context of the problem, or specialized techniques that result in more accurate
    predictive models. This is an activity where domain knowledge and creativity play
    a very important role. This is an important process, which can significantly improve
    the performance of our predictive models. The more context you have about a problem,
    the better your ability to create new and useful features. Basically, the feature
    engineering process converts the features into input values that algorithms can
    understand.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程在使机器学习算法有效工作中起着至关重要的作用，如果执行得当，它可以增强机器学习算法的预测能力。换句话说，特征工程是从原始数据中提取现有特征或利用领域知识、问题背景或专业技术创造新特征的过程，从而得出更准确的预测模型。这是一个领域知识和创造力发挥重要作用的活动。这是一个重要的过程，能够显著提高我们预测模型的性能。你对问题的背景了解得越多，创造新特征的能力就越强。基本上，特征工程过程将特征转换为算法可以理解的输入值。
- en: There are various ways of implementing feature engineering. You might not find
    all of the techniques feasible and may end up excluding a few. The motive here
    is not to have an academic discussion about this topic, but to show you some of
    the common things that we do when we work with features and when we try to create
    new features. The first one is scaling features, used to transform their range
    to a more suitable one. The other one is to encode information in a better way,
    and we will see an example of this later in this chapter. Feature engineering
    involves creating new features from existing ones so that you can combine existing
    features by performing some mathematical operations on them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实现特征工程的方式有很多种。你可能不会发现所有技术都可行，可能最终会排除一些方法。这里的目的不是进行学术讨论，而是展示一些我们在处理特征和尝试创建新特征时常用的技术。第一种是特征缩放，用于将特征的范围转换为更合适的范围。另一种是更好的编码信息，稍后我们将在本章中看到一个例子。特征工程涉及从现有特征中创建新特征，以便通过对它们进行一些数学运算来组合现有特征。
- en: Another way of creating new features is by using a dimensionality reduction
    technique, such as PCA, which we saw previously. It doesn't matter what technique
    you use, as long as you make it creative. As mentioned previously, the more knowledge
    you have about the problem, the better.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种创建新特征的方式是使用降维技术，比如我们之前看到的 PCA。无论你使用什么技术，只要能够富有创造性就行。如前所述，你对问题了解得越多，效果就越好。
- en: Creating new features
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建新特征
- en: 'We will be using the credit card default and diamond datasets here. Now, let’s
    go to the Jupyter Notebook to create new features and see what these techniques
    are in practice:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用信用卡违约和钻石数据集。现在，让我们进入 Jupyter Notebook，创建新特征，并看看这些技术如何在实践中应用：
- en: '![](img/1f149084-6127-4ec7-be75-0550b89c0895.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f149084-6127-4ec7-be75-0550b89c0895.png)'
- en: 'Let''s import the credit card default dataset by executing a few commands,
    as shown in the following screenshot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行几个命令来导入信用卡违约数据集，具体操作请参考下面的截图：
- en: '![](img/2fcfac44-86fd-447e-a414-866ba15d6164.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fcfac44-86fd-447e-a414-866ba15d6164.png)'
- en: The first transformation that we will do is to create another way to encode
    the information that we have in the `education` feature. So far, we have been
    using one encoding technique in the `education` feature, and we will use the context
    of the `x` variable to come up with another encoding. People with graduate-level
    education are more highly educated than people with other levels of education.
    So, we can come up with some sort of points system for these features; for example,
    we can assign two points for people with graduate-level education, maybe one point
    for people with university-level education, and negative points for the other
    levels of education that we have in this dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做的第一个转换是创建另一种编码`education`特征信息的方式。到目前为止，我们在`education`特征中使用了一种编码技术，我们将利用`x`变量的上下文来提出另一种编码方式。具有研究生教育水平的人，比其他教育水平的人更受教育。因此，我们可以为这些特征想出某种积分系统；例如，我们可以给具有研究生教育的人分配两分，给具有大学教育的人分配一分，并为数据集中其他教育水平的人分配负分。
- en: 'Let''s take a look at the following screenshot to see how this is done:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的截图，看看这是如何做到的：
- en: '![](img/c0580fcd-23d3-41a8-b5e1-9ec4978e0dd3.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0580fcd-23d3-41a8-b5e1-9ec4978e0dd3.png)'
- en: 'The previous screenshot reflects the sequence that we have in these education
    levels, so this could be an another way to encode information. This might or might
    not be helpful in predicting defaulters for the next month. However, we can try
    this new technique to encode this information and see the results in the following
    screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图反映了我们在这些教育水平中的顺序，所以这可能是另一种编码信息的方式。这可能有助于或可能没有帮助于预测下个月的违约者。然而，我们可以尝试这种新技术来编码这些信息，并查看接下来的截图中的结果：
- en: '![](img/be40f9c7-3a02-4f08-a59f-befb73a3a7c0.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be40f9c7-3a02-4f08-a59f-befb73a3a7c0.png)'
- en: 'Another technique that we can use in this dataset is to use the bill amount
    and payment amount features in the context of this problem to calculate the difference
    between these two variables/features. So, if we take the bill amount from a particular
    month and subtract the payment amount for that month, we will get an amount or
    quantity. In this example, we are calling the `bill_minus_pay` variable, which
    represents the payment made by the client against the bill for that month. So,
    this newly derived quantity can be used to predict defaulters for the next month.
    We have included them in a potential predictive model for this dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这个数据集中使用的另一个技巧是，利用账单金额和支付金额特征，在这个问题的背景下计算这两个变量/特征之间的差异。因此，如果我们取某个月的账单金额，并减去该月的支付金额，就会得到一个数值或数量。在这个示例中，我们将其称为`bill_minus_pay`变量，代表客户在该月针对账单支付的金额。因此，这个新衍生的数量可以用来预测下个月的违约者。我们已经将其包含在这个数据集的潜在预测模型中：
- en: '![](img/1275ad80-b856-4812-ac04-b562ac2fc4a7.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1275ad80-b856-4812-ac04-b562ac2fc4a7.png)'
- en: 'Let''s now take a look at the following output, which depicts the defaulters
    for a particular month:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下下面的输出，它显示了某个月的违约者情况：
- en: '![](img/7a5678a6-7c8b-4658-9c06-d20905e46e44.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a5678a6-7c8b-4658-9c06-d20905e46e44.png)'
- en: 'Another method that we can use here, now that we have part of the information
    of these features in a new feature called `bill_minus_pay`, is that we can summarize
    the main information of the six features shown in the preceding screenshot in
    just one feature using the PCA technique:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用另一种方法，既然我们已经有了这些特征的一部分信息，在一个名为`bill_minus_pay`的新特征中，我们可以使用PCA技术将前面截图中显示的六个特征的主要信息总结为仅一个特征：
- en: '![](img/fbf447a5-af00-402d-8bb9-b5c0d34cb41d.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbf447a5-af00-402d-8bb9-b5c0d34cb41d.png)'
- en: 'We can do the same operation with the pay features. From the previous analysis,
    we know that the `pay_1` feature is very important for predicting who is going
    to pay next. So, in order to reduce the other five `pay_i` features to just two,
    we are reducing the six bill amount features to just one, and the six `pay_i`
    features to two. Apart from this, we again apply the PCA technique on the remaining
    five `pay_i` features to reduce these five to just one. Take a look at the following
    screenshot:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对支付特征做同样的操作。从之前的分析中，我们知道`pay_1`特征对于预测谁将下一个支付非常重要。因此，为了将其他五个`pay_i`特征减少到仅两个，我们将六个账单金额特征减少到仅一个，并将六个`pay_i`特征减少到两个。除此之外，我们再次在剩余的五个`pay_i`特征上应用PCA技术，将这五个特征减少到仅一个。看看下面的截图：
- en: '![](img/3b49bf31-73fe-48a7-9e42-dc69ef617e7e.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b49bf31-73fe-48a7-9e42-dc69ef617e7e.png)'
- en: These are some of the feature engineering techniques, with examples, that you
    can perform on your datasets, but you might want to create other transformations
    or variables from the existing ones.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些特征工程技术，带有示例，您可以在数据集中执行这些操作，但您可能希望根据现有特征创建其他转换或变量。
- en: 'Now, let''s see a couple of examples in the diamonds dataset. We need to import
    the diamonds dataset by executing a few commands, as shown in the following screenshot:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看几个钻石数据集的例子。我们需要通过执行几个命令来导入钻石数据集，如下图所示：
- en: '![](img/1a5b38e2-d2e8-4ed0-b02e-add5805bddac.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a5b38e2-d2e8-4ed0-b02e-add5805bddac.png)'
- en: 'As seen in the preceding screenshot, we have transformed some of the categorical
    features using the encoding technique. Now, let''s take a look at our imported
    dataset, shown in the following screenshot:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的截图所示，我们已经使用编码技术对一些分类特征进行了转换。现在，让我们来看一下我们导入的数据集，如下图所示：
- en: '![](img/fa90eb24-2ac3-4ab2-8541-8e645dc22c8b.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa90eb24-2ac3-4ab2-8541-8e645dc22c8b.png)'
- en: 'This is what our scatter plot matrix with four features, `x`, `y`, `z`, and
    `price`, looks like. The first three features refer to the measurements of the
    diamond, and `price` represents how those three features are related to the pricing
    of the diamond:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们四个特征的散点图矩阵：`x`、`y`、`z` 和 `price`。前三个特征指的是钻石的测量值，而`price`表示这三个特征与钻石定价之间的关系：
- en: '![](img/050dd302-e6ac-4f0e-a5e2-d12c18a1ec60.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/050dd302-e6ac-4f0e-a5e2-d12c18a1ec60.png)'
- en: In the preceding screenshot, as you can see, there is a very strong linear relationship
    between the first three features, which is one of the interesting things in this
    scatter plot matrix.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，正如你所看到的，前三个特征之间有非常强的线性关系，这也是该散点图矩阵中的一个有趣之处。
- en: As diamonds are three-dimensional objects, we will combine these three features
    into just a single feature, called volume.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于钻石是三维物体，我们将这三个特征合并成一个单一的特征，称为体积。
- en: 'Now, we will multiply the measurements of the `x`, `y`, and `z` axes, which
    will derive a number that is close to the volume of that object:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将乘以`x`、`y`和`z`轴的测量值，这将得出一个接近该物体体积的数字：
- en: '![](img/d50740ee-536d-4157-bc16-bd22cdc23c3c.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d50740ee-536d-4157-bc16-bd22cdc23c3c.png)'
- en: Now, we know that they are not boxes, and they don't have any fixed shape. However,
    this will be a really good approximation of the volume of the diamonds. So, this
    can be another way in which we can create a new feature volume from the existing
    features in this dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道它们不是盒子，而且没有固定的形状。然而，这将是一个非常好的体积近似值。因此，这可以是我们从数据集中现有特征中创建新特征体积的另一种方法。
- en: 'In the next screenshot, we have the volume of our object and also the weight
    of the diamond, which is measured as `carat`, and we will use these to create
    a new feature called `density`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个截图中，我们有了物体的体积，以及钻石的重量（以`克拉`为单位），我们将使用这些来创建一个新的特征，称为`密度`：
- en: '![](img/3af98542-6143-49f9-97c4-1b739b8fe2f1.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3af98542-6143-49f9-97c4-1b739b8fe2f1.png)'
- en: As you can see in the preceding screenshot, we have divided the carat by the
    volume in order to get the density of the diamond object.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的截图中看到的，我们将克拉除以体积，从而得到钻石物体的密度。
- en: 'This is how we created two features from the given context, which justifies
    the statement: "the more the knowledge or context of the problem, the better".
    As you can see, with just the provided knowledge of the problem, we were able
    to come up with new features.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何从给定的背景中创建两个特征，这也证明了“对问题的知识或背景越多，效果越好”这一说法。正如你所看到的，凭借仅有的背景知识，我们能够提出新的特征。
- en: 'Now, let''s try and see how helpful these features might be in predicting models.
    The example we will use here is, how you can combine existing features to produce
    new features. The following plot shows the close relationship between the volume
    and price:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们试着看看这些特征在预测模型中的帮助。我们将在这里使用的例子是，如何将现有特征组合起来生成新的特征。下图展示了体积与价格之间的紧密关系：
- en: '![](img/b91bb49e-1916-444c-9476-2e49dd2d06cb.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b91bb49e-1916-444c-9476-2e49dd2d06cb.png)'
- en: We can assume that the volume will be helpful in predicting the price.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设体积在预测价格中会有帮助。
- en: 'However, in the following scatterplot of density and price, we see that all
    diamonds have the same expected density:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在下面的密度与价格的散点图中，我们发现所有钻石的预期密度都是相同的：
- en: '![](img/c4374d52-538a-45a8-841e-ef21e716744d.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4374d52-538a-45a8-841e-ef21e716744d.png)'
- en: 'When we see the correlation between `price` and `carat`, which we already had,
    it seems that `density` might not relate to `price` much:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到 `price` 和 `carat` 之间的相关性时，正如我们之前所说，似乎 `density` 与 `price` 的关系不大：
- en: '![](img/939ece93-e875-418f-b250-fa4e70de9f2a.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/939ece93-e875-418f-b250-fa4e70de9f2a.png)'
- en: So, this new feature might not help much in prediction. The volume and carat
    features have the same kind of relationship. We might not gain a lot of predictive
    power with this feature, but the main goal behind explaining this example was
    to show how to combine different features that you already have in your dataset
    to create new features.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这个新特征可能对预测帮助不大。`volume` 和 `carat` 特征之间有相似的关系。我们可能无法通过这个特征获得太多的预测能力，但解释这个例子的主要目的是展示如何结合数据集中已有的不同特征来创建新特征。
- en: This is what feature engineering is all about. You might also come up with other
    features for this dataset.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特征工程的核心。你也许还可以为这个数据集提出其他特征。
- en: Improving models with feature engineering
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过特征工程改进模型
- en: Now that we have seen how feature engineering techniques help in building predictive
    models, let's try and improve the performance of these models and evaluate whether
    the newly built model works better than the previous built model. Then, we will
    talk about two very important concepts that you must always keep in mind when
    doing predictive analytics, and these are the reducible and irreducible errors
    in your predictive models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到特征工程技术如何帮助构建预测模型，接下来让我们尝试提高这些模型的性能，并评估新构建的模型是否比之前的模型表现得更好。然后，我们将讨论两个在进行预测分析时必须始终牢记的重要概念，这就是预测模型中的可约误差和不可约误差。
- en: 'Let''s first import the necessary modules, as shown in the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必要的模块，如下图所示：
- en: '![](img/d9b2c9df-4eeb-4402-82d7-a47930ea0880.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9b2c9df-4eeb-4402-82d7-a47930ea0880.png)'
- en: 'So, let''s go to the Jupyter Notebook and take a look at the imported credit
    card default dataset that we saw earlier in this chapter, but as you can see,
    some modifications have been made to this dataset:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们进入 Jupyter Notebook，看看我们在本章前面看到的信用卡违约数据集，但正如你所看到的，这个数据集已经做了一些修改：
- en: '![](img/18835e99-b7bb-4d75-bc85-c5ff4582984f.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18835e99-b7bb-4d75-bc85-c5ff4582984f.png)'
- en: 'For this model, instead of transforming the `sex` and `marriage` features into
    two dummy features, the ones that we have been using were `male` and `married`;
    therefore, let''s encode the information in a slightly different way to see if
    this works better. So, we will encode the information as `married_male` and `not_married_female`,
    and see if this works better. This is the first transformation that we are doing
    here. This is what the dataset looks like:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，之前我们一直在使用的 `sex` 和 `marriage` 特征被转换成了两个虚拟特征——`male` 和 `married`；因此，让我们以稍微不同的方式编码这些信息，看看这种方式是否更有效。所以，我们将这些信息编码为
    `married_male` 和 `not_married_female`，并看看这种方法是否更好。这是我们在这里进行的第一次转换。数据集如下所示：
- en: '![](img/a9782f93-7ebb-4755-89b9-1a18495aad5c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9782f93-7ebb-4755-89b9-1a18495aad5c.png)'
- en: 'Now, let''s do a little bit more feature engineering. The first thing that
    we will do is calculate these new features, which are built from subtracting the
    payment amount from the bill amount, as shown in the following screenshot:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们做一些更多的特征工程。我们首先要做的是计算这些新特征，这些特征是通过从账单金额中减去支付金额得到的，如下图所示：
- en: '![](img/a543d2e0-8351-4c63-b35c-e118875fb5b4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a543d2e0-8351-4c63-b35c-e118875fb5b4.png)'
- en: For this problem, we will perform one mathematical operation. We will use the
    new features shown in the preceding screenshot to predict the target. Most of
    the information in the bill amount features is now encoded in these features,
    which are not needed anymore, but instead of throwing them away, what we can do
    is reduce the six bill amount features to just one using the PCA technique. So,
    let's apply the PCA technique to reduce the six features to just one component.
    Now there is a new feature called `bill_amt_new_feat`. So, this was the second
    feature engineering step that we performed. Finally, for the `pay_i` features,
    we will preserve the first one as is, and apply the PCA technique to the last
    five features, `pay_2`, `pay_3`, `pay_4`, `pay_5`, and `pay_6`, to reduce these
    five features to just two components. You can use the `fit_transform` method on
    the `PCA` object to get the components.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，我们将执行一个数学操作。我们将使用前面截图中显示的新特性来预测目标。账单金额特性中的大部分信息现在已编码到这些特性中，因此这些特性不再需要，但我们可以做的是使用PCA技术将六个账单金额特性减少到一个特性。因此，让我们应用PCA技术将六个特性减少到一个组件。现在有一个新的特性叫做`bill_amt_new_feat`。这就是我们执行的第二个特性工程步骤。最后，对于`pay_i`特性，我们将保留第一个特性不变，并对最后五个特性`pay_2`、`pay_3`、`pay_4`、`pay_5`和`pay_6`应用PCA技术，将这五个特性减少为两个组件。你可以使用`PCA`对象的`fit_transform`方法来获取这些组件。
- en: 'Now, let''s take a look at the following screenshot, showing all of the features
    that have to do with money. As you can see, the variances here are really huge
    because the currency amounts are large:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看下面的截图，展示了所有与金钱相关的特性。正如你所看到的，这里的方差非常大，因为货币金额很大：
- en: '![](img/d9405c76-fd54-48c7-bb70-94af065be775.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9405c76-fd54-48c7-bb70-94af065be775.png)'
- en: 'Now, rescale these features by dividing them by 1,000 in order to reduce the
    variances, as shown in the following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过将这些特性除以1,000来重新缩放它们，以减少方差，如下图所示：
- en: '![](img/3ef1e002-540e-4c2b-8e7a-858c96a183b0.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ef1e002-540e-4c2b-8e7a-858c96a183b0.png)'
- en: This helps us to make these numbers understandable. So, this is the other transformation
    that we did, and now let's train our model with these new features.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于我们使这些数字更易理解。因此，这就是我们进行的另一个转换，现在让我们使用这些新特性来训练我们的模型。
- en: Training your model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练你的模型
- en: 'The following model is a new module, as it has different features compared
    to the other models. Since the features have changed, we need to find the best
    hyperparameters for the `RandomForestClassifier` module using the `GridSearchCV`
    module. So, perhaps the previously found best parameters are not the best for
    these new features; therefore, we will run the `GridSearchCV` algorithm again:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型是一个新模块，因为它与其他模型相比具有不同的特性。由于特性发生了变化，我们需要使用`GridSearchCV`模块为`RandomForestClassifier`模块找到最佳超参数。因此，之前找到的最佳参数可能不适合这些新特性；因此，我们将再次运行`GridSearchCV`算法：
- en: '![](img/7e2ac386-5dc1-4970-b04a-44456a49da23.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e2ac386-5dc1-4970-b04a-44456a49da23.png)'
- en: 'As shown in the following screenshot, in this case the best combination of
    parameters for these new features is `max _depth` of `30`, `max_features` in `auto`,
    and `n_estimators` (number of estimators) should be `100`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，在这种情况下，这些新特性最佳的参数组合是`max_depth`为`30`，`max_features`为`auto`，`n_estimators`（估计器数量）应该是`100`：
- en: '![](img/b4892e44-1011-4f54-9605-577ce1933e06.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4892e44-1011-4f54-9605-577ce1933e06.png)'
- en: 'Now, let''s evaluate this new model that we have built using feature engineering,
    and let''s compare it with the previous metrics that we have from the previously
    built model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估我们通过特性工程构建的这个新模型，并与我们之前构建的模型的度量进行比较：
- en: '![](img/af57d7ee-8abb-400f-b988-7d53e42ed206.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af57d7ee-8abb-400f-b988-7d53e42ed206.png)'
- en: 'As you can see in the preceding screenshot, we are using a threshold of 0.2\.
    This model generates a recall of `71.39`% and a precision of `37.38`. Here, the
    precisions are similar, but, as mentioned earlier, the recall might be the metric
    that we should care about, as it''s slightly different compared to the previous
    one. We got a little better recall for this model; the change may only be 2% or
    3% , which might not look like much, but remember that in these financial applications,
    an improvement of 1% or 2% could, in practice, mean a lot of money. So, we got
    a slight improvement in the predictive power of our model using this little feature
    engineering technique; let''s take a look at the feature importance in the following
    screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的截图中看到的，我们使用了0.2的阈值。该模型生成了`71.39`%的召回率和`37.38`的精确度。这里，精确度相似，但正如前面提到的，召回率可能是我们更应关注的指标，因为它与之前的模型略有不同。我们通过这个模型略微提高了召回率；这个变化可能只有2%或3%，看起来不算多，但请记住，在这些金融应用中，1%或2%的提升在实际中可能意味着大量资金。所以，通过这个小小的特征工程技巧，我们在模型的预测能力上取得了一些微小的改进；接下来我们来看下一个截图中的特征重要性：
- en: '![](img/9225a3eb-041c-46f0-bc51-d6724b30d4f0.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9225a3eb-041c-46f0-bc51-d6724b30d4f0.png)'
- en: 'You can assess whether this feature importance make sense in the following
    screenshot of the random forest model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面这张随机森林模型的截图中评估这个特征重要性是否合理：
- en: '![](img/6062dd52-525e-48e5-90ed-caa3e760dbe1.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6062dd52-525e-48e5-90ed-caa3e760dbe1.png)'
- en: You can compare this feature importance with the previous ones. There are a
    lot of things that you can do after you have applied feature engineering. We may
    improve performance and gain insight from the model. It's been observed that we
    improved our model a little bit by using this technique. Now, you can come up
    with different ways to combine the existing features to improve the model even
    more. This was just a small, simple example to show you that you can actually
    play around with the features in a way that actually makes sense.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个特征重要性与之前的进行比较。在应用特征工程后，你可以做很多事情。我们可能会改善性能，并从模型中获得洞见。观察到通过使用这个技巧，我们的模型有所改进。现在，你可以通过不同的方式组合现有特征，以进一步改善模型。这只是一个小而简单的例子，旨在向你展示，你实际上可以通过合适的方式对特征进行调整。
- en: Reducible and irreducible error
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可约错误和不可约错误
- en: 'Before moving on, there are two really important concepts to be covered for
    predictive analytics. Errors can be divided into the following two types:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，有两个非常重要的概念需要在预测分析中讲解。错误可以分为以下两种类型：
- en: '**Reducible errors**: These errors can be reduced by making certain improvements
    to the model'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可约错误**：这些错误可以通过对模型进行一定的改进来减少。'
- en: '**Irreducible errors**: These errors cannot be reduced at all'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可约错误**：这些错误是无法减少的。'
- en: 'Let''s assume that, in machine learning, there is a relationship between features
    and target that is represented with a function, as shown in the following screenshot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在机器学习中，特征与目标之间的关系通过一个函数表示，如下图所示：
- en: '![](img/4dcab463-bc46-4047-9f46-8d6b362ce700.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dcab463-bc46-4047-9f46-8d6b362ce700.png)'
- en: Let’s assume that the target (**y**) is the underlying supposition of machine
    learning, and the relationship between the features and the target is given by
    a function. Since, in most cases we consider that there is some randomness in
    the relationship between features and target, we add a noise term here, which
    will always be present in reality. This is the underlying supposition in machine
    learning.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 假设目标（**y**）是机器学习的基本假设，特征与目标之间的关系由一个函数表示。由于在大多数情况下，我们认为特征和目标之间的关系具有某种随机性，我们在这里加入了一个噪声项，这在现实中总是存在的。这就是机器学习中的基本假设。
- en: 'In models, we try to approximate the theoretical function by using an actual
    function while performing feature engineering, tuning the parameters, and so on:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中，我们尝试通过使用实际函数来近似理论函数，同时进行特征工程、调优参数等操作：
- en: '![](img/86502f63-aa51-40f7-8605-e6feaecaf2c3.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86502f63-aa51-40f7-8605-e6feaecaf2c3.png)'
- en: So, our predictions are the results of the application of these approximations
    to the conceptual or theoretical **f**. All that we do in machine learning is
    try to approximate this **f** function by training the model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的预测是这些近似结果应用到概念性或理论性**f**的结果。我们在机器学习中所做的一切，都是通过训练模型来尽量逼近这个**f**函数。
- en: 'Training a model means approximating this function. It is possible to show
    mathematically that the expected error, defined as the difference between the
    real **y** and the predicted **y**, can be decomposed into two terms. One term
    is called **Reducible error** and the other one is called **Irreducible error**,
    as shown in the following screenshot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型意味着逼近这个函数。从数学上可以证明，定义为真实**y**与预测**y**之间差异的期望误差，可以分解为两个项。一个项叫做**可约误差**，另一个项叫做**不可约误差**，如以下截图所示：
- en: '![](img/ba4c0671-5d52-4204-806c-85b9c4ecfcd9.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba4c0671-5d52-4204-806c-85b9c4ecfcd9.png)'
- en: Now, the **Irreducible error** term is the variance of this random term. You
    don't have any control over this term. There will always be an irreducible error component.
    So, your model will always make mistakes; it doesn't matter how many features
    and data points you have, your model cannot always be 100% correct. What we must
    try to do is to use better and more sophisticated methods to perform feature engineering,
    and try to approximate our estimation to the real function. Just because you are
    working with more sophisticated models or you have more data, your model will
    not be perfect and you will not be able to predict exactly what **y** is, because
    there is some randomness in almost all the processes that you will work with.
    So this is the end of a very interesting section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**不可约误差** 项是该随机项的方差。你无法控制这个项。总会存在一个不可约误差成分。因此，你的模型总是会出错；无论你有多少特征和数据点，你的模型无法始终达到100%的准确度。我们必须尝试使用更好的、更复杂的方法进行特征工程，并尽量让我们的估计尽可能接近真实函数。仅仅因为你使用了更复杂的模型或拥有更多数据，并不意味着你的模型会完美，你无法精确预测**y**的值，因为几乎所有你将使用的过程都包含一些随机性。所以，这是一个非常有趣部分的结束。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about feature selection methods, how to distinguish
    between useful features, and features that are not likely to be helpful in prediction.
    We talked about dimensionality reduction and we learned how to perform PCA in
    `scikit-learn`. We also talked about feature engineering, and we tried to come
    up with new features in the datasets that we have been using so far. Finally,
    we tried to improve our credit card model by coming up with new features, and
    by working with all of the techniques that we learned in this chapter. I hope
    you have enjoyed this chapter.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了特征选择方法，如何区分有用特征和那些在预测中不太可能有帮助的特征。我们讨论了降维，并学习了如何在`scikit-learn`中执行PCA。我们还讨论了特征工程，并尝试为我们至今使用的数据集提出新的特征。最后，我们通过提出新特征并结合本章所学的所有技术，尝试改进我们的信用卡模型。希望你喜欢本章内容。
- en: In the next chapter, we will learn about artificial neural networks and how
    the `tensorflow` library is used when working with neural networks and artificial
    intelligence.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习人工神经网络，并了解在使用神经网络和人工智能时如何使用`tensorflow`库。
