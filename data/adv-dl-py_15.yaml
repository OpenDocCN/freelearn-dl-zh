- en: Deep Learning for Autonomous Vehicles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶车辆的深度学习
- en: Let's think about how **autonomous vehicles** (**AVs**) will affect our lives.
    For one thing, instead of focusing our attention on driving, we'll be able to
    do something else during our trip. Catering to the needs of such travelers could
    probably spawn a whole industry in itself. But that's just an added bonus. If
    we can be more productive or just relax during our travels, it is likely that
    we'll start traveling more, not to mention the benefits for people with limited
    ability to drive themselves. Making such an essential and basic commodity as transportation more
    accessible has the potential to transform our lives. And that's just the effect
    on us as individuals—AVs can have profound effects on the economy too, starting
    from delivery services to just-in-time manufacturing. In short, making AVs work
    is a very high-stakes game. No wonder, then, that in recent years the research
    in this area has moved from the academic world to the real economy. Companies
    from Waymo, Uber, and NVIDIA to virtually all major vehicle manufacturers are
    rushing to develop AVs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下**自动驾驶汽车**（**AVs**）将如何影响我们的生活。首先，除了集中精力开车，我们可以在旅行过程中做些其他事情。迎合这种旅行者的需求可能会催生一个全新的产业。但这只是附加的好处。如果我们能在旅途中提高生产力或仅仅放松身心，旅行可能会变得更加频繁，更不用说对那些无法自己驾驶的人群的好处。使交通这一基本而重要的商品变得更加可及，具有改变我们生活的潜力。而这只是对个人的影响——自动驾驶汽车对经济的影响也可能深远，从送货服务到即时制造。简而言之，自动驾驶汽车的成功实施是一场高风险的游戏。难怪近年来该领域的研究已经从学术界转向了现实经济。Waymo、Uber、NVIDIA以及几乎所有主要的汽车制造商都在争相开发自动驾驶汽车。
- en: However, we are not there just yet. One of the reasons for this is that self-driving
    is a complex task, composed of multiple subproblems, each a major task in its
    own right. To navigate successfully, the vehicle's program needs an accurate 3D
    model of the environment. The way to construct such a model is to combine the
    signals coming from multiple sensors. Once we have the model, we still need to
    solve the actual driving task. Think about the many unexpected and unique situations
    a driver has to overcome without crashing. But even if we create a driving policy,
    it needs to be accurate almost 100% of the time. Say that our AV will successfully
    stop at 99 out of 100 red traffic lights. 99% accuracy is a great success for
    any other **machine learning **(**ML**) task; not so for autonomous driving, where
    even a single mistake can lead to a crash.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有到达目标。原因之一是自动驾驶是一项复杂的任务，由多个子问题组成，每个子问题本身都是一项重大任务。为了成功导航，车辆的程序需要准确的环境3D模型。构建该模型的方法是将来自多个传感器的信号结合起来。一旦我们拥有模型，仍然需要解决实际的驾驶任务。想想司机必须克服的许多意外和独特的情况，而不发生碰撞。但即使我们创建了一个驾驶策略，它几乎需要每次都做到100%的准确度。假设我们的自动驾驶汽车能够成功地在100个红灯中停下99次。99%的准确率对于任何其他**机器学习**（**ML**）任务来说都是一个巨大成功；但对于自动驾驶而言，即使是一次错误也可能导致事故。
- en: In this chapter, we'll explore the applications of deep learning in AVs. We'll
    look at how to use deep networks to help the vehicle make sense of its surrounding
    environment. We'll also see how to use them in actually controlling the vehicle.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨深度学习在自动驾驶汽车中的应用。我们将了解如何利用深度网络帮助车辆理解周围的环境。我们还将看到它们如何实际应用于控制车辆。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Introduction to AVs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车简介
- en: Components of an AV system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车系统的组成部分
- en: Introduction to 3D data processing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D数据处理简介
- en: Imitation driving policy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿驾驶策略
- en: Driving policy with ChauffeurNet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ChauffeurNet的驾驶策略
- en: Introduction to AVs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶汽车简介
- en: We'll start this section with a brief history of AV research (which started
    surprisingly long ago). We'll also try to define the different levels of AV automation
    according to the **Society of Automotive Engineers** (**SAE**).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将从自动驾驶汽车研究的简史开始（令人惊讶的是，这一研究始于很久以前）。我们还将尝试根据**汽车工程师协会**（**SAE**）定义自动驾驶汽车的不同自动化等级。
- en: Brief history of AV research
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶汽车研究的简史
- en: 'The first serious attempts to implement self-driving cars began in the 1980s
    in Europe and the USA. Since the mid 2000s, progress has rapidly accelerated. The
    first major effort in the area was the Eureka Prometheus Project ([https://en.wikipedia.org/wiki/Eureka_Prometheus_Project](https://en.wikipedia.org/wiki/Eureka_Prometheus_Project)),
    which lasted from 1987 to 1995\. It culminated in 1995, when an autonomous Mercedes-Benz S-Class
    took a 1,600 km trip from Munich to Copenhagen and back using computer vision.
    At some points, the car achieved speeds of up to 175 km/h on the German Autobahn
    (fun fact: some sections of the Autobahn don''t have speed restrictions). The
    car was able to overtake other cars on its own. The average distance between human
    interventions was 9 km, and at one point it drove 158 km without interventions.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次认真尝试实现自动驾驶汽车始于1980年代的欧洲和美国。从2000年代中期起，进展迅速加速。该领域的第一次重大努力是Eureka Prometheus项目
    ([https://en.wikipedia.org/wiki/Eureka_Prometheus_Project](https://en.wikipedia.org/wiki/Eureka_Prometheus_Project))，该项目从1987年持续到1995年。1995年，该项目达到了高潮，一辆自动驾驶的奔驰S级车完成了从慕尼黑到哥本哈根及其返回的1,600公里旅程，使用的是计算机视觉。在某些时段，该车在德国的高速公路上以最高时速达175公里/小时行驶（有趣的是：德国的某些高速公路没有限速）。这辆车能够自主超车。人类干预的平均距离为9公里，在某个时刻，它驾驶了158公里没有任何干预。
- en: 'In 1989, Dean Pomerleau from Carnegie Mellon University published *ALVINN:
    An Autonomous Land Vehicle in a Neural Network* ([https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf](https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf)),
    a pioneering paper on the use of neural networks for AVs. This work is especially
    interesting, as it applied many of the topics we''ve discussed in this book in
    AVs 30 years ago. Let''s look at the most important properties of ALVINN:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '1989年，卡内基梅隆大学的Dean Pomerleau发表了*ALVINN: 一种神经网络中的自主陆地车辆* ([https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf](https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf))，这是一篇关于神经网络在自动驾驶车辆（AV）中应用的开创性论文。这项工作尤其有趣，因为它在30年前就将本书中讨论的许多主题应用到了自动驾驶车辆中。让我们来看看ALVINN的最重要特性：'
- en: It uses a simple neural network to decide the steering angle of a vehicle (it
    doesn't control the acceleration and the brakes).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用一个简单的神经网络来决定车辆的转向角度（它不控制加速和刹车）。
- en: The network is fully connected with one input layer, one hidden layer, and one
    output layer.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络完全连接，包含一个输入层、一个隐藏层和一个输出层。
- en: 'The input consists of the following:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入包含以下内容：
- en: A 30 × 32 single-color image (they used the blue channel from an RGB image)
    from a forward-facing camera mounted on the vehicle.
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自车载前视相机的一个30×32的单色图像（他们使用了RGB图像中的蓝色通道）。
- en: An 8 × 32 image from a laser range finder. This is simply a grid, where each
    cell contains the distance to the nearest obstacle covered by that cell in the
    field of view.
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自激光测距仪的一个8×32的图像。这只是一个网格，其中每个单元包含到该单元视场内最近障碍物的距离。
- en: One scalar input, which indicates the road intensity—that is, whether the road
    is lighter or darker than the nonroad in the image from the camera. This values
    comes recursively from the network output.
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个标量输入，表示道路强度——即，路面是否比图像中来自相机的非路面区域更亮或更暗。这个值是递归地从网络输出中获得的。
- en: A single fully connected hidden layer with 29 neurons.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含29个神经元的单一全连接隐藏层。
- en: A fully connected output layer with 46 neurons. The curvature of the road is
    represented by 45 of those neurons in a way that resembles one-hot encoding—that
    is, if the middle neuron has the highest activation, then the road is straight.
    Conversely, the left and right neurons represent increasing road curvature. The
    final output unit indicates the road intensity.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含46个神经元的全连接输出层。道路的曲率由45个神经元表示，方式类似于独热编码——也就是说，如果中间的神经元激活度最高，则表示道路是直的。相反，左侧和右侧的神经元表示增加的道路曲率。最终的输出单元表示道路强度。
- en: 'The network was trained for 40 epochs on a dataset of 1,200 images:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络在一个包含1,200张图像的数据集上进行了40轮训练：
- en: '![](img/36614e50-69fd-4749-9fc3-5914fdb767fe.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36614e50-69fd-4749-9fc3-5914fdb767fe.png)'
- en: 'The network architecture of ALVINN. Source: The ALVINN paper'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ALVINN的网络架构。来源：ALVINN论文
- en: 'Next, let''s take a look at the more recent timeline of (mostly) commercial
    AV progress:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下（主要是）商业自动驾驶车辆进展的更近期时间线：
- en: The DARPA Grand Challenge ([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge))
    was organized in 2004, 2005, and 2007\. In the first year, the participating teams'
    AVs had to navigate a 240 km route in the Mojave Desert. The best-performing AV
    managed just 11.78 km of that route, before getting hung up on a rock. In 2005,
    the teams had to overcome a 212 km off-road course in California and Nevada. This
    time, five vehicles managed to drive the whole route. The 2007 challenge was to
    navigate a mock urban environment, built in an air force base. The total route
    length was 89 km and the participants had to obey the traffic rules. Six vehicles
    finished the whole course.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DARPA 大挑战 ([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge))
    于2004年、2005年和2007年举办。在第一年，参赛队伍的自动驾驶车辆（AVs）需要在莫哈维沙漠中完成240公里的路线。表现最好的自动驾驶车辆仅行驶了11.78公里，最终因卡在一块岩石上而停下。2005年，队伍们需要在加利福尼亚州和内华达州克服212公里的越野路线。这一次，有五辆车成功完成了整个路线。2007年的挑战是在一个模拟城市环境中导航，该环境建在一个空军基地内。总路线长度为89公里，参赛者需要遵守交通规则。六辆车完成了整个赛程。
- en: In 2009, Google started developing self-driving technology. This effort led
    to the creation of Alphabet's (Google's parent company) subsidiary Waymo ([https://waymo.com/](https://waymo.com/)).
    In December 2018, they launched the first commercial on-demand ride-hailing service
    with AVs in Phoenix, Arizona. In October 2019, Waymo announced the start of the
    first truly driverless cars as part of their robotaxi service (previously, a safety
    driver had always been present).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2009年，谷歌开始开发自动驾驶技术。这项工作最终促成了谷歌母公司字母表（Alphabet）旗下子公司 Waymo 的成立 ([https://waymo.com/](https://waymo.com/))。2018年12月，他们在亚利桑那州凤凰城启动了首个商业化按需自动驾驶打车服务。2019年10月，Waymo
    宣布启动首个真正的无人驾驶汽车服务，这是他们机器人出租车服务的一部分（之前，始终有一名安全司机在场）。
- en: Mobileye ([https://www.mobileye.com/](https://www.mobileye.com/)) uses deep
    neural networks to provide driver-assistance systems (for example, lane-keeping
    assistance). The company has developed a series of **system-on-chip** (**SOC**)
    devices, specifically optimized to run neural networks with low energy consumption,
    required for automotive use. Its products are used by many of the major vehicle
    manufacturers. In 2017, Mobileye was acquired by Intel for $15.3 billion. Since
    then, BMW, Intel, Fiat-Chrysler, SAIC, Volkswagen, NIO, and the automotive supplier
    Delphi (now Aptiv) have cooperated on the joint development of self-driving technology.
    In the first three quarters of 2019, the total sales of Mobileye were $822 million,
    compared to $358 million in all four quarters of 2016.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mobileye ([https://www.mobileye.com/](https://www.mobileye.com/)) 使用深度神经网络提供驾驶辅助系统（例如车道保持辅助）。该公司开发了一系列专门优化低能耗运行神经网络的**系统级芯片**（**SOC**）设备，满足汽车应用的需求。其产品已被许多主要汽车制造商使用。2017年，Mobileye
    被英特尔以153亿美元收购。从那时起，宝马、英特尔、菲亚特-克莱斯勒、上汽、大众、蔚来和汽车供应商德尔福（现为 Aptiv）开始在自动驾驶技术的联合开发上进行合作。2019年前三个季度，Mobileye
    的总销售额为8.22亿美元，而2016年四个季度的总销售额为3.58亿美元。
- en: In 2016, General Motors acquired Cruise Automation ([https://getcruise.com/](https://getcruise.com/)),
    a developer of self-driving technology, for more than $500 million (the exact
    figure is unknown). Since then, Cruise Automation has tested and demonstrated
    multiple AV prototypes, driving in San Francisco. In October 2018, it was announced
    that Honda will also participate in the venture by investing $750 million in return
    for a 5.7% stake. In May 2019, Cruise secured $1.15 billion additional investment
    from a group of new and existing investors.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2016年，通用汽车收购了自动驾驶技术开发公司 Cruise Automation ([https://getcruise.com/](https://getcruise.com/))，交易金额超过5亿美元（具体数字未知）。自那时以来，Cruise
    Automation 已经测试并展示了多个自动驾驶原型车，并在旧金山进行了测试。2018年10月，宣布本田将通过投资7.5亿美元获得5.7%的股权，加入这一合作项目。2019年5月，Cruise获得了一笔来自新老投资者的11.5亿美元追加投资。
- en: In 2017, Ford Motor Co. acquired majority ownership of the self-driving startup
    Argo AI. In 2019, Volkswagen announced that it will invest $2.6 billion in Argo
    AI as part of a larger deal with Ford. Volkswagen would contribute $1 billion
    in funding and its $1.6 billion Autonomous Intelligence Driving subsidiary with
    more than 150 employees, based in Munich.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2017年，福特汽车公司收购了自动驾驶初创公司 Argo AI 的多数股权。2019年，大众宣布将投资26亿美元于 Argo AI，作为与福特达成的更大交易的一部分。大众将提供10亿美元的资金，并将其拥有的价值16亿美元、位于慕尼黑、拥有超过150名员工的自动智能驾驶子公司贡献给该项目。
- en: Levels of automation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化等级
- en: When we talk about AVs, we usually imagine fully driverless vehicles. But in
    reality, we have cars that require a driver, but still provide some automated
    features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论自动驾驶汽车时，通常会想象完全无人驾驶的车辆。但实际上，我们也有一些车辆虽然仍需驾驶员，但也提供某些自动化功能。
- en: 'The SAE has developed a scale of six levels of automation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: SAE制定了六个级别的自动化等级：
- en: '**Level 0**: The driver handles the steering, acceleration, and braking of
    the vehicle. The features at this level can only provide warnings and immediate
    assistance to the driver''s actions. Examples of features of this level include
    the following:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 0**：驾驶员负责车辆的转向、加速和制动。该级别的功能只能提供对驾驶员行为的警告和即时帮助。此级别的功能包括以下几种：'
- en: A lane-departure warning simply warns the driver when the vehicle has crossed
    one of the lane markings.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车道偏离警告仅在车辆越过车道标线时提醒驾驶员。
- en: A blind-spot warning warns the driver when another vehicle is located in the
    blind spot area of the car (the area immediately left or right of the rear end
    of the vehicle).
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盲点警告在车辆的盲区区域（车辆后端的左侧或右侧区域）有其他车辆时提醒驾驶员。
- en: '**Level 1**: Features that provide either steering or acceleration/braking
    assistance to the driver. The most popular of these features in vehicles today
    are the following:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 1**：提供转向或加速/制动辅助功能的系统。当前车辆中最常见的这种功能包括：'
- en: '**Lane-keeping assist** (**LKA**): The vehicle can detect the lane markings
    and use the steering to keep itself centered in the lane.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**车道保持辅助**（**LKA**）：车辆能够检测车道标线，并通过转向使自己保持在车道中央。'
- en: '**Adaptive cruise control** (**ACC**): The vehicle can detect other vehicles
    and use brakes and acceleration to maintain a preset speed or reduce it, depending
    on the circumstances.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应巡航控制**（**ACC**）：车辆可以检测其他车辆，并通过制动和加速来保持预设的速度，或根据情况降低速度。'
- en: '**Automatic emergency braking** (**AEB**): The vehicle can stop automatically
    if it detects an obstacle and the driver doesn''t react.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动紧急制动**（**AEB**）：如果车辆检测到障碍物且驾驶员没有反应，车辆可以自动停止。'
- en: '**Level 2**: Features that provide both steering and brake/acceleration assistance
    to the driver. One such feature is a combination of LKA and adaptive cruise control.
    At this level, the car can return control to the driver without advance warning at
    any moment. Therefore, he or she has to maintain a constant focus on the road
    situation. For example, if the lane markings suddenly disappear, the LKA system
    can prompt the driver to take control of the steering immediately.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 2**：为驾驶员提供转向和制动/加速辅助的功能。一个典型的例子是车道保持辅助（LKA）与自适应巡航控制的结合。在这个级别，车辆可以随时将控制权交还给驾驶员，而无需提前警告。因此，驾驶员必须始终保持对道路情况的关注。例如，如果车道标线突然消失，LKA系统会提示驾驶员立即接管转向。'
- en: '**Level 3**: This is the first level where we can talk about real autonomy.
    It is similar to level 2 in the sense that the car can drive itself under certain
    limited conditions and can prompt the driver to take control; however, this is
    guaranteed to happen in advance with sufficient time to allow an inattentive person
    to familiarize themselves with the road conditions. For example, say that the
    car drives itself on the highway, but the cloud-connected navigation obtains information
    about construction works on the road ahead. The driver will be prompted to take
    control well in advance of reaching the construction area.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 3**：这是我们可以讨论真正自动驾驶的第一个级别。它与Level 2的相似之处在于，车辆在某些有限的条件下可以自动驾驶，并且可以提示驾驶员接管控制；然而，这一过程会提前并给予足够时间，以便注意力不集中的人能及时熟悉道路状况。例如，假设车辆在高速公路上自动驾驶，但云连接的导航系统获取了前方道路施工的信息。驾驶员会在到达施工区域之前提前收到提示，提醒其接管驾驶。'
- en: '**Level 4**: Vehicles at level 4 are fully autonomous in a wider range of situations,
    compared to level 3\. For example, a locally geofenced (that is, limited to a
    certain region) taxi service could be at level 4\. There is no requirement for
    the driver to take control. Instead, if the vehicle goes outside this region,
    it should be able to safely abort the trip.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 4**：Level 4的车辆在更多的情况下具有完全的自动化能力，相较于Level 3。例如，本地限区域的出租车服务可能是Level 4。此时不需要驾驶员接管控制。如果车辆驶出该区域，它应能够安全地中止行程。'
- en: '**Level 5**: Full autonomy under all circumstances. The steering wheel is optional.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 5**：在所有情况下的完全自动驾驶。此时，方向盘是可选的。'
- en: All commercially available vehicles today have features at level 2 at most (even
    Tesla's Autopilot). The only exception (according to the manufacturer) is the
    2018 Audi A8, which has a level 3 feature called AI Traffic Jam Pilot. The system takes
    charge of driving at speeds up to 60 km/h on multilane roads with a physical barrier
    between the two directions of traffic. The driver can be prompted to take control
    with 10 seconds of advance warning. This feature was demonstrated during the launch
    of the vehicle, but as of the writing of this chapter, Audi cites regulatory limitations
    and doesn't include it in all markets. I have no information on where (or if)
    this feature is available.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 今天所有市面上销售的车辆最多只有2级功能（即使是特斯拉的自动驾驶）。唯一的例外（根据制造商的说法）是2018款奥迪A8，它拥有一个名为AI交通堵塞驾驶员的3级功能。该系统在车速最高达到60
    km/h时，能够在有物理隔离带的多车道道路上自动驾驶。驾驶员会在10秒前收到提示，要求其接管控制。这项功能在该车型发布时曾展示过，但截至本章撰写时，奥迪因受限于法规，未在所有市场提供此功能。我没有相关信息显示该功能是否或在哪里可用。
- en: In the next section, we'll look at the components that make up an AV system.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究构成自动驾驶系统的各个组件。
- en: Components of an AV system
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶系统的组件
- en: 'In this section, we''ll outline two types of AV system from a software architecture perspective.
    The first type uses sequential architecture with multiple components, as illustrated
    in the following diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将从软件架构的角度概述两种自动驾驶系统。第一种类型采用顺序架构，包含多个组件，如下图所示：
- en: '![](img/748d39a7-f202-48aa-862e-71ac49331605.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/748d39a7-f202-48aa-862e-71ac49331605.png)'
- en: The components of an AV system
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶系统的组件
- en: 'The system resembles the reinforcement-learning framework we briefly discussed
    in [Chapter 10](f641c4c2-60f2-41cb-a437-a961851dcc7f.xhtml), *Meta Learning*.
    We have a feedback loop where the environment (either the physical world or a
    simulation) provides the agent (vehicle) with its current state. In turn, the
    agent decides on its new trajectory, the environment reacts to it, and so on. Let''s
    start with the environment-perception subsystem, which has the following modules
    (we''ll discuss them in more detail in the following sections):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统类似于我们在[第10章](f641c4c2-60f2-41cb-a437-a961851dcc7f.xhtml)《元学习》中简要讨论过的强化学习框架。我们有一个反馈循环，其中环境（无论是物理世界还是仿真）提供给代理（车辆）其当前状态。代理则决定其新的行驶轨迹，环境对此作出反应，依此类推。让我们从环境感知子系统开始，它包含以下模块（我们将在接下来的章节中详细讨论）：
- en: '**Sensors:** Physical devices, such as cameras and radars.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器：**物理设备，如摄像头和雷达。'
- en: '**Localization:** Determines the exact position of the vehicle (with centimeter
    accuracy) within a high-definition map.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定位：**确定车辆在高清地图中的精确位置（精确到厘米级）。'
- en: '**Moving object detection and tracking:** Detects and tracks other traffic
    participants, such as vehicles and pedestrians.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动物体检测与跟踪：**检测并跟踪其他交通参与者，如车辆和行人。'
- en: 'The output of the perception system combines the data from its various modules
    to produce a **middle-level** virtual representation of the surrounding environment.
    This representation is usually a top-down (birds-eye) 2D view of the environment,
    referred to as the **occupancy map**. The following screenshot shows an example
    occupancy map of the ChauffeurNet system, which we''ll discuss later in the chapter.
    It includes road surfaces (white and yellow lines), traffic lights (red lines),
    and other vehicles (white rectangles). The image is best viewed in color:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 感知系统的输出将来自不同模块的数据进行合成，生成一个**中级**的周围环境虚拟表示。这种表示通常是一个从上方（鸟瞰）看的2D环境视图，称为**占用图**。以下截图展示了ChauffeurNet系统的一个占用图示例，我们将在本章后面详细讨论。它包括路面（白色和黄色线条）、交通信号灯（红色线条）以及其他车辆（白色矩形）。图片最好使用彩色显示：
- en: '![](img/5c78674a-7d9a-4252-82a9-44391a061de9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c78674a-7d9a-4252-82a9-44391a061de9.png)'
- en: Occupancy map of ChauffeurNet. Source: https://arxiv.org/abs/1812.03079
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet的占用图。来源： https://arxiv.org/abs/1812.03079
- en: The occupancy map serves as input for the **path-planning** module, which uses
    it to determine the future trajectory of the vehicle. The **control** module takes
    the desired trajectory and translates it to low-level control inputs to the vehicle.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 占用图作为**路径规划**模块的输入，后者利用它来确定车辆的未来行驶轨迹。**控制**模块将期望的轨迹转化为对车辆的低级控制指令。
- en: The middle-level representation approach has several advantages. Firstly, it
    is well-suited for the functions of the path-planning and control modules. Also, instead
    of using the sensor data to create the top-down image, we can produce it with
    a simulator. In this way, it will be easier to collect training data, as we won't
    have to drive a real car. Even more important is that we'll be able to simulate
    situations that rarely occur in the real world. For example, our AV has to avoid
    crashes at any cost, yet real-world training data will have very few, if any,
    crashes. If we only use real sensor data, one of the most important driving situations
    will be severely underrepresented.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 中级表示方法有几个优点。首先，它非常适合路径规划和控制模块的功能。此外，除了使用传感器数据来创建自上而下的图像，我们还可以使用模拟器来生成图像。这样，我们可以更容易地收集训练数据，因为我们不必驾驶真实的汽车。更重要的是，我们将能够模拟现实世界中很少发生的情况。例如，我们的自动驾驶系统必须避免任何形式的碰撞，但现实世界中的训练数据几乎没有或根本没有碰撞。如果我们仅使用真实的传感器数据，最重要的驾驶情况将被严重低估。
- en: 'The second type of AV system uses a single end-to-end component, which takes
    the raw sensor data as input and produces driving policy in the form of steering
    controls, as shown in the following diagram:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型的自动驾驶系统使用单一的端到端组件，该组件将原始传感器数据作为输入，并以转向控制的形式生成驾驶策略，如下图所示：
- en: '![](img/9696a032-249e-4692-adeb-fbc984c28b16.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9696a032-249e-4692-adeb-fbc984c28b16.png)'
- en: End-to-end AV system
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端自动驾驶系统
- en: In fact, we already mentioned an end-to-end system when we discussed ALVINN
    (in the *Brief history of AV research *section). Next, we'll focus on the different
    modules of the sequential system. We'll cover the end-to-end system in more detail
    later in the chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当我们在*自动驾驶研究简史*章节讨论ALVINN时，已经提到过端到端系统。接下来，我们将重点讨论顺序系统的不同模块。稍后我们将在本章中更详细地介绍端到端系统。
- en: Environment perception
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境感知
- en: For any automation feature to work, the vehicle needs a good perception of its
    surrounding environment. The environment-perception system has to identify the
    exact position, distance, and direction of moving objects, such as pedestrians,
    cyclists, and other vehicles. Additionally, it has to create a precise mapping
    of the road surface and the exact position of the vehicle on that surface and
    in the environment as a whole. Let's discuss the hardware and software components
    that help the AV create this virtual model of the environment.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使任何自动化功能正常工作，车辆需要对周围环境有良好的感知。环境感知系统必须识别移动物体的确切位置、距离和方向，如行人、自行车骑行者和其他车辆。此外，它还需要创建路面和车辆在该路面以及整个环境中的确切位置的精确映射。接下来，我们将讨论帮助自动驾驶系统创建这种虚拟环境模型的硬件和软件组件。
- en: Sensing
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知
- en: 'The key to building a good environment model is the vehicle sensors. The following
    is a list of the most important sensors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 构建良好环境模型的关键是车辆传感器。以下是最重要的传感器列表：
- en: '**Camera**: Its images are used to detect the road surface, lane markings,
    pedestrians, cyclists, other vehicles, and so on. An important camera property
    in the automotive context (besides the resolution) is the field of view. It measures
    how much of the observable world the camera sees at any given moment. For example,
    with a 180^o field of view, it can see everything in front of it and nothing behind.
    With a 360^o field of view, it can see everything in front of it and everything
    behind the vehicle (full observation). The following different types of camera
    systems exist:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄像头**：它的图像用于检测路面、车道标线、行人、自行车骑行者、其他车辆等。在汽车领域中，摄像头的一个重要特性（除了分辨率）是视场角。视场角测量摄像头在任何给定时刻可以看到多少可观察到的世界。例如，具有180^o视场角的摄像头可以看到它前方的所有内容，但看不见后方。具有360^o视场角的摄像头则可以看到它前方和后方的所有内容（全方位观察）。以下是几种不同类型的摄像头系统：'
- en: '**Mono camera**: Uses a single forward-facing camera, usually mounted on the
    top of the windshield. Most automation features rely on this type of camera to
    work. A typical field of view for the mono camera is 125^o.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单目摄像头**：使用一个朝前的单一摄像头，通常安装在挡风玻璃的顶部。大多数自动化功能都依赖这种类型的摄像头来工作。单目摄像头的典型视场角为125^o。'
- en: '**Stereo camera**: A system of two forward-facing cameras, slightly removed
    from each other. The distance between the cameras allows them to capture the same
    picture from a slightly different angle and combine them into a 3D image (in the
    same way we use our eyes). A stereo system can measure the distance to some of
    the objects in the image, while a mono camera relies only on heuristics to do
    this.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**立体相机**：由两个略微分开安装的前向摄像头组成。两个相机之间的距离使得它们可以从略有不同的角度捕捉到同一幅画面，并将其合成成3D图像（与我们使用眼睛的方式相同）。立体系统可以测量图像中某些物体的距离，而单目相机只能依靠启发式方法来完成这一任务。'
- en: '**360^o surrounding ****view of the environment**: some vehicles have a system
    of four cameras (front, back, left, and right).'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**360^o 环视系统**：一些车辆配备了由四个相机（前、后、左、右）组成的系统。'
- en: '**Night vision camera**: a system, where the vehicle includes a special type
    of headlight, which emits light in the infrared spectrum in addition to its regular
    function. The light is recorded from infrared cameras, which can display an enhanced
    image to the driver and detect obstacles during the night.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**夜视相机**：一种系统，车辆配备了一种特殊类型的前照灯，除了常规功能外，还能发射红外光谱中的光。这些光会被红外相机记录，并能向驾驶员显示增强的图像，帮助其在夜间识别障碍物。'
- en: '**Radar**: A system that uses a transmitter to emit electromagnetic waves (in
    the radio or microwave spectrum) in different directions. When the waves reach
    an object, they are usually reflected, some of them in the direction of the radar
    itself. The radar can detect them with a special receiver antenna. Since we know
    that radio waves travel at the speed of light, we can calculate the distance to
    the reflected object by measuring how much time has passed between emitting and
    receiving the signal. We can also calculate the speed of an object (for example,
    another vehicle) by measuring the difference between the frequencies of the outgoing
    and incoming waves (Doppler effect). The "image" of the radar is noisier, narrower,
    and with lower resolution, compared to a camera image. For example, a long-range
    radar can detect objects at a distance of 160 m, but in a narrow 12^o field of
    view. The radar can detect other vehicles and pedestrians, but it won''t be able
    to detect the road surface or lane markings. It is usually used for ACC and AEB,
    while the LKA system uses a camera. Most vehicles have one or two front-facing
    radars and, on rare occasions, a rear-facing radar.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**雷达**：一种系统，通过发射器向不同方向发射电磁波（在无线电或微波谱中）。当这些波到达物体时，通常会被反射，其中一部分波会朝向雷达本身的方向反射回来。雷达可以通过特殊的接收天线探测到这些反射波。因为我们知道无线电波以光速传播，所以通过测量发射和接收信号之间的时间差，我们可以计算出到反射物体的距离。我们还可以通过测量发射波和接收波的频率差（多普勒效应），来计算物体的速度（例如，另一辆车）。与相机图像相比，雷达的“图像”噪声更多，范围更窄，分辨率较低。例如，长距离雷达可以检测到160米远的物体，但视野仅为12^o。雷达可以探测其他车辆和行人，但无法检测到路面或车道标线。雷达通常用于ACC（自适应巡航控制）和AEB（自动紧急制动），而LKA系统则使用相机。大多数车辆配备一个或两个前向雷达，偶尔也会配备后向雷达。'
- en: '**Lidar** (**light detection and ranging**): This sensor issomewhat similar to
    radar, but instead of radio waves, it emits laser beams in the near-infrared spectrum.
    Because of this, one emitted pulse can accurately measure the distance to a single
    point. Lidar emits multiple signals very fast in a pattern, which creates a 3D
    point cloud of the environment (the sensor can rotate very fast). The following
    is a diagram of how a vehicle would see the world with a lidar:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激光雷达**（**光学探测与测距**）：这个传感器有点类似于雷达，但它不是使用无线电波，而是发射近红外光谱中的激光束。因此，一次发射的脉冲可以精确测量到一个单独点的距离。激光雷达非常快速地以一种模式发射多个信号，这样就能创建一个环境的3D点云（该传感器可以非常快速地旋转）。下面是一个车辆如何通过激光雷达看到世界的示意图：'
- en: '![](img/60018263-4727-4132-9f86-8a6e09b8b17a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60018263-4727-4132-9f86-8a6e09b8b17a.png)'
- en: A diagram of how a vehicle sees the world through lidar
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆如何通过激光雷达看到世界的示意图
- en: '**Sonar** (**sound navigation ranging**): This sensor emits pulses of ultrasonic
    waves and maps the environment by listening to the echos of the waves, reflected
    by the surrounding objects. Sonar is inexpensive compared to radar, but has a
    limited effective range of detection. Because of this, they are usually used in
    parking assistance features.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声呐**（**声波导航与测距**）：这种传感器发射超声波脉冲，通过听取波在周围物体上反射回来的回声来绘制环境地图。与雷达相比，声呐成本较低，但其有效探测范围有限。因此，它们通常用于停车辅助功能。'
- en: The data from multiple sensors can be merged into a single environment model
    with a process called **sensor fusion**. Sensor fusion is usually implemented using
    Kalman filters ([https://en.wikipedia.org/wiki/Kalman_filter](https://en.wikipedia.org/wiki/Kalman_filter)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 多个传感器的数据可以通过一种叫做**传感器融合**的过程合并成一个统一的环境模型。传感器融合通常通过使用卡尔曼滤波器（[https://en.wikipedia.org/wiki/Kalman_filter](https://en.wikipedia.org/wiki/Kalman_filter)）来实现。
- en: Localization
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定位
- en: '**Localization** is the process of determining the exact position of the vehicle on the
    map. Why is this important? Companies such as HERE ([https://www.here.com/](https://www.here.com/))
    specialize in creating extremely accurate road maps, where the entire area of
    the road surface is known to within a few centimeters. These maps could also include
    information about static objects of interest, such as lane markings, traffic signs,
    traffic lights, speed restrictions, zebra crossings, speed bumps, and so on. Therefore,
    if we know the exact position of the vehicle on the road, it won''t be hard to
    calculate the optimal trajectory.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**定位**是确定车辆在地图上精确位置的过程。为什么这很重要？像 HERE（[https://www.here.com/](https://www.here.com/)）这样的公司专注于创建极为精确的道路地图，这些地图中的道路表面区域的精度可达几厘米。这些地图还可能包括关于静态物体的信息，如车道标记、交通标志、交通信号灯、限速、斑马线、减速带等。因此，如果我们知道车辆在道路上的精确位置，那么计算最优轨迹就不再困难。'
- en: One obvious solution is to use GPS; however, GPS can be accurate to within 1-2
    meters under perfect conditions. In areas with high-rise buildings or mountains,
    the accuracy can suffer because the GPS receiver won't be able to get a signal
    from a sufficient number of satellites. One way to solve this problem is with **simultaneous
    localization and mapping** (**SLAM**) algorithms. These algorithms are beyond
    the scope of this book, but I encourage you to do your own research on the topic.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显而易见的解决方案是使用 GPS；然而，在完美条件下，GPS 的精度通常只能达到 1-2 米。在高楼大厦或山区等区域，GPS 的精度可能会受到影响，因为
    GPS 接收器无法从足够数量的卫星接收到信号。解决这个问题的一种方法是使用**同步定位与地图构建**（**SLAM**）算法。这些算法超出了本书的范围，但我鼓励你自行研究这个话题。
- en: Moving object detection and tracking
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移动物体检测与跟踪
- en: We now have an idea of what sensors the vehicle uses, and we have briefly mentioned
    the importance of knowing its exact location on the map. With this knowledge,
    the vehicle could theoretically navigate to its destination by simply following
    a breadcrumb trail of fine-grained points. However, the task of autonomous driving
    isn't that simple, because the environment is dynamic, as it includes moving objects
    such as vehicles, pedestrians, cyclists, and so on. An autonomous vehicle must
    constantly know the positions of the moving objects and track them as it plans
    its trajectory. This is one area where we can apply deep-learning algorithms to
    the raw sensor data. First, we'll do this for the camera. In [Chapter 5](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),* Object
    Detection and Image Segmentation*, we discussed how to use **convolutional networks**
    (**CNNs**) in two advanced vision tasks—object detection and semantic segmentation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了车辆使用的传感器，并且简要提到过了解其在地图上精确位置的重要性。通过这些知识，车辆理论上可以通过简单地跟随一系列细粒度的点来导航到目的地。然而，自动驾驶的任务并没有那么简单，因为环境是动态的，包括移动的物体，如车辆、行人、自行车和其他物体。自动驾驶车辆必须时刻了解这些移动物体的位置，并在规划其轨迹时进行跟踪。这是我们可以将深度学习算法应用于原始传感器数据的一个领域。首先，我们将针对摄像头进行这一应用。在[第
    5 章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)，*物体检测与图像分割*中，我们讨论了如何使用**卷积网络**（**CNNs**）进行两项先进的视觉任务——物体检测和语义分割。
- en: To recap, object detection creates a bounding box around different classes of
    objects detected in the image. Semantic segmentation assigns a class label to
    every pixel of the image. We can use segmentation to detect the exact shape of
    the road surface and the lane markings on the camera image. We can use object
    detection to classify and localize the moving objects of interest in the environment;
    however, we have already covered these topics in [Chapter 5](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml), *Object
    Detection and Image Segmentation. *In this chapter, we'll focus on the lidar sensor
    and we'll discuss how to apply CNNs over the 3D point cloud this sensor produces.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，物体检测会在图像中不同类别的物体周围创建一个边界框。语义分割会为图像的每个像素分配一个类别标签。我们可以使用分割来检测路面和车道标记的确切形状。我们可以使用物体检测来分类和定位环境中感兴趣的移动物体；不过，我们已经在[第5章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)中介绍了这些话题，*物体检测与图像分割*。本章将重点讨论激光雷达传感器，并讨论如何在该传感器生成的3D点云上应用CNN。
- en: Now that we've outlined the perception subsystem components, in the next section,
    we'll introduce the path planning subsystem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了感知子系统的组成部分，在下一节中，我们将介绍路径规划子系统。
- en: Path planning
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路径规划
- en: Path planning (or driving policy) is the process of calculating the vehicle trajectory
    and speed. Although we might have an accurate map and exact location of the vehicle,
    we still need to keep in mind the dynamics of the environment. The car is surrounded
    by other moving vehicles, pedestrians, traffic lights, and so on. What happens
    if the vehicle in front stops suddenly? Or if it's moving too slow? Our AV has
    to make the decision to overtake and then execute the maneuver. This is an area
    where ML and DL in particular can be especially useful, and we'll discuss two
    ways to implement these in this chapter. More specifically, we'll discuss using
    an imitation driving policy in an end-to-end learning system, as well as a driving
    policy algorithm called ChauffeurNet, which was developed by Waymo.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 路径规划（或驾驶策略）是计算车辆轨迹和速度的过程。尽管我们可能拥有准确的地图和车辆的精确位置，但我们仍然需要牢记环境的动态变化。汽车周围是其他移动的车辆、行人、交通信号灯等等。如果前方的车辆突然停下来会怎样？或者如果它行驶得太慢呢？我们的自动驾驶汽车必须做出超车的决策，并执行这一操作。这是机器学习和深度学习尤其能够发挥作用的领域，我们将在本章讨论两种实现方法。更具体地说，我们将讨论在端到端学习系统中使用模仿驾驶策略，以及Waymo开发的名为ChauffeurNet的驾驶策略算法。
- en: One obstacle in AV research is that building an AV and obtaining the necessary
    permits to test it is very expensive and time-consuming. Thankfully, we can still
    train our algorithms with the help of AV simulators.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶研究中的一个障碍是，构建一个自动驾驶汽车并获得必要的测试许可非常昂贵且耗时。幸运的是，我们仍然可以借助自动驾驶模拟器来训练我们的算法。
- en: 'Some of the most popular simulators are the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最受欢迎的模拟器：
- en: Microsoft AirSim, built on the Unreal Engine ([https://github.com/Microsoft/AirSim/](https://github.com/Microsoft/AirSim/))
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft AirSim，基于虚幻引擎构建（[https://github.com/Microsoft/AirSim/](https://github.com/Microsoft/AirSim/)）
- en: CARLA, built on the Unreal Engine ([https://github.com/carla-simulator/carla](https://github.com/carla-simulator/carla))
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CARLA，基于虚幻引擎构建（[https://github.com/carla-simulator/carla](https://github.com/carla-simulator/carla)）
- en: Udacity's Self-Driving Car Simulator, built with Unity ([https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim))
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Udacity的自动驾驶汽车模拟器，使用Unity构建（[https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim)）
- en: OpenAI Gym's `CarRacing-v0` environment (we'll see an example of this in the
    *Imitation driving policy *section)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym的`CarRacing-v0`环境（我们将在*模仿驾驶策略*部分看到一个例子）
- en: This concludes our description of the components of an AV system. Next, we'll
    discuss how to process 3D spatial data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对自动驾驶系统各个组成部分的描述。接下来，我们将讨论如何处理3D空间数据。
- en: Introduction to 3D data processing
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D数据处理介绍
- en: 'The lidar produces a point cloud—a set of data points in a three-dimensional
    space. Remember that the lidar emits laser beams. A beam reflecting off of a surface
    and returning to the receiver generates a single data point of the point cloud.
    If we assume that the lidar device is the center of the coordinate system and
    each laser beam is a vector, then a point is defined by the vector''s direction
    and magnitude. Therefore, the point cloud is an **unordered set** of vectors.
    Alternatively, we can define the points by their Cartesian coordinates in ![](img/625cb830-2d9a-49f5-a000-0aab3caee3fd.png) space,
    as illustrated in the left side of the following diagram. In this case, the point
    cloud is a set of vectors ![](img/c75c2878-da97-4a02-aa93-f0662c483bd1.png), where
    each vector ![](img/579fc922-e0cf-4c6a-b7f4-5c2390bd2284.png) contains the three
    coordinates of the point. For the sake of clarity, each point is represented as
    a cube:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 激光雷达（LiDAR）生成点云——一个三维空间中的数据点集。请记住，激光雷达发射激光束。激光束从一个表面反射并返回接收器时，会生成点云的一个数据点。如果我们假设激光雷达设备是坐标系的中心，并且每个激光束是一个向量，那么一个点由向量的方向和大小来定义。因此，点云是一个**无序集合**的向量。或者，我们可以通过它们在![](img/625cb830-2d9a-49f5-a000-0aab3caee3fd.png)空间中的笛卡尔坐标来定义这些点，如下图左侧所示。在这种情况下，点云是一个向量集![](img/c75c2878-da97-4a02-aa93-f0662c483bd1.png)，每个向量![](img/579fc922-e0cf-4c6a-b7f4-5c2390bd2284.png)包含该点的三个坐标。为了清晰起见，每个点被表示为一个立方体：
- en: '![](img/8461de45-e32b-4ef9-be6a-9245e28daec5.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8461de45-e32b-4ef9-be6a-9245e28daec5.png)'
- en: 'Left: Points (represented as cubes) in the 3D space; Right: A 3D grid of voxels'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：3D空间中的点（表示为立方体）；右图：体素的3D网格
- en: Next, let's focus on the input data format for neural networks, and specifically
    CNNs. A 2D color image is represented as a tensor with three slices (one for each
    channel) and each slice is a matrix (2D grid) composed of pixels. The CNN uses
    2D convolutions (see[ Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*). Intuitively, we might think that we can
    use a similar 3D grid of **voxels** (a voxel is a 3D pixel) for 3D point clouds,
    as illustrated in the right image of the preceding diagram. Assuming the point
    cloud points have no color, we can represent the grid as a 3D tensor and use it
    as input to a CNN with 3D convolutions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注神经网络的输入数据格式，特别是卷积神经网络（CNN）。一张2D彩色图像表示为一个张量，具有三个切片（每个通道一个），每个切片是一个由像素组成的矩阵（2D网格）。CNN使用2D卷积（参见[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，*理解卷积网络*）。直观上，我们可能会认为可以使用类似的3D网格体素（体素是3D像素）来处理3D点云，如前述图示的右侧所示。假设点云的点没有颜色，我们可以将网格表示为一个3D张量，并将其作为输入传递给具有3D卷积的CNN。
- en: 'However, if we take a closer look at this 3D grid, we can see that it is sparse.
    For example, in the preceding diagram, we have a point cloud with 8 points, but
    the grid contains 4 x 4 x 4 = 64 cells. In this simple case, we increase the memory
    footprint of the data eightfold, but in the real world the conditions, could be
    even worse. In this section, we''ll introduce PointNet (see *PointNet: Deep Learning
    on Point Sets for 3D* *Classification and Segmentation*, [https://arxiv.org/abs/1612.00593](https://arxiv.org/abs/1612.00593)),
    which provides a solution to this problem.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，如果我们仔细观察这个3D网格，就会发现它是稀疏的。例如，在前面的图示中，我们有一个包含8个点的点云，但网格包含4 x 4 x 4 = 64个单元。在这个简单的例子中，我们将数据的内存占用增加了8倍，但在现实世界中，情况可能更糟。
    在这一部分，我们将介绍PointNet（参见*PointNet: 深度学习在点集上的3D分类与分割*，[https://arxiv.org/abs/1612.00593](https://arxiv.org/abs/1612.00593)），它提供了解决此问题的方案。'
- en: 'PointNet takes as input the set of point cloud vectors **p***[i]*, rather than
    their 3D grid representation. To understand its architecture, we''ll start with
    the properties of the set of point cloud vectors that led to the network design
    (the following bullets contain quotes from the original paper):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet的输入是点云向量集**p***[i]*，而不是它们的3D网格表示。为了理解其架构，我们将从导致网络设计的点云向量集的特性开始（以下要点摘自原始论文）：
- en: '**Unordered**: Unlike pixel arrays in images or voxel arrays in 3D grids, a
    point cloud is a set of points without a specific order. Therefore, a network
    that consumes *N* 3D point sets needs to be invariant to *N*! permutations of
    the input set in data-feeding order.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无序**：与图像中的像素数组或3D网格中的体素数组不同，点云是一个没有特定顺序的点集。因此，消耗*N* 3D点集的网络需要对输入集在数据馈送顺序中的*N*!排列不变。'
- en: '**Interaction among points**: Similar to the pixels of an image, the distance
    between 3D points can indicate the level of relation among them—that is, it''s
    more likely that nearby points are part of the same object, compared to distant
    ones. Therefore, the model needs to be able to capture local structures from nearby
    points and the combinatorial interactions among local structures.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点之间的交互**：类似于图像的像素，3D 点之间的距离可以表示它们之间的关系水平——即，附近的点更可能属于同一物体，而远离的点则可能不属于同一物体。因此，模型需要能够捕捉来自附近点的局部结构及其局部结构之间的组合交互。'
- en: '**Invariance under transformations**: As a geometric object, the learned representation
    of the point set should be invariant to certain transformations. For example,
    rotating and translating points all together should not modify the global point
    cloud category, nor the segmentation of the points.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换下的不变性**：作为一个几何对象，点集的学习表示应该对某些变换具有不变性。例如，旋转和移动所有点不应改变全局点云类别，也不应改变点的分割。'
- en: 'Now that we know these prerequisites, let''s see how PointNet addresses them.
    We''ll start with the network architecture and then we''ll discuss its components in
    more detail:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了这些前提条件，让我们来看看 PointNet 如何解决这些问题。我们将从网络架构开始，然后更详细地讨论其组件：
- en: '![](img/ed73d15b-694e-481b-9f54-a357d3b2435b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed73d15b-694e-481b-9f54-a357d3b2435b.png)'
- en: PointNet architecture. Source: https://arxiv.org/abs/1612.00593
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet架构。来源：https://arxiv.org/abs/1612.00593
- en: PointNet is a **multilayer perceptron** (**MLP**). This is a feed-forward network
    that consists only of fully connected layers (and max pooling, but more on that
    later). As we mentioned, the set of input point cloud vectors **p***[i]* is represented
    as an *n* × 3 tensor. It is important to note that the network (up until the max
    pooling layer) is **shared** among all points of the set. That is, although the
    input size is *n* × 3, we can think of PointNet as applying the same network *n*
    times over *n* input vectors of size 1 × 3\. In other words, the network weights
    are shared among all points of the point cloud. This sequential arrangement also
    allows for an arbitrary number of input points.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet 是一个**多层感知器**（**MLP**）。这是一个前馈网络，仅由全连接层组成（还有最大池化层，但稍后会详细介绍）。正如我们所提到的，输入点云向量集合
    **p***[i]* 被表示为 *n* × 3 张量。需要注意的是，网络（直到最大池化层）是**共享的**，应用于集合中所有点。也就是说，尽管输入大小是 *n*
    × 3，我们可以认为 PointNet 是对大小为 1 × 3 的 *n* 个输入向量应用相同的网络 *n* 次。换句话说，网络权重在所有点云点之间是共享的。这种顺序安排还允许输入点的数量是任意的。
- en: The input passes through the input transform (we'll look at this in more detail
    later), which outputs another *n* × 3 tensor, where each of the *n* points is
    defined by three components (similar to the input tensor). This tensor is fed
    to an upsampling fully connected layer, which encodes each point to a 64-dimensional
    vector for *n* × 64 output. The network continues with another transformation,
    similar to the input transform. The result is then gradually upsampled with 64,
    then 128, and finally 1,024 fully connected layers to produce the final *n* ×
    1024 output. This tensor serves as input to a max pooling layer, which takes the
    maximum element of the same location among all *n* points and produces a 1,024-dimensional
    output vector. This vector is an aggregated representation of the whole set of
    points.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输入经过输入变换（稍后会详细介绍），输出另一个 *n* × 3 张量，其中每个 *n* 点由三个分量定义（类似于输入张量）。这个张量被输入到一个上采样全连接层，该层将每个点编码为一个64维向量，输出为
    *n* × 64。网络继续进行另一次变换，类似于输入变换。然后，结果通过 64、128 和最终 1,024 个全连接层逐渐上采样，产生最终的 *n* × 1024
    输出。这个张量作为输入传递到一个最大池化层，该层从所有 *n* 点中相同位置的元素中取最大值，并生成一个 1,024 维的输出向量。这个向量是整个点集的聚合表示。
- en: But why use max pooling in the first place? Remember that max pooling is a symmetric
    operation—that is, it will produce the same output regardless of the order of
    the inputs. At the same time, the set of points is unordered as well. Using max
    pooling ensures that the network will produce the same result regardless of the
    order of the points. The authors of the paper chose max pooling over other symmetric
    functions, such as average pooling and sum, because max pooling demonstrated the
    highest accuracy in the benchmark datasets.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么一开始就使用最大池化呢？请记住，最大池化是一个对称操作——也就是说，无论输入的顺序如何，它都会产生相同的输出。同时，点的集合本身也是无序的。使用最大池化可以确保网络无论点的顺序如何，都会产生相同的结果。论文的作者选择最大池化而非其他对称函数（如平均池化和求和），是因为在基准数据集中，最大池化表现出了最高的准确率。
- en: 'After the max pooling, the network splits into two networks, depending on the
    type of task (see the preceding diagram):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大池化后，网络根据任务类型分成两条网络（见前面的图示）：
- en: '**Classification**: The 1024D aggregate vector serves as input to several fully
    connected layers, which end with *k*-way softmax, where *k* is the number of classes.
    This is a standard classification pipeline.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：1024维的聚合向量作为输入进入多个全连接层，最终输出*k*类的softmax，其中*k*是类别的数量。这是一个标准的分类流程。'
- en: '**Segmentation**: This assigns a class to each point of the set. An extension
    of the classification net, this task requires a combination of local and global
    knowledge. As the diagram illustrates, we concatenate each of the *n* 64D intermediate
    point representations with the global 1024D vector for a combined *n* × 1088 tensor.
    Like the initial segment of the network, this path is also shared among all points.
    The vector of each point is downsampled to 128D with a series (1088 to 512, then
    to 256, and finally, to 128) fully connected layers. The final fully connected
    layer has *m* units (one for each class) and softmax activation.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割**：该任务为集合中的每个点分配一个类别。作为分类网络的扩展，这个任务需要结合局部和全局知识。如图所示，我们将每个*n*个64维的中间点表示与全局1024维向量进行拼接，形成一个*n*
    × 1088的张量。与网络的初始部分一样，这条路径也在所有点之间共享。每个点的向量通过一系列全连接层（从1088到512，再到256，最终到128）被下采样到128维。最后的全连接层有*m*个单元（每个类别一个）并使用softmax激活函数。'
- en: 'So far, we have explicitly addressed the unordered nature of the input data
    with the max pooling operation, but we still have to address the invariance and
    interaction among points. This is where the input and feature transforms will
    help. Let''s start with the input transform (in the preceding diagram, this is
    T-net). T-net is an MLP, which is similar to the full PointNet (it is referred
    to as a mini-PointNet), as illustrated in the following diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过最大池化操作明确处理了输入数据的无序性，但我们仍然需要解决点与点之间的变换不变性和相互作用。这就是输入和特征变换发挥作用的地方。我们先从输入变换开始（在前面的图示中，这是T-net）。T-net是一个多层感知机（MLP），与完整的PointNet类似（它被称为迷你PointNet），如下图所示：
- en: '![](img/cb033604-0864-4697-93ed-c98a43d30f3d.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb033604-0864-4697-93ed-c98a43d30f3d.png)'
- en: Input (and feature) transform T-nets
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输入（和特征）变换T-net
- en: The input transform T-net takes as input the *n* × 3 set of points (the same
    input as the full network). Like the full PointNet, T-net is shared among all
    points. First, the input is upsampled to *n* × 1024 with 64-, then 128-, and finally,
    1024-unit fully connected layers. The upsampled output is fed to a max pooling
    operation, which outputs 1 × 1024 vector. Then, the vector is downsampled to 1
    × 256 using two 512- and 256-unit fully connected layers. The 1 × 256 vector is
    multiplied by a 256 × 9 matrix of global (shared) learnable weights. The result
    is reshaped as a 3 × 3 matrix, which is multiplied by the original input point **p***[i]* over
    all points to produce the final *n* × 3 output tensor. The intermediate 3 × 3
    matrix acts as a type of learnable affine transformation matrix over the set of
    points. In this way, the points are normalized into a familiar perspective with
    respect to the network—that is, the network becomes invariant under transformations.
    The second T-net (feature transform) is almost identical to the first, with the
    exception that the input tensor is *n* × 64, which results in a 64 × 64 matrix.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输入变换T-net将*n* × 3点集（与全网络相同的输入）作为输入。与完整的PointNet一样，T-net在所有点上共享。首先，输入通过64单元、128单元和最终1024单元的全连接层上采样到*n*
    × 1024。上采样后的输出送入最大池化操作，输出1 × 1024向量。然后，该向量通过两个512单元和256单元的全连接层下采样为1 × 256。该1 ×
    256向量与256 × 9的全局（共享）可学习权重矩阵相乘，结果被重新调整为3 × 3矩阵，再与原始输入点**p***[i]*逐点相乘，最终生成*n* ×
    3的输出张量。中间的3 × 3矩阵充当点集上的可学习仿射变换矩阵。通过这种方式，点相对于网络进行规范化——也就是说，网络在变换下保持不变。第二个T-net（特征变换）与第一个几乎相同，唯一不同的是输入张量为*n*
    × 64，输出为64 × 64矩阵。
- en: 'Although the global max pooling layer ensures that the network is not influenced
    by the order of the data, it has another disadvantage, because it creates a single
    representation of the whole input set of points; however, these points might belong
    to different objects (for example, vehicles and pedestrians). In situations like
    this, the global aggregation could be problematic. To solve this, the authors
    of PointNet introduced PointNet++ (see *PointNet++: Deep Hierarchical Feature
    Learning on Point Sets in a Metric Space *at [https://arxiv.org/abs/1706.02413](https://arxiv.org/abs/1706.02413)),
    which is a hierarchical neural network that applies PointNet recursively on a
    nested partitioning of the input point set.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管全局最大池化层确保网络不受数据顺序的影响，但它也有另一个缺点，因为它创建了整个输入点集的单一表示；然而，这些点可能属于不同的物体（例如，车辆和行人）。在这种情况下，全局聚合可能会带来问题。为了解决这个问题，PointNet的作者提出了PointNet++（详见*PointNet++:
    Deep Hierarchical Feature Learning on Point Sets in a Metric Space*，可访问[https://arxiv.org/abs/1706.02413](https://arxiv.org/abs/1706.02413)），这是一种分层神经网络，通过对输入点集的嵌套划分递归应用PointNet。'
- en: In this section, we looked at 3D data processing in the context of the AV environment-perception
    system. In the next section, we'll shift our attention to the path-planning system
    with an imitation driving policy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了在自动驾驶环境感知系统中处理3D数据的内容。在下一节中，我们将把注意力转向模仿驾驶策略的路径规划系统。
- en: Imitation driving policy
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿驾驶策略
- en: 'In the *Components of an AV system* section, we outlined several modules that
    were necessary for a self-driving system. In this section, we''ll look at how
    to implement one of them—the driving policy—with the help of DL. One way to do
    this is with RL, where the car is the agent and the environment is, well, the
    environment. Another popular approach is **imitation learning**, where the model
    (network) learns to imitate the actions of an expert (human). Let''s look at the
    properties of imitation learning in the AV scenario:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在*自动驾驶系统的组件*部分，我们概述了构建自动驾驶系统所必需的几个模块。在本节中，我们将介绍如何通过深度学习实现其中之一——驾驶策略。实现这种方法的一种方式是使用强化学习（RL），其中汽车是智能体，环境就是环境。另一种流行的方法是**模仿学习**，其中模型（网络）学习模仿专家（人类）的行为。我们来看一下在自动驾驶场景中模仿学习的特性：
- en: We'll use a type of imitation learning, known as **behavioral cloning**. This
    simply means that we'll train our network in a supervised way. Alternatively,
    we could use imitation learning in a reinforcement learning (RL) scenario, which
    is known as inverse RL.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用一种模仿学习方法，称为**行为克隆**。这意味着我们将以监督的方式训练我们的网络。另一种选择是，在强化学习（RL）场景中使用模仿学习，这种方法被称为逆强化学习（inverse
    RL）。
- en: The output of the network is the driving policy, represented by the desired
    steering angle and/or acceleration or braking. For example, we can have one regression
    output neuron for the steering angle and one neuron for acceleration or braking
    (as we cannot have both at the same time).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的输出是驾驶策略，表示为期望的转向角度和/或加速度或刹车。例如，我们可以为转向角度设置一个回归输出神经元，为加速度或刹车设置一个神经元（因为不能同时控制两者）。
- en: 'The network input can be either of the following:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络输入可以是以下任意一种：
- en: Raw sensor data for end-to-end systems—for example, an image from the forward-facing
    camera. AV systems, where a single model uses raw sensor inputs and outputs a
    driving policy, are referred to as **end-to-end**.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端系统的原始传感器数据——例如来自前向摄像头的图像。AV系统中，单一模型使用原始传感器输入并输出驾驶策略，称为**端到端**。
- en: Middle-level environment representation for sequential composite systems.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于顺序复合系统的中级环境表示。
- en: 'We''ll create the training dataset with the help of an expert. We''ll let the
    expert drive the vehicle manually, either in the real world or in a simulator.
    At each step of the journey, we''ll record the following:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将借助专家创建训练数据集。我们让专家手动驾驶车辆，无论是在真实世界还是模拟器中。在每一步旅程中，我们将记录以下内容：
- en: The current state of the environment. This could be the raw sensor data or the
    top-down view representation. We'll use the current state as input for the model.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前的环境状态。这可以是原始传感器数据或自上而下的视图表示。我们将使用当前状态作为模型的输入。
- en: The actions of the expert in the current state of the environment (steering
    angle and braking/acceleration). This will be the target data for the network.
    During training, we'll simply minimize the error between the network predictions
    and the driver actions using the familiar gradient descent. In this way, we'll
    teach the network to imitate the driver.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前环境状态下专家的行为（转向角度和刹车/加速度）。这将是网络的目标数据。在训练过程中，我们只需通过常见的梯度下降方法最小化网络预测与驾驶员行为之间的误差。通过这种方式，我们将教会网络模仿驾驶员。
- en: 'The behavioral cloning scenario is illustrated in the following diagram:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆场景如以下图所示：
- en: '![](img/ceac0735-09ab-4cc5-b4f8-b8d8793e6663.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ceac0735-09ab-4cc5-b4f8-b8d8793e6663.png)'
- en: Behavioral cloning scenario
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆场景
- en: As we have already mentioned, ALVINN (from the *Brief history of AV research *section)
    is a behavioral cloning end-to-end system. More recently, the paper *End to End
    Learning for Self-Driving Cars* ([https://arxiv.org/abs/1604.07316](https://arxiv.org/abs/1604.07316))
    introduced a similar system, which uses a CNN with five convolutional layers instead
    of a fully connected network. In their experiment, the images of a forward-facing
    camera on the vehicle are fed as input to the CNN. The output of the CNN is a
    single scalar value, which represents the desired steering angle of the car. The
    network doesn't control acceleration and braking. To build the training dataset,
    the authors of the paper collected about 72 hours of real-world driving videos.
    During the evaluation, the car was able to drive itself 98% of the time in a suburban
    area (excluding making lane changes and turns from one road to another). Additionally,
    it managed to drive without intervention for 16 km on a multilane divided highway. In
    the following section, we'll implement something fun—a behavioral cloning example
    with PyTorch.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，ALVINN（来自*自动驾驶研究简史*章节）是一个行为克隆的端到端系统。最近，论文*端到端自驾车学习*([https://arxiv.org/abs/1604.07316](https://arxiv.org/abs/1604.07316))介绍了一个类似的系统，使用具有五个卷积层的CNN，取代了完全连接的网络。在他们的实验中，车载前向摄像头的图像被输入到CNN中。CNN的输出是一个标量值，表示汽车的期望转向角度。网络不控制加速和刹车。为了构建训练数据集，论文的作者收集了大约72小时的真实世界驾驶视频。在评估过程中，车辆能够在郊区区域自行驾驶98%的时间（不包括变道和从一条路转到另一条路）。此外，它还成功地在一条多车道的高速公路上行驶了16公里而没有干预。在接下来的章节中，我们将实现一些有趣的内容——一个使用PyTorch的行为克隆示例。
- en: Behavioral cloning with PyTorch
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行行为克隆
- en: In this section, we'll implement a behavioral cloning example with PyTorch 1.3.1. To
    help us with this task, we'll use OpenAI Gym ([https://gym.openai.com/](https://gym.openai.com/)),
    which is an open source toolkit for the development and comparison of reinforcement
    learning algorithms. It allows us to teach **agents** to undertake various tasks,
    such as walking or playing games such as Pong, Pinball, some other Atari games,
    and even Doom.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个基于 PyTorch 1.3.1 的行为克隆示例。为了帮助我们完成这一任务，我们将使用 OpenAI Gym（[https://gym.openai.com/](https://gym.openai.com/)），这是一个开源工具包，用于强化学习算法的开发和比较。它允许我们训练**智能体**完成各种任务，如走路或玩
    Pong、Pinball 等 Atari 游戏，甚至 Doom。
- en: 'We can install it with `pip`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `pip` 安装它：
- en: '[PRE0]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this example, we''ll use the `CarRacing-v0` OpenAI Gym environment, as shown
    in the following screenshot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 `CarRacing-v0` OpenAI Gym 环境，如下图所示：
- en: '![](img/741b8c00-00c9-4f8a-834c-cbca078fd4f7.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/741b8c00-00c9-4f8a-834c-cbca078fd4f7.png)'
- en: In the CarRacing-v0 environment, the agent is a racing car; a birds-eye view
    is used the whole time
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CarRacing-v0 环境中，智能体是一个赛车；始终使用俯视图。
- en: This example contains multiple Python files. In this section, we'll mention
    the most important parts. The full source code is at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例包含多个 Python 文件。在本节中，我们将提及最重要的部分。完整的源代码可以在 [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning)
    找到。
- en: 'The goal is for the red racing car (referred to as the agent) to drive around
    the track as quickly as it can without sliding off of the road surface. We can
    control the car using four actions: accelerate, brake, turn left, and turn right.
    The input for each action is continuous—for example, we can specify full throttle
    with the value 1.0 and half throttle with the value 0.5 (the same goes for the
    other controls).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是让红色赛车（称为**智能体**）尽可能快速地绕过赛道，同时避免滑出路面。我们可以通过四个动作来控制赛车：加速、刹车、左转和右转。每个动作的输入是连续的——例如，我们可以用
    1.0 表示全油门，用 0.5 表示半油门（其他控制也是如此）。
- en: 'For the sake of simplicity, we''ll assume that we can only specify two discrete
    action values: 0 for no action and 1 for full action. Since, originally, this
    was an RL environment, the agent will receive an award at each step as it progresses
    along the track; however, we''ll not use it, since the agent will learn directly
    from our actions. We''ll perform the following steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们假设只能指定两个离散的动作值：0 表示不做任何动作，1 表示完全动作。由于最初这是一个强化学习环境，智能体会在每一步进行奖励，但我们不使用奖励，因为智能体将直接从我们的动作中学习。我们将执行以下步骤：
- en: Create a training dataset by driving the car around the track ourselves (we'll
    control it with the keyboard arrows). In other words, we'll be the expert that
    the agent tries to imitate. At every step of the episode, we'll record the current
    game frame (state) and the currently pressed keys, and we'll store them in a file.
    The full code for this step is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/keyboard_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/keyboard_agent.py).
    All you have to do is run the file and the game will start. As you play, the episodes
    will be recorded (once every five episodes) in the `imitation_learning/data/data.gzip` file.
    If you want to start over, you can simply delete it. You can exit the game by
    pressing *Escape* and pause the game using the *Spacebar*. You can also start
    a new episode by pressing *Enter*. In this case, the current episode will be discarded
    and its sequence will not be stored. We suggest that you play at least 20 episodes
    for a sufficient size of the training dataset. It would be good to use the brake
    more often because otherwise, the dataset will become too imbalanced. In normal
    play, acceleration is used much more frequently than the brake or the steering.
    Alternatively, if you don't want to play, the GitHub repository already includes
    an existing data file.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个训练数据集，通过自己驾驶汽车绕赛道（我们将使用键盘箭头控制）。换句话说，我们将成为代理试图模仿的专家。在每一集的每一步中，我们将记录当前游戏帧（状态）和当前按下的键，并将它们存储在一个文件中。这一步的完整代码可在
    [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/keyboard_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/keyboard_agent.py)
    找到。你只需运行该文件，游戏即将开始。在你玩耍的过程中，每隔五集，就会在 `imitation_learning/data/data.gzip` 文件中记录一集。如果你想重新开始，只需简单地删除它。你可以通过按
    *Escape* 退出游戏，按 *Spacebar* 暂停游戏。你还可以通过按 *Enter* 开始新一集。在这种情况下，当前集将被丢弃，其序列不会被存储。我们建议你至少玩
    20 集，以获得足够大的训练数据集。建议经常使用制动，因为否则数据集会变得不平衡。在正常游戏中，加速度比制动或转向频繁使用得多。或者，如果你不想玩，仓库中已经包含了现有的数据文件。
- en: The agent is represented by a CNN. We'll train it in a supervised manner using
    the dataset we just generated. The input will be a single game frame and the output
    will be a combination of steering direction and brake/acceleration. The target
    (labels) will be the action recorded for the human operator. If you want to omit
    this step, the repository already has a trained PyTorch network located at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning/data/model.pt](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning/data/model.pt).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理由 CNN 表示。我们将使用刚刚生成的数据集以监督方式对其进行训练。输入将是单个游戏帧，输出将是方向盘转向和制动/加速的组合。目标（标签）将是人类操作者记录的动作。如果你想跳过这一步骤，[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning/data/model.pt](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning/data/model.pt)
    仓库中已经有一个训练好的 PyTorch 网络。
- en: Let the CNN agent play by using the network output to determine the next action
    to send to the environment. You can do this by simply running the [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)[ file.
    If you haven't performed any of the previous two steps, this file will use the
    existing agent. ](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让 CNN 代理通过使用网络输出来决定发送到环境的下一步动作来进行游戏。你可以通过简单运行 [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)
    文件来实现这一点。如果之前两个步骤你都没有执行过，这个文件将使用现有的代理。
- en: With that introduction, let's continue by preparing the training dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍完毕，让我们继续准备训练数据集。
- en: Generating the training dataset
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成训练数据集
- en: In this section, we'll look at how to generate a training dataset and load it
    as an instance of PyTorch's `torch.utils.data.DataLoader` class. We'll highlight
    the most relevant parts of the code, but the full source code is located at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/train.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/train.py).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何生成训练数据集并将其加载为 PyTorch 的 `torch.utils.data.DataLoader` 类的实例。我们会重点讲解代码中最相关的部分，但完整的源代码位于
    [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/train.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/train.py)。
- en: 'We''ll create the training dataset in several steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分几步创建训练数据集：
- en: 'The `read_data` function reads `imitation_learning/data/data.gzip` in two `numpy` arrays:
    one for the game frames and the other for the keyboard combinations associated
    with them.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`read_data` 函数读取 `imitation_learning/data/data.gzip` 中的两个 `numpy` 数组：一个用于游戏帧，另一个用于与之相关的键盘组合。'
- en: 'The environment accepts actions, composed of a three-element array, where the
    following are true:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境接受由三元素数组组成的动作，满足以下条件：
- en: The first element has a value in the range `[-1, 1]` and represents the steering
    angle (`-1` for right, `1` for left).
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个元素的值范围在 `[-1, 1]` 之间，表示转向角度（`-1` 表示右转，`1` 表示左转）。
- en: The second element is in the `[0, 1]` range and represents the throttle.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个元素的值在 `[0, 1]` 范围内，表示油门。
- en: The third element is in the `[0, 1] `range and represents the brake power.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个元素的值在 `[0, 1]` 范围内，表示刹车力度。
- en: We'll use the seven most common key combinations: `[0, 0, 0]` for no action
    (the car is coasting), `[0, 1, 0]` for acceleration, `[0, 0, 1]` for brake, `[-1,
    0, 0]` for left, `[-1, 0, 1]` for a combination of left and brake, `[1, 0, 0]` for
    right, and` [1, 0, 1]` for the right and brake combination. We have deliberately
    prevented the simultaneous use of acceleration and left or right, as the car becomes
    very unstable. The rest of the combinations are implausible. The `read_data` phrase
    will convert these arrays to a single class label from `0` to `6`. In this way,
    we'll simply solve a classification problem with seven classes.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用七种最常见的键位组合：`[0, 0, 0]` 表示没有操作（汽车滑行），`[0, 1, 0]` 表示加速，`[0, 0, 1]` 表示刹车，`[-1,
    0, 0]` 表示左转，`[-1, 0, 1]` 表示左转和刹车的组合，`[1, 0, 0]` 表示右转，以及 `[1, 0, 1]` 表示右转和刹车的组合。我们故意避免了同时使用加速和左转或右转，因为那样会导致汽车非常不稳定。其他的组合是不可能的。`read_data`
    函数会将这些数组转换为一个从 `0` 到 `6` 的单一类别标签。这样，我们就简单地解决了一个具有七个类别的分类问题。
- en: The `read_data` function will also balance the dataset. As we mentioned, acceleration
    is the most common key combination, while some of the others, such as brake, are
    the rarest. Therefore, we'll remove some of the acceleration samples and we'll
    multiply some of the braking (and left/right + brake). However, the author did this in
    a heuristic way by trying multiple combinations of deletion/multiplication ratios
    and selected the ones that work best. If you record your own dataset, your driving
    style may differ, and you may want to modify these ratios.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`read_data` 函数还将平衡数据集。如我们所提到的，加速是最常见的键位组合，而其他一些，比如刹车，则是最少见的。因此，我们将移除一些加速样本，并且会对一些刹车（以及左/右+刹车）样本进行倍增。然而，作者是通过尝试多种删除/倍增比例的组合，并选择表现最佳的方式来进行操作的。如果你记录自己的数据集，你的驾驶风格可能会有所不同，你可能需要调整这些比例。'
- en: Once we have the `numpy` arrays of the training samples, we'll use the `create_datasets`
    function to convert them to `torch.utils.data.DataLoader` instances. These classes
    simply allow us to extract the data in mini batches and apply data augmentation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了训练样本的 `numpy` 数组，我们将使用 `create_datasets` 函数将它们转换为 `torch.utils.data.DataLoader`
    实例。这些类简单地让我们能够以小批量的方式提取数据并应用数据增强。
- en: 'But first, let''s implement the `data_transform` list of transformations, which
    modify the image before feeding it to the network. The full implementation is
    available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/util.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/util.py).
    We''ll convert the image to grayscale, normalize the color values in the `[0,
    1]` range, and crop the bottom part of the frame (the black rectangle, which shows the
    rewards and other information). The implementation is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们实现`data_transform`转换列表，这些转换会在将图像输入网络之前修改图像。完整实现请参见[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/util.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/util.py)。我们将把图像转换为灰度图，规范化颜色值到`[0,
    1]`范围，并裁剪图像的底部（黑色矩形框，显示奖励和其他信息）。实现如下：
- en: '[PRE1]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let''s shift our attention back to the `create_datasets` function. We''ll
    start with the declaration:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将注意力转回到`create_datasets`函数。我们将从声明开始：
- en: '[PRE2]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we''ll implement the `TensorDatasetTransforms` helper class to be able
    to apply the `data_transform` transformations over the input image. The implementation
    is as follows (please bear in mind the indentation, as this code is still part
    of the `create_datasets` function):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将实现`TensorDatasetTransforms`辅助类，以便对输入图像应用`data_transform`转换。实现如下（请注意缩进，因为这段代码仍然是`create_datasets`函数的一部分）：
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we''ll read the previously generated dataset in full:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将完全读取之前生成的数据集：
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we''ll create the training and validation data loaders (`train_loader`
    and `val_loader`). Finally, we''ll return them as the result of the `create_datasets`
    function:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建训练和验证数据加载器（`train_loader`和`val_loader`）。最后，我们将它们作为`create_datasets`函数的结果返回：
- en: '[PRE5]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, let's focus on the agent NN architecture.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注代理的神经网络架构。
- en: Implementing the agent neural network
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现代理神经网络
- en: 'The agent is represented by a CNN with the following properties:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 代理由一个CNN表示，具有以下属性：
- en: A single-input 84 × 84 slice.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单输入的84 × 84切片。
- en: Three convolutional layers with striding for downsampling.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个卷积层，带有步幅用于下采样。
- en: ELU activations.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELU激活函数。
- en: Two fully connected layers.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个全连接层。
- en: Seven output neurons (one for each neuron).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 七个输出神经元（每个神经元一个）。
- en: Batch normalization and dropout, applied after each layer (even the convolutional)
    to prevent overfitting. Overfitting in this task is particularly exaggerated because
    we cannot use any meaningful data augmentation techniques. For example, say that
    we randomly flipped the image horizontally. In this case, we would have to also
    alter the label to reverse the steering value. Therefore, we'll rely on regularization
    as much as we can.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化和丢弃法（dropout），应用于每一层之后（即使是卷积层），以防止过拟合。在这个任务中，过拟合尤为严重，因为我们无法使用任何有意义的数据增强技术。例如，假设我们随机水平翻转了图像。在这种情况下，我们还需要修改标签，*反转*方向盘值。因此，我们将尽可能依赖正则化。
- en: 'The following code block shows the network implementation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了网络实现：
- en: '[PRE6]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Having implemented the training dataset and the agent, we can proceed with the
    training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了训练数据集和代理之后，我们可以继续进行训练。
- en: Training
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'We''ll implement the training itself with the help of the `train` function, which
    takes the network and the `cuda` device as parameters. We''ll use cross-entropy
    loss and the Adam optimizer (the usual combination for classification tasks).
    The function simply iterates `EPOCHS` times and calls the `train_epoch` and `test` functions
    for each epoch. The following is the implementation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过`train`函数实现训练，该函数接受网络和`cuda`设备作为参数。我们将使用交叉熵损失函数和Adam优化器（这是分类任务的常用组合）。该函数简单地迭代`EPOCHS`次数，并对每个周期调用`train_epoch`和`test`函数。以下是实现：
- en: '[PRE7]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we''ll implement the `train_epoch` for a single epoch training. This
    function iterates over all mini batches and performs forward and backward passes
    for each one. The following is the implementation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将实现`train_epoch`函数进行单轮训练。该函数遍历所有的小批量并对每个小批量执行前向和反向传播。以下是实现：
- en: '[PRE8]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `train_epoch` and `test` functions are similar to the ones we implemented
    for the transfer learning code example in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),* Understanding
    Convolutional Networks*. To avoid repetition, we won't implement the `test` function
    here, although it's available in the GitHub repository.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_epoch`和`test`函数类似于我们在[第二章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)中为迁移学习代码示例实现的函数，*理解卷积网络*。为了避免重复，我们在此不实现`test`函数，尽管它可以在GitHub仓库中找到。'
- en: We'll run the training for around 100 epochs, but you can shorten this to 20
    or 30 epochs for rapid experiments. One epoch usually takes less than a minute
    using the default training set. Now that we are familiar with the training, let's
    see how to use the agent NN to drive the race car in our simulated environment.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练大约100个epoch，但你可以将其缩短为20或30个epoch以进行快速实验。使用默认训练集时，一个epoch通常少于一分钟。既然我们已经熟悉了训练过程，让我们看看如何使用智能体神经网络在模拟环境中驾驶赛车。
- en: Letting the agent drive
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让智能体驾驶
- en: We'll start by implementing the `nn_agent_drive` function, which allows the
    agent to play the game (defined in [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)).
    The function will start the `env` environment with an initial state (game frame).
    We'll use it as an input to the network. Then, we'll convert the softmax network
    output from one-hot encoding to an array-based action and we'll send it to the
    environment to make the next step. We'll repeat these steps until the episode
    ends. The `nn_agent_drive` function also allows the user to exit by pressing *Escape*.
    Note that we still use the same `data_transform` transformations as we did for
    the training.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实现`nn_agent_drive`函数开始，该函数允许智能体玩游戏（定义在[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)）中。该函数将以初始状态（游戏帧）启动`env`环境，并将其作为输入传递给网络。然后，我们将把softmax网络输出从one-hot编码转换为基于数组的动作，并将其发送到环境中以执行下一步。我们将重复这些步骤，直到一集结束。`nn_agent_drive`函数还允许用户通过按*Escape*键退出。请注意，我们仍然使用与训练时相同的`data_transform`变换。
- en: 'First, we''ll implement the initialization part, which binds the *Esc* key
    and initializes the environment:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现初始化部分，将*Esc*键与环境初始化绑定：
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we''ll implement the main loop, where the agent (vehicle) takes an `action`,
    the environment returns the new `state`, and so on. This dynamic is reflected
    in the infinite `while` loop (please mind the indentation, as this code is still
    part of `nn_agent_play`):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现主循环，其中智能体（车辆）采取`action`，环境返回新的`state`，以此类推。这个动态反映在无限`while`循环中（请注意缩进，因为这段代码仍然是`nn_agent_play`的一部分）：
- en: '[PRE10]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We now have all the ingredients to run the program, which we will do in the
    following section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了运行程序的所有必要成分，接下来我们将在下一部分中运行它。
- en: Putting it all together
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: Finally, we can run the whole thing. The full code for this is available at
    [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/main.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/main.py).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以运行整个程序。完整代码可以在[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/main.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/main.py)找到。
- en: 'The following snippet builds and restores (if available) the network, runs
    the training, and evaluates the network:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段构建并恢复（如果可用）网络，运行训练并评估网络：
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Although we cannot show the agent in action here, you can easily see it in action
    by following the instructions in this section. Still, we can say that it learns
    well and is able to make full laps of the racing track on a regular basis (but
    not always). Interestingly, the network's driving style strongly resembles the
    style of the operator who generated the dataset. The example also goes to show
    that we shouldn't underestimate supervised learning. We were able to create a
    decently performing agent with a small dataset, and in a relatively short training
    time.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不能在这里展示代理的实际操作，但你可以通过本节中的说明轻松看到它的操作。不过，我们可以说它学习得很好，并且能够定期完成赛车赛道的全程（但并非总是如此）。有趣的是，网络的驾驶风格与生成数据集的操作员的风格非常相似。这个例子也表明，我们不应低估监督学习。我们能够利用一个小数据集，在相对较短的训练时间内创建出一个表现相当不错的代理。
- en: With this, we conclude our imitation learning example. Next, we'll discuss a
    much more sophisticated driving policy algorithm called ChauffeurNet.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们完成了模仿学习的示例。接下来，我们将讨论一个更加复杂的驾驶策略算法——ChauffeurNet。
- en: Driving policy with ChauffeurNet
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ChauffeurNet的驾驶策略
- en: 'In this section, we''ll discuss a recent paper called *ChauffeurNet: Learning
    to Drive by Imitating the Best and Synthesizing the Worst* ([https://arxiv.org/abs/1812.03079](https://arxiv.org/abs/1812.03079)).
    It was released in December 2018 by Waymo, one of the leaders in the field of
    AV. Let''s look at some of the properties of the ChaffeurNet model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将讨论一篇名为*ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing
    the Worst*的最新论文（[https://arxiv.org/abs/1812.03079](https://arxiv.org/abs/1812.03079)）。该论文于2018年12月由Waymo发布，Waymo是自动驾驶领域的领军企业之一。让我们来看看ChauffeurNet模型的一些特性：'
- en: It is a combination of two interconnected networks. The first is a CNN called
    FeatureNet, which extracts features from the environment. These features are fed
    as inputs to a second, recurrent network called AgentRNN, which determines the
    driving policy.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是两个相互连接的网络的组合。第一个是称为FeatureNet的卷积神经网络（CNN），用于从环境中提取特征。这些特征作为输入提供给第二个递归网络AgentRNN，后者决定驾驶策略。
- en: It uses imitation supervised learning in a similar way to the algorithms we
    described in the *Imitation driving policy* section. The training set is generated
    based on records of real-world driving episodes. ChauffeurNet can handle complex
    driving situations, such as lane changes, traffic lights, traffic signs, changing
    from one street to another, and so on.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以类似于我们在*模仿驾驶策略*章节中描述的算法的方式使用模仿监督学习。训练集是基于真实世界驾驶记录生成的。ChauffeurNet能够处理复杂的驾驶场景，如变道、交通信号灯、交通标志、从一条街道变到另一条街道等。
- en: This paper is published by Waymo on arxiv.org and is used here for referential
    purposes only. Waymo and arxiv.org are not affiliated, and do not endorse this
    book, or the authors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本文由Waymo发布在arxiv.org，仅用于参考目的。Waymo与arxiv.org并无隶属关系，且不对本书或其作者表示支持。
- en: We'll start our discussion about ChauffeurNet with the input and output data
    representations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从输入和输出数据表示开始讨论ChauffeurNet。
- en: Input and output representations
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入和输出表示
- en: The end-to-end approach feeds raw sensor data (for example, camera images) to
    the ML algorithm (NN), which in turn produces the driving policy (steering angle
    and acceleration). In contrast, ChauffeurNet uses the middle-level input and output
    that we introduced in the *Components of an AV system *section. Let's look at
    the input to the ML algorithm first. This is a series of top-down (birds-eye)
    view 400 × 400 images, similar to the images of the `CarRacing-v0` environment,
    but much more complex. One moment of time *t* is represented by multiple images,
    where each one contains different elements of the environment.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端方法将原始传感器数据（例如，摄像头图像）输入到机器学习算法（神经网络，NN）中，后者生成驾驶策略（转向角度和加速度）。相比之下，ChauffeurNet使用我们在*自动驾驶系统组成部分*章节中介绍的中间层输入和输出。让我们先来看一下机器学习算法的输入。这是一系列从上到下（鸟瞰视角）400
    × 400的图像，类似于`CarRacing-v0`环境中的图像，但要复杂得多。时刻*t*由多张图像表示，每张图像包含环境中的不同元素。
- en: 'We can see an example of a ChauffeurNet input/output combination in the following
    image:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到ChauffeurNet输入/输出组合的一个例子：
- en: '![](img/5b1c1d0b-b3f6-43aa-b788-9ca9cc18fec2.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b1c1d0b-b3f6-43aa-b788-9ca9cc18fec2.png)'
- en: ChauffeurNet inputs. Source: https://arxiv.org/abs/1812.03079
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet输入。来源：https://arxiv.org/abs/1812.03079
- en: 'Let''s look at the input elements ((a) through (g)) in alphabetical order:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按字母顺序依次查看输入元素（从(a)到(g)）：
- en: (a) is a precise representation of the road map. It is an RGB image, which uses
    different colors to represent various road features, such as lanes, cross-walks,
    traffic signs, and curbs.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (a) 是道路地图的精确表示。它是一张 RGB 图像，使用不同的颜色来表示各种道路特征，如车道、人行道、交通标志和路缘。
- en: (b) is a temporal sequence of grayscale images of the traffic lights. Unlike
    the features of (a), the traffic lights are dynamic—that is, they can be green,
    red, or yellow at different times. In order to properly convey their dynamics,
    the algorithm uses a series of images, displaying the state of the traffic lights
    for each lane at each of the past *T[scene]* seconds up to the current moment.
    The color of the lines in each image represents the state of each traffic light,
    where the brightest color is red, intermediate is for yellow, and the darkest
    is green or unknown.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (b) 是交通信号灯的灰度图像时间序列。与 (a) 的特征不同，交通信号灯是动态的——也就是说，它们可以在不同的时间是绿灯、红灯或黄灯。为了正确地表达它们的动态，算法使用一系列图像，显示过去
    *T[scene]* 秒内每个车道的交通信号灯状态，直到当前时刻。每个图像中线条的颜色表示每个交通信号灯的状态，其中最亮的颜色是红色，居中的颜色是黄色，最暗的颜色是绿色或未知。
- en: (c) is a grayscale image with the known speed limit for each lane. Different
    color intensities represent different speed limits.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (c) 是一张灰度图像，表示每个车道的已知限速。不同的颜色强度表示不同的限速。
- en: (d) is the intended route between the start and the destination. Think of it
    as the directions generated by Google Maps.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (d) 是起点和终点之间的预定路线。可以将其视为 Google Maps 生成的路线指引。
- en: (e) is a grayscale image that represents the current location of the agent (displayed
    as a white box).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (e) 是一张灰度图像，表示代理的当前位置（显示为白色方框）。
- en: (f) is a temporal sequence of grayscale images that represents the dynamic elements
    of the environment (displayed as boxes). These could be other vehicles, pedestrians,
    or cyclists. As these objects change locations over time, the algorithm conveys
    their trajectories with a series of snapshot images, representing their positions
    over the last *T[scene]* seconds. This works in the same way as the traffic lights
    (b).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (f) 是一系列灰度图像，表示环境的动态元素（显示为方框）。这些可能是其他车辆、行人或骑行者。随着这些物体的位置随时间变化，算法通过一系列快照图像来传达它们的轨迹，表示它们在过去
    *T[scene]* 秒内的位置。这与交通信号灯 (b) 的表现方式相同。
- en: (g) is a single grayscale image for the agent trajectory of the past *T[pose]* seconds
    until the current moment. The agent locations are displayed as a series of points
    on the image. Note that we display them in a single image, and not with a temporal
    sequence like the other dynamic elements. The agent at moment *t* is represented
    in the same top-down environment with the properties ![](img/84bc5eb3-b984-40f9-83a0-017cbc17a302.png),
    where ![](img/ef972f65-5d7b-415e-9661-845d5e4be8c8.png)is the coordinates, ![](img/c760b272-4b6d-4b96-bfd5-ea1372ae9872.png) is
    the orientation (or heading), and ![](img/2fa074d1-6bdd-498c-9926-88bc260d5ed9.png) is
    the speed.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (g) 是一张灰度图像，表示过去 *T[pose]* 秒内代理的轨迹，直到当前时刻。代理的位置显示为图像中的一系列点。请注意，我们将它们显示在一张单独的图像中，而不是像其他动态元素那样以时间序列的形式显示。时刻
    *t* 处的代理以相同的俯视图环境表示，属性为 ![](img/84bc5eb3-b984-40f9-83a0-017cbc17a302.png)，其中 ![](img/ef972f65-5d7b-415e-9661-845d5e4be8c8.png)
    是坐标，![](img/c760b272-4b6d-4b96-bfd5-ea1372ae9872.png) 是方向（或航向），而 ![](img/2fa074d1-6bdd-498c-9926-88bc260d5ed9.png)
    是速度。
- en: '(h) is the algorithm middle-level output: the agent''s future trajectory, represented
    as a series of points. These points carry the same meaning as the past trajectory
    (g). The future location output at time *t+1* is generated by using the past trajectory
    (g) up to the current moment *t.* We''ll denote ChauffeurNet as:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (h) 是算法的中间输出：代理的未来轨迹，表示为一系列的点。这些点与过去的轨迹 (g) 具有相同的含义。时间 *t+1* 处的未来位置是通过使用从过去轨迹
    (g) 到当前时刻 *t* 的信息生成的。我们将 ChauffeurNet 表示为：
- en: '![](img/d515d56e-ac45-42e9-9921-252b0148adbe.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d515d56e-ac45-42e9-9921-252b0148adbe.png)'
- en: Here, *I* is all the preceding input images, **p***[t]* is the agent position
    at time *t*, and *δt* is a 0.2 s time delta. The value of *δt* is arbitrary, chosen
    by the authors of the paper. Once we have *t+δt*, we can add it to the past trajectory
    (g) and we can use it to generate the next location at step *t+2δt* in a recurrent
    manner. The newly generated trajectory is fed to the control module of the vehicle,
    which tries its best to execute it via the vehicle controls (steering, acceleration,
    and brakes).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*I*是所有前面的输入图像，**p**[t]*是时间*t*时代理的位置，*δt*是0.2秒的时间差。*δt*的值是任意的，由论文的作者选择。一旦我们得到了*t+δt*，我们可以将其添加到过去的轨迹（g）中，并且可以通过递归方式生成步骤*t+2δt*时的下一个位置。新生成的轨迹会被输入到车辆的控制模块中，控制模块尽力通过车辆控制（转向、加速和刹车）来执行它。
- en: As we mentioned in the *Components of an AV system *section, this middle-level
    input representation allows us to use different sources of training data with
    ease. It can be generated from real-world driving with a fusion of the vehicle
    sensor inputs (such as cameras and lidar) and mapping data (such as streets, traffic
    lights, traffic signs, and so on). But we can also generate images of the same
    format with a simulated environment. The same applies to the middle-level output,
    where the control module can be attached to various types of physical vehicles
    or to a simulated vehicle. Using a simulation makes it possible to learn from
    situations that occur rarely in the real world, such as emergency braking or even
    crashes. To help the agent learn about such situations, the authors of the paper
    explicitly synthesized multiple rare scenarios using simulations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*自动驾驶系统的组成部分*部分中提到的，这种中级输入表示使我们能够轻松地使用不同来源的训练数据。它可以通过将真实世界的驾驶数据与车辆传感器输入（如摄像头和激光雷达）和地图数据（如街道、交通灯、交通标志等）融合生成。但我们也可以通过模拟环境生成相同格式的图像。中级输出同样适用，控制模块可以连接到各种类型的物理车辆或模拟车辆。使用模拟使得我们可以从现实世界中少见的情况中学习，比如紧急刹车甚至碰撞。为了帮助代理学习这些情况，论文的作者明确地通过模拟合成了多种罕见的场景。
- en: Now that we are familiar with the data representation, let's shift our focus
    to the model's core components.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了数据表示，接下来我们将重点关注模型的核心组件。
- en: Model architecture
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'The following diagram illustrates the ChauffeurNet model architecture:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了ChauffeurNet模型架构：
- en: '![](img/9f9e71b0-3b08-4565-8e1c-ff12d9fe8d86.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9e71b0-3b08-4565-8e1c-ff12d9fe8d86.png)'
- en: (a) ChauffeurNet architecture and (b) the memory updates over the iterations Source: https://arxiv.org/abs/1812.03079
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: （a）ChauffeurNet架构和（b）迭代过程中的记忆更新 来源：[https://arxiv.org/abs/1812.03079](https://arxiv.org/abs/1812.03079)
- en: 'First, we have FeatureNet (in the preceding diagram, this is marked by (a)).
    This is a CNN with residual connections, whose inputs are the top-down images
    we looked at in the *Input and output representations *section. The output of FeatureNet
    is a feature vector *F* which represents the synthesized network''s understanding
    of the current environment. This vector serves as one of the inputs to the recurrent
    network AgentRNN, which predicts successive points in the driving trajectory iteratively.
    Let''s say that we want to predict the next point of the agent''s trajectory at
    step *k*. In this case, AgentRNN has the following outputs:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有FeatureNet（在前面的图中，这由（a）标示）。这是一个带有残差连接的卷积神经网络（CNN），其输入是我们在*输入和输出表示*部分看到的自上而下的图像。FeatureNet的输出是一个特征向量*F*，它表示合成网络对当前环境的理解。这个向量作为递归网络AgentRNN的输入之一，AgentRNN会迭代地预测驾驶轨迹的后续点。假设我们想预测代理在步骤*k*时的下一个轨迹点。在这种情况下，AgentRNN有以下输出：
- en: '**p***[k]* is the predicted next point of the driving trajectory at that step.
    As we can see from the diagram, the output of AgentRNN is actually a heatmap with
    the same dimensions as the input images. It represents a probability distribution *P[k](x,
    y)* over the spatial coordinates, which indicates the probability of the next
    waypoint over each cell (pixel) of the heatmap. We use the `arg-max` operation
    to obtain the coarse pose prediction **p***[k]* from this heatmap.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**p**[k]* 是在该步骤中预测的驾驶轨迹的下一个点。从图中可以看到，AgentRNN的输出实际上是与输入图像具有相同维度的热图。它表示空间坐标上的概率分布*P[k](x,
    y)*，指示每个热图单元（像素）上下一个路径点的概率。我们使用`arg-max`操作从这个热图中获得粗略的姿态预测**p**[k]*。'
- en: '*B[k]* is the predicted bounding box of the agent at step *k.* Like the waypoint
    output, *B[k]* is a heatmap, but, here, each cell uses sigmoid activation and
    represents the probability that the agent occupies that particular pixel.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B[k]* 是步骤 *k* 时代理的预测边界框。像航点输出一样，*B[k]* 也是一个热图，但这里每个单元格使用 sigmoid 激活，表示代理占据该特定像素的概率。'
- en: There are also two additional outputs that are not displayed in the diagram: *θ[k]* for
    the heading (or orientation) of the agent and *s[k]* for the desired speed.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有两个额外的输出未在图中显示：*θ[k]* 代表代理的朝向（或方向），*s[k]* 代表期望的速度。
- en: ChauffeurNet also includes an additive memory, denoted by *M* (in the preceding diagram,
    this is marked by (b)). *M* is the single-channel input image (g) that we defined in
    the *Input and output representations *section. It represents the waypoint predictions (**p***[k, ]***p***[k-1], ....,* **p***[0]*)
    of the past steps *k*. The current waypoint **p***[k]* is added to the memory
    at each step, as displayed in the preceding diagram.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet 还包括一个加性记忆，记作 *M*（在前述图中，这由 (b) 标记）。*M* 是我们在 *输入与输出表示* 部分定义的单通道输入图像（g）。它表示过去步骤
    *k* 的航点预测（**p[k], p[k-1], ...., p[0]**）。当前航点 **p[k]** 会在每一步加入记忆，如前述图所示。
- en: 'The outputs **p***[k]* and *B[k]* are fed back recursively as inputs to AgentRNN
    for the next step *k+1*. The formula for the AgentRNN output is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 **p[k]** 和 *B[k]* 会作为输入递归地反馈给 AgentRNN，用于下一步 *k+1*。AgentRNN 输出的公式如下：
- en: '![](img/252b8e39-5d59-4993-93b1-094d00c4ccb2.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/252b8e39-5d59-4993-93b1-094d00c4ccb2.png)'
- en: 'Next, let''s check how ChauffeurNet integrates within the sequential AV pipeline:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看 ChauffeurNet 如何在顺序的自动驾驶管道中进行集成：
- en: '![](img/d3876c6c-0069-4f16-b7dd-c5977df1b337.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3876c6c-0069-4f16-b7dd-c5977df1b337.png)'
- en: ChauffeurNet within the full end-to-end driving pipeline. Source: https://arxiv.org/abs/1812.03079
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet 在完整的端到端驾驶管道中的应用。来源：https://arxiv.org/abs/1812.03079
- en: 'The system resembles the feedback loop that we introduced in the *Components
    of an AV system *section. Let''s look at its components:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统类似于我们在 *自动驾驶系统的组成部分* 部分介绍的反馈回路。我们来看看它的组成部分：
- en: '**Data Renderer**: Receives input from both the environment and the dynamic
    router. Its role is to transform these signals into the top-down input images
    we defined in the *Input and output representations* section.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据渲染器**：接收来自环境和动态路由器的输入。其作用是将这些信号转换为我们在 *输入与输出表示* 部分定义的自上而下的输入图像。'
- en: '**Dynamic Router**: Provides the intended route, which is dynamically updated,
    based on whether the agent was able to reach the previous target coordinates.
    Think of it as a navigation system, where you input a destination and it provides
    you with a route to the target. You start navigating this route and, if you stray
    from it, the system will calculate a new route dynamically based on your current
    location and your destination.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态路由器**：提供预期的路线，并根据代理是否能够到达前一个目标坐标动态更新。可以把它想象成一个导航系统，你输入一个目的地，它就会提供一条到目标的路线。你开始沿着这条路线导航，如果偏离路线，系统会根据你当前的位置和目标动态计算出新的路线。'
- en: '**Neural Net**: The ChauffeurNet module, which outputs the desired future trajectory.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：ChauffeurNet 模块，输出期望的未来轨迹。'
- en: '**Controls Optimization**: Receives the future trajectory and translates it
    into low-level control signals that drive the vehicle.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制优化**：接收未来轨迹，并将其转换为驱动车辆的低级控制信号。'
- en: ChauffeurNet is a rather complex system, so let's now look at how to train it.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet 是一个相当复杂的系统，所以现在我们来看看如何训练它。
- en: Training
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'ChauffeurNet was trained with 30 million expert driving examples using imitation
    supervised learning. The model inputs are the top-down images we defined in the
    *Input and output representations *section, as illustrated in the following flattened
    (aggregated) input image:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet 使用 3000 万个专家驾驶示例进行模仿监督学习训练。模型输入是我们在 *输入与输出表示* 部分定义的自上而下的图像，如下图所示的扁平化（聚合）输入图像：
- en: '![](img/d6203fbf-aed4-4f4b-aaa7-f16469147f55.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6203fbf-aed4-4f4b-aaa7-f16469147f55.png)'
- en: The image is best viewed in color. Source: https://arxiv.org/abs/1812.03079
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图像最好以彩色查看。来源：https://arxiv.org/abs/1812.03079
- en: 'Next, let''s look at the components of the ChauffeurNet training process:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看 ChauffeurNet 训练过程的组成部分：
- en: '![](img/890829fc-0939-4861-8b1e-b450fd4da108.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/890829fc-0939-4861-8b1e-b450fd4da108.png)'
- en: 'ChauffeurNet training components: (a) the model itself, (b) the additional
    networks, and (c) the losses. Source: https://arxiv.org/abs/1812.03079'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet训练组件：（a）模型本身，（b）附加网络，和（c）损失函数。来源： https://arxiv.org/abs/1812.03079
- en: 'We are already familiar with the ChauffeurNet model itself (marked as (a) in
    the preceding image). Let''s focus on the two additional networks involved in
    the process (marked as (b) in the preceding image):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉了ChauffeurNet模型本身（在前图中标记为(a)）。现在让我们关注过程中涉及的另外两个网络（在前图中标记为(b)）：
- en: '**Road Mask** N**et**: Outputs a segmentation mask with the exact area of the
    road surface over the current input images. To better understand this, the following
    image illustrates a target road mask (left) and the network''s predicted road
    mask (right):'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**道路掩码** Net：输出当前输入图像上道路表面确切区域的分割掩码。为了更好地理解这一点，以下图片展示了目标道路掩码（左）和网络预测的道路掩码（右）：'
- en: '![](img/63622968-9603-4650-9a12-80bc4b73e053.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63622968-9603-4650-9a12-80bc4b73e053.png)'
- en: Source: https://arxiv.org/abs/1812.03079
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 来源: https://arxiv.org/abs/1812.03079
- en: '**PerceptionRNN**: Outputs a segmentation mask with the predicted future locations
    of every other dynamic object in the environment (vehicles, cyclists, pedestrians,
    and so on). The output of PerceptionRNN is illustrated in the following diagram,
    which shows the predicted location of other vehicles (the light rectangles):'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PerceptionRNN**：输出一个分割掩码，包含环境中每个其他动态物体的预测未来位置（例如，车辆、骑行者、行人等）。PerceptionRNN的输出在下图中进行了说明，展示了其他车辆的预测位置（浅色矩形）：'
- en: '![](img/eabe2207-e95e-4023-8315-8dd16e9750b4.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eabe2207-e95e-4023-8315-8dd16e9750b4.png)'
- en: Source: https://arxiv.org/abs/1812.03079
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 来源: https://arxiv.org/abs/1812.03079
- en: These networks don't participate in the final vehicle control and are used only
    during training. The goal behind their use is that the FeatureNet network will
    learn better representations if it receives feedback from the tree tasks (AgentRNN,
    Road Mask Net, and PerceptionRNN), compared to simply getting feedback from AgentRNN.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络不参与最终的车辆控制，仅在训练过程中使用。使用它们的目标是，FeatureNet网络在接收来自三个任务（AgentRNN、道路掩码网和PerceptionRNN）的反馈时，将比单独从AgentRNN接收反馈时学到更好的表示。
- en: 'Now, let''s focus on the various loss functions (the bottom section (c) of
    the ChauffeurNet schema). We''ll start with the imitation losses, which reflect
    how the model prediction of the future agent position differs from the human expert
    ground truth. The following list shows the AgentRNN outputs with their corresponding
    loss functions:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注各种损失函数（ChauffeurNet架构的底部部分(c)）。我们从模仿损失开始，这反映了模型预测的未来代理位置与人类专家地面真值之间的差异。以下列表显示了AgentRNN的输出及其相应的损失函数：
- en: 'A probability distribution *P[k](x, y)* over the spatial coordinates of the
    predicted waypoint **p***[k]*. We''ll train this component with the following
    loss:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关于预测路径点**p**[k]*的空间坐标的概率分布*P[k](x, y)*。我们将使用以下损失函数来训练这个组件：
- en: '![](img/06ac107b-6888-48bf-add1-08c315700ae4.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06ac107b-6888-48bf-add1-08c315700ae4.png)'
- en: Here, ![](img/eb3acd58-a819-4e47-b23a-d09f0d696616.png) is the cross-entropy
    loss, ![](img/a8bff431-0e74-4985-bef3-9b94eb2e25e1.png) is the predicted distribution,
    and ![](img/172dac83-1613-41ae-a687-beb7a8e1faf9.png) is the ground truth distribution.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/eb3acd58-a819-4e47-b23a-d09f0d696616.png)是交叉熵损失，![](img/a8bff431-0e74-4985-bef3-9b94eb2e25e1.png)是预测的分布，![](img/172dac83-1613-41ae-a687-beb7a8e1faf9.png)是地面真值分布。
- en: 'A heatmap of the agent bounding box *B[k]*. We can train it with the following
    loss (applied along the cells of the heatmap):'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理边界框*B[k]*的热图。我们可以使用以下损失（应用于热图的各个单元）来训练它：
- en: '![](img/6e47c556-c586-4a2a-8e19-b305c2ecee06.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e47c556-c586-4a2a-8e19-b305c2ecee06.png)'
- en: Here, *W* and *H* are the input image dimensions, ![](img/8ec84f29-ae73-4729-b881-976620f96840.png) is
    the predicted heatmap, and ![](img/ee23c93f-9e74-40a8-aadb-8a18d59e2f70.png) is
    the ground truth heatmap.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*和*H*是输入图像的尺寸，![](img/8ec84f29-ae73-4729-b881-976620f96840.png)是预测的热图，![](img/ee23c93f-9e74-40a8-aadb-8a18d59e2f70.png)是地面真值热图。
- en: 'The heading  (orientation) of the agent *θ[k]* with the following loss:'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理的朝向（方向）*θ[k]*，其损失如下：
- en: '![](img/6e7cd74c-8631-4413-8ed4-b7ce69925a00.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e7cd74c-8631-4413-8ed4-b7ce69925a00.png)'
- en: Here, *θ[k]* is the predicted orientation and ![](img/1d0f268f-c485-4cca-a0c4-2961c61c9c6a.png) is
    the ground truth orientation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*θ[k]*是预测的方向，![](img/1d0f268f-c485-4cca-a0c4-2961c61c9c6a.png)是地面真值方向。
- en: 'The authors of the paper also introduce past motion dropout. We can best explain
    this by citing the paper:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者还介绍了过去运动的丢弃。我们可以通过引用论文来最好地解释这一点：
- en: During training, the model is provided the past motion history as one of the
    inputs (image (g) of the schema in section *Input and output representations*).
    Since the past motion history during training is from an expert demonstration,
    the net can learn to "cheat" by just extrapolating from the past rather than finding
    the underlying causes of the behavior. During closed-loop inference, this breaks
    down because the past history is from the net’s own past predictions. For example,
    such a trained net may learn to only stop for a stop sign if it sees a deceleration
    in the past history, and will therefore never stop for a stop sign during closed-loop
    inference. To address this, we introduce a dropout on the past pose history, where
    for 50% of the examples, we keep only the current position *(u[0], v[0])* of the
    agent in the past agent poses channel of the input data. This forces the net to
    look at other cues in the environment to explain the future motion profile in
    the training example.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型会将过去的运动历史作为输入之一（图像(g)在“*输入和输出表示*”一节中的示意图）。由于训练过程中使用的过去运动历史来源于专家演示，因此网络可以通过仅仅从过去外推来“作弊”，而不是去寻找行为的潜在原因。在闭环推理过程中，这种方法会失败，因为过去的历史来自于网络自身的过去预测。例如，这样训练出来的网络可能只会在看到过去历史中的减速时才为停车标志停车，因此在闭环推理中将永远不会为停车标志停车。为了解决这个问题，我们在过去的姿态历史上引入了一个丢弃机制，对于50%的样本，我们只保留智能体在过去姿态通道中的当前位置*(u[0],
    v[0])*，这样迫使网络通过环境中的其他线索来解释训练样本中的未来运动轮廓。
- en: 'They also observed that the imitation learning approach works well when the
    driving situation does not differ significantly from the expert driving training
    data. However, the agent has to be prepared for many driving situations that are
    not part of the training, such as collisions. If the agent only relies on the
    training data, it will have to learn about collisions implicitly, which is not
    easy. To solve this problem, the paper proposes explicit loss functions for the
    most important situations. These include the following:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还观察到，当驾驶情境与专家驾驶训练数据没有显著差异时，模仿学习方法效果良好。然而，智能体必须为许多训练数据中没有的驾驶情境做好准备，比如碰撞。如果智能体仅依赖于训练数据，它将不得不隐性地学习碰撞知识，这并不容易。为了解决这个问题，论文提出了针对最重要情境的显式损失函数。这些情境包括：
- en: '**Waypoint loss**: The error between the ground truth and the predicted agent
    future position *p[k]*.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**航点损失**：地面真实值与预测的智能体未来位置*p[k]*之间的误差。'
- en: '**Speed loss**: The error between the ground truth and the predicted agent
    future speed *s[k]*.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度损失**：地面真实值与预测的智能体未来速度*s[k]*之间的误差。'
- en: '**Heading loss**: The error between the ground truth and the predicted agent
    future direction *θ[k]*.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标损失**：地面真实值与预测的智能体未来方向*θ[k]*之间的误差。'
- en: '**Agent-box loss**: The error between the ground truth and the predicted agent
    bounding box *B[k]*.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能体边框损失**：地面真实值与预测的智能体边界框*B[k]*之间的误差。'
- en: '**Geometry loss**: Force the agent to explicitly follow the target trajectory,
    independent of the speed profile.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**几何损失**：强制智能体明确地遵循目标轨迹，与速度轮廓无关。'
- en: '**On-road loss**: Force the agent to navigate only over the road surface area
    and avoid the nonroad areas of the environment. This loss will increase if the
    predicted bounding box of the agent overlaps with the nonroad area of the image,
    predicted by the road mask network.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**道路损失**：强制智能体仅在道路表面区域导航，避免进入环境中的非道路区域。如果智能体的预测边界框与由道路掩码网络预测的非道路区域重叠，该损失将增大。'
- en: '**Collision loss**: Explicitly force the agent to avoid collisions. This loss
    will increase if the agent''s predicted bounding box overlaps with the bounding
    boxes of any of the other dynamic objects of the environment.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**碰撞损失**：明确地强制智能体避免碰撞。如果智能体预测的边界框与环境中任何其他动态物体的边界框重叠，则该损失将增大。'
- en: ChauffeurNet performed well in various real-world driving situations. You can
    see some of the results at [https://medium.com/waymo/learning-to-drive-beyond-pure-imitation-465499f8bcb2](https://medium.com/waymo/learning-to-drive-beyond-pure-imitation-465499f8bcb2).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ChauffeurNet 在各种真实世界的驾驶情况中表现良好。你可以在[https://medium.com/waymo/learning-to-drive-beyond-pure-imitation-465499f8bcb2](https://medium.com/waymo/learning-to-drive-beyond-pure-imitation-465499f8bcb2)查看一些结果。
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the applications of deep learning in AVs. We started
    with a brief historical overview of AV research and we discussed the different
    levels of autonomy. Then we described the components of the AV system and identified
    when it's appropriate to use DL techniques. Next, we looked at 3D-data processing
    and PointNet. Then we introduced the topic of implementing driving policies using
    behavioral cloning, and we implemented an imitation learning example with PyTorch.
    Finally, we looked at Waymo's ChauffeurNet system.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们探讨了深度学习在自动驾驶汽车（AVs）中的应用。我们从自动驾驶研究的简要历史回顾开始，并讨论了不同的自主性等级。接着，我们描述了自动驾驶系统的各个组件，并确定了何时使用深度学习技术。然后，我们研究了3D数据处理和PointNet。接着，我们介绍了使用行为克隆实现驾驶策略的主题，并用PyTorch实现了一个模仿学习的示例。最后，我们看了Waymo的ChauffeurNet系统。
- en: This chapter concludes our book. I hope you enjoyed the read!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了我们的书籍。希望你喜欢这次阅读！
