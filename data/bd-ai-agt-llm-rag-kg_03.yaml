- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Exploring LLMs as a Powerful AI Engine
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLM作为强大的AI引擎
- en: In the previous chapter, we saw the structure of a transformer, how it is trained,
    and what makes it so powerful. The transformer is the seed of this revolution
    in **natural language processing** (**NLP**), and today’s **large language models**
    (**LLMs**) are all based on transformers trained at scale. In this chapter, we
    will see what happens when we train huge transformers (more than 100 billion parameters)
    with giant datasets. We will focus on how to enable this training at scale, how
    to fine-tune similar modern ones, how to get more manageable models, and how to
    extend them to multimodal data. At the same time, we will also see what the limitations
    of these models are and what techniques are used to try to overcome these limitations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了Transformer的结构、它的训练方式以及是什么让它如此强大。Transformer是自然语言处理（NLP）革命的基础，今天的**大型语言模型**（LLM）都是基于大规模训练的Transformer。在本章中，我们将看到当我们使用巨型数据集训练巨大的Transformer（超过1000亿参数）时会发生什么。我们将关注如何实现这种大规模训练，如何微调类似的现代模型，如何获得更易于管理的模型，以及如何将它们扩展到多模态数据。同时，我们还将看到这些模型的局限性以及用来尝试克服这些局限性的技术。
- en: 'In this chapter, we''ll be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Discovering the evolution of LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现LLM的演变
- en: Instruction tuning, fine-tuning, and alignment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令调整、微调和对齐
- en: Exploring smaller and more efficient LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索更小、更高效的LLM
- en: Exploring multimodal models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索多模态模型
- en: Understanding hallucinations and ethical and legal issues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解幻觉以及伦理和法律问题
- en: Prompt engineering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Most of this code can be run on a CPU, but it is preferable to run it on a
    GPU. The code is written in PyTorch and uses standard libraries for the most part
    (PyTorch, Hugging Face Transformers, and so on). The code can be found on GitHub:
    [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码可以在CPU上运行，但最好在GPU上运行。代码是用PyTorch编写的，大部分使用标准库（PyTorch、Hugging Face Transformers等）。代码可以在GitHub上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3).
- en: Discovering the evolution of LLMs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现LLM的演变
- en: An LLM is a transformer (although different architectures are beginning to emerge
    today). In general, an LLM is defined as a model that has more than 10 billion
    parameters. Although this number may seem arbitrary, some properties emerge with
    scale. These models are designed to understand and generate human language, and
    over time, they have acquired the ability to generate code and more. To achieve
    this beyond parameter size, they are trained with a huge amount of data. Today’s
    LLMs are almost all trained on **next-word prediction** (**autoregressive** **language
    modeling**).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是一个Transformer（尽管今天开始出现不同的架构）。一般来说，LLM被定义为具有超过100亿参数的模型。尽管这个数字可能看起来是随机的，但随着规模的增加，一些特性开始出现。这些模型旨在理解和生成人类语言，并且随着时间的推移，它们已经获得了生成代码等能力。为了在参数大小之外实现这一点，它们使用大量数据进行训练。今天的LLM几乎都是基于**下一词预测**（**自回归**
    **语言模型**）进行训练的。
- en: 'Parameter growth has been motivated in the transformer field by different aspects:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 参数增长在Transformer领域是由不同方面推动的：
- en: '**Learnability**: According to the scaling law, more parameters should lead
    to greater capabilities and a greater understanding of nuances and complexities
    in the data'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可学习性**：根据规模定律，更多的参数应该导致更大的能力和对数据中细微差别和复杂性的更深入理解'
- en: '**Expressiveness**: The model can express more complex functions, thus increasing
    the ability to generalize and reducing the risk of overfitting'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表达能力**：该模型可以表达更复杂的函数，从而提高泛化能力并降低过拟合风险'
- en: '**Memory**: A larger number of parameters allows for internalizing more knowledge
    (information, entities, differences in topics)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：更多的参数允许内部化更多的知识（信息、实体、主题差异）'
- en: In the next subsections, we will discuss in detail all these elements to explain
    what is happening in the transition from the transformer to the LLM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将详细讨论所有这些元素，以解释从Transformer到LLM的过渡过程中发生了什么。
- en: The scaling law
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模定律
- en: 'It may seem surprising that such large models are trained with such a simple
    task as **language modeling**. Many practical **natural language** tasks can be
    represented as next-word prediction. This flexibility allows us to use LLMs in
    different contexts. For example, sentiment analysis can be cast as a next-word
    prediction. The sentence “*The sentiment of the sentence: ‘I like Pizza’ is*”
    can be used as input for an LLM, and we can extract the probability for the next
    token being *positive* or *negative*. We can then assign the sentiment depending
    on which of the two has the higher probability. Notice how this probability is
    a function of context:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '这样大的模型竟然用如此简单的任务**语言模型**进行训练，这似乎有些令人惊讶。许多实际的自然语言处理任务都可以表示为下一个单词的预测。这种灵活性使我们能够在不同的环境中使用LLM。例如，情感分析可以被视为下一个单词的预测。句子“*The
    sentiment of the sentence: ‘I like Pizza’ is*”可以用作LLM的输入，我们可以提取下一个标记为*正面*或*负面*的概率。然后，我们可以根据哪个概率更高来分配情感。注意，这个概率是上下文的函数：'
- en: 'P(positive| *The sentiment of the sentence: ‘I like* *Pizza’ is*)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'P(positive| *The sentiment of the sentence: ‘I like* *Pizza’ is*)'
- en: 'P(negative| *The sentiment of the sentence: ‘I like* *Pizza’ is*)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 'P(negative| *The sentiment of the sentence: ‘I like* *Pizza’ is*)'
- en: 'Similarly, we can use the same approach for other tasks. **Question answering**
    (**QA**) with an LLM can be thought of as generating the probability of the right
    answer given the question. In text summarization, we want to generate given the
    original context:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以用相同的方法处理其他任务。使用LLM进行**问答**可以被视为在给定问题的情况下生成正确答案的概率。在文本摘要中，我们希望在原始上下文中生成：
- en: 'QA: P(answer| question)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 问答：P(answer| question)
- en: 'Text summarization: P(summary|original article)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要：P(summary|original article)
- en: 'In the following diagram, we can see that using language modeling, we can solve
    almost any task. For example, here, the answer is the most probable token given
    the previous sequence (the question):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们可以看到使用语言模型几乎可以解决任何任务。例如，在这里，答案是给定前一个序列（问题）的最可能标记：
- en: '![Figure 3.1 – Rephrasing of any task as LM](img/B21257_03_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 将任何任务重新表述为语言模型](img/B21257_03_1.jpg)'
- en: Figure 3.1 – Rephrasing of any task as LM
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 将任何任务重新表述为语言模型
- en: What we need is a dataset large enough for the model to both learn knowledge
    and use that knowledge for tasks. For this, specific datasets are assembled for
    training an LLM. These datasets typically consist of billions of words obtained
    from various sources (internet, books, articles, GitHub, different languages,
    and so on). For example, **GPT-3** was trained with Common Crawl (web crawl data,
    410 billion tokens), Books1 and Books2 (book corpora, 12 billion and 55 billion
    tokens, respectively), and Wikipedia (3 billion tokens). Such diversity provides
    specific knowledge but also examples of tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一个足够大的数据集，以便模型既能学习知识，又能使用这些知识来完成任务。为此，为训练大型语言模型（LLM）而专门组装了特定数据集。这些数据集通常包含来自各种来源（互联网、书籍、文章、GitHub、不同语言等）的数十亿个单词。例如，**GPT-3**
    使用了 Common Crawl（网络爬虫数据，4100亿个标记）、Books1 和 Books2（书籍语料库，分别有120亿和550亿个标记）以及维基百科（30亿个标记）进行训练。这种多样性提供了特定的知识，同时也提供了任务示例。
- en: 'In parallel with the growth of training datasets (today, we are talking about
    more than a trillion tokens), the number of parameters has grown. The number of
    parameters in a transformer depends on three factors:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练数据集的增长（今天，我们谈论的超过万亿个标记）并行，参数数量也在增加。一个变换器中的参数数量取决于三个因素：
- en: '**Embedding layer**: The number of parameters on the size of the vector and
    the vocabulary (which, especially for multi-language models, can be very large).
    Attention mechanisms are the heaviest component and hold the most parameters.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入层**：向量大小和词汇表（特别是对于多语言模型，这可以非常大）上的参数数量。注意力机制是最重的组件，拥有最多的参数。'
- en: '**Self-attention mechanism**: This component includes multiple weight matrices
    that can grow in size with context length. Also, there can be multiple heads per
    single self-attention.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力机制**：这个组件包括多个权重矩阵，其大小可以随着上下文长度的增加而增长。此外，每个自注意力机制可以有多个头。'
- en: '**Depth**: Transformers are composed of multiple transformer blocks, and increasing
    the number of these blocks directly adds more parameters to the model.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度**：变换器由多个变换器块组成，增加这些块的数量会直接向模型添加更多参数。'
- en: 'GPT-3 and other studies have shown that the performance of LLMs depends mainly
    on three factors: model size (number of parameters), data size (the size of the
    training dataset), and computing size (amount of computing). So, in theory, to
    increase the performance of our model, we should enlarge the model (add layers
    or attention heads), increase the size of the pre-training dataset, and train
    it for more epochs. These factors have been related by OpenAI with the so-called
    **scaling law**. From a model with a number of parameters *N*, a dataset *D*,
    and computing amount *C*, if two parameters are constant, the loss *L* is the
    following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3和其他研究已经表明，大型语言模型（LLM）的性能主要取决于三个因素：模型大小（参数数量）、数据大小（训练数据集的大小）和计算大小（计算量）。因此，从理论上讲，为了提高我们模型的表现，我们应该扩大模型（增加层或注意力头），增加预训练数据集的大小，并增加训练的轮数。这些因素已经被OpenAI与所谓的**规模定律**联系起来。从一个具有参数数量*N*、数据集*D*和计算量*C*的模型，如果两个参数是常数，损失*L*可以表示为：
- en: <mrow><mrow><mi>L</mi><mfenced close=")" open="("><mi>N</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>N</mi></msub></msup><mi>L</mi><mfenced
    close=")" open="("><mi>D</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>D</mi></msub></msup><mi>L</mi><mfenced
    close=")" open="("><mi>C</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mi>c</mi></msub><mi>C</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>C</mi></msub></msup></mrow></mrow>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>L</mi><mfenced close=")" open="("><mi>N</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>N</mi></msub></msup><mi>L</mi><mfenced
    close=")" open="("><mi>D</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>D</mi></msub></msup><mi>L</mi><mfenced
    close=")" open="("><mi>C</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mi>c</mi></msub><mi>C</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>C</mi></msub></msup></mrow></mrow>
- en: 'This is represented visually in the following diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图表中进行了可视化表示：
- en: '![Figure 3.2 – Language modeling performance improves smoothly with the increase
    of model size, dataset size, and amount of computing (https://arxiv.org/pdf/2001.08361)](img/B21257_03_02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 随着模型大小、数据集大小和计算量的增加，语言模型性能平稳提升](https://arxiv.org/pdf/2001.08361)(img/B21257_03_02.jpg)'
- en: Figure 3.2 – Language modeling performance improves smoothly with the increase
    of model size, dataset size, and amount of computing ([https://arxiv.org/pdf/2001.08361](https://arxiv.org/pdf/2001.08361))
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 随着模型大小、数据集大小和计算量的增加，语言模型性能平稳提升([https://arxiv.org/pdf/2001.08361](https://arxiv.org/pdf/2001.08361))
- en: 'The loss is, in this case, the cross-entropy loss. In successive studies, OpenAI
    has shown that this loss can be decomposed into **irreducible loss** (which cannot
    be eliminated because it is related to data entropy) and reducible loss. This
    scaling law, in other words, allows us to calculate the desired performance of
    the model before training it. We can decide whether to invest more in enlarging
    the model or the dataset to reduce the loss (improve performance). However, these
    constants are dependent on the architecture and other training choices:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，损失是交叉熵损失。在后续的研究中，OpenAI已经表明这种损失可以分解为**不可减少损失**（因为它与数据熵相关，无法消除）和可减少损失。换句话说，这个规模定律允许我们在训练模型之前计算出期望的性能。我们可以决定是否投资更多来扩大模型或数据集以减少损失（提高性能）。然而，这些常数依赖于架构和其他训练选择：
- en: '![Figure 3.3 – Scaling law for an LLM](img/B21257_03_3.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – LLM的规模定律](img/B21257_03_3.jpg)'
- en: Figure 3.3 – Scaling law for an LLM
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – LLM的规模定律
- en: Although this scaling law has been taken for granted, the reality is more nuanced
    than it seems. According to DeepMind’s Chinchilla ([https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)),
    performance depends much more on the number of tokens than OpenAI believes. So,
    LLMs would currently be underfitted because they are trained with fewer tokens
    than expected. Meta’s Llama also states that not just any tokens will do, but
    they must be of quality. So, not all tokens count the same, and according to other
    authors, using tokens produced by other models is just a more sophisticated form
    of distillation. In other words, to train a model at its best, you need a large
    amount of tokens, and they should be preferentially produced by humans and not
    synthetics. Different studies showed the potential risk of the model collapsing
    when trained with synthetic data. In several cases, it has been shown that the
    model, when trained with synthetic data, has a substantial decrease in performance
    (**model collapse**) or may forget some of the skills it has learned (**catastrophic
    forgetting**).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个缩放定律已经被视为理所当然，但现实比这要复杂得多。根据DeepMind的Chinchilla ([https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556))，性能在很大程度上取决于标记的数量，而不仅仅是OpenAI所认为的那样。因此，LLMs目前可能存在欠拟合，因为它们训练的标记数量少于预期。Meta的Llama也指出，不仅仅是任何标记都适用，它们必须是高质量的。因此，并非所有标记都同等重要，根据其他作者的研究，使用其他模型生成的标记只是蒸馏的一种更复杂的形式。换句话说，为了训练一个模型达到最佳状态，你需要大量的标记，并且它们应该优先由人类产生，而不是合成。不同的研究表明，当使用合成数据进行训练时，模型崩溃的潜在风险。在几个案例中，已经表明，当使用合成数据进行训练时，模型性能会大幅下降（**模型崩溃**）或者可能会忘记它所学习的一些技能（**灾难性遗忘**）。
- en: In any case, the scaling law is of great interest because it allows us to experiment
    with different architectures and variants on smaller models and then scale the
    model and training until the desired performance is achieved. A model with more
    than 100 billion parameters is expensive to train (in terms of architecture, time,
    and money), so it is better to experiment with a small proxy model and then leverage
    what has been learned for training the larger model. Also, training such a large
    model can encounter issues (such as training spikes), and being able to predict
    performance with an accurate scaling law is an active area of research.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，缩放定律都非常有意义，因为它允许我们在较小的模型上实验不同的架构和变体，然后扩展模型和训练，直到达到期望的性能。拥有超过1000亿个参数的模型在训练上成本高昂（从架构、时间和金钱的角度来看），因此最好先实验一个小型的代理模型，然后利用所学知识来训练更大的模型。此外，训练如此大的模型可能会遇到问题（例如训练峰值），能够用准确的缩放定律预测性能是一个活跃的研究领域。
- en: This scaling law also monitors performance only in terms of loss. As mentioned
    previously, many tasks can be defined in terms of language modeling (LM), so intuitively,
    better performance in LM also means better performance in downstream tasks. Today,
    however, we try to create scaling laws that are instead specific to performance
    in some desired tasks (if we want a model specifically trained as a code assistant,
    we are more interested in its performance in these tasks than in its overall performance).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个缩放定律也只监控损失方面的性能。如前所述，许多任务可以用语言模型（LM）来定义，所以直观上，在LM中的更好性能也意味着在下游任务中的更好性能。然而，今天我们试图创建的缩放定律是针对某些期望任务中的性能（如果我们想要一个专门训练为代码助手的模型，我们对其在这些任务中的性能比对整体性能更感兴趣）。
- en: Emergent properties
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演化特性
- en: '**Emergent properties** of a model are the main justification for why we have
    gone from 1 billion parameters to over 100 billion. Emergent abilities are defined
    as properties that are not present in a small model but emerge in a large model.
    The second characteristic is that they emerge abruptly at a certain scale. In
    other words, a model has random performances in a certain ability until they emerge
    when a certain size is reached. These properties cannot be predicted beforehand
    but only observed at a certain scale, called the **critical scale**. After this
    critical size, performance increases linearly with the increase in size. Then,
    the model goes from near-zero performance to near-state-of-the-art after a certain
    critical point, thus showing a discontinuous rhythm. This process is also called
    phase transition. It is like a child who grows up appearing to be unable to speak,
    then beyond a certain age begins to articulate words, and then their skills grow
    linearly over time.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的**涌现特性**是我们从 10 亿参数增长到超过 1000 亿参数的主要原因。涌现能力被定义为在小型模型中不存在但在大型模型中出现的特性。第二个特征是它们在某个规模上突然出现。换句话说，模型在某个能力上具有随机的表现，直到达到某个规模时它们才会出现。这些特性不能事先预测，只能在达到某个规模时观察到，这个规模被称为**临界规模**。在这个临界规模之后，性能随着规模的增加而线性增长。然后，模型在达到某个临界点后，从几乎为零的性能到接近最先进水平，从而显示出不连续的节奏。这个过程也被称为相变。就像一个孩子似乎无法说话，然后超过某个年龄开始发音，然后随着时间的推移技能线性增长一样。
- en: '![Figure 3.4 – Example of an emergent property in an LLM](img/B21257_03_4.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – LLM 中涌现特性的示例](img/B21257_03_4.jpg)'
- en: Figure 3.4 – Example of an emergent property in an LLM
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – LLM 中涌现特性的示例
- en: 'Typically, these skills are related to complex skills such as mathematical
    reasoning or multistep processes. The fact that they emerge only beyond a certain
    scale justified the growth of such models, with the hope that beyond a certain
    scale, other properties would appear:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些技能与复杂的技能相关，如数学推理或多步骤过程。它们只在超过某个规模时出现的事实证明了这种模型的增长，希望超过某个规模后，其他特性也会出现：
- en: '![Figure 3.5 – Examples of emergent properties in different LLM families (https://arxiv.org/pdf/2206.07682)](img/B21257_03_5.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 不同大型语言模型家族中涌现特性的示例](https://arxiv.org/pdf/2206.07682)(img/B21257_03_5.jpg)'
- en: Figure 3.5 – Examples of emergent properties in different LLM families ([https://arxiv.org/pdf/2206.07682](https://arxiv.org/pdf/2206.07682))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 不同大型语言模型家族中涌现特性的示例([https://arxiv.org/pdf/2206.07682](https://arxiv.org/pdf/2206.07682))
- en: These properties do not all emerge at the same model size. Some properties would
    emerge beyond 10 billion (arithmetic computation), beyond 100 billion (self-evaluation,
    **figure-of-speech** (**FoS**) detection, logical deduction, and so on), and others
    even beyond 500 billion (causal judgment, geometric shapes, and so on).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性并非在相同的模型规模下出现。一些特性会在超过 100 亿（算术计算）之后出现，超过 1000 亿（自我评估、**修辞**（**FoS**）检测、逻辑推理等），而其他特性甚至会在超过
    5000 亿（因果判断、几何形状等）之后出现。
- en: For the authors of *Emergent Abilities of Large Language Models* ([https://arxiv.org/pdf/2206.07682](https://arxiv.org/pdf/2206.07682)),
    reasoning tasks (especially those involving multiple steps) are difficult for
    LMs. These capabilities would appear naturally after 100 billion parameters. Similarly,
    beyond this threshold, the model is capable of understanding and following instructions
    (instruction following) without necessarily giving it examples of how to follow
    them. From this, it follows that larger models would be capable of executing programs
    (coding ability).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于《大型语言模型的涌现能力》的作者([https://arxiv.org/pdf/2206.07682](https://arxiv.org/pdf/2206.07682))来说，推理任务（尤其是涉及多个步骤的任务）对于
    LMs 来说是困难的。这些能力会在达到 1000 亿参数后自然出现。同样，超过这个阈值，模型能够理解和遵循指令（指令遵循），而无需给出如何遵循它们的示例。从这个意义上说，更大的模型将能够执行程序（编码能力）。
- en: Interest in these emergent properties has cooled, however, because subsequent
    studies question them. LLMs do indeed exhibit these capabilities, but according
    to further studies, it would simply be more noticeable once the LLM has reached
    a certain performance limit. Moreover, it seems that success in these tasks is
    measured poorly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于后续研究对这些特性的质疑，对这些涌现特性的兴趣已经减弱。LLMs 确实表现出这些能力，但根据进一步的研究，一旦 LLM 达到一定的性能极限，这些能力就会更加明显。此外，似乎这些任务的成功衡量标准并不好。
- en: Context length
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文长度
- en: 'LLMs process text in chunks, a fixed context window of a specific number of
    tokens. The size of this context length defines how much information they can
    process at a given time. The greater the context length, the more information
    a model can handle at a given time. Similarly, the computational cost grows quadratically.
    So, a model with a context length of 4,096 tokens needs to do 64 times more computation
    than one of 512\. A longer context length allows for capturing long-range dependencies
    in a text, and this is related to performance in specific tasks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）以块的形式处理文本，具有特定数量的标记的固定上下文窗口。这个上下文长度的尺寸定义了它们在给定时间内可以处理多少信息。上下文长度越大，模型在给定时间内可以处理的信息就越多。同样，计算成本呈二次增长。因此，上下文长度为
    4,096 个标记的模型需要比 512 个标记的模型多进行 64 倍的计算。更长的上下文长度允许捕捉文本中的长距离依赖关系，这与特定任务中的性能相关：
- en: '**Document summarization**: More context allows for more consistent and concise
    summarization, allowing for better capture of information in the document and
    its relationships. The model captures entities and what they are related to in
    the entire document.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档摘要**：更多的上下文允许进行更一致和简洁的摘要，从而更好地捕捉文档及其关系中的信息。模型捕捉整个文档中的实体及其相关内容。'
- en: '**QA**: The model can find complex relationships that underlie the right answer.
    Also, in multi-turn questions, the model is aware of previous answers and questions.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答（QA）**：模型可以找到正确答案背后的复杂关系。此外，在多轮问答中，模型了解之前的答案和问题。'
- en: '**Language translation**: The model better preserves context, especially if
    there are long documents to be translated (especially if there are complex nuances).
    Bigger context lengths help with technical documents, technical jargon, polysemic
    items, and acronyms.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：模型更好地保留了上下文，特别是当有长文档需要翻译时（尤其是如果文档中有复杂的细微差别）。更大的上下文长度有助于处理技术文档、术语、多义词和缩写词。'
- en: '**Conversational AI**: The model can conduct better tracking of the entire
    conversation.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话式人工智能**：模型可以更好地跟踪整个对话。'
- en: 'As we can see in the following figure, the larger the context length, the more
    data the model can access in one prompt. Only one review can be seen by the model
    with a context length of 512, while a model with a larger context window can analyze
    hundreds of them:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图中所示，上下文长度越大，模型在一次提示中可以访问的数据就越多。当上下文长度为 512 时，模型只能看到一条评论，而具有更大上下文窗口的模型可以分析数百条：
- en: '![Figure 3.6 – Number of reviews that can be fit with an increasing context
    length window](img/B21257_03_6.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 随着上下文长度窗口增加所能适配的评论数量](img/B21257_03_6.jpg)'
- en: Figure 3.6 – Number of reviews that can be fit with an increasing context length
    window
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 随着上下文长度窗口增加所能适配的评论数量
- en: Mixture of experts
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家混合
- en: 'As we have seen, there is an intricate relationship between the amount of data,
    model scale, and computing budget. Given a fixed computing budget, it is better
    to train a larger model with fewer steps. A **mixture of experts** (**MoE**) allows
    one to train a model with less computing by scaling up the model with the same
    computing budget (which results in having a model as good as a dense one in less
    time). MoEs are, in general, made up of two components:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，数据量、模型规模和计算预算之间存在复杂的关系。在固定的计算预算下，最好通过减少步骤来训练更大的模型。**专家混合（MoE**）允许在相同的计算预算下通过扩展模型来训练，从而在更短的时间内获得与密集模型相当的效果。MoEs
    通常由两个组件组成：
- en: '**Sparse MoE layers**: Each layer consists of several experts (typically eight,
    but can be more), and each expert is a **neural network** (in the simplest form,
    a **feed-forward network** (**FFN**) layer, but they can also consist of multiple
    layers).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏 MoE 层**：每一层由几个专家组成（通常是八个，但也可以更多），每个专家都是一个**神经网络**（在最简单的情况下，是一个**前馈网络（FFN**）层，但它们也可以由多个层组成）。'
- en: '**A gate network or router**: This component decides what data is sent to each
    of the experts. In the case of an LLM, the router decides which tokens are seen
    by one or more experts. The router has learnable parameters that are trained during
    pre-training along with the rest of the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**门控网络或路由器**：此组件决定将哪些数据发送给每个专家。在大型语言模型（LLM）的情况下，路由器决定哪些标记被一个或多个专家看到。路由器具有可学习的参数，这些参数在预训练期间与模型的其他部分一起训练。'
- en: 'You can see an example of an MoE layer in *Figure 3**.7*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在**图 3.7**中看到一个 MoE 层的示例：
- en: '![Figure 3.7 – Example of an MoE layer: The router decides to which expert
    the token is sent; in this case, the expert is a simple FFN layer (https://arxiv.org/pdf/2101.03961)](img/B21257_03_7.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – MoE层的示例：路由器决定将令牌发送给哪个专家；在这种情况下，专家是一个简单的FFN层 (https://arxiv.org/pdf/2101.03961)](img/B21257_03_7.jpg)'
- en: 'Figure 3.7 – Example of an MoE layer: The router decides to which expert the
    token is sent; in this case, the expert is a simple FFN layer ([https://arxiv.org/pdf/2101.03961](https://arxiv.org/pdf/2101.03961))'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – MoE层的示例：路由器决定将令牌发送给哪个专家；在这种情况下，专家是一个简单的FFN层 ([https://arxiv.org/pdf/2101.03961](https://arxiv.org/pdf/2101.03961))
- en: The idea behind MoEs is that each of the experts focuses on a different subset
    of the training (or, more formally, a different region of the input space) and
    the router learns when to recall this expertise. This is called sparse computation
    because the model is not active on all inputs to the same model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MoE背后的想法是，每个专家都专注于训练的不同子集（或者更正式地说，输入空间的不同区域），路由器学习何时调用这种专业知识。这被称为稀疏计算，因为模型不是对所有相同模型的输入都活跃。
- en: 'This system has several advantages:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统有几个优点：
- en: Pre-training is faster compared to a dense model (classic transformer). The
    model is faster in inference since not all experts are used at the same time on
    all data.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与密集模型（经典Transformer）相比，预训练更快。模型在推理时更快，因为不是所有专家都在所有数据上同时使用。
- en: The system is flexible, can handle complex distribution, and each expert can
    specialize in a subdomain.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该系统灵活，可以处理复杂分布，每个专家都可以专注于子领域。
- en: It is much more scalable since we can have additional experts if needed.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的扩展性更好，因为我们可以在需要时拥有额外的专家。
- en: Better generalization, because we can average the expert predictions (wisdom
    of the crowd).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的泛化能力，因为我们可以平均专家的预测（群众的智慧）。
- en: 'There are some disadvantages, however:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些缺点：
- en: It requires high VRAM because all the experts have to be loaded into memory
    anyway.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要高VRAM，因为所有专家无论如何都必须加载到内存中。
- en: Training is more complex and can lead to overfitting. Also, without some accommodations,
    the model might use only the two or three most popular experts.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练更复杂，可能导致过拟合。此外，如果没有一些调整，模型可能只会使用两个或三个最受欢迎的专家。
- en: Fine-tuning is more complex, but new studies are solving the problem. MoE can
    be efficiently distilled, and we can also extract subnetworks.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调更复杂，但新的研究正在解决这个问题。MoE可以有效地蒸馏，我们还可以提取子网络。
- en: More complex interpretability, since we have now additional components.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更复杂的可解释性，因为我们现在有额外的组件。
- en: This is why many of today’s large models are MoE (for example, GPT-4 or Gemini).
    In the next section, we will see once an LLM is pre-trained how to adapt it to
    better interact with users or how we can fine-tune such a large model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么今天许多大型模型都是MoE（例如，GPT-4或Gemini）。在下一节中，我们将看到一旦LLM进行了预训练，如何适应以更好地与用户互动，或者我们如何微调如此大的模型。
- en: Instruction tuning, fine-tuning, and alignment
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指令调整、微调和对齐
- en: Fine-tuning such large models is potentially very expensive. In classical fine-tuning,
    the idea is to fit the weights of a model for a task or a new domain. Even if
    it is a slight update of the weights for a few steps, for a model of more than
    100 billion parameters, this means having large hardware infrastructure and significant
    costs. So, we need a method that allows us to have efficient and low-cost fine-tuning
    and preferentially keeping the model weights frozen.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 微调如此大的模型可能非常昂贵。在经典微调中，想法是为任务或新领域调整模型的权重。即使是对权重进行几步的轻微更新，对于一个超过1000亿参数的模型来说，这也意味着需要大量的硬件基础设施和显著的成本。因此，我们需要一种方法，使我们能够进行高效且低成本的微调，并优先保持模型权重冻结。
- en: 'The **intrinsic rank hypothesis** suggests that we can capture significant
    changes that occur in a neural network using a lower-dimensional representation.
    In the case of fine-tuning, the model weights after fine-tuning can be defined
    in this way:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**内在秩假设**表明，我们可以使用低维表示来捕捉神经网络中发生的重大变化。在微调的情况下，微调后的模型权重可以这样定义：'
- en: <mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mo>∆</mo><mi>W</mi></mrow></mrow>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mo>∆</mo><mi>W</mi></mrow></mrow>
- en: '*∆W* represents the update of the weights during fine-tuning. For the intrinsic
    rank hypothesis, not all of these elements of *∆W* are important, and instead,
    we can represent it as the product of two matrices with small dimensions *A* and
    *B* (low-rank matrices). So, in this case, the model weights remain frozen, but
    we just need to train these two matrices:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*∆W* 代表微调过程中的权重更新。对于内在秩假设，*∆W* 的所有这些元素并不都是重要的，相反，我们可以将其表示为两个小维度矩阵 *A* 和 *B*（低秩矩阵）的乘积。因此，在这种情况下，模型权重保持冻结，但我们只需要训练这两个矩阵：'
- en: <mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mi>B</mi><mi>A</mi></mrow></mrow>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mi>B</mi><mi>A</mi></mrow></mrow>
- en: A matrix can be decomposed into two smaller matrices that, when multiplied,
    give the original matrix. Also, a matrix (especially larger ones) contains a lot
    of redundant information. A matrix can be reduced into a set of linearly independent
    vectors (the number of linearly independent vectors needed to define a matrix
    is called a **rank**). With that in mind, the idea is to find two matrices that
    have a smaller rank than the original one and that multiplied with each other
    give us the same matrix update weights as if we had done fine-tuning. This process
    is called **Low-Rank** **Adaptation** (**LoRA**).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩阵可以被分解成两个较小的矩阵，当它们相乘时，给出原始矩阵。此外，一个矩阵（尤其是较大的矩阵）包含大量冗余信息。一个矩阵可以被减少为一组线性无关的向量（定义矩阵所需的线性无关向量的数量称为**秩**）。考虑到这一点，想法是找到两个比原始矩阵秩更小的矩阵，并且它们相乘给出与如果我们进行了微调相同的矩阵更新权重。这个过程被称为**低秩****自适应**（**LoRA**）。
- en: LLMs are over-parametrized. Although this is beneficial during the pre-training
    stage, it makes fine-tuning very expensive. Because the weight matrices of an
    LLM have a lot of linear dependence, there is a lot of redundant information,
    which is especially useless for domain adaptation. So, we can learn much smaller
    matrices (*A* and *B*) at a much lower cost.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是过参数化的。虽然这在预训练阶段是有益的，但它使得微调变得非常昂贵。因为LLM的权重矩阵有很多线性依赖，存在大量冗余信息，这对于领域自适应特别无用。因此，我们可以以更低的成本学习到更小的矩阵（*A*和*B*）。
- en: '![Figure 3.8 – Classical fine-tuning versus LoRA](img/B21257_03_8.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 经典微调与LoRA对比](img/B21257_03_8.jpg)'
- en: Figure 3.8 – Classical fine-tuning versus LoRA
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 经典微调与LoRA对比
- en: In LoRA, we keep the original weights of the LLM frozen. We then create two
    matrices (*A* and *B*) that, when multiplied together, will have the same dimensions
    as the model’s weight matrices (*W*). During fine-tuning, we pass the input *X*
    for the frozen model and the change matrix (the product *AB*) and get the output.
    With this output, we calculate the loss, and using this loss, we update the matrices
    *A* and *B* (via classical backpropagation). We continue this process until we
    are satisfied with the result.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoRA中，我们保持LLM的原始权重冻结。然后我们创建两个矩阵（*A*和*B*），当它们相乘时，将具有与模型权重矩阵（*W*）相同的维度。在微调期间，我们传递冻结模型的输入
    *X* 和变化矩阵（乘积 *AB*），并得到输出。有了这个输出，我们计算损失，并使用这个损失来更新矩阵 *A* 和 *B*（通过经典反向传播）。我们继续这个过程，直到我们对结果满意。
- en: '![Figure 3.9 – Different-ranked matrices to obtain the change weight matrix](img/B21257_03_9.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 获取变化权重矩阵的不同秩矩阵](img/B21257_03_9.jpg)'
- en: Figure 3.9 – Different-ranked matrices to obtain the change weight matrix
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 获取变化权重矩阵的不同秩矩阵
- en: In LoRA, we have a hyper-parameter *r* describing the depth of the *A* and *B*
    matrices. The greater the *r* value, the greater the amount of information these
    matrices have (but also a greater number of parameters and thus computational
    cost). The results show that even low-rank matrices perform quite well.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoRA中，我们有一个超参数 *r* 描述 *A* 和 *B* 矩阵的深度。*r* 值越大，这些矩阵包含的信息量就越大（但也意味着更多的参数和计算成本）。结果显示，即使是低秩矩阵也表现相当好。
- en: 'LoRA has several advantages:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA有几个优点：
- en: It is efficient in training (for GPT-3, a model of 175 billion parameters can
    be used efficiently by LoRA by training only 17.5 million parameters).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在训练中效率高（对于GPT-3，一个1750亿参数的模型可以通过LoRA有效地使用，只需训练1750万个参数）。
- en: In inference, it does not increase the computational cost (it is an addition
    where we add the change matrix to the original model weights).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，它不会增加计算成本（这是一个加法，我们将变化矩阵添加到原始模型权重中）。
- en: LoRA will not alter the original capabilities of the model. It also reduces
    the memory cost associated with saving checkpoints during fine-tuning.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA 不会改变模型的原始能力。它还减少了微调期间保存检查点相关的内存成本。
- en: We can create different change matrices for different applications and domains.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以为不同的应用和领域创建不同的变化矩阵。
- en: Another technique that focuses on training only added parameters is **adapters**.
    In this case, we add tunable layers within transformer blocks. These adapters
    are small layers that have **autoencoder** (**AE**)-like structures. For example,
    if the fully connected layers have 1024 dimensions, the adapter projects to 24
    and then reprojects to 1024\. This means that we are adding fewer than 50K parameters
    per adapter. In the original paper, the authors showed that the addition of adapters
    achieved the same performance as fine-tuning **Bidirectional Encoder Representations
    from Transformers** (**BERT**). Adapter require only the additional training of
    3.6 % more parameters. In contrast, fine tuning a model such as BERT in the traditional
    way means conducting training for all model parameters. This means that for the
    same performance, this method is computationally much more efficient.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种专注于仅训练添加参数的技术是**适配器**。在这种情况下，我们在变换器块内添加可调层。这些适配器是具有**自动编码器**（**AE**）结构的小层。例如，如果全连接层有1024个维度，适配器会投影到24，然后重新投影到1024。这意味着我们为每个适配器添加的参数少于50K。在原始论文中，作者展示了添加适配器可以达到与微调**双向编码器表示从变换器**（**BERT**）相同的性能。适配器只需要额外训练3.6%的参数。相比之下，以传统方式微调如BERT这样的模型意味着对所有模型参数进行训练。这意味着对于相同的性能，这种方法在计算上要高效得多。
- en: '![Figure 3.10 – How adapters are added to the transformer block (left); results
    show that the adapters can reach the performance of regular fine-tuning with much
    fewer parameters (right) (https://arxiv.org/pdf/1902.00751)](img/B21257_03_10.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – 如何将适配器添加到变换器块中（左）；结果显示适配器可以以更少的参数达到常规微调的性能（右）](img/B21257_03_10.jpg)'
- en: Figure 3.10 – How adapters are added to the transformer block (left); results
    show that the adapters can reach the performance of regular fine-tuning with much
    fewer parameters (right) ([https://arxiv.org/pdf/1902.00751](https://arxiv.org/pdf/1902.00751))
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 如何将适配器添加到变换器块中（左）；结果显示适配器可以以更少的参数达到常规微调的性能（右）([https://arxiv.org/pdf/1902.00751](https://arxiv.org/pdf/1902.00751))
- en: The advantages of adapters are that you can conduct fine-tuning by training
    far fewer parameters (a few million parameters for an LLM) and that the model
    retains the original capabilities.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器的优势在于，你可以通过训练远 fewer 参数（对于LLM来说，几百万参数）来进行微调，并且模型保留了原始能力。
- en: 'However, many other methods try to solve the problem of conducting fine-tuning
    of the model without conducting training on the original parameters. For example,
    some techniques, such as **prompt tuning**, prepend the model input embeddings
    with a trainable tensor that learns details associated with the new tasks. **Prefix
    tuning** is another technique in which we add trainable tensors to the hidden
    states of all layers. These parameters are learned with gradient descent while
    the rest of the parameters remain frozen. Prompt tuning and prefix tuning can
    still cause instability during training. LoRA and adapters remain the most widely
    used techniques:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多其他方法试图解决在不训练原始参数的情况下对模型进行微调的问题。例如，一些技术，如**提示微调**，在模型输入嵌入之前添加一个可训练的张量，该张量学习与新任务相关的细节。**前缀微调**是另一种技术，我们在所有层的隐藏状态中添加可训练的张量。这些参数在其余参数保持冻结的同时通过梯度下降进行学习。提示微调和前缀微调在训练过程中仍然可能导致不稳定性。LoRA
    和适配器仍然是使用最广泛的技术：
- en: '![Figure 3.11 – Parameter-efficient fine-tuning methods taxonomy (https://arxiv.org/pdf/2303.15647)](img/B21257_03_11.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 参数高效的微调方法分类法](img/B21257_03_11.jpg)'
- en: Figure 3.11 – Parameter-efficient fine-tuning methods taxonomy ([https://arxiv.org/pdf/2303.15647](https://arxiv.org/pdf/2303.15647))
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 参数高效的微调方法分类法([https://arxiv.org/pdf/2303.15647](https://arxiv.org/pdf/2303.15647))
- en: 'Although technically, it can be called a fine-tuning method, **alignment**
    is a method that with additional training attempts to align an LLM with human
    values. Indeed, with increasing model capabilities, there is an increasing fear
    of ethical risks (which will be described in detail in a later section). Alignment
    is meant to reduce these risks by reducing the mismatch between mathematical training
    and the soft skills expected of a human being (helpful, honest, and harmless):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然技术上可以称之为微调方法，**对齐**是一种通过额外训练尝试将LLM与人类价值观对齐的方法。确实，随着模型能力的增强，人们越来越担心伦理风险（将在后面的章节中详细描述）。对齐的目的是通过减少数学训练与人类期望的软技能之间的不匹配来降低这些风险（即，帮助、诚实和无害）：
- en: '![Figure 3.12 – Example to show the difference between the outputs before and
    after alignment (https://arxiv.org/pdf/2308.05374)](img/B21257_03_12.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12 – 展示对齐前后输出差异的示例（https://arxiv.org/pdf/2308.05374）](img/B21257_03_12.jpg)'
- en: Figure 3.12 – Example to show the difference between the outputs before and
    after alignment ([https://arxiv.org/pdf/2308.05374](https://arxiv.org/pdf/2308.05374))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 展示对齐前后输出差异的示例（[https://arxiv.org/pdf/2308.05374](https://arxiv.org/pdf/2308.05374)）
- en: An LLM during pre-training is trained to be nothing more than a sophisticated
    autocomplete model (predict the next word). With this simple objective, however,
    the model learns a vast knowledge and a wide array of skills. Alignment is intended
    to allow the model to use these skills obtained in pre-training in line with human
    values. Since human values can be subjective and difficult to encode in a mathematical
    objective, it was thought to use human feedback. Behind the success of ChatGPT
    is **Reinforcement Learning from Human Feedback** (**RLHF**), which precisely
    uses **reinforcement learning** to optimize an LLM based on human feedback.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，一个大型语言模型（LLM）被训练成仅仅是一个高级的自动补全模型（预测下一个单词）。然而，尽管目标简单，该模型却学会了大量的知识和各种技能。对齐的目的是让模型能够按照人类价值观使用在预训练中获得这些技能。由于人类价值观可能具有主观性，难以用数学目标进行编码，因此人们认为应该使用人类反馈。ChatGPT的成功背后是**基于人类反馈的强化学习**（**RLHF**），它精确地使用**强化学习**来优化基于人类反馈的LLM。
- en: 'RLHF consists of three main steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF包括三个主要步骤：
- en: '**Supervised fine-tuning (SFT)**: We select a list of prompts and ask human
    annotators to write outputs that match these prompts (from 10,000 to 100,000 pairs).
    We take a model that is not aligned (pre-trained LLM on a large text dataset)
    and fine-tune it on the prompts and the corresponding human-generated outputs.
    This is the SFT LLM, a model that tries to mimic annotator responses.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监督式微调（SFT）**：我们选择一个提示列表，并要求人类标注员为这些提示编写匹配的输出（从10,000到100,000对）。我们取一个未对齐的模型（在大文本数据集上预训练的LLM）并在提示和相应的人类生成输出上进行微调。这就是SFT
    LLM，一个试图模仿标注员响应的模型。'
- en: '**Training reward model**: We select a set of prompts and generate multiple
    outputs for each prompt using the SFT LLM. We then ask human annotators to rank
    them from preferred to less preferred (using criteria such as helpfulness or accuracy).
    Using this ranking, we train a reward model. The reward model takes as input the
    output of an LLM and produces a scalar reward signal as a measure of how well
    this output is aligned with human preferences.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练奖励模型**：我们选择一组提示，并使用SFT LLM为每个提示生成多个输出。然后我们请人类标注员根据偏好程度（如有用性或准确性）对它们进行排序。使用这个排序，我们训练一个奖励模型。奖励模型将LLM的输出作为输入，并产生一个标量奖励信号，作为衡量该输出与人类偏好对齐程度的一个指标。'
- en: '**RLHF**: We take a prompt and generate an output from the SFT LLM. We use
    the trained reward model to predict a reward on the output. Using a reinforcement
    learning algorithm (**Proximal Policy Optimization** (**PPO**)), we update the
    SFT LLM with the predicted reward. Adding a penalty term based on the **Kullback-Leibler**
    (**KL**) divergence prevents the model from straying too far from its original
    distribution (in other words, the output text remains consistent after RHLF).'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**RLHF**：我们从一个提示中生成SFT LLM的输出。我们使用训练好的奖励模型来预测输出上的奖励。使用强化学习算法（**近端策略优化**（**PPO**）），我们使用预测的奖励来更新SFT
    LLM。添加基于**Kullback-Leibler**（**KL**）散度的惩罚项，防止模型偏离其原始分布太远（换句话说，输出文本在RHLF后保持一致）。'
- en: '![Figure 3.13 – Diagram illustrating the three-step process (https://arxiv.org/pdf/2203.02155)](img/B21257_03_13.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 阐述三步过程的示意图（https://arxiv.org/pdf/2203.02155）](img/B21257_03_13.jpg)'
- en: Figure 3.13 – Diagram illustrating the three-step process ([https://arxiv.org/pdf/2203.02155](https://arxiv.org/pdf/2203.02155))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 说明三个步骤过程的图解([https://arxiv.org/pdf/2203.02155](https://arxiv.org/pdf/2203.02155))
- en: 'The method is not without its problems, though. Collecting human preference
    data is quite expensive and requires hiring part-time staff as annotators. These
    annotators must also be selected to avoid variability and different quality in
    responses. Second, the process is rather complex and unstable. **Direct Preference
    Optimization** (**DPO**) is an alternative that attempts to solve part of these
    problems by eliminating the need to have a reward model. In short, the dataset
    is created according to this format: *<prompt, worse completion, better completion>*.
    DPO uses a loss function to increase the probability of better completion and
    decrease the probability of worse completion. This allows us to use backpropagation
    and avoid reinforcement learning:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法并非没有问题。收集人类偏好数据相当昂贵，需要雇佣兼职人员作为标注员。这些标注员还必须被选中以避免响应的变异性以及不同质量。其次，这个过程相当复杂且不稳定。**直接偏好优化**（**DPO**）是一种替代方案，试图通过消除需要奖励模型的需求来解决这些问题的一部分。简而言之，数据集是根据以下格式创建的：*<提示，较差的完成，较好的完成>*。DPO
    使用损失函数来增加较好完成的概率并降低较差完成的概率。这使我们能够使用反向传播并避免强化学习：
- en: '![Figure 3.14 – DPO optimizes for human preferences while avoiding RL (https://arxiv.org/pdf/2305.18290)](img/B21257_03_14.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.14 – DPO 在避免强化学习的同时优化人类偏好](https://arxiv.org/pdf/2305.18290))(img/B21257_03_14.jpg)'
- en: Figure 3.14 – DPO optimizes for human preferences while avoiding RL ([https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290))
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – DPO 在避免强化学习的同时优化人类偏好([https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290))
- en: '**Instruction tuning** (**IT**) is a fine-tuning technique that is used to
    improve the model’s capabilities for various tasks and generally in following
    instructions. The principle is similar to alignment: the pre-trained model is
    trained to minimize word prediction on large corpora and not to execute instructions.
    Most user interactions with LLMs are requests to perform a specific task (write
    a text, create a function, summarize an article, and so on):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**指令调整**（**IT**）是一种微调技术，用于提高模型在执行各种任务和通常遵循指令方面的能力。其原理与对齐相似：预训练模型被训练以在大语料库上最小化单词预测，并且不执行指令。大多数与大型语言模型（LLM）的用户交互都是执行特定任务（写文本、创建函数、总结文章等）的请求：'
- en: '![Figure 3.15 – General pipeline of IT (https://arxiv.org/pdf/2308.10792)](img/B21257_03_15.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – IT 的一般流程](https://arxiv.org/pdf/2308.10792))(img/B21257_03_15.jpg)'
- en: Figure 3.15 – General pipeline of IT ([https://arxiv.org/pdf/2308.10792](https://arxiv.org/pdf/2308.10792))
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – IT 的一般流程([https://arxiv.org/pdf/2308.10792](https://arxiv.org/pdf/2308.10792))
- en: To solve this mismatch, IT has been proposed to increase the model’s capabilities
    and controllability. The pre-trained model is further trained on a dataset that
    is a constituted instructions-outputs pair (instructions for the model and the
    desired output). This dataset is constructed from instructions that can be either
    annotated by humans or generated by other LLMs (such as GPT-4). Thus, the idea
    is to train the model to solve a task with a desired output. The model is evaluated
    with the desired output, and we use this output to optimize the model. These instructions
    usually represent NLP tasks and are of various kinds (up to 61 different tasks
    in some datasets), including tasks such as QA, summarization, classification,
    translation, creating writing, and so on). These instructions can then also contain
    additional content (for example, in summarization, we also provide the text to
    be summarized). To build such a dataset, a greater variety of tasks has greater
    benefit (especially tasks where the model must conduct reasoning and better if
    steps to follow are present in the context). Instruction tuning has several advantages.
    It makes the model capable of adapting even to unseen tasks (ensuring versatility)
    and is computationally efficient. It can also be used to fit the model to specific
    tasks for a particular domain (medical, finance, and so on). Also, it can be used
    in conjunction with other alignment techniques such as RLHF.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这种不匹配，IT行业提出了提高模型能力和可控性的方法。在由指令-输出对（模型的指令和期望输出）构成的集合上进一步训练预训练模型。这个数据集由可以由人类标注或由其他LLMs（如GPT-4）生成的指令组成。因此，想法是训练模型以解决具有期望输出的任务。模型使用期望输出进行评估，我们使用这个输出来优化模型。这些指令通常代表NLP任务，种类繁多（在某些数据集中多达61种不同的任务），包括问答、摘要、分类、翻译、创作写作等任务。这些指令还可以包含额外的内容（例如，在摘要中，我们还提供了要总结的文本）。为了构建这样的数据集，具有更多样化的任务更有益（特别是模型必须进行推理的任务，如果步骤在上下文中存在则更好）。指令调整有几个优点。它使模型能够适应甚至未见过的新任务（确保通用性）并且计算效率高。它还可以用于将模型适应特定领域的特定任务（如医疗、金融等）。此外，它还可以与其他对齐技术（如RLHF）结合使用。
- en: Despite these tuning techniques, it has enabled great advancement in the field
    of LLMs. The limitation of these techniques is that annotators can often be biased,
    and it is expensive to obtain quality datasets. In addition, it is always expensive
    to train a model that has billions of parameters. In addition, according to some
    authors, using AI-written instructions (or tests generated by AI) works as a kind
    of distillation but is less advantageous than using texts written by humans. In
    the next section, we will discuss how to obtain small LLMs when we do not want
    to deal with large LLMs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些调整技术，但它已经使LLMs领域取得了巨大的进步。这些技术的局限性在于标注者往往存在偏见，而且获取高质量数据集的成本很高。此外，训练具有数十亿参数的模型始终是昂贵的。此外，根据一些作者的观点，使用AI编写的指令（或由AI生成的测试）作为一种蒸馏方法，但不如使用人类编写的文本有优势。在下一节中，我们将讨论在不处理大型LLMs的情况下，如何获得小型LLMs。
- en: Exploring smaller and more efficient LLMs
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索更小、更高效的LLMs
- en: LLMs show incredible capabilities but are also associated with large costs beyond
    training costs. Expensive infrastructure is also required for deployment, not
    to mention the costs associated with simple inference that grows with the number
    of parameters. These large LLMs are generalist models, and for many tasks, it
    is not necessary to have a model that has 100 billion parameters. Especially for
    many business cases, we need a model that can accomplish a specific task well.
    So, there are many cases where a **small language model** (**SLM**) is sufficient.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs展现出令人难以置信的能力，但也伴随着超出训练成本的大规模成本。部署时还需要昂贵的硬件设施，更不用说随着参数数量的增加而增长的简单推理成本。这些大型LLMs是通用模型，对于许多任务来说，并不需要拥有拥有1000亿参数的模型。特别是对于许多商业案例，我们需要一个能够很好地完成特定任务的模型。因此，在许多情况下，一个**小型语言模型**（**SLM**）就足够了。
- en: SLMs tend to excel in specialized domains, and may therefore lose the contextual
    informativeness that comes from integrating various domains of knowledge. SLMs
    may lose some of the capabilities of LLMs or otherwise exhibit fewer reasoning
    skills (thus being less versatile). On the other hand, they consume far fewer
    resources and can be used on a commercial GPU or even CPU (or, in extreme cases,
    cell phones).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SLMs往往在特定领域表现出色，因此可能会失去来自整合各种知识领域的上下文信息。SLMs可能会失去LLM的一些功能，或者表现出较少的推理能力（因此不如灵活）。另一方面，它们消耗的资源远少得多，可以在商业GPU或甚至CPU上使用（或者在极端情况下，在手机上使用）。
- en: More extensive studies of small models show that shallow models (with few transformer
    blocks) excel in grammar but have problems with consistency. So, a few layers
    are sufficient for syntactic correctness, but more layers are required for content
    coherence and creativity. Models that have hidden sizes might struggle with the
    continuation of a story, as this capability requires an increase in hidden size
    to at least a size of 128\. Higher embedding dimensions impact the ability to
    generate continuations that are more accurate, relevant, and sound more natural
    (small embeddings lead the model to generate nonsense, contradictions, and irrelevant
    outputs). Also, models with a single layer are not capable of following instructions
    (such as continuing a story according to an input); at least two layers are needed,
    and the capacity increases almost proportionally as the layers increase (a single
    layer of attention does not produce a sufficient global representation).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对小型模型更广泛的研究表明，浅层模型（具有较少的Transformer块）在语法方面表现优秀，但在一致性方面存在问题。因此，几层就足以保证句法正确性，但为了内容连贯性和创造性，需要更多的层。具有隐藏大小的模型可能在故事的延续上遇到困难，因为这种能力需要将隐藏大小至少增加到128。更高的嵌入维度会影响生成更准确、相关且听起来更自然的延续的能力（小型嵌入会导致模型生成无意义、矛盾和不相关的输出）。此外，单层模型无法遵循指令（例如，根据输入继续故事）；至少需要两层，并且随着层数的增加，容量几乎成比例增加（单层注意力不足以产生足够的全局表示）。
- en: 'So, there is a trade-off between capacity and model size. In general, we can
    say that there are three main possibilities for obtaining small and efficient
    LLMs:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在容量和模型大小之间存在权衡。一般来说，我们可以这样说，获得小型高效LLM有三种主要可能性：
- en: '**Training a small LLM from scratch**: For example, Mistral 7B or LLaMA 7B
    have been trained from scratch'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从头开始训练小型LLM**：例如，Mistral 7B或LLaMA 7B都是从零开始训练的。'
- en: '**Knowledge distillation**: One leverages a larger model to train a smaller
    model for a specific task (this can also be done using an LLM and a small LLM
    that is pre-trained; for example, using GPT-4 and BERT)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识蒸馏**：一种方法是通过一个更大的模型来训练一个针对特定任务的较小模型（这也可以使用LLM和一个小型预训练的LLM来完成；例如，使用GPT-4和Bert）'
- en: '**Reducing the size of a model**: For example, we can reduce the size of an
    LLM such as Mistral 7B using techniques such as quantization or pruning'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减小模型大小**：例如，我们可以使用量化或剪枝等技术来减小Mistral 7B等LLM的大小。'
- en: We have already discussed in the previous chapter knowledge distillation, and
    since LLMs are transformers, the process is the same. `float64`, `float16`, `int64`,
    `int8`, and so on). Float formats are used to save reals, while int formats can
    express only integers. Greater precision means that a weight can express a greater
    range. This for an LLM translates into more stable and more accurate training,
    though with the need for more hardware, memory, and cost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一章讨论了知识蒸馏，由于LLM是Transformer，因此过程是相同的。`float64`、`float16`、`int64`、`int8`等。浮点格式用于保存实数，而整型格式只能表示整数。更高的精度意味着一个权重可以表示更大的范围。对于LLM来说，这意味着更稳定和更准确的训练，尽管需要更多的硬件、内存和成本。
- en: '![Figure 3.16 – Example of the quantization process](img/B21257_03_16.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 量化过程的示例](img/B21257_03_16.jpg)'
- en: Figure 3.16 – Example of the quantization process
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 量化过程的示例
- en: 'The problem is that the loss of accuracy for weights can translate into a substantial
    drop in model performance. Different quantization techniques attempt to reduce
    the accuracy of a model by avoiding damage to the original performance. One of
    the most popular techniques is **affine quantization mapping**, which allows one
    to go from a higher-precision number to a lower-precision number using two factors.
    Considering *x* with range [α,β], we can get its quantized version xq ϵ [αq,βq]:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于权重的精度损失可以转化为模型性能的显著下降。不同的量化技术试图通过避免损害原始性能来降低模型的精度。最受欢迎的技术之一是**仿射量化映射**，它允许使用两个因子从高精度数字转换为低精度数字。考虑范围在[α,β]的*x*，我们可以得到其量化版本xq
    ∈ [αq,βq]：
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math>
- en: <mrow><mrow><mrow><mi>s</mi><mo>=</mo><mfrac><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow><mrow><msub><mi>β</mi><mi>q</mi></msub><mo>−</mo><msub><mi>α</mi><mi>q</mi></msub></mrow></mfrac><mi>z</mi><mo>=</mo><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>(</mo><mfrac><mrow><mi>β</mi><msub><mi>α</mi><mi>q</mi></msub><mo>−</mo><mi>α</mi><msub><mi>β</mi><mi>q</mi></msub></mrow><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow></mfrac><mo>)</mo></mrow></mrow></mrow>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><mi>s</mi><mo>=</mo><mfrac><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow><mrow><msub><mi>β</mi><mi>q</mi></msub><mo>−</mo><msub><mi>α</mi><mi>q</mi></msub></mrow></mfrac><mi>z</mi><mo>=</mo><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>(</mo><mfrac><mrow><mi>β</mi><msub><mi>α</mi><mi>q</mi></msub><mo>−</mo><mi>α</mi><msub><mi>β</mi><mi>q</mi></msub></mrow><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow></mfrac><mo>)</mo></mrow></mrow></mrow>
- en: Rounding is used to improve mapping. In practice, we also need to conduct clipping
    because, after mapping, the obtained value might be out of range of the new data
    type.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 四舍五入用于提高映射质量。在实践中，我们还需要进行裁剪，因为映射后，获得的价值可能会超出新数据类型的范围。
- en: 'Not all model parameters are useful, both because there is a lot of linear
    dependence and because these models are practically underfitting. In the context
    of neural networks (and LLMs), the process of removing unnecessary weights is
    called **pruning**. This process refers to eliminating weights, connections, or
    even whole layers. **Unstructured pruning** is a simple technique in which, taking
    a pre-trained model, we eliminate connections or individual neurons, zeroing parameters.
    In the simplest form, this means we set to zero the connections that have a value
    below a certain threshold (the weights that are already near zero do not contain
    much information). Unstructured pruning can create sparse models that have suboptimal
    performance in inference, though. **Structured pruning**, on the other hand, is
    a more sophisticated technique in which we eliminate neurons, groups of neurons,
    structural components, entire layers, or blocks. Structural pruning seeks to preserve
    the performance of the original model by balancing accuracy and compression. Algorithms
    and other optimization systems have been developed for this. The two kinds of
    pruning are demonstrated in the following diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有模型参数都是有用的，这既是因为存在大量的线性依赖，也因为这些模型实际上存在欠拟合。在神经网络（和大型语言模型）的背景下，移除不必要权重的过程被称为**剪枝**。这个过程指的是消除权重、连接，甚至整个层。**无结构剪枝**是一种简单技术，其中，我们从一个预训练模型中消除连接或单个神经元，将参数置零。在最简单的情况下，这意味着我们将低于某个阈值（接近零的权重不包含太多信息）的连接设置为零。无结构剪枝可以创建在推理中性能次优的稀疏模型。另一方面，**结构剪枝**是一种更复杂的技术，其中我们消除神经元、神经元组、结构组件、整个层或块。结构剪枝旨在通过平衡准确性和压缩来保留原始模型的表现。为此已经开发了算法和其他优化系统。以下图表展示了这两种剪枝方法：
- en: '![Figure 3.17 – Schematic representation of pruning; the white elements represent
    pruned elements](img/B21257_03_17.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.17 – 剪枝的示意图；白色元素代表被剪枝的元素](img/B21257_03_17.jpg)'
- en: Figure 3.17 – Schematic representation of pruning; the white elements represent
    pruned elements
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – 剪枝的示意图；白色元素代表被剪枝的元素
- en: For classical neural networks, most algorithms are based on eliminating the
    curvature of the loss versus the weights so that we can identify which weights
    are most important and which are not (a method called **optimal brain surgeon**
    (**OBS**)). Alternatively, several approaches involve training the model, reducing
    connectivity, and retraining the compressed model (this process can take several
    cycles). The problem with these classical approaches is that LLMs are composed
    of billions of parameters, and it would be too expensive to proceed with cycles
    of training and pruning. Some have, therefore, proposed possible fine-tuning of
    the model after pruning, but this for large LLMs is still computationally expensive.
    So, approaches are sought that can be used with LLMs without the need for retraining.
    This is not an easy task because overly aggressive pruning often leads to LLM
    collapse (many algorithms fail to remove more than 10% of the weights without
    avoiding collapse). Recently, approaches such as SparseGPT using pruning masks
    have achieved significant results (up to 60% compression on 170-billion-parameter
    models).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经典神经网络，大多数算法都是基于消除损失与权重之间的曲率，以便我们可以识别哪些权重是最重要的，哪些不是（一种称为**最佳脑外科医生**（**OBS**）的方法）。或者，一些方法涉及训练模型，减少连接性，然后重新训练压缩后的模型（这个过程可能需要几个周期）。这些经典方法的问题在于LLM由数十亿个参数组成，进行训练和修剪的周期会非常昂贵。因此，有人提出了在修剪后对模型进行微调的可能性，但对于大型LLM来说，这仍然在计算上非常昂贵。因此，寻求可以用于LLM而不需要重新训练的方法。这不是一项容易的任务，因为过于激进的修剪往往会导致LLM崩溃（许多算法在避免崩溃的情况下无法移除超过10%的权重）。最近，使用修剪掩码的SparseGPT等方法在压缩上取得了显著成果（在1700亿参数模型上实现了高达60%的压缩）。
- en: 'Since the model output can also be seen as the sum of the outputs of the model
    layers plus the embedding of the input, there will be terms in this sum that do
    not contribute much. The problem is that these terms are not exactly independent,
    so eliminating layers can create mismatches. You can study the contribution of
    each layer by looking at the output, though. Also, in each layer, the transformer
    learns a representation of the data, and in a very deep model, some layers will
    learn a similar representation. There is usually a hierarchy, where the deeper
    layers learn a more specialized representation than the initial layers. Some studies
    have started from these assumptions to eliminate layers, especially deeper layers
    that have layers with more similar representation. The results show that larger
    models have many more redundant layers than smaller models and can be efficiently
    compressed without altering performance too much:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型输出也可以看作是模型层输出的总和加上输入的嵌入，这个总和里会有一些项贡献不大。问题是这些项并不完全独立，因此消除层可能会造成不匹配。不过，你可以通过查看输出来研究每一层的贡献。此外，在每个层中，transformer学习数据的表示，在一个非常深的模型中，一些层会学习到相似的表示。通常存在一个层次结构，其中深层层比初始层学习到更专业的表示。一些研究从这些假设开始，消除层，特别是具有更相似表示的深层层。结果显示，大型模型比小型模型有更多的冗余层，并且可以在不太多改变性能的情况下进行有效压缩：
- en: '![Figure 3.18 – Percentage of the dropped layer before the LLM collapse (https://arxiv.org/pdf/2403.17887v1)](img/B21257_03_18.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – 在LLM崩溃之前删除层的百分比 (https://arxiv.org/pdf/2403.17887v1)](img/B21257_03_18.jpg)'
- en: Figure 3.18 – Percentage of the dropped layer before the LLM collapse ([https://arxiv.org/pdf/2403.17887v1](https://arxiv.org/pdf/2403.17887v1))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – 在LLM崩溃之前删除层的百分比 ([https://arxiv.org/pdf/2403.17887v1](https://arxiv.org/pdf/2403.17887v1))
- en: Pruning allows the memory footprint to be reduced and the inference time to
    be reduced. It is also a technique that allows us to study the importance of various
    structural components. In addition, it can be combined with other techniques such
    as quantization for further compression.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪可以减少内存占用和推理时间。它也是一种允许我们研究各种结构组件重要性的技术。此外，它可以与其他技术如量化结合，以进一步压缩。
- en: Exploring multimodal models
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索多模态模型
- en: 'LLMs, as by definition, are trained with text and to generate text. On the
    other hand, efforts have been made since the advent of the transformer to extend
    the model to other modalities. The addition of multimodal input allows the model
    to improve its reasoning capabilities and also to develop others. Human speech
    conveys a whole range of information that is not present in written words: voice,
    intonation, pauses, and facial expressions enhance communication but can also
    drastically change the meaning of a message.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，LLMs 是用文本训练并用于生成文本的。另一方面，自从 transformer 诞生以来，人们一直在努力将模型扩展到其他模态。添加多模态输入允许模型提高其推理能力，并开发其他能力。人类语音传达了书面文字中不存在的一系列信息：声音、语调、停顿和面部表情增强了沟通，但也可以极大地改变信息的含义。
- en: 'We saw earlier that text can be transformed into a numeric vector. If we can
    transform a data type into a vector, we can then feed it to transformer blocks.
    So, the idea is to find a way to get a latent representation for each data type.
    For images, a way to adapt it to images was presented shortly after the original
    transformer was published: the **Vision Transformer** (**ViT**). ViTs are superior
    in several tasks to convolutional networks.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，文本可以被转换成数值向量。如果我们能将数据类型转换成向量，我们就可以将其输入到 transformer 块中。因此，想法是找到一种方法为每种数据类型获取一个潜在表示。对于图像，在原始
    transformer 发布后不久就提出了一种将其适应图像的方法：**视觉Transformer**（**ViT**）。ViT 在多个任务上优于卷积网络。
- en: 'ViTs are typically built by the encoder alone. Having taken an image, it is
    divided into 16 x 16 patches (each patch can be thought of as being a token of
    a text). This is because a simple pixel does not represent much information, so
    it is more convenient to take a group of pixels (a patch). Once divided into patches,
    these are flattened (as if they were a sequence of patches). One clarification:
    since an image has multiple channels (color or RGB images have three channels),
    these must also be considered. Once this is done, there is usually a linear projection
    step to get tokens of the desired size (after this step, patches are no longer
    visually recognizable).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ViTs 通常仅由编码器构建。在获取图像后，它被分成 16 x 16 的补丁（每个补丁可以被视为文本的一个标记）。这是因为单个像素不包含太多信息，所以更方便的是取一组像素（一个补丁）。一旦分成补丁，这些补丁就会被展平（就像它们是一个补丁序列）。一个澄清：由于图像有多个通道（彩色或
    RGB 图像有三个通道），这些也必须考虑。完成这一步后，通常会有一个线性投影步骤来获取所需大小的标记（在此步骤之后，补丁不再在视觉上可识别）。
- en: 'Given an image of height *H*, width *W*, and channels *C*, we get *N* tokens
    if the patch size is *P*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个高度为 *H*、宽度为 *W* 和通道数为 *C* 的图像，如果补丁大小为 *P*，我们将得到 *N* 个标记：
- en: <mrow><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow></mrow>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow></mrow>
- en: 'The length of the token after linearization is P2 multiplied by the number
    of channels (3 if RGB; 1 if the image is black and white). Now, it is projected
    at a size chosen in advance (in the original version, 768, but it can also be
    different):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 线性化后的标记长度是 P2 乘以通道数（如果是 RGB 图像则为 3；如果是黑白图像则为 1）。现在，它被投影到一个事先选择的大小（在原始版本中为 768，但也可以不同）：
- en: '![Figure 3.19 – Process of transforming images into tokens](img/B21257_03_19.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.19 – 将图像转换为标记的过程](img/B21257_03_19.jpg)'
- en: Figure 3.19 – Process of transforming images into tokens
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – 将图像转换为标记的过程
- en: 'At this point, a special token representing the class is added, and a positional
    encoder is added here as well so that the model is aware of the position of the
    patches in the image. At this point, it enters the encoder, and the process is
    the same as if they were textual tokens. The encoder is constituted of transformer
    blocks, just as we saw before:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，添加了一个表示类别的特殊标记，并且还添加了一个位置编码器，以便模型知道补丁在图像中的位置。此时，它进入编码器，过程与它们是文本标记时相同。编码器由与之前所见相同的
    transformer 块组成：
- en: '![Figure 3.20 – ViT encoding process](img/B21257_03_20.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.20 – ViT 编码过程](img/B21257_03_20.jpg)'
- en: Figure 3.20 – ViT encoding process
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – ViT 编码过程
- en: 'ViTs can be used for many different tasks, such as image classification, object
    detection, and segmentation:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ViTs 可以用于许多不同的任务，例如图像分类、目标检测和分割：
- en: '![Figure 3.21 – Examples of computer vision (CV) tasks done with ViTs](img/B21257_03_21.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.21 – 使用 ViT 完成的计算机视觉（CV）任务示例](img/B21257_03_21.jpg)'
- en: Figure 3.21 – Examples of computer vision (CV) tasks done with ViTs
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21 – 使用 ViT 完成的计算机视觉（CV）任务示例
- en: Since musical sequences are also sequences, they too can be analyzed with transformers.
    There are now models that also process time series, DNA, and musical sequences.
    Considering that we have models for each of these modes, we have begun to think
    about combining them into a single model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于音乐序列也是序列，因此它们也可以用变压器进行分析。现在有模型可以处理时间序列、DNA和音乐序列。考虑到我们为这些模式中的每一个都建立了模型，我们已经开始考虑将它们结合成一个单一模型。
- en: In the first chapter, we saw how embedding can be achieved using word2vec. Even
    a transformer can produce a latent representation that can be considered a vector
    embedding for a text. If we remove the last layer of a transformer, we can get
    a contextualized representation of a text (after all, the various layers of a
    transformer learn an increasingly sophisticated and contextualized representation
    of a text). This representation can be useful for many applications, and we will
    see this in detail later. Right now, we are interested in knowing that an LLM
    can generate a vector representing text. At the same time, a ViT can produce a
    vector representation of an image. Each of these models can then produce a single-mode
    embedding for a data type. A multimodal embedding, though, can capture information
    present in both images and text and relate them.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们看到了如何使用word2vec实现嵌入。即使是一个变压器也能产生一个潜在表示，可以被认为是文本的向量嵌入。如果我们移除一个变压器的最后一层，我们可以得到文本的上下文表示（毕竟，变压器的各个层学习的是越来越复杂和上下文化的文本表示）。这种表示可以用于许多应用，我们将在后面详细看到。现在，我们感兴趣的是了解一个LLM可以生成代表文本的向量。同时，ViT可以生成图像的向量表示。每个这些模型都可以为数据类型生成一个单模态嵌入。然而，多模态嵌入可以捕捉图像和文本中存在的所有信息，并将它们联系起来。
- en: 'Since multimodal embedding would project images and text into the same space,
    we could exploit this embedding for tasks that were not possible before. For example,
    given a caption *x*, we could search for all images that are similar to this caption
    (or, obviously, the reverse). The most famous of these is **Contrastive Language-Image
    Pre-Training** (**CLIP**). CLIP was designed as a model that generates embedding
    for both images and text (today, there are multimodal embeddings for other modalities
    as well):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多模态嵌入会将图像和文本投影到同一个空间，我们可以利用这个嵌入来完成以前不可能的任务。例如，给定一个标题 *x*，我们可以搜索所有与这个标题相似（或者显然，相反）的图像。其中最著名的是**对比语言-图像预训练**（**CLIP**）。CLIP
    被设计为一个为图像和文本生成嵌入的模型（今天，也有其他模态的多模态嵌入）：
- en: '![Figure 3.22 – CLIP jointly trains an image encoder and a text encoder to
    predict the correct pairings of a batch of (image, text) (https://arxiv.org/pdf/2103.00020)](img/B21257_03_22.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图3.22 – CLIP 联合训练图像编码器和文本编码器以预测一批（图像，文本）的正确配对](https://arxiv.org/pdf/2103.00020)(img/B21257_03_22.jpg)'
- en: Figure 3.22 – CLIP jointly trains an image encoder and a text encoder to predict
    the correct pairings of a batch of (image, text) ([https://arxiv.org/pdf/2103.00020](https://arxiv.org/pdf/2103.00020))
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 – CLIP 联合训练图像编码器和文本编码器以预测一批（图像，文本）的正确配对([https://arxiv.org/pdf/2103.00020](https://arxiv.org/pdf/2103.00020))
- en: CLIP was trained with a dataset of 400 million (image, text) pairs collected
    from the internet, trying to cover as many visual concepts as possible. CLIP attempts
    to create a representation for both the image and the corresponding caption, using
    an encoder (a transformer model) for each of the two data types. Once an image
    and a caption are embedded by the corresponding encoders, the two embeddings are
    compared via cosine similarity. The model learns to maximize the cosine similarity
    between an image and its corresponding caption. At the same time, it tries to
    minimize the similarity with other incorrect pairings (very similar to what we
    saw for a text embedding, only this time it is multimodal). After that, we use
    this prediction to conduct the update of the model parameters (all two encoders).
    This learning method is called **contrastive learning**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 使用从互联网收集的4亿个（图像，文本）对的数据集进行训练，试图涵盖尽可能多的视觉概念。CLIP 尝试为图像和相应的标题创建表示，使用每个数据类型的编码器（一个变压器模型）。一旦图像和标题被相应的编码器嵌入，这两个嵌入通过余弦相似度进行比较。模型学习最大化图像与其对应标题之间的余弦相似度。同时，它试图最小化与其他不正确配对的相似度（这与我们看到的文本嵌入非常相似，只是这次是多模态的）。之后，我们使用这个预测来更新模型参数（所有两个编码器）。这种学习方法被称为**对比学习**。
- en: The training is framed as a classification task in which the model predicts
    the correct pair. Starting from these predictions, we compare them with the actual
    predictions and use cross-entropy loss. An interesting finding is that although
    the model is used to create an embedding, the authors used pre-trained models
    and combined them into a new model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练被设定为一个分类任务，其中模型预测正确的配对。从这些预测开始，我们将它们与实际预测进行比较，并使用交叉熵损失。一个有趣的发现是，尽管模型用于创建嵌入，但作者使用了预训练模型并将它们组合成一个新的模型。
- en: '![Figure 3.23 – Similarity matrix between captions and images](img/B21257_03_23.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.23 – 标题与图像之间的相似性矩阵](img/B21257_03_23.jpg)'
- en: Figure 3.23 – Similarity matrix between captions and images
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23 – 标题与图像之间的相似性矩阵
- en: 'We can use CLIP to achieve the embedding of not only images but also captions.
    Once we get these embeddings, we can use them to calculate similarity. We can,
    thus, obtain a similarity matrix. This is easy using the Hugging Face libraries:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 CLIP 实现图像和标题的嵌入。一旦我们得到这些嵌入，我们可以使用它们来计算相似度。因此，我们可以获得一个相似性矩阵。使用 Hugging
    Face 库来做这件事非常简单：
- en: '[PRE0]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We are creating an embedding for both images and captions and then computing
    a similarity matrix.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在为图像和标题创建嵌入，然后计算相似性矩阵。
- en: 'In the original article, one of the first applications for which CLIP was conceived
    was **zero-shot classification**. For example, given a set of labels, we can ask
    the model to classify an image:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始文章中，CLIP 被构思的第一个应用之一是**零样本分类**。例如，给定一组标签，我们可以要求模型对图像进行分类：
- en: '![Figure 3.24 – Zero-shot image classification](img/B21257_03_24.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.24 – 零样本图像分类](img/B21257_03_24.jpg)'
- en: Figure 3.24 – Zero-shot image classification
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24 – 零样本图像分类
- en: 'CLIP can also be used for various other tasks, such as large dataset searches
    or conducting image clustering and then assigning keywords to these clusters.
    CLIP, though, cannot be used to generate text like generating a caption for an
    image. For this, we need a **vision-language model** (**VLM**). A VLM essentially
    behaves like an LLM, though it can also answer questions about an image, solving
    a limitation of LLMs. In other words, with a VLM, we can conduct reasoning in
    a similar way to a classical LLM but also with images. An example is **Bootstrapping
    Language-Image Pre-training** (**BLIP-2**), in which instead of creating a model
    from scratch, they took an LLM and ViT and connected them with a bridge (**Q-Former**).
    The Q-Former is an additional component to connect the image encoder with the
    LLM (basically providing eyes to our LLM):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 也可以用于各种其他任务，例如在大数据集上进行搜索或执行图像聚类，然后为这些聚类分配关键词。然而，CLIP 不能用于生成文本，例如为图像生成标题。为此，我们需要一个**视觉-语言模型**（**VLM**）。VLM
    实质上表现得像 LLM，尽管它也可以回答有关图像的问题，解决了 LLM 的一个限制。换句话说，有了 VLM，我们可以以类似于经典 LLM 的方式进行推理，同时也可以使用图像。一个例子是**自举语言-图像预训练**（**BLIP-2**），其中他们没有从头开始创建模型，而是取了一个
    LLM 和 ViT，并通过一个桥梁（**Q-Former**）将它们连接起来。Q-Former 是一个额外的组件，用于将图像编码器与 LLM 连接起来（基本上为我们的
    LLM 提供了眼睛）：
- en: '![Figure 3.25 – Overview of BLIP-2’s framework (https://arxiv.org/pdf/2301.12597)](img/B21257_03_25.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.25 – BLIP-2 框架概述 (https://arxiv.org/pdf/2301.12597)](img/B21257_03_25.jpg)'
- en: Figure 3.25 – Overview of BLIP-2’s framework ([https://arxiv.org/pdf/2301.12597](https://arxiv.org/pdf/2301.12597))
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – BLIP-2 框架概述 ([https://arxiv.org/pdf/2301.12597](https://arxiv.org/pdf/2301.12597))
- en: 'The Q-Former consists of two components (one interacting with ViT and one interacting
    with the LLM); it is the only part of the model that is trained. This process
    occurs in two stages, one for each mode. In the first stage, we use an image-captions
    pair to train the Q-Former to relate images and text. In the second step, the
    embeddings learned by the Q-Former are used as soft prompts to condition the LLM
    on textual representations of the images (make the LLM aware of the images). Once
    the Q-Former has been trained, we can use the model to generate text about the
    images:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Former 由两个组件组成（一个与 ViT 交互，一个与 LLM 交互）；它是模型中唯一被训练的部分。这个过程分为两个阶段，每个模式一个阶段。在第一阶段，我们使用图像-标题对来训练
    Q-Former，使其能够关联图像和文本。在第二阶段，Q-Former 学习到的嵌入被用作软提示，以条件化 LLM 对图像的文本表示（使 LLM 意识到图像）。一旦
    Q-Former 被训练，我们就可以使用该模型生成关于图像的文本：
- en: '![Figure 3.26 – BLIP-2 captioning of the image](img/B21257_03_26.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.26 – BLIP-2 对图像的标题](img/B21257_03_26.jpg)'
- en: Figure 3.26 – BLIP-2 captioning of the image
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – BLIP-2 对图像的标题
- en: 'Since it is a VLM, we can also ask several questions and chat with the model
    about the image:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个VLM，我们也可以提出几个问题，并与模型就图像进行聊天：
- en: '![Figure 3.27 – Different rounds of questions to BLIP-2 about an image](img/B21257_03_27.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图3.27 – 关于图像的BLIP-2不同轮次的提问](img/B21257_03_27.jpg)'
- en: Figure 3.27 – Different rounds of questions to BLIP-2 about an image
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.27 – 关于图像的BLIP-2不同轮次的提问
- en: 'Speaking of multimodal models, another type of model that has had a strong
    expansion in recent times is **text-to-image models**. Stable Diffusion is considered
    a milestone for its quality of image generation, its performance, and its availability
    to the masses. The operation of this model can be summarized at a high level:
    given textual directions (a prompt), the system generates an image according to
    the instructions. Other alternatives also exist today (text-to-video, image modification
    guided by text, and so on), but the principle is similar. At a high level, we
    can say that there are three main ones:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 说到多模态模型，最近一段时间内，另一种模型类型经历了强劲的增长，那就是**文本到图像模型**。Stable Diffusion因其图像生成的质量、性能以及向大众的普及而被视为一个里程碑。这个模型的操作可以概括为：给定文本指示（一个提示），系统根据指示生成图像。今天也存在其他替代方案（文本到视频、文本引导的图像修改等），但原理是相似的。从高层次上讲，我们可以说是有三个主要方面：
- en: '**A text encoder**: The text encoder is a model (usually CLIP or another LLM
    specifically trained for this function) that takes a text and returns a vector
    representation of the text.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本编码器**：文本编码器是一个模型（通常是CLIP或为该功能专门训练的另一个LLM），它接受文本并返回文本的向量表示。'
- en: '**An image generator**: This is a U-Net that generates the image representation.
    During this process, the generation is conditioned on the text.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成器**：这是一个U-Net，它生成图像表示。在这个过程中，生成受文本条件限制。'
- en: '**An image decoder**: The image representation is transformed into an actual
    image. Usually, this component is a ViT or an AE decoder.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像解码器**：将图像表示转换为实际图像。通常，这个组件是一个ViT或AE解码器。'
- en: The heart of the system is the U-Net, and in this component, the diffusion process
    takes place. The U-Net does not work directly on the image but on a compact representation
    called latent representation (which is basically a matrix). This latent representation,
    though, contains the information to generate an image, a process that is then
    conducted in the last step by the decoder.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的核心是U-Net，在这个组件中，扩散过程发生。U-Net不是直接在图像上工作，而是在一个紧凑的表示上工作，称为潜在表示（这基本上是一个矩阵）。然而，这个潜在表示包含了生成图像所需的信息，这个过程随后在解码器的最后一步进行。
- en: During the diffusion process, starting from random noise, we begin to build
    a latent representation that acquires information about the image. Diffusion models
    are based on the idea that a model, given a large enough training set, can learn
    information about the contained patterns. During training, having taken an image,
    we generate some random noise and add a certain amount of noise to the image.
    This allows us to expand our image dataset widely (since we can control the amount
    of noise we can add to an image and thus create different versions of an image
    with more or less noise). The model is then trained to identify and predict the
    noise that has been added to the image (via classical backpropagation). The model
    then predicts the noise that needs to be subtracted in order to get the image
    (not exactly the image, but the distribution of it). By conducting this denoising
    process, we can then obtain a backward image (or, at least, its latent representation).
    So, starting from noise, we can get an image, and the model is trained to find
    an image in the noise. At this point, we use a decoder and we get an image. Up
    to this point, though, we cannot control this generation with text.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散过程中，从随机噪声开始，我们开始构建一个潜在表示，它获取有关图像的信息。扩散模型基于这样的想法：一个模型，如果给定足够大的训练集，可以学习有关包含模式的信息。在训练过程中，我们取了一个图像，生成一些随机噪声，并向图像添加一定量的噪声。这使我们能够广泛地扩展我们的图像数据集（因为我们可以控制可以添加到图像中的噪声量，从而创建具有更多或更少噪声的不同版本的图像）。然后，模型被训练来识别和预测添加到图像中的噪声（通过经典的反向传播）。然后，模型预测需要减去的噪声，以获得图像（不是图像本身，而是其分布）。通过进行这个去噪过程，我们可以获得一个反向图像（至少，是其潜在表示）。因此，从噪声开始，我们可以得到一个图像，模型被训练在噪声中找到图像。在这个阶段，我们使用解码器并得到一个图像。然而，到目前为止，我们无法用文本来控制这种生成。
- en: '![Figure 3.28 – Stable diffusion architecture (https://arxiv.org/pdf/2112.10752)](img/B21257_03_28.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.28 – 稳定扩散架构](https://arxiv.org/pdf/2112.10752)(img/B21257_03_28.jpg)'
- en: Figure 3.28 – Stable diffusion architecture ([https://arxiv.org/pdf/2112.10752](https://arxiv.org/pdf/2112.10752))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.28 – 稳定扩散架构([https://arxiv.org/pdf/2112.10752](https://arxiv.org/pdf/2112.10752))
- en: This is where the text encoder comes in. The choice of LLM is important; the
    better the LLM, the better the information this model can bring. As we saw earlier,
    CLIP has been trained on captions and corresponding images and is capable of producing
    textual embeddings. The idea behind CLIP is that the textual embeddings are close
    in embedding space to that of the corresponding images. Having arrived at textual
    information as an embedding, this information will be used to generate an image.
    In fact, in the U-Net, there is cross-attention that connects this textual information
    with the generation process.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是文本编码器发挥作用的地方。LLM的选择很重要；LLM越好，这个模型能带来的信息就越好。正如我们之前看到的，CLIP已经在标题和相应的图像上进行了训练，并且能够产生文本嵌入。CLIP背后的想法是文本嵌入在嵌入空间中接近相应的图像。到达文本信息作为嵌入后，这些信息将被用来生成图像。实际上，在U-Net中，存在交叉注意力，将这种文本信息与生成过程连接起来。
- en: We have seen how these models can also answer questions about images or generate
    images. These models don’t always answer questions optimally, and this can cause
    serious consequences. Or, at the same time, they can generate problematic images.
    We will discuss exactly this in the next section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到这些模型也可以回答关于图像的问题或生成图像。这些模型并不总是以最佳方式回答问题，这可能会造成严重后果。或者，同时，它们可以生成有问题的图像。我们将在下一节中详细讨论这一点。
- en: Understanding hallucinations and ethical and legal issues
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解幻觉以及伦理和法律问题
- en: 'A well-known problem with LLMs is their tendency to hallucinate. **Hallucination**
    is defined as the production of nonsensical or unfaithful content. This is classified
    into factuality hallucination and faithfulness hallucination. **Factual hallucinations**
    are responses produced by the model that contradict real, verifiable facts. **Faithfulness
    hallucination**, on the other hand, is content that is at odds with instructions
    or context provided by the user. The model is trained to generate consistent text
    but has no way to revise its output or check that it is correct:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: LLM（大型语言模型）的一个众所周知的问题是它们倾向于产生幻觉。**幻觉**被定义为产生无意义或不忠实的内容。这被分类为事实性幻觉和忠实性幻觉。**事实性幻觉**是指模型产生的与真实、可验证的事实相矛盾的回答。**忠实性幻觉**，另一方面，是与用户提供的指令或上下文相矛盾的内容。模型被训练生成一致的文本，但没有方法来修改其输出或检查其正确性：
- en: '![Figure 3.29 – Example of LLM hallucination (https://arxiv.org/pdf/2311.05232)](img/B21257_03_29.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.29 – LLM幻觉示例](https://arxiv.org/pdf/2311.05232)(img/B21257_03_29.jpg)'
- en: Figure 3.29 – Example of LLM hallucination ([https://arxiv.org/pdf/2311.05232](https://arxiv.org/pdf/2311.05232))
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.29 – LLM幻觉示例([https://arxiv.org/pdf/2311.05232](https://arxiv.org/pdf/2311.05232))
- en: The model can also generate toxic content and present stereotypes and negative
    attitudes toward specific demographic groups. It is important to prevent models
    from producing harm. Different studies have highlighted different instances of
    potential harm resulting from the use of AI in general and LLMs in particular.
    One example is **representational harm**, caused by a model that can perpetuate
    stereotypes or bias. This was previously seen with sentiment classifiers that
    assigned lower sentiment and negative emotion to particular groups of people.
    In fact, LLMs can produce offensive or derogatory language when representing minorities,
    or they can perpetuate society’s stereotypes about cultural norms, attitudes,
    and prejudices. This can lead to what is called **allocational harm**, when a
    model allocates resources unfairly. For example, if an LLM is used to decide the
    priority of access to medical treatment (or a job or credit), it might allocate
    access unfairly due to the biases it has inherited from its training.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 模型还可以生成有毒内容，并表现出对特定人口群体的刻板印象和负面态度。防止模型产生伤害非常重要。不同的研究突出了使用 AI（特别是 LLM）可能导致的潜在伤害的不同实例。一个例子是**代表性伤害**，由一个可以持续传播刻板印象或偏差的模型引起。这以前在情感分类器中看到过，这些分类器将较低的积极情感和负面情绪分配给特定的人群。实际上，LLM
    在代表少数群体时可能会产生冒犯性或贬低性的语言，或者它们可能会持续传播关于文化规范、态度和偏见的刻板印象。这可能导致所谓的**分配性伤害**，当模型不公平地分配资源时。例如，如果
    LLM 被用来决定医疗治疗（或工作或信贷）的优先级，它可能会因为从其训练中继承的偏差而分配不公平的访问权限。
- en: Indeed, it had already been noted that embedding models can amplify biases,
    and these biases were reflected within the embedding space. The association of
    harmful content with groups and minorities was identified in the embedding space.
    In some cases, some LLMs used pre-trained embedding model weights as the initialization
    of the embedding layer. Some **debiasing approaches** (removal of bias from the
    model) have shown potential, but they are still far from being effective.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，已经有人指出嵌入模型可以放大偏差，这些偏差在嵌入空间中得到了反映。在嵌入空间中识别了有害内容与群体和少数群体的关联。在某些情况下，一些 LLM
    使用预训练的嵌入模型权重作为嵌入层的初始化。一些**去偏方法**（从模型中移除偏差）显示出潜力，但它们仍然远未有效。
- en: These biases stem from the pre-training dataset, so it is important to detoxify
    and remove problematic content before training. When fine-tuning a model, it is
    important to check for incorrect labels derived from annotator bias. It is also
    important to vary the sources. There is indeed an imbalance in the content used
    to train the model between text produced in the US and other countries. The model,
    therefore, inherits the perspective of the dominant demographics in its pre-training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这些偏差源于预训练数据集，因此在训练之前进行解毒和移除问题内容非常重要。在微调模型时，检查来自标注者偏差的错误标签也很重要。同时，改变数据源也很重要。实际上，在训练模型时，美国和其他国家产生的文本内容之间存在不均衡。因此，模型继承了其在预训练阶段主导人群的视角。
- en: '![Figure 3.30 – Risk associated with hallucination and disinformation (https://aclanthology.org/2023.findings-emnlp.97.pdf)](img/B21257_03_30.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.30 – 幻觉和虚假信息相关的风险](https://aclanthology.org/2023.findings-emnlp.97.pdf)(img/B21257_03_30.jpg)'
- en: Figure 3.30 – Risk associated with hallucination and disinformation ([https://aclanthology.org/2023.findings-emnlp.97.pdf](https://aclanthology.org/2023.findings-emnlp.97.pdf))
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.30 – 幻觉和虚假信息相关的风险([https://aclanthology.org/2023.findings-emnlp.97.pdf](https://aclanthology.org/2023.findings-emnlp.97.pdf))
- en: Another potential risk of LLMs is their use to produce **misinformation**. LLMs
    are capable of producing credible, convincing text. Malicious actors could use
    them to automate the production of misinformation, phishing emails, rage bait,
    and other harmful content. This is why an important research topic is how to detect
    text generated by LLMs (or alternatively add watermarks to text generation).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的另一个潜在风险是它们被用于生成**虚假信息**。LLM 能够生成可信、令人信服的文本。恶意行为者可能利用它们自动化生成虚假信息、钓鱼邮件、愤怒诱饵和其他有害内容。这就是为什么如何检测由
    LLM 生成的文本（或者另加文本生成水印）是一个重要的研究课题。
- en: '![Figure 3.31 – Taxonomy of LLM-generated misinformation (https://arxiv.org/pdf/2309.13788)](img/B21257_03_31.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.31 – LLM 生成虚假信息的分类](https://arxiv.org/pdf/2309.13788)(img/B21257_03_31.jpg)'
- en: Figure 3.31 – Taxonomy of LLM-generated misinformation ([https://arxiv.org/pdf/2309.13788](https://arxiv.org/pdf/2309.13788))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.31 – LLM 生成虚假信息的分类([https://arxiv.org/pdf/2309.13788](https://arxiv.org/pdf/2309.13788))
- en: 'Today, there are several datasets and libraries in Python that allow one to
    study model bias. For example, one of the packages is the Hugging Face library,
    Evaluate. We can, for example, use a set of prompts and change the gender of the
    prompt. After that, we can evaluate with Evaluate how the model completes these
    prompts (the model used is GPT-2). Evaluate uses, in this case, another model
    trained for this purpose:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Python 中有几个数据集和库允许人们研究模型偏差。例如，其中一个库是 Hugging Face 的 Evaluate。我们可以使用一组提示并更改提示的性别。之后，我们可以使用
    Evaluate 来评估模型如何完成这些提示（使用的模型是 GPT-2）。在这种情况下，Evaluate 使用了另一个为此目的训练的模型：
- en: '[PRE1]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As we can see in the following heatmap, we have a difference in how the model
    completes the prompts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下热图所示，我们可以看到模型完成提示的方式存在差异：
- en: '![Figure 3.32 – Heatmap of gendered completion and associated toxicity](img/B21257_03_32.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.32 – 性别化完成和关联毒性的热图](img/B21257_03_32.jpg)'
- en: Figure 3.32 – Heatmap of gendered completion and associated toxicity
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.32 – 性别化完成和关联毒性的热图
- en: 'Models may also have a bias regarding occupations. We can use the same library
    again to also evaluate the polarity of the prompts that have been completed by
    the model. In this case, we evaluate the sentiment associated with each of the
    completed prompts for each of the two professions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 模型也可能对职业有偏见。我们可以再次使用相同的库来评估模型完成的提示的极性。在这种情况下，我们评估了与每个完成的提示相关的每个职业的情感：
- en: '[PRE2]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The completed prompts for CEOs are much more positive than those generated
    for truck drivers:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为首席执行官完成的提示比为卡车司机生成的提示更加积极：
- en: '![Figure 3.33 – Bias distribution for two different professions](img/B21257_03_33.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.33 – 两种不同职业的偏差分布](img/B21257_03_33.jpg)'
- en: Figure 3.33 – Bias distribution for two different professions
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.33 – 两种不同职业的偏差分布
- en: Another point of contention is the **copyright issue**. These models are trained
    on copyrighted text and can regenerate part of the text they are trained on. So
    far, the creators of these LLMs have claimed that they are covered by the fair
    use doctrine, which has allowed various companies to train models on text scraped
    from the internet even without permission. Today, though, some lawsuits are pending
    that could change the political and legal landscape. Some companies, therefore,
    are trying to sign licensing contracts with newspaper publishers or social networks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有争议的问题是**版权问题**。这些模型是在受版权保护文本上训练的，并且可以重新生成它们所训练的部分文本。到目前为止，这些大型语言模型（LLMs）的创作者声称他们受合理使用原则的保护，这允许各种公司在未经许可的情况下从互联网上抓取文本来训练模型。然而，今天，一些悬而未决的诉讼可能会改变政治和法律格局。因此，一些公司正在尝试与报纸出版商或社交网络签订许可合同。
- en: Linked to the same problem is a **privacy issue** risk. These models can leak
    information about their training data. It is possible with adversarial attacks
    to extract information from the model. The model can store a huge amount of information
    in its parameters, and if trained with databases that contain personal information,
    this can later be extracted. Therefore, **machine unlearning** methods are being
    studied to make a model forget personal data. Legislation being studied in different
    countries may require a model to forget information of users who request it. We
    will discuss privacy in detail in [*Chapter 6*](B21257_06.xhtml#_idTextAnchor090).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与相同问题相关联的是**隐私问题**风险。这些模型可能会泄露其训练数据的信息。通过对抗攻击，可以从模型中提取信息。模型可以在其参数中存储大量信息，如果使用包含个人信息的数据库进行训练，这些信息以后可能会被提取。因此，正在研究**机器反学习**方法，使模型忘记个人数据。不同国家正在研究的立法可能要求模型忘记请求的用户信息。我们将在[*第
    6 章*](B21257_06.xhtml#_idTextAnchor090)中详细讨论隐私问题。
- en: A final point is that these models are now capable of generating code, and this
    code can be used to produce malware and viruses. In addition, these models will
    be increasingly connected, and some studies show how these LLMs can potentially
    be used to spread computer viruses. In the next section, we will see how to efficiently
    use these models through prompt techniques.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点是，这些模型现在能够生成代码，这些代码可以用来制作恶意软件和病毒。此外，这些模型将越来越互联，一些研究表明这些大型语言模型（LLMs）可能被用来传播计算机病毒。在下一节中，我们将看到如何通过提示技术有效地使用这些模型。
- en: Prompt engineering
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程
- en: '**In-context learning** (**ICL**) is one of the most fascinating properties
    of LLMs. Traditionally, **machine learning** (**ML**) models are trained to solve
    specific tasks drawn on training data. For example, in a classical classification
    task, we have input-output pairs (*X*,*y*), and the model learns to map the relationship
    that is between input X and output y. Any deviation from this task leads the model
    to have less-than-optimal results. If we train a model for text classification
    in different topics, we have to conduct fine-tuning to make it efficient in sentiment
    analysis. In contrast, ICL allows us not to have to have any model update to use
    the model in a new task. ICL is, thus, an emergent property of LLMs that allows
    the model to perform a new task in inference, taking advantage of the acquired
    knowledge to map a new relationship.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文学习**（**ICL**）是大型语言模型（LLMs）最迷人的特性之一。传统上，**机器学习**（**ML**）模型被训练来解决基于训练数据的特定任务。例如，在一个经典的分类任务中，我们有输入输出对（*X*，*y*），模型学习映射输入X和输出y之间的关系。任何偏离这个任务的偏差都会导致模型得到低于最优的结果。如果我们为不同主题的文本分类训练一个模型，我们必须进行微调以使其在情感分析中更有效。相比之下，ICL允许我们不需要对模型进行任何更新就可以在新任务中使用该模型。因此，ICL是LLMs的一个新兴特性，它允许模型在推理时执行新任务，利用获得的知识来映射新的关系。'
- en: 'ICL was first defined in the article *Language Models are Few-Shot Learners*
    ([https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)). The authors
    define LLMs as few-shot learners because, given a set of examples in the prompt
    (textual input for an LLM), the model can map the relationship between input and
    output and have learned a new task. This new skill is “learned” in context because
    the LLM exploits the examples in the prompt (which then provide context):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ICL首次在文章《Language Models are Few-Shot Learners》（[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)）中定义。作者将LLMs定义为少量样本学习者，因为给定提示（LLM的文本输入）中的一组示例，模型可以映射输入和输出之间的关系，并学习了一个新任务。这种新技能是在上下文中“学习”的，因为LLM利用提示中的示例（这随后提供了上下文）：
- en: '![Figure 3.34 – Example of ICL abilities (https://arxiv.org/pdf/2005.14165)](img/B21257_03_34.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图3.34 – ICL能力示例（https://arxiv.org/pdf/2005.14165）](img/B21257_03_34.jpg)'
- en: Figure 3.34 – Example of ICL abilities ([https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165))
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.34 – ICL能力示例（[https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165)）
- en: In this case, the concept of “learning” is improper because the model is not
    really learning (in fact, there is no update of internal parameters), and therefore
    the learned skill is only transient. In other words, the model exploits what it
    has already learned (its latent representation) to perform a new task. The model
    exploits the relationships that have been learned in pre-training, to map the
    latent function that is between input and output present in the prompt.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“学习”的概念是不恰当的，因为模型实际上并没有真正学习（实际上，内部参数没有更新），因此学到的技能只是暂时的。换句话说，模型利用它已经学到的（其潜在表示）来执行新任务。模型利用在预训练中学习到的关系，来映射提示中输入和输出之间的潜在函数。
- en: 'ICL has different advantages:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ICL具有不同的优势：
- en: It mirrors the human cognitive reasoning process, so it makes it easier to describe
    a problem and exploit an LLM.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它反映了人类的认知推理过程，因此它使得描述问题和利用LLM变得更容易。
- en: It doesn’t require parameter upgrades, so it’s fast and can be used with a model
    in inference. It requires only a few examples.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不需要参数升级，因此速度快，可以与推理中的模型一起使用。它只需要很少的示例。
- en: ICL has shown that in this, the model can achieve competitive performance in
    several benchmarks.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ICL已经证明，在这个方面，该模型在多个基准测试中可以达到有竞争力的性能。
- en: 'At present, it is still not entirely clear how this behavior emerges. According
    to some, the root of ICL is precisely multi-head self-attention and how the various
    attention heads manage to create interconnected circuits between layers. The prompt,
    in general, provides several elements (format, inputs, outputs, and input-output
    mapping), and they are important for the model to succeed in achieving the mapping.
    Initial work suggests that the model succeeds in “locating” latent concepts that
    it acquired during training. In other words, the model infers from the examples
    what the task is, but the other elements of the prompt help it succeed in locating
    in its parameters the latent concepts it needs to do this mapping. Specifically,
    some work states that the format in which demonstrations are presented is the
    most important element (for example, in the form of input-label pairs):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这种行为的产生方式仍然并不完全清楚。据一些观点，ICL的根源正是多头自注意力机制以及各种注意力头如何在层之间创建相互连接的电路。一般来说，提示（prompt）提供了几个元素（格式、输入、输出以及输入输出映射），这些元素对于模型成功实现映射至关重要。初步研究表明，模型成功“定位”了它在训练期间获得的潜在概念。换句话说，模型从例子中推断出任务是什么，但提示的其他元素帮助它在参数中定位到完成这种映射所需的潜在概念。具体来说，一些研究指出，演示呈现的格式是最重要的元素（例如，以输入-标签对的形式）：
- en: '![Figure 3.35 – Prompt structure (https://arxiv.org/pdf/2202.12837)](img/B21257_03_35.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图3.35 – 提示结构 (https://arxiv.org/pdf/2202.12837)](img/B21257_03_35.jpg)'
- en: Figure 3.35 – Prompt structure ([https://arxiv.org/pdf/2202.12837](https://arxiv.org/pdf/2202.12837))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.35 – 提示结构 ([https://arxiv.org/pdf/2202.12837](https://arxiv.org/pdf/2202.12837))
- en: The community has become excited about this ability because ICL allows the model
    to “learn” a task in inference simply by manipulating the prompt. ICL has allowed
    specific techniques to evolve to be able to perform increasingly sophisticated
    tasks without the need to fine-tune the model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ICL允许模型通过操作提示在推理中“学习”一个任务，因此这一能力引起了社区的极大兴趣。ICL使得特定技术得以发展，能够执行越来越复杂的任务，而无需微调模型。
- en: 'For clarity, we can define some terminology and elements in what are prompts
    (or formatting guidelines). First, a prompt typically contains a question or instruction:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们可以定义一些术语和元素，这些术语和元素是提示（或格式指南）。首先，提示通常包含一个问题或指令：
- en: '[PRE3]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding example is a prompt that contains only one question. By convention,
    it is referred to as `generate code for function x in Python`). The model that
    successfully responds to this type of prompt is said to have zero-shot capabilities,
    and this ability is enhanced by the instruction tuning of a pre-trained model:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例是一个只包含一个问题的提示，按照惯例，它被称为`generate code for function x in Python`。能够成功响应此类提示的模型被认为具有零样本能力，而这种能力通过预训练模型的指令调整得到增强：
- en: '[PRE4]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is a typical case of **few-shot prompting** where we provide examples in
    the prompt. More demonstrations usually help the LLM (3-shot, 5-shot, or even
    10-shot are common cases). A prompt can also have context to help the model. We
    can also add the desired format for the response. However, these simple prompts
    have limitations, especially for tasks that require reasoning. Especially when
    this requires multiple reasoning steps, providing examples is not enough to guide
    the model in the right direction. Several techniques have been proposed to avoid
    the need for fine-tuning.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个典型的**少样本提示**案例，我们在提示中提供了示例。更多的演示通常有助于LLM（3次、5次甚至10次是常见情况）。提示也可以包含上下文以帮助模型。我们还可以添加期望的响应格式。然而，这些简单的提示存在局限性，特别是对于需要推理的任务。特别是当这需要多个推理步骤时，仅提供示例不足以引导模型走向正确的方向。已经提出了几种技术来避免需要微调。
- en: 'Especially when dealing with an arithmetic problem, seeing examples and associated
    answers is not very helpful in learning the process. A student has more benefit
    in understanding the rationale before approaching the solution of such a problem.
    Similarly, an LLM has more benefit in getting the rationale of an answer than
    more examples with labels alone. **Chain-of-thought prompting** does exactly that;
    a triplet, <input, chain of thought, output>, is provided in the prompt. A chain
    of thought is the different intermediate steps to solve the problem:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在处理算术问题时，看到示例和相关答案对于学习过程并不很有帮助。学生在理解解决问题的逻辑之前，对理解这样的问题更有益。同样，LLM 在获取答案的推理方面比仅有标签的更多示例更有益。**思维链提示**正是如此；在提示中提供了一个三元组
    <input, 思维链, output>。思维链是解决问题的不同中间步骤：
- en: '![Figure 3.36 – Example of chain-of-thought (https://arxiv.org/pdf/2201.11903)](img/B21257_03_36.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.36 – 思维链示例 (https://arxiv.org/pdf/2201.11903)](img/B21257_03_36.jpg)'
- en: Figure 3.36 – Example of chain-of-thought ([https://arxiv.org/pdf/2201.11903](https://arxiv.org/pdf/2201.11903))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.36 – 思维链示例 ([https://arxiv.org/pdf/2201.11903](https://arxiv.org/pdf/2201.11903))
- en: Adding these demonstrations makes it easier for the model to solve the task.
    It has the disadvantage, though, that we must have quality demonstrations for
    several problems, and collecting such annotated datasets is expensive.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 添加这些演示使得模型解决任务变得更加容易。然而，它也有缺点，那就是我们必须为几个问题提供高质量的演示，而收集这样的标注数据集是昂贵的。
- en: 'The advantage of CoT is that it divides a task for the model into a more manageable
    series of steps. This behavior can be incentivized simply by adding “Let’s think
    step by step” to the prompt. This seemingly simple approach is called **zero-shot
    CoT prompting**. The authors of the paper *Large Language Models are Zero-Shot
    Reasoners* ([https://arxiv.org/pdf/2205.11916](https://arxiv.org/pdf/2205.11916))
    suggest that the model has inherent reasoning skills in zero-shot settings, and
    this approach is therefore versatile because it prompts the model to use the skills
    it has learned in training:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 的优势在于它将模型的任务分解成一系列更易于管理的步骤。这种行为可以通过在提示中添加“让我们一步步思考”来简单激励。这种看似简单的方法被称为 **零样本
    CoT 提示**。论文《大型语言模型是零样本推理者》(*Large Language Models are Zero-Shot Reasoners*) ([https://arxiv.org/pdf/2205.11916](https://arxiv.org/pdf/2205.11916))
    的作者们提出，模型在零样本设置下具有固有的推理技能，因此这种方法是通用的，因为它促使模型使用其在训练中学习的技能：
- en: '![Figure 3.37 – Schematic diagram illustrating various approaches to problem-solving
    with LLMs (https://arxiv.org/pdf/2305.10601)](img/B21257_03_37.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.37 – 说明使用 LLM 解决问题的各种方法的示意图 (https://arxiv.org/pdf/2305.10601)](img/B21257_03_37.jpg)'
- en: Figure 3.37 – Schematic diagram illustrating various approaches to problem-solving
    with LLMs ([https://arxiv.org/pdf/2305.10601](https://arxiv.org/pdf/2305.10601))
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.37 – 说明使用 LLM 解决问题的各种方法的示意图 ([https://arxiv.org/pdf/2305.10601](https://arxiv.org/pdf/2305.10601))
- en: Other techniques, such as **self-consistency**, have also been used to improve
    reasoning skills. The idea behind it is ensembling, in which different models
    can come to the right solution by majority vote. In this case, we generate several
    solutions and then choose the majority solution. **Tree of Thoughts** (**ToT**),
    on the other hand, exploits reasoning and self-evaluation capabilities, where
    the model generates different reasoning intermediates and then evaluates them
    by exploiting search algorithms (breadth-first search and depth-first search).
    One usually has to choose the number of candidate paths and steps. These techniques
    allow a higher reasoning capacity of the model but have a higher computational
    cost since the model has to generate several responses.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术，如 **自洽性**，也被用来提高推理技能。其背后的思想是集成，其中不同的模型可以通过多数投票得出正确答案。在这种情况下，我们生成几个解决方案，然后选择多数解决方案。**思维树**（**ToT**）另一方面，利用推理和自我评估能力，其中模型生成不同的推理中间步骤，然后通过利用搜索算法（广度优先搜索和深度优先搜索）来评估它们。通常需要选择候选路径和步骤的数量。这些技术允许模型具有更高的推理能力，但由于模型需要生成多个响应，因此计算成本也更高。
- en: '![Figure 3.38 – Examples of using the DSPy system (https://arxiv.org/abs/2310.03714)](img/B21257_03_38.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.38 – 使用 DSPy 系统的示例 (https://arxiv.org/abs/2310.03714)](img/B21257_03_38.jpg)'
- en: Figure 3.38 – Examples of using the DSPy system ([https://arxiv.org/abs/2310.03714](https://arxiv.org/abs/2310.03714))
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.38 – 使用 DSPy 系统的示例 ([https://arxiv.org/abs/2310.03714](https://arxiv.org/abs/2310.03714))
- en: '**Declarative Self-improving Language Programs in Python** (**DSPy**) is an
    interesting new paradigm that has been evolving in recent times. Until now, it
    has been assumed that we have to manually create these prompts, and this requires
    a lot of trial and error. Instead, DSPy seeks to standardize this prompting process
    and turn it into a kind of programming. In short, the authors of DSPy ([https://arxiv.org/abs/2310.03714](https://arxiv.org/abs/2310.03714))
    suggest that we can abstract prompts and fine-tune them into signatures while
    prompting techniques are used as modules. The result is that prompt engineering
    can be automated with optimizers. Given a dataset, we create a pipeline of DSPy
    containing signatures and modules (how these techniques are connected), define
    which metrics to optimize, and then optimize (we define what output we search
    for and the optimizer). The process is, then, iterative; DSPy leads to optimizing
    prompts that we can then use.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python中的声明式自我改进语言程序**（**DSPy**）是近年来发展起来的一种有趣的新范式。到目前为止，人们一直认为我们必须手动创建这些提示，这需要大量的尝试和错误。相反，DSPy寻求标准化这个提示过程，并将其转变为一种编程方式。简而言之，DSPy的作者们（[https://arxiv.org/abs/2310.03714](https://arxiv.org/abs/2310.03714)）建议我们可以抽象提示并将其微调为签名，同时将提示技术作为模块使用。结果是，提示工程可以通过优化器实现自动化。给定一个数据集，我们创建一个包含签名和模块（这些技术如何连接）的DSPy管道，定义要优化的指标，然后进行优化（我们定义我们寻找的输出和优化器）。这个过程是迭代的；DSPy引导我们优化提示，然后我们可以使用这些提示。'
- en: The techniques we have seen in this section are the most commonly used. There
    are many others, but they are generally variations of those described here. We
    now have all the elements to be able to successfully use an LLM.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们看到的技术是最常用的。还有很多其他技术，但它们通常是这里描述的那些技术的变体。我们现在拥有了成功使用LLM的所有要素。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the transition from transformers to LLMs. The
    transformer was an elegant evolution and synthesis of 20 years of research in
    NLP, combining the best of research up to that point. In itself, the transformer
    contained a whole series of elements that enabled its success and versatility.
    The beating heart of the model is self-attention, a key tool – but also the main
    limitation of the LLM. On the one hand, it allows for learning sophisticated representations
    of text that make LLMs capable of countless tasks; on the other hand, it has a
    huge computational cost (especially when scaling the model). LLMs are not only
    capable of solving tasks such as classification but also tasks that assume some
    reasoning, all simply by using text instructions. In addition, we have seen how
    to fit the transformer even with multimodal data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了从Transformer到LLM的过渡。Transformer是20年来NLP研究的优雅演变和综合，结合了当时研究中的最佳成果。本身，Transformer包含了一系列使它成功和通用的元素。模型的核心是自注意力，这是一个关键工具——但也是LLM的主要限制。一方面，它允许学习复杂的文本表示，使LLM能够执行无数任务；另一方面，它具有巨大的计算成本（尤其是在扩展模型时）。LLM不仅能够解决分类等任务，还能通过使用文本指令执行需要某些推理的任务。此外，我们还看到了如何将Transformer与多模态数据相结合。
- en: So far, the model produces only text, although it can produce code as well.
    At this point, why not allow the model to be able to execute the code? Why not
    allow it to use tools that can extend its capabilities? This is what we will see
    in the next chapters.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，该模型只能生成文本，尽管它也能生成代码。在这个阶段，为什么不允许模型执行代码呢？为什么不允许它使用可以扩展其能力的工具呢？这就是我们将在下一章中看到的内容。
- en: Further reading
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Everton et al*.*, *Catastrophic Forgetting in Deep Learning: A Comprehensive
    Taxonomy*, 2023, [https://arxiv.org/abs/2312.10549](https://arxiv.org/abs/2312.10549)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everton等人*.*，《深度学习中的灾难性遗忘：一个综合分类》，2023年，[https://arxiv.org/abs/2312.10549](https://arxiv.org/abs/2312.10549)
- en: 'Raieli, *Emergent Abilities in AI: Are We Chasing a Myth?*, 2023, [https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9](https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli，《人工智能中的涌现能力：我们是在追逐一个神话吗？》，2023年，[https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9](https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9)
- en: Rasyl et al., *Preference Tuning LLMs with Direct Preference Optimization Methods*,
    2024, [https://huggingface.co/blog/pref-tuning](https://huggingface.co/blog/pref-tuning)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasyl等人，《使用直接偏好优化方法调整LLMs的偏好》，2024年，[https://huggingface.co/blog/pref-tuning](https://huggingface.co/blog/pref-tuning)
- en: Alemi, *KL is All You Need*, 2024, [https://blog.alexalemi.com/kl-is-all-you-need.html](https://blog.alexalemi.com/kl-is-all-you-need.html)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alemi, *KL is All You Need*，2024，[https://blog.alexalemi.com/kl-is-all-you-need.html](https://blog.alexalemi.com/kl-is-all-you-need.html)
- en: OpenAI, *Proximal Policy* *Optimization*, [https://spinningup.openai.com/en/latest/algorithms/ppo.html](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI, *近端策略* *优化*，[https://spinningup.openai.com/en/latest/algorithms/ppo.html](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- en: Simonini, *Proximal Policy Optimization (PPO)*, 2022, [https://huggingface.co/blog/deep-rl-ppo](https://huggingface.co/blog/deep-rl-ppo)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonini, *近端策略优化 (PPO)*，2022，[https://huggingface.co/blog/deep-rl-ppo](https://huggingface.co/blog/deep-rl-ppo)
- en: Hoffmann et al., *Training Compute-Optimal Large Language Models*, 2022, [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann等人，*训练计算最优的大型语言模型*，2022，[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
- en: Brown et al., *Language Models are Few-Shot Learners*, 2020, [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人，*语言模型是少样本学习者*，2020，[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
- en: 'Part 2: AI Agents and Retrieval of Knowledge'
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：AI代理和知识检索
- en: This part focuses on extending the capabilities of LLMs by enabling them to
    access, retrieve, and reason over external sources of knowledge. It begins with
    the creation of AI agents that can interact with the web, retrieve live information,
    and execute tasks beyond simple question answering. The following chapters explore
    retrieval-augmented generation (RAG), starting from basic pipelines and advancing
    toward more modular and scalable systems that reduce hallucinations and improve
    factual accuracy. The use of structured knowledge through knowledge graphs (GraphRAG)
    is then introduced as a powerful method to represent and reason over information.
    Finally, this part discusses how reinforcement learning can be used to align agent
    behavior and improve decision-making through interaction with dynamic environments.
    These chapters collectively show how to build agents that are not only language-capable
    but also context-aware, goal-driven, and grounded in external information.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分重点关注通过使LLMs能够访问、检索和推理外部知识源来扩展其功能。它从创建能够与网络交互、检索实时信息并执行超出简单问答任务的AI代理开始。接下来的章节探讨了检索增强生成（RAG），从基本的管道开始，逐步发展到更模块化和可扩展的系统，以减少幻觉并提高事实准确性。然后介绍了通过知识图谱（GraphRAG）使用结构化知识作为表示和推理信息的有力方法。最后，本部分讨论了如何通过与动态环境的交互使用强化学习来对齐代理行为并改善决策。这些章节共同展示了如何构建不仅具有语言能力，而且具有情境感知、目标驱动并基于外部信息的代理。
- en: 'This part has the following chapters:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 4*](B21257_04.xhtml#_idTextAnchor058)*, Building a Web Scraping Agent
    with an LLM*'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B21257_04.xhtml#_idTextAnchor058)*，使用LLM构建网络爬虫代理*'
- en: '[*Chapter 5*](B21257_05.xhtml#_idTextAnchor077)*, Extending Your Agent with
    RAG to Prevent Hallucinations*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B21257_05.xhtml#_idTextAnchor077)*，通过RAG扩展您的代理以防止幻觉*'
- en: '[*Chapter 6*](B21257_06.xhtml#_idTextAnchor090)*, Advanced RAG Techniques for
    Information Retrieval and Augmentation*'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B21257_06.xhtml#_idTextAnchor090)*，高级RAG技术在信息检索和增强中的应用*'
- en: '[*Chapter 7*](B21257_07.xhtml#_idTextAnchor113)*, Creating and Connecting a
    Knowledge Graph to an AI Agent*'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B21257_07.xhtml#_idTextAnchor113)*，创建和连接知识图谱到AI代理*'
- en: '[*Chapter 8*](B21257_08.xhtml#_idTextAnchor137)*, Reinforcement Learning and
    AI Agents*'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B21257_08.xhtml#_idTextAnchor137)*，强化学习和人工智能代理*'
