- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Advanced Deep Learning Architectures for Time Series Forecasting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于时间序列预测的高级深度学习架构
- en: In previous chapters, we’ve learned how to create forecasting models using different
    types of neural networks but, so far, we’ve worked with basic architectures such
    as feedforward neural networks or LSTMs. This chapter describes how to build forecasting
    models with state-of-the-art approaches such as DeepAR or Temporal Fusion Transformers.
    These have been developed by tech giants such as Google and Amazon and are available
    in different Python libraries. These advanced deep learning architectures are
    designed to tackle different types of forecasting problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何使用不同类型的神经网络创建预测模型，但到目前为止，我们只处理了基本的架构，如前馈神经网络或 LSTM。本章将介绍如何使用最先进的方法，如
    DeepAR 或 Temporal Fusion Transformers 来构建预测模型。这些方法由 Google 和 Amazon 等科技巨头开发，并已在不同的
    Python 库中提供。这些先进的深度学习架构旨在解决各种类型的预测问题。
- en: 'We’ll cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下几个食谱：
- en: Interpretable forecasting with N-BEATS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 N-BEATS 进行可解释的预测
- en: Optimizing the learning rate with PyTorch Forecasting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch Forecasting 优化学习率
- en: Getting started with GluonTS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GluonTS 入门
- en: Training a DeepAR model with GluonTS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GluonTS 训练 DeepAR 模型
- en: Training a Transformer with NeuralForecast
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NeuralForecast 训练 Transformer
- en: Training a Temporal Fusion Transformer with GluonTS
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GluonTS 训练 Temporal Fusion Transformer
- en: Training an Informer model with NeuralForecast
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NeuralForecast 训练 Informer 模型
- en: Comparing different Transformers with NeuralForecast
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NeuralForecast 比较不同的 Transformer
- en: By the end of this chapter, you’ll be able to train state-of-the-art deep learning
    forecasting models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够训练最先进的深度学习预测模型。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires the following Python libraries:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要以下 Python 库：
- en: '`numpy` (1.23.5)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`（1.23.5）'
- en: '`pandas` (1.5.3)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`（1.5.3）'
- en: '`scikit-learn` (1.2.1)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`（1.2.1）'
- en: '`sktime` (0.24.0)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sktime`（0.24.0）'
- en: '`torch` (2.0.1)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`（2.0.1）'
- en: '`pytorch-forecasting` (1.0.0)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch-forecasting`（1.0.0）'
- en: '`pytorch-lightning` (2.1.0)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch-lightning`（2.1.0）'
- en: '`gluonts` (0.13.5)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gluonts`（0.13.5）'
- en: '`neuralforecast` (1.6.0)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neuralforecast`（1.6.0）'
- en: 'You can install these libraries in one go using `pip`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `pip` 一次性安装这些库：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub 地址找到：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。
- en: Interpretable forecasting with N-BEATS
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 N-BEATS 进行可解释的预测
- en: This recipe introduces **Neural Basis Expansion Analysis for Interpretable Time
    Series Forecasting** (**N-BEATS**), a deep learning method for forecasting problems.
    We’ll show you how to train N-BEATS using PyTorch Forecasting and interpret its
    output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱介绍了 **用于可解释时间序列预测的神经基扩展分析**（**N-BEATS**），这是一种用于预测问题的深度学习方法。我们将向你展示如何使用 PyTorch
    Forecasting 训练 N-BEATS 并解释其输出。
- en: Getting ready
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'N-BEATS is particularly designed for problems involving several univariate
    time series. So, we’ll use the dataset introduced in the previous chapter (check,
    for example, the *Preparing multiple time series for a global* *model* recipe):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS 特别设计用于处理多个单变量时间序列的问题。因此，我们将使用前一章中介绍的数据集（例如，参见 *为全局模型准备多个时间序列* 食谱）：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our goal is to forecast the next seven values (`HORIZON`) of a time series based
    on the past seven lags (`N_LAGS`).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是根据过去七个滞后值（`N_LAGS`），预测时间序列的下七个值（`HORIZON`）。
- en: How to do it…
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let’s create the training, validation, and testing datasets:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建训练、验证和测试数据集：
- en: 'We start by calling the `setup()` method in the `GlobalDataModule` class:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用 `GlobalDataModule` 类中的 `setup()` 方法开始：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'N-BEATS is available off-the-shelf in PyTorch Forecasting. You can define a
    model as follows:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N-BEATS 已经可以在 PyTorch Forecasting 中直接使用。你可以按如下方式定义模型：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We create an `NBeats` instance using the `from_dataset()` method in the preceding
    code. The following parameters need to be defined:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用前面的代码通过 `from_dataset()` 方法创建了一个 `NBeats` 实例。以下参数需要定义：
- en: '`dataset`: The `TimeSeriesDataSet` instance that contains the training set.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset`：包含训练集的 `TimeSeriesDataSet` 实例。'
- en: '`stack_types`: The mode you want to run N-BEATS on. A `trend` and `seasonality`
    type of stack enables the model to be interpretable, while a `[''generic'']` setup
    is usually more accurate.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stack_types`：你希望运行 N-BEATS 的模式。`trend` 和 `seasonality` 类型的堆栈使得模型具有可解释性，而 `[''generic'']`
    设置通常更为准确。'
- en: '`num_blocks`: A block is the cornerstone of the N-BEATS model. It contains
    a set of fully connected layers that model the time series.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_blocks`：块是 N-BEATS 模型的基石。它包含一组完全连接层，用于建模时间序列。'
- en: '`num_block_layers`: The number of fully connected layers in each block.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_block_layers`：每个块中完全连接层的数量。'
- en: '`widths`: The width of the fully connected layers in each block.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`widths`：每个块中完全连接层的宽度。'
- en: '`sharing`: A Boolean parameter that denotes whether the weights are shared
    blocks per stack. In the interpretable mode, this parameter should be set to `True`.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sharing`：一个布尔参数，表示每个堆栈块是否共享权重。在可解释模式下，该参数应设置为 `True`。'
- en: '`backcast_loss_ratio`: The relevance of the backcast loss in the model. Backcasting
    (predicting the input sample) is an important mechanism in the training of N-BEATS.
    This parameter balances the loss of the backcast with the loss of the forecast.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backcast_loss_ratio`：模型中反向预测损失的相关性。反向预测（预测输入样本）是 N-BEATS 训练中的一个重要机制。这个参数平衡了反向预测损失与预测损失。'
- en: 'After creating the model, you can pass it on to a PyTorch Lightning `Trainer`
    to train it:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型后，您可以将其传递给 PyTorch Lightning 的 `Trainer` 进行训练：
- en: '[PRE4]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We also include an early stopping callback to guide the training process. The
    model is trained using the `fit()` method:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还包括了一个早停回调，用于指导训练过程。模型使用 `fit()` 方法进行训练：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We pass the training data loader to train the model, and the validation data
    loader for early stopping.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将训练数据加载器传递给模型进行训练，并使用验证数据加载器进行早停。
- en: 'After fitting a model, we can evaluate its testing performance and use it to
    make predictions. Before that, we need to load the model from the saved checkpoint:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型后，我们可以评估其测试性能，并用它进行预测。在此之前，我们需要从保存的检查点加载模型：
- en: '[PRE6]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can get the forecasts and respective true values from the test set as follows:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下方式从测试集获取预测值及其真实值：
- en: '[PRE7]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We estimate the forecasting performance as the average absolute difference
    between these two quantities (that is, the mean absolute error):'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过计算这两个数量之间的平均绝对差来评估预测性能（即，平均绝对误差）：
- en: '[PRE8]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Depending on your device, you may need to convert the `predictions` object to
    a PyTorch `tensor` object using `predictions.cpu()` before computing the difference
    specified in the preceding code.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据您的设备，您可能需要使用 `predictions.cpu()` 将 `predictions` 对象转换为 PyTorch 的 `tensor`
    对象，然后再计算前面代码中指定的差异。
- en: 'The workflow for forecasting new instances is also made quite simple by the
    data module:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据模块还简化了为新实例进行预测的工作流：
- en: '[PRE9]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Essentially, the data module gets the latest observations and passes them to
    the model, which makes the forecasts.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本质上，数据模块获取最新的观测值并将其传递给模型，后者会生成预测。
- en: 'One of the most interesting aspects of N-BEATS is its interpretability components.
    These can be valuable for inspecting the forecasts, and the driver behind them:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: N-BEATS 最有趣的方面之一是其可解释性组件。这些组件对于检查预测及其背后的驱动因素非常有价值：
- en: 'We can break down the forecasts into different components and plot them using
    the `plot_interpretation()` method. To do that, we need to get the raw forecasts
    beforehand as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将预测拆解成不同的组件，并使用 `plot_interpretation()` 方法将它们绘制出来。为此，我们需要事先获取原始预测，如下所示：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding code, we call the plot for the first instance of the test
    set (`idx=0`). Here’s what the plot looks like:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们为测试集的第一个实例调用了绘图（`idx=0`）。以下是该绘图的样子：
- en: '![Figure 6.1: Breaking down the N-BEATS forecasts into different parts](img/B21145_06_001.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1：将 N-BEATS 预测拆解成不同部分](img/B21145_06_001.jpg)'
- en: 'Figure 6.1: Breaking down the N-BEATS forecasts into different parts'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：将 N-BEATS 预测拆解成不同部分
- en: The preceding figure shows the `trend` and `seasonality` components of the prediction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了预测中的 `trend` 和 `seasonality` 组件。
- en: How it works…
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'N-BEATS is based on two main components:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS 基于两个主要组件：
- en: A double stack of residual connections that involve forecasting and backcasting.
    Backcasting, in the context of N-BEATS, refers to reconstructing a time series’s
    past values. It helps the model learn better data representations by forcing it
    to understand the time series structure in both directions.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含预测和反向预测的残差连接双堆栈。在 N-BEATS 的上下文中，反向预测是指重建时间序列的过去值。它通过强迫模型在两个方向上理解时间序列结构，帮助模型学习更好的数据表示。
- en: A deep stack of densely connected layers.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个深层的密集连接层堆栈。
- en: This combination leads to a model with both high forecasting accuracy and interpretability
    capabilities.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合使得模型既具有高预测准确性，又具备可解释性能力。
- en: 'The workflow for training, evaluating, and using the model follows the framework
    provided by PyTorch Lightning. The data preparation logic is developed in the
    data module component, specifically within the `setup()` function. The modeling
    stage is created in two parts:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练、评估和使用模型的工作流程遵循 PyTorch Lightning 提供的框架。数据准备逻辑是在数据模块组件中开发的，特别是在 `setup()`
    函数中。建模阶段分为两个部分：
- en: First, you define the N-BEATS model architecture. In this example, we use the
    `from_dataset()` method to create an `NBeats` instance based on the input data
    directly.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要定义 N-BEATS 模型架构。在这个示例中，我们使用 `from_dataset()` 方法根据输入数据直接创建 `NBeats` 实例。
- en: Then, the training process logic is defined in the `Trainer` instance, including
    any callback you might need.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，训练过程逻辑在 `Trainer` 实例中定义，包括你可能需要的任何回调函数。
- en: Some callbacks, such as early stopping, save the best version of the model in
    a local file, which you can load after training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些回调函数，比如早停，会将模型的最佳版本保存在本地文件中，你可以在训练后加载该文件。
- en: It’s important to note that the interpretation step, carried out with the `plot_interpretation`
    part, is a special feature of N-BEATS that helps practitioners understand the
    predictions made by the forecasting model. This can also aid in understanding
    the conditions in which the model is not applicable in practice.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，解释步骤是通过 `plot_interpretation` 部分进行的，这是 N-BEATS 的一个特殊功能，帮助从业人员理解预测模型所做的预测。这也有助于了解模型在实际应用中不适用的条件。
- en: 'N-BEATS is an important model to have in your forecasting arsenal. For example,
    in the M5 forecasting competition, which featured a set of demand time series,
    this model was used in many of the best solutions. You can see more details here:
    [https://www.sciencedirect.com/science/article/pii/S0169207021001874](https://www.sciencedirect.com/science/article/pii/S0169207021001874).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS 是一个在预测工具库中非常重要的模型。例如，在 M5 预测竞赛中，该竞赛包含了一组需求时间序列，N-BEATS 模型被应用于许多最佳解决方案。你可以在这里查看更多细节：[https://www.sciencedirect.com/science/article/pii/S0169207021001874](https://www.sciencedirect.com/science/article/pii/S0169207021001874)。
- en: There’s more…
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'There are a few things you can do to maximize the potential of N-BEATS:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以最大化 N-BEATS 的潜力：
- en: 'You can check the PyTorch Forecasting library documentation to get a better
    sense of how to select the values of each parameter: [https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以查看 PyTorch Forecasting 库的文档，以更好地理解如何选择每个参数的值：[https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html)。
- en: 'Another interesting method is NHiTS, which you can read about at the following
    link: [https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nhits.NHiTS.html#pytorch_forecasting.models.nhits.NHiTS](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nhits.NHiTS.html#pytorch_forecasting.models.nhits.NHiTS).
    Its implementation from the PyTorch Forecasting library follows a similar logic
    to N-BEATS.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个有趣的方法是 NHiTS，你可以在以下链接中阅读更多内容：[https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nhits.NHiTS.html#pytorch_forecasting.models.nhits.NHiTS](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nhits.NHiTS.html#pytorch_forecasting.models.nhits.NHiTS)。它在
    PyTorch Forecasting 库中的实现遵循与 N-BEATS 相似的逻辑。
- en: 'As mentioned before, N-BEATS was developed to handle datasets involving several
    univariate time series. Yet, it was extended to handle exogenous variables by
    the N-BEATSx method, which is available in the `neuralforecast` library: [https://nixtla.github.io/neuralforecast/models.nbeatsx.html](https://nixtla.github.io/neuralforecast/models.nbeatsx.html).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，N-BEATS 是为了处理涉及多个单变量时间序列的数据集而开发的。然而，它通过 N-BEATSx 方法得到了扩展，可以处理外生变量，该方法可以在`neuralforecast`库中找到：[https://nixtla.github.io/neuralforecast/models.nbeatsx.html](https://nixtla.github.io/neuralforecast/models.nbeatsx.html)。
- en: 'Regarding interpretability, there are two other approaches you can take besides
    N-BEATS:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可解释性，除了 N-BEATS 外，你还可以采用另外两种方法：
- en: 'Use model-agnostic explainers such as TimeShap: [https://github.com/feedzai/timeshap](https://github.com/feedzai/timeshap).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用像 TimeShap 这样的模型无关解释器：[https://github.com/feedzai/timeshap](https://github.com/feedzai/timeshap)。
- en: 'Use a **Temporal Fusion Transformer** (**TFT**) deep learning model, which
    also contains special interpretability operations. You can check an example at
    the following link: [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Interpret-model](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Interpret-model).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **时序融合变压器**（**TFT**）深度学习模型，该模型还包含特殊的可解释性操作。您可以通过以下链接查看一个示例：[https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Interpret-model](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Interpret-model)。
- en: Optimizing the learning rate with PyTorch Forecasting
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch Forecasting 优化学习率
- en: In this recipe, we show how to optimize the learning rate of a model based on
    PyTorch Forecasting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们展示了如何基于 PyTorch Forecasting 优化模型的学习率。
- en: Getting ready
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The learning rate is a cornerstone parameter of all deep learning methods.
    As the name implies, it controls how quickly the learning process of the network
    is. In this recipe, we’ll use the same setup as the previous recipe:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是所有深度学习方法的基石参数。顾名思义，它控制着网络学习过程的速度。在本示例中，我们将使用与前一个示例相同的设置：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We’ll also use N-BEATS as an example. However, the process is identical for
    all models based on PyTorch Forecasting.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将以 N-BEATS 为例。然而，所有基于 PyTorch Forecasting 的模型，其过程是相同的。
- en: How to do it…
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'The optimization of the learning rate can be carried out using the `Tuner`
    class from PyTorch Lightning. Here is an example with N-BEATS:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的优化可以通过 PyTorch Lightning 的 `Tuner` 类进行。以下是使用 N-BEATS 的示例：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code, we define a `Tuner` instance as a wrapper of a `Trainer`
    object. We also define an `NBeats` model as in the previous section. Then, we
    use the `lr_optim()` method to optimize the learning rate:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个 `Tuner` 实例，作为 `Trainer` 对象的封装。我们还像前一部分一样定义了一个 `NBeats` 模型。然后，我们使用
    `lr_optim()` 方法优化学习率：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After this process, we can check which learning rate value is recommended and
    also inspect the results across the different tested values:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此过程后，我们可以查看推荐的学习率值，并检查不同测试值的结果：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can visualize the results in the following figure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中可视化结果：
- en: '![Figure 6.2: Learning rate optimization with PyTorch Forecasting](img/B21145_06_002.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2：使用 PyTorch Forecasting 进行学习率优化](img/B21145_06_002.jpg)'
- en: 'Figure 6.2: Learning rate optimization with PyTorch Forecasting'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：使用 PyTorch Forecasting 进行学习率优化
- en: In this example, the suggested learning rate is about **0.05**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，推荐的学习率大约是 **0.05**。
- en: How it works…
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The `lr_find()` method from PyTorch Lightning works by testing different learning
    rate values and selecting one that minimizes the loss of the model. This method
    uses the training and validation data loaders to this effect.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 的 `lr_find()` 方法通过测试不同的学习率值，并选择一个最小化模型损失的值来工作。此方法使用训练和验证数据加载器来实现这一效果。
- en: It’s important to select a sensible value for the learning rate because different
    values can lead to models with different performances. A large learning rate converges
    faster but to a sub-optimal solution. However, a small learning rate can take
    a prohibitively long time to converge.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的学习率非常重要，因为不同的学习率值会导致不同性能的模型。较大的学习率收敛较快，但可能会收敛到一个次优解。然而，较小的学习率可能会需要过长的时间才能收敛。
- en: After the optimization is done, you can create a model using the selected learning
    rate as we did in the previous recipe.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化完成后，您可以像我们在之前的示例中一样，使用选定的学习率创建一个模型。
- en: There’s more…
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'You can learn more about how to get the most out of models such as N-BEATS
    in the *Tutorials* section of PyTorch Forecasting, which is available at the following
    link: [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials.html](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials.html).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 PyTorch Forecasting 的 *教程* 部分了解更多关于如何充分利用像 N-BEATS 这样的模型的内容，详情请访问以下链接：[https://pytorch-forecasting.readthedocs.io/en/stable/tutorials.html](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials.html)。
- en: Getting started with GluonTS
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 GluonTS
- en: GluonTS is a flexible and extensible toolkit for probabilistic time series modeling
    using PyTorch. The toolkit provides state-of-the-art deep learning architectures
    specifically designed for time series tasks and an array of utilities for time
    series data processing, model evaluation, and experimentation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: GluonTS 是一个灵活且可扩展的工具包，用于使用 PyTorch 进行概率时间序列建模。该工具包提供了专门为时间序列任务设计的最先进的深度学习架构，以及一系列用于时间序列数据处理、模型评估和实验的实用工具。
- en: The main objective of this section is to introduce the essential components
    of the `gluonts` library, emphasizing its core functionalities, adaptability,
    and user-friendliness.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的主要目标是介绍`gluonts`库的基本组件，强调其核心功能、适应性和用户友好性。
- en: Getting ready
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To begin our journey, ensure that `gluonts` is installed as well as its backend
    dependency, `pytorch`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的学习之旅，确保安装了`gluonts`及其后端依赖`pytorch`：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With the installations complete, we can now dive into the capabilities of `gluonts`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们可以深入探索`gluonts`的功能。
- en: How to do it…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We start by accessing a sample dataset provided by the library:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先访问由库提供的示例数据集：
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will load the `nn5_daily_without_missing` dataset, one of the datasets
    that `gluonts` offers for experimentation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将加载`nn5_daily_without_missing`数据集，这是`gluonts`提供的用于实验的数据集之一。
- en: 'Its characteristics can be inspected after loading a dataset using the `get_dataset()`
    function. Each `dataset` object contains metadata that offers insights into the
    time series frequency, associated features, and other relevant attributes. You
    can learn a bit more about the dataset by checking the metadata as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，可以检查其特性，使用`get_dataset()`函数进行操作。每个`dataset`对象包含元数据，提供关于时间序列频率、相关特征和其他相关属性的信息。你可以通过查看以下元数据了解数据集的更多信息：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To enhance time series data, `gluonts` provides a list of transformers. For
    instance, the `AddAgeFeature` data transformer adds an `age` feature to the dataset,
    representing the lifespan of each time series:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强时间序列数据，`gluonts`提供了一系列转换器。例如，`AddAgeFeature`数据转换器为数据集添加了一个`age`特征，表示每个时间序列的生命周期：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Data intended for training in `gluonts` is commonly denoted as a collection
    of dictionaries, each representing a time series accompanied by potential features:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 用于`gluonts`训练的数据通常表示为一个字典集合，每个字典代表一个时间序列，并附带可能的特征：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'One of the fundamental models in `gluonts` is the `SimpleFeedForwardEstimator`
    model. Here’s its setup:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`gluonts`中的基本模型之一是`SimpleFeedForwardEstimator`模型。以下是它的设置：'
- en: 'First, the estimator is initialized by determining the prediction length, context
    length (indicating the number of preceding time steps to consider), and the data
    frequency, among other parameters:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过确定预测长度、上下文长度（表示考虑的前几个时间步的数量）和数据频率等参数来初始化估算器：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To train the model, simply invoke the `train()` method on the estimator supplying
    the training data:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，只需在估算器上调用`train()`方法并提供训练数据：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This process trains the model using the provided data, resulting in a predictor
    prepared for forecasting. Here’s how we can get the prediction from the model:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程使用提供的数据训练模型，生成一个准备好进行预测的预测器。以下是我们如何从模型中获取预测：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the preceding code, predictions can be generated with the `make_evaluation_predictions()`
    method, which can then be plotted against the actual values. Here’s the plot with
    the forecasts and actual values:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，可以使用`make_evaluation_predictions()`方法生成预测，然后将其与实际值进行对比绘制。以下是包含预测和实际值的图表：
- en: '![Figure 6.3: Comparative analysis of forecasts with and without AddAgeFeature](img/B21145_06_003.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3：使用和不使用AddAgeFeature的预测对比分析](img/B21145_06_003.jpg)'
- en: 'Figure 6.3: Comparative analysis of forecasts with and without AddAgeFeature'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：使用和不使用AddAgeFeature的预测对比分析
- en: In the preceding figure, we show a comparative analysis of the forecasts with
    and without `AddAgeFeature`. The use of this feature improves forecasting accuracy,
    which indicates that it’s an important variable in this dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们展示了使用和不使用`AddAgeFeature`的预测对比分析。使用该特征可以提高预测准确性，这表明它是此数据集中一个重要的变量。
- en: How it works…
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: GluonTS provides a series of built-in features that are useful for time series
    analysis and forecasting. For example, data transformers allow you to quickly
    build new features based on the raw dataset. As utilized in our experiment, the
    `AddAgeFeature` transformer appends an `age` attribute to each time series. The
    age of a time series can often provide relevant contextual information to the
    model. A good example where we can find it useful is when working with stocks,
    where older stocks might exhibit different volatility patterns than newer ones.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: GluonTS提供了一系列内置功能，有助于时间序列分析和预测。例如，数据转换器使你能够快速基于原始数据集构建新特征。如我们实验中所用，`AddAgeFeature`转换器将一个`age`属性附加到每个时间序列。时间序列的年龄往往能为模型提供相关的上下文信息。一个典型的应用场景是股票数据，较旧的股票可能会展现出与较新的股票不同的波动模式。
- en: Training in GluonTS adopts a dictionary-based structure, where each dictionary
    corresponds to a time series and includes additional associated features. This
    structure makes it easier to append, modify, or remove features.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在GluonTS中训练采用基于字典的结构，每个字典对应一个时间序列，并包含其他相关的特征。这种结构使得附加、修改或删除特征变得更为容易。
- en: We tested a simple model in our experiment by using the `SimpleFeedForwardEstimator`
    model. We defined two instances of the model, one that was trained with the `AddAgeFeature`
    and one without. The model trained with the `age` feature showed better forecasting
    accuracy, as we can see in *Figure 6**.3*. This improvement highlights the importance
    of feature engineering in time series analysis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们测试了一个简单的模型，使用了`SimpleFeedForwardEstimator`模型。我们定义了两个模型实例，一个使用了`AddAgeFeature`，另一个没有使用。使用`age`特征训练的模型显示了更好的预测准确性，如我们在*图6.3*中所看到的那样。这一改进突显了在时间序列分析中，特征工程的重要性。
- en: Training a DeepAR model with GluonTS
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GluonTS训练DeepAR模型
- en: DeepAR is a state-of-the-art forecasting method that utilizes autoregressive
    recurrent networks to predict future values of time series data. Amazon introduced
    it; it was designed for forecasting tasks that can benefit from longer horizons,
    such as demand forecasting. The method is particularly powerful when there’s a
    need to generate forecasts for multiple related time series.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR是一种先进的预测方法，利用自回归循环网络来预测时间序列数据的未来值。该方法由亚马逊提出，旨在解决需要较长预测周期的任务，如需求预测。当需要为多个相关的时间序列生成预测时，这种方法特别强大。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We’ll use the same dataset as in the previous recipe:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前一个示例相同的数据集：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, let’s see how to build a DeepAR model with this data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用这些数据构建DeepAR模型。
- en: How to do it…
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We start by formatting the data for training:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从格式化数据开始进行训练：
- en: 'Let’s do this by using the `ListDataset` data structure:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用`ListDataset`数据结构来实现：
- en: '[PRE24]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, define the DeepAR estimator with the `DeepAREstimator` class, specifying
    parameters such as `prediction_length` (forecasting horizon), `context_length`
    (number of lags), and `freq` (sampling frequency):'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`DeepAREstimator`类定义DeepAR估计器，并指定诸如`prediction_length`（预测周期）、`context_length`（滞后数）和`freq`（采样频率）等参数：
- en: '[PRE25]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After defining the estimator, train the DeepAR model using the `train()` method:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义估计器后，使用`train()`方法训练DeepAR模型：
- en: '[PRE26]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With the trained model, make predictions on your test data and visualize the
    results:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型对测试数据进行预测并可视化结果：
- en: '[PRE27]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here’s the plot of the predictions:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预测结果的图表：
- en: '![Figure 6.4: Comparison of predictions from DeepAR and the true values from
    our dataset](img/B21145_06_004.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4：DeepAR预测与我们数据集中的真实值的比较](img/B21145_06_004.jpg)'
- en: 'Figure 6.4: Comparison of predictions from DeepAR and the true values from
    our dataset'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：DeepAR预测与我们数据集中的真实值的比较
- en: The model is able to match the true values closely.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够紧密匹配真实值。
- en: How it works…
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: DeepAR uses an RNN architecture, often leveraging LSTM units or GRUs to model
    time series data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR使用RNN架构，通常利用LSTM单元或GRU来建模时间序列数据。
- en: The `context_length` parameter is crucial as it determines how many past observations
    the model will consider as its context when making a prediction. For instance,
    if you set `context_length` to `7`, the model will use the last week’s data to
    forecast future values.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`context_length`参数至关重要，因为它决定了模型在做出预测时将考虑多少过去的观测值作为其上下文。例如，如果将`context_length`设置为`7`，则模型将使用过去一周的数据来预测未来的值。'
- en: Conversely, the `prediction_length` parameter defines the horizon, (i.e., how
    many steps the model should predict into the future). In the given code, we’ve
    used a horizon of one week.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，`prediction_length`参数定义了预测的时间范围（即模型应预测的未来步数）。在给定的代码中，我们使用了一周的预测范围。
- en: DeepAR also stands out because of its ability to generate probabilistic forecasts.
    Instead of giving a single-point estimate, it provides a distribution over possible
    future values, allowing us to understand the uncertainty associated with our predictions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR的一个突出特点是能够生成概率性预测。它不仅提供单一的点估计，而是提供一个可能未来值的分布，从而帮助我们理解预测中蕴含的不确定性。
- en: Finally, when working with multiple related time series, DeepAR exploits the
    commonalities between the series to make more accurate predictions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在处理多个相关时间序列时，DeepAR利用序列间的共性来提高预测的准确性。
- en: There’s more…
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'DeepAR shines when the following conditions are met:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下条件时，DeepAR表现尤为出色：
- en: You have multiple related time series; DeepAR can use information from all series
    to improve forecasts.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有多个相关时间序列；DeepAR可以利用所有序列中的信息来改善预测。
- en: Your data has seasonality or recurring patterns.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据具有季节性或周期性模式。
- en: You want to generate probabilistic forecasts, which predict a point estimate
    and provide uncertainty intervals. We will discuss uncertainty estimation in the
    next chapter.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望生成概率性预测，这些预测会给出点估计并提供不确定性区间。我们将在下一章讨论不确定性估计。
- en: You can train a single DeepAR model for a global dataset and generate forecasts
    for all time series in the dataset. On the other hand, for individual time series,
    DeepAR can be trained on each series separately, although this might be less efficient.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以训练一个单一的DeepAR模型来处理全局数据集，并为数据集中的所有时间序列生成预测。另一方面，对于单独的时间序列，DeepAR也可以分别训练每个序列，尽管这可能效率较低。
- en: This model could be particularly useful for demand forecasting in retail, stock
    price prediction, and predicting web traffic, among other applications.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型特别适用于零售需求预测、股票价格预测和网站流量预测等应用。
- en: Training a Transformer model with NeuralForecast
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NeuralForecast训练Transformer模型
- en: Now, we turn our attention to Transformer architectures that have been driving
    recent advances in various fields of artificial intelligence. In this recipe,
    we will show you how to train a vanilla Transformer using the NeuralForecast Python
    library.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将目光转向近年来在人工智能各领域推动进步的Transformer架构。在本节中，我们将展示如何使用NeuralForecast Python库训练一个基础的Transformer模型。
- en: Getting ready
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Transformers have become a dominant architecture in the deep learning community,
    especially for **natural language processing** (**NLP**) tasks. Transformers have
    been adopted for various tasks beyond NLP, including time series forecasting.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer已成为深度学习领域的主流架构，尤其在**自然语言处理**（**NLP**）任务中表现突出。Transformer也已被应用于NLP以外的各种任务，包括时间序列预测。
- en: Unlike traditional models that analyze time series data point by point in sequence,
    Transformers evaluate all time steps simultaneously. This approach is similar
    to observing an entire timeline at once, determining the significance of each
    moment in relation to others for a specific point in time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统模型逐点分析时间序列数据不同，Transformer能够同时评估所有时间步。这种方法类似于一次性观察整个时间线，确定每个时刻与其他时刻的相关性，以便对特定时刻进行评估。
- en: At the core of the Transformer architecture is the **attention mechanism**.
    This mechanism calculates a weighted sum of input values, or values from previous
    layers, according to their relevance to a specific input. Unlike RNNs, which process
    inputs step by step, this allows Transformers to consider all parts of an input
    sequence simultaneously.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的核心是**注意力机制**。该机制根据特定输入的相关性，计算输入值或来自前一层的值的加权和。与逐步处理输入的RNN不同，这使得Transformer能够同时考虑输入序列的所有部分。
- en: 'Key components of the Transformer include the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的关键组成部分包括以下内容：
- en: '**Self-attention mechanism**: Computes the attention scores for all pairs of
    input values and then creates a weighted combination of these values based on
    these scores'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力机制**：计算所有输入值对的注意力得分，然后基于这些得分创建加权组合。'
- en: '**Multi-head attention**: The model can focus on different input parts for
    different tasks or reasons by running multiple attention mechanisms in parallel'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力机制**：该模型可以通过并行运行多个注意力机制，针对不同的任务或原因，集中注意力于输入的不同部分'
- en: '**Position-wise feedforward networks**: These apply linear transformations
    to the output of the attention layer'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐位置前馈网络**：这些网络对注意力层的输出应用线性变换'
- en: '**Positional encoding**: Since the Transformer doesn’t have any inherent sense
    of order, positional encodings are added to the input embeddings to provide the
    model with information about the position of each element in the sequence'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码**：由于 Transformer 本身没有任何固有的顺序感知，因此会将位置编码添加到输入嵌入中，以为模型提供序列中每个元素的位置相关信息'
- en: Let’s see how to train a Transformer model. In this recipe, we’ll resort again
    to the dataset provided in the `gluonts` library. We’ll use the Transformer implementation
    available in the NeuralForecast library. NeuralForecast is a Python library that
    contains the implementation of several neural networks that are focused on forecasting
    problems, including several Transformer architectures.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何训练一个 Transformer 模型。在本教程中，我们将再次使用 `gluonts` 库提供的数据集。我们将使用 NeuralForecast
    库中提供的 Transformer 实现。NeuralForecast 是一个 Python 库，包含了多种专注于预测问题的神经网络实现，包括几种 Transformer
    架构。
- en: How to do it…
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行…
- en: 'First, let’s prepare the dataset for the Transformer model. Unlike sequence-to-sequence
    models such as RNNs, LSTMs, or GRUs, which process input sequences step by step,
    Transformers process entire sequences at once. Therefore, how we format and feed
    data into them can be slightly different:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为 Transformer 模型准备数据集。与逐步处理输入序列的序列到序列模型（如 RNN、LSTM 或 GRU）不同，Transformer
    会一次性处理整个序列。因此，如何格式化和输入数据可能会有所不同：
- en: 'Let’s start by loading the dataset and the necessary libraries:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从加载数据集和必要的库开始：
- en: '[PRE28]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, convert the dataset into a pandas DataFrame and standardize it. Recall
    that standardization is key for any deep learning model fitting:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将数据集转换为 pandas DataFrame 并进行标准化。请记住，标准化是任何深度学习模型拟合的关键：
- en: '[PRE29]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'With the data ready, we’ll train a Transformer model. Unlike the DeepAR model,
    which uses recurrent architectures, the Transformer will rely on its attention
    mechanisms to consider various parts of the time series when making predictions:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备好后，我们将训练一个 Transformer 模型。与使用递归架构的 DeepAR 模型不同，Transformer 将依靠其注意力机制，在做出预测时考虑时间序列的各个部分：
- en: '[PRE30]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, visualize the forecast results:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，展示预测结果：
- en: '[PRE31]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following figure includes the Transformer forecasts and actual values of
    the time series:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Transformer 预测值与时间序列实际值的对比：
- en: '![Figure 6.5: Comparison of Transformer predictions and our dataset’s true
    values](img/B21145_06_005.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5：Transformer 预测与我们数据集的真实值对比](img/B21145_06_005.jpg)'
- en: 'Figure 6.5: Comparison of Transformer predictions and our dataset’s true values'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：Transformer 预测与我们数据集的真实值对比
- en: How it works…
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `neuralforecast` library requires the data in a specific format. Each observation
    consists of three pieces of information: the timestamp, the time series identifier,
    and the corresponding value. We started this recipe by preparing the data in this
    format. The Transformer is implemented in the `VanillaTransformer` class. We set
    a few parameters, such as the forecasting horizon, number of training steps, or
    early stopping related inputs. You can check the complete list of the parameters
    at the following link: [https://nixtla.github.io/neuralforecast/models.vanillatransformer.html](https://nixtla.github.io/neuralforecast/models.vanillatransformer.html).
    The training process is carried out by the `fit()` method in the `NeuralForecast`
    class instance.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralforecast` 库要求数据采用特定格式。每个观察值由三部分信息组成：时间戳、时间序列标识符以及对应的值。我们从准备数据集开始，确保其符合此格式。Transformer
    实现于 `VanillaTransformer` 类中。我们设置了一些参数，例如预测范围、训练步数或与提前停止相关的输入。你可以在以下链接查看完整的参数列表：[https://nixtla.github.io/neuralforecast/models.vanillatransformer.html](https://nixtla.github.io/neuralforecast/models.vanillatransformer.html)。训练过程通过
    `NeuralForecast` 类实例中的 `fit()` 方法进行。'
- en: Transformers process time series data by encoding the entire sequence using
    self-attention mechanisms, capturing dependencies without regard for their distance
    in the input sequence. This global perspective is particularly valuable when patterns
    or dependencies exist over long horizons or when the relevance of past data changes
    dynamically.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 通过使用自注意力机制对整个序列进行编码来处理时间序列数据，从而捕捉依赖关系，而不考虑它们在输入序列中的距离。这种全局视角在存在长时间跨度的模式或依赖关系时特别有价值，或者当过去数据的相关性动态变化时。
- en: Positional encodings are used to ensure that the Transformer recognizes the
    order of data points. Without them, the model would treat the time series as a
    bag of values without any inherent order.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码用于确保 Transformer 识别数据点的顺序。没有它们，模型会将时间序列视为一堆没有内在顺序的值。
- en: The multi-head attention mechanism allows the Transformer to focus on different
    time steps and features concurrently, making it especially powerful for complex
    time series with multiple interacting patterns and seasonality.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力机制使 Transformer 能够同时关注不同的时间步长和特征，特别适用于具有多个交互模式和季节性变化的复杂时间序列。
- en: There’s more…
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Transformers can be highly effective for time series forecasting due to the
    following reasons:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于以下原因，Transformer 在时间序列预测中可能非常有效：
- en: Their ability to capture long-term dependencies in the data
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们捕捉数据中长期依赖关系的能力
- en: Scalability with large datasets
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大规模数据集上的可扩展性
- en: Flexibility in modeling both univariate and multivariate time series
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在建模单变量和多变量时间序列时的灵活性
- en: Like other models, Transformers benefit from hyperparameter tuning, such as
    adjusting the number of attention heads, the size of the model (i.e., the number
    of layers and the dimension of the embeddings), and the learning rate.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他模型一样，Transformer 也能通过调整超参数来受益，例如调整注意力头的数量、模型的大小（即层数和嵌入维度）以及学习率。
- en: Training a Temporal Fusion Transformer with GluonTS
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GluonTS 训练一个时序融合变换器
- en: The TFT is an attention-based architecture developed at Google. It has recurrent
    layers to learn temporal relationships at different scales combined with self-attention
    layers for interpretability. TFTs also use variable selection networks for feature
    selection, gating layers to suppress unnecessary components, and quantile loss
    as their loss function to produce forecasting intervals.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: TFT 是一种基于注意力机制的架构，由 Google 开发。它具有递归层，用于学习不同尺度的时间关系，并结合自注意力层以提高可解释性。TFT 还使用变量选择网络进行特征选择，门控层用于抑制不必要的成分，并采用分位数损失作为其损失函数，用以生成预测区间。
- en: In this section, we delve into training and performing inference with a TFT
    model using the GluonTS framework.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将深入探讨如何使用 GluonTS 框架训练并进行 TFT 模型的推理。
- en: Getting ready
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Ensure you have the GluonTS library and PyTorch backend installed in your environment.
    We’ll use the `nn5_daily_without_missing` dataset from the GluonTS repository
    as a working example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的环境中安装了 GluonTS 库和 PyTorch 后端。我们将使用来自 GluonTS 仓库的`nn5_daily_without_missing`数据集作为工作示例：
- en: '[PRE32]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the following section, we’ll train a TFT model with this dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用这个数据集训练一个 TFT 模型。
- en: How to do it…
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'With the dataset in place, let’s define the TFT estimator:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，接下来定义 TFT 估计器：
- en: 'We’ll begin with specifying hyperparameters such as the prediction length,
    context length, and training frequency:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从指定超参数开始，例如预测长度、上下文长度和训练频率：
- en: '[PRE33]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After defining the estimator, proceed to train the TFT model using the training
    dataset:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义估计器后，继续使用训练数据集训练 TFT 模型：
- en: '[PRE34]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once trained, we can make predictions using the model. Utilize the `make_evaluation_predictions``()`
    function to accomplish this:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以使用模型进行预测。利用`make_evaluation_predictions()`函数来实现这一点：
- en: '[PRE35]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Lastly, we can visualize our forecasts to understand the model’s performance:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过可视化预测结果来了解模型的表现：
- en: '[PRE36]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The following is a comparison of the predictions of the model with the actual
    values of the dataset.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型预测与数据集实际值的比较。
- en: '![Figure 6.6: Comparison of predictions from a TFT and the true values from
    our dataset](img/B21145_06_006.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6：TFT 预测与我们数据集中的真实值比较](img/B21145_06_006.jpg)'
- en: 'Figure 6.6: Comparison of predictions from a TFT and the true values from our
    dataset'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：TFT 预测与我们数据集中的真实值比较
- en: How it works…
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We use the implementation of TFT that is available in `gluonts`. The main parameters
    are the number of lags (context length) and forecasting horizon. You can also
    test different values for some of the parameters of the model, such as the number
    of attention heads (`num_heads`) or the size of the Transformer hidden states
    (`hidden_dim`). The full list of parameters can be found at the following link:
    [https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.estimator.html](https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.estimator.html).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`gluonts`中提供的TFT实现。主要参数包括滞后数（上下文长度）和预测时段。你还可以测试模型某些参数的不同值，例如注意力头的数量（`num_heads`）或Transformer隐状态的大小（`hidden_dim`）。完整的参数列表可以在以下链接中找到：[https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.estimator.html](https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.estimator.html)。
- en: 'TFT fits several use cases due to its complete feature set:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: TFT适用于多种使用场景，因为它具备完整的特征集：
- en: '**Temporal processing**: TFT addresses the challenge of integrating past observations
    and known future inputs through a sequence-to-sequence model, leveraging LSTM
    Encoder-Decoders.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间处理**：TFT通过序列到序列模型解决了整合过去观察值和已知未来输入的挑战，利用LSTM编码器-解码器。'
- en: '**Attention mechanism**: The model employs attention mechanisms, enabling it
    to dynamically assign importance to different time steps. This ensures that the
    model remains focused only on relevant historical data.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力机制**：该模型使用注意力机制，能够动态地为不同的时间步分配重要性。这确保了模型只关注相关的历史数据。'
- en: '**Gating mechanisms**: TFT architectures leverage gated residual networks that
    provide flexibility in the modeling process, adapting to the complexity of the
    data. This adaptability is important for handling diverse datasets, especially
    smaller or noisier ones.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**门控机制**：TFT架构利用门控残差网络，提供在建模过程中的灵活性，能够适应数据的复杂性。这种适应性对于处理不同的数据集尤其重要，特别是对于较小或噪声较多的数据集。'
- en: '**Variable selection networks**: This component is used to determine the relevance
    of each covariate to the forecast. By weighting the input features’ importance,
    it filters out noise and relies only on significant predictors.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量选择网络**：该组件用于确定每个协变量与预测的相关性。通过加权输入特征的重要性，它过滤掉噪声，仅依赖于重要的预测因子。'
- en: '**Static covariate encoders**: TFT encodes static information into multiple
    context vectors, enriching the model’s input.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态协变量编码器**：TFT将静态信息编码为多个上下文向量，丰富了模型的输入。'
- en: '**Quantile prediction**: By forecasting various percentiles at each time step,
    TFT provides a range of possible outcomes.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分位数预测**：通过预测每个时间步的不同分位数，TFT提供了可能结果的范围。'
- en: '**Interpretable outputs**: Even though TFT is a deep learning model, it provides
    insights into feature importance, ensuring transparency in predictions.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释输出**：尽管TFT是一个深度学习模型，但它提供了特征重要性方面的见解，确保预测的透明性。'
- en: There’s more…
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Beyond its architectural innovations, TFT’s interpretability makes it a good
    choice when there is a need to explain how the predictions were produced. Components
    such as variable network selection and the temporal multi-head attention layer
    shed light on the importance of different inputs and temporal dynamics, making
    TFT not just a forecasting tool but also an analytical one.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 除了架构创新之外，TFT的可解释性使其成为当需要解释预测是如何生成时的良好选择。诸如变量网络选择和时序多头注意力层等组件，揭示了不同输入和时间动态的重要性，使TFT不仅仅是一个预测工具，还是一个分析工具。
- en: Training an Informer model with NeuralForecast
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NeuralForecast训练Informer模型
- en: In this recipe, we’ll explore the `neuralforecast` Python library to train an
    Informer model, another Transformer-based deep learning approach for forecasting.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探索`neuralforecast` Python库，用于训练Informer模型，Informer是另一种基于Transformer的深度学习预测方法。
- en: Getting ready
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: Informer is a Transformer method tailored for long-term forecasting – that is,
    predicting with a large forecasting horizon. The main difference relative to a
    vanilla Transformer is that Informer provides an improved self-attention mechanism,
    which significantly reduces the computational requirements to run the model and
    generate long-sequence predictions.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Informer是一种针对长期预测而量身定制的Transformer方法——即，具有较长预测时段的预测。与标准Transformer相比，Informer的主要区别在于其改进的自注意力机制，这大大减少了运行模型和生成长序列预测的计算需求。
- en: 'In this recipe, we’ll show you how to train Informer using `neuralforecast`.
    We’ll use the same dataset as in the previous recipes:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将向你展示如何使用`neuralforecast`训练Informer模型。我们将使用与之前教程相同的数据集：
- en: '[PRE37]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How to do it…
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'This time, instead of creating `DataModule` to handle the data preprocessing,
    we’ll use the typical workflow of the `neuralforecast`-based models:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不是创建`DataModule`来处理数据预处理，而是使用基于`neuralforecast`模型的典型工作流程：
- en: 'We start by preparing the time series dataset in the specific format expected
    by the `neuralforecast` methods:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先准备时间序列数据集，以符合`neuralforecast`方法所期望的特定格式：
- en: '[PRE38]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We transformed the dataset into a pandas DataFrame with three columns: `ds`,
    `unique_id`, and `y`. These represent the timestamp, the ID of the time series,
    and the value of the corresponding time series, respectively. In the preceding
    code, we transformed all the time series into a common value range using a standard
    scaler from `scikit-learn`. We also set the validation set size to 20% of the
    size of the time series. Now, we can set up the Informer model as follows:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集转化为一个包含三列的pandas DataFrame：`ds`、`unique_id`和`y`。它们分别表示时间戳、时间序列的ID和对应时间序列的值。在前面的代码中，我们使用`scikit-learn`的标准缩放器将所有时间序列转换为一个共同的数值范围。我们还将验证集的大小设置为时间序列大小的20%。现在，我们可以如下设置Informer模型：
- en: '[PRE39]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We set Informer with a context length (number of lags) of `7` to forecast the
    next 7 values at each time step. The number of training steps was set to `1000`,
    and we also set up an early stopping mechanism to help the fitting process. These
    are only a subset of the parameters you can use to set up Informer. You can check
    out the following link for the complete list of parameters: [https://nixtla.github.io/neuralforecast/models.informer.html](https://nixtla.github.io/neuralforecast/models.informer.html).
    The model is passed on to a `NeuralForecast` class instance, where we also set
    the frequency of the time series to daily (the `D` keyword). Then, the training
    process is done as follows:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将Informer的上下文长度（滞后数）设置为`7`，以便在每个时间步预测接下来的7个值。训练步数设置为`1000`，我们还设置了早期停止机制以帮助拟合过程。这些仅是设置Informer时可以使用的部分参数。你可以通过以下链接查看完整的参数列表：[https://nixtla.github.io/neuralforecast/models.informer.html](https://nixtla.github.io/neuralforecast/models.informer.html)。模型被传递到`NeuralForecast`类实例中，我们还将时间序列的频率设置为每日（`D`关键字）。然后，训练过程如下进行：
- en: '[PRE40]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `nf` object is used to fit the model and can then be used to make predictions:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nf`对象用于拟合模型，然后可以用来进行预测：'
- en: '[PRE41]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The forecasts are structured as a pandas DataFrame, so you can check a sample
    of the forecasts by using the `head()` method.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果以pandas DataFrame的形式结构化，因此你可以通过使用`head()`方法查看预测的样本。
- en: How it works…
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The `neuralforecast` library provides yet another simple framework to train
    powerful models for time series problems. In this case, we handle the data logic
    outside of the framework because it handles the passing of the data to models
    internally.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralforecast`库提供了一个简单的框架，用于训练强大的时间序列问题模型。在这种情况下，我们将数据逻辑处理放在框架外部，因为它会在内部处理数据传递给模型的过程。'
- en: The `NeuralForecast` class instance takes a list of models as input (in this
    case, with a single `Informer` instance) and takes care of the training process.
    This library can be a good solution if you want to use state-of-the-art models
    off the shelf. The limitation is that it is not as flexible as the base PyTorch
    ecosystem.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeuralForecast`类实例接受一个模型列表作为输入（在本例中只有一个`Informer`实例），并负责训练过程。如果你想直接使用最先进的模型，这个库可以是一个不错的解决方案。其限制是，它的灵活性不如基础的PyTorch生态系统。'
- en: There’s more…
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'In this recipe, we described how to train a particular Transformer model using
    `neuralforecast`. But, this library contains other Transformers that you can try,
    including the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们描述了如何使用`neuralforecast`训练一个特定的Transformer模型。但这个库包含了其他你可以尝试的Transformers，包括以下几种：
- en: Vanilla Transformer
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanilla Transformer
- en: TFT
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFT
- en: Autoformer
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Autoformer
- en: PatchTST
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PatchTST
- en: 'You can check the complete list of models at the following link: [https://nixtla.github.io/neuralforecast/core.html](https://nixtla.github.io/neuralforecast/core.html).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接查看完整的模型列表：[https://nixtla.github.io/neuralforecast/core.html](https://nixtla.github.io/neuralforecast/core.html)。
- en: Comparing different Transformers with NeuralForecast
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NeuralForecast比较不同的Transformer
- en: NeuralForecast contains several deep learning methods that you can use to tackle
    time series problems. In this recipe, we’ll walk you through the process of comparing
    different Transformer-based models using `neuralforecast`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralForecast 包含几种深度学习方法，你可以用来解决时间序列问题。在本节中，我们将引导你通过 `neuralforecast` 比较不同基于
    Transformer 的模型的过程。
- en: Getting ready
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We’ll use the same dataset as in the previous recipe (the `df` object). We
    set the validation and test size to 10% of the data size each:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前一节相同的数据集（`df` 对象）。我们将验证集和测试集的大小分别设置为数据集的 10%：
- en: '[PRE42]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, let’s see how to compare different models using `neuralforecast`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何使用 `neuralforecast` 比较不同的模型。
- en: How to do it…
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We start by defining the models we want to compare. In this case, we’ll compare
    an Informer model with a vanilla Transformer, which we set up as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义要比较的模型。在这个例子中，我们将比较一个 Informer 模型和一个基础版 Transformer，我们将模型设置如下：
- en: '[PRE43]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The training parameters are set equally for each model. We can use the `NeuralForecast`
    class to compare different models using the `cross_validation()` method as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的训练参数设置相同。我们可以使用 `NeuralForecast` 类，通过 `cross_validation()` 方法比较不同的模型，方法如下：
- en: '[PRE44]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `cv` object is the result of the comparison. Here’s a sample of the forecasts
    of each model in a particular time series:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv` 对象是比较的结果。以下是每个模型在特定时间序列中的预测样本：'
- en: '![Figure 6.7: Forecasts of two Transformer models in an example time series](img/B21145_06_007.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7：示例时间序列中两个 Transformer 模型的预测结果](img/B21145_06_007.jpg)'
- en: 'Figure 6.7: Forecasts of two Transformer models in an example time series'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：示例时间序列中两个 Transformer 模型的预测结果
- en: 'The Informer model seems to produce better forecasts, which we can check by
    computing the mean absolute error:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Informer 模型似乎产生了更好的预测结果，我们可以通过计算平均绝对误差来验证这一点：
- en: '[PRE45]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The error of Informer is 0.42, which is better than the 0.53 score obtained
    by `VanillaTransformer`.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Informer 的误差为 0.42，优于 `VanillaTransformer` 得到的 0.53 分数。
- en: How it works…
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Under the hood, the `cross_validation()` method works as follows. Each model
    is trained using the training and validation sets. Then, they are evaluated on
    testing instances. The forecasting performance on the test set provides a reliable
    estimate for the performance we expect the models to have when applied in practice.
    So, you should select the model that maximizes forecasting performance, and retrain
    it using the whole dataset.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在背后，`cross_validation()` 方法的工作原理如下。每个模型使用训练集和验证集进行训练。然后，它们在测试实例上进行评估。测试集上的预测性能为我们提供了一个可靠的估计，表示我们期望模型在实际应用中达到的性能。因此，你应该选择能最大化预测性能的模型，并用整个数据集重新训练它。
- en: The `neuralforecast` library contains other models that you can compare. You
    can also compare different configurations of the same method and see which one
    works best for your data.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralforecast` 库包含其他可以进行比较的模型。你也可以比较同一种方法的不同配置，看看哪种最适合你的数据。'
