- en: Modern Methods for Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类现代方法
- en: We now know how to convert text strings to numerical vectors that capture some
    meaning. In this chapter, we will look at how to use those with embedding. Embedding
    is the more frequently used term for word vectors and numerical representations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何将文本字符串转换为捕获一些意义的数值向量。在本章中，我们将探讨如何使用这些向量与嵌入结合。嵌入是比词向量更常用的术语，也是数值表示。
- en: In this chapter, we are still following the broad outline from our first, that
    is, text→ representations → models→ evaluation → deployment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仍然遵循第一章节的总体框架，即文本→表示→模型→评估→部署。
- en: We will continue working with text classification as our example task. This
    is mainly because it's a simple task for demonstration, but we can also extend
    almost all of the ideas in this book to solve other problems. The main focus ahead,
    however, is machine learning for text classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用文本分类作为我们的示例任务。这主要是因为它是一个简单的演示任务，但我们也可以将本书中的几乎所有想法扩展到解决其他问题。然而，接下来的主要焦点是文本分类的机器学习。
- en: 'To sum up, in this chapter we will be looking at the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本章中，我们将探讨以下主题：
- en: Sentiment analysis as a specific class and example of text classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析作为文本分类的一个特定类别和示例
- en: Simple classifiers and how to optimize them for your datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单分类器和如何优化它们以适应你的数据集
- en: Ensemble methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法
- en: Machine learning for text
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本机器学习
- en: There are at least 10 to 20 machine learning techniques that are well known
    in the community, ranging from SVMs to several regressions and gradient boosting
    machines. We will select a small taste of these.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在社区中至少有10到20种广为人知的机器学习技术，从SVM到多种回归和梯度提升机。我们将从中选择一小部分进行尝试。
- en: '![](img/290de774-4008-41d8-91dd-3e7027b94f3b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/290de774-4008-41d8-91dd-3e7027b94f3b.png)'
- en: 'Source: [https://www.kaggle.com/surveys/2017.](https://www.kaggle.com/surveys/2017)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://www.kaggle.com/surveys/2017.](https://www.kaggle.com/surveys/2017)
- en: The preceding graph shows the most popular machine learning techniques used
    by Kagglers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了Kagglers使用最广泛的机器学习技术。
- en: We met **Logistic Regression** in the first chapter while working the 20 newsgroups
    dataset. We will revisit **Logistic Regression** and introduce **Naive Bayes**,
    **SVM**, **Decision Trees**, **Random Forests**, and **XgBoost**. **XgBoost**
    is a popular algorithm used by several Kaggle winners to achieve award-winning
    results. We will use the scikit-learn and XGBoost packages in Python to see the
    previous example in code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理20个新闻组数据集时，我们遇到了**逻辑回归**。我们将重新审视**逻辑回归**并介绍**朴素贝叶斯**、**支持向量机**、**决策树**、**随机森林**和**XgBoost**。**XgBoost**是一种流行的算法，被多位Kaggle获奖者用于获得获奖结果。我们将使用Python中的scikit-learn和XGBoost包来查看之前的代码示例。
- en: Sentiment analysis as text classification
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析作为文本分类
- en: A popular use of classifiers is in sentiment analysis. The end objective here
    is to determine the subjective value of a text document, which is essentially
    how positive or negative the content of a text document is. This is particularly
    handy for quickly understanding what the tone is of, say, the movie you are producing
    or the book you want to read.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的一个流行用途是情感分析。这里的最终目标是确定文本文档的主观价值，这本质上是指文本文档的内容是积极的还是消极的。这对于快速了解你正在制作的影片或你想要阅读的书籍的语气特别有用。
- en: Simple classifiers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单分类器
- en: Let's begin by simply trying a few machine learning classifiers such as Logistic
    Regression, Naive Bayes, and Decision Trees. We'll then move on and try the Random
    Forest and Extra Trees classifiers. For all of these implementations, we won't
    use anything except scikit-learn.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单地尝试几个机器学习分类器开始，如逻辑回归、朴素贝叶斯和决策树。然后我们将尝试随机森林和额外树分类器。对于所有这些实现，我们不会使用除scikit-learn以外的任何东西。
- en: Optimizing simple classifiers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化简单分类器
- en: We can tweak these simple classifiers to improve their performance. For this,
    the most common method is to try several slightly different versions of the classifier.
    We do this by changing the parameters of our classifier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过尝试几个略微不同的分类器版本来调整这些简单的分类器以改善它们的性能。为此，最常见的方法是尝试改变分类器的参数。
- en: We will learn how to automate this search process for the best classifier parameters
    using `GridSearch` and `RandomizedSearch`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用`GridSearch`和`RandomizedSearch`来自动化这个搜索过程以找到最佳分类器参数。
- en: Ensemble methods
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法
- en: Having an ensemble of several different classifiers means we will be using a
    group of models. Ensembling is a very popular and easy to understand machine learning
    technique, and is part of almost every winning Kaggle competition.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一系列不同的分类器意味着我们将使用一组模型。集成是一种非常流行且易于理解的机器学习技术，几乎是每个获胜的Kaggle竞赛的一部分。
- en: Despite initial concerns that this process might be slow, some teams working
    on commercial software have begun using ensemble methods in production software
    as well. This is because it requires very little overhead, is easy to parallelize,
    and allows for a built-in fallback of using a single model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最初担心这个过程可能很慢，但一些在商业软件上工作的团队已经开始在生产软件中使用集成方法。这是因为它需要很少的开销，易于并行化，并且允许内置的单个模型回退。
- en: We will look at some of the simplest ensembling techniques based on simple majority,
    also known as voting ensemble, and will then build using that.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究一些基于简单多数的简单集成技术，也称为投票集成，然后基于此构建。
- en: In summary, this machine learning for NLP section covers simple classifiers,
    parameter optimization, and ensemble methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本节机器学习NLP涵盖了简单的分类器、参数优化和集成方法。
- en: Getting the data
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'We will programmatically download the data using Python''s standard inbuilt
    toolkit called `urlretrieve` from `urllib.request`. The following is our download-from-internet
    piece:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python的标准内置工具`urlretrieve`从`urllib.request`编程下载数据。以下是从互联网下载的部分：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you are using the fastAI environment, all of these imports work. The second
    block simply sets up Tqdm for us to visualize the download progress. Let''s now
    download the data using `urlretrieve`, as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是fastAI环境，所有这些导入都有效。第二个块只是为我们设置Tqdm，以便可视化下载进度。现在让我们使用`urlretrieve`下载数据，如下所示：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s download some data, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载一些数据，如下所示：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s now extract the preceding files and see what the directory contains:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们提取前面的文件，看看目录包含什么：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Notice that we prefer to use `Path from pathlib` over the `os.path` functionality.
    This make it more platform-agnostic as well as Pythonic. This really badly written
    utility tells us that there are at least two folders: `train` and `test`. Each
    of these folders, in turn, has at least three folders, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们更喜欢使用`Path from pathlib`而不是`os.path`功能。这使得它更加平台无关，也更加Pythonic。这个写得非常糟糕的实用工具告诉我们至少有两个文件夹：`train`和`test`。每个文件夹反过来又至少有三个文件夹，如下所示：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `pos` and `neg` folders contain reviews, which are positive and negative
    respectively. The `unsup` folder stands for unsupervised. These folders are useful
    for building language models, especially for deep learning, but we will not use
    that here. Similarly, the `all` folder is redundant because those reviews are
    repeated in either the `pos` or `neg` folder.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`pos`和`neg`文件夹包含评论，分别代表正面和负面。`unsup`文件夹代表无监督。这些文件夹对于构建语言模型很有用，特别是对于深度学习，但在这里我们不会使用。同样，`all`文件夹是多余的，因为这些评论在`pos`或`neg`文件夹中已经重复。'
- en: Reading data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据
- en: 'Let''s read the following data into a Pandas `DataFrame` with the appropriate
    labels:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将以下数据读入一个带有适当标签的Pandas `DataFrame`中：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This function reads the files for a particular `train` or `test` split, both
    positive and negative, for the IMDb dataset. Each split is a `DataFrame` with
    two columns: `text` and `label`. The `label` column gives us our target value,
    or `y`, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数读取特定`train`或`test`分割的文件，包括正负样本，对于IMDb数据集。每个分割都是一个包含两列的`DataFrame`：`text`和`label`。`label`列给出了我们的目标值，或`y`，如下所示：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now read the data in the corresponding `DataFrame` and then split it
    into the following four variables: `X_train`, `y_train`, `X_test`, and `y_test`.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以读取相应的`DataFrame`中的数据，然后将其拆分为以下四个变量：`X_train`、`y_train`、`X_test`和`y_test`。
- en: Simple classifiers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单分类器
- en: 'In order to try some of our classifiers, let''s get the basic imports out of
    the way, as shown in the following code. Here, we will be importing the rest of
    the classifiers as we need them. This ability to import things later is important
    for ensuring we don''t import too many unnecessary components into memory:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试一些我们的分类器，让我们先导入基本库，如下所示。在这里，我们将根据需要导入其余的分类器。这种在稍后导入事物的能力对于确保我们不会将太多不必要的组件导入内存非常重要：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since this section is simply for illustration purposes, we will use the simplest
    feature extraction steps, which are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本节仅用于说明目的，我们将使用最简单的特征提取步骤，如下所示：
- en: Bag of words
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋模型
- en: TF-IDF
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: We encourage you to try the code examples with better text vectorization (for
    example, using direct GloVe or word2vec lookups).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您尝试使用更好的文本向量化（例如，使用直接GloVe或word2vec查找）的代码示例。
- en: Logistic regression
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Let''s now simply replicate the simple logistic regression we did in [Chapter
    1](5625152b-6870-44b1-a39f-5a79bcc675d9.xhtml), *Getting Started with Text Classification*,
    but on our custom dataset, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简单地复制我们在[第1章](5625152b-6870-44b1-a39f-5a79bcc675d9.xhtml)“开始文本分类”中做的简单逻辑回归，但是在我们的自定义数据集上，如下所示：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see in the preceding snippet, `lr_clf` becomes our classifier pipeline.
    We saw the pipeline in our introductory section. A pipeline allows us to queue
    multiple operations in one single Python object.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面代码片段中看到的，`lr_clf`成为我们的分类器管道。我们在介绍部分看到了管道。管道允许我们在一个单一的Python对象中排队多个操作。
- en: We are able to call functions such as `fit`, `predict`, and `fit_transform`
    on our `Pipeline` objects because a pipeline automatically calls the corresponding
    function of the last component in the list.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够调用`fit`、`predict`和`fit_transform`等函数在我们的`Pipeline`对象上，因为管道会自动调用列表中最后一个组件的相应函数。
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As mentioned earlier, we are calling the `predict` function on our pipeline.
    The test reviews go through under the same pre-processing steps, `CountVectorizer()`
    and `TfidfTransformer()`, as the reviews during training, as shown in the following
    snippet:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们正在调用管道上的`predict`函数。测试评论将经过与训练期间相同的预处理步骤，即`CountVectorizer()`和`TfidfTransformer()`，如下面的代码片段所示：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The ease and simplicity of this process makes `Pipeline` one of the most frequently
    used abstractions in software-grade machine learning. However, users might prefer
    to execute each step independently, or build their own pipeline equivalents in
    some research or experimentation use cases:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的简便性和简单性使得`Pipeline`成为软件级机器学习中最常用的抽象之一。然而，用户可能更喜欢独立执行每个步骤，或者在研究或实验用例中构建自己的管道等效物：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How do we find our model accuracy? Well, let's take a quick look at what is
    happening in the preceding line.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何找到我们的模型准确率？好吧，让我们快速看一下前面一行发生了什么。
- en: Consider that our predictions are [1, 1, 1] and the ground truth is [1, 0, 1].
    The equality would return a simple list of Boolean objects, such as `[True, False,
    True]`. When we sum a Boolean list in Python, it returns the number of `True`
    cases, giving us the exact number of times our model made correct predictions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的预测是[1, 1, 1]，而真实值是[1, 0, 1]。等式将返回一个简单的布尔对象列表，例如`[True, False, True]`。当我们对Python中的布尔列表求和时，它返回`True`案例的数量，这给我们提供了模型正确预测的确切次数。
- en: Dividing this value by the total number of predictions made (or, equally, the
    number of test reviews) gives us our accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将此值除以所做预测的总数（或者，同样地，测试评论的数量）可以得到我们的准确率。
- en: 'Let''s write the previous two-line logic into a simple, lightweight function
    to calculate accuracy, as shown in the following snippet. This would prevent us
    from repeating the logic:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将之前的两行逻辑写入一个简单、轻量级的函数来计算准确率，如下面的代码片段所示。这将防止我们重复逻辑：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Removing stop words
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词
- en: 'By simply passing a flag to the `CountVectorizer` step, we can remove the most
    common stop words. We will specify the language in which the stop words we want
    to remove are written. In the following case, that''s `english`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地传递一个标志到`CountVectorizer`步骤，我们可以移除最常见的停用词。我们将指定要移除的停用词所写的语言。在以下情况下，那是`english`：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, this is not very helpful in improving our accuracy. This would
    indicate that the noise added by stop words is being removed or neglected by the
    classifier itself.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这并没有在提高我们的准确率方面起到很大作用。这表明分类器本身正在移除或忽略由停用词添加的噪声。
- en: Increasing ngram range
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加ngram范围
- en: 'Let''s now try to improve the information available to the classifier by including
    bigrams and trigrams, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试通过包括二元组和三元组来改进分类器可用的信息，如下所示：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Multinomial Naive Bayes
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: 'Let''s initialize the classifier in a manner identical to our logistic regression
    classifier, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将与我们的逻辑回归分类器相同的方式初始化分类器，如下所示：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The previous command will measure performance on the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将测量以下方面的性能：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Adding TF-IDF
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加TF-IDF
- en: 'Now, let''s try the preceding model with TF-IDF, as another step after bag-of-words
    (unigrams), as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试在单词袋（单语元）之后作为另一个步骤使用TF-IDF的先前模型，如下所示：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is better than our previous value, but let's see what else we can do to
    improve this further.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们之前的价值要好，但让我们看看我们还能做些什么来进一步提高这个值。
- en: Removing stop words
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词
- en: 'Let''s now remove the stop words for English again, by simply passing `english`
    to the tokenizer as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再次通过将`english`传递给分词器来移除英语中的停用词，如下所示：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This helps improve performance, but only marginally. We might be better off
    simply keeping in the stop words for other classifiers that we try.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于提高性能，但只是略微提高。我们可能不如简单地保留其他分类器中的停用词。
- en: 'As a last manual experiment, let''s try adding bigrams and unigrams, as we
    did lfor ogistic regression, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的手动实验，让我们尝试添加二元组和一元组，就像我们在逻辑回归中做的那样，如下所示：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is significantly better than the previous Multinomial Naive Bayes performance,
    but not as good as the performance of our logistic regression classifier, which
    was close to achieving 88% accuracy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这比之前的多项式朴素贝叶斯性能要好得多，但不如我们的逻辑回归分类器表现好，后者接近88%的准确率。
- en: Let's now try something specific to Bayesian classifiers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试一些针对贝叶斯分类器的特定方法。
- en: Changing fit prior to false
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将fit prior改为false
- en: 'Increasing `ngram_range` did work for us, but changing `prior` from `uniform`
    to fitting it (by changing `fit_prior` to `False`) did not help at all, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 增加`ngram_range`对我们有所帮助，但将`prior`从`uniform`改为拟合（通过将`fit_prior`改为`False`）并没有起到任何作用，如下所示：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have now thought of each combination that might improve our performance.
    Note that this approach is tedious, and also error-prone because it relies too
    greatly on human intuition.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经考虑了所有可能提高我们性能的组合。请注意，这种方法很繁琐，而且容易出错，因为它过于依赖人类的直觉。
- en: Support vector machines
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '**Support vector machines** (**SVM**) continue to remain a hugely popular machine
    learning technique, having made its way from the industry to classrooms and then
    back. In addition to several forms of regression, SVM is one of the techniques
    that forms the backbone of the multi-billion-dollar online ad targeting industry.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）继续保持着一种非常受欢迎的机器学习技术，它从工业界进入课堂，然后再回到工业界。除了多种形式的回归之外，SVM是构成数十亿美元在线广告定位产业支柱的技术之一。'
- en: In academia, work such as that by T Joachim ([https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf))
    recommends support vector classifiers for text classification.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术界，T Joachim的工作（[https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf)）建议使用支持向量分类器进行文本分类。
- en: 'It''s difficult to estimate whether it will be equally effective for us based
    on such literature, mainly due to a difference in the dataset and pre-processing
    steps. Let''s give it a shot nevertheless:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这样的文献，很难估计它对我们是否同样有效，主要是因为数据集和预处理步骤的不同。尽管如此，我们还是试一试：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: While SVM works best with linearly separable data (as we can see, our text is
    not linearly separable), it's still worth giving it a try for completeness.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SVM最适合线性可分的数据（正如我们所见，我们的文本不是线性可分的），但为了完整性，仍然值得一试。
- en: In the previous example, SVM did not perform well, and it also took a really
    long time to train (~150x) when compared to other classifiers. We will not look
    at SVM for this particular dataset again.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，SVM表现不佳，并且与其他分类器相比，训练时间也非常长（约150倍）。我们将不再针对这个特定数据集查看SVM。
- en: Decision trees
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Decision trees are simple, intuitive tools for classification and regression
    alike. They often resemble a flow chart of decisions when seen visually, hence
    the name decision tree. We will reuse our pipeline, simply using the `DecisionTreeClassifier`
    as our main classification technique, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是分类和回归的简单直观工具。当从视觉上看时，它们常常类似于决策流程图，因此得名决策树。我们将重用我们的管道，简单地使用`DecisionTreeClassifier`作为我们的主要分类技术，如下所示：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Random forest classifier
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: 'Let''s now try the first ensemble classifier. The forest in Random forest classifiers
    comes from the fact that each instance of this classifier consists of several
    decision trees. The Random in Random forests comes from the fact that each tree
    selects a finite number of features from all features at random, as shown in the
    following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试第一个集成分类器。随机森林分类器中的“森林”来源于这个分类器的每个实例都由多个决策树组成。随机森林中的“随机”来源于每个树从所有特征中随机选择有限数量的特征，如下面的代码所示：
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Although considered to be very powerful when used in most machine learning tasks,
    the Random Forest approach doesn't do particularly well in our case. This is partially
    because of our rather crude feature extraction.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在大多数机器学习任务中使用时被认为非常强大，但随机森林方法在我们的案例中表现不佳。这部分原因在于我们相当粗糙的特征提取。
- en: Approaches such as decision trees, RFC, and Extra trees classifiers don't do
    well in high-dimensional spaces such as text.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树、随机森林（RFC）和额外的树分类器在文本等高维空间中表现不佳。
- en: Extra trees classifier
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的树分类器
- en: 'The Extra in Extra Trees comes from the idea that it is extremely randomized.
    While the tree splits in a Random Forest classifier are effectively deterministic,
    they are randomized in the Extra Trees classifier. This changes the bias-variance
    trade-off in cases of high-dimensional data such as ours (where every word is
    effectively a dimension or classifier). The following snippet shows the classifier
    in action:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: “额外的树”中的“额外”来源于其极端随机化的想法。虽然随机森林分类器中的树分割是有效确定性的，但在额外的树分类器中是随机的。这改变了高维数据（如我们这里的每个单词都是一个维度或分类器）的偏差-方差权衡。以下代码片段显示了分类器的实际应用：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, this change works in our favor here, but this is not universally
    true. Results will vary across datasets as well as feature extraction pipelines.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这种变化在这里对我们有利，但这并不总是普遍适用的。结果会因数据集以及特征提取管道的不同而有所变化。
- en: Optimizing our classifiers
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化我们的分类器
- en: Let's now focus on our best performing model, logistic regression, and see if
    we can push its performance a little higher. The best performance for our LR-based
    model is an accuracy of 0.88312, as seen earlier.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在专注于我们表现最好的模型——逻辑回归，看看我们是否能将其性能提升一点。我们基于LR的模型的最佳性能是之前看到的0.88312的准确率。
- en: We are using the phrases parameter search and hyperparameter search interchangeably
    here. This is done to stay consistent with deep learning vocabulary.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将短语“参数搜索”和“超参数搜索”互换使用。这样做是为了保持与深度学习词汇的一致性。
- en: We want to select the best performing configuration of our pipeline. Each configuration
    might be different in small ways, such as when we remove stop words, bigrams,
    and trigrams, or similar processes. The total number of such configurations can
    be fairly large, and can sometimes run into the thousands. In addition to manually
    selecting a few combinations to try, we can try all several thousand of these
    combinations and evaluate them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望选择我们管道的最佳性能配置。每个配置可能在某些小方面有所不同，例如当我们移除停用词、二元词和三元词，或类似的过程时。这样的配置总数可能相当大，有时可能达到数千。除了手动选择一些组合进行尝试外，我们还可以尝试所有这些数千种组合并评估它们。
- en: Of course, this process would be far too time-consuming for most small-scale
    experiments such as ours. In large experiments, possible space can run into the
    millions and take several days of computing, making it cost- and time-prohibitive.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个过程对于我们这样的小规模实验来说将过于耗时。在大规模实验中，可能的空间可以达到数百万，需要几天的时间进行计算，这使得成本和时间变得难以承受。
- en: We recommend reading a blog on hyperparameter tuning ([https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning](https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning))
    to become familiar with the vocabulary and ideas discussed here in greater detail.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议阅读一篇关于超参数调整的博客（[https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning](https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning)），以更详细地了解这里讨论的词汇和思想。
- en: Parameter tuning using RandomizedSearch
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机搜索进行参数调整
- en: An alternative approach was proposed by Bergstra and Bengio ([http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf))
    in 2012\. They demonstrated that a random search across a large hyperparameter
    space is more effective than a manual approach, as we did for Multinomial Naive
    Bayes, and often as effective—or more so—than `GridSearch`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Bergstra 和 Bengio 在 2012 年提出了一种替代方法（[http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)）。他们证明了在大超参数空间中进行随机搜索比手动方法更有效，正如我们在多项式朴素贝叶斯中做的那样，并且通常与`GridSearch`一样有效，甚至更有效。
- en: How do we use it here?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里如何使用它？
- en: 'Here, we will build on top of the results such as that of Bergstra and Bengio.
    We will break down our parameter search into the following two steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将基于Bergstra 和 Bengio 等人的结果。我们将把我们的参数搜索分为以下两个步骤：
- en: Using `RandomizedSearch`, go through a wide parameter combination space in a
    limited number of iterations
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `RandomizedSearch`，在有限的迭代次数中遍历广泛的参数组合空间
- en: Use the results from step 1 to run `GridSearch` in a slightly narrow space
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤 1 的结果，在略微狭窄的空间内运行 `GridSearch`
- en: 'We can repeat the previous steps until we stop seeing improvements in our results,
    but we won''t do that here. We''ll leave that as an exercise for the reader. Our
    example is outlined in the following snippet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重复之前的步骤，直到我们不再看到结果中的改进，但在这里我们不会这么做。我们将把这个作为读者的练习。我们的示例在下面的片段中概述：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, the `param_grid` variable defines our search space. In our pipeline,
    we assign names to each estimator such as `vect`, `clf`, and so on. The convention
    of `clf` double underscore (also called dunder) signifies that this `C` is an
    attribute of the `clf` object. Similarly, for `vect` we specify whether stop words
    are to be removed or not. As an example, `english` means removing English stop
    words where the list of stop words is what `scikit-learn` internally uses. You
    can also replace this with a command from spaCy, NLTK, or one more closely customized
    to your tasks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`param_grid` 变量定义了我们的搜索空间。在我们的管道中，我们为每个估计器分配名称，例如 `vect`、`clf` 等。`clf`
    双下划线（也称为 dunder）的约定表示这个 `C` 是 `clf` 对象的属性。同样，对于 `vect`，我们指定是否要移除停用词。例如，`english`
    表示移除英语停用词，其中停用词列表是 `scikit-learn` 内部使用的。您也可以用 spaCy、NLTK 或更接近您任务的命令替换它。
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The preceding code gives us a cross validation accuracy in the range of 0.87\.
    This might vary depending on how the randomized splits are created.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码给出了交叉验证准确率在 0.87 范围内。这可能会根据随机分割的方式而有所不同。
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As shown in the preceding snippet, the classifier performance improves by more
    than 1% by simply changing a few parameters. This is amazing progress!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述片段所示，通过简单地更改几个参数，分类器性能提高了超过 1%。这是一个惊人的进步！
- en: 'Let''s now take a look at what parameters we''re using. In order to compare
    this, you need to know the default values for all of the parameters. Alternatively,
    we can simply look at the parameters from `param_grid` that we wrote and note
    the selected parameter values. For everything not in the grid, the default values
    are chosen and remain unchanged, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看我们正在使用的参数。为了进行比较，您需要知道所有参数的默认值。或者，我们可以简单地查看我们编写的 `param_grid` 参数，并注意选定的参数值。对于不在网格中的所有内容，将选择默认值并保持不变，如下所示：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we notice these things in the best classifier:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们注意到最佳分类器中的这些事情：
- en: The chosen `C` value in `clf` is `100`
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `clf` 中选择的 `C` 值是 `100`
- en: '`lowercase` is set to `False`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lowercase` 被设置为 `False`'
- en: Removing stop words is a bad idea
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词不是一个好主意
- en: Adding bigrams and trigrams helps
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加二元词和三元词有帮助
- en: Observations like the preceding are very specific to this dataset and classifier
    pipeline. In my experience, however, this can and does vary widely.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的观察结果非常具体于这个数据集和分类器管道。然而，根据我的经验，这可以并且确实有很大的变化。
- en: Let's also avoid assuming that the values are always the best we'll get when
    running `RandomizedSearch` for so few iterations. The rule of thumb in this case
    is to run it for at least 60 iterations, and to also use a much larger `param_grid`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也避免假设在运行迭代次数如此之少的 `RandomizedSearch` 时，我们总是能得到最佳值。在这种情况下，经验法则是至少运行 60 次迭代，并且也要使用一个更大的
    `param_grid`。
- en: Here, we used `RandomizedSearch` to understand the broad layout of parameters
    we want to try. We added the best values for some of those to our pipeline itself
    and we will continue to experiment with the values of other parameters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 `RandomizedSearch` 来了解我们想要尝试的参数的广泛布局。我们将其中一些参数的最佳值添加到我们的管道中，并将继续对这些参数的值进行实验。
- en: We have not mentioned what the `C` parameter stands for or how it influences
    the classifier. This is definitely important when understanding and performing
    a manual parameter search. Changing `C` helps simply by trying out different values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有提到 `C` 参数代表什么或它如何影响分类器。在理解和执行手动参数搜索时，这绝对很重要。通过尝试不同的值来改变 `C` 可以简单地帮助。
- en: GridSearch
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索
- en: We will now run `GridSearch` for our selected parameters. Here, we are choosing
    to include bigrams and trigrams while running `GridSearch` over the `C` parameter
    of `LogisticRegression`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为我们选择的参数运行 `GridSearch`。在这里，我们选择在运行 `GridSearch` 时包括二元词和三元词，同时针对 `LogisticRegression`
    的 `C` 参数进行搜索。
- en: Our intention here is to automate as much as possible. Instead of trying varying
    values in `C` during our `RandomizedSearch`, we are trading off human learning
    time (a few hours) with compute time (a few extra minutes). This mindset saves
    us both time and effort.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的意图是尽可能自动化。我们不是在`RandomizedSearch`期间尝试改变`C`的值，而是在人类学习时间（几个小时）和计算时间（几分钟）之间进行权衡。这种思维方式为我们节省了时间和精力。
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding lines of code, we have ran the classifier over our `lr_clf`
    using the new, simpler `param_grid`, which works only over the `C` parameter of
    `LogisticRegression`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码行中，我们已经使用新的、更简单的`param_grid`在`lr_clf`上运行了分类器，这个`param_grid`只在`LogisticRegression`的`C`参数上工作。
- en: 'Let''s see what the steps in our best estimator are, and in particular, what
    the value of `C` is, as shown in the following snippet:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们最佳估计器的步骤，特别是`C`的值，如下面的代码片段所示：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s get the resulting performance directly from our object. Each of these
    objects has an attribute called `best_score_`. This attribute stores the best
    value of the metric we chose. In the following case, we have chosen accuracy:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接从我们的对象中获取结果性能。这些对象中每个都有一个名为`best_score_`的属性。该属性存储了我们选择的度量标准的最优值。在以下情况下，我们选择了准确率：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see in the preceding code, that's almost a ~3% performance gain over
    the non-optimized model, despite the fact we tried very few parameters to optimize.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的代码中看到的，这几乎是非优化模型的3%的性能提升，尽管我们尝试了很少的参数进行优化。
- en: It is worth mentioning that we can and must repeat these steps (`RandomizedSearch`
    and `GridSearch`) to push the model's accuracy even higher.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们可以并且必须重复这些步骤（`RandomizedSearch`和`GridSearch`），以进一步提高模型的准确率。
- en: Ensembling models
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模型
- en: Ensembling models is a very powerful technique for improving your model performance
    across a variety of machine learning tasks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型是提高各种机器学习任务模型性能的非常强大的技术。
- en: In the following section, we have quoted from the Kaggle Ensembling Guide ([https://mlwave.com/kaggle-ensembling-guide/](https://mlwave.com/kaggle-ensembling-guide/))
    written by MLWave.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分，我们引用了由MLWave撰写的Kaggle集成指南（[https://mlwave.com/kaggle-ensembling-guide/](https://mlwave.com/kaggle-ensembling-guide/)）。
- en: We can explain why ensembling helps to reduce error or improve accuracy, as
    well as demonstrate the popular techniques on our chosen task and dataset. While
    each of these techniques might not result in a performance gain for us on our
    dataset specifically, they are still a powerful tool to have in your mental toolkit.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以解释为什么集成可以帮助减少错误或提高精度，同时展示在我们选择的任务和数据集上流行的技术。虽然这些技术可能不会在我们的特定数据集上带来性能提升，但它们仍然是心理工具箱中一个强大的工具。
- en: To ensure that you understand these techniques, we strongly urge you to try
    them on a few datasets.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您理解这些技术，我们强烈建议您在几个数据集上尝试它们。
- en: Voting ensembles – Simple majority (aka hard voting)
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票集成 - 简单多数（也称为硬投票）
- en: The simplest ensembling technique is perhaps to take a simple majority. This
    works on the intuition that a single model might make an error on a particular
    prediction but that several different models are unlikely to make identical errors.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 也许是简单的多数投票是最简单的集成技术。这基于直觉，单个模型可能在某个特定预测上出错，但几个不同的模型不太可能犯相同的错误。
- en: Let's look at an example.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。
- en: 'Ground truth: 11011001'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 真实值：11011001
- en: The numbers 1 and 0 represent a `True` and `False` prediction for an imagined
    binary classifier. Each digit is a single true or false prediction for different
    inputs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数字1和0代表一个假想的二元分类器的`True`和`False`预测。每个数字是对不同输入的单个真或假预测。
- en: 'Let''s assume there are three models with only one error for this example;
    they are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设在这个例子中有三个模型，只有一个错误；它们如下所示：
- en: 'Model A prediction: 10011001'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型A预测：10011001
- en: 'Model B prediction: 11011001'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型B预测：11011001
- en: 'Model C prediction: 11011001'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型C预测：11011001
- en: 'The majority votes gives us the correct answer as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 多数投票给出了以下正确答案：
- en: 'Majority vote: 11011001'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多数投票：11011001
- en: In the case of an even number of models, we can use a tie breaker. A tie breaker
    can be as simple as picking a random result, or more nuanced by picking the results
    with more confidence.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型数量为偶数的情况下，我们可以使用平局决定者。平局决定者可以是简单地随机选择一个结果，或者更复杂地选择更有信心的结果。
- en: To try this on our dataset, we import `VotingClassifier` from scikit-learn.
    `VotingClassifier` does not use the pre-trained models as inputs. It will call
    fit on the models or classifier pipelines, and then use the predictions of all
    models to make the final prediction.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的数据集上尝试这种方法，我们导入`VotingClassifier`从scikit-learn。`VotingClassifier`不使用预训练模型作为输入。它将对模型或分类器管道调用fit，然后使用所有模型的预测来做出最终预测。
- en: 'To counter the hype in favor of ensembles elsewhere, we can demonstrate that
    hard voting may hurt your accuracy performance. If someone claims that ensembling
    always helps, show them the following example for a more constructive discussion:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了反驳其他地方对集成方法的过度炒作，我们可以证明硬投票可能会损害你的准确率性能。如果有人声称集成总是有帮助的，你可以向他们展示以下示例以进行更有建设性的讨论：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We used only two classifiers for demonstration in the preceding example: Extra
    Trees and Random Forest. Individually, each of these classifiers has their performance
    capped at an accuracy of ~74%.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们仅使用了两个分类器进行演示：Extra Trees 和 Random Forest。单独来看，这些分类器的性能上限大约为74%的准确率。
- en: In this particular example, the performance of the voting classifier is worse
    than both of them alone.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，投票分类器的性能比单独使用任何一个都要差。
- en: Voting ensembles – soft voting
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票集成 – 软投票
- en: Soft voting predicts the class label based on class probabilities. The sums
    of the predicted probabilities for each classifier areg calculated for each class
    (which is important in the case of multiple classes). The assigned class is then
    the class with the maximum probability sum or `argmax(p_sum)`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 软投票根据类别概率预测类别标签。为每个分类器计算每个类别的预测概率之和（这在多类别的情况下很重要）。然后，分配的类别是具有最大概率之和的类别或`argmax(p_sum)`。
- en: 'This is recommended for an ensemble of well-calibrated classifiers, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于一组校准良好的分类器是推荐的，如下所示：
- en: Well calibrated classifiers are probabilistic classifiers for which the output
    of the predict_proba method can be directly interpreted as a confidence level.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 校准良好的分类器是概率分类器，其predict_proba方法的输出可以直接解释为置信水平。
- en: '- From the Calibration Docs on sklearn ([http://scikit-learn.org/stable/modules/calibration.html](http://scikit-learn.org/stable/modules/calibration.html))'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '- 来自sklearn的校准文档 ([http://scikit-learn.org/stable/modules/calibration.html](http://scikit-learn.org/stable/modules/calibration.html))'
- en: 'Our code flow is identical to our hard voting classifier except that the parameter
    `voting` is passed as `soft`*,* as shown in the following snippet:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码流程与硬投票分类器相同，只是将参数`voting`传递为`soft`*，如下面的代码片段所示：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we can see that soft voting gives us an absolute accuracy gain of 1.62%.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到软投票为我们带来了1.62%的绝对准确率提升。
- en: Weighted classifiers
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权分类器
- en: The only way for inferior models to overrule the best (expert) model is for
    them to collectively and confidently agree on an alternative.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 次级模型要推翻最佳（专家）模型，唯一的办法是它们必须集体且自信地同意一个替代方案。
- en: To avoid this scenario, we can use a weighted majority vote—but why weighting?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们可以使用加权多数投票——但为什么要加权？
- en: 'Usually, we want to give a better model more weight in a vote. The simplest,
    but computationally inefficient, way to do this is to repeat the classifier pipelines
    under different names, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望在投票中给予更好的模型更多的权重。实现这一目标最简单但计算效率最低的方法是重复使用不同名称的分类器管道，如下所示：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Repeat the experiment with hard voting instead of soft voting. This will tell
    you how the voting strategy influences the accuracy of our ensembled classifier,
    as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 用硬投票而不是软投票重复实验。这将告诉你投票策略如何影响我们集成分类器的准确率，如下所示：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, we can see that weighted voting gives us an absolute accuracy gain of
    1.50%.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到加权投票为我们带来了1.50%的绝对准确率提升。
- en: So, what have we learned so far?
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，到目前为止我们学到了什么？
- en: A simple majority-based voting classifier can perform worse than individual
    models
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于简单多数的投票分类器可能比单个模型表现更差
- en: Soft voting works better than hard voting
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软投票比硬投票更有效
- en: Weighing classifiers by simply repeating classifiers can help
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简单地重复分类器来权衡分类器可以帮助
- en: So far, we have been selecting classifiers seemingly at random. This is less
    than ideal, especially when we are building for a commercial utility where every
    0.001% gain matters.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们似乎是在随机选择分类器。这并不理想，尤其是在我们为商业应用构建模型时，每0.001%的提升都很重要。
- en: Removing correlated classifiers
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除相关分类器
- en: 'Let''s look at this in action by taking three simple models as an example.
    As you can see, the ground truth is all 1s:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过三个简单的模型来观察这个方法在实际中的应用。正如你所见，真实值都是1：
- en: '[PRE36]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'These models are highly correlated in their predictions. When we take a majority
    vote, we see no improvement:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在预测上高度相关。当我们进行多数投票时，我们并没有看到任何改进：
- en: '[PRE37]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, let''s compare that to the following three lower-performing but highly
    uncorrelated models:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这个结果与以下三个性能较低但高度不相关的模型进行比较：
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'When we ensemble this with a majority vote, we get the following result:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用多数投票集成这些模型时，我们得到以下结果：
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, we see a much higher rate of improvement than in any of our individual
    models. Low correlation between model predictions can lead to better performance.
    In practice, this is tricky to get right but is worth investigating nevertheless.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了比我们任何单个模型都要高的改进率。模型预测之间的低相关性可以导致更好的性能。在实践中，这很难做到正确，但仍然值得研究。
- en: We will leave the following section as an exercise for you to try out.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以下部分留给你作为练习尝试。
- en: As a quick hint, you will need to find the correlations among predictions of
    different models and select pairs that are less correlated to each other (ideally
    less than 0.5) and yet have a good enough performance as individual models.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个小提示，你需要找到不同模型预测之间的相关性，并选择那些相互之间相关性较低（理想情况下小于0.5）且作为独立模型有足够好的性能的成对模型。
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So, what result do we get when we use two classifiers from the same approach?
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们使用来自同一方法的两分类器时，我们会得到什么结果呢？
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, the preceding result is not very encouraging either, but remember,
    this is just a hint! We encourage you to go ahead and try this task on your own
    and with more classifiers, including ones we have not discussed here.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，前面的结果也不是很鼓舞人心，但请记住，这只是一个提示！我们鼓励你继续尝试这个任务，并使用更多的分类器，包括我们在这里没有讨论过的分类器。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we looked at several new ideas regarding machine learning.
    The intention here was to demonstrate some of the most common classifiers. We
    looked at how to use them with one thematic idea: translating text to a numerical
    representation and then feeding that to a classifier.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了关于机器学习的几个新想法。这里的目的是展示一些最常见的分类器。我们通过一个主题思想来探讨如何使用它们：将文本转换为数值表示，然后将这个表示输入到分类器中。
- en: This chapter covered a fraction of the available possibilities. Remember, you
    can try anything from better feature extraction using Tfidf to tuning classifiers
    with `GridSearch` and `RandomizedSearch`, as well as ensembling several classifiers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本章只涵盖了可用可能性的一小部分。记住，你可以尝试从使用Tfidf进行更好的特征提取到使用`GridSearch`和`RandomizedSearch`调整分类器，以及集成多个分类器。
- en: This chapter was mostly focused on pre-deep learning methods for both feature
    extraction and classification.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要关注特征提取和分类的深度学习之前的预方法。
- en: Note that deep learning methods also allow us to use a single model where the
    feature extraction and classification are both learned from the underlying data
    distribution. While a lot has been written about deep learning in computer vision,
    we have offered only an introduction to deep learning in natural language processing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，深度学习方法还允许我们使用一个模型，其中特征提取和分类都是从底层数据分布中学习的。虽然关于计算机视觉中的深度学习已经有很多文献，但我们只提供了自然语言处理中深度学习的一个简介。
