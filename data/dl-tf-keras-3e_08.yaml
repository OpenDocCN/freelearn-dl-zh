- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Autoencoders
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'Autoencoders are neural networks that learn by unsupervised learning, also
    sometimes called semi-supervised learning, since the input is treated as the target
    too. In this chapter, you will learn about and implement different variants of
    autoencoders and eventually learn how to stack autoencoders. We will also see
    how autoencoders can be used to create MNIST digits, and finally, also cover the
    steps involved in building a long short-term memory autoencoder to generate sentence
    vectors. This chapter includes the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是通过无监督学习（有时也称为半监督学习）来学习的，因为输入也被视为目标。在本章中，你将学习并实现不同类型的自编码器，最终学习如何堆叠自编码器。我们还将看到如何使用自编码器生成MNIST数字，最后，介绍构建长短期记忆自编码器生成句子向量的步骤。本章包含以下主题：
- en: Vanilla autoencoders
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通自编码器
- en: Sparse autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Denoising autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Convolutional autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: Stacked autoencoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠自编码器
- en: Generating sentences using LSTM autoencoders
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM自编码器生成句子
- en: Variational autoencoders for generating images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器生成图像
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp8](https://packt.link/dltfchp8)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp8](https://packt.link/dltfchp8)找到
- en: Let’s begin!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Introduction to autoencoders
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器简介
- en: 'Autoencoders are a class of neural networks that attempt to recreate input
    as their target using backpropagation. An autoencoder consists of two parts: an
    encoder and a decoder. The encoder will read the input and compress it to a compact
    representation, and the decoder will read the compact representation and recreate
    the input from it. In other words, the autoencoder tries to learn the identity
    function by minimizing the reconstruction error.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一类神经网络，试图通过反向传播将输入重建为目标。自编码器由两部分组成：编码器和解码器。编码器读取输入并将其压缩成紧凑的表示，解码器则读取这个紧凑的表示并从中重建输入。换句话说，自编码器通过最小化重建误差来尝试学习恒等函数。
- en: They have an inherent capability to learn a compact representation of data.
    They are at the center of deep belief networks and find applications in image
    reconstruction, clustering, machine translation, and much more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它们具有学习数据紧凑表示的固有能力。它们处于深度信念网络的核心，广泛应用于图像重建、聚类、机器翻译等多个领域。
- en: You might think that implementing an identity function using deep neural networks
    is boring; however, the way in which this is done makes it interesting. The number
    of hidden units in the autoencoder is typically fewer than the number of input
    (and output) units. This forces the encoder to learn a compressed representation
    of the input, which the decoder reconstructs. If there is a structure in the input
    data in the form of correlations between input features, then the autoencoder
    will discover some of these correlations, and end up learning a low-dimensional
    representation of the data similar to that learned using **principal component
    analysis** (**PCA**).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为使用深度神经网络实现恒等函数很无聊；然而，之所以有趣，是因为实现的方式。自编码器中的隐藏单元数通常少于输入（和输出）单元的数量。这迫使编码器学习输入的压缩表示，而解码器则从中进行重建。如果输入数据中存在某种结构，比如输入特征之间的相关性，那么自编码器将会发现这些相关性，并最终学习到类似于**主成分分析**（**PCA**）所学到的低维表示。
- en: While PCA uses linear transformations, autoencoders on the other hand use non-linear
    transformations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PCA使用线性变换，但自编码器则使用非线性变换。
- en: Once the autoencoder is trained, we would typically just discard the decoder
    component and use the encoder component to generate compact representations of
    the input. Alternatively, we could use the encoder as a feature detector that
    generates a compact, semantically rich representation of our input and build a
    classifier by attaching a softmax classifier to the hidden layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦自编码器训练完成，我们通常会丢弃解码器组件，只使用编码器组件来生成输入的紧凑表示。或者，我们也可以将编码器用作特征检测器，生成输入的紧凑且语义丰富的表示，并通过将softmax分类器附加到隐藏层来构建分类器。
- en: 'The encoder and decoder components of an autoencoder can be implemented using
    either dense, convolutional, or recurrent networks, depending on the kind of data
    that is being modeled. For example, dense networks might be a good choice for
    autoencoders used to build **collaborative filtering** (**CF**) models, where
    we learn a compressed model of user preferences based on actual sparse user ratings.
    Similarly, convolutional neural networks may be appropriate for the use case described
    in the article *iSee: Using Deep Learning to Remove Eyeglasses from Faces*, by
    M. Runfeldt. Recurrent networks, on the other hand, are a good choice for autoencoders
    working on sequential or text data, such as Deep Patient (*Deep Patient: An Unsupervised
    Representation to Predict the Future of Patients from the Electronic Health Records*,
    Miotto et al.) and skip-thought vectors.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '自编码器的编码器和解码器组件可以使用稠密、卷积或递归网络来实现，这取决于所建模的数据类型。例如，稠密网络可能是用于构建**协同过滤**（**CF**）模型的自编码器的不错选择，在这种模型中，我们基于实际的稀疏用户评分学习用户偏好的压缩模型。类似地，卷积神经网络可能适用于文章*《iSee:
    Using Deep Learning to Remove Eyeglasses from Faces》*中描述的用例，作者为M. Runfeldt。另一方面，递归网络对于处理顺序或文本数据的自编码器是一个不错的选择，例如《深度患者：从电子健康记录中预测患者未来的无监督表示》(*Deep
    Patient: An Unsupervised Representation to Predict the Future of Patients from
    the Electronic Health Records*, Miotto等)和跳跃思维向量（skip-thought vectors）。'
- en: 'We can think of autoencoders as consisting of two cascaded networks. The first
    network is an encoder; it takes the input *x*, and encodes it using a transformation
    *h* to an encoded signal *y*, that is:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将自编码器看作由两个级联网络组成。第一个网络是编码器，它接收输入*x*，并通过变换*h*将其编码为编码信号*y*，即：
- en: '*y= h*(*x*)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = h*(*x*)'
- en: 'The second network uses the encoded signal *y* as its input and performs another
    transformation *f* to get a reconstructed signal *r*, that is:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个网络使用编码信号*y*作为输入，执行另一个变换*f*，得到重建信号*r*，即：
- en: '*r= f*(*y*) *= f*(*h*(*x*))'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*r = f*(*y*) *= f*(*h*(*x*))'
- en: We define error, *e*, as the difference between the original input *x* and the
    reconstructed signal *r, e= x- r*. The network then learns by reducing the loss
    function (for example, **mean squared error** (**MSE**)), and the error is propagated
    backward to the hidden layers as in the case of **multilayer perceptrons** (**MLPs**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将误差定义为*e*，即原始输入*x*与重建信号*r*之间的差异，*e = x - r*。然后，网络通过减少损失函数（例如，**均方误差**（**MSE**））来学习，误差像**多层感知机**（**MLP**）中那样向后传播到隐藏层。
- en: 'Depending upon the actual dimensions of the encoded layer with respect to the
    input, the loss function, and constraints, there are various types of autoencoders:
    variational autoencoders, sparse autoencoders, denoising autoencoders, and convolution
    autoencoders.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据编码层相对于输入的实际维度、损失函数和约束条件，存在多种类型的自编码器：变分自编码器、稀疏自编码器、去噪自编码器和卷积自编码器。
- en: Autoencoders can also be stacked by successively stacking encoders that compress
    their input to smaller and smaller representations, then stacking decoders in
    the opposite sequence. Stacked autoencoders have greater expressive power and
    the successive layers of representations capture a hierarchical grouping of the
    input, similar to the convolution and pooling operations in convolutional neural
    networks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器还可以通过依次堆叠编码器来堆叠，编码器将其输入压缩为越来越小的表示，然后按相反顺序堆叠解码器。堆叠自编码器具有更强的表达能力，连续层的表示捕捉了输入的层次化分组，类似于卷积神经网络中的卷积和池化操作。
- en: 'Stacked autoencoders used to be trained layer by layer. For example, in the
    network in *Figure 8.1*, we would first train layer **X** to reconstruct layer
    **X’** using the hidden layer **H1** (ignoring **H2**). We would then train layer
    **H1** to reconstruct layer **H1’** using the hidden layer **H2**. Finally, we
    would stack all the layers together in the configuration shown and fine-tune it
    to reconstruct **X’** from **X**. With better activation and regularization functions
    nowadays, however, it is quite common to train these networks in totality:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠自编码器曾经是按层训练的。例如，在*图 8.1*中的网络，我们首先训练**X**层，通过隐藏层**H1**重建**X’**层（忽略**H2**）。然后，我们训练**H1**层，通过隐藏层**H2**重建**H1’**层。最后，我们将所有层堆叠在一起，按所示配置进行微调，以便从**X**重建**X’**。然而，随着如今更好的激活函数和正则化函数的出现，训练这些网络时通常会一次性训练整个网络：
- en: '![Diagram, shape  Description automatically generated](img/B18331_08_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图示，形状 描述自动生成](img/B18331_08_01.png)'
- en: 'Figure 8.1: Visualization of stacked autoencoders'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：堆叠自编码器的可视化
- en: In this chapter, we will learn about these variations in autoencoders and implement
    them using TensorFlow.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习自编码器中的这些变化，并使用TensorFlow实现它们。
- en: Vanilla autoencoders
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单自编码器
- en: The vanilla autoencoder, as proposed by Hinton in his 2006 paper *Reducing the
    Dimensionality of Data with Neural Networks*, consists of one hidden layer only.
    The number of neurons in the hidden layer is fewer than the number of neurons
    in the input (or output) layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 简单自编码器，正如Hinton在他2006年的论文《使用神经网络减少数据维度》中提出的那样，仅包含一个隐藏层。隐藏层中的神经元数量少于输入层（或输出层）中的神经元数量。
- en: This results in producing a bottleneck effect in the flow of information in
    the network. The hidden layer (*y*) between the encoder input and decoder output
    is also called the “bottleneck layer.” Learning in the autoencoder consists of
    developing a compact representation of the input signal at the hidden layer so
    that the output layer can faithfully reproduce the original input.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致信息流在网络中的瓶颈效应。编码器输入和解码器输出之间的隐藏层（*y*）也被称为“瓶颈层”。自编码器中的学习过程包括在隐藏层上开发输入信号的紧凑表示，以便输出层可以忠实地重建原始输入。
- en: 'In *Figure 8.2*, you can see the architecture of a vanilla autoencoder:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8.2*中，您可以看到一个简单自编码器的架构：
- en: '![Chart, waterfall chart  Description automatically generated](img/B18331_08_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图 描述自动生成](img/B18331_08_02.png)'
- en: 'Figure 8.2: Architecture of the vanilla autoencoder'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：简单自编码器架构
- en: Let’s try to build a vanilla autoencoder. While in the paper Hinton used it
    for dimension reduction, in the code to follow, we will use autoencoders for image
    reconstruction. We will train the autoencoder on the MNIST database and will use
    it to reconstruct the test images. In the code, we will use the TensorFlow Keras
    `Layers` class to build our own encoder and decoder layers, so firstly let’s learn
    a little about the `Layers` class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个简单自编码器。在论文中，Hinton使用它进行维度减少，而在接下来的代码中，我们将使用自编码器进行图像重建。我们将使用MNIST数据库训练自编码器，并用它来重建测试图像。在代码中，我们将使用TensorFlow
    Keras的`Layers`类来构建我们自己的编码器和解码器层，因此首先让我们了解一下`Layers`类。
- en: TensorFlow Keras layers ‒ defining custom layers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Keras 层 ‒ 定义自定义层
- en: 'TensorFlow provides an easy way to define your own custom layer both from scratch
    or as a composition of existing layers. The TensorFlow Keras `layers` package
    defines a `Layers` object. We can make our own layer by simply making it a subclass
    of the `Layers` class. It is necessary to define the dimensions of the output
    while defining the layer. Though input dimensions are optional, if you do not
    define them, it will infer them automatically from the data. To build our own
    layer we will need to implement three methods:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了一种简单的方法，允许您从零开始或将现有层组合起来定义自定义层。TensorFlow Keras的`layers`包定义了一个`Layers`对象。我们可以通过将其作为`Layers`类的子类来创建自己的层。在定义层时，必须定义输出的维度。尽管输入维度是可选的，但如果不定义，系统将自动从数据中推断出来。要构建我们自己的层，我们需要实现三个方法：
- en: '`__init__()`: Here, you define all input-independent initializations.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__()`：在这里，您定义所有与输入无关的初始化。'
- en: '`build()`: Here, we define the shapes of input tensors and can perform rest
    initializations if required. In our example, since we are not explicitly defining
    input shapes, we need not define the `build()` method.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build()`：在这里，我们定义输入张量的形状，并根据需要执行其他初始化。在我们的示例中，由于没有显式定义输入形状，我们无需定义`build()`方法。'
- en: '`call()`: This is where the forward computation is performed.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call()`：这是执行前向计算的地方。'
- en: 'Using the `tensorflow.keras.layers.Layer` class, we now define the encoder
    and decoder layers. First let’s start with the encoder layer. We import `tensorflow.keras`
    as `K`, and create an `Encoder` class. The `Encoder` takes in the input and generates
    the hidden or the bottleneck layer as the output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tensorflow.keras.layers.Layer`类，我们现在定义编码器和解码器层。首先，从编码器层开始。我们导入`tensorflow.keras`为`K`，并创建一个`Encoder`类。`Encoder`接收输入并生成隐藏层或瓶颈层作为输出：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define the `Decoder` class; this class takes in the output from the
    `Encoder` and then passes it through a fully connected neural network. The aim
    is to be able to reconstruct the input to the `Encoder`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`Decoder`类；该类接收来自`Encoder`的输出，并通过一个全连接神经网络传递。目标是能够重建`Encoder`的输入：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we have both the encoder and decoder defined we use the `tensorflow.keras.Model`
    object to build the autoencoder model. You can see in the following code that
    in the `__init__()` function we instantiate the encoder and decoder objects, and
    in the `call()` method we define the signal flow. Also notice the member list
    `self.loss` initialized in the `_init__()`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了编码器和解码器，我们使用`tensorflow.keras.Model`对象来构建自动编码器模型。你可以在以下代码中看到，在`__init__()`函数中我们实例化了编码器和解码器对象，在`call()`方法中我们定义了信号流。还请注意在`_init__()`中初始化的成员列表`self.loss`：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the next section, we will use the autoencoder that we defined here to reconstruct
    handwritten digits.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用这里定义的自动编码器来重建手写数字。
- en: Reconstructing handwritten digits using an autoencoder
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动编码器重建手写数字
- en: 'Now that we have our model autoencoder with its layer encoder and decoder ready,
    let us try to reconstruct handwritten digits. The complete code is available in
    the GitHub repo of the chapter in the notebook `VanillaAutoencoder.ipynb`. The
    code will require the NumPy, TensorFlow, and Matplotlib modules:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了包含编码器和解码器层的自动编码器模型，让我们尝试重建手写数字。完整的代码可以在本章的 GitHub 仓库中找到，文件名为`VanillaAutoencoder.ipynb`。代码将需要
    NumPy、TensorFlow 和 Matplotlib 模块：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before starting with the actual implementation, let’s also define some hyperparameters.
    If you play around with them, you will notice that even though the architecture
    of your model remains the same, there is a significant change in model performance.
    Hyperparameter tuning (refer to *Chapter 1*, *Neural Network Foundations with
    TF*, for more details) is one of the important steps in deep learning. For reproducibility,
    we set the seeds for random calculation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现之前，我们还需要定义一些超参数。如果你尝试调整这些超参数，你会注意到，尽管模型的架构保持不变，但模型性能却有显著变化。超参数调优（有关更多细节，请参阅*第1章*，*使用TF的神经网络基础*）是深度学习中的一个重要步骤。为了保证可重复性，我们为随机计算设置了种子：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For training data, we are using the MNIST dataset available in the TensorFlow
    datasets. We normalize the data so that pixel values lie between [0,1]; this is
    achieved by simply dividing each pixel element by 255.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据，我们使用的是 TensorFlow 数据集中的 MNIST 数据集。我们将数据归一化，使得像素值位于[0,1]之间；这通过将每个像素元素除以255来实现。
- en: 'We reshape the tensors from 2D to 1D. We employ the `from_tensor_slices` function
    to generate a batched dataset with the training dataset sliced along its first
    dimension (slices of tensors). Also note that we are not using one-hot encoded
    labels; this is because we are not using labels to train the network since autoencoders
    learn via unsupervised learning:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将张量从2D重塑为1D。我们使用`from_tensor_slices`函数生成一个批量数据集，并沿着第一个维度对训练数据集进行切片（切片的张量）。另外请注意，我们没有使用独热编码标签；这是因为我们并没有使用标签来训练网络，因为自动编码器是通过无监督学习进行学习的：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we instantiate our autoencoder model object and define the loss and optimizers
    to be used for training. Observe the formulation of the loss function carefully;
    it is simply the difference between the original image and the reconstructed image.
    You may find that the term *reconstruction loss* is also used to describe it in
    many books and papers:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实例化我们的自动编码器模型对象，并定义训练时使用的损失函数和优化器。仔细观察损失函数的公式；它仅仅是原始图像与重建图像之间的差异。你可能会发现，*重建损失*这一术语也常在许多书籍和论文中用来描述它：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Instead of using the auto-training loop, for our custom autoencoder model,
    we will define a custom training. We use `tf.GradientTape` to record the gradients
    as they are calculated and implicitly apply the gradients to all the trainable
    variables of our model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自定义自动编码器模型将定义一个自定义训练过程，而不是使用自动训练循环。我们使用`tf.GradientTape`来记录梯度计算，并隐式地将梯度应用于模型的所有可训练变量：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding `train()` function will be invoked in a training loop, with the
    dataset fed to the model in batches:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的`train()`函数将在训练循环中调用，并将数据集以批次的形式输入到模型中：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s now train our autoencoder:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来训练我们的自动编码器：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And plot our training graph:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 并绘制我们的训练图：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The training graph is shown as follows. We can see that loss/cost is decreasing
    as the network learns and after 50 epochs it is almost constant about a line.
    This means that further increasing the number of epochs will not be useful. If
    we want to improve our training further, we should change the hyperparameters
    like learning rate and `batch_size`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练图如下所示。我们可以看到，随着网络的学习，损失/成本在减少，并且在50个训练周期后几乎保持不变，这意味着进一步增加训练周期将不会有帮助。如果我们希望进一步提高训练效果，我们应该改变像学习率和`batch_size`这样的超参数：
- en: '![Chart  Description automatically generated](img/B18331_08_03.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18331_08_03.png)'
- en: 'Figure 8.3: Loss plot of the vanilla autoencoder'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：基础自编码器的损失曲线
- en: 'In *Figure 8.4*, you can see the original (top) and reconstructed (bottom)
    images; they are slightly blurred, but accurate:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 8.4*中，你可以看到原始图像（上）和重建图像（下）；它们略显模糊，但仍然准确：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Text  Description automatically generated with medium confidence](img/B18331_08_04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成，信心中等](img/B18331_08_04.png)'
- en: 'Figure 8.4: Original and reconstructed images using vanilla autoencoder'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：使用基础自编码器的原始图像与重建图像
- en: 'It is interesting to note that in the preceding code we reduced the dimensions
    of the input from 784 to 128 and our network could still reconstruct the original
    image. This should give you an idea of the power of the autoencoder for dimensionality
    reduction. One advantage of autoencoders over PCA for dimensionality reduction
    is that while PCA can only represent linear transformations, we can use non-linear
    activation functions in autoencoders, thus introducing non-linearities in our
    encodings:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在前面的代码中，我们将输入的维度从784减少到128，并且网络仍然能够重建原始图像。这应该能给你一些关于自编码器在降维方面强大功能的启示。自编码器相较于PCA在降维方面的一个优势是，PCA只能表示线性变换，而我们可以在自编码器中使用非线性激活函数，从而在编码中引入非线性：
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_08_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 描述自动生成](img/B18331_08_05.png)'
- en: 'Figure 8.5: LHS image: The two-dimensional code for 500 digits of each class
    produced by taking the first two principal components of all 60,000 training samples.
    RHS image: The two-dimensional code found by a 784-500-2 autoencoder'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：左侧图：通过取所有60,000个训练样本的前两个主成分得到的每个类别500个数字的二维编码。右侧图：784-500-2自编码器找到的二维编码
- en: '*Figure 8.5* compares the result of a PCA with that of stacked autoencoders
    with architecture consisting of 784-500-2 (here the numbers represent the size
    of the encoder layers in each autoencoder; the autoencoders had a symmetric decoder).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.5* 比较了PCA和堆叠自编码器的结果，堆叠自编码器的结构为784-500-2（这里的数字代表每个自编码器中编码器层的大小；这些自编码器具有对称的解码器）。'
- en: You can see that the colored dots on the right are nicely separated, thus stacked
    autoencoders give much better results compared to PCA. Now that you are familiar
    with vanilla autoencoders, let us see different variants of autoencoders and their
    implementation details.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到右侧的彩色点被很好地分开，因此，堆叠自编码器相比PCA给出了更好的结果。现在你已经熟悉了基础自编码器，接下来让我们看看不同类型的自编码器及其实现细节。
- en: Sparse autoencoder
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: The autoencoder we covered in the previous section works more like an identity
    network; it simply reconstructs the input. The emphasis is on reconstructing the
    image at the pixel level, and the only constraint is the number of units in the
    bottleneck layer. While it is interesting, pixel-level reconstruction is primarily
    a compression mechanism and does not necessarily ensure that the network will
    learn abstract features from the dataset. We can ensure that a network learns
    abstract features from the dataset by adding further constraints.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中讨论的自编码器更像是一个恒等网络；它只是简单地重建输入。重点在于在像素级别重建图像，唯一的约束是瓶颈层中的单元数。虽然像素级重建很有趣，但它主要是一个压缩机制，并不一定确保网络从数据集中学习到抽象特征。我们可以通过增加进一步的约束来确保网络从数据集中学习到抽象特征。
- en: In sparse autoencoders, a sparse penalty term is added to the reconstruction
    error. This tries to ensure that fewer units in the bottleneck layer will fire
    at any given time. We can include the sparse penalty within the encoder layer
    itself.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏自编码器中，增加了一个稀疏惩罚项到重建误差中。这会确保瓶颈层中在任何给定时间点只有较少的单元被激活。我们可以将稀疏惩罚项加入到编码器层中。
- en: 'In the following code, you can see that the dense layer of `Encoder` now has
    an additional parameter, `activity_regularizer`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，你可以看到`Encoder`的稠密层现在增加了一个额外的参数`activity_regularizer`：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The activity regularizer tries to reduce the layer output (refer to *Chapter
    1*, *Neural Network Foundations with TF*). It will reduce both the weights and
    bias of the fully connected layer to ensure that the output is as small as it
    can be. TensorFlow supports three types of `activity_regularizer`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 活动正则化器尝试减少层的输出（参考*第1章*，*TF中的神经网络基础*）。它将减少全连接层的权重和偏差，以确保输出尽可能小。TensorFlow支持三种类型的`activity_regularizer`：
- en: '`l1`: Here the activity is computed as the sum of absolute values'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1`：这里的活动计算为绝对值的和。'
- en: '`l2`: The activity here is calculated as the sum of the squared values'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2`：这里的活动计算为平方值的和。'
- en: '`l1_l2`: This includes both L1 and L2 terms'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_l2`：这包括L1和L2项。'
- en: Keeping the rest of the code the same, and just changing the encoder, you can
    get the sparse autoencoder from the vanilla autoencoder. The complete code for
    the sparse autoencoder is in the Jupyter notebook `SparseAutoencoder.ipynb`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 保持代码其他部分不变，仅更改编码器，你可以从基础自编码器得到稀疏自编码器。稀疏自编码器的完整代码在Jupyter笔记本`SparseAutoencoder.ipynb`中。
- en: 'Alternatively, you can explicitly add a regularization term for sparsity in
    the loss function. To do so you will need to implement the regularization for
    the sparsity term as a function. If *m* is the total number of input patterns,
    then we can define a quantity ![](img/B18331_08_001.png) (you can check the mathematical
    details in Andrew Ng’s lecture here: [https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)),
    which measures the net activity (how many times on average it fires) for each
    hidden layer unit. The basic idea is to put a constraint ![](img/B18331_08_001.png),
    such that it is equal to the sparsity parameter ![](img/B18331_08_003.png). This
    results in adding a regularization term for sparsity in the loss function so that
    now the loss function becomes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以在损失函数中显式地添加一个稀疏正则化项。为此，你需要将稀疏项的正则化实现为一个函数。如果*m*是输入模式的总数，那么我们可以定义一个量![](img/B18331_08_001.png)（你可以在Andrew
    Ng的讲座中查看数学细节：[https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)），它衡量每个隐藏层单元的净活动（即它平均多少次激活）。基本思想是设定一个约束![](img/B18331_08_001.png)，使其等于稀疏参数![](img/B18331_08_003.png)。这样就会在损失函数中添加一个稀疏正则化项，使得损失函数变为：
- en: '*loss = Mean squared error + Regularization for sparsity parameter*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失 = 均方误差 + 稀疏参数的正则化*'
- en: 'This regularization term will penalize the network if ![](img/B18331_08_001.png)
    deviates from ![](img/B18331_08_003.png). One standard way to do this is to use
    **Kullback-Leiber** (**KL**) divergence (you can learn more about KL divergence
    from this interesting lecture: [https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf](https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf))
    between ![](img/B18331_08_003.png) and ![](img/B18331_08_001.png).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则化项会惩罚网络，如果![](img/B18331_08_001.png)偏离![](img/B18331_08_003.png)。一种标准的做法是使用**Kullback-Leiber**（**KL**）散度（你可以通过这场有趣的讲座了解更多关于KL散度的内容：[https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf](https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf)），来计算![](img/B18331_08_003.png)和![](img/B18331_08_001.png)之间的差异。
- en: 'Let’s explore the KL divergence, *D*[KL], a little more. It is a non-symmetric
    measure of the difference between the two distributions, in our case, ![](img/B18331_08_003.png)
    and ![](img/B18331_08_001.png). When ![](img/B18331_08_003.png) and ![](img/B18331_08_001.png)
    are equal then the difference is zero; otherwise, it increases monotonically as
    ![](img/B18331_08_001.png) diverges from ![](img/B18331_08_003.png). Mathematically,
    it is expressed as:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨一下KL散度，*D*[KL]。它是一个非对称的度量，用于衡量两个分布之间的差异，在我们这个例子中，指的是![](img/B18331_08_003.png)和![](img/B18331_08_001.png)之间的差异。当![](img/B18331_08_003.png)和![](img/B18331_08_001.png)相等时，差异为零；否则，当![](img/B18331_08_001.png)偏离![](img/B18331_08_003.png)时，差异会单调增加。数学表达式为：
- en: '![](img/B18331_08_014.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_08_014.png)'
- en: We add this to the loss to implicitly include the sparse term. We will need
    to fix a constant value for the sparsity term ![](img/B18331_08_003.png) and compute
    ![](img/B18331_08_001.png) using the encoder output.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其添加到损失函数中，以隐式地包含稀疏项。我们需要为稀疏项![](img/B18331_08_003.png)设定一个常数值，并使用编码器输出计算![](img/B18331_08_001.png)。
- en: The compact representation of the inputs is stored in weights. Let us visualize
    the weights learned by the network. The following are the weights of the encoder
    layer for the standard and sparse autoencoders respectively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的紧凑表示存储在权重中。让我们可视化网络学习到的权重。以下是标准自编码器和稀疏自编码器的编码器层权重。
- en: 'We can see that in the standard autoencoder (a) many hidden units have very
    large weights (brighter), suggesting that they are overworked, while all the hidden
    units of the sparse autoencoder (b) learn the input representation almost equally,
    and we see a more even color distribution:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在标准自编码器（a）中，许多隐藏单元具有非常大的权重（较亮），这表明它们被过度使用，而稀疏自编码器（b）的所有隐藏单元几乎均等地学习输入表示，我们看到颜色分布更加均匀：
- en: '![A picture containing appliance, grate  Description automatically generated](img/B18331_08_06.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing appliance, grate  Description automatically generated](img/B18331_08_06.png)'
- en: 'Figure 8.6: Encoder weight matrix for (a) standard autoencoder and (b) sparse
    autoencoder'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：编码器权重矩阵（a）标准自编码器和（b）稀疏自编码器
- en: Now that we have learned about sparse autoencoders, we next move to a case where
    autoencoders can learn to remove noise from the image.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了稀疏自编码器，接下来我们转向自编码器能够从图像中去除噪声的案例。
- en: Denoising autoencoders
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: The two autoencoders that we have covered in the previous sections are examples
    of undercomplete autoencoders, because the hidden layer in them has lower dimensionality
    compared to the input (output) layer. Denoising autoencoders belong to the class
    of overcomplete autoencoders because they work better when the dimensions of the
    hidden layer are more than the input layer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的部分中讨论的两个自编码器是欠完备自编码器的例子，因为它们的隐藏层相比输入（输出）层具有较低的维度。去噪自编码器属于完备自编码器类别，因为当隐藏层的维度大于输入层时，它们的效果更好。
- en: A denoising autoencoder learns from a corrupted (noisy) input; it feeds its
    encoder network the noisy input, and then the reconstructed image from the decoder
    is compared with the original input. The idea is that this will help the network
    learn how to denoise an input. It will no longer just make pixel-wise comparisons,
    but in order to denoise, it will learn the information of neighboring pixels as
    well.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器从受损（噪声）输入中学习；它将噪声输入传递给编码器网络，然后解码器重建的图像与原始输入进行比较。其目的是帮助网络学习如何去噪输入。它将不再仅仅进行像素级的比较，而是为了去噪，它将学习邻近像素的信息。
- en: 'A denoising autoencoder has two main differences from other autoencoders: first,
    `n_hidden`, the number of hidden units in the bottleneck layer is greater than
    the number of units in the input layer, `m`, that is, `n_hidden` > `m`. Second,
    the input to the encoder is corrupted input.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器与其他自编码器的主要区别有两点：首先，`n_hidden`，即瓶颈层中的隐藏单元数大于输入层中的单元数`m`，即`n_hidden` > `m`。其次，编码器的输入是受损的输入。
- en: 'To do this, we add a noise term in both the test and training images:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们在测试和训练图像中添加噪声项：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let us see the denoising autoencoder in action next.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们看看去噪自编码器的实际操作。
- en: Clearing images using a denoising autoencoder
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用去噪自编码器清除图像
- en: 'Let us use the denoising autoencoder to clear the handwritten MNIST digits:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用去噪自编码器来清除手写的 MNIST 数字：
- en: 'We start by importing the required modules:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的模块：
- en: '[PRE14]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we define the hyperparameters for our model:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型的超参数：
- en: '[PRE15]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We read in the MNIST dataset, normalize it, and introduce noise to it:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取 MNIST 数据集，对其进行归一化处理，并添加噪声：
- en: '[PRE16]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We use the same encoder, decoder, and autoencoder classes as defined in the
    *Vanilla autoencoders* section:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用与*经典自编码器*部分中定义的相同的编码器、解码器和自编码器类：
- en: '[PRE17]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we create the model and define the loss and optimizers to be used. Notice
    that this time, instead of writing the custom training loop, we are using the
    easier Keras inbuilt `compile()` and `fit()` methods:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建模型并定义损失函数和优化器。请注意，这次我们使用的是更简便的 Keras 内建 `compile()` 和 `fit()` 方法，而不是编写自定义训练循环：
- en: '[PRE18]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s plot the training loss:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们绘制训练损失图：
- en: '[PRE19]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 8.7* shows the loss over epochs:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.7* 显示了各个时代的损失：'
- en: '![Chart, histogram  Description automatically generated](img/B18331_08_07.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, histogram  Description automatically generated](img/B18331_08_07.png)'
- en: 'Figure 8.7: Loss plot of a denoising autoencoder'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：去噪自编码器的损失图
- en: 'And finally, let’s see our model in action:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看我们的模型实际操作：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The top row shows the input noisy image, and the bottom row shows cleaned images
    produced from our trained denoising autoencoder:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 顶排显示的是输入的噪声图像，底排显示的是我们训练后的去噪自编码器生成的清晰图像：
- en: '![A picture containing text  Description automatically generated](img/B18331_08_08.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text  Description automatically generated](img/B18331_08_08.png)'
- en: 'Figure 8.8: The noisy input images and corresponding denoised reconstructed
    images'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：噪声输入图像和对应的去噪重建图像
- en: An impressive reconstruction of images from noisy images, I’m sure you’ll agree.
    You can access the code in the notebook `DenoisingAutoencoder.ipynb` if you want
    to play around with it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从噪声图像中重建图像的效果令人印象深刻，相信你会同意。如果你想尝试一下，你可以在笔记本`DenoisingAutoencoder.ipynb`中访问代码。
- en: Stacked autoencoder
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠自编码器
- en: Until now, we have restricted ourselves to autoencoders with only one hidden
    layer. We can build deep autoencoders by stacking many layers of both encoders
    and decoders; such an autoencoder is called a stacked autoencoder. The features
    extracted by one encoder are passed on to the next encoder as input. The stacked
    autoencoder can be trained as a whole network with the aim of minimizing the reconstruction
    error. Alternatively, each individual encoder/decoder network can first be pretrained
    using the unsupervised method you learned earlier, and then the complete network
    can be fine-tuned. When the deep autoencoder network is a convolutional network,
    we call it a **convolutional autoencoder**. Let us implement a convolutional autoencoder
    in TensorFlow next.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只限制自己使用仅有一个隐藏层的自编码器。我们可以通过堆叠多个编码器和解码器层来构建深度自编码器；这种自编码器称为堆叠自编码器。一个编码器提取的特征会作为输入传递给下一个编码器。堆叠自编码器可以作为一个整体网络进行训练，目标是最小化重建误差。或者，每个单独的编码器/解码器网络可以先使用你之前学过的无监督方法进行预训练，然后对整个网络进行微调。当深度自编码器网络是卷积网络时，我们称之为**卷积自编码器**。接下来，我们将在TensorFlow中实现一个卷积自编码器。
- en: Convolutional autoencoder for removing noise from images
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于去除图像噪声的卷积自编码器
- en: 'In the previous section, we reconstructed handwritten digits from noisy input
    images. We used a fully connected network as the encoder and decoder for the work.
    However, we know that for images, a convolutional network can give better results,
    so in this section, we will use a convolution network for both the encoder and
    decoder. To get better results we will use multiple convolution layers in both
    the encoder and decoder networks; that is, we will make stacks of convolutional
    layers (along with max pooling or upsampling layers). We will also be training
    the entire autoencoder as a single entity:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们从噪声输入图像中重建了手写数字。我们使用了一个全连接网络作为编码器和解码器。然而，我们知道，对于图像来说，卷积网络能够提供更好的结果，所以在本节中，我们将使用卷积网络作为编码器和解码器。为了获得更好的结果，我们将在编码器和解码器网络中使用多个卷积层；也就是说，我们将堆叠卷积层（以及最大池化或上采样层）。我们还将训练整个自编码器作为一个整体：
- en: 'We import all the required modules and the specific layers from `tensorflow.keras.layers`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`tensorflow.keras.layers`导入所有必需的模块和特定层：
- en: '[PRE21]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We specify our hyperparameters. If you look carefully, the list is slightly
    different compared to earlier autoencoder implementations; instead of learning
    rate and momentum, this time we are concerned with filters of the convolutional
    layer:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定了超参数。如果你仔细观察，你会发现这个列表与早期的自编码器实现略有不同；这次我们关注的是卷积层的过滤器，而不是学习率和动量：
- en: '[PRE22]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the next step, we read in the data and preprocess it. Again, you may observe
    a slight variation from the previous code, especially in the way we are adding
    noise and then limiting the range between [0-1]. We are doing so because in this
    case, instead of the mean squared error loss, we will be using binary cross-entropy
    loss and the final output of the decoder will pass through sigmoid activation,
    restricting it between [0-1]:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们读取数据并进行预处理。同样，你可能会注意到与之前的代码相比有一些微小的变化，尤其是在添加噪声和将值限制在[0-1]之间的方式上。我们这样做是因为在这种情况下，我们将使用二元交叉熵损失，而不是均方误差损失，并且解码器的最终输出将通过sigmoid激活，将其限制在[0-1]之间：
- en: '[PRE23]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let us now define our encoder. The encoder consists of three convolutional
    layers, each followed by a max pooling layer. Since we are using the MNIST dataset
    the shape of the input image is 28 × 28 (single channel) and the output image
    is of size 4 × 4 (and since the last convolutional layer has 16 filters, the image
    has 16 channels):'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来定义编码器。编码器由三层卷积层组成，每一层后跟一个最大池化层。由于我们使用的是MNIST数据集，输入图像的形状为28 × 28（单通道），而输出图像的大小为4
    × 4（由于最后一层卷积层有16个滤波器，图像有16个通道）：
- en: '[PRE24]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next comes the decoder. It is the exact opposite of the encoder in design,
    and instead of max pooling, we are using upsampling to increase the size back.
    Notice the commented `print` statements; you can use them to understand how the
    shape gets modified after each step. (Alternatively, you can also use the `model.summary`
    function to get the complete model summary.) Also notice that both the encoder
    and decoder are still classes based on the TensorFlow Keras `Layers` class, but
    now they have multiple layers inside them. So now you know how to build a complex
    custom layer:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是解码器。它在设计上与编码器完全相反，且我们不使用最大池化，而是使用上采样来恢复尺寸。注意那些被注释掉的`print`语句；你可以通过它们来理解每一步之后形状是如何变化的。（或者，你也可以使用`model.summary`函数来获取完整的模型概述。）另外需要注意的是，编码器和解码器依然是基于TensorFlow
    Keras的`Layers`类，但现在它们内部有多个层。现在你知道如何构建一个复杂的自定义层了：
- en: '[PRE25]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We combine the encoder and decoder to make an autoencoder model. This remains
    exactly the same as before:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将编码器和解码器结合起来，构建一个自动编码器模型。这与之前完全相同：
- en: '[PRE26]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we instantiate our model, then specify the binary cross-entropy as the
    loss function and Adam as the optimizer in the `compile()` method. Then, fit the
    model to the training dataset:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们实例化我们的模型，然后在`compile()`方法中指定二元交叉熵作为损失函数，Adam作为优化器。接着，使用训练数据集来训练模型：
- en: '[PRE27]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the loss curve:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制损失曲线：
- en: '[PRE28]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can see the loss curve as the model is trained; in 50 epochs the loss was
    reduced to 0.0988:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到训练过程中的损失曲线；在50个epoch后，损失降到了0.0988：
- en: '![Chart  Description automatically generated](img/B18331_08_09.png)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图表  描述自动生成](img/B18331_08_09.png)'
- en: 'Figure 8.9: Loss plot for the convolutional autoencoder'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.9：卷积自动编码器的损失图
- en: 'And finally, you can see the wonderful reconstructed images from the noisy
    input images:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以看到从噪声输入图像重建出的精彩图像：
- en: '[PRE29]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![A picture containing text  Description automatically generated](img/B18331_08_10.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片  描述自动生成](img/B18331_08_10.png)'
- en: 'Figure 8.10: The inputted noisy images and reconstructed denoised images'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：输入的噪声图像和重建的去噪图像
- en: You can see that the images are much clearer and sharper relative to the previous
    autoencoders we have covered in this chapter. The magic lies in the stacking of
    convolutional layers. The code for this section is available in the Jupyter notebook
    `ConvolutionAutoencoder.ipynb`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，与本章前面介绍的自动编码器相比，这些图像要清晰得多。魔法就在于卷积层的堆叠。该部分的代码可以在Jupyter笔记本`ConvolutionAutoencoder.ipynb`中找到。
- en: A TensorFlow Keras autoencoder example ‒ sentence vectors
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个TensorFlow Keras自动编码器示例 ‒ 句子向量
- en: In this example, we will build and train an LSTM-based autoencoder to generate
    sentence vectors for documents in the Reuters-21578 corpus ([https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection)).
    We have already seen in *Chapter 4*, *Word Embeddings*, how to represent a word
    using word embeddings to create vectors that represent the word’s meaning in the
    context of other words it appears with. Here, we will see how to build similar
    vectors for sentences. Sentences are sequences of words, so a sentence vector
    represents the meaning of a sentence.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将构建并训练一个基于LSTM的自动编码器，以为Reuters-21578语料库中的文档生成句子向量（[https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection)）。我们已经在*第4章*，*词嵌入*中看到过如何使用词嵌入表示一个单词，从而创建代表该单词在其出现的其他单词上下文中意义的向量。在这里，我们将看到如何为句子构建类似的向量。句子是单词的序列，因此句子向量表示句子的含义。
- en: The easiest way to build a sentence vector is to just add up the word vectors
    and divide them by the number of words. However, this treats the sentence as a
    bag of words, and does not take the order of words into account. Thus, the sentences
    *The dog bit the man* and *The man bit the dog* would be treated as identical
    in this scenario. LSTMs are designed to work with sequence input and do take the
    order of words into consideration, thus providing a better and more natural representation
    of the sentence.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 构建句子向量的最简单方法是将所有单词向量相加，并除以单词的数量。然而，这种方法将句子视为一个单词袋，并没有考虑单词的顺序。因此，句子*The dog bit
    the man*和*The man bit the dog*在这种情况下会被视为相同。LSTM被设计为处理序列输入，并考虑单词顺序，因此可以提供更好且更自然的句子表示。
- en: 'First, we import the necessary libraries:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In case you are using Google’s Colab to run the code, you will also need to
    unzip the Reuters corpus by adding the following to the code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Google的Colab运行代码，还需要通过向代码中添加以下内容来解压Reuters语料库：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will be using the GloVe embeddings, so let us download them as well:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用GloVe嵌入，因此我们也需要下载它们：
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that all our tools are in our workspace, we will first convert each block
    of text (documents) into a list of sentences, one sentence per line. Also, each
    word in the sentence is normalized as it is added. The normalization involves
    removing all numbers and replacing them with the number `9`, then converting the
    word to lowercase. Simultaneously we also calculate the word frequencies in the
    same code. The result is the word frequency table, `word_freqs`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有工具都已准备好，我们将首先将每个文本块（文档）转换为一个句子列表，每行一个句子。同时，每个句子的单词在添加时都会进行规范化处理。规范化过程包括去除所有数字并将其替换为数字`9`，然后将单词转换为小写。同时，我们也在同一段代码中计算单词的频率。结果是单词频率表`word_freqs`：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let us use the preceding generated arrays to get some information about the
    corpus that will help us figure out good values for the constants for our LSTM
    network:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前生成的数组来获取一些关于语料库的信息，这将帮助我们确定LSTM网络的常数值：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This gives us the following information about the corpus:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了关于语料库的以下信息：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Based on this information, we set the following constants for our LSTM model.
    We choose our `VOCAB_SIZE` as `5000`; that is, our vocabulary covers the most
    frequent 5,000 words, which covers over 93% of the words used in the corpus. The
    remaining words are treated as **out of vocabulary** (**OOV**) and replaced with
    the token `UNK`. At prediction time, any word that the model hasn’t seen will
    also be assigned the token `UNK`. `SEQUENCE_LEN` is set to approximately half
    the median length of sentences in the training set. Sentences that are shorter
    than `SEQUENCE_LEN` will be padded by a special `PAD` character, and those that
    are longer will be truncated to fit the limit:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些信息，我们为LSTM模型设置了以下常数。我们将`VOCAB_SIZE`设为`5000`；也就是说，我们的词汇表覆盖了最常用的5,000个单词，涵盖了语料库中超过93%的单词。其余的单词被视为**词汇表外**（**OOV**）并替换为`UNK`标记。在预测时，任何模型未见过的单词也将被分配`UNK`标记。`SEQUENCE_LEN`大约设为训练集中文本句子的中位数长度的一半。长度小于`SEQUENCE_LEN`的句子将通过特殊的`PAD`字符进行填充，长度超过的句子将被截断以符合限制：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Since the input to our LSTM will be numeric, we need to build lookup tables
    that go back and forth between words and word IDs. Since we limit our vocabulary
    size to 5,000 and we have to add the two pseudo-words `PAD` and `UNK`, our lookup
    table contains entries for the most frequently occurring 4,998 words plus `PAD`
    and `UNK`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的LSTM输入是数值型的，因此我们需要构建查找表，以便在单词和单词ID之间进行转换。由于我们将词汇表的大小限制为5,000，并且需要添加两个伪单词`PAD`和`UNK`，因此我们的查找表包含了最常出现的4,998个单词以及`PAD`和`UNK`：
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The input to our network is a sequence of words, where each word is represented
    by a vector. Simplistically, we could just use one-hot encoding for each word,
    but that makes the input data very large. So, we encode each word using its 50-dimensional
    GloVe embeddings.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们网络的输入是一个单词序列，其中每个单词由一个向量表示。简单来说，我们可以使用每个单词的独热编码（one-hot encoding），但那样会使输入数据非常庞大。因此，我们使用每个单词的50维GloVe嵌入来编码每个单词。
- en: 'The embedding is generated into a matrix of shape (`VOCAB_SIZE` and `EMBED_SIZE`)
    where each row represents the GloVe embedding for a word in our vocabulary. The
    `PAD` and `UNK` rows (`0` and `1` respectively) are populated with zeros and random
    uniform values respectively:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量被生成到一个形状为（`VOCAB_SIZE`，`EMBED_SIZE`）的矩阵中，其中每一行表示我们词汇表中某个单词的GloVe嵌入向量。`PAD`和`UNK`行（分别为`0`和`1`）分别用零和随机均匀值填充：
- en: '[PRE38]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we use these functions to generate embeddings:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用这些函数生成嵌入向量：
- en: '[PRE39]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Our autoencoder model takes a sequence of GloVe word vectors and learns to produce
    another sequence that is similar to the input sequence. The encoder LSTM compresses
    the sequence into a fixed-size context vector, which the decoder LSTM uses to
    reconstruct the original sequence.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自动编码器模型接收一个GloVe词向量序列，并学习生成一个与输入序列相似的输出序列。编码器LSTM将序列压缩成一个固定大小的上下文向量，解码器LSTM则使用这个向量重建原始序列。
- en: 'A schematic of the network is shown here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的示意图如下所示：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B18331_08_11.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图  描述自动生成](img/B18331_08_11.png)'
- en: 'Figure 8.11: Visualization of the LSTM network'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：LSTM网络的可视化
- en: 'Because the input is quite large, we will use a generator to produce each batch
    of input. Our generator produces batches of tensors of shape (`BATCH_SIZE`, `SEQUENCE_LEN`,
    `EMBED_SIZE`). Here `BATCH_SIZE` is `64`, and since we are using 50-dimensional
    GloVe vectors, `EMBED_SIZE` is `50`. We shuffle the sentences at the beginning
    of each epoch and return batches of 64 sentences. Each sentence is represented
    as a vector of GloVe word vectors. If a word in the vocabulary does not have a
    corresponding GloVe embedding, it is represented by a zero vector. We construct
    two instances of the generator, one for training data and one for test data, consisting
    of 70% and 30% of the original dataset respectively:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入数据量相当大，我们将使用生成器来生成每一批次的输入。我们的生成器生成形状为（`BATCH_SIZE`，`SEQUENCE_LEN`，`EMBED_SIZE`）的张量批次。这里，`BATCH_SIZE`
    是 `64`，并且由于我们使用的是50维的GloVe向量，所以 `EMBED_SIZE` 是 `50`。我们在每个训练周期开始时对句子进行洗牌，并返回64个句子的批次。每个句子都被表示为一个由GloVe词向量组成的向量。如果词汇表中的某个单词没有对应的GloVe嵌入向量，它将用零向量表示。我们构建了两个生成器实例，一个用于训练数据，一个用于测试数据，分别包含原始数据集的70%和30%：
- en: '[PRE40]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now we are ready to define the autoencoder. As we have shown in the diagram,
    it is composed of an encoder LSTM and a decoder LSTM. The encoder LSTM reads a
    tensor of shape (`BATCH_SIZE`, `SEQUENCE_LEN`, `EMBED_SIZE`) representing a batch
    of sentences. Each sentence is represented as a padded fixed-length sequence of
    words of size `SEQUENCE_LEN`. Each word is represented as a 300-dimensional GloVe
    vector. The output dimension of the encoder LSTM is a hyperparameter, `LATENT_SIZE`,
    which is the size of the sentence vector that will come from the encoder part
    of the trained autoencoder later. The vector space of dimensionality `LATENT_SIZE`
    represents the latent space that encodes the meaning of the sentence. The output
    of the LSTM is a vector of size (`LATENT_SIZE`) for each sentence, so for the
    batch, the shape of the output tensor is (`BATCH_SIZE`, `LATENT_SIZE`). This is
    now fed to a `RepeatVector` layer, which replicates this across the entire sequence;
    that is, the output tensor from this layer has the shape (`BATCH_SIZE`, `SEQUENCE_LEN`,
    `LATENT_SIZE`). This tensor is now fed into the decoder LSTM, whose output dimension
    is the `EMBED_SIZE`, so the output tensor has shape (`BATCH_SIZE`, `SEQUENCE_LEN`,
    `EMBED_SIZE`), that is, the same shape as the input tensor.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义自动编码器。如图所示，它由一个编码器LSTM和一个解码器LSTM组成。编码器LSTM读取形状为（`BATCH_SIZE`，`SEQUENCE_LEN`，`EMBED_SIZE`）的张量，表示一批句子。每个句子表示为一个固定长度的填充序列，大小为
    `SEQUENCE_LEN`。每个单词表示为一个300维的GloVe向量。编码器LSTM的输出维度是一个超参数，`LATENT_SIZE`，它是后续从训练好的自动编码器中获得的句子向量的大小。维度为
    `LATENT_SIZE` 的向量空间表示编码句子意义的潜在空间。LSTM的输出是每个句子的一个大小为（`LATENT_SIZE`）的向量，因此，对于整个批次，输出张量的形状为（`BATCH_SIZE`，`LATENT_SIZE`）。接下来，将这个输出传递给
    `RepeatVector` 层，该层会将其复制到整个序列中；也就是说，该层的输出张量形状为（`BATCH_SIZE`，`SEQUENCE_LEN`，`LATENT_SIZE`）。这个张量接着输入到解码器LSTM中，解码器LSTM的输出维度是
    `EMBED_SIZE`，所以输出张量的形状是（`BATCH_SIZE`，`SEQUENCE_LEN`，`EMBED_SIZE`），也就是与输入张量的形状相同。
- en: 'We compile this model with the Adam optimizer and the MSE loss function. The
    reason we use MSE is that we want to reconstruct a sentence that has a similar
    meaning, that is, something that is close to the original sentence in the embedded
    space of dimension `LATENT_SIZE`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Adam优化器和MSE损失函数来编译此模型。之所以使用MSE，是因为我们希望重建一个意义相似的句子，即在维度为`LATENT_SIZE`的嵌入空间中，尽可能接近原始句子：
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We define the loss function as mean squared error and choose the Adam optimizer:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将损失函数定义为均方误差，并选择Adam优化器：
- en: '[PRE42]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We train the autoencoder for 20 epochs using the following code. 20 epochs
    was chosen because the MSE loss converges within this time:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码训练自编码器20个周期。选择20个周期是因为MSE损失在此时间内收敛：
- en: '[PRE43]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The results of the training are shown as follows. The plot below shows the
    loss plot for both training and validation data; we can see that as our model
    learns, the losses decrease as expected:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结果如下所示。下图显示了训练和验证数据的损失图；我们可以看到，随着模型的学习，损失按预期减少：
- en: '![Chart, line chart  Description automatically generated](img/B18331_08_12.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  描述自动生成](img/B18331_08_12.png)'
- en: 'Figure 8.12: Loss plot of the LSTM autoencoder'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：LSTM自编码器的损失图
- en: Since we are feeding in a matrix of embeddings, the output will also be a matrix
    of word embeddings. Since the embedding space is continuous and our vocabulary
    is discrete, not every output embedding will correspond to a word. The best we
    can do is to find a word that is closest to the output embedding in order to reconstruct
    the original text. This is a bit cumbersome, so we will evaluate our autoencoder
    in a different way.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们输入的是一个嵌入矩阵，输出也将是一个词嵌入矩阵。由于嵌入空间是连续的，而我们的词汇表是离散的，并非每个输出嵌入都会对应一个词。我们能做的最好的事情是找到一个最接近输出嵌入的词，以便重建原始文本。这有点繁琐，所以我们将以不同的方式评估我们的自编码器。
- en: Since the objective of the autoencoder is to produce a good latent representation,
    we compare the latent vectors produced from the encoder using the original input
    versus the output of the autoencoder.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自编码器的目标是产生良好的潜在表示，我们比较了使用原始输入与自编码器输出所产生的潜在向量。
- en: 'First, we extract the encoder component into its own network:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编码器部分提取为独立的网络：
- en: '[PRE44]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Then we run the autoencoder on the test set to return the predicted embeddings.
    We then send both the input embedding and the predicted embedding through the
    encoder to produce sentence vectors from each and compare the two vectors using
    *cosine* similarity. Cosine similarities close to “one” indicate high similarity
    and those close to “zero” indicate low similarity.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在测试集上运行自编码器以返回预测的嵌入。我们将输入嵌入和预测嵌入通过编码器送入，以从每个向量生成句子向量，并使用*余弦*相似度比较这两个向量。接近“1”的余弦相似度表示高度相似，接近“0”的则表示相似度低。
- en: 'The following code runs against a random subset of 500 test sentences and produces
    some sample values of cosine similarities, between the sentence vectors generated
    from the source embedding and the corresponding target embedding produced by the
    autoencoder:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在500个随机测试句子的子集上运行，并生成一些余弦相似度样本值，这些值表示源嵌入生成的句子向量与自编码器生成的目标嵌入之间的相似度：
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The first 10 values of cosine similarities are shown as follows. As we can
    see, the vectors seem to be quite similar:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了前10个余弦相似度的值。正如我们所见，这些向量似乎非常相似：
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '*Figure 8.13* shows a histogram of the distribution of values of cosine similarities
    for the sentence vectors from the first 500 sentences.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.13* 显示了来自前500个句子的句子向量的余弦相似度分布直方图。'
- en: 'As previously mentioned, it confirms that the sentence vectors generated from
    the input and output of the autoencoder are very similar, showing that the resulting
    sentence vector is a good representation of the sentence:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这证实了从自编码器的输入和输出生成的句子向量非常相似，显示出生成的句子向量是对句子的良好表示：
- en: '![Chart, histogram  Description automatically generated](img/B18331_08_13.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图  描述自动生成](img/B18331_08_13.png)'
- en: 'Figure 8.13: Cosine similarity distribution'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13：余弦相似度分布
- en: Till now we have focused on autoencoders that can reconstruct data; in the next
    section, we will go through a slightly different variant of the autoencoder –
    the variational autoencoder, which is used to generate data.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于能够重建数据的自编码器；在接下来的部分，我们将介绍自编码器的一种稍微不同的变体——变分自编码器，它用于生成数据。
- en: Variational autoencoders
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: Like DBNs (*Chapter 7*, *Unsupervised Learning*) and GANs (see *Chapter 9*,
    *Generative Models*, for more details), variational autoencoders are also generative
    models. **Variational autoencoders** (**VAEs**) are a mix of the best neural networks
    and Bayesian inference. They are one of the most interesting neural networks and
    have emerged as one of the most popular approaches to unsupervised learning. They
    are autoencoders with a twist. Along with the conventional encoder and decoder
    network of autoencoders, they have additional stochastic layers. The stochastic
    layer, after the encoder network, samples the data using a Gaussian distribution,
    and the one after the decoder network samples the data using Bernoulli’s distribution.
    Like GANs, VAEs can be used to generate images and figures based on the distribution
    they have been trained on.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 像深度置信网络（DBNs，*第7章*，*无监督学习*）和生成对抗网络（GANs，参见*第9章*，*生成模型*，了解更多细节）一样，变分自编码器也是生成模型。**变分自编码器**（**VAEs**）是最佳神经网络与贝叶斯推断的结合体。它们是最有趣的神经网络之一，已成为无监督学习的最流行方法之一。它们是自编码器的一个变种。除了常规的编码器和解码器网络外，VAE还具有额外的随机层。随机层在编码器网络之后，使用高斯分布进行数据采样，而在解码器网络之后，使用伯努利分布进行数据采样。像GANs一样，VAE也可以用来生成基于其训练分布的图像和图形。
- en: 'VAEs allow one to set complex priors in the latent space and thus learn powerful
    latent representations. *Figure 8.14* describes a VAE:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）允许在潜在空间中设置复杂的先验，从而学习强大的潜在表示。*图 8.14* 描述了一个VAE：
- en: '![Diagram  Description automatically generated](img/B18331_08_14.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图示说明自动生成](img/B18331_08_14.png)'
- en: 'Figure 8.14: Architecture of a variational autoencoder'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：变分自编码器的架构
- en: The encoder network ![](img/B18331_08_017.png) approximates the true but intractable
    posterior distribution ![](img/B18331_08_018.png), where *x* is the input to the
    VAE and *z* is the latent representation. The decoder network ![](img/B18331_08_019.png)
    takes the *d*-dimensional latent variables (also called latent space) as its input
    and generates new images following the same distribution as *P*(*x*). As you can
    see from the preceding diagram, the latent representation *z* is sampled from
    ![](img/B18331_08_020.png), and the output of the decoder network samples ![](img/B18331_08_021.png)
    from ![](img/B18331_08_022.png). Here *N* represents a normal distribution with
    mean ![](img/B18331_08_023.png) and variance ![](img/B18331_01_017.png).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络！[](img/B18331_08_017.png)逼近真实但不可求解的后验分布！[](img/B18331_08_018.png)，其中 *x*
    是VAE的输入，*z* 是潜在表示。解码器网络！[](img/B18331_08_019.png) 以 *d* 维潜在变量（也叫潜在空间）作为输入，并生成符合
    *P*(*x*) 分布的新图像。正如从前面的图示中可以看到的，潜在表示 *z* 是从！[](img/B18331_08_020.png)中采样的，而解码器网络的输出从！[](img/B18331_08_021.png)
    中采样！[](img/B18331_08_022.png)。这里 *N* 表示具有均值！[](img/B18331_08_023.png) 和方差！[](img/B18331_01_017.png)的正态分布。
- en: 'Now that we have the basic architecture of VAEs, the question arises of how
    they can be trained, since the maximum likelihood of the training data and posterior
    density are intractable. The network is trained by maximizing the lower bound
    of the log data likelihood. Thus, the loss term consists of two components: generation
    loss, which is obtained from the decoder network through sampling, and the Kullback–Leibler
    divergence term, also called the latent loss.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了VAE的基本架构，问题就来了，如何训练它们，因为训练数据的最大似然和后验密度是不可求解的。网络通过最大化数据对数似然的下界来进行训练。因此，损失项由两个部分组成：生成损失，通过解码器网络的采样获得，以及Kullback-Leibler散度项，也叫潜在损失。
- en: Generation loss ensures that the image generated by the decoder and the image
    used to train the network are similar, and latent loss ensures that the posterior
    distribution ![](img/B18331_08_025.png) is close to the prior ![](img/B18331_08_026.png).
    Since the encoder uses Gaussian distribution for sampling, the latent loss measures
    how closely the latent variables match this distribution.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 生成损失确保解码器生成的图像与用于训练网络的图像相似，潜在损失确保后验分布！[](img/B18331_08_025.png)接近先验分布！[](img/B18331_08_026.png)。由于编码器使用高斯分布进行采样，潜在损失衡量潜在变量与该分布的匹配程度。
- en: Once the VAE is trained, we can use only the decoder network to generate new
    images. Let us try coding a VAE. This time we are using the Fashion-MNIST dataset;
    the dataset contains Zalando’s ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))
    article images. The test-train split is exactly the same as for MNIST, that is,
    60,000 train images and 10,000 test images. The size of each image is also 28
    × 28, so we can easily replace the code running on the MNIST dataset with the
    Fashion-MNIST dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦VAE训练完成，我们可以仅使用解码器网络来生成新图像。让我们尝试编写一个VAE。这次我们使用的是Fashion-MNIST数据集；该数据集包含了Zalando的([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))商品图像。训练和测试集的划分与MNIST完全相同，即60,000张训练图像和10,000张测试图像。每张图像的大小也是28
    × 28，因此我们可以轻松地将运行在MNIST数据集上的代码替换为Fashion-MNIST数据集的代码。
- en: 'The code in this section has been adapted from [https://github.com/dragen1860/TensorFlow-2.x-Tutorials](https://github.com/dragen1860/TensorFlow-2.x-Tutorials).
    As the first step we, as usual, import all the necessary libraries:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码改编自[https://github.com/dragen1860/TensorFlow-2.x-Tutorials](https://github.com/dragen1860/TensorFlow-2.x-Tutorials)。作为第一步，我们像往常一样导入所有必要的库：
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let us fix the seeds for a random number, so that the results are reproducible.
    We can also add an `assert` statement to ensure that our code runs on TensorFlow
    2.0 or above:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们固定随机数种子，以确保结果可复现。我们还可以添加一个`assert`语句，以确保我们的代码在TensorFlow 2.0或更高版本上运行：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Before going ahead with making the VAE, let us also explore the Fashion-MNIST
    dataset a little. The dataset is available in the TensorFlow Keras API:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续创建VAE之前，让我们先稍微了解一下Fashion-MNIST数据集。这个数据集可以在TensorFlow Keras API中找到：
- en: '[PRE49]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We see some sample images:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到一些示例图像：
- en: '[PRE51]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/B18331_08_15.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_08_15.png)'
- en: 'Figure 8.15: Sample images from the Fashion-MNIST dataset'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15：Fashion-MNIST数据集中的示例图像
- en: 'Before we start, let us declare some hyperparameters like learning rate, dimensions
    of the hidden layer and the latent space, batch size, epochs, and so on:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们声明一些超参数，如学习率、隐藏层和潜在空间的维度、批次大小、轮次等：
- en: '[PRE52]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We use the TensorFlow Keras Model API to build a VAE model. The `__init__()`
    function defines all the layers that we will be using:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用TensorFlow Keras Model API来构建VAE模型。`__init__()`函数定义了我们将使用的所有层：
- en: '[PRE53]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We define the functions to give us the encoder output and decoder output and
    reparametrize. The implementation of the encoder and decoder functions are straightforward;
    however, we need to delve a little deeper into the `reparametrize` function. As
    you know, VAEs sample from a random node *z*, which is approximated by ![](img/B18331_08_027.png)
    of the true posterior. Now, to get parameters we need to use backpropagation.
    However, backpropagation cannot work on random nodes. Using reparameterization,
    we can use a new parameter, `eps`, which allows us to reparametrize `z` in a way
    that will allow backpropagation through the deterministic random node ([https://arxiv.org/pdf/1312.6114v10.pdf](https://arxiv.org/pdf/1312.6114v10.pdf)):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了获取编码器输出、解码器输出和重新参数化的函数。编码器和解码器的实现都很直接；然而，我们需要深入探讨一下`reparametrize`函数。正如你所知道的，VAE从随机节点*z*中采样，这个*z*由真实后验的![](img/B18331_08_027.png)来近似。现在，为了获得参数，我们需要使用反向传播。然而，反向传播无法作用于随机节点。通过重新参数化，我们可以使用一个新参数`eps`，它允许我们以一种能够通过确定性随机节点进行反向传播的方式重新参数化*z*
    ([https://arxiv.org/pdf/1312.6114v10.pdf](https://arxiv.org/pdf/1312.6114v10.pdf))：
- en: '[PRE54]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Lastly, we define the `call()` function, which will control how signals move
    through different layers of the VAE:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了`call()`函数，它将控制信号如何在VAE的不同层之间传播：
- en: '[PRE55]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we create the VAE model and declare the optimizer for it. You can see
    the summary of the model:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建VAE模型并声明其优化器。你可以看到模型的摘要：
- en: '[PRE56]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we train the model. We define our loss function, which is the sum of the
    reconstruction loss and KL divergence loss:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们训练模型。我们定义了我们的损失函数，它是重构损失和KL散度损失的总和：
- en: '[PRE58]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Once the model is trained it should be able to generate images similar to the
    original Fashion-MNIST images. To do so we need to use only the decoder network,
    and we will pass to it a randomly generated *z* input:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，它应该能够生成与原始Fashion-MNIST图像相似的图像。为此，我们只需要使用解码器网络，并向其传递一个随机生成的*z*输入：
- en: '[PRE59]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '*Figure 8.16* shows the results after 80 epochs:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.16* 显示了80轮训练后的结果：'
- en: '![](img/B18331_08_16.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_08_16.png)'
- en: 'Figure 8.16: Results after 80 epochs'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16：80轮训练后的结果
- en: The generated images resemble the input space. The generated images are similar
    to the original Fashion-MNIST images as desired.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像与输入空间相似。生成的图像与原始Fashion-MNIST图像相似，符合预期。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we’ve had an extensive look at a new generation of deep learning
    models: autoencoders. We started with the vanilla autoencoder, and then moved
    on to its variants: sparse autoencoders, denoising autoencoders, stacked autoencoders,
    and convolutional autoencoders. We used the autoencoders to reconstruct images,
    and we also demonstrated how they can be used to clean noise from an image. Finally,
    the chapter demonstrated how autoencoders can be used to generate sentence vectors
    and images. The autoencoders learned through unsupervised learning.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们详细探讨了新一代深度学习模型：自编码器。我们从基础的自编码器开始，随后讨论了其变种：稀疏自编码器、去噪自编码器、堆叠自编码器和卷积自编码器。我们使用自编码器重建了图像，并演示了它们如何用来清除图像中的噪声。最后，本章展示了自编码器如何用于生成句子向量和图像。自编码器通过无监督学习进行学习。
- en: In the next chapter, we will delve deeper into generative adversarial networks,
    another interesting deep learning model that learns via an unsupervised learning
    paradigm.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨生成对抗网络，这是一种通过无监督学习范式学习的有趣深度学习模型。
- en: References
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). *Learning Internal
    Representations by Error Propagation*. No. ICS-8506\. University of California,
    San Diego. La Jolla Institute for Cognitive Science: [http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf](http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf)'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., 和 Williams, R. J. (1985). *通过误差传播学习内部表示*。No.
    ICS-8506。加利福尼亚大学圣地亚哥分校，认知科学研究所：[http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf](http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf)
- en: 'Hinton, G. E. and Salakhutdinov, R. R. (2016). *Reducing the dimensionality
    of data with neural networks*. science 313.5786: 504–507: [https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf)'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hinton, G. E. 和 Salakhutdinov, R. R. (2016). *利用神经网络减少数据的维度*。Science 313.5786:
    504–507：[https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf)'
- en: 'Masci, J. et al. (2011). *Stacked convolutional auto-encoders for hierarchical
    feature extraction*. Artificial Neural Networks and Machine Learning–ICANN 2011:
    52–59: [https://www.semanticscholar.org/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e](https://www.semanticscholar.org/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e)'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Masci, J. 等 (2011). *堆叠卷积自编码器用于层次化特征提取*。人工神经网络与机器学习–ICANN 2011: 52–59：[https://www.semanticscholar.org/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e](https://www.semanticscholar.org/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e)'
- en: 'Japkowicz, N., Myers, C., and Gluck, M. (1995). *A novelty detection approach
    to classification*. IJCAI. Vol: [https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf](https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf)'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Japkowicz, N., Myers, C., 和 Gluck, M. (1995). *一种用于分类的新颖性检测方法*。IJCAI。卷：[https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf](https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf)
- en: 'Sedhain, S. (2015). *AutoRec: Autoencoders Meet Collaborative Filtering*. Proceedings
    of the 24th International Conference on World Wide Web, ACM.'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sedhain, S. (2015). *AutoRec: 自编码器遇上协同过滤*。第24届国际万维网大会论文集，ACM。'
- en: Cheng, H. (2016). *Wide & Deep Learning for Recommender Systems*. Proceedings
    of the 1st Workshop on Deep Learning for Recommender Systems, ACM.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cheng, H. (2016). *宽深学习推荐系统*。第一届推荐系统深度学习研讨会论文集，ACM。
- en: Runfeldt, M. *Using Deep Learning to Remove Eyeglasses from Faces*.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Runfeldt, M. *使用深度学习从人脸中去除眼镜*。
- en: 'Miotto, R. (2016). *Deep Patient: An Unsupervised Representation to Predict
    the Future of Patients from the Electronic Health Records*. Scientific Reports.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Miotto, R. (2016). *深度病人：一种无监督表示方法，用于预测电子健康记录中病人的未来*。Scientific Reports。
- en: Kiros, R. (2015). *Skip-Thought Vectors*, Advances in Neural Information Processing
    Systems.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kiros, R. (2015). *跳跃思想向量*，神经信息处理系统进展。
- en: 'Kullback-Leibler divergence: [http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kullback-Leibler散度：[http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)
- en: 'Denoising autoencoders: [https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.xhtml](https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.xhtml)'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去噪自编码器：[https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.xhtml](https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.xhtml)
- en: Join our book’s Discord space
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过 2000 名成员一起学习，网址：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code18312172242788196872.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code18312172242788196872.png)'
