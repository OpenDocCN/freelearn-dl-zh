- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Supervised Fine-Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督微调
- en: '**Supervised Fine-Tuning** (**SFT**) is a crucial step in preparing LLMs for
    real-world applications. Following the initial pre-training phase, where an LLM
    learns to predict the next token in a sequence, SFT refines the model’s capabilities
    using carefully curated pairs of instructions and corresponding answers. This
    process serves two primary purposes: it teaches the model to understand and follow
    a specific chat format, effectively transforming it into a conversational agent,
    and it allows the model to adapt its broad knowledge base to excel in targeted
    tasks or specialized domains.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督微调**（**SFT**）是准备LLMs（大型语言模型）应用于现实世界的关键步骤。在LLM学习预测序列中下一个标记的初始预训练阶段之后，SFT通过精心挑选的指令和相应答案对，进一步细化模型的能力。这个过程有两个主要目的：它教会模型理解和遵循特定的聊天格式，有效地将其转变为对话代理，并允许模型将广泛的知识库适应于特定任务或专业领域。'
- en: The importance of SFT lies in its ability to bridge the gap between a model’s
    general language understanding and its practical utility. By exposing the model
    to examples of desired input-output patterns, SFT shapes the LLM’s behavior to
    align with specific goals, whether they involve task completion (such as summarization
    or translation) or domain expertise (like medical or legal knowledge). This tailored
    approach not only enhances the model’s performance in intended areas but also
    improves its ability to follow instructions and generate more relevant and coherent
    responses.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: SFT的重要性在于其能够弥合模型的一般语言理解和其实际效用之间的差距。通过向模型展示期望的输入输出模式示例，SFT塑造LLM的行为，使其与特定目标保持一致，无论是任务完成（如摘要或翻译）还是领域专业知识（如医学或法律知识）。这种定制方法不仅提高了模型在预期领域的性能，还提高了其遵循指令和生成更相关、更连贯响应的能力。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Creating a high-quality instruction dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建高质量的指令数据集
- en: SFT techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SFT技术
- en: Implementing fine-tuning in practice
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际操作中的微调实现
- en: By the end of this chapter, you will be able to create your own instruction
    datasets and efficiently fine-tune LLMs on them.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够创建自己的指令数据集，并高效地对LLMs进行微调。
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在GitHub上找到，链接为[https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering)。
- en: Creating an instruction dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建指令数据集
- en: In most use cases, creating an instruction dataset is the most difficult part
    of the fine-tuning process. This is due to multiple factors. Most use cases can
    be connected to raw text, but it is rare to find natural pairs of instructions
    and answers. This raw text needs to be transformed into a format that includes
    both instructions and answers. Moreover, the quality of the data is also crucial.
    Because of this, a lot of time is invested in manually checking and verifying
    individual samples. This careful review helps ensure that the dataset is accurate
    and useful for training the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数用例中，创建指令数据集是微调过程中最困难的部分。这是由于多个因素造成的。大多数用例可以与原始文本相关联，但找到自然指令和答案对的情况很少。这些原始文本需要转换成包含指令和答案的格式。此外，数据质量也非常关键。因此，大量的时间被投入到手动检查和验证单个样本中。这种仔细的审查有助于确保数据集的准确性和对模型训练的有用性。
- en: '![A diagram of a data flow  Description automatically generated](img/B31105_05_01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![数据流图  自动生成的描述](img/B31105_05_01.png)'
- en: Figure *5*.1 – Overview of the post-training data pipeline covered in this chapter
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图*5*.1 – 本章涵盖的培训后数据管道概述
- en: In this section, we will introduce a general framework to create your own instruction
    datasets, regardless of the final use case. We will then leverage the scraped
    data from *Chapter 3* and transform it into an instruction dataset. The different
    stages in our data generation pipeline are summarized in *Figure 5.1*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一个通用框架来创建自己的指令数据集，无论最终用途如何。然后，我们将利用来自*第3章*的抓取数据并将其转换为指令数据集。我们数据生成管道的不同阶段总结在*图5.1*中。
- en: General framework
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用框架
- en: 'Instruction datasets are defined as pairs of instructions and answers. The
    instructions are the inputs of the model, used as context during fine-tuning.
    The answers are the expected outputs of the model. During fine-tuning, you can
    choose to train the model on the instructions and answers, or on answers only.
    Pairs of instructions and answers follow a certain template. Some instruction
    templates, such as Alpaca, introduce additional fields like `inputs` and `system`.
    Both of them can be considered subfields of the `instruction` field. In this case,
    “inputs” contain the data the model needs to complete the instruction, and “system”
    is a meta-prompt to steer the general behavior of the model. Here is an example
    from the SlimOrca dataset, with “system” and “instruction”:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 指令数据集被定义为指令和答案的配对。指令是模型的输入，在微调期间用作上下文。答案是模型预期的输出。在微调期间，你可以选择在指令和答案上训练模型，或者只训练答案。指令和答案的配对遵循一定的模板。一些指令模板，如Alpaca，引入了额外的字段，如`inputs`和`system`。它们都可以被认为是`instruction`字段的子字段。在这种情况下，“inputs”包含模型完成指令所需的数据，“system”是一个元提示，用于引导模型的一般行为。以下是从SlimOrca数据集的一个示例，包括“system”和“instruction”：
- en: '| **System**You are a helpful assistant, who always provide explanation. Think
    like you are answering to a five year old. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **系统**你是一个有帮助的助手，总是提供解释。想象你是在对一个五岁的孩子回答。|'
- en: '| **Instruction**Concepts: building, shop, townWrite a sentence that includes
    all these words. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **指令**概念：建造、商店、城镇写一个包含所有这些词的句子。|'
- en: '| **Output**In our little town, there is a shop inside a big building where
    people go to buy their favorite toys and candies. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **输出**在我们的小镇里，有一个大建筑里的商店，人们可以去那里购买他们最喜欢的玩具和糖果。|'
- en: '*Table 5.1* – Example of sample from the Open-Orca/SlimOrca dataset'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.1* – Open-Orca/SlimOrca数据集的样本示例'
- en: This example illustrates how the “system” field is used to define specific behaviors
    for the model, such as being helpful, always providing explanations, and tailoring
    responses as if speaking to a five-year-old. The “instruction” field provides
    the necessary data (the concepts) and the task (constructing a sentence). The
    `output` field shows the expected answer, which, while not the only possible answer,
    represents a high-quality response.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了“系统”字段是如何用来定义模型的具体行为的，例如：提供帮助、始终提供解释，以及像与五岁孩子说话一样定制响应。 “指令”字段提供了必要的数据（概念）和任务（构建句子）。`输出`字段显示了预期的答案，虽然这不是唯一的可能答案，但它代表了一个高质量的响应。
- en: 'To build an instruction dataset, we want to curate data that is representative
    of how the model will be used. Once we have gathered enough samples, our goal
    is to filter them to only keep high-quality data. In this context, high-quality
    data can be described through three main dimensions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个指令数据集，我们希望收集代表模型如何使用的代表性数据。一旦我们收集了足够的样本，我们的目标就是过滤它们，只保留高质量的数据。在这种情况下，高质量的数据可以通过三个主要维度来描述：
- en: '**Accuracy**: It refers to the factual correctness and relevance of the samples.
    In the context of instruction datasets, this means ensuring that responses are
    not only factually accurate but also relevant to their corresponding instructions.
    High accuracy is essential for training models that can provide reliable and trustworthy
    information.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：它指的是样本的事实正确性和相关性。在指令数据集的上下文中，这意味着确保响应不仅事实准确，而且与其对应的指令相关。高准确性对于训练能够提供可靠和可信信息的模型至关重要。'
- en: '**Diversity**: A high-quality dataset should encompass a wide range of use
    cases, covering the potential queries and tasks the deployed LLM might encounter.
    This diversity should span topics, contexts, text lengths, and writing styles.
    By sampling data in a representative manner, we allow models to develop robust
    instruction-following capabilities.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：一个高质量的数据集应该涵盖广泛的使用案例，包括部署的LLM可能遇到的潜在查询和任务。这种多样性应涵盖主题、上下文、文本长度和写作风格。通过以代表性的方式采样数据，我们允许模型发展强大的指令遵循能力。'
- en: '**Complexity**: Trivial or overly simplistic samples do little to improve an
    LLM’s capabilities. Instead, datasets should include complex, multi-step reasoning
    problems and challenging tasks that push the boundaries of what the model is expected
    to handle. This complexity helps in developing models capable of tackling complex
    real-world problems.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：简单或过于简单的样本对提高LLM的能力帮助不大。相反，数据集应包括复杂的多步骤推理问题和具有挑战性的任务，这些任务可以推动模型处理预期内容的边界。这种复杂性有助于开发能够解决复杂现实世界问题的模型。'
- en: In the following sections, we will see techniques to filter and evaluate instruction
    samples according to these dimensions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将看到根据这些维度过滤和评估指令样本的技术。
- en: Data quantity
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据量
- en: The Hugging Face Hub contains numerous instruction datasets, which can be general-purpose
    or designed for particular tasks or domains. When working on a new use case, it
    can be beneficial to look for related open-source datasets to leverage for fine-tuning.
    This is particularly important if your number of samples is too low (for example,
    fewer than 1,000), requiring you to augment it with high-quality data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub包含许多指令数据集，这些数据集可以是通用的，也可以是为特定任务或领域设计的。当处理一个新的用例时，寻找相关的开源数据集以用于微调可能会有所帮助。如果你的样本数量太少（例如，少于1,000个），这尤其重要，需要你用高质量的数据进行增强。
- en: '![A screenshot of a computer  Description automatically generated](img/B31105_05_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](img/B31105_05_02.png)'
- en: Figure *5*.2 – Screenshot of the most-liked datasets on the Hugging Face Hub
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *5*.2 – Hugging Face Hub上最受欢迎的数据集截图
- en: Calculating an ideal number of samples is a difficult task, as both the quality
    of the data and the size of the model can have a dramatic impact. For large models
    (around 70 billion parameters, for example), this number can be as low as 1,000
    high-quality samples (see the LIMA paper in the *References* section). This is
    not true for smaller models (around seven billion parameters, for instance), as
    they need more samples to simply learn the correct chat template. In any case,
    the quality of the data is a crucial factor, and a high number of samples is always
    desirable.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一个理想的样本数量是一项困难的任务，因为数据的质量和模型的大小都可能产生重大影响。对于大型模型（例如，约70亿参数），这个数量可以低至1,000个高质量样本（参见*参考文献*部分中的LIMA论文）。对于较小的模型（例如，约70亿参数），情况并非如此，因为它们需要更多的样本来简单地学习正确的聊天模板。在任何情况下，数据的质量都是一个关键因素，并且总是希望有大量的样本。
- en: 'To provide additional numbers, we can look at the fine-tuned models developed
    by companies and the open-source community. We can distinguish two types of finetunes:
    general-purpose, aimed to reproduce the capabilities of models like GPT, and task-
    or domain-specific models, designed to optimize their performance for a particular
    application.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更多的数据，我们可以查看公司和开源社区开发的微调模型。我们可以区分两种类型的微调：通用型，旨在重现GPT等模型的能力，以及针对特定任务或领域的模型，旨在优化特定应用的性能。
- en: General-purpose models cover more topics, which requires additional samples.
    Among companies, we observe a wide range of values. For instance, Yi models from
    01-ai rely on less than 10,000 samples. At the opposite range of the spectrum,
    Meta reported using 10 million samples for Llama 3 through the entire fine-tuning
    process (including preference alignment). In the open-source community, models
    like OpenHermes and Dolphin use around one million samples. Based on the quality
    of these finetunes, we recommend an instruction dataset of at least one million
    samples to create a good general-purpose instruct model. On the other hand, models
    fine-tuned for a specific purpose require fewer samples. Here, we differentiate
    task-specific models from domain-specific ones.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通用型模型覆盖更多主题，这需要额外的样本。在各个公司中，我们观察到广泛的数值范围。例如，01-ai的Yi模型依赖于不到10,000个样本。在光谱的另一端，Meta报告在整个微调过程中（包括偏好对齐）使用了1,000万个样本。在开源社区中，OpenHermes和Dolphin等模型使用大约一百万个样本。基于这些微调的质量，我们建议创建一个良好的通用指令模型至少需要一百万个样本。另一方面，针对特定目的微调的模型需要更少的样本。在这里，我们将特定任务模型与特定领域模型区分开来。
- en: Task-specific and domain-specific models represent two distinct approaches to
    fine-tuning LLMs. Task-specific models are designed to excel at a particular function,
    such as translation, summarization, or sentiment analysis. These models benefit
    from a focused training approach on a single task, allowing for efficient performance
    even with smaller model sizes (typically less than 8 billion parameters). The
    data required for task-specific fine-tuning is generally more manageable, ranging
    from 100 to 100,000 samples. This makes task-specific fine-tuning an attractive
    option for many applications where resources may be limited.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 特定任务和特定领域的模型代表了微调LLM的两种不同方法。特定任务模型旨在在特定功能上表现出色，例如翻译、摘要或情感分析。这些模型通过在单一任务上采用专注的训练方法而受益，即使在较小的模型尺寸（通常小于80亿参数）下也能实现高效性能。特定任务微调所需的数据通常更容易管理，从100到10万个样本不等。这使得特定任务微调成为许多资源可能有限的应用的吸引人选择。
- en: Domain-specific models, on the other hand, aim to tweak the LLM with specialized
    knowledge and familiarity with the vocabulary and linguistic patterns of a particular
    field. These models are valuable in areas such as medicine, law, finance, e-commerce,
    engineering, and hospitality. The data requirements for domain-specific fine-tuning
    can vary widely depending on the complexity and breadth of the domain. Some fields,
    like medicine or law, may require as much data as general-purpose fine-tuning
    due to their vast technical corpora. Others, such as e-commerce or hospitality,
    might need fewer samples, more in line with task-specific fine-tuning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，特定领域的模型旨在通过专业知识以及对该领域词汇和语言模式的熟悉来调整LLM。这些模型在医学、法律、金融、电子商务、工程和酒店业等领域非常有价值。特定领域微调的数据需求可能因领域的复杂性和广度而大相径庭。一些领域，如医学或法律，可能需要与通用目的微调一样多的数据，因为它们拥有庞大的技术语料库。其他领域，如电子商务或酒店业，可能需要更少的样本，这与特定任务的微调更为一致。
- en: The key factors determining the data needs for domain-specific models are the
    “size” of the domain (i.e., the extent of its specialized knowledge and vocabulary)
    and the representation of that domain in the model’s pre-training data. Domains
    that are well-represented in the original training data may require less fine-tuning,
    while those that are more specialized or underrepresented may need more extensive
    datasets. Even with open-source LLMs, many pre-training datasets are closed-source,
    which requires making educated guesses to determine their composition (e.g., 30%
    code or 20% math).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 决定特定领域模型数据需求的关键因素是领域的“大小”（即其专业知识和词汇的范围）以及该领域在模型预训练数据中的表示。在原始训练数据中表现良好的领域可能需要较少的微调，而那些更加专业或代表性不足的领域可能需要更广泛的数据集。即使对于开源的LLM，许多预训练数据集仍然是封闭的，这需要做出有根据的猜测来确定其组成（例如，30%的代码或20%的数学）。
- en: Data curation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据整理
- en: When it comes to procuring data for fine-tuning, the approaches differ between
    task-specific and domain-specific models. For task-specific models, data curation
    often involves collecting examples of the desired task from existing datasets
    or creating new ones. This might involve gathering pairs of original and summarized
    texts for a summarization model or collecting sentences in different languages
    for a translation model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到获取微调数据时，针对特定任务和特定领域的模型的方法不同。对于特定任务的模型，数据整理通常涉及从现有数据集中收集所需任务的示例或创建新的数据集。这可能包括为摘要模型收集原始文本和摘要文本的配对，或者为翻译模型收集不同语言的句子。
- en: Domain-specific data curation can be more challenging. It often requires collaboration
    with subject matter experts to gather and validate relevant texts, research papers,
    technical documents, and other domain-specific content. In some cases, it may
    involve partnering with organizations or institutions that have access to large
    repositories of specialized information. The quality and relevance of this data
    is crucial, as it directly impacts the model’s ability to understand and generate
    content in the target domain.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域的数据整理可能更具挑战性。它通常需要与领域专家合作收集和验证相关文本、研究论文、技术文档和其他特定领域内容。在某些情况下，可能涉及与拥有大量专业信息库的组织或机构合作。这些数据的质量和相关性至关重要，因为它直接影响模型在目标领域理解和生成内容的能力。
- en: It’s worth noting that few-shot prompting has emerged as an alternative strategy
    to fine-tuning, especially for task-specific applications. This approach leverages
    the capabilities of large, powerful models by providing a few examples of the
    desired task within the input prompt. While not a replacement for fine-tuning
    in all scenarios (e.g., when you want to learn a new domain), few-shot prompting
    can be an efficient way to adapt models to new tasks without the need for extensive
    additional training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，少样本提示已成为微调的替代策略，尤其是在特定任务应用中。这种方法通过在输入提示中提供所需任务的几个示例来利用大型、强大的模型的能力。虽然它不能在所有场景下替代微调（例如，当你想学习一个新领域时），但少样本提示可以是一种在没有需要大量额外训练的情况下适应新任务的效率方法。
- en: In practice, the line between task-specific and domain-specific models can sometimes
    blur. For instance, a model fine-tuned for medical diagnosis could be considered
    both task-specific (focused on diagnosis) and domain-specific (specialized in
    medical knowledge). The key is to understand the primary goal of the fine-tuning
    process and tailor the approach accordingly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，特定任务模型和特定领域模型之间的界限有时会变得模糊。例如，一个针对医疗诊断微调的模型可以被认为是既特定于任务（专注于诊断）又特定于领域（专门化于医学知识）。关键是理解微调过程的主要目标，并相应地调整方法。
- en: At this point in the process, we should have a collection of datasets suited
    for our use case. The next step consists of refining the quality of the samples
    through rule-based filtering, data duplication, data decontamination, and data
    quality evaluation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个处理阶段，我们应该有一组适合我们用例的数据集。下一步是通过基于规则的过滤、数据重复、数据净化和数据质量评估来提高样本的质量。
- en: Rule-based filtering
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的过滤
- en: Rule-based filtering is a systematic approach to data quality control that relies
    on explicit, predefined rules to evaluate and filter data samples. These rules
    are typically designed to address common quality issues and can range from simple
    checks to more complex logical operations. The primary goal of rule-based filtering
    is to maintain a high standard of data quality by removing samples that do not
    meet specific criteria.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的过滤是一种系统化的数据质量控制方法，它依赖于明确的、预定义的规则来评估和过滤数据样本。这些规则通常旨在解决常见的质量问题，范围从简单的检查到更复杂的逻辑运算。基于规则过滤的主要目标是通过对不符合特定标准的数据样本进行移除，保持高标准的数据质量。
- en: '**Length filtering** is a straightforward yet effective rule-based filtering
    technique. This method involves setting thresholds for the acceptable length of
    responses in the dataset. Extremely short responses often lack sufficient information
    to be meaningful, while excessively long ones may contain irrelevant or redundant
    content. It’s important to note that the appropriate length thresholds can vary
    significantly depending on the specific task and domain. For example, a dataset
    for generating concise summaries might have a lower maximum threshold compared
    to one for detailed explanations.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**长度过滤**是一种简单而有效的基于规则的过滤技术。这种方法涉及为数据集中可接受响应长度设置阈值。极短的响应通常缺乏足够的信息以有意义，而过长的响应可能包含无关或冗余的内容。重要的是要注意，适当的长度阈值可能因具体任务和领域而显著不同。例如，用于生成简洁摘要的数据集可能比用于详细解释的数据集具有更低的最高阈值。'
- en: '**Keyword exclusion** is another powerful rule-based filtering technique that
    focuses on the content of the samples rather than their structure. This method
    involves creating a list of keywords or phrases associated with low-quality or
    inappropriate content, and then filtering out any samples that contain these terms.
    The keyword list can include obvious indicators of low quality, such as profanities
    or spam-related terms, as well as domain-specific words that might indicate irrelevant
    or off-topic content. For instance, in a dataset for a professional writing assistant,
    you might exclude samples containing slang terms or informal expressions that
    don’t align with the intended tone and style.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词排除**是另一种强大的基于规则的过滤技术，它关注的是样本的内容而不是其结构。这种方法涉及创建一个与低质量或不适当内容相关的关键词或短语列表，然后过滤掉包含这些术语的任何样本。关键词列表可以包括明显的低质量指标，如粗话或与垃圾邮件相关的术语，以及可能表明无关或不相关内容的特定领域词汇。例如，在一个专业写作助手的数据集中，你可能需要排除包含俚语或不符合预期语气和风格的非正式表达的样本。'
- en: '**Format checking** is recommended for datasets that include structured data
    or follow specific formatting requirements. This technique ensures that all samples
    adhere to the expected format, maintaining consistency and facilitating processing
    downstream. Format checking can be particularly important for datasets containing
    code samples, JSON structures, or other formatted text. For example, in a dataset
    of programming instructions and solutions, you might implement rules to verify
    that code samples are syntactically correct and follow specified style guidelines.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含结构化数据或遵循特定格式要求的数据集，**格式检查**是推荐的。这项技术确保所有样本都遵循预期的格式，保持一致性并便于后续处理。对于包含代码样本、JSON结构或其他格式化文本的数据集，格式检查尤为重要。例如，在一个包含编程指令和解决方案的数据集中，你可能实施规则来验证代码样本在语法上是正确的，并遵循指定的样式指南。
- en: Rule-based filtering offers significant advantages in preparing instruction
    datasets. Its speed and efficiency allow for rapid application to large volumes
    of data, making it highly scalable. The consistency of rule application ensures
    uniform treatment of data, reducing human error and bias. Furthermore, the explicit
    definition of filtering criteria provides transparency and interpretability, facilitating
    easy understanding, auditing, and adjustment. The ability to automate rule-based
    filtering reduces the need for manual intervention and enables continuous data
    quality monitoring.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的过滤在准备指令数据集方面具有显著优势。其速度和效率允许快速应用于大量数据，使其具有高度可扩展性。规则应用的统一性确保了对数据的统一处理，减少了人为错误和偏差。此外，过滤标准的明确定义提供了透明度和可解释性，便于理解、审计和调整。能够自动化基于规则的过滤减少了手动干预的需求，并使持续监控数据质量成为可能。
- en: However, rule-based filtering also has limitations that must be considered.
    Predefined rules may lack the nuance required to capture the full complexity of
    language and context, potentially leading to the removal of valid but unusual
    samples. The typically binary nature of rules (pass/fail) may not always align
    with the nuanced nature of language and instruction quality. Additionally, as
    data patterns and quality standards evolve, rules need regular review and updates
    to remain effective. There’s also a risk that poorly designed rules could inadvertently
    introduce or amplify biases in the dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于规则的过滤也存在必须考虑的限制。预定义的规则可能缺乏捕捉语言和语境完整复杂性的细微差别，可能导致移除有效但异常的样本。规则通常的二进制性质（通过/失败）可能并不总是与语言和指令质量的细微性质相一致。此外，随着数据模式和品质标准的演变，规则需要定期审查和更新以保持有效性。还存在风险，即设计不良的规则可能会无意中引入或放大数据集中的偏差。
- en: Data deduplication
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据去重
- en: 'Dataset diversity is fundamental to training models that can generalize well
    to new, unseen data. When a dataset contains duplicates or near-duplicates, it
    can lead to several issues:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集多样性对于训练能够良好泛化到新、未见数据集的模型至关重要。当数据集中包含重复或近似重复的数据时，可能会导致以下问题：
- en: 'Overfitting: Models may memorize specific examples rather than learning general
    patterns.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度拟合：模型可能会记住特定的例子，而不是学习一般模式。
- en: 'Biased performance: Overrepresented data points may skew the model’s performance
    towards certain types of inputs.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差性能：过度代表的数据点可能会使模型性能偏向某些类型的输入。
- en: 'Inefficient training: Redundant data can increase training time without providing
    additional valuable information.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练效率低下：冗余数据可能会增加训练时间，而不会提供额外的有价值信息。
- en: 'Inflated evaluation metrics: Duplicate data in test sets may lead to overly
    optimistic performance estimates.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标膨胀：测试集中的重复数据可能会导致过于乐观的性能估计。
- en: To deduplicate datasets, we distinguish between exact and fuzzy deduplication.
    **Exact deduplication** removes identical samples through a straightforward process
    involving data normalization, hash generation, and duplicate removal. Data normalization
    standardizes the format of entries, such as converting text to lowercase. Hash
    generation then creates unique hashes for each entry using algorithms like MD5
    or SHA-256\. These hashes are compared to find matches, and duplicates are removed,
    leaving only one instance of each. While effective for identical entries, exact
    deduplication does not detect near-duplicates or semantically similar content,
    requiring more advanced techniques for those cases.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了去重数据集，我们区分精确和模糊去重。**精确去重**通过涉及数据归一化、哈希生成和重复项删除的简单过程来删除相同的样本。数据归一化标准化条目的格式，例如将文本转换为小写。然后，哈希生成使用MD5或SHA-256等算法为每个条目创建唯一的哈希值。这些哈希值被比较以找到匹配项，并删除重复项，只留下每个的唯一实例。虽然对于相同条目有效，但精确去重无法检测近重复项或语义相似内容，需要更高级的技术来处理这些情况。
- en: The most popular approach to **fuzzy deduplication** is MinHash deduplication.
    Compared to other fuzzy techniques, it maintains high accuracy while significantly
    reducing computational complexity. MinHash operates by generating compact representations,
    or signatures, for each data item. These signatures serve as fingerprints that
    capture the essence of the data while drastically reducing its dimensionality.
    In practice, MinHash transforms data items (such as text documents) into sets
    of shingles, applies multiple hash functions to these sets, and selects the minimum
    hash values to form signature vectors. These signatures can then be compared using
    similarity measures like Jaccard similarity to efficiently identify near-duplicates.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的模糊去重方法是MinHash去重。与其他模糊技术相比，它在显著降低计算复杂性的同时保持了高精度。MinHash通过为每个数据项生成紧凑的表示或签名来操作。这些签名作为指纹，捕捉数据的本质，同时极大地降低了其维度。在实践中，MinHash将数据项（如文本文档）转换为shingles集合，对这些集合应用多个哈希函数，并选择最小哈希值来形成签名向量。然后可以使用Jaccard相似度等相似性度量来比较这些签名，从而有效地识别近重复项。
- en: In addition to exact and fuzzy deduplication, **semantic similarity** takes
    a different approach by focusing on the meaning of text for deduplication. This
    method involves converting words or entire samples into vector representations
    using various natural language processing techniques. Word embedding models such
    as Word2Vec, GloVe, and FastText transform individual words into dense vectors,
    capturing semantic relationships.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了精确和模糊去重之外，**语义相似性**通过关注文本的意义来采取不同的去重方法。这种方法涉及使用各种自然语言处理技术将单词或整个样本转换为向量表示。例如，Word2Vec、GloVe和FastText等词嵌入模型将单个单词转换为密集向量，捕捉语义关系。
- en: For more context-aware representations, language models like BERT, sentence
    transformers, or cross-encoders can generate embeddings for entire sentences or
    documents. Once these vector representations are obtained, deduplication can be
    performed by comparing the similarity between vectors. Common similarity measures
    include cosine similarity or Euclidean distance. Samples with high similarity
    scores above a predefined threshold can be considered duplicates. For large datasets,
    clustering techniques may be applied to group similar vectors. Methods like K-means,
    DBSCAN, or hierarchical clustering can efficiently organize the vector space,
    allowing for the identification of clusters that represent semantically similar
    content. Within each cluster, a representative sample can be retained while others
    are marked as duplicates.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更具上下文感知的表示，BERT、句子转换器或交叉编码器等语言模型可以生成整个句子或文档的嵌入。一旦获得这些向量表示，就可以通过比较向量之间的相似性来进行去重。常见的相似性度量包括余弦相似度或欧几里得距离。相似度得分高于预定义阈值的样本可以被认为是重复的。对于大型数据集，可以应用聚类技术来分组相似的向量。例如，K-means、DBSCAN或层次聚类等方法可以有效地组织向量空间，从而识别出代表语义相似内容的簇。在每个簇中，可以保留一个代表性样本，而其他样本则被标记为重复。
- en: Data decontamination
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据净化
- en: Data decontamination is the process of ensuring that the training dataset does
    not contain samples that are identical or highly similar to those in the evaluation
    or test sets. This step is important for ensuring the quality of the model evaluation
    and preventing overfitting or memorization of test data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据去污是确保训练数据集不包含与评估或测试集中相同或高度相似的样本的过程。这一步骤对于确保模型评估的质量以及防止过拟合或测试数据的记忆化至关重要。
- en: Data decontamination uses techniques from data deduplication. Exact matching
    can be used to remove any training samples that are identical to those in the
    evaluation sets. This can be done using hash functions or direct string comparisons.
    Next, we can also use near-duplicate detection methods to identify and remove
    training samples that are very similar to evaluation samples, even if they are
    not exactly the same. This often involves techniques like MinHash or computing
    similarity scores based on n-grams or embeddings.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据去污采用数据去重技术。可以使用精确匹配来移除任何与评估集中相同的训练样本。这可以通过哈希函数或直接字符串比较来完成。接下来，我们还可以使用近似重复检测方法来识别和移除与评估样本非常相似的训练样本，即使它们并不完全相同。这通常涉及像MinHash或基于n-gram或嵌入计算相似度分数的技术。
- en: A simple way to perform data decontamination is to add your evaluation set to
    the instruction dataset during the data deduplication stage. In this case, we
    want to ensure that we only remove samples from the instruction dataset, which
    can be implemented in different ways (only filtering out the first duplicate,
    recording the indexes of the evaluation samples, etc.). Ideally, you can automatically
    add your evaluation sets in the data deduplication stage to fully automate this
    process. This is particularly efficient if you iterate over several versions of
    custom benchmarks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 执行数据去污的一个简单方法是在数据去重阶段将您的评估集添加到指令数据集中。在这种情况下，我们希望确保我们只从指令数据集中移除样本，这可以通过不同的方式实现（仅过滤掉第一个重复项、记录评估样本的索引等）。理想情况下，您可以在数据去重阶段自动添加您的评估集，以完全自动化此过程。如果您迭代多个自定义基准版本，这将特别高效。
- en: Another aspect of data decontamination is filtering out samples that may have
    been derived from the same source as evaluation data. This can involve checking
    for overlapping phrases, similar sentence structures, or common metadata. Practitioners
    may also use provenance tracking (source the data they use) to identify and exclude
    data from specific sources that are known to be used in evaluation sets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据去污的另一个方面是过滤掉可能源自与评估数据相同来源的样本。这可能包括检查重叠的短语、相似的句子结构或共同的元数据。从业者还可能使用来源跟踪（追踪他们使用的数据来源）来识别和排除已知用于评估集的特定来源的数据。
- en: Data quality evaluation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量评估
- en: Data quality evaluation is a critical aspect of machine learning, particularly
    for LLMs. The process involves assessing various characteristics of datasets,
    including accuracy, diversity, and complexity. While some aspects like mathematical
    accuracy can be easily verified using tools such as Python interpreters, evaluating
    subjective or open-ended content remains challenging.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量评估是机器学习的一个关键方面，尤其是对于LLMs。这个过程涉及评估数据集的各种特征，包括准确性、多样性和复杂性。虽然像数学精度这样的方面可以通过Python解释器等工具轻松验证，但评估主观或开放式内容仍然具有挑战性。
- en: Traditional methods of data quality assessment include human annotation, which
    generally provides high accuracy but is resource-intensive. To address scalability
    issues, machine learning techniques have been developed to automate the evaluation
    process. These include using LLMs as judges, reward models, and classifiers trained
    for quality prediction.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量评估的传统方法包括人工标注，这通常提供高精度，但资源密集。为了解决可扩展性问题，已经开发出机器学习技术来自动化评估过程。这包括使用LLMs作为评判者、奖励模型以及用于质量预测的经过训练的分类器。
- en: '**The LLM-as-a-judge** strategy involves prompting LLMs to evaluate the quality
    of each sample. This approach has become popular due to its flexibility and ease
    of use, though it does present some challenges. Different LLMs have different
    levels of performance across tasks, and their evaluations often align more closely
    with those of non-experts. With domain-specific datasets, you might want to use
    domain-specific models instead of better, general-purpose LLMs. Comparative assessment
    methods (e.g., “Is answer A better than answer B?”) generally outperform absolute
    scoring approaches (e.g., “Rate answer A between 1 and 4”), though both can be
    used at scale with sufficient prompt engineering. We recommend iterating through
    different prompts over a representative subset to manually verify the quality
    of the responses. *Table 5.2* shows an example of a custom prompt for a judge
    LLM.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM-as-a-judge**策略涉及提示LLM评估每个样本的质量。这种方法因其灵活性和易用性而变得流行，尽管它确实带来了一些挑战。不同的LLM在任务上的表现水平不同，它们的评估通常与非专家的评估更为接近。对于特定领域的数据集，您可能希望使用特定领域的模型而不是更好的通用LLM。比较评估方法（例如，“答案A是否比答案B更好？”）通常优于绝对评分方法（例如，“对答案A进行1到4的评分”），尽管两者都可以通过足够的提示工程进行大规模使用。我们建议在代表性子集上迭代不同的提示，以手动验证响应的质量。*表5.2*展示了为裁判LLM定制的提示示例。'
- en: '| **Instruction**You are a data quality evaluator. Your goal is to assess an
    instruction and its corresponding answer, determining how effectively the answer
    addresses the given task.In your evaluation, you will provide feedback detailing
    the strengths and weaknesses of the answer, followed by a score on a scale of
    1 to 4.A score of 1 means that the answer is terrible and irrelevant to the instruction.A
    score of 2 means that the answer is not helpful and misses important aspects of
    the instruction.A score of 3 means that the answer is helpful but could be improved
    in terms of relevance, accuracy, and depth.A score of 4 means that the answer
    is excellent and fully addresses the task.Provide your evaluation as follows:Feedback:
    (strengths and weaknesses you find relevant)Score: (number between 1 and 4) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **指令**您是一位数据质量评估员。您的目标是评估一个指令及其相应的答案，确定答案在多大程度上解决了给定任务。在您的评估中，您将提供反馈，详细说明答案的优点和缺点，然后给出1到4分的评分。1分表示答案糟糕且与指令无关。2分表示答案无帮助且遗漏了指令的重要方面。3分表示答案有帮助，但在相关性、准确性和深度方面可以改进。4分表示答案优秀且完全解决了任务。请按照以下格式提供您的评估：反馈：（您认为相关的优点和缺点）评分：（1到4之间的数字）|'
- en: '*Table 5.2* – Example of LLM-as-a-judge prompt for data quality evaluation'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.2* – LLM-as-a-judge用于数据质量评估的示例提示'
- en: LLM-as-a-judge is known to have several biases. First, it has a position bias
    in comparative scoring, where the LLM judge favors the first answer. This can
    be addressed by randomizing the order of answers A and B. In addition, like humans,
    LLM judges favor long answers. Length normalization techniques can be applied
    to absolute scoring to mitigate this issue. Finally, LLM judges are known to have
    intra-model favoritism, meaning that they prefer models from the same family (GPT-4o
    with GPT-4 and GPT-4o mini, for example). This can be addressed by using several
    models instead of a single one.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-as-a-judge（作为裁判的LLM）已知存在几个偏见。首先，它在比较评分中存在立场偏见，即LLM裁判倾向于第一个答案。这可以通过随机化答案A和B的顺序来解决。此外，像人类一样，LLM裁判倾向于长答案。可以通过应用长度归一化技术来缓解这个问题。最后，LLM裁判存在模型内偏好，意味着他们更喜欢同一家族的模型（例如，GPT-4o与GPT-4和GPT-4o
    mini）。这可以通过使用多个模型而不是单个模型来解决。
- en: In general, to improve evaluation reliability, strategies such as using multiple
    LLMs as a jury reduce bias and improve consistency. Leveraging a jury of smaller
    LLMs can also reduce costs while increasing accuracy and mitigating intra-model
    favoritism. For specific applications like chatbots, it’s advisable to aim for
    high agreement between LLM judges and human evaluators (around 80%). Simple grading
    scales (with few-shot prompting) and task-specific benchmarks are also recommended
    to ensure relevant and interpretable evaluations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了提高评估可靠性，使用多个LLM作为陪审团等策略可以减少偏见并提高一致性。利用较小LLM的陪审团还可以降低成本，同时提高准确性和减轻模型内偏好。对于像聊天机器人这样的特定应用，建议LLM裁判和人类评估员之间达到高一致性（大约80%）。简单的评分标准（带有少量提示）和特定任务的基准也是推荐的，以确保相关和可解释的评估。
- en: '**Reward models** are another way to re-purpose LLMs for data quality evaluation.
    The term “reward model” comes from Reinforcement Learning from Human Feedback
    (RLHF, see *Chapter 6*). They can be broadly defined as models that take an instruction
    and answer pair and return a score as output. Generally, reward models are created
    by adding a linear head on top of a decoder-only architecture like Gemma or Llama.
    They are then trained for this specific purpose, using either reinforcement learning
    or traditional fine-tuning. *Figure 5.3* shows ArmoRM-Llama3-8B-v0.1’s architecture,
    which adds regression and gating layers on top of a Llama 3 8B model. This model
    outputs multiple scores to target specific dimensions, such as helpfulness, correctness,
    coherence, complexity, and verbosity. This allows for a more fine-grained approach
    to data quality evaluation.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励模型**是另一种将LLM重新用于数据质量评估的方法。术语“奖励模型”来自人类反馈的强化学习（RLHF，见*第6章*）。它们可以被广泛定义为接受指令和答案对并返回分数作为输出的模型。通常，奖励模型是在Gemma或Llama这样的仅解码器架构之上添加一个线性头创建的。然后，它们为此特定目的进行训练，使用强化学习或传统的微调。*图5.3*显示了ArmoRM-Llama3-8B-v0.1的架构，该架构在Llama
    3 8B模型之上添加了回归和门控层。该模型输出多个分数以针对特定维度，如有用性、正确性、连贯性、复杂性和冗长性。这允许对数据质量评估采取更细致的方法。'
- en: '![](img/B31105_05_03.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_05_03.png)'
- en: 'Figure *5*.3 – Architecture of RLHFlow/ArmoRM-Llama3-8B-v0.1, based on Llama
    3 (Source: [https://doi.org/10.48550/arXiv.2406.12845](https://doi.org/10.48550/arXiv.2406.12845))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *5*.3 – RLHFlow/ArmoRM-Llama3-8B-v0.1的架构，基于Llama 3（来源：[https://doi.org/10.48550/arXiv.2406.12845](https://doi.org/10.48550/arXiv.2406.12845))
- en: The Allen Institute for AI’s RewardBench leaderboard, hosted on Hugging Face
    (allenai/reward-bench), is a good resource for comparing different reward models.
    It combines various types of reward models (generative, classifiers, DPO, etc.)
    and evaluates them on a curated set of chosen and rejected answers for each instruction.
    While this task is not directly related to instruction data quality, it is a good
    resource for finding models capable of differentiating between good and bad answers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由Hugging Face（allenai/reward-bench）托管的艾伦人工智能研究所的RewardBench排行榜是一个比较不同奖励模型的良好资源。它结合了各种类型的奖励模型（生成式、分类器、DPO等），并在每个指令的精选和拒绝的答案集上对它们进行评估。虽然这项任务与指令数据质量没有直接关系，但它是一个寻找能够区分好答案和坏答案的模型的良好资源。
- en: '**Classifiers or encoder-only models** can be trained to perform data quality
    evaluation. A good example is HuggingFaceFW/fineweb-edu-classifier, a classifier
    designed to judge the educational value of web pages. This model was designed
    as a quality filter for pretraining data but a similar approach can be taken to
    evaluate instruction samples at scale. In practice, fineweb-edu-classifier adds
    a classification head to an embedding model (Snowflake/snowflake-arctic-embed-m)
    and trains it for 20 epochs on 450,000 samples that are annotated by Llama 3 70B
    Instruct.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类器或仅编码器模型**可以被训练以执行数据质量评估。一个很好的例子是HuggingFaceFW/fineweb-edu-classifier，这是一个旨在判断网页教育价值的分类器。该模型被设计为预训练数据的质量过滤器，但可以采用类似的方法来评估大规模的指令样本。在实践中，fineweb-edu-classifier向一个嵌入模型（Snowflake/snowflake-arctic-embed-m）添加了一个分类头，并在由Llama
    3 70B Instruct标注的450,000个样本上训练了20个epoch。'
- en: This approach relies on encoder-only models, which are both smaller and better
    suited to classification tasks. Thanks to their low number of parameters, these
    models are faster to run and can scale to millions of samples. However, they are
    not as accurate as bigger models, particularly for complex reasoning tasks where
    they lack the ability to capture nuances. At smaller scale, encoder-only models
    are still valuable to filter out outliers or as part of an automated data pipeline,
    which requires faster processing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法依赖于仅编码器模型，这些模型既更小，又更适合分类任务。由于它们的参数数量较少，这些模型运行速度更快，可以扩展到数百万个样本。然而，它们的准确性不如更大的模型，尤其是在需要捕捉细微差别的高级推理任务中。在小规模上，仅编码器模型仍然有价值，用于过滤异常值或作为自动化数据管道的一部分，这需要更快的处理速度。
- en: Data exploration
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: Data exploration is a continuous process that requires practitioners to become
    familiar with the training data. It involves both manual inspection and automated
    analysis, each playing a crucial role in understanding the dataset’s characteristics,
    strengths, and potential shortcomings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索是一个持续的过程，需要从业者熟悉训练数据。它涉及手动检查和自动化分析，每个都在理解数据集的特征、优势和潜在不足方面发挥着关键作用。
- en: '**Manual dataset exploration**, though time-consuming, is an important step.
    It reveals errors and inconsistencies that automated processes might miss, including
    formatting issues, data entry mistakes, incoherent reasoning, and factual inaccuracies.
    This process provides qualitative insights into the dataset’s content and style.
    To enhance efficiency, researchers can employ techniques like stratified sampling
    (selecting diverse samples), systematic review (using a criteria checklist), and
    collaborative review (involving multiple reviewers).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**手动数据集探索**，尽管耗时，但是一个重要的步骤。它揭示了自动化过程可能遗漏的错误和不一致性，包括格式问题、数据输入错误、不连贯的推理和事实不准确。这个过程为数据集的内容和风格提供了定性的洞察。为了提高效率，研究人员可以采用分层抽样（选择多样化的样本）、系统回顾（使用标准清单）和协作回顾（涉及多个审阅者）等技术。'
- en: '*Figure 5.4* shows an example with Argilla, a collaborative platform for manual
    data quality evaluation and exploration.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.4*展示了使用Argilla（一个用于手动数据质量评估和探索的协作平台）的示例。'
- en: '![A screenshot of a computer  Description automatically generated](img/B31105_05_04.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B31105_05_04.png)'
- en: Figure *5*.4 – Argilla’s interface for collaborative data quality evaluation
    and exploration
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图*5*.4 – Argilla的协作数据质量评估和探索界面
- en: '**Statistical analysis** is a complementary technique that reveals vocabulary
    diversity, potential biases, and concept representation. This process utilizes
    natural language processing libraries like NLTK or spaCy for tokenization and
    analysis of large text volumes. Visualization tools such as Matplotlib or Seaborn
    create histograms and word clouds, enabling intuitive pattern recognition. These
    techniques provide insights into dataset composition, language breadth, and possible
    cultural or contextual preferences, which can influence model outputs.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计分析**是一种补充技术，可以揭示词汇多样性、潜在偏见和概念表示。这个过程利用自然语言处理库如NLTK或spaCy进行分词和分析大量文本。可视化工具如Matplotlib或Seaborn创建直方图和词云，使直观的图案识别成为可能。这些技术提供了对数据集组成、语言广度和可能的文化或情境偏好的洞察，这些可能影响模型输出。'
- en: '**Topic clustering** automatically groups similar documents or pieces of text
    together, revealing underlying themes and patterns within the data. This process
    is especially important for understanding the content of large text corpora, identifying
    trends, and organizing information in a meaningful way. It is often associated
    with data visualization, with figures that show clusters of similar samples.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题聚类**自动将相似的文档或文本片段分组在一起，揭示数据中的潜在主题和模式。这个过程对于理解大型文本语料库的内容、识别趋势和以有意义的方式组织信息尤为重要。它通常与数据可视化相关联，其中包含显示相似样本聚类的图形。'
- en: Let’s consider the task of building an instruction dataset about various programming
    languages. You have collected a vast corpus of programming-related text from online
    forums, documentation, and tutorials. First, topic clustering can help identify
    the distinct programming languages present in the dataset (Python, JavaScript,
    etc.). Second, within each language cluster, you can further identify sub-topics
    like `error handling`, `data structures`, and `web frameworks`. This allows a
    balanced representation of each language and sub-topic in the corpus.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑构建关于各种编程语言的指令数据集的任务。您已经从在线论坛、文档和教程中收集了大量与编程相关的文本。首先，主题聚类可以帮助识别数据集中存在的不同编程语言（Python、JavaScript等）。其次，在每种语言的聚类内部，您可以进一步识别如`错误处理`、`数据结构`和`网络框架`等子主题。这允许在语料库中对每种语言和子主题进行平衡表示。
- en: This makes sure that each topic is correctly covered for each programming language.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了每个主题在每个编程语言中都被正确覆盖。
- en: '![](img/B31105_05_05.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_05_05.png)'
- en: Figure *5*.5 – Representation of the historical TikTok dataset made with Nomic
    Atlas
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图*5*.5 – 使用Nomic Atlas制作的TikTok历史数据集的表示
- en: Several tools are available for performing topic clustering, each with its own
    strengths and approaches. For example, Hugging Face’s text-clustering provides
    a simple pipeline with sentence transformers for embedding text into vector space,
    UMAP for dimensionality reduction, and DBSCAN for clustering. It also automatically
    labels clusters using an LLM and can output visualizations. Nomic Atlas (see *Figure
    5.5*), BunkaTopics, and Lilac are alternatives proposing similar approaches with
    additional features.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具可用于执行主题聚类，每种工具都有其自身的优势和途径。例如，Hugging Face的text-clustering提供了一个简单的管道，使用sentence
    transformers将文本嵌入到向量空间，UMAP进行降维，以及DBSCAN进行聚类。它还可以使用LLM自动标记聚类，并可以输出可视化。Nomic Atlas（见*图5.5*）、BunkaTopics和Lilac是提出类似方法并增加额外功能的替代方案。
- en: Data generation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成
- en: When the available instruction datasets are not sufficient, creating custom
    data becomes necessary. This is particularly relevant for specialized applications
    where publicly available data is scarce.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当可用的指令数据集不足时，创建自定义数据变得必要。这对于公开数据稀缺的专业应用尤其相关。
- en: Additionally, it serves as a method to augment underrepresented areas in a dataset,
    like insufficient examples of JavaScript error-handling techniques in our previous
    example. While data can be generated manually by individuals or through crowdsourcing,
    these approaches often incur significant costs and time investments. Synthetic
    data generation using LLMs offers a more efficient and scalable alternative. This
    method, when combined with well-designed prompt engineering, can produce high-quality
    data at a much larger scale, effectively addressing the limitations of manual
    data creation processes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还作为一种方法来增强数据集中代表性不足的区域，例如在我们之前的例子中JavaScript错误处理技术的示例不足。虽然数据可以通过个人手动生成或通过众包生成，但这些方法通常会产生显著的成本和时间投入。使用LLM生成合成数据提供了一种更高效和可扩展的替代方案。这种方法与精心设计的提示工程相结合，可以在更大的规模上生成高质量的数据，有效地解决手动数据创建过程的局限性。
- en: The process of synthetic data generation typically begins with the preparation
    of a set of carefully designed prompts (sometimes called taxonomy). These serve
    as the foundation for generating new, diverse examples. Five seed prompts used
    in the original Alpaca dataset can be seen in *Table 5.3*. The quality of synthetically
    generated data largely depends on the prompts and techniques used in the generation
    process. Well-crafted prompts can guide the language model to produce diverse,
    relevant, and high-quality instruction-response pairs. These prompts often include
    specific instructions, examples, and constraints to ensure the generated data
    aligns with the desired format and content.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据生成的过程通常从准备一组精心设计的提示（有时称为分类法）开始。这些提示是生成新、多样化示例的基础。原始Alpaca数据集中使用的五个种子提示可以在*表5.3*中看到。合成生成数据的质量在很大程度上取决于生成过程中使用的提示和技术。精心设计的提示可以引导语言模型产生多样化、相关且高质量的操作-响应对。这些提示通常包括具体的指令、示例和约束，以确保生成的数据与期望的格式和内容一致。
- en: '| **Seed instructions**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '| **种子指令**'
- en: Is there anything I can eat for breakfast that doesn’t include eggs, yet includes
    protein, and has roughly 700-1000 calories?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我早餐能吃什么，不包含鸡蛋，但包含蛋白质，并且大约有700-1000卡路里？
- en: 'What is the relation between the given pairs? Input: Night : Day :: Right :
    Left'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '给定的成对之间有什么关系？输入：Night : Day :: Right : Left'
- en: 'Generate a one-sentence description for each of the following people. Input:
    -Barack Obama\n- Elon Musk\n- Taylor Swift'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为以下每个人生成一句描述。输入：-Barack Obama\n- Elon Musk\n- Taylor Swift
- en: 'Describe a situation in which the given stereotype can harm you. Input: All
    Asians are smart!'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述一个给定刻板印象可能对你造成伤害的情况。输入：所有亚洲人都很聪明！
- en: 'Generate an appropriate subjective title for the following email: Input: “Hi
    [person name],\n\nI’m writing to ask you if you are happy to be a panelist in
    our workshop on multimodality at CVPR. The workshop will be held on June 20, 2023\.
    \n\nBest,\n[my name]'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为以下电子邮件生成一个合适的标题：输入：“Hi [person name],\n\n我写信是想问你是否愿意成为我们CVPR多模态研讨会的一名评审。研讨会将于2023年6月20日举行。\n\n最好的问候，\n[my
    name]”
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '*Table 5.3* – Examples of seed prompts used in the original Alpaca dataset'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.3* – 原始Alpaca数据集中使用的种子提示示例'
- en: Many synthetic data generation pipelines incorporate multiple steps to ensure
    data quality. This may include generating an initial set of questions or instructions,
    followed by generating corresponding answers or responses. Some systems also implement
    validation steps, where another model or set of rules checks the generated pairs
    for accuracy, relevance, and adherence to specified criteria.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 许多合成数据生成流程包含多个步骤以确保数据质量。这可能包括生成一组初始问题或指令，然后生成相应的答案或响应。一些系统还实施了验证步骤，其中另一个模型或一组规则检查生成的对是否准确、相关以及是否符合指定标准。
- en: An important aspect of synthetic data generation is the ability to control various
    attributes of the generated data. This includes factors such as the complexity
    of the instructions, the length of the responses, the tone or style of the language
    used, and the specific topics or domains covered. By fine-tuning these parameters,
    it’s possible to create datasets that are tailored to specific training objectives
    or that complement existing datasets in targeted ways. Structured generation using
    libraries like Outlines can also be beneficial to adhere to specific formats.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据生成的一个重要方面是能够控制生成数据的各种属性。这包括诸如指令的复杂性、响应的长度、使用的语言语气或风格以及特定主题或领域等因素。通过微调这些参数，可以创建针对特定训练目标定制的数据集，或者以有针对性的方式补充现有数据集。使用如Outlines等库进行结构化生成也有助于遵守特定格式。
- en: Furthermore, synthetic data generation can be particularly useful for addressing
    biases and gaps in existing datasets. By carefully designing the generation process,
    it’s possible to create more balanced and inclusive datasets that represent a
    wider range of perspectives, topics, and language styles. This can help in training
    LLMs that are more equitable and capable of serving diverse user bases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，合成数据生成在解决现有数据集的偏差和空白方面特别有用。通过精心设计生成过程，可以创建更加平衡和包容的数据集，这些数据集代表了更广泛的视角、主题和语言风格。这有助于训练更加公平且能够服务于多样化用户群体的LLM。
- en: However, synthetic data generation also comes with challenges. One primary concern
    is the potential for the generated data to inherit biases or errors from the underlying
    language model used for generation. To mitigate this, many approaches incorporate
    human oversight, diverse prompts, and additional filtering mechanisms to ensure
    the quality and appropriateness of the generated data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，合成数据生成也面临着挑战。一个主要问题是生成的数据可能会继承用于生成的底层语言模型的偏差或错误。为了减轻这一问题，许多方法都采用了人工监督、多样化的提示和额外的过滤机制，以确保生成数据的质量和适宜性。
- en: Another consideration is the need for the generated data to be sufficiently
    diverse and challenging. If the synthetic data is too simplistic or repetitive,
    it may not provide the level of complexity required to train a robust LLM. Advanced
    techniques in synthetic data generation often focus on creating varied and nuanced
    instruction-response pairs that can push the boundaries of what the model can
    learn.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是生成数据需要足够多样化和具有挑战性。如果合成数据过于简单或重复，可能无法提供训练稳健LLM所需的复杂度。合成数据生成的高级技术通常专注于创建多样化的、细微的指令-响应对，以推动模型学习边界的扩展。
- en: Data augmentation
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'In this context, data augmentation refers to the process of increasing both
    the quantity and the quality of data samples. Unlike data generation, we use pre-existing
    instruction samples as inputs in this stage. While it is possible to upsample
    pairs of instructions and answers, data augmentation is mostly used to increase
    the quality of existing samples. In particular, it focuses on two aspects: diversity
    and complexity.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，数据增强指的是增加数据样本数量和质量的过程。与数据生成不同，在这个阶段我们使用预存在的指令样本作为输入。虽然可以增加指令和答案对的样本数量，但数据增强主要用于提高现有样本的质量。特别是，它关注两个方面：多样性和复杂性。
- en: 'A pioneering approach in this field is the Evol-Instruct method, which uses
    LLMs to evolve simple instructions into more qualitative ones. The evolved instructions
    can then be used to generate answers using powerful LLMs. This method employs
    two main strategies: in-depth and in-breadth evolving.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的开创性方法是Evol-Instruct方法，它使用LLM将简单的指令进化为更高质量的指令。然后可以使用这些进化后的指令通过强大的LLM生成答案。这种方法采用两种主要策略：深度和广度进化。
- en: '**In-depth evolving** focuses on enhancing the complexity of existing instructions.
    It includes several techniques:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度演化**专注于提高现有指令的复杂性。它包括几种技术：'
- en: '**Constraints**: It involves introducing additional requirements or limitations
    to the original instruction, making it more challenging to fulfill.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**约束**：这涉及到引入额外的要求或限制到原始指令中，使其更具挑战性。'
- en: '**Deepening**: Instead of shallow questions, it tries to find more deep questions,
    requiring more comprehensive responses.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深化**：它试图寻找更深入的问题，需要更全面的回答，而不是浅层的问题。'
- en: '**Concretizing**: It replaces general concepts with more specific ones, adding
    detail and precision to the instruction.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具体化**：它用更具体的概念替换一般概念，为指令添加细节和精确度。'
- en: '**Increasing reasoning steps**: It modifies instructions to explicitly request
    multiple-step reasoning, promoting more complex problem-solving.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加推理步骤**：它修改指令以明确请求多步推理，促进更复杂的解决问题。'
- en: '**Complicating input**: This involves adding more complex data formats or structures
    to the instruction, such as XML, JSON, or code snippets.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂化输入**：这涉及到向指令中添加更复杂的数据格式或结构，例如XML、JSON或代码片段。'
- en: '**In-breadth evolving**, on the other hand, aims to expand the diversity of
    the instruction dataset. It generates entirely new instructions inspired by existing
    ones, focusing on creating more rare or long-tailed examples within the same domain.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**广度演化**，另一方面，旨在扩展指令数据集的多样性。它通过现有指令的灵感生成全新的指令，专注于在同一领域内创建更多罕见或长尾的示例。'
- en: As an example of concrete implementation, in-depth evolving can be automated
    with the following prompt, from the AutoEvol paper. You simply need to provide
    the instruction you want to evolve as input, and a powerful model like GPT-4o
    will return a more complex version of the original instruction.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具体实现的例子，深度演化可以通过以下来自AutoEvol论文的提示自动化。你只需提供要演化的指令作为输入，一个强大的模型如GPT-4o将返回一个更复杂的原始指令版本。
- en: '| You are an Instruction Rewriter that rewrites the given #Instruction# into
    a more complex version. Please follow the steps below to rewrite the given “#Instruction#”
    into a more complex version.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '| 你是一个指令重写者，将给定的#指令#重写为一个更复杂的版本。请按照以下步骤将给定的“#指令#”重写为一个更复杂的版本。'
- en: 'Step 1: Please read the “#Instruction#” carefully and list all the possible
    methods to make this instruction more complex (to make it a bit harder for well-known
    AI assistants such as ChatGPT and GPT4 to handle). Please do not provide methods
    to'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1步：请仔细阅读“#指令#”，并列出所有可能的使该指令更复杂的方法（使它对像ChatGPT和GPT4这样的知名AI助手来说更具挑战性）。请勿提供方法
- en: change the language of the instruction!
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变指令的语言！
- en: 'Step 2: Please create a comprehensive plan based on the #Methods List# generated
    in Step 1 to make the #Instruction# more complex. The plan should include several
    methods from the #Methods List#.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2步：请根据第1步生成的#方法列表#制定一个全面的计划，使#指令#更加复杂。该计划应包括#方法列表#中的几个方法。
- en: 'Step 3: Please execute the plan step by step and provide the #Rewritten Instruction#.
    #Rewritten Instruction# can only add 10 to 20 words into the “#Instruction#”.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3步：请逐步执行计划并提供#重写指令#。#重写指令#只能将10到20个单词添加到“#指令#”中。
- en: 'Step 4: Please carefully review the #Rewritten Instruction# and identify any
    unreasonable parts. Ensure that the #Rewritten Instruction# is only a more complex
    version of the #Instruction#. Just provide the #Finally Rewritten Instruction#
    without anyexplanation.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4步：请仔细审查#重写指令#，并识别任何不合理的地方。确保#重写指令#只是#指令#的一个更复杂的版本。只需提供#最终重写指令#，无需任何解释。
- en: 'Please reply strictly in the following format:Step 1 #Methods List#:Step 2
    #Plan#:Step 3 #Rewritten Instruction#:Step 4 #Finally Rewritten Instruction#:#Instruction#:{Instruction}
    |'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '请严格按照以下格式回复：Step 1 #方法列表#：Step 2 #计划#：Step 3 #重写指令#：Step 4 #最终重写指令#：#指令#:{Instruction}
    |'
- en: '*Table 5.4* – Evol LLM prompt from the “Automatic Instruction Evolving for
    Large Language Models” paper by Zeng et al. (2024)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.4* – 来自Zeng等人（2024年）“为大型语言模型自动指令演化”论文的Evol LLM提示'
- en: '**The UltraFeedback method** is another innovative approach, focused on answer
    quality instead of instruction quality. It employs AI feedback to enhance the
    quality and diversity of model responses. Unlike Evol-Instruct, which evolves
    instructions, UltraFeedback uses a large pool of diverse instructions and models
    to generate a wide range of responses.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**超反馈方法**是另一种创新方法，它专注于答案质量而不是指令质量。它使用AI反馈来提高模型响应的质量和多样性。与Evol-Instruct不同，后者演进指令，UltraFeedback使用大量多样化的指令和模型来生成广泛范围的响应。'
- en: It then leverages advanced language models like GPT-4 to provide detailed critiques
    and numerical scores for these responses across multiple dimensions such as instruction-following,
    truthfulness, honesty, and helpfulness.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它利用像GPT-4这样的高级语言模型，从多个维度如指令遵循、真实性、诚实性和有用性等方面，为这些响应提供详细的批评和数值评分。
- en: Based on these ideas, you can create your own augmentation techniques to create
    a more challenging and diverse instruction dataset. By refining and evolving existing
    instructions and answers, the resulting dataset can better train models to handle
    complex, multi-step tasks, and improve their performance across a wider range
    of applications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些想法，你可以创建自己的增强技术，以创建更具挑战性和多样化的指令数据集。通过改进和演进现有的指令和答案，生成的数据集可以更好地训练模型以处理复杂的多步骤任务，并提高它们在更广泛的应用范围内的性能。
- en: Creating our own instruction dataset
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们自己的指令数据集
- en: 'In this section, we will create our own instruction dataset based on the crawled
    data from *Chapter 3*. To create a high-quality instruction dataset, we need to
    address two main issues: the unstructured nature of our data and the limited number
    of articles we can crawl.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将根据从*第三章*爬取的数据创建自己的指令数据集。为了创建高质量的指令数据集，我们需要解决两个主要问题：我们数据的不结构化性质以及我们能够爬取的文章数量有限。
- en: This unstructured nature comes from the fact that we are dealing with raw text
    (articles), instead of pairs of instructions and answers. To address this issue,
    we will use an LLM to perform this transformation. Specifically, we will employ
    a combination of backtranslation and rephrasing. Backtranslation refers to the
    process of providing the expected answer as output and generating its corresponding
    instruction. However, using a chunk of text like a paragraph as an answer might
    not always be appropriate. This is why we want to rephrase the raw text to ensure
    we’re outputting properly formatted, high-quality answers. Additionally, we can
    ask the model to follow the author’s writing style to stay close to the original
    paragraph. While this process involves extensive prompt engineering, it can be
    automated and used at scale, as we will see in the following implementation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不结构化性质源于我们处理的是原始文本（文章），而不是指令和答案的配对。为了解决这个问题，我们将使用一个大型语言模型（LLM）来完成这个转换。具体来说，我们将采用回译和改写相结合的方法。回译是指提供预期的答案作为输出并生成相应的指令。然而，使用一段文本，如一个段落作为答案可能并不总是合适的。这就是为什么我们想要改写原始文本，以确保我们输出的答案是格式正确、高质量的。此外，我们可以要求模型遵循作者的写作风格，以保持与原始段落的接近。虽然这个过程涉及大量的提示工程，但它可以自动化并大规模使用，正如我们将在以下实现中看到的那样。
- en: Our second issue regarding the limited number of samples is quite common in
    real-world use cases. The number of articles we can retrieve is limited, which
    constrains the size of the instruction dataset we are able to create. In this
    example, the more samples we have, the better the model becomes at imitating the
    original authors. To address this problem, we will divide our articles into chunks
    and generate three instruction-answer pairs for each chunk. This will multiply
    the number of samples we create while maintaining diversity in the final dataset.
    For simplicity, we will do it using OpenAI’s GPT-4o-mini model, but you can also
    use open-source models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关于样本数量有限的第二个问题在现实世界的用例中相当常见。我们能检索到的文章数量有限，这限制了我们可以创建的指令数据集的大小。在这个例子中，我们拥有的样本越多，模型在模仿原始作者方面的表现就越好。为了解决这个问题，我们将文章分成块，并为每个块生成三组指令-答案对。这将增加我们创建的样本数量，同时保持最终数据集的多样性。为了简单起见，我们将使用OpenAI的GPT-4o-mini模型来完成这项工作，但你也可以使用开源模型。
- en: However, LLMs are not reliable when it comes to producing structured output.
    Even when given specific templates or instructions, there’s no guarantee that
    the model will consistently adhere to them. This inconsistency often necessitates
    additional string parsing to ensure the output meets the desired format.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到生成结构化输出时，LLM并不可靠。即使给出了特定的模板或指令，也无法保证模型会始终遵循它们。这种不一致性通常需要额外的字符串解析，以确保输出符合所需的格式。
- en: To simplify this process and ensure properly structured results, we can employ
    structured generation techniques. Structured generation is an effective method
    to force an LLM to follow a predefined template, such as JSON, pydantic classes,
    or regular expressions. In the following, we will use OpenAI’s JSON mode feature,
    which provides a more robust way to return valid JSON objects and reduce the need
    for extensive post-processing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这个过程并确保结果结构良好，我们可以采用结构化生成技术。结构化生成是一种有效的方法，可以迫使大型语言模型（LLM）遵循预定义的模板，例如JSON、pydantic类或正则表达式。在下面的内容中，我们将使用OpenAI的JSON模式功能，它提供了一种更健壮的方式来返回有效的JSON对象，并减少了对大量后处理的依赖。
- en: Based on this description, the following figure summarizes every step of the
    synthetic data pipeline we want to build.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个描述，以下图总结了我们要构建的合成数据管道的每个步骤。
- en: '![A diagram of a tree  Description automatically generated with medium confidence](img/B31105_05_06.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![一棵树的图示，描述自动生成，置信度中等](img/B31105_05_06.png)'
- en: Figure *5*.6 – Synthetic data generation pipeline from raw text to instruction
    dataset
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 从原始文本到指令数据集的合成数据生成管道
- en: 'Let’s now implement it in Python. You can implement it as part of the LLMOps
    pipeline, or as a standalone script:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将其在Python中实现。您可以将它作为LLMOps管道的一部分实现，或者作为一个独立的脚本：
- en: We want to make sure that the following libraries are installed. The OpenAI
    library will allow us to interact with a model to generate the instruction data,
    and datasets will format it into a Hugging Face-compatible format. The tqdm library
    is installed to visualize the progress during the data generation process.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要确保以下库已安装。OpenAI库将使我们能够与模型交互以生成指令数据，并将数据集格式化为Hugging Face兼容的格式。tqdm库已安装，以便在数据生成过程中可视化进度。
- en: '[PRE0]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We import all the required libraries as follows.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按照以下方式导入所有必需的库。
- en: '[PRE1]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The raw data we have is a JSON file. We create a Hugging Face dataset from
    this JSON file by extracting specific fields from each article: `id`, `content`,
    `platform`, `author_id`, `author name`, and `link`.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们拥有的原始数据是一个JSON文件。我们通过从每篇文章中提取特定字段（`id`、`content`、`platform`、`author_id`、`author
    name`和`link`）来从该JSON文件创建一个Hugging Face数据集。
- en: '[PRE2]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we simply load our dataset as a pandas dataframe, it returns the following
    table.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们简单地将数据集作为pandas数据框加载，它将返回以下表格。
- en: '|  | **id** | **content** | **platform** | **author_id** | **author_full_name**
    | **link** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | **id** | **content** | **platform** | **author_id** | **author_full_name**
    | **link** |'
- en: '| 0 | ab2f9e2e-5459-4dd6-97d6-c291de4a7093 | The Importance of Data Pipelines
    in the Era of... | medium | e6b945ba-6a9a-4cde-b2bf-0890af79732b | Alex Vesa |
    [https://medium.com/decodingml/the-importance-o...](https://medium.com/decodingml/the-importance-o...)
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 0 | ab2f9e2e-5459-4dd6-97d6-c291de4a7093 | The Importance of Data Pipelines
    in the Era of... | medium | e6b945ba-6a9a-4cde-b2bf-0890af79732b | Alex Vesa |
    [https://medium.com/decodingml/the-importance-o...](https://medium.com/decodingml/the-importance-o...)
    |'
- en: '| 1 | ccfe70f3-d324-40b6-ba38-86e72786dcf4 | Change Data Capture: Enabling
    Event-Driven Arc... | medium | e6b945ba-6a9a-4cde-b2bf-0890af79732b | Alex Vesa
    | [https://medium.com/decodingml/the-3nd-out-of-1...](https://medium.com/decodingml/the-3nd-out-of-1...)
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ccfe70f3-d324-40b6-ba38-86e72786dcf4 | Change Data Capture: Enabling
    Event-Driven Arc... | medium | e6b945ba-6a9a-4cde-b2bf-0890af79732b | Alex Vesa
    | [https://medium.com/decodingml/the-3nd-out-of-1...](https://medium.com/decodingml/the-3nd-out-of-1...)
    |'
- en: '| 2 | 4c9f68ae-ec8b-4534-8ad5-92372bf8bb37 | The Role of Feature Stores in
    Fine-Tuning LLMs... | medium | e6b945ba-6a9a-4cde-b2bf-0890af79732b | Alex Vesa
    | [https://medium.com/decodingml/the-role-of-feat...](https://medium.com/decodingml/the-role-of-feat...)
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4c9f68ae-ec8b-4534-8ad5-92372bf8bb37 | The Role of Feature Stores in
    Fine-Tuning LLMs... | medium | e6b945ba-6a9a-4cde-b2bf-0890af79732b | Alex Vesa
    | [https://medium.com/decodingml/the-role-of-feat...](https://medium.com/decodingml/the-role-of-feat...)
    |'
- en: '| ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... | ... |'
- en: '| 73 | 68795a4d-26c2-43b7-9900-739a80b9b7dc | DML: 4 key ideas you must know
    to train an LLM... | decodingml.substack.com | 1519b1d1-1a5d-444c-a880-926c9eb6539e
    | Paul Iusztin | [https://decodingml.substack.com/p/dml-4-key-id...](https://decodingml.substack.com/p/dml-4-key-id...)
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 73 | 68795a4d-26c2-43b7-9900-739a80b9b7dc | DML: 训练LLM你必须知道的4个关键想法... | decodingml.substack.com
    | 1519b1d1-1a5d-444c-a880-926c9eb6539e | Paul Iusztin | [https://decodingml.substack.com/p/dml-4-key-id...](https://decodingml.substack.com/p/dml-4-key-id...)
    |'
- en: '| 74 | d91b17c0-05d8-4838-bf61-e2abc1573622 | DML: How to add real-time monitoring
    & metrics... | decodingml.substack.com | 1519b1d1-1a5d-444c-a880-926c9eb6539e
    | Paul Iusztin | https://decodingml.substack.com/p/dml-how-to-a... |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 74 | d91b17c0-05d8-4838-bf61-e2abc1573622 | DML: 如何添加实时监控与指标... | decodingml.substack.com
    | 1519b1d1-1a5d-444c-a880-926c9eb6539e | Paul Iusztin | https://decodingml.substack.com/p/dml-how-to-a...
    |'
- en: '| 75 | dcf55b28-2814-4480-a18b-a77d01d44f5f | DML: Top 6 ML Platform Features
    You Must Know ... | decodingml.substack.com | 1519b1d1-1a5d-444c-a880-926c9eb6539e
    | Paul Iusztin | [https://decodingml.substack.com/p/dml-top-6-ml...](https://decodingml.substack.com/p/dml-top-6-ml...)
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 75 | dcf55b28-2814-4480-a18b-a77d01d44f5f | DML: 你必须知道的6大ML平台特性... | decodingml.substack.com
    | 1519b1d1-1a5d-444c-a880-926c9eb6539e | Paul Iusztin | [https://decodingml.substack.com/p/dml-top-6-ml...](https://decodingml.substack.com/p/dml-top-6-ml...)
    |'
- en: If we inspect the content of some articles a little further, we realize that
    some of them have special characters and redundant whitespaces. We can clean this
    with a simple regex.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们进一步检查一些文章的内容，我们会意识到其中一些文章包含特殊字符和多余的空白。我们可以使用简单的正则表达式来清理这些内容。
- en: First, we use `[^\w\s.,!?']` to remove non-alphanumeric characters except for
    apostrophes, periods, commas, exclamation marks, and question marks. Then, we
    use `\s+` to replace multiple consecutive whitespace characters with a single
    space.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`[^\w\s.,!?']`来移除非字母数字字符（除了撇号、句号、逗号、感叹号和问号）。然后，我们使用`\s+`将多个连续的空白字符替换为单个空格。
- en: Finally, we implement `strip()` to remove any leading or trailing whitespace.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现`strip()`来移除任何前导或尾随的空白。
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we can load our articles, we need to chunk them before turning them
    into pairs of instructions and answers. Ideally, you would want to use headlines
    or paragraphs to produce semantically meaningful chunking.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们能够加载我们的文章，在将它们转换成指令和答案的对之前，我们需要对它们进行分块。理想情况下，您会希望使用标题或段落来产生语义上有意义的分块。
- en: However, in our example, like in the real world, raw data tends to be messy.
    Due to improper formatting, we cannot extract paragraphs or headlines for every
    article in our raw dataset. Instead, we will extract sentences using a regex to
    get chunks between 1,000 and 2,000 characters. This number can be optimized depending
    on the density of the information contained in the text.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的示例中，就像现实世界一样，原始数据往往很杂乱。由于格式不正确，我们无法从原始数据集中的每一篇文章中提取段落或标题。相反，我们将使用正则表达式提取句子，以获取1,000到2,000个字符的块。这个数字可以根据文本中包含的信息密度进行优化。
- en: The `extract_substrings` function processes each article in the dataset by first
    cleaning the text and then using a regex to split it into sentences. It then builds
    chunks of text by concatenating these sentences until each chunk is between 1,000
    and 2,000 characters long.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract_substrings` 函数通过首先清理文本然后使用正则表达式将其分割成句子来处理数据集中的每一篇文章。然后，通过连接这些句子来构建文本块，直到每个块的字数在1,000到2,000个字符之间。'
- en: '[PRE4]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we want to create instruction-answer pairs from the extracted chunks of
    text. To manage these pairs effectively, we introduce the `InstructionAnswerSet`
    class. This class allows us to create instances directly from JSON strings, which
    is useful when parsing the output from the OpenAI API.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们想要从提取的文本块中创建指令-答案对。为了有效地管理这些对，我们引入了`InstructionAnswerSet`类。这个类允许我们直接从JSON字符串创建实例，这在解析来自OpenAI
    API的输出时非常有用。
- en: '[PRE5]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have a set of extracts from the articles with a reasonable length,
    we can use an LLM to transform them into pairs of instructions and answers. Note
    that this step is model-agnostic and can be implemented with any open-source or
    closed-source model. Because this output is grounded in the context we provide,
    it doesn’t require complex reasoning or high-performing models.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经从文章中提取了一组长度合理的文本块，我们可以使用LLM将它们转换成指令和答案的对。请注意，这一步是模型无关的，可以使用任何开源或闭源模型实现。因为这个输出基于我们提供的上下文，所以它不需要复杂的推理或高性能的模型。
- en: For convenience, we will use GPT-4o mini in this example. This choice is motivated
    by the low cost and good performance of this model. Prompt engineering is the
    most important aspect of this data transformation stage and requires several iterations
    to produce the expected outputs. We recommend starting with simple prompts and
    adding complexity when required to be more accurate, modify the style, or output
    multiple responses.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们将在这个例子中使用 GPT-4o mini。这个选择是基于该模型低成本和高性能的动机。提示工程是数据转换阶段最重要的方面，需要多次迭代以产生预期的输出。我们建议从简单的提示开始，并在需要更精确、修改风格或输出多个响应时增加复杂性。
- en: In our example, we want to create instructions like “Write a paragraph about
    X topic” and corresponding answers that are factual and imitate the writer’s style.
    To implement this, we need to provide an extract that will ground the model’s
    responses. For efficiency, we also choose to generate five instruction-answer
    pairs for each extract. Here’s the beginning of our function for instruction generation,
    including our prompt.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们想要创建像“关于 X 主题写一段段落”这样的指令和相应的、事实性且模仿作者风格的答案。为了实现这一点，我们需要提供一个将使模型响应有根据的摘录。为了提高效率，我们还选择为每个摘录生成五个指令-答案对。以下是我们的指令生成函数的开始，包括我们的提示。
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In addition to the user prompt, we can also specify a system prompt to guide
    the model into generating the expected instructions. Here, we repeat our high-level
    task in the system prompt.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了用户提示外，我们还可以指定一个系统提示来引导模型生成预期的指令。在这里，我们在系统提示中重复我们的高级任务。
- en: The concatenation of the system and user prompts is fed to the OpenAI API, using
    the GPT-4o mini model in JSON mode and a maximum of 1,200 tokens in the answer.
    We also use a standard temperature of `0.7` to encourage diverse responses. The
    generated text is directly parsed using the InstructionAnswerSet class to return
    pairs of instructions and answers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示和用户提示的连接被输入到 OpenAI API 中，使用 GPT-4o mini 模型以 JSON 模式和最多 1,200 个标记的答案。我们还使用标准的
    `0.7` 温度来鼓励多样化的响应。生成的文本直接使用 InstructionAnswerSet 类进行解析，以返回指令和答案的对。
- en: '[PRE7]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s create a main function to automate the process. It extracts substrings
    from the input dataset, then uses concurrent processing via Python’s `ThreadPoolExecutor`
    to efficiently generate instruction-answer pairs for each extract.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个主函数来自动化这个过程。它从输入数据集中提取子字符串，然后通过 Python 的 `ThreadPoolExecutor` 进行并发处理，以高效地为每个摘录生成指令-答案对。
- en: We use a default `max_workers` value of 4 because higher values tend to exceed
    OpenAI’s rate limits, potentially causing API request failures or throttling.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用默认的 `max_workers` 值为 4，因为更高的值往往会超过 OpenAI 的速率限制，可能导致 API 请求失败或限制。
- en: '[PRE8]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can create our instruction dataset by calling this function. Running it over
    the raw data with GPT-4o mini costs less than 0.5$.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过调用此函数来创建我们的指令数据集。使用 GPT-4o mini 在原始数据上运行的成本低于 0.5 美元。
- en: We can now create a main function to orchestrate the entire pipeline. It loads
    the raw data, creates the instruction dataset, splits it into training and testing
    sets, and pushes the result to the Hugging Face Hub.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个主函数来协调整个流程。它加载原始数据，创建指令数据集，将其分为训练集和测试集，并将结果推送到 Hugging Face Hub。
- en: '[PRE9]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We obtained 3,335 pairs with this process. You can find our version of the dataset
    at [https://huggingface.co/datasets/mlabonne/llmtwin](https://huggingface.co/datasets/mlabonne/llmtwin).
    The Hugging Face Hub provides a convenient dataset viewer (see *Figure 5.7*) to
    explore instructions and answers and make sure that there are no obvious mistakes
    in these samples. Due to the small size of the dataset, there is no need for comprehensive
    exploration and topic clustering.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过这个过程获得了 3,335 对数据。您可以在 [https://huggingface.co/datasets/mlabonne/llmtwin](https://huggingface.co/datasets/mlabonne/llmtwin)
    找到我们版本的数据集。Hugging Face Hub 提供了一个方便的数据集查看器（见 *图 5.7*），用于探索指令和答案，并确保这些样本中没有明显的错误。由于数据集规模较小，没有必要进行全面探索和主题聚类。
- en: '![](img/B31105_05_07.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_05_07.png)'
- en: Figure *5*.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face
    Hub
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *5*.7 – Hugging Face Hub 上的 mlabonne/llmtwin 指令数据集
- en: As seen in the previous section, we could refine this instruction dataset by
    increasing the diversity and complexity of our samples. More advanced prompt engineering
    could also increase the quality of the generated data by providing examples of
    the expected results, for instance. Finally, quality evaluation could help filter
    out low-quality samples by reviewing them individually. For conciseness and simplicity,
    we will keep a straightforward approach for this instruction dataset and explore
    more advanced methods in *Chapter 6* when we create a preference dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，我们可以通过增加样本的多样性和复杂性来细化这个指令数据集。更高级的提示工程也可以通过提供预期结果的示例来提高生成数据的质量，例如。最后，通过逐个审查，质量评估可以帮助过滤掉低质量样本。为了简洁和简单，我们将保持对这一指令数据集的直接方法，并在创建偏好数据集时探索更高级的方法，如第6章所述。
- en: In the next section, we will introduce SFT techniques, as well as related concepts.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍SFT技术以及相关概念。
- en: Exploring SFT and its techniques
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索SFT及其技术
- en: SFT consists of re-training pre-trained models on a smaller dataset composed
    of pairs of instructions and answers. The goal of SFT is to turn a base model,
    which can only perform next-token prediction, into a useful assistant, capable
    of answering questions and following instructions. SFT can also be used to improve
    the general performance of the base model (general-purpose SFT), instill new knowledge
    (e.g., new languages, domains, etc.), focus on specific tasks, adopt a particular
    voice, and so on.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 微调（SFT）包括在由指令和答案对组成的小数据集上重新训练预训练模型。SFT的目标是将只能进行下一个标记预测的基础模型转变为一个有用的助手，能够回答问题和遵循指令。SFT还可以用于提高基础模型的一般性能（通用SFT）、灌输新知识（例如，新语言、领域等）、专注于特定任务、采用特定语气等。
- en: 'In this section, we will discuss when to use fine-tuning and explore related
    concepts with storage formats and chat templates. Finally, we will introduce three
    popular ways of implementing SFT: full-finetuning, **Low-Rank Adaptation** (**LoRA**)
    and **Quantization-aware Low-Rank Adaptation** (**QLoRA**).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论何时使用微调，并探讨与存储格式和聊天模板相关的相关概念。最后，我们将介绍三种流行的SFT实现方式：全微调、**低秩适应**（**LoRA**）和**量化感知低秩适应**（**QLoRA**）。
- en: When to fine-tune
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时进行微调
- en: In most scenarios, it is recommended to start with prompt engineering instead
    of directly fine-tuning models. Prompt engineering can be used with either open-weight
    or closed-source models. By using techniques like few-shot prompting or **retrieval
    augmented generation** (**RAG**), numerous problems can efficiently be tackled
    without SFT. Prompt engineering also allows us to build a robust evaluation pipeline,
    which measures metrics like accuracy, but also cost and latency. If these results
    do not match the requirements, we can explore the possibility of creating an instruction
    dataset, as illustrated in the previous section. If enough data is available,
    fine-tuning becomes an option.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，建议先从提示工程开始，而不是直接微调模型。提示工程可以与开放权重或封闭源模型一起使用。通过使用诸如少样本提示或**检索增强生成**（**RAG**）等技术，可以有效地解决许多问题，而无需SFT。提示工程还允许我们构建一个健壮的评估流程，该流程不仅测量准确性等指标，还测量成本和延迟。如果这些结果不符合要求，我们可以探索创建指令数据集的可能性，如前文所述。如果数据足够，微调就成为一个选项。
- en: '![](img/B31105_05_08.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_05_08.png)'
- en: Figure *5*.8 – Basic flowchart to determine when fine-tuning is an option on
    a technical level
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 技术层面确定何时进行微调的基本流程图
- en: Beyond these technical considerations, SFT answers common needs in terms of
    control (“know your data”) and customizability (the fine-tuned model is unique).
    Instead of building applications around a chatbot, fine-tuning allows developers
    to create more diverse interactions with LLMs, like tool analytics, moderation,
    and additional context. Note that if we focus on open-weight models in this book,
    several LLM providers offer automated fine-tuning services. While they don’t offer
    the same level of control and customizability as managing your own fine-tuning
    pipeline, it can be an interesting trade-off in specific scenarios (e.g., limited
    resources in terms of machine learning engineering).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些技术考虑因素之外，SFT在控制（“了解你的数据”）和可定制性（微调模型是独特的）方面满足了常见需求。与围绕聊天机器人构建应用程序相比，微调允许开发者与LLMs创建更多样化的交互，如工具分析、监管和附加上下文。请注意，如果我们在这本书中关注开放权重模型，几个LLM提供商提供自动微调服务。虽然它们提供的控制度和可定制性不如管理自己的微调管道，但在特定场景下（例如，机器学习工程资源有限）这可能是一个有趣的权衡（例如）。
- en: Despite these advantages, fine-tuning also has limitations. It is generally
    understood that SFT leverages pre-existing knowledge in the base model’s weights
    and refocuses the parameters for a specific purpose. This has several implications.
    First of all, knowledge that is too distant from what has been learned in the
    pre-training set (such as an unknown or rare language) can be difficult to learn
    effectively.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些优势，微调也有局限性。通常认为SFT利用基础模型权重中的现有知识，并重新聚焦参数以实现特定目的。这有几个影响。首先，与预训练集中学习到的知识过于遥远（如未知或罕见语言）的知识可能难以有效学习。
- en: Even worse, a study showed that fine-tuning a model on new knowledge could result
    in more frequent hallucinations. Depending on the SFT technique that is used,
    we’re also at risk of erasing knowledge that was present in the base model (a
    common issue referred to as “catastrophic forgetting”).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，一项研究表明，在新知识上微调模型可能导致更频繁的幻觉。根据使用的SFT技术，我们还面临删除在基础模型中存在的知识的风险（这是一个常见的问题，被称为“灾难性遗忘”）。
- en: Instruction dataset formats
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令数据集格式
- en: Instruction datasets are stored in a particular format to organize instructions
    and answers. Typically, each sample in the dataset can be represented as a Python
    dictionary, where keys are prompt types like `system`, `instruction`, `output`,
    and values corresponding to the actual text. The three most standard formats are
    Alpaca, ShareGPT, and OpenAI. The following table shows how these data formats
    are generally organized.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 指令数据集以特定格式存储，以组织指令和答案。通常，数据集中的每个样本都可以表示为一个Python字典，其中键是提示类型，如`system`、`instruction`、`output`，而值对应实际文本。最标准的三个格式是Alpaca、ShareGPT和OpenAI。以下表格显示了这些数据格式通常是如何组织的。
- en: '| **Name** | **JSONL format** |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **JSONL格式** |'
- en: '| Alpaca | {“instruction”: “...”, “input”: “...”, “output”: “...”}{“instruction”:
    “...”, “output”: “...”} |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | {“instruction”: “...”, “input”: “...”, “output”: “...”}{“instruction”:
    “...”, “output”: “...”} |'
- en: '| ShareGPT | {“conversations”: [{“from”: “...”, “value”: “...”}, …]} |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ShareGPT | {“conversations”: [{“from”: “...”, “value”: “...”}, …]} |'
- en: '| OpenAI | {“conversations”: [{“role”: “...”, “content”: “...”}, …]} |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | {“conversations”: [{“role”: “...”, “content”: “...”}, …]} |'
- en: '| OASST | {“INSTRUCTION”: “...”, “RESPONSE”: “...”} |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| OASST | {“INSTRUCTION”: “...”, “RESPONSE”: “...”} |'
- en: '| Raw text | {“text”: “...”} |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 原始文本 | {“text”: “...”} |'
- en: '*Table 5.5* – Examples of instruction data storage format'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.5* – 指令数据存储格式的示例'
- en: Note that for Alpaca, the “`input`" key is optional. The content of the “`input`"
    key is only appended to the content of the “`instruction`" key when it exists.
    We also added the “`raw text`" data format to show that SFT is not inherently
    different from pre-training. If you choose to re-train a model on raw text, this
    is a type of fine-tuning generally called “continual pre-training.”
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于Alpaca，`input`键是可选的。当`input`键存在时，其内容仅附加到`instruction`键的内容中。我们还添加了`raw text`数据格式，以表明SFT本质上并不与预训练不同。如果你选择在原始文本上重新训练模型，这通常被称为“持续预训练”的微调类型。
- en: The dataset we created in the previous section has two columns (“`instruction`"
    and “`output`") and corresponds to the Alpaca format. Alpaca is sufficient for
    single-turn instructions and answers, which means it is limited to one instruction
    and one answer. When you want to process conversations (multiple instructions
    and answers), formats like ShareGPT or OpenAI are a better fit. By storing each
    message as a dictionary in a list, they can represent an arbitrarily long conversation
    in each sample.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中我们创建的数据集有两列（“`指令`" 和 “`输出`"）并对应于 Alpaca 格式。Alpaca 对于单轮指令和答案来说是足够的，这意味着它仅限于一个指令和一个答案。当您想要处理对话（多个指令和答案）时，ShareGPT
    或 OpenAI 这样的格式更适合。通过将每条消息作为列表中的字典存储，它们可以在每个样本中表示任意长度的对话。
- en: The choice of single-turn and multi-turn conversations directly impacts the
    storage type and depends on the end use case.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 单轮和多轮对话的选择直接影响到存储类型，并取决于最终用途。
- en: Chat templates
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天模板
- en: Once the instruction-answer pairs are parsed from the dataset format, we want
    to structure them in a chat template. Chat templates offer a unified way to present
    the instructions and answers to the model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从数据集格式中解析出指令-答案对，我们希望以聊天模板的形式组织它们。聊天模板为向模型展示指令和答案提供了一种统一的方式。
- en: In general, they also include special tokens to identify the beginning and the
    end of a message, or who is the author of the message. Since base models are not
    designed to follow instructions, they don’t have a chat template. This means that
    you can choose any template when you fine-tune a based model. If you want to fine-tune
    an instruct model (not recommended), you need to use the same template or it might
    degrade your performance.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，它们还包括特殊的标记来识别消息的开始和结束，或者识别消息的作者。由于基础模型不是设计来遵循指令的，它们没有聊天模板。这意味着当您微调一个基础模型时，您可以选择任何模板。如果您想微调一个指令模型（不推荐），您需要使用相同的模板，否则可能会降低您的性能。
- en: 'Like instruction dataset formats, there are different chat templates: ChatML,
    Llama 3, Mistral, and many others. In the open-source community, the ChatML template
    (originally from OpenAI) is a popular option. It simply adds two special tokens
    `(<|im_start|> and <|im_end|>`) to indicate who is speaking. To give you an example,
    here is what we obtain when we apply the ChatML template to the instruction-answer
    pair shown in *Table 5.1*:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与指令数据集格式类似，存在不同的聊天模板：ChatML、Llama 3、Mistral 以及许多其他模板。在开源社区中，ChatML 模板（最初来自 OpenAI）是一个流行的选择。它只是简单地添加了两个特殊标记
    `(<|im_start|> 和 <|im_end|>`) 来指示说话者是谁。为了给您一个例子，以下是当我们将 ChatML 模板应用于 *表 5.1* 中显示的指令-答案对时得到的结果：
- en: '|'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '*Table 5.6* – Sample from *Table 5.1* with the ChatML chat template'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.6* – 使用 ChatML 聊天模板从 *表 5.1* 中提取的样本'
- en: 'As you can see, we still have three distinct parts: system, user, and assistant.
    Each part starts with the `<|im_start|>` token and ends with `<|im_end|>.` The
    current speaker is identified by a string (like “`system`") instead of a special
    token. This is the exact string that is tokenized and used as input by the model
    during fine-tuning.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们仍然有三个不同的部分：系统、用户和助手。每个部分都以 `<|im_start|>` 标记开始，以 `<|im_end|>.` 标记结束。当前说话者通过一个字符串（如“`system`"）来识别，而不是特殊标记。这是在微调期间由模型标记化和用作输入的确切字符串。
- en: However, during inference, we can’t provide the expected answer. In this case,
    we provide the system and user part as shown in *Figure 5.6*, and prompt the model
    to answer by adding `<|im_start|>assistant\n`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在推理过程中，我们无法提供预期的答案。在这种情况下，我们提供系统和用户部分，如图 *图 5.6* 所示，并通过添加 `<|im_start|>assistant\n`
    来提示模型回答。
- en: Because the model has been fine-tuned with this template, it understands that
    the next tokens should be an answer relevant to the user instruction and guided
    by the system prompt. This is how fine-tuned models acquire instruction-following
    capabilities.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型已经使用此模板进行了微调，它理解下一个标记应该是与用户指令相关且由系统提示引导的答案。这就是微调模型获得遵循指令能力的方式。
- en: A common issue with chat templates is that every single whitespace and line
    break is extremely important. Adding or removing any character would result in
    a wrong tokenization, which negatively impacts the performance of the model. For
    this reason, it is recommended to use reliable templates like Jinja, as implemented
    in the Transformers library. *Table 5.7* shows a few examples of such templates,
    including Alpaca, which is both the name of an instruction dataset format and
    a chat template.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模板的一个常见问题是每个空格和换行都非常重要。添加或删除任何字符都可能导致错误的标记化，这会负面影响模型的性能。因此，建议使用像Transformers库中实现的Jinja这样的可靠模板。*表5.7*展示了此类模板的一些示例，包括Alpaca，它既是指令数据集格式也是聊天模板的名称。
- en: '| **Name** | **Jinja template** |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **Jinja模板** |'
- en: '| Alpaca |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca |'
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ChatML |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| ChatML |'
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama 3 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 |'
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Phi-3 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3 |'
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Gemma |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Gemma |'
- en: '[PRE15]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '*Table 5.7* – Example of common chat templates'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.7* – 常见聊天模板的示例'
- en: Jinja implements loops and conditions, which allow the same template to be used
    for training and inference (`add_generation_prompt`).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Jinja实现了循环和条件，允许相同的模板用于训练和推理（`add_generation_prompt`）。
- en: Parameter-efficient fine-tuning techniques
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数高效的微调技术
- en: 'While many techniques exist in the literature, SFT has converged on three main
    techniques: full fine-tuning, LoRA, and QLoRA. We will introduce each technique
    individually, and weigh their pros and cons depending on your use cases.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文献中存在许多技术，但SFT已经收敛到三种主要技术：完全微调、LoRA和QLoRA。我们将分别介绍每种技术，并根据您的用例权衡它们的优缺点。
- en: '![](img/B31105_05_09.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_05_09.png)'
- en: Figure 5.9 – Architectural differences of the three main SFT techniques at the
    module level
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 三种主要SFT技术在模块级别的架构差异
- en: Full fine-tuning
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全微调
- en: Full fine-tuning refers to the most straightforward SFT technique, consisting
    of re-training every parameter in the base model. Like pre-training, SFT uses
    next-token prediction as its training objective. This means that the previously
    discussed structure of the dataset can be seen as the main difference between
    continual pre-training and full fine-tuning.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 完全微调是指最直接的SFT技术，包括重新训练基础模型中的每一个参数。与预训练类似，SFT使用下一个标记预测作为其训练目标。这意味着之前讨论的数据集结构可以看作是持续预训练和完全微调之间的主要区别。
- en: 'This method often provides the best results but requires significant computational
    resources. Memory usage depends on several factors, including model size, training
    techniques, and optimization methods. At its simplest, using a single-GPU setting,
    the memory required can be estimated using the following formula:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通常提供最佳结果，但需要大量的计算资源。内存使用量取决于多个因素，包括模型大小、训练技术和优化方法。在最简单的情况下，使用单GPU设置，所需的内存可以使用以下公式估计：
- en: '![](img/B31105_05_001.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_05_001.png)'
- en: 'For a basic setup using **32-bit floating point** (**fp32**) precision, we
    can estimate:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用**32位浮点**（**fp32**）精度的基本设置，我们可以估计：
- en: '**Parameters**: Learnable weights and biases within a neural network. In a
    large language model, these are typically the weights in the attention mechanisms,
    feed-forward layers, and embedding layers. Cost: 4 bytes/parameter (FP32) or 2
    bytes/parameter (FP16/BF16).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：神经网络中的可学习权重和偏差。在一个大型语言模型中，这些通常是注意力机制、前馈层和嵌入层中的权重。成本：每个参数4字节（FP32）或2字节（FP16/BF16）。'
- en: '**Gradients**: Gradients are the partial derivatives of the loss function with
    respect to each model parameter. They indicate how much each parameter should
    be adjusted to minimize the loss. During training, gradients are computed for
    each parameter through backpropagation and are used to update the model parameters.
    Cost: 4 bytes/parameter.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度**：梯度是损失函数相对于每个模型参数的偏导数。它们指示每个参数应该调整多少以最小化损失。在训练过程中，通过反向传播为每个参数计算梯度，并用于更新模型参数。成本：每个参数4字节。'
- en: '**Optimizer states**: Optimizer states are additional values maintained by
    optimization algorithms like Adam or AdamW. These typically include running averages
    of past gradients and past squared gradients for each parameter. They help in
    adapting the learning rate for each parameter and navigating the loss landscape
    more effectively. For instance, Adam maintains two additional values (momentum
    and variance) per parameter. Cost: 8 bytes/parameter (for Adam optimizer).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器状态**：优化器状态是优化算法（如Adam或AdamW）维护的附加值。这些通常包括每个参数过去梯度和过去平方梯度的运行平均值。它们有助于为每个参数调整学习率并更有效地导航损失景观。例如，Adam为每个参数维护两个额外的值（动量和方差）。成本：每参数8字节（对于Adam优化器）。'
- en: '**Activations**: Activations are the intermediate outputs of each layer in
    the neural network during the forward pass. For transformer-based models, this
    includes the outputs of attention mechanisms, feed-forward layers, and normalization
    layers. Activations need to be kept in memory during the forward pass to compute
    gradients in the backward pass, unless techniques like activation checkpointing
    are used. Cost: variable, but often negligible for small batch sizes.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活**：激活是神经网络在正向传递过程中每个层的中间输出。对于基于transformer的模型，这包括注意力机制、前馈层和归一化层的输出。在正向传递期间，需要将激活保存在内存中，以便在反向传递中计算梯度，除非使用像激活检查点这样的技术。成本：可变，但对于小批量大小通常可以忽略不计。'
- en: This gives us a baseline of 16 bytes per parameter. This translates into 112
    GB of VRAM for a 7 B model and 1,120 GB for a 70 B model. However, this is often
    an underestimate, as it doesn’t account for additional memory needed for activations,
    temporary buffers, and overhead from various training techniques.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个每参数16字节的基线。这相当于7B模型的112 GB VRAM和70B模型的1,120 GB VRAM。然而，这通常是一个低估，因为它没有考虑到激活、临时缓冲区和各种训练技术开销所需的额外内存。
- en: Several techniques can be employed to reduce memory usage during LLM fine-tuning.
    Model parallelism spreads the workload across multiple GPUs, though it adds some
    overhead. Gradient accumulation enables larger effective batch sizes without proportional
    memory increase. Memory-efficient optimizers like 8-bit Adam can reduce the footprint
    of optimizer states. Activation checkpointing trades computation for memory by
    recalculating certain activations. When combined, these techniques can significantly
    lower memory usage. For instance, using mixed precision with model parallelism
    might reduce costs to around 14-15 bytes per parameter, compared to the 16-byte
    baseline. However, memory requirements remain substantial for large models even
    with these optimizations.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 可以采用几种技术来减少在LLM微调期间的内存使用。模型并行将工作负载分散到多个GPU上，尽管这会增加一些开销。梯度累积允许在不成比例增加内存的情况下实现更大的有效批量大小。内存高效的优化器，如8位Adam，可以减少优化器状态的大小。通过重新计算某些激活，激活检查点以计算成本换取内存。当结合使用时，这些技术可以显著降低内存使用。例如，使用混合精度和模型并行可能将成本降低到每参数14-15字节，而基线为16字节。然而，即使有这些优化，大型模型的内存需求仍然很大。
- en: In addition, full fine-tuning directly modifies the pre-training weights, which
    makes it destructive by nature. If training doesn’t behave as expected, it might
    erase previous knowledge and skills – a phenomenon referred to as “catastrophic
    forgetting.” The same phenomenon can happen with continual pre-training, which
    generally makes these techniques more difficult to use. Due to this additional
    complexity and its high computational requirements, parameter-efficient techniques
    are often preferred to full fine-tuning to create task and domain-specific models.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，完全微调会直接修改预训练权重，这使得它本质上具有破坏性。如果训练行为不符合预期，可能会擦除以前的知识和技能——这种现象被称为“灾难性遗忘”。同样的现象也可能发生在持续预训练中，这通常使得这些技术更难使用。由于这种额外的复杂性和其高计算需求，参数高效的技术通常比完全微调更受欢迎，以创建特定任务和领域的模型。
- en: LoRA
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoRA
- en: LoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address
    the computational challenges associated with adapting massive neural networks,
    LoRA has quickly become a cornerstone technique in LLM fine-tuning.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA是一种用于微调LLM的参数高效技术。为了解决适应大规模神经网络相关的计算挑战而开发，LoRA很快成为LLM微调的一个基石技术。
- en: 'The primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly
    reduced computational resources. This is achieved by introducing trainable low-rank
    matrices that modify the behavior of the model without changing its original parameters.
    The key advantages of LoRA include:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的主要目的是通过引入可训练的低秩矩阵来降低 LLM 微调所需的计算资源。这是通过在不改变模型原始参数的情况下修改模型行为来实现的。LoRA 的关键优势包括：
- en: Dramatically reduced memory usage during training
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中显著降低内存使用
- en: Faster fine-tuning process
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快的微调过程
- en: Preservation of pre-trained model weights (non-destructive)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留预训练模型权重（非破坏性）
- en: Ability to switch between tasks efficiently by swapping LoRA weights
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够通过交换 LoRA 权重来高效地在任务之间切换
- en: These benefits have made LoRA particularly attractive for researchers and developers
    working with limited computational resources, effectively democratizing the process
    of LLM fine-tuning.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处使得 LoRA 对于使用有限计算资源的研发人员特别有吸引力，有效地民主化了 LLM 微调的过程。
- en: At its core, LoRA employs a low-rank decomposition technique to update model
    weights efficiently. Instead of directly modifying the original weight matrix
    ![](img/B31105_05_002.png), LoRA introduces two smaller matrices, ![](img/B31105_05_003.png)
    and ![](img/B31105_05_004.png), which together form a low-rank update to ![](img/B31105_05_002.png).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，LoRA 使用低秩分解技术来高效地更新模型权重。而不是直接修改原始权重矩阵 ![](img/B31105_05_002.png)，LoRA 引入了两个较小的矩阵，![](img/B31105_05_003.png)
    和 ![](img/B31105_05_004.png)，它们共同构成了对 ![](img/B31105_05_002.png) 的低秩更新。
- en: '![A diagram of a diagram  Description automatically generated with medium confidence](img/B31105_05_10.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的图表描述，自动生成，中等置信度](img/B31105_05_10.png)'
- en: Figure 5.10 – LoRA adds the two trainable matrices ![](img/B31105_05_006.png)
    and ![](img/B31105_05_007.png) and keeps the pre-trained weights ![](img/B31105_05_008.png)
    frozen
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – LoRA 添加了两个可训练矩阵 ![](img/B31105_05_006.png) 和 ![](img/B31105_05_007.png)，并保持预训练权重
    ![](img/B31105_05_008.png) 冻结
- en: 'Mathematically, this can be represented as:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这可以表示为：
- en: '![](img/B31105_05_009.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_05_009.png)'
- en: Here, ![](img/B31105_05_002.png) is the original weight matrix, ![](img/B31105_05_007.png)
    and ![](img/B31105_05_006.png) are the LoRA matrices, and ![](img/B31105_05_013.png)
    is the effective weight matrix used during inference.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B31105_05_002.png) 是原始权重矩阵，![](img/B31105_05_007.png) 和 ![](img/B31105_05_006.png)
    是 LoRA 矩阵，而 ![](img/B31105_05_013.png) 是推理过程中使用的有效权重矩阵。
- en: The dimensions of matrices A and B are chosen such that their product has the
    same shape as ![](img/B31105_05_014.png), but with a much lower rank. This rank,
    typically denoted as ![](img/B31105_05_015.png), is a crucial hyperparameter in
    LoRA. During training, the original weights ![](img/B31105_05_014.png) remain
    frozen, while only ![](img/B31105_05_006.png) and ![](img/B31105_05_007.png) are
    updated. This approach significantly reduces the number of trainable parameters,
    leading to substantial memory savings and faster training times.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 A 和 B 的维度被选择，使得它们的乘积与 ![](img/B31105_05_014.png) 的形状相同，但秩要低得多。这个秩，通常表示为 ![](img/B31105_05_015.png)，是
    LoRA 中的一个关键超参数。在训练过程中，原始权重 ![](img/B31105_05_014.png) 保持冻结状态，而只有 ![](img/B31105_05_006.png)
    和 ![](img/B31105_05_007.png) 被更新。这种方法显著减少了可训练参数的数量，从而实现了大量的内存节省和更快的训练时间。
- en: 'To implement LoRA effectively, we need to select the correct hyperparameters
    and target modules. LoRA comes with two hyperparameters:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地实现 LoRA，我们需要选择正确的超参数和目标模块。LoRA 包含两个超参数：
- en: '**Rank** (![](img/B31105_05_019.png)): Determines the size of the LoRA matrices.
    A common starting point is ![](img/B31105_05_020.png), but values up to 256 have
    shown good results in some cases. Larger ranks may capture more diverse tasks
    but could lead to overfitting.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩** (![](img/B31105_05_019.png))：决定了 LoRA 矩阵的大小。一个常见的起点是 ![](img/B31105_05_020.png)，但在某些情况下，值高达
    256 已经显示出良好的效果。更大的秩可能能够捕捉更多样化的任务，但也可能导致过拟合。'
- en: '**Alpha** (![](img/B31105_05_021.png)): A scaling factor applied to the LoRA
    update. In practice, we update the frozen weights ![](img/B31105_05_022.png) by
    a factor of ![](img/B31105_05_023.png). This is why a common heuristic is to set
    ![](img/B31105_05_021.png) to twice the value of ![](img/B31105_05_019.png), effectively
    applying a scaling factor of 2 to the LoRA update. You can experiment with different
    ratios in case of overfitting or underfitting.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alpha** (![](img/B31105_05_021.png))：应用于LoRA更新的缩放因子。在实践中，我们通过一个因子![](img/B31105_05_023.png)更新冻结的权重![](img/B31105_05_022.png)。这就是为什么一个常见的启发式方法是设置![](img/B31105_05_021.png)为![](img/B31105_05_019.png)的两倍，从而在LoRA更新中应用一个缩放因子2。在过拟合或欠拟合的情况下，您可以尝试不同的比率。'
- en: In addition, it is possible to add a drop-out layer to prevent overfitting.
    The dropout rate is usually set between 0 and 0.1 as an optional regularization
    factor, which slightly decreases training speed.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以添加一个dropout层来防止过拟合。dropout率通常设置为0到0.1，作为一个可选的正则化因子，它略微降低了训练速度。
- en: 'LoRA can be applied to various parts of the model architecture. Initially,
    LoRA was primarily focused on modifying the attention mechanism, specifically
    the **query** (**Q**) and **value** (**V**) matrices in transformer layers. However,
    experiments have demonstrated significant benefits in extending LoRA’s application
    to other key components of the model. These additional target modules include:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA可以应用于模型架构的各个部分。最初，LoRA主要关注修改注意力机制，特别是transformer层中的**查询**（**Q**）和**值**（**V**）矩阵。然而，实验表明，将LoRA的应用扩展到模型的其它关键组件也能带来显著的好处。这些额外的目标模块包括：
- en: '**Key** (**K**) matrices in attention layers'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力层中的**关键**（**K**）矩阵
- en: Output projection layers (often denoted as O) in attention mechanisms
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制中的输出投影层（通常表示为O）
- en: Feed-forward or **Multi-Layer Perceptron** (**MLP**) blocks between attention
    layers
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力层之间的前馈或**多层感知器**（**MLP**）块
- en: Linear output layers
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性输出层
- en: However, it’s important to note that increasing the number of LoRA-adapted modules
    also increases the number of trainable parameters and, consequently, the memory
    requirements.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，增加LoRA适配模块的数量也会增加可训练参数的数量，从而增加内存需求。
- en: Using LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU
    with as little as 14-18 GB of VRAM, depending on the specific configuration. This
    is a dramatic reduction compared to full fine-tuning, which would typically require
    multiple high-end GPUs. In terms of trainable parameters, LoRA drastically reduces
    the number compared to full fine-tuning. For example, even when targeting every
    module with a rank of 16, a Llama 3 8 B model only has 42 million trainable LoRA
    parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA，可以在单个GPU上微调一个7B参数模型，只需14-18 GB的VRAM，具体取决于配置。与通常需要多个高端GPU的全微调相比，这是一个巨大的减少。在可训练参数方面，LoRA与全微调相比大幅减少了数量。例如，即使针对每个具有16阶的模块，Llama
    3 8 B模型也只有8亿参数中的4200万可训练LoRA参数，占模型参数的0.5196%。
- en: In terms of quality, LoRA can also achieve comparable or sometimes better results
    than full-fine-tuning. Multiple sets of LoRA weights can be combined for different
    tasks or domains, allowing flexible deployment and task switching without retraining.
    Different projects are specialized in multiple-LoRA serving, such as LoRAX. It’s
    also a feature supported by Hugging Face’s **Text Generation Inference** (**TGI**)
    and **Nvidia Inference Microservices** (**NIM**).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在质量方面，LoRA也能达到与全微调相当甚至更好的结果。可以通过组合不同任务或领域的多个LoRA权重集来实现灵活部署和任务切换，而无需重新训练。多个项目专注于多LoRA服务，例如LoRAX。这也是Hugging
    Face的**文本生成推理**（**TGI**）和**Nvidia推理微服务**（**NIM**）支持的功能。
- en: QLoRA
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QLoRA
- en: Introduced by Dettmers et al., QLoRA is a method for fine-tuning LLMs that addresses
    the challenges of high computational costs. By combining quantization techniques
    with LoRA, QLoRA allows developers to fine-tune models on relatively small, widely
    available GPUs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 由Dettmers等人提出的QLoRA是一种针对高计算成本挑战的LLM微调方法。通过结合量化技术与LoRA，QLoRA允许开发者在小型且广泛可用的GPU上微调模型。
- en: The core of QLoRA’s approach involves quantizing the base model parameters to
    a custom **4-bit NormalFloat** (**NF4**) data type, which significantly reduces
    memory usage. Like LoRA, instead of updating all model parameters during fine-tuning,
    QLoRA introduces small, trainable low-rank matrices (adapters) to specific layers
    of the model. Only these adapters are updated during training, while the original
    model weights remain unchanged. To further reduce memory usage, QLoRA employs
    double quantization, which quantizes the quantization constants themselves. Additionally,
    it uses paged optimizers to manage memory spikes during training by leveraging
    Nvidia’s unified memory feature.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA方法的核心是将基础模型参数量化到自定义的**4位NormalFloat**（NF4）数据类型，这显著降低了内存使用。与LoRA类似，QLoRA在微调期间不是更新所有模型参数，而是引入小的、可训练的低秩矩阵（适配器）到模型的特定层。只有这些适配器在训练期间被更新，而原始模型权重保持不变。为了进一步减少内存使用，QLoRA采用双重量化，量化量化常数本身。此外，它使用分页优化器通过利用Nvidia的统一内存功能来管理训练期间的内存峰值。
- en: QLoRA provides significant memory savings compared to LoRA, reducing peak GPU
    memory usage by up to 75%. For example, for a 7B model, QLoRA reduces peak memory
    usage from 14 GB to 9.1 GB during initialization, a 35% reduction. During fine-tuning,
    the memory savings increase to 40%, from 15.6 GB for LoRA to 9.3 GB for QLoRA.
    However, this memory efficiency comes at the cost of increased training time,
    with QLoRA being about 30% slower than LoRA. In terms of model performance, QLoRA
    shows only minor differences compared to LoRA.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 与LoRA相比，QLoRA提供了显著的内存节省，可以将峰值GPU内存使用量降低高达75%。例如，对于一个70亿参数的模型，QLoRA在初始化期间将峰值内存使用量从14
    GB降低到9.1 GB，减少了35%。在微调期间，内存节省增加到40%，从LoRA的15.6 GB增加到QLoRA的9.3 GB。然而，这种内存效率是以增加训练时间为代价的，QLoRA比LoRA慢约30%。在模型性能方面，QLoRA与LoRA相比只有细微的差异。
- en: In summary, QLoRA is particularly beneficial when memory constraints are the
    primary concern, such as when working with very large models or on hardware with
    limited GPU memory. However, if training speed is crucial and sufficient memory
    is available, LoRA might be the preferred choice.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当内存限制是主要关注点时，例如在处理非常大的模型或在有限的GPU内存的硬件上工作时，QLoRA特别有益。然而，如果训练速度至关重要且内存充足，LoRA可能是一个更好的选择。
- en: The decision between QLoRA and LoRA should be based on the specific requirements
    of the project, available hardware, and the need to balance memory usage, training
    speed, and model performance.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA和LoRA之间的选择应基于项目的具体要求、可用硬件以及平衡内存使用、训练速度和模型性能的需求。
- en: Training parameters
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练参数
- en: When fine-tuning LLMs, several hyperparameters guide the training process and
    significantly impact the model’s convergence, generalization, and overall effectiveness.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调LLM时，几个超参数指导训练过程，并显著影响模型的收敛、泛化能力和整体有效性。
- en: Learning rate and scheduler
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率和调度器
- en: The learning rate is the most important hyperparameter. It controls how much
    the model’s parameters are updated during training. It typically ranges from very
    small values like `1e-6` to larger values like `1e-3`. A common starting point
    for transformer models is often around `1e-5`. If the learning rate is too low,
    training progresses slowly and may get stuck in suboptimal solutions. Conversely,
    if it’s too high, training can become unstable or diverge, leading to poor performance.
    It’s often beneficial to experiment with different learning rates to find the
    optimal value for your specific task and model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是最重要的超参数。它控制模型参数在训练期间更新的程度。它通常范围从非常小的值，如`1e-6`，到较大的值，如`1e-3`。对于transformer模型，一个常见的起点通常是`1e-5`。如果学习率太低，训练进展缓慢，可能会陷入次优解。相反，如果太高，训练可能会变得不稳定或发散，导致性能不佳。通常，尝试不同的学习率以找到特定任务和模型的最佳值是有益的。
- en: The learning rate scheduler adjusts the learning rate throughout the training
    process. It typically starts with a higher learning rate to enable rapid initial
    progress, then gradually decreases it in later stages to fine-tune the model more
    precisely. The two most common types of schedulers are linear and cosine. A linear
    scheduler decreases the learning rate steadily over time, while a cosine scheduler
    follows a cosine curve, decreasing more slowly at first and then more rapidly
    toward the end of training. For example, you might start with a learning rate
    of 3e-4 and decrease it to 1e-7 over the course of training. The specific values
    and decay schedule depend on your model and dataset, but a common approach is
    to use a warmup period (e.g., 5% of total steps) where the learning rate increases
    from 0 to the initial value, followed by a decay period for the remaining 95%
    of steps. This approach helps stabilize early training and allows for more refined
    updates as the model converges. In general, linear and cosine schedulers provide
    the same level of performance.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度器在整个训练过程中调整学习率。它通常以较高的学习率开始，以实现快速初始进展，然后在后期逐渐降低，以更精确地微调模型。最常见的两种调度器类型是线性调度器和余弦调度器。线性调度器随着时间的推移稳步降低学习率，而余弦调度器遵循余弦曲线，在训练初期降低较慢，然后在训练后期加快。例如，你可能会从3e-4的学习率开始，在训练过程中降低到1e-7。具体值和衰减计划取决于你的模型和数据集，但常见的方法是使用预热期（例如，总步骤的5%）在此期间学习率从0增加到初始值，然后是剩余95%步骤的衰减期。这种方法有助于稳定早期训练，并在模型收敛时允许更精细的更新。一般来说，线性调度器和余弦调度器提供相同级别的性能。
- en: Batch size
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理大小
- en: The batch size determines the number of samples processed before the model’s
    weights are updated. Typical batch sizes for LLM fine-tuning range from 1 to 32,
    with common values being 1, 2, 4, 8, or 16\. Larger batch sizes generally lead
    to more stable gradient estimates and can improve training speed, as they provide
    a better approximation of the true gradient of the entire dataset.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小决定了在模型权重更新之前处理的样本数量。对于LLM微调，典型的批处理大小范围从1到32，常见的值有1、2、4、8或16。较大的批处理大小通常会导致更稳定的梯度估计，并可以提高训练速度，因为它们提供了对整个数据集真实梯度的更好近似。
- en: However, they also require more memory, which can be a limiting factor on GPUs
    with less VRAM. For instance, a batch size of 16 might work well on a high-end
    GPU with 24GB of memory, while a smaller GPU with 8 GB might only handle a batch
    size of 2 or 4.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们也需要更多的内存，这可能是具有较少VRAM的GPU的限制因素。例如，一个16的批处理大小在一个具有24GB内存的高端GPU上可能工作得很好，而一个较小的8GB
    GPU可能只能处理2或4的批处理大小。
- en: To overcome memory constraints while still benefiting from larger batch sizes,
    a technique called gradient accumulation can be used. It works by performing multiple
    forward and backward passes with smaller mini-batches, accumulating the gradients
    over these steps before applying a single update to the model’s parameters. This
    approach is particularly useful when working with large models or limited GPU
    memory. For example, if you want to achieve an effective batch size of 32 but
    your GPU can only handle 8 samples at a time, you can set the gradient accumulation
    steps to 4\. This means you’ll process 4 mini-batches of 8 samples each, accumulating
    the gradients, and then update the model as if you had processed all 32 samples
    at once.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在仍然受益于较大批处理大小的同时克服内存限制，可以使用一种称为梯度累积的技术。它通过执行多个带有较小迷你批次的正向和反向传递，在这些步骤中累积梯度，然后在应用单个更新到模型参数之前，对累积的梯度进行更新。这种方法在处理大型模型或有限的GPU内存时特别有用。例如，如果你想达到32个有效批处理大小，但你的GPU一次只能处理8个样本，你可以将梯度累积步骤设置为4。这意味着你将处理4个包含8个样本的迷你批次，累积梯度，然后像一次性处理所有32个样本一样更新模型。
- en: 'The number of gradient accumulation steps typically ranges from 1 (no accumulation)
    to 8 or 16, depending on the desired effective batch size and available computational
    resources. When choosing the number of steps, consider the trade-off between training
    speed and memory usage. More accumulation steps allow for larger effective batch
    sizes but increase the time required for each update. Here’s a simple formula
    to determine the effective batch size:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积步骤的数量通常从1（无累积）到8或16不等，具体取决于期望的有效批处理大小和可用的计算资源。在选择步骤数量时，请考虑训练速度和内存使用之间的权衡。更多的累积步骤允许更大的有效批处理大小，但会增加每次更新的时间。以下是一个简单的公式，用于确定有效批处理大小：
- en: '![](img/B31105_05_026.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_05_026.png)'
- en: For instance, if you’re using 2 GPUs, each processing a batch of 4 samples,
    with 4 gradient accumulation steps, your effective batch size would be `4 * 2
    * 4 = 32` samples.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你使用2个GPU，每个处理4个样本的批次，并且有4个梯度累积步骤，你的有效批次大小将是`4 * 2 * 4 = 32`个样本。
- en: Maximum length and packing
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大长度和打包
- en: The maximum sequence length determines the longest input the model can process.
    It’s typically set between 512 and 4,096 tokens but can go up to 128,000 or more,
    depending on the task and available GPU memory. For example, a maximum length
    of 2,048 tokens is common for many language generation tasks, while RAG applications
    might use up to 8,192 tokens or more. When processing input data, sequences longer
    than this limit are truncated, meaning excess tokens are removed. Truncation can
    occur at the beginning (left truncation) or end (right truncation) of the sequence.
    For instance, with a maximum length of 1,024 tokens, a 1,500-token input would
    have 476 tokens removed. This parameter directly impacts batch size and memory
    usage; a batch size of 12 with a max length of 1,024 would contain 12,288 tokens
    (`12 * 1,024`), while the same batch size with a max length of 512 would only
    contain 6,144 tokens. It’s important to balance this parameter with your GPU capabilities
    and the nature of your training data to optimize performance and resource utilization.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最大序列长度决定了模型可以处理的最长输入。它通常设置在512到4,096个标记之间，但根据任务和可用的GPU内存，可以高达128,000个或更多。例如，对于许多语言生成任务，最大长度为2,048个标记是常见的，而RAG应用可能使用高达8,192个标记或更多。在处理输入数据时，超过此限制的序列将被截断，这意味着多余的标记将被移除。截断可以发生在序列的开始（左截断）或结束（右截断）。例如，最大长度为1,024个标记的情况下，1,500个标记的输入将移除476个标记。此参数直接影响批次大小和内存使用；一个最大长度为1,024的批次大小为12将包含12,288个标记（`12
    * 1,024`），而相同批次大小但最大长度为512的批次将只包含6,144个标记。平衡此参数与你的GPU能力和训练数据的性质对于优化性能和资源利用非常重要。
- en: Packing maximizes the utilization of each training batch. Instead of assigning
    one sample per batch, packing combines multiple smaller samples into a single
    batch, effectively increasing the amount of data processed in each iteration.
    For example, if your maximum sequence length is 1,024 tokens, but many of your
    samples are only 200-300 tokens long, packing could allow you to fit 3-4 samples
    into each batch slot. This approach can significantly improve training efficiency,
    especially when dealing with datasets containing many short sequences. However,
    packing requires careful implementation to ensure that model attention doesn’t
    cross between packed samples. This is typically achieved by using attention masks
    that prevent the model from attending to tokens from different samples within
    the same packed sequence.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 打包最大化了每个训练批次的利用率。而不是每个批次分配一个样本，打包将多个较小的样本组合成一个批次，从而有效地增加了每次迭代的处理数据量。例如，如果你的最大序列长度是1,024个标记，但许多样本只有200-300个标记长，打包可能允许你将3-4个样本放入每个批次槽位。这种方法可以显著提高训练效率，尤其是在处理包含许多短序列的数据集时。然而，打包需要谨慎实现，以确保模型注意力不会在打包样本之间交叉。这通常是通过使用注意力掩码来实现的，它可以防止模型关注同一打包序列中来自不同样本的标记。
- en: Number of epochs
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练轮数
- en: 'The number of epochs is another important parameter, representing the number
    of complete passes through the entire training dataset. For LLM fine-tuning, the
    typical range is 1 to 10 epochs, with many successful runs using 2 to 5 epochs.
    The optimal number depends on factors such as task complexity, dataset size, and
    model architecture. More epochs allow the model to refine its learning, potentially
    improving performance. However, there’s a crucial trade-off: too few epochs may
    lead to underfitting, while too many can cause overfitting. For example, a large
    model fine-tuned on a small dataset might only need 1-3 epochs, while a smaller
    model fine-tuned on a larger dataset could benefit from 5-10 epochs. It is helpful
    to monitor validation performance during training and implement early stopping
    if the model’s performance plateaus or degrades. This approach helps determine
    the optimal number of epochs dynamically and prevents overfitting.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数是另一个重要参数，表示整个训练数据集的完整遍历次数。对于LLM微调，典型的范围是1到10次迭代，许多成功的运行使用了2到5次迭代。最佳迭代次数取决于任务复杂性、数据集大小和模型架构等因素。更多的迭代次数允许模型细化其学习，从而可能提高性能。然而，存在一个关键的权衡：迭代次数太少可能导致欠拟合，而太多则可能导致过拟合。例如，在一个小型数据集上微调的大型模型可能只需要1-3次迭代，而一个在大型数据集上微调的小型模型可能从5-10次迭代中受益。在训练期间监控验证性能并实施早期停止，如果模型性能停滞或下降，是有帮助的。这种方法有助于动态确定最佳迭代次数并防止过拟合。
- en: Optimizers
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: Optimizers adjust the model’s parameters to minimize the loss function. For
    LLM fine-tuning, AdamW (Adaptive Moment Estimation with Weight Decay) is highly
    recommended, particularly its 8-bit version. AdamW 8-bit performs comparably to
    the 32-bit version while using less GPU memory (but it doesn’t improve training
    speed). AdamW combines adaptive learning rates with weight decay regularization,
    often leading to better training stability and model performance.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器通过调整模型参数来最小化损失函数。对于LLM微调，强烈推荐使用AdamW（自适应动量估计与权重衰减），尤其是其8位版本。AdamW 8位在性能上与32位版本相当，同时使用更少的GPU内存（但它不会提高训练速度）。AdamW结合了自适应学习率与权重衰减正则化，通常会导致更好的训练稳定性和模型性能。
- en: For scenarios with severe memory constraints, AdaFactor presents an alternative
    designed for memory efficiency. It works well without explicit learning rate tuning,
    making it particularly useful in resource-constrained environments. However, it
    may not always match AdamW’s performance in all cases. In situations involving
    extremely large models or limited GPU memory, paged versions of optimizers, such
    as paged AdamW 8-bit, can further reduce memory consumption by offloading to CPU
    RAM. If memory allows and maximum performance is the priority, the non-quantized
    `adamw_torch` optimizer may be the best choice.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存约束严重的场景，AdaFactor提供了一种针对内存效率设计的替代方案。它无需显式调整学习率即可良好工作，因此在资源受限的环境中特别有用。然而，它可能并不总是能在所有情况下匹配AdamW的性能。在涉及极大型模型或有限GPU内存的情况下，优化器的分页版本，如分页AdamW
    8-bit，可以通过将部分工作卸载到CPU RAM来进一步减少内存消耗。如果内存允许且最大性能是优先考虑的，非量化的`adamw_torch`优化器可能是最佳选择。
- en: Weight decay
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重衰减
- en: Weight decay works by adding a penalty for large weights to the loss function,
    encouraging the model to learn simpler, more generalizable features. This helps
    the model avoid relying too heavily on any single input feature, which can improve
    its performance on unseen data. Typically, weight decay values range from 0.01
    to 0.1, with 0.01 being a common starting point. For example, if you’re using
    the AdamW optimizer, you might set the weight decay to 0.01.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减通过在损失函数中添加对大权重的惩罚来实现，鼓励模型学习更简单、更通用的特征。这有助于模型避免过度依赖任何单个输入特征，从而提高其在未见数据上的性能。通常，权重衰减的值范围在0.01到0.1之间，0.01是一个常见的起始点。例如，如果你使用AdamW优化器，你可能将权重衰减设置为0.01。
- en: While weight decay can be beneficial, setting it too high can impede learning
    by making it difficult for the model to capture important patterns in the data.
    Conversely, setting it too low may not provide sufficient regularization. The
    optimal weight decay value often depends on the specific model architecture and
    dataset, so it’s generally a good practice to experiment with different values.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然权重衰减可能有益，但设置得太高可能会阻碍学习，使模型难以捕捉数据中的重要模式。相反，设置得太低可能不足以提供足够的正则化。最佳权重衰减值通常取决于特定的模型架构和数据集，因此通常是一个好习惯，即尝试不同的值进行实验。
- en: Gradient checkpointing
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度检查点
- en: Gradient checkpointing is a technique that reduces memory consumption during
    training by storing only a subset of intermediate activations generated in the
    forward pass. In standard training procedures, all intermediate activations are
    retained in memory to facilitate gradient calculation during the backward pass.
    However, for very deep networks like LLMs, this approach can quickly become impractical
    due to hardware limitations, especially on GPUs with limited memory capacity.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点是一种在训练过程中通过仅存储正向传递中生成的中间激活子集来减少内存消耗的技术。在标准训练过程中，所有中间激活都保留在内存中，以方便反向传播期间的梯度计算。然而，对于像LLMs这样非常深的网络，这种方法可能会因为硬件限制而迅速变得不切实际，尤其是在内存容量有限的GPU上。
- en: Gradient checkpointing addresses this challenge by selectively saving activations
    at specific layers within the network. For layers where activations are not saved,
    they are recomputed during the backward pass as needed for gradient computation.
    This approach creates a trade-off between computation time and memory usage. While
    it significantly reduces memory requirements, it may increase overall computation
    time due to the need to recalculate some activations.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点通过在网络中特定层选择性地保存激活来解决这一挑战。对于未保存激活的层，在反向传播过程中需要时，将重新计算这些激活以进行梯度计算。这种方法在计算时间和内存使用之间产生权衡。虽然它显著降低了内存需求，但由于需要重新计算一些激活，它可能会增加整体计算时间。
- en: Other parameters and techniques exist but play a minor role compared to those
    previously discussed. In the next section, we will explore how to select and tune
    these parameters using a concrete example.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参数和技术存在，但与之前讨论的那些相比，它们的作用较小。在下一节中，我们将通过具体示例探讨如何选择和调整这些参数。
- en: Fine-tuning in practice
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际微调
- en: Let’s now fine-tune an open-source model on our custom dataset. In this section,
    we will show an example that implements LoRA and QLoRA for efficiency. Depending
    on the hardware you have available, you can select the technique that best corresponds
    to your configuration.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在我们的定制数据集上微调一个开源模型。在本节中，我们将展示一个实现LoRA和QLoRA以提高效率的示例。根据您可用的硬件，您可以选择最适合您配置的技术。
- en: 'There are many efficient open-weight models we can leverage for task or domain-specific
    use cases. To select the most relevant LLM, we need to consider three main parameters:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任务或领域特定的用例，我们可以利用许多高效的开放权重模型。为了选择最相关的LLM，我们需要考虑三个主要参数：
- en: '**License**: Some model licenses only allow non-commercial work, which is a
    problem if we want to fine-tune for a company. Custom licenses are common in this
    field, and can target companies with a certain number of users, for example.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可**：某些模型许可仅允许非商业工作，如果我们想为公司进行微调，这会成为一个问题。在这个领域，定制许可很常见，可以针对具有一定用户数量的公司，例如。'
- en: '**Budget**: Models with smaller parameter sizes (<10 B) are a lot cheaper to
    fine-tune and deploy for inference than larger models. This is due to the fact
    that they can be run on cheaper GPUs and process more tokens per second.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预算**：参数规模较小的模型（小于10 B）比大型模型更便宜，更适合进行微调和推理部署。这是因为它们可以在更便宜的GPU上运行，并且每秒可以处理更多的标记。'
- en: '**Performance**: Evaluating the base model on general-purpose benchmarks or,
    even better, domain- or task-specific benchmarks relevant to the final use case,
    is crucial. This helps ensure that the model has the necessary capabilities to
    perform well on the intended tasks after fine-tuning.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：在通用基准或更好的情况下，在特定领域或任务基准上评估基础模型至关重要。这有助于确保模型在微调后具有在预期任务上表现良好的必要能力。'
- en: In this chapter, we will choose Llama 3.1 8B, an open-weight model released
    by Meta. It has a permissive custom license (“Llama 3.1 Community License Agreement”)
    that allows commercial use. With 8B parameters, it is small enough to fit on most
    GPUs while reaching a high level of performance compared to its competitors. We
    can verify this using the Open LLM Leaderboard, as well as other benchmarks detailed
    in the model card.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将选择Meta发布的开源模型Llama 3.1 8B。它有一个许可的定制许可（“Llama 3.1社区许可协议”），允许商业使用。具有8B参数，它足够小，可以适应大多数GPU，同时与竞争对手相比达到高水平的表现。我们可以使用Open
    LLM排行榜以及其他在模型卡片中详细说明的基准来验证这一点。
- en: 'There are specialized tools and libraries to fine-tune models. In particular,
    we recommend the following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 存在专门用于微调模型的工具和库。特别是，我们推荐以下工具：
- en: '**TRL**: This is a library created and maintained by Hugging Face to train
    LLMs using SFT and preference alignment. It is a popular and reliable library
    that tends to be the most up-to-date in terms of algorithms. It works in single
    and multi-GPU settings with FSDP and DeepSpeed.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TRL**：这是一个由Hugging Face创建和维护的库，用于使用SFT和偏好对齐来训练LLMs。它是一个流行且可靠的库，在算法方面通常是最新的。它支持使用FSDP和DeepSpeed的单GPU和多GPU设置。'
- en: '**Axolotl**: Created by Wing Lian, this tool streamlines the fine-tuning of
    LLMs with reusable YAML configuration files. It is based on TRL but includes many
    additional features, such as automatically combining datasets stored in various
    formats. It also supports single- and multi-GPU settings with FSDP and DeepSpeed.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Axolotl**：由Wing Lian创建，这个工具通过可重用的YAML配置文件简化了LLMs的微调过程。它基于TRL，但包含许多额外功能，例如自动组合存储在不同格式的数据集。它还支持使用FSDP和DeepSpeed的单GPU和多GPU设置。'
- en: '**Unsloth**: Created by Daniel and Michael Han, Unsloth uses custom kernels
    to speed up training (2-5x) and reduce memory use (up to 80% less memory). It
    is based on TRL and provides many utilities, such as automatically converting
    models into the GGUF quantization format. At the time of writing, it is only available
    for single-GPU settings.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unsloth**：由Daniel和Michael Han创建，Unsloth使用自定义内核来加速训练（2-5倍）并减少内存使用（最多减少80%的内存）。它基于TRL，并提供许多实用工具，例如自动将模型转换为GGUF量化格式。在撰写本文时，它仅适用于单GPU设置。'
- en: To maximize efficiency, we will perform fine-tuning using the Unsloth library.
    The following code is designed as part of our LLMOps pipeline, but can also be
    used as a stand-alone script. It can also be executed in different environments,
    like SageMaker, cloud GPUs (like Lambda Labs or RunPod), Google Colab, and many
    others. We tested it on different GPUs, like A40, A100, and L4.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化效率，我们将使用Unsloth库进行微调。以下代码是我们LLMOps管道的一部分，但也可以作为独立的脚本使用。它还可以在不同的环境中执行，如SageMaker、云GPU（如Lambda
    Labs或RunPod）、Google Colab等。我们在A40、A100和L4等不同的GPU上进行了测试。
- en: 'To install the Unsloth library and its dependencies, we recommend directly
    installing from the GitHub repository of the book ([https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering))
    or Unsloth’s repo ([https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)).
    This approach is recommended because the installation steps are regularly updated
    to address potential conflicts with dependencies:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Unsloth库及其依赖项，我们建议直接从本书的GitHub仓库([https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering))或Unsloth的仓库([https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth))安装。这种方法推荐，因为安装步骤会定期更新以解决潜在的依赖项冲突：
- en: 'First, we want to access a gated model and (optionally) upload our fine-tuned
    model to Hugging Face ([https://huggingface.co/](https://huggingface.co/)). This
    requires being logged in to an account. If you don’t have an account, you can
    create it and store your API key (**Settings | Access Tokens | Create new token**)
    in the .env file:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们想要访问一个门控模型，并且（可选地）将我们的微调模型上传到Hugging Face ([https://huggingface.co/](https://huggingface.co/))。这需要登录到一个账户。如果您没有账户，您可以创建一个账户，并在.env文件中存储您的API密钥（**设置
    | 访问令牌 | 创建新令牌**）：
- en: '[PRE16]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Make sure that your Comet ML API key is also in the .env file:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的Comet ML API密钥也位于.env文件中：
- en: '[PRE17]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Import all the necessary packages:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包：
- en: '[PRE18]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s now load the model to fine-tune and its corresponding tokenizer. We use
    Unsloth’s FastLaguageModel class with the `.from_pretrained()` method. In addition
    to the model name, we need to specify the max sequence length (2,048 in this example).
    Finally, the `load_in_4bit` argument indicates if we want to use **QLoRA** (**quantized
    pre-trained weights**) or LoRA.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们加载模型和相应的分词器进行微调。我们使用Unsloth的FastLaguageModel类和`.from_pretrained()`方法。除了模型名称外，我们还需要指定最大序列长度（本例中为2,048）。最后，`load_in_4bit`参数表示我们是否想要使用**QLoRA**（**量化预训练权重**）或LoRA。
- en: We’ll use LoRA in this example because of faster training and higher quality,
    but you can easily switch to QLoRA if you don’t meet the VRAM requirements.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用LoRA，因为它训练速度快，质量高，但如果您不满足VRAM要求，可以轻松切换到QLoRA。
- en: '[PRE19]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that the model is loaded, we can define our LoRA configuration. Here, we
    use a rank of 32 that is large enough to imitate the writing style and copy the
    knowledge from our instruction samples. You can increase this value to 64 or 128
    if your results are underwhelming. We also set an alpha of 32, without dropout
    and without bias, to speed up training. Finally, we target every linear layer
    to maximize the quality of the fine-tuning process.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已经加载，我们可以定义我们的LoRA配置。在这里，我们使用32的秩，这足以模仿写作风格并复制我们的指令样本中的知识。如果你的结果不尽人意，你可以将这个值增加到64或128。我们还设置了一个32的alpha值，没有dropout和偏差，以加快训练速度。最后，我们将每个线性层的目标设置为最大化微调过程的质量。
- en: '[PRE20]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we need to prepare the data in the right format for fine-tuning. In this
    example, we don’t have a lot of samples in the llmtwin dataset (3,000 samples).
    This is an issue because the model might not correctly learn the chat template.
    To address this, we will upsample it with a high-quality general-purpose dataset
    called FineTome. This is a filtered version of `arcee-ai/The-Tome using the fineweb-edu-classifier`.
    Instead of using the 100,000 samples of this dataset, we will specify we only
    want 10,000 in the train split. We concatenate these two datasets to create our
    final set.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要准备合适格式的数据以进行微调。在这个例子中，我们在llmtwin数据集中没有很多样本（3,000个样本）。这是一个问题，因为模型可能无法正确学习聊天模板。为了解决这个问题，我们将使用一个名为FineTome的高质量通用数据集进行上采样。这是`arcee-ai/The-Tome`使用`fineweb-edu-classifier`过滤后的版本。我们不会使用这个数据集的100,000个样本，而是在训练分割中指定我们只想使用10,000个。我们将这两个数据集拼接起来以创建我们的最终集。
- en: '[PRE21]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we need to format this data using a chat template. Let’s use the Alpaca
    template for convenience. This template doesn’t require additional tokens, which
    makes it less error-prone (but can slightly impact performance compared to ChatML).
    Here, we map all the instructions and answers to the Alpaca template. We manually
    add the **end of sentence** (**EOS**) token at the end of each message to ensure
    that the model learns to output it. Without it, it will keep generating answers
    without ever stopping.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要使用聊天模板来格式化这些数据。为了方便，让我们使用Alpaca模板。这个模板不需要额外的标记，这使得它更不容易出错（但与ChatML相比可能会稍微影响性能）。在这里，我们将所有指令和答案映射到Alpaca模板。我们手动在每个消息的末尾添加**句子结束**（**EOS**）标记，以确保模型学会输出它。没有它，它将不断生成答案而不会停止。
- en: '[PRE22]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once the dataset is ready, we can divide it into training (95%) and test (5%)
    sets for validation during training.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集准备就绪，我们可以将其分为训练集（95%）和测试集（5%）以进行训练期间的验证。
- en: '[PRE23]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The model is now ready to be trained. The SFTTrainer() class stores all the
    hyperparameters for our training. In addition, we provide the model, tokenizer,
    LoRA configuration, and datasets. Following the recommendations from the previous
    section, we set a learning rate of `3e-4` with a linear scheduler and a maximum
    sequence length of 2048\. We train this model for three epochs with a batch size
    of 2 and 8 gradient accumulation steps (for an effective batch size of 16). We
    also choose the `adamw_8bit` optimizer with a `weight_decay` of 0.01\. Depending
    on the GPU we use, it will automatically use FP16 or BF16 for the activations.
    Finally, we report our training run to Comet ML for experiment tracking.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型现在已准备好进行训练。SFTTrainer()类存储了我们训练的所有超参数。此外，我们提供了模型、分词器、LoRA配置和数据集。根据上一节的建议，我们设置了一个学习率为`3e-4`的线性调度器，最大序列长度为2048。我们以2个批大小和8个梯度累积步骤（有效批大小为16）训练这个模型三个周期。我们还选择了`adamw_8bit`优化器，`weight_decay`为0.01。根据我们使用的GPU，它将自动使用FP16或BF16进行激活。最后，我们将我们的训练运行报告给Comet
    ML以进行实验跟踪。
- en: '[PRE24]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Training this model on our concatenated dataset can take a few hours. For example,
    it takes 50 minutes on an A100 GPU.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的拼接数据集上训练这个模型可能需要几个小时。例如，在一个A100 GPU上需要50分钟。
- en: Once it’s done, we can test it with a quick example. The goal is not to properly
    evaluate the fine-tuned model, but to make sure that there are no obvious errors
    related to the tokenizer or chat template.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以用一个快速示例来测试它。目标不是正确评估微调后的模型，而是确保没有与分词器或聊天模板相关的明显错误。
- en: 'For fast inference, we can use `FastLanguageModel.for_inference()` from Unsloth.
    We directly format an instruction with the Alpaca format. Note that we provide
    an empty answer to append the assistant header (`### Response`): at the end of
    the user instruction. This forces the model to answer the instruction instead
    of completing it. We also use a text streamer to stream the generation instead
    of waiting for it to be complete before printing it.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速推理，我们可以使用Unsloth中的`FastLanguageModel.for_inference()`。我们直接使用Alpaca格式格式化一个指令。请注意，我们在用户指令的末尾提供一个空答案以附加助手标题（`###
    Response`）：这迫使模型回答指令而不是完成它。我们还使用文本流器来流式传输生成，而不是等待它完成后再打印。
- en: '[PRE25]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is the answer provided by our model:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是我们模型提供的答案：
- en: '[PRE26]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is correct and properly formatted with the Alpaca chat template.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对的，并且已经按照Alpaca聊天模板正确格式化。
- en: Now that our model has been successfully fine-tuned, we can save it locally
    and/or push it to the Hugging Face Hub using the following functions.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经成功微调了我们的模型，我们可以使用以下函数将其保存到本地和/或将它推送到Hugging Face Hub。
- en: '[PRE27]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Congratulations on fine-tuning a base model from scratch! During training, you
    can access Comet ML to monitor your training loss, validation loss, and many other
    metrics. You want to make sure that these metrics correspond to what is expected.
    *Figure 5.11* shows the training run corresponding to the previous code in Comet
    ML.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您从头开始微调了一个基础模型！在训练过程中，您可以访问Comet ML来监控您的训练损失、验证损失以及许多其他指标。您需要确保这些指标与预期相符。*图5.11*显示了Comet
    ML中对应于之前代码的训练运行。
- en: '![A graph of a graph  Description automatically generated with medium confidence](img/B31105_05_11.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![一个图形的图形  描述自动生成，置信度中等](img/B31105_05_11.png)'
- en: Figure 5.11 – Four monitored metrics during fine-tuning in Comet ML
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 在Comet ML中微调期间监控的四个指标
- en: 'In particular, three of these metrics are important to monitor:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是以下三个指标是重要的监控对象：
- en: '**Training loss**: It measures how well the model is performing on the task
    it’s being trained for. The loss should continuously decrease on average, indicating
    improving performance. We expect a rapid decrease at the beginning of training,
    followed by a long plateau. Spikes and continuous increases in the loss value
    are signs that the training is failing. In this case, you might want to check
    the quality of your data, issues with the tokenizer, and tune parameters like
    learning rate and batch size. In *Figure 5.11* (loss), you can see three different
    phases corresponding to our three epochs.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练损失**：它衡量模型在训练任务上的表现如何。损失应该平均持续下降，表明性能在提高。我们预计在训练开始时会迅速下降，然后是一个漫长的平台期。损失值的峰值和持续增加是训练失败的迹象。在这种情况下，您可能需要检查数据的质量、分词器的问题，并调整学习率、批量大小等参数。在*图5.11*（loss）中，您可以看到对应于我们三个不同阶段的三个不同阶段。'
- en: '**Validation loss**: It measures the loss using the validation set instead
    of the training set; a well-fitted model typically shows both training and validation
    losses decreasing and eventually stabilizing, with a small gap between them. This
    gap should be minimal but is expected to exist as the model will always perform
    slightly better on the training data. If the training loss continues to decrease
    while the validation loss starts to increase, it’s a sign of overfitting. Conversely,
    if both curves remain flat at a relatively high loss value, it indicates underfitting.
    There are no universal “recommended ranges” for loss values, as these depend on
    the specific problem and loss function used. However, you should look for convergence
    and stability in both curves. In *Figure 4.11* (eval_loss), we see a slight increase
    at step 340\. This is still acceptable but might indicate that the model starts
    to overfit.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证损失**：它使用验证集而不是训练集来衡量损失；一个拟合良好的模型通常显示训练和验证损失都在下降，并最终稳定，两者之间有一个小的差距。这个差距应该是最小的，但预期会存在，因为模型在训练数据上总是表现得稍微好一些。如果训练损失继续下降而验证损失开始增加，这是过拟合的迹象。相反，如果两条曲线都保持在一个相对较高的损失值上，这表明欠拟合。没有关于损失值的“推荐范围”，因为这些取决于具体问题和使用的损失函数。然而，您应该在两条曲线上寻找收敛和稳定性。在*图4.11*（eval_loss）中，我们看到在第340步时略有上升。这仍然是可接受的，但可能表明模型开始过拟合。'
- en: '**Gradient norm**: It represents the magnitude of the gradient vector during
    training. Large gradient norms can indicate training instability like overfitting,
    especially if accompanied by a divergence between training and validation losses.
    On the other hand, a stable or decreasing gradient norm generally means that the
    model is converging toward a local optimum. To mitigate issues associated with
    large gradient norms, gradient clipping can be employed. This technique involves
    setting a maximum threshold for the gradient norm, effectively limiting the size
    of parameter updates.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度范数**：它表示训练过程中梯度向量的幅度。大的梯度范数可能表明训练不稳定，如过拟合，尤其是如果伴随着训练和验证损失之间的差异。另一方面，稳定或下降的梯度范数通常意味着模型正在收敛到一个局部最优。为了减轻与大的梯度范数相关的问题，可以采用梯度裁剪。这项技术涉及设置梯度范数的最大阈值，从而有效地限制参数更新的规模。'
- en: It is often interesting to try different learning rates and select the best
    model based on the minimal loss. Note that this is a proxy for real evaluations,
    which are covered in the next chapter.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率并基于最小损失选择最佳模型通常很有趣。请注意，这只是一个对真实评估的代理，相关内容将在下一章中介绍。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered essential aspects of LLM fine-tuning, both in theory and
    practice. We examined the instruction data pipeline and how to create high-quality
    datasets, from curation to augmentation. Each pipeline stage offers optimization
    opportunities, particularly in quality assessment, data generation, and enhancement.
    This flexible pipeline can be adapted to your use cases by selecting the most
    relevant stages and techniques.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了LLM微调的必要方面，包括理论和实践。我们考察了指令数据管道以及如何从整理到增强创建高质量数据集。每个管道阶段都提供了优化机会，尤其是在质量评估、数据生成和增强方面。这个灵活的管道可以通过选择最相关的阶段和技术来适应您的用例。
- en: 'We applied this framework to real-world data from *Chapter 3*, using an LLM
    to convert raw text into instruction-answer pairs. We then explored SFT techniques.
    This included an analysis of SFT’s advantages and limitations, methods for storing
    and parsing instruction datasets with chat templates, and an overview of three
    primary SFT techniques: full fine-tuning, LoRA, and QLoRA. We compared these methods
    based on their impact on memory usage, training efficiency, and output quality.
    The chapter concluded with a practical demonstration that involved fine-tuning
    a Llama 3.1 8 B model on our custom instruction dataset. This example highlighted
    key steps and implementation details for successful fine-tuning.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此框架应用于第3章的真实世界数据，使用LLM将原始文本转换为指令-答案对。然后我们探讨了SFT技术。这包括对SFT的优势和局限性的分析，使用聊天模板存储和解析指令数据集的方法，以及三种主要SFT技术的概述：全微调、LoRA和QLoRA。我们根据它们对内存使用、训练效率和输出质量的影响进行了比较。本章以一个实际演示结束，涉及在我们的自定义指令数据集上微调Llama
    3.1 8 B模型。这个例子突出了成功微调的关键步骤和实现细节。
- en: In the next chapter, we will use preference alignment techniques to create a
    new version of TwinLlama-3.1-8B. We will generate a new dataset with chosen and
    rejected answers that will help us calibrate the type of answers we expect from
    our model. We will detail many applications that can benefit from this framework
    and how to implement it.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用偏好对齐技术创建TwinLlama-3.1-8B的新版本。我们将生成一个包含所选和拒绝答案的新数据集，这将帮助我们校准我们期望从模型获得的答案类型。我们将详细介绍许多可以从这个框架中受益的应用以及如何实现它。
- en: References
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Tahori, Gulrajani, Zhang, Dubois, et al.. “*Alpaca: A Strong, Replicable Instruction-Following
    Model*” [crfm.stanford.edu](https://crfm.stanford.edu), March 13, 2023, [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html).'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tahori, Gulrajani, Zhang, Dubois, 等人。“*Alpaca：一个强大的、可复制的指令遵循模型*” [crfm.stanford.edu](https://crfm.stanford.edu)，2023年3月13日，[https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)。
- en: 'Subhabrata Mukherjee et al.. “*Orca: Progressive Learning from Complex Explanation
    Traces of GPT-4*.” arXiv preprint arXiv:2306.02707, June 2023.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subhabrata Mukherjee 等人。“*Orca：从GPT-4的复杂解释轨迹中进行渐进式学习*。” arXiv预印本 arXiv:2306.02707，2023年6月。
- en: Wing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet
    Vong and “Teknium”. “*Open-Orca/OpenOrca.” huggingface.co*, 2023, [https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca).
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wing Lian 和 Bleys Goodson 和 Eugene Pentland 和 Austin Cook 和 Chanvichet Vong
    和 “Teknium”。“*Open-Orca/OpenOrca.*” huggingface.co，2023年，[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)。
- en: Weihao Zeng et al.. “*Automatic Instruction Evolving for Large Language Models*.”
    arXiv preprint arXiv:2406.00770, June 2024.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng Weihao等. “*大型语言模型的自动指令进化*.” arXiv预印本 arXiv:2406.00770, 2024年6月.
- en: 'Chunting Zhou et al.. “*LIMA: Less Is More for Alignment*.” arXiv preprint
    arXiv:2305.11206, May 2023'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周纯等. “*少即是多：对齐的LIMA*.” arXiv预印本 arXiv:2305.11206, 2023年5月
- en: '01\. AI. “*Yi: Open Foundation Models by 01.AI*.” arXiv preprint arXiv:2403.04652,
    March 2024.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 01\. AI. “*Yi：01.AI的开源基础模型*.” arXiv预印本 arXiv:2403.04652, 2024年3月.
- en: Alex Birch. “*LLM finetuning memory requirements*.” [blog.scottlogic.com](https://blog.scottlogic.com),
    November 24, 2023, [https://blog.scottlogic.com/2023/11/24/llm-mem.html](https://blog.scottlogic.com/2023/11/24/llm-mem.html).
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alex Birch. “*LLM微调的内存需求*.” [blog.scottlogic.com](https://blog.scottlogic.com),
    2023年11月24日, [https://blog.scottlogic.com/2023/11/24/llm-mem.html](https://blog.scottlogic.com/2023/11/24/llm-mem.html).
- en: Quentin Anthony et al.. “*Transformer Math 101*.” blog.eleuther.ai, April 18,
    2023, [https://blog.eleuther.ai/transformer-math/](https://blog.eleuther.ai/transformer-math/).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quentin Anthony等. “*Transformer数学101*.” blog.eleuther.ai, 2023年4月18日, [https://blog.eleuther.ai/transformer-math/](https://blog.eleuther.ai/transformer-math/).
- en: 'Edward J. Hu et al.. “*LoRA: Low-Rank Adaptation of Large Language Models*.”
    arXiv preprint arXiv:2106.09685, June 2021.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Edward J. Hu等. “*LoRA：大型语言模型的低秩自适应*.” arXiv预印本 arXiv:2106.09685, 2021年6月.
- en: 'Tim Dettmers et al.. “*QLoRA: Efficient Finetuning of Quantized LLMs*.” arXiv
    preprint arXiv:2305.14314, May 2023.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tim Dettmers等. “*QLoRA：量化LLM的高效微调*.” arXiv预印本 arXiv:2305.14314, 2023年5月.
- en: Join our book’s Discord space
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code79969828252392890.png)'
