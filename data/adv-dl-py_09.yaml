- en: Language Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言建模
- en: This chapter is the first of several in which we'll discuss different neural
    network algorithms in the context of **natural language processing** (**NLP**).
    NLP teaches computers to process and analyze natural language data in order to
    perform tasks such as machine translation, sentiment analysis, natural language
    generation, and so on. But to successfully solve such complex problems, we have
    to represent the natural language in a way that the computer can understand, and
    this is not a trivial task.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是几章中的第一章，我们将讨论不同的神经网络算法在**自然语言处理**（**NLP**）中的应用。NLP教会计算机处理和分析自然语言数据，以执行诸如机器翻译、情感分析、自然语言生成等任务。但要成功解决这些复杂的问题，我们必须以计算机能够理解的方式表示自然语言，而这并非一项简单的任务。
- en: To understand why, let's go back to image recognition. The neural network input
    is fairly intuitive—a 2D tensor with preprocessed pixel intensities, which preserves
    the spatial features of the image. Let's take a 28 x 28 MNIST image, which contains
    784 pixels. All the information about the digit in the image is contained within
    these pixels only and we don't need any external information to classify the image.
    We can also safely assume that each pixel (perhaps excluding the ones near the
    image borders) carries the same information weight. Therefore, we feed them all
    to the network to do its magic and we let the results speak for themselves.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解原因，让我们回到图像识别。神经网络的输入相当直观——一个二维张量，包含预处理后的像素强度，保留了图像的空间特征。我们以28 x 28的MNIST图像为例，它包含784个像素。关于图像中数字的所有信息都包含在这些像素中，我们不需要任何外部信息来分类图像。我们还可以安全地假设每个像素（也许除了靠近图像边缘的像素）承载着相同的信息量。因此，我们将所有像素输入到网络中，让它进行处理，并让结果自己证明。
- en: Now, let's focus on text data. Unlike an image, we have 1D (as opposed to 2D)
    data—a single long sequence of words. A general rule of thumb is that a single-spaced
    A4 page contains 500 words. To feed a network (or any ML algorithm) the informational
    equivalent of a single MNIST image, we need 1.5 pages of text. The text structure
    has several hierarchical levels; starting from characters, then words, sentences,
    and paragraphs, all of which can fit within 1.5 pages of text. All the pixels
    of the image relate to one digit; however, we don't know whether all the words
    relate to the same subject. To avoid this complexity, NLP algorithms usually work
    with shorter sequences. Even though some algorithms use **recurrent neural networks**
    (**RNNs**), which take into account all previous inputs, in practice, they are
    still limited to a relatively short window of the immediately preceding words.
    Therefore, an NLP algorithm has to do more (perform well) with less (a smaller
    amount of input information).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注文本数据。与图像不同，我们有1D（而非2D）数据——一长串单词序列。一般来说，单倍行距的A4纸上大约有500个单词。为了向网络（或任何机器学习算法）输入与单个MNIST图像等价的信息，我们需要1.5页的文本。文本结构有多个层级；从字符开始，然后是单词、句子和段落，这些都可以容纳在1.5页文本中。图像的所有像素与一个数字相关；然而，我们无法确定所有单词是否与同一主题相关。为了避免这种复杂性，NLP算法通常处理较短的序列。尽管一些算法使用**递归神经网络**（**RNNs**），它们考虑了所有先前的输入，但实际上，它们仍然局限于相对较短的前一个单词窗口。因此，NLP算法必须在较少的输入信息下做更多的工作（表现得更好）。
- en: To help us with this, we'll use a special type of vector word representation
    (language model). The language models we'll discuss use the context of a word
    (its surrounding words) to create a unique embedding vector associated with that
    word. These vectors carry more information about the word, compared to, say, one-hot
    encoding. They serve as a base for various NLP tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们解决这个问题，我们将使用一种特殊类型的**向量**词表示（语言模型）。我们将讨论的语言模型利用一个词的上下文（即周围的词）来创建与该词相关的独特嵌入向量。与例如单热编码相比，这些向量包含了更多关于该词的信息。它们为各种NLP任务提供了基础。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding *n*-grams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解*n*-grams
- en: 'Introducing neural language models:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入神经语言模型：
- en: Neural probabilistic language model
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经概率语言模型
- en: Word2Vec and fastText
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec 和 fastText
- en: Global Vectors for Word Representation
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于词表示的全局向量
- en: Implementing language models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现语言模型
- en: Understanding n-grams
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解n-grams
- en: 'A word-based language model defines a probability distribution over sequences
    of words. Given a sequence of words of length *m* (for example, a sentence), it
    assigns a probability *P*(*w1, ... , w[m]*) to the full sequence of words. We
    can use these probabilities as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单词的语言模型定义了一个关于单词序列的概率分布。给定一个长度为*m*的单词序列（例如，一个句子），它为整个单词序列分配一个概率*P*（*w1, ...
    , w[m]*）。我们可以按如下方式使用这些概率：
- en: To estimate the likelihood of different phrases in NLP applications.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于估计不同短语在自然语言处理应用中的可能性。
- en: As a generative model to create new text. A word-based language model can compute
    the likelihood of a given word following a sequence of words.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一种生成模型来创建新的文本，基于单词的语言模型可以计算给定单词跟随一系列单词的概率。
- en: 'The inference of the probability of a long sequence, say *w[1], ..., w[m]*,
    is typically infeasible. We can calculate the joint probability of *P*(*w[1],
    ... , w[m]*) with the chain rule of joint probability ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个长序列，如*w[1], ..., w[m]*，推断其概率通常是不可行的。我们可以通过联合概率的链式法则计算联合概率*P*（*w[1], ...
    , w[m]*）（[第一章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*）：
- en: '![](img/d57f7d17-249c-4cbf-9346-c37d15d16f32.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d57f7d17-249c-4cbf-9346-c37d15d16f32.png)'
- en: 'The probability of the later words given the earlier words would be especially
    difficult to estimate from the data. That''s why this joint probability is typically
    approximated by an independence assumption that the *i-*th word is only dependent
    on the *n-1* previous words. We''ll only model the joint probabilities of combinations
    of *n* sequential words, called *n*-grams. For example, in the phrase *the quick
    brown fox*, we have the following *n*-grams:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 给定前面的单词，后面的单词的概率尤其难以从数据中估计。这就是为什么这个联合概率通常通过独立性假设来近似，即第*i*个单词仅依赖于前面*n-1*个单词。我们只会对*n*个连续单词的联合概率进行建模，称为*n*-gram。例如，在短语*the
    quick brown fox*中，我们有以下*n*-gram：
- en: '**1-gram**: *The*, *quick*, *brown*, and *fox* (also known as a unigram).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1-gram**：*The*，*quick*，*brown*，和*fox*（也称为一元组）。'
- en: '**2-grams**: *The quick*, *quick brown*, and *brown fox* (also known as a bigram).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2-gram**：*The quick*，*quick brown*，和*brown fox*（也称为二元组）。'
- en: '**3-grams**: *The quick brown* and *quick brown fox* (also known as a trigram).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3-gram**：*The quick brown*和*quick brown fox*（也称为三元组）。'
- en: '**4-grams**: *The quick brown fox*.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4-gram**：*The quick brown fox*。'
- en: The inference of the joint distribution is approximated with the help of *n*-gram
    models that split the joint distribution into multiple independent parts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 联合分布的推断通过*n*-gram模型来近似，这些模型将联合分布拆分为多个独立部分。
- en: The term *n*-grams can refer to other types of sequences of length *n*, such
    as *n* characters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-gram一词可以指其他类型的长度为*n*的序列，例如*n*个字符。'
- en: 'If we have a large corpus of text, we can find all the *n*-grams up until a
    certain *n* (typically 2 to 4) and count the occurrence of each *n*-gram in that
    corpus. From these counts, we can estimate the probabilities of the last word
    of each *n*-gram, given the previous *n-1* words:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个大规模的文本语料库，我们可以找到所有的*n*-gram，直到某个*n*（通常是2到4），并计算每个*n*-gram在语料库中的出现次数。通过这些计数，我们可以估计给定前面*n-1*个单词时每个*n*-gram的最后一个单词的概率：
- en: '**1-gram**:[![](img/fb25070e-7589-4481-89ba-4ae57914aea4.png)]'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1-gram**：[![](img/fb25070e-7589-4481-89ba-4ae57914aea4.png)]'
- en: '**2-gram**: [![](img/25b8576c-7d29-43b9-b271-9f93061e8f1a.png)]'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2-gram**：[![](img/25b8576c-7d29-43b9-b271-9f93061e8f1a.png)]'
- en: '**N-gram**: [![](img/25f22551-a6df-486d-92b9-7f1cfdd37d1f.png)]'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N-gram**：[![](img/25f22551-a6df-486d-92b9-7f1cfdd37d1f.png)]'
- en: The independent assumption that the *i-*th word is only dependent on the previous *n-1*
    words can now be used to approximate the joint distribution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设第*i*个单词仅依赖于前面*n-1*个单词的独立性假设现在可以用于近似联合分布。
- en: 'For example, for a unigram, we can approximate the joint distribution by using
    the following formula:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一元组，我们可以使用以下公式来近似联合分布：
- en: '![](img/5a638abc-d9c6-43ab-98f0-d0800f5b8579.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a638abc-d9c6-43ab-98f0-d0800f5b8579.png)'
- en: 'For a trigram, we can approximate the joint distribution by using the following
    formula:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三元组，我们可以使用以下公式来近似联合分布：
- en: '![](img/6de5cc70-b831-454a-b92a-d7d6b822a585.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6de5cc70-b831-454a-b92a-d7d6b822a585.png)'
- en: We can see that, based on the vocabulary size, the number of *n*-grams grows
    exponentially with *n*. For example, if a small vocabulary contains 100 words,
    then the number of possible 5-grams would be *100⁵ = 10,000,000,000* different
    5-grams. In comparison, the entire works of Shakespeare contain around 30,000
    different words, illustrating the infeasibility of using *n*-grams with a large *n*.
    Not only is there the issue of storing all the probabilities, but we would also
    need a very large text corpus to create decent *n*-gram probability estimations
    for larger values of *n*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，基于词汇大小，*n*-gram的数量会随着*n*的增加呈指数增长。例如，如果一个小词汇表包含100个单词，那么可能的5-gram数量将是*100⁵
    = 10,000,000,000*种不同的5-gram。相比之下，莎士比亚的所有作品包含大约30,000个不同的单词，这说明使用大型*n*的*n*-gram是不可行的。我们不仅面临着存储所有概率的问题，而且还需要非常大的文本语料库来为更大值的*n*创建合理的*n*-gram概率估计。
- en: This problem is known as the curse of dimensionality. When the number of possible
    input variables (words) increases, the number of different combinations of these
    input values increases exponentially. The curse of dimensionality arises when
    the learning algorithm needs at least one example per relevant combination of
    values, which is the case in *n*-gram modeling. The larger our *n*, the better
    we can approximate the original distribution and the more data we would need to
    make good estimations of the *n*-gram probabilities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被称为维度灾难。当可能的输入变量（单词）数量增加时，这些输入值的不同组合数量会呈指数增长。维度灾难出现在学习算法需要至少一个示例来表示每种相关的值组合时，这正是*n*-gram模型的情况。我们的*n*越大，就能越好地近似原始分布，同时我们需要更多的数据来对*n*-gram概率进行良好的估计。
- en: Now that we are familiar with the *n*-gram model and the curse of dimensionality,
    let's discuss how to solve it with the help of neural language models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了*n*-gram模型和维度灾难，让我们来讨论如何借助神经语言模型来解决这个问题。
- en: Introducing neural language models
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入神经语言模型
- en: 'One way to overcome the curse of dimensionality is by learning a lower-dimensional,
    distributed representation of the words (*A Neural Probabilistic Language Model*, [http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)).
    This distributed representation is created by learning an embedding function that
    transforms the space of words into a lower-dimensional space of word embeddings
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 克服维度灾难的一种方法是通过学习单词的低维分布式表示（*A Neural Probabilistic Language Model*，[http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)）。这种分布式表示是通过学习一个嵌入函数来创建的，该函数将单词空间转化为一个低维的单词嵌入空间，具体如下：
- en: '![](img/651755db-3fcd-4cb6-b39d-91f34cb191a7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/651755db-3fcd-4cb6-b39d-91f34cb191a7.png)'
- en: Words -> one-hot encoding -> word embedding vectors
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 -> 独热编码 -> 单词嵌入向量
- en: Words from the vocabulary with size *V* are transformed into one-hot encoding
    vectors of size *V* (each word is encoded uniquely). Then, the embedding function
    transforms this *V*-dimensional space into a distributed representation of size *D* (here,
    *D*=4).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 来自大小为*V*的词汇表的单词被转化为大小为*V*的独热编码向量（每个单词都被唯一编码）。然后，嵌入函数将这个*V*维空间转化为一个大小为*D*的分布式表示（此处，*D*=4）。
- en: The idea is that the embedding function learns semantic information about the
    words. It associates each word in the vocabulary with a continuous-valued vector
    representation, that is, the word embedding. Each word corresponds to a point
    in this embedding space, and different dimensions correspond to the grammatical
    or semantic properties of these words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思想是，嵌入函数学习关于单词的语义信息。它将词汇表中的每个单词与一个连续值的向量表示关联，也就是单词嵌入。每个单词对应于嵌入空间中的一个点，不同的维度对应于这些单词的语法或语义属性。
- en: The goal is to ensure that the words close to each other in the embedding space
    have similar meanings. In this way, the information that some words are semantically
    similar can be exploited by the language model. For example, it might learn that
    *fox* and *cat* are semantically related and that both *the quick brown fox* and
    *the quick brown cat* are valid phrases. A sequence of words can then be replaced
    with a sequence of embedding vectors that capture the characteristics of these
    words. We can use this sequence as a base for various NLP tasks. For example,
    a classifier trying to classify the sentiment of an article might be trained on
    using previously learned word embeddings, instead of one-hot encoding vectors.
    In this way, the semantic information of the words becomes readily available for
    the sentiment classifier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是确保在嵌入空间中相近的词语具有相似的意义。通过这种方式，语言模型可以利用一些词语在语义上的相似性。例如，它可能会学到*fox*和*cat*在语义上是相关的，并且*the
    quick brown fox*和*the quick brown cat*都是有效的短语。然后，一组词语可以被一组嵌入向量替换，这些嵌入向量捕捉到这些词语的特征。我们可以将这个序列作为各种自然语言处理任务的基础。例如，一个试图分类文章情感的分类器，可能会使用先前学到的词嵌入，而不是独热编码向量。通过这种方式，词语的语义信息变得易于被情感分类器使用。
- en: Word embeddings are one of the central paradigms when solving NLP tasks. We
    can use them to improve the performance of other tasks where there might not be
    a lot of labeled data available. Next, we'll discuss the first neural language
    model that was introduced in 2001 (which serves as an example that many of the
    concepts in deep learning are not new).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是解决自然语言处理任务时的核心范式之一。我们可以使用它们来提高在标注数据稀缺的任务中的表现。接下来，我们将讨论2001年引入的第一个神经语言模型（这个例子表明，深度学习中的许多概念并不新颖）。
- en: We usually denote vectors with bold non-italic lowercase letters, such as **w**.
    But the convention in neural language models is to use italic lowercase, such
    as *w.* In this chapter, we'll use this convention.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常用粗体非斜体的小写字母来表示向量，如**w**。但在神经语言模型中，约定使用斜体小写字母，如*w*。在本章中，我们将遵循这一约定。
- en: In the next section, we will take a look at the **neural probabilistic language
    model** (**NPLM**).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍**神经概率语言模型**（**NPLM**）。
- en: Neural probabilistic language model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经概率语言模型
- en: 'It is possible to learn the language model and, implicitly, the embedding function
    via a feedforward fully connected network. Given a sequence of *n-1* words (*w[t-n+1] ,
    ..., w[t-1]*), it tries to output the probability distribution of the next word, *w[t]* (the
    following diagram is based on [http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过一个前馈全连接网络来学习语言模型，并隐式地学习嵌入函数。给定一个由*n-1*个词语（*w[t-n+1]*, ..., *w[t-1]*）组成的序列，它会尝试输出下一个词语*w[t]*的概率分布（以下图基于[http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)）：
- en: '![](img/4ddf599f-8890-4f57-86bf-872e19c666f9.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ddf599f-8890-4f57-86bf-872e19c666f9.png)'
- en: A neural network language model that outputs the probability distribution of
    the word w[t], given the words *w[t-n+1]* ... *w[t-1]*. *C* is the embedding matrix
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络语言模型，根据给定的词语*w[t-n+1]* ... *w[t-1]*，输出词语*w[t]*的概率分布。**C**是嵌入矩阵。
- en: 'The network layers play different roles, such as the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 网络层扮演着不同的角色，具体如下：
- en: The embedding layer takes the one-hot representation of the word *w[i]* and
    transforms it into the word's embedding vector by multiplying it with the embedding
    matrix, **C**. This computation can be efficiently implemented with table lookup.
    The embedding matrix, **C**, is shared between the words, so all words use the
    same embedding function. **C** is a *V * D* matrix, where *V* is the size of the
    vocabulary and *D* is the size of the embedding. In other words, the matrix, **C**,
    represents the network weights of the hidden *tanh* layer.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入层将词语*w[i]*的独热表示转换为词语的嵌入向量，通过与嵌入矩阵**C**相乘来实现。这个计算可以通过表查找高效实现。嵌入矩阵**C**在所有词语间共享，因此所有词语使用相同的嵌入函数。**C**是一个*V
    * D*矩阵，其中*V*是词汇表的大小，*D*是嵌入的维度。换句话说，矩阵**C**表示隐藏层*tanh*的网络权重。
- en: The resulting embeddings are concatenated and serve as an input to the hidden
    layer, which uses *tanh* activation. The output of the hidden layer is thus represented
    by the [![](img/75975090-e212-41c9-b67a-286040eb447a.png) ]function, where **H**
    represents the embedding to hidden layer weights and *d* represents the hidden
    biases.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的嵌入被连接起来并作为输入传递给隐藏层，隐藏层使用*tanh*激活函数。因此，隐藏层的输出由[![](img/75975090-e212-41c9-b67a-286040eb447a.png)]函数表示，其中**H**表示嵌入到隐藏层的权重，*d*表示隐藏偏置。
- en: 'Finally, we have the output layer with weights, **U**, bias, *b*, and softmax
    activation, which map the hidden layer to the word space probability distribution:
    [![](img/1b6b697c-5fc7-4038-9f98-f897be806696.png)]*.*'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有一个带有权重的输出层，**U**，偏置，*b*，以及softmax激活函数，将隐藏层映射到词空间概率分布：[![](img/1b6b697c-5fc7-4038-9f98-f897be806696.png)]*。
- en: This model simultaneously learns an embedding of all the words in the vocabulary
    (embedding layer) and a model of the probability function for sequences of words
    (network output). It is able to generalize this probability function to sequences
    of words that were not seen during training. A specific combination of words in
    the test set might not be seen in the training set, but a sequence with similar
    embedding features is much more likely to be seen during training. Since we can
    construct the training data and labels based on the positions of the words (which
    already exist in the text), training this model is an unsupervised learning task.
    Next, we'll discuss the word2vec language model, which was introduced in 2013
    and sparked an interest in the field of NLP in the context of neural networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型同时学习了词汇表中所有词的嵌入（嵌入层）以及词序列的概率函数模型（网络输出）。它能够将此概率函数推广到训练过程中未出现的词序列。测试集中的特定词组合可能未在训练集中出现，但具有相似嵌入特征的序列在训练过程中更可能出现。由于我们可以基于词的位置（这些位置已经存在于文本中）来构建训练数据和标签，因此训练该模型是一项无监督学习任务。接下来，我们将讨论word2vec语言模型，它于2013年推出，并在神经网络的NLP领域引发了广泛的关注。
- en: Word2Vec
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec
- en: A lot of research has gone into creating better word embedding models, in particular
    by omitting learning the probability function over sequences of words. One of
    the most popular ways to do this is with word2vec ([http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781),
    [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)). Similar to
    NPLM, word2vec creates embedding vectors based on the context (surrounding words)
    of the word in focus. It comes in two flavors: **continuous bag of words** (**CBOW**)
    and **Skip-gram**. We'll start with CBOW and then we'll discuss Skip-gram.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究致力于创建更好的词嵌入模型，特别是通过省略学习词序列的概率函数。其中一种最流行的方法是使用word2vec（[http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
    和 [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)， [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)）。类似于NPLM，word2vec基于词的上下文（周围的词）创建嵌入向量。它有两种形式：**连续词袋模型**（**CBOW**）和**Skip-gram**。我们将从CBOW开始，然后讨论Skip-gram。
- en: CBOW
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CBOW
- en: 'CBOW predicts the most likely word given its context (surrounding words). For
    example, given the sequence *The quick* _____ *fox jumps*, the model will predict
    *brown*. The context is the *n* preceding and the *n* following words of the word
    in focus (unlike NPLM, where only the preceding words participate). The following
    screenshot shows the context window as it slides across the text:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW根据上下文（周围的词）预测最可能的词。例如，给定序列*The quick* _____ *fox jumps*，模型将预测*brown*。上下文是指在关注的词前后*n*个词（与NPLM不同，NPLM仅参与前面的词）。以下截图展示了上下文窗口在文本中滑动的过程：
- en: '![](img/cf31bab8-c35a-4cfd-817f-49f3bd0aa8d6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf31bab8-c35a-4cfd-817f-49f3bd0aa8d6.png)'
- en: A word2vec sliding context window with *n = 2*. The same type of context window
    applies to both CBOW and Skip-gram
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有*n = 2*的word2vec滑动上下文窗口。相同类型的上下文窗口适用于CBOW和Skip-gram。
- en: 'CBOW takes all words within the context with equal weights and doesn''t consider
    their order (hence the *bag* in the name). It is somewhat similar to NPLM, but
    because it learns only the embedding vectors, we''ll train the model with the
    help of the following simple neural network:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW将上下文中的所有单词赋予相同的权重，并且不考虑它们的顺序（因此名字中有*bag*）。它与NPLM有些相似，但由于它只学习词嵌入向量，因此我们将通过以下简单的神经网络来训练模型：
- en: '![](img/7bc7f5b4-39f6-4097-9b72-a0f4a0c64f8a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bc7f5b4-39f6-4097-9b72-a0f4a0c64f8a.png)'
- en: A CBOW model network
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CBOW模型网络
- en: 'Here''s how it works:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其工作原理如下：
- en: The network has input, hidden, and output layers.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络有输入层、隐藏层和输出层。
- en: The input is the one-hot encoded word representations. The one-hot encoded vector
    size of each word is equal to the size of the vocabulary, *V*.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是通过一热编码表示的词。这些词的一热编码向量大小等于词汇表的大小，*V*。
- en: The embedding vectors are represented by the input-to-hidden weights, **W***[V×D]*,
    of the network. They are *V × D-*shaped matrix, where *D* is the length of the
    embedding vector (which is the same as the number of units in the hidden layer).
    As in NPLM, we can think of the weights as a lookup table, where each row represents
    one word embedding vector. Because each input word is one-hot encoded, it will
    always activate a single row of the weights. That is, for each input sample (word),
    only the word's own embedding vector will participate.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入向量由网络的输入到隐藏层权重**W***[V×D]*表示。它们是*V × D-*形状的矩阵，其中*D*是嵌入向量的长度（即隐藏层单元的数量）。和NPLM一样，我们可以将权重看作查找表，其中每一行表示一个词的嵌入向量。因为每个输入词是通过一热编码表示的，它总是激活权重的某一行。也就是说，对于每个输入样本（词），只有该词的嵌入向量会参与。
- en: The embedding vectors of all context words are averaged to produce the output
    of the hidden network layer (there is no activation function).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有上下文词的嵌入向量被平均以生成隐藏层网络的输出（没有激活函数）。
- en: The hidden activations serve as input to the output softmax layer of size *V*
    (with the weight vector **W^'***[D×V]*), which predicts the most likely word to
    be found in the context (proximity) of the input words. The index with the highest
    activation represents the one-hot encoded related word.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的激活值作为输入传递到输出的softmax层，大小为*V*（其权重向量为**W^'***[D×V]*），该层预测在输入词的上下文（邻近）中最有可能找到的词。激活值最高的索引表示一热编码的相关词。
- en: We'll train the network with gradient descent and backpropagation. The training
    set consists of (context and label) one-hot encoded pairs of words, appearing
    in close proximity to each other in the text. For example, if part of the text
    is the sequence `[the, quick, brown, fox, jumps]` and *n = 2*, the training tuples
    will include `([quick, brown], the)`, `([the, brown, fox], quick)`, `([the, quick,
    fox jumps], brown)`, and so on. Since we are only interested in the embeddings, **W***[V×D]*,
    we'll discard the rest of the network weights, **W^'***[V×D]*, when the training
    is finished.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过梯度下降和反向传播来训练网络。训练集由（上下文和标签）的一对一热编码词组成，这些词在文本中彼此靠近。例如，如果文本的一部分是序列`[the,
    quick, brown, fox, jumps]`，并且*n = 2*，则训练元组将包括`([quick, brown], the)`、`([the, brown,
    fox], quick)`、`([the, quick, fox jumps], brown)`等。由于我们只关心词嵌入**W***[V×D]*，当训练完成时，我们会丢弃网络中其余的权重**W^'***[V×D]*。
- en: CBOW will tell us which word is most likely to appear in a given context. This
    could be a problem for rare words. For example, given the context *The weather
    today is really* ____, the model will predict the word *beautiful* rather than
    *fabulous* (hey, it's just an example). CBOW is several times faster to train
    than the Skip-gram and achieves slightly better accuracy for frequent words.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW将告诉我们在给定上下文中最有可能出现哪个词。这对于稀有词来说可能是一个问题。例如，给定上下文*The weather today is really*____，模型会预测词*beautiful*，而不是*fabulous*（嘿，这只是一个例子）。与Skip-gram相比，CBOW的训练速度快几倍，并且在频繁出现的单词上略微提高了准确性。
- en: Skip-gram
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Skip-gram
- en: 'Given an input word, the Skip-gram model can predict its context (the opposite
    of CBOW). For example, the word *brown* will predict the words *The quick fox
    jumps*. Unlike CBOW, the input is a single one-hot word. But how do we represent
    the context words in the output? Instead of trying to predict the whole context
    (all surrounding words) simultaneously, Skip-gram transforms the context into
    multiple training pairs such as `(fox, the)`, `(fox, quick)`, `(fox, brown)`,
    and `(fox, jumps)`. Once again, we can train the model with a simple one-layer
    network:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入词，Skip-gram模型可以预测其上下文（与CBOW相反）。例如，词*brown*将预测词*The quick fox jumps*。与CBOW不同，输入是一个单一的one-hot词。但我们如何表示输出中的上下文词呢？Skip-gram并不是尝试同时预测整个上下文（所有周围词），而是将上下文转化为多个训练对，如`(fox,
    the)`、`(fox, quick)`、`(fox, brown)`和`(fox, jumps)`。再次地，我们可以通过一个简单的单层网络来训练模型：
- en: '![](img/463ba7aa-d28f-4a91-bfb9-a43718ae33d5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/463ba7aa-d28f-4a91-bfb9-a43718ae33d5.png)'
- en: A Skip-gram model network
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Skip-gram模型网络
- en: As with CBOW, the output is a softmax, which represents the one-hot encoded
    most probable context word. The input-to-hidden weights, **W***[V×D]*, represent
    the word embeddings lookup table and the hidden-to-output weights, **W^'***[D×V]*,
    are only relevant during training. The hidden layer doesn't have an activation
    function (that is, it uses linear activation).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与CBOW一样，输出是一个softmax，它表示最可能的上下文词的one-hot编码。输入到隐藏层的权重**W***[V×D]*表示词嵌入查找表，隐藏到输出的权重**W^'***[D×V]*仅在训练过程中相关。隐藏层没有激活函数（也就是说，它使用线性激活）。
- en: 'We''ll train the model with backpropagation (no surprises here). Given a sequence
    of words, *w[1], ..., w[M]*, the objective of the Skip-gram model is to maximize
    the average log probability where *n* is the window size:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过反向传播训练模型（这没有惊讶）。给定一个单词序列*w[1], ..., w[M]*，Skip-gram模型的目标是最大化平均对数概率，其中*n*是窗口大小：
- en: '![](img/57a3d779-efa4-4e17-b47f-9ea3e2ac7cba.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57a3d779-efa4-4e17-b47f-9ea3e2ac7cba.png)'
- en: 'The model defines the probability, [![](img/cfd1f18b-16bd-4da0-b9ca-6f037633c6fb.png),] as
    the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义了概率[![](img/cfd1f18b-16bd-4da0-b9ca-6f037633c6fb.png)]，如下所示：
- en: '![](img/a1bf9184-f2d2-4ed3-87f9-def2a0b5f267.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1bf9184-f2d2-4ed3-87f9-def2a0b5f267.png)'
- en: In this example, *w[I]* and *w[O]* are the input and output words and **v***[w]* and **v***^'[w]*
    are the corresponding word vectors in the input and output weights **W***[V×D]*
    and **W^'***[D×V]*, respectively (we keep the original notation of the paper).
    Since the net doesn't have a hidden activation function, its output value for
    one input/output word pair is simply the multiplication of the input word vector, [![](img/198b51f8-3363-4fe6-b6be-69cea1dd7fdc.png),] and
    the output word vector, [![](img/408cc438-39b1-44e4-8904-02094aa20d34.png)] (hence
    the transpose operation).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*w[I]*和*w[O]*分别是输入和输出词，**v***[w]*和**v***^'[w]*分别是输入和输出权重**W***[V×D]*和**W^'***[D×V]*中的对应词向量（我们保持了论文中的原始符号）。由于网络没有隐藏激活函数，它对于一个输入/输出词对的输出值仅仅是输入词向量[![](img/198b51f8-3363-4fe6-b6be-69cea1dd7fdc.png)]与输出词向量[![](img/408cc438-39b1-44e4-8904-02094aa20d34.png)]的乘积（因此需要进行转置操作）。
- en: The authors of the word2vec paper note that word representations cannot represent
    idiomatic phrases that are not compositions of the individual words. For example,
    *New York Times* is a newspaper, and not just a natural combination of the meanings
    of *New*, *York, *and *Times*. To overcome this, the model can be extended to
    include whole phrases. However, this significantly increases the vocabulary size.
    And, as we can see from the preceding formula, the softmax denominator needs to
    compute the output vectors for all words of the vocabulary. Additionally, every
    weight of the **W^'***[D×V]* matrix is updated on every training step, which slows
    the training.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec论文的作者指出，词向量无法表示那些不是个别单词组合的习惯用语。例如，*New York Times*是一个报纸，而不仅仅是*New*、*York*和*Times*意思的自然组合。为了克服这个问题，可以将模型扩展为包括完整的短语。然而，这会显著增加词汇表的大小。而且，正如我们从前面的公式中可以看到的，softmax分母需要计算词汇表中所有词的输出向量。此外，**W^'***[D×V]*矩阵的每个权重在每次训练步骤中都会更新，这会减慢训练过程。
- en: To solve this, we can replace the softmax with the so-called **negative sampling**
    (**NEG**). For each training sample, we'll take the positive training pair (for
    example, `(fox, brown)`), as well as *k* additional negative pairs (for example,
    `(fox, puzzle)`), where *k* is usually in the range of [5,20]. Instead of predicting
    the word that best matches the input word (softmax), we'll simply predict whether
    the current pair of words is true or not. In effect, we convert the multinomial
    classification problem (classify as one of many classes) to a binary logistic
    regression (or binary classification) problem. By learning the distinction between
    positive and negative pairs, the classifier will eventually learn the word vectors
    in the same way, as with multinomial classification. In word2vec, the words for
    the negative pairs are drawn from a special distribution, which draws less frequent
    words more often, compared to more frequent ones.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以用所谓的**负采样**（**NEG**）来替代softmax。对于每个训练样本，我们将使用正向训练对（例如，`(fox, brown)`），以及*k*个额外的负向对（例如，`(fox,
    puzzle)`），其中*k*通常在[5,20]的范围内。与其预测与输入词最匹配的词（softmax），我们干脆预测当前的词对是否真实。实际上，我们将多项式分类问题（分类为多类之一）转化为二元逻辑回归（或二元分类）问题。通过学习正向和负向词对的区别，分类器最终将像多项式分类一样学习词向量。在word2vec中，负向词对的词是从一个特殊的分布中抽取的，这个分布倾向于抽取不太常见的词，而不是常见的词。
- en: 'Some of the most frequent words to occur carry less information value compared
    to the rare words. Examples of such words are the definite and indefinite articles
    *a*, *an*, and *the*. The model will benefit more from observing the pairs *London* and
    *city* compared to *the* and *city* because almost all words co-occur frequently
    with *the*. The opposite is also true—the vector representations of frequent words
    do not change significantly after training on a large number of examples. To counter
    the imbalance between the rare and frequent words, the authors of the paper propose
    a subsampling approach, where each word, *w[i]*, of the training set is discarded
    with some probability, computed by the heuristic formula where *f(w[i])* is the
    frequency of word *w[i]* and *t* is a threshold (usually around 10^(-5)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最常见的词汇携带的信息量比稀有词汇要少。例如，定冠词和不定冠词*a*、*an*和*the*就是这样的词。模型通过观察词对*London*和*city*比观察*the*和*city*更能获益，因为几乎所有的词都与*the*频繁共现。相反的情况也是如此——频繁词汇的词向量在经过大量示例的训练后变化不大。为了应对稀有词和常见词之间的不平衡，论文的作者提出了一种下采样方法，其中训练集中的每个词*w[i]*，都有一定的概率被丢弃，该概率通过启发式公式计算，其中*f(w[i])*是词*w[i]*的频率，*t*是阈值（通常约为10^(-5)）：
- en: '![](img/95e5cdee-8daa-4410-8b6b-e56d7b5653ce.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95e5cdee-8daa-4410-8b6b-e56d7b5653ce.png)'
- en: It aggressively subsamples words with a frequency of greater than *t*, but also
    preserves the ranking of the frequencies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它积极地对出现频率大于*t*的词进行下采样，同时保持词频的排名。
- en: In conclusion, we can say that, in general, Skip-gram performs better on rare
    words compared to CBOW, but it takes longer to train.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以说，通常情况下，Skip-gram在处理稀有词汇时比CBOW表现更好，但它的训练时间更长。
- en: fastText
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fastText
- en: 'fastText ([https://fasttext.cc/](https://fasttext.cc/)) is a library for learning word
    embeddings and text classification created by the **Facebook AI Research** (**FAIR**)
    group. Word2Vec treats each word in the corpus as an atomic entity and generates
    a vector for each word, but this approach ignores the internal structure of the
    words. In contrast, fastText decomposes each word, *w*, to a bag of character
    *n*-grams. For example, if *n = 3*, we can decompose the word *there* to the character
    3-grams and the special sequence *<there>* for the whole word:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: fastText ([https://fasttext.cc/](https://fasttext.cc/)) 是一个由**Facebook AI Research**（**FAIR**）团队创建的用于学习词嵌入和文本分类的库。Word2Vec将语料库中的每个词视为一个原子实体，并为每个词生成一个向量，但这种方法忽略了词的内部结构。与此相对，fastText将每个词*w*分解为字符*n*-gram的袋子。例如，如果*n
    = 3*，我们可以将词*there*分解为字符3-gram，并为整个词生成特殊序列*<there>*：
- en: '*<th*, *the*, *her*, *ere*, *re>*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*<th*, *the*, *her*, *ere*, *re>*'
- en: Note the use of the special characters *<* and *>* to indicate the start and
    the end of the word. This is necessary to avoid mismatching between *n*-grams
    from different words. For example, the word *her* will be represented as *<her>*
    and it will not be mistaken for the *n*-gram *her* from the word *there*. The
    authors of fastText suggest ***3 ≤ n ******≤ 6***.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用特殊字符*<*和*>*来标示词的开始和结束。这是为了避免来自不同词的*n*-grams发生错配。例如，词*her*将被表示为*<her>*，它不会与词*there*中的*n*-gram*her*混淆。fastText的作者建议***3
    ≤ n ≤ 6***。
- en: 'Recall the softmax formula we introduced in the *Skip-gram* section. Let''s
    generalize it by replacing the vector multiplication operation of the word2vec
    network with a generic scoring function, ***s***,where *w[t]* is the input word
    and *w[c]* is the context word:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在*Skip-gram*部分介绍的softmax公式。我们通过用通用评分函数***s***替代word2vec网络中的向量乘法操作来推广它，其中*w[t]*是输入词，*w[c]*是上下文词：
- en: '![](img/5e4e75aa-b63f-41af-bd4c-9fb77323debb.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e4e75aa-b63f-41af-bd4c-9fb77323debb.png)'
- en: 'In the case of fastText, we''ll represent a word with the sum of the vector
    representations of its *n*-grams. Let''s denote the set of *n*-grams that appear
    in word *w* with *G[w] = {1 ... G}*, the vector representation of an *n*-gram, *g*,
    with **v*[g]***, and the potential vector of the context word, *c*, with **v***''**[c]***.
    Then, the scoring function defined by fastText becomes the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在fastText的情况下，我们通过将词的*n*-grams的向量表示相加来表示一个词。我们用*G[w] = {1 ... G}*表示出现在词*w*中的*n*-grams集合，用**v*[g]***表示*n*-gram的向量表示，用**v***'**[c]***表示上下文词的潜在向量。那么，fastText定义的评分函数如下：
- en: '![](img/6470be2a-59c7-461e-869d-1e98f2cd83d3.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6470be2a-59c7-461e-869d-1e98f2cd83d3.png)'
- en: In effect, we train the fastText model with Skip-gram type word pairs, but the
    input word is represented as a bag of *n*-grams.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们使用Skip-gram类型的词对训练fastText模型，但输入词通过*n*-grams的方式表示。
- en: 'Using character *n*-grams has several advantages over the traditional word2vec
    model:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字符*n*-grams相比传统的word2vec模型有几个优点：
- en: It can classify unknown or misspelled words if they share *n*-grams with other
    words familiar to the model.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个词与模型中其他熟悉的词共享*n*-grams，它可以分类未知或拼写错误的词。
- en: It can generate better word embeddings for rare words. Even if a word is rare,
    its character *n*-grams are still shared with other words, so the embeddings can
    still be good.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以为稀有词生成更好的词向量。即使一个词很稀有，它的字符*n*-grams仍然与其他词共享，因此词向量仍然可以很好。
- en: Now that we are familiar with word2vec, we'll introduce the Global Vectors for
    Word Representation language model, which improves some word2vec deficiencies.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了word2vec，接下来我们将介绍全局词向量表示语言模型，这个模型改进了word2vec的一些不足之处。
- en: Global Vectors for Word Representation model
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局词向量表示模型
- en: One disadvantage of word2vec is that it only uses the local context of words
    and doesn't consider their global co-occurrences. In this way, the model loses
    a readily available, valuable source of information. As the name suggests, the
    **Global Vectors for Word Representation** (**GloVe**) model tries to solve this
    ([https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec的一个缺点是它只使用词的局部上下文，而没有考虑它们的全局共现。这样，模型丧失了一个现成的、宝贵的信息来源。如其名字所示，**全局词向量表示**（**GloVe**）模型试图解决这个问题（[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)）。
- en: 'The algorithm starts with the global word-word co-occurrence matrix, **X**.
    A cell, *X[ij]*, indicates how often the word *j* appears in the context of word
    *i*. The following table shows the co-occurrence matrix for a window with size
    *n *= 2 of the sequence *I like DL. I like NLP. I enjoy cycling*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从全局的词-词共现矩阵**X**开始。一个单元格，*X[ij]*，表示词*j*在词*i*的上下文中出现的频率。以下表格展示了序列*I like DL.
    I like NLP. I enjoy cycling*，窗口大小*n*=2的共现矩阵：
- en: '![](img/ec13a661-4de4-403e-93dc-99355e343fcd.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec13a661-4de4-403e-93dc-99355e343fcd.png)'
- en: A co-occurrence matrix of the sequence I like DL. I like NLP. I enjoy cycling
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 序列I like DL. I like NLP. I enjoy cycling的共现矩阵
- en: 'Let''s denote the number of times any word appears in the context of word *i*
    with ![](img/89690123-8c01-410a-a982-61a58a6ea93b.png) and the probability that
    word *j* appears in the context of word *i* with ![](img/760aa31f-aec2-40d3-9a1a-a0597141079e.png).
    To better understand how this can help us, we''ll use an example that shows the
    co-occurrence probabilities for the target words *ice* and *steam* with selected
    context words from a 6 billion token corpus:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设定任何词在词*i*上下文中出现的次数为[![](img/89690123-8c01-410a-a982-61a58a6ea93b.png)]，词*j*在词*i*上下文中出现的概率为[![](img/760aa31f-aec2-40d3-9a1a-a0597141079e.png)]。为了更好地理解这如何帮助我们，我们将使用一个示例，展示来自60亿词汇的语料库中目标词*冰*和*蒸汽*与选定上下文词的共现概率：
- en: '![](img/e01e3c90-35c6-49cf-9cb9-408a1cc6ff0f.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e01e3c90-35c6-49cf-9cb9-408a1cc6ff0f.png)'
- en: 'Co-occurrence probabilities for the target words ice and steam with selected
    context words from a 6 billion token corpus: source: https://nlp.stanford.edu/pubs/glove.pdf'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目标词冰和蒸汽与选定上下文词的共现概率（来自60亿词汇的语料库）：来源：[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)
- en: The bottom row shows the ratio of the probabilities. The word **solid** (the
    first column) is related to **ice**, but less related to **steam**, so the ratio
    between their probabilities is large. Conversely, **gas** is more related to **steam**
    than to **ice** and the ratio between their probabilities is very small. The words
    **water** and **fashion** are equally related to both target words, hence the
    ratio of the probabilities is close to one. The ratio is better in distinguishing
    relevant words *(***solid** and **gas***)* from irrelevant words *(***water**
    and **fashion***),* compared to the raw probabilities*.* Additionally, it is better
    at discriminating between the two relevant words.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最底行显示的是概率比率。**固体**（第一列）与**冰**相关，但与**蒸汽**的相关性较低，因此它们的概率比率较大。相反，**气体**与**蒸汽**的相关性高于**冰**，因此它们的概率比率非常小。**水**和**时尚**这两个词与这两个目标词的相关性相等，因此它们的概率比率接近1。与原始概率相比，这一比率在区分相关词（*(**固体**和**气体**）*）和不相关词（*(**水**和**时尚**）*）时表现更好。此外，它还更能区分两个相关词之间的差异。
- en: 'With the previous argument, the authors of GloVe propose starting the word
    vector learning with the ratios of co-occurrence probabilities, rather than the
    probabilities themselves. With that starting point, and keeping in mind that the
    ratio [![](img/a59c08e9-51d4-4beb-8523-4ef9f0ca0ca9.png)] depends on three words—*i*,
    *j*, and *k—*we can define the most general form of the GloVe model as follows, where [![](img/a4e0ad61-266b-42b5-97eb-67e1b19d4fa8.png) ]are
    the word vectors and [![](img/f2464866-daa8-4cb7-99e4-60f99a9a8536.png)] is a
    special context vector, which we''ll discuss later ([![](img/b5075698-73c1-4bbd-99eb-4ff0d272b288.png)] is
    the *D*-dimensional vector space of real numbers):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的论述，GloVe的作者建议从共现概率的比率开始进行词向量学习，而不是从概率本身开始。以此为起点，并且记住比率[![](img/a59c08e9-51d4-4beb-8523-4ef9f0ca0ca9.png)]依赖于三个词——*i*、*j*和*k——我们可以将GloVe模型的最一般形式定义如下，其中[![](img/a4e0ad61-266b-42b5-97eb-67e1b19d4fa8.png)]是词向量，[![](img/f2464866-daa8-4cb7-99e4-60f99a9a8536.png)]是一个特殊的上下文向量，我们稍后会讨论([![](img/b5075698-73c1-4bbd-99eb-4ff0d272b288.png)]是*D*维实数向量空间)：
- en: '![](img/6d2c29e4-9762-486b-a188-e70f17501842.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d2c29e4-9762-486b-a188-e70f17501842.png)'
- en: 'In other words, *F* is such a function that, when computed with these three
    specific vectors (we assume that we already know them), will output the ratio
    of probabilities. Furthermore, *F* should encode the information of the probabilities
    ratio because we''ve already identified its importance. Since vector spaces are
    inherently linear, one way to encode this information is with the vector difference
    of the two target words. Therefore, the function becomes the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*F*是这样一个函数，当用这三个特定的向量计算时（我们假设我们已经知道它们），将输出概率比率。此外，*F*应该编码概率比率的信息，因为我们已经识别出它的重要性。由于向量空间本质上是线性的，一种编码这些信息的方法是通过目标词的向量差异。因此，函数变成了如下形式：
- en: '![](img/1a938fcb-379e-4210-a6a3-3a3ade67f6d9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a938fcb-379e-4210-a6a3-3a3ade67f6d9.png)'
- en: 'Next, let''s note that the function arguments are vectors, but the ratio of
    probabilities is scalar. To solve this issue, we can take the dot product of the
    arguments:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们注意到函数的参数是向量，但概率比率是标量。为了解决这个问题，我们可以计算参数的点积：
- en: '![](img/a7494734-0fd3-4297-a393-de1ec3c20c1c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7494734-0fd3-4297-a393-de1ec3c20c1c.png)'
- en: 'Then, let''s observe that the distinction between a word and its context word
    is arbitrary and we can freely exchange the two roles. Therefore, we should have
    [![](img/f11ecfab-81ef-4534-ac16-afd696d1af6d.png)], but the preceding equation
    doesn''t satisfy this condition. Long story short (there is a more detailed explanation
    in the paper), to satisfy this condition, we need to introduce another restriction
    in the form of the following equation where, [![](img/8edde8d5-a0e9-40e3-ba81-3e61184d7bbd.png)]
    and [![](img/e189ae1c-eb9b-4faf-8ceb-6f7faaddb01b.png)] are bias scalar values:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以观察到，单词与其上下文单词之间的区分是任意的，我们可以自由地交换这两者的角色。因此，我们应该有[![](img/f11ecfab-81ef-4534-ac16-afd696d1af6d.png)]，但前面的方程并不满足这一条件。长话短说（论文中有更详细的解释），为了满足这个条件，我们需要引入另一个限制，形式如下的方程，其中，[![](img/8edde8d5-a0e9-40e3-ba81-3e61184d7bbd.png)]
    和 [![](img/e189ae1c-eb9b-4faf-8ceb-6f7faaddb01b.png)] 是偏差标量值：
- en: '![](img/109aeb0d-0d5a-421b-a793-02edb89b4361.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/109aeb0d-0d5a-421b-a793-02edb89b4361.png)'
- en: 'One issue with this formula is that *log(0)* is undefined, but the majority
    of the *X[ik]* entries will be *0*. Additionally, it takes all co-occurrences
    with the same weight, but rare co-occurrences are noisy and carry less information
    than the more frequent ones. To solve all these issues, the authors propose a
    least squares regression model with a weighting function, *f(X[ij])*, for each
    co-occurrence. The model has the following cost function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式的一个问题是，*log(0)*是未定义的，但大多数的 *X[ik]* 条目将是 *0*。此外，它对所有共现赋予相同的权重，但稀有的共现通常噪声较多，所携带的信息量小于频繁的共现。为了解决这些问题，作者提出了一种最小二乘回归模型，并为每个共现引入了加权函数
    *f(X[ij])*。该模型具有以下成本函数：
- en: '![](img/75a06b7d-7aaf-486b-8759-a77da7a76184.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75a06b7d-7aaf-486b-8759-a77da7a76184.png)'
- en: 'Finally, the weighting function, *f*, should satisfy several properties. First, *f(0)
    = 0*. Then, *f(x)* should be non-decreasing so that rare co-occurrences are not
    overweighted. And finally, *f(x)* should be relatively small for large values
    of *x*, so that frequent co-occurrences are not overweighted. Based on these properties
    and their experiments, the authors propose the following function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，加权函数 *f* 应满足几个性质。首先，*f(0) = 0*。然后，*f(x)* 应该是单调不减的，以避免稀有共现被过度加权。最后，对于较大的 *x*
    值，*f(x)* 应该相对较小，以避免频繁共现被过度加权。根据这些性质和他们的实验，作者提出了以下函数：
- en: '![](img/c7a7a719-5075-4504-b703-0ac6c60ff2e8.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7a7a719-5075-4504-b703-0ac6c60ff2e8.png)'
- en: 'The following graph shows *f(x)*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 *f(x)*：
- en: '![](img/49d5223a-7ba8-42a4-b3ea-5bc5c794964c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49d5223a-7ba8-42a4-b3ea-5bc5c794964c.png)'
- en: Weighting function *f(X[ij])* with a cut-off value of *x[max]*= 100 and *α =
    3/4.* The authors' experiments show that these parameters work best; source: https://nlp.stanford.edu/pubs/glove.pdf
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 加权函数 *f(X[ij])*，其截止值为 *x[max]*= 100，且 *α = 3/4*。作者的实验表明，这些参数效果最佳；来源：[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)
- en: 'The model generates two sets of word vectors: *W* and ![](img/e95f0f4d-b4cb-40e3-bb19-e5df3bbcfd59.png).
    When *X* is symmetric, *W* and ![](img/f0df870d-e98c-411b-b409-ce351e2fbb86.png) are
    equivalent and differ only as a result of their random initializations. But the
    authors note that training an ensemble of networks and averaging their results
    usually helps to prevent overfitting. To mimic this behavior, they choose to use the
    sum [![](img/3b86fd4d-d51e-4d81-87d3-073d63bd0b21.png)] as the final word vectors,
    observing a small increase in the performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型生成两组单词向量：*W* 和 ![](img/e95f0f4d-b4cb-40e3-bb19-e5df3bbcfd59.png)。当 *X* 对称时，*W*
    和 ![](img/f0df870d-e98c-411b-b409-ce351e2fbb86.png) 是等价的，仅由于它们的随机初始化有所不同。但作者指出，训练多个网络并对它们的结果进行平均通常有助于防止过拟合。为了模拟这种行为，他们选择使用总和[![](img/3b86fd4d-d51e-4d81-87d3-073d63bd0b21.png)]作为最终的单词向量，并观察到性能略有提高。
- en: This concludes our discussion about neural language models. In the next section,
    we'll see how to train and visualize a word2vec model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于神经语言模型的讨论。在下一节中，我们将看到如何训练和可视化一个word2vec模型。
- en: Implementing language models
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现语言模型
- en: In this section, we'll implement a short pipeline for preprocessing text sequences
    and training a word2vec model with the processed data. We'll also implement another
    example to visualize embedding vectors and check some of their interesting properties.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将实现一个简短的流程，用于预处理文本序列并使用处理后的数据训练word2vec模型。我们还将实现另一个示例来可视化嵌入向量，并检查它们的一些有趣特性。
- en: 'The code in this section requires the following Python packages:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码需要以下Python包：
- en: '**Gensim** (version 3.80, [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/))
    is an open source Python library for unsupervised topic modeling and NLP. It supports
    all three models that we have discussed so far (word2vec, GloVe, and fastText).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gensim**（版本 3.80，[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)）是一个开源
    Python 库，专注于无监督主题建模和自然语言处理。它支持我们迄今为止讨论的所有三种模型（word2vec、GloVe 和 fastText）。'
- en: The **Natural Language Toolkit** (**NLTK**, [https://www.nltk.org/](https://www.nltk.org/), ver
    3.4.4) is a Python suite of libraries and programs for symbolic and statistical
    NLP.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**，[https://www.nltk.org/](https://www.nltk.org/)，ver 3.4.4）是一个用于符号和统计自然语言处理的
    Python 库和程序套件。'
- en: Scikit-learn (ver 0.19.1, [https://scikit-learn.org/](https://scikit-learn.org/))
    is an open source Python ML library with various classification, regression, and
    clustering algorithms. More specifically, we'll use **t-Distributed Stochastic
    Neighbor Embedding** (**t-SNE**, [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))
    to visualize high-dimensional embedding vectors (more on that later).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn（ver 0.19.1，[https://scikit-learn.org/](https://scikit-learn.org/)）是一个开源
    Python 机器学习库，包含多种分类、回归和聚类算法。更具体地说，我们将使用 **t-分布随机邻居嵌入**（**t-SNE**，[https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)）来可视化高维嵌入向量（稍后会详细介绍）。
- en: With this introduction, let's continue with training the language model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一介绍，我们继续进行语言模型的训练。
- en: Training the embedding model
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练嵌入模型
- en: 'In the first example, we''ll train a word2vec model on the classic novel *War
    and Peace* by Leo Tolstoy. The novel is stored as a regular text file in the code
    repository. Let''s start:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们将对列夫·托尔斯泰的经典小说 *战争与和平* 进行 word2vec 模型训练。该小说作为常规文本文件存储在代码库中。让我们开始吧：
- en: 'As the tradition goes, we''ll do the imports:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照惯例，我们将进行导入：
- en: '[PRE0]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we''ll set the logging level to `INFO` so we can track the training progress:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将设置日志级别为 `INFO`，以便跟踪训练进度：
- en: '[PRE1]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we''ll implement the text tokenization pipeline. Tokenization refers
    to the breaking up of a text sequence into pieces (or **tokens**) such as words,
    keywords, phrases, symbols, and other elements. Tokens can be individual words,
    phrases, or even whole sentences. We''ll implement two-level tokenization; first,
    we''ll split the text into sentences and then we''ll split each sentence into
    individual words:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现文本标记化流水线。标记化是指将文本序列分解为若干部分（或 **tokens**），如单词、关键词、短语、符号和其他元素。tokens
    可以是单个单词、短语甚至整个句子。我们将实现两级标记化；首先将文本拆分为句子，然后再将每个句子拆分为单独的单词：
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `TokenizedSentences` iterator takes as an argument the text filename, where
    the novel is located. Here''s how the rest of it works:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`TokenizedSentences` 迭代器以文本文件名作为参数，文件中包含小说的内容。以下是它的工作原理：'
- en: The iteration starts by reading the full contents of the file in the `corpus`
    variable.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代从读取文件的完整内容开始，并将其存储在 `corpus` 变量中。
- en: The raw text is split into a list of sentences (the `raw_sentences` variable)
    with the help of NLTK's `nltk.tokenize.sent_tokenize(corpus)` function. For example,
    it will return a `['I like DL.', 'I like NLP.'`, `'I enjoy cycling.']` for input
    list `'I like DL. I like NLP. I enjoy cycling.'`.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始文本通过 NLTK 的 `nltk.tokenize.sent_tokenize(corpus)` 函数被拆分为句子列表（`raw_sentences`
    变量）。例如，对于输入列表 `'I like DL. I like NLP. I enjoy cycling.'`，它将返回 `['I like DL.',
    'I like NLP.', 'I enjoy cycling.']`。
- en: Next, each `sentence` is preprocessed with the `gensim.utils.simple_preprocess(sentence,
    min_len=2, max_len=15)` function. It converts a document into a list of lowercase
    tokens, ignoring tokens that are too short or too long. For example, the `'I like
    DL'` sentence will be tokenized to the `['like', 'dl']` list. The punctuation
    characters are also removed. The tokenized sentence is yielded as the final result.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，每个 `sentence` 使用 `gensim.utils.simple_preprocess(sentence, min_len=2, max_len=15)`
    函数进行预处理。该函数将文档转换为小写的 token 列表，并忽略过短或过长的 token。例如，`'I like DL'` 句子将被标记为 `['like',
    'dl']` 列表。标点符号也会被移除。处理后的句子作为最终结果返回。
- en: 'Then, we''ll instantiate `TokenizedSentences`:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将实例化 `TokenizedSentences`：
- en: '[PRE3]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we''ll instantiate Gensim''s word2vec training model:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化 Gensim 的 word2vec 训练模型：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The model takes `sentences` as a training dataset. `Word2Vec` supports all parameters
    and variants of the model that we've discussed in this chapter. For example, you
    can switch between CBOW or Skip-gram with the `sg` parameter. You can also set
    the context window size, negative sampling count, number of epochs, and other
    things. You can explore all parameters in the code itself.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以 `sentences` 作为训练数据集。`Word2Vec` 支持我们在本章中讨论的所有参数和模型变体。例如，你可以通过 `sg` 参数在 CBOW
    和 Skip-gram 之间切换。你还可以设置上下文窗口大小、负采样数量、训练轮次等参数。你可以在代码本身中探索所有参数。
- en: Alternatively, you can use the fastText model by replacing `gensim.models.word2vec.Word2Vec`
    with `gensim.models.fasttext.FastText` (it works with the same input parameters).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以通过将 `gensim.models.word2vec.Word2Vec` 替换为 `gensim.models.fasttext.FastText`
    来使用 fastText 模型（它与相同的输入参数一起工作）。
- en: 'The `Word2Vec` constructor also initiates the training. After a short time
    (you don''t need the GPU, as the training dataset is small), the generated embedding
    vectors are stored in the `model.wv` object. On one hand, it acts like a dictionary
    and you can access the vector for each word with `model.wv[''WORD_GOES_HERE''],`
    however, it also supports some other interesting functionality. You can measure
    the similarity between different words based on the difference of their word vectors
    with the `model.wv.most_similar` method. First, it converts each word vector to
    a unit vector (a vector with a length of one). Then, it computes the dot product
    between the unit vector of the target word and the unit vectors of all other words.
    The higher the dot product between two vectors, the more similar they are. For
    example, `pprint.pprint(model.wv.most_similar(positive=''mother'', topn=5))` will
    output the five most similar words to the word `''mother''` and their dot products:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Word2Vec` 构造函数也启动了训练。在短时间内（你不需要 GPU，因为训练数据集很小），生成的嵌入向量会存储在 `model.wv` 对象中。一方面，它像字典一样，你可以通过
    `model.wv[''WORD_GOES_HERE'']` 访问每个词的向量，然而，它也支持一些其他有趣的功能。你可以通过 `model.wv.most_similar`
    方法来衡量不同词语之间的相似性。首先，它将每个词向量转换为单位向量（长度为 1 的向量）。然后，它计算目标词的单位向量与所有其他词的单位向量之间的点积。两个向量的点积越大，它们的相似性越高。例如，`pprint.pprint(model.wv.most_similar(positive=''mother'',
    topn=5))` 将输出与词语 `''mother''` 最相似的五个词及其点积：'
- en: '[PRE5]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result serves as a kind of proof that the word vectors correctly encode
    the meaning of the words. The word `'mother'` is indeed related by meaning to
    `'sister'`, `'daughter'`, and so on.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明了词向量正确地编码了词语的含义。词语`'mother'` 确实在意义上与 `'sister'`、`'daughter'` 等相关联。
- en: 'We can also find the most similar words to a combination of target words. For
    example, `model.wv.most_similar(positive=[''woman'', ''king''], topn=5)` will
    take the mean of the word vectors of `''woman''` and `''king''` and then it will
    find the words with the most similar vectors to the new mean value:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以找到与目标词组合最相似的词。例如，`model.wv.most_similar(positive=['woman', 'king'], topn=5)`
    将计算 `'woman'` 和 `'king'` 的词向量的均值，然后找到与这个新均值最相似的词：
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see that some of the words are relevant (`'heiress'`), but most aren't
    (`'creature'`, `'admirable'`). Perhaps our training dataset is too small to capture
    more complex relations like these.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些词是相关的（`'heiress'`），但大多数不是（`'creature'`、`'admirable'`）。也许我们的训练数据集太小，无法捕捉到像这样的复杂关系。
- en: Visualizing embedding vectors
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化嵌入向量
- en: 'To obtain better word vectors, compared to the ones in the *Training embedding
    model* section, we''ll train another word2vec model. However, this time, we will
    use a larger corpus—the `text8` dataset, which consists of the first 100,000,000
    bytes of plain text from Wikipedia. The dataset is included in Gensim and it''s tokenized
    as a single long list of words. With that, let''s start:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得比*训练嵌入模型*部分更好的词向量，我们将训练另一个 word2vec 模型。然而，这次我们将使用一个更大的语料库——`text8` 数据集，它由维基百科的前
    1 亿字节的纯文本组成。该数据集已包含在 Gensim 中，并且它被标记为一个包含单词的长列表。现在，让我们开始吧：
- en: 'As usual, the imports are first. We''ll also set the logging to `INFO` for
    good measure:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一如既往，首先是导入。我们还将日志设置为 `INFO` 级别，以便更好地查看：
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we''ll train the `Word2vec` model. This time, we''ll use CBOW for faster
    training. We''ll load the dataset with `gensim_downloader.load(''text8'')`:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将训练 `Word2vec` 模型。这一次，我们将使用 CBOW 来加速训练。我们将通过 `gensim_downloader.load('text8')`
    加载数据集：
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To see if this model is better, we can try to find the words most similar to
    `''woman''` and `''king''`, but most dissimilar to `''man''`. Ideally, one of
    the words would be `''queen''`. We can do this with the expression `pprint.pprint(model.wv.most_similar(positive=[''woman'',
    ''king''], negative=[''man'']))`. The output is as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了判断这个模型是否更好，我们可以尝试找到与`'woman'`和`'king'`最相似但与`'man'`最不相似的词。理想情况下，其中一个词应该是`'queen'`。我们可以使用表达式`pprint.pprint(model.wv.most_similar(positive=['woman',
    'king'], negative=['man']))`来实现。输出结果如下：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Indeed, the most similar word is `'queen'`, but the rest of the words are relevant
    as well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，最相似的词是`'queen'`，但其余的词也相关。
- en: 'Next, we''ll display the words in a 2D plot with the help of a t-SNE visualization
    model on the collected word vectors. The t-SNE models each high-dimensional embedding
    vector on a two- or three-dimensional point in a way where similar objects are
    modeled on nearby points and dissimilar objects are modeled on distant points
    with a high probability. We''ll start with several `target_words` and then we''ll
    collect clusters of the *n* most similar words (and their vectors) to each target
    word. The following is the code that does this:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将利用t-SNE可视化模型，在收集到的词向量上展示这些词在2D图中的分布。t-SNE将每个高维嵌入向量映射到二维或三维空间中的一个点，使得相似的对象被映射到附近的点，不相似的对象则被映射到远离的点，并且这种映射具有较高的概率。我们将从几个`target_words`开始，然后收集与每个目标词最相似的*n*个词（及其词向量）。以下是执行此操作的代码：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we''ll train a t-SNE visualization model on the collected clusters with the
    following parameters:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用以下参数训练一个t-SNE可视化模型，基于收集到的聚类结果：
- en: '`perplexity` is loosely related to the number of nearest neighbors considered
    when matching the original and the reduced vectors for each point. In other words,
    it determines whether the algorithm will focus on the local or the global properties
    of the data.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perplexity`与在匹配每个点的原始向量和降维向量时所考虑的最近邻居数量有关。换句话说，它决定了算法是否会关注数据的局部特性或全局特性。'
- en: '`n_components=2` specifies the number of output vector dimensions.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components=2`指定了输出向量的维度数。'
- en: '`n_iter=5000` is the number of training iterations.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_iter=5000`是训练迭代的次数。'
- en: '`init=''pca''` to use **principal component analysis** (**PCA**)-based initialization.'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init=''pca''`使用**主成分分析**（**PCA**）进行初始化。'
- en: 'The model takes the `embedding_groups` clusters as input and outputs the `embeddings_2d` array
    with 2D embedding vectors. The following is the implementation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以`embedding_groups`聚类为输入，输出带有2D嵌入向量的`embeddings_2d`数组。以下是实现代码：
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we''ll display the new 2D embeddings. To do this, we''ll initialize the
    plot and some of its properties for better visibility:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将展示新的2D嵌入。为此，我们将初始化图表及其某些属性，以提高可视性：
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we''ll iterate over each `similar_words` cluster and we''ll display its
    words on a scatter plot as points. We''ll use a unique marker for each cluster.
    The points will be annotated with their corresponding words:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将遍历每个`similar_words`聚类，并将其词语作为点展示在散点图上。每个聚类使用唯一的标记。点将标注对应的词语：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we''ll display the plot:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将展示图表：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can see how each cluster of related words is grouped in a close region of
    the 2D plot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个相关词的聚类被分组在2D图的一个接近区域中：
- en: '![](img/65557fb2-c974-4a23-a244-897b53e74984.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65557fb2-c974-4a23-a244-897b53e74984.png)'
- en: t-SNE visualization of the target words and their clusters of the most similar
    words
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE可视化目标词及其最相似词的聚类
- en: The graph, once again, proves that the obtained word vectors contain relevant
    information for the words. With the end of this example, we conclude the chapter
    as well.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该图再次证明，获得的词向量包含了词语的相关信息。随着这个示例的结束，我们也总结了本章内容。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This was the first chapter devoted to NLP. Appropriately, we started with the
    basic building blocks of most NLP algorithms today—the words and their context-based
    vector representations. We started with *n*-grams and the need to represent words
    as vectors. Then, we discussed the word2vec, fastText, and GloVe models. Finally,
    we implemented a simple pipeline to train an embedding model and we visualized
    word vectors with t-SNE.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是专门讨论NLP的第一章。恰如其分，我们从当今大多数NLP算法的基本构建模块——词语及其基于上下文的向量表示开始。我们从*n*-gram和将词表示为向量的需求开始。然后，我们讨论了word2vec、fastText和GloVe模型。最后，我们实现了一个简单的管道来训练嵌入模型，并使用t-SNE可视化了词向量。
- en: In the next chapter, we'll discuss RNNs—a neural network architecture that naturally
    lends itself to NLP tasks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论RNN——一种自然适用于NLP任务的神经网络架构。
