- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Deep Learning Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础
- en: In this chapter, we will introduce **deep learning** (**DL**) and **deep neural
    networks** (**DNNs**) – that is, **neural networks** (**NNs**) with multiple hidden
    layers. You might be wondering what the point of using more than one hidden layer
    is, given the universal approximation theorem. This is in no way a naive question,
    and for a long time, NNs were used in that way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将介绍**深度学习**（**DL**）和**深度神经网络**（**DNNs**）——即具有多个隐藏层的**神经网络**（**NNs**）。你可能会疑惑，既然有通用逼近定理，为什么还需要使用多个隐藏层？这个问题并非天真，长期以来，神经网络确实是以这种方式使用的。
- en: Without going into too much detail, one reason is that approximating a complex
    function might require a huge number of units in the hidden layer, making it impractical
    to use. There is also another, more important, reason for using deep networks,
    which is not directly related to the number of hidden layers, but to the level
    of learning. A deep network does not simply learn to predict output *Y* given
    input, *X*; it also understands the basic features of the input. It’s able to
    learn abstractions of features of input samples, understand the basic characteristics
    of the samples, and make predictions based on those characteristics. This level
    of abstraction is missing in other basic **machine learning** (**ML**) algorithms
    and shallow NNs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要过多细节解释，原因之一是，逼近一个复杂的函数可能需要隐藏层中大量的单元，这使得使用它变得不切实际。还有另一个更为重要的原因，虽然它与隐藏层的数量无关，但与学习的层次相关。一个深度网络不仅仅是学习如何根据输入*X*预测输出*Y*；它还能够理解输入的基本特征。它能够学习输入样本特征的抽象，理解样本的基本特性，并基于这些特性进行预测。这种抽象层次在其他基本的**机器学习**（**ML**）算法和浅层神经网络中是缺失的。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论以下主要主题：
- en: Introduction to DL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Fundamental DL concepts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的基本概念
- en: Deep neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: Training deep networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度网络
- en: Applications of DL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: Introducing popular DL libraries
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍流行的深度学习库
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and Keras
    as part of **TensorFlow** (**TF**). If you don’t have an environment set up with
    these tools, fret not – the example is available as a Jupyter notebook on Google
    Colab. You can find the code examples in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch和Keras作为**TensorFlow**（**TF**）的一部分来实现示例。如果你还没有设置好这些工具的环境，别担心——示例代码已经作为Jupyter笔记本文件提供在Google
    Colab上。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03)。
- en: Introduction to DL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: 'In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton published a milestone
    paper titled *ImageNet Classification with Deep Convolutional Neural Networks*
    ([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)).
    The paper describes their use of NNs to win the ImageNet competition of the same
    year, which we mentioned in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047). At
    the end of their paper, they noted that the network’s performance degrades even
    if a single layer is removed. Their experiments demonstrated that removing any
    of the middle layers resulted in an about 2% top-1 accuracy loss of the model.
    They concluded that network depth is important for the performance of the network.
    The basic question is: what makes the network’s depth so important?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 发表了一篇具有里程碑意义的论文，题为 *使用深度卷积神经网络进行ImageNet分类*
    ([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf))。该论文描述了他们使用神经网络（NN）赢得同年ImageNet竞赛的过程，这一点我们在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中提到过。论文结尾指出，即使移除单一层，网络的性能也会下降。他们的实验表明，移除任何中间层都会导致模型大约2%的Top-1准确率损失。他们得出结论，网络的深度对性能至关重要。那么，基本问题是：是什么让网络的深度如此重要呢？
- en: 'A typical English saying is a picture is worth a thousand words. Let’s use
    this approach to understand what DL is. We’ll use images from the highly cited
    paper *Convolutional Deep Belief Networks for Scalable Unsupervised Learning of
    Hierarchical Representations* ([https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf](https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf)).
    Here, the authors trained an NN with pictures of different categories of either
    objects or animals. The following figure shows how the different layers of the
    network learn different characteristics of the input data. In the first layer,
    the network learns to detect some small basic features, such as lines and edges,
    which are common for all images in all categories:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的英文谚语是“一图胜千言”。让我们用这种方法来理解什么是深度学习（DL）。我们将使用来自广泛引用的论文《卷积深度信念网络：可扩展的无监督学习层次表示》中的图像（[https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf](https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf)）。在这篇论文中，作者用不同类别的物体或动物图片训练了一个神经网络（NN）。下图展示了网络的不同层如何学习输入数据的不同特征。在第一层，网络学习检测一些小的基础特征，例如线条和边缘，这些特征在所有类别的所有图像中都是常见的：
- en: '![Figure 3.1 – The ﬁrst layer weights (top) and the second layer weights (bottom)
    after training](img/B19627_03_1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 第一层权重（上）和第二层权重（下）训练后的结果](img/B19627_03_1.jpg)'
- en: Figure 3.1 – The ﬁrst layer weights (top) and the second layer weights (bottom)
    after training
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 第一层权重（上）和第二层权重（下）训练后的结果
- en: 'But the next layers, which we can see in the following figure, combine those
    lines and edges to compose more complex features that are specific to each category.
    In the first row of the bottom-left image, we can see how the network can detect
    different features of human faces, such as eyes, noses, and mouths. In the case
    of cars, these would be wheels, doors, and so on, as seen in the second image
    from the left in the following figure. These features are **abstract** – that
    is, the network has learned the generic shape of a feature (such as a mouth or
    a nose) and can detect this feature in the input data, despite the variations
    it might have:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是接下来的层，如下图所示，将这些线条和边缘结合起来，组成了更复杂的特征，这些特征对每个类别都是特定的。在左下角图像的第一行中，我们可以看到网络如何检测到人脸的不同特征，例如眼睛、鼻子和嘴巴。对于汽车而言，这些特征可能是车轮、车门等等，如下图中的第二张图所示。这些特征是**抽象的**——即，网络已经学会了一个特征（如嘴巴或鼻子）的通用形状，并且能够在输入数据中检测到这个特征，尽管它可能具有不同的变化：
- en: '![Figure 3.2 – Columns 1 to 4 represent the second-layer (top) and third-layer
    (bottom) weights learned for a speciﬁc object category (class). Column 5 represents
    the weights learned for a mixture of four object categories (faces, cars, airplanes,
    and motorbikes)](img/B19627_03_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 第1至第4列表示为特定物体类别（类）学习的第二层（上）和第三层（下）权重。第5列表示为四个物体类别（人脸、汽车、飞机和摩托车）混合学习的权重](img/B19627_03_2.jpg)'
- en: Figure 3.2 – Columns 1 to 4 represent the second-layer (top) and third-layer
    (bottom) weights learned for a speciﬁc object category (class). Column 5 represents
    the weights learned for a mixture of four object categories (faces, cars, airplanes,
    and motorbikes)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 第1至第4列表示为特定物体类别（类）学习的第二层（上）和第三层（下）权重。第5列表示为四个物体类别（人脸、汽车、飞机和摩托车）混合学习的权重
- en: In the second row of the preceding figure, we can see how, in the deeper layers,
    the network combines these features in even more complex ones, such as faces and
    whole cars. One strength of DNNs is that they can learn these high-level abstract
    representations by themselves, deducting them from the training data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图像的第二行中，我们可以看到，在更深的层次中，网络将这些特征组合成更复杂的特征，如人脸和完整的汽车。深度神经网络（DNN）的一个优势是，它们能够自主地学习这些高层次的抽象表示，并从训练数据中推导出这些表示。
- en: Next, let’s discuss these properties of DNNs in more detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地讨论DNN的这些特性。
- en: Fundamental DL concepts
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的基本概念
- en: In 1801, Joseph Marie Charles invented the **Jacquard loom**. Charles was not
    a scientist, but simply a merchant. The Jacquard loom used a set of punched cards,
    where each card represented a pattern to be reproduced on the loom. At the same
    time, each card was an abstract representation of that pattern. Punched cards
    have been used, for example, in the tabulating machine invented by Herman Hollerith
    in 1890, or in the first computers as a means to input code. In the tabulating
    machine, the cards were simply abstractions of samples to be fed into the machine
    to calculate statistics on a population. But in the Jacquard loom, their use was
    subtler, and each card represented the abstraction of a pattern that could be
    combined with others to create more complex patterns. The punched card is an abstract
    representation of a feature of reality, the final weaved design.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 1801年，Joseph Marie Charles 发明了**贾卡尔织机**。Charles 不是一名科学家，而仅仅是一个商人。贾卡尔织机使用了一套打孔卡片，每张卡片代表着织机上要复制的图案。同时，每张卡片也是该图案的抽象表示。例如，打孔卡片曾被用在1890年由
    Herman Hollerith 发明的统计机器中，或者作为第一代计算机输入代码的一种方式。在统计机器中，卡片仅仅是要输入机器以计算人口统计数据的样本的抽象。然而，在贾卡尔织机中，卡片的使用更加微妙，每张卡片代表了可以与其他卡片组合起来形成更复杂图案的模式抽象。打孔卡片是现实特征的抽象表示，最终编织出来的设计就是这一抽象的体现。
- en: In a way, the Jacquard loom sowed the seeds of what DL is today, the definition
    of reality through the representations of its features. A DNN does not simply
    recognize what makes a cat a cat, or a squirrel a squirrel, but it understands
    what features are present in a cat and a squirrel, respectively. It learns to
    design a cat or a squirrel using those features. If we were to design a weaving
    pattern in the shape of a cat using a Jacquard loom, we would need to use punched
    cards that have whiskers on the nose, such as those of a cat, and an elegant and
    slender body. Conversely, if we were to design a squirrel, we would need to use
    a punched card that makes a furry tail. A deep network that learns basic representations
    of its output can make classifications using the assumptions it has made. For
    example, if there is no furry tail, it will probably not be a squirrel, but rather
    a cat. In this way, the amount of information the network learns is much more
    complete and robust, and the most exciting part is that DNNs learn to do this
    automatically.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，贾卡尔织机为今天的深度学习（DL）播下了种子，它通过对特征的表示定义了现实的含义。深度神经网络（DNN）不仅仅是识别猫是什么，或者松鼠是什么，它更理解猫和松鼠分别具备哪些特征。它学会了如何利用这些特征来设计一只猫或一只松鼠。如果我们要用贾卡尔织机设计一个猫形状的织物图案，我们需要使用具有猫鼻子上的胡须，且有优雅而纤细的身体的打孔卡片。相反，如果我们要设计一只松鼠，则需要使用能够表现松鼠毛茸茸尾巴的打孔卡片。一个学习了基本特征表示的深度网络，可以根据它所做的假设进行分类。例如，如果没有毛茸茸的尾巴，它可能就不是松鼠，而是猫。通过这种方式，网络学习到的信息更为完整且更具鲁棒性，最令人兴奋的是，深度神经网络能够自动完成这一学习过程。
- en: Feature learning
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征学习
- en: 'To illustrate how DL works, let’s consider the task of recognizing a simple
    geometric figure, such as a cube, as seen in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明深度学习是如何工作的，我们来考虑一个简单的几何图形识别任务，例如识别一个立方体，见下图所示：
- en: '![Figure 3.3 – An abstraction of an NN representing a cube. Diﬀerent layers
    encode features with diﬀerent levels of abstraction](img/B19627_03_3.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 一个表示立方体的神经网络（NN）的抽象图。不同的层次编码了具有不同抽象层级的特征](img/B19627_03_3.jpg)'
- en: Figure 3.3 – An abstraction of an NN representing a cube. Diﬀerent layers encode
    features with diﬀerent levels of abstraction
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 一个表示立方体的神经网络（NN）的抽象图。不同的层次编码了具有不同抽象层级的特征。
- en: The cube is composed of edges (or lines), which intersect in vertices. Let’s
    say that each possible point in the three-dimensional space is associated with
    a unit (forget for a moment that this will require an infinite number of units).
    All the points/units are in the first (input) layer of a multilayer feedforward
    network. An input point/unit is active if the corresponding point lies on a line.
    The points/units that lie on a common line (edge) have strong positive connections
    to a single common edge/unit in the next layer. Conversely, they have negative
    connections to all other units in the next layer. The only exceptions are the
    units that lie on the vertices. Each such unit lies simultaneously on three edges
    and is connected to its three corresponding units in the subsequent layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个立方体由边（或线）组成，这些边在顶点处相交。假设三维空间中的每一个可能点都与一个单位相关联（暂时忽略这将需要无穷多个单位）。所有这些点/单位都位于多层前馈网络的第一层（输入层）。如果相应的点位于一条线上，则输入点/单位是激活的。位于同一条线（边）上的点/单位与下一层中的单一公共边/单位之间有强的正向连接。相反，它们与下一层中所有其他单位之间有负向连接。唯一的例外是位于顶点上的单位。每个这样的单位同时位于三条边上，并与下一层中的三个对应单位相连接。
- en: Now, we have two hidden layers, with different levels of abstraction – the first
    for points and the second for edges. However, this is not enough to encode a whole
    cube in the network. Let’s try this with another layer for vertices. Here, each
    three active edges/units of the second layer, which form a vertex, have a significant
    positive connection to a single common vertex/unit of the third layer. Since an
    edge of the cube forms two vertices, each edge/unit will have positive connections
    to two vertices/units and negative connections to all others. Finally, we’ll introduce
    the last hidden layer (the cube). The four vertices/units forming the cube will
    have positive connections to a single cube/unit from the cube/layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了两个隐藏层，分别具有不同的抽象层次——第一个层次处理点，第二个层次处理边。然而，这还不足以在网络中编码一个完整的立方体。我们尝试通过增加一个顶点层来解决这个问题。在这里，第二层中每三个激活的边/单位（形成一个顶点）与第三层中的单一公共顶点/单位之间有显著的正向连接。由于立方体的每条边形成两个顶点，因此每条边/单位将与两个顶点/单位有正向连接，并与所有其他单位有负向连接。最后，我们将引入最后一个隐藏层（立方体）。形成立方体的四个顶点/单位将与来自立方体/层的单一立方体/单位之间有正向连接。
- en: This cube representation example is oversimplified, but we can draw several
    conclusions from it. One of them is that DNNs lend themselves well to hierarchically
    organized data. For example, an image consists of pixels, which form lines, edges,
    regions, and so on. This is also true for speech, where the building blocks are
    called **phonemes**, as well as text, where we have characters, words, and sentences.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个立方体表示的例子过于简化，但我们可以从中得出几个结论。其中之一是深度神经网络（DNN）非常适合层次化组织的数据。例如，一张图像由像素组成，这些像素形成线条、边缘、区域等。同样，语音也如此，其中的基本构件叫做**音素**，而文本则有字符、单词和句子。
- en: In the preceding example, we dedicated layers to specific cube features deliberately,
    but in practice, we wouldn’t do that. Instead, a deep network will “discover”
    features automatically during training. These features might not be immediately
    obvious and, in general, wouldn’t be interpretable by humans. Also, we wouldn’t
    know the level of the features encoded in the different layers of the network.
    Our example is more akin to classic ML algorithms, where the user has to use their
    own experience to select what they think are the best features. This process is
    called **feature engineering**, and it can be labor-intensive and time-consuming.
    Allowing a network to automatically discover features is not only easier, but
    those features are highly abstract, which makes them less sensitive to noise.
    For example, human vision can recognize objects of different shapes, sizes, in
    different lighting conditions, and even when their view is partly obscured. We
    can recognize people with different haircuts and facial features, even when they
    wear a hat or a scarf that covers their mouth. Similarly, the abstract features
    the network learns will help it recognize faces better, even in more challenging
    conditions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们有意为特定的立方体特征分配了层，但实际上，我们不会这么做。相反，深度网络会在训练过程中自动“发现”特征。这些特征可能不会立刻显现出来，而且通常人类也无法解释它们。此外，我们无法知道网络中不同层所编码的特征的层次。我们的例子更像是经典的机器学习（ML）算法，在这些算法中，用户必须凭借自己的经验选择他们认为最好的特征。这一过程被称为**特征工程**，它可能既费力又耗时。让网络自动发现特征不仅更容易，而且这些特征通常非常抽象，从而使它们对噪声的敏感度较低。例如，人类视觉可以识别不同形状、大小、光照条件下的物体，甚至在物体的视角部分被遮挡时也能识别。我们能认出不同发型和面部特征的人，即使他们戴着帽子或围巾遮住嘴巴。类似地，网络学习到的抽象特征将帮助它更好地识别面孔，即使在更具挑战性的条件下。
- en: In the next section, we’ll discuss some of the reasons DL has become so popular.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论深度学习（DL）变得如此流行的一些原因。
- en: The reasons for DL’s popularity
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习流行的原因
- en: If you’ve followed ML for some time, you may have noticed that many DL algorithms
    are not new. **Multilayer perceptrons** (**MLPs**) have been around for nearly
    50 years. Backpropagation was discovered a couple of times but finally gained
    recognition in 1986\. Yann LeCun, a famous computer scientist, perfected his work
    on convolutional networks in the 1990s. In 1997, Sepp Hochreiter and Jürgen Schmidhuber
    invented long short-term memory, a type of recurrent NN still in use today. In
    this section, we’ll try to understand why we have AI summer now, and why we only
    had AI winters ([https://en.wikipedia.org/wiki/AI_winter](https://en.wikipedia.org/wiki/AI_winter))
    before.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经关注机器学习（ML）一段时间，你可能会注意到许多深度学习（DL）算法并不新鲜。**多层感知机**（**MLPs**）已经存在近50年。反向传播算法曾多次被发现，但最终在1986年获得了认可。著名计算机科学家Yann
    LeCun在1990年代完善了他的卷积网络工作。在1997年，Sepp Hochreiter和Jürgen Schmidhuber发明了长短期记忆网络（LSTM），这是一种至今仍在使用的递归神经网络（RNN）。在这一节中，我们将尝试理解为什么如今我们迎来了AI的春天，而之前只有AI的冬天（[https://en.wikipedia.org/wiki/AI_winter](https://en.wikipedia.org/wiki/AI_winter)）。
- en: The first reason is that today, we have a lot more data than in the past. The
    rise of the internet and software in different industries has generated a lot
    of computer-accessible data. We also have more benchmark datasets, such as ImageNet.
    With this comes the desire to extract value from that data by analyzing it. And,
    as we’ll see later, DL algorithms work better when they are trained with a lot
    of data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是，今天我们拥有的数据比过去多得多。互联网和软件在各个行业的兴起生成了大量可以通过计算机访问的数据。我们还拥有更多的基准数据集，例如ImageNet。随着这些数据的增加，人们也希望通过分析数据来提取价值。正如我们稍后会看到的，深度学习（DL）算法在使用大量数据进行训练时表现得更好。
- en: The second reason is the increased computing power. This is most visible in
    the drastically increased processing capacity of **graphical processing units**
    (**GPUs**). NNs are organized in such a way as to take advantage of this parallel
    architecture. Let’s see why. As we learned in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047),
    units from a network layer are not connected to units from the same layer. We
    also learned that we could represent many layer operations as matrix multiplications.
    Matrix multiplication is embarrassingly parallel (trust me, this is a term – you
    can Google it!). The computation of each output cell is not related to the computation
    of any other output cell. Therefore, we can compute all of the outputs in parallel.
    Not coincidentally, GPUs are well suited for highly parallel operations like this.
    On the one hand, a GPU has a high number of computational cores compared to a
    **central processing unit** (**CPU**). Even though a CPU core is faster than a
    GPU one, we can still compute a lot more output cells in parallel. But what’s
    even more important is that GPUs are optimized for memory bandwidth, while CPUs
    are optimized for latency. This means that a CPU can fetch small chunks of memory
    very quickly but will be slow when it comes to fetching large chunks. The GPU
    does the opposite. For matrix multiplication in a deep network with a lot of wide
    layers, bandwidth becomes the bottleneck, not latency. In addition, the L1 cache
    of the GPU is much faster than the L1 cache for the CPU and is also larger. The
    L1 cache represents the memory of the information that the program is likely to
    use next, and storing this data can speed up the process. Much of the memory gets
    reused in DNNs, which is why L1 cache memory is important.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是计算能力的提升。最显著的表现就是**图形处理单元**（**GPU**）的处理能力大幅提高。神经网络的组织方式使得它能够充分利用这种并行架构。让我们看看为什么。正如我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中所学到的，网络层的单元与同一层的单元没有直接连接。我们还学到了，许多层的操作可以表示为矩阵乘法。矩阵乘法是显著并行的（相信我，这是一个术语——你可以去Google查找！）。每个输出单元的计算与其他输出单元的计算无关。因此，我们可以并行计算所有的输出。并且，GPU非常适合执行这样的高并行操作。一方面，GPU的计算核心数量远超**中央处理单元**（**CPU**）。尽管CPU的核心速度比GPU核心更快，但我们仍然可以在GPU上并行计算更多的输出单元。但更重要的是，GPU在内存带宽方面进行了优化，而CPU则优化了延迟。这意味着CPU可以非常快速地获取小块内存，但当需要获取大块内存时则会变得较慢。而GPU则相反。对于一个深度网络中有许多宽层的矩阵乘法，带宽成为瓶颈，而不是延迟。此外，GPU的L1缓存比CPU的L1缓存更快，而且更大。L1缓存代表了程序下一步可能使用的信息存储，存储这些数据可以加速处理过程。在深度神经网络（DNN）中，大量的内存会被重复使用，这也是L1缓存非常重要的原因。
- en: In the next section, *Deep neural networks*, we’ll give a more precise definition
    of the key NN architectures that will be thoroughly introduced in the coming chapters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节《深度神经网络》中，我们将给出神经网络关键架构的更精确定义，并将在接下来的章节中详细介绍这些架构。
- en: Deep neural networks
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: 'We could define DL as a class of ML techniques, where information is processed
    in hierarchical layers to understand representations and features from data in
    increasing levels of complexity. In practice, all DL algorithms are NNs, which
    share some common basic properties. They all consist of a graph of interconnected
    operations, which operate with input/output tensors. Where they differ is network
    architecture (or the way units are organized in the network), and sometimes in
    the way they are trained. With that in mind, let’s look at the main classes of
    NNs. The following list is not exhaustive, but it represents most NN types in
    use today:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将深度学习（DL）定义为机器学习（ML）技术的一类，其中信息通过分层处理，以逐步深入的方式理解数据中的表示和特征，复杂度逐渐增加。实际上，所有深度学习算法都是神经网络（NN），它们共享一些基本的共同特性。它们都由一个互联操作的图构成，操作过程使用输入/输出张量。它们的不同之处在于网络架构（或网络中单元的组织方式），有时也体现在训练方法上。考虑到这一点，让我们来看看神经网络的主要类型。以下列表并不详尽，但它代表了今天大多数使用中的神经网络类型：
- en: '**Multilayer perceptron** (**MLP**): An NN with feedforward propagation, fully
    connected layers, and at least one hidden layer. We introduced MLPs in [*Chapter
    2*](B19627_02.xhtml#_idTextAnchor047).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）：一种具有前馈传播、全连接层且至少有一个隐藏层的神经网络。我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中介绍了MLP。'
- en: '**Convolutional neural network** (**CNN**): A CNN is a feedforward NN with
    several types of special layers. For example, convolutional layers apply a filter
    to the input image (or sound) by sliding that filter all across the incoming signal,
    to produce an *n*-dimensional activation map. There is some evidence that units
    in CNNs are organized similarly to how biological cells are organized in the visual
    cortex of the brain. We’ve mentioned CNNs several times so far, and that’s not
    a coincidence – today, they outperform all other ML algorithms on many computer
    vision and NLP tasks. We’ll discuss CNNs in [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）：CNN是一种前馈神经网络，具有几种特殊类型的层。例如，卷积层通过滑动滤波器到输入图像（或声音）上，从而应用该滤波器，生成*n*维激活图。有证据表明，CNN中的单元以类似生物细胞在大脑视觉皮层中的组织方式进行组织。到目前为止，我们已经提到过CNN很多次，这并非巧合——今天，CNN在许多计算机视觉和自然语言处理任务中优于所有其他机器学习算法。我们将在[*第4章*](B19627_04.xhtml#_idTextAnchor107)中讨论CNN。'
- en: '**Recurrent neural network** (**RNN**): This type of NN has an internal state
    (or memory), which is based on all, or part of, the input data that’s already
    been fed to the network. The output of a recurrent network is a combination of
    its internal state (memory of inputs) and the latest input sample. At the same
    time, the internal state changes to incorporate newly input data. Because of these
    properties, recurrent networks are good candidates for tasks that work on sequential
    data, such as text or time series data. We’ll discuss recurrent networks in [*Chapter
    6*](B19627_06.xhtml#_idTextAnchor185).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）：这种类型的神经网络具有一个内部状态（或记忆），该状态基于已输入网络的所有或部分数据。循环网络的输出是其内部状态（输入的记忆）和最新输入样本的组合。同时，内部状态会发生变化，以纳入新输入的数据。由于这些特性，循环网络非常适合处理顺序数据任务，例如文本或时间序列数据。我们将在[*第6章*](B19627_06.xhtml#_idTextAnchor185)中讨论循环网络。'
- en: '**Transformer**: Like RNNs, the transformer is suited to work with sequential
    data. It uses a mechanism called **attention**, which allows it *direct simultaneous
    access* to all elements of the input sequence. This is unlike an RNN, which processes
    the sequence elements one by one and updates its internal state after each element.
    As we’ll see in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202), the attention
    mechanism has several major advantages over the classic RNNs. Because of this,
    in recent years, transformers have superseded RNNs in many tasks.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer**：与RNN类似，transformer适合处理序列数据。它使用一种叫做**注意力机制**的方式，使得模型能够*直接同时访问*输入序列中的所有元素。这与RNN不同，后者是逐个处理序列元素，并在每个元素之后更新其内部状态。正如我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中看到的，注意力机制相较于经典的RNN具有多个重要优势。正因为如此，近年来，transformer已经在许多任务中取代了RNN。'
- en: '**Autoencoders**: As we mentioned in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016),
    autoencoders are a class of unsupervised learning algorithms, in which the output
    shape is the same as the input, which allows the network to better learn basic
    representations.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**：正如我们在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中提到的，自编码器是一类无监督学习算法，其输出形状与输入相同，这使得网络能够更好地学习基本表示。'
- en: Now that we’ve outlined the major types of DNNs, let’s discuss how to train
    them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了主要的深度神经网络类型，让我们讨论一下如何训练它们。
- en: Training deep neural networks
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练深度神经网络
- en: Historically, the scientific community has understood that deeper networks have
    greater representational power compared to shallow ones. However, there were various
    challenges in training networks with more than a few hidden layers. We now know
    that we can successfully train DNNs using a combination of gradient descent and
    backpropagation, just as we discussed in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).
    In this section, we’ll see how to improve them so that we can solve some of the
    problems that exist uniquely for DNNs and not shallow NNs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，科学界一直认为，深度网络相比浅层网络具有更强的表示能力。然而，训练拥有多个隐藏层的网络曾面临许多挑战。我们现在知道，结合梯度下降和反向传播，我们可以成功训练深度神经网络（DNN），正如我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中讨论过的那样。在本节中，我们将看到如何改进这些网络，以解决一些只在深度神经网络中出现的问题，而不是浅层神经网络。
- en: 'The first edition of this book included networks such as **Restricted Boltzmann
    Machines** (**RBMs**) and **Deep Belief Networks** (**DBNs**). They were popularized
    by Geoffrey Hinton, a Canadian scientist, and one of the most prominent DL researchers.
    Back in 1986, he was also one of the inventors of backpropagation. RBMs are a
    special type of generative NN, where the units are organized into two layers,
    namely visible and hidden. Unlike feedforward networks, the data in an RBM can
    flow in both directions – from visible to hidden units, and vice versa. In 2002,
    Prof. Hinton introduced **contrastive divergence**, which is an unsupervised algorithm
    for training RBMs. In 2006, he introduced **deep belief networks** (**DBNs**),
    which are DNNs that are formed by stacking multiple RBMs. Thanks to their novel
    training algorithm, it was possible to create a DBN with more hidden layers than
    had previously been possible. But even with contrastive divergence, training a
    DBN is not easy. It is a two-step process:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第一版包括了如**限制玻尔兹曼机**（**RBMs**）和**深度信念网络**（**DBNs**）等网络。它们由加拿大科学家Geoffrey Hinton推广，他是最著名的深度学习研究者之一。早在1986年，他也是反向传播算法的发明者之一。RBM是一种特殊类型的生成性神经网络，其中单元被组织成两个层次，即可见层和隐藏层。与前馈网络不同，RBM中的数据可以双向流动——从可见单元到隐藏单元，反之亦然。2002年，Hinton教授引入了**对比散度**，这是一种用于训练RBM的无监督算法。2006年，他引入了**深度信念网络**（**DBNs**），这些是通过堆叠多个RBM形成的深度神经网络。由于其创新的训练算法，DBN可以拥有比以前更多的隐藏层。但即便有了对比散度，训练一个DBN也不是件容易的事。这是一个两步过程：
- en: First, we have to train each RBM with contrastive divergence, and gradually
    stack them on top of each other. This phase is called **pre-training**.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须使用对比散度训练每个RBM，并逐渐将它们堆叠在一起。这个阶段叫做**预训练**。
- en: In effect, pre-training serves as a sophisticated weight initialization algorithm
    for the next phase, called **fine-tuning**. With fine-tuning, we transform the
    DBN into a regular MLP and continue training it using supervised backpropagation
    and gradient descent, in the same way as we saw in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际上，预训练作为下一阶段的一个复杂的权重初始化算法，叫做**微调**。通过微调，我们将DBN转化为一个常规的MLP，并继续使用监督反向传播和梯度下降训练它，就像我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中看到的那样。
- en: Thanks to some algorithmic advances, it’s now possible to train deep networks
    using plain old backpropagation, thus effectively eliminating the pre-training
    phase. These advances rendered DBNs and RBMs obsolete. They are, without a doubt,
    interesting from a research perspective, but they are rarely used in practice
    anymore and we’ll omit them from this edition.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些算法的进展，现在可以使用传统的反向传播算法训练深度网络，从而有效地消除了预训练阶段。这些进展使得DBN和RBM变得过时。它们无疑在研究中非常有趣，但在实践中已经很少使用，我们将在本版本中省略它们。
- en: Next, let’s discuss the algorithmic advances that made training of NNs with
    backpropagation possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一些使得使用反向传播训练神经网络成为可能的算法进展。
- en: Improved activation functions
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进的激活函数
- en: 'But why is training deep networks so hard? One of the main challenges that
    pre-training solved is the so-called **vanishing gradients** problem. To understand
    it, we’ll assume that we’ll use backpropagation to train a regular MLP with multiple
    hidden layers and logistic sigmoid activation at each layer. Let’s focus on the
    sigmoid function (the same applies to tanh). As a reminder, it is computed as
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/233.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/234.png):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '那么，为什么训练深度网络如此困难呢？预训练解决的主要挑战之一就是所谓的**梯度消失**问题。为了理解这一点，我们假设使用反向传播训练一个普通的多层感知机（MLP），该网络具有多个隐藏层，并在每个层使用逻辑
    sigmoid 激活函数。我们先聚焦于 sigmoid 函数（tanh 函数的情况也是如此）。提醒一下，sigmoid 函数的计算公式为 ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/233.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/234.png):'
- en: '![Figure 3.4 – Logistic sigmoid (uninterrupted) and its derivative (interrupted)
    (left); consecutive sigmoid activations, which “squash” the data (right)](img/B19627_03_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 逻辑 sigmoid（未中断）及其导数（中断）（左）；连续的 sigmoid 激活，将数据“压缩”（右)](img/B19627_03_4.jpg)'
- en: Figure 3.4 – Logistic sigmoid (uninterrupted) and its derivative (interrupted)
    (left); consecutive sigmoid activations, which “squash” the data (right)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 逻辑 sigmoid（未中断）及其导数（中断）（左）；连续的 sigmoid 激活，将数据“压缩”（右）
- en: 'The vanishing gradients manifest themselves in the following ways:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失现象表现为以下几种方式：
- en: In the forward phase, the outputs of the first sigmoid layer are represented
    by the blue uninterrupted line (both left and right images in the preceding figure)
    and fall in the range (0, 1). The dotted lines on the right image represent the
    sigmoid activations of each of the consecutive layers after the first. Even after
    three layers, we can see that the activation is “squashed” in a narrow range and
    converges to around 0.66, regardless of the input value. For example, if the input
    value of the first layer is 2, then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.881</mml:mn></mml:math>](img/235.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.71</mml:mn></mml:math>](img/236.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.67</mml:mn></mml:math>](img/237.png),
    and so on. This peculiarity of the sigmoid function acts as an eraser of any information
    coming from the preceding layers.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前向传播阶段，第一个sigmoid层的输出由前面图像中的蓝色不中断线表示，且其值位于（0, 1）范围内。右侧图像中的虚线表示每一层连续层之后的sigmoid激活值。即使经过三层，我们也可以看到激活值在一个狭窄的范围内“压缩”，并且无论输入值如何，它都趋向于约0.66。例如，如果第一层的输入值为2，那么![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.881</mml:mn></mml:math>](img/235.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.71</mml:mn></mml:math>](img/236.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.67</mml:mn></mml:math>](img/237.png)，等等。sigmoid函数的这一特性相当于擦除从前一层传来的信息。
- en: We now know that to train an NN, we need to compute the derivative of the activation
    function (along with all the other derivatives) for the backward phase. The derivative
    of the sigmoid function is represented by the green interrupted line on the left
    image in the preceding figure. We can see that it has a significant value in a
    very narrow interval, centered around 0, and converges toward 0 in all other cases.
    In networks with many layers, the derivative would likely converge to 0 when propagated
    to the first layers of the network. Effectively, this means we cannot propagate
    the error to these layers and we cannot update their weights in a meaningful way.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在知道，要训练一个神经网络（NN），我们需要计算激活函数的导数（以及所有其他的导数）以供反向传播阶段使用。在前面的图像中，左侧的绿色中断线表示了sigmoid函数的导数。我们可以看到，它在一个非常窄的区间内有显著的值，且该区间围绕0居中，而在其他所有情况下则趋向于0。在有多个层的网络中，当导数传播到网络的前几层时，它很可能会趋向于0。实际上，这意味着我们无法将误差传播到这些层，也无法以有意义的方式更新它们的权重。
- en: 'Thankfully, the **ReLU** activation we introduced in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)
    can solve both of these problems with a single stroke. To recap, the following
    figure shows the ReLU graph and its derivative:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在[**第二章**](B19627_02.xhtml#_idTextAnchor047)中引入的**ReLU**激活函数可以一举解决这两个问题。回顾一下，下面的图像展示了ReLU图形及其导数：
- en: '![Figure 3.5 – ReLU activation (uninterrupted) and its derivative (interrupted)](img/B19627_03_5.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – ReLU激活函数（不中断）及其导数（中断）](img/B19627_03_5.jpg)'
- en: Figure 3.5 – ReLU activation (uninterrupted) and its derivative (interrupted)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – ReLU激活函数（不中断）及其导数（中断）
- en: 'ReLU has the following desirable properties:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU具有以下理想特性：
- en: It is **idempotent**. If we pass a value through an arbitrary number of ReLU
    activations, it will not change; for example, *ReLU(2) = 2*, *ReLU(ReLU(2)) =
    2*, and so on. This is not the case for a sigmoid. The idempotence of ReLU makes
    it theoretically possible to create networks with more layers compared to the
    sigmoid.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 **幂等的**。如果我们通过任意次数的 ReLU 激活传递一个值，它将保持不变；例如，*ReLU(2) = 2*，*ReLU(ReLU(2)) =
    2*，依此类推。这对于 sigmoid 函数来说并不成立。ReLU 的幂等性使得理论上可以创建比 sigmoid 更深层的网络。
- en: We can also see that its derivative is either 0 or 1, regardless of the backpropagated
    value. In this way, we can avoid vanishing gradients in the backward pass as well.
    Strictly speaking, the derivative ReLU at value 0 is undefined, which makes the
    ReLU only semi-differentiable (more information about this can be found at [https://en.wikipedia.org/wiki/Semi-differentiability](https://en.wikipedia.org/wiki/Semi-differentiability)).
    But in practice, it works well enough.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以看到，它的导数无论反向传播的值如何，都是 0 或 1。通过这种方式，我们还可以避免在反向传播中梯度消失的问题。严格来说，ReLU 在值为 0
    时的导数是未定义的，这使得 ReLU 只在半微分意义下有效（关于这一点的更多信息可以参考 [https://en.wikipedia.org/wiki/Semi-differentiability](https://en.wikipedia.org/wiki/Semi-differentiability)）。但在实践中，它足够有效。
- en: It creates sparse activations. Let’s assume that the weights of the network
    are initialized randomly through normal distribution. Here, there is a 0.5 chance
    that the input for each ReLU unit is < 0\. Therefore, the output of about half
    of all activations will also be 0\. The sparse activations have several advantages,
    which we can roughly summarize as Occam’s razor in the context of NNs – it’s better
    to achieve the same result with a simpler data representation than a complex one.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它产生稀疏的激活。假设网络的权重通过正态分布随机初始化。在这种情况下，每个 ReLU 单元的输入有 0.5 的概率小于 0。因此，大约一半的激活输出也将为
    0。这种稀疏激活有几个优势，我们可以粗略地总结为在神经网络中的奥卡姆剃刀原则——用更简单的数据表示来实现相同的结果，比复杂的表示方式更好。
- en: It’s faster to compute in both the forward and backward passes.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在前向和反向传播中计算速度更快。
- en: 'Despite these ReLU advantages, during training, the network weights can be
    updated in such a way that some of the ReLU units in a layer will always receive
    inputs smaller than 0, which, in turn, will cause them to permanently output 0
    as well. This phenomenon is known as **dying ReLUs**. To solve this, several ReLU
    modifications have been proposed. The following is a non-exhaustive list:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ReLU 有这些优势，但在训练过程中，网络权重可能会被更新到某些 ReLU 单元总是接收小于 0 的输入，从而导致它们始终输出 0。这种现象被称为
    **死亡 ReLU（dying ReLUs）**。为了解决这个问题，已经提出了几种 ReLU 的改进方法。以下是一个非详尽的列表：
- en: '**Leaky ReLU**: When the input is larger than 0, leaky ReLU repeats its input
    in the same way as the regular ReLU does. However, when *x < 0*, the leaky ReLU
    outputs *x* multiplied by some constant, α (*0 < α < 1*), instead of 0\. The following
    diagram shows the leaky ReLU formula, its derivative, and their graphs for *α
    =* *0.2*:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU**：当输入大于 0 时，Leaky ReLU 与普通 ReLU 相同，直接输出输入值。然而，当 *x < 0* 时，Leaky
    ReLU 输出 *x* 与某个常数 α (*0 < α < 1*) 的乘积，而不是 0。下图展示了 Leaky ReLU 的公式、它的导数以及它们的图形，*α
    = 0.2*：'
- en: '![Figure 3.6 – The leaky ReLU activation function](img/B19627_03_6.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – Leaky ReLU 激活函数](img/B19627_03_6.jpg)'
- en: Figure 3.6 – The leaky ReLU activation function
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – Leaky ReLU 激活函数
- en: '**Parametric ReLU** (**PReLU**; see *Delving Deep into Rectifiers: Surpassing
    Human-Level Performance on ImageNet Classification*, [https://arxiv.org/abs/1502.01852):](https://arxiv.org/abs/1502.01852):)
    This activation is the same as the leaky ReLU, but *α* is tunable and is adjusted
    during training.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parametric ReLU** (**PReLU**；参见 *深入探讨激活函数：超越人类水平的 ImageNet 分类*， [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852))：该激活函数与
    Leaky ReLU 相同，但 *α* 是可调的，并且在训练过程中会进行调整。'
- en: '**Exponential linear unit** (**ELU**; see *Fast and Accurate Deep Network Learning
    by Exponential Linear Units (ELUs)*, [https://arxiv.org/abs/1511.07289):](https://arxiv.org/abs/1511.07289):)
    When the input is larger than 0, ELU repeats its input in the same way as ReLU
    does. However, when *x < 0*, the ELU output becomes ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/238.png),
    where *α* is a tunable parameter. The following diagram shows the ELU formula,
    its derivative, and their graphs for *α =* *0.2*:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数线性单元** (**ELU**；见 *通过指数线性单元（ELUs）进行快速准确的深度网络学习*，[https://arxiv.org/abs/1511.07289](https://arxiv.org/abs/1511.07289)):
    当输入大于0时，ELU与ReLU的工作方式相同。然而，当 *x < 0* 时，ELU的输出变为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/238.png)，其中
    *α* 是一个可调参数。下图展示了ELU公式、它的导数及其图像，适用于 *α =* *0.2*：'
- en: '![Figure 3.7 – The ELU activation function](img/B19627_03_7.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – ELU 激活函数](img/B19627_03_7.jpg)'
- en: Figure 3.7 – The ELU activation function
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – ELU 激活函数
- en: '**Scaled exponential linear unit** (**SELU**; see *Self-Normalizing Neural
    Networks*, [https://arxiv.org/abs/1706.02515):](https://arxiv.org/abs/1706.02515):)
    This activation is like ELU, except that the output (both smaller and larger than
    0) is scaled with an additional training parameter, *λ*. The SELU is part of a
    larger concept called **self-normalizing NNs** (**SNNs**), which is described
    in the source paper.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放指数线性单元** (**SELU**；见 *自归一化神经网络*，[https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515)):
    该激活函数类似于ELU，除了输出（大于或小于0）通过一个附加的训练参数 *λ* 进行缩放。SELU 是一个更大概念的组成部分，称为 **自归一化神经网络**
    (**SNNs**)，这一概念在源论文中有所描述。'
- en: '**Sigmoid Linear Unit (SiLU)**, **Gaussian Error Linear Unit** (**GELU**; see
    *Gaussian Error Linear Units (GELUs)*, [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)),
    and **Swish** (see *Searching for Activation Functions*, [https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)):
    This is a collection of three similar (but not the same) functions that closely
    resemble ReLU but are differentiable at the 0 point. For the sake of simplicity,
    we’ll only show the SiLU graph (*σ* is the sigmoid function):'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid线性单元 (SiLU)**，**高斯误差线性单元** (**GELU**；见 *高斯误差线性单元 (GELUs)*，[https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415))，以及
    **Swish**（见 *激活函数搜索*，[https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)):
    这是一个由三个相似（但不完全相同）函数组成的集合，它们与ReLU非常相似，但在0点处是可微的。为了简化，我们只展示SiLU的图像（*σ* 是Sigmoid函数）：'
- en: '![Figure 3.8 – The SiLU activation function](img/B19627_03_8.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – SiLU 激活函数](img/B19627_03_8.jpg)'
- en: Figure 3.8 – The SiLU activation function
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – SiLU 激活函数
- en: 'Finally, we have softmax, which is the activation function of the output layer
    in classification problems. Let’s assume that the output of the final network
    layer is a vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/239.png).
    Each of the *n* elements represents one of *n* classes, to which the input sample
    might belong. To determine the network prediction, we’ll take the index, *i*,
    of the highest value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/240.png),
    and assign the input sample to the class it represents. However, we can also interpret
    the network output as a probability distribution of a discrete random variable
    – that is, each value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/241.png),
    represents the probability that the input sample belongs to that particular class.
    To help us with this, we’ll use the softmax activation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了 softmax，它是分类问题中输出层的激活函数。假设最终网络层的输出是一个向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/239.png)。每个
    *n* 元素代表可能属于的 *n* 个类别之一。为了确定网络的预测结果，我们将取最大值的索引 *i*，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/240.png)，并将输入样本分配给它所代表的类别。然而，我们也可以将网络的输出解释为离散随机变量的概率分布——即，每个值，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/241.png)，代表输入样本属于特定类别的概率。为了帮助我们实现这一点，我们将使用
    softmax 激活函数：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>i</mi></msub></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>j</mi></msub></mfenced></mrow></mrow></mfrac></mrow></mrow></math>](img/242.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>i</mi></msub></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><msub><mi>z</mi><mi>j</mi></msub></mfenced></mrow></mrow></mfrac></mrow></mrow></math>](img/242.png)'
- en: 'It has the following properties:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有以下属性：
- en: 'The denominator in the formula acts as a normalizer. This is important for
    the probability interpretation we just introduced:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公式中的分母充当了归一化器。这对于我们刚才介绍的概率解释非常重要：
- en: Every value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/243.png),
    is constrained within the [0, 1] range, which allows us to treat it as a probability
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个值，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/243.png)，都被限制在[0,
    1]的范围内，这使我们可以将其视为一个概率。
- en: 'The total sum of values of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/244.png)
    is equal to 1: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mi>j</mi><mrow
    /></msubsup><mrow><mi>f</mi><mfenced open="(" close=")"><msub><mi>z</mi><mi>j</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/245.png),
    which also aligns with the probability interpretation'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '值的总和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/244.png)
    等于 1: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mi>j</mi><mrow
    /></msubsup><mrow><mi>f</mi><mfenced open="(" close=")"><msub><mi>z</mi><mi>j</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/245.png)，这也与概率解释相一致。'
- en: A bonus (in fact, obligatory) is that the function is differentiable.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个额外的（实际上是强制性的）条件是该函数是可微的。
- en: The softmax activation has one more subtle property. Before we normalize the
    data, we transform each vector component exponentially with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/246.png).
    Let’s imagine that two of the vector components are ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/247.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math>](img/248.png).
    Here, we would have exp(1) = 2.7 and exp(2) = 7.39\. As we can see, the ratios
    between the components before and after the transformation are very different
    – 0.5 and 0.36\. In effect, the softmax function increases the probability of
    higher scores compared to lower ones.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softmax 激活函数还有一个更微妙的属性。在我们对数据进行归一化之前，我们对每个向量组件进行指数变换，变换公式为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/246.png)。假设两个向量组件为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/247.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math>](img/248.png)。在这种情况下，我们会得到
    exp(1) = 2.7 和 exp(2) = 7.39。如我们所见，变换前后组件的比率有很大不同——0.5 和 0.36。实际上，softmax 函数增强了较高分数的概率，相对于较低的分数。
- en: 'In practice, **softmax** is often used in combination with the **cross-entropy
    loss** function. It compares the difference between the estimated class probabilities
    and the actual class distribution (the difference is known as cross-entropy).
    We can define the cross-entropy loss for a single training sample as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，**softmax** 常常与 **交叉熵损失** 函数结合使用。它比较估计的类别概率与实际类别分布之间的差异（这种差异称为交叉熵）。我们可以将单个训练样本的交叉熵损失定义如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/249.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/249.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/250.png)
    is the estimated probability of the output belonging to class *j* (out of *n*
    total classes) and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/251.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/252.png)
    is the actual probability. The actual distribution, *P(X)*, is usually a one-hot-encoded
    vector, where the real class has a probability of 1, and all others have a probability
    of 0\. In this case, the cross-entropy loss will only capture the error on the
    target class and will discard all other errors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/250.png)
    是输出属于类 *j*（从 *n* 个类中） 的估计概率，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/251.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/252.png)
    是实际的概率。实际分布 *P(X)* 通常是一个独热编码向量，其中真实的类具有 1 的概率，其他所有类的概率为 0。在这种情况下，交叉熵损失函数将仅捕捉目标类的误差，并忽略其他类的误差。
- en: Now that we’ve learned how to prevent vanishing gradients and we’re able to
    interpret the NN output as a probability distribution, we’ll focus on the next
    challenge in front of DNNs – overfitting.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何防止梯度消失，并且能够将神经网络输出解释为概率分布，我们将重点关注 DNN 面临的下一个挑战——过拟合。
- en: DNN regularization
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNN 正则化
- en: So far, we’ve learned that an NN can approximate any function. But with great
    power comes great responsibility. The NN may learn to approximate the noise of
    the target function rather than its useful components. For example, imagine that
    we are training an NN to classify whether an image contains a car or not, but
    for some reason, the training set contains mostly red cars. It may turn out that
    the NN will associate the color red with the car, rather than its shape. Now,
    if the network sees a green car in inference mode, it may not recognize it as
    such because the color doesn’t match. As we discussed in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016),
    this problem is referred to as overfitting and it is central to ML (and even more
    so in deep networks). In this section, we’ll discuss several ways to prevent it.
    Such techniques are collectively known as regularization.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到神经网络（NN）可以逼近任何函数。但强大的能力伴随着巨大的责任。神经网络可能会学习逼近目标函数的噪声，而不是其有用的部分。例如，假设我们正在训练一个神经网络来分类图像是否包含汽车，但由于某种原因，训练集大多数是红色的汽车。结果可能是，神经网络会将红色与汽车关联，而不是其形状。现在，如果网络在推理模式下看到一辆绿色的汽车，它可能无法识别为汽车，因为颜色不匹配。正如我们在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中讨论的那样，这个问题被称为过拟合，它是机器学习（ML）的核心问题（在深度网络中尤为严重）。在本节中，我们将讨论几种防止过拟合的方法。这些技术统称为正则化。
- en: 'In the context of NNs, these regularization techniques usually impose some
    artificial limitations or obstacles on the training process to prevent the network
    from approximating the target function too closely. They try to guide the network
    to learn generic rather than specific approximation of the target function in
    the hope that this representation will generalize well on previously unseen examples
    of the test dataset. Let’s start with regularization techniques that apply to
    the input data before we feed it to the NN:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的上下文中，这些正则化技术通常会在训练过程中施加一些人工的限制或障碍，以防止网络过度逼近目标函数。它们试图引导网络学习目标函数的一般性而非特定的逼近方式，期望这种表示能够在之前未见过的测试数据集示例上良好地泛化。让我们先从应用于输入数据的正则化技术开始，然后再将其输入到神经网络中：
- en: '**Min-max normalization**: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math>](img/253.png).
    Here, *x* is a single element of the input vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/254.png)
    is the smallest element of the training dataset, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/255.png)
    is the largest element. This operation scales all the inputs in the [0, 1] range.
    For example, a grayscale image will have a min color value of 0 and a max color
    value of 255\. Then, a pixel with an intensity of 125 would have a scaled value
    of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>125</mml:mn><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>255</mml:mn><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.49</mml:mn></mml:math>](img/256.png).
    Min-max is fast and easy to implement. One problem with this normalization is
    that data outliers could have an outsized impact on the result over the whole
    dataset. For example, if a single erroneous element has a very large value, it
    will enter the formula as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/257.png)
    and it will drive all normalized dataset values toward 0.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大归一化**：![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math>](img/253.png)。这里，*x*
    是输入向量的单个元素，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/254.png)
    是训练数据集中最小的元素，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/255.png)
    是最大的元素。此操作将所有输入缩放到 [0, 1] 范围内。例如，一个灰度图像的最小颜色值为 0，最大颜色值为 255。然后，一个强度为 125 的像素，其缩放值为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>125</mml:mn><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mn>255</mml:mn><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.49</mml:mn></mml:math>](img/256.png)。最小-最大归一化快速且易于实现。此归一化的一个问题是数据中的异常值可能对整个数据集的结果产生过大的影响。例如，如果一个单一的错误元素有非常大的值，它会进入公式计算中，并成为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/257.png)，这将使所有归一化后的数据集值趋近于
    0。'
- en: '**Standard score** (or **z-score**): ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:math>](img/258.png).
    It handles data outliers better than min-max. To understand how, let’s focus on
    the formula:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准分数**（或**z分数**）：![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:math>](img/258.png)。它比最小-最大方法更好地处理数据中的异常值。为了理解其原因，让我们专注于这个公式：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/259.png)
    is the mean value of all elements of the dataset, where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)
    is a single element of the input vector and *N* is the total size of the dataset.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/259.png)
    是数据集中所有元素的均值，其中 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)
    是输入向量中的单个元素，*N* 是数据集的总大小。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:math>](img/261.png)
    is the **standard deviation** of all dataset elements. It measures how far apart
    the dataset values are from the mean value. There is also **variance,** **![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/262.png)**![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/263.png),
    which removes the square root from the standard deviation. The variance is theoretically
    correct but is less intuitive than standard deviation, which is measured in the
    same units as the original data, *x*.'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:math>](img/261.png)
    是所有数据集元素的**标准差**。它衡量数据集的值与均值的偏离程度。还有**方差，** **![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/262.png)**![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/263.png)，它去除了标准差中的平方根。方差在理论上是正确的，但比标准差不那么直观，因为标准差与原始数据的单位相同，*x*。'
- en: Alternatively, we can compute μ and σ per sample if it’s not practical to compute
    them over the entire dataset. The standard score maintains the dataset’s mean
    value close to 0 and its standard deviation close to 1.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另外，如果在整个数据集上计算μ和σ不实际，我们也可以按样本计算它们。标准分数保持数据集的均值接近0，标准差接近1。
- en: '**Data augmentation**: This is where we artificially increase the size of the
    training set by applying random modifications to the training samples before feeding
    them to the network. In the case of images, these would be rotation, skew, scaling,
    and so on.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：这是通过在将训练样本输入到网络之前，对其进行随机修改，从而人为地增加训练集的大小。在图像的情况下，这些修改可能包括旋转、倾斜、缩放等。'
- en: 'The next class of regularization techniques are applied within the DNN structure
    itself:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下一类正则化技术应用于DNN结构本身：
- en: '**Dropout**: Here, we randomly and periodically remove some of the units of
    a layer (along with their input and output connections) from the network. During
    a training mini-batch, each unit has a probability, *p*, of being stochastically
    dropped. This is to ensure that no unit ends up relying too much on other units
    and “learns” something useful for the NN instead. Dropout is only applied during
    the training phase and all the units in the network fully participate during the
    inference phase. In the following figure, we can see a dropout for fully connected
    layers:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃法**：在这里，我们随机且周期性地从网络中去除某些层的单元（连同它们的输入和输出连接）。在每个训练小批量中，每个单元都有一个概率 *p*，使其随机丢弃。这样做是为了确保没有单元过度依赖其他单元，而是“学习”对神经网络有用的内容。丢弃法仅在训练阶段应用，所有单元在推理阶段都会完全参与。在下图中，我们可以看到全连接层的丢弃法：'
- en: "![Figure 3.9 – An example of dropout on full\uFEFFy connected layers](img/B19627_03_9.jpg)"
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 全连接层丢弃法示例](img/B19627_03_9.jpg)'
- en: Figure 3.9 – An example of dropout on fully connected layers
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 全连接层丢弃法示例
- en: '**Batch normalization** (**BN**; see *Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift*, [https://arxiv.org/abs/1502.03167):](https://arxiv.org/abs/1502.03167):)
    This is a way to apply data processing, not unlike the standard score, for the
    hidden layers of the network. It normalizes the outputs of the hidden layer for
    each mini-batch (hence the name) in a way that maintains its mean activation value
    close to 0 (**re-centering**) and its standard deviation close to 1 (**re-scaling**).
    The intuition is that as information is propagated through the layers, these values
    can deviate from the desired values. Let’s say that the mini-batch is represented
    by an *m×n* matrix, **X**. Each row of **X**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/264.png),
    represents a single input vector (this vector is an output of a preceding layer).
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/265.png)
    is the *j*-th element of the *i*-th vector. We can compute BN for each matrix
    element in the following way:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化**（**BN**；参见 *Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift*，[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)）：这是一种对网络隐藏层应用数据处理的方法，类似于标准分数。它对每个小批量（因此得名）隐藏层的输出进行归一化，使其均值接近0（**重新中心化**），标准差接近1（**重新缩放**）。其直观理解是，随着信息在各层之间传播，这些值可能会偏离期望值。假设小批量由
    *m×n* 矩阵 **X** 表示。**X** 的每一行，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/264.png)，表示一个单独的输入向量（该向量是前一层的输出）。![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/265.png)
    是第 *i* 个向量的第 *j* 个元素。我们可以通过以下方式计算每个矩阵元素的BN：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/266.png):
    This is the mini-batch mean. We compute a single *μ* value over all cells of the
    mini-batch matrix.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/266.png):
    这是小批量均值。我们通过对小批量矩阵的所有单元格计算一个单一的*μ*值。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/267.png):
    This is the mini-batch variance. We compute a single ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi>σ</mi><mrow
    /><mn>2</mn></msubsup></mrow></math>](img/268.png) value over all cells of the
    mini-batch matrix.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/267.png):
    这是小批量方差。我们通过对小批量矩阵的所有单元格计算一个单一的![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi>σ</mi><mrow
    /><mn>2</mn></msubsup></mrow></math>](img/268.png)值。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:math>](img/269.png):
    We normalize each cell of the matrix. *ε* is a constant that’s added for numerical
    stability, so the denominator cannot become 0.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:math>](img/269.png)：我们对矩阵的每个单元格进行归一化处理。*ε*
    是为了数值稳定性而添加的常数，这样分母就不会变为 0。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>γ</mml:mi><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>≡</mml:mo><mml:mtext>B</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>γ,β</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/270.png):
    This formula represents the scale and shift of the original data. *γ* and *β*
    are learnable parameters and we compute them over each location, *ij* (![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/271.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/272.png)),
    over all cells of the mini-batch matrix.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>γ</mml:mi><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>≡</mml:mo><mml:mtext>B</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>γ,β</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/270.png)：该公式表示原始数据的尺度和偏移。*γ*
    和 *β* 是可学习的参数，我们在每个位置上计算它们，*ij* (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/271.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/272.png))，在整个小批量矩阵的所有单元格上计算。'
- en: '**Layer normalization** (**LN**; see *Layer Normalization*, [https://arxiv.org/abs/1607.06450):](https://arxiv.org/abs/1607.06450):)
    LN is similar to BN, but with one key difference: the mean and variance are computed
    separately over each mini-batch sample. This is unlike BN, where these values
    are computed across the whole mini-batch. As with BN, the mini-batch is an *m×n*
    matrix, **X**, and each row vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/273.png),
    is the output of a preceding layer, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/274.png)
    is the *j*-th element of the *i*-th vector. Then, we have the following for the
    *i*-th input vector:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层归一化** (**LN**；见 *层归一化*， [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450))：LN
    类似于 BN，但有一个关键区别：均值和方差是分别在每个小批量样本上计算的。这与 BN 不同，BN 是在整个小批量上计算这些值。与 BN 一样，小批量是一个
    *m×n* 矩阵，**X**，每一行向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/273.png)，是前一层的输出，且
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/274.png)
    是第 *i* 个向量的第 *j* 个元素。那么，我们对于第 *i* 个输入向量有如下公式：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/275.png)'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/275.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/276.png)'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/276.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mstyle scriptlevel="+1"><mfrac><mrow><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>μ</mi><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub><mn>2</mn></msubsup><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mrow></math>](img/277.png)'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mstyle scriptlevel="+1"><mfrac><mrow><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>μ</mi><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><msub><mi
    mathvariant="bold">x</mi><mi>i</mi></msub><mn>2</mn></msubsup><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mrow></math>](img/277.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>γ</mml:mi><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>≡</mml:mo><mml:mtext>L</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>γ,β</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/278.png)'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>γ</mml:mi><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>≡</mml:mo><mml:mtext>L</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>γ,β</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/278.png)'
- en: '**Root mean square layer normalization** (**RMSNorm**; see [https://arxiv.org/abs/1910.07467):](https://arxiv.org/abs/1910.07467):)
    The authors of RMSNorm argue that the main benefit of LN comes just from the re-scaling,
    rather than the combination of re-centering and re-scaling. Therefore, RMSNorm
    is a simplified and faster version of LN, which only applies re-scaling using
    the root mean square statistic. We’ll use the same notation as with LN. So, we
    can define RMSNorm as follows:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根层归一化** (**RMSNorm**; 参见 [https://arxiv.org/abs/1910.07467](https://arxiv.org/abs/1910.07467)):
    RMSNorm 的作者认为，LN 的主要好处仅来自重新缩放，而不是重新中心化和重新缩放的结合。因此，RMSNorm 是 LN 的简化且更快速的版本，它只使用均方根统计量进行重新缩放。我们将使用与
    LN 相同的符号。因此，我们可以将 RMSNorm 定义如下：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMS</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math>](img/279.png).'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMS</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math>](img/279.png).'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>RMS</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/280.png):
    Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/281.png)
    is the gain parameter used to re-scale the standardized summed inputs (it is set
    to 1 at the beginning). It is equivalent to the *γ* parameter in BN.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>RMS</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/280.png):
    这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/281.png)
    是增益参数，用于重新缩放标准化的输入和求和（初始值为 1）。它等同于 BN 中的 *γ* 参数。'
- en: 'The following figure illustrates the difference between BN and LN. On the left,
    we compute single *μ* and *σ* values across the whole mini-batch. To the right,
    we can see ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/282.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/283.png)
    for each row:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了 BN 和 LN 之间的差异。在左侧，我们计算整个小批量中单个 *μ* 和 *σ* 值。右侧，我们可以看到每行分别为 ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml></mml:mrow></mml:msub></mml:math>](img/282.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml></mml:mrow></mml:msub></mml:math>](img/283.png)：
- en: '![Figure 3.10 – BN and LN computation of μ and σ](img/B19627_03_10.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – BN 和 LN 计算 μ 和 σ](img/B19627_03_10.jpg)'
- en: Figure 3.10 – BN and LN computation of μ and σ
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – BN 和 LN 计算 μ 和 σ
- en: 'The final type of regularization we’ll introduce is **L2 regularization**.
    This technique adds a special regularization term to the cost function. To understand
    it, let’s take the MSE cost. We can add L2 regularization to it in the following
    way (the underscored part of the formula):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的最终正则化类型是**L2 正则化**。该技术在成本函数中添加一个特殊的正则化项。为了理解它，让我们以 MSE 成本为例。我们可以通过以下方式将
    L2 正则化添加到其中（公式中的下划线部分）：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/284.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>θ</mml></mml:mrow><mml:mrow><mml:mi>j</mml></mml:mrow><mml:mrow><mml:mn>2</mml></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:munder></mml:mfenced></mml:math>](img/284.png)'
- en: 'Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png)
    is one of *m* total network weights and *λ* is the weight decay coefficient. The
    rationale is that if the network weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/174.png),
    are large, then the cost function will also increase. In effect, weight decay
    penalizes large weights (hence the name). This prevents the network from relying
    too heavily on a few features associated with these weights. There is less chance
    of overfitting when the network is forced to work with multiple features. In practical
    terms, when we compute the derivative of the weight decay cost function (the preceding
    formula) concerning each weight and then propagate it to the weights themselves,
    the weight update rule changes from the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png)
    是 *m* 个总网络权重之一，*λ* 是权重衰减系数。其原理是，如果网络权重，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/174.png)，较大，那么代价函数也会增大。实际上，权重衰减会惩罚大权重（因此得名）。这可以防止网络过度依赖与这些权重相关的少数特征。当网络被迫使用多个特征时，过拟合的机会会减少。实际操作中，当我们计算权重衰减代价函数（前面的公式）对每个权重的导数并将其传播到权重本身时，权重更新规则发生如下变化：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/287.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/288.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/287.png)
    到 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/288.png)'
- en: With this discussion of DNN regularization, we’ve covered our theoretical base.
    Next, let’s see what the real-world applications of DNNs are.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对DNN正则化的讨论，我们已经涵盖了理论基础。接下来，让我们看看DNN的实际应用是什么。
- en: Applications of DL
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: ML in general, and DL in particular, is producing more and more astonishing
    results in terms of the quality of predictions, feature detection, and classification.
    Many of these recent results have made the news. Such is the pace of progress
    that some experts are worrying that machines will soon be more intelligent than
    humans. But I hope that any such fears you might have will be alleviated after
    you have read this book. For better or worse, we’re still far from machines having
    human-level intelligence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，机器学习（ML），尤其是深度学习（DL），在预测质量、特征检测和分类方面取得了越来越令人惊叹的结果。许多这些最新的成果已经成为新闻头条。进步的速度如此之快，以至于一些专家担心机器很快会比人类更聪明。但我希望你在读完这本书后，能够缓解你可能有的任何这种担忧。无论如何，机器仍然远未达到人类级别的智能。
- en: In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047), we mentioned how DL algorithms
    have occupied the leaderboard of the ImageNet competition. They are successful
    enough to make the jump from academia to industry.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中，我们提到过深度学习算法如何占据了ImageNet竞赛的领先位置。它们的成功足以从学术界跳跃到工业界。
- en: 'Let’s talk about some real-world use cases of DL:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看深度学习的一些实际应用案例：
- en: Nowadays, new cars have a suite of safety and convenience features that aim
    to make the driving experience safer and less stressful. One such feature is automated
    emergency braking if the car sees an obstacle. Another one is lane-keeping assist,
    which allows the vehicle to stay in its current lane without the driver needing
    to make corrections with the steering wheel. To recognize lane markings, other
    vehicles, pedestrians, and cyclists, these systems use a
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如今，新的汽车配备了一系列安全性和便利性功能，旨在使驾驶体验更安全、更轻松。其中一项功能是自动紧急刹车系统，当汽车看到障碍物时会自动刹车。另一个功能是车道保持辅助，它可以让车辆在不需要驾驶员操控方向盘的情况下保持当前车道。为了识别车道标记、其他车辆、行人和骑行者，这些系统使用了
- en: forward-facing camera. One of the most prominent suppliers of such systems,
    Mobileye ([https://www.mobileye.com/](https://www.mobileye.com/)), has produced
    custom chips that use CNNs to detect these objects on the road ahead. To give
    you an idea of the importance of this sector, in 2017, Intel acquired Mobileye
    for $15.3 billion. This is not an outlier, and Tesla’s famous Autopilot system
    also relies on CNNs to achieve the same results. The former director of AI at
    Tesla, Andrej Karpathy ([https://karpathy.ai/](https://karpathy.ai/)), is a well-known
    researcher in the field of DL. We can speculate that future autonomous vehicles
    will also use deep networks for computer vision.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前置摄像头。该领域最著名的供应商之一，Mobileye ([https://www.mobileye.com/](https://www.mobileye.com/))，已经生产了定制芯片，使用卷积神经网络（CNN）来检测前方道路上的物体。为了让你了解这个行业的重要性，2017年，英特尔以153亿美元收购了Mobileye。这并非个案，特斯拉著名的自动驾驶系统也依赖CNN来实现相同的效果。特斯拉前AI总监安德烈·卡帕西（Andrej
    Karpathy）([https://karpathy.ai/](https://karpathy.ai/))是深度学习领域的知名研究者。我们可以推测，未来的自动驾驶汽车也将使用深度网络进行计算机视觉。
- en: Both Google’s **Vision API** ([https://cloud.google.com/vision/](https://cloud.google.com/vision/))
    and Amazon’s **Rekognition** ([https://aws.amazon.com/rekognition/](https://aws.amazon.com/rekognition/))
    services use DL models to provide various computer vision capabilities. These
    include recognizing and detecting objects and scenes in images, text recognition,
    face recognition, content moderation, and so on.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的**视觉API** ([https://cloud.google.com/vision/](https://cloud.google.com/vision/))
    和亚马逊的**Rekognition** ([https://aws.amazon.com/rekognition/](https://aws.amazon.com/rekognition/))
    服务都使用深度学习模型提供各种计算机视觉能力。这些功能包括图像中的物体和场景识别、文本识别、人脸识别、内容审核等。
- en: If these APIs are not enough, you can run your own models in the cloud. For
    example, you can use Amazon’s AWS DL AMIs (short for **Amazon Machine Images**;
    see [https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/)),
    which are virtual machines that come configured with some of the most popular
    DL libraries. Google offers a similar service with their Cloud AI ([https://cloud.google.com/products/ai/](https://cloud.google.com/products/ai/)),
    but they’ve gone one step further. They created **tensor processing units** (**TPUs**;
    see https://cloud.google.com/tpu/) – microprocessors that are optimized for fast
    NN operations such as matrix multiplication and activation functions.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这些API不够，你还可以在云端运行自己的模型。例如，你可以使用亚马逊的AWS DL AMI（即**亚马逊机器镜像**；见[https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/)），这些是预配置了流行深度学习库的虚拟机。谷歌也提供类似的服务，通过其Cloud
    AI（[https://cloud.google.com/products/ai/](https://cloud.google.com/products/ai/)），但他们更进一步。他们创建了**张量处理单元**（**TPUs**；见https://cloud.google.com/tpu/）——这是一种专为快速神经网络操作（如矩阵乘法和激活函数）优化的微处理器。
- en: 'DL has a lot of potential for medical applications. However, strict regulatory
    requirements, as well as patient data confidentiality, have slowed down its adoption.
    Nevertheless, we’ll identify several areas in which DL could have a high impact:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（DL）在医学应用中具有很大的潜力。然而，严格的监管要求以及患者数据的保密性限制了其普及。尽管如此，我们仍然可以识别出深度学习在以下几个领域可能产生重大影响：
- en: Medical imaging is an umbrella term for various non-invasive methods of creating
    visual representations of the inside of the body. Some of these include **magnetic
    resonance images** (**MRIs**), ultrasound, **computed axial tomography** (**CAT**)
    scans, X-rays, and histology images. Typically, such an image is analyzed by a
    medical professional to determine the patient’s condition.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学影像学是指多种非侵入性方法，用于创建身体内部的视觉表现。其中包括**磁共振影像**（**MRIs**）、超声、**计算机断层扫描**（**CAT**）扫描、X射线和组织学影像。通常，这些图像需要由医学专业人员分析，以确定患者的病情。
- en: Computer-aided diagnosis, and computer vision, in particular, can help specialists
    by detecting and highlighting important features of images. For example, to determine
    the degree of malignancy of colon cancer, a pathologist would have to analyze
    the morphology of the glands using histology imaging. This is a challenging task
    because morphology can vary greatly. A DNN could segment the glands from the image
    automatically, leaving the pathologist to verify the results. This would reduce
    the time needed for analysis, making it cheaper and more accessible.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机辅助诊断，特别是计算机视觉，可以通过检测和突出显示图像中的重要特征来帮助专家。例如，为了确定结肠癌的恶性程度，病理学家需要使用组织学影像分析腺体的形态学。这是一项具有挑战性的任务，因为形态学可能有很大的变化。深度神经网络（DNN）可以自动从图像中分割出腺体，剩下的工作由病理学家来验证结果。这将减少分析所需的时间，使得分析更加廉价且更易获得。
- en: Another medical area that could benefit from DL is the analysis of medical history
    records. When a doctor diagnoses a patient’s condition and prescribes treatment,
    they consult the patient’s medical history first. A DL algorithm could extract
    the most relevant and important information from those records, even if they are
    handwritten. In this way, the doctor’s job would be made easier, and the risk
    of errors would also be reduced.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可以受益于深度学习的医学领域是病历记录的分析。当医生诊断患者的病情并开具治疗方案时，他们首先会查阅患者的病历。深度学习算法可以从这些记录中提取最相关和最重要的信息，即使它们是手写的。这样，医生的工作将变得更轻松，同时也减少了错误的风险。
- en: One area where DNNs have already had an impact is in protein folding. Proteins
    are large, complex molecules, whose function depends on their 3D shape. The building
    blocks of proteins are amino acids, and their sequence determines the shape of
    the protein. The protein folding problem seeks to understand the relationship
    between the initial amino acid sequence and the final 3D shape of the protein.
    DeepMind’s **AlphaFold 2** model (believed to be based on transformers; see [https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe](https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe))
    has managed to predict 200 million protein structures, which represents almost
    all known cataloged proteins.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）已在一个领域取得了显著的影响——蛋白质折叠。蛋白质是大型复杂分子，其功能取决于其三维结构。蛋白质的基本构件是氨基酸，其序列决定了蛋白质的形状。蛋白质折叠问题旨在理解初始氨基酸序列与蛋白质最终三维结构之间的关系。DeepMind的**AlphaFold
    2**模型（据信基于变换器架构；见 [https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe](https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe)）成功预测了2亿种蛋白质结构，这几乎涵盖了所有已知的已编目蛋白质。
- en: Google’s Neural Machine Translation API ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    uses – you guessed it – DNNs for machine translation.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google的神经机器翻译API ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    使用了——你猜对了——深度神经网络（DNNs）进行机器翻译。
- en: Siri ([https://machinelearning.apple.com/2017/10/01/hey-siri.html](https://machinelearning.apple.com/2017/10/01/hey-siri.html)),
    Google Assistant, and Amazon Alexa ([https://aws.amazon.com/deep-learning/](https://aws.amazon.com/deep-learning/))
    rely on deep networks for speech recognition.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siri ([https://machinelearning.apple.com/2017/10/01/hey-siri.html](https://machinelearning.apple.com/2017/10/01/hey-siri.html))、Google助手和Amazon
    Alexa ([https://aws.amazon.com/deep-learning/](https://aws.amazon.com/deep-learning/))
    依赖深度网络进行语音识别。
- en: '**AlphaGo** is an AI machine based on DL that made the news in 2016 by beating
    the world Go champion, Lee Sedol. AlphaGo had already made the news, in January
    2016, when it beat the European champion, Fan Hui. At the time, however, it seemed
    unlikely that it could go on to beat the world champion. Fast-forward a couple
    of months and AlphaGo was able to achieve this remarkable feat by sweeping its
    opponent in a 4-1 victory series. This was an important milestone because Go has
    many more possible game variations than other games, such as chess, and it’s impossible
    to consider every possible move in advance. Also, unlike chess, in Go, it’s very
    difficult to even judge the current position or value of a single stone on the
    board. In 2017, DeepMind released an updated version of AlphaGo called **AlphaZero**
    ([https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)), and in
    2019, they released a further update called **MuZero** ([https://arxiv.org/abs/1911.08265](https://arxiv.org/abs/1911.08265)).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaGo** 是基于深度学习（DL）的人工智能机器，它在2016年通过战胜世界围棋冠军李世石而成为新闻焦点。AlphaGo在2016年1月就已经引起了媒体的关注，当时它击败了欧洲围棋冠军范睿。然而，当时似乎不太可能击败世界冠军。几个月后，AlphaGo以4-1的胜利系列取得了这一非凡成就。这是一个重要的里程碑，因为围棋有比其他棋类（如国际象棋）更多的可能变化，而且在事先很难考虑所有可能的走法。此外，与国际象棋不同，在围棋中，即便是判断棋盘上单颗棋子的当前局势或价值也非常困难。2017年，DeepMind发布了AlphaGo的更新版**AlphaZero**
    ([https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815))，而在2019年，他们发布了一个进一步更新的版本，名为**MuZero**
    ([https://arxiv.org/abs/1911.08265](https://arxiv.org/abs/1911.08265))。'
- en: Tools such as GitHub Copilot ([https://github.com/features/copilot](https://github.com/features/copilot))
    and ChatGPT ([https://chat.openai.com/](https://chat.openai.com/)) utilize generative
    DNN models to transform natural language requests into source code snippets, functions,
    and entire programs. We already mentioned Stable Diffusion ([https://stability.ai/blog/stable-diffusion-public-release](https://stability.ai/blog/stable-diffusion-public-release))
    and DALL-E ([https://openai.com/dall-e-2/](https://openai.com/dall-e-2/)), which
    can generate realistic images based on text description.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像GitHub Copilot ([https://github.com/features/copilot](https://github.com/features/copilot))
    和ChatGPT ([https://chat.openai.com/](https://chat.openai.com/))这样的工具利用生成型深度神经网络模型将自然语言请求转化为源代码片段、函数或完整的程序。我们之前提到的Stable
    Diffusion ([https://stability.ai/blog/stable-diffusion-public-release](https://stability.ai/blog/stable-diffusion-public-release))
    和DALL-E ([https://openai.com/dall-e-2/](https://openai.com/dall-e-2/))，则能够根据文本描述生成逼真的图像。
- en: With this short list, we aimed to cover the main areas in which DL is applied,
    such as computer vision, NLP, speech recognition, and **reinforcement learning**
    (**RL**). This list is not exhaustive, however, as there are many other uses for
    DL algorithms. Still, I hope this has been enough to spark your interest. Next,
    we’ll formally introduce two of the most popular DL libraries – PyTorch and Keras.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的列表中，我们旨在涵盖深度学习（DL）应用的主要领域，如计算机视觉、自然语言处理（NLP）、语音识别和**强化学习**（**RL**）。然而，这个列表并不详尽，因为深度学习算法还有许多其他的应用。不过，我希望这些内容足以激发你的兴趣。接下来，我们将正式介绍两个最受欢迎的深度学习库——PyTorch和Keras。
- en: Introducing popular DL libraries
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍流行的深度学习库
- en: 'We already implemented a simple example with PyTorch in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016).
    In this section, we’ll introduce this library, and Keras, more systemically. Let’s
    start with the common features of most DNN libraries:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中实现了一个使用PyTorch的简单示例。在本节中，我们将更系统地介绍该库以及Keras。我们从大多数深度神经网络（DNN）库的共同特点开始：
- en: All libraries use Python.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有库都使用Python。
- en: The basic unit for data storage is the **tensor**. Mathematically, the definition
    of a tensor is more complex, but in the context of DL libraries, they are multi-dimensional
    (with an arbitrary number of axes) arrays of base values.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储的基本单元是**张量**。从数学角度来看，张量的定义更加复杂，但在深度学习库的语境中，张量是多维的（具有任意数量的轴）基本值数组。
- en: NNs are represented as a **computational graph** of operations. The nodes of
    the graph represent the operations (weighted sum, activation function, and so
    on). The edges represent the flow of data, which is how the output of one operation
    serves as an input for the next one. The inputs and outputs of the operations
    (including the network inputs and outputs) are tensors.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络（NN）表示为**计算图**，图中的节点代表操作（加权求和、激活函数等）。边缘代表数据流动，数据如何从一个操作的输出作为下一个操作的输入。操作的输入和输出（包括网络的输入和输出）都是张量。
- en: All libraries include **automatic differentiation**. This means that all you
    need to do is define the network architecture and activation functions, and the
    library will automatically figure out all of the derivatives required for training
    with backpropagation.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有库都包含**自动微分**功能。这意味着你只需要定义网络架构和激活函数，库将自动计算训练过程中反向传播所需的所有导数。
- en: So far, we’ve referred to GPUs in general, but in reality, the vast majority
    of DL projects work exclusively with NVIDIA GPUs. This is because of the better
    software support NVIDIA provides. These libraries are no exception – to implement
    GPU operations, they rely on the CUDA Toolkit ([https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit))
    in combination with the cuDNN library ([https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)).
    cuDNN is an extension of CUDA, built specifically for DL applications. As mentioned
    in the *Applications of DL* section, you can also run your DL experiments in the
    cloud.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，我们提到了GPU，但实际上，绝大多数深度学习项目仅使用NVIDIA GPU。这是因为NVIDIA提供了更好的软件支持。这些库也不例外——为了实现GPU操作，它们依赖于CUDA工具包（[https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)）和cuDNN库（[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)）。cuDNN是CUDA的扩展，专为深度学习应用构建。如在*深度学习应用*部分所提到的，你也可以在云端运行你的深度学习实验。
- en: PyTorch is an independent library, while Keras is built on top of TF and acts
    as a user-friendly TF interface. We’ll continue by implementing a simple classification
    example using both PyTorch and Keras.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个独立的库，而Keras则建立在TF之上，作为一个用户友好的TF接口。接下来，我们将使用PyTorch和Keras实现一个简单的分类示例。
- en: Classifying digits with Keras
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras进行数字分类
- en: 'Keras exists either as a standalone library with TF as the backend or as a
    sub-component of TF itself. You can use it in both flavors. To use Keras as part
    of TF, we need only to install TF itself. Once we’ve done this, we can use the
    library with the following import:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Keras作为独立库存在，其中TF作为后端，也可以作为TF本身的子组件使用。你可以选择这两种方式之一。若要将Keras作为TF的一部分使用，我们只需安装TF本身。安装完成后，我们可以通过以下导入使用该库：
- en: '[PRE0]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The standalone Keras supports different backends besides TF, such as Theano.
    In this case, we can install Keras itself and then use it with the following import:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 独立版Keras支持除TF外的不同后端，如Theano。在这种情况下，我们可以安装Keras本身，然后通过以下导入来使用它：
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The large majority of Keras’s use is with the TF backend. The author of Keras
    recommends using the library as a TF component (the first option) and we’ll do
    so in the rest of this book.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的大多数使用都基于TF后端。Keras的作者推荐将该库作为TF的一个组件使用（即第一种方式），我们将在本书的其余部分遵循这个方式。
- en: 'In this section, we’ll use Keras via TF to classify the images of the MNIST
    dataset. It’s comprised of 70,000 examples of digits that have been handwritten
    by different people. The first 60,000 are typically used for training and the
    remaining 10,000 for testing:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将通过TF使用Keras来分类MNIST数据集的图像。该数据集包含了70,000个由不同人手写的数字示例。前60,000个通常用于训练，剩下的10,000个用于测试：
- en: '![Figure 3.11 – A sample of digits taken from the MNIST dataset](img/B19627_03_11.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 从MNIST数据集中提取的数字样本](img/B19627_03_11.jpg)'
- en: Figure 3.11 – A sample of digits taken from the MNIST dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 从MNIST数据集中提取的数字样本
- en: 'We’ll build a simple MLP with one hidden layer. Let’s start:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个简单的多层感知机（MLP），并且只包含一个隐藏层。让我们开始：
- en: 'One of the advantages of Keras is that it can import this dataset for you without
    you needing to explicitly download it from the web (it will download it for you):'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras的一个优点是它可以为你导入这个数据集，而不需要你显式地从网上下载（它会为你自动下载）：
- en: '[PRE2]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `(X_train, Y_train)` is the training images and labels, and `(X_validation,
    Y_validation)` is the test images and labels.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`(X_train, Y_train)`是训练图像和标签，`(X_validation, Y_validation)`是测试图像和标签。
- en: 'We need to modify the data so that we can feed it to the NN. `X_train` contains
    60,000 28×28 pixel images, and `X_validation` contains 10,000\. To feed them to
    the network as inputs, we want to reshape each sample as a 784-pixel-long array,
    rather than a 28×28 two-dimensional matrix. We’ll also normalize them in the [0:1]
    range. We can accomplish this with these two lines:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要修改数据，以便将其输入到神经网络中。`X_train`包含60,000张28×28像素的图像，`X_validation`包含10,000张。为了将它们作为输入提供给网络，我们希望将每个样本重塑为一个784像素长度的数组，而不是28×28的二维矩阵。我们还会将其归一化到[0:1]的范围内。我们可以通过以下两行代码来实现：
- en: '[PRE3]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The labels indicate the value of the digit depicted in the images. We want
    to convert this into a 10-entry **one-hot-encoded** vector comprised of 0s and
    just one 1 in the entry corresponding to the digit. For example, 4 is mapped to
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. Conversely, our network will have 10 output units:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签表示图像中数字的值。我们希望将其转换为一个包含0和一个1的10维**独热编码**向量，其中1出现在与数字对应的索引位置。例如，4被映射为[0, 0,
    0, 0, 1, 0, 0, 0, 0, 0]。相应地，我们的网络将有10个输出单元：
- en: '[PRE4]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the NN. In this case, we’ll use the `Sequential` model, where each layer
    serves as an input to the next. In Keras, `Dense` means a fully connected layer.
    We’ll use a network with one hidden layer with 100 units, BN, ReLU activation,
    and softmax output:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络。在这个例子中，我们将使用`Sequential`模型，其中每一层都是下一层的输入。在Keras中，`Dense`表示全连接层。我们将使用一个包含100个单元、BN、ReLU激活函数和softmax输出的隐藏层：
- en: '[PRE5]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can define our gradient descent parameters. We’ll use the Adam optimizer
    and categorical cross-entropy loss (this is cross entropy, optimized for softmax
    outputs):'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义梯度下降的参数。我们将使用Adam优化器和分类交叉熵损失函数（这是针对softmax输出优化的交叉熵）：
- en: '[PRE6]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, run the training for 100 epochs and a batch size of 100\. In Keras, we
    can do this with the `fit` method, which iterates over the dataset internally.
    Keras will default to GPU training, but if a GPU is not available, it will fall
    back to the CPU:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行100轮训练，批次大小为100。在Keras中，我们可以使用`fit`方法，它会在内部遍历整个数据集。Keras默认使用GPU进行训练，但如果没有可用的GPU，它会回退到CPU：
- en: '[PRE7]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'All that’s left to do is add code to evaluate the network’s accuracy on the
    test data:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩下的就是添加代码以评估网络在测试数据上的准确性：
- en: '[PRE8]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And that’s it. The validation accuracy will be about 97.7%, which is not a great
    result, but this example runs in less than 30 seconds on a CPU. We can make some
    simple improvements, such as a larger number of hidden units, or a higher number
    of epochs. We’ll leave those experiments to you so that you can familiarize yourself
    with the code.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就这样。验证准确率大约为97.7%，虽然结果不算非常优秀，但这个示例在CPU上运行不到30秒。我们可以做一些简单的改进，比如增加更多的隐藏单元，或者增加更多的训练轮数。我们将把这些实验留给你，以便你能熟悉代码。
- en: 'To see what the network has learned, we can visualize the weights of the hidden
    layer. The following code allows us to obtain them:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查看网络学到了什么，我们可以可视化隐藏层的权重。以下代码可以帮助我们获取它们：
- en: '[PRE9]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Reshape the weights for each unit back to a 28×28 two-dimensional array and
    then display them:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个单元的权重重塑为28×28的二维数组，然后显示它们：
- en: '[PRE10]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see the result in the following figure:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到结果：
- en: '![Figure 3.12 – A composite ﬁgure of what was learned by all the hidden units](img/B19627_03_12.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12 – 所有隐藏单元学习到的复合图](img/B19627_03_12.jpg)'
- en: Figure 3.12 – A composite ﬁgure of what was learned by all the hidden units
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 所有隐藏单元学习到的复合图
- en: Now, let us see the example for PyTorch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下PyTorch的示例。
- en: Classifying digits with PyTorch
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch进行数字分类
- en: 'In this section, we’ll implement the same example that we did in the *Classifying
    digits with Keras* section but this time with PyTorch. Let’s start:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现与*使用Keras进行数字分类*部分中相同的示例，但这次使用PyTorch。让我们开始：
- en: 'First, we’ll select the device we’re using (CPU or GPU). We’ll try with the
    GPU first and fall back to the CPU if the GPU is not available:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将选择我们正在使用的设备（CPU或GPU）。我们将首先尝试GPU，如果GPU不可用，则回退到CPU：
- en: '[PRE11]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Like Keras, PyTorch supports MNIST out of the box. Here’s how we can instantiate
    the train and validation sets:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和Keras一样，PyTorch也开箱即用支持MNIST。以下是如何实例化训练集和验证集的方法：
- en: '[PRE12]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The dataset is automatically downloaded and split into training and validation
    parts. The `ToTensor()` transformation converts the images from `numpy` arrays
    into PyTorch tensors and normalizes them in the [0:1] range (as opposed to [0:255]
    originally). The `torch.flatten` transform flattens the two-dimensional 28×28
    images to a one-dimensional 784 tensor so that we can feed it to the NN.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集会自动下载并分为训练集和验证集。`ToTensor()`转换将图像从`numpy`数组转换为PyTorch张量，并将其标准化到[0:1]范围内（而非原来的[0:255]）。`torch.flatten`变换将二维28×28的图像展平成一维的784个元素，以便我们将其传递给神经网络。
- en: 'Next, we’ll wrap the datasets in `DataLoader` instances:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集封装在`DataLoader`实例中：
- en: '[PRE13]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The data `DataLoader` instance takes care of creating mini-batches and shuffles
    the data randomly. They are also iterators, which supply mini-batches one at a
    time.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据`DataLoader`实例负责创建小批量并随机打乱数据。它们也是迭代器，一次提供一个小批量。
- en: 'Then, we’ll define the NN `model`. We’ll use the same MLP with a single hidden
    layer, as in the Keras example:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义神经网络`model`。我们将使用与Keras示例中相同的具有单一隐藏层的MLP：
- en: '[PRE14]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This definition is like the one in Keras. One difference is that the `Linear`
    (fully connected) layers require both input and output dimensions since they cannot
    extract the output dimension of the preceding layer. The activations are defined
    as separate operations.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个定义类似于Keras中的定义。唯一的区别是，`Linear`（全连接）层需要输入和输出维度，因为它们无法自动提取前一层的输出维度。激活函数被定义为独立的操作。
- en: 'Next, let’s define the cross-entropy loss and the Adam optimizer:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义交叉熵损失和Adam优化器：
- en: '[PRE15]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can define the `train_model` function, which, as its name suggests,
    takes care of training the model. It takes our predefined `model`, `cost_function`,
    `optimizer`, and `data_loader` and runs the training for a single epoch:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义`train_model`函数，正如其名字所示，该函数负责训练模型。它接受我们预定义的`model`、`cost_function`、`optimizer`和`data_loader`并运行一个epoch的训练：
- en: '[PRE16]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unlike Keras and its `fit` function, we have to implement the PyTorch training
    ourselves. `train_model` iterates over all mini-batches provided by `train_loader`.
    For each mini-batch, `optimizer.zero_grad()` resets the gradients from the previous
    iteration. Then, we initiate the forward and backward passes, and finally the
    weight updates.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与Keras及其`fit`函数不同，我们需要自己实现PyTorch的训练过程。`train_model`会遍历由`train_loader`提供的所有小批量数据。对于每个小批量，`optimizer.zero_grad()`会重置前一次迭代的梯度。然后，我们开始前向传播和反向传播，最后进行权重更新。
- en: 'We’ll also define the `test_model` function, which will run the model in inference
    mode to check its results:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将定义`test_model`函数，它将在推理模式下运行模型以检查其结果：
- en: '[PRE17]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: BN and dropout layers are not used in evaluation (only in training), so `model.eval()`
    turns them off. We iterate over the validation set, initiate a forward pass, and
    aggregate the validation loss and accuracy.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BN和dropout层在评估时不会使用（只在训练时使用），因此`model.eval()`会关闭它们。我们遍历验证集，启动前向传播，并汇总验证损失和准确率。
- en: 'Let’s run the training for 20 epochs:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行训练20个epoch：
- en: '[PRE18]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This model achieves 97.6% accuracy.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型实现了97.6%的准确率。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explained what DL is and how it’s related to DNNs. We discussed
    the different types of DNNs and how to train them, and we paid special attention
    to various regularization techniques that help with the training process. We also
    mentioned many real-world applications of DL and tried to analyze the reasons
    for its efficiency. Finally, we introduced two of the most popular DL libraries,
    namely PyTorch and Keras. We also implemented identical MNIST classification examples
    with both libraries.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们解释了什么是深度学习（DL），以及它与深度神经网络（DNNs）之间的关系。我们讨论了不同类型的DNN及其训练方法，并特别关注了帮助训练过程的各种正则化技术。我们还提到了许多深度学习的实际应用，并尝试分析它们为何如此高效。最后，我们介绍了两种最流行的深度学习库——PyTorch和Keras，并用这两个库实现了相同的MNIST分类示例。
- en: In the next chapter, we’ll discuss how to solve classification tasks over more
    complex image datasets with the help of convolutional networks – one of the most
    popular and effective deep network models. We’ll talk about their structure, building
    blocks, and what makes them uniquely suited to computer vision tasks. To spark
    your interest, let’s recall that convolutional networks have consistently won
    the popular ImageNet challenge since 2012, delivering top-five accuracy from 74.2%
    to 99%.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论如何借助卷积神经网络解决更复杂的图像数据集上的分类任务——这是最流行且最有效的深度网络模型之一。我们将探讨其结构、构建模块，以及是什么让它们特别适合于计算机视觉任务。为了激发你的兴趣，我们回顾一下，自2012年以来，卷积神经网络一直在热门的ImageNet挑战赛中获胜，连续多年保持前五名的准确率，从74.2%提升到99%。
- en: 'Part 2:'
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：
- en: Deep Neural Networks for Computer Vision
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在计算机视觉中的应用
- en: In this part, we’ll introduce **convolutional neural networks** (**CNNs**) –
    a type of neural network suitable for computer vision applications. Building on
    top of the first three chapters, we’ll discuss the rationale behind CNNs, their
    building blocks, and their architecture. We’ll also outline the most popular CNN
    models in use today. Finally, we’ll focus on the advanced applications of CNNs
    – object detection, image segmentation, and image generation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍**卷积神经网络**（**CNNs**）——一种适用于计算机视觉应用的神经网络类型。在前面三章的基础上，我们将讨论CNN的基本原理、构建模块以及其架构。我们还将概述当前最流行的CNN模型。最后，我们将重点讲解CNN的高级应用——目标检测、图像分割和图像生成。
- en: 'This part has the following chapters:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分包含以下章节：
- en: '[*Chapter 4*](B19627_04.xhtml#_idTextAnchor107), *Computer Vision with Convolutional
    Networks*'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B19627_04.xhtml#_idTextAnchor107)，*使用卷积网络进行计算机视觉*'
- en: '[*Chapter 5*](B19627_05.xhtml#_idTextAnchor146), *Advanced Computer Vision
    Applications*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B19627_05.xhtml#_idTextAnchor146)，*计算机视觉的高级应用*'
