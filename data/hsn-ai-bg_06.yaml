- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are the most flexible form of networks
    and are widely used in **natural language processing **(**NLP**), financial services,
    and a variety of other fields. Vanilla feedforward networks, as well as their
    convolutional varieties, accept a fixed input vector and output a fixed vector;
    they assume that all of your input data is independent of each other. RNNs, on
    the other hand, operate on sequences of vectors and output sequences of vectors,
    and allow us to handle many exciting types of data. RNNs are actually turing-complete,
    in that they can simulate arbitrary tasks, and hence are very appealing models
    from the perspective of the Artificial Intelligence scientist.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络** (**RNNs**) 是最灵活的网络形式，并且广泛应用于 **自然语言处理** (**NLP**)、金融服务以及其他多个领域。普通的前馈网络及其卷积变种接收固定的输入向量并输出固定的向量；它们假设所有输入数据相互独立。而
    RNN 则是处理向量序列并输出向量序列的网络，能够帮助我们处理多种令人兴奋的数据类型。RNN 实际上是图灵完备的，它们能够模拟任意任务，因此从人工智能科学家的角度来看，RNN
    是非常有吸引力的模型。'
- en: In this chapter, we'll introduce ...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍...
- en: Technical requirements
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will be utilizing TensorFlow in Python 3\. The corresponding code
    for this chapter is available in the book's GitHub repository.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用 Python 3 中的 TensorFlow。对应的代码可以在本书的 GitHub 仓库中找到。
- en: The building blocks of RNNs
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 的构建模块
- en: 'When we think about how a human thinks, we don''t just observe a situation
    once; we constantly update what we''re thinking based on the context of the situation.
    Think about reading a book: each chapter is an amalgamation of words that make
    up its meaning. Vanilla feedforward networks don''t take sequences as inputs,
    and so it becomes very difficult to model unstructured data such as natural language.
    RNNs can help us achieve this.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考人类如何思考时，我们不仅仅是一次性观察某个情境；我们会根据情境的上下文不断更新我们的思维。想象一下读书：每一章都是由多个单词组成的，这些单词构成了它的意义。普通的前馈网络不会将序列作为输入，因此很难对像自然语言这样的非结构化数据进行建模。而
    RNN 可以帮助我们实现这一点。
- en: Basic structure
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本结构
- en: RNNs differ from other networks in the fact that they have a recursive structure;
    they are recurring over time. RNNs utilize recursive loops, which allow information
    to persist within the network. We can think of them as multiple copies of the
    same network, with information being passed between each successive iteration.
    Without recursion, an RNN tasked with learning a sentence of 10 words would need
    10 connected copies of the same layer, one for each word. RNNs also share parameters
    across the network. Remember in the past few chapters how the number of parameters
    we had in our network could get unreasonably large with complex layers? With recursion
    and parameter sharing, we are able to more effectively learn increasingly long
    sequence structures and minimize the amount of overall parameters that we have
    to learn.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 与其他网络的区别在于它们具有递归结构；它们在时间上是递归的。RNN 利用递归循环，使得信息能够在网络中保持。我们可以把它们看作是同一个网络的多个副本，信息在每次迭代中不断传递。如果没有递归，一个需要学习
    10 个单词的句子的 RNN 就需要 10 个连接的相同层，每个单词一个。RNN 还在网络中共享参数。回想过去几章中我们提到的，在复杂的层次结构中，我们网络的参数数量可能会变得非常庞大。通过递归和参数共享，我们能够更有效地学习越来越长的序列结构，同时最小化需要学习的总参数量。
- en: 'Basically, recurrent networks take in items from a sequence and recursively
    iterate over them. In the following diagram, our sequence *x *gets fed into the
    network:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，循环网络接收来自序列的项，并对其进行递归迭代。在下面的图示中，我们的序列 *x* 被输入到网络中：
- en: '![](img/7d33b17a-8b3b-477d-8737-1c5a7d49788a.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d33b17a-8b3b-477d-8737-1c5a7d49788a.png)'
- en: RNNs have memory, or **hidden states**, which help them manage and learn complex
    knowledge; we represent them with the variable *h.* These hidden states capture
    important information from the previous pass, and store it for future passes. RNN
    hidden states are initialized at zero and updated during the training process. Each
    pass in an RNN's is called a **time step**; if we have a 40-character sequence,
    our network will have 40 time steps. During an RNN time step, our network takes
    the input sequence *x* and returns both an output vector *y*as well as an updated
    hidden state. After the first initial time step, the hidden state *h* also gets
    fed into the network at each new step along with the input *x. *The output of
    the network is then the value of the hidden state after the last time step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RNN具有记忆或**隐藏状态**，帮助它们管理和学习复杂的知识；我们用变量*h*表示它们。这些隐藏状态捕捉来自前一次传递的重要信息，并将其存储以供后续使用。RNN的隐藏状态在初始时被设置为零，并在训练过程中不断更新。每次RNN的传递称为**时间步骤**；如果我们有一个40字符的序列，那么我们的网络就会有40个时间步骤。在每个RNN时间步骤中，网络会接受输入序列*x*并返回一个输出向量*y*以及更新后的隐藏状态。在第一次初始时间步骤之后，隐藏状态*h*也会与输入*x*一起，在每个新的时间步骤传递给网络。网络的输出是最后一个时间步骤之后隐藏状态的值。
- en: 'While RNNs are recursive, we could easily unroll them to graph what their structure
    looks like at each time step. Instead of viewing the RNN as a box, we can unpack
    its contents to examine its inner workings:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RNN是递归的，但我们可以轻松地将它们展开，绘制出它们在每个时间步骤的结构。我们可以将RNN看作一个框架，而不是一个黑盒，我们可以拆解它的内容，检查其内部工作原理：
- en: '![](img/f381e150-58cb-4aad-8a03-21573b2ff6f9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f381e150-58cb-4aad-8a03-21573b2ff6f9.png)'
- en: 'In the preceding diagram, our RNN is represented as the function *f* to which
    we apply the weights *w*, just as we would with any other neural network. In RNNs,
    however, we call this function the **recurrence formula**, and it looks something
    such as this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们的RNN被表示为函数*f*，我们将权重*w*应用到这个函数上，就像在任何其他神经网络中一样。然而，在RNN中，我们称这个函数为**递归公式**，它看起来大致如下：
- en: '![](img/dd0434c2-a294-498e-a5f5-b80982e06abd.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd0434c2-a294-498e-a5f5-b80982e06abd.png)'
- en: Here, *h[t] *represents our new state that will be the output of a single time
    step. The *h[t-1] *expression represents our previous state, and *x* is our input
    data from the sequence. This recurrence formula gets applied to an element of
    a sequence until we run out of time stamps.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*h[t]*表示我们的新状态，它将是单一时间步骤的输出。*h[t-1]*表示我们的前一个状态，而*x*是来自序列的输入数据。这个递归公式会应用于序列中的每个元素，直到我们用完时间戳。
- en: Let's apply and dissect how a basic RNN works by walking through its construction
    in TensorFlow.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在TensorFlow中构建基本的RNN来应用并分析它的工作原理。
- en: Vanilla recurrent neural networks
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普通递归神经网络
- en: Let's walk through the architecture of a basic RNN; to illustrate, we are going
    to create a basic RNN that predicts the next letter in a sequence. For our training
    data, we will be using the opening paragraph of Shakespeare's Hamlet. Let's go
    ahead and save that `corpus` as a variable that we can use in our network.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起走过基本RNN的架构；为了说明，我们将创建一个基本RNN，预测序列中的下一个字母。作为训练数据，我们将使用莎士比亚《哈姆雷特》开头的段落。我们先将这个`corpus`保存为一个变量，以便在网络中使用。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: One-to-many
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对多
- en: 'Image captioning represents a **one-to-many** scenario. We take in a single
    vector that represents that input image, and output a variable length description
    of the image. One-to-many scenarios output a label, word, or other output at each
    time step:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述是**一对多**情境的一个例子。我们输入一个表示该图像的单一向量，并输出图像的变长描述。一对多情境在每个时间步骤输出标签、单词或其他输出：
- en: '![](img/2bc2ca76-b366-4eca-aa5f-0ac379ac3779.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bc2ca76-b366-4eca-aa5f-0ac379ac3779.png)'
- en: Music generation is another example of a one-to-many scenario, where we output
    a variable set of notes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐生成是另一种一对多情境的例子，其中我们输出一组变长的音符。
- en: Many-to-one
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多对一
- en: 'In cases such as sentiment analysis, we are interested in a single and final
    hidden state for the network. We call this a **many-to-one** scenario. When we
    arrive at the last time step, we output a classification or some other value,
    represented in the computational graph by *yt*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在情感分析等任务中，我们关注的是网络的单一且最终的隐藏状态。我们称这种情况为**多对一**情境。当我们到达最后一个时间步骤时，我们输出一个分类或其他值，这个值在计算图中由*yt*表示：
- en: '![](img/1015fed0-a2e7-42ca-85c0-6fc82cba321b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1015fed0-a2e7-42ca-85c0-6fc82cba321b.png)'
- en: Many-to-one scenarios can also be used for tasks such as music genre labeling,
    where the network takes in notes and predicts a genre.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一情境也可以用于诸如音乐流派标签等任务，其中网络输入音符并预测流派。
- en: Many-to-many
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多对多
- en: 'If we wanted to classify the expression of a person during a video, or perhaps
    label a scene at any given time, or even for speech-to-text recognition, we would
    use a **many-to-many** architecture. Many-to-many architectures take in a variable''s
    length sequence while also outputting a variable length sequence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想对视频中某个人的表情进行分类，或许标注某个时刻的场景，甚至进行语音识别，我们会使用**多对多**架构。多对多架构能够接收变长的输入序列，同时输出变长的序列：
- en: '![](img/a6ed4412-0a2a-4c8e-b50c-fef908b3d6f7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6ed4412-0a2a-4c8e-b50c-fef908b3d6f7.png)'
- en: An output vector is computed at every step of the process, and we can compute
    individual losses at every step in the sequence. Frequently, we utilize a softmax
    loss for explicit labeling tasks. The final loss will be the sum of these individual
    losses.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中都会计算一个输出向量，我们可以在序列的每一步计算各自的损失。通常，在显式标注任务中，我们会使用 softmax 损失函数。最终的损失将是这些单独损失的总和。
- en: Backpropagation through time
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: RNN utilizes a special variation of regular backpropagation called **backpropagation
    through time**. Like regular old backpropagation, this process is often handled
    for us in TensorFlow; however, it's important to note how it differs from standard
    backpropagation for feedforward networks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 使用一种叫做**时间反向传播**的特殊变种，属于常规反向传播的一部分。像常规反向传播一样，这个过程通常会在 TensorFlow 中为我们处理；然而，值得注意的是，它与标准的前馈网络反向传播有所不同。
- en: Let's recall that RNNs utilize small *copies* of the same network, each with
    its own weights and bias factors. When we backpropagate through RNNs, we calculate
    the gradients at each time step, and sum the gradients across the entire network
    when computing the loss. We'll have a separate gradient from the weight that flows
    to the computation that happens at each of the time steps, and the final gradient
    for *W* will be the sum of the ...
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，RNN 使用相同网络的小*副本*，每个副本都有自己的权重和偏差项。当我们在 RNN 中进行反向传播时，我们会在每个时间步计算梯度，并在计算损失时对整个网络的梯度进行求和。我们将有一个来自每个时间步的权重梯度，并且
    *W* 的最终梯度将是这些梯度的总和……
- en: Memory units – LSTMs and GRUs
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储单元——LSTM 和 GRU
- en: While regular RNNs can theoretically ingest information from long sequences,
    such as full documents, they are limited in *how far back* they can look to learn
    information. To overcome this, researchers have developed variants on the traditional
    RNN that utilize a unit called a **memory cell**, which helps the network *remember*
    important information. They were developed as a means to solve the vanishing gradient
    problem that occurs with traditional RNN models. There are two main variations
    of RNN that utilize memory cell architectures, known as the **GRU** and the **LSTM**.
    These architectures are the most widely used RNN architectures, so we'll pay some
    what attention to their mechanics.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然常规的 RNN 理论上可以接收长序列的信息，比如完整的文档，但它们在*回顾*信息时存在限制，无法回溯得太远。为了解决这个问题，研究人员开发了传统 RNN
    的变种，使用了一种叫做**记忆单元**的单元，帮助网络*记住*重要信息。它们是为了解决传统 RNN 模型中的消失梯度问题而开发的。有两种主要的 RNN 变体利用了记忆单元架构，分别是**GRU**
    和 **LSTM**。这两种架构是目前应用最广泛的 RNN 架构，因此我们会稍微关注它们的机制。
- en: LSTM
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM
- en: LSTMs are a special form of RNN that excel at learning long-term dependencies.
    Developed by Hochreiter and Schmidhuber in 1997, LSTMs have several different
    layers that information passes through to help them keep what is important and
    jettison the rest. Unlike vanilla recurrent networks, the LSTM has not one but
    two states; the standard hidden state that we've been representing as *h[t]*,
    as well as a state that is specific to the LSTM cell called the **cell state**,
    which we will denote with *c[t]*. These LSTM states are able to update or adjust
    these states with **gating mechanisms**. These gates help to control the processing
    of information through the cell, and consist of an activation function and a basic
    **point wise operation**, such as vector multiplication. ...
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种特殊形式的 RNN，擅长学习长期依赖关系。LSTM 由 Hochreiter 和 Schmidhuber 在 1997 年提出，LSTM
    拥有多个不同的层，信息会通过这些层来帮助它保留重要的信息并丢弃其他信息。与普通的循环网络不同，LSTM 不仅有一个标准的隐藏状态 *h[t]*，还有一个专属于
    LSTM 单元的状态，称为**单元状态**，我们将用 *c[t]* 表示。LSTM 的这些状态能够通过**门控机制**进行更新或调整。这些门控机制帮助控制信息通过单元的处理，包括一个激活函数和基本的**逐点操作**，比如向量乘法。
- en: GRUs
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GRU
- en: 'GRUs were developed in 2014 as a new take on the classic LSTM cell. Instead
    of having four separate gates, it combines the forget and input gates into a single
    update gate; you can think of this in the sense of: whatever is not written, gets
    forgotten. It also merges the cell state *c[t]* from the LSTM with the overall
    hidden state *h[t]*. At the end of a cycle, the GRU exposes the entire hidden
    state:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GRU是在2014年作为对经典LSTM单元的全新改进而提出的。与LSTM的四个独立门不同，GRU将遗忘门和输入门合并成一个更新门；你可以这样理解：没有被写入的内容就会被遗忘。它还将LSTM中的细胞状态
    *c[t]* 与整体的隐藏状态 *h[t]* 合并。在一个周期结束时，GRU暴露出整个隐藏状态：
- en: '**Update gate**: The update gate combines the forget and input gates, essentially
    stating that whatever is not written into memory is forgotten. The update gate
    takes in our input data *x* and multiplies it by the hidden state. The result
    of that expression is then multiplied by the weight matrix and fed through a sigmoid
    function:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新门**：更新门将遗忘门和输入门结合起来，本质上表明，未被写入记忆的内容会被遗忘。更新门接受我们的输入数据 *x* 并将其与隐藏状态相乘。这个表达式的结果随后与权重矩阵相乘，并通过一个sigmoid函数：'
- en: '![](img/8e6d1821-5f52-4e09-88a1-3ecbe59d4af9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e6d1821-5f52-4e09-88a1-3ecbe59d4af9.png)'
- en: '**Reset gate**: The reset gate functions similarly to the output gate in the
    LSTM. It decides what and how much is to be written to memory. Mathematically,
    it is the same as the update gate:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重置门**：重置门的功能类似于LSTM中的输出门。它决定了什么内容以及多少内容会被写入记忆中。从数学上讲，它与更新门是相同的：'
- en: '![](img/9c0f8c8a-6885-4739-ba77-39b5ac47e859.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c0f8c8a-6885-4739-ba77-39b5ac47e859.png)'
- en: '**Memory gate**: A tanh operation that actually stores the output of the reset
    gate into memory, similar to the write gate in the LSTM. The results of the reset
    gate are combined with the raw input, and put through the memory gate to calculate
    a provisional hidden state:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆门**：一个tanh操作，实际上将重置门的输出存储到记忆中，类似于LSTM中的写入门。重置门的结果与原始输入结合，并通过记忆门来计算一个暂定的隐藏状态：'
- en: '![](img/72573546-7ff8-400a-81b0-61c7682c3493.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72573546-7ff8-400a-81b0-61c7682c3493.png)'
- en: 'Our end state is calculated by taking the provisional hidden state and conducting
    an operation with the output of the update gate:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终状态是通过将暂定隐藏状态与更新门的输出进行操作来计算的：
- en: '![](img/b34874a5-54cd-4f9c-a89b-3dc2ba3f1d12.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b34874a5-54cd-4f9c-a89b-3dc2ba3f1d12.png)'
- en: The performance of a GRU is similar to that of the LSTM; where GRUs really provide
    benefit is that they are more computationally efficient due to their streamlined
    structure. In language modeling tasks, such as the intelligent assistant that
    we will create in a later chapter, GRUs tend to perform better especially in situations
    with less training data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: GRU的性能与LSTM相似；GRU真正的优势在于由于其简化的结构，计算效率更高。在语言建模任务中，例如我们将在后面的章节中创建的智能助手，GRU往往在数据较少的情况下表现更好。
- en: Sequence processing with RNNs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN进行序列处理
- en: 'Now that we''ve learned about the components of RNNs, let''s dive into what
    we can do with them. In this section, we''ll look at two primary examples: **machine
    translation** and **generating image captions**. Later on in this book, we''ll
    utilize RNNs to build a variety of end-to-end systems.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了RNN的组件，接下来我们来探讨一下我们可以用它们做什么。在这一部分，我们将讨论两个主要示例：**机器翻译** 和 **生成图像描述**。在本书的后面部分，我们将利用RNN构建各种端到端系统。
- en: Neural machine translation
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经机器翻译
- en: Machine translation represents a sequence-to-sequence problem; you'll frequently
    see these networks described as **sequence-to-sequence** (or **Seq2Seq**) models. Instead
    of utilizing traditional techniques that involve feature engineering and n-gram
    counts, neural machine translation maps the overall meaning of a sentence to a
    singular vector, and we then generate a translation based on that singular meaning
    vector.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译是一个序列到序列的问题；你会经常看到这些网络被描述为**序列到序列**（或**Seq2Seq**）模型。与传统的特征工程和n-gram计数技术不同，神经机器翻译将整个句子的意义映射到一个单一的向量，然后基于该单一的意义向量生成翻译。
- en: '**Machine translation** models rely on an important concept in artificial intelligence
    known as the **encoder**/**decoder** paradigm. In a nutshell:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器翻译**模型依赖于人工智能中的一个重要概念，即**编码器**/**解码器**范式。简而言之：'
- en: '**Encoders** parse over input and output a condensed, vector representation
    of the input. We typically use a GRU or LSTM for this task'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**解析输入并输出输入的压缩向量表示。我们通常使用GRU或LSTM来完成这个任务'
- en: '**Decoders** take the condensed representation, and extrapolate to create a
    new sequence from it'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**接收压缩后的表示，并从中推断出新的序列。'
- en: These concepts are extremely import in understanding generative adversarial
    networks, which we will learn about in a coming chapter. The first part of this
    network acts as an encoder that will parse over your sentence in English; it represents
    the summarization of the sentence in English.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念对于理解生成对抗网络（GAN）至关重要，我们将在接下来的章节中学习。该网络的第一部分充当编码器，解析你的英文句子；它表示该句子的英文摘要。
- en: 'Architecturally, neural machine translation networks are many-to-one and one-to-many,
    which sit back-to-back with each other, as demonstrated in the following diagram:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构上看，神经机器翻译网络是多对一和一对多的，它们背靠背排列，正如下图所示：
- en: '![](img/c7641cfc-342f-4982-b03c-b2a3f676a93a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7641cfc-342f-4982-b03c-b2a3f676a93a.png)'
- en: This is the same architecture that Google uses for Google Translate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是谷歌用于Google Translate的相同架构。
- en: Attention mechanisms
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: '**Neural machine translation **(**NMT**) models suffer from the same long-term
    dependency issues that RNNs in general suffer from. While we saw that LSTMs can
    mitigate much of this behavior, it still becomes problematic with long sentences.
    Especially in machine translation, where the translation of the sentence is largely
    dependent on how much information is contained within the hidden state of the
    encoder network, we must ensure that those end states are as rich as possible.
    We solve this with something called **attention mechanisms**.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经机器翻译**（**NMT**）模型面临与一般RNN相同的长期依赖性问题。尽管我们看到LSTM能够缓解大部分这种行为，但在长句子中仍然会出现问题。特别是在机器翻译中，句子的翻译很大程度上依赖于编码器网络的隐藏状态所包含的信息量，我们必须确保这些最终状态尽可能丰富。我们通过一种叫做**注意力机制**的方式解决了这个问题。'
- en: Attention mechanisms allow the decoder to select parts of the input sentence
    based on context and what has generated thus far. We utilize a vector called a
    **context vector** to store scores from the ...
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制允许解码器根据上下文和到目前为止生成的内容选择输入句子的某些部分。我们利用一个叫做**上下文向量**的向量来存储来自...
- en: Generating image captions
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成图片描述
- en: 'RNNs can also work with problems that require fixed input to be transformed
    into a variable sequence. Image captioning takes in a fixed input picture, and
    outputs a completely variable description of that picture. These models utilize
    a CNN to input the image, and then feed the output of that CNN into an RNN, which
    will generate the caption one word at a time:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）也可以处理那些需要将固定输入转换为可变序列的问题。图片描述处理输入固定的图片，并输出该图片的完全可变描述。这些模型利用卷积神经网络（CNN）来输入图片，然后将CNN的输出传递给RNN，RNN会逐字生成描述：
- en: '![](img/757eb8e8-e8e5-4820-9285-6c0a0d4fab09.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/757eb8e8-e8e5-4820-9285-6c0a0d4fab09.png)'
- en: We'll be building a neural captioning model based on the Flicker 30 dataset,
    provided by the University of California, Berkley, which you can find in the corresponding
    GitHub repository for this chapter. In this case, we'll be utilizing pretrained
    image embeddings for the sake of time; however you can find an example of an end-to-end
    model in the corresponding GitHub repository.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个基于Flicker 30数据集的神经网络描述模型，该数据集由加利福尼亚大学伯克利分校提供，你可以在本章对应的GitHub仓库中找到。在这个案例中，我们会利用预训练的图像嵌入来节省时间；然而，你也可以在相应的GitHub仓库中找到端到端模型的示例。
- en: 'Let''s start with our imports:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入开始：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let's load the image data from the files you've downloaded from the repository.
    First, let's define a path to where we can find the images.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们从你从仓库下载的文件中加载图像数据。首先，我们定义一个路径来存放这些图像。
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we can actually load in the images and the various captions. We'll call
    these captions and images.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以加载图像和各种描述。我们将这些描述和图像称为“描述”和“图像”。
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we''ll have to store the occurrence count for the number of times the
    words appear:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要存储每个单词出现次数的计数：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'First, we''ll have to construct a vocabulary to draw from; we''ll need to do
    a bit of preprocessing for our captions beforehand. You can go through the following
    code:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要构建一个词汇表；我们需要对描述进行一些预处理。可以参考以下代码：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can construct the model itself. Follow the comments below to understand
    what each section is doing:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以构建模型了。按照下面的注释理解每个部分的作用：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will train the cycle for the model:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将为模型训练这个循环：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: During training, we utilize a separate weight matrix to help model the image
    information, and utilize that weight matrix to ingest the image information at
    every RNN time step cycle.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们利用一个独立的权重矩阵来帮助模型处理图像信息，并在每个RNN时间步周期中使用该权重矩阵来接收图像信息。
- en: 'At each step, our RNN will compute a distribution over all scores in the network''s
    vocabulary, sample the most likely word from that distribution, and utilize that
    word as the input for the next RNN time step. These models are typically trained
    end-to-end, meaning that backpropagation for both RNN and the CNN happens simultaneously.
    In this case, we only need to worry about our RNN:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，我们的RNN将计算网络词汇表中所有分数的分布，从该分布中抽取最可能的词，并将该词作为下一个RNN时间步的输入。这些模型通常是端到端训练的，这意味着RNN和CNN的反向传播是同时进行的。在这种情况下，我们只需要关注我们的RNN：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s initialize the caption RNN model and start building the model.
    The code for that is as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化标题RNN模型并开始构建模型。相关代码如下：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The last thing we need to do is train the model; we can do that by simply calling
    the `training` function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要做的就是训练模型；我们可以通过简单地调用`training`函数来实现：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Often, you may see many variants of RNNs for text generation called the **character**-**level
    RNN**, or **ChaRnn**. It's advisable to stay away from character-level RNN models.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可能会看到许多用于文本生成的RNN变体，被称为**字符**-**级别RNN**，或**ChaRnn**。建议避免使用字符级别的RNN模型。
- en: Extensions of RNNs
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的扩展
- en: 'There have been many extensions of vanilla RNNs over the past several years.
    This is by no means an exhaustive list of all of the great advances in RNNs that
    are happening in the community, but we''re going to review a couple of the most
    notable ones: Bidirectional RNNs, and NTM.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，普通RNN有很多扩展。这里列出的并不是所有RNN领域的重大进展，但我们将回顾其中几个最值得注意的：双向RNN和NTM。
- en: Bidirectional RNNs
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向RNN
- en: In recent years, researchers have developed several improvements on the traditional
    RNN structure. **Bidirectional RNNs **were developed with the idea that they may
    not only depend on the information that came before in a sequence, but also the
    information that comes afterwards. Structurally, they are just two RNNs that are
    stacked on top of each other, and their output is a combination of the hidden
    states of each of the individual networks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，研究人员对传统RNN结构进行了若干改进。**双向RNN**的出现是基于这样一个想法：它们不仅可以依赖序列中之前的信息，还可以依赖之后的信息。从结构上讲，双向RNN只是两个堆叠在一起的RNN，它们的输出是每个单独网络的隐藏状态的组合。
- en: Neural turing machines
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图灵机
- en: '**Neural turing machines** (**NTM**) are a form of RNN that were developed
    by Alex Graves of DeepMind in 2014\. Instead of having an internal state such
    as an LSTM, NTMs have external memory, which increases their ability to handle
    complex computational tasks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经图灵机**（**NTM**）是一种RNN形式，由DeepMind的Alex Graves于2014年开发。与具有内部状态的LSTM不同，NTM具有外部记忆，这增加了它们处理复杂计算任务的能力。'
- en: 'NTMs consist of two main components: a **controller** and a **memory bank**.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: NTM由两个主要组件组成：**控制器**和**记忆库**。
- en: '**NTM controller**: The controller in an NTM is the neural network itself;
    it manages the flow of information between input and memory, and learns to manage
    its own memory throughout'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NTM控制器**：NTM中的控制器就是神经网络本身；它管理输入和记忆之间的信息流，并学习如何在整个过程中管理自己的记忆。'
- en: '**NTM memory bank**: The actual external memory, usually represented in tensor
    form'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NTM记忆库**：实际的外部记忆，通常以张量形式表示'
- en: Provided there's enough memory, an NTM can replicate any algorithm by simply
    updating its own memory to reflect that algorithm. The architecture ...
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 只要内存足够，NTM就可以通过简单地更新其自身的记忆来反映算法，从而复制任何算法。其架构...
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: RNNs are the primary means by which we reason over textual inputs, and come
    in a variety of forms. In this chapter, we learned about the recurrent structure
    of RNNs, and special versions of RNNs that utilize memory cells. RNNs are used
    for any type of sequence prediction, generating music, text, image captions, and
    more.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是我们推理文本输入的主要手段，并且有多种形式。在本章中，我们学习了RNN的递归结构，以及使用记忆单元的特殊版本。RNN广泛应用于各种类型的序列预测，如生成音乐、文本、图像描述等。
- en: RNNs are different from feedforward networks in that they have recurrence; each
    step in the RNN is dependent on the network's memory at the last state, along
    with its own weights and bias factors. As a result, vanilla RNNs struggle with
    long-term dependencies; they find it difficult to remember sequences beyond a
    specific number of time steps back. GRU and LSTM utilize memory gating mechanisms
    to control what they remember and forget, and hence, overcome the problem of dealing
    with long-term dependencies that many RNNs run into. RNN/CNN hybrids with attention
    mechanisms actually provide state-of-the-art performance on classification tasks.
    We'll create one of these in the [Chapter 9](f1757dce-003c-4295-ac10-aea45860cbe2.xhtml),
    *Deep Learning for Intelligent Assistants* about creating a basic agent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RNN与前馈网络不同，它们具有递归性；RNN的每一步都依赖于网络在上一个状态下的记忆，以及其自身的权重和偏置因素。因此，普通的RNN在处理长期依赖问题时表现不佳；它们很难记住超过特定时间步数的序列。GRU和LSTM利用记忆门控机制来控制它们记住和忘记的内容，从而克服了许多RNN在处理长期依赖时遇到的问题。带有注意力机制的RNN/CNN混合体在分类任务中实际上提供了最先进的性能。我们将在[第9章](f1757dce-003c-4295-ac10-aea45860cbe2.xhtml)中创建一个，*智能助手的深度学习*，讲解如何创建一个基本的智能代理。
- en: In the next chapter, we'll move beyond RNNs into one of the most exciting classes
    of networks, **generative networks**, where we'll learn how to use unlabeled data,
    and how to generate images and paintings from scratch.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将超越RNN，进入最激动人心的网络类别之一——**生成网络**，在这里我们将学习如何使用未标记的数据，以及如何从零开始生成图像和画作。
