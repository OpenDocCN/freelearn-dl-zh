- en: Overview of Reinforcement Learning with R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用R进行强化学习概述
- en: '**Reinforcement learning** (**RL**) is a very exciting area of machine learning,
    used in a variety of applications ranging from autonomous cars to games. Learning
    is a process that manifests itself in the form of lasting adaptive changes in
    behavior induced by individual experience. The ability to learn—to establish causal
    relationships between events, to modify one''s behavior based on those experiences,
    and to memorize those relationships—is made possible by the functional organization
    of our nervous system. Animal studies have shown that the brain has one or more
    neural mechanisms through which stimuli and actions can be associated with each
    other. Based on these considerations, a new paradigm has been formulated that
    takes concepts from cognitive learning and transfers them to machine learning.
    In this paradigm, the concepts of environment, agent, and reward become essential
    in the search for a policy that guides us in making the correct decision.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** (**RL**) 是机器学习中一个非常激动人心的领域，广泛应用于从自动驾驶汽车到游戏的各种应用中。学习是一种过程，表现为由个体经验引发的持久适应性行为变化。学习的能力——即建立事件之间的因果关系、根据这些经验修改行为并记忆这些关系——是通过我们神经系统的功能组织得以实现的。动物研究表明，大脑具有一个或多个神经机制，通过这些机制，刺激和动作可以相互关联。基于这些考虑，提出了一个新的范式，它将认知学习的概念转移到机器学习中。在这个范式中，环境、智能体和奖励的概念成为寻找决策策略的关键，这些策略帮助我们做出正确的选择。'
- en: In RL, algorithms are created that can learn and adapt to environmental changes.
    Interaction with the outside world occurs through external feedback signals (reward
    signals) generated by the environment based on the choices made by the algorithm.
    A correct choice involves a reward while an incorrect choice leads to a penalty.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，算法是通过学习和适应环境变化而创建的。与外界的互动通过环境基于算法选择生成的外部反馈信号（奖励信号）来实现。正确的选择会带来奖励，而错误的选择则会导致惩罚。
- en: In this chapter, you will be introduced to the R environment, and you will learn
    how it can be used to solve problems with the use of RL. We will take a first
    look at some packages available in R to solve Markov decision problems, and we
    will see many examples of applications from the real world.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将接触到R环境，并学习如何利用它通过强化学习解决问题。我们将首次了解R中可用的解决马尔可夫决策问题的包，并看到许多来自现实世界的应用示例。
- en: By the end of the chapter, we will have learned about the concepts underlying RL and
    the different approaches to this technique. We will also have started to learn
    about the packages available in the R environment to deal with this technology,
    and we will have learned about some different practical applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将了解强化学习的基本概念和该技术的不同方法。我们还将开始学习如何在R环境中使用可用的包来处理这项技术，并了解一些不同的实际应用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Understanding RL algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解强化学习算法
- en: Choosing R for RL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择R进行强化学习
- en: Working with the MDPtoolbox package
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MDPtoolbox包
- en: RL applications
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习应用
- en: Introduction to RL
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: 'RL is the main behavioral approach that an individual adopts in the environment
    in which they live. The child who learns the correct behavior in the classroom
    thanks to the verbal rewards received from the teacher, the basketball player
    who learns to make a good shot in the basket by making numerous attempts, the
    strategist who plans their moves by analyzing the possible opposing countermeasures:
    all are examples of adaptive behavior. In these contexts, everything depends on
    the environmental conditions in which the agent is immersed. RL is the term used
    to indicate both the problem arising from the examples just described and the
    set of computational methods suitable for maximizing a certain reward function.
    It is the discipline that studies these problems and the possible methods of resolution. In RL,
    the system does not provide the correct answer to the problem, but returns a criticism
    to a response.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是个体在其所生活的环境中所采用的主要行为方式。通过老师给予的口头奖励，学习正确行为的孩子；通过反复练习学会投篮的篮球运动员；通过分析可能的对方反制措施来规划自己行动的战略家：这些都是适应性行为的例子。在这些情境中，一切都取决于代理所处的环境条件。强化学习是指解决上述问题的过程以及适用于最大化某个奖励函数的一组计算方法。它是研究这些问题及其可能解决方法的学科。在强化学习中，系统并不会提供问题的正确答案，而是对一个响应进行批评。
- en: Earlier, we mentioned that RL represents a new machine learning paradigm, so
    before proceeding, it is useful to distinguish the different approaches available
    in artificial intelligence. Machine learning refers to the ability to learn from
    experience without any outside help, which is what we humans do in most cases.
    Why should it not be the same for machines?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们提到过，强化学习（**RL**）代表了一种新的机器学习范式，因此，在继续之前，区分人工智能中不同的方法是有用的。机器学习指的是能够在没有外部帮助的情况下从经验中学习，这正是我们人类在大多数情况下所做的。为什么机器不能做到这一点呢？
- en: The success of machine learning in solving multiple problems of everyday life
    is because of the quality of its algorithms. These algorithms have been improved
    and updated over time. It is possible to diversify by grouping them into large
    categories that are dependent on the nature of the signal that is used for learning
    or the type of feedback adopted by the system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在解决日常生活中的多个问题中的成功，归功于其算法的质量。这些算法经过时间的不断改进和更新。可以通过根据用于学习的信号性质或系统采用的反馈类型将其分组为大的类别，从而实现多样化。
- en: 'These categories are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别如下：
- en: '**Supervised learning**: This algorithm is based on the observation of a series
    of examples in which each data input has been previously labeled. Through this
    preliminary analysis, it generates a function that links the input values to the
    desired output. It is used to build predictive models.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：该算法基于一系列示例的观察，每个数据输入都已事先标注。通过这种初步分析，它生成一个将输入值与期望输出关联的函数。它用于构建预测模型。'
- en: '**Unsupervised learning**: This algorithm family tries to derive knowledge
    from a nonlabeled generic input. These algorithms are used to build descriptive
    models. A typical example of the application of these algorithms is search engines.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：该算法家族试图从非标记的通用输入中提取知识。这些算法用于构建描述性模型。一个典型的应用例子是搜索引擎。'
- en: '**RL**: This kind of algorithm can learn depending on the changes that occur
    in the environment in which it is performed. In fact, since every action has some
    effect on the environment concerned, the algorithm is driven by the same feedback
    environment. Some of these algorithms are used in speech or text recognition.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL**：这种算法可以根据其执行环境中发生的变化进行学习。事实上，由于每个动作都会对相关环境产生影响，算法便受到相同反馈环境的驱动。这些算法中的一些应用于语音或文本识别。'
- en: The subdivisions that we have just proposed do not prohibit the use of hybrid
    approaches between some or all of these different areas, which have often recorded
    good results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才提出的细分并不禁止在这些不同领域之间使用混合方法，事实上，这些方法常常取得良好的结果。
- en: RL means to make calculations that can learn and adjust to natural changes.
    This programming strategy depends on the idea of accepting outside upgrades relying
    upon the calculation decisions. A correct decision will include a reward, while
    an off-base decision will prompt a penalty. The objective of the framework is
    to accomplish the most ideal outcome, obviously. In this section, we learned about
    the rudiments of RL.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是指进行可以学习并适应自然变化的计算。这种编程策略依赖于根据计算决策接受外部升级的思想。正确的决策会得到奖励，而错误的决策会导致惩罚。系统的目标是实现最优结果，显然。在这一节中，我们了解了RL的基础。
- en: Let's try to better characterize the different paradigms we introduced, starting
    with supervised algorithms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试更好地描述我们所介绍的不同范式，从监督算法开始。
- en: Supervised learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: In supervised learning, we try to build a model, starting from the labeled training
    data, with which we will try to make predictions about data that is not available
    or which is in the future. **Supervision**, therefore, means that in our set of
    samples (the dataset), the desired output signals are already known as previously
    labeled. In this type of learning, based on labels of discrete classes, we will
    therefore have a task based on classification techniques. Another type of technique
    used in supervised learning is **regression**, where output signals are continuous
    values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们尝试构建一个模型，从带标签的训练数据开始，利用该模型预测不可用的数据或未来的数据。因此，**监督**意味着在我们的样本集（数据集）中，期望的输出信号已经被标记为已知。在这种学习方式中，基于离散类的标签，我们将进行基于分类技术的任务。监督学习中使用的另一种技术是**回归**，其中输出信号是连续的数值。
- en: 'In the following diagram, the samples belonging to two sets have been labeled
    with a different symbol, thereby making it easy to identify the line that separates
    the two sets:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，属于两个集合的样本被标记为不同的符号，从而使得容易识别分隔这两个集合的线：
- en: '![](img/44f0845a-7453-4423-a4bd-c73750941c1b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44f0845a-7453-4423-a4bd-c73750941c1b.png)'
- en: Supervised learning algorithms, starting from an adequate number of examples,
    allow us to create a derived function that is able to approximate the searched-for
    function. If the algorithm returns an adequate degree of approximation, providing
    input data to the derived function, we should be able to obtain output responses
    like those provided by the searched-for function. These algorithms are based on
    the notion that similar inputs correspond to similar outputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法，从足够数量的示例开始，使我们能够创建一个派生函数，该函数能够近似目标函数。如果算法返回了足够的近似度，提供输入数据给派生函数，我们应该能够得到类似于目标函数所提供的输出响应。这些算法基于这样一个概念：相似的输入对应相似的输出。
- en: 'In the real world, this is not always true; however, there are some situations
    in which this approximation can be considered acceptable. The success of these
    algorithms depends significantly on the input data. If there are only a few training
    inputs, the algorithm may not have enough experience to provide the correct output.
    Conversely, too many input values ​​can make it too slow because the derivative
    function generated by many inputs could be very complicated. Furthermore, erroneous
    data can make the whole system unreliable and lead to the agent making the wrong
    decisions, which tells us that the supervised algorithms are very sensitive to
    noise. Supervised algorithms are divided into two large families:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，这并不总是成立；然而，在某些情况下，这种近似是可以接受的。这些算法的成功在很大程度上依赖于输入数据。如果只有少量的训练输入，算法可能没有足够的经验来提供正确的输出。相反，输入值过多可能会使算法变得非常慢，因为由许多输入生成的导数函数可能非常复杂。此外，错误的数据会使整个系统不可靠，并导致智能体做出错误的决策，这告诉我们监督算法对噪声非常敏感。监督算法分为两大类：
- en: '**Classification**:If the output value is categorical—for example, belonging
    or not belonging to a class—this is a classification problem.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：如果输出值是类别性的——例如，属于某个类或不属于某个类——这就是一个分类问题。'
- en: '**Regression**: If the output is a continuous real value in a given range,
    then it is a regression problem.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：如果输出是某个范围内的连续实数值，那么这就是一个回归问题。'
- en: In real life, we don't always have labeled data available to learn from. When
    this happens, we need to address the problem through a different approach, as
    we will see in the following section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，我们并不总是有带标签的数据可以用于学习。当这种情况发生时，我们需要通过不同的方式来解决问题，正如我们将在下一节中看到的那样。
- en: Unsupervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'Unlike supervised learning, with unsupervised learning we have data without
    a label, or unstructured data. With these techniques, we can observe the data
    structure and extract meaningful information. In these techniques, however, one
    cannot rely on a known variable relating to the result, or a reward function.
    In the following diagram, we see an example of **clustering**, which is an exploratory
    technique that allows us to aggregate data within groups (called **clusters**)
    that we had no previous knowledge of belonging to the groups:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，无监督学习中我们有没有标签的数据或非结构化数据。通过这些技术，我们可以观察数据结构并提取有意义的信息。然而，在这些技术中，不能依赖于已知与结果相关的变量或奖励函数。在下面的图表中，我们看到了**聚类**的例子，这是一种探索性技术，允许我们将数据聚合到之前不知道属于的组（称为**聚类**）中：
- en: '![](img/92cdbcc9-99cd-4b4e-96b4-016adb27a561.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92cdbcc9-99cd-4b4e-96b4-016adb27a561.png)'
- en: The success of these algorithms depends on the importance of the information
    they can extract from databases. In principle, these algorithms are based on comparing
    data and searching for similarities or differences. The input data must contain
    only the set of functionalities necessary for the description of each example.
    Unsupervised algorithms achieve excellent results if the input data consists of
    numerical elements, but are much less precise with nonnumerical data. Obviously,
    they work correctly in the presence of data that contains a clearly identifiable
    order or grouping. Learning from data is the approach on which supervised and
    unsupervised algorithms are based. In both cases, there is no interaction in real
    time with the environment, and this limits the use of these technologies for the
    solution of numerous problems. In the next section, we will see how to deal with
    these problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的成功取决于它们能够从数据库中提取信息的重要性。原则上，这些算法基于比较数据并搜索相似性或差异。输入数据必须仅包含描述每个示例所需的功能集。如果输入数据由数值元素组成，无监督算法将实现出色的结果，但对非数值数据的精度要低得多。显然，在存在包含明确可识别的顺序或分组的数据时，它们可以正常工作。从数据中学习是监督和无监督算法基于的方法。在这两种情况下，都没有与环境的实时交互，这限制了这些技术用于解决众多问题的能力。在接下来的部分中，我们将看到如何处理这些问题。
- en: RL
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: The third type of machine learning paradigm is RL. The objective of this type
    of learning is to build a system (agent) that improves its performance through
    interactions with the environment. In order to improve the functionality of the
    system, reinforcements are introduced—that is, reward signals. This reinforcement
    is not given by the labels or the correct values of truth, but it is a measurement
    of the quality of the actions taken by the system. For this reason, it cannot
    be used in supervised learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的第三种范式是强化学习。这种学习的目标是通过与环境的互动来提高系统（代理）的性能。为了提高系统功能，引入了强化信号，即奖励信号。这种强化不是由标签或正确值来确定，而是衡量系统所采取的行动质量的度量。因此，它不能用于监督学习。
- en: 'In the following diagram, we can see the agent–environment interaction scheme:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，我们可以看到代理-环境交互方案的示意图：
- en: '![](img/9f96ca6b-ac91-4494-ac63-344bc1fc6064.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f96ca6b-ac91-4494-ac63-344bc1fc6064.png)'
- en: In the real world, we do not always have an explicit indication of what the
    correct output is; we often only have qualitative information (reinforcement signals).
    The data available often does not provide any information on how to update the
    agent's behavior, and so there is no policy that indicates how to update weights.
    It is not possible to define a cost function or a gradient. In these cases, we
    can define a system with the aim of creating intelligent agents able to learn
    from their own experience.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们并不总是有明确的指示来确定正确的输出；我们通常只有定性信息（强化信号）。现有的数据通常不提供任何有关如何更新代理行为的信息，因此没有指示如何更新权重的策略。无法定义成本函数或梯度。在这些情况下，我们可以定义一个系统，旨在创建能够从自身经验中学习的智能代理。
- en: Interaction with the environment is the main concept on which this technology
    is based. Let's try to understand in depth how this all happens.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与环境的互动是这项技术的主要概念。让我们深入理解这一切是如何发生的。
- en: Understanding RL algorithms
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解强化学习算法
- en: As we have seen in the previous sections, RL is a programming technique that
    aims to develop algorithms that can learn and adapt to changes in the environment.
    This programming technique is based on the assumption that an agent is able to
    receive stimuli from the outside and change its actions according to them. So
    a correct choice will result in a reward while an incorrect choice will lead to
    the penalization of the system. The goal of the system is to achieve the highest
    possible reward and, consequently, the best possible result.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面章节中所看到的，强化学习（**RL**）是一种编程技术，旨在开发能够学习和适应环境变化的算法。这种编程技术基于一个假设，即代理能够从外部接收刺激并根据这些刺激改变自己的行为。因此，正确的选择会带来奖励，而错误的选择则会导致系统的惩罚。系统的目标是获得尽可能高的奖励，从而得到最佳的结果。
- en: 'This result can be obtained through two approaches:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果可以通过两种方法来实现：
- en: The first approach involves evaluating the choices of the algorithm and then
    rewarding or punishing the algorithm based on the result. These techniques can
    also adapt to substantial changes in the environment. An example is an image recognition
    program that improves its performance with use. In this case, we can say that
    learning takes place continuously.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法涉及评估算法的选择，然后根据结果对算法进行奖励或惩罚。这些技术还可以适应环境中的重大变化。一个例子是随着使用而提高性能的图像识别程序。在这种情况下，我们可以说学习是持续进行的。
- en: In the second approach, a first phase is applied in which the algorithm is previously
    trained, and when the system is considered reliable, it becomes crystallized and
    no longer modifiable. This derives from the observation that constantly evaluating
    the actions of the algorithm can be a process that cannot be automated or that
    is very expensive.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二种方法中，首先进行一个阶段，在这个阶段中算法进行预先训练，当系统被认为是可靠的时，它变得固定且不可修改。这源于一个观察结果，即不断评估算法的行为可能是一个无法自动化或非常昂贵的过程。
- en: These are only implementation choices, so it may be the case that an algorithm
    includes the newly analyzed approaches.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仅仅是实现选择，因此可能存在某些算法包含了新分析的方法。
- en: So far, we have introduced the basic concepts of RL. Now we can analyze the
    various ways in which these concepts have been transformed into algorithms. In
    this section, we will list them, providing an overview and looking in depth at
    practical cases that we will address in the following chapters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了强化学习的基本概念。现在我们可以分析这些概念如何转化为算法。在本节中，我们将列出这些方法，提供一个概述，并深入探讨我们将在接下来的章节中处理的实际案例。
- en: Dynamic programming
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划
- en: '**Dynamic programming** (**DP**) represents a set of algorithms that can be
    used to calculate an optimal policy given a perfect model of the environment in
    the form of a **Markov decision process** (**MDP**). The fundamental idea of dynamic
    programming, as well as RL in general, is the use of state values and actions
    to look for good policies.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**（**DP**）代表一类算法，用于在环境的完美模型（即**马尔可夫决策过程**（**MDP**））下计算最优策略。动态规划的基本思想，以及强化学习（**RL**）的一般理念，是利用状态值和动作寻找好的策略。'
- en: The Monte Carlo methods
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**蒙特卡洛方法**'
- en: The Monte Carlo methods for estimating the value function and discovering excellent
    policies do not require the presence of a model of the environment. They are able
    to learn through the use of the agent's experience alone or from samples of state
    sequences, actions, and rewards obtained from the interactions between the agent
    and environment. The experience can be acquired by the agent in line with the
    learning process or emulated by a previously populated dataset. The possibility
    of gaining experience during learning (online learning) is interesting because
    it allows the agent to obtain excellent behavior even in the absence of *a priori*
    knowledge of the dynamics of the environment. Even learning through an already-populated
    experience dataset can be interesting, because when combined with online learning,
    it makes automatic policy improvement induced by others' experiences possible.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法用于估计价值函数和发现优秀策略，不需要环境模型的存在。它们能够仅通过智能体的经验或从智能体与环境交互中获得的状态序列、动作和奖励的样本来进行学习。经验可以通过智能体在学习过程中获得，或者通过预先填充的数据集进行模拟。学习过程中获得经验的可能性（在线学习）非常有趣，因为它使得智能体即使在没有*先验*环境动态知识的情况下，也能够获得优秀的行为。即使是通过一个已经填充的经验数据集进行学习也很有趣，因为当它与在线学习结合时，它使得通过他人经验引发的自动策略改进成为可能。
- en: Temporal difference learning
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序差分学习
- en: '**Temporal difference** (**TD**) learning algorithms are based on reducing
    the differences between estimates made by the agent at different times. The TD
    algorithm tries to predict a quantity that depends on the future values of a given
    signal. Its name is derived from the differences used in the predictions of successive
    time steps to guide the learning process. The prediction at any time is updated
    to bring it closer to the prediction of the same quantity in the next time step.
    In RL, these predictions are used to predict a measure of the total amount of
    reward expected in the future.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**时序差分**（**TD**）学习算法基于减少智能体在不同时间点的估计差异。TD算法试图预测一个依赖于给定信号未来值的量。其名称来源于在预测连续时间步时所使用的差异，以引导学习过程。任何时刻的预测都会更新，以使其更接近下一个时间步预测的同一量。在强化学习（RL）中，这些预测用于预测未来期望的总奖励量。'
- en: In the next sections, we will introduce three families of algorithms that deal
    with TD learning, each with a different approach.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍三种处理时序差分学习的算法家族，每种家族采用不同的方法。
- en: SARSA
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA
- en: The SARSA algorithm implements an on-policy time-difference method, in which
    the update of the action–value function is performed based on the outcome of the
    transition from state *s* to state *t* through action *a*, based on a selected
    policy, ![](img/984bfd99-18ed-4698-8e76-f3edd9202840.png)(*s*, *a*).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA算法实现了一种基于策略的时序差分方法，其中动作-价值函数的更新是基于从状态 *s* 到状态 *t* 通过动作 *a* 的转移结果，并基于选择的策略进行更新，
    ![](img/984bfd99-18ed-4698-8e76-f3edd9202840.png)(*s*, *a*)。
- en: Q learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习
- en: Q learning is one of the most-used RL algorithms. This is because of its ability
    to compare the expected utility of the available actions without requiring an
    environment model. Thanks to this technique, it is possible to find an optimal
    action for every given state in a finished MDP.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是最常用的强化学习算法之一。这是因为它能够在不需要环境模型的情况下比较可用动作的预期效用。得益于这一技术，可以在已完成的MDP中找到每个给定状态的最优动作。
- en: Deep Q learning
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: The term **deep Q learning** identifies an RL method of approximation of function.
    It therefore represents an evolution of the basic Q learning method since the
    state–action table is replaced by a neural network, with the aim of approximating
    the optimal value function.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度Q学习**这个术语标识了一种强化学习方法，用于函数近似。因此，它代表了基本Q学习方法的进化，因为状态-动作表被神经网络所取代，目的是近似最优价值函数。'
- en: Compared to the previous approaches, where it was used to structure the network
    in order to request both input and action and provide its expected return, deep
    Q learning revolutionizes the structure in order to request only the state of
    the environment and supply as many status–action values as there are actions that
    can be performed in the environment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法相比，它们用于构建网络，以请求输入和动作并提供预期回报，深度Q学习通过结构上的革命，仅请求环境的状态，并提供与环境中可执行的动作数量相同的状态-动作值。
- en: RL therefore represents an advanced technology that solves different real-life
    problems. Now that the basics of this technology have been introduced in detail,
    it is time to explore the programming platform that we will be using for the rest
    of this book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RL代表了一种先进技术，能够解决不同的现实生活问题。现在，随着这项技术基础知识的详细介绍，是时候探索我们将在本书其余部分中使用的编程平台了。
- en: Choosing R for RL
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择R用于RL
- en: R represents an interpreted scripting language, where the interpreted term tells
    us that an application will be executed without it needing to be compiled beforehand.
    R adopts the object-oriented programming paradigm, through which it will be possible
    to create modern and flexible applications; in the R environment, everything represents
    an object that can be reused for specific needs. R is also an environment that
    was originally developed for statistical calculation and producing quality graphs.
    It consists of a language and a runtime environment with a graphical interface,
    a debugger, and access to some system functions, and offers the ability to execute
    programs stored in script files.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: R表示一种解释型脚本语言，其中“解释型”一词意味着应用程序将在不需要事先编译的情况下执行。R采用面向对象的编程范式，通过这种方式，它将能够创建现代且灵活的应用程序；在R环境中，一切都是可以根据特定需求重用的对象。R还是一个最初为统计计算和生成高质量图形而开发的环境。它由一个语言和一个运行时环境组成，拥有图形界面、调试器、以及访问一些系统功能的能力，并提供执行存储在脚本文件中的程序的能力。
- en: Its predisposition toward statistics does not derive from the nature of the
    language, but from the availability of large collections of statistical functions
    and from the interests of the researchers who invented it and developed it over
    time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它对统计学的偏向并非源自语言的性质，而是源自大量统计函数的可用性以及最初发明并随着时间发展这一语言的研究人员的兴趣。
- en: The core of R is an interpreted programming language that allows the use of
    common structures for the control of information flow, and the use of modular
    programming using functions. Most of the functions visible to the user in the
    R environment are written in R itself. R is also an open source program, and its
    popularity reflects a change in the type of software used within the company.
    In this regard, we should remember that open source software is free from any
    constraint not only in its use, but even more importantly, in its development.
    The strong point of R is its flexibility in analyzing and representing data. The
    language is specialized in this branch, and it has innumerable functions to make
    life easier for the statistician or data scientist on duty.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: R的核心是一个解释型编程语言，允许使用常见的结构来控制信息流，并使用函数进行模块化编程。在R环境中，用户可见的大多数函数是用R本身编写的。R还是一个开源程序，其受欢迎程度反映了公司内部使用的软件类型的变化。在这方面，我们应该记住，开源软件不仅在使用上没有任何限制，更重要的是，在开发过程中也没有限制。R的强项在于其在数据分析和表示方面的灵活性。该语言专注于这一领域，并拥有无数的功能，以便为统计学家或数据科学家提供便利。
- en: 'We can summarize the features of R as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结R的特点如下：
- en: Simplicity in data management and manipulation
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理和操作的简便性
- en: The availability of a suite of tools for calculating vectors, matrices, and
    other complex operations
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一套计算向量、矩阵和其他复杂操作的工具
- en: Access to a vast set of integrated tools for statistical analysis
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问大量集成的统计分析工具
- en: The production of numerous particularly flexible graphic potentials
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产生大量特别灵活的图形潜力
- en: The possibility of using a real object-oriented programming language that allows
    the use of conditional and cyclical structures, as well as user-created functions
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用面向对象的编程语言的可能性，该语言允许使用条件和循环结构，以及用户创建的函数
- en: What makes R so useful and helps to explain its rapid acceptance by the user?
    The reason lies in the fact that statisticians, engineers, and scientists who
    over time have used the software to improve code or write variants for specific
    tasks have developed a large collection of scripts that have been grouped together
    in the form of packages. Packages written in R can add advanced algorithms, colored
    graphics with textures, and data mining techniques to analyze the information
    contained in a database in more detail. Now let's see how to install the R environment
    on our machine so we can replicate the examples in the book.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么使R如此有用，并有助于解释它为何迅速获得用户的接受？原因在于，统计学家、工程师和科学家们随着时间的推移，使用该软件改进代码或为特定任务编写变种，已经开发出了大量的脚本，并将其以包的形式汇集在一起。用R编写的包可以为分析数据库中信息提供更详细的高级算法、带有纹理的彩色图形以及数据挖掘技术。现在，让我们看看如何在我们的计算机上安装R环境，以便能够复制书中的示例。
- en: Installing R
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装R
- en: First, let's see where we can get the software to install R on our machine to
    start programming with it. The packages we need to install are available on the
    official website of the language at [https://www.r-project.org/](https://www.r-project.org/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看可以在哪里获取软件以便在我们的计算机上安装R，开始用它进行编程。我们需要安装的包可以在语言的官方网站上找到，[https://www.r-project.org/](https://www.r-project.org/)。
- en: The **Comprehensive R Archive Network** (CRAN) is a network of servers located
    all over the world (updated in real time) that memorize identical versions of
    the source code and documentation relating to R. CRAN is directly accessible from
    the R site, and on this site, it is also possible to find information on R, some
    technical manuals, the R magazine, and details on the packages that have been
    developed for R and are stored on CRAN repositories.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**综合R档案网络**（CRAN）是一个分布在全球各地的服务器网络（实时更新），用于存储与R相关的源代码和文档的相同版本。CRAN可以通过R网站直接访问，在该网站上，还可以找到关于R的信息、一些技术手册、R杂志，以及开发用于R并存储在CRAN仓库中的包的详细信息。'
- en: 'In the following screenshot, we can see the official website of the R Project:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，我们可以看到R项目的官方网站：
- en: '![](img/460460d9-b5a2-420e-9421-9825b6bf6c31.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/460460d9-b5a2-420e-9421-9825b6bf6c31.png)'
- en: Naturally, before downloading the software versions, we will have to learn about
    the types of machine available to us and the operating system installed on them;
    however, bear in mind that R is available in practice for all operating systems
    in circulation. Programming in the R environment is facilitated by the availability
    of numerous packages. Let's see what they are.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，在下载软件版本之前，我们需要了解可用的计算机类型以及它们上面安装的操作系统；然而，值得注意的是，R实际上可以在所有流通中的操作系统上使用。R环境中的编程得益于众多包的可用性。我们来看一下这些包是什么。
- en: R packages
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R包
- en: The packages use the functionalities of R to decompose a complex algorithm into
    simple units that perform a specific task and facilitate data sharing. In programming,
    we often use repetitive pieces of code, either because the same operation must
    be performed on data deriving from different sources or, more simply, because
    two different programs perform similar procedures. In such cases, it is patently
    unproductive to rewrite the same code unit each time to perform similar operations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包利用R的功能将复杂的算法分解成执行特定任务的简单单元，从而便于数据共享。在编程中，我们经常使用重复的代码片段，可能是因为需要对来自不同来源的数据执行相同的操作，或者更简单地说，两个不同的程序执行相似的过程。在这种情况下，每次为执行类似操作而重新编写相同的代码单元显然是低效的。
- en: In this regard, R, like all high-level programming languages, allows the realization
    of subprograms that represent parts of programs written in separate files, or
    in the same file, that act as independent units with the possibility of being
    recalled by the main program. Starting from the first version of the R scripting
    language, it is possible to exploit the packages that represent a modern and extremely
    effective way of exchanging information between different program units and implementing
    additional functions that enhance the programming environment. The R environment
    is formed of a collection of functions aggregated in the packages—that is, in
    groups of functions that are generally specialized to achieve certain goals and
    objectives. A package is, therefore, a related set of functions, help files, and
    data files that have been grouped into a single file. The packages available in
    R are like the Perl modules that are used with the libraries that are present
    in C or C ++ and the Java classes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，R 语言与所有高级编程语言一样，允许实现子程序，这些子程序可以表示在单独文件中编写的程序部分，或者在同一个文件中作为独立单元，并且可以由主程序调用。从
    R 脚本语言的第一个版本开始，就可以利用包（packages），这是一种现代且极其有效的信息交换方式，用于在不同程序单元之间交换信息，并实现增强编程环境的附加功能。R
    环境由一系列函数组成，这些函数被聚合到包中——也就是说，这些包是一般专门用于实现某些目标和任务的函数组。因此，一个包就是一组相关的函数、帮助文件和数据文件，它们被整合到一个单独的文件中。R
    中的包就像 Perl 模块，它们与 C 或 C++ 中的库以及 Java 类一起使用。
- en: 'The R distribution installed on our machine already has a series of packages
    installed by default. These packages are joined by many others that need to be
    activated when the need arises. Furthermore, there are a very large number of
    highly specialized packages (contributed packages) that provide useful functions
    for performing types of calculations and analysis. These packages must first be
    installed. In general, the installation is carried out automatically: R connects
    via the internet to a repository (a package archive), allows you to choose the
    package you want, downloads the required program, and installs it. The installed
    program must then be activated by loading it in the work area. R offers good tools
    for installing packages within the GUI, but does not provide an equally effective
    way to find a specific package. Fortunately, it is quite easy to find a package
    on the net, thanks to the use of a simple web browser. For example, we will be
    able to search our package on the CRAN website at [https://cran.r-project.org/web/packages/](https://cran.r-project.org/web/packages/).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们机器上安装的 R 发行版默认已经安装了一系列包。这些包与许多其他包一起，在需要时可以激活。此外，还有大量高度专业化的包（贡献包），它们提供执行各种计算和分析类型的有用功能。这些包必须先进行安装。一般而言，安装过程是自动进行的：R
    会通过互联网连接到一个存储库（包存档），允许你选择所需的包，下载所需的程序并进行安装。安装后的程序必须通过在工作区加载它来激活。R 提供了良好的工具，可以在
    GUI 内安装包，但没有提供同样有效的方式来查找特定的包。幸运的是，借助简单的网页浏览器，在网上找到包非常容易。例如，我们可以在 CRAN 网站上搜索我们的包，网址为 [https://cran.r-project.org/web/packages/](https://cran.r-project.org/web/packages/)。
- en: 'Currently, the CRAN package repository features 14,345 available packages.
    The following screenshot shows the web page of the CRAN repository:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，CRAN 包存储库中提供 14,345 个可用包。以下截图显示了 CRAN 存储库的网页：
- en: '![](img/9b20d7cd-321d-4f3c-a54f-c429e0bcf8fe.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b20d7cd-321d-4f3c-a54f-c429e0bcf8fe.png)'
- en: At this point, it will be enough to identify the packages needed to implement
    an algorithm based on RL—let's start with the `MDPtoolbox` package.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，确定实现基于 RL 的算法所需的包就足够了——让我们从 `MDPtoolbox` 包开始。
- en: Working with the MDPtoolbox package
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MDPtoolbox 包
- en: In RL, it is usually assumed that the environment can be described by an MDP.
    This topic will be discussed further in [Chapter 3](7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml),
    *Markov Decision Processes in Action*. For now, we will discuss the `MDPtoolbox`
    package, which is a specific package in R that was created to address MDP-based
    problems. This package proposes functions related to the resolution of discrete-time
    Markov decision processes—such as finite horizon, value iteration, policy iteration,
    and linear programming algorithms (with some variants)—and also proposes some
    functions related to RL.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，通常假设环境可以通过马尔可夫决策过程（MDP）来描述。这个话题将在[第3章](7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml)中进一步讨论，*马尔可夫决策过程的应用*。现在，我们将讨论
    `MDPtoolbox` 包，它是一个专门为解决基于马尔可夫决策过程的问题而创建的R语言包。该包提供了与解决离散时间马尔可夫决策过程相关的函数——如有限时域、值迭代、策略迭代和线性规划算法（以及一些变体）——并且还提供了与强化学习（RL）相关的一些函数。
- en: 'The following table gives some information about this package:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了该包的一些信息：
- en: '| Package | `MDPtoolbox` |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 包名 | `MDPtoolbox` |'
- en: '| Date | 2017-03-02 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2017-03-02 |'
- en: '| Version | 4.0.3 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 4.0.3 |'
- en: '| Title | Markov Decision Processes Toolbox |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 马尔可夫决策过程工具包 |'
- en: '| Authors | Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia,
    Regis Sabbadin |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia,
    Regis Sabbadin |'
- en: 'The following list shows the most useful functions contained in this package,
    with a short description from the official documentation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了该包中最有用的函数，并附上了来自官方文档的简短描述：
- en: '`mdp_Q_learning`: Solves discounted MDP using the Q learning algorithm (RL)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_Q_learning`: 使用Q学习算法（RL）解决折扣马尔可夫决策过程（MDP）'
- en: '`mdp_computePR`: Computes a reward matrix for any form of transition and reward
    function'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_computePR`: 计算任意形式的转移和奖励函数的奖励矩阵'
- en: '`mdp_eval_policy_iterative`: Evaluates a policy using an iterative method'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_eval_policy_iterative`: 使用迭代方法评估策略'
- en: '`mdp_LP`: Solves discounted MDP using a linear-programming algorithm'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_LP`: 使用线性规划算法解决折扣马尔可夫决策过程（MDP）'
- en: '`mdp_policy_iteration`: Solves discounted MDP using a policy-iteration algorithm'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_policy_iteration`: 使用策略迭代算法解决折扣马尔可夫决策过程（MDP）'
- en: '`mdp_relative_value_iteration`: Solves MDP with an average reward using a relative-value-iteration
    algorithm'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_relative_value_iteration`: 使用相对值迭代算法解决具有平均奖励的马尔可夫决策过程（MDP）'
- en: '`mdp_value_iteration`: Solves discounted MDP using a value-iteration algorithm'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mdp_value_iteration`: 使用值迭代算法解决折扣马尔可夫决策过程（MDP）'
- en: 'The `MDPtoolbox` solves a Markov decision process by finding the optimal policy
    after setting an optimization criterion. Based on this criterion, policies are
    identified that will provide the highest number of accumulated rewards. The package
    uses four of the most commonly used optimization criteria. RL is particularly
    useful in different applications in real life: let''s see some of these.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`MDPtoolbox` 通过设定优化准则来解决马尔可夫决策过程，找出最优策略。在这个准则的基础上，识别出能提供最高累计奖励的策略。该包使用了四种最常用的优化准则。强化学习（RL）在现实生活中的各种应用中尤其有用：我们来看看其中一些应用。'
- en: RL applications
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习（RL）应用
- en: 'From mobile phones to driverless cars, the consumer society has begun to consider
    the power of RL. In recent years, in fact, RL has emerged as a fundamental technology
    in various fields: from voice, textual, and facial recognition to multilingual
    translation, from traffic control systems to internet traffic control systems.
    But the latest examples of the application of this technology in the real world
    involve medical diagnostics, internet security, and the construction of forecasting
    models designed to make important business decisions.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从手机到无人驾驶汽车，消费社会已经开始关注强化学习（RL）的力量。事实上，近年来，强化学习作为一项基础技术在多个领域崭露头角：从语音、文本和面部识别到多语言翻译，从交通控制系统到互联网流量控制系统。然而，近年来，这项技术在现实世界中的最新应用例子包括医学诊断、互联网安全以及构建预测模型来做出重要商业决策。
- en: 'In fact, RL teaches robots and machines to do what humans do naturally: interact
    with the environment and learn from experience. New, low-cost hardware has shaped
    the use of deep and multilayer neural networks that simulate the neural networks
    of the human brain. Production technologies have thus acquired new, extraordinary
    abilities to recognize images and trends, to make predictions, and to make intelligent
    decisions. Starting from a basic logic developed during the initial training,
    the algorithms based on RL can continually refine the performance through the
    feedback provided by the agent that monitors the state of the environment. In
    the following sections, we will examine some examples.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，强化学习（RL）教会机器人和机器像人类一样自然地做事：与环境互动并从经验中学习。新的低成本硬件推动了深度和多层神经网络的使用，这些网络模拟人脑的神经网络。因此，生产技术获得了识别图像和趋势、做出预测以及做出智能决策的全新能力。从最初训练期间发展出的基本逻辑开始，基于强化学习的算法可以通过监控环境状态的代理提供的反馈，持续优化性能。在接下来的部分，我们将讨论一些示例。
- en: Software fault prediction
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件故障预测
- en: Modern software systems are constantly growing in complexity, and this increase
    in complexity leads to an increase in software failures that play a significant
    role in system failures. The aim we have is to achieve the highest possible probability
    that the system will not fail for a certain period, known as the mission time.
    Treating software failures in general is a complex operation. One of the main
    problems is the reproducibility of a failure, which is the ability to identify
    the fault-activation pattern. Testing activities have proven to be inadequate
    at dealing with this type of failure. Since it is practically impossible to identify
    all possible failures, critical systems adopt fault tolerance mechanisms.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现代软件系统的复杂性不断增长，而这种复杂性的增加也导致了软件故障的增加，后者在系统故障中发挥着重要作用。我们的目标是尽可能地提高系统在某段时间内不发生故障的概率，这段时间被称为任务时间。处理软件故障通常是一项复杂的工作，其中一个主要问题是故障的可重现性，即识别故障激活模式的能力。测试活动在应对这种类型的故障时被证明是不足的。由于几乎不可能识别所有可能的故障，关键系统通常采用容错机制。
- en: A fault tolerance mechanism consists of a set of procedures that allow the system
    to continue operating even in the presence of faults. One of the main techniques
    for validating fault tolerance mechanisms is software fault injection, or the
    introduction of software failures (bugs) within a system component to analyze
    the impact on the other components and in the overall system. This technique is
    of great importance as software failures are a significant cause of system failure.
    Modern approaches to the problem involve the use of methodologies based on RL
    for failure injection into complex software systems. This system architecture
    allows us to perform an analysis of the algorithms in a simple and effective way
    that is easily integrated into a complex system, and that is easily extensible.
    Moreover, it allows us to carry out an exploratory analysis of the algorithms
    in order to evaluate the applicability of the approach in significant cases with
    a view to future integration.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 容错机制由一系列程序组成，使得系统即使在出现故障的情况下也能继续运行。验证容错机制的主要技术之一是软件故障注入，即在系统组件中引入软件故障（错误），以分析它对其他组件以及整个系统的影响。这一技术非常重要，因为软件故障是系统故障的一个重要原因。现代的解决方案采用基于强化学习（RL）的方法，在复杂的软件系统中进行故障注入。这种系统架构使我们能够以简单而有效的方式分析算法，易于集成到复杂系统中，并且具有良好的可扩展性。此外，它还允许我们对算法进行探索性分析，以评估该方法在重大案例中的适用性，并为未来的集成做好准备。
- en: Adaptive traffic flow control
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应交通流量控制
- en: Road traffic should ideally be studied in the same way as the methods used to
    study fluid dynamics. In the same way, vehicular traffic has a natural inertia
    that leads levels to settle over time, in the same way that fluid creates a situation
    of unstable imbalance where it accumulates because of its nature. However, the
    onset of any bottleneck hinders the flow, and a traffic jam is always around the
    corner. In the adaptive adjustment of a signalized intersection, the objective
    is to adjust an isolated intersection to optimize its capacity and minimize vehicle
    delays. There are numerous approaches to adaptive control that generally consider
    the available detection capabilities.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 道路交通理想情况下应像流体动力学研究方法那样进行研究。同样，交通流有自然的惯性，会随着时间的推移稳定下来，就像流体因其固有特性而在积聚时形成不稳定的失衡情况一样。然而，任何瓶颈的出现都会阻碍流动，交通拥堵总是迫在眉睫。在信号控制交叉口的自适应调整中，目标是调整孤立的交叉口，以优化其容量并最小化车辆延误。自适应控制方法有许多种，通常会考虑可用的检测能力。
- en: The most widespread are based on flow rate and density measurements at the different
    entrances to the intersection, at distances varying from the intersection. Other
    more recent approaches make use of measures derived from cameras, such as tail
    lengths and turning maneuvers. The data that can be useful is therefore basically
    the basic variables flow rate and density (or employment rate), the queue lengths,
    and the turning maneuvers. In these systems, technologies based on machine learning
    have been repeatedly used to solve the problem. RL is particularly suitable given
    its ability to interact with the environment through measurements detected in
    real time by sensors placed near signalized intersections.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛的自治方法是基于不同交叉口入口处的流量和密度测量，这些入口距离交叉口的距离不同。其他较新的方法则利用摄像头提供的数据，如尾巴长度和转弯动作。因此，有用的数据基本上是基本变量——流量和密度（或使用率）、排队长度以及转弯动作。在这些系统中，基于机器学习的技术已多次被用于解决这一问题。强化学习（RL）尤其适合，因为它能够通过传感器实时检测到的测量与环境进行交互，这些传感器通常放置在信号控制交叉口附近。
- en: Display advertising
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示广告
- en: Display advertising uses the commercial spaces on a page of content to promote
    a product or service. This advertising technique differs from a pay per click
    model because it also uses a graphic element. The company acquires the space of
    one or more pages belonging to a **circuit** of sites, and in the spaces acquired,
    it can show the user its own advertisement. Furthermore, the search engine does
    not show ads randomly in the spaces purchased by the advertiser, but only shows
    ads relevant to the searches made by the user and in line with the history of
    the pages that they have viewed. To place an ad automatically and optimally, it
    is essential that advertisers develop a learning algorithm to make an intelligent
    offer of a real-time ad impression.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 展示广告使用网页上的商业空间来推广产品或服务。这种广告方式不同于按点击付费模式，因为它还利用了图形元素。公司购买一个或多个属于**广告网络**的网页空间，并在这些已购买的空间中展示自己的广告。此外，搜索引擎不会随机地在广告商购买的空间中展示广告，而是仅展示与用户搜索相关的广告，并与用户浏览的页面历史记录相符。为了自动且最佳地投放广告，广告商必须开发一种学习算法，以便在实时展示广告时提供智能报价。
- en: Most of the algorithms used so far address the problem with an approach based
    on the static optimization of the treatment of the value of each impression. Optimization
    occurs either independently or by setting an offer price for each ad volume segment;
    however, offers for an advertising campaign are repeated several times during
    its duration before the budget expires. Bid decision-making can be treated as
    an RL problem in which the status space is represented by auction information
    and real-time campaign parameters, while an action is the bid price to be set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，使用的大多数算法采用了基于静态优化的方式来处理每次展示的价值。优化过程要么独立进行，要么为每个广告量段设定报价；然而，在广告活动的过程中，广告报价会在预算到期之前多次重复。出价决策可以视为一个强化学习问题，其中状态空间由拍卖信息和实时的活动参数表示，而动作则是需要设置的出价。
- en: Robot autonomy
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器人自治
- en: Self-governance implies the capacity to work in unique and unstructured conditions
    without the requirement for consistent human involvement. In such conditions,
    a considerable number of circumstances are not known from previous experience,
    obliging the robot (or any self-governing framework) to have the option to identify
    the notable qualities of the present circumstance and to carry on as needed, choosing
    what moves to make.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自我治理意味着在独特和非结构化的条件下工作，且不需要持续的人类参与。在这种情况下，许多情况是无法通过以往经验得知的，这要求机器人（或任何自我治理系统）能够识别当前环境的显著特征，并根据需要进行行动，决定采取哪些措施。
- en: The need to stay away from human mediation over extensive stretches of time
    suggests that the full capacity of the robot to self-oversee and get by (for instance,
    by evading obstacles to remain physically free or avoid totally draining its energy
    supply) must be a priority. For the most part, these machines perform the tasks
    given within work cells that facilitate such operations, where access to any foreign
    element, including humans, is forbidden. Robots of this type can be created by
    estimating numerous points of view related to the workplace, and most of the time,
    large programmed control calculations are used to give the robot the ability to
    work by exploiting the data collected from the environment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 避免在长时间内进行人类干预的需求意味着，机器人自我管理和生存的完全能力（例如，通过避开障碍保持身体自由或避免完全耗尽能量供应）必须是首要任务。通常，这些机器在工作单元内执行任务，工作单元能促进这些操作，并且禁止接触任何外部元素，包括人类。这类机器人可以通过评估与工作场所相关的多个角度来创建，且大多数情况下，会使用大型的编程控制算法来赋予机器人通过利用从环境中收集的数据来工作的能力。
- en: Computer vision recognition
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉识别
- en: Computer vision is a set of processes that aim to create an approximate model
    of the real world (3D) starting from two-dimensional (2D) images. The main purpose
    of machine vision is to reproduce human vision. Seeing is understood not only
    as the acquisition of a two-dimensional photograph of an area, but above all as
    the interpretation of the content of that area. Information, in this case, is
    understood as something that implies an automatic decision.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是一组旨在通过二维图像创建现实世界（3D）近似模型的过程。机器视觉的主要目的是再现人类视觉。视觉不仅仅是理解为获取某一地区的二维照片，更重要的是对该地区内容的解释。在这种情况下，信息被理解为意味着自动决策的事物。
- en: Machine learning has become a standard in solving various activities related
    to computer vision, such as feature detection, image segmentation, object recognition,
    and tracking. In modern control systems, robots are equipped with visual sensors
    that they can use to learn the status of a surrounding environment by solving
    corresponding computer vision tasks. These systems are used to make decisions
    about possible future actions. RL is used both to solve computer vision problems—such
    as object detection, visual detection, and the recognition of actions—and robot
    navigation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习已经成为解决与计算机视觉相关的各种任务的标准，例如特征检测、图像分割、物体识别和跟踪。在现代控制系统中，机器人配备了视觉传感器，可以利用这些传感器通过解决相应的计算机视觉任务来学习周围环境的状态。这些系统用于决策关于可能的未来行动。强化学习（RL）用于解决计算机视觉问题——如物体检测、视觉检测和动作识别——以及机器人导航。
- en: Games
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏
- en: Games are a privileged research field for **artificial intelligence** (**AI**),
    as they provide convenient models of real problems. In fact, games, while presenting
    problems of complexity comparable to problems in the real world, have well-defined
    and formalizable rules. Furthermore, for each game, there are experts who are
    able to judge the quality of the results elaborated by a machine. It is therefore
    convenient to work and experiment first in a well-defined environment, such as
    the game world, to then generalize and adapt the results obtained to a more variable
    environment, such as the real world.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏是**人工智能**（**AI**）的一个重要研究领域，因为它们提供了现实问题的便捷模型。事实上，游戏虽然呈现出与现实世界问题相当的复杂性，但却有明确且可形式化的规则。此外，每个游戏都有能够评估机器所产生结果质量的专家。因此，首先在一个明确的环境中进行工作和实验（如游戏世界），然后将获得的结果推广和适应到更具变化性的环境中（如现实世界），是非常方便的。
- en: Among the various types of existing games, those that have had the most success
    at the scientific research level are perfect information games, which are deterministic
    with two players. This category includes chess, checkers, and Go. Researchers'
    attention has therefore mainly focused on the game of chess. In past years, thanks
    to the commitment and dedication of several scientists, it has been possible to
    create an AI player capable of playing at world-champion level.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的各种游戏类型中，最成功的科学研究对象是完美信息游戏，即具有两个玩家的确定性游戏。这个类别包括国际象棋、跳棋和围棋。因此，研究人员的注意力主要集中在国际象棋游戏上。在过去的几年中，得益于多位科学家的努力和奉献，已经创造出了一个能够达到世界冠军水平的人工智能棋手。
- en: The most important success in this project was achieved when Deep Blue defeated
    the chess world champion Garry Kasparov. Another example concerns AI that learned
    how to play Go, a game of Chinese origin that is similar to chess. In early 2016,
    a historic game of Go was played between South Korean Lee Sedol and Google's artificial
    intelligence AlphaGo.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目中最重要的成功是在深蓝击败国际象棋世界冠军加里·卡斯帕罗夫时取得的。另一个例子是人工智能学习如何下围棋——这是一种类似于国际象棋的中国起源的游戏。2016年初，韩国棋手李世石与谷歌的人工智能AlphaGo之间进行了一场历史性的围棋对局。
- en: Forecasting in financial markets
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融市场预测
- en: In recent times, the analysis of financial markets has undergone an important
    development, both in terms of basic research and in terms of direct applications
    on the market. In particular, the computerization of data has made detailed information
    on price trends and volumes traded easily available, thereby creating new fields
    of investigation. At the same time, the introduction of electronic trading systems
    has meant that large financial institutions are interested in automating trading
    processes through algorithms.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，金融市场的分析经历了重要的发展，无论是在基础研究方面还是在市场直接应用方面。尤其是数据的计算机化使得关于价格趋势和交易量的详细信息变得轻松可得，从而创造了新的研究领域。同时，电子交易系统的引入使得大型金融机构有兴趣通过算法自动化交易过程。
- en: These models, however, cannot be applied blindly. Although there are general
    rules, it is the researcher's task to determine the characteristic parameters
    that best fit the description of the problem presented. In this context, RL fits
    in naturally, which, given its ability to interact with the environment, can verify
    in real time the market reaction to the forecasts that have been made. Based on
    the feedback received, it can correct a forecast by diverting the data in the
    right direction.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型不能盲目应用。虽然有一些通用的规则，但研究者的任务是确定最适合描述所提出问题的特征参数。在这种背景下，强化学习（RL）自然地适应了这个环境，因为它具有与环境互动的能力，可以实时验证市场对已做出的预测的反应。根据收到的反馈，它可以通过将数据引导到正确的方向来修正预测。
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have explored the fantastic world of machine learning and
    have analyzed the three available paradigms according to the nature of the signal
    used for learning and the type of feedback adopted by the system. We took a tour
    of the most popular RL algorithms to choose the right one for our needs and to
    understand what is best suited to our needs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了机器学习的奇妙世界，并根据用于学习的信号性质以及系统采用的反馈类型，分析了三种可用的范式。我们游览了最流行的强化学习（RL）算法，以选择最适合我们需求的算法，并理解什么最适合我们的需求。
- en: 'Then, we introduced the R scripting language and the features that make it
    particularly suitable for dealing with RL problems. We then explored the `MDPtoolbox`
    package—this package proposes functions related to the resolution of discrete-time
    Markov decision processes: finite horizon, value iteration, policy iteration,
    and linear programming algorithms. Finally, we analyzed a series of RL applications,
    the most modern ones that are spreading in the real world, showing surprising
    results.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们介绍了R脚本语言及其使其特别适合处理强化学习问题的特点。然后，我们探索了`MDPtoolbox`包——该包提供了与离散时间马尔可夫决策过程求解相关的函数：有限时域、值迭代、策略迭代和线性规划算法。最后，我们分析了一系列强化学习应用，特别是那些正在现实世界中迅速传播并取得惊人结果的现代应用。
- en: In the next chapter, we will learn about the agent–environment interface. We
    will learn how to work with Markov's decision-making process. We will also learn
    about the gradient methods of norms and we will look at the most widely used RL
    package, R.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习代理–环境接口。我们将学习如何使用马尔可夫决策过程。我们还将了解规范的梯度方法，并且将研究最广泛使用的强化学习包——R。
