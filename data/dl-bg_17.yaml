- en: Generative Adversarial Networks
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Reading about making sushi is easy; actually cooking a new kind of sushi is
    harder than we might think. In deep learning, the creative process is harder,
    but not impossible. We have seen how to build models that can classify numbers,
    using dense, convolutional, or recurrent networks, and today we will see how to
    build a model that can create numbers. This chapter introduces a learning approach
    known as generative adversarial networks, which belong to the family of adversarial
    learning and generative models. The chapter explains the concepts of generators
    and discriminators and why having good approximations of the distribution of the
    training data can lead to the success of the model in other areas such as *data
    augmentation*. By the end of the chapter, you will know why adversarial training
    is important; you will be able to code the necessary mechanisms for training a
    generator and a discriminator on questionable data; and you will code a **Generative
    Adversarial Network** (**GAN**) to generate images from a learned latent space.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读关于制作寿司的资料很容易；然而，实际上制作一种新的寿司比我们想象的要困难。在深度学习中，创造性过程更为复杂，但并非不可能。我们已经看到如何构建可以分类数字的模型，使用密集网络、卷积网络或递归网络，今天我们将看到如何构建一个可以生成数字的模型。本章介绍了一种被称为生成对抗网络的学习方法，它属于对抗学习和生成模型的范畴。本章解释了生成器和判别器的概念，以及为什么拥有良好的训练数据分布近似可以使模型在其他领域取得成功，比如*数据增强*。在本章结束时，你将知道为什么对抗训练如此重要；你将能够编写训练生成器和判别器所需的机制；并且你将编写一个**生成对抗网络**（**GAN**）来从学习的潜在空间生成图像。
- en: 'This chapter is organized as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构如下：
- en: Introducing adversarial learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入对抗学习
- en: Training a GAN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练GAN
- en: Comparing GANs and VAEs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较GAN和VAE
- en: Thinking about the ethical implications of generative models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思考生成模型的伦理影响
- en: Introducing adversarial learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入对抗学习
- en: 'Recently, there has been interest in adversarial training using adversarial
    neural networks (Abadi, M., et al. (2016)). This is due to adversarial neural
    networks that can be trained to protect the model itself from AI-based adversaries.
    We could categorize adversarial learning into two major branches:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，使用对抗神经网络进行对抗训练引起了广泛关注（Abadi, M.等，2016）。这是因为对抗神经网络可以训练以保护模型免受基于AI的对抗者攻击。我们可以将对抗学习分为两个主要分支：
- en: '**Black box**: In this category, a machine learning model exists as a black
    box, and the adversary can only learn to attack the black box to make it fail.
    The adversary arbitrarily (within some bounds) creates fake input to make the
    black box model fail, but it has no access to the model it is attacking (Ilyas,
    A., et al. (2018)).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**黑盒**：在这一类别中，机器学习模型作为一个黑盒存在，对抗者只能学习攻击黑盒，使其失败。对抗者任意地（在某些界限内）创建伪输入使黑盒模型失败，但无法访问其攻击的模型（Ilyas,
    A.等，2018）。'
- en: '**Insider**: This type of adversarial learning is meant to be part of the training
    process of the model it aims to attack. The adversary has an influence on the
    outcome of a model that is trained *not* to be fooled by such an adversary (Goodfellow,
    I., et al. (2014)).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部人员**：这种类型的对抗学习旨在成为训练模型的一部分，模型的目标是抵抗这种攻击。对抗者会影响一个被训练为*不*易被此类对抗者欺骗的模型的结果（Goodfellow,
    I.等，2014）。'
- en: 'There are pros and cons to each of these:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法各有优缺点：
- en: '| **Black box pros** | **Black box cons** | **Insider pros** | **Insider cons**
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **黑盒优点** | **黑盒缺点** | **内部人员优点** | **内部人员缺点** |'
- en: '| It gives the ability to explore more generative approaches. | Does not have
    a way to influence or change the black box model. | The model that is trained
    adversarially can be more robust to specific black box attacks. | The options
    for generating attacks are currently limited. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 它提供了探索更多生成方法的能力。 | 没有办法影响或改变黑盒模型。 | 经过对抗训练的模型可能对特定的黑盒攻击更加鲁棒。 | 目前生成攻击的选项有限。
    |'
- en: '| It is usually fast and likely to find a way to break a model. | The generator
    usually focuses only on perturbing existing data. | The generator can be used
    to *augment* datasets. | It is usually slower. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 它通常较快，且可能找到破坏模型的方法。 | 生成器通常只关注扰动现有数据。 | 生成器可以用于*增强*数据集。 | 它通常较慢。 |'
- en: '|  | The generator may not be usable in *augmenting* datasets. |  |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | 生成器可能无法用于*增强*数据集。 |  |  |'
- en: 'Since this book is for beginners, we will focus on one of the simplest models:
    an insider model known as a GAN. We will look at its parts and discuss the batch
    training of it.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本书是给初学者的，我们将专注于一个最简单的模型：一个被称为GAN的内部模型。我们将查看其各个部分并讨论其批量训练。
- en: GANs have historically been used to generate realistic images (Goodfellow, I., et
    al. (2014)), generally solving multi-agent problems (Sukhbaatar, S., *et al.*
    (2016)), and even cryptography (Rivas, P., et al. (2020)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GANs在历史上被用来生成逼真的图像（Goodfellow, I., 等人 (2014)），通常解决多智能体问题（Sukhbaatar, S., *等人*
    (2016)），甚至是密码学（Rivas, P., *等人* (2020)）。
- en: Let's briefly discuss adversarial learning and GANs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论对抗学习和GANs。
- en: Learning using an adversary
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用对手进行学习
- en: 'A machine learning model can learn traditionally to do classification or regression
    and other tasks, among which there may be a model trying to learn to distinguish
    whether the input is legitimate or fake. In this scenario, an machine learning
    model can be created to be an adversary that produces fake inputs, as shown in
    *Figure 14.1*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习模型可以传统地学习进行分类或回归等任务，其中可能有一个模型试图学习区分输入是否合法或伪造的情况。在这种情况下，一个机器学习模型可以被创建为一个生成伪造输入的对手，如*图14.1*所示：
- en: '![](img/91180858-c5ad-465e-a34a-2a50c08f263e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91180858-c5ad-465e-a34a-2a50c08f263e.png)'
- en: Figure 14.1 - Adversarial learning
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 - 对抗学习
- en: In this paradigm, a machine learning model needs to learn to distinguish between
    true inputs and fake ones. When it makes a mistake, it needs to *learn* to adjust
    itself to make sure it properly recognizes true input. On the other hand, the
    adversary will need to keep producing fake inputs with the aim of making the model
    fail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种范例中，机器学习模型需要学会区分真实输入和假输入。当它犯错时，它需要*学会*调整自己以确保正确识别真实输入。另一方面，对手需要继续生成假输入，目的是使模型失败。
- en: 'Here is what success looks like for each model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的成功看起来是这样的：
- en: The **machine learning main model** is successful if it can successfully distinguish
    fake from real input.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习主模型**成功的条件是能够成功区分假的和真实的输入。'
- en: The **Adversary model** is successful if it can fool the machine learning main
    model into passing fake data as real.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对手模型**成功的条件是能够愚弄机器学习主模型，使其将假数据通过为真实数据。'
- en: As you can see, they are competing against each other. One's success is the
    failure of the other, and vice versa.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，它们彼此竞争。一个的成功是另一个的失败，反之亦然。
- en: During the learning process, the machine learning main model will continuously
    call for batches of real and fake data to learn, adjust, and repeat until we are
    satisfied with the performance, or some other stopping criteria have been met.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，机器学习主模型将持续调用批量的真实和假数据来学习、调整和重复，直到我们对性能满意，或者达到其他停止标准。
- en: In general in adversarial learning, there is no specific requirement on the
    adversary, other than to produce fake data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在对抗学习中，对手没有具体要求，除了产生假数据。
- en: '**Adversarial robustness** is a new term that is used to certify that certain
    models are robust against adversarial attacks. These certificates are usually
    designated for particular types of adversaries. See Cohen, J. M., *et al.* (2019)
    for further details.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**对抗鲁棒性**是一个新术语，用于证明某些模型对抗对手攻击的能力。这些证书通常是针对特定类型的对手。详细内容请参见Cohen, J. M., *等人*
    (2019)。'
- en: A popular type of adversarial learning takes place within a GAN, which we will
    discuss next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的对抗学习类型发生在GAN内部，接下来我们将讨论它。
- en: GANs
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs
- en: 'A GAN is one of the simplest neural-based models that implements adversarial
    learning, and was initially conceived in a bar in Montreal by Ian Goodfellow and
    collaborators (Goodfellow, I., et al. (2014)). It is based on a min-max optimization
    problem that can be posed as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是实施对抗学习的最简单的基于神经网络的模型之一，最初由Ian Goodfellow和合作者在蒙特利尔的一家酒吧里构思出来（Goodfellow,
    I., 等人 (2014)）。它基于一个极小极大化的优化问题，可以表述如下：
- en: '![](img/0b573800-6bdf-4433-8909-35c9a245825c.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b573800-6bdf-4433-8909-35c9a245825c.png)'
- en: 'There are several parts to this equation that require an explanation, so here
    we go:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式有几个部分需要解释，所以我们开始吧：
- en: '![](img/530da91e-fd4b-408a-9c71-b41264bb9b22.png): In a GAN, this is the discriminator,
    which is a neural network that takes input data ![](img/cc202992-bba0-4e9f-8a96-2438a06b6f27.png) and
    determines whether it is fake or real, as shown in *Figure 14.2*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/530da91e-fd4b-408a-9c71-b41264bb9b22.png)：在 GAN 中，这是鉴别器，它是一个神经网络，接收输入数据
    ![](img/cc202992-bba0-4e9f-8a96-2438a06b6f27.png)，并判断它是真还是假，如 *图 14.2* 所示。'
- en: '![](img/b29516df-1c42-4df7-b399-1cb96d67038b.png): In a GAN, this is the generator,
    which is also a neural network, but its input is random noise, ![](img/7e4747cc-6204-46b0-a126-eeaab1448a2b.png),
    with the probability ![](img/3db1149e-bb25-4f0c-8c17-d75a8d6624f7.png):'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/b29516df-1c42-4df7-b399-1cb96d67038b.png)：在 GAN 中，这是生成器，它也是一个神经网络，但其输入是随机噪声
    ![](img/7e4747cc-6204-46b0-a126-eeaab1448a2b.png)，概率为 ![](img/3db1149e-bb25-4f0c-8c17-d75a8d6624f7.png)：'
- en: '![](img/09cb1a8f-dc00-4c81-86ca-0e958b50a8ec.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09cb1a8f-dc00-4c81-86ca-0e958b50a8ec.png)'
- en: Figure 14.2 - GAN main paradigm
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 - GAN 主要范式
- en: Ideally, we want to maximize the correct predictions of the discriminator ![](img/612c1e2c-f7da-4500-9b2c-56397fee42b0.png),
    while, at the same time, we want to minimize the error of the generator, ![](img/90d6dec2-992d-4841-9a0b-27f5b6b5610a.png), producing
    a sample that does not fool the discriminator, which is expressed as ![](img/12e9c0b3-a3e2-45a1-aac7-1bdff68ccaf0.png).
    The formulation of expectations and logarithms comes from the standard cross-entropy
    loss function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望最大化鉴别器的正确预测 ![](img/612c1e2c-f7da-4500-9b2c-56397fee42b0.png)，同时，我们还希望最小化生成器的误差
    ![](img/90d6dec2-992d-4841-9a0b-27f5b6b5610a.png)，生成一个不会欺骗鉴别器的样本，表示为 ![](img/12e9c0b3-a3e2-45a1-aac7-1bdff68ccaf0.png)。期望值和对数的公式来自标准的交叉熵损失函数。
- en: To recap, in a GAN, the generator and the discriminator are neural networks.
    The generator draws random noise from a random distribution, and uses that noise
    to generate *fake* input to fool the discriminator.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在 GAN 中，生成器和鉴别器都是神经网络。生成器从随机分布中提取随机噪声，并利用这些噪声生成 *假* 输入来欺骗鉴别器。
- en: With this in mind, let's proceed and code a simple GAN.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，我们继续进行简单 GAN 的编码。
- en: Training a GAN
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 GAN
- en: We will begin our implementation with a simple MLP-based model, that is, our
    generator and discriminator will be dense, fully connected, networks. Then, we
    will move on to implementing a convolutional GAN.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的基于 MLP 的模型开始实现，也就是说，我们的生成器和鉴别器将是密集的、完全连接的网络。然后，我们将继续实现卷积式 GAN。
- en: An MLP model
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个 MLP 模型
- en: 'We will now focus in creating the model shown in *Figure 14.3*. The model has
    a generator and discriminator that are distinct in terms of their numbers of layers
    and total parameters. It is usually the case that the generator takes more resources
    to build than the discriminator. This is intuitive if you think about it: the
    creative process is usually more complex than the process of recognition. In life,
    it might be easy to recognize a painting from Pablo Picasso if you see all of
    his paintings repeatedly.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将专注于创建 *图 14.3* 中所示的模型。该模型的生成器和鉴别器在层数和总参数量上有所不同。通常情况下，生成器的构建比鉴别器需要更多的资源。如果你仔细想想，这是直观的：创作过程通常比识别过程更为复杂。在生活中，如果你反复看到毕加索的所有画作，识别他的画作可能会很容易。
- en: 'However, it might be much harder, in comparison, to actually paint like Picasso:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际像毕加索那样绘画，可能要比这难得多：
- en: '![](img/5abff9d3-4f0b-4129-a969-40b777d0c526.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5abff9d3-4f0b-4129-a969-40b777d0c526.png)'
- en: Figure 14.3 - MLP-based GAN architecture
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 - 基于 MLP 的 GAN 架构
- en: This figure depicts an icon that simply represents the fact that the discriminator
    will be taking both fake and valid data and learning from both worlds. One thing
    that you must always remember about GANs is that they **generate **data from **random
    noise**. Just think about that for a minute and you will realize that this is
    very cool.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了一个图标，简单地表示鉴别器将同时处理假数据和真实数据，并从这两个世界中学习。关于 GAN，你必须始终记住的一点是，它们 **生成** 来自
    **随机噪声** 的数据。想一想这一点，你就会意识到这非常酷。
- en: So, the architecture in *Figure 14.3* does not have any new items we have not
    discovered before. However, the design itself is what is original. Also, the way
    to create it in Keras is quite the task. So, we will show the whole code, with
    as many comments as possible to make things clear.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，*图 14.3* 中的架构并没有什么我们之前没有发现的新东西。然而，设计本身是原创的。此外，用 Keras 创建它的方式确实是一项挑战。因此，我们将展示完整的代码，并尽可能多地添加注释以便让大家理解。
- en: 'Here is the full code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define the generator as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将生成器定义如下：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we can define the discriminator as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以如下定义判别器：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to put things together as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将各部分结合起来，如下所示：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will make the training happen inside a loop that will run for as many
    epochs as we want:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把训练放入一个循环中，这个循环将运行多个纪元，直到我们想要的次数：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces output similar to the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生类似以下的输出：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This might look different in your system because this is all based on **random**
    noise. This randomness aspect will most likely take your model in a different
    direction. However, what you will see is that your generator's loss should decrease
    gradually, and if the generator is working properly, the accuracy should be getting
    closer to random change, that is, close to 50%. If your discriminator is always
    100%, then your generator is not good enough, and if your discriminator is around
    50% accuracy, then your generator might be too good or your discriminator too
    weak.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一切基于**随机**噪声，这在你的系统中可能看起来不同。这个随机性可能会让你的模型朝不同的方向发展。然而，你会看到的是，如果生成器正常工作，它的损失应该会逐渐减少，并且准确率应该接近于随机变化，也就是接近50%。如果判别器始终保持100%的准确率，说明生成器表现不佳；如果判别器的准确率接近50%，那么可能是生成器过于优秀，或者判别器过于弱。
- en: Now, let's plot a couple of things; the learning curves (losses and accuracy),
    and the samples generated across epochs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制几样东西：学习曲线（损失和准确率），以及不同纪元生成的样本。
- en: 'The following code will plot the learning curves:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将绘制学习曲线：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This generates the plot shown in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下图所示的图表：
- en: '![](img/54e97377-4afd-4ae0-bec1-940d89686a15.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54e97377-4afd-4ae0-bec1-940d89686a15.png)'
- en: Figure 14.4 - Loss of generator and discriminator across epochs. Accuracy across
    epochs for an MLP-based GAN
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 - 生成器和判别器在多个纪元中的损失变化。基于MLP的GAN在多个纪元中的准确率。
- en: As the plot indicates, the loss of the discriminator is initially low, as also
    indicated by the accuracy. However, as epochs advance, the generator gets better
    (loss decreases) and accuracy slowly decreases.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，判别器的损失最初较低，准确率也显示了这一点。然而，随着纪元的推进，生成器的表现变得更好（损失减少），而准确率则缓慢下降。
- en: '*Figure 14.5* shows a couple of images at every sampled epoch that were produced
    from random noise:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.5*显示了在每个采样的纪元下生成的一些图像，这些图像是由随机噪声生成的：'
- en: '![](img/0692c795-14c9-40d4-a63e-9c561043e645.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0692c795-14c9-40d4-a63e-9c561043e645.png)'
- en: Figure 14.5 - GAN-generated images across epochs
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 - GAN在不同纪元生成的图像
- en: 'As you can see, the initial images look noisy, while the later images have
    more detail and familiar shapes. This would confirm the decrease in the discriminator
    accuracy since these images can easily pass as real. *Figure 14.5* was produced
    using the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，最初的图像看起来噪声较大，而后来的图像则有更多的细节和熟悉的形状。这将证实判别器准确率的下降，因为这些图像看起来几乎与真实图像相似。*图14.5*是使用以下代码生成的：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s consider a few takeaways from this model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下这个模型的一些收获：
- en: The model, as it has been presented, has room for improvements if we make the
    model larger where needed.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述的模型，如果我们在需要的地方将模型做大，仍然有提升的空间。
- en: If what we need is a good generator, we can extend the generator, or change
    it into a convolutional one (next section).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们需要一个好的生成器，我们可以扩展生成器，或者将其改为卷积生成器（见下一节）。
- en: If we want, we could save the `discriminator` and retrain it (fine-tune it)
    for the classification of digits.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们愿意，可以保存`discriminator`并重新训练它（微调）用于数字分类。
- en: If we want, we could use the `generator` to augment the dataset with as many
    images as we want.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们愿意，可以使用`generator`来增强数据集，添加任意数量的图像。
- en: In spite of the *decent* quality of the MLP-based GAN, we can appreciate that
    the shapes might not be as well defined as original samples. However, convolutional
    GANs can help.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于MLP的GAN质量*尚可*，我们可以看到，生成的形状可能不如原始样本定义得那么清晰。然而，卷积GAN能够提供帮助。
- en: Let's proceed and change the MLP-based model into a convolutional one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续，将基于MLP的模型转换为卷积模型。
- en: A convolutional model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个卷积模型
- en: The convolutional approach to a GAN was made popular by Radford, A., *et al.*
    (2015). The proposed model was called **Deep Convolutional GAN** (**DCGAN**).
    The primary goal is to make a series of convolutional layers learn feature representations
    to produce *fake* images or to *distinguish* between valid or fake images.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积方法在GAN中的应用由Radford等人（2015）广泛推广。所提出的模型被称为**深度卷积GAN**（**DCGAN**）。其主要目标是通过一系列卷积层学习特征表示，以生成*虚假*图像或*区分*真实和虚假图像。
- en: 'Moving forward, we will be **intentionally** using a different name for the
    discriminator network, which we will call **critic**. Both terms are used in the
    literature. However, there is a new trend to use the term *critic* and the old
    term may go away at some point. Regardless, you should know that both terms refer
    to the same thing: a network that is tasked with determining whether input is
    valid (from the original dataset) or fake (from an adversarial generator).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将**故意**使用不同的名称来指代判别网络，我们称之为**批评者**。这两个术语在文献中都有使用。然而，新的趋势是使用术语*批评者*，而旧的术语可能会逐渐消失。无论如何，你应该知道这两个术语指的都是同一个东西：一个任务是判断输入是有效的（来自原始数据集）还是伪造的（来自对抗生成器）。
- en: 'We will be implementing the model depicted in the following diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现以下图示中的模型：
- en: '![](img/0ef4b620-6eac-4a91-9886-7b8fa5c1bb55.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ef4b620-6eac-4a91-9886-7b8fa5c1bb55.png)'
- en: Figure 14.6 - CNN-based GAN architecture
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6 - 基于CNN的GAN架构
- en: This model has something new never before seen in this book: `Conv2DTranspose`.
    This type of layer is exactly like the traditional convolutional layer, `Conv2D`,
    except that it works in the exact opposite direction. While a `Conv2D` layer learns
    filters (feature maps) that split the input into filtered information, a `Conv2DTranspose` layer
    takes filtered information and joins it together.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型有一些在本书中前所未见的新内容：`Conv2DTranspose`。这种层与传统的卷积层`Conv2D`非常相似，不同之处在于它的工作方向正好相反。`Conv2D`层学习过滤器（特征图），将输入拆分为过滤后的信息，而`Conv2DTranspose`层则是将过滤后的信息合并在一起。
- en: Some people refer to `Conv2DTranspose` as *deconvolution*. However, I personally
    think it is incorrect to do so since *deconvolution* is a mathematical operation
    significantly different from what `Conv2DTranspose` does. Either way, you need
    to remember that if you read *deconvolution* in the context of CNNs, it means `Conv2DTranspose`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人将`Conv2DTranspose`称为*反卷积*。然而，我个人认为这样做是不正确的，因为*反卷积*是一个与`Conv2DTranspose`执行的操作有显著不同的数学运算。无论如何，你需要记住，如果在CNN的上下文中看到*反卷积*，它指的就是`Conv2DTranspose`。
- en: 'The remainder of the elements in the model are things that we have already
    discussed previously. The full code, which omits comments, is the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的其余元素是我们之前已经讨论过的内容。完整的代码（去除注释）如下：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next we define the generator as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义如下的生成器：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we define the critic networks as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义如下的批评网络：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next we put things together and set the parameters of the model as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将各部分组合起来并设置模型的参数如下：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we train using the following cycle:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下周期进行训练：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'About 70% of the preceding code is the same as before. However, the convolutional
    network design was new. The code would print summaries for both the generator
    and critic. Here is the summary for the generator:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前面70%的代码与之前相同。然而，卷积网络的设计是新的。代码将打印生成器和批评网络的摘要。以下是生成器的摘要：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the summary for the critic:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是批评者的摘要：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A sample output for the training steps would look like the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤的一个示例输出如下所示：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'From the training output, we can see that the convolutional network is able
    to reduce the loss of the generator faster than its MLP counterpart. It appears
    that for the remainder of the epochs, the critic learns slowly to be more robust
    against the generator of fake input. This can be more clearly observed by plotting
    the results using the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练输出中，我们可以看到卷积网络比MLP对手更快地减少生成器的损失。似乎在剩余的epochs中，批评者学习得较慢，以便更能抵抗生成的伪输入。通过使用以下代码绘制结果，可以更清楚地观察到这一点：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The code produces the plot shown in *Figure 14.7*. From the diagram, we can
    appreciate the claims made on faster convergence to small losses and slow recovery
    of the critic''s accuracy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成了如*图14.7*所示的图表。从图中，我们可以看到关于更快收敛到小损失和批评者准确度恢复较慢的声明：
- en: '![](img/0c37a8ce-3502-4c4a-8f69-986f32a2abaf.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c37a8ce-3502-4c4a-8f69-986f32a2abaf.png)'
- en: Figure 14.7 - Learning curves for CNN-based GANs
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7 - 基于CNN的GAN学习曲线
- en: 'We can also display the samples generated as the convolutional GAN was being
    trained. The results are shown in *Figure 14.8*. These results are consistent
    with a poor-quality generator trained under 2,000 epochs. After 5,000 epochs,
    the generator is able to produce well-defined numerals that can easily pass as
    valid:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示在训练卷积生成对抗网络时生成的样本。结果如*图14.8*所示。这些结果与在2000个epochs下训练出的低质量生成器一致。在经过5000个epochs后，生成器能够生成定义明确的数字，这些数字可以轻松通过验证：
- en: '![](img/4b8424b6-2de1-4879-a650-d6d4318e39dd.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b8424b6-2de1-4879-a650-d6d4318e39dd.png)'
- en: Figure 14.8 - Samples generated during training
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8 - 训练过程中生成的样本
- en: For reference, we can compare *Figure 14.5* and *Figure 14.8* for the MLP-based
    and convolutional-based approach, respectively. Such a comparison can offer insights
    on the fundamental differences between a general-purpose GAN (MLP-based) or a
    GAN specialized in spatial relationships (CNN-based).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，我们可以分别比较*图 14.5*和*图 14.8*，它们展示了基于MLP和卷积的不同方法。这样的比较有助于了解通用GAN（基于MLP）和专注于空间关系的GAN（基于CNN）之间的根本差异。
- en: Now, we would like to discuss briefly the generative abilities that **Variational
    Autoencoders** (**VAEs**) and GANs bring to the table.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们简要讨论一下**变分自编码器**（**VAEs**）和生成对抗网络（GANs）所带来的生成能力。
- en: Comparing GANs and VAEs
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较GAN和VAE
- en: 'In [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*,
    we discussed VAEs as a mechanism for dimensionality reduction that aims to learn
    the parameters of the distribution of the input space, and effect reconstruction
    based on random draws from the latent space using the learned parameters. This
    offered a number of advantages we already discussed in [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational
    Autoencoders*, such as the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)中，我们讨论了VAE作为一种降维机制，旨在学习输入空间分布的参数，并基于从潜在空间中随机抽取的样本，利用学习到的参数进行重建。这提供了我们在[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)中已讨论过的若干优点，如下所示：
- en: The ability to reduce the effect of noisy inputs, since it learns the distribution
    of the input, not the input itself
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于VAE学习的是输入的分布，而非输入本身，因此它有能力减小噪声输入的影响。
- en: The ability to generate samples by simply querying the latent space
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简单查询潜在空间生成样本的能力
- en: 'On the other hand, GANs can also be used to generate samples, like the VAE.
    However, the learning of both is quite different. In GANs, we can think of the
    model as having two major parts: a critic and a generator. In VAEs, we also have
    two networks: an encoder and a decoder.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GAN也可以像VAE一样生成样本。然而，两者的学习过程有很大不同。在GAN中，我们可以将模型视为由两个主要部分组成：一个判别器和一个生成器。而在VAE中，我们也有两个网络：一个编码器和一个解码器。
- en: If we were to make any connection between the two, it would be that the decoder
    and generator play a very similar role in VAEs and GANs, respectively. However,
    an encoder and a critic have very different goals. An encoder will learn to find
    a rich latent representation, usually with very few dimensions compared to the
    input space. Meanwhile, a critic does not aim to find any representations, but
    to solve a growing complex binary classification problem.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要在两者之间做任何联系，那就是解码器和生成器在VAE和GAN中分别扮演着非常相似的角色。然而，编码器和判别器的目标则截然不同。编码器的目标是学习找到一个丰富的潜在表示，通常这个表示的维度远小于输入空间。而判别器的目标不是去找到任何表示，而是解决一个日益复杂的二分类问题。
- en: We could make a case that the critic is certainly learning features from the
    input space; however, the claim that features in the deepest layers are similar
    in both the critic and encoder requires more evidence.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以主张判别器肯定是在学习输入空间的特征；然而，判别器和编码器中最深层的特征是否相似，这一说法还需要更多的证据。
- en: One thing we can do to make a comparison is to take the deep VAE model shown
    in [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*,
    *Figure 14.7*, train it, and draw some random samples from the generator in the
    VAE, and do the same for the convolutional GAN.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过比较的方法是，采用[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)中的深度VAE模型，*变分自编码器*，*图14.7*，对其进行训练，并从VAE的生成器中随机抽取一些样本，同时对卷积GAN做同样的操作。
- en: 'We can start by displaying the samples from the convolutional GAN and executing
    the following code immediately after the last piece of code in the previous section,
    which contains the trained GAN. Here is the code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过显示卷积GAN的样本并在前一节最后一段代码后立即执行以下代码来开始。该代码包含了已训练好的GAN。代码如下：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code will produce 400 numerals from random noise! The plot is shown in
    *Figure 14.9*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将从随机噪声中生成400个数字！绘图如*图14.9*所示：
- en: '![](img/b787b620-e438-4ddd-b25c-8715396e7cee.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b787b620-e438-4ddd-b25c-8715396e7cee.png)'
- en: Figure 14.9 - 400 numerals produced by a convolutional GAN
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9 - 由卷积GAN生成的400个数字
- en: Recall that these numerals were produced after 12,000 epochs. The quality seems
    relatively good. Most of these numerals could actually fool a human being into
    thinking they were really written by a human.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，这些数字是在经过 12,000 次训练后生成的。质量似乎相对较好。这些数字大多数可能会欺骗人类，让他们以为它们真的是人类写的。
- en: Now, we want to take a look at the quality of the numerals generated with a
    VAE. For this, you will need to go to [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational
    Autoencoders*, and use the code provided to implement the deep VAE and train it
    for, say, 5,000 epochs. After training it, you can use the decoder to generate
    samples from random noise by choosing random parameters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想看看使用 VAE 生成的数字质量。为此，你需要参考[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自编码器*，并使用提供的代码实现深度
    VAE，并训练大约 5,000 次迭代。训练完成后，你可以使用解码器通过选择随机参数从随机噪声中生成样本。
- en: 'Here is the code you should use *once* the training of the VAE is complete:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 VAE 的训练完成，以下是你应该使用的代码：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A couple of visible differences is that the VAE assumes that the parameters
    of the latent space follow a normal distribution; also, the output needs to be
    reshaped to 28x28, as opposed to the GAN, which gives the output already in its
    correct shape thanks to the 2D convolutional output layer. The output of this
    code is shown in *Figure 14.10*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 几个明显的区别是，VAE 假设潜在空间的参数遵循正态分布；此外，输出需要被重塑为 28x28，而 GAN 则通过 2D 卷积输出层直接生成正确形状的输出。这段代码的输出如*图
    14.10*所示：
- en: '![](img/84984cdc-549e-4db3-ad8c-921d787020f6.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84984cdc-549e-4db3-ad8c-921d787020f6.png)'
- en: Figure 14.10 - 400 samples of numerals generated by the decoder of a VAE using
    random noise
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10 - 由 VAE 解码器使用随机噪声生成的 400 个数字样本
- en: As you can see from the diagram, some of these numerals look very good; some
    might say too good. They look smooth, well-rounded, and perhaps we can say noise-free.
    The numerals produced by the VAE lack the distinctive quality of looking noisy
    compared to the ones produced by the GAN. However, this can be a good thing or
    a bad thing depending on what you want to do.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，这些数字中的一些看起来非常好；有些人可能会说看起来太好。它们看起来平滑、圆润，或许我们可以说没有噪点。与由 GAN 生成的数字相比，VAE
    生成的数字缺少那种带有噪音感的独特特征。然而，这可以是好事，也可以是坏事，取决于你想做什么。
- en: Say that you want to have clean-looking samples that might be easily identified
    as *fake,* then a VAE is the best choice. Now, say that we want samples that can
    easily fool a human into thinking they are not produced by a machine; here, perhaps
    a GAN fits better.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要一些干净的样本，容易被识别为*伪造的*，那么 VAE 是最好的选择。现在，假设我们希望生成的样本能轻易地让人类相信它们不是机器生成的；在这种情况下，可能
    GAN 更合适。
- en: Regardless of these differences, both can be used to augment your datasets if
    you need to have more data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些区别，两者都可以用来扩充你的数据集，如果你需要更多的数据。
- en: Thinking about the ethical implications of GANs
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考 GAN 的伦理影响
- en: Some ethical thoughts about generative models have already been provided in
    [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*.
    However, a second round of thoughts is in order given the adversarial nature of
    GANs. That is, there is an implicit demand from a GAN to *trick *a critic in a
    min-max game where the generator needs to come out victorious (or the critic as
    well). This concept generalized to adversarial learning provides the means to *attack*
    existing machine learning models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成模型的一些伦理思考已经在[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自编码器*中提到。然而，鉴于
    GAN 的对抗性质，第二轮的思考是必要的。也就是说，GAN 隐含地要求在最小-最大博弈中“欺骗”一个判别器，生成器需要取得胜利（或者判别器也可以）。这一概念推广到对抗学习，为攻击现有的机器学习模型提供了手段。
- en: Very successful computer vision models such as VGG16 (a CNN model) have been
    attacked by models that perform adversarial attacks. There are *patches* that
    you can print, put on a t-shirt, cap, or any object, and as soon as the patch
    is present in the input to the models being attacked, they are fooled into thinking
    that the existing object is a completely different one (Brown, T. B., et al. (2017)).
    Here is an example of an adversarial patch that tricks a model into thinking that
    a banana is a toaster: [https://youtu.be/i1sp4X57TL4](https://youtu.be/i1sp4X57TL4).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 很成功的计算机视觉模型，如VGG16（一个CNN模型），已经遭受过执行对抗攻击的模型攻击。有一些*补丁*可以打印出来，放在T恤、帽子或任何物体上，当这些补丁出现在输入到被攻击的模型中时，模型会被欺骗，认为原本存在的物体是完全不同的（Brown,
    T. B., 等人（2017））。这里有一个示例，通过对抗性补丁将香蕉欺骗成烤面包机：[https://youtu.be/i1sp4X57TL4](https://youtu.be/i1sp4X57TL4)。
- en: Now that these types of adversarial attacks are known to exist, researchers
    have found vulnerabilities in their current systems. Therefore, it has become
    almost an obligation for us, the deep learning practitioners, to make sure our
    models are robust against adversarial attacks. This is particularly important
    for systems that involve sensitive information, or systems that make decisions
    that affect human life.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 既然这些类型的对抗攻击已经被确认存在，研究人员也发现了当前系统中的漏洞。因此，作为深度学习从业者，我们几乎有责任确保我们的模型在面对对抗攻击时具有鲁棒性。这对于涉及敏感信息的系统，或做出可能影响人类生命的决策的系统尤为重要。
- en: For example, a deep learning model that is deployed in an airport to assist
    security efforts needs to be tested so as to avoid a person wearing a t-shirt
    with a printed adversarial patch aiming to avoid being recognized as a person
    in a restricted area. This is critical for the security of the population. However,
    a deep learning system to automatically tune the audio of a person singing might
    not be particularly critical.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，部署在机场以协助安全工作的深度学习模型需要进行测试，以避免某人穿着印有对抗性补丁的T恤，意图避免被识别为受限区域内的人员。这对人口安全至关重要。然而，用于自动调节某人唱歌音频的深度学习系统可能并不是特别关键。
- en: What is required from you is to look into testing your models for adversarial
    attacks. There are several resources online being updated frequently that you
    can easily find if you search for them. If you come to find a vulnerability in
    a deep learning model, you should report it to the creators immediately, for the
    well-being of our society.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的是测试你的模型是否受到对抗性攻击的影响。网上有多个资源，并且经常更新，只需搜索就可以轻松找到。如果你发现深度学习模型中存在漏洞，你应该立即向开发者报告，以确保我们社会的福祉。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This advanced chapter showed you how to create GAN networks. You learned the
    major components of GANs, a generator and a critic, and their role in the learning
    process. You learned about adversarial learning in the context of breaking models
    and making them robust against attacks. You coded an MLP-based and a convolutional-based
    GAN on the same dataset and observed the differences. At this point, you should
    feel confident explaining why adversarial training is important. You should be
    able to code the necessary mechanisms to train a generator and a discriminator
    of a GAN. You should feel confident about coding a GAN and comparing it to a VAE
    to generate images from a learned latent space. You should be able to design generative
    models, considering the societal implications and the responsibilities that come
    with using generative models.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章高级内容向你展示了如何创建GAN网络。你了解了GAN的主要组件：生成器和判别器，以及它们在学习过程中的作用。你了解了对抗性学习的概念，尤其是在打破模型并使其对攻击具备鲁棒性方面。你基于同一数据集编写了基于MLP和基于卷积的GAN，并观察了它们的差异。到此为止，你应该能够自信地解释为什么对抗训练如此重要。你应该能够编写必要的机制来训练GAN的生成器和判别器。你应该对编写GAN代码并与VAE进行对比，从已学习的潜在空间生成图像充满信心。你应该能够设计生成模型，考虑到其社会影响及使用生成模型时应承担的责任。
- en: GANs are very interesting and have yielded amazing research and applications.
    They have also exposed the vulnerabilities of other systems. The present state
    of deep learning involves combinations of AEs, GANs, CNNs, and RNNs, using specific
    components of each, and gradually increasing the potential of applications of
    deep learning across different fields. The world of deep learning is exciting
    right now, and you are now ready to embrace it and dive deeper into whatever area
    you feel you like. [Chapter 15](216a275e-ae7e-451c-a8c6-f31eac314d3f.xhtml), *Final
    Remarks on the Future of Deep Learning*, will present brief comments on how we
    see the future of deep learning. It attempts to use some kind of *prophetic* voice
    about the things to come. But before you go, quiz yourself with the following
    questions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GAN非常有趣，并且已经带来了令人惊叹的研究和应用。它们还暴露了其他系统的脆弱性。目前深度学习的状态涉及AE、GAN、CNN和RNN的组合，利用每种方法的特定组件，并逐步提高深度学习在各个领域的应用潜力。如今深度学习的世界充满了激动人心的前景，你现在已经准备好去拥抱它，并深入探讨你感兴趣的任何领域。[第15章](216a275e-ae7e-451c-a8c6-f31eac314d3f.xhtml)，*关于深度学习未来的最终评价*，将简要评论我们对深度学习未来的看法。它尝试以某种*预言性的*语气谈论未来的事情。但在你离开之前，请用以下问题自我测试一下。
- en: Questions and answers
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Who is the adversary in a GAN?**'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在GAN中，谁是对手？**'
- en: The generator. It acts as a model whose sole purpose is to make the critic fail;
    it is the critic's adversary.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器。它充当一个模型，唯一的目的就是让评论员失败；它是评论员的对手。
- en: '**Why is the generator model bigger than the critic? **'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么生成器模型比评论员模型更大？**'
- en: This is not always the case. The models discussed here were more interesting
    as generators of data. However, we could use the critic and retrain it for classification,
    in which case, the critic model might be bigger.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这并非总是如此。这里讨论的模型作为数据生成器更为有趣。然而，我们可以利用评论员并重新训练它用于分类，在这种情况下，评论员模型可能会更大。
- en: '**What is adversarial robustness?**'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**什么是对抗性鲁棒性？**'
- en: It is a new field in deep learning tasked with researching ways to certify that
    deep learning models are robust against adversarial attacks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习中的一个新领域，致力于研究如何验证深度学习模型能够抵御对抗性攻击。
- en: '**Which is better – a GAN or a VAE?**'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**哪一个更好——GAN还是VAE？**'
- en: This depends on the application. GANs tend to produce more "interesting" results
    than VAEs, but VAEs are more stable. Also, it is often faster to train a GAN than
    a VAE.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于应用场景。与VAE相比，GAN往往能产生更“有趣”的结果，但VAE更稳定。此外，训练GAN通常比训练VAE更快。
- en: '**Are there any risks associated with GANs?**'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**GAN是否有任何风险？**'
- en: Yes. There is a known problem called *mode collapse*, which refers to the inability
    of a GAN to produce novel, different, results across epochs. It seems like the
    network gets stuck on a few samples that can cause sufficient confusion in the
    critic so as to produce a low loss, while having no diversity of generated data.
    This is still an open problem with no universal solution. A lack of diversity
    in a GAN's generator is an indication that it has collapsed. To find out more
    about mode collapse, read Srivastava, A., et al. (2017).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。有一个已知的问题叫做*模式崩溃*，它指的是GAN在不同训练周期中无法生成新颖且不同的结果。网络似乎会卡在一些样本上，这些样本足以在评论员中引起足够的混淆，从而生成较低的损失，但生成的数据缺乏多样性。这仍然是一个悬而未决的问题，没有普遍的解决方案。GAN生成器缺乏多样性是其崩溃的迹象。想了解更多关于模式崩溃的信息，请阅读Srivastava,
    A., 等人（2017年）。
- en: References
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abadi, M., and Andersen, D. G. (2016). *Learning to protect communications with
    adversarial neural cryptography*. *arXiv preprint* arXiv:1610.06918.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi, M., 和 Andersen, D. G. (2016). *使用对抗性神经密码学保护通信*。*arXiv预印本* arXiv:1610.06918。
- en: Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. (2018). *Black box adversarial
    attacks with limited queries and information*. *arXiv preprint* arXiv:1804.08598.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas, A., Engstrom, L., Athalye, A., 和 Lin, J. (2018). *有限查询和信息下的黑盒对抗性攻击*。*arXiv预印本*
    arXiv:1804.08598。
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., and Bengio, Y. (2014). *Generative adversarial nets*. In *Advances in neural
    information processing systems* (pp. 2672-2680).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., 和 Bengio, Y. (2014). *生成对抗网络*。载于 *神经信息处理系统进展*（第2672-2680页）。
- en: Sukhbaatar, S., and Fergus, R. (2016). *Learning multi-agent communication with
    backpropagation*. In *Advances in neural information processing systems* (pp.
    2244-2252).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar, S., 和 Fergus, R. (2016). *使用反向传播学习多智能体通信*。载于 *神经信息处理系统进展*（第2244-2252页）。
- en: Rivas, P., and Banerjee, P. (2020). *Neural-Based Adversarial Encryption of
    Images in ECB Mode with 16-bit Blocks*. In *International Conference on Artificial
    Intelligence*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivas, P., 和 Banerjee, P. (2020). *基于神经网络的ECB模式图像对抗加密，采用16位块*。发表于 *国际人工智能会议*。
- en: Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. (2019). *Certified adversarial
    robustness via randomized smoothing*. *arXiv preprint* arXiv:1902.02918.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen, J. M., Rosenfeld, E., 和 Kolter, J. Z. (2019). *通过随机平滑获得认证的对抗性鲁棒性*。*arXiv
    预印本* arXiv:1902.02918。
- en: Radford, A., Metz, L., and Chintala, S. (2015). *Unsupervised representation
    learning with deep convolutional generative adversarial networks*. *arXiv preprint*
    arXiv:1511.06434.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford, A., Metz, L., 和 Chintala, S. (2015). *使用深度卷积生成对抗网络进行无监督表示学习*。*arXiv
    预印本* arXiv:1511.06434。
- en: Brown, T. B., Mané, D., Roy, A., Abadi, M., and Gilmer, J. (2017). *Adversarial
    patch*. *arXiv preprint* arXiv:1712.09665.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown, T. B., Mané, D., Roy, A., Abadi, M., 和 Gilmer, J. (2017). *对抗性补丁*。*arXiv
    预印本* arXiv:1712.09665。
- en: 'Srivastava, A., Valkov, L., Russell, C., Gutmann, M. U., and Sutton, C. (2017).
    *Veegan: Reducing mode collapse in GANs using implicit variational learning*.
    In *Advances in Neural Information Processing Systems* (pp. 3308-3318).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava, A., Valkov, L., Russell, C., Gutmann, M. U., 和 Sutton, C. (2017).
    *Veegan：使用隐式变分学习减少GAN中的模式崩溃*。发表于 *神经信息处理系统进展*（第3308-3318页）。
