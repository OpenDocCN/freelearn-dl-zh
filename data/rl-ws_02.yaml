- en: 2\. Markov Decision Processes and Bellman Equations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 马尔可夫决策过程与贝尔曼方程
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will cover more of the theory behind reinforcement learning. We
    will cover Markov chains, Markov reward processes, and Markov decision processes.
    We will learn about the concepts of state values and action values along with
    Bellman equations to calculate previous quantities. By the end of this chapter,
    you will be able to solve Markov decision processes using linear programming methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将更深入地探讨强化学习背后的理论。我们将涵盖马尔可夫链、马尔可夫奖励过程和马尔可夫决策过程的内容。我们将学习状态值和动作值的概念，以及如何通过贝尔曼方程计算这些量。到本章结束时，您将能够使用线性编程方法求解马尔可夫决策过程。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we studied the main elements of **Reinforcement Learning**
    (**RL**). We described an agent as an entity that can perceive an environment's
    state and act by modifying the environment state in order to achieve a goal. An
    agent acts through a policy that represents its behavior, and the way the agent
    selects an action is based on the environment state. In the second half of the
    previous chapter, we introduced Gym and Baselines, two Python libraries that simplify
    the environment representation and the algorithm implementation, respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了**强化学习**（**RL**）的主要元素。我们将智能体描述为一个能够感知环境状态并通过修改环境状态来实现目标的实体。智能体通过策略进行行为，策略代表其行为方式，而智能体选择动作的方式则基于环境状态。在上一章的后半部分，我们介绍了Gym和Baselines，这两个Python库分别简化了环境表示和算法实现。
- en: We mentioned that RL considers problems as **Markov Decision Processes** (**MDPs**),
    without entering into the details and without giving a formal definition.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到，强化学习将问题视为**马尔可夫决策过程**（**MDP**），但没有深入细节，也没有给出正式的定义。
- en: In this chapter, we will formally describe what an MDP is, its properties, and
    its characteristics. When facing a new problem in RL, we have to ensure that the
    problem can be formalized as an MDP; otherwise, applying RL techniques is impossible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将正式描述MDP的定义、特性和属性。在面对强化学习（RL）中的新问题时，我们必须确保该问题能够形式化为MDP；否则，应用强化学习技术是不可行的。
- en: Before presenting a formal definition of MDPs, we need to understand **Markov
    Chains** (**MCs**) and **Markov Reward Processes** (**MRPs**). MCs and MRPs are
    specific cases (simplified) of MDPs. An MC only focuses on state transitions without
    modeling rewards and actions. Consider the example of the game of snakes and ladders,
    where the next action is completely dependent on the number displayed on the dice.
    MRPs also include the reward component in the state transition. MRPs and MCs are
    useful in understanding the characteristics of MDPs gradually. We will be looking
    at specific examples of MCs and MRPs later in the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在正式定义MDP之前，我们需要理解**马尔可夫链**（**MCs**）和**马尔可夫奖励过程**（**MRPs**）。MCs和MRPs是MDP的特定情况（简化版本）。MC只关注状态转移，不建模奖励和动作。考虑经典的蛇梯游戏，其中下一步动作完全依赖于骰子上显示的数字。MRPs则在状态转移中也包含了奖励成分。MRPs和MCs有助于逐步理解MDP的特性。我们将在本章后面介绍MCs和MRPs的具体示例。
- en: Along with MDPs, this chapter also presents the concepts of the state-value
    function and the action-value function, which are used to evaluate how good a
    state is for an agent and how good an action taken in a given state is. State-value
    functions and action-value functions are the building blocks of the algorithms
    used to solve real-world problems. The concepts of state-value functions and action-value
    functions are highly related to the agent's policy and the environment dynamics,
    as we will learn later in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章除了介绍马尔可夫决策过程（MDP）外，还介绍了状态值函数和动作值函数的概念，这些概念用于评估一个状态对于智能体的好坏以及在给定状态下采取的动作的好坏。状态值函数和动作值函数是用于解决实际问题的算法的构建模块。状态值函数和动作值函数的概念与智能体的策略和环境动态密切相关，正如我们将在本章后面学到的那样。
- en: The final part of this chapter presents two **Bellman equations**, namely the
    **Bellman expectation equation** and the **Bellman optimality equation**. These
    equations are helpful in the context of RL in order to evaluate the behavior of
    an agent and find a policy that maximizes the agent's performance in an MDP.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后部分介绍了两个**贝尔曼方程**，即**贝尔曼期望方程**和**贝尔曼最优性方程**。这些方程在强化学习中有助于评估智能体的行为，并找到一个能够最大化智能体在MDP中表现的策略。
- en: In this chapter, we will practice with some MDP examples, such as the student
    MDP and Gridworld. We will implement the solution methods and equations explained
    in this chapter using Python, SciPy, and NumPy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过一些MDP的例子进行实践，比如学生MDP和Gridworld。我们将使用Python、SciPy和NumPy实现本章中解释的求解方法和方程。
- en: Markov Processes
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫过程
- en: In the previous chapter, we described the RL loop as an agent observing a representation
    of the environment state, interacting with an environment through actions, and
    receiving a reward based on the action and the environment state. This interaction
    process is called an MDP. In this section, we will understand what an MDP is,
    starting with the simplest case of an MDP, an MC. Before describing the various
    types of MDPs, it is useful to formalize the underlying property of all these
    processes, the Markov property.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们描述了强化学习（RL）循环：智能体观察环境状态的表示，通过行动与环境互动，并根据行动和环境状态获得奖励。这个互动过程称为MDP。在这一节中，我们将从MDP的最简单形式——马尔可夫链（MC）开始，理解什么是MDP。在描述不同类型的MDP之前，有必要将所有这些过程的基础性质——马尔可夫性质——形式化。
- en: The Markov Property
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫性质
- en: 'Let''s start with two examples to help us to understand what the Markov property
    is. Consider a Rubik''s cube. When formalizing the solving of a Rubik''s cube
    as an RL task, we can define the environment state as the state of the cube. The
    agent can perform actions corresponding to the rotation of the cube''s faces.
    The action results in a state transition that changes the cube. Here, the history
    is not important – that is, the sequence of actions yielding the current state
    – in determining the next state. The current state and the present action are
    the only components that influence the future state:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从两个例子开始，帮助我们理解什么是马尔可夫性质。考虑一个魔方。当将解魔方的任务形式化为强化学习任务时，我们可以将环境状态定义为魔方的状态。智能体可以执行相应的动作来旋转魔方的面。这些动作会导致状态转移，改变魔方的状态。在这里，历史并不重要——也就是说，决定当前状态的动作序列——因为它不会影响下一个状态。当前状态和当前行动是唯一影响未来状态的因素：
- en: '![](img/B16182_02_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16182_02_01.jpg)'
- en: 'Figure 2.1: Rubik''s cube representation'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：魔方表示
- en: 'Looking at the preceding figure, suppose the current environment state is the
    cube with the `Present` label. The current state can be reached by the two states
    to its left, with the labels `Past #1` and `Past #2`, using two different actions,
    represented as black arrows. By rotating the face on the left downwards, in the
    current state, we get the future state on the right, denoted by the label `Future`.
    The next state, in this case, is independent of the past, in the sense that only
    the present state and action determine it. It does not matter what the former
    state was, whether it was `Past #1` or `Past #2`; in both cases, we end up with
    the same future state.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '看看上面的图，假设当前环境状态是标记为`Present`的立方体。当前状态可以通过左边标记为`Past #1`和`Past #2`的两个状态到达，并且使用两个不同的动作（用黑色箭头表示）。通过将左边的面旋转向下，在当前状态下，我们得到右边的未来状态，标记为`Future`。在这种情况下，下一个状态与过去无关，因为只有当前状态和行动决定了它。过去的状态无关紧要，不管是`Past
    #1`还是`Past #2`，我们最终都会到达相同的未来状态。'
- en: 'Let''s now consider another classic example: the Breakout game.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再考虑一个经典的例子：Breakout游戏。
- en: Breakout is a classic Atari game. In the game, there is a layer of bricks at
    the top of the screen; the goal is to break the bricks using a ball, without allowing
    the ball to touch the bottom of the screen. The player can only move a paddle
    horizontally. When formalizing the Breakout game as an RL task, we can define
    the environment state as the image pixels at a certain moment. The agent has at
    its disposal three possible actions, "Left," "Right," and "None," corresponding
    to the paddle's movement.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Breakout是一个经典的Atari游戏。在这个游戏中，屏幕顶部有一层砖块；目标是使用球打破砖块，同时不让球触碰到屏幕底部。玩家只能水平移动挡板。当将Breakout游戏形式化为一个强化学习任务时，我们可以将环境状态定义为某一时刻的图像像素。智能体可以选择三种可能的动作："左"、"右"和"无"，分别对应挡板的移动。
- en: Here, there is a difference with respect to the Rubik's cube example. [*Figure
    2.2*](B16182_02_Final_SZ_ePub.xhtml#_idTextAnchor100) explains the difference
    visually. If we represent the environment state using only the current frame,
    the future is not determined only by the current state and the current action.
    We can easily visualize this problem by looking at the ball.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与魔方的例子有所不同。[*图 2.2*](B16182_02_Final_SZ_ePub.xhtml#_idTextAnchor100) 直观地解释了这个区别。如果我们仅使用当前帧表示环境状态，那么未来不仅仅由当前状态和当前动作决定。我们可以通过观察球来轻松地可视化这个问题。
- en: In the left part of [*Figure 2.2*](B16182_02_Final_SZ_ePub.xhtml#_idTextAnchor100),
    we can see two possible past states yielding the same present state. With the
    arrow, we represent the ball movement. In both cases, the agent's action is "Left."
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*图 2.2*](B16182_02_Final_SZ_ePub.xhtml#_idTextAnchor100)的左侧部分，我们可以看到两个可能的过去状态，它们都导致相同的当前状态。通过箭头表示球的运动。在这两种情况下，代理的动作都是“左”。
- en: 'In the right part of the figure, we have two possible future states, `Future
    #1` and `Future #2`, starting from the present state and performing the same action
    (the "Left" action).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '在图的右侧部分，我们有两个可能的未来状态，`未来 #1`和`未来 #2`，它们从当前状态开始并执行相同的动作（“左”动作）。'
- en: By looking only at the current state, it is not possible to decide with certainty
    which of the two future states will be the next one, as we cannot infer the ball's
    direction, whether it is going toward the top of the screen or the bottom. We
    need to know the history, that is, which of the two previous states was the actual
    previous state, in order to understand what the next state will be.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过观察当前状态，我们无法确定下一个未来状态是哪一个，因为我们无法推断出球的运动方向，它是朝着屏幕的顶部还是底部。我们需要了解历史，即需要知道两个前一个状态中的哪一个是真正的前一个状态，才能理解下一个状态将是什么。
- en: 'In this case, the future state is not independent of the past:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，未来状态并不独立于过去：
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that the arrow is not actually present in the environment state. We have
    drawn it in the frame for ease of presentation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，箭头实际上并不存在于环境状态中。我们在图框中绘制它是为了方便展示。
- en: '![Figure 2.2: Atari game representation'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2：雅达利游戏表示'
- en: '](img/B16182_02_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_02.jpg)'
- en: 'Figure 2.2: Atari game representation'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：雅达利游戏表示
- en: 'In the Rubik''s cube example, the current state contained enough information
    to determine, together with the current action, the next state. In the Atari example,
    this is not true. The current state does not contain a crucial piece of information:
    the movement component. In this case, we need not only the current state but also
    the past states to determine the next ones.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在魔方的例子中，当前状态包含了足够的信息，结合当前的动作，可以确定下一个状态。而在雅达利的例子中，这并不成立。当前状态并不包含一个至关重要的信息：运动成分。在这种情况下，我们不仅需要当前状态，还需要过去的状态来确定下一个状态。
- en: The Markov property explains exactly the difference between the two examples
    in mathematical terms. The Markov property states that "the future is independent
    of the past, given the present."
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质恰好用数学术语解释了两个例子之间的区别。马尔可夫性质表明“在给定当前状态的情况下，未来与过去无关。”
- en: 'This means that the future state depends only on the present state, the present
    state is the only thing influencing the future state, and that we can get rid
    of the past states. The Markov property can be formalized in the following way:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着未来状态仅依赖于当前状态，当前状态是唯一影响未来状态的因素，我们可以忽略过去的状态。马尔可夫性质可以通过以下方式形式化：
- en: '![Figure 2.3: Expression for the Markov property'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：马尔可夫性质的表达式'
- en: '](img/B16182_02_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_03.jpg)'
- en: 'Figure 2.3: Expression for the Markov property'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：马尔可夫性质的表达式
- en: The probability, ![1](img/B16182_02_03a.png), of the next state, ![2](img/B16182_02_03b.png),
    given the current one, ![3](img/B16182_02_03c.png), is equal to the probability
    of the next state given the state history, ![4](img/B16182_02_03d.png). This means
    that the past states, ![a](img/B16182_02_03e.png), have no influence over the
    next state distribution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定当前状态![1](img/B16182_02_03a.png)，下一个状态![2](img/B16182_02_03b.png)的概率等于给定状态历史![4](img/B16182_02_03d.png)下下一个状态的概率![3](img/B16182_02_03c.png)。这意味着过去的状态![a](img/B16182_02_03e.png)对下一个状态分布没有影响。
- en: In other words, to describe the probability distribution of the next state,
    we only need the information contained in the current state. Almost all RL environments,
    being MDPs, assume that the Markov property holds true. We need to remember this
    property when designing RL tasks; otherwise, the main RL assumptions won't be
    true anymore, causing the algorithms to fail miserably.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，要描述下一个状态的概率分布，我们只需要当前状态中的信息。几乎所有的强化学习（RL）环境，作为马尔可夫决策过程（MDP），都假设马尔可夫性质成立。在设计
    RL 任务时，我们需要记住这一性质；否则，主要的 RL 假设将不再成立，导致算法失败。
- en: Note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In statistical language, the term "given" means that the probability is influenced
    by some information. In other words, the probability function depends on some
    other information.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，术语“给定”意味着概率受某些信息的影响。换句话说，概率函数依赖于一些其他信息。
- en: 'Most of the time, the Markov property holds true; however, there are cases
    in which we need to design the environment state to ensure the independence of
    the next state from the past states. This is exactly the case in Breakout. To
    restore the Markov property, we can define the state as multiple consequent frames
    so that it is possible to infer the ball direction. Refer to the following figure
    for a visual representation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，马尔可夫性质成立；然而，也有一些情况需要我们设计环境状态，以确保下一个状态与过去的状态相互独立。这正是《Breakout》游戏中的情况。为了恢复马尔可夫性质，我们可以将状态定义为多个连续帧，这样就可以推断球的方向。请参见以下图示：
- en: '![Figure 2.4: Markov state for Breakout'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4：Breakout 的马尔可夫状态'
- en: '](img/B16182_02_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_04.jpg)'
- en: 'Figure 2.4: Markov state for Breakout'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：Breakout 的马尔可夫状态
- en: As you can see in the preceding figure, the state is represented by three consequent
    frames.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，状态由三个连续的帧表示。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There are other tricks you can use to restore the Markov property. One of these
    tricks consists of using policies represented as **Recurrent Neural Networks**
    (**RNNs**). Using RNNs, the agent can also take into account past states when
    determining the current action. The usage of RNNs as RL policies will be discussed
    later on in the book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用其他技巧来恢复马尔可夫性质。其中一种技巧是使用作为**循环神经网络**（**RNN**）表示的策略。通过使用 RNN，代理在确定当前行动时，也可以考虑过去的状态。本书后续将讨论将
    RNN 作为 RL 策略的使用。
- en: In the context of MDPs, the probability of the next state given the current
    one, ![6](img/B16182_02_04a.png) is referred to as a transition function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MDP 的背景下，给定当前状态下一个状态的概率，![6](img/B16182_02_04a.png) 被称为过渡函数。
- en: 'If the state space is finite, composed of *N* states, we can arrange the transition
    functions evaluated for each couple of states in an N x N matrix, where the sum
    of all the columns is 1, as we are summing a probability distribution over transition
    function elements:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态空间是有限的，由 *N* 个状态组成，我们可以将为每一对状态计算出的过渡函数排列在一个 N x N 的矩阵中，其中所有列的和为 1，因为我们是在对过渡函数元素的概率分布求和：
- en: '![Figure 2.5: Transition probability matrix'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：过渡概率矩阵'
- en: '](img/B16182_02_05.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_05.jpg)'
- en: 'Figure 2.5: Transition probability matrix'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：过渡概率矩阵
- en: In the rows, we have the source states, and in the columns, we have the destination
    states.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 行表示源状态，列表示目标状态。
- en: 'The probability matrix summarizes the transition function. It can be read as
    follows: ![7](img/B16182_02_05a.png) is the probability of landing in state ![8](img/B16182_02_05b.png)
    starting from state ![9](img/B16182_02_05c.png).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 概率矩阵总结了过渡函数。它可以按以下方式读取：![7](img/B16182_02_05a.png) 是从状态 ![9](img/B16182_02_05c.png)
    开始，落入状态 ![8](img/B16182_02_05b.png) 的概率。
- en: Markov Chains
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫链
- en: 'An MC, or, simply, a Markov process, is defined as a tuple of state space ![10](img/B16182_02_05d.png)
    and transition function ![12](img/B16182_02_05e.png). The state space, together
    with the transition function, defines a memory-less sequence of random states,
    ![11](img/B16182_02_05f.png), satisfying the Markov property. A sample from a
    Markov process is simply a sequence of states, which is also called an episode
    in the context of RL:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链（MC），或者简而言之，马尔可夫过程，被定义为状态空间 ![10](img/B16182_02_05d.png) 和过渡函数 ![12](img/B16182_02_05e.png)
    的元组。状态空间与过渡函数一起定义了一个无记忆的随机状态序列，![11](img/B16182_02_05f.png)，满足马尔可夫性质。从马尔可夫过程中的一个样本简单来说就是一系列状态，这在
    RL 上下文中也被称为一个回合：
- en: '![Figure 2.6: MC with three states'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6：具有三个状态的马尔可夫链'
- en: '](img/B16182_02_06.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_06.jpg)'
- en: 'Figure 2.6: MC with three states'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：具有三个状态的马尔可夫链
- en: Consider the preceding MC. As you can see, we have three states represented
    by circles. The probability function evaluated for the state pairs is reported
    on the edges connecting the different states. Looking at the edges starting from
    each state, we can see that the sum of the probabilities associated with each
    edge is 1, as it defines a probability distribution. The transition function for
    a couple of states that are not linked by an edge is 0.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前述的MC。正如您所见，我们有三个用圆圈表示的状态。针对连接不同状态的边缘报告的概率函数被评估为状态对。查看从每个状态开始的边缘，我们可以看到与每个边缘相关联的概率之和为1，因为它定义了概率分布。对于未通过边缘连接的一对状态的转移函数为0。
- en: 'The transition function can be arranged in a matrix, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 转移函数可以按矩阵方式排列，如下所示：
- en: '![Figure 2.7: Transition matrix for the MC in Figure 2.6'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7：图 2.6中MC的转移矩阵'
- en: '](img/B16182_02_07.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_07.jpg)'
- en: 'Figure 2.7: Transition matrix for the MC in Figure 2.6'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：图 2.6中MC的转移矩阵
- en: The matrix form of the transition function is very convenient from a programming
    perspective as it allows us to perform calculations easily.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程角度来看，转移函数的矩阵形式非常方便，因为它使我们能够轻松进行计算。
- en: Markov Reward Processes
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫奖励过程
- en: An MRP is an MC with values associated with state transitions, called rewards.
    The reward function evaluates how useful it is to transition from one state to
    another.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MRP是具有与状态转换相关联的值的MC，称为奖励。奖励函数评估从一个状态转移到另一个状态的实用性。
- en: 'An MRP is a tuple of ![15](img/B16182_02_07a.png) such that the following is
    true:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MRP是满足 ![15](img/B16182_02_07a.png) 的元组，使以下条件成立：
- en: S is a finite set of states.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S是有限状态集。
- en: P is the transition probability, where ![16](img/B16182_02_07b.png) is the probability
    of transitioning from state ![17](img/B16182_02_07c.png) to state ![18](img/B16182_02_07d.png).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P是转移概率，其中 ![16](img/B16182_02_07b.png) 是从状态 ![17](img/B16182_02_07c.png) 过渡到状态
    ![18](img/B16182_02_07d.png) 的概率。
- en: R is a reward function, where ![19](img/B16182_02_07e.png) is the reward associated
    with the transition from state ![21](img/B16182_02_07f.png) to state ![20](img/B16182_02_07g.png).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R是奖励函数，其中 ![19](img/B16182_02_07e.png) 是从状态 ![21](img/B16182_02_07f.png) 过渡到状态
    ![20](img/B16182_02_07g.png) 的奖励。
- en: '![21](img/B16182_02_07h.png) is the discount factor associated with future
    rewards, ![22](img/B16182_02_07i.png):'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![21](img/B16182_02_07h.png) 是与未来奖励相关的折扣因子， ![22](img/B16182_02_07i.png)：'
- en: '![Figure 2.8: An example of an MRP'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：MRP的一个示例'
- en: '](img/B16182_02_08.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_08.jpg)'
- en: 'Figure 2.8: An example of an MRP'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：MRP的一个示例
- en: As you can see in the previous figure, the rewards are represented by `r` and
    are associated with state transitions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的图中所见，奖励由 `r` 表示，并与状态转换相关联。
- en: Let's consider the MRP in *Figure 2.8*. The highest reward (`10`) is associated
    with transitions *1->3* and the self-loop, *3->3*. The lowest reward is associated
    with transitions *3->2*, and it is equal to `-1`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑图 2.8 中的MRP。最高奖励（`10`）与转换 *1->3* 和自环 *3->3* 相关联。最低奖励与转换 *3->2* 相关，等于 `-1`。
- en: In an MRP, it is possible to calculate the discounted return as the cumulative
    sum of discounted rewards.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在MRP中，可以计算折扣回报作为折扣奖励的累积和。
- en: In this context, we use the term "trajectory" or "episode" to denote a sequence
    of states traversed by the process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用术语“轨迹”或“剧集”来表示过程遍历的状态序列。
- en: Let's now calculate the discounted return for a given trajectory; for example,
    the trajectory of 1-2-3-3-3 with discount factor ![23](img/B16182_02_08a.png).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算给定轨迹的折扣回报；例如，带有折扣因子 ![23](img/B16182_02_08a.png) 的1-2-3-3-3轨迹。
- en: 'The discounted return is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣回报如下：
- en: '![Figure 2.9: Discounted return for the trajectory of 1-2-3-3-3'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：1-2-3-3-3轨迹的折扣回报'
- en: '](img/B16182_02_09.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_09.jpg)'
- en: 'Figure 2.9: Discounted return for the trajectory of 1-2-3-3-3'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：1-2-3-3-3轨迹的折扣回报
- en: 'We can also calculate the discounted return for a different trajectory, for
    example, 1-3-3-3-3:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算不同轨迹的折扣回报，例如1-3-3-3-3：
- en: '![Figure 2.10: Discounted return for the trajectory of 1-3-3-3-3'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10：1-3-3-3-3轨迹的折扣回报'
- en: '](img/B16182_02_10.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_10.jpg)'
- en: 'Figure 2.10: Discounted return for the trajectory of 1-3-3-3-3'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：1-3-3-3-3轨迹的折扣回报
- en: In this example, the second trajectory is more convenient than the first one,
    having a higher return. This means that the associated path is better in comparison
    to the first one. The return does not represent an absolute feature of a trajectory;
    it represents the relative goodness with respect to the other trajectories. Trajectory
    returns of different MRPs are not comparable to each other.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，第二个轨迹比第一个轨迹更方便，具有更高的回报。这意味着相应的路径比第一个路径更好。回报并不代表轨迹的绝对特征；它表示相对于其他轨迹的相对优越性。不同MRP的轨迹回报不能相互比较。
- en: 'Considering an MRP composed of N states, the reward function can be represented
    in an N x N matrix, similar to the transition matrix:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑由N个状态组成的MRP，奖励函数可以表示为一个N x N矩阵，类似于转移矩阵：
- en: '![Figure 2.11: Reward matrix'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.11：奖励矩阵'
- en: '](img/B16182_02_11.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_11.jpg)'
- en: 'Figure 2.11: Reward matrix'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：奖励矩阵
- en: In the rows, we represent the source states, and in the columns, we represent
    the destination states.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在行中，我们表示源状态，在列中，我们表示目标状态。
- en: 'The reward matrix can be read as follows: ![24](img/B16182_02_11a.png) is the
    reward associated with the state transition, ![25](img/B16182_02_11b.png).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励矩阵可以按如下方式读取：![24](img/B16182_02_11a.png)是与状态转移相关的奖励，![25](img/B16182_02_11b.png)。
- en: 'For the example in *Figure 2.11*, the reward function arranged in a matrix
    is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*图2.11*中的示例，按矩阵排列的奖励函数如下所示：
- en: '![Figure 2.12: Reward matrix for the MRP example'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.12：MRP示例的奖励矩阵'
- en: '](img/B16182_02_12.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_12.jpg)'
- en: 'Figure 2.12: Reward matrix for the MRP example'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12：MRP示例的奖励矩阵
- en: When a reward is not specified, we assume that the reward is `0`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当奖励未指定时，我们假设奖励为`0`。
- en: 'Using Python and NumPy, we can represent the transition matrix in this way:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python和NumPy，我们可以以这种方式表示转移矩阵：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In a similar way, the reward matrix can be represented like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，奖励矩阵可以这样表示：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are now ready to introduce the concepts of value functions and Bellman equations
    for MRPs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以引入MRP的价值函数和贝尔曼方程的概念。
- en: Value Functions and Bellman Equations for MRPs
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MRP的价值函数和贝尔曼方程
- en: The **value function** in an MRP evaluates the long-term value of a given state,
    intended as the expected return starting from that state. In this way, the value
    function expresses a preference over states. A state with a higher value in comparison
    to another state represents a better state – in other words, a state that it is
    more rewarding to be in.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MRP中的**价值函数**评估给定状态的长期价值，表示从该状态出发的预期回报。通过这种方式，价值函数表达了对状态的偏好。与另一个状态相比，具有更高价值的状态代表着一个更好的状态——换句话说，一个在其中待着更有回报的状态。
- en: 'Mathematically, the value function is formalized as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，价值函数的形式化表示如下：
- en: '![Figure 2.13: Expression for the value function'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.13：价值函数的表达式'
- en: '](img/B16182_02_13.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_13.jpg)'
- en: 'Figure 2.13: Expression for the value function'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：价值函数的表达式
- en: The value function of state ![ formula](img/B16182_02_13a.png) is represented
    by ![ formula](img/B16182_02_13b.png). The expectation on the right side of the
    equation is the expected value, represented by ![ formula](img/B16182_02_13c.png)
    of the return, ![ formula](img/B16182_02_13d.png), considering the fact that the
    current state is precisely equal to state ![ formula](img/B16182_02_13e.png) –
    the state for which we are evaluating the value function. The expectation is taken
    according to the transition function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 状态![公式](img/B16182_02_13a.png)的价值函数表示为![公式](img/B16182_02_13b.png)。方程右侧的期望是回报的期望值，表示为![公式](img/B16182_02_13c.png)，考虑到当前状态恰好等于状态![公式](img/B16182_02_13e.png)——我们正在评估该状态的价值函数。期望是根据转移函数计算的。
- en: 'The value function can be decomposed into two parts by considering the immediate
    reward and the discounted value function of the successor state:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑即时奖励和后继状态的折扣价值函数，价值函数可以分解为两部分：
- en: '![Figure 2.14: Decomposition of the value function'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.14：价值函数的分解'
- en: '](img/B16182_02_14.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_14.jpg)'
- en: 'Figure 2.14: Decomposition of the value function'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14：价值函数的分解
- en: The last equation is a recursive equation, known as the **Bellman expectation
    equation for MRPs**, in which the value function of given states depends on the
    value function of the successor states.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一方程是一个递归方程，称为**MRP的贝尔曼期望方程**，其中给定状态的价值函数依赖于后继状态的价值函数。
- en: To highlight the dependency of the equation on the transition function, we can
    rewrite the expectation as a summation of the possible states weighted by the
    transition probability. We define with ![ formula](img/B16182_02_14a.png) the
    expectation of the reward function in state ![ formula](img/B16182_02_14b.png),
    which can also be defined as the average reward.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出方程对转移函数的依赖性，我们可以将期望重写为可能状态的加权求和，权重由转移概率决定。我们使用 ![公式](img/B16182_02_14a.png)
    定义状态 ![公式](img/B16182_02_14b.png) 中的奖励函数期望，也可以将其定义为平均奖励。
- en: 'We can write ![ formula](img/B16182_02_14c.png) in the following ways:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 ![公式](img/B16182_02_14c.png) 写成以下几种形式：
- en: '![Figure 2.15: Expression for the expectation of the reward function in state
    s'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：状态 s 中奖励函数期望的表达式'
- en: '](img/B16182_02_15.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_15.jpg)'
- en: 'Figure 2.15: Expression for the expectation of the reward function in state
    s'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：状态 s 中奖励函数期望的表达式
- en: 'We can now rewrite the value function in a more convenient way:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以以更便捷的方式重写价值函数：
- en: '![Figure 2.16: Revised expression for the expectation of the value function
    in state s'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16：状态 s 中价值函数期望的修正表达式'
- en: '](img/B16182_02_16.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_16.jpg)'
- en: 'Figure 2.16: Revised expression for the expectation of the value function in
    state s'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16：状态 s 中价值函数期望的修正表达式
- en: 'This expression can be translated into code, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式可以转换为如下的代码：
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we calculated the expected reward for each state by multiplying
    element-wise the probability matrix and the reward matrix. Please note that the
    `keepdims` parameter is required to obtain a column vector.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们通过逐元素相乘概率矩阵和奖励矩阵，计算了每个状态的期望奖励。请注意，`keepdims` 参数是必须的，用来获得列向量。
- en: 'This formulation makes it possible to rewrite the Bellman equation using matrix
    notation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式使得我们可以使用矩阵表示法重写贝尔曼方程：
- en: '![Figure 2.17: Matrix form of the Bellman equation'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17：贝尔曼方程的矩阵形式'
- en: '](img/B16182_02_17.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_17.jpg)'
- en: 'Figure 2.17: Matrix form of the Bellman equation'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17：贝尔曼方程的矩阵形式
- en: 'Here, `V` is a column vector with state values, ![a](img/B16182_02_17a.png)
    is the expected reward for each state, and `P` is the transition matrix:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`V` 是包含状态值的列向量，![a](img/B16182_02_17a.png) 是每个状态的期望奖励，`P` 是转移矩阵：
- en: '![Figure 2.18: Matrix form of the Bellman equation'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18：贝尔曼方程的矩阵形式'
- en: '](img/B16182_02_18.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_18.jpg)'
- en: 'Figure 2.18: Matrix form of the Bellman equation'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18：贝尔曼方程的矩阵形式
- en: 'Using matrix notation, it is also possible to solve the Bellman equation for
    `V`, finding the value function associated with each state:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵表示法，也可以求解贝尔曼方程，得到与每个状态关联的 `V` 值函数：
- en: '![Figure 2.19: Value function using the Bellman equation'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19：使用贝尔曼方程的价值函数'
- en: '](img/B16182_02_19.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_19.jpg)'
- en: 'Figure 2.19: Value function using the Bellman equation'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19：使用贝尔曼方程的价值函数
- en: Here, `I` is an identity matrix of size N x N, and `N` is the number of states
    in the MRP.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`I` 是大小为 N x N 的单位矩阵，`N` 是 MRP 中的状态数量。
- en: Solving Linear Systems of an Equation Using SciPy
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 SciPy 求解线性方程组
- en: SciPy ([https://github.com/scipy/scipy](https://github.com/scipy/scipy)) is
    a Python library used for scientific computing based on NumPy. SciPy offers, inside
    the `linalg` module (linear algebra), useful methods for solving systems of equations.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy ([https://github.com/scipy/scipy](https://github.com/scipy/scipy)) 是一个基于
    NumPy 的用于科学计算的 Python 库。SciPy 在 `linalg` 模块（线性代数）中提供了用于求解方程组的有用方法。
- en: In particular, we can use `linalg.solve(A, b)` to solve a system of equations
    in the form of ![ formula](img/B16182_02_19a.png). This is precisely the method
    we can use to solve the system ![ formula](img/B16182_02_19b.png), where ![ formula](img/B16182_02_19c.png)
    is the matrix, `A`; `V` is the vector of variables, `x`; and ![ formula](img/B16182_02_19d.png)
    is the vector, `b`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们可以使用 `linalg.solve(A, b)` 来求解形式为 ![公式](img/B16182_02_19a.png) 的方程组。这正是我们可以用来求解系统
    ![公式](img/B16182_02_19b.png) 的方法，其中 ![公式](img/B16182_02_19c.png) 是矩阵 `A`，`V` 是变量向量
    `x`，而 ![公式](img/B16182_02_19d.png) 是向量 `b`。
- en: 'When translated into code, it should look like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为代码时，应该如下所示：
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, we have declared the elements of the Bellman equation and are
    using `scipy.linalg` to calculate the `value` function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经声明了贝尔曼方程的各个元素，并使用 `scipy.linalg` 来计算 `value` 函数。
- en: Let's now strengthen our understanding further by completing an exercise.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过完成一个练习来进一步加强我们的理解。
- en: 'Exercise 2.01: Finding the Value Function in an MRP'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习2.01：在MRP中找到值函数
- en: 'In this exercise, we are going to solve the Bellman expectation equation by
    finding the value function for the MRP in the following figure. We will use `scipy`
    and the `linalg` module to solve the linear equation presented in the previous
    section. We will also demonstrate how to define a transition probability matrix
    and how to calculate the expected reward for each state:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过找到以下图中MRP的值函数来解决Bellman期望方程。我们将使用`scipy`和`linalg`模块来解决前一节中提出的线性方程。我们还将演示如何定义转移概率矩阵以及如何计算每个状态的预期奖励：
- en: '![Figure 2.20: Example of an MRP with three states'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.20: Example of an MRP with three states'
- en: '](img/B16182_02_20.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_20.jpg)'
- en: 'Figure 2.20: Example of an MRP with three states'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20：具有三个状态的MRP示例
- en: 'Import the required NumPy and SciPy packages:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的NumPy和SciPy包：
- en: '[PRE4]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the transition probability matrix:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义转移概率矩阵：
- en: '[PRE5]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should obtain the following output:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该获得以下输出：
- en: '[PRE6]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's check the correctness of the matrix. The probability of going from state
    1 to state 1 is ![ formula](img/B16182_02_20a.png). This is correct as there are
    no self-loops in state 1\. The probability of going from state 1 to state 2 is
    ![ formula](img/B16182_02_20b.png) as it's the probability associated with edge
    *1->2*. This can be done for all elements of the transition matrix. Note that,
    here, the transition matrix elements are indexed by the state, not by their position
    in the matrix. This means that with ![ formula](img/B16182_02_20c.png), we refer
    to element 0,0 of the matrix.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们检查矩阵的正确性。从状态1到状态1的转移概率是![ formula](img/B16182_02_20a.png)。由于状态1没有自环，这是正确的。从状态1到状态2的转移概率是![
    formula](img/B16182_02_20b.png)，因为它是与边缘*1->2*相关联的概率。这可以应用于转移矩阵的所有元素。请注意，在这里，转移矩阵的元素按状态而不是矩阵中的位置索引。这意味着对于![
    formula](img/B16182_02_20c.png)，我们指的是矩阵的0,0元素。
- en: 'Check that the sum of all the columns is exactly equal to `1`, being a probability
    matrix:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查所有列的总和是否完全等于`1`，作为概率矩阵：
- en: '[PRE7]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `assert` function is used to ensure that a particular condition will return
    `true`. In this case, the `assert` function will make sure that the sum of all
    the columns is exactly `1`.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`assert`函数用于确保特定条件返回`true`。在这种情况下，`assert`函数将确保所有列的总和恰好为`1`。'
- en: 'We can calculate the expected immediate reward for each state using the reward
    matrix and the transition probability matrix:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用奖励矩阵和转移概率矩阵来计算每个状态的预期即时奖励：
- en: '[PRE8]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should obtain the following column vector:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该获得以下列向量：
- en: '[PRE9]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `R_expected` vector is the expected immediate reward for each state. State
    1 has an expected reward of `3.7`, which is exactly equal to *0.7 * 1 + 0.3*10*.
    The same logic applies to state 2 and state 3.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`R_expected`向量是每个状态的预期即时奖励。状态1的预期奖励为`3.7`，这恰好等于*0.7 * 1 + 0.3 * 10*。状态2和状态3也适用相同的逻辑。'
- en: 'Now we need to define `gamma`, and we are ready to solve the Bellman equation
    as a linear equation, ![ formula](img/B16182_02_20d.png). We have ![ formula](img/B16182_02_20e.png)
    and ![ formula](img/B16182_02_20f.png):'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要定义`gamma`，并且准备将Bellman方程作为线性方程求解，![ formula](img/B16182_02_20d.png)。我们有![
    formula](img/B16182_02_20e.png)和![ formula](img/B16182_02_20f.png)：
- en: '[PRE10]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should obtain the following output:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该获得以下输出：
- en: '[PRE11]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The vector, `V`, represents the value for each state. State 3 has the highest
    value (`77.58`). This means that state 3 is the state providing the highest expected
    return. It is the best state in this MRP. Intuitively, state 3 is the best state
    because, with a high probability (0.9), the transition brings the agent to the
    same state, and the reward associated with the transition is high (+10).
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 向量`V`表示每个状态的值。状态3具有最高的值（`77.58`）。这意味着状态3提供了最高的预期回报。在这个MRP中，状态3是最佳状态。直觉上，状态3是最佳状态，因为转移高概率（0.9）会将代理带到同一状态，并且与转移相关联的奖励很高（+10）。
- en: Note
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37o5ZH4](
    https://packt.live/37o5ZH4).
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/37o5ZH4]( https://packt.live/37o5ZH4)。
- en: You can also run this example online at [https://packt.live/3dU8cfW](https://packt.live/3dU8cfW).
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/3dU8cfW](https://packt.live/3dU8cfW)上在线运行此示例。
- en: In this exercise, we solved the Bellman equation for an MRP by finding the state
    values for our toy problem. The state values describe quantitatively the benefit
    of being in each state. We described the MRP in terms of a transition probability
    matrix and a reward matrix. These two matrices permit us to solve the linear system
    associated with the Bellman equation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们通过为我们的玩具问题求解贝尔曼方程，找到 MRPs 的状态值。状态值定量描述了处于每个状态时的收益。我们通过转移概率矩阵和奖励矩阵来描述
    MRP。这两个矩阵使我们能够求解与贝尔曼方程相关的线性系统。
- en: Note
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The computational complexity of the solution of the Bellman equation is `O(n`3`)`
    ; it is cubic in the number of states. Therefore, it is only possible for small
    MRPs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程的计算复杂度是 `O(n^3)`；它与状态数量的立方成正比。因此，只适用于小型 MRP。
- en: In the next section, we will consider an active agent that can perform actions,
    thus arriving at the description of an MDP.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将考虑一个可以执行动作的主动智能体，从而得出 MDP 的描述。
- en: Markov Decision Processes
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（Markov Decision Processes）
- en: An MDP is an MRP with decisions. In this context, we have a set of actions available
    to an agent that can condition the transition probability to the next state. While,
    in MRPs, the transition probability depends only on the state of the environment,
    in MDPs, the agent can perform actions influencing the transition probability.
    In this way, the agent becomes an active entity in the framework, interacting
    with the environment through actions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 是带有决策的 MRP。在这种情况下，我们有一组可供智能体执行的动作，这些动作可以影响到下一个状态的转移概率。与 MRP 中转移概率仅依赖于环境状态不同，在
    MDP 中，智能体可以执行影响转移概率的动作。通过这种方式，智能体成为框架中的主动实体，通过行动与环境互动。
- en: 'Formally, an MDP is a tuple, ![ formula](img/B16182_02_20g.png), in which the
    following is true:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，MDP 是一个元组， ![ formula](img/B16182_02_20g.png)，其中以下条件成立：
- en: '![ formula](img/B16182_02_20h.png) is the set of states.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_20h.png) 是状态集。'
- en: '![ formula](img/B16182_02_20i.png) is the set of actions.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_20i.png) 是动作集。'
- en: '![b](img/B16182_02_20j.png) is the reward function, ![ formula](img/B16182_02_20k.png).
    ![ formula](img/B16182_02_20l.png) is the expected reward resulting in action
    ![ formula](img/B16182_02_20m.png) and state ![ formula](img/B16182_02_20n.png).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![b](img/B16182_02_20j.png) 是奖励函数，![ formula](img/B16182_02_20k.png)。![ formula](img/B16182_02_20l.png)
    是在执行动作![ formula](img/B16182_02_20m.png) 和处于状态![ formula](img/B16182_02_20n.png)时的期望奖励。'
- en: '![ formula](img/B16182_02_20o.png) is the transition probability function in
    which ![ formula](img/B16182_02_20p.png) is the probability of landing in state
    ![ formula](img/B16182_02_20q.png) starting from the current state, ![b](img/B16182_02_20r.png),
    and performing an action, ![ formula](img/B16182_02_20s.png).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_20o.png) 是转移概率函数，其中![ formula](img/B16182_02_20p.png)
    是从当前状态![b](img/B16182_02_20r.png)出发，执行动作![ formula](img/B16182_02_20s.png)，并且到达状态![
    formula](img/B16182_02_20q.png)的概率。'
- en: '![ formula](img/B16182_02_20t.png) is the discount factor associated with future
    rewards, ![ formula](img/B16182_02_20u.png).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_20t.png) 是与未来奖励相关的折扣因子，![ formula](img/B16182_02_20u.png)。'
- en: 'The difference between an MRP and an MDP is the fact that the agent has at
    its disposal a set of actions from which it can choose to condition the transition
    probability to have a higher possibility of landing in good states. If an MRP
    and MC are only a description of Markov processes without an objective, an MDP
    contains the concept of a policy and a goal. In an MDP, the agent should take
    decisions about which action to take, maximizing the discounted return:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: MRP 和 MDP 之间的区别在于，智能体可以选择一组动作来调节转移概率，从而提高进入良好状态的可能性。如果 MRP 和 MC 只是对马尔可夫过程的描述，而没有目标，那么
    MDP 就包含了策略和目标的概念。在 MDP 中，智能体需要对采取何种行动做出决策，以最大化折扣后的回报：
- en: '![Figure 2.21: A student MDP'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.21: 学生 MDP'
- en: '](img/B16182_02_21.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_21.jpg)'
- en: 'Figure 2.21: A student MDP'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：学生 MDP
- en: '*Figure 2.21* is an example of an MDP representing the day of a university
    student. There are six possible states: `Class 1`, `Class 2`, `Class 3`, `Social`,
    `Bed`, and `Pub`. The edges between the states represent state transitions. On
    the edges, we have the action and the reward, denoted by `r`. Possible actions
    are `Study`, `Social`, `Beer`, and `Sleep`. The initial state, represented by
    the incoming arrow, is `Class 1`. The goal of the student is to select the best
    actions in each state, maximizing their return.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.21* 是一个代表大学生一天活动的MDP示例。共有六个可能的状态：`Class 1`、`Class 2`、`Class 3`、`Social`、`Bed`
    和 `Pub`。状态之间的边表示状态转换。边上标注了动作和奖励，奖励用`r`表示。可能的动作有`Study`、`Social`、`Beer` 和 `Sleep`。初始状态由输入箭头表示，为`Class
    1`。学生的目标是在每个状态中选择最佳动作，从而最大化他们的回报。'
- en: In the following paragraphs, we will discuss some possible strategies for this
    MDP.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我们将讨论这个MDP的一些可能策略。
- en: A student agent starts from `Class 1`. They can decide to study and complete
    all of the lessons. Each study decision comes with a small negative reward, `-2`.
    If the student decides to sleep after `Class 3`, they will land in the absorbing
    state, `Bed`, with a high positive reward of `+10`. This represents a very common
    situation in daily routines. You have to sacrifice some immediate reward in order
    to obtain a higher reward in the future. In this case, by deciding to study in
    `Class 1` and `2`, you obtain a negative reward but are compensated by the positive
    reward after `Class 3`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一名学生代理从`Class 1`开始。他们可以决定学习并完成所有课程。每次学习的决策都会带来一个小的负奖励，`-2`。如果学生在`Class 3`之后决定睡觉，他们将进入吸收状态`Bed`，并获得一个高的正奖励`+10`。这代表了日常生活中非常常见的情况：为了获得未来更高的回报，你必须牺牲一些即时的奖励。在这种情况下，通过决定在`Class
    1`和`Class 2`学习，虽然获得了负奖励，但在`Class 3`之后通过高额的正奖励得到了补偿。
- en: Another possible strategy in this MDP is to select a `Social` action right after
    the `Class 1` state. This action comes with a small negative reward. The student
    can continue doing the same action, and each time they get the same reward. The
    student can also decide to `Study` from the `Social` state (notice that `Social`
    is both a state and an action) by returning to `Class 1`. Feeling guilty, in `Class
    1`, the student can decide to study. After having studied a bit, they may feel
    tired and decide to sleep for a little while, ending up in the `Bed` state. Having
    performed the `Social` action, the agent has cumulated a negative return.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这个MDP中另一种可能的策略是在`Class 1`状态后立刻选择`Social`行动。这个行动带来一个小的负奖励。学生可以继续做同样的动作，每次都会获得相同的奖励。学生还可以选择从`Social`状态（注意，`Social`既是一个状态，也是一个行动）回到`Class
    1`继续学习。在`Class 1`感到内疚时，学生可以决定继续学习。学习一会儿后，他们可能会感到疲倦并决定稍微休息，最终进入`Bed`状态。执行了`Social`行动后，代理已累积了负回报。
- en: 'Let''s evaluate the possible strategies for this example. We will assume a
    discount factor of ![ formula](img/B16182_02_21a.png), that is, no discount:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估一下这个例子的可能策略。我们假设折扣因子为![公式](img/B16182_02_21a.png)，即无折扣：
- en: 'Strategy: Good student. The good student strategy was the first strategy that
    was described. Supposing the student will end in `Class 1`, they can perform the
    following actions: `Study`, `Study`, and `Study`. The associated sequence of states
    is thus `Class 1`, `Class 2`, `Class 3`, and `Sleep`. The associated return is,
    therefore, the sum of the rewards along the trajectory:![Figure 2.22: Return for
    the good student'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '策略：优秀学生。优秀学生策略是第一个描述的策略。假设学生最终会回到`Class 1`，他们可以执行以下动作：`Study`、`Study`、`Study`。因此，相关的状态序列是`Class
    1`、`Class 2`、`Class 3`和`Sleep`。因此，相关的回报是沿着轨迹奖励的总和：![Figure 2.22: 优秀学生的回报'
- en: '](img/B16182_02_22.jpg)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_02_22.jpg)'
- en: 'Figure 2.22: Return for the good student'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22：优秀学生的回报
- en: 'Strategy: Social student. The social student strategy is the second strategy
    described. The student can perform the following actions: `Social`, `Social`,
    `Social`, `Study`, `Study`, and `Sleep`. The associated sequence of states is
    `Class 1`, `Social`, `Social`, `Social`, `Class 1`, `Class 2`, and `Bed`. The
    associated return is, in this case, as follows:![Figure 2.23: Return for the social
    student'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '策略：社交学生。社交学生策略是第二种描述的策略。学生可以执行以下动作：`Social`、`Social`、`Social`、`Study`、`Study`
    和 `Sleep`。因此，相关的状态序列是`Class 1`、`Social`、`Social`、`Social`、`Class 1`、`Class 2`
    和 `Bed`。在这种情况下，相关的回报如下：![Figure 2.23: 社交学生的回报'
- en: '](img/B16182_02_23.jpg)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_02_23.jpg)'
- en: 'Figure 2.23: Return for the social student'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.23：社交学生的回报
- en: By looking at the associated return, we can see that the good student strategy
    is a better strategy in comparison to the social student strategy, having a higher
    return.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看相关回报，我们可以看到，优秀学生策略相比社交学生策略具有更高的回报，因此是一个更好的策略。
- en: 'The question you may ask at this point is how can an agent decide which action
    to take in order to maximize the return? To answer the question, we need to introduce
    two useful functions: the state-value function and the action-value function.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你此时可能会问，智能体如何决定采取哪个动作以最大化回报呢？为了回答这个问题，我们需要引入两个有用的函数：状态值函数和动作值函数。
- en: The State-Value Function and the Action-Value Function
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态值函数和动作值函数
- en: In the context of MDPs, we can define a function by evaluating how good it is
    to be in a given state. However, we should take into account the agent's policy,
    as it defines the agent's decisions and conditions the probability over trajectories,
    that is, the sequence of future states. So, the value function depends on the
    agent policy, ![ formula](img/B16182_02_23a.png).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP的背景下，我们可以通过评估在某个给定状态下有多好的状态来定义一个函数。然而，我们应该考虑到智能体的策略，因为策略定义了智能体的决策并调节了轨迹上的概率，即未来状态的序列。因此，值函数依赖于智能体的策略，![公式](img/B16182_02_23a.png)。
- en: 'The `s` and follows the policy, ![ formula](img/B16182_02_23c.png):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`s`并遵循策略，![公式](img/B16182_02_23c.png)：'
- en: '![Figure 2.24: Definition of the state-value function'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.24：状态值函数的定义'
- en: '](img/B16182_02_24.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_24.jpg)'
- en: 'Figure 2.24: Definition of the state-value function'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.24：状态值函数的定义
- en: In MDPs, we are also interested in defining the benefit of taking an action
    in a given state. This function is called the action-value function.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP（马尔科夫决策过程）中，我们还希望定义在给定状态下执行某个动作的收益。这个函数称为动作值函数。
- en: 'The **action-value function**, ![ formula](img/B16182_02_24a.png), (also called
    the q-function), can be termed as the expected return starting from state ![ formula](img/B16182_02_24b.png),
    which takes action ![ formula](img/B16182_02_24c.png) and follows the policy ![
    formula](img/B16182_02_24d.png):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作值函数**，![公式](img/B16182_02_24a.png)，（也称为q函数），可以定义为从状态![公式](img/B16182_02_24b.png)开始，执行动作![公式](img/B16182_02_24c.png)并遵循策略![公式](img/B16182_02_24d.png)时的期望回报：'
- en: '![Figure 2.25: Definition of the action-value function'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.25：动作值函数的定义'
- en: '](img/B16182_02_25.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_25.jpg)'
- en: 'Figure 2.25: Definition of the action-value function'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.25：动作值函数的定义
- en: The state-value function, as we will learn later in the book, provides information
    that is only useful when it comes to evaluating a policy. The action-value function
    also provides information about control, that is, for selecting an action in a
    state.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们将在书中稍后学习的那样，状态值函数提供的信息仅在评估策略时有用。动作值函数则提供关于控制的信息，也就是在状态中选择动作的信息。
- en: Suppose that we know the action-value function for an MDP. If we are in given
    state, ![a](img/B16182_02_25a.png), which action would be the best one?
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们知道MDP的动作值函数。如果我们处于给定状态![a](img/B16182_02_25a.png)，哪个动作是最佳选择？
- en: 'Well, the best action is the one that yields the highest discounted return.
    The action-value function measures the discounted return that is obtained by starting
    from a state and performing an action. In this way, the action-value function
    provides an ordering (or a preference) over the actions in a state. The best action
    to perform is the one with the highest q-function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，最佳动作是产生最高折扣回报的动作。动作值函数衡量的是从某个状态开始并执行一个动作所获得的折扣回报。通过这种方式，动作值函数为状态中的动作提供了一个排序（或偏好）。要执行的最佳动作是具有最高q值的动作：
- en: '![Figure 2.26: Best action using the action-value function'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.26：使用动作值函数选择最佳动作'
- en: '](img/B16182_02_26.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_26.jpg)'
- en: 'Figure 2.26: Best action using the action-value function'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26：使用动作值函数选择最佳动作
- en: Note that, in this case, we are only doing a one-step optimization of the current
    policy; that is, we are modifying, possibly, the action in a given state under
    the assumption that the following actions are taken with the current policy. If
    we do this, we do not select the best action in this state, but we select the
    best action under this policy.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，我们只是对当前策略进行一步优化；也就是说，我们可能会在假设后续动作遵循当前策略的前提下，修改某个给定状态下的动作。如果我们这样做，我们不会在这个状态下选择最佳动作，而是选择在此策略下的最佳动作。
- en: 'Just like in an MRP, in an MDP, the state-value function and the action-value
    function can be decomposed in a recursive way:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 MRP 中一样，在 MDP 中，状态值函数和动作值函数也可以递归地分解：
- en: '![Figure 2.27: The state-value function in an MDP'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.27：MDP 中的状态值函数'
- en: '](img/B16182_02_27.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_27.jpg)'
- en: 'Figure 2.27: The state-value function in an MDP'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.27：MDP 中的状态值函数
- en: '![Figure 2.28: The action-value function in an MDP'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.28：MDP 中的动作值函数'
- en: '](img/B16182_02_28.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_28.jpg)'
- en: 'Figure 2.28: The action-value function in an MDP'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.28：MDP 中的动作值函数
- en: These equations are known as the Bellman expectation equations for MDPs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程被称为 MDP 的贝尔曼期望方程。
- en: Bellman expectation equations are recursive as the state-value function of a
    given state depends on the state-value function of another state. This is also
    true for the action-value function.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼期望方程是递归的，因为给定状态的状态值函数依赖于另一个状态的状态值函数。对于动作值函数来说也是如此。
- en: In the action-value function equation, the action, ![ formula](img/B16182_02_28a.png),
    for which we are evaluating the function, is an arbitrary action. It is not taken
    from the action distribution defined by the policy. Instead, the action, ![ formula](img/B16182_02_28b.png),
    taken in the following step, is taken according to the action distribution defined
    in state ![ formula](img/B16182_02_28c.png).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作值函数方程中，我们正在评估的动作，![ formula](img/B16182_02_28a.png)，是一个任意的动作。这个动作不是从策略定义的动作分布中获取的。相反，接下来一步采取的动作，![
    formula](img/B16182_02_28b.png)，是根据在状态![ formula](img/B16182_02_28c.png)下由动作分布定义的。
- en: 'Let''s rewrite the state-value function and the action-value function to highlight
    the contribution of the agent''s policy, ![ formula](img/B16182_02_28d.png):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写状态值函数和动作值函数，以突出代理策略的贡献，![ formula](img/B16182_02_28d.png)：
- en: '![Figure 2.29: The state-value function to highlight the policy contribution'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.29：突出显示策略贡献的状态值函数'
- en: '](img/B16182_02_29.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_29.jpg)'
- en: 'Figure 2.29: The state-value function to highlight the policy contribution'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.29：突出显示策略贡献的状态值函数
- en: 'Let''s analyze the two terms of the equation:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析方程的两个项：
- en: '![ formula](img/B16182_02_29a.png): This term is the expectation of the immediate
    rewards given the action distribution defined by the agent''s policy. Each immediate
    reward for a state-action pair is weighted by the probability of the action given
    the state, which is defined as ![ formula](img/B16182_02_29b.png).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_29a.png)：这个项是给定代理策略定义的动作分布时即时奖励的期望值。每个状态-动作对的即时奖励都会根据给定状态下的动作概率加权，这个概率定义为![
    formula](img/B16182_02_29b.png)。'
- en: '![ formula](img/B16182_02_29c.png) is the discounted expected value of the
    state-value function, given the state distribution defined by the transition function.
    Note that here the action, `a`, is defined by the agent''s policy. Being an expected
    value, every state value, ![ formula](img/B16182_02_29d.png), is weighed by the
    probability of the transition from state ![ formula](img/B16182_02_29e.png) to
    state ![ formula](img/B16182_02_29f.png), given the action, ![ formula](img/B16182_02_29g.png).
    This is represented by ![ formula](img/B16182_02_29h.png) .'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_29c.png)是给定转移函数定义的状态分布下，状态值函数的折扣期望值。请注意，这里的动作，`a`，是由代理的策略定义的。作为期望值，每个状态值，![
    formula](img/B16182_02_29d.png)，都会根据从状态![ formula](img/B16182_02_29e.png)到状态![
    formula](img/B16182_02_29f.png)的转移概率加权，给定动作![ formula](img/B16182_02_29g.png)。这由![
    formula](img/B16182_02_29h.png)表示。'
- en: 'The action-value function can be rewritten to highlight the dependency on the
    transition and value functions:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值函数可以重写，以突出它对转移函数和值函数的依赖：
- en: '![Figure 2.30: The action-value function, highlighting the dependency on the
    transition and value functions of the next state'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.30：动作值函数，突出显示下一个状态的转移函数和值函数的依赖性'
- en: '](img/B16182_02_30.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_30.jpg)'
- en: 'Figure 2.30: The action-value function, highlighting the dependency on the
    transition and value functions of the next state'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.30：动作值函数，突出显示下一个状态的转移函数和值函数的依赖性
- en: The action-value function, therefore, is given by the summation of the immediate
    reward and the expected value of the state-value function of the successor state
    under the environment dynamic (`P`).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，动作值函数是立即奖励和后继状态的状态值函数在环境动态（`P`）下的期望值之和。
- en: 'By comparing the two equations, we obtain an important relationship between
    the state value and the action value:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较这两个方程，我们可以得到状态值和动作值之间的重要关系：
- en: '![Figure 2.31: Expression for the state-value function, in terms of the action-value
    function'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.31：基于动作值函数的状态值函数表达式'
- en: '](img/B16182_02_31.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_31.jpg)'
- en: 'Figure 2.31: Expression for the state-value function, in terms of the action-value
    function'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.31：基于动作值函数的状态值函数表达式
- en: In other words, the state-value state, ![ formula](img/B16182_02_31a.png), under
    the policy, ![ formula](img/B16182_02_31b.png), is the expected value of the action-value
    function under the actions selected by ![ formula](img/B16182_02_31c.png). Each
    action-value function is weighted by the probability of the action given the state.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，状态值状态 ![ formula](img/B16182_02_31a.png)，在策略下，![
- en: 'The state-value function can also be rewritten in matrix form, as in the MRP
    case:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值函数也可以以矩阵形式重写，如同在 MRP 情况下：
- en: '![Figure 2.32: Matrix form for the state-value function'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.32：状态值函数的矩阵形式'
- en: '](img/B16182_02_32.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_32.jpg)'
- en: 'Figure 2.32: Matrix form for the state-value function'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.32：状态值函数的矩阵形式
- en: 'There is a direct solution, as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个直接的解法，如下所示：
- en: '![Figure 2.33: Direct solution for the state values'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.33：状态值的直接解法'
- en: '](img/B16182_02_33.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_33.jpg)'
- en: 'Figure 2.33: Direct solution for the state values'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.33：状态值的直接解法
- en: 'Here, you can see the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到以下内容：
- en: '![ formula](img/B16182_02_33a.png) (column vector) is the expected value of
    the immediate reward induced by the policy for each state:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_33a.png)（列向量）是由策略引导的每个状态的即时奖励的期望值：'
- en: '![Figure 2.34: Expected immediate reward'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.34：期望即时奖励'
- en: '](img/B16182_02_34.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_34.jpg)'
- en: 'Figure 2.34: Expected immediate reward'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.34：期望即时奖励
- en: '![ formula](img/B16182_02_34a.png) is the column vector of the state values
    for each state.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_34a.png) 是每个状态的状态值列向量。'
- en: '![ formula](img/B16182_02_34b.png) is the transition matrix based on the action
    distribution. It is an ![ formula](img/B16182_02_34c.png) matrix, where ![ formula](img/B16182_02_34d.png)
    is the number of states in the MDP. Given two states, ![ formula](img/B16182_02_34e.png)
    and ![ formula](img/B16182_02_34f.png), we have the following:![Figure 2.35: Transition
    matrix conditioned on an action distribution'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![ formula](img/B16182_02_34b.png) 是基于动作分布的转移矩阵。它是一个![ formula](img/B16182_02_34c.png)
    矩阵，其中 ![ formula](img/B16182_02_34d.png) 是 MDP 中状态的数量。给定两个状态 ![ formula](img/B16182_02_34e.png)
    和 ![ formula](img/B16182_02_34f.png)，我们得到以下内容：![图 2.35：基于动作分布的转移矩阵'
- en: '](img/B16182_02_35.jpg)'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_02_35.jpg)'
- en: 'Figure 2.35: Transition matrix conditioned on an action distribution'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.35：基于动作分布的转移矩阵
- en: Therefore, the transition matrix is the probability of transitioning from state
    ![ formula](img/B16182_02_35a.png) to state ![ formula](img/B16182_02_35b.png)
    given the actions selected by the policy and the transition function defined by
    the MDP.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，转移矩阵是从状态 ![ formula](img/B16182_02_35a.png) 到状态 ![ formula](img/B16182_02_35b.png)
    的转移概率，这取决于策略选择的动作和由 MDP 定义的转移函数。
- en: 'Following the same steps, we can also find the matrix form of the action-value
    function:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 按照相同的步骤，我们也可以找到动作值函数的矩阵形式：
- en: '![Figure 2.36: Matrix form equation for the action-value function'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.36：动作值函数的矩阵形式方程'
- en: '](img/B16182_02_36.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_36.jpg)'
- en: 'Figure 2.36: Matrix form equation for the action-value function'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.36：动作值函数的矩阵形式方程
- en: Here, ![ formula](img/B16182_02_36a.png) is a column vector with ![ formula](img/B16182_02_36b.png)
    entries. ![ formula](img/B16182_02_36c.png) is the vector of immediate rewards
    with the same shape of ![ formula](img/B16182_02_36d.png). ![ formula](img/B16182_02_36e.png)
    is the transition matrix with a shape of ![ formula](img/B16182_02_36f.png) rows
    and ![ formula](img/B16182_02_36g.png) columns. ![ formula](img/B16182_02_36h.png)
    represents the state values for each state.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![ formula](img/B16182_02_36a.png) 是一个列向量，包含有![ formula](img/B16182_02_36b.png)
    个条目。![ formula](img/B16182_02_36c.png) 是即时奖励的向量，形状与![ formula](img/B16182_02_36d.png)
    相同。![ formula](img/B16182_02_36e.png) 是转移矩阵，形状为![ formula](img/B16182_02_36f.png)
    行和![ formula](img/B16182_02_36g.png) 列。![ formula](img/B16182_02_36h.png) 表示每个状态的状态值。
- en: 'The explicit form of ![b](img/B16182_02_36i.png) and ![ formula](img/B16182_02_36j.png)
    is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![b](img/B16182_02_36i.png) 和 ![ formula](img/B16182_02_36j.png) 的显式形式如下：'
- en: '![Figure 2.37: Explicit matrix form of the action-value function and the transition
    function'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.37：动作值函数和转移函数的显式矩阵形式'
- en: '](img/B16182_02_37.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_37.jpg)'
- en: 'Figure 2.37: Explicit matrix form of the action-value function and the transition
    function'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.37：动作值函数和转移函数的显式矩阵形式
- en: Here, the number of actions associated with state ![ formula](img/B16182_02_37a.png)
    is indicated by ![ formula](img/B16182_02_37b.png), thus ![ formula](img/B16182_02_37c.png).
    The number of actions of the MDP is obtained by summing up the actions associated
    with each state.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，状态 ![公式](img/B16182_02_37a.png) 相关的动作数量由 ![公式](img/B16182_02_37b.png) 表示，因此是
    ![公式](img/B16182_02_37c.png)。MDP 的动作数量是通过对每个状态关联的动作数量求和得出的。
- en: Let's now implement our understanding of the state- and action-value functions
    for our student MDP example. In this example, we will use the calculation of the
    state-value function and the action-value function for the student MDP in *Figure
    2.21*. We will consider the case of an undecided student, that is, a student with
    a random policy for each state. This means that the probability of each action
    for each state is exactly 0.5.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现对学生 MDP 示例的状态-和动作-值函数的理解。在这个例子中，我们将使用 *图 2.21* 中的状态值函数和动作值函数计算。我们将考虑一个尚未决定的学生，也就是对于每个状态都有随机策略的学生。这意味着每个状态的每个动作的概率正好是
    0.5。
- en: We will examine a different case for a myopic student in the following example.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的示例中考察一个近视学生的不同情况。
- en: 'Import the required libraries as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式导入所需的库：
- en: '[PRE12]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the environment properties:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 定义环境属性：
- en: '[PRE13]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`P_pi` contains the contribution of the transition matrix and the policy of
    the agent. `R` is the reward matrix.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`P_pi` 包含了转移矩阵和代理的策略的贡献。`R` 是奖励矩阵。'
- en: 'We will use the following state encoding:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下状态编码：
- en: '`0`: Class 1'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`: 类别 1'
- en: '`1`: Class 2'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`: 类别 2'
- en: '`2`: Class 3'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2`: 类别 3'
- en: '`3`: Social'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`: 社交'
- en: '`4`: Pub'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4`: 酒吧'
- en: '`5`: Bed'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`5`: 床'
- en: 'Create the transition matrix by considering a random policy:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 按照随机策略创建转移矩阵：
- en: '[PRE14]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Print `P_pi`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 打印 `P_pi`：
- en: '[PRE15]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE16]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the reward matrix, `R`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 创建奖励矩阵 `R`：
- en: '[PRE17]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Print `R`:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 打印 `R`：
- en: '[PRE18]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output will be as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE19]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Being a probability matrix, the sum of all the columns of `P_pi` should be
    `1`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个概率矩阵，`P_pi` 的所有列的总和应为 1：
- en: '[PRE20]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The assertion should be verified.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 应该验证该断言。
- en: 'We can now calculate the expected reward for each state, using `R` and `P_pi`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `R` 和 `P_pi` 来计算每个状态的期望奖励：
- en: '[PRE21]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The expected reward, in this case, is as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 期望奖励，在这种情况下如下所示：
- en: '[PRE22]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `R_expected` vector contains the expected immediate reward for each state.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`R_expected` 向量包含每个状态的期望即时奖励。'
- en: 'We are ready to solve the Bellman equation to find the value for each state.
    For this, we can use `scipy.linalg.solve`:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好求解贝尔曼方程，以找出每个状态的值。为此，我们可以使用 `scipy.linalg.solve`：
- en: '[PRE23]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The vector, `V`, contains the following values:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 `V` 包含以下值：
- en: '[PRE24]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This is the vector of the state values. State `0` has a value of `-1.7`, state
    `1` has a value of `4.4`, and so on:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这是状态值的向量。状态 `0` 的值为 `-1.7`，状态 `1` 的值为 `4.4`，以此类推：
- en: '![Figure 2.38: State values of the student MDP for'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.38：学生 MDP 的状态值](img/B16182_02_38.jpg)'
- en: '](img/B16182_02_38.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_38.jpg)'
- en: 'Figure 2.38: State values of the student MDP for ![b](img/B16182_02_38caption.png)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.38：学生 MDP 的状态值 ![b](img/B16182_02_38caption.png)
- en: 'Let''s examine how the results change with ![ formula](img/B16182_02_38a.png),
    which is the condition assumed for a myopic random student:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下结果是如何随着 ![公式](img/B16182_02_38a.png) 的变化而变化的，这是为近视随机学生假设的条件：
- en: '[PRE25]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE26]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The visual representation is as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 其视觉表示如下：
- en: '![Figure 2.39: State values of the student MDP for γ=0'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.39：γ=0 时学生 MDP 的状态值](img/B16182_02_39.jpg)'
- en: '](img/B16182_02_39.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_39.jpg)'
- en: 'Figure 2.39: State values of the student MDP for γ=0'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.39：学生 MDP 的状态值 γ=0
- en: As you can see, using ![ formula](img/B16182_02_39a.png), the value of each
    state is exactly equal to the expected immediate reward according to the policy.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用 ![公式](img/B16182_02_39a.png)，每个状态的值正好等于根据策略计算出的期望即时奖励。
- en: 'Now we can calculate the action-value function. We need to use a different
    form of immediate reward using a matrix with a shape of ![ formula](img/B16182_02_39b.png).
    Each row corresponds to a state-action pair, and the value is the immediate reward
    for that pair:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算动作值函数。我们需要使用一个不同形式的即时奖励，使用一个形状为 ![公式](img/B16182_02_39b.png) 的矩阵。每一行对应一个状态-动作对，值为该对的即时奖励：
- en: '![Figure 2.40: Immediate rewards'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.40：即时奖励](img/B16182_02_40.jpg)'
- en: '](img/B16182_02_40.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_40.jpg)'
- en: 'Figure 2.40: Immediate rewards'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.40：即时奖励
- en: 'Translate it into code as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式将其转换为代码：
- en: '[PRE27]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output will be as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE28]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now have to define the transition matrix of the student MDP. The transition
    matrix contains the probability of landing in a given state, starting from a state
    and an action. In the rows, we have the source state and action, and in the columns,
    we have the landing state:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要定义学生MDP的转移矩阵。转移矩阵包含了从一个状态和一个动作出发，到达给定状态的概率。在行中，我们有源状态和动作，在列中，我们有目标状态：
- en: '![Figure 2.41: Transition matrix of the student MDP'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.41：学生MDP的转移矩阵'
- en: '](img/B16182_02_41.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_41.jpg)'
- en: 'Figure 2.41: Transition matrix of the student MDP'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.41：学生MDP的转移矩阵
- en: 'When translating the probability transition matrix into code, you should see
    the following:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 当将概率转移矩阵转换为代码时，您应该看到以下内容：
- en: '[PRE29]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now calculate the action-value function using ![a](img/B16182_02_41a.png):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用![a](img/B16182_02_41a.png)来计算动作-价值函数：
- en: '[PRE30]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The action-value vector contains the following values:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 动作-价值向量包含以下值：
- en: '[PRE31]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`Q_sa_pi` is the action-value vector. For each state-action pair, we have the
    value of the action in that state. The action-value function is represented in
    the following figure. Action values are represented with ![ formula](img/B16182_02_41b.png):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`Q_sa_pi`是动作-价值向量。对于每一对状态-动作，我们都有该状态下的动作价值。动作-价值函数在下图中表示。动作价值通过![公式](img/B16182_02_41b.png)表示：'
- en: '![Figure 2.42: Action values for the student MDP'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.42：学生MDP的动作值'
- en: '](img/B16182_02_42.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_42.jpg)'
- en: 'Figure 2.42: Action values for the student MDP'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.42：学生MDP的动作值
- en: 'We are now interested in extracting the best action for each state:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在感兴趣的是为每个状态提取最佳动作：
- en: '[PRE32]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE33]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In this way, performing the `argmax` function, we obtain the index of the best
    action in each state:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行`argmax`函数，我们可以获得每个状态中最佳动作的索引：
- en: '[PRE34]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `best_actions` vector contains the following values:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`best_actions`向量包含以下值：'
- en: '[PRE35]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The best actions can be visualized as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳动作可以如下可视化：
- en: '![Figure 2.43: The student MDP best actions'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.43：学生MDP最佳动作'
- en: '](img/B16182_02_42.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_42.jpg)'
- en: 'Figure 2.43: The student MDP best actions'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.43：学生MDP最佳动作
- en: In *Figure 2.43*, the dotted arrows are the best actions in each state. We can
    easily find them by looking at the action maximizing the `q` function in each
    state.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 2.43*中，虚线箭头表示每个状态中的最佳动作。我们可以通过查看每个状态中最大化`q`函数的动作，轻松找到它们。
- en: 'From the action-value calculation, we can see that when ![ formula](img/B16182_02_43a.png),
    the action-value function is equal to the expected immediate reward:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从动作-价值计算中，我们可以看到当![公式](img/B16182_02_43a.png)时，动作-价值函数等于期望的即时奖励：
- en: '[PRE36]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will be as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE37]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Reshape the columns with `n_actions = 2`, as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`n_actions = 2`重新排列列，如下所示：
- en: '[PRE38]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE39]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By performing the `argmax` function, we obtain the index of the best action
    in each state as follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行`argmax`函数，我们可以获得每个状态中最佳动作的索引，如下所示：
- en: '[PRE40]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output will be as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE41]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The state diagram can be visualized as follows:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 状态图可以如下可视化：
- en: '![Figure 2.44: The best actions and the action-value function for the student
    MDP when'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.44：学生MDP的最佳动作和动作-价值函数，当'
- en: '](img/B16182_02_44.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_44.jpg)'
- en: 'Figure 2.44: The best actions and the action-value function for the student
    MDP when ![formula](img/B16182_02_44_caption.png)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.44：学生MDP的最佳动作和动作-价值函数，当![公式](img/B16182_02_44_caption.png)
- en: It is interesting to note how the best actions are changed by only modifying
    the discount factor. Here, the best action the agent can take, starting from `Class
    1`, is `Social` as it provides a bigger immediate reward compared to the `Study`
    action. The `Social` action brings the agent to state `Social`. Here, the best
    the agent can do is to repeat the `Social` action, cumulating negative rewards.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，最佳动作是如何仅通过修改折扣因子发生变化的。在这里，代理从`Class 1`开始时，最佳动作是`Social`，因为它比`Study`动作提供了更大的即时奖励。`Social`动作将代理带到状态`Social`。在这里，代理能做的最好的事情是重复`Social`动作，累积负奖励。
- en: In this example, we learned how to calculate the state-value function using
    `scipy.linalg.solve` and how to calculate the action-value function using the
    matrix form. We noticed that both the state values and the action values depend
    on the discount factor.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们学习了如何使用`scipy.linalg.solve`计算状态-价值函数，以及如何使用矩阵形式计算动作-价值函数。我们注意到，状态值和动作值都依赖于折扣因子。
- en: In the next section, we will illustrate the Bellman optimality equation, which
    makes it possible to solve MDPs by finding the best policy and the best state
    values.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将阐明贝尔曼最优方程，它使得通过找到最佳策略和最佳状态值来解决MDP成为可能。
- en: Bellman Optimality Equation
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝尔曼最优方程
- en: It is natural to ask whether it is possible to define an order for policies
    that determines whether one policy is better than another one. It turns out that
    the value function provides ordering over policies.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，我们会问是否有可能为策略定义一个顺序，来判断一个策略是否优于另一个策略。结果证明，值函数提供了一个对策略的排序。
- en: 'Policy ![ formula](img/B16182_02_44a.png) can be considered better than or
    equal to ![ formula](img/B16182_02_44b.png) policy ![ formula](img/B16182_02_44c.png)
    if the expected return from that policy is greater than or equal to the expected
    return of ![ formula](img/B16182_02_44d.png) for all states:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 策略![公式](img/B16182_02_44a.png)可以被认为优于或等于![公式](img/B16182_02_44b.png)策略![公式](img/B16182_02_44c.png)，如果该策略的期望回报在所有状态下大于或等于![公式](img/B16182_02_44d.png)的期望回报：
- en: '![Figure 2.45: Preference over policies'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.45：策略的偏好](img/B16182_02_45a.png)'
- en: '](img/B16182_02_45.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_45.jpg)'
- en: 'Figure 2.45: Preference over policies'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.45：策略的偏好
- en: In this example, we substituted the expected return in a state with the state-value
    function, using the state-value function definition.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们用状态值函数替代了在一个状态中的期望回报，使用了状态值函数的定义。
- en: 'Following the previous definition, an optimal policy is a policy that is better
    than or equal to all other policies in all states. The optimal state-value function,
    ![ formula](img/B16182_02_45a.png), and the optimal action-value function, ![
    formula](img/B16182_02_45b.png), are simply the ones associated with the best
    policy:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的定义，最优策略是指在所有状态下，优于或等于其他所有策略的策略。最优状态值函数，![公式](img/B16182_02_45a.png)，和最优动作值函数，![公式](img/B16182_02_45b.png)，简单来说，就是与最佳策略相关联的函数：
- en: '![Figure 2.46: Optimal state-value function'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.46：最优状态值函数](img/B16182_02_46.jpg)'
- en: '](img/B16182_02_46.jpg)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_46.jpg)'
- en: 'Figure 2.46: Optimal state-value function'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.46：最优状态值函数
- en: '![Figure 2.47: Optimal action-value function'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.47：最优动作值函数](img/B16182_02_47a.png)'
- en: '](img/B16182_02_47.jpg)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_47.jpg)'
- en: 'Figure 2.47: Optimal action-value function'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.47：最优动作值函数
- en: 'Some important properties of MDPs are as follows:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: MDP的一些重要属性如下：
- en: There is always at least one optimal (deterministic) policy maximizing the state-value
    function in every state.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个状态下总是至少存在一个最优（确定性）策略，最大化状态值函数。
- en: All optimal policies share the same optimal state-value function.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有最优策略共享相同的最优状态值函数。
- en: An MDP is solved if we know the optimal state-value function and the optimal
    action-value function.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道最优状态值函数和最优动作值函数，则MDP已被解决。
- en: 'Knowing the optimal value function, ![ formula](img/B16182_02_47a.png), makes
    it possible to find the optimal policy of the MDP by maximizing over ![ formula](img/B16182_02_47b.png).
    We can define the optimal policy associated with the optimal action-value function
    as follows:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 通过知道最优值函数![公式](img/B16182_02_47a.png)，可以通过最大化![公式](img/B16182_02_47b.png)来找到MDP的最优策略。我们可以按以下方式定义与最优动作值函数相关联的最优策略：
- en: '![Figure 2.48: Optimal policy associated with the optimal action-value function'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.48：与最优动作值函数相关联的最优策略](img/B16182_02_48a.png)'
- en: '](img/B16182_02_48.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_48.jpg)'
- en: 'Figure 2.48: Optimal policy associated with the optimal action-value function'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.48：与最优动作值函数相关联的最优策略
- en: As you can see, this policy is simply telling us to perform the action, ![ formula](img/B16182_02_48a.png),
    with a probability of 1 (essentially, in a deterministic way) if the action, ![
    formula](img/B16182_02_48b.png), maximizes the action-value function in this state.
    In other words, we need to take the action that guarantees the highest discounted
    return following the optimal policy. All other actions, being suboptimal, are
    taken with probability 0; therefore, they are, essentially, never taken. Notice
    that the policy obtained in this way is deterministic, not stochastic.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这个策略实际上是在告诉我们，在动作![公式](img/B16182_02_48a.png)能最大化该状态的动作值函数时，以1的概率（本质上是确定性的）执行该动作![公式](img/B16182_02_48b.png)。换句话说，我们需要采取能保证最高折扣回报的动作，遵循最优策略。所有其他子最优动作的概率为0，因此基本上是从不执行的。请注意，这种方式得到的策略是确定性的，而不是随机性的。
- en: 'Analyzing this result, we uncover two essential facts:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 分析这个结果，我们揭示了两个重要事实：
- en: There is always a deterministic optimal policy for any MDP.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何MDP，总是存在一个确定性的最优策略。
- en: The optimal policy is determined by the knowledge of the optimal action-value
    function, ![ formula](img/B16182_02_48c.png).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优策略由最优动作-值函数![公式](img/B16182_02_48c.png)的知识决定。
- en: 'The optimal value functions are related to the Bellman optimality equation.
    The Bellman optimality equation states that the optimal state-value function in
    a state is equal to the overall maximum actions of the optimal action-value function
    in the same state:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 最优值函数与贝尔曼最优性方程相关。贝尔曼最优性方程表明，某一状态下的最优状态值函数等于该状态下最优动作-值函数的整体最大值：
- en: '![Figure 2.49: The Bellman optimality equation'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.49：贝尔曼最优性方程'
- en: '](img/B16182_02_49.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_49.jpg)'
- en: 'Figure 2.49: The Bellman optimality equation'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.49：贝尔曼最优性方程
- en: 'Using the definition of the action-value function, we can expand the previous
    equation to a more explicit form:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 使用动作-值函数的定义，我们可以将前面的方程展开为更显式的形式：
- en: '![Figure 2.50: The Bellman optimality equation in terms of the action-value
    function'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.50：基于动作-值函数的贝尔曼最优性方程'
- en: '](img/B16182_02_50.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_50.jpg)'
- en: 'Figure 2.50: The Bellman optimality equation in terms of the action-value function'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.50：基于动作-值函数的贝尔曼最优性方程
- en: The previous equation tells us that the optimal value function of a state is
    equal to the maximum over actions of the immediate reward, ![ formula](img/B16182_02_50a.png),
    plus the discounted ![ formula](img/B16182_02_50b.png), expected optimal value
    of the successor state, ![ formula](img/B16182_02_50c.png), where the expected
    value is determined by the transition function.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程告诉我们，某一状态的最优值函数等于对所有动作的即时奖励的最大值，![公式](img/B16182_02_50a.png)，加上折扣后的![公式](img/B16182_02_50b.png)，即后继状态的期望最优值，![公式](img/B16182_02_50c.png)，其中期望值由转移函数确定。
- en: 'Also, the optimal action-value function has an explicit formulation, known
    as the Bellman optimality equation, for ![ formula](img/B16182_02_50d.png):'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最优动作-值函数具有显式的形式，被称为贝尔曼最优性方程，用于![公式](img/B16182_02_50d.png)：
- en: '![Figure 2.51: The Bellman optimality equation for q'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.51：q的贝尔曼最优性方程'
- en: '](img/B16182_02_51.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_51.jpg)'
- en: 'Figure 2.51: The Bellman optimality equation for q'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.51：q的贝尔曼最优性方程
- en: 'This can be rewritten only in terms of ![ formula](img/B16182_02_51a.png) by
    using the relationship between ![ formula](img/B16182_02_51b.png) and ![ formula](img/B16182_02_51c.png):'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用![公式](img/B16182_02_51b.png)与![公式](img/B16182_02_51c.png)之间的关系，可以将此公式仅以![公式](img/B16182_02_51a.png)的形式重写：
- en: '![Figure 2.52: The Bellman optimality equation, using the relationship between
    and'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.52：使用![a](img/B16182_02_52_Caption1.png)与![b](img/B16182_02_45a.png)之间的关系的贝尔曼最优性方程'
- en: '](img/B16182_02_52.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_52.jpg)'
- en: 'Figure 2.52: The Bellman optimality equation, using the relationship between
    ![a](img/B16182_02_52_Caption1.png) and ![b](img/B16182_02_45a.png)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.52：使用![a](img/B16182_02_52_Caption1.png)与![b](img/B16182_02_45a.png)之间的关系的贝尔曼最优性方程
- en: The Bellman optimality equation for ![ formula](img/B16182_02_52a.png) expresses
    the fact that the optimal state-value function must equal the expected return
    for the best action in that state. Similarly, the Bellman optimality equation
    for ![ formula](img/B16182_02_52b.png) expresses the fact that the optimal q-function
    must equal the immediate reward plus the discounted return of the best action
    in the next state according to the environment dynamic.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼最优性方程对于![公式](img/B16182_02_52a.png)表达了这样一个事实：最优状态值函数必须等于该状态下最佳动作的期望回报。类似地，贝尔曼最优性方程对于![公式](img/B16182_02_52b.png)表达了这样一个事实：最优q-函数必须等于即时奖励加上根据环境动态折扣后的下一个状态中最佳动作的回报。
- en: Solving the Bellman Optimality Equation
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 求解贝尔曼最优性方程
- en: The presence of a maximization makes the Bellman optimality equation non-linear.
    This means that we do not have a closed-form solution for these equations in the
    general case. However, there are many iterative solution methods that we will
    analyze in the next sections and chapters.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在最大化操作，贝尔曼最优性方程是非线性的。这意味着在一般情况下我们没有这些方程的封闭形式解。然而，存在许多迭代求解方法，我们将在接下来的部分和章节中进行分析。
- en: The main methods include value iteration, policy iteration, Q learning, and
    SARSA, which we will study in later chapters.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 主要方法包括值迭代、策略迭代、Q学习和SARSA，我们将在后续章节中学习这些方法。
- en: Solving MDPs
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 求解MDPs
- en: Now that we have gained a fair understanding of all the important concepts and
    equations, let's move on to solving actual MDPs.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经充分理解了所有重要的概念和方程，接下来我们将开始求解实际的MDP问题。
- en: Algorithm Categorization
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法分类
- en: 'Before considering the different algorithms for solving MDPs, it is beneficial
    for us to understand the family of algorithms along with their pros and cons.
    Knowing the main family of algorithms makes it possible for us to select the correct
    family based on our task:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑解决MDP的不同算法之前，理解算法家族及其优缺点对我们是有益的。了解主要的算法家族使我们能够根据任务选择正确的算法家族：
- en: '![Figure 2.53: Taxonomy of RL algorithms'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.53：强化学习算法的分类'
- en: '](img/B16182_02_53.jpg)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_53.jpg)'
- en: 'Figure 2.53: Taxonomy of RL algorithms'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.53：强化学习算法的分类
- en: 'The first main distinction is between model-based algorithms and model-free
    algorithms:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主要区分是基于模型的算法和无模型的算法：
- en: 'A model-based algorithm requires knowledge of the environment dynamic (model).
    This is a strong requirement, as the environment model is usually unknown. Let''s
    consider an autonomous driving problem. Here, knowing the environment dynamic
    means that we should know exactly how the agent''s actions influence the environment
    and the next state distribution. This depends on many factors: the street state,
    weather conditions, car characteristics, and much more. For many problems, the
    dynamic is unknown, too complex, or too inaccurate to be used successfully. Nonetheless,
    the dynamic provides beneficial information for solving the task.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的算法需要了解环境的动态（模型）。这是一个很强的要求，因为环境模型通常是未知的。让我们考虑一个自动驾驶问题。在这里，了解环境动态意味着我们应该准确知道智能体的行为如何影响环境以及下一个状态的分布。这取决于许多因素：街道状况、天气条件、汽车特性等等。对于许多问题，动态是未知的、过于复杂的，或过于不准确，无法成功使用。尽管如此，动态提供了有益的信息来解决任务。
- en: When the model is known (`Model Exploiting`), model-based algorithms are preferred
    over their counterparts for their sample efficiency, as they require fewer samples
    to learn good policies.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当模型已知时（`模型利用`），基于模型的算法比其对手更受欢迎，因为它们的样本效率更高，需要更少的样本来学习良好的策略。
- en: The environment model in these cases can also be unknown; the algorithm itself
    explicitly learns an environment model (`Model Learning`) and uses it to plan
    its actions. Dynamic programming algorithms use this model knowledge to perform
    bootstrapping, which uses a previous estimation for the estimate of another quantity.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这些情况下，环境模型也可以是未知的；算法本身明确地学习环境模型（`模型学习`），并利用它来规划自己的行动。动态规划算法使用这些模型知识来执行自助式学习，即利用先前的估算来估算其他量。
- en: Model-free algorithms do not require a model of the environment. These types
    of algorithms are, therefore, preferred for real-world applications. Note that
    these algorithms may build an environment representation internally, taking into
    account the environment dynamic. However, usually, this process is implicit, and
    the users just don't care about these aspects.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模型算法不需要环境模型。因此，这些类型的算法更适合用于现实世界的应用。请注意，这些算法可能会在内部构建一个环境表示，考虑到环境动态。然而，通常这个过程是隐式的，用户并不关注这些方面。
- en: Model-free algorithms can also be classified as value-based algorithms or policy-search
    algorithms.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型算法也可以被归类为基于价值的算法或策略搜索算法。
- en: Value-Based Algorithms
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于价值的算法
- en: A value-based algorithm focuses on learning the action-value function and the
    state-value function. Learning the value functions is done by using the Bellman
    equations presented in the previous sections. An example of a value-based algorithm
    is Q learning, where the objective is to learn the action-value function, which,
    in turn, is used for control. A deep Q network is an extension of Q learning in
    which a neural network is used to approximate the q-function. Value-based algorithms
    are usually off-policy, which means they can reuse previous samples collected
    with a different policy with respect to the policy being optimized at the moment.
    This is a very powerful property as it allows us to obtain more efficient algorithms
    in terms of samples. We will learn about Q learning and deep Q networks in more
    detail in *Chapter 9*, *What Is Deep Q-Learning?*.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的算法专注于学习动作价值函数和状态价值函数。学习价值函数是通过使用前面章节介绍的贝尔曼方程完成的。一个基于价值的算法示例是Q学习，其目标是学习动作价值函数，进而用于控制。深度Q网络是Q学习的扩展，其中使用神经网络来近似q函数。基于价值的算法通常是脱离策略的，这意味着它们可以重用之前收集的样本，这些样本是使用与当前正在优化的策略不同的策略收集的。这是一个非常强大的特性，因为它允许我们在样本方面获得更高效的算法。我们将在*第9章*中更详细地学习Q学习和深度Q网络，*什么是深度Q学习？*。
- en: Policy Search Algorithms
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略搜索算法
- en: '**Policy Search** (**PS**) methods explore the policy space directly. In PS,
    the RL problem is formalized as the maximization of the performance measure depending
    on the policy parameters. You will study PS methods and policy gradients in more
    detail in *Chapter 11*, *Policy-Based Methods for Reinforcement Learning*.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略搜索**（**PS**）方法直接探索策略空间。在PS中，强化学习问题被形式化为依赖于策略参数的性能度量最大化。你将在*第11章*，*基于策略的强化学习方法*中更详细地学习PS方法和策略梯度。'
- en: Linear Programming
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性规划
- en: Linear programming is an optimization technique that is used for problems with
    linear constraints and linear objective functions. The objective function describes
    the quantity to be optimized. In the case of RL, this quantity is the expected
    discounted return of all the states weighted by the initial state distribution,
    which is the probability of starting an episode in that state.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划是一种优化技术，用于具有线性约束和线性目标函数的问题。目标函数描述了需要优化的数量。在强化学习（RL）的情况下，这个数量是所有状态的期望折扣回报，按初始状态分布加权，即在该状态开始一个回合的概率。
- en: When the starting state is precisely one, this simplifies to the optimization
    of the expected discounted return starting from the initial state.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 当起始状态恰好为1时，这简化为优化从初始状态开始的期望折扣回报。
- en: Linear programming is a model-based, model-exploiting technique. Solving an
    MDP with linear programming, therefore, requires perfect knowledge of the environment
    dynamics, which translates into knowledge of the transition probability matrix,
    ![ formula](img/B16182_02_53a.png). Using linear programming, we can solve MDPs
    by finding the best state values for each state. From our knowledge of state values,
    we can derive knowledge of the optimal action-value function. In this way, we
    can find a control policy for our agent and maximize its performance in the given
    task.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划是一种基于模型并利用模型的技术。因此，使用线性规划求解MDP需要对环境动态的完美知识，这转化为对转移概率矩阵![公式](img/B16182_02_53a.png)的了解。通过使用线性规划，我们可以通过找到每个状态的最佳状态值来解决MDP。从对状态值的了解中，我们可以推导出最优的动作值函数。通过这种方式，我们可以为代理找到一个控制策略，并最大化其在给定任务中的表现。
- en: 'The basic idea follows on from the definition of ordering over policies; we
    want to find the state-value function by maximizing the value of each state weighted
    by the initial state distribution, ![ formula](img/B16182_02_53b.png), subject
    to a feasibility constraint:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想源自对策略排序的定义；我们希望通过最大化每个状态的价值，按初始状态分布加权，来找到状态价值函数，![公式](img/B16182_02_53b.png)，并满足可行性约束：
- en: '![Figure 2.54: Linear programming formulation for solving MDPs'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.54：求解MDP的线性规划公式](img/B16182_02_54.jpg)'
- en: '](img/B16182_02_54.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_54.jpg)'
- en: 'Figure 2.54: Linear programming formulation for solving MDPs'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.54：求解MDP的线性规划公式
- en: Here, we have ![ formula](img/B16182_02_54a.png) variables and ![ formula](img/B16182_02_54b.png)
    constraints. The variables are the values, ![ formula](img/B16182_02_54c.png),
    for each state, `s`, in the state space, `S`.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有![公式](img/B16182_02_54a.png)个变量和![公式](img/B16182_02_54b.png)个约束。变量是每个状态`S`中的值，![公式](img/B16182_02_54c.png)，`s`为状态空间中的每个状态。
- en: Note that the maximization role is taken by the constraints, while we need to
    minimize the objective function because, otherwise, an optimal solution would
    have infinite values for all variables, ![ formula](img/B16182_02_54d.png).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最大化作用由约束承担，而我们需要最小化目标函数，否则，最优解会使所有变量的值无限大，![公式](img/B16182_02_54d.png)。
- en: The constraints are based on the idea that the value of a state must be greater
    than or equal to the immediate reward plus the discounted expected value of the
    successor states. This must be true for all states and all actions.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 约束基于这样的思想：一个状态的价值必须大于或等于即时奖励加上后继状态的折扣期望值。这对于所有状态和所有动作都必须成立。
- en: The huge number of variables and constraints makes it possible to use linear
    programming techniques for only finite-state and finite-action MDPs.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变量和约束的数量庞大，线性规划技术仅适用于有限状态和有限动作的MDP。
- en: 'We will be using the following notation:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下符号：
- en: '![Figure 2.55: Linear programming notation'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.55：线性规划符号](img/B16182_02_55.jpg)'
- en: '](img/B16182_02_55.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_55.jpg)'
- en: 'Figure 2.55: Linear programming notation'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.55：线性规划符号
- en: In the preceding notation, `c` is the vector of coefficients of the objective
    function, ![ formula](img/B16182_02_55a.png) is the matrix of the upper bound
    constraints, and ![ formula](img/B16182_02_55b.png) is the associated coefficient
    vector.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述表示法中，`c` 是目标函数的系数向量，![公式](img/B16182_02_55a.png) 是上界约束的矩阵，![公式](img/B16182_02_55b.png)
    是关联的系数向量。
- en: In Python, SciPy offers the `linprog` function (inside the `optimize` module,
    `scipy.optimize.linprog`), which optimizes linear programs given the objective
    function and the constraints.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，SciPy 提供了 `linprog` 函数（在 `optimize` 模块中，`scipy.optimize.linprog`），该函数根据目标函数和约束条件优化线性规划。
- en: The signature of the function is `scipy.optimize.linprog(c, A_ub, b_ub)`.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的签名为 `scipy.optimize.linprog(c, A_ub, b_ub)`。
- en: 'To rephrase the problem using upper bounds, we have the following:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用上界重新表述问题，我们有以下内容：
- en: '![Figure 2.56: Linear programming constraints using upper bounds'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.56：使用上界的线性规划约束](img/B16182_02_56.jpg)'
- en: '](img/B16182_02_56.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_56.jpg)'
- en: 'Figure 2.56: Linear programming constraints using upper bounds'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.56：使用上界的线性规划约束
- en: Note
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For further reading on linear programming for MDPs, refer to the following
    paper from *de Farias, D. P. (2002): The Linear Programming Approach to Approximate
    Dynamic Programming: Theory and Application*: [http://www.mit.edu/~pucci/discountedLP.pdf](http://www.mit.edu/~pucci/discountedLP.pdf).'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '如需进一步了解线性规划在 MDP 中的应用，请参阅 *de Farias, D. P. (2002): 近似动态规划的线性规划方法：理论与应用* 一文：[http://www.mit.edu/~pucci/discountedLP.pdf](http://www.mit.edu/~pucci/discountedLP.pdf)。'
- en: Let's now solve a quick exercise to strengthen our understanding of linear programming.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在做一个简单的练习，以加深我们对线性规划的理解。
- en: 'Exercise 2.02: Determining the Best Policy for an MDP Using Linear Programming'
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：使用线性规划确定 MDP 的最佳策略
- en: 'The goal of this exercise is to solve the MDP in the following figure using
    linear programming. In this MDP, the environment model is straightforward and
    the transition function is deterministic, determined uniquely by the action. We
    will be finding the best action (the one with the maximum reward) taken by the
    agent, which determines the best policy of the environment, using linear programming:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习的目标是使用线性规划求解下图中的 MDP。在这个 MDP 中，环境模型非常简单，转移函数是确定性的，由动作唯一决定。我们将使用线性规划找到代理所采取的最佳动作（即具有最大奖励的动作），从而确定环境的最佳策略：
- en: '![Figure 2.57: Simple MDP with three states and two actions'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.57：具有三个状态和两个动作的简单 MDP](img/B16182_02_57.jpg)'
- en: '](img/B16182_02_57.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_57.jpg)'
- en: 'Figure 2.57: Simple MDP with three states and two actions'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.57：具有三个状态和两个动作的简单 MDP
- en: The variables of the linear program are the state values. The coefficients are
    given by the initial state distribution, which, in our case, is a deterministic
    function, as state 1 is the initial state. Therefore, the coefficients of the
    objective function are `[1, 0, 0]`.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划的变量是状态值。系数由初始状态分布给出，在我们这里它是一个确定性函数，因为状态 1 是初始状态。因此，目标函数的系数为 `[1, 0, 0]`。
- en: 'We are now ready to tackle our problem:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好解决我们的实际问题：
- en: 'As always, import the required libraries:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一如既往，导入所需的库：
- en: '[PRE42]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the number of states and actions and the discount factor for this problem:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义该问题的状态数、动作数和折扣因子：
- en: '[PRE43]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the initial state distribution. In our case, it is a deterministic function:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义初始状态分布。在我们这里，它是一个确定性函数：
- en: '[PRE44]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output will be as follows:'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE45]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now we need to build the upper bound coefficients for action `A`:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要为动作 `A` 构建上界系数：
- en: '[PRE46]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output will be as follows:'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE47]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define the transition matrix for action `A`:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义动作 `A` 的转移矩阵：
- en: '[PRE48]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be as follows:'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE49]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We are ready to build the upper bound matrix for action `A`:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好为动作 `A` 构建上界矩阵：
- en: '[PRE50]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output will be as follows:'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE51]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We need to do the same for action `B`:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要对动作 `B` 做同样的事情：
- en: '[PRE52]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be as follows:'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE53]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We are ready to concatenate the results for the two actions:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好将两个动作的结果进行拼接：
- en: '[PRE54]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The only thing we have to do now is to solve the linear program using `scipy.optimize.linprog`:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在唯一需要做的就是使用 `scipy.optimize.linprog` 求解线性规划：
- en: '[PRE55]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let''s collect the results:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们收集结果：
- en: '[PRE56]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s analyze the results. We can see that the value of state 2 is the lowest
    one, as expected. The values of states 1 and 3 are very close to each other and
    are approximately equal to 1e+2:'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们分析一下结果。我们可以看到状态 2 的值是最低的，正如预期的那样。状态 1 和状态 3 的值非常接近，且大约等于 1e+2：
- en: '![Figure 2.58: Representation of the optimal value function for the MDP'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.58：MDP 的最优价值函数表示'
- en: '](img/B16182_02_58.jpg)'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_02_58.jpg)'
- en: 'Figure 2.58: Representation of the optimal value function for the MDP'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.58：MDP 的最优价值函数表示
- en: 'Now we can calculate the optimal policy by calculating the optimal action-value
    function for each state-action pair:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以通过计算每个状态-动作对的最优动作-价值函数来计算最优策略：
- en: '[PRE57]'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output will be as follows:'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE58]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Use the action-value formula to calculate the action values for each state-action
    pair:'
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用动作-价值公式计算每个状态-动作对的动作值：
- en: '[PRE59]'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE60]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Reshape and use the `argmax` function to better understand the best actions:'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新构造并使用 `argmax` 函数，以更好地理解最佳动作：
- en: '[PRE61]'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output will be as follows:'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE62]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Use the following code to better understand the best actions:'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下代码来更好地理解最佳动作：
- en: '[PRE63]'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output will be as follows:'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE64]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'By visually inspecting the result, we can see that action `B` is the best action
    for all states, having acquired the highest q values for all states. Thus, the
    optimal policy decides to always take action `B`. Doing this, we will land in
    state `3`, and we will follow the self-loop, cumulating high positive rewards:'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过直观地检查结果，我们可以看到，动作 `B` 是所有状态的最佳动作，因为它为所有状态获得了最高的 q 值。因此，最优策略决定始终执行动作 `B`。这样，我们将进入状态
    `3`，并沿着自环进行，累积大量正奖励：
- en: '![Figure 2.59: Representation of the optimal policy and the optimal'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.59：最优策略和最优价值函数的表示'
- en: value function for the MDP
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 的价值函数
- en: '](img/B16182_02_59.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_59.jpg)'
- en: 'Figure 2.59: Representation of the optimal policy and the optimal value function
    for the MDP'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.59：MDP 的最优策略和最优价值函数表示
- en: The optimal policy is represented in *Figure 2.59*. The dotted arrows represent
    the best action for each state.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略在 *图 2.59* 中表示。虚线箭头代表每个状态的最佳动作。
- en: Note
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Arr9rO](https://packt.live/2Arr9rO).
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2Arr9rO](https://packt.live/2Arr9rO)。
- en: You can also run this example online at [https://packt.live/2Ck6neR](https://packt.live/2Ck6neR).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是 [https://packt.live/2Ck6neR](https://packt.live/2Ck6neR)。
- en: In this exercise, we used linear programming techniques to solve a simple MDP
    with finite states and actions. By using the correspondence between the state-value
    function and the action-value function, we extracted the value of each state-action
    pair. From this knowledge, we extracted the optimal policy for this environment.
    In this case, the best policy is always just to take action `B`.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们使用了线性规划技术来解决一个具有有限状态和动作的简单 MDP。通过利用状态-价值函数和动作-价值函数之间的对应关系，我们提取了每个状态-动作对的价值。根据这些知识，我们提取了该环境的最优策略。在这种情况下，最好的策略始终是执行动作
    `B`。
- en: In the next activity, we will use the Bellman expectation equation to evaluate
    a policy for a more complex task. Before that, let's explore the environment that
    we are going to use in the activity, Gridworld.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将使用贝尔曼期望方程评估一个更复杂任务的策略。在此之前，让我们先了解我们将在活动中使用的环境——网格世界。
- en: Gridworld
  id: totrans-542
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格世界
- en: 'Gridworld is a classical RL environment with many variants. The following figure
    displays the visual representation of the environment:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 网格世界是一个经典的强化学习环境，有许多变种。下图展示了该环境的视觉表示：
- en: '![Figure 2.60: The Gridworld environment'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.60：网格世界环境'
- en: '](img/B16182_02_60.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_60.jpg)'
- en: 'Figure 2.60: The Gridworld environment'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.60：网格世界环境
- en: 'As you can see, the states are represented by cells, and there are 25 states
    arranged in a 5 x 5 grid. There are four available actions: left, right, up, and
    down. These actions move the current state in the direction of the action, and
    the associated reward is 0 for all actions. The exceptions are as follows:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，状态通过网格中的单元格表示，且有 25 个状态按 5 x 5 网格排列。共有四个可用动作：左、右、上、下。这些动作会将当前状态移向动作的方向，并且所有动作的奖励为
    0。例外情况如下：
- en: 'Border cells: If an action takes the agent outside of the grid, the agent state
    does not change, and the agent receives a reward of -1.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界单元格：如果一个动作将智能体带出网格，智能体的状态不会改变，且智能体将获得 -1 奖励。
- en: 'Good cells: ![ formula](img/B16182_02_60a.png)and ![ formula](img/B16182_02_60b.png)
    are good cells. For these cells, each action brings the agent to states ![ formula](img/B16182_02_60c.png)
    and ![ formula](img/B16182_02_60d.png), respectively. The associated reward is
    +10 for going outside state ![ formula](img/B16182_02_60e.png) and +5 for going
    outside state ![ formula](img/B16182_02_60f.png).'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好单元格：![公式](img/B16182_02_60a.png) 和 ![公式](img/B16182_02_60b.png) 是好单元格。对于这些单元格，每个动作将代理引导到状态
    ![公式](img/B16182_02_60c.png) 和 ![公式](img/B16182_02_60d.png)，对应的回报分别是：从状态 ![公式](img/B16182_02_60e.png)
    外移的回报是 +10，从状态 ![公式](img/B16182_02_60f.png) 外移的回报是 +5。
- en: 'Bad cells: ![ formula](img/B16182_02_60g.png) and ![ formula](img/B16182_02_60h.png)
    are bad cells. For these cells, the associated reward is -1 for all actions.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏单元格：![公式](img/B16182_02_60g.png) 和 ![公式](img/B16182_02_60h.png) 是坏单元格。对于这些单元格，所有动作的回报都是
    -1。
- en: Now that we have an understanding of the environment, let's attempt an activity
    that implements it.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了环境，让我们尝试一个实现它的活动。
- en: 'Activity 2.01: Solving Gridworld'
  id: totrans-552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.01：解决 Gridworld 问题
- en: In this activity, we will be working on the Gridworld environment. The goal
    of the activity is to calculate and visually represent the state values for a
    random policy, in which the agent selects each action with an equal probability
    (1/4) in all states. The discount factor is assumed to be equal to 0.9\.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将处理 Gridworld 环境。活动的目标是计算并可视化一个随机策略下的状态值，其中代理在所有状态下以相等的概率（1/4）选择每个动作。假设折扣因子为
    0.9。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该活动：
- en: Import the required libraries. Import `Enum` and `auto` from `enum`, `matplotlib.pyplot`,
    `scipy`, and `numpy`, and import `tuple` from `typing`.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。导入 `Enum` 和 `auto` 从 `enum`，`matplotlib.pyplot`，`scipy` 和 `numpy`，以及从
    `typing` 导入 `tuple`。
- en: Define the visualization function and the possible actions for the agent.
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义可视化函数和代理可能的动作。
- en: Write a policy class that returns the action probability in a given state; for
    a random policy, the state can be ignored.
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个策略类，该类返回给定状态下的动作概率；对于随机策略，状态可以被忽略。
- en: Write an `Environment` class with a step function that returns the next state
    and the associated reward given the current state and action.
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个 `Environment` 类，并实现一个步进函数，该函数根据当前状态和动作返回下一个状态及其相关的回报。
- en: Loop for all states and actions and build a transition matrix (width*height,
    width*height) and a reward matrix of the same dimension. The transition matrix
    contains the probability of going from one state to another, so the sum of the
    first axis should be equal to 1 for all rows.
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有状态和动作进行循环，构建一个转换矩阵（宽度 * 高度，宽度 * 高度）和一个相同维度的回报矩阵。转换矩阵包含从一个状态到另一个状态的概率，因此第一轴的所有行的和应该等于
    1。
- en: Use the matrix form of the Bellman expectation equation to compute the state
    values for each state. You can use `scipy.linalg.solve` or directly compute the
    inverse matrix and solve the system.
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝尔曼期望方程的矩阵形式来计算每个状态的状态值。你可以使用 `scipy.linalg.solve` 或直接计算逆矩阵并解系统方程。
- en: 'The output will be as follows:'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.61: State values of Gridworld'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.61：Gridworld 的状态值'
- en: '](img/B16182_02_61.jpg)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_61.jpg)'
- en: 'Figure 2.61: State values of Gridworld'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.61：Gridworld 的状态值
- en: Note
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is useful to visualize the state values and the expected reward, so write
    a function visually representing the calculated matrices.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化状态值和期望回报是有用的，因此编写一个函数来可视化计算得到的矩阵。
- en: The solution to this activity can found on page 689.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解答可以在第 689 页找到。
- en: Summary
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned the differences between MCs, MRPs, and MDPs. An
    MC is the most straightforward description of a generic process that is composed
    of states and a probability function that describes the transition between states.
    An MRP includes the concept of rewards as a measure of how good a transition is.
    The MDP is what we are most interested in; it includes the concept of actions,
    policies, and goals.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 MC、MRP 和 MDP 之间的区别。MC 是对通用过程的最简单描述，它由状态和描述状态之间转换的概率函数组成。MRP 包括回报的概念，作为衡量转换质量的标准。MDP
    是我们最感兴趣的，它包括动作、策略和目标的概念。
- en: In the context of Markov processes, we introduced Bellman equations in different
    forms and also analyzed the relationship between the state-value function and
    the action-value function.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫过程的背景下，我们介绍了不同形式的贝尔曼方程，并分析了状态值函数和动作值函数之间的关系。
- en: We discussed various methods for solving MDPs, categorizing algorithms based
    on the information they require and on the methods they use. These algorithms
    will be presented in more detail in the following chapters. We focused on linear
    programming, showing how it is possible to solve MDPs using these techniques.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了解决MDP的各种方法，基于所需信息和使用的方法对算法进行了分类。接下来的章节将更详细地介绍这些算法。我们重点介绍了线性规划，展示了如何使用这些技术解决MDP问题。
- en: In the next chapter, you will learn how to use TensorFlow 2 to implement deep
    learning algorithms and machine learning models.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用TensorFlow 2来实现深度学习算法和机器学习模型。
