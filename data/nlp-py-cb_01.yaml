- en: Corpus and WordNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库与WordNet
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下示例：
- en: Accessing in-built corpora
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问内置语料库
- en: Download an external corpus, load it, and access it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载外部语料库，加载并访问它
- en: Counting all the wh words in three different genres in the Brown corpus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算布朗语料库中三个不同类型的`wh`词的数量
- en: Explore frequency distribution operations on one of the web and chat text corpus
    files
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索对某个网页和聊天文本语料库文件进行频率分布操作
- en: Take an ambiguous word and explore all its senses using WordNet
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选取一个模糊的词，并使用WordNet探索其所有含义
- en: Pick two distinct synsets and explore the concepts of hyponyms and hypernyms
    using WordNet
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择两个不同的同义词集并使用WordNet探索下义词和上义词的概念
- en: Compute the average polysemy of nouns, verbs, adjectives, and adverbs according
    to WordNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据WordNet计算名词、动词、形容词和副词的平均多义性
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: To solve any real-world **Natural Language Processing** (**NLP**) problems,
    you need to work with huge amounts of data. This data is generally available in
    the form of a corpus out there in the open diaspora and as an add-on of the NLTK
    package. For example, if you want to create a spell checker, you need a huge corpus
    of words to match against.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决任何实际的**自然语言处理**（**NLP**）问题，您需要处理大量数据。这些数据通常以语料库的形式存在于开放的分布中，并作为NLTK包的附加组件提供。例如，如果您想创建一个拼写检查器，您需要一个庞大的单词语料库进行匹配。
- en: 'The goal of this chapter is to cover the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是涵盖以下内容：
- en: Introducing various useful textual corpora available with NLTK
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍NLTK提供的各种有用文本语料库
- en: How to access these in-built corpora from Python
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从Python访问这些内置语料库
- en: Working with frequency distributions
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用频率分布
- en: An introduction to WordNet and its lexical features
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WordNet及其词汇特性简介
- en: We will try to understand these things from a practical standpoint. We will
    perform some exercises that will fulfill all of these goals through our recipes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实践的角度来理解这些问题。我们将通过一些练习，借助我们的示例来实现这些目标。
- en: Accessing in-built corpora
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问内置语料库
- en: As already explained, we have many corpuses available for use with NLTK. We
    will assume that you have already downloaded and installed NLTK data on your computer.
    If not, you can find the same at [http://www.nltk.org/data.html](http://www.nltk.org/data.html).
    Also, a complete list of corpora that you can use from within NLTK data is available
    at [http://www.nltk.org/nltk_data/](http://www.nltk.org/nltk_data/).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们有许多可用于NLTK的语料库。我们假设您已经在计算机上下载并安装了NLTK数据。如果没有，您可以在[http://www.nltk.org/data.html](http://www.nltk.org/data.html)找到相应的内容。另外，您可以在[http://www.nltk.org/nltk_data/](http://www.nltk.org/nltk_data/)查看NLTK数据中可用的语料库完整列表。
- en: Now, our first task/recipe involves us learning how to access any one of these
    corpora. We have decided to do some tests on the Reuters corpus or the same. We
    will import the corpus into our program and try to access it in different ways.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的第一个任务/示例是学习如何访问这些语料库中的任何一个。我们决定对Reuters语料库进行一些测试，或者说，使用它。我们将把语料库导入到程序中，并尝试以不同的方式访问它。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Create a new file named `reuters.py` and add the following import line in the
    file. This will specifically allow access to only the `reuters` corpus in our
    program from the entire NLTK data:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`reuters.py`的新文件，并在文件中添加以下导入语句。这将专门允许我们的程序从整个NLTK数据中访问仅`reuters`语料库：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we want to check what exactly is available in this corpus. The simplest
    way to do this is to call the `fileids()` function on the corpus object. Add the
    following line in your program:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们想检查这个语料库中究竟有哪些内容。最简单的方法是调用语料库对象的`fileids()`函数。在您的程序中添加以下代码：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now run the program and you shall get an output similar to this:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在运行程序，您将得到类似这样的输出：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These are the lists of files and the relative paths of each of them in the `reuters`
    corpus.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是`reuters`语料库中各文件及其相对路径的列表。
- en: 'Now we will access the actual content of any of these files. To do this, we
    will use the `words()` function on the corpus object as follows, and we will access
    the `test/16097` file:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将访问这些文件的实际内容。为此，我们将使用`words()`函数对语料库对象进行操作，如下所示，我们将访问`test/16097`文件：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the program again and an extra new line of output will appear:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行程序，将会出现一行新的输出：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, the list of words in the `test/16097` file is shown. This is
    curtailed though the entire list of words is loaded in the memory object.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`test/16097`文件中的单词列表已显示。尽管列表被截断，但所有单词都已加载到内存对象中。
- en: 'Now we want to access a specific number of words (`20`) from the same file,
    `test/16097`. Yes! We can specify how many words we want to access and store them
    in a list for use. Append the following two lines in the code:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们想要从同一个文件中获取特定数量的单词（`20`），文件名为`test/16097`。没错！我们可以指定想要获取的单词数，并将其存储在一个列表中以供使用。请在代码中添加以下两行：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run this code and another extra line of output will be appended, which will
    look like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，输出中将附加一行额外的内容，如下所示：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Moving forward, the `reuters` corpus is not just a list of files but is also
    hierarchically categorized into 90 topics. Each topic has many files associated
    with it. What this means is that, when you access any one of the topics, you are
    actually accessing the set of all files associated with that topic. Let''s first output
    the list of topics by adding the following code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，`reuters`语料库不仅仅是一个文件列表，它还被层级地分类为90个主题。每个主题都有许多相关文件。这意味着，当你访问某个主题时，实际上是访问与该主题相关的所有文件。我们首先通过添加以下代码来输出主题列表：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the code and the following line of output will be added to the output console:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，控制台中将添加以下输出内容：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: All 90 categories are displayed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 显示所有90个类别。
- en: 'Finally, we will write four simple lines of code that will not only access
    two topics but also print out the words in a loosely sentenced fashion as one
    sentence per line. Add the following code to the Python file:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将编写四行简单的代码，这不仅能访问两个主题，还会将单词以松散的句子形式一行一行地打印出来。请在Python文件中添加以下代码：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To explain briefly, we first selected the categories `''bop''` and `''cocoa''`
    and printed every word from these two categories'' files. Every time we encountered
    a dot (`.`), we inserted a new line. Run the code and something similar to the
    following will be the output on the console:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单来说，我们首先选择了`'bop'`和`'cocoa'`这两个类别，并打印了这两个类别文件中的每个单词。每次遇到句号（`.`）时，我们都会插入一个换行符。运行代码后，控制台中会输出如下内容：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Download an external corpus, load it, and access it
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载外部语料库、加载并访问它
- en: Now that we have learned how to load and access an inbuilt corpus, we will learn
    how to download and also how to load and access any external corpus. Many of these
    inbuilt corpora are very good use cases for training purposes, but for solving
    any real-world problem, you will normally need an external dataset. For this recipe's
    purpose, we will be using the **Cornell CS Movie** review corpus, which is already
    labelled for positive and negative reviews and used widely for training sentiment
    analysis modules.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何加载和访问内置语料库，接下来我们将学习如何下载外部语料库以及如何加载和访问它们。许多内置语料库非常适合用于训练，但如果要解决实际的现实问题，通常需要使用外部数据集。本教程中我们将使用**康奈尔电影**评论语料库，它已经标注了正面和负面评论，广泛用于训练情感分析模型。
- en: Getting ready
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: First and foremost, you will need to download the dataset from the Internet.
    Here's the link: [http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip)](http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip)).
    Download the dataset, unzip it, and store the resultant `Reviews` directory at
    a secure location on your computer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要从互联网上下载数据集。链接如下：[http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip](http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip)。下载数据集，解压，并将解压后的`Reviews`目录保存在计算机的安全位置。
- en: How to do it...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create a new file named `external_corpus.py` and add the following import line
    to it:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`external_corpus.py`的新文件，并在其中添加以下导入行：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since the corpus that we have downloaded is already categorized, we will use
    `CategorizedPlaintextCorpusReader` to read and load the given corpus. This way,
    we can be sure that the categories of the corpus are captured, in this case, positive
    and negative.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们下载的语料库已经按类别进行了分类，我们将使用`CategorizedPlaintextCorpusReader`来读取和加载给定的语料库。这样，我们就可以确保语料库的类别被捕获，在本例中是正面和负面。
- en: 'Now we will read the corpus. We need to know the absolute path of the `Reviews`
    folder that we unzipped from the downloaded file from Cornell. Add the following
    four lines of code:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将读取语料库。我们需要知道从康奈尔下载的已解压`Reviews`文件夹的绝对路径。请添加以下四行代码：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first line is where you are reading the corpus by calling the `CategorizedPlaintextCorpusReader`
    constructor. The three arguments from left to right are Absolute Path to the `txt_sentoken`
    folder on your computer, all sample document names from the `txt_sentoken` folder,
    and the categories in the given corpus (in our case, `''pos''` and `''neg''`).
    If you look closely, you''ll see that all the three arguments are regular expression
    patterns. The next two lines will validate whether the corpus is loaded correctly
    or not, printing the associated categories and filenames of the corpus. Run the
    program and you should see something similar to the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行是通过调用`CategorizedPlaintextCorpusReader`构造函数来读取语料库。三个参数从左到右分别是计算机中`txt_sentoken`文件夹的绝对路径、`txt_sentoken`文件夹中的所有样本文档名以及给定语料库中的类别（在我们的例子中是`'pos'`和`'neg'`）。如果你仔细观察，你会发现这三个参数都是正则表达式模式。接下来的两行将验证语料库是否正确加载，打印语料库的相关类别和文件名。运行程序后，你应该看到类似以下内容：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we''ve made sure that the corpus is loaded correctly, let''s get on
    with accessing any one of the sample documents from both the categories. For that,
    let''s first create a list, each containing samples of both the categories, `''pos''`
    and `''neg''`, respectively. Add the following two lines of code:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经确认语料库加载正确，接下来让我们从两个类别中访问任意一个样本文档。为此，首先创建一个包含`'pos'`和`'neg'`类别样本的列表。添加以下两行代码：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `reader.fileids()` method takes the argument category name. As you can see,
    what we are trying to do in the preceding two lines of code is straightforward
    and intuitive.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`reader.fileids()`方法接受类别名称作为参数。如你所见，前两行代码的目的非常直接且直观。'
- en: 'Now let''s select a file randomly from each of the lists of `posFiles` and
    `negFiles`. To do so, we will need the `randint()` function from the `random`
    library of Python. Add the following lines of code and we shall elaborate what
    exactly we did immediately after:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们从`posFiles`和`negFiles`的文件列表中随机选择一个文件。为此，我们将需要Python的`random`库中的`randint()`函数。添加以下代码行，我们将在接下来详细说明我们所做的工作：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first line imports the `randint()` function from the `random` library. The
    next two files select a random file, each from the set of positive and negative
    category reviews. The last two lines just print the filenames.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行从`random`库中导入了`randint()`函数。接下来的两行代码从正面和负面类别评论集中随机选择一个文件。最后两行代码仅仅是打印文件名。
- en: 'Now that we have selected the two files, let''s access them and print them
    on the console sentence by sentence. We will use the same methodology that we
    used in the first recipe to print a line-by-line output. Append the following
    lines of code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经选择了这两个文件，让我们逐句访问它们，并在控制台中打印出来。我们将使用在第一个食谱中使用的方法，按行输出。追加以下代码行：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These `for` loops read every file one by one and will print on the console
    line by line. The output of the complete recipe should look similar to this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`for`循环逐个读取每个文件，并按行在控制台打印。完整食谱的输出应类似于以下内容：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The quintessential ingredient of this recipe is the `CategorizedPlaintextCorpusReader`
    class of NLTK. Since we already know that the corpus we have downloaded is categorized,
    we only need provide appropriate arguments when creating the `reader` object.
    The implementation of the `CategorizedPlaintextCorpusReader` class internally
    takes care of loading the samples in appropriate buckets (`'pos'` and `'neg'`
    in this case).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱的核心成分是NLTK的`CategorizedPlaintextCorpusReader`类。因为我们已经知道下载的语料库是分类的，所以在创建`reader`对象时只需要提供适当的参数。`CategorizedPlaintextCorpusReader`类的实现会在内部处理样本的加载，并将其放入适当的桶中（在本例中为`'pos'`和`'neg'`）。
- en: Counting all the wh words in three different genres in the Brown corpus
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计布朗语料库中三个不同类型的wh词
- en: The Brown corpus is part of the NLTK data package. It's one of the oldest text
    corpuses assembled at Brown University. It contains a collection of 500 texts
    broadly categorized in to 15 different genres/categories such as news, humor,
    religion, and so on. This corpus is a good use case to showcase the categorized
    plaintext corpus, which already has topics/concepts assigned to each of the texts
    (sometimes overlapping); hence, any analysis you do on it can adhere to the attached
    topic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Brown 语料库是 NLTK 数据包的一部分。它是布朗大学组建的最古老的文本语料库之一。它包含了500篇文本，广泛地分为15个不同的类别/体裁，如新闻、幽默、宗教等。这个语料库是展示分类纯文本语料库的一个很好的用例，它已经为每篇文本分配了主题/概念（有时会有重叠）；因此，您在其上进行的任何分析都可以遵循所附的主题。
- en: Getting ready
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: The objective of this recipe is to get you to perform a simple counting task
    on any given corpus. We will be using `nltk` library's `FreqDist` object for this
    purpose here, but more elaboration on the power of `FreqDist` will follow in the
    next recipe. Here, we will just concentrate on the application problem.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小节的目标是让你在任何给定的语料库上执行一个简单的计数任务。我们将在这里使用 `nltk` 库的 `FreqDist` 对象来实现这一点，但对于 `FreqDist`
    的强大功能，我们将在下一篇小节中做更详细的讲解。在这里，我们将专注于应用问题。
- en: How to do it...
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Create a new file named `BrownWH.py` and add the following `import` statements
    to begin:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `BrownWH.py` 的新文件，并添加以下 `import` 语句来开始：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have imported the `nltk` library and the Brown corpus.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经导入了 `nltk` 库和 Brown 语料库。
- en: 'Next up, we will check all the genres in the corpus and will pick any three
    categories from them to proceed with our task:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将检查语料库中的所有体裁，并从中选择任意三个类别来继续我们的任务：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `brown.categories()` function call will return the list of all genres in
    the Brown corpus. When you run this line, you will see the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`brown.categories()` 函数调用将返回 Brown 语料库中所有体裁的列表。当你运行这一行时，你将看到如下输出：'
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let''s pick three `genres`--`fiction`, `humor` and `romance`--from this
    list as well as the `whwords` that we want to count out from the text of these
    three `genres`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们从这个列表中挑选三个 `genres`——`fiction`（小说）、`humor`（幽默）和 `romance`（浪漫）——以及我们希望从这三个
    `genres` 的文本中统计的 `whwords`：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have created a list containing the three picked `genres` and another list
    containing the seven `whwords`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含三个选择的 `genres` 列表，以及另一个包含七个 `whwords` 的列表。
- en: Your list can be longer or shorter depending on what do you consider as `whwords`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你的列表可以更长或更短，取决于你认为哪些是 `whwords`。
- en: 'Since we have the `genres` and the words we want to count in lists, we will
    be extensively using the `for` loop to iterate over them and optimize the number
    of lines of code. So first, we write a `for` iterator on the `genres` list:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们已经将 `genres` 和我们想要计数的单词放入列表中，因此我们将广泛使用 `for` 循环来遍历它们，并优化代码行数。所以首先，我们在 `genres`
    列表上写一个 `for` 循环：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These four lines of code will only start iterating on the list `genres` and
    load the entire text of each genre in the `genre_text` variable as a continuous
    list words.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这四行代码只会开始遍历 `genres` 列表，并将每个体裁的整个文本加载到 `genre_text` 变量中，作为一个连续的单词列表。
- en: 'Next up is a complex little statement where we will use the `nltk` library''s
    `FreqDist` object. For now, let''s understand the syntax and the broad-level output
    we will get from it:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一个复杂的小语句，我们将使用 `nltk` 库的 `FreqDist` 对象。现在，我们先了解语法和它的粗略输出：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`FreqDist()` accepts a list of words and returns an object that contains the
    map word and its respective frequency in the input word list. Here, the `fdist`
    object will contain the frequency of each of the unique words in the `genre_text`
    word list.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`FreqDist()` 接受一个单词列表，并返回一个包含单词及其在输入单词列表中相应频率的对象。在这里，`fdist` 对象将包含 `genre_text`
    单词列表中每个唯一单词的频率。'
- en: 'I''m sure you''ve already guessed what our next step is going to be. We will
    simply access the `fdist` object returned by `FreqDist()` and get the count of
    each of the `wh` words. Let''s do it:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我相信你已经猜到我们接下来的步骤是什么了。我们将简单地访问由 `FreqDist()` 返回的 `fdist` 对象，并获取每个 `wh` 单词的计数。我们开始吧：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We are iterating over the `whwords` word list, accessing the `fdist` object
    with each of the `wh` words as index, getting back the frequency/count of all
    of them, and printing them out.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在遍历 `whwords` 单词列表，使用每个 `wh` 单词作为索引访问 `fdist` 对象，获取它们的频率/计数并将其打印出来。
- en: 'After running the complete program, you will get this output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完整的程序后，你将得到以下输出：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: On analyzing the output, you can clearly see that we have the word count of
    all seven `wh` words for the three picked `genres` on our console. By counting
    the population of `wh` words, you can, to a degree, gauge whether the given text
    is high on relative clauses or question sentences. Similarly, you may have a populated
    ontology list of important words that you want to get a word count of to understand
    the relevance of the given text to your ontology. Counting word populations and
    analyzing distributions of counts is one of the oldest, simplest, and most popular
    tricks of the trade to start any kind of textual analysis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析输出，你可以清楚地看到我们在控制台中得到了三个选定`genres`中所有七个`wh`词的词频。通过统计`wh`词的数量，你可以在一定程度上判断给定文本中相对从句或疑问句的使用频率。同样，你也可以列出一个重要单词的本体列表，统计这些单词的词频，以便了解给定文本与本体的相关性。统计单词的数量并分析词频分布是文本分析中最古老、最简单、最常用的技巧之一。
- en: Explore frequency distribution operations on one of the web and chat text corpus
    files
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一个网页和聊天文本语料库文件上探索频率分布操作
- en: Web and chat text corpus is non-formal literature that, as the name implies,
    contains content from Firefox discussion forums, scripts of movies, wine reviews,
    personal advertisements, and overheard conversations. Our objective here in this
    recipe is to understand the use of frequency distribution and its features/functions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 网页和聊天文本语料库是非正式文学，顾名思义，包含来自Firefox讨论论坛、电影剧本、葡萄酒评论、个人广告和偷听到的对话的内容。在本食谱中，我们的目标是理解频率分布的使用及其特性/功能。
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In keeping with the objective of this recipe, we will run the frequency distribution
    on the personal advertising file inside `nltk.corpus.webtext`. Following that,
    we will explore the various functionalities of the `nltk.FreqDist` object such
    as the count of distinct words, 10 most common words, maximum-frequency words,
    frequency distribution plot, and tabulation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本食谱的目标，我们将在`nltk.corpus.webtext`中的个人广告文件上运行频率分布。接着，我们将探索`nltk.FreqDist`对象的各种功能，比如不同单词的计数、10个最常见的单词、最大频率单词、频率分布图和表格等。
- en: How to do it...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Create a new file named `webtext.py` and add the following three lines to it:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`webtext.py`的新文件，并向其中添加以下三行代码：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We just imported the required libraries and the `webtext` corpus; along with
    that, we also printed the constituent file''s names. Run the program and you shall
    see the following output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚导入了所需的库和`webtext`语料库；同时，我们还打印出了组成文件的名称。运行程序，你将看到以下输出：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we will select the file that contains personal advertisement data and and
    run frequency distribution on it. Add the following three lines for it:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将选择包含个人广告数据的文件，并对其进行频率分布处理。为此，添加以下三行代码：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`singles.txt` contains our target data; so, we loaded the words from that file
    in `wbt_words` and ran frequency distribution on it to get the `FreqDist` object
    `fdist`.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`singles.txt`包含我们的目标数据，因此我们从该文件加载了单词到`wbt_words`中，并对其进行了频率分布操作，得到了`FreqDist`对象`fdist`。'
- en: 'Add the following lines, which will show the most commonly appearing word (with
    the `fdist.max()` function) and the count of that word (with the `fdist[fdist.max()`]
    operation):'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下代码，这将显示最常出现的单词（使用`fdist.max()`函数）以及该单词的计数（使用`fdist[fdist.max()]`操作）：
- en: '[PRE29]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following line will show us the count of distinct words in the bag of our
    frequency distribution using the  `fdist.N()` function. Add the line in your code:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下这行将通过`fdist.N()`函数显示我们频率分布包中不同单词的计数。在你的代码中添加这一行：
- en: '[PRE30]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let''s find out the 10 most common words in the selected corpus bag. The
    function `fdist.most_common()` will do this for us. Add the following two lines
    in the code:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们找出所选语料库包中最常见的10个单词。`fdist.most_common()`函数将为我们完成这个任务。在代码中添加以下两行：
- en: '[PRE31]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let us tabulate the entire frequency distribution using the `fdist.tabulate()`
    function. Add these lines in the code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`fdist.tabulate()`函数列出整个频率分布。在代码中添加这些行：
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we will plot the graph of the frequency distribution with `cumulative`
    frequencies using the `fdist.plot()` function:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`fdist.plot()`函数绘制带有`cumulative`频率的频率分布图：
- en: '[PRE33]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s run the program and see the output; we will discuss the same in the
    following section:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行程序并查看输出；我们将在下一节中讨论该输出：
- en: '[PRE34]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You will also see the following graph pop up:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将看到以下图表弹出：
- en: '>![](img/d21e36af-4aae-4c1c-97a9-fe2f637fefe2.png)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '>![](img/d21e36af-4aae-4c1c-97a9-fe2f637fefe2.png)'
- en: How it works...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Upon analyzing the output, we realize that all of it is very intuitive. But
    what is peculiar is that most of it is not making sense. The token with maximum
    frequency count is `,`. And when you look at the `10` most common tokens, again
    you can't make out much about the target dataset. The reason is that there is
    no preprocessing done on the corpus. In the third chapter, we will learn one of
    the most fundamental preprocessing steps called stop words treatment and will
    also see the difference it makes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析输出后，我们意识到所有内容都非常直观。但奇怪的是，其中大多数并没有什么意义。出现频率最高的标记是`,`。当你查看`10`个最常见的标记时，仍然无法从目标数据集中获取太多信息。原因是语料库没有经过预处理。在第三章中，我们将学习一种最基础的预处理步骤，称为停用词处理，并且我们还将看到它带来的差异。
- en: Take an ambiguous word and explore all its senses using WordNet
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用WordNet探索一个模糊词汇的所有含义
- en: From this recipe onwards, we will turn our attention to WordNet. As you can
    read in the title, we are going to explore what word sense is. To give an overview,
    English is a very ambiguous language. Almost every other word has a different
    meaning in different contexts. For example, let's take the simplest of words,
    *bat* which you will learn as part of the first 10 English words in a language
    course almost anywhere on the planet. The first meaning is a club used for hitting
    the ball in various sports such as cricket, baseball, tennis, squash, and so on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个配方开始，我们将把注意力转向WordNet。正如标题中所说，我们将探索什么是词义。概述一下，英语是一种非常模糊的语言。几乎每个词在不同的上下文中都有不同的意思。例如，让我们以最简单的词之一*bat*为例，这是几乎在地球上任何语言课程中都会教授的前10个英语单词之一。第一个含义是用来击球的球棒，广泛应用于板球、棒球、网球、壁球等多种运动。
- en: Now a *bat* can also mean a nocturnal mammal that flies at nights. The *Bat*
    is also Batman's preferred and most advanced transportation vehicle according
    to DC comics. These are all noun variants; let's consider verb possibilities.
    *Bat* can also mean a slight wink (bat an eyelid). Consequently, it can also mean
    beating someone to pulp in a fight or a competition. We believe that's enough
    of an introduction; with this, let's move on to the actual recipe.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*bat* 还可以指代一种夜间活动的哺乳动物，它在夜间飞行。*Bat* 也是根据DC漫画，蝙蝠侠首选并最先进的交通工具。这些都是名词的变体；让我们来考虑动词的可能性。*Bat*
    还可以指稍微眨一下眼睛（bat an eyelid）。因此，它还可以意味着在打斗或竞赛中把某人打得粉碎。我们认为这已经足够做为介绍；接下来我们将进入实际的配方。
- en: Getting ready
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: Keeping the objective of the recipe in mind we have to choose a word for which
    we would be exploring its various senses as understood by WordNet. And yes, NLTK
    comes equipped with WordNet; you need not worry about installing any further libraries.
    So let's choose another simple word, *CHAIR*, as our sample for the purpose of
    this recipe.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记配方的目标，我们需要选择一个单词，来探索WordNet理解下的各种含义。是的，NLTK已经内置了WordNet，你不必担心安装任何额外的库。所以，让我们选择另一个简单的词，*CHAIR*，作为本配方的示例。
- en: How to do it...
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Create a new file named `ambiguity.py` and add the following lines of code
    to start with:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`ambiguity.py`的新文件，并添加以下代码行作为起点：
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here we imported the required NLTK corpus reader `wordnet` as the `wn` object.
    We can import it just like any another corpus readers we have used so far. In
    preparation for the next steps, we have created our string variable containing
    the word `chair`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入了所需的NLTK语料库读取器`wordnet`作为`wn`对象。我们可以像使用其他任何语料库读取器一样导入它。为了准备接下来的步骤，我们创建了一个包含单词`chair`的字符串变量。
- en: 'Now is the most important step. Let''s add two lines and I will elaborate what
    we are doing:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是最重要的步骤。让我们添加两行代码，我会详细说明我们正在做的事情：
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The first line, though it looks simple, is actually the API interface that
    is accessing the internal WordNet database and fetching all the senses associated
    with the word `chair`. WordNet calls each of these senses  `synsets`. The next
    line simply asks the interpreter to print what it has fetched. Run this much and
    you should get an output like:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行，尽管看起来很简单，实际上是访问内部WordNet数据库并提取与单词`chair`相关的所有含义的API接口。WordNet将每个这样的含义称为`synsets`。接下来的一行只是要求解释器打印它提取的内容。运行这一部分，你应该得到类似的输出：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see, the list contains seven `Synsets`, which means seven different
    senses of the word `Chair` exist in the WordNet database.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，列表中包含七个`Synsets`，这意味着在WordNet数据库中存在七种不同的`Chair`的含义。
- en: 'We will add the following `for` loop, which will iterate over the list of `synsets`
    we have obtained and perform certain operations:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加以下`for`循环，它将遍历我们获取的`synsets`列表并执行某些操作：
- en: '[PRE38]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We are iterating over the list of `synsets` and printing the definition of
    each sense, associated lemmas/synonymous words, and example usage of each of the
    senses in a sentence. One typical iteration will print something similar to this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在遍历`synsets`列表并打印每个意义的定义，相关的Lemmas/同义词以及每个意义在句子中的示例用法。一个典型的迭代将打印类似于这样的内容：
- en: '[PRE39]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The first line is the name of `Synset`, the second line is the definition of
    this sense/`Synset`, the third line contains `Lemmas` associated with this `Synset`,
    and the fourth line is an example sentence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行是`Synset`的名称，第二行是这个意义/`Synset`的定义，第三行包含与这个`Synset`相关的`Lemmas`，第四行是一个示例句子。
- en: 'We will obtain this output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下输出：
- en: '[PRE40]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: As you can see, definitions, Lemmas, and example sentences of all seven senses
    of the word `chair` are seen in the output. Straightforward API interfaces are
    available for each of the operations as elaborated in the preceding code sample.
    Now, let's talk a little bit about how WordNet arrives at such conclusions. WordNet
    is a database of words that stores all information about them in a hierarchical
    manner. If we take a look at the current example Write about `synsets` and hierarchical
    nature of WordNet storage. The following diagram will explain it in more detail.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，单词`chair`的七个意义的定义、Lemmas和示例句子都在输出中看到。每个操作都有直接的API接口，如前面代码示例中所详细阐述的。现在，让我们稍微谈一下WordNet是如何得出这样的结论的。WordNet是一个按层次结构存储所有关于单词的信息的数据库。如果我们看一下当前示例，写一下关于`synsets`和WordNet存储的层次性质。以下图表将更详细地解释这一点。
- en: Pick two distinct synsets and explore the concepts of hyponyms and hypernyms
    using WordNet
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择两个不同的synsets并使用WordNet探索下义词和上义词的概念
- en: A hyponym is a word of a more specific meaning than a more generic word such
    as *bat,* which we explored in the introduction section of our previous recipe.
    What we mean by *more specific* is, for example, cricket bat, baseball bat, carnivorous
    bat, squash racket, and so on. These are more specific in terms of communicating
    what exactly we are trying to mean.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下义词是一个比更通用的词（如*bat*）更具体含义的词，我们在前一篇食谱的介绍部分探讨过。我们所说的*更具体*是指，例如，板球拍、棒球棒、食肉蝙蝠、壁球拍等。这些在传达我们确切意图方面更具体。
- en: As opposed to a hyponym, a hypernym is a more general form or word of the same
    concept. For our example, *bat* is a more generic word and it could mean club,
    stick, artifact, mammal, animal, or organism. We can go as generic as the physical
    entity, living thing, or object and still be considered as a hypernym of the word
    *bat*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与下义词相反，上义词是同一概念的更一般形式或词语。以*bat*为例，它是一个更通用的词，可以表示棍棒、球棒、器物、哺乳动物、动物或有机体。我们可以将其泛化为物理实体、生物体或物体，仍然被视为*bat*的上义词。
- en: Getting ready
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: For the purpose of exploring the concepts of hyponym and hypernym, we have decided
    to select the synsets `bed.n.01` (first word sense of bed) and `woman.n.01` (second
    word sense of woman). Now we will explain the usage and meaning of the hypernym
    and hyponym APIs in the actual recipe section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索下义词和上义词的概念，我们决定选择`bed.n.01`（bed的第一个词义）和`woman.n.01`（woman的第二个词义）这两个同义词集。现在我们将在实际的步骤部分解释上义词和下义词API的用法和含义。
- en: How to do it...
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create a new file named `HypoNHypernyms.py` and add following three lines:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`HypoNHypernyms.py`的新文件，并添加以下三行：
- en: '[PRE41]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We've imported the libraries and initialized the two synsets that we will use
    in later processing.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经导入了库并初始化了稍后将在处理中使用的两个synsets。
- en: 'Add the following two lines:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下两行：
- en: '[PRE42]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'It''s a simple call to the `hypernyms()` API function on the woman `Synset`;
    it will return the set of synsets that are direct parents of the same. However,
    the `hypernym_paths()` function is a little tricky. It will return a list of sets.
    Each set contains the path from the root node to the woman `Synset`. When you
    run these two statements, you will see the two direct parents of the `Synset`
    woman as follows in the console:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对woman `Synset`上的`hypernyms()` API函数的简单调用；它将返回直接父级的同义词集。然而，`hypernym_paths()`函数有点棘手。它将返回一组集合。每个集合包含从根节点到woman
    `Synset`的路径。当您运行这两个语句时，您将在控制台中看到`Synset` woman的两个直接父级如下：
- en: '[PRE43]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Woman belongs to the adult and female categories in the hierarchical structure
    of the WordNet database.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在WordNet数据库的层级结构中，"woman"属于成人和女性类别。
- en: 'Now we will try to print the paths from root node to the `woman.n.01` node.
    To do so, add the following lines of code and nested `for` loop:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将尝试打印从根节点到`woman.n.01`节点的路径。为此，添加以下几行代码并嵌套`for`循环：
- en: '[PRE44]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As explained, the returned object is a list of sets ordered in such a way that
    it follows the path from the root to the `woman.n.01` node exactly as stored in
    the WordNet hierarchy. When you run, here''s an example `Path`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，返回的对象是一个按特定顺序排列的集合列表，这个顺序精确地跟随从根节点到`woman.n.01`节点的路径，正如它在WordNet层级中存储的那样。当你运行时，下面是一个示例`Path`：
- en: '[PRE45]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now let''s work with `hyponyms`. Add the following two lines, which will fetch
    the `hyponyms` for the synset `bed.n.01` and print them to the console:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们来处理`hyponyms`。添加以下两行代码，它们将获取`bed.n.01`的`hyponyms`并打印到控制台：
- en: '[PRE46]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As explained, run them and you will see the following 20 synsets as output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，运行它们，你将看到以下20个synset作为输出：
- en: '[PRE47]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: These are `Hyponyms` or more specific terms for the word sense `bed.n.01` within
    WordNet.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是WordNet中`sense`为`bed.n.01`的`Hyponyms`，即更具体的术语。
- en: 'Now let''s print the actual words or `lemmas` that will make more sense to
    humans. Add the following line of code:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们打印实际的词汇或`lemmas`，这些词汇对于人类来说更有意义。添加以下一行代码：
- en: '[PRE48]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This line of code is pretty similar to what we did in the hypernym example
    nested `for` loop written in four lines, which is clubbed in a single line here
    (in other words, we''re just showing off our skills with Python here). It will
    print the 26 `lemmas` that are very meaningful and specific words. Now let''s
    look at the final output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码和我们之前在超类示例中的嵌套`for`循环非常相似，四行代码合并成一行（换句话说，我们在这里展示了Python的技巧）。它将打印出26个非常有意义且具体的`lemmas`。现在，让我们看看最终输出：
- en: '[PRE49]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: How it works...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: As you can see, `woman.n.01` has two hypernyms, namely adult and female, but
    it follows four different routes in the hierarchy of WordNet database from the
    root node `entity` to `woman` as shown in the output.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`woman.n.01`有两个上位词，分别是成人和女性，但它在WordNet数据库的层级结构中从根节点`entity`到`woman`有四条不同的路径，正如输出中所示。
- en: Similarly, the Synset `bed.n.01` has 20 hyponyms; they are more specific and
    less ambiguous (for nothing is unambiguous in English). Generally the hyponyms
    correspond to leaf nodes or nodes very much closer to the leaves in the hierarchy
    as they are the least ambiguous ones.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Synset `bed.n.01`有20个下位词；它们更为具体且不那么模糊（毕竟英语中没有绝对不模糊的词）。通常，这些下位词对应的是叶节点或靠近叶节点的节点，因为它们是最不模糊的词。
- en: Compute the average polysemy of nouns, verbs, adjectives, and adverbs according
    to WordNet
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据WordNet计算名词、动词、形容词和副词的平均多义性。
- en: First, let's understand what polysemy is. Polysemy means many possible meanings
    of a word or a phrase. As we have already seen, English is an ambiguous language
    and more than one meaning usually exists for most of the words in the hierarchy.
    Now, turning back our attention to the problem statement, we must calculate the
    average polysemy based on specific linguistic properties of all words in WordNet.
    As we'll see, this recipe is different from previous recipes. It's not just an
    API concept discovery but we are going to discover a linguistic concept here (I'm
    all emotional to finally get a chance to do so in this chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解什么是多义性。多义性意味着一个词或短语有多种可能的含义。正如我们已经看到的，英语是一种模糊的语言，对于层级中的大多数词汇，通常会有不止一个含义。现在，回到问题陈述，我们必须根据WordNet中所有词汇的特定语言学属性来计算平均多义性。正如我们将看到的，这个配方不同于之前的配方。它不仅仅是一个API概念的发现，而是我们将发现一个语言学概念（我激动得终于有机会在这一章中做这件事）。
- en: Getting ready
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: I have decided to write the program to compute the polysemy of any one of the
    POS types of words and will leave it to you guys to modify the program to do so
    for the other three. I mean we shouldn't just spoon-feed everything, right? Not
    to worry! I will provide enough hints in the recipe itself to make it easier for
    you (for those who think it's already not very intuitive). Let's get on with the
    actual recipe then; we will compute the average polysemy of nouns alone.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定编写程序计算任何词性类型词汇的多义性，并将其余三个类型的计算留给大家修改。我是说，我们不应该什么都喂给你们对吧？别担心！我会在实际代码中提供足够的提示，让你们更容易理解（对于那些觉得已经不太直观的人）。那么，让我们开始实际的代码吧；我们将单独计算名词的平均多义性。
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create a new file named `polysemy.py` and add these two initialization lines:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的文件，命名为`polysemy.py`，并添加以下两行初始化代码：
- en: '[PRE50]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We have initialized the POS type of words we are interested in and, of course,
    imported the required libraries. To be more descriptive, `n` corresponds to nouns.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经初始化了我们感兴趣的词性类型，当然，也导入了所需的库。为了更具描述性，`n`代表名词。
- en: 'This is the most important line of code of this recipe:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是这段配方中最重要的代码行：
- en: '[PRE51]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This API returns all `synsets` of type `n` that is a noun present in the WordNet
    database, full coverage. Similarly, if you change the POS type to a verb, adverb,
    or adjective, the API will return all words of the corresponding type (hint #1).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个API返回WordNet数据库中所有类型为`n`（名词）的`synsets`，实现全覆盖。同样，如果你将词性类型更改为动词、副词或形容词，API将返回相应类型的所有单词（提示#1）。
- en: 'Now we will consolidate all `lemmas` in each of the `synset` into a single
    mega list that we can process further. Add the following code to do that:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将把每个`synset`中的所有`lemmas`合并成一个单一的超大列表，供我们进一步处理。添加以下代码来实现这一点：
- en: '[PRE52]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This piece of code is pretty intuitive; we have a nested `for` loop that iterates
    over the list of `synsets` and the `lemmas` in each `synset` and adds them up
    in our mega list lemmas.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常直观；我们有一个嵌套的`for`循环，它遍历`synsets`列表和每个`synset`中的`lemmas`，并将它们加到我们的超大列表`lemmas`中。
- en: 'Although we have all `lemmas` in the mega list, there is a problem. There are
    some duplicates as it''s a list. Let''s remove the duplicates and take the count
    of distinct `lemmas`:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们在超大列表中拥有所有`lemmas`，但是有一个问题。由于它是一个列表，其中存在一些重复项。让我们移除这些重复项并统计不同的`lemmas`数量：
- en: '[PRE53]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Converting a list into a set will automatically deduplicate (yes, it's a valid
    English word, I invented it) the list.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表转换为集合会自动去重（是的，这是一个有效的英语单词，我发明的）列表。
- en: 'Now, the second most important step in the recipe. We count the senses of each
    `lemma` in the WordNet database:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，配方中的第二个最重要的步骤。我们在WordNet数据库中统计每个`lemma`的词义：
- en: '[PRE54]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Most of the code is intuitive; let''s focus on the the API `wn.synsets(lemma,
    type)`. This API takes as input a word/lemma (as the first argument) and the POS
    type it belongs to and returns all the senses (`synsets`) belonging to the `lemma`
    word. Note that depending on what you provide as the POS type, it will return
    senses of the word of only the given POS type (hint #2).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码是直观的；我们来专注于API `wn.synsets(lemma, type)`。这个API接受一个单词/词元（作为第一个参数）和它所属的词性类型，并返回所有属于该`lemma`单词的词义（`synsets`）。注意，根据你提供的词性类型，它只会返回该类型的词义（提示#2）。
- en: 'We have all the counts we need to compute the average polysemy. Let''s just
    do it and print it on the console:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经得到了计算平均多义性所需的所有计数。现在我们直接计算并打印到控制台：
- en: '[PRE55]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This prints the total distinct lemmas, the count of senses, and the average
    polysemy of POS type `n` or nouns:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出名词类型`n`的所有不同词元的总数、词义的计数以及平均多义性：
- en: '[PRE56]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'There is nothing much to say in this section, so I will instead give you some
    more information on how to go about computing the polysemy of the rest of the
    types. As you saw, *Noun -> ''n''*. Similarly, *Verbs -> ''v''*, *Adverbs -> ''r''*,
    and *Adjective -> ''a''* (hint # 3).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分没什么太多要说的，所以我会给你一些关于如何计算其他类型多义性的更多信息。正如你所看到的，*名词 -> 'n'*。同样，*动词 -> 'v'*，*副词
    -> 'r'*，*形容词 -> 'a'*（提示#3）。
- en: Now, I hope I have given you enough hints to get on with writing an NLP program
    of your own and not be dependent on the feed of the recipes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我希望我已经给了你足够的提示，帮助你自己动手编写一个NLP程序，而不依赖于配方的推送。
