- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Exploring Unsupervised Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索无监督深度学习
- en: Unsupervised learning works with data that does not have any labels. More broadly,
    unsupervised learning aims to uncover the intrinsic patterns hidden within the
    data. The most rigorous and expensive part of a supervised machine learning project
    is the labels required for a given data. In the real world, there is tons of unlabeled
    data available with tons of information that could be learned from. Frankly, it’s
    impossible to obtain labels for all of the data that exist in the world. Unsupervised
    learning is the key to unlocking the potential of the abundant unlabeled digital
    data we have today. Let’s explore a hypothetical situation below to understand
    this better.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习适用于没有标签的数据。更广泛地说，无监督学习旨在揭示数据中隐藏的内在模式。监督机器学习项目中最严格和最昂贵的部分是为给定数据所需的标签。在现实世界中，有大量的无标签数据，其中包含可以学习到的丰富信息。坦率地说，获取世界上所有数据的标签是不可能的。无监督学习是解锁我们今天拥有的丰富无标签数字数据潜力的关键。让我们通过以下假设情境更好地理解这一点。
- en: Imagine that it costs 1 USD and 1 minute to obtain a label for a row of data
    for whatever use case it could be, and a single unit of information can be obtained
    through supervised learning. To get 10,000 units of information, 10,000 USD would
    need to be spent, and 10,000 minutes need to be contributed to obtain 10,000 pieces
    of labeled data. Both time and money are painful things to burn. However, for
    unsupervised learning, it costs 0 USD and 0 minutes to obtain 0.01 units of information
    through the same data without a label. Since the amount of data is not impeded
    by time or money, we can easily get 100 times more data than the 10,000 samples
    and get the same information that can be learned through a model. When money and
    time aren’t an issue, the amount of information your model can learn is endless,
    assuming that your unsupervised learning model has the capacity and ability to
    do so.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，获取一行数据的标签，无论其使用场景是什么，都需要花费1美元和1分钟，并且通过监督学习可以获得单个信息单元。要获取10,000个信息单元，需要花费10,000美元并贡献10,000分钟来获得10,000条标注数据。无论是时间还是金钱，都是令人痛苦的消耗。然而，对于无监督学习，通过相同的数据（没有标签），获取0.01个信息单元的成本是0美元和0分钟。由于数据量不受时间或金钱的限制，我们可以轻松地获取比10,000个样本多100倍的数据，并通过模型学习到相同的信息。当金钱和时间不再是问题时，假设无监督学习模型具备足够的容量和能力，你的模型可以学习到的知识量是无限的。
- en: 'Deep learning provides a competitive edge to the unsupervised learning field,
    given the huge capacity and ability to learn complex information. If we can crack
    the code of unsupervised deep learning, it will set the stage for models that
    can come close to general intelligence one day! In this chapter, we will explore
    the notable components of unsupervised deep learning by taking a look at the following
    topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为无监督学习领域提供了竞争优势，因为它具有巨大的容量和学习复杂信息的能力。如果我们能够破解无监督深度学习的代码，它将为未来的模型铺平道路，这些模型有望接近通用智能！在本章中，我们将通过以下主题来探讨无监督深度学习的关键组成部分：
- en: Exploring unsupervised deep learning applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索无监督深度学习应用
- en: Creating pretrained network weights for downstream tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为下游任务创建预训练网络权重
- en: Creating general representations through unsupervised deep learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过无监督深度学习创建通用表示
- en: Exploring zero-shot learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索零样本学习
- en: Exploring the dimensionality reduction component of unsupervised deep learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索无监督深度学习中的降维组件
- en: Detecting anomalies in external data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测外部数据中的异常
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些**Python**编程语言中的实际实现。要完成本章内容，您需要一台安装了以下库的计算机：
- en: '`pandas`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`CLIP`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLIP`'
- en: '`numpy`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`pytorch`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`pillow`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pillow`'
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上的[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9)找到本章的代码文件。
- en: Exploring unsupervised deep learning applications
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索无监督深度学习应用
- en: 'Today, practitioners have been able to leverage unsupervised deep learning
    to tap into their unlabeled data to achieve either one of the following use cases.
    These have been put in descending order in terms of their impact and usefulness:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，实践者已经能够利用无监督深度学习挖掘其未标记数据，从而实现以下任一应用场景。这些应用按其影响力和实用性从高到低排列：
- en: Creating pretrained network weights for downstream tasks
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为下游任务创建预训练网络权重
- en: Creating general representations that can be used as-is in downstream supervised
    tasks by predictive supervised models
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建可以直接用于下游监督任务的通用表示，这些表示可以被预测的监督模型直接使用
- en: Achieving one-shot and zero-shot learning
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一-shot学习和零-shot学习
- en: Performing dimensionality reduction
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行降维
- en: Detect anomalies in external data
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测外部数据中的异常
- en: Clustering the provided training data into groups
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提供的训练数据进行聚类分组
- en: To start, note that pure clustering is still a core application of unsupervised
    learning in general, but not for deep learning. **Clustering** is where unlabeled
    data is grouped into multiple arbitrary clusters or classes. This will be useful
    in use cases such as customer segmentation for targeted responses, or topic modeling
    to figure out trendy topics people are discussing on social media. In clustering,
    the relationship between the unlabeled data samples is leveraged to find groups
    of data that are close together. Some clustering techniques group this data by
    assuming a spherical distribution in each cluster, such as **K-means**. Some other
    clustering techniques are more adaptive and can find clusters of multiple distributions
    and sizes, such as **HDBSCAN**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，纯聚类仍然是无监督学习的核心应用，但对深度学习而言并非如此。**聚类**是将未标记的数据分成多个任意的聚类或类别。这对于一些应用场景非常有用，例如针对性响应的客户细分，或者通过主题建模来发现人们在社交媒体上讨论的流行话题。在聚类中，利用未标记数据样本之间的关系来寻找彼此接近的数据群体。有些聚类技术通过假设每个聚类中数据呈球形分布来分组数据，例如**K-means**。其他一些聚类技术则更具适应性，能够找到多种分布和大小的聚类，例如**HDBSCAN**。
- en: Deep learning methods have not been able to produce any significant improvement
    from non-deep learning clustering methods such as the simple *k*-means algorithm
    or the HDBSCAN algorithm. However, efforts have been made to utilize clustering
    itself to aid the unsupervised pretraining of neural network models. To realize
    it in a neural network model as a component, the clustering model has to be differentiable
    so that gradients can still be propagated to the entire network. These methods
    are not superior to non-deep learning techniques such as *k*-means or HDBSCAN
    and are simply a variation so that the concept of clustering can be realized in
    a neural network. An example application of clustering in unsupervised pretraining
    is **SwaV**, which will be introduced in the next section. However, for completeness,
    one example of a neural network-based clustering algorithm that is used traditionally
    is self-organizing maps, but the network itself is not considered a deep network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法未能在传统非深度学习聚类方法如简单的*k*-均值算法或HDBSCAN算法上带来显著的改进。然而，已经有尝试通过利用聚类本身来辅助神经网络模型的无监督预训练。为了将其作为神经网络中的一个组件实现，聚类模型必须是可微的，以便梯度可以传递到整个网络。这些方法并不优于非深度学习技术，如*k*-均值或HDBSCAN，只不过是聚类概念的一种变体，以便在神经网络中实现聚类。无监督预训练中的一个聚类应用实例是**SwaV**，将在下一节介绍。然而，为了完整性，传统上使用的一种基于神经网络的聚类算法是自组织映射（Self-Organizing
    Maps），但该网络本身并不被视为深度网络。
- en: In the next few sections, we will discover the other five applications more
    comprehensively, ordered by their impact and usefulness, as shown previously.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将更全面地探讨另外五个应用，按其影响力和实用性排序，如前所示。
- en: Creating pretrained network weights for downstream tasks
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为下游任务创建预训练网络权重
- en: Also known as unsupervised transfer learning, this method is analogous to supervised
    transfer learning and naturally reaps the same benefits as described in the *Transfer
    learning* section in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125), *Exploring
    Supervised Deep Learning*. But as a recap, let’s go through an analogy. Imagine
    you’re a chef who has spent years learning how to cook a variety of dishes, from
    pasta and steak to desserts. One day, you’re asked to cook a new dish you’ve never
    tried before; let’s call it “Dish X.” Instead of starting from scratch, you use
    your prior knowledge and experience to simplify the process. You know how to chop
    vegetables, how to use the oven, and how to adjust the heat, so you don’t have
    to relearn all of these steps. You can focus your energy on learning the specific
    ingredients and techniques required for Dish X This is similar to how transfer
    learning works in machine learning, which applies to both unsupervised learning
    and supervised learning. A model that has already been trained on a related task
    can be used as a starting point, allowing the model to learn new tasks more quickly
    and effectively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也被称为无监督迁移学习，它类似于有监督迁移学习，并自然地获得了与[*第8章*](B18187_08.xhtml#_idTextAnchor125)《探索有监督深度学习》中所描述的相同好处。但是，为了回顾一下，让我们通过一个类比来说明。假设你是一名厨师，已经花了多年时间学习如何烹饪各种菜肴，从意大利面和牛排到甜点。一天，你被要求做一道从未尝试过的新菜肴，我们称之为“菜肴X”。你不是从零开始，而是利用已有的知识和经验来简化过程。你知道如何切菜，如何使用烤箱，如何调节火候，因此不需要重新学习这些步骤。你可以将精力集中在学习“菜肴X”所需的特定食材和技巧上。这类似于迁移学习在机器学习中的应用，适用于无监督学习和有监督学习。已经在相关任务上训练过的模型可以作为起点，从而使得模型能够更快速、有效地学习新任务。
- en: Besides being able to use the unlimited amount of unlabeled data available in
    the world, unsupervised transfer learning also bears another benefit. Labels in
    supervised learning often hold biases that the model will adopt. The biases acquired
    through learning can obstruct the acquisition of more generalized knowledge that
    would be more useful for downstream tasks, to varying extents. Other than biases
    in the labels, there are also situations where labels are wrong. Being unsupervised
    means that the model is stripped of any possibility of learning biases or errors
    from any labels. However, note that biases are more prominent in some datasets.
    A dataset with a complex task that has quality-related labels derived qualitatively
    from human judgment tends to have more biases compared to a simple task such as
    classifying whether a picture has a face or not.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能够利用世界上无尽的未标注数据，无监督迁移学习还具有另一个好处。监督学习中的标签通常包含偏差，而模型会采纳这些偏差。通过学习获得的偏差可能会妨碍获取更通用的知识，而这些知识对下游任务可能更有用。除了标签中的偏差外，还有一些情况下标签本身就是错误的。无监督的含义是，模型不会从任何标签中学习到偏差或错误。然而，需要注意的是，在某些数据集中，偏差更为显著。具有质量相关标签（这些标签通常通过人工判断得出）的复杂任务数据集，相较于像判断图片是否有脸这样的简单任务，通常会包含更多的偏差。
- en: Now, let’s dive into the techniques. Unsupervised transfer learning has been
    rated as the most impactful and useful application due to contributions that were
    made in the NLP field. Transformers, introduced in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092),
    *Understanding Neural Network Transformers*, paved the way for the paradigm to
    pre-train your model with an unsupervised learning technique more commonly known
    as **self-supervised learning**. How this method is categorized here is a matter
    of perspective and not everybody would agree with it. Self-supervised learning
    leverages only the relationship of co-occurring data to pre-train a neural network
    with relational knowledge without labels. Seeing it this way, check out the following
    question, and decide your own answer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨这些技术。无监督迁移学习被评为最具影响力和实用的应用之一，特别是在自然语言处理（NLP）领域的贡献。变压器模型（Transformers），在[*第6章*](B18187_06.xhtml#_idTextAnchor092)《理解神经网络变压器》中介绍，为使用一种更常见的无监督学习技术——**自监督学习**预训练模型的范式铺平了道路。此方法的分类问题更多的是视角的问题，并不是所有人都会同意这一分类。自监督学习仅利用共现数据之间的关系，在没有标签的情况下预训练神经网络以获得关系性知识。从这个角度来看，看看以下问题，并决定你自己的答案。
- en: Try it yourself
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 亲自试试
- en: Traditional unsupervised learning data preprocessing techniques such as **Principal
    Component Analysis** (**PCA**) leverage the relationship of co-occurring data
    to build new features that can better represent impactful patterns. Do you see
    PCA as its own category with self-supervising, under the umbrella of supervised
    learning, or under the umbrella of unsupervised learning?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的无监督学习数据预处理技术如 **主成分分析**（**PCA**）利用数据的共现关系来构建可以更好地表示重要模式的新特征。你是否认为 PCA 是自监督学习的一个类别，其下属于监督学习的大伞下，还是属于无监督学习的大伞下？
- en: '*The author’s* *answer: unsupervised*!'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者的* *答案：无监督*！'
- en: The unsupervised learning techniques that are used in transformers are masked
    language modeling and next-sentence prediction. These tasks help the model learn
    the relationships between words and sentences, allowing it to better understand
    the meaning and context of language data. A model that has been trained on masked
    language modeling and next-sentence prediction tasks can use its understanding
    of language to perform better on a variety of NLP downstream tasks. These are
    proven by the SoTA predictive performances on various datasets from leading transformers
    today, such as DeBERTa, as introduced in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092),
    *Understanding Neural* *Network Transformers*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 transformers 中使用的无监督学习技术包括遮蔽语言建模和下一句预测。这些任务帮助模型学习单词和句子之间的关系，使其能够更好地理解语言数据的含义和上下文。通过在遮蔽语言建模和下一句预测任务上训练的模型可以利用其对语言的理解在各种
    NLP 下游任务上表现更好。这些技术在当今领先的 transformers（如在 [*第 6 章*](B18187_06.xhtml#_idTextAnchor092)
    中介绍的 DeBERTa）上已被证明具有 SoTA 的预测性能。
- en: 'Let’s briefly go through other examples of unsupervised pre-training:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要介绍其他无监督预训练的例子：
- en: '**A simple framework for contrastive learning of visual representations** (**SimCLR**):
    SimCLR utilizes a method called **contrastive learning** to pretrain convolutional
    neural networks. Contrastive learning is a key technique in unsupervised deep
    learning that helps neural networks learn representations of data by optimizing
    the distance between related features. The core idea behind contrastive learning
    is to bring features of similar data points closer together and push features
    of dissimilar data points further apart in the feature space, using a contrastive
    loss function. While there are various forms of contrastive loss today, the general
    idea is to minimize the distance between similar examples and maximize the distance
    between dissimilar examples. This distance can be measured in various ways, such
    as Euclidean distance or cosine distance. Although this method requires labels
    for learning and is technically a supervised learning loss, the features learned,
    along with the selection of similar and dissimilar samples for label-free samples,
    make this loss function a crucial technique in unsupervised deep learning. The
    simplest representation of such a contrastive loss is as follows:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对视觉表征进行对比学习的简单框架**（**SimCLR**）：SimCLR 利用一种称为 **对比学习** 的方法对卷积神经网络进行预训练。对比学习是无监督深度学习中的一种关键技术，帮助神经网络通过优化相关特征之间的距离来学习数据的表示。对比学习的核心思想是将相似数据点的特征拉近，将不相似数据点的特征推远，在特征空间中使用对比损失函数进行。尽管今天有各种形式的对比损失函数，但其一般思想是最小化相似样本之间的距离，并最大化非相似样本之间的距离。这种距离可以通过不同方式衡量，如欧氏距离或余弦距离。虽然此方法需要用于学习的标签，从技术上讲是一种监督学习损失，但所学习的特征以及对类别自由样本的相似和不相似样本的选择，使得此损失函数成为无监督深度学习中的关键技术。这种对比损失的最简单表现如下：'
- en: 'For dissimilar samples:'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于不相似样本：
- en: loss = − distance
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: loss = − distance
- en: 'For similar samples:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于相似样本：
- en: loss = distance
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: loss = distance
- en: SimCLR focuses on image data and uses crafted image augmentation techniques
    to generate image pairs that could optimize the network to produce closer features.
    Random cropping and random color distortions are the most general augmentations
    that can be useful setups for most image datasets to perform unsupervised pre-training
    with SimCLR.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SimCLR 专注于图像数据，并使用精心设计的图像增强技术生成可以优化网络以生成更接近特征的图像对。随机裁剪和随机颜色扭曲是最通用的增强技术，可用于大多数图像数据集的
    SimCLR 无监督预训练设置。
- en: '**Swapping assignments between multiple views of the same image** (**SwaV**):
    SwaV adopts a similar concept to SimCLR in utilizing image augmentations with
    convolutional neural networks. It also uses the concept of clustering and embeddings
    to optimize the model to produce features that make sure the two images are mapped
    to the same feature space.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在同一图像的多个视角之间交换分配**（**SwaV**）：SwaV采用类似SimCLR的概念，通过卷积神经网络利用图像增强技术。它还使用聚类和嵌入的概念来优化模型，确保两张图像被映射到相同的特征空间。'
- en: 'The learning technique is executed as follows:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习技术的执行步骤如下：
- en: A pre-set amount of cluster number is determined, called *K*.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预设一个聚类数目，称为*K*。
- en: Featurize the two images with the same convolutional neural network.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的卷积神经网络对两张图像进行特征提取。
- en: The two sets of features will then be assigned independently to specific clusters
    by using an external technique called Optimal Transport Solver using an embedding
    layer that represents the representative features of the *K* clusters. Two features
    will always be assigned to different clusters.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用一种外部技术，称为最优传输求解器（Optimal Transport Solver），通过一个嵌入层将两个特征分别分配到具体的聚类中，该嵌入层表示*K*聚类的代表性特征。两个特征将始终被分配到不同的聚类中。
- en: Dot products between the convolutional features and all the cluster embeddings
    are computed and a softmax operation is applied.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算卷积特征与所有聚类嵌入之间的点积，并应用softmax操作。
- en: The assigned clusters for both image features will then be swapped, where cross-entropy
    between the swapped cluster assignments and the resulting values from the softmax
    operation will be used to optimize the weights of both the CNN and the embeddings
    layer.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将两张图像特征分配的聚类进行交换，通过交换后的聚类分配与softmax操作结果之间的交叉熵，来优化CNN和嵌入层的权重。
- en: The idea is to jointly learn the embedding weights and convolutional network
    weights that consistently categorize together multiple augmentations of the same
    image. The technique can be described as contrastive clustering. Both SwaV and
    SimCLR are competitively close in multiple downstream task performances.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个方法的核心思想是共同学习嵌入权重和卷积网络权重，确保将同一图像的多个增强版本一致地分类在一起。这项技术可以描述为对比聚类。SwaV和SimCLR在多个下游任务的表现上非常接近。
- en: '**SEER**: SEER is the combination of SwaV, an extremely high amount of unlabeled
    data for images at the billion scales instead of the more common million scale,
    and using high-capacity models to pre-train using random, uncurated, and unlabeled
    images. This allowed SEER to achieve SoTA downstream supervised task performance
    and outperformed both SimCLR and SwaV alone.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SEER**：SEER是SwaV的结合体，使用了极大量的无标签图像数据，规模达到十亿级别，而非更常见的百万级别，并且利用高容量模型进行预训练，使用随机、未经筛选的无标签图像。这使得SEER在下游的有监督任务中实现了最先进的性能，并超越了单独使用SimCLR和SwaV的结果。'
- en: '**UP-DETR, by the researchers from SCTU and Tencent Wechat AI**: This method
    pre-trains the transformer that has an encoder-decoder architecture with CNN features
    for image object detection tasks in an unsupervised way. UP-DETR managed to improve
    the performance of transformers on downstream supervised image object detection
    datasets. The interesting thing to remember here is that it structured the network
    in a way that allowed random image patches to be fed separately to the decoder
    to predict the bounding box of these patches on the original image. The original
    image is fed into the encoder part of the transformer and combined with the random
    image patches at the decoder part.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UP-DETR，由SCTU和腾讯微信AI的研究人员提出**：该方法预训练了一个具有编码器-解码器架构的变换器，使用CNN特征进行图像目标检测任务的无监督训练。UP-DETR成功地提升了变换器在下游有监督图像目标检测数据集上的表现。值得记住的是，它将网络结构设计为允许将随机图像块单独输入到解码器中，以预测这些图像块在原始图像上的边界框。原始图像则输入到变换器的编码器部分，并与随机图像块在解码器部分进行结合。'
- en: '**Wav2vec 2.0**: Wav2vec 2.0 showed how feasible it is to train a reliable
    speech recognition model with limited amounts of labeled data by leveraging self-supervised
    pretraining as a pretext task. It also uses contrastive, loss that is simply the
    cosine similarity between samples. The method uses CNNs as an audio feature extractor
    and quantizes the representations into a discrete array of values that can be
    trained before passing it into a transformer. The unsupervised tasks of masked
    speech modeling and contrastive loss are applied here. Let’s look at how these
    two methods can be combined:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wav2vec 2.0**：Wav2vec 2.0展示了通过利用自监督预训练作为前置任务，如何在有限的标注数据下训练出可靠的语音识别模型。它还使用了对比损失，简单来说就是样本之间的余弦相似度。该方法使用CNN作为音频特征提取器，并将表示量化为一个离散值数组，可以在传递到变换器之前进行训练。这里应用了遮蔽语音建模和对比损失的无监督任务。让我们看看这两种方法是如何结合在一起的：'
- en: A random location of the quantized latent speech representations is masked.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机遮蔽量化潜在语音表示的某个位置。
- en: The transformers output at the same location of the masked quantized latent
    speech representation will be used as the prediction of the missing masked quantized
    latent speech representation.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换器在被遮蔽的量化潜在语音表示的相同位置输出，将被用作对缺失的遮蔽量化潜在语音表示的预测。
- en: A few parts of the non-masked quantized latent representations of the same sample
    will be used to compute the contrastive loss against the predicted missing masked
    quantized latent speech representation, which effectively enforces parts of the
    same audio sample to be in a similar latent domain.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同一样本的非遮蔽量化潜在表示的部分将用于计算对比损失，并与预测的缺失遮蔽量化潜在语音表示进行对比，这有效地强制同一音频样本的部分内容在类似的潜在领域中。
- en: 'This process is demonstrated in *Figure 9**.1*:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该过程在*图 9.1*中进行了演示：
- en: '![](img/B18187_09_01.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18187_09_01.jpg)'
- en: 'To PD: This image is sent for redraw.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PD：此图像已发送重新绘制。
- en: Figure 9.1 – Wav2vec 2.0 model structure
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – Wav2vec 2.0模型结构
- en: The methods we’ve introduced here were meant for a specific modality. However,
    with some effort methods, they can be adapted to other modalities to replicate
    performance. For example, augmentations were used for image-based methods along
    with contrastive learning. To adapt this method to text-based modality, using
    text augmentations that preserve the meaning, such as word replacement or back
    translation, should work well. The modalities involved in the unsupervised methods
    introduced here were images, text data, and audio data. These modalities were
    chosen due to their generalizability factor for downstream tasks. Other forms
    of modality, such as graph data or numerical data, are highly customizable to
    individual use cases where there is fundamentally no information that can be transferred
    to downstream tasks. Before you attempt to run an unsupervised deep learning method
    to create pre-trained weights, consider listing the information that can be transferred
    and evaluate qualitatively whether it makes sense to proceed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里介绍的方法是针对特定模态设计的。然而，经过一些努力，这些方法可以适配到其他模态以复制性能。例如，图像处理方法使用了数据增强和对比学习。要将该方法适配到基于文本的模态，可以使用保持语义的文本增强技术，例如词汇替换或回译，应该能够取得不错的效果。这里介绍的无监督方法所涉及的模态包括图像、文本数据和音频数据。这些模态因其在下游任务中的普适性而被选择。其他形式的模态，如图形数据或数值数据，可以根据具体使用案例进行高度定制，前提是没有任何信息可以转移到下游任务。在尝试运行无监督深度学习方法来创建预训练权重之前，考虑列出可以转移的信息，并定性评估是否值得继续。
- en: But what if the pre-trained network weights are already capable of producing
    very generalizable features across different domains? Just like how CNNs trained
    on the ImageNet dataset in a supervised way can be used as feature extractors,
    there is no limiting the immediate usage of networks trained by unsupervised methods
    such as SwaV or Wav2vec 2.0 as feature extractors. Feel free to try it out yourself!
    However, a few unsupervised learning techniques use neural networks that are made
    to use their generated features instead of their weights directly. In the next
    section, we will discover exactly that.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果预训练网络权重已经能够在不同领域间生成非常通用的特征呢？就像在ImageNet数据集上以监督方式训练的CNN可以作为特征提取器一样，无法限制通过无监督方法（如SwaV或Wav2vec
    2.0）训练的网络作为特征提取器的即时使用。可以自己尝试一下！不过，一些无监督学习技术使用的神经网络是为了利用其生成的特征，而不是直接使用它们的权重。在下一节中，我们将探讨这一点。
- en: Creating general representations through unsupervised deep learning
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过无监督深度学习创建通用表示
- en: 'The representations that are learned through unsupervised deep learning can
    be directly used as-is in downstream supervised tasks by predictive supervised
    models or consumed directly by end users. There are a handful of generally impactful
    unsupervised methods that utilize neural networks that are meant to be used primarily
    as feature extractors. Let’s take a look at a couple of unsupervised feature extractors:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过无监督深度学习学到的表示可以直接用于下游监督任务，由预测监督模型使用，或者直接被最终用户使用。有一些常见的无监督方法利用神经网络，主要作为特征提取器使用。让我们来看看几个无监督特征提取器：
- en: '**Unsupervised pre-trained word tokenizers**: These are used heavily by variants
    of the transformers architecture and were introduced in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning,* in the *Representing text data for supervised
    deep* *learning* section.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督预训练词标记化器**：这些方法被变换器架构的多个变种广泛使用，并在[*第8章*](B18187_08.xhtml#_idTextAnchor125)中引入，*监督深度学习探索*部分的*为监督深度学习表示文本数据*章节中有介绍。'
- en: '**Unsupervised pre-trained word embeddings**: These methods leverage unsupervised
    learning and attempt to perform language modeling, similar to masked language
    modeling in transformers. However, word embeddings-based methods have been overtaken
    by transformer-based pretraining with sub-word-based text tokenization in terms
    of metric performance. The word embeddings method still stays relevant today due
    to the runtime efficiency it has over transformer-based methods. Note that not
    every project has the GPU available that is needed to run a big transformer in
    a reasonable runtime. Some projects only have access to CPU processing, and word
    embeddings provide the perfect inference runtime versus metric performance tradeoff.
    Additionally, word embeddings are a natural solution for some use cases that require
    words as a result, such as word-to-word translation from one language into another
    language, or even finding synonyms or antonyms. Examples of methods that produce
    pre-trained word embeddings are **fastText** and **word2vec**. *Figure 9**.2*
    exemplifies the architecture of word embedding methods:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督预训练词嵌入**：这些方法利用无监督学习并尝试进行语言建模，类似于变换器中的掩码语言建模。然而，基于词嵌入的方法在度量性能上已被基于变换器的预训练方法和子词级文本标记化所超越。尽管如此，词嵌入方法今天仍然具有相关性，因为它在运行时效率上优于基于变换器的方法。请注意，并不是每个项目都有足够的GPU来在合理的运行时间内运行大型变换器。一些项目只能使用CPU处理，而词嵌入提供了一个完美的推理运行时与度量性能之间的权衡。此外，词嵌入是一些需要以单词为结果的用例的自然解决方案，比如从一种语言到另一种语言的词对词翻译，甚至是寻找同义词或反义词。生成预训练词嵌入的方法有**fastText**和**word2vec**。*图
    9.2*展示了词嵌入方法的架构：'
- en: '![Figure 9.2 – The word embeddings architecture with a two-layer MLP and trainable
    embeddings](img/B18187_09_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 带有双层MLP和可训练嵌入的词嵌入架构](img/B18187_09_02.jpg)'
- en: Figure 9.2 – The word embeddings architecture with a two-layer MLP and trainable
    embeddings
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 带有双层MLP和可训练嵌入的词嵌入架构
- en: The task is either to predict the middle word based on the summed embeddings
    of the surrounding words or to predict the surrounding words based on the embeddings
    of the current word. *Figure 9**.2* shows the former case. After training the
    word embeddings with the MLP with a dictionary of N words, the MLP is then dumped,
    and the word embeddings are saved as a dictionary for simple look-up utilization
    during inference. FastText differs from word2vec by using not words but subwords
    to generate embeddings and thus can better handle missing words. A word embedding
    for FastText is produced by summing up embeddings of subwords that form the full
    word. Go to [https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)
    to learn how to use word embeddings that have pre-trained for 157 languages, or
    how to pre-train FastText embeddings on your custom dataset!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 任务要么是根据周围单词的嵌入总和预测中间单词，要么是根据当前单词的嵌入预测周围单词。*图 9.2*展示了前一种情况。在使用包含N个单词的字典训练词嵌入时，MLP完成训练后，MLP会被丢弃，词嵌入将作为字典保存，方便在推理时进行简单查找。FastText与word2vec的不同之处在于，它不是使用单词，而是使用子词来生成嵌入，因此能更好地处理缺失单词。FastText的词嵌入是通过将构成完整单词的子词的嵌入相加来生成的。访问[https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)，了解如何使用已预训练的157种语言的词嵌入，或者如何在自定义数据集上预训练FastText嵌入！
- en: '**Autoencoders**: Autoencoders are encoder-decoder architectures that can be
    trained to denoise data and reduce the dimensionality of the data while optimizing
    it to be reconstructible. They are generally trained to extract useful and core
    information by limiting the number of features in the bottleneck section of the
    architecture right after the encoder and right before the decoder. Go back to
    *Chapter 5*, *Understanding Autoencoders*, to find out more!'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**：自编码器是编码器-解码器架构，可以训练用于去噪数据并减少数据的维度，同时优化数据以使其能够被重建。它们通常通过限制在架构的瓶颈部分（即编码器之后和解码器之前）的特征数量来训练，以提取有用且核心的信息。返回到*第5章*，*理解自编码器*，了解更多内容！'
- en: '**Contrastive Language-Image Pretraining** (**CLIP**): CLIP is a method that
    trains a text language transformer encoder and a CNN image encoder with contrastive
    learning. It provides a dataset consisting of around 400 million image text pairs
    constructed by combining multiple publicly available datasets. This method produces
    a powerful image and text feature encoder that can be used independently. This
    method became a main part of the current SoTA of text-to-image methods by assisting
    during training to optimize to generate an image that has CLIP-encoded image embeddings
    close to the CLIP-encoded text embeddings. *Figure 9**.3* shows the CLIP architecture:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比语言-图像预训练** (**CLIP**)：CLIP是一种通过对比学习训练文本语言变换器编码器和CNN图像编码器的方法。它提供了一个由大约4亿对图像文本组成的数据集，这些图像文本是通过将多个公开数据集结合而成的。该方法生成了一个强大的图像和文本特征编码器，可以独立使用。通过在训练过程中优化生成具有与CLIP编码图像嵌入相似的CLIP编码文本嵌入的图像，这种方法成为了当前文本到图像方法的主要技术之一。*图
    9.3*展示了CLIP架构：'
- en: '![Figure 9.3 – CLIP architecture](img/B18187_09_03.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – CLIP架构](img/B18187_09_03.jpg)'
- en: Figure 9.3 – CLIP architecture
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – CLIP架构
- en: 'A small implementation detail here is that the outputs of the image encoder
    and text encoder are fed separately to a linear layer so that the number of features
    matches up for contrastive learning loss to be computed. Specifically, for CLIP,
    the contrastive loss is applied in the following way:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个小实现细节是，图像编码器和文本编码器的输出分别送入一个线性层，以便使特征的数量匹配，从而计算对比学习损失。具体来说，对于CLIP，交叉熵损失是以以下方式应用的：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that cross-entropy is still applied after applying pairwise cosine similarity
    between the image and text embeddings, which exemplifies the many variations of
    contrastive loss, where the core workhorse still comes down to using the distance
    metric. Since CLIP leverages co-occurring data, it does not belong to the self-supervised
    learning sub-category.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在应用图像和文本嵌入之间的成对余弦相似度之后，仍然使用交叉熵，这展示了对比损失的多种变体，其中核心工作仍然是使用距离度量。由于CLIP利用了共现数据，它不属于自监督学习子类别。
- en: '**Generative models**: Let’s look at some examples of these models:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**：让我们看一些这些模型的例子：'
- en: '**Transformer models**. Examples include GPT-3 and ChatGPT. Both are transformer
    models that are trained with masked language modeling and next-sentence prediction
    tasks. The difference with ChatGPT is that it is fine-tuned using reinforcement
    learning through human feedback and training. Both generate new text data by predicting
    in an autoregressive manner.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器模型**。例如，GPT-3和ChatGPT。两者都是经过掩蔽语言建模和下一句预测任务训练的变换器模型。与ChatGPT的区别在于，它是通过人类反馈和训练使用强化学习进行微调的。两者都通过自回归方式预测生成新的文本数据。'
- en: '**Text to image generators**. Examples include DALL.E 2 and Stable Diffusion.
    Both methods utilize the diffusion model. At a high level, the Stable Diffusion
    method slowly generates an extremely high-quality image from a base image full
    of noise.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本到图像生成器**。例如，DALL.E 2和Stable Diffusion。两种方法都利用扩散模型。从高层次来看，Stable Diffusion方法从充满噪声的基础图像开始，逐渐生成一个极高质量的图像。'
- en: '**Generative adversarial networks** (**GANs**): GANs utilize two neural network
    components during training, called **discriminators** and **generators**. The
    discriminator is a classifier that is meant to discern fake from real images.
    The generator and discriminator are trained iteratively until a point where the
    discriminator can’t discern that the image generated by the generator is fake.
    GANs can generate good-quality images but are generally surpassed by diffusion-based
    models as they produce much higher-quality images.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）：GANs在训练过程中使用两个神经网络组件，分别是**判别器**和**生成器**。判别器是一个分类器，用于区分真假图像。生成器和判别器会进行反复训练，直到判别器无法分辨生成器生成的图像是真是假。GANs能够生成高质量的图像，但通常被基于扩散的模型所超越，因为后者能够生成更高质量的图像。'
- en: It’s important to note that the methods to learn and create feature representations
    with unsupervised learning are not limited to those based on neural networks.
    PCA and TF-IDF, which are considered data pre-processing techniques, also belong
    in this category. However, the key difference between them and deep learning-based
    methods is that the latter require more training time but offer better generalization
    capabilities.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，使用无监督学习来学习和创建特征表示的方法并不限于基于神经网络的方法。PCA和TF-IDF，这些被认为是数据预处理技术，也属于这一类。然而，它们与基于深度学习的方法的主要区别在于，后者需要更多的训练时间，但提供了更好的泛化能力。
- en: The key in unsupervised representation learning is leveraging relationships
    between co-occurring data. The techniques that were introduced in the last two
    sections have public repositories that can be utilized immediately. For most of
    them, there are already pre-trained weights that you can leverage out of the box
    to either fine-tune further on downstream supervised tasks or use as plain feature
    extractors. In the next section, we will explore a special type of utilization
    of CLIP called zero-shot learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督表示学习的关键是利用共同出现数据之间的关系。上一节介绍的技术有公开的代码库，可以立即使用。对于大多数方法，已经有预训练的权重，可以直接利用这些权重，进一步在下游监督任务中进行微调，或作为简单的特征提取器使用。在下一节中，我们将探讨一种CLIP的特殊应用方式，称为零-shot学习。
- en: Exploring zero-shot learning
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索零-shot学习
- en: '**Zero-shot learning** is a paradigm that involves utilizing a trained machine
    learning model to tackle new tasks without training and learning directly on the
    new task. The method implements transfer learning at its core but instead of requiring
    additional learning in the downstream task, no learning is done. The method that
    we will be using to realize zero-shot learning here is CLIP as a base and thus
    is an extension of an unsupervised learning method.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**零-shot学习**是一种范式，涉及利用已经训练好的机器学习模型来处理新任务，而无需在新任务上进行训练或学习。该方法的核心实现了迁移学习，但与需要在下游任务中进行额外学习不同，零-shot学习不进行任何学习。我们将在这里使用CLIP来实现零-shot学习，因此它是无监督学习方法的一种扩展。'
- en: CLIP can be used to perform zero-shot learning on a wide variety of downstream
    tasks. To recap, CLIP is pre-trained with the task of image-text retrieval. So
    long as CLIP is applied to downstream tasks without any additional learning process,
    it can be considered as zero-shot learning. The tested use cases include tasks
    such as object character recognition, action recognition in videos, geo-localization
    based on images, and many types of fine-grained image object classification. Additionally,
    there are basic ways people have been testing and giving demos on zero-shot learning
    for object detection.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP可以用于在各种下游任务上执行零-shot学习。回顾一下，CLIP是通过图像-文本检索任务进行预训练的。只要CLIP在下游任务中应用时不进行额外的学习过程，就可以被视为零-shot学习。经过测试的用例包括对象特征识别、视频中的动作识别、基于图像的地理定位，以及多种精细化的图像对象分类任务。此外，人们还通过一些基本方法测试并展示了零-shot学习在物体检测中的应用。
- en: 'In this chapter, we will implement a non-documented zero-shot application of
    CLIP, which is image object counting. Counting means the model will be performing
    regression. Let’s start the implementation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将实现一个未记录的CLIP零-shot应用，即图像对象计数。计数意味着模型将执行回归任务。让我们开始实现：
- en: 'First, let’s import all the necessary libraries. We will be using the open
    source `clip` library from [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
    to utilize a pretrained version of CLIP using a visual transformer model:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所有必要的库。我们将使用来自[https://github.com/openai/CLIP](https://github.com/openai/CLIP)的开源`clip`库，以利用基于视觉变换器模型的预训练
    CLIP 版本：
- en: '[PRE1]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will load the pretrained CLIP model in either CPU mode or GPU mode
    if the CUDA toolkit is installed:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，如果安装了 CUDA 工具包，我们将以 CPU 模式或 GPU 模式加载预训练的 CLIP 模型：
- en: '[PRE2]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model variable we loaded here is a class that contains the individual methods
    to encode images and text with the encoder model loaded with pretrained weights.
    Additionally, the preprocess variable is a method that needs to be executed on
    the input before it’s fed to the image encoder. It performs normalization on the
    data that was used during training.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里加载的模型变量是一个类，它包含了使用加载了预训练权重的编码器模型对图像和文本进行编码的单独方法。此外，preprocess 变量是一个在将输入传递给图像编码器之前需要执行的方法。它对训练时使用的数据进行归一化处理。
- en: To predict with this model more conveniently, we will create a helper method
    with the following structure.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更方便地使用此模型进行预测，我们将创建一个具有以下结构的辅助方法。
- en: '[PRE3]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Remember that CLIP is trained to generate the image encoder and text encoder
    outputs in a way that they’re mapped into the same feature space. Matching text
    descriptions to an image will produce text and image features that are close together
    in distance. Since the metric we used is cosine similarity, the higher the similarity
    value, the lower the distance. The main technique to realize zero-shot learning
    from CLIP is to think of multiple descriptions that represent a certain label
    and choose the label that has the highest similarity score against an image. The
    similarity score will then be normalized against all the other labels to obtain
    a probability score. Additionally, `top_k` is used to control how many top highest
    similarity score text description indices and scores to return. We will come back
    to how to design the descriptions objectively for zero-shot learning later, once
    we’ve defined the code that will belong in the predict method we defined previously.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，CLIP 被训练成以一种方式生成图像编码器和文本编码器的输出，使它们映射到相同的特征空间。将文本描述与图像匹配时，会生成文本和图像特征，它们的距离较近。由于我们使用的度量标准是余弦相似度，因此相似度值越高，距离越小。实现
    CLIP 零-shot 学习的主要技巧是考虑多个描述来代表某个标签，并选择与图像的相似度得分最高的标签。然后，将该相似度得分与所有其他标签进行归一化，以获得概率得分。此外，`top_k`
    用于控制返回多少个最高相似度得分的文本描述索引和得分。我们稍后会回到如何为零-shot 学习客观设计描述的方法，一旦我们定义了属于我们先前定义的预测方法中的代码。
- en: 'The first part of this method will be to preprocess the provided single image
    input and multiple text descriptions, called `clip_labels`. The image will be
    preprocessed according to the provided preprocessor, while the multiple text descriptions
    will be tokenized according to the sub-word tokenizer used in the text transformer
    provided by the `clip` library:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法的第一部分是预处理提供的单张图像输入和多个文本描述，这些文本描述称为 `clip_labels`。图像将根据提供的预处理器进行预处理，而多个文本描述将根据
    `clip` 库提供的文本变换器中使用的子词分词器进行分词：
- en: '[PRE4]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will encode the preprocessed image and text inputs using the image
    encoder and text encoder, respectively:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用图像编码器和文本编码器分别对预处理后的图像和文本输入进行编码：
- en: '[PRE5]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In PyTorch, remember that we need to make sure no gradients are computed during
    inference mode as we are not training the model and don’t want that extra computation
    or RAM wastage.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，请记住，我们需要确保在推理模式下不计算梯度，因为我们并不是在训练模型，并且不希望浪费额外的计算或内存。
- en: 'Now that the image and text features with the same column dimensions have been
    extracted, we will compute the cosine similarity score of the provided image input
    features against all the text description label features:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，图像和文本特征已经提取完毕，并且它们具有相同的列维度，我们将计算提供的图像输入特征与所有文本描述标签特征之间的余弦相似度得分：
- en: '[PRE6]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In addition to the similarity score, the similarity among all the text descriptions
    is normalized using Softmax so that the scores will all add up to one. View this
    as a way to compare the similarity scores against other samples. This will essentially
    convert distance scores into a multiclass prediction setting where the provided
    text descriptions as labels are all the possible classes.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了相似度分数外，所有文本描述之间的相似度还通过Softmax进行归一化处理，以确保所有分数加起来为1。将其视为一种方法，用于将相似度分数与其他样本进行比较。这实际上会将距离分数转化为多类预测设置，其中提供的文本描述作为标签，代表所有可能的类别。
- en: 'Next, we will extract the `top_k` highest similarity score and return their
    similarity score probability and indices to indicate which label the score belongs
    to:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将提取`top_k`最高的相似度分数，并返回它们的相似度概率和索引，以指示该分数属于哪个标签：
- en: '[PRE7]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that the method is complete, we can load an image using Pillow, create
    some descriptions, and feed it into the method to perform zero-shot learning!
    In this tutorial, we will work on object counting. This can range from counting
    how many cars to properly account for parking availability to counting how many
    people to account for personnel that are required to service the people. In this
    tutorial, we will count the number of paper clips as a pet project. Note that
    this can easily be extended to other counting datasets and projects. We will use
    the dataset at [https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download](https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download)
    to achieve this. Be sure to download the dataset in the same folder where the
    code exists. Let’s load up a simple version with fewer paper clips than 2:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在方法已经完成，我们可以使用Pillow加载一张图片，创建一些描述，并将其输入到该方法中进行零-shot学习！在本教程中，我们将进行物体计数。这可以从计数有多少辆车以正确统计停车位的可用性，到计数有多少人以统计需要为人群提供服务的人员。在本教程中，我们将把纸夹的数量作为一个小项目进行计数。请注意，这可以很容易地扩展到其他计数数据集和项目。我们将使用[https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download](https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download)的数据集来实现这一目标。确保将数据集下载到与代码所在相同的文件夹中。让我们加载一个简单的版本，其中的纸夹少于2个：
- en: '[PRE8]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will get the following output:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 9.4 – Example paper clip counting from easy (A), medium (B), to hard
    (C)](img/B18187_09_04.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 纸夹计数示例：从容易（A）、中等（B）到困难（C）](img/B18187_09_04.jpg)'
- en: Figure 9.4 – Example paper clip counting from easy (A), medium (B), to hard
    (C)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 纸夹计数示例：从容易（A）、中等（B）到困难（C）
- en: '*Figure 9**.4 B* and *Figure 9**.4 C* are harder examples that we will explore
    after we go through predictions for *Figure* *9**.4 A*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.4 B*和*图9.4 C*是更难的例子，我们将在处理完*图9.4 A*的预测后再探索它们。'
- en: 'Now, we need multiple sets of text descriptions to account for all the possible
    counts of paper clips. A simple description that just uses numbers as text is
    not descriptive enough to even get close to a good similarity score. Instead,
    let’s add some additional text along with the number, as shown in the following
    code:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要多组文本描述来涵盖所有可能的纸夹计数。仅仅使用数字作为文本的简单描述不足以接近一个良好的相似度分数。相反，让我们在数字旁边添加一些额外的文本，如下所示的代码：
- en: '[PRE9]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this code, we use all numbers from 0 to 99 with the same surrounding text.
    More effort can be put into designing more variations of the pretext needed. It
    is also possible to utilize multiple pretexts with the same raw labels. The more
    descriptive the text is of the image, the more likely there will be a description
    that produces the closest features to the image.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用了从0到99的所有数字，配合相同的周围文本。可以投入更多精力设计更多变化的前置文本。也可以使用多个前置文本与相同的原始标签。文本对图像的描述越详细，就越有可能找到一个描述，能产生与图像最接近的特征。
- en: 'Let’s run a prediction on this example and see how it performs:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在这个例子上运行预测，看看它的表现如何：
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It accurately predicted two paper clips!
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它准确地预测了两个纸夹！
- en: 'Now, let’s go through two more harder examples from *Figure 9**.4 B* and *Figure*
    *9**.4 C*:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看两个更难的例子，来自*图9.4 B*和*图9.4 C*：
- en: '[PRE11]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This produces an error of 1 since the image contains three paper clips. Predicting
    on *Figure 9**.4 C* produces the following results:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这产生了1的误差，因为图像包含了三只纸夹。对*图9.4 C*的预测结果如下：
- en: '[PRE12]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This produces an error of two since the image contains 16 paper clips. Not bad
    at all!
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这产生了2的误差，因为图像包含了16只纸夹。还不错！
- en: 'Now, let’s evaluate the mean error on a partitioned validation dataset of 1,000
    examples:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在一个划分好的验证数据集（包含1,000个样本）上评估平均误差：
- en: '[PRE13]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This results in a `23.8122` average count error. All of a sudden, this doesn’t
    look that usable. In some of the examples, the model couldn’t properly count clips
    that were partially occluded, even when in the description it was specified that
    there would be some partially occluded. It might make sense to add a description
    stating that the clips are in different sizes, or even that it is on top of a
    piece of lined paper. Try it for yourself and do some experiments!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了`23.8122`的平均计数误差。突然之间，这看起来并不那么实用。在一些示例中，即使描述中指出会有部分遮挡，模型仍未能正确计数部分遮挡的片段。也许可以考虑添加描述，指出这些片段的大小不同，或者甚至是在一张有线纸上。试试看并做些实验吧！
- en: Training a ridge regressor model with a pretrained SqueezeNet featurizer on
    this dataset separately and validated on the same validation partition in *step
    12* results in a root mean squared error of `1.9102`. This shows that zero-shot
    learning by itself is not as reliable as a supervised model for this use case,
    but it might be able to discern and predict nicely on simpler images. In the paper,
    the authors emphasized that CLIP-based zero-shot learning can work well and achieve
    performance close to supervised learning techniques in some specific use cases
    and datasets. However, in most use cases and datasets, CLIP-based zero-shot learning
    still falls way behind proper supervised learning methods. This tutorial is an
    example of where it could work, but it doesn’t work so reliably that it can be
    utilized in the real world. However, it does show promise in the base unsupervised
    method that CLIP was trained in. A few more years and some more research down
    the line, and we’ll be sure to see an even better performance that will likely
    be generally the same supervised learning or better!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上，使用预训练的SqueezeNet特征提取器单独训练一个岭回归模型，并在*步骤12*中的相同验证分区上验证，结果得到了`1.9102`的均方根误差。这表明，仅凭零-shot学习在此用例中并不像监督模型那样可靠，但它可能能在较简单的图像上识别并做出不错的预测。在论文中，作者强调，基于CLIP的零-shot学习在一些特定的用例和数据集上可以表现良好，并达到接近监督学习技术的性能。然而，在大多数用例和数据集上，基于CLIP的零-shot学习仍然远远落后于适当的监督学习方法。本教程展示了它可以工作的一个例子，但它并不那么可靠，无法在现实世界中得到应用。然而，它确实展示了CLIP在基础无监督方法下的潜力。再过几年，随着更多的研究，我们肯定会看到更好的性能，可能会普遍达到甚至超越监督学习的水平！
- en: Next, let’s explore the dimensionality reduction component of unsupervised learning.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨无监督学习中的降维组件。
- en: Exploring the dimensionality reduction component of unsupervised deep learning
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索无监督深度学习的降维组件
- en: Dimensionality reduction is a technique that can be useful in cases where a
    faster runtime is needed to train and perform inference on your model or when
    the model has a hard time learning from too much data. The most well-known unsupervised
    deep learning method for dimensionality reduction is based on autoencoders, which
    we discussed in [*Chapter 5*](B18187_05.xhtml#_idTextAnchor085), *Understanding
    Autoencoders*. A typical autoencoder network is trained to reproduce the input
    data as an unsupervised learning method. This is done through the encoder-decoder
    structure. At inference time, using only the encoder will allow you to perform
    dimensionality reduction as the outputs of the encoder will contain the most compact
    representation, which can fully reconstruct the original input data. Autoencoders
    can support different modalities, with one modality at any one time, which makes
    it a very versatile unsupervised dimensionality reduction method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是一种在需要更快运行时间以训练和推理模型，或者当模型在处理大量数据时难以学习的情况下非常有用的技术。最著名的无监督深度学习降维方法是基于自编码器的，这一点我们在[*第5章*](B18187_05.xhtml#_idTextAnchor085)《理解自编码器》中进行了讨论。典型的自编码器网络会作为一种无监督学习方法被训练，以重构输入数据。这是通过编码器-解码器结构实现的。在推理时，仅使用编码器就能执行降维操作，因为编码器的输出会包含最紧凑的表示，这些表示可以完全重建原始输入数据。自编码器支持不同的模态，每次只能使用一种模态，这使得它成为一种非常灵活的无监督降维方法。
- en: Other examples include unsupervised methods to create word embeddings that use
    shallow neural networks, such as FastText or Word2vec. The number of unique words
    that exist in a language is huge. Even if this gets scaled down to the total amount
    of unique words in the training data, this can balloon up to 100,000 words easily.
    One simple way to encode the words is by using one-hot-encoding or TF-IDF. Both
    methods produce 100,000 column features when the dataset contains 100,000 unique
    words. This will easily blow up the RAM requirements and can make or break the
    potential of a solution. However, word embeddings can be tuned to have the desired
    amount of feature columns as you can choose the embedding size during pretraining
    with the language model you’re using.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其他例子包括使用浅层神经网络创建词嵌入的无监督方法，例如FastText或Word2vec。语言中存在的独特词汇数量是巨大的。即使将其缩小到训练数据中的唯一词汇总数，这个数字也可以轻易膨胀到100,000个词。编码这些词的一种简单方法是使用one-hot编码或TF-IDF。当数据集中包含100,000个独特词汇时，这两种方法会产生100,000列特征。这将轻松增加内存需求，并可能决定解决方案的可行性。然而，词嵌入可以进行调整，选择在预训练语言模型时所需的特征列数。
- en: Finally, let’s learn how to detect anomalies in external data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们学习如何检测外部数据中的异常。
- en: Detecting anomalies in external data
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测外部数据中的异常
- en: '**Anomaly detection** is also considered to be an important application of
    unsupervised learning in general. Anomaly detection can be used in cases where
    you want to perform any kind of filtering of your existing data, called **outlier
    detection**, and also act as a real-time detector during the inference stage given
    new external data, known as **novelty detection**. Here are some examples of end
    user use cases of anomaly detection:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**异常检测**通常也被认为是无监督学习的重要应用之一。异常检测可以用于需要对现有数据进行任何形式过滤的情况，称为**离群值检测**，并且还可以作为推理阶段的实时检测器，检测新的外部数据，这被称为**新颖性检测**。以下是异常检测的最终用户应用实例：'
- en: Removing noise in your dataset that will be fed into a supervised feature learning
    process to enable more stable learning.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除数据集中的噪声，以便将其输入到有监督的特征学习过程中，从而实现更稳定的学习。
- en: Removing defective products in the production line. This can range from the
    manufacturing production of semiconductor wafers to egg production.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产线上去除缺陷产品。这可以涵盖从半导体晶圆的制造生产到蛋类生产的各个领域。
- en: Fraud prevention by detecting anomalous transactions.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检测异常交易来预防欺诈。
- en: Scam detection through SMS, email, or direct messenger platforms.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过短信、电子邮件或直接消息平台进行诈骗检测。
- en: Anomaly detection is a two-class or binary problem. This means that an alternative
    way people approach these example use cases listed is to use supervised learning
    and collect a bunch of negative samples and positive samples. The supervised learning
    approach can work well when a good-quality dataset containing negative/anomalous
    data has been collected and labeled because usually, there will already be a good
    amount of positive/normal data. Go back to the *Preparing data* section of [*Chapter
    1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*, to recap what
    it means to have quality data for machine learning! However, in reality, we can’t
    possibly capture all the possible variations and types of negative samples that
    can occur. Anomaly detection-specific algorithms are built to handle anomalies
    or negative samples more generally and can extrapolate well to unseen examples.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是一个二分类问题。这意味着人们在处理这些列举的例子时，常常采用有监督学习方法，收集大量的负样本和正样本。当已经收集并标注了包含负/异常数据的高质量数据集时，有监督学习方法会表现得很好，因为通常情况下，已经存在相当多的正/正常数据。请回到[第1章](B18187_01.xhtml#_idTextAnchor015)的*准备数据*部分，复习一下什么是机器学习中的高质量数据！然而，实际上，我们不可能捕捉到所有可能出现的负样本和类型。异常检测特定算法被设计用来更通用地处理异常或负样本，并且能够很好地推断出未见过的示例。
- en: 'One of the more widely known deep learning methods to achieve anomaly detection
    on external data that’s not used for training is the autoencoder model. The model
    performs something called a **one-class classifier**, analogous to the traditional
    one-class support vector machines. This means that autoencoders can be trained
    to reconstruct the original data that’s fed into it to achieve either or a combination
    of four tasks – dimensionality reduction, noise remover, general featurizer, and
    anomaly detection. Depending on the objective at hand, the autoencoder should
    be trained differently. For dimensionality reduction, noise remover, and general
    featurizer, the autoencoder should be trained normally as introduced. For anomaly
    detection, the following things need to be done to ensure it achieves the desired
    behavior:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在未用于训练的外部数据上实现异常检测的较为广为人知的深度学习方法之一是自动编码器模型。该模型执行所谓的**单类分类器**，类似于传统的单类支持向量机。这意味着自动编码器可以训练以重构输入的原始数据，以实现四种任务中的一种或多种组合——降维、去噪、通用特征提取器和异常检测。根据手头的目标，应该以不同方式训练自动编码器。对于降维、去噪和通用特征提取器，应该像之前介绍的那样正常训练自动编码器。对于异常检测，需要执行以下步骤以确保其实现期望的行为：
- en: The data without anomalies is to be used as the training data. If you know there
    might be anomalies, use a simple outlier detection algorithm such as the local
    outlier factor, and remove outliers in the data.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无异常的数据将用作训练数据。如果可能存在异常，请使用简单的异常值检测算法，如局部离群因子，并移除数据中的异常值。
- en: Use a combination of anomalous and non-anomalous data to form the validation
    and holdout partition. If such categorization is not known beforehand, use an
    outlier-based anomaly detection algorithm from *step 1* to label the data.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用异常和非异常数据的组合形成验证和保留分区。如果事先不知道这种分类，可以使用*步骤 1*中的基于异常的异常检测算法对数据进行标记。
- en: If *step 2* is somehow not possible, train and overfit the training data using
    the mean squared error loss. This is so that you can make sure the model will
    only be able to reconstruct the data that has the same characteristics as the
    training data. Overfitting can be done by training and making sure the training
    reconstruction loss becomes lower each epoch.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*步骤 2*不可行，请使用均方误差损失训练和过度拟合训练数据。这样可以确保模型只能重构具有与训练数据相同特征的数据。过拟合可以通过每个时期训练并确保训练重构损失降低来完成。
- en: If *step 2* is possible, train, validate and evaluate with the given cross-validation
    partitioning strategy and follow the tips introduced in the *Training supervised
    deep learning models effectively* section in the previous chapter.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*步骤 2*可行，请使用给定的交叉验证分区策略进行训练、验证和评估，并按照前一章节中*有效训练监督深度学习模型*部分介绍的提示进行操作。
- en: The predict function of the autoencoder for testing, validation, and inference
    can be set based on how well the model can reproduce the input data indicated
    through the mean squared error. A threshold will need to be used as a cut-off
    mechanism to determine which data is anomalous or not. The higher the mean squared
    error, the more probable it is that the provided data is anomalous. Since anomaly
    detection is a binary classification problem, once labels are available, perform
    performance analysis by using ROC curves, confusion metrics, and the log loss
    error using different mean squared error thresholds. This will be introduced more
    comprehensively in the next chapter.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动编码器的预测功能用于测试、验证和推断，可以根据模型通过均方误差重现输入数据的能力进行设置。需要使用阈值作为截断机制来确定哪些数据是异常的。均方误差越高，提供的数据是异常的可能性就越大。由于异常检测是二元分类问题，一旦标签可用，应使用ROC曲线、混淆矩阵和使用不同均方误差阈值的对数损失错误来进行性能分析。这将在下一章更全面地介绍。
- en: In anomaly detection in general, note that anomalies are a vague description
    of what it means to be an anomaly. Different algorithms encode their definition
    of what an anomaly is given a dataset, its feature space, and its feature distribution.
    When an algorithm does not work well for your predefined case of anomalies, it
    does not mean that the algorithm is not a good model in general. The autoencoder
    approach won’t always be the best method that captures your intuition of anomalies.
    When it comes to unsupervised anomaly detection learning, make sure you train
    a bunch of models with proper hyperparameter settings and analyze individually
    which models match your intuition of what makes data anomalous.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般的异常检测中，需要注意的是，异常是一种模糊的描述，指的是什么样的数据被认为是异常。不同的算法根据数据集、特征空间和特征分布对异常有不同的定义。当一个算法在你预定义的异常情况下表现不佳时，并不意味着该算法在一般情况下就不是一个好的模型。自编码器方法并不总是能够最好地捕捉你对异常的直观理解。在无监督异常检测学习中，确保你训练多个模型，并使用适当的超参数设置，分别分析哪些模型符合你对数据异常性的直觉。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Deep learning has made significant contributions to the field of unsupervised
    learning, leading to the development of several innovative methods. But by far
    the most impactful method is unsupervised pretraining, which leverages the abundance
    of free data available on the internet today to improve the model performance
    of the downstream supervised tasks and create generalizable representations. With
    deeper research and time, unsupervised learning will aid in closing the gap toward
    general artificial intelligence. Overall, deep learning has been a valuable tool
    in the unsupervised learning domain, helping practitioners make the most of the
    large amounts of free data available on the internet today.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习对无监督学习领域作出了重大贡献，推动了几种创新方法的发展。但迄今为止，最具影响力的方法是无监督预训练，它利用当今互联网上丰富的免费数据来提升下游监督任务的模型性能，并创建具有普适性的表示。随着研究的深入和时间的推移，无监督学习将在缩小通往通用人工智能的差距方面发挥重要作用。总体而言，深度学习在无监督学习领域已成为一种宝贵的工具，帮助从业者充分利用互联网上大量的免费数据。
- en: In the next chapter, we will dive into the first chapter of the second part
    of this book, which is meant to introduce methods that provide insights about
    a trained deep learning model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨本书第二部分的第一章，该章节旨在介绍提供关于已训练深度学习模型洞察的方法。
- en: Part 2 – Multimodal Model Insights
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分 – 多模态模型洞察
- en: In this part of the book, we delve into the fascinating world of multimodal
    model insights, taking you on a comprehensive journey through various aspects
    of evaluating, interpreting, and securing deep learning models. This part offers
    a comprehensive understanding of various facets of model assessment and enhancement
    while emphasizing the importance of responsible and effective AI deployment in
    real-world applications. Throughout these chapters, you will explore methods for
    evaluating and understanding model predictions, interpreting neural networks,
    and addressing ethical and security concerns, such as bias, fairness, and adversarial
    performance.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们深入探讨了多模态模型洞察的迷人世界，带你全面了解评估、解释和保护深度学习模型的各个方面。本部分提供了关于模型评估与提升的全面理解，同时强调了在现实应用中负责且有效地部署AI的重要性。在这些章节中，你将探索评估和理解模型预测的方法、解释神经网络，并解决伦理和安全问题，如偏见、公平性和对抗性表现。
- en: By the end of this part, you will have a solid understanding of the importance
    of model evaluation, interpretation, and security, enabling you to create robust,
    reliable, and equitable deep learning systems and solutions that not only excel
    in performance but also consider ethical implications and potential vulnerabilities
    while standing the test of time for real-world applications.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到本部分结束时，你将全面理解模型评估、解释和安全性的重要性，使你能够创建健壮、可靠、公平的深度学习系统和解决方案，这些系统和解决方案不仅在性能上表现出色，还能考虑伦理问题和潜在的漏洞，同时经得起现实应用中的考验。
- en: 'This part contains the following chapters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '*Chapter 10*, *Exploring Model Evaluation Methods*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第10章*，*探索模型评估方法*'
- en: '*Chapter 11*, *Explaining Neural Network Predictions*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第11章*，*解释神经网络预测*'
- en: '*Chapter 12*, *Interpreting Neural Networks*'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第12章*，*解释神经网络*'
- en: '*Chapter 13*, *Exploring Bias and Fairness*'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第13章*，*探索偏见与公平性*'
- en: '*Chapter 14*, *Analyzing Adversarial Performance*'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第14章*，*分析对抗性表现*'
