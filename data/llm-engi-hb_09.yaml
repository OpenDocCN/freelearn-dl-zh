- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: RAG Inference Pipeline
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 推理流程
- en: Back in *Chapter 4*, we implemented the **retrieval-augmented generation** (**RAG**)
    feature pipeline to populate the vector **database** (**DB**). Within the feature
    pipeline, we gathered data from the data warehouse, cleaned, chunked, and embedded
    the documents, and, ultimately, loaded them to the vector DB. Thus, at this point,
    the vector DB is filled with documents and ready to be used for RAG.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 回到第 4 章，我们实现了**检索增强生成**（**RAG**）特征流程，以填充向量**数据库**（**DB**）。在特征流程中，我们从数据仓库中收集数据，对其进行清理、分块和嵌入文档，最终将它们加载到向量数据库中。因此，到目前为止，向量数据库已经填充了文档，并准备好用于
    RAG。
- en: 'Based on the RAG methodology, you can split your software architecture into
    three modules: one for retrieval, one to augment the prompt, and one to generate
    the answer. We will follow a similar pattern by implementing a retrieval module
    to query the vector DB. Within this module, we will implement advanced RAG techniques
    to optimize the search. Afterward, we won’t dedicate a whole module to augmenting
    the prompt, as that would be overengineering, which we try to avoid. However,
    we will write an inference service that inputs the user query and context, builds
    the prompt, and calls the LLM to generate the answer. To summarize, we will implement
    two core Python modules, one for retrieval and one for calling the LLM using the
    user’s input and context as input. When we glue these together, we will have an
    end-to-end RAG flow.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 RAG 方法，你可以将你的软件架构分为三个模块：一个用于检索，一个用于增强提示，一个用于生成答案。我们将通过实现一个检索模块来查询向量数据库。在这个模块中，我们将实现高级
    RAG 技术来优化搜索。之后，我们不会为增强提示单独设置一个模块，因为这将是过度设计，我们试图避免这种情况。然而，我们将编写一个推理服务，该服务输入用户查询和上下文，构建提示，并调用
    LLM 生成答案。总结来说，我们将实现两个核心 Python 模块，一个用于检索，一个用于使用用户的输入和上下文作为输入调用 LLM。当我们把它们粘合在一起时，我们将拥有一个端到端的
    RAG 流程。
- en: In *Chapters 5* and *6*, we fine-tuned our LLM Twin model, and in *Chapter 8*,
    we learned how to optimize it for inference. Thus, at this point, the LLM is ready
    for production. What is left is to build and deploy the two modules described
    above.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章和第 6 章中，我们对我们的 LLM 双胞胎模型进行了微调，在第 8 章中，我们学习了如何对其进行推理优化。因此，到目前为止，LLM 已经准备好投入生产。剩下的是构建和部署上述两个模块。
- en: We will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin
    model to AWS SageMaker, as an AWS SageMaker inference endpoint. Thus, the focus
    of this chapter is to dig into the advanced RAG retrieval module implementation.
    We have dedicated a whole chapter to the retrieval step because this is where
    the magic happens in an RAG system. At the retrieval step (and not when calling
    the LLM), you write most of the RAG inference code. This step is where you have
    to wrangle your data to ensure that you retrieve the most relevant data points
    from the vector DB. Hence, most of the advanced RAG logic goes within the retrieval
    step.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中完全致力于将我们的微调后的 LLM 双胞胎模型部署到 AWS SageMaker，作为 AWS SageMaker 推理端点。因此，本章的重点是深入探讨高级
    RAG 检索模块的实现。我们专门用一章来讲述检索步骤，因为这是 RAG 系统中发生魔法的地方。在检索步骤（而不是调用 LLM 时），你将编写大部分 RAG
    推理代码。这一步是你必须整理数据以确保从向量数据库中检索到最相关数据点的步骤。因此，大部分高级 RAG 逻辑都包含在检索步骤中。
- en: 'To sum up, in this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本章中，我们将涵盖以下主题：
- en: Understanding the LLM Twin’s RAG inference pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 LLM 双胞胎的 RAG 推理流程
- en: Exploring the LLM Twin’s advanced RAG techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 LLM 双胞胎的高级 RAG 技术
- en: Implementing the LLM Twin’s RAG inference pipeline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 LLM 双胞胎的 RAG 推理流程
- en: By the end of this chapter, you will know how to implement an advanced RAG retrieval
    module, augment a prompt using the retrieved context, and call an LLM to generate
    the final answer. Ultimately, you will know how to build a production-ready RAG
    inference pipeline end to end.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解如何实现一个高级 RAG 检索模块，使用检索到的上下文增强提示，并调用 LLM 生成最终答案。最终，你将了解如何从头到尾构建一个生产就绪的
    RAG 推理流程。
- en: Understanding the LLM Twin’s RAG inference pipeline
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 LLM 双胞胎的 RAG 推理流程
- en: Before implementing the RAG inference pipeline, we want to discuss its software
    architecture and advanced RAG techniques. *Figure 9.1* illustrates an overview
    of the RAG inference flow. The inference pipeline starts with the input query,
    retrieves the context using the retrieval module (based on the query), and calls
    the LLM SageMaker service to generate the final answer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现RAG推理管道之前，我们想要讨论其软件架构和高级RAG技术。*图9.1*展示了RAG推理流程的概述。推理管道从输入查询开始，使用检索模块（基于查询）检索上下文，并调用LLM
    SageMaker服务生成最终答案。
- en: '![](img/B31105_09_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_09_01.png)'
- en: 'Figure 9.1: RAG inference pipeline architecture'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图9.1: RAG推理管道架构'
- en: The feature pipeline and the retrieval module, defined in *Figure 9.1*, are
    independent processes. The feature pipeline runs on a different machine on a schedule
    to populate the vector DB. At the same time, the retrieval module is called on
    demand, within the inference pipeline, on every user request.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特征管道和检索模块，如*图9.1*所示，是独立的过程。特征管道在不同的机器上按计划运行，以填充向量数据库。同时，检索模块在推理管道中按需调用，针对每个用户请求。
- en: By separating concerns between the two components, the vector DB is always populated
    with the latest data, ensuring feature freshness, while the retrieval module can
    access the latest features on every request. The input of the RAG retrieval module
    is the user’s query, based on which we have to return the most relevant and similar
    data points from the vector DB, which will be used to guide the LLM in generating
    the final answer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在两个组件之间分离关注点，向量数据库总是填充最新的数据，确保特征的新鲜性，而检索模块可以访问每个请求上的最新特征。RAG检索模块的输入是用户的查询，基于此，我们必须从向量数据库中返回最相关和相似的数据点，这些数据点将被用来指导LLM生成最终答案。
- en: 'To fully understand the dynamics of the RAG inference pipeline, let’s go through
    the architecture flow from *Figure 9.1* step by step:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全理解RAG推理管道的动态，让我们逐步通过*图9.1*的架构流程：
- en: '**User query**:We begin with the user who makes a query, such as “Write an
    article about...”'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用户查询**: 我们从提出查询的用户开始，例如“写一篇关于...的文章”。'
- en: '**Query expansion**:We expand the initial query to generate multiple queries
    that reflect different aspects or interpretations of the original user query.
    Thus, instead of one query, we will use *xN* queries. By diversifying the search
    terms, the retrieval module increases the likelihood of capturing a comprehensive
    set of relevant data points. This step is crucial when the original query is too
    narrow or vague.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询扩展**: 我们扩展初始查询以生成多个查询，这些查询反映了原始用户查询的不同方面或解释。因此，我们不会使用一个查询，而是使用*xN*个查询。通过多样化搜索词，检索模块增加了捕获全面相关数据点的可能性。当原始查询过于狭窄或模糊时，这一步至关重要。'
- en: '**Self-querying**: We extract useful metadata from the original query, such
    as the author’s name. The extracted metadata will be used as filters for the vector
    search operation, eliminating redundant data points from the query vector space
    (making the search more accurate and faster).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自我查询**: 我们从原始查询中提取有用的元数据，例如作者的名字。提取的元数据将被用作向量搜索操作的过滤器，从查询向量空间中消除冗余数据点（使搜索更准确和更快）。'
- en: '**Filtered vector search**: We embed each query and perform a similarity search
    to find each search’s top *K* data points. We execute xN searches corresponding
    to the number of expanded queries. We call this step a filtered vector search
    as we leverage the metadata extracted from the self-query step as query filters.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过滤向量搜索**: 我们将每个查询嵌入并执行相似性搜索，以找到每个搜索的前*K*个数据点。我们执行xN个搜索，对应于扩展查询的数量。我们称这一步为过滤向量搜索，因为我们利用从自我查询步骤中提取的元数据作为查询过滤器。'
- en: '**Collecting results**:We get up to *xK* results closest to its specific expanded
    query interpretation for each search operation. Further, we aggregate the results
    of all the xN searches, ending up with a list of *N* x *K* results containing
    a mix of articles, posts, and repositories chunks. The results include a broader
    set of potentially relevant chunks, offering multiple relevant angles based on
    the original query’s different facets.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集结果**: 对于每个搜索操作，我们获取最接近其特定扩展查询解释的*xK*个结果。进一步，我们聚合所有xN个搜索的结果，最终得到一个包含文章、帖子、和代码块片段的*N*
    x *K*结果列表。这些结果包括更广泛的一组可能相关的片段，基于原始查询的不同方面提供多个相关角度。'
- en: '**Reranking**:To keep only the top *K* most relevant results from the list
    of *N* x *K* potential items, we must filter the list further. We will use a reranking
    algorithm that scores each chunk based on the relevance and importance relative
    to the initial user query.We will leverage a neural cross-encoder model to compute
    the score, a value between 0 and 1, where 1 means the result is entirely relevant
    to the query. Ultimately, we sort the *N* x *K* results based on the score and
    pick the top *K* items. Thus, the output is a ranked list of *K* chunks, with
    the most relevant data points situated at the top.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重新排序**：为了只保留从*N* x *K*个潜在项目列表中的前*K*个最相关结果，我们必须进一步过滤列表。我们将使用一个重新排序算法，根据相关性和相对于初始用户查询的重要性对每个片段进行评分。我们将利用一个神经交叉编码器模型来计算分数，这是一个介于0和1之间的值，其中1表示结果与查询完全相关。最终，我们将*N*
    x *K*个结果根据分数排序，并选择前*K*个项目。因此，输出是一个排序的*K*个片段列表，其中最相关的数据点位于顶部。'
- en: '**Build the prompt and call the LLM**:We map the final list of the most relevant
    K chunks to a string used to build the final prompt. We create the prompt using
    a prompt template, the retrieved context, and the user’s query. Ultimately, the
    augmented prompt is sent to the LLM (hosted on AWS SageMaker exposed as an API
    endpoint).'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建提示并调用LLM**：我们将最相关的K个片段的最终列表映射到一个用于构建最终提示的字符串。我们使用提示模板、检索到的上下文和用户的查询来创建提示。最终，增强后的提示被发送到LLM（托管在AWS
    SageMaker上，作为API端点暴露）。'
- en: '**Answer**: We are waiting for the answer to be generated. After the LLM processes
    the prompt, the RAG logic finishes by sending the generated response to the user.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案**：我们正在等待生成答案。在LLM处理提示后，RAG逻辑通过将生成的响应发送给用户来完成。'
- en: That wraps up the overview of the RAG inference pipeline. Now, let’s dig deeper
    into the details.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了RAG推理管道概述。现在，让我们深入了解细节。
- en: Exploring the LLM Twin’s advanced RAG techniques
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLM双胞胎的先进RAG技术
- en: 'Now that we understand the overall flow of our RAG inference pipeline, let’s
    explore the advanced RAG techniques we used in our retrieval module:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们的RAG推理管道的整体流程，让我们来探索我们在检索模块中使用的先进RAG技术：
- en: '**Pre-retrieval step**: Query expansion and self-querying'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预检索步骤**：查询扩展和自我查询'
- en: '**Retrieval step**: Filtered vector search'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索步骤**：过滤向量搜索'
- en: '**Post-retrieval step**: Reranking'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后检索步骤**：重新排序'
- en: Before digging into each method individually, let’s lay down the Python interfaces
    we will use in this section, which are available at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/base.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/base.py).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨每种方法之前，让我们列出我们将在此部分使用的Python接口，这些接口可在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/base.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/base.py)找到。
- en: 'The first is a prompt template factory that standardizes how we instantiate
    prompt templates. As an interface, it inherits from `ABC` and exposes the `create_template()`
    method, which returns a LangChain `PromptTemplate` instance. Even if we avoid
    being heavily reliant on LangChain, as we want to implement everything ourselves
    to understand the engineering behind the scenes, some objects, such as the `PromptTemplate`
    class, are helpful to speed up the development without hiding too much functionality:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是提示模板工厂，它标准化了提示模板的实例化方式。作为一个接口，它继承自`ABC`并公开了`create_template()`方法，该方法返回一个LangChain
    `PromptTemplate`实例。即使我们避免过度依赖LangChain，因为我们想自己实现所有内容以了解背后的工程，但某些对象，如`PromptTemplate`类，有助于加快开发速度，同时不会隐藏太多功能：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We also want to define a `RAGStep` interface used to standardize the interface
    of advanced RAG steps such as query expansion and self-querying. As these steps
    are often dependent on other LLMs, it has a `mock` attribute to reduce costs and
    debugging time during development:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望定义一个`RAGStep`接口，用于标准化高级RAG步骤（如查询扩展和自我查询）的接口。由于这些步骤通常依赖于其他LLM，它有一个`mock`属性，以减少开发过程中的成本和调试时间：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Ultimately, we must understand how we modeled the `Query` domain entity to
    wrap the user’s input with other metadata required for advanced RAG. Thus, let’s
    look at its implementation. First, we import the necessary classes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们必须了解我们如何对`Query`域实体进行建模，以便用其他所需的高级RAG元数据包装用户的输入。因此，让我们看看它的实现。首先，我们导入必要的类：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we define the `Query` entity class, which inherits from the `VectorBaseDocument`
    **object-vector mapping** (**OVM**) class, discussed in *Chapter 4*. Thus, each
    query can easily be saved or retrieved from the vector DB:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了继承自第4章中讨论的`VectorBaseDocument` **对象-向量映射（OVM**）类的`Query`实体类。因此，每个查询都可以轻松地保存或从向量数据库中检索：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'What is essential to notice are the class’s attributes used to combine the
    user’s query with a bunch of metadata fields:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意该类用于将用户的查询与一系列元数据字段组合的属性：
- en: '`content`: A string containing input query.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`content`：包含输入查询的字符串。'
- en: '`author_id`: An optional UUID4 identifier extracted from the query used as
    a filter within the vector search operation to retrieve chunks written only by
    a specific author'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`author_id`：从查询中提取的可选UUID4标识符，用作向量搜索操作中的过滤器，以检索仅由特定作者编写的块'
- en: '`author_full_name`: An optional string used to query the `author_id`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`author_full_name`：用于查询`author_id`的可选字符串'
- en: '`metadata`: A dictionary for any additional metadata, initialized as an empty
    `dict` by default'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`：用于任何附加元数据的字典，默认初始化为空的`dict`'
- en: 'Besides the standard definition of a domain class, we also define a `from_str()`
    class method to create a `Query` instance directly from a string. This allows
    us to standardize how we clean the query string before constructing the `query`
    object, such as stripping any leading or trailing whitespace and newline characters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准定义的域类之外，我们还定义了一个`from_str()`类方法，可以直接从字符串创建`Query`实例。这允许我们标准化在构建`query`对象之前清理查询字符串的方式，例如删除任何前导或尾随空白和换行符：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Additionally, there’s an instance method called `replace_content()` used to
    create a new `Query` instance with updated content while retaining the original
    query’s `id`, `author_id`, `author_full_name`, and `metadata`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个名为`replace_content()`的实例方法，用于创建一个新的`Query`实例，其中包含更新的内容，同时保留原始查询的`id`、`author_id`、`author_full_name`和`metadata`：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This can be particularly useful when modifying the query text, for example,
    during preprocessing or normalization, without losing the associated metadata
    or identifiers. Following the `Query` class, we define the `EmbeddedQuery` class:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这在修改查询文本时特别有用，例如在预处理或归一化过程中，而不会丢失相关的元数据或标识符。在`Query`类之后，我们定义了`EmbeddedQuery`类：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `EmbeddedQuery` class extends `Query` by adding the embedding field. The
    `EmbeddedQuery` entity encapsulates all the data and metadata necessary to perform
    vector search operations on top of Qdrant (or another vector DB).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`EmbeddedQuery`类通过添加嵌入字段扩展了`Query`类。`EmbeddedQuery`实体封装了执行在Qdrant（或另一个向量数据库）上执行向量搜索操作所需的所有数据和元数据。'
- en: Now that we understand all the interfaces and new domain entities used within
    the RAG inference pipeline, let’s move on to our advanced RAG pre-retrieval optimization
    techniques.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了RAG推理管道中使用的所有接口和新域实体，让我们继续了解我们的高级RAG预检索优化技术。
- en: 'Advanced RAG pre-retrieval optimizations: query expansion and self-querying'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级RAG预检索优化：查询扩展和自查询
- en: 'We implemented two methods to optimize the pre-retrieval optimization step:
    query expansion and self-querying. The two methods work closely with the filtered
    vector search step, which we will touch on in the next section. For now, however,
    we will start with understanding the code for query expansion and move to implementing
    self-querying.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了两种方法来优化预检索优化步骤：查询扩展和自查询。这两种方法与过滤向量搜索步骤紧密合作，我们将在下一节中讨论。然而，现在我们将从理解查询扩展的代码开始，然后转向实现自查询。
- en: Within these two methods, we will leverage OpenAI’s API to generate variations
    of the original query within the query expansion step and to extract the necessary
    metadata within the self-querying algorithm. When we wrote this book, we used
    `GPT-4o-mini` in all our examples, but as OpenAI’s models quickly evolve, the
    model might get deprecated. But that’s not an issue, as you can quickly change
    it in your `.env` file by configuring the `OPENAI_MODEL_ID` environment variable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个方法中，我们将在查询扩展步骤中利用OpenAI的API生成原始查询的变体，并在自查询算法中提取必要的元数据。当我们编写这本书时，我们在所有示例中都使用了`GPT-4o-mini`，但随着OpenAI模型的快速演变，该模型可能会被弃用。但这不是问题，因为您可以通过配置`OPENAI_MODEL_ID`环境变量快速在您的`.env`文件中更改它。
- en: Query expansion
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询扩展
- en: The *problem* in a typical retrieval step is that you query your vector DB using
    a single vector representation of your original question. This approach covers
    only a small area of the embedding space, which can be limiting. If the embedding
    doesn’t contain all the required information or nuances of your query, the retrieved
    context may not be relevant. This means essential documents that are semantically
    related but not near the query vector might be overlooked.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的检索步骤中，*问题*在于你使用原始问题的单个向量表示查询你的向量数据库。这种方法只覆盖了嵌入空间的一小部分，这可能有限制。如果嵌入不包含你查询的所有必要信息或细微差别，检索到的上下文可能不相关。这意味着与查询向量语义相关但不在其附近的重要文档可能会被忽略。
- en: The *solution* is based on query expansion, which offers a way to overcome this
    limitation. Using an LLM to generate multiple queries based on your initial question,
    you create various perspectives that capture different facets of your query. These
    expanded queries, when embedded, target other areas of the embedding space that
    are still relevant to your original question. This increases the likelihood of
    retrieving more relevant documents from the vector DB.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案*基于查询扩展，提供了一种克服这种限制的方法。使用LLM根据你的初始问题生成多个查询，你创建了各种视角，以捕捉查询的不同方面。这些扩展查询在嵌入时，针对的是与你的原始问题仍然相关的嵌入空间的其他区域。这增加了从向量数据库检索更多相关文档的可能性。'
- en: Implementing query expansion can be as straightforward as crafting a detailed
    zero-shot prompt to guide the LLM in generating these alternative queries. Thus,
    after implementing query expansion, instead of having only one query to search
    relevant context, you will have xN queries, hence xN searches.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实现查询扩展可以像编写一个详细的零样本提示来引导LLM生成这些替代查询一样简单。因此，在实现查询扩展后，你将不再只有一个查询来搜索相关上下文，而是将有xN个查询，因此有xN次搜索。
- en: Increasing the number of searches can impact your latency. Thus, you must experiment
    with the number of queries you generate to ensure the retrieval step meets your
    application requirements. You can also optimize the searches by parallelizing
    them, drastically reducing the latency, which we will do in the `ContextRetriever`
    class implemented at the end of this chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 增加搜索次数可能会影响你的延迟。因此，你必须实验性地调整你生成的查询数量，以确保检索步骤满足你的应用需求。你还可以通过并行化搜索来优化搜索，这可以大幅减少延迟，我们将在本章末尾实现的`ContextRetriever`类中这样做。
- en: 'Query expansion is also known as multi-query, but the principles are the same.
    For example, this is an example of LangChain’s implementation called `MultiQueryRetriver`:
    [https://python.langchain.com/docs/how_to/MultiQueryRetriever/](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展也称为多查询，但其原理是相同的。例如，这是一个LangChain实现的多查询检索器的例子，称为`MultiQueryRetriver`：[https://python.langchain.com/docs/how_to/MultiQueryRetriever/](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)
- en: 'Now, let’s dig into the code. We begin by importing the necessary modules and
    classes required for query expansion:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入代码。我们首先导入查询扩展所需的必要模块和类：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we define the `QueryExpansion` class, which generates expanded query
    versions. The class implementation can be found at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/query_expanison.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/query_expanison.py):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`QueryExpansion`类，该类生成扩展查询版本。类的实现可以在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/query_expanison.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/query_expanison.py)找到：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the `generate` method, we first ensure that the number of expansions requested
    (`expand_to_n`) is greater than zero. If the instance is in mock mode (`self._mock
    is True`), it simply returns a list containing copies of the original query to
    simulate expansion without actually calling the API. If not in mock mode, we proceed
    to create the prompt and initialize the language model:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在`generate`方法中，我们首先确保请求的扩展数量（`expand_to_n`）大于零。如果实例处于模拟模式（`self._mock is True`），它将简单地返回一个包含原始查询副本的列表，以模拟扩展而不实际调用API。如果不处于模拟模式，我们继续创建提示并初始化语言模型：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we instantiate `QueryExpansionTemplate` and create a prompt tailored
    to generate `expand_to_n - 1` new queries (excluding the original). We initialize
    the `ChatOpenAI` model with the specified settings and set the temperature to
    0 for deterministic output. We then create a LangChain chain by combining the
    prompt with the model and invoke it with the user’s question:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实例化`QueryExpansionTemplate`并创建一个针对生成`expand_to_n - 1`个新查询（不包括原始查询）的定制提示。我们使用指定的设置初始化`ChatOpenAI`模型并将温度设置为0以获得确定性输出。然后我们通过将提示与模型结合创建一个LangChain链，并用用户的问题调用它：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'By piping the prompt into the model (`prompt | model`), we set up a chain that
    generates expanded queries when invoked with the original query. The response
    from the model is captured in the `result` object. After receiving the response,
    we parse and clean the expanded queries:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将提示管道输入到模型中（`prompt | model`），我们设置了一个在调用原始查询时生成扩展查询的链。模型的响应被捕获在`result`对象中。在收到响应后，我们解析和清理扩展查询：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We split the result using the separator defined in the template to get individual
    queries. Starting with a list containing the original query, we append each expanded
    query after stripping any extra whitespace.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模板中定义的分隔符来分割结果，以获取单个查询。从包含原始查询的列表开始，我们在去除任何额外空格后添加每个扩展查询。
- en: 'Finally, we define the `QueryExpansionTemplate` class, which constructs the
    prompt used for query expansion. The class and other prompt templates can be accessed
    at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了`QueryExpansionTemplate`类，该类构建用于查询扩展的提示。该类和其他提示模板可以在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py)找到：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This class defines a prompt instructing the language model to generate multiple
    versions of the user’s question. It uses placeholders like `{expand_to_n}`, `{separator}`,
    and `{question}` to customize the prompt.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类定义了一个提示，指示语言模型生成用户问题的多个版本。它使用像`{expand_to_n}`、`{separator}`和`{question}`这样的占位符来自定义提示。
- en: It takes `expand_to_n` as an input parameter to define how many queries we wish
    to generate while we build the `PromptTemplate` instance. The separator property
    provides a unique string to split the generated queries. The `expand_to_n` and
    `separator` variables are passed as `partial_variables`, making them immutable
    at runtime. Meanwhile, the `{question}` placeholder will be changed every time
    the LLM chain is called.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受`expand_to_n`作为输入参数，以定义在构建`PromptTemplate`实例时我们希望生成多少个查询。分隔符属性提供了一个独特的字符串来分割生成的查询。`expand_to_n`和`separator`变量作为`partial_variables`传递，使它们在运行时不可变。同时，`{question}`占位符将在每次调用LLM链时更改。
- en: 'Now that we have finished studying the query expansion implementation, let’s
    look at an example of how to use the `QueryExpansion` class. Let’s run the following
    code using this `python -m llm_engineering.application.rag.query_expansion` command:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了查询扩展实现的研读，接下来让我们看看如何使用`QueryExpansion`类的一个示例。使用以下`python -m llm_engineering.application.rag.query_expansion`命令来运行以下代码：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the following variations of the original query. As you can observe,
    the query expansion method was successful in providing more details and different
    perspectives of the initial query, such as highlighting the effectiveness of advanced
    RAG methods or the overview of these methods (remember that the first query is
    the original one):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了原始查询的以下变体。如您所观察到的，查询扩展方法成功地提供了更多细节和初始查询的不同视角，例如突出高级RAG方法的有效性或这些方法的概述（请记住，第一个查询是原始查询）：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s move to the next pre-retrieval optimization method: self-querying.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向下一个预检索优化方法：自查询。
- en: Self-querying
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自查询
- en: The *problem* when embedding your query into a vector space is that you cannot
    guarantee that all the aspects required by your use case are present with enough
    signal in the embedding vector. For example, you want to be 100% sure that your
    retrieval depends on the tags provided in the user’s input. Unfortunately, you
    can’t control the signal left within the embedding that emphasizes the tag. By
    embedding the query prompt alone, you can never be sure that the tags are sufficiently
    represented in the embedding vector or have enough signal when computing the distance
    against other vectors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当将查询嵌入到向量空间时遇到的问题是，你不能保证你的用例所需的所有方面都在嵌入向量中有足够的信号。例如，你想要确保你的检索完全依赖于用户输入中提供的标签。不幸的是，你无法控制嵌入中留下的强调标签的信号。仅通过嵌入查询提示，你永远无法确定标签在嵌入向量中得到了充分的表示，或者在与其他向量计算距离时具有足够的信号。
- en: This problem stands for any other metadata you want to present during the search,
    such as IDs, names, or categories.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题代表了你希望在搜索过程中展示的任何其他元数据，例如ID、名称或类别。
- en: The *solution* is to use self-queryingto extract the tags or other critical
    metadata within the query and use them alongside the vector search as filters.
    Self-querying uses an LLM to extract various metadata fields crucial for your
    business use case, such as tags, IDs, number of comments, likes, shares, etc.
    Afterward, you have complete control over how the extracted metadata is considered
    during retrieval. In our LLM Twin use case, we extract the author’s name and use
    it as a filter. Self-queries work hand-in-hand with filtered vector searches,
    which we will explain in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案*是使用自查询来提取查询中的标签或其他关键元数据，并将其与向量搜索一起用作过滤器。自查询使用LLM提取对您的业务用例至关重要的各种元数据字段，如标签、ID、评论数、点赞、分享等。之后，您将完全控制提取的元数据在检索过程中的考虑方式。在我们的LLM
    Twin用例中，我们提取作者的名字并将其用作过滤器。自查询与过滤向量搜索协同工作，我们将在下一节中解释。'
- en: 'Now, let’s move on to the code. We begin by importing the necessary modules
    and classes on which our code relies:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向代码。我们首先导入代码所依赖的必要模块和类：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we define the `SelfQuery` class, which inherits from `RAGStep` and implements
    the `generate()` method. The class can be found at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/self_query.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/self_query.py):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了继承自 `RAGStep` 并实现了 `generate()` 方法的 `SelfQuery` 类。该类可以在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/self_query.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/self_query.py)找到：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the `generate()` method, we check if the `_mock` attribute is set to `True`.
    If it is, we will return the original query object unmodified. This allows us
    to bypass calling the model while testing and debugging. If not in mock mode,
    we create the prompt template and initialize the language model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `generate()` 方法中，我们检查 `_mock` 属性是否设置为 `True`。如果是，我们将返回未经修改的原始查询对象。这允许我们在测试和调试时绕过调用模型。如果不是模拟模式，我们将创建提示模板并初始化语言模型。
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we instantiate the prompt using the `SelfQueryTemplate` factory class
    and create a `ChatOpenAI` model instance (similar to the query expansion implementation).
    We then combine the prompt and the model into a chain and invoke it with the user’s
    query.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `SelfQueryTemplate` 工厂类实例化提示，并创建一个 `ChatOpenAI` 模型实例（类似于查询扩展实现）。然后我们将提示和模型组合成一个链，并用用户的查询调用它。
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We extract the content from the LLM response and strip any leading or trailing
    whitespace to obtain the `user_full_name` value. Next, we check if the model was
    able to extract any user information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从LLM响应中提取内容，并去除任何前导或尾随空白，以获得 `user_full_name` 值。接下来，我们检查模型是否能够提取任何用户信息。
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If the response is `"none"`, it means no user name was found in the query,
    so we return the original query object. If a user name is found, we will split
    the `user_full_name` into the `first_name` and `last_name` variables using a utility
    function. Then, based on the user’s details, we retrieve or create a `UserDocument`
    user instance:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应是 `"none"`，则表示查询中没有找到用户名，因此我们返回原始查询对象。如果找到用户名，我们将使用一个实用函数将 `user_full_name`
    分割成 `first_name` 和 `last_name` 变量。然后，根据用户的详细信息，我们检索或创建一个 `UserDocument` 用户实例：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we update the query object with the extracted author information and
    return it:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将提取的作者信息更新到查询对象中，并返回它：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The updated query now contains the `author_id` and `author_full_name` values,
    which can be used in subsequent steps of the RAG pipeline.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的查询现在包含了`author_id`和`author_full_name`值，这些值可以在RAG管道的后续步骤中使用。
- en: 'Let’s look at the `SelfQueryTemplate` class, which defines the prompt to extract
    user information:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看定义提取用户信息的提示的`SelfQueryTemplate`类：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the `SelfQueryTemplate` class, we define a prompt instructing the AI model
    to extract the *user name* or *ID* from the input question. The prompt uses few-shot
    learning to guide the model on how to respond in different scenarios. When the
    template is invoked, the `{question}` placeholder will be replaced with the actual
    user question.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SelfQueryTemplate`类中，我们定义了一个提示，指示AI模型从输入问题中提取*用户名*或*ID*。该提示使用少量样本学习来指导模型在不同场景下的响应。当模板被调用时，`{question}`占位符将被替换为实际的用户问题。
- en: By implementing self-querying, we ensure that critical metadata required for
    our use case is explicitly extracted and used during retrieval. This approach
    overcomes the limitations of relying solely on the semantics of the embeddings
    to capture all necessary aspects of a query.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现自我查询，我们确保了对于我们的用例所必需的关键元数据被明确提取并在检索过程中使用。这种方法克服了仅依赖嵌入语义来捕获查询所有必要方面的局限性。
- en: 'Now that we’ve implemented the `SelfQuery` class, let’s provide an example.
    Run the following code using the `python -m llm_engineering.application.rag.self_query`
    CLI command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了`SelfQuery`类，让我们提供一个示例。使用`python -m llm_engineering.application.rag.self_query`
    CLI命令运行以下代码：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get the following results where the author’s full name and ID were extracted
    correctly:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果，其中正确提取了作者的完整姓名和ID：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now that we understand how self-querying works, let’s explore how it can be
    used together with filtered vector search within the retrieval optimization step.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了自我查询的工作原理，让我们探索如何在检索优化步骤中将它与过滤向量搜索结合使用。
- en: 'Advanced RAG retrieval optimization: filtered vector search'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级RAG检索优化：过滤向量搜索
- en: Vector search is pivotal in retrieving relevant information based on semantic
    similarity. A plain vector search, however, can introduce significant challenges
    that affect both the accuracy and latency of information retrieval. This is primarily
    because it operates solely on the numerical proximity of vector embeddings without
    considering the contextual or categorical nuances that might be crucial for relevance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语义相似性检索相关信息是向量搜索的关键。然而，普通的向量搜索可能会引入显著挑战，这些挑战会影响信息检索的准确性和延迟。这主要是因为它仅基于向量嵌入的数值邻近性进行操作，而没有考虑可能对相关性至关重要的上下文或分类细微差别。
- en: One of the primary issues with plain vector search is retrieving semantically
    similar but contextually irrelevant documents. Since vector embeddings capture
    general semantic meanings, they might assign high similarity scores to content
    that shares language patterns or topics but doesn’t align with the specific intent
    or constraints of the query. For instance, searching for “Java” could retrieve
    documents about the programming language or the Indonesian island, depending solely
    on semantic similarity, leading to ambiguous or misleading results.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 普通向量搜索的一个主要问题是检索到语义相似但上下文无关的文档。由于向量嵌入捕获了一般的语义含义，它们可能会将高相似度分数分配给共享语言模式或主题的内容，但这些内容并不符合查询的具体意图或约束。例如，搜索“Java”可能会根据语义相似性检索关于编程语言或印度尼西亚岛屿的文档，导致模糊或误导性的结果。
- en: Moreover, as the size of the dataset increases, plain vector search can suffer
    from scalability issues. The lack of filtering means the search algorithm has
    to compute similarities across the entire vector space, which can significantly
    increase latency.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着数据集大小的增加，普通向量搜索可能会遇到可扩展性问题。缺乏过滤意味着搜索算法必须在整个向量空间中计算相似度，这可能会显著增加延迟。
- en: This exhaustive search slows response times and consumes more computational
    resources, making it inefficient for real-time or large-scale applications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种详尽的搜索会减慢响应时间并消耗更多计算资源，对于实时或大规模应用来说效率低下。
- en: Filtered vector search emerges as a solution by filtering after additional criteria,
    such as metadata tags or categories, reducing the search space before computing
    vector similarities. By applying these filters, the search algorithm narrows the
    pool of potential results to those contextually aligned with the query’s intent.
    This targeted approach enhances accuracy by eliminating irrelevant documents that
    might have otherwise been considered due to their semantic similarities alone.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过过滤额外的标准，如元数据标签或类别，过滤向量搜索成为解决方案，在计算向量相似性之前减少搜索空间。通过应用这些过滤器，搜索算法将潜在结果池缩小到与查询意图上下文一致的结果。这种有针对性的方法通过消除由于语义相似性而被考虑的不相关文档来提高准确性。
- en: Additionally, filtered vector search improves latency by reducing the number
    of comparisons the algorithm needs to perform. Working with a smaller, more relevant
    subset of data decreases the computational overhead, leading to faster response
    times. This efficiency is crucial for applications requiring real-time interactions
    or handling large queries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过减少算法需要执行的比较次数，过滤向量搜索提高了延迟。处理更小、更相关的数据子集可以降低计算开销，从而缩短响应时间。这种效率对于需要实时交互或处理大量查询的应用至关重要。
- en: As the metadata used within the filtered vector search is often part of the
    user’s input, we have to extract it before querying the vector DB. That’s precisely
    what we did during the self-query step, where we extracted the author’s name to
    reduce the vector space only to the author’s content. Thus, as we processed the
    query within the self-query step, it went into the pre-retrieval optimization
    category, whereas when the filtered vector search optimized the query, it went
    into the retrieval optimization bin.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过滤向量搜索中使用的元数据通常是用户输入的一部分，我们必须在查询向量数据库之前提取它。这正是我们在自我查询步骤中所做的，我们提取了作者的名字，以将向量空间仅限于作者的内容。因此，当我们处理自我查询步骤中的查询时，它被归类为检索前优化，而当我们过滤向量搜索优化查询时，它被归类为检索优化。
- en: 'For example, when using Qdrant, to add a filter that looks for a matching `author_id`
    within the metadata of each document, you must implement the following code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用Qdrant时，为了在每份文档的元数据中查找匹配的`author_id`，你必须实现以下代码：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In essence, while plain vector search provides a foundation for semantic retrieval,
    its limitations can slow performance in practical applications. Filtered vector
    search addresses these challenges by combining the strengths of vector embeddings
    with contextual filtering, resulting in more accurate and efficient information
    retrieval in RAG systems. The last step for optimizing our RAG pipeline is to
    look into reranking.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，虽然普通向量搜索为语义检索提供了一个基础，但其局限性可能会在实际应用中降低性能。过滤向量搜索通过结合向量嵌入和上下文过滤的优势来解决这些挑战，从而在RAG系统中实现更准确和高效的信息检索。优化我们的RAG管道的最后一步是考虑重新排序。
- en: 'Advanced RAG post-retrieval optimization: reranking'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级RAG检索后优化：重新排序
- en: 'The *problem*in RAG systems is that the retrieved context may contain irrelevant
    chunks that only:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统中*问题*在于检索到的上下文可能包含不相关的片段，这些片段只会：
- en: '**Add noise**: The retrieved context might be irrelevant, cluttering the information
    and potentially confusing the language model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加噪声**：检索到的上下文可能是不相关的，这会扰乱信息并可能使语言模型产生混淆。'
- en: '**Make the prompt bigger**: Including unnecessary chunks increases the prompt
    size, leading to higher costs. Moreover, language models are usually biased toward
    the context’s first and last pieces. So, if you add a large amount of context,
    there’s a big chance it will miss the essence.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使提示更大**：包括不必要的片段会增加提示的大小，导致成本更高。此外，语言模型通常偏向于上下文的第一部分和最后一部分。因此，如果你添加了大量上下文，有很大可能会错过本质。'
- en: '**Be come unaligned with your question**: Chunks are retrieved based on the
    similarity between the query and chunk embeddings. The issue is that the embedding
    model might not be tuned to your question, resulting in high similarity scores
    for chunks that aren’t entirely relevant.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与你的问题不一致**：片段是根据查询与片段嵌入之间的相似性检索的。问题是嵌入模型可能没有针对你的问题进行调整，导致不完全不相关的片段具有很高的相似度评分。'
- en: The *solution*is touse reranking to order all the N × K retrieved chunks based
    on their relevance relative to the initial question, where the first chunk will
    be the most relevant and the last the least. N represents the number of searches
    after query expansion, while K is the number of chunks retrieved per search. Hence,
    we retrieve a total of N x K chunks. In RAG systems, reranking serves as a critical
    post-retrieval step that refines the initial results obtained from the retrieval
    model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案*是使用重排序对所有检索到的 N × K 个片段进行排序，根据它们相对于初始问题的相关性，其中第一个片段将是最相关的，最后一个是最不相关的。N
    代表查询扩展后的搜索次数，而 K 是每次搜索检索的片段数。因此，我们总共检索了 N x K 个片段。在 RAG 系统中，重排序作为关键的后检索步骤，可以细化从检索模型获得的初始结果。'
- en: We assess each chunk’s relevance to the original query by applying the reranking
    algorithm, which often uses advanced models like neural cross-encoders. These
    models evaluate the semantic similarity between the query and each chunk more
    accurately than initial retrieval methods based on embeddings and the cosine similarity
    distance, as explained in more detail in *Chapter 4* in the *An overview of advanced
    RAG* section.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过应用重排序算法来评估每个片段对原始查询的相关性，该算法通常使用像神经交叉编码器这样的高级模型。这些模型比基于嵌入和余弦相似度距离的初始检索方法更准确地评估查询与每个片段之间的语义相似性，如在第
    4 章中更详细地解释的 *高级 RAG 概述* 部分。
- en: 'Ultimately, we pick the top K most relevant chunks from the sorted list of
    N x K items based on the reranking score. Reranking works well when combined with
    **query expansion**. First, let’s understand how reranking works without query
    expansion:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们根据重排序分数从排序后的 N x K 项中挑选出最相关的 K 个片段。当与**查询扩展**结合使用时，重排序效果良好。首先，让我们了解在没有查询扩展的情况下重排序是如何工作的：
- en: '**Search for > K chunks**: Retrieve more than K chunks to have a broader pool
    of potentially relevant information.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**搜索 > K 个片段**：检索超过 K 个片段，以获得更广泛的潜在相关信息。'
- en: '**Reorder using rerank**: Apply reranking to this larger set to evaluate the
    actual relevance of each chunk relative to the query.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用重排序重新排序**：将重排序应用于这个更大的集合，以评估每个片段相对于查询的实际相关性。'
- en: '**Take top K**: Select the top K chunks to use them as context in the final
    prompt.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择前 K 个**：选择前 K 个片段，在最终提示中使用它们作为上下文。'
- en: 'Thus, when combined with query expansion, we gather potential valuable context
    from multiple points in space rather than just looking for more than K samples
    in a single location. Now the flow looks like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当与查询扩展结合使用时，我们从空间中的多个点收集潜在的有价值上下文，而不仅仅是寻找单个位置中的超过 K 个样本。现在流程看起来是这样的：
- en: '**Search for N × K chunks**: Retrieve multiple sets of chunks using the expanded
    queries.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**搜索 N × K 个片段**：使用扩展查询检索多个片段集。'
- en: '**Reorder using rerank**: Rerank all the retrieved chunks based on their relevance.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用重排序重新排序**：根据相关性对所有检索到的片段进行重排序。'
- en: '**Take top K**: Select the most relevant chunks for the final prompt.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择前 K 个**：选择最相关的片段作为最终提示。'
- en: Integrating reranking into the RAG pipeline enhances the quality and relevance
    of the retrieved context and efficiently uses computational resources. Let’s look
    at implementing the LLM Twin’s reranking step to understand what we described
    above, which can be accessed on GitHub at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/reranking.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/reranking.py).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将重排序集成到 RAG 流程中可以增强检索上下文的质量和相关性，并有效地使用计算资源。让我们看看如何实现 LLM Twin 的重排序步骤，以了解我们上面描述的内容，该步骤可以在
    GitHub 上找到：[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/reranking.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/reranking.py)。
- en: 'We begin by importing the necessary modules and classes for our reranking process:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入重排序过程中所需的必要模块和类：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we define the `Reranker` class, which is responsible for reranking the
    retrieved documents based on their relevance to the query:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义 `Reranker` 类，该类负责根据查询的相关性对检索到的文档进行重排序：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the initializer of the Reranker class, we instantiate our cross-encoder model
    by creating an instance of `CrossEncoderModelSingleton`. This is the cross-encoder
    model used to score the relevance of each document chunk with respect to the query.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Reranker` 类的初始化器中，我们通过创建 `CrossEncoderModelSingleton` 的实例来实例化我们的交叉编码器模型。这是用于对每个文档片段相对于查询的相关性进行评分的交叉编码器模型。
- en: 'The core functionality of the `Reranker` class is implemented in the `generate()`
    method:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`Reranker`类的核心功能在`generate()`方法中实现：'
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `generate()` method takes a query, a list of chunks (document segments),
    and the number of top documents to keep (`keep_top_k`). If we’re in mock mode,
    it simply returns the original chunks. Otherwise, it performs the following steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate()`方法接受一个查询、一个块列表（文档段）以及要保留的前`keep_top_k`个文档数。如果我们处于模拟模式，它将简单地返回原始块。否则，它执行以下步骤：'
- en: Creates pairs of the query content and each chunk’s content
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建查询内容和每个块内容的对
- en: Uses the cross-encoder model to score each pair, assessing how well the chunk
    matches the query
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉编码器模型对每一对进行评分，评估块与查询的匹配程度
- en: Zips the scores with the corresponding chunks to create a scored list of tuples
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分数与相应的块进行匹配，创建一个分数列表的元组
- en: Sorts this list in descending order based on the scores
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据分数按降序排序此列表
- en: Selects the top `keep_top_k` chunks
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择前`keep_top_k`个块
- en: Extracts the chunks from the tuples and returns them as the reranked documents
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从元组中提取块并将它们作为重新排序的文档返回
- en: 'Before defining the `CrossEncoder` class, we import the necessary components:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义`CrossEncoder`类之前，我们导入必要的组件：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We import the `CrossEncoder` class from the sentence_transformers library,
    which provides the functionality for scoring text pairs. We also import `SingletonMeta`
    from our base module to ensure our model class follows the singleton pattern,
    meaning only one instance of the model exists throughout the application. Now,
    we define the `CrossEncoderModelSingleton` class:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从sentence_transformers库中导入`CrossEncoder`类，该库提供了评分文本对的功能。我们还从我们的基础模块中导入`SingletonMeta`，以确保我们的模型类遵循单例模式，这意味着在整个应用程序中只有一个模型实例。现在，我们定义`CrossEncoderModelSingleton`类：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This class initializes the cross-encoder model using the specified `model_id`
    and `device` from the global `settings` loaded from the `.env` file. We set the
    model to evaluation mode using `self._model.model.eval()` to ensure the model
    is ready for inference.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用从`.env`文件加载的全局`settings`中指定的`model_id`和`device`初始化交叉编码器模型。我们使用`self._model.model.eval()`将模型设置为评估模式，以确保模型已准备好进行推理。
- en: 'The `CrossEncoderModelSingleton` class includes a callable method to score
    text pairs:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrossEncoderModelSingleton`类包含一个可调用的方法来评分文本对：'
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `__call__` method allows us to pass in a list of text `pairs` (each consisting
    of the query and a document chunk) and receive their relevance scores. The method
    uses the model’s `predict()` function to call the model and compute the scores.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`__call__`方法允许我们传入一个包含文本`pairs`（每个包含查询和文档块）的列表，并接收它们的相关性分数。该方法使用模型的`predict()`函数调用模型并计算分数。'
- en: The `CrossEncoderModelSingleton` class is a wrapper over the `CrossEncoder`
    class, which we wrote for two purposes. The first one is for the singleton pattern,
    which allows us to easily access the same instance of the cross-encoder model
    from anywhere within the application without loading the model in memory every
    time we need it. The second reason is that by writing our wrapper, we defined
    our interface for a cross-encoder model (or any other model used for reranking).
    This makes the code future-proof as in case we need a different implementation
    or strategy for reranking, for example, using an API, we only have to write a
    different wrapper that follows the same interface and swap the old class with
    the new one. Thus, we can introduce new reranking methods without touching the
    rest of the code.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrossEncoderModelSingleton`类是对`CrossEncoder`类的包装，我们编写它有两个目的。第一个目的是为了单例模式，这使得我们能够轻松地从应用程序的任何地方访问相同的交叉编码器模型实例，而无需每次需要时都将其加载到内存中。第二个原因是，通过编写我们的包装器，我们定义了交叉编码器模型（或用于重新排序的任何其他模型）的接口。这使得代码具有前瞻性，因为如果我们需要不同的实现或策略进行重新排序，例如使用API，我们只需编写一个遵循相同接口的不同包装器，并用新类替换旧类。因此，我们可以引入新的重新排序方法，而无需触及代码的其他部分。'
- en: We now understand all the advanced RAG techniques used within our architecture.
    In the next section, we will examine the `ContextRetriever` class that connects
    all these methods and explain how to use the retrieval module with an LLM for
    an end-to-end RAG inference pipeline.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在理解了架构中使用的所有高级RAG技术。在下一节中，我们将检查`ContextRetriever`类，该类连接所有这些方法，并解释如何使用检索模块与LLM结合进行端到端的RAG推理管道。
- en: Implementing the LLM Twin’s RAG inference pipeline
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现LLM Twin的RAG推理管道
- en: 'As explained at the beginning of this chapter, the RAG inference pipeline can
    mainly be divided into three parts: the retrieval module, the prompt creation,
    and the answer generation, which boils down to calling an LLM with the augmented
    prompt. In this section, our primary focus will be implementing the retrieval
    module, where most of the code and logic go. Afterward, we will look at how to
    build the final prompt using the retrieved context and user query.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所述，RAG推理管道主要可以分为三个部分：检索模块、提示创建和答案生成，这归结为调用一个LLM并使用增强的提示。在本节中，我们的主要重点是实现检索模块，其中大部分代码和逻辑都在这里。之后，我们将看看如何使用检索到的上下文和用户查询构建最终的提示。
- en: Ultimately, we will examine how to combine the retrieval module, prompt creation
    logic, and the LLM to capture an end-to-end RAG workflow. Unfortunately, we won’t
    be able to test out the LLM until we finish *Chapter 10*, as we haven’t deployed
    our fine-tuned LLM Twin module to AWS SageMaker.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将探讨如何结合检索模块、提示创建逻辑和LLM来捕获端到端的RAG工作流程。不幸的是，我们无法在完成*第10章*之前测试LLM，因为我们尚未将微调的LLM
    Twin模块部署到AWS SageMaker。
- en: Thus, by the end of this section, you will learn how to implement the RAG inference
    pipeline, which you can test out end to end only after finishing *Chapter 10*.
    Now, let’s start by looking at the implementation of the retrieval module.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节结束时，你将学习如何实现RAG推理管道，你只能在完成*第10章*后才能从头到尾进行测试。现在，让我们先看看检索模块的实现。
- en: Implementing the retrieval module
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现检索模块
- en: 'Let’s dive into the `ContextRetriever` class implementation, which orchestrates
    the retrieval step in our RAG system by integrating all the advanced techniques
    we previously used: query expansion, self-querying, reranking, and filtered vector
    search. The class can be found on GitHub at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/retriever.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/retriever.py).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入到`ContextRetriever`类的实现，该类通过整合我们之前使用过的所有高级技术（查询扩展、自我查询、重新排序和过滤向量搜索）来协调我们RAG系统中的检索步骤。该类可以在GitHub上找到：[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/retriever.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/retriever.py)。
- en: '![](img/B31105_09_02.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_09_02.png)'
- en: 'Figure 9.2: Search logic of the RAG retrieval module'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：RAG检索模块的搜索逻辑
- en: The entry point function of the `ContextRetriever` class is the `search()` method,
    which calls all the advanced steps discussed in this chapter. *Figure 9.2* shows
    in more detail how the search method glues together all the steps required to
    search results similar to the user’s query. It highlights how the extracted author
    details from the self-query step are used within the filtered vector search. Also,
    it zooms in on the search operation itself, where, for each query, we do three
    searches to the vector DB, looking for articles, posts, or repositories similar
    to the query. For each search (out of N searches), we want to retrieve a maximum
    of K results. Thus, we retrieve a maximum of K / 3 items for each data category
    (as we have three categories). Therefore, when summed up, we will have a list
    of `≤ K` chunks. The retrieved list is `≤ K` (and not equal to K) when a particular
    data category or more returns `< K / 3` items after applying the author filters
    due to missing chunks for that specific author or data category.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`ContextRetriever` 类的入口函数是 `search()` 方法，它调用本章讨论的所有高级步骤。*图9.2* 更详细地展示了搜索方法如何将所有用于搜索与用户查询相似结果的步骤粘合在一起。它突出了从自我查询步骤中提取的作者详细信息如何在过滤向量搜索中使用。同时，它还聚焦于搜索操作本身，对于每个查询，我们向向量数据库进行三次搜索，寻找与查询相似的文章、帖子或存储库。对于每次搜索（N次搜索中的每一次），我们希望检索最多K个结果。因此，对于每个数据类别，我们最多检索K
    / 3个条目（因为我们有三个类别）。因此，当汇总时，我们将有一个`≤ K`个块组成的列表。当某个特定数据类别或更多在应用作者过滤器后返回`< K / 3`个条目时（由于缺少该特定作者或数据类别的块），检索到的列表将是`≤
    K`（而不是等于K）。'
- en: '![](img/B31105_09_03.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_09_03.png)'
- en: 'Figure 9.3: Processing the results flow of the RAG retrieval module'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：RAG检索模块的结果处理流程
- en: '*Figure 9.3* illustrates how we process the results returned by the xN searches.
    As each search returns `≤ K` items, we will end up with `≤ N x K` chunks that
    we aggregate into a single list. As some results might overlap between searchers,
    we must deduplicate the aggregated list to ensure each chunk is unique. Ultimately,
    we send the results to the rerank model, order them based on their reranking score,
    and pick the **most** relevant top **K** chunks we will use as context for RAG.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.3*说明了我们如何处理xN搜索返回的结果。由于每次搜索返回`≤ K`个项目，我们最终将得到`≤ N x K`个块，我们将它们聚合到一个单独的列表中。由于一些结果可能在搜索者之间重叠，我们必须去重聚合列表以确保每个块是唯一的。最终，我们将结果发送到重排序模型，根据它们的重排序分数进行排序，并选择**最**相关的**K**个块作为RAG的上下文。'
- en: 'Let’s understand how everything from *Figures 9.2* and *9.3* is implemented
    in the `ContextRetriever` class. First, we initialize the class by setting up
    instances of the `QueryExpansion`, `SelfQuery`, and `Reranker` classes:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解从*图9.2*和*图9.3*中的一切如何在`ContextRetriever`类中实现。首先，我们通过设置`QueryExpansion`、`SelfQuery`和`Reranker`类的实例来初始化该类：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the `search()` method, we convert the user’s input string into a `query`
    object. We then use the `SelfQuery` instance to extract the `author_id` and `author_full_name`
    from the query:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在`search()`方法中，我们将用户的输入字符串转换为`query`对象。然后我们使用`SelfQuery`实例从查询中提取`author_id`和`author_full_name`：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we expand the query to generate multiple semantically similar queries
    using the `QueryExpansion` instance:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`QueryExpansion`实例扩展查询，生成多个语义相似的查询：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then perform the search concurrently for all expanded queries using a thread
    pool. Each query is processed by the `_search()` method, which we’ll explore shortly.
    The results are flattened, deduplicated, and collected into a single list:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用线程池并发地对所有扩展查询进行搜索。每个查询都通过我们很快将要探讨的`_search()`方法进行处理。结果被扁平化、去重并收集到一个单独的列表中：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After retrieving the documents, we rerank them based on their relevance to
    the original query and keep only the top *k* documents:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索到文档后，我们根据它们与原始查询的相关性对它们进行重排序，并仅保留前*k*个文档：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `_search()` method performs the filtered vector search across different
    data categories like posts, articles, and repositories. It uses the `EmbeddingDispatcher`
    to convert the query into an `EmbeddedQuery`, which includes the query’s embedding
    vector and any extracted metadata:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`_search()`方法在帖子、文章和存储库等不同数据类别上执行过滤向量搜索。它使用`EmbeddingDispatcher`将查询转换为`EmbeddedQuery`，该查询包括查询的嵌入向量和任何提取的元数据：'
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We used the same `EmbeddingDispatcher` to embed the query as in the RAG feature
    pipeline to embed the document chunks stored in the vector DB. Using the same
    class ensures we use the same embedding model at ingestion and query time, which
    is critical for the retrieval step.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与RAG特征管道中嵌入文档块相同的`EmbeddingDispatcher`来嵌入查询。使用相同的类确保我们在摄取和查询时使用相同的嵌入模型，这对于检索步骤至关重要。
- en: 'We search each data category separately by leveraging the local `_search_data_category()`
    function. Within the `_search_data_category()` function, we apply the filters
    extracted from the `embedded_query` object. For instance, if an `author_id` is
    present, we use it to filter the search results only to include documents from
    that author. The results from all categories are then combined:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用本地`_search_data_category()`函数分别对每个数据类别进行搜索。在`_search_data_category()`函数中，我们应用从`embedded_query`对象中提取的过滤器。例如，如果存在`author_id`，我们使用它来过滤搜索结果，仅包括该作者的文档。然后，将所有类别的结果合并：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, the `rerank()` method takes the original query and the list of retrieved
    documents to reorder them based on relevance:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`rerank()`方法接收原始查询和检索到的文档列表，根据相关性对它们进行重新排序：
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Calling the code from above using the following CLI command: `poetry poe call-rag-retrieval-module`.
    This outputs the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下CLI命令调用上面的代码：`poetry poe call-rag-retrieval-module`。这将输出以下内容：
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can observe in the output above, along with the retrieved content, we
    have access to all kinds of metadata, such as the embedding model used for retrieval
    or the link from which the chunk was taken. These can quickly be added to a list
    of references when generating the result for the user, increasing trust in the
    final results.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在输出中观察到的，除了检索到的内容外，我们还可以访问各种元数据，例如用于检索的嵌入模型或提取块的链接。这些可以快速添加到生成用户结果时的参考列表中，从而增加对最终结果的信任。
- en: Now that we understand how the retrieval module works, let’s take a final step
    and examine the end-to-end RAG inference pipeline.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了检索模块的工作原理，让我们再迈出最后一步，检查端到端的 RAG 推理管道。
- en: Bringing everything together into the RAG inference pipeline
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合到 RAG 推理管道中
- en: To fully implement the RAG flow, we still have to build the prompt using the
    context from the retrieval model and call the LLM to generate the answer. This
    section will discuss these two steps and wrap everything together into a single
    `rag()` function. The functions from this section can be accessed on GitHub at
    [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api.py).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全实现 RAG 流程，我们仍然需要使用检索模型中的上下文构建提示，并调用 LLM 生成答案。本节将讨论这两个步骤，并将所有内容整合到一个单一的 `rag()`
    函数中。本节中的函数可以在 GitHub 上访问：[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api.py)。
- en: 'Let’s start by looking at the `call_llm_service()`function, responsible for
    interfacing with the LLM service. It takes in a user’s query and an optional context,
    sets up the language model endpoint, executes the inference, and returns the generated
    answer. The context is optional; you can call the LLM without it, as you would
    when interacting with any other LLM:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看 `call_llm_service()` 函数开始，该函数负责与 LLM 服务进行接口交互。它接收用户的查询和一个可选的上下文，设置语言模型端点，执行推理，并返回生成的答案。上下文是可选的；你可以不使用它来调用
    LLM，就像与其他任何 LLM 交互时一样：
- en: '[PRE42]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This function makes an HTTP request to our fine-tuned LLM Twin model, which
    is hosted as an AWS SageMaker inference endpoint. We will explore all the SageMaker
    details in the next chapter, where we will dig into the `LLMInferenceSagemakerEndpoint`
    and `InferenceExecutor` classes. For now, what is essential to know is that we
    use this function to call our fine-tuned LLM. Still, we must highlight how the
    query and context, passed to the `InferenceExecutor` class, are transformed into
    the final prompt. We do that using a simple prompt template that is customized
    using the user query and retrieved context:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数向我们的微调 LLM Twin 模型发送 HTTP 请求，该模型托管在 AWS SageMaker 推理端点上。我们将在下一章中探索所有 SageMaker
    的细节，我们将深入研究 `LLMInferenceSagemakerEndpoint` 和 `InferenceExecutor` 类。目前，我们需要知道的是，我们使用此函数调用我们的微调
    LLM。然而，我们必须强调查询和上下文是如何传递给 `InferenceExecutor` 类的，并且如何转换成最终的提示。我们通过使用一个简单的提示模板来完成这个任务，该模板使用用户查询和检索到的上下文进行定制：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Moving on to the `rag()` function, this is where the RAG logic comes together.
    It handles retrieving relevant documents based on the query, mapping the documents
    to the context that will be injected into the prompt, and obtaining the final
    answer from the LLM:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看 `rag()` 函数，这是 RAG 逻辑汇聚的地方。它负责根据查询检索相关文档，将文档映射到将要注入提示的上下文中，并从 LLM 获取最终答案：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As we modularized all the RAG steps into independent classes, we reduced the
    high-level `rag()` function to five lines of code (encapsulating all the complexities
    of the system) similar to what we see in tools such as LangChain, LlamaIndex,
    or Haystack. Instead of their high-level implementation, we learned how to build
    an advanced RAG service from scratch. Also, by clearly separating the responsibility
    of each class, we can use them like LEGOs. Thus, you can quickly call the LLM
    independently without context or use the retrieval module as a query engine on
    top of your vector DB. In the next chapter, we will see the `rag()` function in
    action after we deploy our fine-tuned LLM to an AWS SageMaker inference endpoint.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将所有 RAG 步骤模块化为独立的类，我们将高级的 `rag()` 函数简化为五行代码（封装了系统的所有复杂性），类似于我们在 LangChain、LlamaIndex
    或 Haystack 等工具中看到的那样。我们不是使用它们的高级实现，而是学习了如何从头开始构建高级 RAG 服务。此外，通过明确分离每个类的责任，我们可以像乐高积木一样使用它们。因此，你可以快速独立调用
    LLM 而无需上下文，或者将检索模块用作你向量数据库上的查询引擎。在下一章中，我们将部署我们的微调 LLM 到 AWS SageMaker 推理端点后，看到
    `rag()` 函数的实际应用。
- en: Before ending this chapter, we want to discuss potential improvements you could
    add to the RAG inference pipeline. As we are building a chatbot, the first one
    is to add a conversation memory that stores all the user prompts and generated
    answers in memory. Thus, when interacting with the chatbot, it will be aware of
    the whole conversation, not only the latest prompt. When prompting the LLM, along
    with the new user input and context, we also pass the conversation history from
    the memory. As the conversation history can get long, to avoid exceeding the context
    window or higher costs, you have to implement a way to reduce the size of your
    memory. As illustrated in *Figure 9.4*, the simplest one is to keep only the latest
    K items from your chat history. Unfortunately, using this strategy, the LLM will
    never be aware of the whole conversation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我们想讨论您可以添加到RAG推理管道中的潜在改进。由于我们正在构建聊天机器人，第一个改进是添加一个对话记忆，它将存储所有用户提示和生成的答案。因此，当与聊天机器人交互时，它将意识到整个对话，而不仅仅是最新的提示。当提示LLM时，除了新的用户输入和上下文外，我们还传递内存中的对话历史。由于对话历史可能会很长，为了避免超出上下文窗口或更高的成本，你必须实现一种方法来减少你内存的大小。如图9.4所示，最简单的方法是只保留聊天历史中的最新K个项目。不幸的是，使用这种策略，LLM将永远不会意识到整个对话。
- en: Therefore, another way to add the chat history to your prompt is to keep a summary
    of the conversation along with the latest K replies. There are multiple ways to
    compute this summary, which might defeat the purpose of this book if we get into
    them all, but the simplest way is to always update the summary on every user prompt
    and generate an answer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将聊天历史添加到提示的另一种方法是保留对话的摘要以及最新的K个回复。计算这个摘要有多种方法，如果我们详细讨论它们，可能会违背本书的目的，但最简单的方法是在每次用户提示时始终更新摘要并生成答案。
- en: '![](img/B31105_09_04.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_09_04.png)'
- en: 'Figure 9.4: Routing and memory examples'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：路由和内存示例
- en: As for each search, we send three queries to the vector DB, one for each data
    category. Thus, the second improvement is to add a router between the query and
    the search. The router will be a multi-category classifier that predicts the data
    categories we must retrieve for that specific query. Hence, instead of making
    three requests for every search, we can often reduce it to one or two. For example,
    if the user wants to write a theoretical paragraph about RAG for an article, then
    most probably, it’s valuable to query only the article’s collection. In this case,
    the router will predict the article class, which we can use to decide what collection
    we must query.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次搜索，我们向向量数据库发送三个查询，每个数据类别一个。因此，第二个改进是在查询和搜索之间添加一个路由器。该路由器将是一个多类别分类器，它将预测我们必须为该特定查询检索的数据类别。因此，我们不必为每次搜索发出三个请求，通常可以减少到一或两个。例如，如果用户想为一篇文章撰写关于RAG的理论段落，那么查询文章集合可能非常有价值。在这种情况下，路由器将预测文章类别，我们可以用它来决定我们必须查询哪个集合。
- en: Another example would be if we want to illustrate a piece of code that shows
    how to build a RAG pipeline. In this case, the router would have to predict the
    article and repository data category, as we need to look up examples in both collections
    for an exhaustive context.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，如果我们想展示一段代码，说明如何构建RAG管道。在这种情况下，路由器必须预测文章和存储库数据类别，因为我们需要在两个集合中查找示例以获得详尽的环境。
- en: Usually, the router strategy decides what model to call based on a user’s input,
    such as whether to use GPT-4 or a self-hosted Llama 3.1 model for that specific
    query. However, in our particular use case, we can adapt the router algorithm
    to optimize the retrieval step.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，路由策略根据用户的输入决定调用哪个模型，例如是否为特定查询使用GPT-4或自托管Llama 3.1模型。然而，在我们的特定用例中，我们可以调整路由算法以优化检索步骤。
- en: 'We can further optimize the retrieval using a hybrid search algorithm that
    combines the vector search (based on embeddings) with a keyword search algorithm,
    such as BM25\. Search algorithms used BM25 (or similar methods) to find similar
    items in a DB before vector search algorithms became popular. By merging the methods,
    hybrid search retrieves results that match the exact terms, such as RAG, LLM,
    or SageMaker, and the query semantics, increasing the accuracy and relevance of
    your retrieved results. Fundamentally, the hybrid search algorithms follow the
    next mechanics:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用一种结合了基于嵌入的向量搜索和关键字搜索算法（如BM25）的混合搜索算法来进一步优化检索。在向量搜索算法变得流行之前，搜索算法使用BM25（或类似方法）在数据库中找到相似项。通过合并这些方法，混合搜索可以检索到匹配精确术语的结果，例如RAG、LLM或SageMaker，以及查询语义，从而提高检索结果的准确性和相关性。从根本上讲，混合搜索算法遵循以下机制：
- en: '**Parallel processing**: The search query is processed simultaneously through
    both the vector search and BM25 algorithms. Each algorithm retrieves a set of
    relevant documents based on its criteria.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**并行处理**：搜索查询同时通过向量搜索和BM25算法进行处理。每个算法根据其标准检索一组相关文档。'
- en: '**Score normalization**: The results from both searches are assigned relevance
    scores, which are then normalized to ensure comparability. This step is crucial
    because vector search and BM25 scoring mechanisms work at different scales. Thus,
    they can’t be compared or merged without normalization.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分数归一化**：对两次搜索的结果分配相关性分数，然后对这些分数进行归一化以确保可比性。这一步至关重要，因为向量搜索和BM25评分机制在不同的尺度上工作。因此，没有归一化，它们无法进行比较或合并。'
- en: '**Result merging**: The normalized scores are combined, often through a weighted
    sum, to produce a final ranking of documents. Adjusting the weights allows for
    fine-tuning the emphasis on the semantic or keyword search algorithm.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结果合并**：归一化后的分数通过加权求和等方式合并，以产生文档的最终排名。调整权重允许对语义或关键字搜索算法的强调进行微调。'
- en: To conclude, by combining the semantic and exact keyword search algorithms,
    you can improve the accuracy of your retrieval step. Vector search helps recognize
    synonyms or related concepts, ensuring that relevant information isn’t overlooked
    due to vocabulary differences. Keyword search ensures that documents containing
    critical keywords are emphasized appropriately, particularly in technical fields
    with specific terminology.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，通过结合语义和精确关键字搜索算法，你可以提高检索步骤的准确性。向量搜索有助于识别同义词或相关概念，确保由于词汇差异而不会遗漏相关信息。关键字搜索确保包含关键字的文档得到适当的强调，尤其是在具有特定术语的技术领域。
- en: One last improvement we can make to our RAG system is to use multi-index vector
    structures instead of indexing based only on the content’s embedding. Let’s detail
    how multi-indexing works. Instead of using the embeddings of a single field to
    do the vector search for a particular collection, it combines multiple fields.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对我们的RAG系统进行最后一次改进，即使用多索引向量结构而不是仅基于内容嵌入进行索引。让我们详细说明多索引是如何工作的。不是使用单个字段的嵌入来对特定集合进行向量搜索，而是结合多个字段。
- en: 'For example, in our LLM Twin use case, we used only the content field of our
    articles, posts, or repositories to query the vector DB. When using a multi-index
    strategy, along with the content field, we could index the embeddings of the platform
    where the content was posted or when the content was published. This could impact
    the final accuracy of your retrieval as different platforms have different types
    of content, or more recent content is usually more relevant. Frameworks such as
    Superlinked make multi-indexing easy. For example, in the code snippet below,
    using Superlinked, we defined a multi-index on the content and platform for our
    article collection in just a few lines of code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的LLM Twin用例中，我们只使用了我们文章、帖子或存储库的内容字段来查询向量数据库。当使用多索引策略时，除了内容字段外，我们还可以索引内容发布平台或内容发布时的嵌入。这可能会影响检索的最终准确性，因为不同的平台有不同的内容类型，或者更近期的内容通常更相关。例如，Superlinked等框架使多索引变得简单。例如，在下面的代码片段中，使用Superlinked，我们只需几行代码就在我们的文章集合的内容和平台上定义了一个多索引：
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Superlinked is a powerful Python tool for any use case that includes vector
    computing, such as RAG, recommender systems, and semantic search. It offers an
    ecosystem where you can quickly ingest data into a vector DB, write complex queries
    on top of it, and deploy the service as a RESTful API.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Superlinked是一个强大的Python工具，适用于包括向量计算在内的任何用例，如RAG、推荐系统和语义搜索。它提供了一个生态系统，您可以将数据快速导入到向量数据库中，在其上编写复杂的查询，并将服务作为RESTful
    API部署。
- en: The world of LLMs and RAG is experimental, similar to any other AI domain. Thus,
    when building real-world products, it’s important to quickly build an end-to-end
    solution that works but is not necessarily the best. Then, you can reiterate with
    various experiments until you completely optimize it for your use case. This is
    standard practice in the industry and lets you iterate fast while providing value
    to the business and gathering user feedback as quickly as possible in the product’s
    lifecycle.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: LLM和RAG的世界是实验性的，类似于任何其他AI领域。因此，在构建现实世界产品时，重要的是快速构建一个工作但未必是最好的端到端解决方案。然后，您可以通过各种实验进行迭代，直到完全优化以适应您的用例。这是行业中的标准做法，让您能够快速迭代，同时为业务提供价值，并在产品的生命周期中尽可能快地收集用户反馈。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter taught us how to build an advanced RAG inference pipeline. We started
    by looking into the software architecture of the RAG system. Then, we zoomed in
    on the advanced RAG methods we used within the retrieval module, such as query
    expansion, self-querying, filtered vector search, and reranking. Afterward, we
    saw how to write a modular `ContextRetriever` class that glues all the advanced
    RAG components under a single interface, making searching for relevant documents
    a breeze. Ultimately, we looked into how to connect all the missing dots, such
    as the retrieval, the prompt augmentation, and the LLM call, under a single RAG
    function that will serve as our RAG inference pipeline.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本章教我们如何构建高级RAG推理管道。我们首先研究了RAG系统的软件架构。然后，我们聚焦于检索模块中使用的先进RAG方法，例如查询扩展、自我查询、过滤向量搜索和重新排序。之后，我们看到了如何编写一个模块化的`ContextRetriever`类，它将所有高级RAG组件粘合在一个单一接口下，使得搜索相关文档变得轻而易举。最终，我们探讨了如何将所有缺失的环节，如检索、提示增强和LLM调用，连接到一个单一的RAG函数，该函数将作为我们的RAG推理管道。
- en: As highlighted a few times in this chapter, we couldn’t test our fine-tuned
    LLM because we haven’t deployed it yet to AWS SageMaker as an inference endpoint.
    Thus, in the next chapter, we will learn how to deploy the LLM to AWS SageMaker,
    write an inference interface to call the endpoint, and implement a FastAPI web
    server to serve as our business layer.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章多次强调，我们无法测试我们微调的LLM，因为我们还没有将其部署到AWS SageMaker作为推理端点。因此，在下一章中，我们将学习如何将LLM部署到AWS
    SageMaker，编写一个调用端点的推理接口，并实现一个FastAPI网络服务器作为我们的业务层。
- en: References
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*A real-time retrieval system for social media data | VectorHub by SuperLinked*.
    (n.d.). [https://superlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-data](https://superlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-data)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社交媒体数据的实时检索系统 | SuperLinked的VectorHub*。（未注明日期）。[https://superlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-data](https://superlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-data)'
- en: '*Building a Router from Scratch - LlamaIndex*. (n.d.). [https://docs.llamaindex.ai/en/stable/examples/low_level/router/](https://docs.llamaindex.ai/en/stable/examples/low_level/router/)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从头开始构建路由器 - LlamaIndex*。（未注明日期）。[https://docs.llamaindex.ai/en/stable/examples/low_level/router/](https://docs.llamaindex.ai/en/stable/examples/low_level/router/)'
- en: '*How to add memory to chatbots | LangChain*. (n.d.). [https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory](https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory
    )'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何为聊天机器人添加内存 | LangChain*。（未注明日期）。[https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory](https://python.langchain.com/docs/how_to/chatbots_memory/#summary-memory)'
- en: '*How to do “self-querying” retrieval | LangChain*. (n.d.). [https://python.langchain.com/docs/how_to/self_query/](https://python.langchain.com/docs/how_to/self_query/)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何进行“自我查询”检索 | LangChain*。（未注明日期）。[https://python.langchain.com/docs/how_to/self_query/](https://python.langchain.com/docs/how_to/self_query/)'
- en: '*How to route between sub-chains | LangChain*. (n.d.). [https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity](https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何在子链之间进行路由 | LangChain*。（未注明日期）。[https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity](https://python.langchain.com/docs/how_to/routing/#routing-by-semantic-similarity)'
- en: '*How to use the MultiQueryRetriever | LangChain*. (n.d.). [https://python.langchain.com/docs/how_to/MultiQueryRetriever/](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何使用 MultiQueryRetriever | LangChain*。（n.d.）。[https://python.langchain.com/docs/how_to/MultiQueryRetriever/](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)'
- en: '*Hybrid Search explained*. (2023, January 3). Weaviate. [https://weaviate.io/blog/hybrid-search-explained](https://weaviate.io/blog/hybrid-search-explained)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合搜索解释*。（2023，1月3日）。Weaviate。[https://weaviate.io/blog/hybrid-search-explained](https://weaviate.io/blog/hybrid-search-explained)'
- en: Iusztin, P. (2024, August 20). 4 Advanced RAG Algorithms You Must Know | Decoding
    ML. *Medium*. [https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2](https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iusztin, P.（2024，8月20日）。你必须知道的 4 个高级 RAG 算法 | 解码机器学习。*Medium*。[https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2](https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2)
- en: 'Monigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation:
    From Theory to LlamaIndex Implementation. *Medium*. [https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monigatti, L.（2024，2月19日）。高级检索增强生成：从理论到 LlamaIndex 实现。*Medium*。[https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930)
- en: '*Multi-attribute search with vector embeddings | VectorHub by Superlinked*.
    (n.d.). [https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search](https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用向量嵌入进行多属性搜索 | Superlinked 的 VectorHub*。（n.d.）。[https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search](https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search)'
- en: '*Optimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked*.
    (n.d.). [https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用混合搜索和重排序优化 RAG | Superlinked 的 VectorHub*。（n.d.）。[https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)'
- en: Refactoring.Guru. (2024, January 1). *Singleton*. [https://refactoring.guru/design-patterns/singleton](https://refactoring.guru/design-patterns/singleton)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refactoring.Guru.（2024，1月1日）。*Singleton*。[https://refactoring.guru/design-patterns/singleton](https://refactoring.guru/design-patterns/singleton)
- en: Stoll, M. (2024, September 7). Visualize your RAG Data—Evaluate your Retrieval-Augmented
    Generation System with Ragas. *Medium*. [https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557](https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stoll, M.（2024，9月7日）。可视化你的 RAG 数据——使用 Ragas 评估你的检索增强生成系统。*Medium*。[https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557](https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557)
- en: '*Using LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM
    applications*. (n.d.). [https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6](https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 LLM 进行检索和重排序——LlamaIndex，LLM 应用数据框架*。（n.d.）。[https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6](https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6)'
- en: Join our book’s Discord space
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code79969828252392890.png)'
