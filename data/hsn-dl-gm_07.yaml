- en: Introducing DRL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DRL
- en: '**Deep reinforcement learning** (**DRL**) is currently taking the world by
    storm and is seen as the "it" of machine learning technologies, the it goal of
    reaching some form of general AI. Perhaps it is because DRL approaches the cusp
    of general AI or what we perceive as general intelligence. It is also likely to
    be one of the main reasons you are reading this book. Fortunately, this chapter,
    and the majority of the rest of the book, focuses deeply on **reinforcement learning**
    (**RL**) and its many variations. In this chapter, we start learning the basics
    of RL and how it can be adapted to **deep learning** (**DL**). We will explore
    the **OpenAI Gym** environment, a great RL playground, and see how to use it with
    some simple DRL techniques.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度强化学习**（**DRL**）目前正席卷全球，被视为机器学习技术中的“热点”，即达到某种形式的通用人工智能的目标。也许正是因为DRL接近了通用人工智能的边界，或者是我们所理解的通用智能。它也可能是你正在阅读本书的主要原因之一。幸运的是，本章以及本书大部分内容，都将深入探讨**强化学习**（**RL**）及其许多变体。在本章中，我们将开始学习RL的基础知识，以及它如何与**深度学习**（**DL**）相结合。我们还将探索**OpenAI
    Gym**环境，这是一个很棒的RL实验场，并学习如何利用一些简单的DRL技术进行实践。'
- en: Keep in mind, this is a hands-on book, so we will be keeping technical theory
    to a minimum, and instead we will explore plenty of working examples. Some readers
    may feel lost without the theoretical background and feel the need to explore
    the more theoretical side of RL on their own.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这本书是一本实践性很强的书，因此我们将尽量减少技术理论的讨论，而是通过大量的工作实例进行探索。一些读者可能会觉得没有理论背景会感到迷茫，并且希望自己去深入探索RL的理论部分。
- en: For other readers not familiar with the theoretical background of RL, we will
    cover several core concepts, but this is the abridged version, so it is recommended
    you seek theoretical knowledge from other sources when you are ready.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不熟悉RL理论背景的读者，我们将介绍几个核心概念，但这是简略版，因此建议您在准备好之后从其他来源学习更多的理论知识。
- en: 'In this chapter, we will start learning about DRL, a topic that will carry
    through to many chapters. We will start with the basics and then look to explore
    some working examples adapted to DL. Here is what we will cover in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始学习DRL，这一主题将贯穿多个章节。我们将从基础开始，然后探索一些适应于DL的工作示例。以下是本章内容：
- en: Reinforcement learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: The Q-learning model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习模型
- en: Running the OpenAI gym
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行OpenAI gym
- en: The first DRL with Deep Q-Network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个深度Q网络（DQN）与DRL
- en: RL experiments
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL实验
- en: 'For those of you who like to jump around books: yes, it is OK to start this
    book from this chapter. However, you may need to go back to previous chapters
    in order to complete some exercises. We will also assume that your Python environment
    is configured with TensorFlow and Keras, but if you are unsure, check out the
    `requirements.txt` file in the project folder.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于喜欢跳跃阅读的读者：是的，从本章开始阅读本书是完全可以的。不过，您可能需要回到前几章来完成一些练习。我们还假设您的Python环境已经配置好TensorFlow和Keras，如果不确定，可以查看项目文件夹中的`requirements.txt`文件。
- en: All the projects in this book are built with Visual Studio 2017 (Python), and
    it is the recommended editor for the examples in this book. If you use VS 2017
    with Python, you can easily manage the samples by opening the chapter solution
    file. Of course, there are plenty of other excellent Python editors and tools,
    so use what you are comfortable with.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有项目都是在Visual Studio 2017（Python）中构建的，这是本书示例推荐的编辑器。如果您使用VS 2017与Python，您可以通过打开章节解决方案文件轻松管理示例。当然，也有许多其他优秀的Python编辑器和工具，您可以使用自己习惯的工具。
- en: Reinforcement learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: RL currently leads the pack in advances compared to other machine learning methodologies.
    Note the use of the word *methodology* and not *technology*. RL is a methodology
    or algorithm that applies a principle we can use with neural networks, whereas,
    neural networks are a machine learning technology that can be applied to several
    methodologies. Previously, we looked at other methodologies that blended with
    DL, but we focused more on the actual implementation. However, RL introduces a
    new methodology that requires us to understand more of the inner and outer workings
    before we understand how to apply it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于其他机器学习方法，RL目前在进展方面领先于其他技术。注意，这里使用的是“*方法论*”而非“*技术*”这个词。RL是一种方法论或算法，它应用了一个我们可以与神经网络一起使用的原理，而神经网络是可以应用于多种方法论的机器学习技术。之前，我们讨论了与DL结合的其他方法论，但我们更专注于实际的实现。然而，RL引入了一种新的方法论，它要求我们在理解如何应用之前，先了解更多的内在和外在机制。
- en: RL was popularized by Richard Sutton, a Canadian, and current professor at the
    University of Alberta. Sutton has also assisted in the development of RL at Google's
    DeepMind, and is quite often regarded as the father of RL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）由加拿大人理查德·萨顿（Richard Sutton）普及，他是阿尔伯塔大学的教授。萨顿还参与了Google DeepMind的RL开发，并且经常被视为RL的奠基人。
- en: 'At the heart of any machine learning system is the need for training. Often,
    the AI agent/brain knows nothing, and then we feed it data through some automated
    process for it to learn. As we have seen, the most common way of doing this is
    called **supervised training**. This is when we first label our training data.
    We have also looked at **unsupervised training**, where our **Generative Adversarial
    Networks** (**GANs**) were trained by competing against each other. However, neither
    system replicated the type of learning or training we see in **Biology**, and
    that is often referred to as **rewards** or RL: the type of learning that lets
    you teach your dog to bark for a treat, fetch the paper, and use the outdoors
    for nature''s calling, a type of learning that lets an agent explore its own environment
    and learn for itself. This is not unlike the type of learning a general AI would
    be expected to use; after all, RL is likely similar to the system we use, or so
    we believe.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习系统的核心都需要训练。通常，AI代理/大脑一开始什么也不知道，我们通过某种自动化过程向其输入数据让它学习。正如我们所见，最常见的做法是**监督训练**。这是指我们首先给训练数据打上标签。我们还研究了**无监督训练**，在这种情况下，**生成对抗网络**（**GANs**）通过相互对抗进行训练。然而，这两种方法都没有复制我们在**生物学**中看到的那种学习或训练方式，这种方式通常被称为**奖励**或强化学习（RL）：一种让你教会狗狗为零食叫唤、捡报纸并在户外排泄的学习方式，一种让代理探索自己的环境并自我学习的方式。这与我们期望通用AI所使用的学习方式并无太大不同；毕竟，RL可能与我们所使用的系统类似，或者说我们是这么认为的。
- en: David Silver, a former student of Prof Sutton's and now head of DeepMind, has
    an excellent video series on the theoretical background of RL. The first five
    videos are quite interesting and recommended viewing, but the later content gets
    quite deep and may not be for everyone. Here's the link for the videos: [https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·西尔弗（David Silver），前萨顿教授的学生，现在是DeepMind的负责人，拥有一系列关于强化学习（RL）理论背景的优秀视频。前五个视频非常有趣，推荐观看，但后续内容较为深奥，可能并不适合所有人。以下是视频链接：[https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)
- en: 'RL defines its own type of training called by the same name. This form of reward-based
    training is shown in the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习定义了一种名为自身的训练方式。这种基于奖励的训练在下图中显示：
- en: '![](img/9b176d0d-9b93-423d-9d0c-cb1998759379.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b176d0d-9b93-423d-9d0c-cb1998759379.png)'
- en: Reinforcement learning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: The diagram shows an agent in an environment. That agent reads the state of
    the environment and then decides and performs an action. This action may, or may
    not, give a reward, and that reward could be good or bad. After each action and
    possible reward, the agent collects the state of the environment again. The process
    repeats itself until the agent reaches a terminal or end state. That is, until
    it reaches the goal; perhaps it dies or just gets tired. It is important to note
    a couple of subtle things about the preceding diagram. First, the agent doesn't
    always receive a reward, meaning rewards could be delayed, until some future goal
    is reached. This is quite different from the other forms of learning we explored
    earlier, which provided immediate feedback to our training networks. Rewards can
    be good or bad, and it is often just as effective to negatively train agents this
    way, but less so for humans.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图中展示了一个代理在一个环境中。该代理读取环境的状态，然后决定并执行一个动作。这个动作可能会或不会带来奖励，而奖励可能是好是坏。在每次动作和可能的奖励后，代理会再次收集环境的状态。这个过程会重复，直到代理达到一个**终结状态**。也就是说，直到它达到目标；也许它死掉，或者只是累了。需要注意的是，前图中有一些细微的地方。首先，代理并不总是会获得奖励，这意味着奖励可能会延迟，直到某个未来的目标达到。这与我们之前探索的其他学习方式不同，后者会立即给训练网络反馈。奖励可以是好是坏，而负面训练代理同样有效，但对人类来说则不太适用。
- en: Now, as you might expect with any powerful learning model, the mathematics can
    be quite complex and certainly daunting to the newcomer. We won't go too far into
    the theoretical details other than to describe some of the foundations of RL in
    the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如你所料，任何强大的学习模型，其数学可能会相当复杂，且对新手来说确实具有一定的挑战性。我们不会深入探讨理论细节，只会在下一节中描述一些强化学习的基础。
- en: The multi-armed bandit
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机
- en: The diagram we saw earlier describes the full RL problem as we will use for
    most of the rest of this book. However, we often teach a simpler one-step variation
    of this problem called the **multi-armed bandit**. The armed bandit is in reference
    to the Vegas slot machine and nothing more nefarious. We use these simpler scenarios
    in order to explain the basics of RL in the form of a one-step or one-state problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的图示描述了完整的强化学习（RL）问题，这也是我们在本书后续大部分内容中使用的模型。然而，我们经常讲授一个简化的单步变体问题，称为**多臂老虎机**。多臂老虎机这个名称源于拉斯维加斯的老虎机，而并非指某些其他不正当的含义。我们使用这些简化的场景来解释强化学习的基础知识，呈现为单步或单状态问题。
- en: 'In the case of the multi-armed bandit, picture a fictional multi-armed Vegas
    slot machine that awards different prizes based on which arm is pulled, but the
    prize for each arm is always the same. The agent''s goal in this scenario would
    be to figure out the correct arm to pull every time. We could further model this
    in an equation such as the one shown here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在多臂老虎机的情况下，可以想象一个虚构的多臂拉斯维加斯老虎机，根据拉动的臂不同奖励不同的奖品，但每个臂的奖品始终相同。智能体在这种情况下的目标是找出每次应拉动哪个臂。我们可以进一步将其建模为如下所示的方程：
- en: '![](img/cce0e6d0-2ea0-4ea0-b7e2-6e13a93eb5fb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cce0e6d0-2ea0-4ea0-b7e2-6e13a93eb5fb.png)'
- en: 'Consider the following equation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下方程：
- en: '![](img/cfabb87d-9a06-4b96-8661-72ef51f3bb0a.png) = vector of values (1,2,3,4)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/cfabb87d-9a06-4b96-8661-72ef51f3bb0a.png) = 值的向量（1,2,3,4）'
- en: '![](img/77c8a845-a682-4c89-b796-9055f0bd6b1f.png) = action'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/77c8a845-a682-4c89-b796-9055f0bd6b1f.png) = 行动'
- en: '![](img/f7eea114-134d-4145-b713-94d4fb0ba75e.png) = alpha = learning rate'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/f7eea114-134d-4145-b713-94d4fb0ba75e.png) = α = 学习率'
- en: '![](img/728ee055-bd4e-4791-be8b-c26fb766e47a.png) = reward'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/728ee055-bd4e-4791-be8b-c26fb766e47a.png) = 奖励'
- en: 'This equation calculates the value (*V*), a vector, for each action the agent
    takes. Then, it feeds back these values into itself, subtracted from the reward
    and multiplied by a learning rate. This calculated value can be used to determine
    which arm to pull, but first the agent needs to pull each arm at least once. Let''s
    quickly model this in code, so as game/simulation programmers, we can see how
    this works. Open the `Chapter_5_1.py` code and follow these steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程计算智能体所采取每个行动的值（*V*），这是一个向量。然后，它将这些值反馈到自身，从奖励中减去并乘以学习率。计算得出的值可以用来决定拉动哪个臂，但首先智能体需要至少拉动每个臂一次。让我们快速在代码中建模这一过程，作为游戏/仿真程序员，我们可以看到它是如何工作的。打开
    `Chapter_5_1.py` 代码并按照以下步骤操作：
- en: 'The code for this exercise is as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本练习的代码如下：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code creates the required setup variables, the `arms` (`gold`, `silver`,
    and `bronze`), and the value vector `v` (all zeros). Then, the code loops through
    a number of iterations (`10`) where each arm is pulled and the value, `v`, is
    calculated and updated based on the equation. Note that the reward value is replaced
    by the value of the arm pull, which is the term `arms[a][1]`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码创建了所需的设置变量，`arms`（`gold`、`silver` 和 `bronze`）以及值向量 `v`（全为零）。然后，代码会进行多次迭代（`10`次），每次都拉动一个臂并根据方程计算并更新值
    `v`。请注意，奖励值被臂拉动的值所替代，即项 `arms[a][1]`。
- en: Run the example, and you will see the output generated showing the value for
    each action, or in this case an arm pull.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例，你将看到生成的输出，显示每个行动的值，或者在这种情况下是一个臂的拉动。
- en: As we saw, with a simple equation, we were able to model the multi-armed bandit
    problem and arrive at a solution that will allow an agent to consistently pull
    the correct arm. This sets the foundation for RL, and in the next section, we
    take the next step and look at **contextual bandits**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，通过一个简单的方程式，我们能够建模多臂老虎机问题，并得出一个解决方案，使得智能体能够持续地选择正确的臂。这为强化学习奠定了基础，在接下来的章节中，我们将迈出下一步，探讨**上下文老虎机**。
- en: Contextual bandits
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文老虎机
- en: 'We can now elevate the single multi-armed bandit problem into a problem with
    multiple multi-armed bandits, each with its own set of arms. Now our problem introduces
    context or state into the equation. With each bandit defining its own context/state,
    now we evaluate our equation in terms of quality and action. Our modified equation
    is shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将单个多臂老虎机问题提升为包含多个多臂老虎机的问题，每个老虎机都有自己的一组臂。现在，我们的问题引入了上下文或状态到方程中。随着每个老虎机定义自己的上下文/状态，我们现在在质量和行动的角度来评估我们的方程。我们修改后的方程如下所示：
- en: '![](img/c461e128-42f5-4362-9277-a839f0c1589d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c461e128-42f5-4362-9277-a839f0c1589d.png)'
- en: 'Consider the following equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下方程：
- en: '![](img/259fdfe3-f4e2-4625-8dc9-c073c9051569.png) = table/matrix of values'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/259fdfe3-f4e2-4625-8dc9-c073c9051569.png) = 值的表格/矩阵'
- en: '[1,2,3,4'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,2,3,4'
- en: 2,3,4,5
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 2,3,4,5
- en: 4,2,1,4]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 4,2,1,4]
- en: '![](img/276ce14a-f047-46d2-afcb-e45b7a3caa43.png) = state'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/276ce14a-f047-46d2-afcb-e45b7a3caa43.png) = 状态'
- en: '![](img/dea129da-958b-4492-8437-591af763c9b2.png) = action'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/dea129da-958b-4492-8437-591af763c9b2.png) = 动作'
- en: '![](img/ca094865-f756-450b-b7c2-9ebc383af35f.png) = alpha = learning rate'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/ca094865-f756-450b-b7c2-9ebc383af35f.png) = alpha = 学习率'
- en: '![](img/3083521e-5c5f-4684-bc1c-2453b810c4c3.png) = reward'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/3083521e-5c5f-4684-bc1c-2453b810c4c3.png) = 回报'
- en: 'Let''s open up `Chapter_5_2.py` and observe the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开`Chapter_5_2.py`并观察以下步骤：
- en: 'Open the code up, as follows, and follow the changes made from the previous
    sample:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开代码，如下所示，并按照与之前示例所做的更改进行操作：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code sets up a number of multi-armed bandits, each with its own set of
    arms. It then iterates through a number of iterations, but this time as it loops,
    it also loops through each bandit. During each loop, it picks a random arm to
    pull and evaluates the quality.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码设置了多个多臂老虎机，每个都有一组臂。然后，它会进行多次迭代，但这次在每次循环时，它还会循环遍历每个老虎机。在每次循环中，它会随机选择一个臂来拉动，并评估其质量。
- en: Run the sample and look at the output of `q`. Note how, even after selecting
    random arms, the equation again consistently selected the gold arm, the arm with
    the highest reward, to pull.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例并查看`q`的输出。注意，即使在选择随机臂后，方程依然持续选择金臂，即回报最高的臂来拉动。
- en: Feel free to play around with this sample some more and look to the exercises
    for additional inspiration. We will expand on the complexity of our RL problems
    when we discuss Q-Learning. However, before we get to that section, we will take
    a quick diversion and look at setting up the OpenAI Gym in order to conduct more
    RL experiments.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 随意多玩这个示例，并查看练习以获取更多灵感。当我们讨论Q-Learning时，我们将扩展我们的RL问题的复杂度。然而，在进入那个部分之前，我们将稍作偏离，看看如何设置OpenAI
    Gym，以便进行更多的RL实验。
- en: RL with the OpenAI Gym
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAI Gym进行强化学习（RL）
- en: RL has become so popular that there is now a race to just build tools that help
    build RL algorithms. The two major competitors in this area right now are **OpenAI
    Gym** and **Unity**. Unity has quickly become the RL racing machine we will explore
    extensively later. For now, we will put our training wheels on and run OpenAI
    Gym to explore the fundamentals of RL further.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）已经变得非常流行，现在有一场竞赛，专门开发帮助构建RL算法的工具。目前，这个领域的两个主要竞争者是**OpenAI Gym**和**Unity**。Unity迅速成为了我们稍后将深入探索的RL竞速机器。现在，我们将启用辅助功能，使用OpenAI
    Gym来进一步探索RL的基本原理。
- en: 'We need to install the OpenAI Gym toolkit before we can continue, and installation
    may vary greatly depending on your operating system. As such, we will focus on
    the Windows installation instructions here, as it is likely other OS users will
    have less difficulty. Follow the next steps to install OpenAI Gym on Windows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装OpenAI Gym工具包才能继续，安装方法可能因操作系统的不同而有所差异。因此，我们将在这里专注于Windows的安装说明，其他操作系统用户可能会遇到较少的困难。按照以下步骤在Windows上安装OpenAI
    Gym：
- en: Install a C++ compiler; if you have Visual Studio 2017 installed, you may already
    have a recommended one. You can find other supported compilers here: [https://wiki.python.org/moin/WindowsCompilers](https://wiki.python.org/moin/WindowsCompilers).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装一个C++编译器；如果你已经安装了Visual Studio 2017，可能已经有一个推荐的编译器了。你可以在这里找到其他支持的编译器：[https://wiki.python.org/moin/WindowsCompilers](https://wiki.python.org/moin/WindowsCompilers)。
- en: 'Be sure to have Anaconda installed, and open an Anaconda command prompt and
    run the following commands:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保已安装Anaconda，并打开Anaconda命令提示符，然后运行以下命令：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For our purposes, in the short term, we don't need to install any other Gym
    modules. Gym has plenty of example environments, Atari games and MuJoCo (robotics
    simulator) being some of the most fun to work with. We will take a look at the
    Atari games module later in this chapter.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就我们目前的目的而言，短期内不需要安装任何其他Gym模块。Gym有很多示例环境，其中Atari游戏和MuJoCo（机器人仿真器）是最有趣的几种。我们将在本章稍后查看Atari游戏模块。
- en: That should install the Gym environment for your system. Most of what we need
    will work with minimal setup. If you decide to do more with Gym, then you will
    likely want to install other modules; there are several. In the next section,
    we are going to test this new environment as we learn about Q-Learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会为你的系统安装Gym环境。我们需要的大多数功能将通过最小设置即可工作。如果你决定深入使用Gym，那么你可能需要安装其他模块；有很多模块。在下一节中，我们将测试这个新环境，边学习Q-Learning。
- en: A Q-Learning model
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个Q-Learning模型
- en: RL is deeply entwined with several mathematical and dynamic programming concepts
    that could fill a textbook, and indeed there are several. For our purposes, however,
    we just need to understand the key concepts in order to build our DRL agents.
    Therefore, we will choose not to get too burdened with the math, but there are
    a few key concepts that you will need to understand to be successful. If you covered
    the math in the [Chapter 1](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml), *Deep
    Learning for Games*, this section will be a breeze. For those that didn't, just
    take your time, but you can't miss this one.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: RL 深深地与几种数学和动态编程概念交织在一起，这些概念可以填满一本教科书，实际上也确实有几本类似的书。然而，对于我们的目的来说，我们只需要理解关键概念，以便构建我们的
    DRL 智能体。因此，我们将选择不去过多纠结于数学细节，但仍有一些关键概念是你需要理解的，才能取得成功。如果你在[第一章](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml)《深度学习与游戏》中覆盖了数学内容，那么这一部分将会非常轻松。对于那些没有覆盖的朋友，慢慢来，但这个部分你不能错过。
- en: In order to understand the Q-Learning model, which is a form of RL, we need
    to go back to the basics. In the next section, we talk about the importance of
    the **Markov decision process** and the **Bellman** e**quation**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 Q-Learning 模型，这是 RL 的一种形式，我们需要回到基础知识。在接下来的部分，我们将讨论 **马尔可夫决策过程** 和 **贝尔曼**
    方程的重要性。
- en: Markov decision process and the Bellman equation
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程和贝尔曼方程
- en: 'At the heart of RL is the **Markov decision process** (**MDP**). An MDP is
    often described as a discrete time stochastic control process. In simpler terms,
    this just means it is a control program that functions by time steps to determine
    the probability of actions, provided each action leads to a reward. This process
    is already used for most automation control of robotics, drones, networking, and
    of course RL. The classic way we picture this process is shown in the following
    diagram:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RL 的核心是 **马尔可夫决策过程** (**MDP**) 。一个 MDP 通常被描述为离散时间随机控制过程。简单来说，这意味着它是一个通过时间步来运作的控制程序，用于确定每个动作的概率，并且每个动作都会导致一个奖励。这个过程已经被广泛用于大多数机器人控制、无人机、网络控制，当然也包括
    RL。我们通常通过下面的图示来描述这个过程：
- en: '![](img/434ef2cd-a9d0-4c70-9cd8-e0cddd217734.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/434ef2cd-a9d0-4c70-9cd8-e0cddd217734.png)'
- en: The Markov decision process
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: 'Where represent an MDP as a tuple or vector ![](img/2f668c37-4a3c-4fb3-bea6-89ee1da65240.png),
    using the following variables:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个元组或向量来表示一个 MDP ![](img/2f668c37-4a3c-4fb3-bea6-89ee1da65240.png)，使用以下变量：
- en: '![](img/db442408-6913-47ba-8e98-f0dc1ac4d37e.png) - being a finite set of states,'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/db442408-6913-47ba-8e98-f0dc1ac4d37e.png) - 是一个有限的状态集合，'
- en: '![](img/fd0b1ca8-560b-4a27-baa6-603d0ce3229d.png) - being a finite set of actions,'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/fd0b1ca8-560b-4a27-baa6-603d0ce3229d.png) - 是一个有限的动作集合，'
- en: '![](img/02393b19-30c2-43a5-b8d6-8e27fee233c9.png) - the probability that action
    ![](img/bf5cc02b-c6f0-43ba-a6d4-bfd93a7b3364.png) in state ![](img/612e1b85-51ac-4943-bf9f-d3043356022d.png) at
    time ![](img/2e0c7122-44b2-490a-9541-7005d2b217bd.png) will lead to state ![](img/db1fa5b6-7a59-4f45-904c-928e1e782c1f.png) at
    time ![](img/ed7e16b3-966f-43cd-8e6b-2cbccf8ab57a.png),'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/02393b19-30c2-43a5-b8d6-8e27fee233c9.png) - 动作 ![](img/bf5cc02b-c6f0-43ba-a6d4-bfd93a7b3364.png)
    在状态 ![](img/612e1b85-51ac-4943-bf9f-d3043356022d.png) 时刻 ![](img/2e0c7122-44b2-490a-9541-7005d2b217bd.png)
    导致状态 ![](img/db1fa5b6-7a59-4f45-904c-928e1e782c1f.png) 在时刻 ![](img/ed7e16b3-966f-43cd-8e6b-2cbccf8ab57a.png)
    的概率，'
- en: '![](img/a964622c-26ef-4841-865b-40ccc8197365.png) - is the immediate reward'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/a964622c-26ef-4841-865b-40ccc8197365.png) - 是即时奖励'
- en: '![](img/aaa15109-a497-4e52-a731-a66acb40c59d.png) - gamma is a discount factor
    we apply in order to discount the significance or provide significance to future
    rewards'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/aaa15109-a497-4e52-a731-a66acb40c59d.png) - gamma 是一个折扣因子，用来折扣未来奖励的重要性或为未来奖励提供重要性'
- en: The diagram works by picturing yourself as an agent in one of the states. You
    then determine actions based on the probability, always taking a random action.
    As you move to the next state, the action gives you a reward and you update the
    probability based on the reward. Again, David Silver covers this piece very well
    in his lectures.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该图通过将你自己想象成一个处于某个状态的智能体来工作。然后你根据概率确定动作，总是采取一个随机动作。当你移动到下一个状态时，该动作会给你一个奖励，你根据这个奖励更新概率。同样，David
    Silver 在他的讲座中非常好地涵盖了这一部分内容。
- en: 'Now, the preceding process works, but another variation came along that provided
    for better future reward evaluation, and that was done by introducing the **Bellman
    Equation** and the concept of a policy/value iteration. Whereas before we had
    a value, ![](img/d62b7e72-e9f9-470c-aafa-95bf8254b13f.png), we now have a policy
    (![](img/8aedee8a-b54e-4b76-96ec-ebd39c12acc0.png)) for a value called ![](img/6ae870a0-2615-43ca-a224-1b08d456a58f.png), and
    this yields us a new equation, shown here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，前面的过程是有效的，但随后出现了另一种变体，通过引入 **Bellman 方程** 和策略/价值迭代的概念，提供了更好的未来奖励评估。之前我们有一个值，![](img/d62b7e72-e9f9-470c-aafa-95bf8254b13f.png)，现在我们有一个策略（![](img/8aedee8a-b54e-4b76-96ec-ebd39c12acc0.png)）来表示一个值（![](img/6ae870a0-2615-43ca-a224-1b08d456a58f.png)），这为我们提供了一个新的方程，如下所示：
- en: '![](img/16cb44c4-34b7-4914-b933-ce07125fabf0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16cb44c4-34b7-4914-b933-ce07125fabf0.png)'
- en: We won't cover much more about this equation other than to say to keep the concept
    of quality iteration in mind. In the next section, we will see how we can reduce
    this equation back to a quality indicator of each action and use that for Q-Learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会进一步讲解这个方程，只是提醒大家牢记质量迭代的概念。在接下来的部分中，我们将看到如何将这个方程简化为每个动作的质量指标，并将其用于 Q-Learning。
- en: Q-learning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: 'With the introduction of quality iteration methods, the derivation of a finite
    state method called **Q-learning** or **quality learning** was derived. Q uses
    the technique of quality iteration for a given finite state problem to determine
    the best course of action for an agent. The equation we saw in the previous section
    can now be represented as the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着质量迭代方法的引入，一个称为 **Q-learning** 或 **质量学习** 的有限状态方法被提出。Q 使用质量迭代技术来解决给定的有限状态问题，确定智能体的最佳行动。我们在前面看到的方程现在可以表示为以下形式：
- en: '![](img/649eaf34-9453-4328-9183-5f8437af651f.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/649eaf34-9453-4328-9183-5f8437af651f.png)'
- en: 'Consider the following equation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下方程：
- en: '![](img/74182c8a-795c-46a2-bd88-8679475b3ddd.png) current state'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/74182c8a-795c-46a2-bd88-8679475b3ddd.png) 当前状态'
- en: '![](img/a02d73e5-10d7-4937-b5bf-3b0d34f2807c.png) current action'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/a02d73e5-10d7-4937-b5bf-3b0d34f2807c.png) 当前动作'
- en: '![](img/8fe87e76-4098-487c-8033-e01ad9688d4b.png) next action'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8fe87e76-4098-487c-8033-e01ad9688d4b.png) 下一个动作'
- en: '![](img/1d73936e-4c41-408e-8249-949594662494.png) current reward'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/1d73936e-4c41-408e-8249-949594662494.png) 当前奖励'
- en: '![](img/6035c5ee-320c-4ab2-865c-5294f55549c2.png) learning rate (alpha)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/6035c5ee-320c-4ab2-865c-5294f55549c2.png) 学习率（alpha）'
- en: '![](img/57f56e14-a30f-4531-9da1-e5991077f44b.png) reward discount factor (gamma)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/57f56e14-a30f-4531-9da1-e5991077f44b.png) 奖励折扣因子（gamma）'
- en: 'The Q value is now updated alliteratively, as the agent roams through its environment.
    Nothing demonstrates these concepts better than an example. Open up `Chapter_5_3.py`
    and follow these steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Q 值现在在智能体穿越其环境时被迭代更新。没有什么比一个例子更能展示这些概念了。打开 `Chapter_5_3.py`，并按照以下步骤操作：
- en: 'We start with the various imports and set them up as shown in the following
    code:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从各种导入开始，并按照以下代码设置：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These imports just load the basic libraries we need for this example. Remember,
    you will need to install `Gym` to run this sample.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些导入仅加载了我们这个示例所需的基本库。记住，运行这个示例前，你需要安装 `Gym`。
- en: 'Next, we set up a new environment; in this example, we use the basic `FrozenLake-v0`
    sample, a perfect example to test on Q-learning:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置一个新环境；在这个例子中，我们使用基础的 `FrozenLake-v0` 示例，这是一个测试 Q-learning 的完美示例：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we set up the AI environment (`env`) and a number of other parameters:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们设置 AI 环境（`env`）以及其他一些参数：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this section of the code, we set up a number of variables that we will get
    to shortly. For this sample, we are using a wrapper tool to monitor the environment,
    and this is useful for determining any potential training issues. The other thing
    to note is the setup of the `q_table` array, defined by the environment `observation_space`
    (state) and `action_space` (action); spaces define arrays and not just vectors.
    In this particular example, the `action_space` is a vector, but it could be a
    multi-dimensional array or tensor.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这部分代码中，我们设置了若干变量，稍后会详细说明。在这个示例中，我们使用了一个包装工具来监控环境，这对于确定潜在的训练问题非常有用。另一个需要注意的是
    `q_table` 数组的设置，这个数组由环境的 `observation_space`（状态）和 `action_space`（动作）定义；空间定义的是数组而不仅仅是向量。在这个例子中，`action_space`
    是一个向量，但它也可以是一个多维数组或张量。
- en: 'Pass over the next section of functions and skip to the end, where the training
    iteration occurs and is shown in the following code:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过下一个函数部分，直接跳到最后一部分，在那里训练迭代发生并显示在以下代码中：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Most of the preceding code is relatively straightforward and should be easy
    to follow. Look at how the `env` (environment) is using the `action` generated
    from the `act` function; this is used to step or conduct an action on the agent.
    The output of the `step` function is `next_state`, `reward`, and `done`, which
    we use to determine the optimum Q policy by using the `learn` function.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码大部分都比较直接，应该容易理解。看看 `env`（环境）是如何使用从 `act` 函数生成的 `action`；这会用于执行智能体的步骤或动作。`step`
    函数的输出是 `next_state`，`reward` 和 `done`，我们用这些来通过 `learn` 函数来确定最佳的 Q 策略。
- en: Before we get into the action and learning functions, run the sample and watch
    how the agent trains. It may take a while to train, so feel free to return to
    the book.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们深入研究动作和学习函数之前，先运行样本，观察智能体如何进行训练。训练可能需要一些时间，所以可以随时返回书本继续阅读。
- en: 'The following is an example of the OpenAI Gym FrozenLake environment running
    our Q-learning model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 OpenAI Gym FrozenLake 环境运行我们 Q-learning 模型的示例：
- en: '![](img/c2cbf3e7-ae6a-4e0c-a5c8-3961aa6119c6.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2cbf3e7-ae6a-4e0c-a5c8-3961aa6119c6.png)'
- en: FrozenLake Gym environment
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: FrozenLake Gym 环境
- en: As the sample runs, you will see a simple text output showing the environment.
    `S` represents the start, `G` the goal, `F` a frozen section, and `H` a hole.
    The goal for the agent is to find its way through the environment, without falling
    in a hole, and reach the goal. Pay special attention to how the agent moves and
    finds it way around the environment. In the next section, we unravel the `learn`
    and `act` functions and understand the importance of exploration.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本运行时，你将看到一个简单的文本输出，显示环境的状态。`S` 代表起点，`G` 代表目标，`F` 代表冻结区域，`H` 代表陷阱。智能体的目标是找到一条通过环境的路径，避免掉入陷阱，并到达目标。特别注意智能体是如何移动的，如何在环境中找到自己的路径。在下一节中，我们将深入分析
    `learn` 和 `act` 函数，并理解探索的重要性。
- en: Q-learning and exploration
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning 和探索
- en: 'One problem we face with the policy iterative models such as Q-learning is
    the problem of exploration versus exploitation. The Q-model equation assumes the
    use of maximum quality to determine an action and we refer to this as exploitation
    (exploiting the model). The problem with this is that it can often corner an agent
    into a solution that only looks for the best short-term benefits. Instead, we
    need to allow the agent some flexibility to explore the environment and learn
    on its own. We do this by introducing a dissolving exploration factor into the
    training. Let''s see how this looks by again opening up the `Chapter_5_3.py` example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用如 Q-learning 等策略迭代模型时，面临的一个问题是探索与利用的权衡。Q 模型方程假设通过最大化质量来决定一个行动，我们称之为利用（exploiting
    the model）。这个问题在于，它常常会将智能体局限于只追求最佳短期收益的解法。相反，我们需要允许智能体有一定的灵活性去探索环境并自主学习。我们通过引入一个逐渐消失的探索因子到训练中来实现这一点。让我们通过再次打开
    `Chapter_5_3.py` 示例来看看这一过程是怎样的：
- en: 'Scroll down to the `act` and `is_explore` functions as shown:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动，查看 `act` 和 `is_explore` 函数，如下所示：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that in the `act` function, it first tests whether the agent wants to or
    needs to explore with `is_explore()`. In the `is_explore` function, we can see
    that the global `epsilon` value is decayed over each iteration with `epsilon_decay`
    to a global minimum value, `epsilon_min`. When the agent starts an episode, their
    exploration `epsilon` is high, making them more probable to explore. Over time,
    as the episode progresses, the `epsilon` decreases. We do in with the assumption
    that over time the agent will need to explore less and less. This trade-off between
    exploration and exploitation is quite important and something to understand with
    respect to the size of the environment state. We will see this trade-off explored
    more throughout this book.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在 `act` 函数中，首先测试智能体是否希望或需要进行探索，使用的是 `is_explore()`。在 `is_explore` 函数中，我们可以看到，全局的
    `epsilon` 值在每次迭代时都会通过 `epsilon_decay` 逐渐衰减，直到达到全局最小值 `epsilon_min`。当智能体开始一个回合时，他们的探索
    `epsilon` 值较高，这使得他们更可能进行探索。随着时间的推移，回合的进行，`epsilon` 值会逐渐降低。我们这样做是基于假设，随着时间的推移，智能体将需要进行越来越少的探索。探索与利用之间的权衡非常重要，特别是在环境状态空间较大时，理解这一点至关重要。本书的后续内容将深入探讨这一权衡。
- en: Note that the agent uses an exploration function and just selects a random action.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，智能体使用探索函数并随机选择一个动作。
- en: 'Finally, we get to the `learn` function. This function is where the `Q` value
    is calculated, as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们来到了 `learn` 函数。这个函数是用来计算 `Q` 值的，具体如下：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, the equation is broken up and simplified, but this is the step that calculates
    the value the agent will use when exploiting.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，方程被拆解并简化，但这是计算代理在利用时将使用的值的步骤。
- en: Keep the agent running until it finishes. We just completed the first full reinforcement
    learning problem, albeit the one that had a finite state. In the next section,
    we greatly expand our horizons and look at deep learning combined with reinforcement
    learning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 保持代理继续运行，直到完成。我们刚刚完成了第一个完整的强化学习问题，尽管这是一个有限状态的问题。在下一节中，我们将大大扩展我们的视野，探索深度学习与强化学习的结合。
- en: First DRL with Deep Q-learning
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个深度强化学习与深度Q-learning
- en: Now that we understand the reinforcement learning process in detail, we can
    look to adapt our Q-learning model to work with deep learning. This, as you could
    likely guess, is the culmination of our efforts and where the true power of RL
    shines. As we learned through earlier chapters, deep learning is essentially a
    complex system of equations that can map inputs through a non-linear function
    to generate a trained output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细了解了强化学习过程，我们可以着手将我们的Q-learning模型与深度学习结合。这正如你可能猜到的，是我们努力的巅峰所在，也是强化学习真正威力显现的地方。正如我们在前面的章节中所学，深度学习本质上是一个复杂的方程系统，可以通过非线性函数映射输入，从而生成训练输出。
- en: A neural network is just another, simpler method of solving a non-linear equation.
    We will look at how to use DNN to solve other equations later, but for now we
    will focus on using it to solve the Q-learning equation we saw in the previous
    section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络只是一种更简单的求解非线性方程的方法。我们稍后会学习如何使用DNN来解决其他方程，但现在我们专注于使用它来解决我们在上一节看到的Q-learning方程。
- en: We will use the **CartPole** training environment from the OpenAI Gym toolkit.
    This environment is pretty much the standard used to learn **Deep Q-learning**
    (**DQN**).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用OpenAI Gym工具包中的**CartPole**训练环境。这个环境几乎是学习**深度Q-learning**（**DQN**）的标准。
- en: 'Open up `Chapter_5_4.py` and follow the next steps to see how we convert our
    solver to use deep learning:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Chapter_5_4.py`文件，按照以下步骤查看如何将我们的求解器转换为使用深度学习：
- en: 'As usual, we look at the imports and some initial starting parameters, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，我们查看导入的模块和一些初始的起始参数，如下所示：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we are going to create a class this time to contain the functionality
    of the DQN agent. The `__init__` function is as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个类来包含DQN代理的功能。`__init__`函数如下所示：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Most of the parameters have already been covered, but note a new one called
    `memory`, which is a **deque** collection that holds that last 2,000 steps. This
    allows us to batch train our neural network in a sort of replay mode.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大部分参数已经涵盖，但需要注意一个新参数叫做`memory`，它是一个**deque**集合，保存了最近的2,000步。这使得我们能够在一种回放模式下批量训练神经网络。
- en: 'Next, we look at how the neural network model is built with the `_build_model`
    function, as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们查看神经网络模型是如何通过`_build_model`函数构建的，如下所示：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This builds a fairly simple model, compared to others we have already seen,
    with three **dense** layers outputting a value for each action. The input into
    this network is the state.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个模型比我们已经看到的其他模型要简单一些，包含三个**dense**层，为每个动作输出一个值。这个网络的输入是状态。
- en: 'Jump down to the bottom of the file and look at the training iteration loop,
    shown as follows:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳到文件的底部，查看训练迭代循环，如下所示：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this sample, our training takes place in a real-time `render` loop. The
    important sections of the code are highlighted, showing the reshaping of the state
    and calling the `agent.remember` function. The `agent.replay` function at the
    end is where the network trains. The `remember` function is as follows:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例中，我们的训练发生在一个实时`render`循环中。代码中的重要部分被高亮显示，展示了状态的重塑和调用`agent.remember`函数。最后的`agent.replay`函数是网络进行训练的地方。`remember`函数如下所示：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This function just stores the `state`, `action`, `reward`, `next_state`,  and
    `done` parameters for the replay training. Scroll down more to the `replay` function,
    as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数只是存储`state`、`action`、`reward`、`next_state`和`done`参数，以供回放训练。向下滚动查看`replay`函数，具体如下：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `replay` function is where the network training occurs. We first define
    a `minibatch`, which is defined from a random sampling of previous experiences
    grouped by `batch_size`. Then, we loop through the batches setting `reward` to
    the `target` and if not `done` calculating a new target based on the model prediction
    on the `next_state`. After that, we use the `model.predict` function on the `state`
    to determine the final target. Finally, we use the `model.fit` function to backpropagate
    the trained target back into the network.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`replay`函数是网络训练发生的地方。我们首先定义一个`minibatch`，它是通过从先前的经验中进行随机抽样并按`batch_size`分组定义的。然后，我们循环遍历批次，将`reward`设置为`target`，如果没有`done`，则根据模型对`next_state`的预测计算一个新的目标。之后，我们在`state`上使用`model.predict`函数来确定最终的目标。最后，我们使用`model.fit`函数将训练后的目标反向传播回网络。'
- en: As this section is important, let's reiterate. Note the line where the variable
    `target` is calculated and set. These lines of code may look familiar, as they
    match the Q value equation we saw earlier. This `target` value is the value that
    should be predicted for the current action. This is the value that is backpropagated
    back for the current action and set by the returned `reward`.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这一部分很重要，我们再强调一下。注意`target`变量计算并设置的那一行。这些代码行可能看起来很熟悉，因为它们与我们之前看到的Q值方程一致。这个`target`值是当前动作应该预测的值。这就是当前动作的回报值，并由返回的`reward`设置。
- en: 'Run the sample and watch the agent train to balance the pole on the cart. The
    following shows the environment as it is being trained:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例并观察智能体训练如何使杆子在小车上保持平衡。以下是训练过程中环境的显示：
- en: '![](img/48db2129-1bae-40e8-8dd2-1c61deb1dbc1.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48db2129-1bae-40e8-8dd2-1c61deb1dbc1.png)'
- en: CartPole OpenAI Gym environment
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole OpenAI Gym环境
- en: The example environment uses the typical first environment, CartPole, we use
    to learn to build our first DRL model. In the next section, we will look at how
    to use the DQNAgent in other scenarios and other models supplied through the Keras-RL
    API.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 示例环境使用的是我们通常用于学习构建第一个DRL模型的典型第一个环境——CartPole。下一节中，我们将查看如何在其他场景和通过Keras-RL API提供的其他模型中使用DQNAgent。
- en: RL experiments
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习实验
- en: Reinforcement learning is quickly advancing, and the DQN model we just looked
    at has quickly become outpaced by more advanced algorithms. There are several
    variations and advancements in RL algorithms that could fill several chapters,
    but most of that material would be considered academic. As such, we will instead
    look at some more practical examples of the various RL models the Keras RL API
    provides.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习正在迅速发展，我们刚刚看到的DQN模型很快就被更先进的算法所取代。RL算法有许多变种和进展，足以填满几章内容，但大多数材料会被认为是学术性的。因此，我们将重点介绍Keras
    RL API提供的各种RL模型的更实际示例。
- en: 'The first simple example we can work with is changing our previous example
    to work with a new `gym` environment. Open up `Chapter_5_5.py` and follow the
    next exercise:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的第一个简单示例是将之前的示例修改为适应新的`gym`环境。打开`Chapter_5_5.py`并按照以下练习进行操作：
- en: 'Change the environment name in the following code:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码中更改环境名称：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In this case, we are going to use the `MountainCar` environment, as shown:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用`MountainCar`环境，如下所示：
- en: '![](img/e4ed1377-b31b-4cc7-adf3-e86c57db2247.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4ed1377-b31b-4cc7-adf3-e86c57db2247.png)'
- en: Example of MountainCar environment
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: MountainCar环境示例
- en: Run the code as you normally would and see how the DQNAgent solves the hill-climbing
    problem.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像平常一样运行代码，看看DQNAgent是如何解决爬山问题的。
- en: You can see how quickly we were able to switch environments and test the DQNAgent
    in another environment. In the next section, we look at training Atari games with
    the various RL algorithms that the Keras-RL API provides.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们是如何快速切换环境并在另一个环境中测试DQNAgent的。在下一节中，我们将讨论如何使用Keras-RL API提供的不同RL算法来训练Atari游戏。
- en: Keras RL
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras RL
- en: 'Keras provides a very useful RL API that wraps several variations such as DQN,
    DDQN, SARSA, and so on. We won''t get into the details of those various RL variations
    right now, but we will cover the important parts later, as we get into more complex
    models. For now, though, we are going to look at how you can quickly build a DRL
    model to play Atari games. Open up `Chapter_5_6.py` and follow these steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个非常有用的RL API，它封装了几种变体，如DQN、DDQN、SARSA等。我们现在不会详细讨论这些不同的RL变体，但随着我们进入更复杂的模型，稍后会介绍重要的部分。不过，目前，我们将看看如何快速构建一个DRL模型来玩Atari游戏。打开`Chapter_5_6.py`并按照这些步骤操作：
- en: 'We first need to install several dependencies with `pip`; open a command shell
    or Anaconda window, and enter the following commands:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要通过 `pip` 安装几个依赖项；打开命令行或 Anaconda 窗口，并输入以下命令：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will install the Keras RL API, `Pillow`, an image framework, and the Atari
    environment for `gym`.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将安装 Keras RL API、`Pillow`（一个图像框架）以及 `gym` 的 Atari 环境。
- en: 'Run the example code as you normally would. This sample does take script arguments,
    but we don''t need to use them here. An example of the rendered Atari Breakout
    environment follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照平常的方式运行示例代码。这个示例确实需要脚本参数，但在这里我们不需要使用它们。以下是渲染的 Atari Breakout 环境示例：
- en: '![](img/3f3e5a5c-4ad3-419f-8f5b-4a57934c63c3.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f3e5a5c-4ad3-419f-8f5b-4a57934c63c3.png)'
- en: Atari Breakout environment
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Atari Breakout 环境
- en: 'Unfortunately, you cannot see the game run as the agent plays, because all
    the action takes place in the background, but let the agent run until it completes
    and saves the model. Here''s how we would run the sample:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你无法看到智能体玩游戏的过程，因为所有的动作都在后台进行，但可以让智能体运行直到它完成并保存模型。以下是我们运行示例的方法：
- en: You can rerun the sample using `--mode test` as an argument to let the agent
    run over 10 episodes and see the results.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用 `--mode test` 作为参数重新运行示例，让智能体在 10 回合中运行并查看结果。
- en: 'As the sample runs, look through the code and pay special attention to the
    model, as follows:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在示例运行时，浏览代码并特别注意模型，如下所示：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note how our model is using `Convolution`, with pooling. This is because this
    example reads each screen/frame of the game as input (state) and responds accordingly.
    In this case, the model state is massive, and this demonstrates the real power
    of DRL. In this case, we are still training to a state model, but in future chapters,
    we will look at training a policy, rather than a model.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们的模型如何使用 `Convolution`（卷积），并且带有池化。这是因为这个示例将每一帧游戏画面作为输入（状态）并做出响应。在这种情况下，模型的状态非常庞大，这展示了深度强化学习（DRL）的真正威力。在本例中，我们仍在训练一个状态模型，但在未来的章节中，我们将研究如何训练一个策略，而不是模型。
- en: This was a simple introduction to RL, and we have omitted several details that
    can get lost on newcomers. As we plan to cover several more chapters on RL, and
    in particular the **Proximal Policy Optimization** (**PPO**)in more detail in [Chapter
    8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding PPO*, don't fret
    too much about differences such as policy and model-based RL.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是强化学习（RL）的简单介绍，我们省略了一些可能让新手困惑的细节。由于我们计划在接下来的章节中更详细地讨论 RL，特别是在[第 8 章](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml)中深入讲解
    **近端策略优化**（**PPO**）和 *理解 PPO*，因此不必过于担心策略和基于模型的 RL 等差异。
- en: There is an excellent example of this same DQN in TensorFlow at this GitHub
    link: [https://github.com/floodsung/DQN-Atari-Tensorflow](https://github.com/floodsung/DQN-Atari-Tensorflow).
    The code may be a bit dated, but it is a simple and excellent example that is
    worth taking a look at.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很好的 TensorFlow DQN 示例，位于 GitHub 链接：[https://github.com/floodsung/DQN-Atari-Tensorflow](https://github.com/floodsung/DQN-Atari-Tensorflow)。代码可能有点过时，但它是一个简单且优秀的示例，值得一看。
- en: We won't look any further at the code, but the reader is certainly invited to.
    Now let's try some exercises.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会进一步查看代码，但读者可以自由地深入研究。现在让我们尝试一些练习。
- en: Exercises
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'As always, use the exercises in this section to get a better understanding
    of the material you learn. Try to work through at least two or three exercises
    in this section:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，使用本节中的练习来更好地理解你学到的内容。尽量做至少两到三个本节中的练习：
- en: Return to the example `Chapter_5_1.py` and change the **alpha** (`learning_rate`)
    variable and see what effect this has on the values calculated.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 `Chapter_5_1.py` 示例，并更改 **alpha**（`learning_rate`）变量，看看这对计算结果的影响。
- en: Return to the example `Chapter_5_2.py` and alter the arm positions on the various
    bandits.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 `Chapter_5_2.py` 示例并更改各个老虎机的位置。
- en: Change the learning rate on the example `Chapter_5_2.py` and see what effect
    this has on the Q results output.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在示例 `Chapter_5_2.py` 中更改学习率，并查看这对 Q 值结果输出的影响。
- en: Alter the gamma reward discount factor in the `Chapter_5_3.py` example, and
    see what effect this has on agent training.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变 `Chapter_5_3.py` 示例中的 gamma 奖励折扣因子，并查看这对智能体训练的影响。
- en: Change the exploration epsilon in the `Chapter_5_3.py` to different values and
    rerun the sample. See what effect altering the various exploration parameters
    has on training the agent.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Chapter_5_3.py` 中改变探索的 epsilon 值并重新运行示例，看看更改各种探索参数对训练智能体的影响。
- en: Alter the various parameters (**exploration**, **alpha**, and **gamma**) in
    the `Chapter_5_4.py` example and see what effect this has on training.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`Chapter_5_4.py`示例中的各种参数（**exploration**，**alpha**，和**gamma**），观察这些参数对训练的影响。
- en: Alter the size of the memory in the `Chapter_5_4.py` example, either higher
    or lower, and see what effect this has on training.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`Chapter_5_4.py`示例中内存的大小，增加或减少，观察这对训练的影响。
- en: Try to use different Gym environments in the DQNAgent example from `Chapter_5_5.py`.
    You can do a quick Google search to see the other possible environments you can
    choose from.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在`Chapter_5_5.py`中的DQNAgent示例中使用不同的Gym环境。你可以通过快速Google搜索查看其他可选的环境。
- en: The `Chapter_5_6.py` example currently uses a form-exploration policy called
    `LinearAnnealedPolicy`; change the policy to use the `BoltzmannQPolicy` policy
    as mentioned in the code comments.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Chapter_5_6.py`示例当前使用了一种叫做`LinearAnnealedPolicy`的形式探索策略；将策略更改为使用代码注释中提到的`BoltzmannQPolicy`策略。'
- en: Be sure to download and run other Keras-RL examples from [https://github.com/keras-rl/keras-rl](https://github.com/keras-rl/keras-rl).
    Again, you may have to install other Gym environments to get them working.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一定要从[https://github.com/keras-rl/keras-rl](https://github.com/keras-rl/keras-rl)下载并运行其他Keras-RL示例。同样，你可能需要安装其他Gym环境才能使它们正常工作。
- en: There are plenty of other examples, videos, and other materials to study with
    respect to RL. Learn as much as you can, as this material is extensive and complex
    and not something you will pick up overnight.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 还有大量关于RL的其他示例、视频和学习资料可供研究。尽可能多地学习，因为这些材料广泛且复杂，不是短时间内能够掌握的。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: RL is the machine learning technology currently dominating the interest of many
    researchers. It is typically appealing to us, because it fits well with games
    and simulations. In this chapter, we covered some of the foundations of RL by
    starting with the fundamental introductory problems of the multi-armed and contextual
    bandits. Then, we quickly looked at installing the OpenAI Gym RL toolkit. We then
    looked at Q-learning and how to implement that in code and train it on an OpenAI
    Gym environment. Finally, we looked at how we could conduct various other experiments
    with Gym by loading a couple of other environments, including the Atari games
    simulator.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: RL（强化学习）是目前主导许多研究者兴趣的机器学习技术。它之所以吸引我们，通常是因为它非常适合游戏和仿真。在本章中，我们通过从多臂老虎机和上下文赌博机的基本入门问题开始，介绍了RL的一些基础知识。然后，我们快速了解了如何安装OpenAI
    Gym RL工具包。接着，我们研究了Q学习以及如何在代码中实现它并在OpenAI Gym环境中训练。最后，我们看到了如何通过加载其他环境（包括Atari游戏模拟器）来进行各种其他实验。
- en: In the next chapter, we look at the quickly evolving a cutting-edge RL platform
    that Unity is currently developing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨Unity目前正在开发的一个快速发展的前沿RL平台。
