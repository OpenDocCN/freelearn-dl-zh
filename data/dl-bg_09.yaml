- en: Autoencoders
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器
- en: This chapter introduces the autoencoder model by explaining the relationship
    between encoding and decoding layers. We will be showcasing a model that belongs
    to the unsupervised learning family. This chapter also introduces a loss function
    commonly associated with the autoencoder model, and it also applies it to the
    dimensionality reduction of MNIST data and its visualization in an autoencoder-induced
    latent space.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过解释编码层和解码层之间的关系来介绍自动编码器模型。我们将展示一个属于无监督学习范畴的模型。本章还介绍了与自动编码器模型常关联的损失函数，并将其应用于MNIST数据的降维及其在自动编码器诱导的潜在空间中的可视化。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Introduction to unsupervised learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习简介
- en: Encoding and decoding layers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和解码层
- en: Applications in dimensionality reduction and visualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维与可视化的应用
- en: Ethical implications of unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习的伦理影响
- en: Introduction to unsupervised learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习简介
- en: 'As machine learning has progressed over the last few years, I have come across
    many ways to categorize the different types of learning. Recently, at the NeurIPS
    2018 conference in Montreal, Canada, Dr. Alex Graves shared information about
    the different types of learning, shown in *Figure 7.1*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习在过去几年中的发展，我遇到了许多分类不同类型学习的方法。最近，在2018年蒙特利尔举行的NeurIPS大会上，Alex Graves博士分享了关于不同类型学习的信息，见*图7.1*：
- en: '![](img/5bee9eca-50c2-4a2f-ad2d-2d7f4dd7de2e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bee9eca-50c2-4a2f-ad2d-2d7f4dd7de2e.png)'
- en: Figure 7.1 – Different types of learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 不同类型的学习
- en: Such efforts at categorization are very useful today when there are many learning
    algorithms being studied and improved. The first row depicts *active* learning,
    which means that there is a sense of interaction between the learning algorithm
    and the data. For example, in reinforcement learning and active learning operating
    over *labeled data*, the reward policies can inform what type of data the model
    will read in the following iterations. However, traditional supervised learning,
    which is what we have studied so far, involves no interaction with the data source
    and instead assumes that the dataset is fixed and that its dimensionality and
    shape will not change; these non-interactive approaches are known as *passive*
    learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类方法在今天非常有用，因为目前有许多学习算法正在被研究和改进。第一行描述了*主动*学习，这意味着学习算法与数据之间存在交互。例如，在强化学习和作用于*标记数据*的主动学习中，奖励策略可以指示模型在接下来的迭代中将读取哪种类型的数据。然而，传统的监督学习（即我们迄今为止所学习的内容）不与数据源交互，而是假设数据集是固定的，并且其维度和形状不会改变；这些非交互式方法被称为*被动*学习。
- en: The second column in the table in the *Figure 7.1* represents a special kind
    of learning algorithm that requires *no labels* to learn from data. Other algorithms
    require you to have a dataset that has data ![](img/52ff04b1-d1b4-4173-846d-eb4fc6caaa71.png) that
    is associated with a label ![](img/2e108673-48ce-4bf6-886c-961518e1492f.png);
    that is: ![](img/16a7ebb1-2910-4ea4-be45-11490e9bdd33.png). However, unsupervised
    algorithms have no need for labels to "do things" with data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1*中表格的第二列代表一种特殊类型的学习算法，它不需要*标签*来从数据中学习。其他算法则要求你拥有一个带有数据的![](img/52ff04b1-d1b4-4173-846d-eb4fc6caaa71.png)与标签![](img/2e108673-48ce-4bf6-886c-961518e1492f.png)相关联的数据集；也就是说：![](img/16a7ebb1-2910-4ea4-be45-11490e9bdd33.png)。然而，无监督算法不需要标签来“处理”数据。'
- en: You can think of labels as a **teacher**. A teacher tells the learner that **x** corresponds
    to [![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)] and the learner attempts
    to learn the relationship between [![](img/af098bbe-7b09-4ffa-a98f-1537109b18f5.png)] and [![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)] iteratively
    by trial and error, adjusting its *beliefs* (parameters) until it gets it right.
    However, if there is no teacher, the learner does not know anything about the
    label ![](img/4f8dc03e-cf59-479f-89d9-7e277b089165.png) and therefore learns *something*
    about ![](img/cd1d240b-75a4-4b5b-b53e-e6f9147a5425.png) by itself, provided some
    boundaries, and it forms its own beliefs about ![](img/af098bbe-7b09-4ffa-a98f-1537109b18f5.png) without
    ever knowing anything about ![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把标签看作是**老师**。老师告诉学习者**x**对应于[![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)]，然后学习者通过反复试验来尝试学习[![](img/af098bbe-7b09-4ffa-a98f-1537109b18f5.png)]和[![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)]之间的关系，不断调整其*信念*（参数），直到理解正确。然而，如果没有老师，学习者对标签![](img/4f8dc03e-cf59-479f-89d9-7e277b089165.png)一无所知，因此学习者通过自己从![](img/cd1d240b-75a4-4b5b-b53e-e6f9147a5425.png)中学习*某些东西*，并在一定的边界条件下形成自己对![](img/af098bbe-7b09-4ffa-a98f-1537109b18f5.png)的信念，而永远不会知道![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)的任何信息。
- en: In the following chapters, we will study **unsupervised learning**, which is
    the type of learning that assumes that the data we have will not change in its
    shape or form and will remain consistent during the learning process and also
    during its deployment. These algorithms are guided by something other than labels,
    for example, a unique loss function for data compression. On the other hand, there
    are other algorithms that have an exploration mechanism or a specific motivation
    to learn from data in an interactive way, and these algorithms are **active learning**
    algorithms. We will not discuss these algorithms in this book since this book
    is intended to be introductory and for absolute beginners. However, we will discuss
    at length some of the most robust *unsupervised *deep learning models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习**无监督学习**，这是一种假设我们所拥有的数据在形态或形式上不会发生变化，并且在学习过程以及部署过程中保持一致的学习方式。这些算法的指导依据是其他因素，而非标签，例如，针对数据压缩的独特损失函数。另一方面，还有一些算法具有探索机制或特定的动机，以交互方式从数据中学习，这些算法被称为**主动学习**算法。本书不会讨论这些算法，因为本书旨在作为入门教程，适合完全初学者。然而，我们将详细讨论一些最强大的*无监督*深度学习模型。
- en: 'We will begin by learning about **autoencoders**. An autoencoder has the sole
    purpose of taking input data into a neural network that is composed of two parts:
    an **encoder** and a **decoder**. The encoder portion has the mission of encoding
    the input data, usually into a lower-dimensional space, thereby compressing or
    encoding the input data. The decoder portion of the model is in charge of taking
    the encoded (or compressed) latent representation of the input data and then reconstructing
    it back to its original shape and to its original values without losing any data.
    That is, in an ideal autoencoder, the input is equal to the output. Let''s discuss
    this in more detail in the following sections.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先学习**自编码器**。自编码器的唯一目的是将输入数据输入到由两部分组成的神经网络中：**编码器**和**解码器**。编码器部分的任务是对输入数据进行编码，通常是将其压缩到一个低维空间，从而压缩或编码输入数据。模型的解码器部分负责将编码（或压缩）的输入数据的潜在表示重新构建回原始形状和原始值，而不丢失任何数据。也就是说，在理想的自编码器中，输入等于输出。让我们在接下来的章节中更详细地讨论这一点。
- en: Encoding and decoding layers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码和解码层
- en: 'The autoencoder can be broken down into two major components that serve specific
    purposes during an unsupervised learning process. The left side of *Figure 7.2*
    shows an autoencoder that is implemented using fully connected (dense) layers.
    It receives as input some vector ![](img/51804466-39c7-495b-a3c7-6b5a0e8462fe.png) and
    then it goes into six hidden layers; the first three, with 6, 4, and 2 neurons,
    respectively, are meant to compress the input data down to two dimensions, since
    the output of two neurons is two scalar values. This first set of layers is known
    as the **encoder**:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以分解为两个主要组件，这些组件在无监督学习过程中起到特定的作用。*图 7.2*的左侧展示了一个使用全连接（密集）层实现的自编码器。它接收一些向量作为输入，![](img/51804466-39c7-495b-a3c7-6b5a0e8462fe.png)，然后进入六个隐藏层；前三个层分别有6、4和2个神经元，目的是将输入数据压缩到二维，因为两个神经元的输出是两个标量值。这个第一组层被称为**编码器**：
- en: '![](img/f308db9c-1cf1-42e1-b1eb-7c8bfea595b9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f308db9c-1cf1-42e1-b1eb-7c8bfea595b9.png)'
- en: 'Figure 7.2 – Two representations of an autoencoder. Left: full and descriptive
    model. Right: compact and abstracted model representation'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 自动编码器的两种表示。左侧：完整且描述性的模型。右侧：简洁且抽象的模型表示
- en: The second set of neurons is meant to reconstruct the input data back to its
    original dimensionality and values ![](img/9d520487-cfc5-48c1-aa44-2e0a69486ce5.png) using
    three layers with 4, 6, and 8 neurons, respectively; this group of layers is known
    as the **decoder**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组神经元用于将输入数据重建回其原始维度和数值！[](img/9d520487-cfc5-48c1-aa44-2e0a69486ce5.png)，它通过三层，分别有4、6和8个神经元来实现；这一组层被称为**解码器**。
- en: Note that the last layer of the autoencoder *must have* the same number of neurons
    as the number of dimensions of the input vector. Otherwise, the reconstruction
    would not match the input data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，自动编码器的最后一层*必须具有*与输入向量的维度相同的神经元数量。否则，重建结果将无法与输入数据匹配。
- en: 'In this case, the autoencoder shown on the left in *Figure 7.2* acts as a compression
    network, in the sense that after training the model to achieve good reconstruction,
    if we disconnect the decoder, we end up with a neural network that encodes the
    input data into two dimensions (or any dimensions we choose, for that matter).
    This presents a unique advantage over supervised models: in a supervised model,
    we teach a network to look for a pattern that will permit an association with
    a given target label; however, in unsupervised learning (or in this autoencoder,
    for example), the network does not look for a specific pattern but rather learns
    to use the input space in any way that preserves the most representative and most
    important information of the input data, so as to allow good reconstruction in
    the decoder.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*图 7.2* 左侧显示的自动编码器充当压缩网络，意思是，在训练模型达到良好重建后，如果我们断开解码器，我们就得到了一个神经网络，它将输入数据编码为二维（或我们选择的任何维度）。这相比于监督学习模型，具有独特的优势：在监督学习模型中，我们教网络寻找一个可以与给定目标标签关联的模式；然而，在无监督学习中（比如在这个自动编码器中），网络并不寻找一个特定的模式，而是学习以任何能够保留输入数据最具代表性和最重要信息的方式使用输入空间，从而允许在解码器中进行良好的重建。
- en: Think of a neural network and an autoencoder that takes input images of cats
    and dogs; a traditional neural network can be trained to distinguish between dogs
    and cats and it is tasked with finding important patterns in the images of dogs
    and cats so as to tell the difference between them; however, an autoencoder will
    train to learn the most important patterns, the most representative of all patterns,
    so as to preserve that information and allow good reconstruction regardless of
    the label. In a way, the traditional supervised neural network is biased to see
    the world in terms of cats and dogs, while the autoencoder is free to learn from
    the world regardless of cats or dogs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个神经网络和一个自动编码器，它们接受猫和狗的图像作为输入；传统的神经网络可以被训练来区分猫和狗，它的任务是寻找狗和猫图像中的重要模式，以便区分它们；然而，自动编码器将训练来学习最重要的模式，所有模式中最具代表性的模式，以便保留这些信息并允许无论标签是什么都能进行良好的重建。从某种程度上说，传统的监督神经网络是带有偏见的，它只能从猫和狗的角度来看待世界，而自动编码器则可以自由地从任何角度学习，不管是猫还是狗。
- en: The diagram on the right of *Figure 7.2* depicts an alternative representation
    of an autoencoder that is more abstract and compact. This type of representation
    is useful when describing a relatively deep autoencoder, when the number of layers
    is large to the point of it being difficult to represent all the neurons and all
    the layers one by one (as in the left side of the *Figure 7.2*). We will use those
    trapezoidal shapes to denote that there is an encoder/decoder; we note also that
    this abstraction will allow us the freedom to use other types of layers and not
    only dense (fully connected) layers. The diagram on the right of *Figure 7.2*
    depicts an autoencoder that takes an image as input, then encodes the input into
    a *d-*dimensional space, and then reconstructs the *latent* vector back to the
    input (image) space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.2* 右侧的图示展示了自动编码器的另一种表示，它更加抽象且紧凑。这种表示方式在描述相对深的自动编码器时非常有用，当网络层数多到难以逐一表示所有神经元和所有层时（如*图
    7.2* 左侧所示）。我们将使用这些梯形图形来表示编码器/解码器；我们还注意到，这种抽象化将允许我们使用其他类型的层，而不仅仅是密集（全连接）层。*图 7.2*
    右侧的图示展示了一个自动编码器，该编码器将图像作为输入，接着将输入编码到*d-*维空间，然后将*潜在*向量重建回输入（图像）空间。'
- en: A **latent space**is a space where the learned lower-dimensional patterns are
    mapped. It is also known as the *learned representation space*. Ideally, this
    latent space is rich in important information about the input data and has fewer
    dimensions than the input data without any loss of information.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在空间**是一个映射学习到的低维模式的空间。它也被称为 *学习到的表示空间*。理想情况下，这个潜在空间包含关于输入数据的重要信息，而且其维度少于输入数据，同时不会丢失任何信息。'
- en: Let's now go ahead and implement each of the autoencoder parts based on the
    simple model on the left in *Figure 7.2*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现每个自编码器部分，基于左侧的简单模型，如 *图 7.2* 所示。
- en: Encoding layers
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码层
- en: 'The TensorFlow and Keras libraries that we will be using are `Input` and `Dense`
    from `tensorflow.keras.layers` and `Model` from `tensorflow.keras.models`. We
    will be using the `keras` functional approach as opposed to *sequential* modeling.
    Import the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 TensorFlow 和 Keras 库包括来自 `tensorflow.keras.layers` 的 `Input` 和 `Dense`，以及来自
    `tensorflow.keras.models` 的 `Model`。我们将采用 `keras` 的函数式方法，而不是 *顺序* 建模。请导入以下内容：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `Input` layer will be used to describe the dimensionality of the input
    vector, which in our case will be `8`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`Input` 层将用来描述输入向量的维度，在我们的例子中是 `8`：'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, considering all of our activation functions as `sigmoid` just for the
    sake of this example, we can define the pipeline of the encoder layers as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，考虑到所有的激活函数都为 `sigmoid`，仅仅为了这个例子，我们可以如下定义编码器各层的流水线：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `Dense` class constructor receives the number of neurons and the activation
    function as parameters and at the end of the definition (on the right side), we
    must include what the input to the layer is, and this is assigned a name on the
    left side. Thus, in the line `elayer1 = Dense(6, activation='sigmoid')(inpt_vec)`,
    the name assigned to the layer is `elayer1`, then `6` is the number of neurons,
    `activation='sigmoid'` assigns a `sigmoid` activation function to the dense layer,
    and `inpt_vec` is the input to this layer in the pipeline.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense` 类构造函数接收神经元数量和激活函数作为参数，在定义的末尾（右侧）我们必须包括该层的输入，这个输入在左侧赋予名称。因此，在代码行 `elayer1
    = Dense(6, activation=''sigmoid'')(inpt_vec)` 中，分配给该层的名称是 `elayer1`，`6` 是神经元数量，`activation=''sigmoid''`
    为该密集层分配了 `sigmoid` 激活函数，而 `inpt_vec` 是该层的输入。'
- en: 'In the preceding three lines of code, we have defined the layers of the encoder,
    and the `encoder` variable points to the object that can output the latent variable
    if we make it a model and call `predict()` on it:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面三行代码中，我们定义了编码器的各层，而 `encoder` 变量指向可以输出潜在变量的对象，如果我们将其作为模型并调用 `predict()` 方法：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this line of code, `latent_ncdr` contains the model that can map the input
    data to the latent space once it is trained. But before we do that, let's go ahead
    and define the layers of the decoder in a similar way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行代码中，`latent_ncdr` 包含可以将输入数据映射到潜在空间的模型，一旦训练完成。但在此之前，让我们以类似的方式定义解码器的各层。
- en: Decoding layers
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码层
- en: 'We can define the decoder layers as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下定义解码器各层：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that in the preceding code, the number of neurons usually goes in increasing
    order until the last layer that matches the input dimension. In this case, 4,
    6, and 8 are defined as `inpt_dim`. Similarly, the `decoder` variable points to
    the object that can output the reconstructed input if we make it a model and call `predict()` on
    it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码中，神经元的数量通常是递增的，直到最后一层与输入维度匹配。在此案例中，4、6 和 8 被定义为 `inpt_dim`。类似地，`decoder`
    变量指向能够输出重建输入的对象，如果我们将其作为模型并调用 `predict()` 方法。
- en: 'We have separated the encoder and decoder intentionally here, simply to show
    that we could have the ability to access the different components of the network
    if we choose to do so. However, we should probably also define the autoencoder
    as a whole, from input to output, by using the `Model` class as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意将编码器和解码器分开，仅仅是为了展示如果需要的话，我们可以访问网络的不同组件。然而，我们可能也应该将自编码器从输入到输出作为一个整体来定义，使用
    `Model` 类，如下所示：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is exactly what we meant earlier when we said "if we make it a model and
    call `predict()` on it." This declaration is making a model that takes as input
    the input vector defined in `inpt_vec` and retrieves the output from the `decoder` layer.
    Then, we can use this as a model object that has a few nice functions in Keras
    that will allow us to pass input, read output, train, and do other things that
    we will discuss in the upcoming sections. For now, since we have defined our model,
    and before we can train it, we should define what the objective of the training
    will be, that is*, *what our loss function will be.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们之前所说的“如果我们将其做成一个模型并调用 `predict()`”的意思。这个声明实际上是在创建一个模型，它以`inpt_vec`中定义的输入向量作为输入，并从`decoder`层中获取输出。然后，我们可以使用这个模型对象，它在Keras中有一些方便的功能，允许我们传入输入、读取输出、训练以及进行其他我们将在接下来的章节中讨论的操作。现在，由于我们已经定义了模型，并且在训练之前，我们需要定义训练的目标，也就是我们的损失函数。
- en: Loss function
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'Our loss function has to be in terms of the goal of the autoencoder. This goal
    is to reconstruct the input perfectly. That means that our input ![](img/5609bfb0-1712-46de-89cc-f6748f485e75.png) and
    our reconstruction ![](img/20073a1c-ab85-4c68-9710-7dc75e3838d9.png) have to be
    identical in an ideal autoencoder. This implies that the absolute difference must
    be zero:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数必须与自编码器的目标相关。这个目标是完美地重建输入。也就是说，在理想的自编码器中，我们的输入 ![](img/5609bfb0-1712-46de-89cc-f6748f485e75.png)
    和我们的重建 ![](img/20073a1c-ab85-4c68-9710-7dc75e3838d9.png) 必须是相同的。这意味着它们之间的绝对差异必须为零：
- en: '![](img/46e56da7-bf75-4217-8bda-4ebfce996ffa.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46e56da7-bf75-4217-8bda-4ebfce996ffa.png)'
- en: 'However, this may not be realistic, and it is not in terms of a function that
    we can easily differentiate. For this, we can come back to the classic mean squared
    error function, which is defined as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能不太现实，并且它并不是一个我们容易求导的函数。为此，我们可以回到经典的均方误差函数，其定义如下：
- en: '![](img/c7136505-52c9-429a-8d05-5d958e6983f1.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7136505-52c9-429a-8d05-5d958e6983f1.png)'
- en: 'We want to make ![](img/cf4788bf-0dbb-4180-9e74-0c6f8c31b7b0.png), ideally,
    or at best minimize it as much as possible. We interpret this loss function as
    minimizing the average of the squared differences between the input and its reconstruction.
    If we use a standard backprop strategy, say, some type of standard gradient descent
    technique, we can compilethe model and prepare it for training as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使 ![](img/cf4788bf-0dbb-4180-9e74-0c6f8c31b7b0.png) 变得理想，或者至少尽可能将其最小化。我们将这个损失函数解释为最小化输入与其重建之间的平方差的平均值。如果我们使用标准的反向传播策略，比如某种标准的梯度下降技术，我们可以像下面这样编译模型并准备训练：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `compile()` method prepares the model for training. The loss function defined
    previously is given as a parameter, `loss='mean_squared_error'`, and the optimization
    technique chosen here is known as **stochastic gradient descent** (**SGD**)*,*
    `optimizer='sgd'`. For more information on SGD, please see Amari, S. I. (1993).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`compile()` 方法为训练做好准备。先前定义的损失函数作为参数传入，`loss=''mean_squared_error''`，这里选择的优化技术被称为**随机梯度下降**
    (**SGD**)*，* `optimizer=''sgd''`。有关SGD的更多信息，请参见 Amari, S. I. (1993)。'
- en: Learning and testing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习与测试
- en: Since this is an introductory example of a simple autoencoder, we will train
    only with one data point and begin the learning process. We also want to show
    the encoded version and the reconstructed version.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个简单自编码器的入门示例，我们将只使用一个数据点进行训练，并开始学习过程。我们还希望展示编码版本和重建版本。
- en: 'We will use the number 39 in binary as eight digits, which corresponds to 00100111\.
    We will declare this as our input vector as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用数字39的二进制表示，作为八位数字，它对应于 00100111\. 我们将声明其为我们的输入向量，具体如下：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can then perform the training as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以执行如下的训练：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `fit()` method performs the training. Its first two arguments are the input
    data and the desired target output; in the case of the autoencoder, they are both
    `x`. The number of epochs is specified as `epochs=10000`, since the model can
    produce a decent output at this point, and we set the verbosity to zero since
    we do not need to visualize every epoch, using `verbose=0`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 方法执行训练。它的前两个参数是输入数据和期望的目标输出；在自编码器的情况下，它们都是 `x`。训练的轮次通过 `epochs=10000`
    来指定，因为此时模型已经能够生成不错的输出，我们将冗余输出设置为零，使用 `verbose=0`，因为我们不需要可视化每个epoch。'
- en: In Google Colab or Jupyter Notebook, it is not a good idea to visualize more
    than 1,000 epochs on the screen at a time. The web browser might become unresponsive
    to the JavaScript code in charge of displaying all these epochs. Beware.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Colab或Jupyter Notebook中，一次性在屏幕上可视化超过1,000个时期并不是一个好主意。网页浏览器可能会对负责显示所有这些时期的JavaScript代码无响应。请注意。
- en: 'The `predict()` method in the latent encoder model, `latent_ncdr`, and in the
    `autoencoder` model produce the output at the specified layers. If we retrieve
    `encdd`, we can see the latent representation of the input, and if we retrieve `x_hat`,
    we can see the reconstruction. We can even calculate the mean squared error manually
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在编码器模型`latent_ncdr`和`autoencoder`模型中的`predict()`方法会在指定的层产生输出。如果我们检索`encdd`，我们可以看到输入的潜在表示，如果我们检索`x_hat`，我们可以看到重建。我们甚至可以手动计算均方误差，如下所示：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This produces the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The numbers here will vary due to the unsupervised nature of the learning algorithm.
    The first output vector can be any real numbers. The second output vector will
    likely have real numbers close to zero and close to one, resembling the original
    binary vector, but the exact values will vary every single time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数字会因为学习算法的非监督特性而有所变化。第一个输出向量可以是任意实数。第二个输出向量可能会有接近零和接近一的实数，类似于原始的二进制向量，但确切的值每次都会有所不同。
- en: The first vector of two elements is the latent representation, [0.55, 0.43];
    this may not mean much to us at this point, but it will be very important to us
    in terms of data compression. It means that we are able to represent eight digits
    using two.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个由两个元素组成的向量是潜在表示，[0.55, 0.43]；此时这可能对我们来说意义不大，但在数据压缩方面，它将非常重要。这意味着我们可以用两个数字来表示八个数字。
- en: Although this is a toy example and representing a binary number with two digits
    is not very exciting, the theory behind this is that we could take any eight floating-point
    digits in the range [0, 1] and compress them down to two digits in the same range.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这只是一个玩具示例，用两位数字表示二进制数并不是非常令人兴奋，但其背后的理论是，我们可以取[0, 1]范围内的任意八个浮点数，并将它们压缩到同一范围内的两个数字中。
- en: 'The second vector displayed shows evidence of a good reconstruction: something
    that should be a zero is a 0.08 and something that should be a one is a 0.91\.
    The **mean squared error** (**MSE**) as calculated manually yields a 0.007, which,
    although not zero, is small enough to be good.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个显示的向量显示了良好重建的证据：应该是零的东西是0.08，应该是一的东西是0.91。手动计算的**均方误差**（**MSE**）为0.007，虽然不是零，但足够小以达到良好的效果。
- en: 'We can visualize the decay of the MSE throughout the training phase using the
    information stored in the `hist` object defined during the invocation of `fit()`.
    This object contains the information of the loss function value across epochs
    and allows us to visualize the process with the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在调用`fit()`时定义的`hist`对象中存储的信息来可视化训练阶段的MSE衰减过程。该对象包含跨时期的损失函数值信息，并允许我们使用以下代码可视化该过程：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This produces what you see in *Figure 7.3*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了您在*图7.3*中看到的内容：
- en: '![](img/2207d736-a57d-4b78-b766-5d47cdb5e6b3.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2207d736-a57d-4b78-b766-5d47cdb5e6b3.png)'
- en: Figure 7.3 – Reconstruction loss across epochs of training of an autoencoder
    as described in terms of the MSE
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 自编码器训练过程中的重建损失随时期的变化
- en: 'Okay, once again, this was a toy example with one data point. We would never
    do this in *real life*. To show how bad an idea this is, we can take the binary
    string we used to train the model and invert every single bit, which gives 11011000
    (or 216 in decimal). If we gave this to the autoencoder, we would expect a *good*
    reconstruction, but let''s see what happens if we try to do that:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，再次强调，这只是一个使用一个数据点的玩具示例。我们在*现实生活*中绝不会这样做。为了展示这是一个多么糟糕的主意，我们可以取用于训练模型的二进制字符串并反转每一个位，得到11011000（或者十进制的216）。如果我们将这个输入给自编码器，我们期望会有一个*良好的*重建，但让我们看看如果我们尝试这样做会发生什么：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We get the following output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once again, the numbers here will vary due to the unsupervised nature of the
    learning algorithm. If your results are different from what you see here (I'm
    sure they are), that is not a problem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这里的数字会因为学习算法的非监督特性而有所变化。如果您的结果与此处所见不同（我敢肯定会有），那也没有问题。
- en: If you compare these results with the ones from before, you will notice that
    the latent representation is not that different, and the reconstructed output
    does not at all match the given input. It is evident that the model **memorized**
    the input on which it was trained. This is evident when we calculate the MSE and
    we obtain a value of 0.84, which is large compared to the one previously obtained.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将这些结果与之前的结果进行比较，你会发现潜在表示并没有太大变化，且重构输出与给定输入完全不匹配。显然，模型**记住**了它所训练的输入。当我们计算均方误差
    (MSE) 并得到 0.84 时，可以看出这个值相比之前获得的要大得多。
- en: The solution to this is, of course, adding more data. But this concludes the
    toy example of building an autoencoder. What really changes after this is the
    type and amount of data, the number of layers, and the types of layers. In the
    next section, we will look at the application of a simple autoencoder in dimensionality
    reduction problems.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法当然是增加更多数据。但这仅是构建自编码器的玩具示例。从此之后，真正变化的是数据的类型和数量、层数以及层的类型。在下一节中，我们将探讨简单自编码器在降维问题中的应用。
- en: Applications in dimensionality reduction and visualization
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在降维和可视化中的应用
- en: Among some of the most interesting applications of autoencoders is dimensionality
    reduction [Wang, Y., et al. (2016)]. Given that we live in a time where data storage
    is easily accessible and affordable, large amounts of data are currently stored
    everywhere. However, not everything is relevant information. Consider, for example,
    a database of video recordings of a home security camera that always faces one
    direction. Chances are that there is a lot of repeated data in every video frame
    or image and very little of the data gathered will be useful. We would need a
    strategy to look at what is really important in those images. Images, by their
    nature, have a lot of redundant information, and there is usually correlation
    among image regions, which makes autoencoders very useful in compressing the information
    in images (Petscharnig, S., et al. (2017)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的一些最有趣的应用之一是降维 [Wang, Y., 等 (2016)]。鉴于我们生活在一个数据存储易于访问且价格合理的时代，当前到处都有大量数据被存储。然而，并非所有数据都是相关的信息。举个例子，考虑一个始终朝一个方向的家庭安防摄像头的视频录制数据库。可以想象，每一帧视频或图像中有大量重复数据，且收集的数据中只有极少部分是有用的。我们需要一种策略来查看这些图像中真正重要的部分。图像本身具有大量冗余信息，且图像区域之间通常存在相关性，这使得自编码器在压缩图像信息时非常有用
    (Petscharnig, S., 等 (2017))。
- en: To demonstrate the applicability of autoencoders in dimensionality reduction
    for images, we will use the well-known MNIST dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示自编码器在图像降维中的适用性，我们将使用著名的 MNIST 数据集。
- en: MNIST data preparation
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST 数据准备
- en: 'For details about MNIST, please go to [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml),
    *Preparing Data*. Here we will only mention that the MNIST data will be scaled
    to the range [0, 1]. We also need to convert all images into vectors by reshaping
    the 28 by 28 digit images into a 784-dimensional vector. This can be achieved
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 MNIST 的详细信息，请参阅[第 3 章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)，*准备数据*。在这里，我们只提到
    MNIST 数据将被缩放到范围[0, 1]。我们还需要通过将 28 x 28 的数字图像重塑为 784 维的向量来将所有图像转换为向量。可以通过以下方式实现：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will use `x_train` to train the autoencoder and `x_test` to test the generalization
    capability of the autoencoder to both encode and decode MNIST digits. For visualization
    purposes, we will need `y_test`, but `y_train` can be ignored since we do not
    need labels in unsupervised machine learning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `x_train` 来训练自编码器，并使用 `x_test` 来测试自编码器对 MNIST 数字的编码和解码泛化能力。出于可视化目的，我们将需要
    `y_test`，但由于我们不需要在无监督学习中使用标签，`y_train` 可以忽略。
- en: '*Figure 7.4* depicts the first eight samples in `x_test`. These samples will
    be used across a few experiments to show the capabilities of different autoencoder
    models:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.4* 描述了 `x_test` 中的前八个样本。这些样本将在多个实验中用于展示不同自编码器模型的能力：'
- en: '![](img/6d1cc115-5ef0-473a-a4a5-f470dc712f1f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d1cc115-5ef0-473a-a4a5-f470dc712f1f.png)'
- en: Figure 7.4 – Test MNIST digits to be used for comparison
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 用于比较的测试 MNIST 数字
- en: Autoencoders for MNIST
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST 的自编码器
- en: We can design a few experiments with different numbers of layers to see how
    the autoencoder changes its performance for MNIST. We can start with an autoencoder
    with four layers, always using a latent dimension of two. This is done to facilitate
    visualizing the MNIST digits in a two-dimensional space induced by the autoencoders.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设计几个不同层数的实验，以观察自编码器在MNIST上的性能变化。我们可以从一个四层的自编码器开始，始终使用一个二的潜在维度。这样做是为了方便在自编码器引导的二维空间中可视化MNIST数字。
- en: 'Based on the previously defined autoencoder, we can propose the following four-layer
    base autoencoder:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前定义的自编码器，我们可以提出以下四层基础自编码器：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will be the base of the subsequent models. There are a few things that
    are highlighted that are new and need to be introduced properly. The first important
    thing is a new activation function called the **hyperbolic tangent**. This activation
    function is defined as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是后续模型的基础。这里有一些新的内容需要特别强调，并需要适当介绍。首先要介绍的一个重要内容是一个新的激活函数，叫做**双曲正切**。这个激活函数的定义如下：
- en: '![](img/52343b1b-860d-4414-b38c-dcd47901d881.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52343b1b-860d-4414-b38c-dcd47901d881.png)'
- en: 'The corresponding first derivative is relatively simple:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的第一导数相对简单：
- en: '![](img/787eef00-1a67-4308-a579-98b187cf7ab2.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/787eef00-1a67-4308-a579-98b187cf7ab2.png)'
- en: Besides having a nice and easily calculable derivative, the hyperbolic tangent
    activation function has a nice output range of [-1, 1]. This allows a neutral
    range that is not necessarily constrained to the sigmoid range [0, 1]. For visualization
    purposes, sometimes it is interesting to visualize in the hyperbolic tangent range,
    but it is not necessary to do so.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拥有一个漂亮且易于计算的导数外，双曲正切激活函数还具有一个良好的输出范围[-1, 1]。这提供了一个中立的范围，不必局限于sigmoid范围[0,
    1]。为了可视化的目的，有时在双曲正切范围内可视化是很有趣的，但并非必须这么做。
- en: 'Another new element we have introduced is the loss function called **binary
    cross-entropy**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入的另一个新元素是称为**二元交叉熵**的损失函数：
- en: '![](img/b277ace3-8e19-46d5-969d-3c8eb2469cac.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b277ace3-8e19-46d5-969d-3c8eb2469cac.png)'
- en: In general, binary cross-entropy uses information, which are theoretical ideas
    to calculate the error between the target data ![](img/64dc5f89-e650-4e23-a54d-ee7b42d906f2.png) and
    the reconstructed (or predicted) data ![](img/491f7fd1-ce72-4085-a1f5-9de08fee77d6.png).
    In a way, it measures the amount of entropy, or surprise, between the target and
    the prediction. For example, in an ideal autoencoder, it is not surprising that
    the target ![](img/d759bd24-540c-47ec-a39e-533ee26397b9.png) is equal to its reconstruction ![](img/1383cd76-dc04-4c7e-8437-9e74de440010.png),
    and the loss should be zero. However, if the target ![](img/131b4632-5c63-4ab0-856c-21ef58000492.png) is
    not equal to ![](img/c67111eb-15ba-4682-9dae-c6468ad25fe8.png), that would be
    surprising and would yield a high loss.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，二元交叉熵使用信息，这是理论上的一种方法，用来计算目标数据 ![](img/64dc5f89-e650-4e23-a54d-ee7b42d906f2.png) 和重构（或预测）数据 ![](img/491f7fd1-ce72-4085-a1f5-9de08fee77d6.png) 之间的误差。从某种意义上说，它衡量了目标和预测之间的熵或惊讶量。例如，在理想的自编码器中，目标 ![](img/d759bd24-540c-47ec-a39e-533ee26397b9.png) 与其重构 ![](img/1383cd76-dc04-4c7e-8437-9e74de440010.png) 相等并不令人惊讶，损失应该为零。然而，如果目标 ![](img/131b4632-5c63-4ab0-856c-21ef58000492.png) 不等于 ![](img/c67111eb-15ba-4682-9dae-c6468ad25fe8.png)，这将是令人惊讶的，并且会产生较大的损失。
- en: For a more complete discussion on autoencoders using cross-entropy loss, see
    (Creswell, A., et. al. (2017)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用交叉熵损失的自编码器的更完整讨论，请参见(Creswell, A., et. al. (2017))。
- en: A new optimizer called **Adam** has also been introduced (Kingma, D. P., et.
    al. (2014)). It is an algorithm for stochastic optimization that uses an adaptive
    learning rate that has proven to be very fast in some deep learning applications.
    Speed is a nice property when we are dealing with deep learning models and large
    datasets. Time is of the essence, and Adam provides a nice approach that has become
    popular.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 还引入了一种新的优化器，叫做**Adam**（Kingma, D. P., et. al. (2014)）。它是一种用于随机优化的算法，使用自适应学习率，在某些深度学习应用中已被证明非常快速。当我们处理深度学习模型和大数据集时，速度是一个重要的特性。时间至关重要，而Adam提供了一种非常有效的方法，已经变得相当流行。
- en: Finally, the last new thing we added was on the `fit()` method. You should have
    noticed that there are two new parameters: `shuffle=True`, which allows the shuffling
    of data during the training process; and `validation_data=( , )`, which specifies
    a tuple of data that is used to monitor the loss using validation data, or data
    that the model has never seen, and will never use, for training.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加的新内容是在`fit()`方法中。你应该已经注意到有两个新的参数：`shuffle=True`，它允许在训练过程中对数据进行洗牌；以及`validation_data=(
    , )`，它指定了一个数据元组，用于监控使用验证数据的损失，验证数据是模型从未见过的，也不会用于训练。
- en: 'That is all the new things we have introduced. The next step is to explain
    the autoencoder architectures we will try in our experiments. Please see *Figure
    7.5* for a reference regarding the experiments we will perform:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所介绍的所有新内容。下一步将解释我们在实验中尝试的自编码器架构。请参见*图7.5*，以作为我们将进行的实验的参考：
- en: '![](img/9ded95c1-d8fa-45fe-aa65-6c97a457ad43.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ded95c1-d8fa-45fe-aa65-6c97a457ad43.png)'
- en: Figure 7.5 – Different autoencoder configurations to demonstrate the difference
    in the quality of the latent representations
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 不同的自编码器配置，展示潜在表示质量的差异
- en: In the figure, you will notice that we are using the abstracted representation
    of an autoencoder, and on the right side of the *Figure 7.5* are the different
    layers that each autoencoder architecture will use. The first architecture shown
    corresponds to the code shown in this section. That is, the code shows an autoencoder
    with encoding layers of 392, 28, 10, and 2 neurons, respectively; while the decoding
    layers contain 10, 28, 392, and 784 neurons, respectively. The next model on the
    right contains the same layers except for removing the pair of layers corresponding
    to 392 neurons, and so on.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，你会注意到我们使用的是自编码器的抽象表示，而*图7.5*的右侧展示了每个自编码器架构将使用的不同层。第一个架构对应于本节中展示的代码。也就是说，代码展示了一个自编码器，其中编码层分别有392、28、10和2个神经元；解码层则分别有10、28、392和784个神经元。右侧的下一个模型包含相同的层，只是删除了对应于392个神经元的那一对层，依此类推。
- en: The last autoencoder model only contains two layers, one encoding (two neurons)
    and one decoding (784 neurons). At this point, you should be able to modify the
    Python code to remove the necessary layers and replicate the models depicted in
    *Figure 7.5*. The next step is to train the models in *Figure 7.5* and visualize
    the quality of the output.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个自编码器模型仅包含两层，一层编码（两个神经元）和一层解码（784个神经元）。此时，你应该能够修改Python代码，删除必要的层，并复制*图7.5*中展示的模型。下一步是训练*图7.5*中的模型，并可视化输出质量。
- en: Training and visualization
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与可视化
- en: 'The execution of `autoencoder.fit()` for 100 epochs produces a viable model
    that can easily encode into two dimensions as specified. Looking closely into
    the loss function during training, we can observe that it converges properly:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`autoencoder.fit()`进行100轮训练，生成一个可行的模型，能够轻松地按要求将数据编码为二维。仔细观察训练过程中的损失函数，我们可以看到它已经正确收敛：
- en: '![](img/1eaad2b2-ec72-436a-a4d6-7551d485dc5a.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1eaad2b2-ec72-436a-a4d6-7551d485dc5a.png)'
- en: Figure 7.6 – Loss function monitoring during the training of a four-layer autoencoder
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 四层自编码器训练过程中损失函数的监控
- en: 'Once the model has been trained successfully, we can retrieve the encoded representation
    using the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型成功训练完成，我们可以使用以下方法来提取编码后的表示：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We are using the test set, `x_test`. This encoding will encode into two dimensions,
    as specified, and will produce a latent representation in the range [-1, 1], as
    specified. Similarly, we can always take the test set and use the autoencoder
    to compress it and reconstruct it to see how similar the input is to the reconstruction.
    We do so with this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是测试集`x_test`。此编码将按要求压缩为二维，并生成一个范围为[-1, 1]的潜在表示。类似地，我们总是可以拿测试集，使用自编码器对其进行压缩和重构，看看输入和重构结果有多相似。我们可以用以下方式进行：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Before we look into the latent representations learned from MNIST, we can look
    into the reconstruction quality as a way of assessing the quality of the learned
    model. *Figure 7.7* shows the reconstruction results (in `x_hat`) using *Figure
    7.4* as a reference to the input provided to each model. The figure is broken
    down into four parts, each part corresponding to the models described in *Figure
    7.5*: a) the model with eight layers, b) the model with six layers, c) the model
    with four layers, and d) the model with two layers:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究 MNIST 学到的潜在表示之前，可以先看看重建质量，作为评估学习模型质量的一种方式。*图 7.7* 显示了使用*图 7.4*作为参考的每个模型输入的重建结果（在`x_hat`中）。该图分为四部分，每一部分对应*图
    7.5*中描述的模型：a) 八层模型，b) 六层模型，c) 四层模型，d) 两层模型：
- en: '![](img/ba164838-ce99-4c8b-9915-f1d7c9312e19.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba164838-ce99-4c8b-9915-f1d7c9312e19.png)'
- en: 'Figure 7.7 – Autoencoder reconstruction for the models in Figure 7.5: a) the
    model with eight layers, b) the model with six layers, c) the model with four layers,
    and d) the model with two layers'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 图 7.5 中模型的自编码器重建：a) 八层模型，b) 六层模型，c) 四层模型，d) 两层模型
- en: 'From *Figure 7.7.a*, we can see that the model with eight layers (392, 28,
    10, 2, 10, 28, 392, 784) is capable of producing generally good reconstructions
    with the exception of the numbers 4 and 9\. It is evident that both digits are
    closely related (visually) and the autoencoder has some difficulty distinguishing
    clearly between the two digits. To further explore this observation, we can visualize
    test data in the latent space (in `encdd`), which is depicted in *Figure 7.8*:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 7.7.a*中，我们可以看到，具有八层（392、28、10、2、10、28、392、784）的模型在重建结果上表现普遍良好，除了数字 4 和 9。显然，这两个数字在视觉上非常相似，因此自编码器很难清楚地区分这两个数字。为了进一步探索这一观察，我们可以将测试数据在潜在空间中可视化（在`encdd`中），如*图
    7.8*所示：
- en: '![](img/0d99e2d5-f02f-416d-bd0a-38ec29e7ef4d.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d99e2d5-f02f-416d-bd0a-38ec29e7ef4d.png)'
- en: Figure 7.8 – Four-layer encoder using MNIST test data
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 使用 MNIST 测试数据的四层编码器
- en: The overlap between digits four and nine is evident in the latent space produced
    by the autoencoder. However, most of the other groups of digits have relatively
    clear separate clusters. *Figure 7.8* also explains the natural closeness of other
    numbers that look like each other; for example, one and seven appear to be close
    to each other, as well as zero and six, and so do three and eight. However, numbers
    that do not look alike are in opposite sections of the latent space – for example,
    zero and one.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器生成的潜在空间中，数字四和九之间的重叠非常明显。然而，大多数其他数字组之间有相对清晰的独立簇。*图 7.8* 也解释了其他相似数字之间的自然接近性；例如，一和七看起来彼此接近，零和六也是如此，三和八也如此。然而，那些看起来不相似的数字则位于潜在空间的对立部分——例如，零和一。
- en: '*Figure 7.9* depicts the three-layer autoencoder that removed the layer with
    392 neurons and left a 28, 10, 2 neuron architecture. Clearly, the quality of
    the latent space is significantly reduced, although some of the major structure
    is consistent. That is, zero and one are on opposite sides, and other numbers
    that look alike are closer together; the overlap is greater in comparison to *Figure
    7.8*. The quality of this three-layer autoencoder is consistently lower, as shown
    in *Figure 7.7.b*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.9* 描述了移除了 392 个神经元层的三层自编码器，保留了 28、10、2 神经元的架构。显然，潜在空间的质量显著降低，尽管一些主要结构保持一致。也就是说，零和一位于对立面，其他看起来相似的数字更为接近；与*图
    7.8*相比，重叠度更大。这个三层自编码器的质量持续较低，如*图 7.7.b*所示：'
- en: '![](img/4a3f6fa6-12e8-472d-9309-baa2396e9346.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a3f6fa6-12e8-472d-9309-baa2396e9346.png)'
- en: Figure 7.9 – Three-layer encoder using MNIST test data
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 使用 MNIST 测试数据的三层编码器
- en: 'In *Figure 7.10*, we can observe the results of the two-layer autoencoder with
    10 and 2 neurons, which again has a greater digit overlap than the previous autoencoders;
    this is also evident in the poor reconstruction shown in *Figure 7.7.c*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.10*中，我们可以观察到具有 10 和 2 个神经元的两层自编码器的结果，与之前的自编码器相比，数字重叠度更大；这一点在*图 7.7.c*中表现得尤为明显，重建效果较差：
- en: '![](img/b60d2d57-df05-46f3-ac6a-8fbded4dfb34.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b60d2d57-df05-46f3-ac6a-8fbded4dfb34.png)'
- en: Figure 7.10 – Two-layer encoder using MNIST test data
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 使用 MNIST 测试数据的两层编码器
- en: 'Finally, *Figure 7.11* shows the latent space of the one-layer autoencoder.
    Clearly, this is a terrible idea. Just consider what we are asking the autoencoder
    to do: we are asking just two neurons to find a way to look at an entire image
    of a digit and find a way (learning weights [![](img/e2c8f2cb-9a89-4d71-9848-c755d1cd2c58.png)])
    to map all images to two dimensions. It is just not possible to do that. Logically,
    if we only have one layer, we would want at least 10 neurons to adequately model
    each of the 10 digits in MNIST:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*图 7.11* 显示了单层自编码器的潜在空间。显然，这是一个糟糕的主意。只要考虑我们要求自编码器做什么：我们要求仅仅两个神经元找到一种方式来查看数字的整个图像，并找到一种方法（学习权重 [![](img/e2c8f2cb-9a89-4d71-9848-c755d1cd2c58.png)]）将所有图像映射到二维空间。这根本不可能做到。从逻辑上讲，如果我们只有一层，我们至少需要
    10 个神经元才能充分建模 MNIST 中的 10 个数字：
- en: '![](img/73bc6359-d02c-4f03-b66e-0f8a11f34068.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73bc6359-d02c-4f03-b66e-0f8a11f34068.png)'
- en: Figure 7.11 – One-layer encoder using MNIST test data – a bad idea
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 使用 MNIST 测试数据的单层编码器 – 一个糟糕的主意
- en: Close observation of *Figure 7.11* also makes it clear that the scale of the
    axes varies just slightly; this can be interpreted as the encoder not being able
    to separate into different regions of the latent space all the digits of MNIST.
    In practice, please do not use autoencoders with a few layers with a few neurons,
    unless the dimensionality of the input space is already very low. Autoencoders
    might be more successful in deep configurations, as shown in this experiment.
    Learn more about deep autoencoders in the next chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *图 7.11* 的仔细观察也表明，轴的尺度变化略微不同；这可以解释为编码器无法将所有 MNIST 数字分离到潜在空间的不同区域。实际上，除非输入空间的维度已经非常低，否则请不要使用层数和神经元很少的自编码器。正如本实验所示，自编码器在深层配置中可能更成功。在下一章中将学习更多关于深度自编码器的内容。
- en: Ethical implications of unsupervised learning
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习的伦理影响
- en: Unsupervised learning, such as what we see happening in the autoencoder we have
    been exploring so far, is not magical. It is well established and has very rigorous
    boundaries that are known and pre-defined. It does not have the capability of
    learning new things outside the limitations given by the data. Remember, unsupervised
    learning is **passive** learning as explained in the introductory section of this
    chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习，比如我们在之前探讨的自编码器中看到的，并不是魔法。它是成熟的，并且有非常严格的边界，这些边界是已知且预先定义的。它没有能力学习数据所给定限制之外的新事物。记住，正如本章引言部分所解释的，无监督学习是**被动**学习。
- en: However, even the most robust of unsupervised learning models have ethical risks
    associated with them. One of the major problems is that they create difficulties
    when dealing with outliers or data that may contain edge cases. For example, say
    that there is a large amount of data for IT recruitment, which includes years
    of experience, current salary, and programming languages that a candidate knows.
    If the data mostly contains data about candidates with the same programming language
    experience, and only a few know Python, then those candidates that know the Python
    language might be placed into a boundary or a region that might be difficult to
    visualize clearly, because the model has learned that since Python is an infrequent
    language, it may not be relevant in terms of data compression, dimensionality
    reduction, or data visualization. Furthermore, consider what would happen if 5
    years later, you used that same model despite there being newer programming languages
    that were not known about during training 5 years ago. The model may or may not
    map such information properly for visualization or data compression applications.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使是最强大的无监督学习模型，也存在伦理风险。一个主要的问题是，它们在处理异常值或可能包含边缘案例的数据时会造成困难。例如，假设有大量关于 IT
    招聘的数据，其中包含候选人的工作经验、当前薪资以及掌握的编程语言。如果这些数据大部分都是关于具有相同编程语言经验的候选人，只有少数人懂 Python，那么那些懂
    Python 的候选人可能会被放置到一个难以清晰可视化的边界或区域，因为模型已经学会了，Python 作为一种不常见的语言，可能在数据压缩、降维或数据可视化方面不相关。此外，想象一下，5
    年后，即使出现了在 5 年前训练时未知的编程语言，你仍然使用那个模型。这时模型可能无法正确映射这些信息，以供可视化或数据压缩应用使用。
- en: You must be very careful about what data is used to train an autoencoder, and
    having a variety of cases is important for the reliability of any model. If there
    is not enough diversity in the data, the autoencoder will be biased toward learning
    only from one input space. Imagine that you train an autoencoder on images of
    the 10 MNIST digits from earlier – you would not expect the autoencoder to perform
    properly on images of cats; that would be a mistake and would likely produce unwanted
    results. When using, for example, images of people, you must make sure that there
    is enough variety and diversity in the training data to produce proper training
    and a robust model that does not perform incorrectly for images of people that
    were not considered part of the training data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须非常小心选择用于训练自编码器的数据，且拥有多样化的案例对于任何模型的可靠性都至关重要。如果数据缺乏多样性，自编码器将倾向于只从一个输入空间中学习。假设你在先前提到的10个MNIST数字的图像上训练自编码器——你不会期望自编码器在猫的图像上表现良好；那样做是错误的，并且很可能产生不想要的结果。例如，当使用人的图像时，你必须确保训练数据中有足够的多样性和广泛性，以便进行适当的训练，并且模型能够对未包含在训练数据中的人类图像做出正确的反应。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter showed that autoencoders are very simple models that can be used
    to encode and decode data for different purposes, such as data compression, data
    visualization, and simply finding latent spaces where only important features
    are preserved. We showed that the number of neurons and the number of layers in
    the autoencoder are important for the success of the model. Deeper (more layers)
    and wider (more neurons) traits are often ingredients for good models, even if
    that leads to slower training times.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了自编码器是非常简单的模型，可以用于对数据进行编码和解码，应用于不同的目的，比如数据压缩、数据可视化，以及仅保留重要特征的潜在空间的发现。我们展示了自编码器中神经元的数量和层数对模型成功的重要性。更深（更多层）和更宽（更多神经元）的特征通常是好模型的关键，尽管这可能导致训练时间变慢。
- en: 'At this point, you should know the difference between supervised and unsupervised
    learning in terms of passive learning. You should also feel comfortable implementing
    the two basic components of an autoencoder: the encoder and the decoder. Similarly,
    you should be able to modify the architecture of an autoencoder to fine-tune it
    to achieve better performance. Taking the example we discussed in this chapter,
    you should be able to apply an autoencoder to a dimensionality reduction problem
    or to a data visualization problem. Also, you should be considering the risks
    and responsibilities associated with unsupervised learning algorithms when it
    comes to the data used to train them.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你应该知道监督学习和无监督学习在被动学习方面的区别。你也应该能够熟练地实现自编码器的两个基本组成部分：编码器和解码器。同样，你应该能够修改自编码器的架构，微调它以实现更好的性能。以本章中讨论的例子为例，你应该能够将自编码器应用于降维问题或数据可视化问题。此外，你还应该考虑使用无监督学习算法时与训练数据相关的风险和责任。
- en: '[Chapter 8](6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml), *Deep Autoencoders,* will
    continue with deeper and wider autoencoder architectures that go beyond the introduction
    we covered in this chapter. The next chapter will introduce the idea of deep belief
    networks and the significance of this type of deep unsupervised learning. It will
    explain such concepts by introducing deep autoencoders and contrasting them with
    shallow autoencoders. The chapter will also give important advice on optimizing
    the number of neurons and layers to maximize performance.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml)，*深度自编码器*，将继续介绍比本章所涉及的更深更宽的自编码器架构。下一章将介绍深度置信网络的概念及其在深度无监督学习中的重要性。通过介绍深度自编码器并与浅层自编码器进行对比，来解释这些概念。该章还将提供关于优化神经元数量和层数以最大化性能的重要建议。'
- en: Questions and answers
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Is overfitting a bad thing for an autoencoder? **'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过拟合对自编码器来说是坏事吗？**'
- en: Actually, no. You want the autoencoder to overfit! That is, you want it to exactly
    replicate the input data in the output. However, there is a caveat. Your dataset
    must be really large in comparison to the size of the model; otherwise, the memorization
    of the data will prevent the model from generalizing to unseen data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其实不是。你希望自编码器出现过拟合！也就是说，你希望它能够精确地在输出中重现输入数据。然而，这里有一个警告。你的数据集必须相对于模型的大小足够大；否则，数据的记忆将会阻碍模型在未见过的数据上的泛化能力。
- en: '**Why did we use two neurons in the encoder''s last layer?**'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么我们在编码器的最后一层使用了两个神经元？**'
- en: For visualization purposes only. The two-dimensional latent space produced by
    the two neurons allows us to easily visualize the data in the latent space. In
    the next chapter, we will use other configurations that do not necessarily have
    a two-dimensional latent space.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用于可视化目的。由两个神经元产生的二维潜在空间使我们能够轻松地在潜在空间中可视化数据。在下一章中，我们将使用其他配置，这些配置不一定具有二维潜在空间。
- en: '**What is so cool about autoencoders again?**'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自编码器到底有什么酷的地方？**'
- en: They are simple neural models that learn without a teacher (unsupervised). They
    are not biased toward learning specific labels (classes). They learn about the
    world of data through iterative observations, aiming to learn the most representative
    and relevant features. They can be used as feature extraction models, but we will
    discuss more about that in future chapters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是简单的神经网络模型，无需教师进行学习（无监督学习）。它们不会偏向于学习特定的标签（类别）。通过迭代观察，它们学习数据的世界，旨在学习最具代表性和相关性的特征。它们可以用作特征提取模型，但我们将在未来的章节中进一步讨论这一点。
- en: References
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Amari, S. I. (1993). Backpropagation and stochastic gradient descent method. *Neurocomputing*,
    5(4-5), 185-196.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari, S. I. (1993). 反向传播与随机梯度下降方法。*Neurocomputing*, 5(4-5), 185-196.
- en: Wang, Y., Yao, H., & Zhao, S. (2016). Auto-encoder based dimensionality reduction.
    *Neurocomputing*, 184, 232-242.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, Y., Yao, H., & Zhao, S. (2016). 基于自编码器的降维方法。*Neurocomputing*, 184, 232-242.
- en: Petscharnig, S., Lux, M., & Chatzichristofis, S. (2017). Dimensionality reduction
    for image features using deep learning and autoencoders. In *Proceedings of the
    15th International Workshop on Content-Based Multimedia Indexing* (p. 23). ACM.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petscharnig, S., Lux, M., & Chatzichristofis, S. (2017). 使用深度学习和自编码器进行图像特征的降维。发表于*第十五届国际基于内容的多媒体索引研讨会*（第23页）。ACM。
- en: Creswell, A., Arulkumaran, K., & Bharath, A. A. (2017). On denoising autoencoders
    trained to minimize binary cross-entropy. *arXiv preprint* arXiv:1708.08487.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Creswell, A., Arulkumaran, K., & Bharath, A. A. (2017). 关于通过最小化二元交叉熵训练去噪自编码器的研究。*arXiv
    预印本* arXiv:1708.08487.
- en: 'Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma, D. P., & Ba, J. (2014). Adam：一种随机优化方法。arXiv 预印本 arXiv:1412.6980.
