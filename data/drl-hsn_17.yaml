- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Black-Box Optimizations in RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¸­çš„é»‘ç›’ä¼˜åŒ–
- en: 'In this chapter, we will change our perspective on reinforcement learning (RL)
    training again and switch to the so-called black-box optimizations. These methods
    are at least a decade old, but recently, several research studies were conducted
    that showed their applicability to large-scale RL problems and their competitiveness
    with the value iteration and policy gradient methods. Despite their age, this
    family of methods is still more efficient in some situations. In particular, this
    chapter will cover two examples of black-box optimization methods:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å†æ¬¡æ”¹å˜å¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„çœ‹æ³•ï¼Œè½¬å‘æ‰€è°“çš„é»‘ç›’ä¼˜åŒ–æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•è‡³å°‘å·²æœ‰åå¹´å†å²ï¼Œä½†æœ€è¿‘è¿›è¡Œçš„ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œå®ƒä»¬åœ¨å¤§è§„æ¨¡RLé—®é¢˜ä¸­çš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸”ä¸ä»·å€¼è¿­ä»£å’Œç­–ç•¥æ¢¯åº¦æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚å°½ç®¡å®ƒä»¬å·²ç»æœ‰äº›å¹´å¤´ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™ç±»æ–¹æ³•ä»ç„¶æ›´ä¸ºé«˜æ•ˆã€‚æœ¬ç« å°†ä»‹ç»ä¸¤ç§é»‘ç›’ä¼˜åŒ–æ–¹æ³•çš„ä¾‹å­ï¼š
- en: Evolution strategies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿›åŒ–ç­–ç•¥
- en: Genetic algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é—ä¼ ç®—æ³•
- en: Black-box methods
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é»‘ç›’æ–¹æ³•
- en: To begin with, letâ€™s discuss the whole family of black-box methods and how it
    differs from what weâ€™ve covered so far. Black-box optimization methods are the
    general approach to the optimization problem, when you treat the objective that
    youâ€™re optimizing as a black box, without any assumptions about the differentiability,
    the value function, the smoothness of the objective, and so on. The only requirement
    that those methods expose is the ability to calculate the fitness function, which
    should give us the measure of suitability of a particular instance of the optimized
    entity at hand. One of the simplest examples in this family is random search,
    which is when you randomly sample the thing youâ€™re looking for (in the case of
    RL, itâ€™s the policy, Ï€(a|s)), check the fitness of this candidate, and if the
    result is good enough (according to some reward criteria), then youâ€™re done. Otherwise,
    you repeat the process again and again. Despite the simplicity and even naivety
    of this approach, especially when compared to the sophisticated methods that youâ€™ve
    seen so far, this is a good example to illustrate the idea of black-box methods.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹é»‘ç›’æ–¹æ³•çš„æ•´ä½“å®¶æ—ï¼Œä»¥åŠå®ƒä¸æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„å†…å®¹çš„åŒºåˆ«ã€‚é»‘ç›’ä¼˜åŒ–æ–¹æ³•æ˜¯ä¼˜åŒ–é—®é¢˜çš„é€šç”¨æ–¹æ³•ï¼Œå®ƒå°†ä½ æ­£åœ¨ä¼˜åŒ–çš„ç›®æ ‡è§†ä¸ºé»‘ç›’ï¼Œä¸å¯¹ç›®æ ‡çš„å¯å¾®æ€§ã€ä»·å€¼å‡½æ•°ã€ç›®æ ‡çš„å¹³æ»‘æ€§ç­‰åšä»»ä½•å‡è®¾ã€‚å”¯ä¸€çš„è¦æ±‚æ˜¯è¿™äº›æ–¹æ³•èƒ½å¤Ÿè®¡ç®—é€‚åº”åº¦å‡½æ•°ï¼Œè¿™åº”è¯¥ä¸ºæˆ‘ä»¬æä¾›ä¼˜åŒ–å®ä½“çš„ç‰¹å®šå®ä¾‹çš„é€‚åº”åº¦åº¦é‡ã€‚è¿™ä¸ªå®¶æ—ä¸­æœ€ç®€å•çš„ä¸€ä¸ªä¾‹å­æ˜¯éšæœºæœç´¢ï¼Œå®ƒæ˜¯æŒ‡ä½ éšæœºé€‰æ‹©ä½ è¦å¯»æ‰¾çš„å¯¹è±¡ï¼ˆåœ¨RLä¸­æ˜¯ç­–ç•¥Ï€(a|s)ï¼‰ï¼Œæ£€æŸ¥è¯¥å€™é€‰å¯¹è±¡çš„é€‚åº”åº¦ï¼Œå¦‚æœç»“æœè¶³å¤Ÿå¥½ï¼ˆæ ¹æ®æŸäº›å¥–åŠ±æ ‡å‡†ï¼‰ï¼Œé‚£ä¹ˆä½ å°±å®Œæˆäº†ã€‚å¦åˆ™ï¼Œä½ ä¼šä¸€éåˆä¸€éåœ°é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚å°½ç®¡è¿™ç§æ–¹æ³•ç®€å•ï¼Œç”šè‡³æœ‰äº›å¤©çœŸï¼Œç‰¹åˆ«æ˜¯ä¸åˆ°ç›®å‰ä¸ºæ­¢ä½ æ‰€çœ‹åˆ°çš„å¤æ‚æ–¹æ³•ç›¸æ¯”ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œèƒ½è¯´æ˜é»‘ç›’æ–¹æ³•çš„æ€æƒ³ã€‚
- en: 'Furthermore, with some modifications, as you will see shortly, this simple
    approach can be compared in terms of efficiency and the quality of the resulting
    policies to the deep Q-network (DQN) and policy gradient methods. In addition
    to that, black-box methods have several very appealing properties:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œé€šè¿‡ä¸€äº›ä¿®æ”¹ï¼Œæ­£å¦‚ä½ å¾ˆå¿«ä¼šçœ‹åˆ°çš„ï¼Œè¿™ç§ç®€å•çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œç”Ÿæˆçš„ç­–ç•¥è´¨é‡æ–¹é¢ï¼Œå¯ä»¥ä¸æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰å’Œç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›¸æ¯”è¾ƒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œé»‘ç›’æ–¹æ³•è¿˜æœ‰ä¸€äº›éå¸¸å¸å¼•äººçš„ç‰¹æ€§ï¼š
- en: They are at least two times faster than gradient-based methods, as we donâ€™t
    need to perform the backpropagation step to obtain the gradients.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä»¬è‡³å°‘æ¯”åŸºäºæ¢¯åº¦çš„æ–¹æ³•å¿«ä¸¤å€ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æ‰§è¡Œåå‘ä¼ æ’­æ­¥éª¤æ¥è·å¾—æ¢¯åº¦ã€‚
- en: There are very few assumptions about the optimized objective and the policy
    that are treated as a black box. Traditional methods struggle with situations
    when the reward function is non-smooth or the policy contains steps with random
    choice. All of this is not an issue for black-box methods, as they donâ€™t expect
    much from the black-box internals.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºä¼˜åŒ–ç›®æ ‡å’Œä½œä¸ºé»‘ç›’å¤„ç†çš„ç­–ç•¥ï¼Œå‡ ä¹æ²¡æœ‰ä»»ä½•å‡è®¾ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨å¥–åŠ±å‡½æ•°ä¸å¹³æ»‘æˆ–ç­–ç•¥ä¸­åŒ…å«éšæœºé€‰æ‹©æ­¥éª¤æ—¶ä¼šé‡åˆ°å›°éš¾ã€‚è€Œè¿™ä¸€åˆ‡å¯¹äºé»‘ç›’æ–¹æ³•æ¥è¯´éƒ½ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬å¯¹é»‘ç›’çš„å†…éƒ¨å®ç°æ²¡æœ‰å¤ªå¤šè¦æ±‚ã€‚
- en: The methods can generally be parallelized very well. For example, the aforementioned
    random search can easily scale up to thousands of central processing units (CPUs)
    or graphics processing units (GPUs) working in parallel, without any dependency
    on each other. This is not the case for DQN or policy gradient methods, when you
    need to accumulate the gradients and propagate the current policy to all parallel
    workers, which decreases the parallelism.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•é€šå¸¸å¯ä»¥å¾ˆå¥½åœ°å¹¶è¡ŒåŒ–ã€‚ä¾‹å¦‚ï¼Œä¸Šè¿°çš„éšæœºæœç´¢å¯ä»¥è½»æ¾æ‰©å±•åˆ°æ•°åƒä¸ªä¸­å¤®å¤„ç†å•å…ƒï¼ˆCPUï¼‰æˆ–å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰å¹¶è¡Œå·¥ä½œï¼Œä¸”å½¼æ­¤ä¹‹é—´æ²¡æœ‰ä»»ä½•ä¾èµ–å…³ç³»ã€‚å¯¹äº
    DQN æˆ–ç­–ç•¥æ¢¯åº¦æ–¹æ³•è€Œè¨€ï¼Œæƒ…å†µåˆ™ä¸åŒï¼Œå› ä¸ºéœ€è¦ç§¯ç´¯æ¢¯åº¦å¹¶å°†å½“å‰ç­–ç•¥ä¼ æ’­ç»™æ‰€æœ‰å¹¶è¡Œå·¥ä½œè€…ï¼Œè¿™ä¼šé™ä½å¹¶è¡Œæ€§ã€‚
- en: The downside of the preceding is usually lower sample efficiency. In particular,
    the naÃ¯ve random search of the policy, parameterized with the neural network (NN)
    with half a million parameters, has a very low probability of succeeding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å‰è¿°æ–¹æ³•çš„ç¼ºç‚¹é€šå¸¸æ˜¯æ ·æœ¬æ•ˆç‡è¾ƒä½ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨å…·æœ‰äº”åä¸‡ä¸ªå‚æ•°çš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰è¿›è¡Œçš„å¤©çœŸéšæœºæœç´¢ç­–ç•¥ï¼ŒæˆåŠŸçš„æ¦‚ç‡éå¸¸ä½ã€‚
- en: Evolution strategies
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›åŒ–ç­–ç•¥
- en: One subset of black-box optimization methods is called evolution strategies
    (ES), and it was inspired by the evolution process. With ES, the most successful
    individuals have the highest influence on the overall direction of the search.
    There are many different methods that fall into this class, and in this chapter,
    we will consider the approach taken by the OpenAI researchers Salimans et al.
    in their paper, Evolution strategies as a scalable alternative to reinforcement
    learning [[Sal+17](#)], published in March 2017.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é»‘ç›’ä¼˜åŒ–æ–¹æ³•çš„ä¸€ç±»è¢«ç§°ä¸ºè¿›åŒ–ç­–ç•¥ï¼ˆESï¼‰ï¼Œå®ƒçš„çµæ„Ÿæ¥æºäºè¿›åŒ–è¿‡ç¨‹ã€‚åœ¨ ES ä¸­ï¼Œæœ€æˆåŠŸçš„ä¸ªä½“å¯¹æœç´¢çš„æ•´ä½“æ–¹å‘æœ‰æœ€å¤§çš„å½±å“ã€‚è¿™ä¸ªç±»åˆ«ä¸‹æœ‰è®¸å¤šä¸åŒçš„æ–¹æ³•ï¼Œåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®º
    OpenAI ç ”ç©¶äººå‘˜ Salimans ç­‰äººåœ¨ä»–ä»¬çš„è®ºæ–‡ã€Šè¿›åŒ–ç­–ç•¥ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•æ›¿ä»£æ–¹æ³•ã€‹[[Sal+17](#)]ä¸­æå‡ºçš„æ–¹æ³•ï¼Œè¯¥è®ºæ–‡äº2017å¹´3æœˆå‘å¸ƒã€‚
- en: The underlying idea of ES methods is that on every iteration, we perform random
    perturbation of our current policy parameters and evaluate the resulting policy
    fitness function. Then, we adjust the policy weights proportionally to the relative
    fitness function value.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ES æ–¹æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯åœ¨æ¯æ¬¡è¿­ä»£æ—¶ï¼Œæˆ‘ä»¬å¯¹å½“å‰çš„ç­–ç•¥å‚æ•°è¿›è¡Œéšæœºæ‰°åŠ¨ï¼Œå¹¶è¯„ä¼°ç”±æ­¤äº§ç”Ÿçš„ç­–ç•¥é€‚åº”åº¦å‡½æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®ç›¸å¯¹çš„é€‚åº”åº¦å‡½æ•°å€¼è°ƒæ•´ç­–ç•¥æƒé‡ã€‚
- en: The concrete method used by Salimans et al. is called covariance matrix adaptation
    evolution strategy (CMA-ES), in which the perturbation performed is the random
    noise sampled from the normal distribution with the zero mean and identity variance.
    Then, we calculate the fitness function of the policy with weights equal to the
    weights of the original policy plus the scaled noise. Next, according to the obtained
    value, we adjust the original policy weights by adding the noise multiplied by
    the fitness function value, which moves our policy toward weights with a higher
    value of the fitness function. To improve the stability, the update of the weights
    is performed by averaging the batch of such steps with different random noise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Salimans ç­‰äººä½¿ç”¨çš„å…·ä½“æ–¹æ³•è¢«ç§°ä¸ºåæ–¹å·®çŸ©é˜µé€‚åº”è¿›åŒ–ç­–ç•¥ï¼ˆCMA-ESï¼‰ï¼Œå…¶ä¸­æ‰§è¡Œçš„æ‰°åŠ¨æ˜¯ä»å‡å€¼ä¸ºé›¶ã€æ–¹å·®ä¸ºå•ä½çŸ©é˜µçš„æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·çš„éšæœºå™ªå£°ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—å…·æœ‰æƒé‡ç­‰äºåŸå§‹ç­–ç•¥æƒé‡åŠ ä¸Šç¼©æ”¾å™ªå£°çš„ç­–ç•¥çš„é€‚åº”åº¦å‡½æ•°å€¼ã€‚æ¥ä¸‹æ¥ï¼Œæ ¹æ®å¾—åˆ°çš„å€¼ï¼Œæˆ‘ä»¬é€šè¿‡å°†å™ªå£°ä¹˜ä»¥é€‚åº”åº¦å‡½æ•°å€¼æ¥è°ƒæ•´åŸå§‹ç­–ç•¥æƒé‡ï¼Œè¿™æ ·å¯ä»¥ä½¿æˆ‘ä»¬çš„ç­–ç•¥æœç€å…·æœ‰æ›´é«˜é€‚åº”åº¦å‡½æ•°å€¼çš„æƒé‡æ–¹å‘ç§»åŠ¨ã€‚ä¸ºäº†æé«˜ç¨³å®šæ€§ï¼Œæƒé‡æ›´æ–°æ˜¯é€šè¿‡å¯¹å…·æœ‰ä¸åŒéšæœºå™ªå£°çš„å¤šä¸ªæ­¥éª¤æ‰¹æ¬¡è¿›è¡Œå¹³å‡æ¥å®Œæˆçš„ã€‚
- en: 'More formally, this method could be expressed as the following sequence of
    steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ­£å¼åœ°è¯´ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥è¡¨è¾¾ä¸ºä»¥ä¸‹æ­¥éª¤åºåˆ—ï¼š
- en: Initialize the learning rate, Î±, the noise standard deviation, Ïƒ, and the initial
    policy parameters, ğœƒ[0].
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–å­¦ä¹ ç‡ Î±ã€å™ªå£°æ ‡å‡†å·® Ïƒ å’Œåˆå§‹ç­–ç•¥å‚æ•° ğœƒ[0]ã€‚
- en: 'For t = 0,1,â€¦ perform:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹ t = 0,1,â€¦ æ‰§è¡Œï¼š
- en: 'The sample batch of noise with the shape of the weights from the normal distribution
    with zero mean and variance of one: ğœ–[1],â€¦,ğœ–[n] âˆ¼ğ’©(0,I)'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å™ªå£°æ ·æœ¬æ‰¹æ¬¡ï¼Œå…¶å½¢çŠ¶ä¸æ¥è‡ªå‡å€¼ä¸ºé›¶ã€æ–¹å·®ä¸ºä¸€çš„æ­£æ€åˆ†å¸ƒçš„æƒé‡ç›¸åŒï¼šğœ–[1],â€¦,ğœ–[n] âˆ¼ğ’©(0,I)
- en: Compute returns F[i] = F(ğœƒ[t] + Ïƒğœ–[i]) for i = 1,â€¦,n
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—å›æŠ¥ F[i] = F(ğœƒ[t] + Ïƒğœ–[i])ï¼Œå…¶ä¸­ i = 1,â€¦,n
- en: 'Update weights:'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ›´æ–°æƒé‡ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq69.png)'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq69.png)'
- en: This algorithm is the core of the method presented in the paper, but, as usual
    in the RL domain, the method alone is not enough to obtain good results. So, the
    paper includes several tweaks to improve the method, although the core is the
    same.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•æ˜¯è®ºæ–‡ä¸­æå‡ºæ–¹æ³•çš„æ ¸å¿ƒï¼Œä½†åƒå¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­çš„å¸¸è§æƒ…å†µä¸€æ ·ï¼Œå•é è¿™ä¸ªæ–¹æ³•ä¸è¶³ä»¥è·å¾—è‰¯å¥½çš„ç»“æœã€‚å› æ­¤ï¼Œè®ºæ–‡ä¸­åŒ…å«äº†å‡ ä¸ªè°ƒæ•´ä»¥æ”¹å–„æ–¹æ³•ï¼Œå°½ç®¡æ ¸å¿ƒæ–¹æ³•ä¿æŒä¸å˜ã€‚
- en: Implementing ES on CartPole
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ CartPole ä¸Šå®ç° ES
- en: 'Letâ€™s implement and test the method from the paper on our fruit fly environment:
    CartPole. Youâ€™ll find the complete example in Chapter17/01_cartpole_es.py.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„æœè‡ç¯å¢ƒä¸­å®ç°å¹¶æµ‹è¯•è®ºæ–‡ä¸­çš„æ–¹æ³•ï¼šå€’ç«‹æ‘†ï¼ˆCartPoleï¼‰ã€‚ä½ å¯ä»¥åœ¨ Chapter17/01_cartpole_es.py æ–‡ä»¶ä¸­æ‰¾åˆ°å®Œæ•´çš„ä¾‹å­ã€‚
- en: In this example, we will use the single environment to check the fitness of
    the perturbed network weights. Our fitness function will be the undiscounted total
    reward for the episode.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å•ä¸€ç¯å¢ƒæ¥æ£€æŸ¥æ‰°åŠ¨åçš„ç½‘ç»œæƒé‡çš„é€‚åº”åº¦ã€‚æˆ‘ä»¬çš„é€‚åº”åº¦å‡½æ•°å°†æ˜¯è¯¥å›åˆçš„æœªæŠ˜æ‰£æ€»å¥–åŠ±ã€‚
- en: 'We start with the imports:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å¯¼å…¥å¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: From the import statements, you will notice how self-contained our example is.
    Weâ€™re not using PyTorch optimizers, as we donâ€™t perform backpropagation at all.
    In fact, we could avoid using PyTorch completely and work only with NumPy, as
    the only thing we use PyTorch for is to perform a forward pass and calculate the
    networkâ€™s output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¯¼å…¥è¯­å¥ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ä¾‹å­æ˜¯å¤šä¹ˆè‡ªåŒ…å«ã€‚æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ PyTorch ä¼˜åŒ–å™¨ï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬ä¸è¿›è¡Œåå‘ä¼ æ’­ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å®Œå…¨å¯ä»¥é¿å…ä½¿ç”¨ PyTorchï¼Œåªä½¿ç”¨
    NumPyï¼Œå› ä¸ºæˆ‘ä»¬å”¯ä¸€ç”¨ PyTorch çš„åœ°æ–¹å°±æ˜¯æ‰§è¡Œå‰å‘ä¼ æ’­å¹¶è®¡ç®—ç½‘ç»œçš„è¾“å‡ºã€‚
- en: 'Next, we define the hyperparameters:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è¶…å‚æ•°ï¼š
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The number of hyperparameters is also small and includes the following values:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°çš„æ•°é‡ä¹Ÿå¾ˆå°‘ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå€¼ï¼š
- en: 'MAX_BATCH_EPISODES and MAX_BATCH_STEPS: The limit of episodes and steps we
    use for training'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAX_BATCH_EPISODES å’Œ MAX_BATCH_STEPSï¼šç”¨äºè®­ç»ƒçš„å›åˆæ•°å’Œæ­¥éª¤æ•°çš„é™åˆ¶
- en: 'NOISE_STD: The standard deviation, Ïƒ, of the noise used for weight perturbation'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NOISE_STDï¼šç”¨äºæƒé‡æ‰°åŠ¨çš„å™ªå£°æ ‡å‡†å·®ï¼ŒÏƒ
- en: 'LEARNING_RATE: The coefficient used to adjust the weights on the training step'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LEARNING_RATEï¼šç”¨äºè°ƒæ•´è®­ç»ƒæ­¥éª¤ä¸­æƒé‡çš„ç³»æ•°
- en: We alse define a type alias for list of tensors containing weightsâ€™ noises.
    It will simplify the code, as weâ€™ll deal with noise a lot.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªç±»å‹åˆ«åï¼Œè¡¨ç¤ºåŒ…å«æƒé‡å™ªå£°çš„å¼ é‡åˆ—è¡¨ã€‚è¿™æ ·å¯ä»¥ç®€åŒ–ä»£ç ï¼Œå› ä¸ºæˆ‘ä»¬å°†ä¼šé¢‘ç¹å¤„ç†å™ªå£°ã€‚
- en: 'Now letâ€™s check the network:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ£€æŸ¥ç½‘ç»œï¼š
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model weâ€™re using is a simple one-hidden-layer NN, which gives us the action
    to take from the observation. Weâ€™re using PyTorch NN machinery here only for convenience,
    as we need only the forward pass, but it could be replaced by the multiplication
    of matrices and nonlinearities application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹æ˜¯ä¸€ä¸ªç®€å•çš„å•éšå±‚ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ï¼Œå®ƒæ ¹æ®è§‚å¯Ÿç»“æœç»™å‡ºéœ€è¦é‡‡å–çš„è¡ŒåŠ¨ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ PyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å—ä»…æ˜¯ä¸ºäº†æ–¹ä¾¿ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å‰å‘ä¼ æ’­ï¼Œä½†å®ƒä¹Ÿå¯ä»¥é€šè¿‡çŸ©é˜µä¹˜æ³•å’Œéçº¿æ€§åº”ç”¨æ¥æ›¿ä»£ã€‚
- en: 'The evaluate() function plays a full episode using the given policy and returns
    the total reward and the number of steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: evaluate() å‡½æ•°ä½¿ç”¨ç»™å®šçš„ç­–ç•¥è¿›è¡Œå®Œæ•´çš„å›åˆï¼Œå¹¶è¿”å›æ€»å¥–åŠ±å’Œæ­¥æ•°ï¼š
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The reward will be used as a fitness value, while the count of steps is needed
    to limit the amount of time we spend on forming the batch. The action selection
    is performed deterministically by calculating argmax from the network output.
    In principle, we could do the random sampling from the distribution, but weâ€™ve
    already performed the exploration by adding noise to the network parameters, so
    the deterministic action selection is fine here.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å°†ä½œä¸ºé€‚åº”åº¦å€¼ï¼Œè€Œæ­¥æ•°åˆ™ç”¨äºé™åˆ¶æˆ‘ä»¬èŠ±è´¹åœ¨å½¢æˆæ‰¹æ¬¡ä¸Šçš„æ—¶é—´ã€‚è¡ŒåŠ¨é€‰æ‹©é€šè¿‡è®¡ç®—ç½‘ç»œè¾“å‡ºçš„æœ€å¤§å€¼ï¼ˆargmaxï¼‰ä»¥ç¡®å®šæ€§æ–¹å¼è¿›è¡Œã€‚åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ä»åˆ†å¸ƒä¸­è¿›è¡Œéšæœºé‡‡æ ·ï¼Œä½†æˆ‘ä»¬å·²ç»é€šè¿‡å‘ç½‘ç»œå‚æ•°æ·»åŠ å™ªå£°æ¥è¿›è¡Œæ¢ç´¢ï¼Œå› æ­¤åœ¨è¿™é‡Œè¿›è¡Œç¡®å®šæ€§è¡ŒåŠ¨é€‰æ‹©æ˜¯å¯ä»¥çš„ã€‚
- en: 'In the sample_noise() function, we create random noise with zero mean and unit
    variance equal to the shape of our network parameters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ sample_noise() å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å‡å€¼ä¸ºé›¶ã€æ–¹å·®ä¸ºä¸€çš„éšæœºå™ªå£°ï¼Œå…¶å½¢çŠ¶ä¸æˆ‘ä»¬çš„ç½‘ç»œå‚æ•°ç›¸åŒï¼š
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function returns two sets of noise tensors: one with positive noise and
    another with the same random values taken with a negative sign. These two samples
    are later used in a batch as independent samples. This technique is known as mirrored
    sampling and is used to improve the stability of the convergence. In fact, without
    the negative noise, the convergence becomes very unstable because positive noise
    pushes weights in a single direction.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°è¿”å›ä¸¤ç»„å™ªå£°å¼ é‡ï¼šä¸€ç»„æ˜¯æ­£å™ªå£°ï¼Œå¦ä¸€ç»„æ˜¯å–è´Ÿå·åçš„ç›¸åŒéšæœºå€¼ã€‚è¿™ä¸¤ç»„æ ·æœ¬ç¨åä¼šä½œä¸ºç‹¬ç«‹æ ·æœ¬æ”¾å…¥ä¸€ä¸ªæ‰¹æ¬¡ä¸­ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸ºé•œåƒé‡‡æ ·ï¼Œç”¨æ¥æé«˜æ”¶æ•›çš„ç¨³å®šæ€§ã€‚å®é™…ä¸Šï¼Œå¦‚æœæ²¡æœ‰è´Ÿå™ªå£°ï¼Œæ”¶æ•›ä¼šå˜å¾—éå¸¸ä¸ç¨³å®šï¼Œå› ä¸ºæ­£å™ªå£°ä¼šå°†æƒé‡æ¨å‘å•ä¸€æ–¹å‘ã€‚
- en: 'The eval_with_noise() function takes the noise array created by sample_noise()
    and evaluates the network with noise added:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: eval_with_noise() å‡½æ•°æ¥å—ç”± sample_noise() åˆ›å»ºçš„å™ªå£°æ•°ç»„ï¼Œå¹¶åœ¨æ·»åŠ å™ªå£°åè¯„ä¼°ç½‘ç»œï¼š
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To achieve this, we add the noise to the networkâ€™s parameters and call the evaluate
    function to obtain the reward and number of steps taken. After this, we need to
    restore the network weights to their original state, which is completed by loading
    the state dictionary of the network.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†å™ªå£°åŠ åˆ°ç½‘ç»œçš„å‚æ•°ä¸Šï¼Œå¹¶è°ƒç”¨è¯„ä¼°å‡½æ•°æ¥è·å¾—å¥–åŠ±å’Œæ‰€é‡‡å–çš„æ­¥éª¤æ•°ã€‚ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦æ¢å¤ç½‘ç»œçš„æƒé‡åˆ°å…¶åŸå§‹çŠ¶æ€ï¼Œè¿™é€šè¿‡åŠ è½½ç½‘ç»œçš„çŠ¶æ€å­—å…¸æ¥å®Œæˆã€‚
- en: 'The last and the central function of the method is train_step(), which takes
    the batch with noise and respective rewards and calculates the update to the network
    parameters by applying the formula:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹æ³•çš„æœ€åä¸€ä¸ªä¹Ÿæ˜¯æ ¸å¿ƒçš„å‡½æ•°æ˜¯`train_step()`ï¼Œå®ƒæ¥å—å¸¦å™ªå£°å’Œç›¸åº”å¥–åŠ±çš„æ‰¹æ¬¡ï¼Œå¹¶é€šè¿‡åº”ç”¨å…¬å¼è®¡ç®—å¯¹ç½‘ç»œå‚æ•°çš„æ›´æ–°ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq69.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq69.png)'
- en: 'This can be implemented as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ï¼š
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the beginning, we normalize rewards to have zero mean and unit variance,
    which improves the stability of the method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸ºä¸€ï¼Œè¿™æœ‰åŠ©äºæé«˜æ–¹æ³•çš„ç¨³å®šæ€§ã€‚
- en: 'Then, we iterate every pair (noise, reward) in our batch and multiply the noise
    values with the normalized reward, summing together the respective noise for every
    parameter in our policy:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸€å¯¹ï¼ˆå™ªå£°ï¼Œå¥–åŠ±ï¼‰ï¼Œå°†å™ªå£°å€¼ä¸å½’ä¸€åŒ–çš„å¥–åŠ±ç›¸ä¹˜ï¼Œå¹¶å°†æ¯ä¸ªå‚æ•°çš„ç›¸åº”å™ªå£°ç›¸åŠ ï¼š
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As a final step, we use the accumulated scaled noise to adjust the network
    parameters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ç§¯ç´¯çš„ç¼©æ”¾å™ªå£°æ¥è°ƒæ•´ç½‘ç»œå‚æ•°ï¼š
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Technically, what we do here is a gradient ascent, although the gradient was
    not obtained from backpropagation but from the random sampling (also known as
    Monte Carlo sampling). This fact was also demonstrated by Salimans et al., where
    the authors showed that CMA-ES is very similar to the policy gradient methods,
    differing in just the way that we get the gradientsâ€™ estimation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŠ€æœ¯ä¸Šè®²ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„æ˜¯æ¢¯åº¦ä¸Šå‡ï¼Œå°½ç®¡æ¢¯åº¦å¹¶ä¸æ˜¯é€šè¿‡åå‘ä¼ æ’­è·å¾—çš„ï¼Œè€Œæ˜¯é€šè¿‡éšæœºé‡‡æ ·ï¼ˆä¹Ÿç§°ä¸ºè’™ç‰¹å¡æ´›é‡‡æ ·ï¼‰å¾—åˆ°çš„ã€‚Salimansç­‰äººä¹Ÿè¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œä½œè€…å±•ç¤ºäº†CMA-ESä¸ç­–ç•¥æ¢¯åº¦æ–¹æ³•éå¸¸ç›¸ä¼¼ï¼ŒåŒºåˆ«ä»…åœ¨äºæˆ‘ä»¬è·å¾—æ¢¯åº¦ä¼°è®¡çš„æ–¹å¼ã€‚
- en: 'The preparation before the training loop is simple; we create the environment
    and the network:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯å‰çš„å‡†å¤‡å·¥ä½œå¾ˆç®€å•ï¼›æˆ‘ä»¬åˆ›å»ºç¯å¢ƒå’Œç½‘ç»œï¼š
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Every iteration of the training loop starts with batch creation, where we sample
    the noise and obtain rewards for both positive and negated noise:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è®­ç»ƒå¾ªç¯çš„è¿­ä»£éƒ½ä»æ‰¹æ¬¡åˆ›å»ºå¼€å§‹ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å¯¹å™ªå£°è¿›è¡Œé‡‡æ ·å¹¶è·å¾—æ­£å‘å’Œåå‘å™ªå£°çš„å¥–åŠ±ï¼š
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When we reach the limit of episodes in the batch, or the limit of the total
    steps, we stop gathering the data and do a training update.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è¾¾åˆ°æ‰¹æ¬¡çš„æœ€å¤§é›†æ•°ï¼Œæˆ–è€…æ€»æ­¥æ•°çš„ä¸Šé™æ—¶ï¼Œæˆ‘ä»¬åœæ­¢æ”¶é›†æ•°æ®å¹¶è¿›è¡Œè®­ç»ƒæ›´æ–°ã€‚
- en: 'To perform the update of the network, we call the train_step() function that
    weâ€™ve already seen:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œç½‘ç»œçš„æ›´æ–°ï¼Œæˆ‘ä»¬è°ƒç”¨å·²ç»çœ‹åˆ°çš„`train_step()`å‡½æ•°ï¼š
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The goal of the train_step() function is to scale the noise according to the
    total reward and then adjust the policy weights in the direction of the averaged
    noise.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_step()`å‡½æ•°çš„ç›®æ ‡æ˜¯æ ¹æ®æ€»å¥–åŠ±æ¥ç¼©æ”¾å™ªå£°ï¼Œç„¶åå°†ç­–ç•¥æƒé‡è°ƒæ•´åˆ°å¹³å‡å™ªå£°çš„æ–¹å‘ã€‚'
- en: 'The final steps in the training loop write metrics into TensorBoard and show
    the training progress on the console:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯çš„æœ€åæ­¥éª¤å°†åº¦é‡æ•°æ®å†™å…¥ TensorBoardï¼Œå¹¶åœ¨æ§åˆ¶å°ä¸Šæ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼š
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: CartPole results
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPole ç»“æœ
- en: 'Training can be started by just running the program without the arguments:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¯ä»¥é€šè¿‡ç›´æ¥è¿è¡Œç¨‹åºè€Œä¸éœ€è¦å‚æ•°æ¥å¼€å§‹ï¼š
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: From my experiments, it usually takes ES about 40â€“60 batches to solve CartPole.
    The convergence dynamics for the preceding run are shown in FigureÂ [17.1](#x1-315020r1)
    and FigureÂ [17.2](#x1-315021r2).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘çš„å®éªŒï¼ŒES é€šå¸¸éœ€è¦å¤§çº¦40åˆ°60ä¸ªæ‰¹æ¬¡æ¥è§£å†³ CartPoleã€‚å‰é¢è¿è¡Œçš„æ”¶æ•›åŠ¨æ€å¦‚å›¾[17.1](#x1-315020r1)å’Œå›¾[17.2](#x1-315021r2)æ‰€ç¤ºã€‚
- en: '![PIC](img/B22150_17_01.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_01.png)'
- en: 'FigureÂ 17.1: The maximum reward (left) and policy update (right) for ES on
    CartPole'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾17.1ï¼šESåœ¨CartPoleä¸Šçš„æœ€å¤§å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œç­–ç•¥æ›´æ–°ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_17_02.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_02.png)'
- en: 'FigureÂ 17.2: The mean (left) and standard deviation (right) of reward for ES
    on CartPole'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾17.2ï¼šESåœ¨CartPoleä¸Šçš„å¥–åŠ±å‡å€¼ï¼ˆå·¦ï¼‰å’Œæ ‡å‡†å·®ï¼ˆå³ï¼‰
- en: The preceding graphs looks quite goodâ€”being able to solve the environment in
    30 seconds is on par with the cross-entropy method from ChapterÂ [4](ch008.xhtml#x1-740004).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„å›¾è¡¨çœ‹èµ·æ¥ç›¸å½“ä¸é”™â€”â€”åœ¨30ç§’å†…è§£å†³ç¯å¢ƒé—®é¢˜ä¸ç¬¬[4ç« ](ch008.xhtml#x1-740004)ä¸­çš„äº¤å‰ç†µæ–¹æ³•ç›¸å½“ã€‚
- en: ES on HalfCheetah
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ES åœ¨ HalfCheetah ä¸Šçš„è¡¨ç°
- en: In the next example, we will go beyond the simplest ES implementation and look
    at how this method can be parallelized efficiently using the shared seed strategy
    proposed by Salimans et al. To show this approach, we will use the HalfCheetah
    environment using the MuJoCo physics simulator. We already experimented with it
    in the previous chapter, so if you havenâ€™t installed the gymnasium[mujoco] package,
    you should do so.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è¶…è¶Šæœ€ç®€å•çš„ESå®ç°ï¼Œæ¢è®¨å¦‚ä½•ä½¿ç”¨Salimansç­‰äººæå‡ºçš„å…±äº«ç§å­ç­–ç•¥é«˜æ•ˆåœ°å¹¶è¡ŒåŒ–è¯¥æ–¹æ³•ã€‚ä¸ºäº†å±•ç¤ºè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨MuJoCoç‰©ç†æ¨¡æ‹Ÿå™¨ä¸­çš„HalfCheetahç¯å¢ƒã€‚æˆ‘ä»¬å·²ç»åœ¨å‰ä¸€ç« ä¸­è¿›è¡Œäº†å®éªŒï¼Œå› æ­¤å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…gymnasium[mujoco]åŒ…ï¼Œåº”è¯¥å…ˆå®‰è£…ã€‚
- en: First, letâ€™s discuss the idea of shared seeds. The performance of the ES algorithm
    is mostly determined by the speed at which we can gather our training batch, which
    consists of sampling the noise and checking the total reward of the perturbed
    noise. As our training batch items are independent, we can easily parallelize
    this step to a large number of workers sitting on remote machines. (Thatâ€™s a bit
    similar to the example from ChapterÂ [12](ch016.xhtml#x1-20300012), when we gathered
    gradients from A3C workers.) However, naÃ¯ve implementation of this parallelization
    will require a large amount of data to be transferred from the worker process
    to the central master, which is supposed to combine the noise checked by the workers
    and perform the policy update. Most of this data is the noise vectors, the size
    of which is equal to the size of our policy parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¨è®ºå…±äº«ç§å­çš„æ¦‚å¿µã€‚ESç®—æ³•çš„æ€§èƒ½ä¸»è¦å–å†³äºæˆ‘ä»¬æ”¶é›†è®­ç»ƒæ‰¹æ¬¡çš„é€Ÿåº¦ï¼Œè®­ç»ƒæ‰¹æ¬¡ç”±é‡‡æ ·å™ªå£°å’Œæ£€æŸ¥æ‰°åŠ¨å™ªå£°çš„æ€»å¥–åŠ±ç»„æˆã€‚ç”±äºæˆ‘ä»¬çš„è®­ç»ƒæ‰¹æ¬¡é¡¹æ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†è¿™ä¸€æ­¥å¹¶è¡ŒåŒ–åˆ°å¤§é‡ä½äºè¿œç¨‹æœºå™¨ä¸Šçš„å·¥äººï¼ˆè¿™æœ‰ç‚¹ç±»ä¼¼äºç¬¬[12](ch016.xhtml#x1-20300012)ç« ä¸­çš„ç¤ºä¾‹ï¼Œå½“æ—¶æˆ‘ä»¬ä»A3Cå·¥äººé‚£é‡Œæ”¶é›†æ¢¯åº¦ï¼‰ã€‚ç„¶è€Œï¼Œå¤©çœŸçš„å¹¶è¡ŒåŒ–å®ç°å°†éœ€è¦å°†å¤§é‡æ•°æ®ä»å·¥äººè¿›ç¨‹ä¼ è¾“åˆ°ä¸­å¤®ä¸»è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹åº”å½“åˆå¹¶å·¥äººæ£€æŸ¥è¿‡çš„å™ªå£°å¹¶æ‰§è¡Œç­–ç•¥æ›´æ–°ã€‚å¤§éƒ¨åˆ†æ•°æ®æ˜¯å™ªå£°å‘é‡ï¼Œå…¶å¤§å°ç­‰äºæˆ‘ä»¬ç­–ç•¥å‚æ•°çš„å¤§å°ã€‚
- en: To avoid this overhead, a quite elegant solution was proposed by Salimans et
    al. As noise sampled on a worker is produced by a pseudo-random number generator,
    which allows us to set the random seed and reproduce the random sequence generated,
    the worker can transfer to the master only the seed that was used to generate
    the noise. Then, the master can generate the same noise vector again using the
    seed. Of course, the seed on every worker needs to be generated randomly to still
    have a random optimization process. This has the effect of dramatically decreasing
    the amount of data that needs to be transferred from workers to the master, improving
    the scalability of the method. For example, Salimans et al. reported linear speed
    up in optimizations involving 1,440 CPUs in the cloud. In our example, we will
    look at local parallelization using the same approach.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…è¿™ç§å¼€é”€ï¼ŒSalimansç­‰äººæå‡ºäº†ä¸€ä¸ªç›¸å½“ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆã€‚ç”±äºå·¥äººä¸Šé‡‡æ ·çš„å™ªå£°æ˜¯ç”±ä¼ªéšæœºæ•°ç”Ÿæˆå™¨äº§ç”Ÿçš„ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥è®¾ç½®éšæœºç§å­å¹¶é‡ç°ç”Ÿæˆçš„éšæœºåºåˆ—ï¼Œå› æ­¤å·¥äººåªéœ€å°†ç”¨äºç”Ÿæˆå™ªå£°çš„ç§å­ä¼ è¾“ç»™ä¸»è¿›ç¨‹ã€‚ç„¶åï¼Œä¸»è¿›ç¨‹å¯ä»¥ä½¿ç”¨è¯¥ç§å­å†æ¬¡ç”Ÿæˆç›¸åŒçš„å™ªå£°å‘é‡ã€‚å½“ç„¶ï¼Œæ¯ä¸ªå·¥äººçš„ç§å­éœ€è¦éšæœºç”Ÿæˆï¼Œä»¥ä¿æŒéšæœºä¼˜åŒ–è¿‡ç¨‹çš„æ€§è´¨ã€‚è¿™æ ·åšçš„æ•ˆæœæ˜¯æ˜¾è‘—å‡å°‘äº†éœ€è¦ä»å·¥äººä¼ è¾“åˆ°ä¸»è¿›ç¨‹çš„æ•°æ®é‡ï¼Œä»è€Œæé«˜äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚ä¾‹å¦‚ï¼ŒSalimansç­‰äººæŠ¥å‘Šäº†åœ¨äº‘ç«¯ä½¿ç”¨1,440ä¸ªCPUè¿›è¡Œä¼˜åŒ–æ—¶çš„çº¿æ€§åŠ é€Ÿã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•è¿›è¡Œæœ¬åœ°å¹¶è¡ŒåŒ–ã€‚
- en: Implementing ES on HalfCheetah
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨HalfCheetahä¸Šå®ç°ES
- en: The code is placed in Chapter17/02_cheetah_es.py. As the code significantly
    overlaps with the CartPole version, we will focus here only on the differences.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç ä½äºChapter17/02_cheetah_es.pyä¸­ã€‚ç”±äºä»£ç ä¸CartPoleç‰ˆæœ¬æœ‰æ˜¾è‘—é‡å ï¼Œæˆ‘ä»¬åœ¨æ­¤åªå…³æ³¨å…¶ä¸­çš„å·®å¼‚ã€‚
- en: 'We will begin with the worker, which is started as a separate process using
    the PyTorch multiprocessing wrapper. The workerâ€™s responsibilities are simple:
    for every iteration, it obtains the network parameters from the master process,
    and then it performs the fixed number of iterations, where it samples the noise
    and evaluates the reward. The result with the random seed is sent to the master
    using the queue.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»å·¥äººå¼€å§‹ï¼Œå·¥äººä½œä¸ºå•ç‹¬çš„è¿›ç¨‹ä½¿ç”¨PyTorchçš„å¤šè¿›ç¨‹å°è£…å™¨å¯åŠ¨ã€‚å·¥äººçš„èŒè´£å¾ˆç®€å•ï¼šæ¯æ¬¡è¿­ä»£æ—¶ï¼Œå®ƒä»ä¸»è¿›ç¨‹è·å–ç½‘ç»œå‚æ•°ï¼Œç„¶åæ‰§è¡Œå›ºå®šæ¬¡æ•°çš„è¿­ä»£ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­å®ƒé‡‡æ ·å™ªå£°å¹¶è¯„ä¼°å¥–åŠ±ã€‚å¸¦æœ‰éšæœºç§å­çš„ç»“æœé€šè¿‡é˜Ÿåˆ—å‘é€ç»™ä¸»è¿›ç¨‹ã€‚
- en: 'The following dataclass is used by the worker to send the results of the perturbed
    policy evaluation to the master process:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ•°æ®ç±»ç”±å·¥äººä½¿ç”¨ï¼Œç”¨äºå°†æ‰°åŠ¨ç­–ç•¥è¯„ä¼°çš„ç»“æœå‘é€åˆ°ä¸»è¿›ç¨‹ï¼š
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It includes the random seed, the rewards obtained with the positive and negative
    noise, and the total number of steps we performed in both tests.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåŒ…æ‹¬éšæœºç§å­ã€é€šè¿‡æ­£è´Ÿå™ªå£°è·å¾—çš„å¥–åŠ±ï¼Œä»¥åŠæˆ‘ä»¬åœ¨ä¸¤ä¸ªæµ‹è¯•ä¸­æ‰§è¡Œçš„æ€»æ­¥éª¤æ•°ã€‚
- en: 'On every training iteration, the worker waits for the network parameters to
    be broadcasted from the master:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶ï¼Œå·¥ä½œç¨‹åºä¼šç­‰å¾…ä¸»æ§ç¨‹åºå¹¿æ’­ç½‘ç»œå‚æ•°ï¼š
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The value of None means that the master wants to stop the worker.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: None çš„å€¼è¡¨ç¤ºä¸»æ§ç¨‹åºå¸Œæœ›åœæ­¢å·¥ä½œç¨‹åºã€‚
- en: 'The rest is almost the same as the previous example, with the main difference
    being in the random seed generated and assigned before the noise generation. This
    allows the master to regenerate the same noise, only from the seed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä½™éƒ¨åˆ†å‡ ä¹ä¸å‰ä¸€ä¸ªç¤ºä¾‹ç›¸åŒï¼Œä¸»è¦çš„åŒºåˆ«åœ¨äºåœ¨å™ªå£°ç”Ÿæˆä¹‹å‰ç”Ÿæˆå¹¶åˆ†é…çš„éšæœºç§å­ã€‚è¿™ä½¿å¾—ä¸»æ§ç¨‹åºèƒ½å¤Ÿé‡æ–°ç”Ÿæˆç›¸åŒçš„å™ªå£°ï¼Œåªæ˜¯ä»ç§å­å¼€å§‹ï¼š
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Another difference lies in the function used by the master to perform the training
    step:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªåŒºåˆ«åœ¨äºä¸»æ§ç¨‹åºæ‰§è¡Œè®­ç»ƒæ­¥éª¤æ—¶ä½¿ç”¨çš„å‡½æ•°ï¼š
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the CartPole example, we normalized the batch of rewards by subtracting the
    mean and dividing by the standard deviation. According to Salimans et al., better
    results could be obtained using ranks instead of actual rewards. As ES has no
    assumptions about the fitness function (which is a reward in our case), we can
    make any rearrangements in the reward that we want, which wasnâ€™t possible in the
    case of DQN, for example.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ CartPole ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®å¯¹å¥–åŠ±æ‰¹æ¬¡è¿›è¡Œäº†å½’ä¸€åŒ–ã€‚æ ¹æ® Salimans ç­‰äººçš„è¯´æ³•ï¼Œä½¿ç”¨ç§©è€Œéå®é™…å¥–åŠ±å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚ç”±äº
    ES å¯¹é€‚åº”åº¦å‡½æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­å³ä¸ºå¥–åŠ±ï¼‰æ²¡æœ‰å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å¥–åŠ±è¿›è¡Œä»»ä½•é‡æ–°æ’åˆ—ï¼Œè¿™åœ¨ DQN çš„æƒ…å†µä¸‹æ˜¯ä¸å¯è¡Œçš„ã€‚
- en: Here, rank transformation of the array means replacing the array with indices
    of the sorted array. For example, array [0.1, 10, 0.5] will have the rank array
    [0, 2, 1]. The compute_centered_ranks function takes the array with the total
    rewards of the batch, calculates the rank for every item in the array, and then
    normalizes those ranks. For example, an input array of [21.0, 5.8, 7.0] will have
    ranks [2, 0, 1], and the final centered ranks will be [0.5, -0.5, 0.0].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ•°ç»„çš„ç§©å˜æ¢æ„å‘³ç€ç”¨æ’åºæ•°ç»„çš„ç´¢å¼•æ›¿æ¢åŸæ•°ç»„ã€‚ä¾‹å¦‚ï¼Œæ•°ç»„ [0.1, 10, 0.5] å°†å˜ä¸ºç§©æ•°ç»„ [0, 2, 1]ã€‚`compute_centered_ranks`
    å‡½æ•°æ¥å—ä¸€ä¸ªåŒ…å«æ‰¹æ¬¡æ€»å¥–åŠ±çš„æ•°ç»„ï¼Œè®¡ç®—æ•°ç»„ä¸­æ¯ä¸ªé¡¹ç›®çš„ç§©ï¼Œç„¶åå¯¹è¿™äº›ç§©è¿›è¡Œå½’ä¸€åŒ–ã€‚ä¾‹å¦‚ï¼Œè¾“å…¥æ•°ç»„ [21.0, 5.8, 7.0] å°†å¾—åˆ°ç§© [2, 0,
    1]ï¼Œæœ€ç»ˆçš„å±…ä¸­ç§©å°†æ˜¯ [0.5, -0.5, 0.0]ã€‚
- en: 'Another major difference in the training function is the use of PyTorch optimizers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå‡½æ•°çš„å¦ä¸€ä¸ªä¸»è¦åŒºåˆ«æ˜¯ä½¿ç”¨äº† PyTorch ä¼˜åŒ–å™¨ï¼š
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To understand why they are used and how this is possible without doing backpropagation,
    some explanations are required.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨è¿™äº›æ–¹æ³•ä»¥åŠå¦‚ä½•åœ¨ä¸è¿›è¡Œåå‘ä¼ æ’­çš„æƒ…å†µä¸‹å®ç°è¿™ä¸€ç‚¹ï¼Œå¿…é¡»åšä¸€äº›è§£é‡Šã€‚
- en: First, Salimans et al. showed that the optimization method used by the ES algorithm
    is very similar to gradient ascent on the fitness function, with the difference
    being how the gradient is calculated. The way the stochastic gradient descent
    (SGD) method is usually applied is that the gradient is obtained from the loss
    function by calculating the derivative of the network parameters with respect
    to the loss value. This imposes the limitation on the network and loss function
    to be differentiable, which is not always the case; for example, the rank transformation
    performed by the ES method is not differentiable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒSalimans ç­‰äººè¡¨æ˜ï¼ŒES ç®—æ³•ä½¿ç”¨çš„ä¼˜åŒ–æ–¹æ³•ä¸æ¢¯åº¦ä¸Šå‡æ³•éå¸¸ç›¸ä¼¼ï¼ŒåŒºåˆ«åœ¨äºæ¢¯åº¦çš„è®¡ç®—æ–¹å¼ã€‚é€šå¸¸åº”ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ–¹æ³•æ—¶ï¼Œæ¢¯åº¦æ˜¯é€šè¿‡è®¡ç®—ç½‘ç»œå‚æ•°å…³äºæŸå¤±å€¼çš„å¯¼æ•°ä»æŸå¤±å‡½æ•°ä¸­è·å¾—çš„ã€‚è¿™è¦æ±‚ç½‘ç»œå’ŒæŸå¤±å‡½æ•°æ˜¯å¯å¾®çš„ï¼Œä½†è¿™å¹¶éæ€»æ˜¯æˆç«‹ï¼›ä¾‹å¦‚ï¼ŒES
    æ–¹æ³•æ‰§è¡Œçš„ç§©å˜æ¢æ˜¯ä¸å¯å¾®çš„ã€‚
- en: 'On the other hand, optimization performed by ES works differently. We randomly
    sample the neighborhood of our current parameters by adding the noise to them
    and calculating the fitness function. According to the fitness function change,
    we adjust the parameters, which pushes our parameters in the direction of a higher
    fitness function. The result of this is very similar to gradient-based methods,
    but the requirements imposed on our fitness function are much looser: the only
    requirement is our ability to calculate it.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼ŒES æ‰§è¡Œçš„ä¼˜åŒ–è¿‡ç¨‹åˆ™æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬é€šè¿‡å‘å½“å‰å‚æ•°æ·»åŠ å™ªå£°å¹¶è®¡ç®—é€‚åº”åº¦å‡½æ•°ï¼Œéšæœºé‡‡æ ·å‘¨å›´çš„é‚»åŸŸã€‚æ ¹æ®é€‚åº”åº¦å‡½æ•°çš„å˜åŒ–ï¼Œæˆ‘ä»¬è°ƒæ•´å‚æ•°ï¼Œæ¨åŠ¨å‚æ•°æœç€æ›´é«˜çš„é€‚åº”åº¦å‡½æ•°æ–¹å‘å‰è¿›ã€‚å…¶ç»“æœä¸åŸºäºæ¢¯åº¦çš„æ–¹æ³•éå¸¸ç›¸ä¼¼ï¼Œä½†å¯¹é€‚åº”åº¦å‡½æ•°çš„è¦æ±‚è¦å®½æ¾å¾—å¤šï¼šå”¯ä¸€çš„è¦æ±‚æ˜¯èƒ½å¤Ÿè®¡ç®—å®ƒã€‚
- en: However, if weâ€™re estimating some kind of gradient by randomly sampling the
    fitness function, we can use standard optimizers from PyTorch. Normally, optimizers
    adjust the parameters of the network using gradients accumulated in the parametersâ€™
    grad fields.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡éšæœºé‡‡æ ·é€‚åº”åº¦å‡½æ•°æ¥ä¼°è®¡æŸç§æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ PyTorch ä¸­çš„æ ‡å‡†ä¼˜åŒ–å™¨ã€‚é€šå¸¸ï¼Œä¼˜åŒ–å™¨ä½¿ç”¨ç§¯ç´¯åœ¨å‚æ•° `grad` å­—æ®µä¸­çš„æ¢¯åº¦æ¥è°ƒæ•´ç½‘ç»œçš„å‚æ•°ã€‚
- en: 'Those gradients are accumulated after the backpropagation step, but due to
    PyTorchâ€™s flexibility, the optimizer doesnâ€™t care about the source of the gradients.
    So, the only thing we need to do is copy the estimated parametersâ€™ update in the
    grad fields and ask the optimizer to update them. Note that the update is copied
    with a negative sign, as optimizers normally perform gradient descent (as in a
    normal operation, we minimize the loss function), but in this case, we want to
    do gradient ascent. This is very similar to the actor-critic method we covered
    in ChapterÂ [12](ch016.xhtml#x1-20300012), when the estimated policy gradient is
    taken with the negative sign, as it shows the direction to improve the policy.
    The last chunk of differences in the code is taken from the training loop performed
    by the master process. Its responsibility is to wait for data from worker processes,
    perform the training update of the parameters, and broadcast the result to the
    workers. The communication between the master and workers is performed by two
    sets of queues. The first queue is per-worker and is used by the master to send
    the current policy parameters to use. The second queue is shared by the workers
    and is used to send the already mentioned RewardItem structure with the random
    seed and rewards:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ­¥éª¤åè¢«ç§¯ç´¯ï¼Œä½†ç”±äº PyTorch çš„çµæ´»æ€§ï¼Œä¼˜åŒ–å™¨ä¸å…³å¿ƒæ¢¯åº¦çš„æ¥æºã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åšçš„å”¯ä¸€äº‹æƒ…å°±æ˜¯å°†ä¼°è®¡çš„å‚æ•°æ›´æ–°å¤åˆ¶åˆ° `grad`
    å­—æ®µï¼Œå¹¶è¦æ±‚ä¼˜åŒ–å™¨æ›´æ–°å®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œæ›´æ–°æ˜¯å¸¦æœ‰è´Ÿå·çš„ï¼Œå› ä¸ºä¼˜åŒ–å™¨é€šå¸¸æ‰§è¡Œæ¢¯åº¦ä¸‹é™ï¼ˆå¦‚åŒæ­£å¸¸æ“ä½œä¸­æˆ‘ä»¬æœ€å°åŒ–æŸå¤±å‡½æ•°ä¸€æ ·ï¼‰ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰§è¡Œæ¢¯åº¦ä¸Šå‡ã€‚è¿™ä¸æˆ‘ä»¬åœ¨ç¬¬
    [12](ch016.xhtml#x1-20300012) ç« ä¸­æ¶‰åŠçš„æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•éå¸¸ç›¸ä¼¼ï¼Œå…¶ä¸­ä¼°è®¡çš„ç­–ç•¥æ¢¯åº¦å¸¦æœ‰è´Ÿå·ï¼Œå› ä¸ºå®ƒæ˜¾ç¤ºäº†æ”¹è¿›ç­–ç•¥çš„æ–¹å‘ã€‚ä»£ç ä¸­çš„æœ€åä¸€éƒ¨åˆ†å·®å¼‚æ¥è‡ªä¸»è¿›ç¨‹æ‰§è¡Œçš„è®­ç»ƒå¾ªç¯ã€‚å®ƒçš„èŒè´£æ˜¯ç­‰å¾…æ¥è‡ªå·¥ä½œè¿›ç¨‹çš„æ•°æ®ï¼Œæ‰§è¡Œå‚æ•°çš„è®­ç»ƒæ›´æ–°ï¼Œå¹¶å°†ç»“æœå¹¿æ’­åˆ°å·¥ä½œè¿›ç¨‹ã€‚ä¸»è¿›ç¨‹å’Œå·¥ä½œè¿›ç¨‹ä¹‹é—´çš„é€šä¿¡é€šè¿‡ä¸¤ç»„é˜Ÿåˆ—æ¥å®Œæˆã€‚ç¬¬ä¸€ç»„é˜Ÿåˆ—æ˜¯æ¯ä¸ªå·¥ä½œè¿›ç¨‹çš„é˜Ÿåˆ—ï¼Œç”¨äºä¸»è¿›ç¨‹å‘é€å½“å‰ä½¿ç”¨çš„ç­–ç•¥å‚æ•°ã€‚ç¬¬äºŒç»„é˜Ÿåˆ—æ˜¯ç”±å·¥ä½œè¿›ç¨‹å…±äº«çš„ï¼Œç”¨äºå‘é€å·²ç»æåˆ°çš„
    `RewardItem` ç»“æ„ï¼Œå…¶ä¸­åŒ…å«éšæœºç§å­å’Œå¥–åŠ±ï¼š
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At the beginning of the master, we create all those queues, start the worker
    processes, and create the optimizer.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸»è¿›ç¨‹å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬åˆ›å»ºæ‰€æœ‰é˜Ÿåˆ—ï¼Œå¯åŠ¨å·¥ä½œè¿›ç¨‹ï¼Œå¹¶åˆ›å»ºä¼˜åŒ–å™¨ã€‚
- en: 'Every training iteration starts with the network parameters being broadcast
    to the workers:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è®­ç»ƒè¿­ä»£å¼€å§‹æ—¶ï¼Œç½‘ç»œå‚æ•°ä¼šå¹¿æ’­åˆ°å·¥ä½œè¿›ç¨‹ï¼š
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, in the loop, the master waits for enough data to be obtained from the
    workers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œåœ¨å¾ªç¯ä¸­ï¼Œä¸»è¿›ç¨‹ç­‰å¾…ä»å·¥ä½œè¿›ç¨‹è·å–è¶³å¤Ÿçš„æ•°æ®ï¼š
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Every time a new result arrives, we reproduce the noise using the random seed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“æ–°ç»“æœåˆ°è¾¾æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºç§å­é‡æ–°ç”Ÿæˆå™ªå£°ã€‚
- en: 'As the last step in the training loop, we call the train_step() function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè®­ç»ƒå¾ªç¯çš„æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬è°ƒç”¨ `train_step()` å‡½æ•°ï¼š
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Youâ€™ve already seen this function, which calculates the update from the noise
    and rewards, and calls the optimizer to adjust the weights.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å·²ç»è§è¿‡è¿™ä¸ªå‡½æ•°ï¼Œå®ƒè®¡ç®—æ¥è‡ªå™ªå£°å’Œå¥–åŠ±çš„æ›´æ–°ï¼Œå¹¶è°ƒç”¨ä¼˜åŒ–å™¨è°ƒæ•´æƒé‡ã€‚
- en: HalfCheetah results
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HalfCheetah ç»“æœ
- en: 'The code supports the optional --dev flag, but from my experiments, I got a
    slowdown if GPU was enabled: without GPU, the average speed was 20-21k observations
    per second, but with CUDA, it was just 9k. This might look counter-intuitive,
    but we can explain this with the very small network and batch size of a single
    observation. Potentially, we might decrease the gap (or even get some speedup)
    with a higher batch size, but it will complicate our code.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä»£ç æ”¯æŒå¯é€‰çš„ `--dev` æ ‡å¿—ï¼Œä½†æ ¹æ®æˆ‘çš„å®éªŒï¼Œå¦‚æœå¯ç”¨äº† GPUï¼Œé€Ÿåº¦ä¼šå˜æ…¢ï¼šæ²¡æœ‰ GPU æ—¶ï¼Œå¹³å‡é€Ÿåº¦æ˜¯æ¯ç§’ 20-21k æ¬¡è§‚å¯Ÿï¼Œä½†å¯ç”¨
    CUDA ååªæœ‰ 9kã€‚è¿™çœ‹èµ·æ¥å¯èƒ½æœ‰äº›åç›´è§‰ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç”¨éå¸¸å°çš„ç½‘ç»œå’Œå•æ¬¡è§‚å¯Ÿçš„æ‰¹é‡å¤§å°æ¥è§£é‡Šè¿™ä¸€ç‚¹ã€‚å¯èƒ½é€šè¿‡å¢åŠ æ‰¹é‡å¤§å°æ¥å‡å°‘è¿™ä¸€å·®è·ï¼ˆç”šè‡³å¯èƒ½å®ç°åŠ é€Ÿï¼‰ï¼Œä½†è¿™ä¼šä½¿æˆ‘ä»¬çš„ä»£ç å˜å¾—æ›´åŠ å¤æ‚ã€‚
- en: 'During the training, we show the mean reward, the speed of training (in observations
    per second), and two timing values (showing how long it took to gather data and
    perform the training step):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºå¹³å‡å¥–åŠ±ã€è®­ç»ƒé€Ÿåº¦ï¼ˆæ¯ç§’è§‚å¯Ÿæ¬¡æ•°ï¼‰ä»¥åŠä¸¤ä¸ªæ—¶é—´å€¼ï¼ˆæ˜¾ç¤ºæ”¶é›†æ•°æ®å’Œæ‰§è¡Œè®­ç»ƒæ­¥éª¤æ‰€èŠ±è´¹çš„æ—¶é—´ï¼‰ï¼š
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The dynamics of the training show very quick policy improvement in the beginning:
    in just 100 updates, which is 9 minutes of training, the agent was able to reach
    the score of 1,500-1,600\. After 30 minutes, the peak reward was 2,833; but with
    more training, the policy was degrading.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒçš„åŠ¨æ€æ˜¾ç¤ºå‡ºå¼€å§‹æ—¶ç­–ç•¥çš„å¿«é€Ÿæ”¹è¿›ï¼šä»…åœ¨100æ¬¡æ›´æ–°å†…ï¼Œè®­ç»ƒ9åˆ†é’Ÿï¼Œä»£ç†å°±èƒ½å¤Ÿè¾¾åˆ°1,500-1,600çš„åˆ†æ•°ã€‚30åˆ†é’Ÿåï¼Œå³°å€¼å¥–åŠ±ä¸º2,833ï¼›ä½†éšç€æ›´å¤šè®­ç»ƒï¼Œç­–ç•¥å¼€å§‹é€€åŒ–ã€‚
- en: The maximum, mean, and standard deviation of reward are shown in FigureÂ [17.3](#x1-318022r3)
    and FigureÂ [17.4](#x1-318023r4).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±çš„æœ€å¤§å€¼ã€å‡å€¼å’Œæ ‡å‡†å·®å¦‚å›¾[17.3](#x1-318022r3)å’Œå›¾[17.4](#x1-318023r4)æ‰€ç¤ºã€‚
- en: '![PIC](img/B22150_17_03.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_17_03.png)'
- en: 'FigureÂ 17.3: The maximum reward (left) and policy update (right) for ES on
    HalfCheetah'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾17.3ï¼šESåœ¨HalfCheetahä¸Šçš„æœ€å¤§å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œç­–ç•¥æ›´æ–°ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_17_04.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_17_04.png)'
- en: 'FigureÂ 17.4: The mean (left) and standard deviation (right) of reward for ES
    on HalfCheetah'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾17.4ï¼šESåœ¨HalfCheetahä¸Šçš„å¥–åŠ±å‡å€¼ï¼ˆå·¦ï¼‰å’Œæ ‡å‡†å·®ï¼ˆå³ï¼‰
- en: Genetic algorithms
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—ä¼ ç®—æ³•
- en: Another popular class of black-box methods is genetic algorithms (GAs). It is
    a large family of optimization methods with more than two decades of history behind
    it and a simple core idea of generating a population of N individuals (concrete
    model parameters), each of which is evaluated with the fitness function. Then,
    some subset of top performers is used to produce the next generation of the population
    (this process is called mutation). This process is repeated until weâ€™re satisfied
    with the performance of our population.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç±»æµè¡Œçš„é»‘ç®±æ–¹æ³•æ˜¯é—ä¼ ç®—æ³•ï¼ˆGAï¼‰ã€‚å®ƒæ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…ã€æ‹¥æœ‰äºŒåå¤šå¹´å†å²çš„å¤§å‹ä¼˜åŒ–æ–¹æ³•å®¶æ—ï¼Œæ ¸å¿ƒæ€æƒ³ç®€å•ï¼Œå³ç”Ÿæˆä¸€ä¸ª N ä¸ªä¸ªä½“ï¼ˆå…·ä½“æ¨¡å‹å‚æ•°ï¼‰çš„äººå£ï¼Œæ¯ä¸ªä¸ªä½“éƒ½é€šè¿‡é€‚åº”åº¦å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚ç„¶åï¼Œéƒ¨åˆ†è¡¨ç°æœ€å¥½çš„ä¸ªä½“ç”¨äºç”Ÿæˆä¸‹ä¸€ä»£ç§ç¾¤ï¼ˆæ­¤è¿‡ç¨‹ç§°ä¸ºå˜å¼‚ï¼‰ã€‚è¿™ä¸€è¿‡ç¨‹ä¼šä¸€ç›´é‡å¤ï¼Œç›´åˆ°æˆ‘ä»¬å¯¹ç§ç¾¤çš„è¡¨ç°æ„Ÿåˆ°æ»¡æ„ä¸ºæ­¢ã€‚
- en: 'There are a lot of different methods in the GA family, for example, how to
    perform the mutation of the individuals for the next generation or how to rank
    the performers. Here, we will consider the simple GA method with some extensions,
    published in the paper by Such et al., called Deep neuroevolution: Genetic algorithms
    are a competitive alternative for training deep neural networks for reinforcement
    learning [[Suc+17](#)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: é—ä¼ ç®—æ³•ï¼ˆGAï¼‰å®¶æ—ä¸­æœ‰è®¸å¤šä¸åŒçš„æ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œå¦‚ä½•æ‰§è¡Œä¸ªä½“çš„å˜å¼‚ä»¥ç”Ÿæˆä¸‹ä¸€ä»£ï¼Œæˆ–è€…å¦‚ä½•å¯¹è¡¨ç°è€…è¿›è¡Œæ’åã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸€äº›æ‰©å±•çš„ç®€å•GAæ–¹æ³•ï¼Œæœ€æ—©ç”±Suchç­‰äººå‘å¸ƒï¼Œåä¸ºâ€œæ·±åº¦ç¥ç»è¿›åŒ–ï¼šé—ä¼ ç®—æ³•æ˜¯è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æœ‰åŠ›ç«äº‰è€…â€[[Suc+17](#)]ã€‚
- en: 'In this paper, the authors analyzed the simple GA method, which performs Gaussian
    noise perturbation of the parentâ€™s weights to perform mutation. On every iteration,
    the top performer was copied without modification. In an algorithm form, the steps
    of a simple GA method can be written as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…åˆ†æäº†ç®€å•çš„GAæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹çˆ¶ä»£æƒé‡æ–½åŠ é«˜æ–¯å™ªå£°æ‰°åŠ¨æ¥æ‰§è¡Œå˜å¼‚ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œè¡¨ç°æœ€å¥½çš„ä¸ªä½“ä¼šè¢«å¤åˆ¶ä¸”ä¸åšä¿®æ”¹ã€‚ç®€å•GAæ–¹æ³•çš„æ­¥éª¤å¯ä»¥ç”¨ç®—æ³•å½¢å¼å†™æˆå¦‚ä¸‹ï¼š
- en: 'Initialize the mutation power, Ïƒ, the population size, N, the number of selected
    individuals, T, and the initial population, Pâ°, with N randomly initialized policies
    and their fitness: Fâ° = {F(P[i]â°)|i = 1â€¦N}'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–å˜å¼‚å¼ºåº¦ Ïƒã€ç§ç¾¤å¤§å° Nã€é€‰æ‹©ä¸ªä½“çš„æ•°é‡ T å’Œåˆå§‹ç§ç¾¤ Pâ°ï¼Œå…¶ä¸­ Pâ° æ˜¯éšæœºåˆå§‹åŒ–çš„ N ä¸ªç­–ç•¥åŠå…¶é€‚åº”åº¦ï¼šFâ° = {F(P[i]â°)|i
    = 1â€¦N}
- en: 'For generation g = 1â€¦G:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºä»£æ•° g = 1â€¦Gï¼š
- en: Sort generation P^(nâˆ’1) in the descending order of the fitness function value
    F^(gâˆ’1)
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰ç…§é€‚åº”åº¦å‡½æ•°å€¼ F^(gâˆ’1) çš„é™åºå¯¹ä¸Šä¸€ä»£ P^(nâˆ’1) è¿›è¡Œæ’åº
- en: Copy elite P[1]^g = P[1]^(gâˆ’1),F[1]^g = F[1]^(gâˆ’1)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤åˆ¶ç²¾è‹± P[1]^g = P[1]^(gâˆ’1)ï¼ŒF[1]^g = F[1]^(gâˆ’1)
- en: 'For individual i = 2â€¦N:'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºä¸ªä½“ i = 2â€¦Nï¼š
- en: 'Choose the k: random parent from 1â€¦T'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹© kï¼šä» 1â€¦T ä¸­éšæœºé€‰æ‹©çˆ¶ä»£
- en: Sample ğœ– âˆ¼ğ’©(0,I)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡‡æ · ğœ– âˆ¼ğ’©(0,I)
- en: 'Mutate the parent: P[i]^g = P[i]^(gâˆ’1) + Ïƒğœ–'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å˜å¼‚çˆ¶ä»£ï¼šP[i]^g = P[i]^(gâˆ’1) + Ïƒğœ–
- en: 'Get its fitness: F[i]^g = F(P[i]^g)'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è·å–å…¶é€‚åº”åº¦ï¼šF[i]^g = F(P[i]^g)
- en: There have been several improvements to this basic method from the paper [2],
    which we will discuss later. For now, letâ€™s check the implementation of the core
    algorithm.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªæ–‡çŒ®[2]çš„åŸºç¡€æ–¹æ³•å·²æœ‰å¤šä¸ªæ”¹è¿›ï¼Œæˆ‘ä»¬å°†åœ¨åæ–‡è®¨è®ºã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ ¸å¿ƒç®—æ³•çš„å®ç°ã€‚
- en: GA on CartPole
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPoleä¸Šçš„GA
- en: 'The source code is in Chapter17/03_cartpole_ga.py, and it has a lot in common
    with our ES example. The difference is in the lack of the gradient ascent code,
    which was replaced by the network mutation function:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æºä»£ç ä½äºChapter17/03_cartpole_ga.pyï¼Œä¸æˆ‘ä»¬çš„ESç¤ºä¾‹æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚ä¸åŒä¹‹å¤„åœ¨äºç¼ºå°‘æ¢¯åº¦ä¸Šå‡ä»£ç ï¼Œè€Œæ˜¯ç”¨ç½‘ç»œå˜å¼‚å‡½æ•°ä»£æ›¿ï¼š
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The goal of the function is to create a mutated copy of the given policy by
    adding a random noise to all weights. The parentâ€™s weights are kept untouched,
    as a random selection of the parent is performed with replacement, so this network
    could be used again later.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°çš„ç›®æ ‡æ˜¯é€šè¿‡å‘æ‰€æœ‰æƒé‡æ·»åŠ éšæœºå™ªå£°ï¼Œåˆ›å»ºç»™å®šç­–ç•¥çš„å˜å¼‚å‰¯æœ¬ã€‚çˆ¶ä»£çš„æƒé‡ä¿æŒä¸å˜ï¼Œå› ä¸ºçˆ¶ä»£æ˜¯é€šè¿‡æ›¿æ¢æ–¹å¼éšæœºé€‰æ‹©çš„ï¼Œå› æ­¤è¯¥ç½‘ç»œç¨åå¯èƒ½ä¼šå†æ¬¡ä½¿ç”¨ã€‚
- en: 'The count of hyperparameters is even smaller than with ES and includes the
    standard deviation of the noise added-on mutation, the population size, and the
    number of top performers used to produce the subsequent generation:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°çš„æ•°é‡ç”šè‡³æ¯”ESæ–¹æ³•è¿˜è¦å°‘ï¼ŒåŒ…æ‹¬å˜å¼‚æ—¶æ·»åŠ å™ªå£°çš„æ ‡å‡†å·®ã€ç§ç¾¤å¤§å°å’Œç”¨äºç”Ÿæˆåç»­ä¸–ä»£çš„é¡¶çº§è¡¨ç°è€…æ•°é‡ï¼š
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Before the training loop, we create the population of randomly initialized
    networks and obtain their fitness:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬åˆ›å»ºéšæœºåˆå§‹åŒ–çš„ç½‘ç»œç§ç¾¤ï¼Œå¹¶è·å–å®ƒä»¬çš„é€‚åº”åº¦ï¼š
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At the beginning of every generation, we sort the previous generation according
    to its fitness and record statistics about future parents:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€ä»£å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬æ ¹æ®ä¸Šä¸€ä»£çš„é€‚åº”åº¦å¯¹å…¶è¿›è¡Œæ’åºï¼Œå¹¶è®°å½•å…³äºæœªæ¥çˆ¶ä»£çš„ç»Ÿè®¡æ•°æ®ï¼š
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In a separate loop over new individuals to be generated, we randomly sample
    a parent, mutate it, and evaluate its fitness score:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªå•ç‹¬çš„å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬éšæœºé€‰å–ä¸€ä¸ªçˆ¶ä»£ï¼Œè¿›è¡Œå˜å¼‚ï¼Œå¹¶è¯„ä¼°å…¶é€‚åº”åº¦å¾—åˆ†ï¼š
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After starting the implementation, you should see the following (concrete output
    and count of steps might vary due to randomness in execution):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: å¯åŠ¨å®ç°åï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°å¦‚ä¸‹å†…å®¹ï¼ˆå…·ä½“è¾“å‡ºå’Œæ­¥éª¤æ•°å¯èƒ½å› æ‰§è¡Œä¸­çš„éšæœºæ€§è€Œæœ‰æ‰€ä¸åŒï¼‰ï¼š
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see, the GA method is even more efficient than the ES method.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼ŒGAæ–¹æ³•æ¯”ESæ–¹æ³•æ›´é«˜æ•ˆã€‚
- en: GA tweaks
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAæ”¹è¿›
- en: 'Such et al. proposed two tweaks to the basic GA algorithm:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Suchç­‰äººæå‡ºäº†å¯¹åŸºæœ¬GAç®—æ³•çš„ä¸¤é¡¹æ”¹è¿›ï¼š
- en: The first, with the name deep GA, aimed to increase the scalability of the implementation.
    We will implement this later in the GA on HalfCheetah section.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ–¹æ³•ï¼Œåä¸ºæ·±åº¦GAï¼Œæ—¨åœ¨æé«˜å®ç°çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å°†åœ¨åé¢çš„GA on HalfCheetahéƒ¨åˆ†å®ç°è¿™ä¸€ç‚¹ã€‚
- en: The second, called novelty search, was an attempt to replace the reward objective
    with a different metric of the episode. Weâ€™ve left this as an exercise for you
    to try out.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ–¹æ³•ï¼Œå«åšæ–°é¢–æ€§æœç´¢ï¼Œæ˜¯å°è¯•ç”¨ä¸åŒçš„æŒ‡æ ‡æ›¿ä»£å¥–åŠ±ç›®æ ‡ã€‚æˆ‘ä»¬å°†è¿™ä¸€éƒ¨åˆ†ç•™ç»™ä½ ä½œä¸ºä¸€ä¸ªç»ƒä¹ æ¥å°è¯•ã€‚
- en: In the example used in the following GA on HalfCheetah section, we will implement
    the first improvement, whereas the second one is left as an optional exercise.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„GA on HalfCheetahéƒ¨åˆ†ä½¿ç”¨çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ç¬¬ä¸€ä¸ªæ”¹è¿›ï¼Œè€Œç¬¬äºŒä¸ªæ”¹è¿›åˆ™ä½œä¸ºä¸€ä¸ªå¯é€‰ç»ƒä¹ ã€‚
- en: Deep GA
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·±åº¦GA
- en: 'Being a gradient-free method, GA is potentially even more scalable than ES
    methods in terms of speed, with more CPUs involved in the optimization. However,
    the simple GA algorithm that you have seen has a similar bottleneck to ES methods:
    the policy parameters have to be exchanged between the workers. Such et al. (the
    authors) proposed a trick similar to the shared seed approach but taken to an
    extreme (as weâ€™re using seeds to track thousands of mutations). They called it
    deep GA, and at its core, the policy parameters are represented as a list of random
    seeds used to create this particular policyâ€™s weights.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§æ— æ¢¯åº¦æ–¹æ³•ï¼ŒGAåœ¨é€Ÿåº¦ä¸Šå¯èƒ½æ¯”ESæ–¹æ³•æ›´å…·å¯æ‰©å±•æ€§ï¼Œå› ä¸ºä¼˜åŒ–è¿‡ç¨‹æ¶‰åŠæ›´å¤šçš„CPUã€‚ç„¶è€Œï¼Œä½ çœ‹åˆ°çš„ç®€å•GAç®—æ³•åœ¨ä¸ESæ–¹æ³•ç›¸ä¼¼çš„ç“¶é¢ˆä¸Šä¹Ÿå­˜åœ¨é—®é¢˜ï¼šç­–ç•¥å‚æ•°å¿…é¡»åœ¨å·¥ä½œè€…ä¹‹é—´äº¤æ¢ã€‚Suchç­‰äººï¼ˆä½œè€…ï¼‰æå‡ºäº†ä¸€ä¸ªç±»ä¼¼äºå…±äº«ç§å­æ–¹æ³•çš„æŠ€å·§ï¼Œä½†ä»–ä»¬å°†å…¶æ¨å‘äº†æé™ï¼ˆå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ç§å­æ¥è·Ÿè¸ªæˆåƒä¸Šä¸‡çš„å˜å¼‚ï¼‰ã€‚ä»–ä»¬ç§°ä¹‹ä¸ºæ·±åº¦GAï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œç­–ç•¥å‚æ•°è¢«è¡¨ç¤ºä¸ºä¸€ç»„éšæœºç§å­çš„åˆ—è¡¨ï¼Œè¿™äº›ç§å­ç”¨äºåˆ›å»ºè¯¥ç­–ç•¥çš„æƒé‡ã€‚
- en: In fact, the initial networkâ€™s weights were generated randomly on the first
    population, so the first seed in the list defines this initialization. On every
    population, mutations are also fully specified by the random seed for every mutation.
    So, the only thing we need to reconstruct the weights is the seeds themselves.
    In this approach, we need to reconstruct the weights on every worker, but usually,
    this overhead is much less than the overhead of transferring full weights over
    the network.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œåˆå§‹ç½‘ç»œçš„æƒé‡æ˜¯åœ¨ç¬¬ä¸€æ¬¡ç§ç¾¤ä¸­éšæœºç”Ÿæˆçš„ï¼Œå› æ­¤åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªç§å­å®šä¹‰äº†è¿™ç§åˆå§‹åŒ–ã€‚åœ¨æ¯ä¸€ä»£ç§ç¾¤ä¸­ï¼Œå˜å¼‚ä¹Ÿå®Œå…¨ç”±æ¯ä¸ªå˜å¼‚çš„éšæœºç§å­æ¥æŒ‡å®šã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é‡æ„æƒé‡çš„å”¯ä¸€ä¿¡æ¯å°±æ˜¯è¿™äº›ç§å­æœ¬èº«ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ªå·¥ä½œè€…ä¸Šé‡æ„æƒé‡ï¼Œä½†é€šå¸¸ï¼Œè¿™ç§å¼€é”€è¿œå°äºåœ¨ç½‘ç»œä¸­ä¼ è¾“å®Œæ•´æƒé‡çš„å¼€é”€ã€‚
- en: Novelty search
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–°é¢–æ€§æœç´¢
- en: 'Another modification to the basic GA method is novelty search (NS), which was
    proposed by Lehman and Stanley in their paper, Abandoning objectives: Evolution
    through the search for novelty alone, which was published in 2011 [[LS11](#)].'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŸºæœ¬é—ä¼ ç®—æ³•ï¼ˆGAï¼‰æ–¹æ³•çš„å¦ä¸€ä¸ªä¿®æ”¹æ˜¯æ–°é¢–æ€§æœç´¢ï¼ˆNSï¼‰ï¼Œè¿™æ˜¯Lehmanå’ŒStanleyåœ¨ä»–ä»¬çš„è®ºæ–‡ã€Šæ”¾å¼ƒç›®æ ‡ï¼šä»…é€šè¿‡å¯»æ‰¾æ–°é¢–æ€§è¿›è¡Œè¿›åŒ–ã€‹ï¼ˆAbandoning
    objectives: Evolution through the search for novelty aloneï¼‰ä¸­æå‡ºçš„ï¼Œè¯¥è®ºæ–‡äº2011å¹´å‘å¸ƒ[[LS11](#)]ã€‚'
- en: The idea of NS is to change the objective in our optimization. Weâ€™re no longer
    trying to increase our total reward from the environment but, rather, reward the
    agent for exploring the behavior that it has never checked before (that is, novel).
    According to the authorsâ€™ experiments on the maze navigation problem, with many
    traps for the agent, NS works much better than other reward-driven approaches.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: NSçš„æ€æƒ³æ˜¯æ”¹å˜æˆ‘ä»¬ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ç›®æ ‡ã€‚æˆ‘ä»¬ä¸å†è¯•å›¾å¢åŠ æ¥è‡ªç¯å¢ƒçš„æ€»å¥–åŠ±ï¼Œè€Œæ˜¯å¥–åŠ±ä»£ç†æ¢ç´¢å®ƒä»¥å‰ä»æœªæ£€æŸ¥è¿‡çš„è¡Œä¸ºï¼ˆå³æ–°é¢–çš„è¡Œä¸ºï¼‰ã€‚æ ¹æ®ä½œè€…åœ¨è¿·å®«å¯¼èˆªé—®é¢˜ä¸­çš„å®éªŒï¼Œè¿·å®«ä¸­æœ‰è®¸å¤šé™·é˜±ï¼ŒNSæ¯”å…¶ä»–åŸºäºå¥–åŠ±çš„ç®—æ³•è¡¨ç°å¾—æ›´å¥½ã€‚
- en: To implement NS, we define the so-called behavior characteristic (BC) (Ï€), which
    describes the behavior of the policy and a distance between two BCs. Then, the
    k-nearest neighbors approach is used to check the novelty of the new policy and
    drive the GA according to this distance. In the paper by Such et al., sufficient
    exploration by the agent was needed. The approach of NS significantly outperformed
    the ES, GA, and other more traditional approaches to RL problems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°æ–°é¢–æ€§æœç´¢ï¼ˆNSï¼‰ï¼Œæˆ‘ä»¬å®šä¹‰äº†æ‰€è°“çš„è¡Œä¸ºç‰¹å¾ï¼ˆBCï¼‰ï¼ˆÏ€ï¼‰ï¼Œå®ƒæè¿°äº†ç­–ç•¥çš„è¡Œä¸ºå’Œä¸¤ä¸ªBCä¹‹é—´çš„è·ç¦»ã€‚ç„¶åï¼Œä½¿ç”¨kè¿‘é‚»æ–¹æ³•æ£€æŸ¥æ–°ç­–ç•¥çš„æ–°é¢–æ€§ï¼Œå¹¶æ ¹æ®è¿™ä¸ªè·ç¦»é©±åŠ¨é—ä¼ ç®—æ³•ã€‚åœ¨Suchç­‰äººçš„è®ºæ–‡ä¸­ï¼Œä»£ç†çš„å……åˆ†æ¢ç´¢æ˜¯å¿…éœ€çš„ã€‚NSæ–¹æ³•æ˜¾è‘—ä¼˜äºè¿›åŒ–ç­–ç•¥ï¼ˆESï¼‰ã€é—ä¼ ç®—æ³•ï¼ˆGAï¼‰å’Œå…¶ä»–æ›´ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚
- en: GA on HalfCheetah
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠçŒè±¹ä¸Šçš„é—ä¼ ç®—æ³•ï¼ˆGAï¼‰
- en: In our final example in this chapter, we will implement the parallelized deep
    GA on the HalfCheetah environment. The complete code is in 04_cheetah_ga.py. The
    architecture is very close to the parallel ES version, with one master process
    and several workers. The goal of every worker is to evaluate the batch of networks
    and return the result to the master, which merges partial results into the complete
    population, ranks the individuals according to the obtained reward, and generates
    the next population to be evaluated by the workers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„æœ€åä¸€ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨åŠçŒè±¹ç¯å¢ƒä¸­å®ç°å¹¶è¡ŒåŒ–æ·±åº¦é—ä¼ ç®—æ³•ï¼ˆGAï¼‰ã€‚å®Œæ•´ä»£ç è§04_cheetah_ga.pyã€‚æ¶æ„ä¸å¹¶è¡Œè¿›åŒ–ç­–ç•¥ï¼ˆESï¼‰ç‰ˆæœ¬éå¸¸ç›¸ä¼¼ï¼Œæœ‰ä¸€ä¸ªä¸»è¿›ç¨‹å’Œå¤šä¸ªå·¥ä½œè¿›ç¨‹ã€‚æ¯ä¸ªå·¥ä½œè¿›ç¨‹çš„ç›®æ ‡æ˜¯è¯„ä¼°ä¸€æ‰¹ç½‘ç»œå¹¶å°†ç»“æœè¿”å›ç»™ä¸»è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹å°†éƒ¨åˆ†ç»“æœåˆå¹¶æˆå®Œæ•´çš„ç§ç¾¤ï¼Œå¹¶æ ¹æ®è·å¾—çš„å¥–åŠ±å¯¹ä¸ªä½“è¿›è¡Œæ’åºï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªå¾…è¯„ä¼°çš„ç§ç¾¤ã€‚
- en: Every individual is encoded by a list of random seeds used to initialize the
    initial network weights and all subsequent mutations. This representation allows
    very compact encoding of the network, even when the number of parameters in the
    policy is not very large. For example, in our network with with one hidden layer
    of 64 neurons, we have 1,542 float values (the input is 17 values and the action
    is 6 floats, which gives 17 Ã— 64 + 64 + 64 Ã— 6 + 6 = 1542). Every float occupies
    4 bytes, which is the same size used by the random seed. So, the deep GA representation
    proposed by the paper will be smaller up to 1,542 generations in the optimization.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªä¸ªä½“ç”±ä¸€ä¸ªéšæœºç§å­åˆ—è¡¨ç¼–ç ï¼Œç”¨äºåˆå§‹åŒ–åˆå§‹ç½‘ç»œæƒé‡å’Œæ‰€æœ‰åç»­å˜å¼‚ã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼å…è®¸éå¸¸ç´§å‡‘åœ°ç¼–ç ç½‘ç»œï¼Œå³ä½¿åœ¨ç­–ç•¥ä¸­å‚æ•°æ•°é‡ä¸å¤šçš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«64ä¸ªç¥ç»å…ƒçš„éšè—å±‚çš„ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬æœ‰1542ä¸ªæµ®åŠ¨å€¼ï¼ˆè¾“å…¥ä¸º17ä¸ªå€¼ï¼ŒåŠ¨ä½œä¸º6ä¸ªæµ®åŠ¨å€¼ï¼Œå› æ­¤17
    Ã— 64 + 64 + 64 Ã— 6 + 6 = 1542ï¼‰ã€‚æ¯ä¸ªæµ®åŠ¨å€¼å ç”¨4ä¸ªå­—èŠ‚ï¼Œè¿™ä¸éšæœºç§å­ä½¿ç”¨çš„å¤§å°ç›¸åŒã€‚å› æ­¤ï¼Œè®ºæ–‡æå‡ºçš„æ·±åº¦é—ä¼ ç®—æ³•è¡¨ç¤ºæ–¹å¼å°†ä½¿ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ç§ç¾¤è§„æ¨¡æœ€å¤šç¼©å°åˆ°1542ä»£ã€‚
- en: Implementation
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ç°
- en: In our example, we will perform parallelization on local CPUs so the amount
    of data transferred back and forth doesnâ€™t matter much; however, if you have a
    couple of hundred cores to utilize, the representation might become a significant
    issue.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬åœ°CPUä¸Šè¿›è¡Œå¹¶è¡ŒåŒ–å¤„ç†ï¼Œå› æ­¤æ•°æ®æ¥å›ä¼ è¾“çš„æ•°é‡å¹¶ä¸å¤ªé‡è¦ï¼›ç„¶è€Œï¼Œå¦‚æœä½ æœ‰å‡ ç™¾ä¸ªæ ¸å¿ƒå¯ç”¨ï¼Œé‚£ä¹ˆè¿™ç§è¡¨ç¤ºæ–¹å¼å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªæ˜¾è‘—çš„é—®é¢˜ã€‚
- en: 'The set of hyperparameters is the same as in the CartPole example, with the
    difference of a larger population size:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°é›†ä¸CartPoleç¤ºä¾‹ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ç§ç¾¤è§„æ¨¡è¾ƒå¤§ï¼š
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'There are two functions used to build the networks based on the seeds given.
    The first one performs one mutation on the already created policy network:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ä¸ªå‡½æ•°ç”¨äºæ ¹æ®ç»™å®šçš„ç§å­æ„å»ºç½‘ç»œã€‚ç¬¬ä¸€ä¸ªå‡½æ•°å¯¹å·²ç»åˆ›å»ºçš„ç­–ç•¥ç½‘ç»œæ‰§è¡Œä¸€æ¬¡å˜å¼‚æ“ä½œï¼š
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding function can perform the mutation in place or by copying the target
    network based on arguments (copying is needed for the first generation).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„å‡½æ•°å¯ä»¥åŸåœ°æ‰§è¡Œå˜å¼‚ï¼Œæˆ–è€…æ ¹æ®å‚æ•°å¤åˆ¶ç›®æ ‡ç½‘ç»œï¼ˆå¯¹äºç¬¬ä¸€ä»£éœ€è¦å¤åˆ¶ï¼‰ã€‚
- en: 'The second function creates the network from scratch using the list of seeds:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå‡½æ•°ä»å¤´å¼€å§‹ä½¿ç”¨ç§å­åˆ—è¡¨åˆ›å»ºç½‘ç»œï¼š
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, the first seed is passed to PyTorch to influence the network initialization,
    and subsequent seeds are used to apply network mutations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œç¬¬ä¸€ä¸ªç§å­ä¼ é€’ç»™ PyTorchï¼Œç”¨äºå½±å“ç½‘ç»œåˆå§‹åŒ–ï¼Œåç»­çš„ç§å­ç”¨äºåº”ç”¨ç½‘ç»œçªå˜ã€‚
- en: 'The worker function obtains the list of seeds to evaluate and outputs individual
    OutputItem dataclass items for every result obtained:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: worker å‡½æ•°è·å–å¾…è¯„ä¼°çš„ç§å­åˆ—è¡¨ï¼Œå¹¶ä¸ºæ¯ä¸ªè·å¾—çš„ç»“æœè¾“å‡ºå•ç‹¬çš„ OutputItem æ•°æ®ç±»é¡¹ï¼š
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This function maintains the cache of networks to minimize the amount of time
    spent recreating the parameters from the list of seeds. This cache is cleared
    for every generation, as every new generation is created from the current generation
    winners, so there is only a tiny chance that old networks can be reused from the
    cache.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°ç»´æŠ¤äº†ç½‘ç»œçš„ç¼“å­˜ï¼Œä»¥æœ€å°åŒ–é‡æ–°åˆ›å»ºç§å­åˆ—è¡¨ä¸­å‚æ•°æ‰€èŠ±è´¹çš„æ—¶é—´ã€‚æ¯æ¬¡ç”Ÿæˆéƒ½ä¼šæ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºæ¯ä¸€ä»£æ–°ç½‘ç»œéƒ½æ˜¯ä»å½“å‰ä»£çš„èµ¢å®¶ä¸­åˆ›å»ºçš„ï¼Œæ‰€ä»¥æ—§ç½‘ç»œä»ç¼“å­˜ä¸­å¤ç”¨çš„å¯èƒ½æ€§éå¸¸å°ã€‚
- en: 'The code of the master process is also straightforward:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¿›ç¨‹çš„ä»£ç ä¹Ÿå¾ˆç®€å•ï¼š
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: For every generation, we send the current populationâ€™s seeds to workers for
    evaluation and wait for the results. Then, we sort the results and generate the
    next population based on the top performers. On the masterâ€™s side, the mutation
    is just a seed number generated randomly and appended to the list of seeds of
    the parent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸€ä»£ï¼Œæˆ‘ä»¬å°†å½“å‰ç§ç¾¤çš„ç§å­å‘é€ç»™å·¥ä½œè€…è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç­‰å¾…ç»“æœã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ç»“æœè¿›è¡Œæ’åºï¼Œå¹¶åŸºäºè¡¨ç°æœ€å¥½çš„ä¸ªä½“ç”Ÿæˆä¸‹ä¸€ä»£ã€‚åœ¨ä¸»è¿›ç¨‹ç«¯ï¼Œçªå˜åªæ˜¯ä¸€ä¸ªéšæœºç”Ÿæˆçš„ç§å­ç¼–å·ï¼Œè¿½åŠ åˆ°çˆ¶ä»£çš„ç§å­åˆ—è¡¨ä¸­ã€‚
- en: Results
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: In this example, weâ€™re using the MuJoCo HalfCheetah environment, which doesnâ€™t
    have any health checks internally, so every episode takes 2,000 steps. Because
    of this, every training step requires about a minute, so be patient. After 300
    mutation rounds (which took about 7 hours), the best policy was able to get a
    reward of 6454, which is a great result. If you remember our experiments in the
    previous chapter, only the SAC method was able to get a higher reward of 7063
    on MuJoCo HalfCheetah. Of course, HalfCheetah is not very challenging, but still
    â€” very good.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ MuJoCo HalfCheetah ç¯å¢ƒï¼Œå®ƒå†…éƒ¨æ²¡æœ‰å¥åº·æ£€æŸ¥ï¼Œå› æ­¤æ¯ä¸ªå›åˆéœ€è¦ 2,000 æ­¥ã€‚ç”±äºè¿™ä¸ªåŸå› ï¼Œæ¯ä¸ªè®­ç»ƒæ­¥éª¤å¤§çº¦éœ€è¦ä¸€åˆ†é’Ÿï¼Œå› æ­¤éœ€è¦è€å¿ƒç­‰å¾…ã€‚åœ¨
    300 è½®çªå˜åï¼ˆå¤§çº¦ç”¨äº† 7 å°æ—¶ï¼‰ï¼Œæœ€ä½³ç­–ç•¥è·å¾—äº† 6454 çš„å¥–åŠ±ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç»“æœã€‚å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬åœ¨ä¸Šä¸€ç« çš„å®éªŒï¼Œåªæœ‰ SAC æ–¹æ³•èƒ½åœ¨ MuJoCo
    HalfCheetah ä¸Šè·å¾—æ›´é«˜çš„å¥–åŠ± 7063ã€‚å½“ç„¶ï¼ŒHalfCheetah çš„æŒ‘æˆ˜æ€§ä¸å¤§ï¼Œä½†ä»ç„¶â€”â€”éå¸¸å¥½ã€‚
- en: The plots are shown in FigureÂ [17.5](#x1-326002r5) and FigureÂ [17.6](#x1-326003r6).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨è§å›¾ [17.5](#x1-326002r5) å’Œå›¾ [17.6](#x1-326003r6)ã€‚
- en: '![PIC](img/B22150_17_05.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_05.png)'
- en: 'FigureÂ 17.5: The maximum (left) and mean rewards (right) for GA on HalfCheetah'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.5ï¼šGA åœ¨ HalfCheetah ä¸Šçš„æœ€å¤§ï¼ˆå·¦ï¼‰å’Œå¹³å‡ï¼ˆå³ï¼‰å¥–åŠ±
- en: '![PIC](img/B22150_17_06.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_06.png)'
- en: 'FigureÂ 17.6: Standard deviation of reward for GA on HalfCheetah'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17.6ï¼šGA åœ¨ HalfCheetah ä¸Šçš„å¥–åŠ±æ ‡å‡†å·®
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'In this chapter, you saw two examples of black-box optimization methods: evolution
    strategies and genetic algorithms, which can provide competition for other analytical
    gradient methods. Their strength lies in good parallelization on a large number
    of resources and the smaller number of assumptions that they have on the reward
    function.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œä½ çœ‹åˆ°äº†ä¸¤ç§é»‘ç›’ä¼˜åŒ–æ–¹æ³•çš„ç¤ºä¾‹ï¼šè¿›åŒ–ç­–ç•¥å’Œé—ä¼ ç®—æ³•ï¼Œå®ƒä»¬å¯ä»¥ä¸å…¶ä»–åˆ†ææ¢¯åº¦æ–¹æ³•ç«äº‰ã€‚å®ƒä»¬çš„ä¼˜åŠ¿åœ¨äºå¯ä»¥åœ¨å¤§é‡èµ„æºä¸Šè¿›è¡Œè‰¯å¥½çš„å¹¶è¡ŒåŒ–ï¼Œå¹¶ä¸”å¯¹å¥–åŠ±å‡½æ•°çš„å‡è®¾è¾ƒå°‘ã€‚
- en: 'In the next chapter, we will take a look at a very important aspect of RL:
    advanced exploration methods.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªéå¸¸é‡è¦çš„æ–¹é¢ï¼šé«˜çº§æ¢ç´¢æ–¹æ³•ã€‚
