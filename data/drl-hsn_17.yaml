- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Black-Box Optimizations in RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的黑盒优化
- en: 'In this chapter, we will change our perspective on reinforcement learning (RL)
    training again and switch to the so-called black-box optimizations. These methods
    are at least a decade old, but recently, several research studies were conducted
    that showed their applicability to large-scale RL problems and their competitiveness
    with the value iteration and policy gradient methods. Despite their age, this
    family of methods is still more efficient in some situations. In particular, this
    chapter will cover two examples of black-box optimization methods:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将再次改变对强化学习（RL）训练的看法，转向所谓的黑盒优化方法。这些方法至少已有十年历史，但最近进行的一些研究表明，它们在大规模RL问题中的适用性，并且与价值迭代和策略梯度方法具有竞争力。尽管它们已经有些年头，但在某些情况下，这类方法仍然更为高效。本章将介绍两种黑盒优化方法的例子：
- en: Evolution strategies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化策略
- en: Genetic algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗传算法
- en: Black-box methods
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黑盒方法
- en: To begin with, let’s discuss the whole family of black-box methods and how it
    differs from what we’ve covered so far. Black-box optimization methods are the
    general approach to the optimization problem, when you treat the objective that
    you’re optimizing as a black box, without any assumptions about the differentiability,
    the value function, the smoothness of the objective, and so on. The only requirement
    that those methods expose is the ability to calculate the fitness function, which
    should give us the measure of suitability of a particular instance of the optimized
    entity at hand. One of the simplest examples in this family is random search,
    which is when you randomly sample the thing you’re looking for (in the case of
    RL, it’s the policy, π(a|s)), check the fitness of this candidate, and if the
    result is good enough (according to some reward criteria), then you’re done. Otherwise,
    you repeat the process again and again. Despite the simplicity and even naivety
    of this approach, especially when compared to the sophisticated methods that you’ve
    seen so far, this is a good example to illustrate the idea of black-box methods.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论一下黑盒方法的整体家族，以及它与我们之前讨论过的内容的区别。黑盒优化方法是优化问题的通用方法，它将你正在优化的目标视为黑盒，不对目标的可微性、价值函数、目标的平滑性等做任何假设。唯一的要求是这些方法能够计算适应度函数，这应该为我们提供优化实体的特定实例的适应度度量。这个家族中最简单的一个例子是随机搜索，它是指你随机选择你要寻找的对象（在RL中是策略π(a|s)），检查该候选对象的适应度，如果结果足够好（根据某些奖励标准），那么你就完成了。否则，你会一遍又一遍地重复这个过程。尽管这种方法简单，甚至有些天真，特别是与到目前为止你所看到的复杂方法相比，但它是一个很好的例子，能说明黑盒方法的思想。
- en: 'Furthermore, with some modifications, as you will see shortly, this simple
    approach can be compared in terms of efficiency and the quality of the resulting
    policies to the deep Q-network (DQN) and policy gradient methods. In addition
    to that, black-box methods have several very appealing properties:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过一些修改，正如你很快会看到的，这种简单的方法在效率和生成的策略质量方面，可以与深度Q网络（DQN）和策略梯度方法相比较。除此之外，黑盒方法还有一些非常吸引人的特性：
- en: They are at least two times faster than gradient-based methods, as we don’t
    need to perform the backpropagation step to obtain the gradients.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们至少比基于梯度的方法快两倍，因为我们不需要执行反向传播步骤来获得梯度。
- en: There are very few assumptions about the optimized objective and the policy
    that are treated as a black box. Traditional methods struggle with situations
    when the reward function is non-smooth or the policy contains steps with random
    choice. All of this is not an issue for black-box methods, as they don’t expect
    much from the black-box internals.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于优化目标和作为黑盒处理的策略，几乎没有任何假设。传统方法在奖励函数不平滑或策略中包含随机选择步骤时会遇到困难。而这一切对于黑盒方法来说都不是问题，因为它们对黑盒的内部实现没有太多要求。
- en: The methods can generally be parallelized very well. For example, the aforementioned
    random search can easily scale up to thousands of central processing units (CPUs)
    or graphics processing units (GPUs) working in parallel, without any dependency
    on each other. This is not the case for DQN or policy gradient methods, when you
    need to accumulate the gradients and propagate the current policy to all parallel
    workers, which decreases the parallelism.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法通常可以很好地并行化。例如，上述的随机搜索可以轻松扩展到数千个中央处理单元（CPU）或图形处理单元（GPU）并行工作，且彼此之间没有任何依赖关系。对于
    DQN 或策略梯度方法而言，情况则不同，因为需要积累梯度并将当前策略传播给所有并行工作者，这会降低并行性。
- en: The downside of the preceding is usually lower sample efficiency. In particular,
    the naïve random search of the policy, parameterized with the neural network (NN)
    with half a million parameters, has a very low probability of succeeding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法的缺点通常是样本效率较低。特别是，使用具有五十万个参数的神经网络（NN）进行的天真随机搜索策略，成功的概率非常低。
- en: Evolution strategies
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化策略
- en: One subset of black-box optimization methods is called evolution strategies
    (ES), and it was inspired by the evolution process. With ES, the most successful
    individuals have the highest influence on the overall direction of the search.
    There are many different methods that fall into this class, and in this chapter,
    we will consider the approach taken by the OpenAI researchers Salimans et al.
    in their paper, Evolution strategies as a scalable alternative to reinforcement
    learning [[Sal+17](#)], published in March 2017.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒优化方法的一类被称为进化策略（ES），它的灵感来源于进化过程。在 ES 中，最成功的个体对搜索的整体方向有最大的影响。这个类别下有许多不同的方法，在本章中，我们将讨论
    OpenAI 研究人员 Salimans 等人在他们的论文《进化策略作为强化学习的可扩展替代方法》[[Sal+17](#)]中提出的方法，该论文于2017年3月发布。
- en: The underlying idea of ES methods is that on every iteration, we perform random
    perturbation of our current policy parameters and evaluate the resulting policy
    fitness function. Then, we adjust the policy weights proportionally to the relative
    fitness function value.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ES 方法的基本思想是在每次迭代时，我们对当前的策略参数进行随机扰动，并评估由此产生的策略适应度函数。然后，我们根据相对的适应度函数值调整策略权重。
- en: The concrete method used by Salimans et al. is called covariance matrix adaptation
    evolution strategy (CMA-ES), in which the perturbation performed is the random
    noise sampled from the normal distribution with the zero mean and identity variance.
    Then, we calculate the fitness function of the policy with weights equal to the
    weights of the original policy plus the scaled noise. Next, according to the obtained
    value, we adjust the original policy weights by adding the noise multiplied by
    the fitness function value, which moves our policy toward weights with a higher
    value of the fitness function. To improve the stability, the update of the weights
    is performed by averaging the batch of such steps with different random noise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Salimans 等人使用的具体方法被称为协方差矩阵适应进化策略（CMA-ES），其中执行的扰动是从均值为零、方差为单位矩阵的正态分布中采样的随机噪声。然后，我们计算具有权重等于原始策略权重加上缩放噪声的策略的适应度函数值。接下来，根据得到的值，我们通过将噪声乘以适应度函数值来调整原始策略权重，这样可以使我们的策略朝着具有更高适应度函数值的权重方向移动。为了提高稳定性，权重更新是通过对具有不同随机噪声的多个步骤批次进行平均来完成的。
- en: 'More formally, this method could be expressed as the following sequence of
    steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，这个方法可以表达为以下步骤序列：
- en: Initialize the learning rate, α, the noise standard deviation, σ, and the initial
    policy parameters, 𝜃[0].
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化学习率 α、噪声标准差 σ 和初始策略参数 𝜃[0]。
- en: 'For t = 0,1,… perform:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 t = 0,1,… 执行：
- en: 'The sample batch of noise with the shape of the weights from the normal distribution
    with zero mean and variance of one: 𝜖[1],…,𝜖[n] ∼𝒩(0,I)'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 噪声样本批次，其形状与来自均值为零、方差为一的正态分布的权重相同：𝜖[1],…,𝜖[n] ∼𝒩(0,I)
- en: Compute returns F[i] = F(𝜃[t] + σ𝜖[i]) for i = 1,…,n
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报 F[i] = F(𝜃[t] + σ𝜖[i])，其中 i = 1,…,n
- en: 'Update weights:'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq69.png)'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq69.png)'
- en: This algorithm is the core of the method presented in the paper, but, as usual
    in the RL domain, the method alone is not enough to obtain good results. So, the
    paper includes several tweaks to improve the method, although the core is the
    same.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法是论文中提出方法的核心，但像强化学习领域中的常见情况一样，单靠这个方法不足以获得良好的结果。因此，论文中包含了几个调整以改善方法，尽管核心方法保持不变。
- en: Implementing ES on CartPole
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 CartPole 上实现 ES
- en: 'Let’s implement and test the method from the paper on our fruit fly environment:
    CartPole. You’ll find the complete example in Chapter17/01_cartpole_es.py.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的果蝇环境中实现并测试论文中的方法：倒立摆（CartPole）。你可以在 Chapter17/01_cartpole_es.py 文件中找到完整的例子。
- en: In this example, we will use the single environment to check the fitness of
    the perturbed network weights. Our fitness function will be the undiscounted total
    reward for the episode.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用单一环境来检查扰动后的网络权重的适应度。我们的适应度函数将是该回合的未折扣总奖励。
- en: 'We start with the imports:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入开始：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: From the import statements, you will notice how self-contained our example is.
    We’re not using PyTorch optimizers, as we don’t perform backpropagation at all.
    In fact, we could avoid using PyTorch completely and work only with NumPy, as
    the only thing we use PyTorch for is to perform a forward pass and calculate the
    network’s output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从导入语句中，你可以看到我们的例子是多么自包含。我们没有使用 PyTorch 优化器，因为我们根本不进行反向传播。事实上，我们完全可以避免使用 PyTorch，只使用
    NumPy，因为我们唯一用 PyTorch 的地方就是执行前向传播并计算网络的输出。
- en: 'Next, we define the hyperparameters:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义超参数：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The number of hyperparameters is also small and includes the following values:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的数量也很少，包括以下几个值：
- en: 'MAX_BATCH_EPISODES and MAX_BATCH_STEPS: The limit of episodes and steps we
    use for training'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAX_BATCH_EPISODES 和 MAX_BATCH_STEPS：用于训练的回合数和步骤数的限制
- en: 'NOISE_STD: The standard deviation, σ, of the noise used for weight perturbation'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NOISE_STD：用于权重扰动的噪声标准差，σ
- en: 'LEARNING_RATE: The coefficient used to adjust the weights on the training step'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LEARNING_RATE：用于调整训练步骤中权重的系数
- en: We alse define a type alias for list of tensors containing weights’ noises.
    It will simplify the code, as we’ll deal with noise a lot.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个类型别名，表示包含权重噪声的张量列表。这样可以简化代码，因为我们将会频繁处理噪声。
- en: 'Now let’s check the network:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查网络：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model we’re using is a simple one-hidden-layer NN, which gives us the action
    to take from the observation. We’re using PyTorch NN machinery here only for convenience,
    as we need only the forward pass, but it could be replaced by the multiplication
    of matrices and nonlinearities application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的模型是一个简单的单隐层神经网络（NN），它根据观察结果给出需要采取的行动。我们在这里使用 PyTorch 的神经网络模块仅是为了方便，因为我们只需要前向传播，但它也可以通过矩阵乘法和非线性应用来替代。
- en: 'The evaluate() function plays a full episode using the given policy and returns
    the total reward and the number of steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: evaluate() 函数使用给定的策略进行完整的回合，并返回总奖励和步数：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The reward will be used as a fitness value, while the count of steps is needed
    to limit the amount of time we spend on forming the batch. The action selection
    is performed deterministically by calculating argmax from the network output.
    In principle, we could do the random sampling from the distribution, but we’ve
    already performed the exploration by adding noise to the network parameters, so
    the deterministic action selection is fine here.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励将作为适应度值，而步数则用于限制我们花费在形成批次上的时间。行动选择通过计算网络输出的最大值（argmax）以确定性方式进行。原则上，我们可以从分布中进行随机采样，但我们已经通过向网络参数添加噪声来进行探索，因此在这里进行确定性行动选择是可以的。
- en: 'In the sample_noise() function, we create random noise with zero mean and unit
    variance equal to the shape of our network parameters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 sample_noise() 函数中，我们创建了均值为零、方差为一的随机噪声，其形状与我们的网络参数相同：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function returns two sets of noise tensors: one with positive noise and
    another with the same random values taken with a negative sign. These two samples
    are later used in a batch as independent samples. This technique is known as mirrored
    sampling and is used to improve the stability of the convergence. In fact, without
    the negative noise, the convergence becomes very unstable because positive noise
    pushes weights in a single direction.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回两组噪声张量：一组是正噪声，另一组是取负号后的相同随机值。这两组样本稍后会作为独立样本放入一个批次中。这种技术称为镜像采样，用来提高收敛的稳定性。实际上，如果没有负噪声，收敛会变得非常不稳定，因为正噪声会将权重推向单一方向。
- en: 'The eval_with_noise() function takes the noise array created by sample_noise()
    and evaluates the network with noise added:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: eval_with_noise() 函数接受由 sample_noise() 创建的噪声数组，并在添加噪声后评估网络：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To achieve this, we add the noise to the network’s parameters and call the evaluate
    function to obtain the reward and number of steps taken. After this, we need to
    restore the network weights to their original state, which is completed by loading
    the state dictionary of the network.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将噪声加到网络的参数上，并调用评估函数来获得奖励和所采取的步骤数。之后，我们需要恢复网络的权重到其原始状态，这通过加载网络的状态字典来完成。
- en: 'The last and the central function of the method is train_step(), which takes
    the batch with noise and respective rewards and calculates the update to the network
    parameters by applying the formula:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 方法的最后一个也是核心的函数是`train_step()`，它接受带噪声和相应奖励的批次，并通过应用公式计算对网络参数的更新：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq69.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq69.png)'
- en: 'This can be implemented as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下方式实现：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the beginning, we normalize rewards to have zero mean and unit variance,
    which improves the stability of the method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，我们对奖励进行归一化，使其均值为零，方差为一，这有助于提高方法的稳定性。
- en: 'Then, we iterate every pair (noise, reward) in our batch and multiply the noise
    values with the normalized reward, summing together the respective noise for every
    parameter in our policy:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历批次中的每一对（噪声，奖励），将噪声值与归一化的奖励相乘，并将每个参数的相应噪声相加：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As a final step, we use the accumulated scaled noise to adjust the network
    parameters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用积累的缩放噪声来调整网络参数：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Technically, what we do here is a gradient ascent, although the gradient was
    not obtained from backpropagation but from the random sampling (also known as
    Monte Carlo sampling). This fact was also demonstrated by Salimans et al., where
    the authors showed that CMA-ES is very similar to the policy gradient methods,
    differing in just the way that we get the gradients’ estimation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们在这里做的是梯度上升，尽管梯度并不是通过反向传播获得的，而是通过随机采样（也称为蒙特卡洛采样）得到的。Salimans等人也证明了这一点，作者展示了CMA-ES与策略梯度方法非常相似，区别仅在于我们获得梯度估计的方式。
- en: 'The preparation before the training loop is simple; we create the environment
    and the network:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环前的准备工作很简单；我们创建环境和网络：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Every iteration of the training loop starts with batch creation, where we sample
    the noise and obtain rewards for both positive and negated noise:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每次训练循环的迭代都从批次创建开始，在这里我们对噪声进行采样并获得正向和反向噪声的奖励：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When we reach the limit of episodes in the batch, or the limit of the total
    steps, we stop gathering the data and do a training update.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们达到批次的最大集数，或者总步数的上限时，我们停止收集数据并进行训练更新。
- en: 'To perform the update of the network, we call the train_step() function that
    we’ve already seen:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行网络的更新，我们调用已经看到的`train_step()`函数：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The goal of the train_step() function is to scale the noise according to the
    total reward and then adjust the policy weights in the direction of the averaged
    noise.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_step()`函数的目标是根据总奖励来缩放噪声，然后将策略权重调整到平均噪声的方向。'
- en: 'The final steps in the training loop write metrics into TensorBoard and show
    the training progress on the console:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环的最后步骤将度量数据写入 TensorBoard，并在控制台上显示训练进度：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: CartPole results
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPole 结果
- en: 'Training can be started by just running the program without the arguments:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可以通过直接运行程序而不需要参数来开始：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: From my experiments, it usually takes ES about 40–60 batches to solve CartPole.
    The convergence dynamics for the preceding run are shown in Figure [17.1](#x1-315020r1)
    and Figure [17.2](#x1-315021r2).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的实验，ES 通常需要大约40到60个批次来解决 CartPole。前面运行的收敛动态如图[17.1](#x1-315020r1)和图[17.2](#x1-315021r2)所示。
- en: '![PIC](img/B22150_17_01.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_01.png)'
- en: 'Figure 17.1: The maximum reward (left) and policy update (right) for ES on
    CartPole'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：ES在CartPole上的最大奖励（左）和策略更新（右）
- en: '![PIC](img/B22150_17_02.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_02.png)'
- en: 'Figure 17.2: The mean (left) and standard deviation (right) of reward for ES
    on CartPole'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：ES在CartPole上的奖励均值（左）和标准差（右）
- en: The preceding graphs looks quite good—being able to solve the environment in
    30 seconds is on par with the cross-entropy method from Chapter [4](ch008.xhtml#x1-740004).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表看起来相当不错——在30秒内解决环境问题与第[4章](ch008.xhtml#x1-740004)中的交叉熵方法相当。
- en: ES on HalfCheetah
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ES 在 HalfCheetah 上的表现
- en: In the next example, we will go beyond the simplest ES implementation and look
    at how this method can be parallelized efficiently using the shared seed strategy
    proposed by Salimans et al. To show this approach, we will use the HalfCheetah
    environment using the MuJoCo physics simulator. We already experimented with it
    in the previous chapter, so if you haven’t installed the gymnasium[mujoco] package,
    you should do so.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将超越最简单的ES实现，探讨如何使用Salimans等人提出的共享种子策略高效地并行化该方法。为了展示这种方法，我们将使用MuJoCo物理模拟器中的HalfCheetah环境。我们已经在前一章中进行了实验，因此如果你还没有安装gymnasium[mujoco]包，应该先安装。
- en: First, let’s discuss the idea of shared seeds. The performance of the ES algorithm
    is mostly determined by the speed at which we can gather our training batch, which
    consists of sampling the noise and checking the total reward of the perturbed
    noise. As our training batch items are independent, we can easily parallelize
    this step to a large number of workers sitting on remote machines. (That’s a bit
    similar to the example from Chapter [12](ch016.xhtml#x1-20300012), when we gathered
    gradients from A3C workers.) However, naïve implementation of this parallelization
    will require a large amount of data to be transferred from the worker process
    to the central master, which is supposed to combine the noise checked by the workers
    and perform the policy update. Most of this data is the noise vectors, the size
    of which is equal to the size of our policy parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论共享种子的概念。ES算法的性能主要取决于我们收集训练批次的速度，训练批次由采样噪声和检查扰动噪声的总奖励组成。由于我们的训练批次项是独立的，我们可以轻松地将这一步并行化到大量位于远程机器上的工人（这有点类似于第[12](ch016.xhtml#x1-20300012)章中的示例，当时我们从A3C工人那里收集梯度）。然而，天真的并行化实现将需要将大量数据从工人进程传输到中央主进程，主进程应当合并工人检查过的噪声并执行策略更新。大部分数据是噪声向量，其大小等于我们策略参数的大小。
- en: To avoid this overhead, a quite elegant solution was proposed by Salimans et
    al. As noise sampled on a worker is produced by a pseudo-random number generator,
    which allows us to set the random seed and reproduce the random sequence generated,
    the worker can transfer to the master only the seed that was used to generate
    the noise. Then, the master can generate the same noise vector again using the
    seed. Of course, the seed on every worker needs to be generated randomly to still
    have a random optimization process. This has the effect of dramatically decreasing
    the amount of data that needs to be transferred from workers to the master, improving
    the scalability of the method. For example, Salimans et al. reported linear speed
    up in optimizations involving 1,440 CPUs in the cloud. In our example, we will
    look at local parallelization using the same approach.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种开销，Salimans等人提出了一个相当优雅的解决方案。由于工人上采样的噪声是由伪随机数生成器产生的，这使得我们可以设置随机种子并重现生成的随机序列，因此工人只需将用于生成噪声的种子传输给主进程。然后，主进程可以使用该种子再次生成相同的噪声向量。当然，每个工人的种子需要随机生成，以保持随机优化过程的性质。这样做的效果是显著减少了需要从工人传输到主进程的数据量，从而提高了方法的可扩展性。例如，Salimans等人报告了在云端使用1,440个CPU进行优化时的线性加速。在我们的示例中，我们将使用相同的方法进行本地并行化。
- en: Implementing ES on HalfCheetah
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在HalfCheetah上实现ES
- en: The code is placed in Chapter17/02_cheetah_es.py. As the code significantly
    overlaps with the CartPole version, we will focus here only on the differences.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于Chapter17/02_cheetah_es.py中。由于代码与CartPole版本有显著重叠，我们在此只关注其中的差异。
- en: 'We will begin with the worker, which is started as a separate process using
    the PyTorch multiprocessing wrapper. The worker’s responsibilities are simple:
    for every iteration, it obtains the network parameters from the master process,
    and then it performs the fixed number of iterations, where it samples the noise
    and evaluates the reward. The result with the random seed is sent to the master
    using the queue.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从工人开始，工人作为单独的进程使用PyTorch的多进程封装器启动。工人的职责很简单：每次迭代时，它从主进程获取网络参数，然后执行固定次数的迭代，在每次迭代中它采样噪声并评估奖励。带有随机种子的结果通过队列发送给主进程。
- en: 'The following dataclass is used by the worker to send the results of the perturbed
    policy evaluation to the master process:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数据类由工人使用，用于将扰动策略评估的结果发送到主进程：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It includes the random seed, the rewards obtained with the positive and negative
    noise, and the total number of steps we performed in both tests.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括随机种子、通过正负噪声获得的奖励，以及我们在两个测试中执行的总步骤数。
- en: 'On every training iteration, the worker waits for the network parameters to
    be broadcasted from the master:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每次训练迭代时，工作程序会等待主控程序广播网络参数：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The value of None means that the master wants to stop the worker.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: None 的值表示主控程序希望停止工作程序。
- en: 'The rest is almost the same as the previous example, with the main difference
    being in the random seed generated and assigned before the noise generation. This
    allows the master to regenerate the same noise, only from the seed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分几乎与前一个示例相同，主要的区别在于在噪声生成之前生成并分配的随机种子。这使得主控程序能够重新生成相同的噪声，只是从种子开始：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Another difference lies in the function used by the master to perform the training
    step:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别在于主控程序执行训练步骤时使用的函数：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the CartPole example, we normalized the batch of rewards by subtracting the
    mean and dividing by the standard deviation. According to Salimans et al., better
    results could be obtained using ranks instead of actual rewards. As ES has no
    assumptions about the fitness function (which is a reward in our case), we can
    make any rearrangements in the reward that we want, which wasn’t possible in the
    case of DQN, for example.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CartPole 示例中，我们通过减去均值并除以标准差对奖励批次进行了归一化。根据 Salimans 等人的说法，使用秩而非实际奖励可以获得更好的结果。由于
    ES 对适应度函数（在我们的案例中即为奖励）没有假设，我们可以对奖励进行任何重新排列，这在 DQN 的情况下是不可行的。
- en: Here, rank transformation of the array means replacing the array with indices
    of the sorted array. For example, array [0.1, 10, 0.5] will have the rank array
    [0, 2, 1]. The compute_centered_ranks function takes the array with the total
    rewards of the batch, calculates the rank for every item in the array, and then
    normalizes those ranks. For example, an input array of [21.0, 5.8, 7.0] will have
    ranks [2, 0, 1], and the final centered ranks will be [0.5, -0.5, 0.0].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数组的秩变换意味着用排序数组的索引替换原数组。例如，数组 [0.1, 10, 0.5] 将变为秩数组 [0, 2, 1]。`compute_centered_ranks`
    函数接受一个包含批次总奖励的数组，计算数组中每个项目的秩，然后对这些秩进行归一化。例如，输入数组 [21.0, 5.8, 7.0] 将得到秩 [2, 0,
    1]，最终的居中秩将是 [0.5, -0.5, 0.0]。
- en: 'Another major difference in the training function is the use of PyTorch optimizers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练函数的另一个主要区别是使用了 PyTorch 优化器：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To understand why they are used and how this is possible without doing backpropagation,
    some explanations are required.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么使用这些方法以及如何在不进行反向传播的情况下实现这一点，必须做一些解释。
- en: First, Salimans et al. showed that the optimization method used by the ES algorithm
    is very similar to gradient ascent on the fitness function, with the difference
    being how the gradient is calculated. The way the stochastic gradient descent
    (SGD) method is usually applied is that the gradient is obtained from the loss
    function by calculating the derivative of the network parameters with respect
    to the loss value. This imposes the limitation on the network and loss function
    to be differentiable, which is not always the case; for example, the rank transformation
    performed by the ES method is not differentiable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Salimans 等人表明，ES 算法使用的优化方法与梯度上升法非常相似，区别在于梯度的计算方式。通常应用随机梯度下降（SGD）方法时，梯度是通过计算网络参数关于损失值的导数从损失函数中获得的。这要求网络和损失函数是可微的，但这并非总是成立；例如，ES
    方法执行的秩变换是不可微的。
- en: 'On the other hand, optimization performed by ES works differently. We randomly
    sample the neighborhood of our current parameters by adding the noise to them
    and calculating the fitness function. According to the fitness function change,
    we adjust the parameters, which pushes our parameters in the direction of a higher
    fitness function. The result of this is very similar to gradient-based methods,
    but the requirements imposed on our fitness function are much looser: the only
    requirement is our ability to calculate it.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ES 执行的优化过程则有所不同。我们通过向当前参数添加噪声并计算适应度函数，随机采样周围的邻域。根据适应度函数的变化，我们调整参数，推动参数朝着更高的适应度函数方向前进。其结果与基于梯度的方法非常相似，但对适应度函数的要求要宽松得多：唯一的要求是能够计算它。
- en: However, if we’re estimating some kind of gradient by randomly sampling the
    fitness function, we can use standard optimizers from PyTorch. Normally, optimizers
    adjust the parameters of the network using gradients accumulated in the parameters’
    grad fields.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们通过随机采样适应度函数来估计某种梯度，我们可以使用 PyTorch 中的标准优化器。通常，优化器使用积累在参数 `grad` 字段中的梯度来调整网络的参数。
- en: 'Those gradients are accumulated after the backpropagation step, but due to
    PyTorch’s flexibility, the optimizer doesn’t care about the source of the gradients.
    So, the only thing we need to do is copy the estimated parameters’ update in the
    grad fields and ask the optimizer to update them. Note that the update is copied
    with a negative sign, as optimizers normally perform gradient descent (as in a
    normal operation, we minimize the loss function), but in this case, we want to
    do gradient ascent. This is very similar to the actor-critic method we covered
    in Chapter [12](ch016.xhtml#x1-20300012), when the estimated policy gradient is
    taken with the negative sign, as it shows the direction to improve the policy.
    The last chunk of differences in the code is taken from the training loop performed
    by the master process. Its responsibility is to wait for data from worker processes,
    perform the training update of the parameters, and broadcast the result to the
    workers. The communication between the master and workers is performed by two
    sets of queues. The first queue is per-worker and is used by the master to send
    the current policy parameters to use. The second queue is shared by the workers
    and is used to send the already mentioned RewardItem structure with the random
    seed and rewards:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些梯度在反向传播步骤后被积累，但由于 PyTorch 的灵活性，优化器不关心梯度的来源。因此，我们需要做的唯一事情就是将估计的参数更新复制到 `grad`
    字段，并要求优化器更新它们。请注意，更新是带有负号的，因为优化器通常执行梯度下降（如同正常操作中我们最小化损失函数一样），但在这种情况下，我们希望执行梯度上升。这与我们在第
    [12](ch016.xhtml#x1-20300012) 章中涉及的演员-评论员方法非常相似，其中估计的策略梯度带有负号，因为它显示了改进策略的方向。代码中的最后一部分差异来自主进程执行的训练循环。它的职责是等待来自工作进程的数据，执行参数的训练更新，并将结果广播到工作进程。主进程和工作进程之间的通信通过两组队列来完成。第一组队列是每个工作进程的队列，用于主进程发送当前使用的策略参数。第二组队列是由工作进程共享的，用于发送已经提到的
    `RewardItem` 结构，其中包含随机种子和奖励：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At the beginning of the master, we create all those queues, start the worker
    processes, and create the optimizer.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在主进程开始时，我们创建所有队列，启动工作进程，并创建优化器。
- en: 'Every training iteration starts with the network parameters being broadcast
    to the workers:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每次训练迭代开始时，网络参数会广播到工作进程：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, in the loop, the master waits for enough data to be obtained from the
    workers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在循环中，主进程等待从工作进程获取足够的数据：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Every time a new result arrives, we reproduce the noise using the random seed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每当新结果到达时，我们使用随机种子重新生成噪声。
- en: 'As the last step in the training loop, we call the train_step() function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作为训练循环的最后一步，我们调用 `train_step()` 函数：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You’ve already seen this function, which calculates the update from the noise
    and rewards, and calls the optimizer to adjust the weights.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经见过这个函数，它计算来自噪声和奖励的更新，并调用优化器调整权重。
- en: HalfCheetah results
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HalfCheetah 结果
- en: 'The code supports the optional --dev flag, but from my experiments, I got a
    slowdown if GPU was enabled: without GPU, the average speed was 20-21k observations
    per second, but with CUDA, it was just 9k. This might look counter-intuitive,
    but we can explain this with the very small network and batch size of a single
    observation. Potentially, we might decrease the gap (or even get some speedup)
    with a higher batch size, but it will complicate our code.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码支持可选的 `--dev` 标志，但根据我的实验，如果启用了 GPU，速度会变慢：没有 GPU 时，平均速度是每秒 20-21k 次观察，但启用
    CUDA 后只有 9k。这看起来可能有些反直觉，但我们可以用非常小的网络和单次观察的批量大小来解释这一点。可能通过增加批量大小来减少这一差距（甚至可能实现加速），但这会使我们的代码变得更加复杂。
- en: 'During the training, we show the mean reward, the speed of training (in observations
    per second), and two timing values (showing how long it took to gather data and
    perform the training step):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们展示平均奖励、训练速度（每秒观察次数）以及两个时间值（显示收集数据和执行训练步骤所花费的时间）：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The dynamics of the training show very quick policy improvement in the beginning:
    in just 100 updates, which is 9 minutes of training, the agent was able to reach
    the score of 1,500-1,600\. After 30 minutes, the peak reward was 2,833; but with
    more training, the policy was degrading.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的动态显示出开始时策略的快速改进：仅在100次更新内，训练9分钟，代理就能够达到1,500-1,600的分数。30分钟后，峰值奖励为2,833；但随着更多训练，策略开始退化。
- en: The maximum, mean, and standard deviation of reward are shown in Figure [17.3](#x1-318022r3)
    and Figure [17.4](#x1-318023r4).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的最大值、均值和标准差如图[17.3](#x1-318022r3)和图[17.4](#x1-318023r4)所示。
- en: '![PIC](img/B22150_17_03.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_17_03.png)'
- en: 'Figure 17.3: The maximum reward (left) and policy update (right) for ES on
    HalfCheetah'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3：ES在HalfCheetah上的最大奖励（左）和策略更新（右）
- en: '![PIC](img/B22150_17_04.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_17_04.png)'
- en: 'Figure 17.4: The mean (left) and standard deviation (right) of reward for ES
    on HalfCheetah'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4：ES在HalfCheetah上的奖励均值（左）和标准差（右）
- en: Genetic algorithms
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遗传算法
- en: Another popular class of black-box methods is genetic algorithms (GAs). It is
    a large family of optimization methods with more than two decades of history behind
    it and a simple core idea of generating a population of N individuals (concrete
    model parameters), each of which is evaluated with the fitness function. Then,
    some subset of top performers is used to produce the next generation of the population
    (this process is called mutation). This process is repeated until we’re satisfied
    with the performance of our population.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类流行的黑箱方法是遗传算法（GA）。它是一个历史悠久、拥有二十多年历史的大型优化方法家族，核心思想简单，即生成一个 N 个个体（具体模型参数）的人口，每个个体都通过适应度函数进行评估。然后，部分表现最好的个体用于生成下一代种群（此过程称为变异）。这一过程会一直重复，直到我们对种群的表现感到满意为止。
- en: 'There are a lot of different methods in the GA family, for example, how to
    perform the mutation of the individuals for the next generation or how to rank
    the performers. Here, we will consider the simple GA method with some extensions,
    published in the paper by Such et al., called Deep neuroevolution: Genetic algorithms
    are a competitive alternative for training deep neural networks for reinforcement
    learning [[Suc+17](#)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法（GA）家族中有许多不同的方法，例如，如何执行个体的变异以生成下一代，或者如何对表现者进行排名。在这里，我们将考虑一些扩展的简单GA方法，最早由Such等人发布，名为“深度神经进化：遗传算法是训练深度神经网络进行强化学习的有力竞争者”[[Suc+17](#)]。
- en: 'In this paper, the authors analyzed the simple GA method, which performs Gaussian
    noise perturbation of the parent’s weights to perform mutation. On every iteration,
    the top performer was copied without modification. In an algorithm form, the steps
    of a simple GA method can be written as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，作者分析了简单的GA方法，该方法通过对父代权重施加高斯噪声扰动来执行变异。在每次迭代中，表现最好的个体会被复制且不做修改。简单GA方法的步骤可以用算法形式写成如下：
- en: 'Initialize the mutation power, σ, the population size, N, the number of selected
    individuals, T, and the initial population, P⁰, with N randomly initialized policies
    and their fitness: F⁰ = {F(P[i]⁰)|i = 1…N}'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变异强度 σ、种群大小 N、选择个体的数量 T 和初始种群 P⁰，其中 P⁰ 是随机初始化的 N 个策略及其适应度：F⁰ = {F(P[i]⁰)|i
    = 1…N}
- en: 'For generation g = 1…G:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于代数 g = 1…G：
- en: Sort generation P^(n−1) in the descending order of the fitness function value
    F^(g−1)
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照适应度函数值 F^(g−1) 的降序对上一代 P^(n−1) 进行排序
- en: Copy elite P[1]^g = P[1]^(g−1),F[1]^g = F[1]^(g−1)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制精英 P[1]^g = P[1]^(g−1)，F[1]^g = F[1]^(g−1)
- en: 'For individual i = 2…N:'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于个体 i = 2…N：
- en: 'Choose the k: random parent from 1…T'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 k：从 1…T 中随机选择父代
- en: Sample 𝜖 ∼𝒩(0,I)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样 𝜖 ∼𝒩(0,I)
- en: 'Mutate the parent: P[i]^g = P[i]^(g−1) + σ𝜖'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变异父代：P[i]^g = P[i]^(g−1) + σ𝜖
- en: 'Get its fitness: F[i]^g = F(P[i]^g)'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取其适应度：F[i]^g = F(P[i]^g)
- en: There have been several improvements to this basic method from the paper [2],
    which we will discuss later. For now, let’s check the implementation of the core
    algorithm.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 来自文献[2]的基础方法已有多个改进，我们将在后文讨论。现在，让我们检查一下核心算法的实现。
- en: GA on CartPole
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPole上的GA
- en: 'The source code is in Chapter17/03_cartpole_ga.py, and it has a lot in common
    with our ES example. The difference is in the lack of the gradient ascent code,
    which was replaced by the network mutation function:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码位于Chapter17/03_cartpole_ga.py，与我们的ES示例有很多相似之处。不同之处在于缺少梯度上升代码，而是用网络变异函数代替：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The goal of the function is to create a mutated copy of the given policy by
    adding a random noise to all weights. The parent’s weights are kept untouched,
    as a random selection of the parent is performed with replacement, so this network
    could be used again later.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的目标是通过向所有权重添加随机噪声，创建给定策略的变异副本。父代的权重保持不变，因为父代是通过替换方式随机选择的，因此该网络稍后可能会再次使用。
- en: 'The count of hyperparameters is even smaller than with ES and includes the
    standard deviation of the noise added-on mutation, the population size, and the
    number of top performers used to produce the subsequent generation:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的数量甚至比ES方法还要少，包括变异时添加噪声的标准差、种群大小和用于生成后续世代的顶级表现者数量：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Before the training loop, we create the population of randomly initialized
    networks and obtain their fitness:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环之前，我们创建随机初始化的网络种群，并获取它们的适应度：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At the beginning of every generation, we sort the previous generation according
    to its fitness and record statistics about future parents:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一代开始时，我们根据上一代的适应度对其进行排序，并记录关于未来父代的统计数据：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In a separate loop over new individuals to be generated, we randomly sample
    a parent, mutate it, and evaluate its fitness score:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个单独的循环中，我们随机选取一个父代，进行变异，并评估其适应度得分：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After starting the implementation, you should see the following (concrete output
    and count of steps might vary due to randomness in execution):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 启动实现后，你应该能看到如下内容（具体输出和步骤数可能因执行中的随机性而有所不同）：
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see, the GA method is even more efficient than the ES method.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，GA方法比ES方法更高效。
- en: GA tweaks
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GA改进
- en: 'Such et al. proposed two tweaks to the basic GA algorithm:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Such等人提出了对基本GA算法的两项改进：
- en: The first, with the name deep GA, aimed to increase the scalability of the implementation.
    We will implement this later in the GA on HalfCheetah section.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个方法，名为深度GA，旨在提高实现的可扩展性。我们将在后面的GA on HalfCheetah部分实现这一点。
- en: The second, called novelty search, was an attempt to replace the reward objective
    with a different metric of the episode. We’ve left this as an exercise for you
    to try out.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个方法，叫做新颖性搜索，是尝试用不同的指标替代奖励目标。我们将这一部分留给你作为一个练习来尝试。
- en: In the example used in the following GA on HalfCheetah section, we will implement
    the first improvement, whereas the second one is left as an optional exercise.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的GA on HalfCheetah部分使用的示例中，我们将实现第一个改进，而第二个改进则作为一个可选练习。
- en: Deep GA
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度GA
- en: 'Being a gradient-free method, GA is potentially even more scalable than ES
    methods in terms of speed, with more CPUs involved in the optimization. However,
    the simple GA algorithm that you have seen has a similar bottleneck to ES methods:
    the policy parameters have to be exchanged between the workers. Such et al. (the
    authors) proposed a trick similar to the shared seed approach but taken to an
    extreme (as we’re using seeds to track thousands of mutations). They called it
    deep GA, and at its core, the policy parameters are represented as a list of random
    seeds used to create this particular policy’s weights.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种无梯度方法，GA在速度上可能比ES方法更具可扩展性，因为优化过程涉及更多的CPU。然而，你看到的简单GA算法在与ES方法相似的瓶颈上也存在问题：策略参数必须在工作者之间交换。Such等人（作者）提出了一个类似于共享种子方法的技巧，但他们将其推向了极限（因为我们使用种子来跟踪成千上万的变异）。他们称之为深度GA，其核心思想是，策略参数被表示为一组随机种子的列表，这些种子用于创建该策略的权重。
- en: In fact, the initial network’s weights were generated randomly on the first
    population, so the first seed in the list defines this initialization. On every
    population, mutations are also fully specified by the random seed for every mutation.
    So, the only thing we need to reconstruct the weights is the seeds themselves.
    In this approach, we need to reconstruct the weights on every worker, but usually,
    this overhead is much less than the overhead of transferring full weights over
    the network.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，初始网络的权重是在第一次种群中随机生成的，因此列表中的第一个种子定义了这种初始化。在每一代种群中，变异也完全由每个变异的随机种子来指定。因此，我们需要重构权重的唯一信息就是这些种子本身。在这种方法中，我们需要在每个工作者上重构权重，但通常，这种开销远小于在网络中传输完整权重的开销。
- en: Novelty search
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新颖性搜索
- en: 'Another modification to the basic GA method is novelty search (NS), which was
    proposed by Lehman and Stanley in their paper, Abandoning objectives: Evolution
    through the search for novelty alone, which was published in 2011 [[LS11](#)].'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '基本遗传算法（GA）方法的另一个修改是新颖性搜索（NS），这是Lehman和Stanley在他们的论文《放弃目标：仅通过寻找新颖性进行进化》（Abandoning
    objectives: Evolution through the search for novelty alone）中提出的，该论文于2011年发布[[LS11](#)]。'
- en: The idea of NS is to change the objective in our optimization. We’re no longer
    trying to increase our total reward from the environment but, rather, reward the
    agent for exploring the behavior that it has never checked before (that is, novel).
    According to the authors’ experiments on the maze navigation problem, with many
    traps for the agent, NS works much better than other reward-driven approaches.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: NS的思想是改变我们优化过程中的目标。我们不再试图增加来自环境的总奖励，而是奖励代理探索它以前从未检查过的行为（即新颖的行为）。根据作者在迷宫导航问题中的实验，迷宫中有许多陷阱，NS比其他基于奖励的算法表现得更好。
- en: To implement NS, we define the so-called behavior characteristic (BC) (π), which
    describes the behavior of the policy and a distance between two BCs. Then, the
    k-nearest neighbors approach is used to check the novelty of the new policy and
    drive the GA according to this distance. In the paper by Such et al., sufficient
    exploration by the agent was needed. The approach of NS significantly outperformed
    the ES, GA, and other more traditional approaches to RL problems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现新颖性搜索（NS），我们定义了所谓的行为特征（BC）（π），它描述了策略的行为和两个BC之间的距离。然后，使用k近邻方法检查新策略的新颖性，并根据这个距离驱动遗传算法。在Such等人的论文中，代理的充分探索是必需的。NS方法显著优于进化策略（ES）、遗传算法（GA）和其他更传统的强化学习（RL）方法。
- en: GA on HalfCheetah
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半猎豹上的遗传算法（GA）
- en: In our final example in this chapter, we will implement the parallelized deep
    GA on the HalfCheetah environment. The complete code is in 04_cheetah_ga.py. The
    architecture is very close to the parallel ES version, with one master process
    and several workers. The goal of every worker is to evaluate the batch of networks
    and return the result to the master, which merges partial results into the complete
    population, ranks the individuals according to the obtained reward, and generates
    the next population to be evaluated by the workers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一个例子中，我们将在半猎豹环境中实现并行化深度遗传算法（GA）。完整代码见04_cheetah_ga.py。架构与并行进化策略（ES）版本非常相似，有一个主进程和多个工作进程。每个工作进程的目标是评估一批网络并将结果返回给主进程，主进程将部分结果合并成完整的种群，并根据获得的奖励对个体进行排序，生成下一个待评估的种群。
- en: Every individual is encoded by a list of random seeds used to initialize the
    initial network weights and all subsequent mutations. This representation allows
    very compact encoding of the network, even when the number of parameters in the
    policy is not very large. For example, in our network with with one hidden layer
    of 64 neurons, we have 1,542 float values (the input is 17 values and the action
    is 6 floats, which gives 17 × 64 + 64 + 64 × 6 + 6 = 1542). Every float occupies
    4 bytes, which is the same size used by the random seed. So, the deep GA representation
    proposed by the paper will be smaller up to 1,542 generations in the optimization.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 每个个体由一个随机种子列表编码，用于初始化初始网络权重和所有后续变异。这种表示方式允许非常紧凑地编码网络，即使在策略中参数数量不多的情况下也是如此。例如，在我们有一个包含64个神经元的隐藏层的网络中，我们有1542个浮动值（输入为17个值，动作为6个浮动值，因此17
    × 64 + 64 + 64 × 6 + 6 = 1542）。每个浮动值占用4个字节，这与随机种子使用的大小相同。因此，论文提出的深度遗传算法表示方式将使优化过程中的种群规模最多缩小到1542代。
- en: Implementation
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: In our example, we will perform parallelization on local CPUs so the amount
    of data transferred back and forth doesn’t matter much; however, if you have a
    couple of hundred cores to utilize, the representation might become a significant
    issue.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将在本地CPU上进行并行化处理，因此数据来回传输的数量并不太重要；然而，如果你有几百个核心可用，那么这种表示方式可能会成为一个显著的问题。
- en: 'The set of hyperparameters is the same as in the CartPole example, with the
    difference of a larger population size:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数集与CartPole示例相同，唯一的区别是种群规模较大：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'There are two functions used to build the networks based on the seeds given.
    The first one performs one mutation on the already created policy network:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个函数用于根据给定的种子构建网络。第一个函数对已经创建的策略网络执行一次变异操作：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding function can perform the mutation in place or by copying the target
    network based on arguments (copying is needed for the first generation).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数可以原地执行变异，或者根据参数复制目标网络（对于第一代需要复制）。
- en: 'The second function creates the network from scratch using the list of seeds:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数从头开始使用种子列表创建网络：
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, the first seed is passed to PyTorch to influence the network initialization,
    and subsequent seeds are used to apply network mutations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一个种子传递给 PyTorch，用于影响网络初始化，后续的种子用于应用网络突变。
- en: 'The worker function obtains the list of seeds to evaluate and outputs individual
    OutputItem dataclass items for every result obtained:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: worker 函数获取待评估的种子列表，并为每个获得的结果输出单独的 OutputItem 数据类项：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This function maintains the cache of networks to minimize the amount of time
    spent recreating the parameters from the list of seeds. This cache is cleared
    for every generation, as every new generation is created from the current generation
    winners, so there is only a tiny chance that old networks can be reused from the
    cache.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数维护了网络的缓存，以最小化重新创建种子列表中参数所花费的时间。每次生成都会清除缓存，因为每一代新网络都是从当前代的赢家中创建的，所以旧网络从缓存中复用的可能性非常小。
- en: 'The code of the master process is also straightforward:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程的代码也很简单：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: For every generation, we send the current population’s seeds to workers for
    evaluation and wait for the results. Then, we sort the results and generate the
    next population based on the top performers. On the master’s side, the mutation
    is just a seed number generated randomly and appended to the list of seeds of
    the parent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一代，我们将当前种群的种子发送给工作者进行评估，并等待结果。然后，我们对结果进行排序，并基于表现最好的个体生成下一代。在主进程端，突变只是一个随机生成的种子编号，追加到父代的种子列表中。
- en: Results
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果
- en: In this example, we’re using the MuJoCo HalfCheetah environment, which doesn’t
    have any health checks internally, so every episode takes 2,000 steps. Because
    of this, every training step requires about a minute, so be patient. After 300
    mutation rounds (which took about 7 hours), the best policy was able to get a
    reward of 6454, which is a great result. If you remember our experiments in the
    previous chapter, only the SAC method was able to get a higher reward of 7063
    on MuJoCo HalfCheetah. Of course, HalfCheetah is not very challenging, but still
    — very good.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用的是 MuJoCo HalfCheetah 环境，它内部没有健康检查，因此每个回合需要 2,000 步。由于这个原因，每个训练步骤大约需要一分钟，因此需要耐心等待。在
    300 轮突变后（大约用了 7 小时），最佳策略获得了 6454 的奖励，这是一个很好的结果。如果你还记得我们在上一章的实验，只有 SAC 方法能在 MuJoCo
    HalfCheetah 上获得更高的奖励 7063。当然，HalfCheetah 的挑战性不大，但仍然——非常好。
- en: The plots are shown in Figure [17.5](#x1-326002r5) and Figure [17.6](#x1-326003r6).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图表见图 [17.5](#x1-326002r5) 和图 [17.6](#x1-326003r6)。
- en: '![PIC](img/B22150_17_05.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_05.png)'
- en: 'Figure 17.5: The maximum (left) and mean rewards (right) for GA on HalfCheetah'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5：GA 在 HalfCheetah 上的最大（左）和平均（右）奖励
- en: '![PIC](img/B22150_17_06.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_17_06.png)'
- en: 'Figure 17.6: Standard deviation of reward for GA on HalfCheetah'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.6：GA 在 HalfCheetah 上的奖励标准差
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you saw two examples of black-box optimization methods: evolution
    strategies and genetic algorithms, which can provide competition for other analytical
    gradient methods. Their strength lies in good parallelization on a large number
    of resources and the smaller number of assumptions that they have on the reward
    function.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你看到了两种黑盒优化方法的示例：进化策略和遗传算法，它们可以与其他分析梯度方法竞争。它们的优势在于可以在大量资源上进行良好的并行化，并且对奖励函数的假设较少。
- en: 'In the next chapter, we will take a look at a very important aspect of RL:
    advanced exploration methods.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨强化学习中的一个非常重要的方面：高级探索方法。
