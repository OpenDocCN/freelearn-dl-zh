- en: Practical Implementation for Resolving RL Challenges
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决强化学习挑战的实用实现
- en: In this chapter, we will wrap up some of the concepts behind **deep reinforcement
    learning** (**deep RL**) algorithms that we explained in the previous chapters
    to give you a broad view of their use and establish a general rule for choosing
    the most suitable one for a given problem. Moreover, we will propose some guidelines
    so that you can start the development of your own deep RL algorithm. This guideline
    shows the steps you need to take from the start of development so that you can
    easily experiment without losing too much time on debugging. In the same section,
    we also list the most important hyperparameters to tune and additional normalization
    processes to take care of.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将总结前几章中解释的**深度强化学习**（**深度RL**）算法的部分概念，以便给你一个广泛的视角来了解它们的应用，并为选择最适合某一问题的算法制定一个通用的规则。此外，我们还将提出一些指导方针，帮助你开始开发自己的深度RL算法。这些指导方针展示了从开发开始时需要采取的步骤，使你可以轻松地进行实验，而不至于在调试过程中浪费太多时间。在同一节中，我们还列出了最重要的超参数调优以及需要处理的额外标准化过程。
- en: Then, we'll address the main challenges of this field by addressing issues such
    as stability, efficiency, and generalization. We'll use these three main problems
    as a pivotal point to transition to more advanced reinforcement learning techniques
    such as unsupervised RL and transfer learning. Unsupervised RL and transfer learning
    are of fundamental importance for deploying and solving demanding RL tasks. This
    is because they are techniques that address the three challenges we mentioned
    previously.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过解决稳定性、效率和泛化等问题，探讨这一领域的主要挑战。我们将以这三个主要问题为转折点，过渡到更先进的强化学习技术，如无监督强化学习和迁移学习。无监督强化学习和迁移学习对于部署和解决要求高的强化学习任务至关重要，因为它们是解决我们之前提到的三个挑战的技术。
- en: We will also look into how we can apply RL to real-world problems and how RL
    algorithms can be used for bridging the gap between simulation and the real world.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何将强化学习应用于实际问题，以及强化学习算法如何在弥合仿真与现实世界之间的差距方面发挥作用。
- en: To conclude this chapter and this book as a whole, we'll discuss the future
    of reinforcement learning from both a technical and social perspective.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本章及整本书，我们将从技术和社会角度讨论强化学习的未来。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Best practices of deep RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度强化学习的最佳实践
- en: Challenges in deep RL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度强化学习的挑战
- en: Advanced techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级技术
- en: RL in the real world
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习在现实世界中的应用
- en: Future of RL and its impact on society
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的未来及其对社会的影响
- en: Best practices of deep RL
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习的最佳实践
- en: Throughout this book, we covered plenty of reinforcement learning algorithms,
    some of which are only upgrades (for example TD3, A2C, and so on), while others
    were fundamentally different from the others (such as TRPO and DPG) and propose
    an alternative way to reach the same objective. Moreover, we addressed non-RL
    optimization algorithms such as imitation learning and evolution strategies to
    solve sequential decision-making tasks. All of these alternatives may have created
    confusion and you may not know exactly which algorithm is best for a particular
    problem. If that is the case, don't worry, as we'll now go through some rules
    that you can use in order to decide which is the best algorithm to use for a given
    task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们介绍了许多强化学习算法，其中一些仅是升级版（例如 TD3、A2C 等），而其他一些则与其他算法有根本性的不同（例如 TRPO 和 DPG），并提出了达成相同目标的替代方式。此外，我们还探讨了非强化学习优化算法，如模仿学习和进化策略，以解决序列决策任务。所有这些替代方案可能会让你感到困惑，你可能不确定哪种算法最适合某一特定问题。如果是这种情况，别担心，我们现在将介绍一些规则，帮助你决定在给定任务中使用最合适的算法。
- en: Also, if you implemented some of the algorithms we went through in this book,
    you might find it hard to put all the pieces together to make the algorithm work
    properly. Deep RL algorithms are notoriously difficult to debug and train, and
    the training time is very long. As a result, the whole training process is very
    slow and arduous. Luckily, there are a few strategies that you can adopt that
    will prevent some terrible headaches while developing deep RL algorithms. But
    before looking at what these strategies are, let's deal with choosing the appropriate
    algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你在本书中实现了我们介绍的一些算法，可能会发现很难将所有部分整合起来，使算法正常工作。深度强化学习算法因调试和训练难度大而著称，而且训练时间非常长。因此，整个训练过程非常缓慢且艰难。幸运的是，有一些策略可以帮助你避免在开发深度强化学习算法时遇到一些可怕的头痛问题。但在讨论这些策略之前，我们先来看看如何选择合适的算法。
- en: Choosing the appropriate algorithm
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的算法
- en: The main driving force that differentiates the various types of RL algorithms
    is sample efficiency and training time.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 区分不同类型的强化学习（RL）算法的主要驱动力是样本效率和训练时间。
- en: We consider sample efficiency as the number of interactions with the environment
    that an agent has to make in order to learn the task. The numbers that we'll provide
    are an indication of the efficiency of the algorithm and are measured with respect
    to other algorithms on typical environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将样本效率视为一个智能体在学习任务时与环境交互的次数。我们提供的数字是算法效率的一个指标，是相对于其他算法在典型环境中的表现进行衡量的。
- en: Clearly, there are other parameters that influence this choice, but usually,
    they have a minor impact and are of less importance. Just to give you an idea,
    the other parameters to be evaluated are the availability of CPUs and GPUs, the
    type of reward function, the scalability, and the complexity of the algorithm,
    as well as that of the environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，还有其他参数会影响算法选择，但通常这些参数的影响较小，重要性较低。举个例子，其他需要评估的参数包括CPU和GPU的可用性、奖励函数的类型、可扩展性、算法的复杂度以及环境的复杂度。
- en: For this comparison, we will take into consideration gradient-free black-box
    algorithms such as evolution strategies, model-based RL such as DAgger, and model-free
    RL. Of the latter, we will differentiate between policy gradient algorithms such
    as DDPG and TRPO and value-based algorithms such as DQN.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次比较中，我们将考虑无梯度黑箱算法（如进化策略）、基于模型的RL算法（如DAgger）以及无模型的RL算法。在后者中，我们将区分策略梯度算法（如DDPG和TRPO）和基于价值的算法（如DQN）。
- en: 'The following diagram shows the data efficiency of these four categories of
    algorithms (note that the leftmost methods are less sample efficient than the
    rightmost methods). In particular, the efficiency of the algorithm increases as
    you move to the right of the diagram. So, you can see that gradient-free methods
    are those that require more data points from the environment, followed by policy
    gradient methods, value-based methods, and finally model-based RL, which are the
    most sample efficient:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了这四类算法的数据效率（注意，最左侧的方法在样本效率上低于最右侧的方法）。特别地，随着图表向右移动，算法的效率逐渐增加。因此，你可以看到，无梯度方法需要从环境中获取更多的数据点，其次是策略梯度方法、基于价值的方法，最后是基于模型的RL，它们是最具样本效率的。
- en: '![](img/8c99a8da-3b3b-4316-8252-e88cb7fd277a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c99a8da-3b3b-4316-8252-e88cb7fd277a.png)'
- en: Figure 13.1\. Sample efficiency comparison between model-based RL methods, policy
    gradient algorithms, value-based algorithms, and gradient-free algorithms (the
    leftmost methods are less efficient than the rightmost methods)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：基于模型的RL方法、策略梯度算法、基于价值的算法和无梯度算法的样本效率比较（最左侧的方法效率低于最右侧的方法）。
- en: Conversely, the training time of these algorithms is inversed related to their
    sample efficiency. This relationship is summarized in the following diagram (note
    that the leftmost methods are slower to train than the rightmost methods). We
    can see that Model-based algorithms are way slower to train than Value-based algorithms,
    almost by a factor of 5, which in turn almost quintuples the time of policy gradient
    algorithms, which are about **5x **slower to train than gradient-free methods.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这些算法的训练时间与样本效率呈反比关系。这个关系在以下图表中总结（注意，最左侧的方法训练速度慢于最右侧的方法）。我们可以看到，基于模型的算法的训练速度远低于基于价值的算法，几乎慢了五倍，而基于策略梯度的算法则比无梯度方法慢约**5倍**。
- en: 'Be aware that these numbers are just to highlight the average case, and the
    training time is only related to the speed at which the algorithm is trained,
    and not to the time needed to acquire new transitions from the environment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些数字只是为了突出平均情况，训练时间仅与算法的训练速度相关，而与从环境中获取新过渡所需的时间无关：
- en: '![](img/6faa04da-a432-46dd-989d-ab505bd60a14.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6faa04da-a432-46dd-989d-ab505bd60a14.png)'
- en: Figure 13.2\. Training time efficiency comparison between model-based RL methods,
    policy gradient algorithms, value-based algorithms, and gradient-free algorithms
    (the leftmost methods are slower to train than the rightmost methods)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2\. 模型基础RL方法、策略梯度算法、值基算法和无梯度算法的训练时间效率比较（最左侧的方法训练速度比最右侧的慢）
- en: We can see that the sample efficiency of an algorithm is complementary to its
    training time, meaning that an algorithm that is data efficient is slow to train
    and vice versa. Thus, because the overall learning time of an agent takes into
    account both the training time and the speed of the environment, you have to find
    a trade-off between sample efficiency and training time that meet your needs.
    In fact, the main purpose of model-based and more efficient model-free algorithms
    is to reduce the number of steps with the environment so that these algorithms
    are easier to deploy and train in the real world, where the interactions are slower
    than in simulators.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，算法的样本效率与其训练时间是互补的，这意味着一个数据高效的算法训练起来很慢，反之亦然。因此，由于智能体的总体学习时间既考虑到训练时间，也考虑到环境的速度，你需要在样本效率和训练时间之间找到一个平衡点，以满足你的需求。事实上，基于模型的算法和更高效的无模型算法的主要目的是减少与环境的交互步骤，使得这些算法在实际世界中更容易部署和训练，因为实际世界中的交互速度通常比模拟器中的慢。
- en: From zero to one
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零到一
- en: 'Once you have defined the algorithm that best fits your needs, whether that''s
    one of the well-known algorithms or a new one, you have to develop it. As you
    saw throughout this book, reinforcement learning algorithms don''t have much in
    common with supervised learning algorithms. For this reason, there are different
    aspects that are worth pointing out in order to facilitate the debugging, experimentation,
    and tuning of the algorithm:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了最适合你需求的算法，无论它是一个广为人知的算法还是一个新的算法，你就需要开发它。正如本书所示，强化学习算法与监督学习算法没有太多共同之处。出于这个原因，有一些不同的方面值得注意，以便于调试、实验和调整算法：
- en: '**Start with easy problems**: Initially, you would want to experiment with
    a workable version of the code as fast as possible. However, it would be advisable
    to gradually proceed with increasingly complex environments. This will greatly
    help to reduce the overall training and debugging time. Let me present an example.
    You can start with CartPole-v1 or RoboschoolInvertedPendulum-v1 if you need a
    discrete or continuous environment, respectively. Then, you can move to a medium-complexity
    environment such as RoboschoolHopper-v1, LunarLander-v2, or a related environment
    with RGB images. At this point, you should have a bug-free code that you can finally
    train and tune on your final task. Moreover, you should be as familiar as possible
    with the easier tasks so that you know what to look for if something is not working.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从简单问题开始**：最初，你可能会想尽快尝试一个可行版本的代码。然而，建议你逐步进行，逐渐增加环境的复杂度。这将大大帮助减少整体的训练和调试时间。让我举个例子。如果你需要一个离散环境，可以从CartPole-v1开始；如果需要一个连续环境，可以选择RoboschoolInvertedPendulum-v1。接着，你可以进入一个中等复杂度的环境，例如RoboschoolHopper-v1、LunarLander-v2，或者一个带有RGB图像的相关环境。在这个阶段，你应该已经拥有一份没有bug的代码，最终可以在你的最终任务上进行训练和调整。此外，你应该尽可能熟悉这些简单的任务，这样如果出现问题，你能够知道应该查找哪些方面。'
- en: '**Training is slow**: Training deep reinforcement learning algorithms takes
    time and the learning curve can follow any kind of shape. As we saw in the previous
    chapters, the learning curves (that is, the cumulative reward of the trajectories
    with respect to the number of steps) can resemble a logarithm function, a hyperbolic
    tangent function, as shown in the following diagram, or a more complex function.
    The possible shapes depend on the reward function, its sparsity, and the complexity
    of the environment. If you are working on a new environment and you don''t know
    what to expect, the suggestion here is to be patient and leave it running until
    you are sure that the progress has stopped. Also, don''t get too involved with
    the plots while training.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练很慢**：训练深度强化学习算法需要时间，学习曲线可能呈现任何形状。正如我们在前几章中看到的，学习曲线（即相对于步数的轨迹累积奖励）可能类似于对数函数、双曲正切函数（如下图所示），或更复杂的函数。可能的形状取决于奖励函数、其稀疏性和环境的复杂性。如果你正在处理一个新环境，并且不知道会有什么结果，建议你保持耐心，直到你确定进展已停止为止。同时，训练过程中不要过于关注图表。'
- en: '**Develop some baselines**: For new tasks, the suggestion is to develop at
    least two baselines so that you can compare your algorithm with them. One baseline
    could simply be a random agent, with the other being an algorithm such as REINFORCE
    or A2C. These baselines can then be used as a lower bound for performance and
    efficiency.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发一些基准线**：对于新任务，建议至少开发两个基准线，以便与它们比较你的算法。一个基准线可以是一个随机代理，另一个基准线可以是像REINFORCE或A2C这样的算法。这些基准线可以用作性能和效率的下限。'
- en: '**Plots and histograms**: To monitor the progress of the algorithm and to help
    during the debugging phase, an important factor is to plot and display histograms
    of key parameters such as the loss function, the cumulative reward, the actions
    (if possible), the length of the trajectories, the KL penalty, the entropy, and
    the value function. In addition to plotting the means, you can add the minimum
    and maximum values and the standard deviation. In this book, we primarily used TensorBoard
    to visualize this information, but you can use any tool you want.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图表和直方图**：为了监控算法的进展并帮助调试阶段，一个重要的因素是绘制并显示关键参数的直方图，如损失函数、累积奖励、动作（如果可能）、轨迹的长度、KL惩罚、熵和价值函数。除了绘制均值外，你还可以添加最小值、最大值和标准差。在本书中，我们主要使用 TensorBoard
    来可视化这些信息，但你可以使用任何你喜欢的工具。'
- en: '**Use multiple seeds**: Deep reinforcement learning embeds stochasticity both
    in the neural networks and in the environments, which often makes the results
    incoherent between different runs. So, to ensure consistency and stability, it''s
    better to use multiple random seeds.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用多个种子**：深度强化学习在神经网络和环境中都嵌入了随机性，这通常导致不同运行之间的结果不一致。因此，为了确保一致性和稳定性，最好使用多个随机种子。'
- en: '**Normalization:** Depending on the design of the environment, it could be
    helpful to normalize the rewards, the advantage, and the observations. The advantage
    values (for example, in TRPO and PPO) can be normalized in a batch to have a mean
    of 0 and a standard deviation of 1\. Additionally, the observations can be normalized
    using a set of initial random steps. Instead, the rewards can be normalized by
    a running estimate of the mean and standard deviation of the discounted or undiscounted
    reward.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**：根据环境的设计，归一化奖励、优势和观测值可能会有所帮助。优势值（例如，在TRPO和PPO中）可以在一个批次中进行归一化，使其均值为0，标准差为1。此外，观测值可以使用一组初始随机步骤进行归一化。奖励则可以通过折扣或未折扣奖励的均值和标准差的运行估计来归一化。'
- en: '**Hyperparameter tuning**: Hyperparameters change a lot based on the class
    and type of algorithm. For example, value-based methods have multiple distinct
    hyperparameters compared to policy gradients, but also instances of these classes
    such as TRPO and PPO have many unique hyperparameters. That being said, for each
    algorithm that was introduced throughout this book, we specified the hyperparameters
    that were used and the most important ones to tune. Among them, there are at least
    two hyperparameters that are used by all the RL algorithms: learning rate and
    discount factor. The former is slightly less important than in supervised learning,
    but nevertheless, it remains one of the first hyperparameters to tune so that
    we have a working algorithm. The discount factor is unique to RL algorithms. The
    introduction of a discount factor may introduce bias as it modifies the objective
    function. However, in practice, it produces a better policy. Thus, to a certain
    degree, the shorter the horizon, the better it is, as it reduces instability:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调优**：超参数根据算法的类别和类型变化很大。例如，基于值的方法与策略梯度方法相比有多个不同的超参数，但这些类别的实例，如 TRPO 和 PPO，也有许多独特的超参数。也就是说，对于本书中介绍的每一个算法，我们都指定了所使用的超参数以及最重要的调优参数。在这些参数中，至少有两个是所有强化学习算法都会用到的：学习率和折扣因子。前者在监督学习中稍微不那么重要，但仍然是调优的首要参数之一，因为它决定了我们是否能够得到一个有效的算法。折扣因子是强化学习算法特有的。引入折扣因子可能会引入偏差，因为它会修改目标函数。然而，在实际应用中，它能够生成更好的策略。因此，从某种程度上来说，时间跨度越短越好，因为它可以减少不稳定性：'
- en: '![](img/6c0e587b-2b78-49e9-a61f-e3454508efbb.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c0e587b-2b78-49e9-a61f-e3454508efbb.png)'
- en: Figure 13.3\. Example of a logarithmic and hyperbolic tangent function
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3\. 对数函数和双曲正切函数的示例
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章中提到的所有颜色参考，请参阅[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)中的彩色图像包。
- en: Adopt these techniques and you'll be able to train, develop, and deploy your
    algorithms much more easily. Furthermore, you'll have algorithms that are more
    stable and robust.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这些技术后，你将能够更轻松地训练、开发和部署算法。此外，你将拥有更加稳定和健壮的算法。
- en: Having a critical view and understanding of the drawbacks of deep reinforcement
    learning is a key factor when it comes to actually pushing the boundaries on what
    RL algorithms can do to design better state-of-the-art algorithms. In the following
    section, we'll present the main challenges of deep RL in a more concise view.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对深度强化学习的缺点有一个批判性的视角和理解，是推动强化学习算法发展的关键因素，这样才能设计出更好的最先进算法。在接下来的部分中，我们将以更简洁的方式呈现深度强化学习的主要挑战。
- en: Challenges in deep RL
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习中的挑战
- en: The efforts that have been put into the research of reinforcement learning algorithms
    in recent years has been huge. Especially since the introduction of the deep neural
    network as a function approximation, the advancement and results have been outstanding.
    Yet some major issues remain unsolved. These limit the applicability of RL algorithms
    to more extensive and interesting tasks. We are talking about the issues of stability,
    reproducibility, efficiency, and generalization, although scalability and the
    exploration problem could be added to this list.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在强化学习算法的研究中投入的努力巨大。特别是深度神经网络作为函数逼近方法引入之后，进展和成果都非常突出。然而，一些主要问题仍未解决，这些问题限制了强化学习算法在更广泛和有趣任务中的应用。我们在这里讨论的是稳定性、可重复性、效率和泛化性问题，尽管可扩展性和探索问题也可以列入此清单。
- en: Stability and reproducibility
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定性和可重复性
- en: Stability and reproducibility are somehow interconnected with each other as
    the goal is to design an algorithm that is capable of consistency across multiple
    runs and that is not too invariant to small tweaks. For example, the algorithm
    shouldn't be too sensitive to changes in the values of the hyperparameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定性和可重复性在某种程度上是相互关联的，因为目标是设计一个能够在多次运行中保持一致的算法，并且不会对微小的调整产生过大敏感性。例如，算法不应该对超参数值的变化过于敏感。
- en: The main factor that makes deep RL algorithms difficult to replicate is intrinsic
    to the nature of deep neural networks. This is mainly due to random initialization
    of the deep neural networks and the stochasticity of optimization. Moreover, this
    situation is exacerbated in RL, considering that the environments are stochastic.
    Combined, these factors are also to the detriment of the interpretability of results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使深度强化学习算法难以复制的主要因素，源自深度神经网络的固有特性。这主要是因为深度神经网络的随机初始化和优化过程的随机性。此外，在强化学习中，这种情况因环境的随机性而加剧。综合来看，这些因素也不利于结果的可解释性。
- en: Stability is also put to the test by the high instability of RL algorithms,
    as we saw in Q-learning and REINFORCE. For example, in value-based algorithms,
    there isn't any guarantee of convergence and the algorithms suffer from high bias
    and instability. DQN uses many tricks to stabilize the learning process, such
    as an experienced replay and a delay in the update of the target network. Though
    these strategies can alleviate the instability problems, they don't go away.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定性还面临强化学习（RL）算法高不稳定性的考验，就像我们在Q-learning和REINFORCE中看到的那样。例如，在基于值的算法中，没有任何收敛的保证，算法会受到高偏差和不稳定性的困扰。DQN使用了许多技巧来稳定学习过程，比如经验回放和目标网络更新的延迟。尽管这些策略可以缓解不稳定性问题，但它们并未完全消除这些问题。
- en: 'To overcome any constraints that are intrinsic to the algorithm in terms of
    stability and reproducibility, we need to intervene outside of it. To this end,
    many different benchmarks and some rules of thumb can be employed to ensure a
    good level of reproducibility and consistency of results. These are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服算法在稳定性和可重复性方面的固有限制，我们需要在算法之外进行干预。为此，可以采用许多不同的基准测试和一些经验法则，以确保较高的可重复性和结果一致性。具体方法如下：
- en: Whenever possible, test the algorithms on multiple but similar environments.
    For example, test it on a suite of environments such as Roboschool or Atari Gym
    where the tasks are comparable to each other in terms of action and state spaces
    but have different goals.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能，应该在多个相似的环境中测试算法。例如，可以在Roboschool或Atari Gym等环境集上测试，其中任务在动作和状态空间上相似，但目标不同。
- en: Run many trials across different random seeds. The results may vary significantly
    by changing the seeds. As an example of this, the following diagram shows two
    runs of the exact same algorithm with the same hyperparameters, but with a different
    seed. You can see that the differences are large. So, depending on your goal,
    it could be helpful to use multiple random seeds, generally between three and
    five. For example, in academic papers, it is good practice to average all the
    results across five runs and take the standard deviation into account as well.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行多个不同随机种子的实验。通过改变种子，结果可能会有显著差异。以下图示为例，展示了相同超参数的算法在不同种子下的两次运行，你可以看到差异很大。因此，根据你的目标，使用多个随机种子（一般为三到五个）可能会有帮助。例如，在学术论文中，通常的做法是对五次运行的结果进行平均，并考虑标准差。
- en: 'If the results are unsteady, consider using a more stable algorithm or employing
    some further strategies. Also, keep in mind that the effects in the changes of
    the hyperparameters can vary significantly across algorithms and environments:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果结果不稳定，考虑使用更稳定的算法或采取进一步的策略。同时，请记住，超参数的变化对不同算法和环境的影响可能会有显著差异：
- en: '![](img/a8b58095-2ae3-4764-b795-fde9589f58ab.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8b58095-2ae3-4764-b795-fde9589f58ab.png)'
- en: Figure 13.4\. Performance of two trials of the same algorithm with different
    random seeds
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4. 两次相同算法在不同随机种子下的性能对比
- en: Efficiency
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 效率
- en: In the previous section, *Choosing the appropriate algorithm*, we saw that the
    sample efficiency between the algorithms is highly variable. Moreover, from the
    previous chapters, we saw that more efficient methods, such as value-based learning,
    still require a substantial number of interactions with the environment to learn.
    Maybe only model-based RL can save itself from the hunger of data. Unfortunately,
    model-based methods have other downsides, such as a lower performance bound.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节*选择合适的算法*中，我们看到算法之间的样本效率差异很大。此外，从前几章我们了解到，更高效的方法，如基于值的学习，仍然需要大量的环境交互才能学习。也许只有基于模型的强化学习才能解决数据匮乏的问题。不幸的是，基于模型的方法也有其他缺点，比如较低的性能上限。
- en: For this reason, hybrid model-based and model-free approaches have been built.
    However, these are difficult to engineer and are impractical for use in real-world
    problems. As you can see, the efficiency-related problem is very hard to solve
    but at the same time very important to address so that we can deploy RL methods
    in the real world.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经构建了混合的基于模型和无模型的方法。然而，这些方法在工程实现上较为困难，且不适用于现实世界中的问题。正如你所看到的，效率相关的问题非常难以解决，但同时也非常重要，需要解决这些问题才能将强化学习方法应用于现实世界。
- en: There are two alternative ways to deal with very slow environments such as the
    physical world. One is to use a lower-fidelity simulator in the first place and
    then fine-tune the agent in the final environment. The other is to train the agent
    directly in the final environment, but transferring some prior related knowledge
    so as to avoid learning the task from scratch. It's like learning to drive when
    you've already trained your sensory system. In both cases, because we are transferring
    knowledge from one environment to another, we talk about a methodology called
    transfer learning. We'll elaborate on this methodology very soon in the *Advanced
    techniques* section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种替代方法可以处理像物理世界这样非常缓慢的环境。一种方法是在最初使用低保真度的模拟器，然后在最终环境中微调智能体。另一种方法是直接在最终环境中训练智能体，但需要转移一些先前的相关知识，以避免从头开始学习任务。这就像是在你已经训练了自己的感官系统后学习驾驶。无论哪种方式，因为我们是在将知识从一个环境转移到另一个环境，所以我们讨论了一种叫做迁移学习的方法。我们将在*高级技术*部分进一步详细阐述这种方法。
- en: Generalization
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泛化
- en: The concept of generalization refers to two aspects that are different, but
    somehow related. In general terms, the concept of generalization in reinforcement
    learning refers to the capability of an algorithm to obtain good performance in
    a related environment. For example, if an agent has been trained to walk on dirty
    roads, we might expect that the same agent will perform well on paved roads. The
    better the generalization capabilities, the better the agent will perform in different
    environments. The second and lesser-used means of generalization refers to the
    property of the algorithm to achieve good performance in an environment where
    only limited data can be gathered.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的概念涉及两个不同但又相关的方面。一般而言，强化学习中的泛化概念指的是算法在相关环境中获得良好表现的能力。例如，如果一个智能体已经接受了在泥泞道路上行走的训练，我们可能会期望该智能体在铺设的道路上表现良好。泛化能力越强，智能体在不同环境中的表现就越好。第二种较少使用的泛化方式是指算法在只能收集有限数据的环境中取得良好表现的特性。
- en: In RL, the agent can choose the states to visit by itself and do so for as long
    as it wants so that it can also overfit on a certain problem space. However, if
    good generalization capabilities are required, a trade-off has to be found. This
    is only partially true if the agent is allowed to gather potentially infinite
    data for the environment as it will act as a sort of self-regularization method.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，智能体可以自行选择要访问的状态，并且可以任意停留在这些状态上，从而可能会在某个特定问题空间上过拟合。然而，如果需要良好的泛化能力，就必须找到一种折中方法。如果允许智能体为环境收集潜在的无限数据，这种做法只在某种程度上是有效的，因为它会作为一种自我正则化方法。
- en: Nonetheless, to help with generalization across other environments, an agent
    must be capable of abstract reasoning to discern from the mere state-action mapping
    and interpret the task using multiple factors. Examples of abstract reasoning
    can be found in model-based reinforcement learning, transfer learning, and in
    the use of auxiliary tasks. We'll cover the latter topic later, but in brief,
    it is a technique that's used to improve generalization and sample efficiency
    by augmenting an RL agent with auxiliary tasks that were learned jointly with
    the main task.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，为了帮助智能体在其他环境中实现泛化，它必须具备抽象推理的能力，从简单的状态-动作映射中辨别出任务，并使用多个因素来解释任务。抽象推理的例子可以在基于模型的强化学习、迁移学习以及使用辅助任务中找到。我们稍后会介绍后者，但简而言之，这是一种通过将辅助任务与主任务共同学习来改善泛化和样本效率的技术。
- en: Advanced techniques
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级技术
- en: The challenges we listed previously have no simple solutions. However, there
    has been an effort in trying to overcome them and to come up with novel strategies
    to improve efficiency, generalization, and stability. Two of the most widespread
    and promising techniques that focus on efficiency and generalization are unsupervised
    reinforcement learning and transfer learning. In most cases, these strategies
    work in symbiosis with the deep reinforcement learning algorithms that we developed
    in the previous chapters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前列出的挑战没有简单的解决方案。然而，已经有努力在尝试克服这些挑战，并提出新的策略来提高效率、泛化能力和稳定性。专注于效率和泛化的两种最广泛且有前景的技术是无监督强化学习和迁移学习。在大多数情况下，这些策略与我们在前几章中开发的深度强化学习算法是相辅相成的。
- en: Unsupervised RL
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督强化学习
- en: Unsupervised RL is related to the usual unsupervised learning in how both methods
    don't use any source of supervision. While in unsupervised learning the data isn't
    labeled, in the reinforced counterpart, the reward is not given. That is, given
    an action, the environment returns only the next state. Both the reward and the
    **done** status are removed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督强化学习与常规的无监督学习相关，因为两者都不使用任何监督来源。在无监督学习中，数据没有标签，而在强化学习的对等体中，奖励并没有给出。也就是说，在给定一个动作时，环境只返回下一个状态。奖励和**完成**状态都被移除。
- en: Unsupervised RL can be helpful in many occurrences, for example, when the annotation
    of the environment with hand-designed rewards is not scalable, or when an environment
    can serve multiple tasks. In the latter case, unsupervised learning can be employed so
    that we can learn about the dynamics of the environment. Methods that are able
    to learn from unsupervised sources can also be used as an additional source of
    information in environments with very sparse rewards.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督强化学习（Unsupervised RL）在许多场景中都有帮助，例如，当用手工设计的奖励对环境进行标注不可扩展，或者当一个环境可以服务于多个任务时。在后一种情况下，可以采用无监督学习，以便我们能够了解环境的动态。能够从无监督来源中学习的方法，也可以作为在奖励非常稀疏的环境中额外的信息来源。
- en: How can we design an algorithm that can learn about the environment without
    any source of supervision? Can't we just employ model-based learning? Well, model-based
    RL still needs the reward signal to plan or infer the next actions. Therefore,
    a different solution is required.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何设计一个能够在没有任何监督来源的情况下学习环境的算法？难道我们不能仅仅采用基于模型的学习吗？实际上，基于模型的强化学习仍然需要奖励信号来规划或推断下一个动作。因此，需要一个不同的解决方案。
- en: Intrinsic reward
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内在奖励
- en: A potential fair alternative is to develop a reward function that is intrinsic
    to the agent, meaning that it's controlled exclusively by the belief of the agent.
    This method comes close to the approach that's used by newborns to learn. In fact,
    they employ a pure explorative paradigm to navigate the world without an immediate
    benefit. Nonetheless, the knowledge that's acquired may be useful later in life.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个潜在的公平替代方法是开发一个内在的奖励函数，这个函数是由智能体的信念独立控制的。这种方法接近新生儿学习所采用的方式。实际上，他们采用纯探索的范式，在没有直接利益的情况下探索世界。然而，所获得的知识可能在以后的生活中有所帮助。
- en: The intrinsic reward is a sort of exploration bonus based on the estimation
    of the novelty of a state. The more unfamiliar a state is, the higher the intrinsic
    reward. Thus, with it, the agent is incentivized to explore new spaces of the
    environment. It may have become clear by now that the intrinsic reward can be
    used as an alternative exploration strategy. In fact, many algorithms use it in
    combination with the extrinsic reward (that is the usual reward that's returned
    by the environment) to boost the exploration in very sparse environments such
    as Montezuma's revenge. However, though the methods to estimate the intrinsic
    reward are very similar to those we studied in [Chapter 12](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml), *Developing
    ESBAS Algorithm*, to incentivize policy exploration (these exploration strategies
    were still related to the extrinsic reward), here, we are only concentrating on
    pure unsupervised exploration methods.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内在奖励是一种基于对状态新颖性估计的**探索奖励**。一个状态越陌生，它的内在奖励就越高。因此，代理通过这种奖励被激励去探索环境中的新空间。到现在为止，可能已经清楚，内在奖励可以作为一种替代的探索策略。事实上，许多算法将内在奖励与外在奖励（即环境返回的常规奖励）结合使用，以增强在稀疏环境中的探索，例如《Montezuma's
    Revenge》。然而，尽管估算内在奖励的方法与我们在[第12章](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml)《开发ESBAS算法》中研究的非常相似，用于激励策略探索（这些探索策略仍与外部奖励相关），但在这里，我们仅专注于纯粹的无监督探索方法。
- en: 'Two primary curiosity-driven strategies that provide rewards on unfamiliar
    states and explore the environment efficiently are count-based and dynamics-based:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 两种主要的基于好奇心驱动的策略，它们能在未知状态下提供奖励并有效探索环境，分别是基于计数的策略和基于动态的策略：
- en: Count-based strategies (also known as **visitation counts** strategies) aim
    to count or estimate the visitation count of each state and encourage the exploration
    of those states with low visitation, assigning a high intrinsic reward to them.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于计数的策略（也称为**访问计数**策略）旨在计数或估算每个状态的访问次数，并鼓励探索那些访问次数较少的状态，对这些状态分配较高的内在奖励。
- en: Dynamics-based strategies train a dynamic model of the environment, along with
    the agent's policy, and compute the intrinsic reward either on the prediction
    error, on the prediction uncertainty, or on the prediction improvement. The underlying
    idea is that by fitting a model on the states visited, the new and unfamiliar
    states will have a higher uncertainty or estimation error. These values are then
    used to compute the intrinsic reward and incentivize the exploration of unknown
    states.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于动态的策略训练一个环境的动态模型，并与智能体的策略结合，计算内在奖励，通常基于预测误差、预测不确定性或预测改进。其基本思想是通过对已访问状态进行建模，新颖和未知的状态将具有更高的不确定性或估计误差。然后，这些值被用来计算内在奖励，并激励探索未知状态。
- en: What happens if we apply only curiosity-driven approaches to the usual environments?
    The paper *Large-scale study of curiosity-driven learning* addressed this question
    and found that, on Atari games, pure curiosity-driven agents can learn and master
    the tasks without any external reward. Furthermore, they noted that, on Roboschool,
    walking behavior emerged purely out of these unsupervised algorithms based on
    intrinsic reward. The authors of the paper also suggested that these findings
    were due to the way in which the environments have been designed. Indeed, in human-designed environments
    (such as games), the extrinsic reward is often aligned with the objective of seeking
    novelty. Nonetheless, in environments that are not gamified, pure curiosity-driven
    unsupervised approaches are able to explore and learn about the environment exclusively
    by themselves without any need for supervision whatsoever. Alternatively, RL algorithms
    can also benefit from a huge boost in exploration and consequently in performance by
    combining the intrinsic with the extrinsic reward.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅对常规环境应用好奇心驱动的方法，会发生什么？论文《好奇心驱动学习的大规模研究》提出了这个问题，并发现，在Atari游戏中，纯粹的好奇心驱动代理可以在没有任何外部奖励的情况下学习并掌握任务。此外，他们还注意到，在Roboschool中，步态行为完全是由这些基于内在奖励的无监督算法产生的。论文的作者还提出，这些发现是由于环境设计的方式。确实，在人类设计的环境中（如游戏），外部奖励通常与寻求新奇的目标是一致的。然而，在非游戏化的环境中，纯粹的好奇心驱动的无监督方法能够完全依靠自己探索并学习环境，而无需任何监督。或者，RL算法也可以通过结合内在奖励与外在奖励，从而显著提高探索能力，并因此提升性能。
- en: Transfer learning
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Transferring knowledge between two environments, especially if these environments
    are similar to each other, is a hard task. Transfer learning strategies propose
    to bridge the knowledge gap so that the transition from an initial environment
    to a new one is as easy and smooth as possible. Specifically, transfer learning
    is the task of efficiently transferring knowledge from a source environment (or
    multiple environments) to a target environment. Thus, the more experience that
    has been acquired from a set of source tasks and transferred to a new target task,
    the faster the agent will learn and the better it will perform on the target task.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个环境之间迁移知识，尤其是当这些环境彼此相似时，是一项困难的任务。迁移学习策略旨在弥合知识差距，使得从初始环境到新环境的过渡尽可能平滑和容易。具体而言，迁移学习是将知识从源环境（或多个源环境）高效地迁移到目标环境的任务。因此，从一组源任务中获取的经验越多，并且迁移到新目标任务，智能体学习的速度就越快，在目标任务上的表现也会越好。
- en: 'Generally speaking, when you think about an agent that hasn''t been trained
    yet, you have to imagine a system that does not have any kind of information in
    it. Instead, when you play a game, you use a lot of prior knowledge. For example,
    you may guess the meaning of the enemies from their shapes and colors, as well
    as their dynamics. This implies that you are able to recognize the enemies when
    they shoot you, like in the Space Invaders game that''s shown in the following
    diagram. Also, you can easily guess the general dynamics of the game. Instead,
    at the start of the training, an RL agent won''t know anything. This comparison
    is important because it provides valuable insight into the importance of transferring
    knowledge between multiple environments. An agent that has the ability to use
    the experience that was acquired from a source task can learn exponentially faster
    on the target environment. For example, if the source environment is Pong and
    the target environment is Breakout, then many of the visual components could be
    reused, saving a lot of time for computation. To have an accurate understanding
    of its overall importance, imagine the efficiency that''s gained in much more
    complex environments:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当你想到一个尚未经过训练的智能体时，你需要想象一个完全没有任何信息的系统。相反，当你玩游戏时，你会使用大量的先验知识。例如，你可能会根据敌人的形状、颜色以及它们的动态来推测敌人的含义。这意味着，当敌人向你开火时，你能识别出敌人，就像下面的《太空侵略者》游戏截图所示。此外，你还可以轻松地猜测游戏的基本动态。然而，在训练的开始阶段，强化学习智能体什么都不知道。这个比较非常重要，因为它提供了关于在多个环境之间传递知识重要性的宝贵洞察。一个能够利用从源任务中获得的经验的智能体，在目标环境中可以更快地学习。例如，如果源环境是Pong而目标环境是Breakout，那么许多视觉组件可以被重用，从而节省大量计算时间。为了准确理解其总体重要性，想象一下在更复杂的环境中获得的效率：
- en: '![](img/89e22bd8-eb9f-445b-8bb4-2ff4300b3183.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89e22bd8-eb9f-445b-8bb4-2ff4300b3183.png)'
- en: Figure 13.5\. A screenshot of Space Invaders. Are you able to infer the role
    of the sprites?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5\. 《太空侵略者》的截图。你能推测出精灵的角色吗？
- en: When speaking about transfer learning, we refer to 0-shot learning, 1-shot learning,
    and so on, as the number of attempts required in the target domain. For example,
    0-shot learning means that the policy that has been trained on a source domain
    is directly employed on the target domain without further training. In this case,
    the agent must develop strong generalization capabilities to adjust itself to
    the new task.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 说到迁移学习时，我们通常提到0-shot学习、1-shot学习，依此类推，这些术语指的是在目标领域中所需的尝试次数。例如，0-shot学习意味着在源领域上训练的策略可以直接应用到目标领域，而无需进一步训练。在这种情况下，智能体必须具备强大的泛化能力，以便适应新任务。
- en: Types of transfer learning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习的类型
- en: Many types of transfer learning exist, and their usage depends on the specific
    case and needs. One of the distinctions is related to the number of source environments.
    Obviously, the more source environments you are training the agent on, the more
    diversity it has, and the more experience can be used in the target domain. Transfer
    learning from multiple source domains is called multi-task learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种类型的迁移学习，其使用取决于具体情况和需求。一个区分点与源环境的数量有关。显然，你训练智能体的源环境越多，它的多样性就越大，能够在目标领域使用的经验也就越多。从多个源领域迁移学习被称为多任务学习。
- en: 1-task learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1-任务学习
- en: '1-task learning or simply transfer learning is the task of training the policy
    on one domain and transferring it onto a new one. Three major techniques can be
    employed to do that. These are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 单任务学习或称为迁移学习，是在一个领域上训练策略并将其迁移到另一个领域的任务。可以采用三种主要技术来实现这一目标，具体如下：
- en: '**Fine-tuning**: This involves the refinement of the learned model on the target
    task. If you get involved in machine learning, and especially in computer vision
    or natural language processing, you have probably used this technique already.
    Unfortunately, in reinforcement learning, fine-tuning is not as easy as it is
    in the aforementioned fields, as it requires more careful engineering and generally has
    lower benefits. The reason for this is that, in general, the gap between the two
    RL tasks is bigger than the gap between two different image domains. For example,
    the differences between the classification of a cat and a dog are minor compared
    to the differences between Pong and Breakout. Nonetheless, fine-tuning can also
    be used in RL and tuning just the last few layers (or substituting them if the
    action space is totally different) could give better generalization properties.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：这涉及对目标任务上学到的模型进行优化。如果你参与过机器学习，尤其是在计算机视觉或自然语言处理领域，你可能已经使用过这种技术。不幸的是，在强化学习中，微调并不像在上述领域那样容易，因为它需要更精细的工程设计，并且通常带来的收益较小。其原因在于，一般而言，两个强化学习任务之间的差距比两个不同图像领域之间的差距要大。例如，猫和狗的分类差异较小，而Pong和Breakout之间的差异则较大。然而，微调仍然可以在强化学习中使用，调整最后几层（或者在动作空间完全不同的情况下替换它们）可能会带来更好的泛化能力。'
- en: '**Domain randomization**: This is based on the idea that the diversification
    of the dynamics on a source domain increases the robustness of the policy on a
    new environment. Domain randomization works by manipulating the source domain,
    for example, by varying the physics of the simulator, so that the policy that
    has been trained on multiple randomly modified source domains is robust enough
    to perform well on a target domain. This strategy is more effective for training
    agents that need to be employed in the real world. In such circumstances, the
    policy is more robust and the simulation doesn''t have to be exactly the same
    as the physical world to provide the required levels of performance.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域随机化**：这一方法基于这样一个思想，即源领域的动态多样化可以增强策略在新环境中的鲁棒性。领域随机化通过操控源领域来实现，例如，通过改变仿真器的物理特性，使得在多个随机修改的源领域上训练的策略足够鲁棒，能够在目标领域中表现良好。这种策略对于训练需要应用于现实世界的代理更加有效。在这种情况下，策略更具鲁棒性，仿真不必与物理世界完全相同，仍然能够提供所需的性能水平。'
- en: '**Domain Adaptation**: This is another process that''s used, especially to
    map a policy from a simulation-based source domain to a target physical world.
    Domain adaptation consists of changing the data distribution of the source domain
    to match that of the target. It is mainly used in image-based tasks, and the models
    usually make use of **generative adversarial networks** (**GANs**) to turn synthetic
    images into realistic ones.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域适应**：这是另一个常用的过程，尤其用于将仿真源领域的策略映射到目标物理世界。领域适应的核心是将源领域的数据分布调整为与目标领域匹配。它主要应用于基于图像的任务，模型通常使用**生成对抗网络**（**GANs**）将合成图像转化为真实图像。'
- en: Multi-task learning
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务学习
- en: In multi-task learning, the higher the number of environments the agent has
    been trained on, the more diversity and the better performance the agent will
    achieve on the target environment. The multiple source tasks can either be learned
    by one or multiple agents. If only one agent has been trained, then its deployment
    on the target task is easy. Otherwise, if multiple agents learned separate tasks,
    then the resulting policies can either be used as an ensemble, and the predictions
    on the target task averaged, or an intermediate step called distillation is employed
    to merge the policies into one. Specifically, the process of distillation compresses
    the knowledge of an ensemble of models into a single one that is easier to deploy
    and that infers faster.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习中，代理已训练的环境数量越多，代理在目标环境上的多样性和性能就会越好。多个源任务可以由一个或多个代理来学习。如果只有一个代理被训练，那么将其部署到目标任务上很容易。否则，如果多个代理学习了不同的任务，那么得到的策略可以作为一个集成模型使用，并对目标任务的预测进行平均，或者使用一个名为蒸馏的中间步骤，将多个策略合并为一个。具体来说，蒸馏过程将多个模型的知识压缩成一个，便于部署且推理速度更快。
- en: RL in the real world
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实世界中的强化学习
- en: So far, in this chapter, we went through the best practices when developing
    deep RL algorithms and the challenges behind RL. We also saw how unsupervised
    RL and meta-learning can alleviate the problem of low efficiency and bad generalization.
    Now, we want to show you the problems that need to be addressed when employing
    an RL agent in the real world, and how the gap within a simulated environment
    can be bridged.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们介绍了开发深度强化学习算法的最佳实践和强化学习背后的挑战。我们还看到无监督强化学习和元学习如何缓解低效和泛化不良的问题。现在，我们想向你展示在将强化学习智能体应用于真实世界时需要解决的问题，以及如何弥合模拟环境中的差距。
- en: Designing an agent that is capable of performing actions in the real world is
    demanding. But most reinforcement learning applications need to be deployed in
    the world. Thus, we have to understand the main challenges that we face when dealing
    with the complexity of the physical world and consider some useful techniques.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个能够在真实世界中执行动作的智能体是非常具有挑战性的。但大多数强化学习应用需要在现实世界中部署。因此，我们必须了解在面对物理世界的复杂性时所面临的主要挑战，并考虑一些有用的技术。
- en: Facing real-world challenges
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面对真实世界的挑战
- en: Besides the big problems of sample-efficiency and generalization, when dealing
    with the real world, we need to face problems such as safety and domain constraints.
    In fact, the agent is often not free to interact with the world due to safety
    and cost constraints. A solution may come from the use of constraint algorithms
    such as TRPO and PPO, which are embedded into the system mechanisms to limit the
    change of actions while training. This could prevent the agent from a drastic
    change in its behavior. Unfortunately, in highly sensitive domains, this is not
    enough. For example, nowadays, you cannot start training a self-driving car on
    the road straight away. The policy may take hundreds or thousands of cycles to
    understand that falling off a cliff leads to a bad conclusion and learn to avoid
    it. The alternative option of training the policy in a simulation first is a viable
    option. Nevertheless, when employed in cities, more safety-related decisions have
    to be made.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了样本效率和泛化能力方面的重大问题之外，在处理真实世界时，我们还需要面对诸如安全性和领域约束等问题。事实上，由于安全性和成本约束，智能体通常不能自由地与世界进行交互。一种解决方案可能来自使用约束算法，例如TRPO和PPO，这些算法被嵌入系统机制中，以限制在训练过程中行动的变化。这可以防止智能体行为的剧烈变化。不幸的是，在高度敏感的领域，这还不够。例如，如今，你不能直接在道路上开始训练自动驾驶汽车。政策可能需要经过数百或数千个周期才能理解从悬崖上掉下来会导致不良结果，并学会避免这种情况。先在模拟中训练政策的替代方案是可行的。然而，当应用到城市环境时，必须做出更多与安全相关的决策。
- en: 'As we just hinted at, a simulation-first solution is a feasible approach and
    depending on the complexity of the real task, it may lead to good performance.
    However, the simulator has to mimic the real-world environment as closely as possible.
    For example, the simulator on the left-hand side of the following image cannot
    be used if the world resembles the right-hand side of the same image. This gap
    between the real and the simulated world is known as the reality gap:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才提到的，模拟优先的解决方案是一种可行的方法，根据实际任务的复杂性，它可能会带来良好的性能。然而，模拟器必须尽可能地模仿真实世界的环境。例如，如果下图左侧的模拟器与右侧的真实世界相似，那么该模拟器就无法使用。真实世界与模拟世界之间的差距被称为现实差距：
- en: '![](img/0687367a-cd9f-4f80-b4ab-570182383d94.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0687367a-cd9f-4f80-b4ab-570182383d94.png)'
- en: Figure 13.6\. Comparison between an artificial world and the physical world
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6\. 人工世界与物理世界的比较
- en: On the other hand, using a highly accurate and realistic environment may not
    be feasible either. The bottleneck is now the computation power that's required
    by the simulator. This limitation can be partially overcome by starting with a
    faster and less accurate simulator, and then progressively increasing the fidelity
    so as to decrease the reality gap. Eventually, this is to the detriment of the
    speed, but at this point, the agent should have already learned most of the tasks
    and may need only a few iterations to fine-tune itself. However, it is very difficult
    to develop highly accurate simulators that mimic the physical world. Thus, in
    practice, the reality gap will remain and techniques that improve generalization
    will have the responsibility to handle the situation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用高度准确和现实的环境可能也不可行。瓶颈现在是模拟器所需的计算能力。这个限制可以通过从一个速度更快、精度更低的模拟器开始，并逐步增加保真度以缩小现实差距来部分克服。最终，这将牺牲速度，但此时，代理应该已经学习了大部分任务，可能只需要少量的迭代来进行微调。然而，开发高度准确的模拟器来模拟物理世界是非常困难的。因此，在实践中，现实差距将依然存在，改善泛化的技术将有责任处理这种情况。
- en: Bridging the gap between simulation and the real world
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弥合模拟和现实世界之间的差距
- en: To seamlessly transition from the simulation to the real world and thus overtake
    the reality gap, some generalization techniques that we presented earlier, such
    as domain adaptation and domain randomization, could be used. For example, in
    the paper *Learning Dexterous In-Hand Manipulation*, the authors trained a human-like
    robot to manipulate physical objects with incredible dexterity using domain randomization.
    The policy learned from many different parallel simulations that were designed
    to provide a variety of experiences with random physical and visual attributes.
    This mechanism that prefers generalization over realism overall has been key,
    considering that the system, when deployed, showed a rich set of in-hand dexterous
    manipulation strategies, many of which are used by humans as well.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从模拟到现实世界无缝过渡，从而克服现实差距，我们之前提出的一些泛化技术，例如领域自适应和领域随机化，可以被使用。例如，在论文《学习灵巧的手内操作》中，作者通过领域随机化训练了一个类人机器人，利用惊人的灵巧性操作物理物体。该策略通过许多不同的平行模拟进行训练，这些模拟旨在提供具有随机物理和视觉属性的多样化体验。这种偏向于泛化而非现实的机制至关重要，考虑到系统在部署时展示了一系列丰富的手内灵巧操作策略，其中许多策略也被人类使用。
- en: Creating your own environment
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建你自己的环境
- en: For educational purposes, in this book, we have predominantly used fast and
    small-scale tasks that could best fit our needs. However, there are plenty of
    simulators in existence for locomotion tasks (such as Gazebo, Roboschool, and
    Mujoco), mechanical engineering, transportation, self-driving cars, security,
    and many more. These existing environments are diverse, but there isn't one for
    every possible application. Thus, in some situations, you may find yourself in
    charge of creating your own.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 出于教育目的，在本书中，我们主要使用了适合我们需求的快速且小规模的任务。然而，现有许多模拟器可以用于运动任务（如Gazebo、Roboschool和Mujoco）、机械工程、运输、自动驾驶汽车、安全等。这些现有环境种类繁多，但并不是每个应用都有现成的模拟器。因此，在某些情况下，你可能需要自己创建环境。
- en: The reward function by itself is difficult to design, but it is a key part of
    RL. With the wrong reward function, the environment can be impossible to solve
    and the agent may learn the wrong behaviors. In [Chapter 1](0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml),
    *The Landscape of Reinforcement Learning*, we gave the example of the boat-racing
    game, in which the boat maximized the reward by driving in a circle to capture
    repopulating targets instead of running toward the end of the trajectory as fast
    as possible. These are the kinds of behaviors to avoid while designing the reward
    function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数本身很难设计，但它是强化学习（RL）中的关键部分。如果奖励函数设置错误，环境可能无法解决，代理可能会学到错误的行为。在[第一章](0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml)《强化学习的全景》中，我们举了一个赛船游戏的例子，其中船通过绕圈子来最大化奖励，捕捉重新生成的目标，而不是尽可能快地朝着轨迹终点行驶。这些就是在设计奖励函数时需要避免的行为。
- en: The general advice for designing the reward function (that can be applied in
    any environment) is to use positive rewards to incentive exploration and discourage
    the terminal states or negative rewards if the goal is to reach a terminal state
    as quickly as possible. The shape of the reward function is important to consider.
    Throughout this book, we have warned against sparse rewards. An optimal reward
    function should offer a smooth and dense function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 设计奖励函数的一般建议（适用于任何环境）是使用正奖励来激励探索，并通过负奖励来抑制终止状态，或者如果目标是尽可能快速地达到终止状态，则使用负奖励。奖励函数的形状是一个重要的考虑因素。在本书中，我们曾多次警告稀疏奖励的问题。一个理想的奖励函数应该提供平滑且密集的反馈。
- en: 'If, for some reason, the reward function is very difficult to put into formulas,
    there are two additional ways in which a supervision signal can be provided:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某些原因，奖励函数非常难以用公式表示，可以通过两种额外方式提供监督信号：
- en: Give a demonstration of the task using imitation learning or inverse reinforcement
    learning.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模仿学习或逆强化学习展示任务。
- en: Use human preferences to provide feedback about the agent's behavior.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人类偏好来提供关于智能体行为的反馈。
- en: The latter point is still a novel approach and if you are interested in it,
    you may find the paper *Deep Reinforcement Learning from Policy-Dependent Human
    Feedback* an interesting read ([https://arxiv.org/abs/1902.04257](https://arxiv.org/abs/1902.04257)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 后者仍然是一种新颖的方法，如果你对此感兴趣，可以阅读论文《*从依赖策略的人类反馈中进行深度强化学习*》([https://arxiv.org/abs/1902.04257](https://arxiv.org/abs/1902.04257))，这将是一本有趣的读物。
- en: Future of RL and its impact on society
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的未来及其对社会的影响
- en: The first foundations of AI were built more than 50 years ago, but only in the
    last few years has the innovation brought by AI spread through the world as a
    mainstream technology. This new wave of innovation is mainly due to the evolution
    of deep neural networks in supervised learning systems. However, the most recent
    breakthrough in artificial intelligence involves reinforcement learning, and most
    notably, deep reinforcement learning. Results like the ones that were obtained
    in the game of Go and Dota highlight the impressive quality of RL algorithms that
    are able to show long-term planning, ability in teamwork, and discover new game
    strategies that are difficult to comprehend even for humans.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的最初基础建立于50多年前，但直到最近几年，人工智能所带来的创新才作为主流技术传播到世界各地。这一波创新的浪潮主要得益于深度神经网络在监督学习系统中的演变。然而，人工智能的最新突破涉及强化学习，特别是深度强化学习。在围棋和Dota游戏中取得的成果突显了强化学习算法的卓越质量，这些算法能够进行长期规划、展现团队协作能力，并发现新的游戏策略，甚至是人类难以理解的策略。
- en: The remarkable results that were obtained in the simulated environments started
    a new wave of applications of reinforcement learning in the physical world. We
    are only at the beginning, but many areas are and will be impacted, bringing with
    it profound transformations. RL agents that are embedded in our everyday life
    can enhance the quality of life by automating tedious work, addressing world-level
    challenges, and discovering new drugs – just to name a few possibilities. However,
    these systems, which will populate both our world and our lives, need to be safe
    and reliable. We aren't at this point yet, but we are on the right track.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟环境中取得的显著成果开启了强化学习在物理世界中的新应用浪潮。我们才刚刚开始，但许多领域正在并将继续受到影响，带来深刻的变革。嵌入日常生活中的强化学习智能体可以通过自动化繁琐的工作、解决全球性挑战、发现新药物等方式提升生活质量——这些只是一些可能性。然而，这些将在我们世界和生活中广泛应用的系统需要安全可靠。目前我们尚未达到这一点，但我们在朝着正确的方向迈进。
- en: The ethical use of AI has become a broad concern, such as in the employment
    of autonomous weapons. With this rapid technological progress, it is hard for
    the policymakers and the population to be at the forefront of creating open discussions
    about these issues. Many influential and reputable people also suggest that AI
    is a potential threat to humanity. But the future is impossible to predict, and
    the technology has a long way to go before developing agents that can actually
    show abilities that are comparable to those of humans. We have creativity, emotions,
    and adaptability that, for now, cannot be emulated by RL.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: AI的伦理使用已经成为广泛关注的问题，比如在使用自主武器时。随着技术的快速发展，政策制定者和民众很难在这些问题上进行开放讨论。许多有影响力和有声望的人也认为AI是对人类的潜在威胁。但未来是无法预测的，且技术还有很长的路要走，才能开发出具备与人类相当能力的代理。我们有创造力、情感和适应性，这些现在是强化学习无法模拟的。
- en: With careful attention, the near-term benefits brought by RL can dramatically
    outweigh the negative side. But to embed sophisticated RL agents in the physical
    environment, we need to work on the RL challenges we outlined previously. These
    are solvable and, once addressed, reinforcement learning has the potential to
    decrease social inequalities, improve the quality of our life, and the quality
    of our planet.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过细心的关注，强化学习带来的短期益处可以显著超过其负面影响。但是，要将复杂的强化学习代理嵌入到物理环境中，我们需要解决之前提到的强化学习挑战。这些问题是可以解决的，一旦解决，强化学习有可能减少社会不平等，改善我们的生活质量以及地球的质量。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Throughout this book, we learned and implemented many reinforcement learning
    algorithms, but all this variety can be quite confusing when it comes to choosing
    one. For this reason, in this final chapter, we provided a rule of thumb that
    can be used to pick the class of RL algorithms that best fits your problem. It
    mainly considers the computational time and the sample efficiency of the algorithm.
    Furthermore, we provided some tips and tricks so that you can train and debug
    deep reinforcement learning algorithms better so as to make the process easier.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们学习并实现了许多强化学习算法，但所有这些不同的算法在选择时可能会让人感到困惑。因此，在最后一章中，我们提供了一个经验法则，帮助你选择最适合你问题的强化学习算法类别。它主要考虑算法的计算时间和样本效率。此外，我们还提供了一些技巧，以帮助你更好地训练和调试深度强化学习算法，从而使这个过程更容易。
- en: 'We also discussed the hidden challenges of reinforcement learning: stability
    and reproducibility, efficiency, and generalization. These are the main issues
    that have to be overcome in order to employ RL agents in the physical world. In
    fact, we detailed unsupervised reinforcement learning and transfer learning, two
    strategies that can be used to greatly improve generalization and sample efficiency.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了强化学习中的隐性挑战：稳定性与可复现性、效率和泛化能力。这些是必须克服的主要问题，以便将强化学习代理应用于物理世界。事实上，我们详细介绍了无监督强化学习和迁移学习，这两种策略可以大大提高泛化能力和样本效率。
- en: Additionally, we detailed the most critical open problems and the cultural and
    technological impacts that reinforcement learning may have on our lives.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们详细阐述了强化学习可能对我们的生活产生的最关键的开放性问题，以及它在文化和技术方面的影响。
- en: We hope that this book has provided you with a comprehensive understanding of
    reinforcement learning and piqued your interest in this fascinating field.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这本书能让你对强化学习有一个全面的了解，并激发你对这个迷人领域的兴趣。
- en: Questions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How would you rank DQN, A2C, and ES based on their sample efficiency?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何根据样本效率来排名DQN、A2C和ES？
- en: What would their rank be if they were rated on the training time and 100 CPUs
    were available?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果根据训练时间并且提供100个CPU，他们的排名会如何？
- en: Would you start debugging an RL algorithm on CartPole or MontezumaRevenge?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会在调试强化学习算法时从CartPole还是MontezumaRevenge开始？
- en: Why is it better to use multiple seeds when comparing multiple deep RL algorithms?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在比较多个深度强化学习算法时，使用多个种子更好？
- en: Does the intrinsic reward help with the exploration of an environment?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内在奖励是否有助于环境探索？
- en: What's transfer learning?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: Further reading
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For an approach that uses a pure curiosity-driven approach in the Atari games,
    read the paper *Large-scale study of curiosity-driven learning *([https://arxiv.org/pdf/1808.04355.pdf](https://arxiv.org/pdf/1808.04355.pdf)).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对在Atari游戏中使用纯粹的好奇心驱动方法感兴趣，可以阅读这篇论文 *Large-scale study of curiosity-driven
    learning* ([https://arxiv.org/pdf/1808.04355.pdf](https://arxiv.org/pdf/1808.04355.pdf))。
- en: For practical use of domain randomization for learning dexterous in-hand manipulation,
    read the paper *Learning Dexterous In-Hand Manipulation* ([https://arxiv.org/pdf/1808.00177.pdf](https://arxiv.org/pdf/1808.00177.pdf)).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于将领域随机化应用于学习灵巧的手内操作，阅读论文 *学习灵巧的手内操作* ([https://arxiv.org/pdf/1808.00177.pdf](https://arxiv.org/pdf/1808.00177.pdf))。
- en: For some work that shows how human feedback can be applied as an alternative
    to the reward function, read the paper *Deep Reinforcement Learning from Policy-Dependent
    Human Feedback* ([https://arxiv.org/pdf/1902.04257.pdf](https://arxiv.org/pdf/1902.04257.pdf)).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于展示如何将人类反馈作为奖励函数的替代方法的工作，阅读论文 *基于策略依赖的人类反馈的深度强化学习* ([https://arxiv.org/pdf/1902.04257.pdf](https://arxiv.org/pdf/1902.04257.pdf))。
