- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Hyperparameter Tuning, MLOps, and AutoML
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优、MLOps 和 AutoML
- en: Developing a **Machine Learning** (**ML**) model is an iterative process; the
    presence of so many models, each with a large number of hyperparameters, complicates
    things for beginners. This chapter continues from the previous chapter and explains
    the need for continuous training in ML pipelines. It will provide a glimpse of
    the AutoML options currently available for your ML workflow, expand on the situations
    in which no-code/low-code solutions are useful, and explore the solutions provided
    by major cloud providers in terms of their ease of use, features, and model explainability.
    This chapter will also explore orchestration tools such as Kubeflow and Vertex
    AI, which you can use to manage the continuous training and deployment of your
    ML models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一个**机器学习**（**ML**）模型是一个迭代的过程；那么多的模型，每个模型又有大量的超参数，这使得初学者感到复杂。本章继续上章内容，解释了在
    ML 管道中需要持续训练的原因。它将提供目前可用于你的 ML 工作流的 AutoML 选项，扩展在无代码/低代码解决方案有用的情况，并探讨主要云服务商在易用性、功能性和模型可解释性方面提供的解决方案。本章还将探讨如
    Kubeflow 和 Vertex AI 等编排工具，它们可以帮助你管理 ML 模型的持续训练和部署。
- en: After completing this chapter, you will be familiar with the concept of hyperparameter
    tuning and popular off-the-shelf AutoML and ML orchestration tools.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将熟悉超参数调优的概念以及流行的现成 AutoML 和 ML 编排工具。
- en: 'In this chapter, these topics will be covered in the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以下主题将通过以下章节进行讲解：
- en: Introducing AutoML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 AutoML
- en: Introducing H2O AutoML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 H2O AutoML
- en: Working with Azure AutoML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure AutoML
- en: Understanding AWS SageMaker Autopilot
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 AWS SageMaker Autopilot
- en: The need for MLOps
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps 的需求
- en: TFX – a scalable end-to-end platform for AI/ML workflows
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFX – 一个可扩展的端到端 AI/ML 工作流平台
- en: Understanding Kubeflow
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 Kubeflow
- en: Katib for hyperparameter tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katib 用于超参数调优
- en: Vertex AI
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8 installed, as well as certain
    Python packages, listed here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求你安装 Python 3.8，并且安装以下 Python 包：
- en: TensorFlow (>=2.7.0)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow (>=2.7.0)
- en: NumPy
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: 'You need to install H2O as follows:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要按以下方式安装 H2O：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, you can install it this way:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以通过以下方式安装：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Install the Azure ML client library:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Azure ML 客户端库：
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Introduction to AutoML
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoML 介绍
- en: Anyone who has worked in the domain of ML can tell you that building ML models
    is a complex and iterative process. You start with a dataset and a set of features,
    and then train a model on that data. As you get more data, you add more features,
    and you retrain your model. This process continues until you have a model that
    generalizes well to new data. The task is complicated by the fact that there is
    a multitude of hyperparameters and that they have a kind of non-linear relationship
    to model performance. Choosing the right model and selecting the optimum hyperparameters
    is still considered alchemy by many.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在机器学习领域工作过的人都能告诉你，构建 ML 模型是一个复杂且迭代的过程。你从一个数据集和一组特征开始，然后在这些数据上训练模型。随着数据的增加，你会加入更多的特征，并重新训练模型。这个过程会持续下去，直到你拥有一个能很好地泛化到新数据的模型。这个任务被多种超参数所复杂化，而这些超参数与模型表现之间存在一种非线性关系。选择合适的模型和选择最优的超参数依然被许多人认为是炼金术。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: You can refer to *Has artificial intelligence become alchemy? Matthew Hutson,
    Science, Vol 360, Issue 6388* for more information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考*Has artificial intelligence become alchemy? Matthew Hutson, Science, Vol
    360, Issue 6388*获取更多信息。
- en: Whether AI is alchemy or not is a hot debate. While many who start experimenting
    with AI feel that it is alchemy, there are experts, including us authors, who
    believe it is not so. AI, like any other experimental science, is based on technological
    foundations. Initially, the focus of AI was on the development of AI models and
    architectures, but now the focus is slowly shifting toward responsible and explainable
    AI. As more and more work happens in this area, we will be able to understand
    AI just like any other technology.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AI 是否是炼金术是一个热门的辩论话题。虽然许多开始实验 AI 的人觉得它像炼金术一样，但也有专家，包括我们这些作者，认为事实并非如此。AI 就像任何其他实验科学一样，基于技术基础。最初，AI
    的重点是开发 AI 模型和架构，但现在重点逐渐转向负责任且可解释的 AI。随着越来越多的工作发生在这个领域，我们将能够像理解其他技术一样理解 AI。
- en: Finding the right model, the perfect features, and the best set of hyperparameters
    can be a time-consuming and frustrating task. **Automated ML** (**AutoML**) helps
    you select a model and hyperparameters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 找到合适的模型、完美的特征以及最佳的超参数集可能是一个耗时且令人沮丧的任务。**自动化机器学习**（**AutoML**）帮助你选择一个模型和超参数。
- en: It allows start-ups and low-budget organizations to benefit from the power of
    AI without investing in expensive and difficult-to-find AI talent. Therefore,
    to reduce the barriers to ML and help experts from other domains to use ML in
    their tasks, the automatic generation of ML models has been investigated by most
    major players working in the ML domain.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它使得初创公司和低预算组织能够在不需要投资昂贵且难以找到的AI人才的情况下，受益于AI的强大功能。因此，为了降低机器学习的门槛，并帮助来自其他领域的专家在他们的任务中使用机器学习，大多数在机器学习领域的主要参与者都在研究自动生成机器学习模型的技术。
- en: 'Broadly, we can divide building AI models into the following three iterative
    processes. This is illustrated in *Figure 6**.1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，我们可以将构建AI模型分为以下三个迭代过程。这个过程在*图 6.1*中进行了说明：
- en: '![Figure 6.1 – The iterative processes in the ML pipeline](img/Figure_6.1_B18681.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 机器学习流程中的迭代过程](img/Figure_6.1_B18681.jpg)'
- en: Figure 6.1 – The iterative processes in the ML pipeline
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 机器学习流程中的迭代过程
- en: 'Let’s discuss each one:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一讨论每个过程：
- en: '**Feature engineering**: Feature engineering is an essential part of the ML
    process. Simply put, it is the process of creating and selecting (new) features
    from existing data that can help with model training. This can be done in several
    ways, but the most common methods involve either transformation or aggregation.
    Transformation involves transforming existing data into a new form, such as converting
    text into numerical values. Aggregation involves combining multiple data points
    into a single value, such as taking the average of a group of numbers. Feature
    engineering can be time-consuming, but it is often essential for building accurate
    models. With careful planning and execution, it is possible to create powerful
    features that can make all the difference to your ML results.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：特征工程是机器学习过程中的一个重要部分。简而言之，它是从现有数据中创建和选择（新的）特征的过程，这些特征有助于模型训练。这可以通过多种方式完成，但最常见的方法是变换或聚合。变换涉及将现有数据转化为新形式，例如将文本转化为数值。聚合则是将多个数据点合并为单一值，例如求一组数字的平均值。特征工程可能非常耗时，但它往往是构建准确模型的关键。通过细心的规划和执行，创建强有力的特征是可能的，它们可以对你的机器学习结果产生决定性影响。'
- en: '**Hyperparameter tuning**: This is the process of optimizing an ML algorithm
    by fine-tuning the hyperparameters to obtain better performance. The hyperparameters
    are a set of variables that control the model training process. They differ from
    the parameters of the ML models (for example, weights and biases in neural networks),
    which are learned during the training process. Hyperparameter tuning is crucial
    in any ML project, as it can significantly impact a model’s performance. However,
    it can be a time-consuming and expensive process, especially for large and complex
    datasets. There are various methods for hyperparameter tuning, including manual
    tuning, grid search, and random search. Each method has its advantages and disadvantages,
    and it is essential to select the right method for the specific problem at hand.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调优**：这是通过微调超参数来优化机器学习算法以获得更好性能的过程。超参数是一组控制模型训练过程的变量，它们与机器学习模型的参数（例如，神经网络中的权重和偏差）不同，后者是在训练过程中学习的。超参数调优在任何机器学习项目中都至关重要，因为它可以显著影响模型的性能。然而，这一过程可能是耗时且昂贵的，特别是在处理大型和复杂数据集时。超参数调优有多种方法，包括手动调优、网格搜索和随机搜索。每种方法都有其优缺点，选择合适的方法对于解决当前问题至关重要。'
- en: '**Model selection**: Model selection is the process of choosing the ML algorithm
    that is best suited to a specific task. There are many factors to consider when
    selecting a deep learning model, such as the type of data, the desired output,
    and the computational resources available. The model selection process can be
    daunting. In the last few years, deep learning models have developed near-human
    performance in specific tasks. Many of these models were carefully crafted by
    AI experts, and the process of finding the model and its optimum architecture
    was not straightforward. Instead, it involved much human intuition and many trials
    and failures.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型选择**：模型选择是选择最适合特定任务的机器学习算法的过程。在选择深度学习模型时，需要考虑多个因素，如数据类型、期望输出和可用的计算资源。模型选择过程可能令人生畏。近年来，深度学习模型在特定任务上达到了接近人类的表现。这些模型中的许多是由AI专家精心设计的，而找到最佳模型及其架构的过程并不简单。相反，这涉及大量的人工直觉以及许多尝试和失败。'
- en: Domain experts unfamiliar with deep learning technologies can easily use AutoML
    to create ML solutions. AutoML is designed to simplify the process of creating
    ML models and reduce the expenses of building ML solutions by hand, which it does
    by automating the complete ML pipeline, including data preparation, feature engineering,
    and automatic model generation. Ultimately, the goal of AutoML is to make deep
    learning more accessible to everyone so that we can all benefit from its power.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉深度学习技术的领域专家来说，AutoML可以轻松帮助他们创建机器学习解决方案。AutoML旨在简化创建机器学习模型的过程，并通过自动化整个机器学习管道（包括数据准备、特征工程和自动模型生成）来降低手动构建机器学习解决方案的成本。最终，AutoML的目标是让深度学习变得更加易于接触，让每个人都能从其强大功能中受益。
- en: Automating the creation and tuning of ML end-to-end pipelines offers simpler
    solutions. It helps reduce the time to produce them and ultimately might produce
    architectures that could outperform models crafted by hand. It is an active and
    open research area, with the first research paper on AutoML published at the end
    of 2016.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化创建和调优机器学习（ML）端到端管道提供了更简单的解决方案。它有助于减少生成这些管道的时间，并最终可能生成比手工设计的模型更优的架构。这是一个活跃且开放的研究领域，AutoML的首篇研究论文于2016年底发布。
- en: 'Most commercially available AutoML platforms take structured data and assume
    that data preparation and feature engineering have already been done, but they
    still offer model generation. Model generation can be divided into two parts –
    the search space and optimization methods. The search space identifies the different
    model structures (such as support vector machines, *k*-nearest neighbors, and
    deep neural networks) that can be designed, and optimization methods refine the
    chosen model by adjusting its parameters to improve its performance. Automated
    **Neural Architecture Search** (**NAS**) is gaining much attention nowadays. In
    their survey paper, Elsken et al. divide NAS into the following three major components:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数商业化的AutoML平台接收结构化数据，并假设数据准备和特征工程已经完成，但它们仍提供模型生成。模型生成可以分为两个部分——搜索空间和优化方法。搜索空间确定可以设计的不同模型结构（如支持向量机、*k*-最近邻和深度神经网络），而优化方法通过调整模型参数来细化选定的模型，从而提高其性能。自动化**神经架构搜索**（**NAS**）如今正受到越来越多的关注。在他们的调查论文中，Elsken等人将NAS分为以下三个主要部分：
- en: '**Search space**: This consists of a set of operations and how these can be
    connected to make a valid network architecture.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索空间**：这由一组操作组成，定义了如何将这些操作连接起来形成有效的网络架构。'
- en: '**Search algorithms**: This involves algorithms that are used to find a model
    architecture with high performance in the search space.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索算法**：这涉及用于在搜索空间中找到高性能模型架构的算法。'
- en: '**Model evaluation**: This involves predicting the performance of the proposed
    models.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型评估**：这涉及预测所提议模型的性能。'
- en: '*Figure 6**.2* provides an overview of the NAS pipeline, listing various methods,
    algorithms, and strategies employed in the process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.2* 提供了NAS管道的概览，列出了在该过程中采用的各种方法、算法和策略：'
- en: "![Figure 6.2 – An overview of \uFEFFthe NAS pipeline (adapted from Figure 5\
    \ of AutoML: \uFEFFA survey of the state of the art, Xin He et al\uFEFF., Knowledge-Based\
    \ Systems 212 (2021): 106622)](img/Figure_6.2_B18681.jpg)"
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 自动机器学习（AutoML）管道概览（改编自《AutoML：现状调查，Xin He等，Knowledge-Based Systems
    212（2021）：106622》中的图5）](img/Figure_6.2_B18681.jpg)'
- en: 'Figure 6.2 – An overview of the NAS pipeline (adapted from Figure 5 of AutoML:
    A survey of the state of the art, Xin He et al., Knowledge-Based Systems 212 (2021):
    106622)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.2 – NAS 流水线概述（改编自《AutoML：当前状态的调查》，Xin He 等，Knowledge-Based Systems 212
    (2021): 106622 的图 5）'
- en: Let’s now explore some of the platforms offering AutoML services.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索一些提供 AutoML 服务的平台。
- en: Introducing H2O AutoML
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 H2O AutoML
- en: H2O is a fast, scalable ML and deep learning framework developed by **H2O.ai**,
    released under the open source Apache license. According to the company website,
    at the time of writing, more than 20,000 organizations currently use H2O for their
    ML/deep learning needs. The company offers many products, such as H2O AI Cloud,
    H2O Driverless AI, H2O Wave, and H2O Sparkling Water. In this section, we will
    explore its open source product H2O AutoML.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 是由 **H2O.ai** 开发的一个快速、可扩展的机器学习和深度学习框架，采用开源 Apache 许可证发布。根据公司官网的信息，截至撰写本文时，已有超过
    20,000 个组织在其机器学习/深度学习需求中使用 H2O。公司提供了许多产品，如 H2O AI Cloud、H2O Driverless AI、H2O
    Wave 和 H2O Sparkling Water。在本节中，我们将探讨其开源产品 H2O AutoML。
- en: '**H2O AutoML** is an effort to create a user-friendly ML interface that beginners
    and non-experts can utilize. It automates the process of training and tuning a
    wide range of candidate models. The interface is designed in a way that users
    only need to specify their dataset, the input and output features, and any limitations
    on the number of total models trained or time constraints. The rest of the work
    is done by AutoML; it recognizes the best-performing models within the specified
    time period and provides a *leaderboard*. It is often observed that the stacked
    ensemble model, an ensemble of all the previously trained models, typically holds
    the top position on the leaderboard. Advanced users have countless options to
    choose from; details of these options and their various features are available
    at [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**H2O AutoML** 是一个旨在创建用户友好的机器学习界面的努力，旨在使初学者和非专家也能使用。它自动化了训练和调优多种候选模型的过程。该界面的设计使得用户只需指定数据集、输入和输出特征，以及对训练模型的总数或时间限制的任何要求。其余的工作由
    AutoML 完成；它会在指定的时间内识别表现最好的模型，并提供一个 *排行榜*。通常观察到，堆叠集成模型（由所有先前训练的模型组成的集成模型）通常位居排行榜的顶部。高级用户有无数选项可供选择；这些选项及其各种功能的详细信息可以在
    [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)
    上查看。'
- en: 'You can learn more about H2O on their website: [http://h2o.ai](http://h2o.ai).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在他们的官网上了解更多关于 H2O 的信息：[http://h2o.ai](http://h2o.ai)。
- en: 'Let’s try H2O AutoML on synthetically created data. Before you start, make
    sure you have H2O installed:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在合成数据上尝试 H2O AutoML。在开始之前，确保你已经安装了 H2O：
- en: 'Using scikit-learn’s `make_circles` method, we first create a synthetic dataset
    and save it as a CSV file:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `make_circles` 方法，我们首先创建一个合成数据集并将其保存为 CSV 文件：
- en: '[PRE3]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we initiate the H2O server – before we can use H2O, this is an essential
    step. This can be done using the `init()` function:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们启动 H2O 服务器 – 在使用 H2O 之前，这是一个必要的步骤。可以通过 `init()` 函数来完成：
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the H2O server has been initialized, it will show details about the H2O
    cluster, as shown in *Figure 6.3*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 H2O 服务器初始化完成，它将显示有关 H2O 集群的详细信息，如 *图 6.3* 所示：
- en: '![Figure 6.3 – Output of the H2O init command](img/Figure_6.3_B18681.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – H2O 初始化命令的输出](img/Figure_6.3_B18681.jpg)'
- en: Figure 6.3 – Output of the H2O init command
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – H2O 初始化命令的输出
- en: 'Next, we read the file containing the synthetic data we created earlier. Since
    we want to regard the problem as a classification one, whether the points lie
    in a circle or not, we redefine our label of `y` as `asfactor()` – this will tell
    the H2O AutoML module to treat the `y` variable as categorical and, thus, the
    problem as a classification one. The dataset is split into training, validation,
    and test datasets in a ratio of 60:20:20:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们读取之前创建的合成数据文件。由于我们希望将问题视为分类问题（无论数据点是否位于圆内），我们将 `y` 标签重新定义为 `asfactor()`
    – 这将告诉 H2O AutoML 模块将 `y` 变量视为分类变量，从而将问题视为分类问题。数据集按 60:20:20 的比例划分为训练集、验证集和测试集：
- en: '[PRE5]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we invoke the AutoML module from H2O and train it on our training dataset.
    AutoML will search a maximum of 10 models – you can change the `max_models` parameter
    to increase or decrease the number of models to test:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们从 H2O 调用 AutoML 模块，并在我们的训练数据集上进行训练。AutoML 将搜索最多 10 个模型 – 你可以更改 `max_models`
    参数，以增加或减少要测试的模型数量：
- en: '[PRE6]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For each of the models, H2O provides a performance summary. For example, in
    *Figure 6.4*, you can refer to the evaluation summary for `BinomialGLM`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，H2O 提供了性能摘要。例如，在*图 6.4*中，您可以参考 `BinomialGLM` 的评估摘要：
- en: '![Figure 6.4 – The performance summary of one of the models by H2O AutoML](img/Figure_6.4_B18681.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – H2O AutoML 模型的性能摘要](img/Figure_6.4_B18681.jpg)'
- en: Figure 6.4 – The performance summary of one of the models by H2O AutoML
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – H2O AutoML 模型的性能摘要
- en: 'You can check the performance of all the models evaluated by H2O AutoML on
    the leaderboard:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在排行榜上查看所有由 H2O AutoML 评估的模型的性能：
- en: '[PRE7]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Figure 6**.5* shows a snippet of the leaderboard:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.5* 显示了排行榜的一部分：'
- en: '![Figure 6.5 – Leaderboard summary – H2O AutoML](img/Figure_6.5_B18681.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 排行榜摘要 – H2O AutoML](img/Figure_6.5_B18681.jpg)'
- en: Figure 6.5 – Leaderboard summary – H2O AutoML
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 排行榜摘要 – H2O AutoML
- en: Besides AutoML, H2O also provides methods for explaining a model. Please refer
    to [*Chapter 9*](B18681_09.xhtml#_idTextAnchor198) and the Jupyter notebook on
    H2O in the book’s GitHub repository to learn more about the model explainability
    features provided by H2O.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 AutoML，H2O 还提供了模型解释的方法。请参阅[*第 9 章*](B18681_09.xhtml#_idTextAnchor198)以及书籍
    GitHub 仓库中的 H2O Jupyter 笔记本，了解更多关于 H2O 提供的模型可解释性特性。
- en: Next, we will explore the Microsoft AutoML tool, Azure AutoML.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索 Microsoft AutoML 工具——Azure AutoML。
- en: Working with Azure AutoML
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Azure AutoML 一起工作
- en: Azure provides AutoML solutions for tabular, text, and image data. Azure uses
    Bayesian optimization to find the optimal model architecture, along with collaborative
    filtering to search for the optimum pipeline for data transformation. To be able
    to use Azure AutoML, you will need to have an Azure subscription account, create
    an Azure Machine Learning workspace, and create a computer cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 提供了适用于表格、文本和图像数据的 AutoML 解决方案。Azure 使用贝叶斯优化来寻找最佳的模型架构，并结合协同过滤来搜索最适合数据转换的最佳管道。要使用
    Azure AutoML，您需要拥有 Azure 订阅账户，创建 Azure 机器学习工作区，并创建计算集群。
- en: Azure AutoML is a cloud-based service that supports classification, regression,
    and time-series forecasting tasks. Not only does it perform hyperparameter tuning
    and model searching, but it can also perform feature engineering tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Azure AutoML 是一项基于云的服务，支持分类、回归和时间序列预测任务。它不仅执行超参数调优和模型搜索，还可以执行特征工程任务。
- en: You can interact with Azure AutoML either using ML Studio (which provides a
    no-code interface) or via the Python SDK.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 ML Studio（提供无代码界面）或通过 Python SDK 与 Azure AutoML 交互。
- en: 'The basic steps involved in building an AutoML pipeline using the Python SDK
    are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python SDK 构建 AutoML 管道的基本步骤如下：
- en: 'Connect to the Azure Machine Learning workspace using the following code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码连接到 Azure 机器学习工作区：
- en: '[PRE8]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Get the data and convert it into an ML table format.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据并将其转换为 ML 表格格式。
- en: Configure the AutoML job using the `automl` module.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `automl` 模块配置 AutoML 作业。
- en: Submit the job for training.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交作业进行训练。
- en: Get the best model.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最佳模型。
- en: Each of these steps has various configuration settings based on your needs.
    For example, you can choose the type of cross-validation strategy, the evaluation
    metrics, and the maximum amount of training time. You can also choose whether
    you want to use automated feature engineering or not. If you want to run the model
    on different platforms and devices, you can also configure Azure AutoML to only
    look into the models that can be converted to the **Open Neural Network Exchange**
    (**ONNX**) standard, an open source format for storing deep learning models. Microsoft
    provides $200 for a free trial – once it is over, you can opt for the pay-as-you-go
    model or buy a subscription.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤都有不同的配置设置，具体取决于您的需求。例如，您可以选择交叉验证策略的类型、评估指标和最大训练时间。您还可以选择是否使用自动特征工程。如果您希望在不同的平台和设备上运行模型，您还可以配置
    Azure AutoML，只查看那些可以转换为**开放神经网络交换**（**ONNX**）标准的模型，ONNX 是一个用于存储深度学习模型的开源格式。微软为免费试用提供了
    200 美元的额度——试用期结束后，您可以选择按需付费或购买订阅。
- en: Understanding Amazon SageMaker Autopilot
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 Amazon SageMaker Autopilot
- en: When you use Amazon SageMaker with the Autopilot features enabled, many of the
    most time-consuming parts of the AutoML process are handled automatically. It
    investigates your data, chooses algorithms suited to your type of problem, and
    cleans up the data to make model training and tuning easier. When necessary, Autopilot
    will automatically subject all potential algorithms to a cross-validation resampling
    procedure to gauge how well they can predict data, other than what they were taught
    to expect. Additionally, it generates metrics to evaluate the potential predictive
    quality of its ML model candidates. By automating these fundamental steps of the
    AutoML process, your ML experience will be greatly streamlined. It ranks all of
    the fine-tuned models that were evaluated in the order of their performance. It
    quickly determines the highest-performing model that can be used with minimal
    effort.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启用Autopilot功能使用Amazon SageMaker时，AutoML过程中的许多耗时步骤都会自动处理。它会自动检查你的数据，选择适合你问题类型的算法，并清理数据，以便更轻松地进行模型训练和调优。必要时，Autopilot会自动对所有潜在算法进行交叉验证重采样，以评估它们预测数据的能力，尤其是它们在未曾学习的情况下如何预测其他数据。此外，它还会生成指标来评估其机器学习模型候选的预测质量。通过自动化这些AutoML过程中的基本步骤，你的机器学习体验将大大简化。它会按性能顺序对所有经过调优的模型进行排名，并迅速确定能够以最小努力使用的最高性能模型。
- en: You can use Autopilot in a number of ways, either with full automation (hence
    the name) or with varying degrees of human guidance, code-free via Amazon SageMaker
    Studio, and code-based via one of the AWS SDKs. Autopilot currently works with
    the following problem types – regression, binary classification, and multiclass
    classification. It works with tabular data in the form of CSV or Parquet files,
    where each column represents a feature of a particular data type, and each row
    represents an observation. Columns can store numeric, categorical, textual, or
    time-series data in the form of comma-separated strings. Autopilot allows you
    to construct ML models on datasets that are hundreds of gigabytes in size.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过多种方式使用Autopilot，无论是完全自动化（因此得名），还是通过不同程度的人类指导，或通过Amazon SageMaker Studio进行无代码操作，或者通过AWS
    SDK中的一种进行基于代码的操作。Autopilot目前支持以下问题类型——回归、二分类和多分类。它支持CSV或Parquet格式的表格数据，每列代表特定数据类型的特征，每行代表一个观测值。列可以存储数值、类别、文本或时间序列数据，数据形式为逗号分隔的字符串。Autopilot允许你在大小达到数百GB的数据集上构建机器学习模型。
- en: 'The following diagram outlines the tasks managed by Azure AutoML AutoPilot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表概述了Azure AutoML Autopilot管理的任务：
- en: '![Figure 6.6 – The Azure AutoML Autopilot pipeline](img/Figure_6.6_B18681.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – Azure AutoML Autopilot管道](img/Figure_6.6_B18681.jpg)'
- en: Figure 6.6 – The Azure AutoML Autopilot pipeline
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – Azure AutoML Autopilot管道
- en: In this section, we introduced AutoML and briefly touched on various AutoML
    platforms. While each service has its own unique workflow and user experience,
    they all begin with the same two steps – activating an API and transferring data
    into a storage repository, or *bucket*. Following the launch of an experiment
    run, models can be exported locally or deployed immediately.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了AutoML，并简要讨论了各种AutoML平台。尽管每个服务都有其独特的工作流程和用户体验，但它们都始于相同的两步——激活API并将数据传输到存储库或*桶*中。启动实验运行后，模型可以导出到本地或立即部署。
- en: The primary distinction between the frameworks lies in the method of entry –
    web-based, via the command line, or using an SDK. Now that we have covered AutoML,
    we will move on to the next step, MLOps, the automation process of building models
    and then deploying and managing them.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架之间的主要区别在于入口方式——基于Web、通过命令行，或使用SDK。现在我们已经介绍了AutoML，接下来将进入MLOps，构建模型、部署并管理模型的自动化过程。
- en: The need for MLOps
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps的需求
- en: The journey from AI research to production is long and full of hurdles. The
    complete AI/ML workload, whether building models, deploying models, or allocating
    web resources, is cumbersome, as any change in one step leads to changes in another.
    Even with advancements in deep learning, the process of taking an idea to production
    can be pretty lengthy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从AI研究到生产的旅程漫长且充满障碍。完整的AI/ML工作负载，无论是构建模型、部署模型，还是分配网络资源，都很繁琐，因为每一步的变化都会导致其他步骤的变化。即使在深度学习方面取得了进展，将一个想法从研究转化为生产的过程依然可能相当漫长。
- en: '*Figure 6**.7* shows the different components of an ML system. We can see that
    only a small fraction of an ML system is involved in the actual learning and prediction;
    however, it requires the support of a vast and complex infrastructure. The problem
    is aggravated by the fact that **Changing Anything Changes Everything** (**CACE**),
    such that minorly tweaking the hyperparameters, changing the learning settings,
    or modifying the data selection methods can mean that the whole system needs to
    change:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.7* 显示了 ML 系统的不同组件。我们可以看到，实际上只有 ML 系统的一小部分参与了实际的学习和预测；然而，它需要庞大而复杂的基础设施支持。问题在于
    **改变任何事物都会改变一切**（**CACE**），即使是微小的调整超参数、改变学习设置或修改数据选择方法，都可能意味着整个系统都需要做出改变：'
- en: '![Figure 6.7 – The different components of an ML system](img/Figure_6.7_B18681.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – ML 系统的不同组件](img/Figure_6.7_B18681.jpg)'
- en: Figure 6.7 – The different components of an ML system
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – ML 系统的不同组件
- en: 'In the IT sector, speed, reliability, and access to information are the main
    ingredients of success and, hence, provide a competitive advantage. IT agility
    is required regardless of the remit of an organization. This becomes indispensable
    when AI/ML-based solutions and products are considered. Currently, most industries
    play out ML tasks manually, with huge delays between building an ML model and
    its deployment. This is shown in *Figure 6**.8*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IT 行业中，速度、可靠性和信息获取是成功的关键因素，因此，提供竞争优势。无论组织的职能如何，都需要 IT 敏捷性。当考虑基于 AI/ML 的解决方案和产品时，这一点尤为重要。目前，大多数行业仍然手动执行
    ML 任务，构建 ML 模型和部署之间存在巨大的延迟。这在 *图 6**.8* 中有所展示：
- en: '![Figure 6.8 – An ML product life cycle (image source: Figure 2: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)](img/Figure_6.8_B18681.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – ML 产品生命周期（图片来源：图 2: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)](img/Figure_6.8_B18681.jpg)'
- en: 'Figure 6.8 – An ML product life cycle (image source: Figure 2: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.8 – ML 产品生命周期（图片来源：图 2: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning）'
- en: The data collected is prepared and processed (with normalization, feature engineering,
    and so on) to provide input to the ML model. Model training followed by model
    evaluation over numerous metrics and techniques is sent to the model registry,
    where it is containerized to be served.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到的数据经过准备和处理（如标准化、特征工程等），以为 ML 模型提供输入。模型训练后，经过多种度量标准和技术的模型评估，将其发送到模型注册表，并容器化以供服务。
- en: From data analysis to model serving, each task is conducted manually. Moreover,
    the transition from one task to another is also manual. The data scientist works
    independently of the Ops team; the trained model is handed to the development
    team, who then deploy the model within their API infrastructure. This can bring
    about training-serving skew – that is, a discrepancy between the model’s performance
    during training and its performance when it is deployed (served).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据分析到模型服务，每个任务都是手动进行的。此外，从一个任务到另一个任务的过渡也是手动的。数据科学家独立于运维团队工作；训练好的模型交给开发团队，由他们在其
    API 基础设施中部署该模型。这可能会导致训练与服务的偏差——即，模型在训练期间的表现与部署后的表现之间存在差异（即服务时的表现）。
- en: 'As model development is separate from its final deployment, there are infrequent
    release iterations. Moreover, an immense setback is the scarcity of active performance
    monitoring. The prediction service does not track or maintain a log of the model
    predictions necessary to detect any degradation or drift in the behavior of the
    model and its performance. Theoretically, this manual process could be sufficient
    if the model were rarely changed or trained. However, in practice, models often
    fail when they are deployed in the real world. The failure is manifold:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型开发与最终部署分离，发布迭代很少。此外，一个巨大的障碍是缺乏主动的性能监控。预测服务没有跟踪或维护模型预测的日志，这对于检测模型行为和性能的任何退化或漂移是必需的。从理论上讲，如果模型很少更改或训练，这一手动过程可能是足够的。然而，在实际操作中，模型在部署到现实世界后常常失败。失败的原因是多方面的：
- en: '**Model degradation**: The model’s accuracy drops with time. In the traditional
    ML pipeline, no continuous monitoring is done to recognize a drop in the model’s
    performance to redress it. The end user bears the brunt of this. Imagine you provide
    services to a fashion house that recommends new apparel designs based on the past
    purchases of customers and fashion trends. However, style changes emphatically
    with time; the “in” colors in autumn no longer work in winter. If your model does
    not ingest recent fashion data and utilize it to give customers recommendations,
    they will complain, and eventually, the site’s user base will shrink. Eventually,
    the business team will notice, and much later, upon distinguishing the issue,
    you will be approached to update the model as per the latest data. This situation
    can be avoided if there is an option to monitor the model’s performance continuously,
    and the systems in place to implement continuous training for newly acquired data.
    This is shown in *Figure 6**.9*:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型退化**：随着时间的推移，模型的准确度会下降。在传统的机器学习管道中，没有持续监控来识别模型性能的下降并加以纠正。最终，最终用户将承受这个问题。假设你为一家时尚公司提供服务，该公司根据客户的过去购买记录和时尚趋势推荐新款服装。然而，风格随着时间的变化而发生显著变化；秋季的“流行色”在冬季不再适用。如果你的模型不采纳最新的时尚数据并利用这些数据为客户提供推荐，他们会抱怨，最终，网站的用户群体将会缩小。最终，业务团队会注意到这一点，并且在很久之后，通过识别问题，你将被要求根据最新数据更新模型。如果可以持续监控模型的性能，并且有系统来实施针对新数据的持续训练，这种情况是可以避免的。见*图6.9*：'
- en: '![Figure 6.9 – Continuous training](img/Figure_6.9_B18681.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 持续训练](img/Figure_6.9_B18681.jpg)'
- en: Figure 6.9 – Continuous training
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 持续训练
- en: '**Data drift**: The difference between the joint distribution of the input
    features and the output for training and test datasets can cause dataset drift.
    When a model is initially deployed, the real-world data has a similar distribution
    to the training dataset, but this distribution tends to change with time. For
    instance, you build a model to detect network intrusion based on the data available
    at that time. After six months, do you think it will work as proficiently as it
    did at the time of deployment? It may, but chances are it will be fast drifting
    away – in the rapidly changing internet world, six months is almost six generations!
    This problem can be resolved if there are options to slice metrics on recent data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移**：训练和测试数据集的输入特征与输出联合分布之间的差异可能导致数据集漂移。当模型最初部署时，真实世界的数据与训练数据集有相似的分布，但这种分布会随着时间的推移发生变化。例如，你建立了一个基于当时可用数据来检测网络入侵的模型。六个月后，你认为它还能像部署时那样有效吗？可能能，但很有可能会迅速偏离——在快速变化的互联网世界里，六个月几乎就是六代！如果有选项可以基于最新数据对指标进行切片，这个问题就能得到解决。'
- en: '**Feedback loops**: Unintentional feedback can manipulate a model''s prediction
    and sway its training data. Suppose somebody works for a music streaming company
    in which a recommendation system is utilized to suggest new music albums based
    on a user’s listening history and profile. The system recommends albums with,
    let’s say, more than a 70% confidence level. Now, the company has decided to add
    a like or dislike feature. Initially, the company will be excited, as the recommended
    albums will receive increasing likes. However, over time, the user’s viewing history
    will impact the model’s predictions. As a result, the system will start recommending
    music similar to what the user has previously listened to, potentially missing
    out on new music that the user may enjoy discovering. Continuous system metric
    monitoring is a potential solution to issues such as this.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反馈循环**：无意中的反馈可能会操控模型的预测并影响其训练数据。假设某人正在为一家音乐流媒体公司工作，该公司利用推荐系统根据用户的听歌历史和个人资料推荐新专辑。系统推荐的专辑信心度，假设超过70%。现在，公司决定增加一个喜欢或不喜欢的功能。最初，公司会感到兴奋，因为推荐的专辑会获得越来越多的点赞。然而，随着时间的推移，用户的观看历史将影响模型的预测。因此，系统将开始推荐与用户之前听过的音乐类似的内容，可能会错过用户可能会喜欢的新音乐。持续的系统指标监控是解决此类问题的潜在方案。'
- en: 'To learn more about the technical debt incurred by ML models, I suggest you
    read the paper titled *Machine Learning: The High-Interest Credit Card of Technical
    Debt* by Sculley et al. In this paper, a detailed discussion of the technical
    debt in ML is carried out, and the maintenance costs associated with systems using
    AI/ML solutions are also covered.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解机器学习模型所带来的技术债务，我建议您阅读Sculley等人所写的论文《*机器学习：技术债务的高利贷信用卡*》。在这篇论文中，详细讨论了机器学习中的技术债务，并涵盖了使用AI/ML解决方案的系统所带来的维护成本。
- en: Although it is impossible and unnecessary to obliterate technical debt completely,
    a holistic approach can reduce it. What is needed is a system that permits us
    to integrate standard DevOps pipelines into our ML workflows – ML pipeline automation,
    or MLOps.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管完全消除技术债务既不可能也不必要，但采用整体方法可以减少技术债务。所需的是一个能够将标准DevOps管道集成到我们的机器学习工作流中的系统——即机器学习管道自动化，或MLOps。
- en: MLOps, or DevOps for ML, is a bunch of best practices that consolidate software
    **development** (**Dev**) and **operations** (**Ops**) to speed up the delivery
    of ML models and prediction-as-a-service apps. The primary objective of MLOps
    is to improve the quality and speed of model development, deployment, and management
    while mitigating the perils associated with deployment, making it a salient tool
    to gain a competitive edge in the market.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps，或称为机器学习（ML）的DevOps，是一系列最佳实践，旨在将软件**开发**（**Dev**）和**运维**（**Ops**）整合，以加快机器学习模型和预测即服务应用程序的交付。MLOps的主要目标是提高模型开发、部署和管理的质量与速度，同时减少与部署相关的风险，使其成为在市场中获得竞争优势的重要工具。
- en: Close collaboration between data scientists and engineers is mandatory when
    it comes to deploying ML models to production. MLOps thus helps automate an ML
    workflow and caters to end-to-end tracing, from data preparation to model training
    and from prediction to serving. It also makes ML models reproducible and auditable,
    which is essential for compliance with regulatory requirements such as the GDPR.
    MLOps can be difficult to implement, but there are plenty of tools and frameworks
    that can help, such as Jenkins for **Continuous Integration** (**CI**), Spinnaker
    for **Continuous Delivery** (**CD**), and Kubeflow for ML model management. Many
    open source tools that can be leveraged to create an MLOps pipeline are also up
    for grabs. Beyond choosing the right tools, culture and collaboration are equally
    important for implementing MLOps. A successful, cross-functional MLOps team is
    required. A clear comprehension of the business goals and how ML can assist in
    accomplishing them should also be considered. Ultimately, MLOps is about moving
    faster while maintaining high quality and compliance. When done correctly, it
    can assist businesses in acquiring the upper hand by speeding up ML models and
    application delivery.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和工程师之间的紧密协作对于将机器学习模型部署到生产环境中是必不可少的。因此，MLOps有助于自动化机器学习工作流，并支持端到端的追踪，从数据准备到模型训练，从预测到服务交付。它还使机器学习模型可复现并可审计，这对于遵守诸如GDPR等监管要求至关重要。虽然MLOps的实施可能比较困难，但有许多工具和框架可以提供帮助，例如**持续集成**（**CI**）的Jenkins，**持续交付**（**CD**）的Spinnaker，以及用于机器学习模型管理的Kubeflow。许多开源工具也可以用来创建MLOps管道。除了选择合适的工具外，文化和协作在实施MLOps时同样重要。一个成功的跨职能MLOps团队是必不可少的。还需要清晰地理解业务目标以及机器学习如何帮助实现这些目标。最终，MLOps的目标是加速进程，同时保持高质量和合规性。如果做得正确，它可以帮助企业通过加速机器学习模型和应用交付获得优势。
- en: 'Let’s consider the different ML model life cycle steps (refer to *Figure 6**.8*).
    Each of these steps can be completed manually or via an automatic pipeline. The
    level of automation of these steps determines the time gap between training new
    models and their deployment and, thus, can help solve the problems discussed in
    the previous section. The automated ML pipeline should be able to do the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看不同的机器学习模型生命周期步骤（参考*图6.8*）。这些步骤可以通过手动方式或通过自动化管道完成。这些步骤的自动化水平决定了从训练新模型到模型部署之间的时间差，因此可以帮助解决前一部分讨论的问题。自动化机器学习管道应该能够完成以下任务：
- en: '**Allow different teams involved in product development to work independently**:
    Ideally, many teams are involved in an AI/ML workflow, moving from data collection,
    data ingestion, and model development to model deployment. As discussed in the
    *The need for MLOps* section, any change one of the teams makes affects everything
    else (CACE). An ideal ML pipeline automation should allow teams to work independently
    on various components without any interference from others.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许不同的团队独立工作**：理想情况下，许多团队会参与到AI/ML工作流中，从数据收集、数据摄取、模型开发到模型部署。正如*《MLOps的需求》*部分所讨论的那样，任何一个团队的改变都会影响到其他团队（CACE）。理想的ML管道自动化应该允许团队在各自的组件上独立工作，而不受其他团队干扰。'
- en: '**Actively monitor the model in production**: Building the model is not the
    real challenge. The real challenge resides in maintaining the model’s accuracy
    in production. This is possible if the model in production is actively monitored,
    with logs maintained and triggers generated if the model’s performance falls below
    a certain threshold. This allows you to detect any degradation in performance.
    This can be done by carrying out online model validation.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在生产环境中主动监控模型**：构建模型并不是最大的挑战，真正的挑战在于保持模型在生产环境中的准确性。只有在生产中的模型被积极监控、日志被维护且在模型性能低于某个阈值时触发警报，才能实现这一点。这使得你能够检测到性能的任何退化。可以通过进行在线模型验证来实现这一目标。'
- en: '**Accommodating data drift**: The model should evolve with new data patterns
    that emerge when new data comes in. This can be accomplished by adding an automated
    data validation step to the production pipeline. Any skew in data schema (missing
    features or unexpected values for features) should trigger the investigation of
    the data science team. Any substantial change in the data’s statistical properties
    should trigger retraining the model.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应数据漂移**：模型应该随新数据模式的出现而进化，这些新模式在新数据进入时出现。这可以通过在生产管道中增加一个自动化的数据验证步骤来实现。任何数据模式的偏差（缺失特征或特征值异常）应触发数据科学团队进行调查。数据统计特性发生重大变化时，应触发模型的再训练。'
- en: '**Continuous Training (CT)**: In the field of AI/ML, new model architectures
    appear every week, and you may be keen to experiment with the latest model or
    tweak your hyperparameters. The automated pipeline should allow for CT. It also
    becomes necessary when a production model falls below its performance threshold,
    or a substantial data drift is noticed.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续训练（CT）**：在AI/ML领域，每周都会出现新的模型架构，你可能急于试验最新的模型或调整超参数。自动化管道应支持持续训练。当生产模型性能低于其阈值，或检测到数据发生显著漂移时，持续训练变得必要。'
- en: '**Reproducibility**: Additionally, reproducibility is a major issue in AI,
    to such an extent that NeurIPS, the premier AI conference, has established a reproducibility
    chair. The aim for researchers is to submit a reproducibility checklist, empowering
    others to reproduce the results. Modularized components allow teams to work independently
    and make changes without impacting other teams. This allows teams to narrow down
    issues to a given component and, thus, helps with reproducibility.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重复性**：此外，可重复性是AI领域的一个重要问题，甚至到了NeurIPS这样顶级AI会议设立可重复性主席的地步。研究人员的目标是提交一个可重复性检查表，帮助其他人复现结果。模块化的组件允许团队独立工作并进行更改而不影响其他团队，这有助于团队将问题缩小到特定组件，从而帮助实现可重复性。'
- en: '**CI/CD**: For an expeditious and dependable update at the production level,
    there should be a **robust CI/CD system**. Delivering AI/ML solutions rapidly,
    reliably, and securely can improve your organization’s performance.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CI/CD**：为了在生产级别上实现快速且可靠的更新，应该建立一个**强大的CI/CD系统**。快速、可靠且安全地交付AI/ML解决方案可以提升组织的整体表现。'
- en: '**Testing**: Finally, you may want to perform A/B testing before applying a
    model to live traffic; this can be accomplished by configuring the new model to
    serve 10–20% of live traffic. If the new model performs better, he engineer/developer
    can serve all the traffic; otherwise, roll back to the old model.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：最后，你可能希望在将模型应用于实际流量之前进行A/B测试；这可以通过将新模型配置为处理10%到20%的实际流量来实现。如果新模型表现更好，工程师/开发人员可以将所有流量转交给新模型；否则，回滚到旧模型。'
- en: 'In essence, we need MLOps as an integrated engineering solution that unifies
    ML system development and ML system operation. This will allow data scientists
    to explore various model architectures, experiment with feature engineering techniques
    and hyperparameters, and push changes automatically to deployment. *Figure 6**.10*
    shows the different stages of the ML CI/CD automation pipeline:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们需要 MLOps 作为一个集成的工程解决方案，统一 ML 系统的开发和操作。这将允许数据科学家探索各种模型架构，尝试特征工程技术和超参数，并自动将更改推送到部署环境中。*图
    6.10* 显示了 ML CI/CD 自动化流水线的不同阶段：
- en: '![Figure 6.10 – The stages of the automated ML pipeline with CI/CD](img/Figure_6.10_B18681.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 带有 CI/CD 的自动化 ML 流水线的各个阶段](img/Figure_6.10_B18681.jpg)'
- en: Figure 6.10 – The stages of the automated ML pipeline with CI/CD
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 带有 CI/CD 的自动化 ML 流水线的各个阶段
- en: 'The complete automation pipeline comprises six stages:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的自动化流水线包含六个阶段：
- en: '**Development/experimentation**: At this stage, the data scientist iteratively
    tries various ML algorithms and architectures. Once satisfied, they push the model’s
    source code to the source code repository.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开发/实验**：在此阶段，数据科学家反复尝试各种 ML 算法和架构。一旦满意，他们会将模型的源代码推送到源代码仓库。'
- en: '**CI of the pipeline**: This stage involves building the source code and identifying
    and outputting the packages, executables, and artifacts that need to be deployed
    later.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**流水线的 CI**：此阶段涉及构建源代码，并识别和输出需要稍后部署的包、可执行文件和工件。'
- en: '**CD of the pipeline**: The artifacts produced in *stage 2* are deployed to
    the target environment.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**流水线的 CD**：在*阶段 2* 中生成的工件被部署到目标环境。'
- en: '**CT**: Depending upon the triggers set, a trained model is pushed to the model
    registry at this stage.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**CT**：根据设置的触发器，训练后的模型在此阶段被推送到模型注册表。'
- en: '**CD of the model**: Here, a model prediction service is deployed.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型的 CD**：在此阶段，部署一个模型预测服务。'
- en: '**Monitoring**: At this stage, the model performance statistics are collected
    and used to set triggers to execute the pipeline or a new experiment cycle.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控**：在此阶段，收集模型性能统计数据，并用来设置触发器以执行流水线或新的实验周期。'
- en: In the upcoming sections, we will cover some **GCP** (short for **Google Cloud
    Platform**) tools you can use to implement MLOps. We will talk about Cloud Run,
    TensorFlow Extended, and Kubeflow, the latter of which is the focus of the chapter
    (along with Vertex AI).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一些可以用于实施 MLOps 的 **GCP**（即 **Google Cloud Platform**）工具。我们将讨论
    Cloud Run、TensorFlow 扩展和 Kubeflow，后者是本章的重点（以及 Vertex AI）。
- en: TFX – a scalable end-to-end platform for AI/ML workflows
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TFX – 一个可扩展的端到端 AI/ML 工作流平台
- en: '**TensorFlow Extended** (**TFX**) is a scalable end-to-end platform for creating
    and deploying TensorFlow AI/ML workflows. TFX comprises libraries for data validation,
    data pre-processing, feature engineering, AI/ML model creation and training, model
    performance evaluation, and finally, providing models as REST and gRPC APIs. You
    can measure the worth of TFX by the fact that it powers several Google products,
    such as Chrome, Google Search, and Gmail. Google, Airbnb, PayPal, and Twitter
    all use TFX. As a platform, TFX employs a number of libraries for creating an
    end-to-end ML process.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 扩展** (**TFX**) 是一个可扩展的端到端平台，用于创建和部署 TensorFlow AI/ML 工作流。TFX 包括用于数据验证、数据预处理、特征工程、AI/ML
    模型创建与训练、模型性能评估，最后将模型提供为 REST 和 gRPC API 的库。你可以通过它为多个 Google 产品提供支持来衡量 TFX 的价值，例如
    Chrome、Google 搜索和 Gmail。Google、Airbnb、PayPal 和 Twitter 都在使用 TFX。作为一个平台，TFX 使用多个库来创建端到端的
    ML 过程。'
- en: 'Let’s see these libraries and what they can do:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些库以及它们能做什么：
- en: '**TensorFlow Data Validation** (**TFDV**): This library includes modules for
    exploring and validating your data. It lets you see the data used to train and/or
    test a model. The statistical summary provided by it can be utilized to discover
    any anomalies in the data. It includes an automatic schema creation tool describing
    the expected data range. It can also be used to detect data drift when comparing
    various experiments and runs.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 数据验证** (**TFDV**)：该库包括用于探索和验证数据的模块。它可以让你查看用于训练和/或测试模型的数据。它提供的统计摘要可用于发现数据中的任何异常。它包括一个自动化的模式创建工具，用于描述期望的数据范围。它还可以在比较不同实验和运行时检测数据漂移。'
- en: '**TensorFlow Transform** (**TFT**): You can pre-process your data at scale
    using TFT. The TFT library’s functions can be used to evaluate data, transform
    data, and conduct advanced feature engineering activities. The use of TFT has
    the advantage of modularizing the pre-processing procedure. A combination of TensorFlow
    and Apache Beam allows you to process the full dataset – such as getting the maximum
    and minimum values or all the available categories – and convert the data batch
    into tensors. It takes advantage of Google Dataflow, a cloud service.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 转换** (**TFT**)：你可以使用 TFT 对数据进行大规模预处理。TFT 库的函数可以用来评估数据、转换数据并进行高级特征工程活动。使用
    TFT 的优势在于它可以模块化预处理过程。TensorFlow 和 Apache Beam 的结合使你能够处理整个数据集——例如，获取最大值和最小值或所有可用的类别——并将数据批次转换为张量。它还利用了
    Google Dataflow，这是一项云服务。'
- en: '**TensorFlow Estimator and Keras**: This is the standard TensorFlow framework
    you can use to design and train your models. It also gives you access to a large
    number of pre-trained models.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Estimator 和 Keras**：这是你可以用来设计和训练模型的标准 TensorFlow 框架。它还为你提供了大量的预训练模型。'
- en: '**TensorFlow Model Analysis** (**TFMA**): This enables you to evaluate your
    trained model, in a distributed manner, on massive volumes of data, using the
    same model evaluation metrics you established while training. It allows you to
    analyze and comprehend trained models.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 模型分析** (**TFMA**)：这使你能够以分布式方式评估训练好的模型，处理海量数据，并使用训练时设定的相同模型评估指标。它可以让你分析并理解训练好的模型。'
- en: '**TensorFlow Serving** (**TFServing**): Finally, after you’re happy with your
    trained model, you can deliver it via REST and gRPC APIs for online production.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 服务** (**TFServing**)：最后，当你对训练好的模型满意时，你可以通过 REST 和 gRPC API 将其交付到在线生产环境中。'
- en: 'The figure here shows how the different libraries are integrated to form a
    TFX-based AI/ML pipeline:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的图表展示了不同的库如何集成，形成基于 TFX 的 AI/ML 流水线：
- en: '![Figure 6.11 – A TFX-based AI/ML pipeline (image source: Figure 4: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)](img/Figure_6.11_B18681.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 基于 TFX 的 AI/ML 流水线（图片来源：图 4: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)](img/Figure_6.11_B18681.jpg)'
- en: 'Figure 6.11 – A TFX-based AI/ML pipeline (image source: Figure 4: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.11 – 基于 TFX 的 AI/ML 流水线（图片来源：图 4: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning）'
- en: Each of these stages can be performed manually; however, as mentioned in the
    preceding section on MLOps, it is preferable for these procedures to be performed
    automatically. To accomplish this, we require an orchestration tool that connects
    the various blocks (components) of the ML workflow – this is where Kubeflow comes
    into play, which will be the topic of the next section.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阶段中的每一个都可以手动执行；然而，如前一节关于 MLOps 所提到的那样，最好让这些过程自动化执行。为了实现这一点，我们需要一个协调工具，将机器学习工作流中的各个模块（组件）连接起来——这正是
    Kubeflow 发挥作用的地方，接下来的章节将讨论这一主题。
- en: Understanding Kubeflow
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubeflow
- en: You can manage the full AI/ML life cycle with Kubeflow. It is a native Kubernetes
    **Operations Support System** (**OSS**) platform for developing, deploying, and
    managing scalable, end-to-end ML workloads in hybrid and multi-cloud settings.
    Kubeflow Pipelines, a Kubeflow service, aids in the automation of a complete AI/ML
    life cycle, allowing you to compose, orchestrate, and automate your AI/ML workloads.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Kubeflow 管理整个 AI/ML 生命周期。它是一个原生 Kubernetes **操作支持系统** (**OSS**) 平台，用于在混合云和多云环境中开发、部署和管理可扩展的端到端机器学习工作负载。Kubeflow
    Pipelines 是 Kubeflow 的一项服务，帮助自动化整个 AI/ML 生命周期，使你能够编排、组织和自动化你的 AI/ML 工作负载。
- en: 'It is an open source project, and the following diagram of the commits shows
    that it is an active and growing project. One of Kubeflow’s key goals is to make
    it simple for anybody to design, implement, and manage portable, scalable ML.
    At the time of writing, the Kubeflow GitHub project had 121,000 stars and over
    2,000 forks ([https://github.com/kubeflow/kubeflow/graphs/contributors](https://github.com/kubeflow/kubeflow/graphs/contributors)):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个开源项目，以下提交图表表明它是一个活跃且不断发展的项目。Kubeflow 的一个关键目标是简化任何人设计、实施和管理可移植、可扩展的机器学习。撰写本文时，Kubeflow
    的 GitHub 项目已有 121,000 个星标和超过 2,000 个分叉 ([https://github.com/kubeflow/kubeflow/graphs/contributors](https://github.com/kubeflow/kubeflow/graphs/contributors))：
- en: '![Figure 6.12 – Contributions to the Kubeflow GitHub repo](img/Figure_6.12_B18681.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 对Kubeflow GitHub仓库的贡献](img/Figure_6.12_B18681.jpg)'
- en: Figure 6.12 – Contributions to the Kubeflow GitHub repo
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 对Kubeflow GitHub仓库的贡献
- en: 'The best thing is that you can use the Kubeflow API to design your AI/ML workflow
    even if you don’t know much about Kubernetes. Kubeflow can be used on your local
    PC and any cloud (such as GCP or Azure AWS) with a single node or cluster specified;
    it is designed to work reliably across different settings. Google released Kubeflow
    1.2 in November 2022, allowing organizations to operate their ML process across
    environments. Kubeflow is based on the following three fundamental principles:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最棒的是，即使你对Kubernetes了解不多，你也可以使用Kubeflow API来设计你的AI/ML工作流。Kubeflow可以在本地PC和任何云平台（如GCP、Azure、AWS）上使用，支持单节点或集群指定；它旨在在不同环境下可靠地运行。Google在2022年11月发布了Kubeflow
    1.2，允许组织在不同环境中运行他们的ML流程。Kubeflow基于以下三个基本原则：
- en: '**Composability**: Kubeflow expands Kubernetes’ capacity to perform separate
    and adjustable tasks by utilizing ML-specific frameworks (such as TensorFlow,
    PyTorch, and others) and libraries (scikit-learn and pandas). This gives you distinct
    libraries for different tasks in an AI/ML workflow. Different TensorFlow versions,
    for example, may be necessary for data preparation and testing. Each job in an
    AI/ML process may, therefore, be independently containerized and worked upon.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可组合性**：Kubeflow通过使用特定于ML的框架（如TensorFlow、PyTorch等）和库（如scikit-learn和pandas），扩展了Kubernetes执行独立且可调整任务的能力。这为AI/ML工作流中的不同任务提供了不同的库。例如，数据准备和测试可能需要不同版本的TensorFlow。因此，AI/ML流程中的每个工作都可以独立容器化并处理。'
- en: '**Portability**: You can execute all of your AI/ML workflow components from
    anywhere you want – in the cloud, on-premises, or on your laptop while on vacation
    – as long as they are all running on Kubeflow.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：你可以从任何你想要的地方执行所有AI/ML工作流组件——无论是在云端、内部服务器，还是在度假时用笔记本电脑——只要它们都在Kubeflow上运行。'
- en: '**Scalability**: You can use more resources when you wish to and then release
    them when they are no longer required. Kubeflow increases Kubernetes’ ability
    to maximize resource availability and scale resources with minimal manual effort.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：当你需要更多资源时，可以使用它们，且在不再需要时释放它们。Kubeflow提升了Kubernetes在最大化资源可用性和最小化手动操作的情况下扩展资源的能力。'
- en: 'Some advantages of using Kubeflow are as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubeflow的一些优势如下：
- en: It is standardized on a common infrastructure.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于一个共同的基础设施进行标准化。
- en: It uses open source, cloud-native ecosystems to create, orchestrate, deploy,
    and run scalable and portable AI/ML workloads across an AI/ML life cycle.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用开源、云原生生态系统来创建、编排、部署和运行可扩展、可移植的AI/ML工作负载，贯穿AI/ML生命周期。
- en: It runs AI/ML workflows in hybrid and multi-cloud environments.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在混合云和多云环境中运行AI/ML工作流。
- en: In addition, when running on **GKE** (short for **Google Kubernetes Engine**),
    you can take advantage of GKE’s enterprise-grade security, logging, autoscaling,
    and identifying features.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，在**GKE**（即**Google Kubernetes Engine**）上运行时，你可以利用GKE的企业级安全性、日志记录、自动扩展和身份识别功能。
- en: Kubeflow populates clusters with **Custom Resource Definitions** (**CRDs**).
    It makes use of containers and Kubernetes. Therefore, it can be used everywhere
    Kubernetes is already in use.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 使用**自定义资源定义**（**CRDs**）来填充集群。它利用容器和Kubernetes，因此可以在任何已经使用Kubernetes的地方使用。
- en: 'The Kubeflow applications and components listed here can be used to organize
    your AI/ML workflow on top of Kubernetes:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下面列出的Kubeflow应用和组件可以用来在Kubernetes上组织你的AI/ML工作流：
- en: '**Jupyter notebooks**: Jupyter notebooks are the de facto tool for quick data
    analysis for AI/ML practitioners. The majority of data science projects begin
    with a Jupyter notebook. They serve as the foundation for the contemporary, cloud-native
    ML pipeline. Kubeflow notebooks allow you to execute your experiments locally,
    or you can take the data, train the model, and serve it all within a notebook.
    Notebooks work nicely with the rest of the architecture to provide access to other
    services in the Kubeflow cluster through the cluster IP addresses. They are also
    compatible with access control and authentication. Kubeflow allows you to set
    up several notebook servers, each of which can run multiple notebooks. Depending
    on a server’s project or team, each notebook server belongs to a specific namespace.
    Kubeflow supports many users through namespaces, making cooperating and controlling
    access easier. Using a notebook on Kubeflow allows you to scale resources dynamically.
    The best part is that it includes all of the plugins/dependencies you’ll need
    to train a model in Jupyter, including TensorBoard visualizations and customizable
    compute resources. Kubeflow Notebooks offers the same experience as Jupyter notebooks
    locally but with the added benefits of scalability, access control, collaboration,
    and direct job submission to the Kubernetes cluster.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter notebooks**: Jupyter notebooks 是 AI/ML 从业者进行快速数据分析的事实标准工具。绝大多数数据科学项目都是从
    Jupyter notebook 开始的。它们构成了现代云原生机器学习管道的基础。Kubeflow notebooks 允许您在本地执行实验，或者您可以在
    notebook 中处理数据、训练模型并进行服务。Notebooks 与架构中的其他部分很好地配合，通过集群的 IP 地址提供对 Kubeflow 集群中其他服务的访问。它们也兼容访问控制和身份验证。Kubeflow
    允许您设置多个 notebook 服务器，每个服务器可以运行多个 notebooks。根据服务器的项目或团队，每个 notebook 服务器属于特定的命名空间。Kubeflow
    通过命名空间支持多个用户，使得协作和访问控制变得更加容易。在 Kubeflow 上使用 notebook 允许您动态扩展资源。最棒的是，它包括您在 Jupyter
    中训练模型所需的所有插件/依赖项，包括 TensorBoard 可视化和可定制的计算资源。Kubeflow Notebooks 提供与本地 Jupyter
    notebooks 相同的体验，但增加了可扩展性、访问控制、协作和直接提交作业到 Kubernetes 集群的好处。'
- en: '**Kubeflow User Interface (UI)**: A UI for running pipelines, creating and
    starting experiments, exploring your pipeline’s graph, configuration, and output,
    and even scheduling runs.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeflow User Interface (UI)**: 用于运行管道、创建和启动实验、探索管道的图形、配置和输出，甚至调度运行的用户界面。'
- en: '**Katib**: The tuning of hyperparameters is an important step in the AI/ML
    workflow. Finding the correct hyperparameter space can be time-consuming. Katib
    facilitates hyperparameter tuning, early halting, and NAS. It aids in determining
    the best configuration for production based on the metrics of choice.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Katib**: 超参数调优是 AI/ML 工作流中的一个重要步骤。找到正确的超参数空间可能是一个耗时的过程。Katib 促进了超参数调优、早期停止和神经架构搜索（NAS）。它有助于根据所选指标确定最佳的生产配置。'
- en: '**Kubeflow Pipelines**: Kubeflow Pipelines enables you to create a series of
    stages that include everything from data collection to providing a trained model.
    Because they are based on containers, each phase is portable and scalable. Kubeflow
    Pipelines can be used to orchestrate end-to-end ML workflows. Kubeflow Pipelines
    takes advantage of the fact that ML operations can be broken down into a series
    of standard stages that can be placed in the form of a directed graph. Each Kubeflow
    pipeline job is a self-contained piece of code packaged as a Docker image, complete
    with inputs (arguments) and outputs. This job containerization enables portability,
    since the pipelines are self-contained programs that can be run anywhere. Furthermore,
    the same code can be utilized in another AI/ML pipeline – tasks can be reused.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeflow Pipelines**: Kubeflow Pipelines 使您能够创建一系列阶段，涵盖从数据收集到提供训练好的模型的所有内容。由于它们基于容器，每个阶段都是可移植和可扩展的。Kubeflow
    Pipelines 可用于协调端到端的机器学习工作流。Kubeflow Pipelines 利用机器学习操作可以分解成一系列标准阶段这一事实，这些阶段可以以有向图的形式进行排列。每个
    Kubeflow 管道作业都是一个自包含的代码单元，打包为 Docker 镜像，包含输入（参数）和输出。这种作业容器化使得可移植性成为可能，因为管道是自包含的程序，可以在任何地方运行。此外，相同的代码可以在另一个
    AI/ML 管道中重复使用——任务可以被复用。'
- en: '**Metadata**: It is helpful to keep track of and manage the metadata that is
    produced in AI/ML workflows. Tracking metadata can be utilized to perform real-time
    model evaluation. It can aid in the detection of data drift or training-serving
    skews. It can also be used for auditing and compliance, allowing you to see which
    models are in production and how they perform. The metadata component is installed
    with Kubeflow by default. Many Kubeflow components write to the metadata server.
    In addition, you can write to the metadata server using your code. The Kubeflow
    UI can be used to view the metadata – through the artifact store.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Metadata（元数据）**：在 AI/ML 工作流中，跟踪和管理元数据是非常有用的。元数据的跟踪可以用于执行实时模型评估，有助于检测数据漂移或训练与服务之间的偏差。它还可以用于审计和合规，帮助您查看哪些模型正在生产中并监控其表现。元数据组件默认随
    Kubeflow 安装。许多 Kubeflow 组件会写入元数据服务器。此外，您还可以通过您的代码将数据写入元数据服务器。可以通过 Kubeflow UI
    来查看元数据——通过工件存储。'
- en: '**KFServing**: This allows you to serve AI/ML models on arbitrary frameworks.
    It includes features such as auto-scaling, networking, and canary rollouts. It
    has an easy-to-use interface to serve models to production. Using a YAML file,
    you can provision the resources to serve and compute a model and its prediction.
    Canary rollouts enable you to test and update models without impacting the user
    experience.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KFServing**：这允许您在任意框架上提供 AI/ML 模型服务。它包括自动扩展、网络和金丝雀发布等功能，且具有一个易于使用的界面来将模型提供到生产环境。通过使用
    YAML 文件，您可以配置资源来服务和计算模型及其预测。金丝雀发布使您能够在不影响用户体验的情况下测试和更新模型。'
- en: '**Fairing**: This is a Python package that allows you to build, train, and
    deploy your AI/ML models in hybrid cloud environments.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fairing**：这是一个 Python 包，允许您在混合云环境中构建、训练和部署您的 AI/ML 模型。'
- en: To summarize, *Kubeflow provides a curated set of compatible tools and artifacts
    that lie at the heart of running production-enabled AI/ML apps*. This helps organizations
    to standardize a uniform modeling infrastructure across an ML life cycle. Another
    important step in an AI/ML workflow is hyperparameter tuning, so next, we will
    explore Katib, which Kubernetes cluster and provides options for both hyperparameter
    tuning and NAS.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，*Kubeflow 提供了一套兼容的工具和工件，它们是运行生产级 AI/ML 应用的核心*。这帮助组织在整个机器学习生命周期中标准化统一的建模基础设施。AI/ML
    工作流中的另一个重要步骤是超参数调优，因此接下来我们将探讨 Katib，它是一个 Kubernetes 集群，提供超参数调优和 NAS 的选项。
- en: Katib for hyperparameter tuning
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Katib 用于超参数调优
- en: 'Katib is a scalable, Kubernetes-native AutoML platform that facilitates both
    hyperparameter tuning and NAS. *Figure 6**.13* shows the design of Katib. To learn
    more about how it works, readers should refer to *Katib: A Distributed General
    AutoML Platform* *on Kubernetes:*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'Katib 是一个可扩展的、原生 Kubernetes 的 AutoML 平台，支持超参数调优和 NAS。*图 6.13* 显示了 Katib 的设计。要了解其工作原理，读者应参考
    *Katib: A Distributed General AutoML Platform* *on Kubernetes*：'
- en: "![Figure 6.13 – The design of Katib as a general AutoML system (Figure 2\uFEFF\
    \ from the paper: https://www.usenix.org/system/files/opml19papers-zhou.pdf)](img/Figure_6.13_B18681.jpg)"
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: "![图 6.13 – Katib 作为通用 AutoML 系统的设计（论文中的图 2\uFEFF： https://www.usenix.org/system/files/opml19papers-zhou.pdf）](img/Figure_6.13_B18681.jpg)"
- en: 'Figure 6.13 – The design of Katib as a general AutoML system (Figure 2 from
    the paper: https://www.usenix.org/system/files/opml19papers-zhou.pdf)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – Katib 作为通用 AutoML 系统的设计（论文中的图 2：https://www.usenix.org/system/files/opml19papers-zhou.pdf）
- en: 'Katib supports hyperparameter adjustment through the command line via a YAML
    file specification, as well as the Jupyter Notebook and the Python SDK. It also
    has a graphical UI for specifying tuning settings and visualizing the results,
    as shown here:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Katib 通过 YAML 文件规范、Jupyter Notebook 和 Python SDK 支持命令行超参数调优。它还提供图形界面来指定调优设置并可视化结果，如下所示：
- en: '![Figure 6.14 – The graphical interface of Katib](img/Figure_6.14_B18681.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – Katib 的图形界面](img/Figure_6.14_B18681.jpg)'
- en: Figure 6.14 – The graphical interface of Katib
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – Katib 的图形界面
- en: Katib allows you to choose a measure and whether to reduce or increase it. You
    can choose the hyperparameters you want to tweak and see the results of the entire
    experiment and individual runs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Katib 允许您选择度量标准，并决定是减少还是增加它。您可以选择要调整的超参数，并查看整个实验和各个运行的结果。
- en: '*Figure 6**.15* depicts the outcome of a Katib run, with validation accuracy
    as the metric and the learning rate, layer count, and optimizer as the hyperparameters
    to be tuned:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.15* 展示了 Katib 运行的结果，以验证准确度为指标，学习率、层数和优化器作为需要调优的超参数：'
- en: '![Figure 6.15 – The results of hyperparameter tuning generated by Katib](img/Figure_6.15_B18681.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – Katib生成的超参数调整结果](img/Figure_6.15_B18681.jpg)'
- en: Figure 6.15 – The results of hyperparameter tuning generated by Katib
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – Katib生成的超参数调整结果
- en: Another interesting platform for both MLOps and AutoML is Vertex AI, offered
    by Google, which we will cover next.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的平台，既支持MLOps又支持AutoML，是Google提供的Vertex AI，我们接下来将介绍它。
- en: Vertex AI
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vertex AI
- en: Kubeflow allows you to orchestrate the MLOPs workflow. However, you still need
    to deal with the Kubernetes cluster. A far better solution would be to not need
    to worry about the management of clusters at all – enter Vertex AI pipelines.
    You can use Kubeflow Pipelines or TFX pipelines in Vertex AI. Vertex AI provides
    tools for every step of the ML workflow, from managing datasets to different ways
    of training the model, evaluating and deploying it, and making predictions. In
    short, Vertex AI provides a soup-to-nuts solution for your AI needs. Whether you
    are a beginner with no code experience but have a great idea that could utilize
    AI, or you are a seasoned AI engineer, Vertex AI brings something to the table
    for you. Vertex AI’s AutoML feature makes it easy for beginners to get started
    with ML; all you have to do is load your data, use the data exploration tools
    that Vertex AI provides, and start training a model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow允许您编排MLOps工作流程。但是，您仍然需要处理Kubernetes集群。一个更好的解决方案是根本不需要担心集群的管理——这就是Vertex
    AI管道的用武之地。您可以在Vertex AI中使用Kubeflow管道或TFX管道。Vertex AI为ML工作流的每个步骤提供工具，从管理数据集到不同的模型训练方式，评估和部署，以及进行预测。简而言之，Vertex
    AI为您的AI需求提供了一站式解决方案。无论您是一个没有编码经验但有一个可以利用AI的好点子的初学者，还是一个经验丰富的AI工程师，Vertex AI都能为您带来一些价值。Vertex
    AI的AutoML功能使初学者可以轻松开始ML；您只需加载数据，使用Vertex AI提供的数据探索工具，并开始训练模型。
- en: 'Expert AI engineers can create their own training loops, cloud-train their
    models, and use endpoints to put them into production. Additionally, Vertex AI
    allows you to do local model training, deployment, and monitoring. Vertex AI is
    an all-in-one AI/ML platform that features a streamlined UI, as shown in *Figure
    6**.16*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 专业的AI工程师可以创建自己的训练循环，通过云端训练他们的模型，并使用端点将其投入生产。此外，Vertex AI允许您进行本地模型训练、部署和监控。Vertex
    AI是一个全面的AI/ML平台，具有简化的用户界面，如*图6**.16*所示：
- en: '![Figure 6.16 – Vertex AI’s unified interface for the complete AI/ML workflow](img/Figure_6.16_B18681.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – Vertex AI完整AI/ML工作流的统一界面](img/Figure_6.16_B18681.jpg)'
- en: Figure 6.16 – Vertex AI’s unified interface for the complete AI/ML workflow
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – Vertex AI完整AI/ML工作流的统一界面
- en: 'The Vertex AI dashboard is depicted in *Figure 6**.17*; in the following sections,
    we will examine its many useful features:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.17*展示了Vertex AI仪表板；在接下来的章节中，我们将探讨其许多有用的功能：'
- en: '![Figure 6.17 – Vertex AI dashboard](img/Figure_6.17_B18681.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – Vertex AI仪表板](img/Figure_6.17_B18681.jpg)'
- en: Figure 6.17 – Vertex AI dashboard
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – Vertex AI仪表板
- en: Next, we will explore what type of data is supported by Vertex AI and how to
    handle it.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨Vertex AI支持的数据类型及其处理方法。
- en: Datasets
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'Images, videos, text, and tables are all supported by Vertex AI. You can see
    a breakdown of the AI/ML tasks that can be completed with Vertex AI-managed datasets
    in the following table:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图像、视频、文本和表格都受Vertex AI支持。您可以在下表中查看使用Vertex AI管理的数据集完成的AI/ML任务的详细信息：
- en: '| **Type** **of data** | **Tasks supported** |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型** **的类型** | **支持的任务** |'
- en: '| Image |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 图像 |'
- en: Image classification (single-label)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类（单标签）
- en: Image classification (multi-label)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类（多标签）
- en: Image object detection
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像目标检测
- en: Image segmentation
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Video |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 视频 |'
- en: Video action recognition
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频动作识别
- en: Video classification
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频分类
- en: Video object tracking
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频物体跟踪
- en: '|'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Text |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 文本 |'
- en: Text classification (single-label)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类（单标签）
- en: Text classification (multi-label)
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类（多标签）
- en: Text entity extraction
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本实体提取
- en: Text sentiment analysis
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本情感分析
- en: '|'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Tabular |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 表格 |'
- en: Regression
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Classification
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Forecasting
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.1 – The types of data and types of AI tasks supported on each in Vertex
    AI
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – Vertex AI支持的数据类型和AI任务类型
- en: For image, video, and text datasets, if you do not have labels, you can use
    the annotation service provided by Google. Files containing image URIs and labels
    can also be imported from your computer. Moreover, you can import data from Google
    Cloud Storage. It is important to remember that uploaded data will use Google
    Cloud Storage to store the files you upload from your computer. Only `.csv`) files
    are supported by Vertex AI for tabular data. You can upload them from your local
    machine, a cloud service, or an import from BigQuery.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像、视频和文本数据集，如果没有标签，您可以使用 Google 提供的标注服务。包含图像 URI 和标签的文件也可以从您的计算机导入。此外，您还可以从
    Google Cloud Storage 导入数据。需要注意的是，上传的数据将使用 Google Cloud Storage 来存储您从计算机上传的文件。Vertex
    AI 仅支持 `.csv` 文件用于表格数据。您可以从本地计算机、云服务或从 BigQuery 导入它们。
- en: Vertex AI allows you to browse and analyze data once it has been specified.
    If the data is not labeled, you can simply browse it in the browser and assign
    labels. In addition, Vertex AI provides the option of automating or manually performing
    the test-training validation split. *Figure 6**.18* depicts an analysis performed
    on the HR analytics data ([https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv))
    using the Vertex AI managed datasets service.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 允许您在指定数据后浏览和分析数据。如果数据没有标签，您可以直接在浏览器中浏览数据并分配标签。此外，Vertex AI 提供了自动化或手动执行测试-训练验证分割的选项。*图
    6.18* 展示了使用 Vertex AI 管理数据集服务对 HR 分析数据进行的分析（[https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv)）。
- en: '![Figure 6.18 – AutoML configurations for tabular data – choosing the training
    parameters and transformations](img/Figure_6.18_B18681.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – AutoML 配置用于表格数据 – 选择训练参数和变换](img/Figure_6.18_B18681.jpg)'
- en: Figure 6.18 – AutoML configurations for tabular data – choosing the training
    parameters and transformations
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – AutoML 配置用于表格数据 – 选择训练参数和变换
- en: Vertex AI also provides a **Feature Store** option, which can be used to analyze
    the features of your data. Using the same feature data distribution for training
    and serving can help reduce training-serving skews. Additionally, **Feature Store**
    can help identify model or data drift. In addition, Vertex AI offers a data annotation
    service for those who need it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 还提供了**特征库**选项，可用于分析数据的特征。使用相同的特征数据分布进行训练和服务可以帮助减少训练-服务偏差。此外，**特征库**还可以帮助识别模型或数据漂移。此外，Vertex
    AI 为需要此服务的用户提供数据标注服务。
- en: Training and experiments in Vertex AI
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI 中的训练和实验
- en: The training tasks you have completed and are currently working on are displayed
    in the **Training** tab of the Vertex AI dashboard. This can also be the starting
    point for an entirely new training sequence. Simply select **Create** and then
    **Create** again, and then follow the onscreen prompts to finish the process.
    You can specify the transformations applied to the tabular data and pick and choose
    which features to use for training if you choose the AutoML route. The objective
    function can also be customized. After making your selections, all you have to
    do is set aside how much time you can afford to train for (the bare minimum is
    an hour). If the model’s performance isn’t improving during training, it’s best
    to stop it early, as depicted in *Figure 6**.19*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您已完成和正在进行的训练任务将在 Vertex AI 仪表板的**训练**选项卡中显示。这也可以作为一个全新训练序列的起点。只需选择**创建**，然后再次选择**创建**，然后按照屏幕上的提示完成该过程。如果选择
    AutoML 路径，您可以指定应用于表格数据的变换，并选择用于训练的特征。目标函数也可以自定义。完成选择后，您只需设置您能够投入的训练时间（最短为一个小时）。如果模型在训练过程中没有取得进展，最好提前停止训练，如*图
    6.19*所示。
- en: 'The **Experiments** tab lets you track, visualize, and compare ML experiments
    and share them with others:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验**选项卡可以让您跟踪、可视化和比较 ML 实验，并与他人分享：'
- en: '![Figure 6.19 – AutoML configurations for tabular data – selecting a budget
    and early stopping](img/Figure_6.19_B18681.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – AutoML 配置用于表格数据 – 选择预算和提前停止](img/Figure_6.19_B18681.jpg)'
- en: Figure 6.19 – AutoML configurations for tabular data – selecting a budget and
    early stopping
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – AutoML 配置用于表格数据 – 选择预算和提前停止
- en: We have demonstrated Vertex AI AutoML using HR analytics data ([https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv))
    to foresee a data scientist’s intention to switch jobs. When training the model,
    we used everything we had available except the enrolled ID. There is a column
    in the data files that indicates whether the data scientist is actively seeking
    employment, and we use this as the target column.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已使用 HR 分析数据（[https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv)）展示了如何预测数据科学家是否打算换工作。在训练模型时，我们使用了除了注册
    ID 之外的所有可用数据。数据文件中有一列显示数据科学家是否积极寻找工作，我们使用此列作为目标列。
- en: Models and endpoints in Vertex AI
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 中的模型与端点
- en: The **Models** tab contains comprehensive information regarding trained models.
    These models are complete with a test dataset evaluation (when trained using managed
    datasets). Additionally, feature importance for all features can be accessed in
    the case of tabular data. The data can be viewed visually or read as text directly
    on the dashboard.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**标签包含关于训练模型的全面信息。这些模型已完整评估了测试数据集（当使用托管数据集训练时）。此外，对于表格数据，所有特征的特征重要性也可以访问。数据可以在仪表盘上以可视化或直接以文本的形式查看。'
- en: 'The training of models was allotted a 1-node-per-hour budget. The total duration
    of the training was about 1 hour and 35 minutes. The AutoML-trained model’s test-dataset
    evaluation is displayed in *Figure 6**.20*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的预算为每小时 1 个节点。训练总时长约为 1 小时 35 分钟。AutoML 训练的模型在*图 6.20*中展示了测试数据集评估结果：
- en: '![Figure 6.20 – The model evaluation for the test dataset](img/Figure_6.20_B18681.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – 测试数据集的模型评估](img/Figure_6.20_B18681.jpg)'
- en: Figure 6.20 – The model evaluation for the test dataset
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – 测试数据集的模型评估
- en: 'Now refer to the following figure, which shows the matrix and feature importance:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请参考下图，展示了矩阵和特征重要性：
- en: '![Figure 6.21 – The confusion matrix and feature importance](img/Figure_6.21_B18681.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – 混淆矩阵和特征重要性](img/Figure_6.21_B18681.jpg)'
- en: Figure 6.21 – The confusion matrix and feature importance
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 混淆矩阵和特征重要性
- en: 'The dashboard makes it easy to examine a model’s forecast. For testing purposes,
    the model must be deployed to an endpoint. Vertex AI also provides the option
    to save the model in a container (in the TensorFlow **SavedModel** format), which
    can then be used to launch the model in any other on-premises or cloud service.
    Let’s go ahead and select **Deploy to Endpoint** to send the model somewhere.
    The following choices are available for deployment:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表盘使得检查模型预测变得简单。为了测试，模型必须部署到端点。Vertex AI 还提供了将模型保存在容器中的选项（以 TensorFlow **SavedModel**
    格式），然后可以将模型用于任何其他本地或云服务。让我们继续选择**部署到端点**，将模型发送到某个地方。以下是可用的部署选项：
- en: Give a name to the endpoint.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为端点命名。
- en: Choose the traffic split – for a single model, it is 100%, but if you have more
    than one model, you can split the traffic.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择流量拆分 – 对于单个模型，流量为 100%，但如果有多个模型，可以拆分流量。
- en: Choose the minimum number of compute nodes.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最少数量的计算节点。
- en: Select the machine type.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器类型。
- en: Vertex AI provides a sampled Shapley explainability method for tabular data
    if you choose the model explainability option.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果选择了模型可解释性选项，Vertex AI 为表格数据提供了一个采样的 Shapley 可解释性方法。
- en: Select which aspects of the model you wish to keep an eye on (feature drift
    or training-serving skews) and adjust your alert settings accordingly.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择你希望关注的模型方面（特征漂移或训练与服务偏差），并相应调整警报设置。
- en: Deployment is quick and easy after the initial setup is complete. Let’s now
    put the prediction to the test (both individual and batch predictions are available).
    It is clear from *Figure 6**.22* that the data scientist is not actively seeking
    employment at a confidence level of approximately 0.67, given the inputs that
    were chosen. We can use either a REST API or a Python client to send a test request
    to the model. The Vertex AI endpoint provides the relevant code to accommodate
    both sample requests.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 初步设置完成后，部署过程快速且简单。现在让我们来测试一下预测（支持单次和批量预测）。从*图 6.22*可以清晰看出，数据科学家在选择的输入条件下，信心水平约为
    0.67，因此并未积极寻求就业。我们可以使用 REST API 或 Python 客户端向模型发送测试请求。Vertex AI 端点提供了相关代码，支持这两种示例请求。
- en: '![Figure 6.22 – The model prediction](img/Figure_6.22_B18681.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.22 – 模型预测](img/Figure_6.22_B18681.jpg)'
- en: Figure 6.22 – The model prediction
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22 – 模型预测
- en: All the models and endpoints deployed in the project are listed in the **Models**
    and **Endpoints** tabs on the dashboard respectively.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 项目中部署的所有模型和端点分别列在仪表板的 **Models** 和 **Endpoints** 标签页中。
- en: Vertex AI Workbench
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI Workbench
- en: Vertex AI Workbench is compatible with JupyterLab and the Jupyter Notebook.
    A user can choose between pre-made notebooks and managed notebooks. All the popular
    deep learning frameworks and modules are included in managed notebooks, and you
    can even add your own Jupyter kernels using Docker images. The user-managed notebooks
    provide a number of useful starting points. Users can configure their notebooks
    with a choice of **virtual central processing units** (**vCPUs**) and **graphics
    processing units** (**GPUs**). If you’re new to Vertex AI and want to get started
    right away, we recommend starting with the managed notebooks. However, if you’re
    looking for more administrative power, user-managed notebooks are the way to go.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI Workbench 与 JupyterLab 和 Jupyter Notebook 兼容。用户可以选择预制的笔记本或托管笔记本。在托管笔记本中，所有流行的深度学习框架和模块都已包含，您甚至可以使用
    Docker 镜像添加自己的 Jupyter 内核。用户管理的笔记本提供了许多有用的起点。用户可以根据需要配置笔记本，选择 **虚拟中央处理单元**（**vCPUs**）和
    **图形处理单元**（**GPUs**）。如果你是 Vertex AI 的新手，建议从托管笔记本开始。然而，如果你需要更多的管理权限，用户管理的笔记本是更好的选择。
- en: In order to enter your JupyterLab environment after a notebook has been made,
    click on the **OPEN** **JUPYTERLAB** link.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入您的 JupyterLab 环境，在笔记本创建完成后，点击 **OPEN** **JUPYTERLAB** 链接。
- en: "![Figure 6.23 – A list of Jupyter notebooks in \uFEFFVertex AI Workbench](img/Figure_6.23_B18681.jpg)"
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.23 – Vertex AI Workbench 中的 Jupyter 笔记本列表](img/Figure_6.23_B18681.jpg)'
- en: Figure 6.23 – A list of Jupyter notebooks in Vertex AI Workbench
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – Vertex AI Workbench 中的 Jupyter 笔记本列表
- en: Vertex AI Workbench can be used in conjunction with TFX or Kubeflow to perform
    data exploration, model construction, and training and execute code.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI Workbench 可以与 TFX 或 Kubeflow 配合使用，执行数据探索、模型构建、训练并运行代码。
- en: Vertex AI provides a unified interface for all the AI/ML process components.
    You can set up pipelines for model training and experimentation. The interface
    provides a simple method to tweak hyperparameters. Custom training can be selected,
    allowing a user to choose from containers and load their code straight away to
    train on their chosen machine. Vertex AI also features AutoML integration to accelerate
    the ML workflow. Using AutoML, you can obtain an efficient ML model for controlled
    datasets with minimal ML skills. Using feature attribution, Vertex AI also provides
    explainability for its models. When your model is complete, you can select endpoints
    for batch or single predictions and deploy your model to them. The most important
    aspect of deployment is the ability to deploy on edge devices and place your model
    where the data is.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 提供了统一的界面，涵盖所有 AI/ML 流程组件。您可以为模型训练和实验设置管道。该界面提供了一种简单的方法来调整超参数。可以选择自定义训练，允许用户从容器中选择并直接加载代码进行训练。Vertex
    AI 还集成了 AutoML，以加速 ML 工作流。通过 AutoML，您可以在受控数据集上获得高效的 ML 模型，并且所需的 ML 技能最少。使用特征归因，Vertex
    AI 还为其模型提供了解释性。当您的模型完成后，您可以选择端点进行批量或单次预测，并将模型部署到这些端点上。部署的最重要方面是能够在边缘设备上进行部署，并将模型放置在数据所在的位置。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided a comprehensive overview of AutoML and MLOps. It began
    by introducing the concept of AutoML and explained how it is used to automate
    the process of training and tuning a large selection of candidate models. The
    chapter also included a hands-on example of using H2O AutoML and discussed the
    AutoML features provided by Microsoft Azure and Amazon SageMaker Autopilot. Following
    that, the chapter introduced the concept of MLOps and the importance of incorporating
    it into AI/ML workflows.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了有关 AutoML 和 MLOps 的全面概述。首先介绍了 AutoML 的概念，并解释了如何使用它自动化训练和调整大量候选模型的过程。本章还包括了使用
    H2O AutoML 的实践示例，并讨论了 Microsoft Azure 和 Amazon SageMaker Autopilot 提供的 AutoML
    功能。接着，本章介绍了 MLOps 的概念以及将其纳入 AI/ML 工作流的重要性。
- en: The chapter covered the features of TFX briefly, which is a TensorFlow-based
    toolkit to build a full-fledged ML pipeline. It provides a set of libraries and
    pre-built components that can be used to build and deploy ML models easily.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了TFX的特点，TFX是一个基于TensorFlow的工具包，用于构建完整的机器学习管道。它提供了一系列库和预构建组件，可以轻松地用来构建和部署机器学习模型。
- en: Kubeflow, an open source project that aims to make running ML workloads on Kubernetes
    simple, portable, and scalable, was also covered. We also showed how to perform
    hyperparameter tuning using Katib, which is a Kubernetes-based system for hyperparameter
    tuning.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 还介绍了Kubeflow，一个旨在简化在Kubernetes上运行机器学习工作负载的开源项目，使其具有可移植性和可扩展性。我们还展示了如何使用Katib进行超参数调优，Katib是一个基于Kubernetes的超参数调优系统。
- en: Finally, Google’s Vertex AI was explored. This platform provides a simple and
    easy-to-use interface to build, deploy, and manage ML models. Using Vertex AI,
    we trained and deployed a model trained on HR analytics data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探索了谷歌的 Vertex AI。该平台提供了一个简单易用的接口，用于构建、部署和管理机器学习模型。通过使用 Vertex AI，我们训练并部署了一个基于人力资源分析数据训练的模型。
- en: Now that you are familiar with the ML pipeline and the various tools used to
    automate it, in the next chapter, we will move on to the concept of fairness in
    data collection.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了机器学习管道以及用于自动化的各种工具，在接下来的章节中，我们将讨论数据收集中的公平性概念。
- en: Further reading
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Large-Scale Evolution of Image Classifiers*. In *International Conference
    on Machine Learning*, pp. 2902–2911\. PMLR, 2017, Real, Esteban, Moore, Sherry,
    Selle, Andrew, Saxena, Saurabh, Suematsu, Yutaka Leon, Tan, Jie, Le, Quoc V.,
    and Kurakin, Alexey: [http://proceedings.mlr.press/v70/real17a/real17a.pdf](http://proceedings.mlr.press/v70/real17a/real17a.pdf).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大规模图像分类器的演化*。发表于*国际机器学习会议*，第2902-2911页，PMLR，2017年，Real, Esteban, Moore, Sherry,
    Selle, Andrew, Saxena, Saurabh, Suematsu, Yutaka Leon, Tan, Jie, Le, Quoc V.,
    和 Kurakin, Alexey：[http://proceedings.mlr.press/v70/real17a/real17a.pdf](http://proceedings.mlr.press/v70/real17a/real17a.pdf)。'
- en: '*Neural Architecture Search with Reinforcement Learning*. *arXiv preprint arXiv:1611.01578*
    (2016), Zoph, Barret, and Le, Quoc V: [https://arxiv.org/pdf/1611.01578.pdf](https://arxiv.org/pdf/1611.01578.pdf).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过强化学习进行神经架构搜索*。*arXiv预印本 arXiv:1611.01578*（2016），Zoph, Barret, 和 Le, Quoc
    V：[https://arxiv.org/pdf/1611.01578.pdf](https://arxiv.org/pdf/1611.01578.pdf)。'
- en: '*Neural Architecture Search: A Survey*. *The Journal of Machine Learning Research*,
    20, no. 1 (2019): 1997–2017, Elsken, Thomas, Metzen, Jan Hendrik, and Hutter,
    Frank: [https://www.jmlr.org/papers/volume20/18-598/18-598.pdf?ref=https://githubhelp.com](https://www.jmlr.org/papers/volume20/18-598/18-598.pdf?ref=https://githubhelp.com).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经架构搜索：综述*。*机器学习研究杂志*，20，第1期（2019）：1997-2017，Elsken, Thomas, Metzen, Jan Hendrik,
    和 Hutter, Frank：[https://www.jmlr.org/papers/volume20/18-598/18-598.pdf?ref=https://githubhelp.com](https://www.jmlr.org/papers/volume20/18-598/18-598.pdf?ref=https://githubhelp.com)。'
- en: '*H2O AutoML: Scalable Automatic Machine Learning*. In *Proceedings of the AutoML
    Workshop at ICML*, vol. 2020\. 2020, LeDell, Erin, and Poirier, Sebastien: [https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf](https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H2O AutoML：可扩展的自动化机器学习*。发表于*ICML自动化机器学习研讨会论文集*，2020年，LeDell, Erin, 和 Poirier,
    Sebastien：[https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf](https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf)。'
- en: '*Towards Automated Machine Learning: Evaluation and Comparison of AutoML Approaches
    and Tools,* Truong, Anh, Walters, Austin, Goodsitt, Jeremy, Hines, Keegan, Bruss,
    C. Bayan, and Farivar, Reza. In 2019 IEEE 31st international conference on tools
    with artificial intelligence (ICTAI), pp. 1471–1479\. IEEE, 2019: [https://arxiv.org/pdf/1908.05557&ved=2ahUKEwjS0Zes2ermAhUqTt8KHdCF
    AhkQFjAGegQIBxAS&usg=AOvVaw0b_JUomS-A1rtsy7v5ZA64.pdf](https://arxiv.org/pdf/1908.05557&ved=2ahUKEwjS0Zes2ermAhUqTt8KHdCFAhkQFjAGegQIBxAS&usg=AOvVaw0b_JUomS-A1rtsy7v5ZA64.pdf)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*走向自动化机器学习：AutoML 方法和工具的评估与比较*，Truong, Anh, Walters, Austin, Goodsitt, Jeremy,
    Hines, Keegan, Bruss, C. Bayan, 和 Farivar, Reza。发表于2019年IEEE第31届人工智能工具国际会议（ICTAI），第1471-1479页，IEEE，2019年：[https://arxiv.org/pdf/1908.05557&ved=2ahUKEwjS0Zes2ermAhUqTt8KHdCF
    AhkQFjAGegQIBxAS&usg=AOvVaw0b_JUomS-A1rtsy7v5ZA64.pdf](https://arxiv.org/pdf/1908.05557&ved=2ahUKEwjS0Zes2ermAhUqTt8KHdCF%20AhkQFjAGegQIBxAS&usg=AOvVaw0b_JUomS-A1rtsy7v5ZA64.pdf)'
- en: '*AutoML: A Survey of the State-of-the-Art. Knowledge-Based Systems*, 212 (2021):
    106622, He, Xin, Zhao, Kaiyong, and Chu, Xiaowen. : https://arxiv.org/pdf/1908.00709.pdf?arxiv.org.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AutoML：最前沿技术的调查。基于知识的系统*，212（2021）：106622，He, Xin, Zhao, Kaiyong, 和 Chu, Xiaowen。:
    https://arxiv.org/pdf/1908.00709.pdf?arxiv.org。'
- en: '*Hidden Technical Debt in Machine Learning Systems,* Sculley, David, Holt,
    Gary, Golovin, Daniel, Davydov, Eugene, Phillips, Todd, Ebner, Dietmar, Chaudhary,
    Vinay, Young, Michael, Crespo, Jean-Francois, and Dennison, Dan. Advances in Neural
    Information Processing Systems, 28 (2015): [https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习系统中的隐性技术债务*，Sculley, David, Holt, Gary, Golovin, Daniel, Davydov, Eugene,
    Phillips, Todd, Ebner, Dietmar, Chaudhary, Vinay, Young, Michael, Crespo, Jean-Francois,
    和 Dennison, Dan。《神经信息处理系统进展》，28（2015）：[https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)。'
- en: '*A Distributed General AutoML Platform on Kubernetes,* Zhou, Jinan, Velichkevich,
    Andrey, Prosvirov, Kirill, Garg, Anubhav, Oshima, Yuji, and Dutta, Debo. Katib.
    In 2019 USENIX Conference on Operational Machine Learning (OpML 19), pp. 55–57\.
    2019: [https://www.usenix.org/system/files/opml19papers-zhou.pdf](https://www.usenix.org/system/files/opml19papers-zhou.pdf).'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于Kubernetes的分布式通用AutoML平台*，Zhou, Jinan, Velichkevich, Andrey, Prosvirov,
    Kirill, Garg, Anubhav, Oshima, Yuji, 和 Dutta, Debo. Katib。发表于2019年USENIX运维机器学习大会（OpML
    19），第55–57页，2019年：[https://www.usenix.org/system/files/opml19papers-zhou.pdf](https://www.usenix.org/system/files/opml19papers-zhou.pdf)。'
- en: 'Part 3: Design Patterns for Model Optimization and Life Cycle Management'
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：模型优化与生命周期管理的设计模式
- en: This part of the book delves into crucial ethical considerations and challenges
    surrounding AI and machine learning systems, focusing on fairness, explainability,
    and model governance. It begins by examining various fairness notions and the
    importance of fair data collection, highlighting the impact of biased data on
    model performance and societal consequences. The discussion then extends to fairness
    in model optimization, presenting techniques to mitigate biases and ensure equitable
    outcomes. Model explainability is also addressed; we'll explore methods and tools
    for interpreting complex models and fostering trust in AI systems. Finally, the
    broader ethical implications and challenges of AI are tackled, emphasizing the
    significance of model governance, accountability, and transparency in the development
    and deployment of AI solutions. By offering a combination of theoretical insights
    and practical guidance, this part equips you with a deeper understanding of the
    ethical dimensions of AI and machine learning, enabling the development and deployment
    of responsible and equitable AI systems.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的这一部分深入探讨了围绕人工智能和机器学习系统的关键伦理问题和挑战，重点关注公平性、可解释性和模型治理。首先，讨论了各种公平性概念以及公平数据收集的重要性，突出了偏见数据对模型性能和社会影响的作用。接着，讨论了模型优化中的公平性，提出了减少偏差和确保公平结果的技术方法。还讨论了模型可解释性；我们将探索解释复杂模型的方法和工具，并促进对AI系统的信任。最后，书中探讨了人工智能的更广泛伦理影响和挑战，强调了在AI解决方案的开发和部署中，模型治理、问责制和透明度的重要性。通过提供理论洞察和实际指导，本部分将帮助你深入理解AI和机器学习的伦理维度，推动负责任和公平的AI系统的开发与部署。
- en: 'This part is made up of the following chapters:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分由以下章节组成：
- en: '[*Chapter 7*](B18681_07.xhtml#_idTextAnchor146), *Fairness Notions and Fair
    Data Collection*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B18681_07.xhtml#_idTextAnchor146)，*公平性概念与公平数据收集*'
- en: '[*Chapter 8*](B18681_08.xhtml#_idTextAnchor176), *Fairness in Model Optimization*'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B18681_08.xhtml#_idTextAnchor176)，*模型优化中的公平性*'
- en: '[*Chapter 9*](B18681_09.xhtml#_idTextAnchor198), *Model Explainability*'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B18681_09.xhtml#_idTextAnchor198)，*模型可解释性*'
- en: '[*Chapter 10*](B18681_10.xhtml#_idTextAnchor218), *Ethics and Model Governance*'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B18681_10.xhtml#_idTextAnchor218)，*伦理学与模型治理*'
