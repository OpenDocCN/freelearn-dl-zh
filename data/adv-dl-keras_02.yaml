- en: Chapter 2. Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章：深度神经网络
- en: 'In this chapter, we''ll be examining deep neural networks. These networks have
    shown excellent performance in terms of the accuracy of their classification on
    more challenging and advanced datasets like ImageNet, CIFAR10, and CIFAR100\.
    For conciseness, we''ll only be focusing on two networks, **ResNet** [2][4] and
    **DenseNet** [5]. While we will go into much more detail, it''s important to take
    a minute to introduce these networks:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究深度神经网络。这些网络在像 ImageNet、CIFAR10 和 CIFAR100 等更具挑战性和高级的数据集上的分类准确度表现优秀。为了简洁起见，我们将重点关注两个网络：**ResNet**
    [2][4] 和 **DenseNet** [5]。虽然我们会进行更详细的讲解，但在深入之前，有必要简要介绍这两个网络：
- en: ResNet introduced the concept of residual learning which enabled it to build
    very deep networks by addressing the vanishing gradient problem in deep convolutional
    networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 引入了残差学习的概念，通过解决深度卷积网络中的梯度消失问题，使得它能够构建非常深的网络。
- en: DenseNet improved the ResNet technique further by allowing every convolution
    to have direct access to inputs, and lower layer feature maps. It's also managed
    to keep the number of parameters low in deep networks by utilizing both the **Bottleneck**
    and **Transition** layers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 通过允许每个卷积层直接访问输入和较低层的特征图，进一步改进了 ResNet 技术。它还通过利用 **瓶颈** 层和 **过渡** 层，在深度网络中成功地保持了较低的参数数量。
- en: But why these two models, and not others? Well, since their introduction, there
    have been countless models such as **ResNeXt** [6] and **FractalNet** [7] which
    have been inspired by the technique used by these two networks. Likewise, with
    an understanding of both ResNet and DenseNet, we'll be able to use their design
    guidelines to build our own models. By using transfer learning, this will also
    allow us to take advantage of pretrained ResNet and DenseNet models for our own purposes.
    These reasons alone, along with their compatibility with Keras, make the two models
    ideal for exploring and complimenting the advanced deep learning scope of this
    book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择这两个模型，而不是其他模型？自从它们问世以来，已有无数模型（如 **ResNeXt** [6] 和 **FractalNet** [7]）受到了这两个网络使用的技术的启发。同样，通过理解
    ResNet 和 DenseNet，我们能够利用它们的设计指南构建自己的模型。通过迁移学习，这也将使我们能够利用预训练的 ResNet 和 DenseNet
    模型来为我们自己的目标服务。这些原因，加上它们与 Keras 的兼容性，使得这两个模型非常适合本书中探讨的高级深度学习内容。
- en: While this chapter's focus is on deep neural networks; we'll begin this chapter
    by discussing an important feature of Keras called the **Functional API**. This
    API acts as an alternative method for building networks in Keras and enables us
    to build more complex networks that cannot be accomplished by the sequential model.
    The reason why we're focusing so much on this API is that it will become a very
    useful tool for building deep networks such as the two we're focusing on in this
    chapter. It's recommended that you've completed, [Chapter 1](ch01.html "Chapter 1. Introducing
    Advanced Deep Learning with Keras"), *Introducing Advanced Deep Learning with
    Keras*, before moving onto this chapter as we'll refer to introductory level code
    and concepts explored in that chapter as we take them to an advanced level in
    this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章的重点是深度神经网络；我们将从讨论 Keras 中一个重要特性 **功能性 API** 开始。这个 API 作为构建网络的替代方法，可以帮助我们构建比顺序模型更复杂的网络。我们之所以如此关注这个
    API，是因为它将在构建深度网络时变得非常有用，尤其是本章所聚焦的两个网络。建议在继续阅读本章之前，先完成[第 1 章](ch01.html "第 1 章：介绍
    Keras 高级深度学习")，*介绍 Keras 高级深度学习*，因为我们将在本章中使用该章的入门级代码和概念，并将其提升到更高级的水平。
- en: 'The goals of this chapter is to introduce:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍：
- en: The Functional API in Keras, as well as exploring examples of networks running
    it
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 中的功能性 API，并探索运行此 API 的网络实例
- en: Deep Residual Networks (ResNet versions 1 and 2) implementation in Keras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度残差网络（ResNet 版本 1 和 2）在 Keras 中的实现
- en: The implementation of Densely Connected Convolutional Networks (DenseNet) into
    Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将密集连接卷积网络（DenseNet）实现到 Keras 中
- en: Explore two popular deep learning models, **ResNet,** and **DenseNet**
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索两个流行的深度学习模型：**ResNet** 和 **DenseNet**
- en: Functional API
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功能性 API
- en: In the sequential model that we first introduced in [Chapter 1](ch01.html "Chapter 1. Introducing
    Advanced Deep Learning with Keras"), *Introducing Advanced Deep Learning with
    Keras*, a layer is stacked on top of another layer. Generally, the model will
    be accessed through its input and output layers. We also learned that there is
    no simple mechanism if we find ourselves wanting to add an auxiliary input at
    the middle of the network, or even to extract an auxiliary output before the last
    layer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在[第1章](ch01.html "第1章：深入了解Keras中的高级深度学习")中首先介绍的顺序模型中，*《深入了解Keras中的高级深度学习》*，层是堆叠在另一个层之上的。通常，模型将通过其输入层和输出层进行访问。我们还了解到，如果我们想在网络中间添加一个辅助输入，或者在倒数第二层之前提取一个辅助输出，就没有简单的机制。
- en: That model also had its downside, for example, it doesn't support graph-like
    models or models that behave like Python functions. In addition, it's also difficult
    to share layers between the two models. Such limitations are addressed by the
    functional API and are the reason why it's a vital tool for anyone wanting to
    work with deep learning models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那个模型也有其不足之处，例如，它不支持图形化模型或像Python函数一样行为的模型。此外，也很难在两个模型之间共享层。函数式API解决了这些限制，这也是它成为任何想从事深度学习模型工作的人不可或缺的工具的原因。
- en: 'The Functional API is guided by the following two concepts:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式API遵循以下两个概念：
- en: A layer is an instance that accepts a tensor as an argument. The output of a layer
    is another tensor. To build a model, the layer instances are objects that are
    chained to one another through both input and output tensors. This will have similar
    end-result as would stacking multiple layers in the sequential model have. However,
    using layer instances makes it easier for models to have either auxiliary or multiple
    inputs and outputs since the input/output of each layer will be readily accessible.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层是一个接受张量作为参数的实例。层的输出是另一个张量。为了构建模型，层实例是通过输入和输出张量相互链接的对象。这将产生与在顺序模型中堆叠多个层相似的最终结果。然而，使用层实例使得模型更容易具有辅助输入或多个输入输出，因为每个层的输入/输出都可以直接访问。
- en: A model is a function between one or more input tensors and output tensors.
    In between the model input and output, tensors are the layer instances that are
    chained to one another by layer input and output tensors. A model is, therefore,
    a function of one or more input layers and one or more output layers. The model
    instance formalizes the computational graph on how the data flows from input(s)
    to output(s).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是一个在一个或多个输入张量和输出张量之间的函数。在模型的输入和输出之间，张量是通过层输入和输出张量相互链接的层实例。因此，模型是一个或多个输入层和一个或多个输出层的函数。模型实例规范了计算图，描述了数据如何从输入流向输出。
- en: 'After you''ve completed building the functional API model, the training and
    evaluation are then performed by the same functions used in the sequential model.
    To illustrate, in a functional API, a 2D convolutional layer, `Conv2D`, with 32 filters
    and with `x` as the layer input tensor and `y` as the layer output tensor can
    be written as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 完成函数式API模型构建后，训练和评估是通过与顺序模型相同的函数来执行的。举个例子，在函数式API中，具有32个滤波器的2D卷积层`Conv2D`，以`x`作为层的输入张量，以`y`作为层的输出张量，可以写成：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We''re also able to stack multiple layers to build our models. For example,
    we can rewrite the CNN on MNIST code, the same code we created in the last chapter,
    as shown in following listing:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还能够堆叠多个层来构建我们的模型。例如，我们可以像下面的代码示例那样，重写在上章创建的CNN on MNIST代码：
- en: 'You''ll find Listing 2.1.1, `cnn-functional-2.1.1.py`, as follows. This shows
    us how we can convert the `cnn-mnist-1.4.1.py` code using the functional API:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你将会看到以下列表2.1.1，`cnn-functional-2.1.1.py`，它展示了我们如何使用函数式API转换`cnn-mnist-1.4.1.py`代码：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By default, `MaxPooling2D` uses `pool_size=2`, so the argument has been removed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`MaxPooling2D`使用`pool_size=2`，因此该参数已被移除。
- en: In the preceding listing every layer is a function of a tensor. They each generate
    a tensor as an output which becomes the input to the next layer. To create this
    model, we can call `Model()` and supply both the `inputs` and `outputs` tensors,
    or alternatively the lists of tensors. Everything else remains the same.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，每个层都是张量的函数。它们每个都会生成一个张量作为输出，这个输出会成为下一个层的输入。为了创建这个模型，我们可以调用`Model()`并提供`inputs`和`outputs`张量，或者提供张量的列表。其他部分保持不变。
- en: The same listing can also be trained and evaluated using the `fit()` and `evaluate()`
    functions, similar to the sequential model. The `sequential` class is, in fact,
    a subclass of the `Model` class. We need to remember that we inserted the `validation_data`
    argument in the `fit()` function to see the progress of validation accuracy during
    training. The accuracy ranges from 99.3% to 99.4% in 20 epochs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的列表也可以使用 `fit()` 和 `evaluate()` 函数进行训练和评估，与顺序模型类似。`sequential` 类实际上是 `Model`
    类的子类。我们需要记住在 `fit()` 函数中插入了 `validation_data` 参数以查看训练过程中验证准确度的进展。准确度在 20 个 epoch
    中的范围从 99.3% 到 99.4%。
- en: Creating a two-input and one-output model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个两输入一输出的模型
- en: We're now going to do something really exciting, creating an advanced model
    with two inputs and one output. Before we start, it's important to know that this
    is something that is not straightforward in the sequential model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将要做一些非常激动人心的事情，创建一个具有两个输入和一个输出的高级模型。在我们开始之前，重要的是要知道，这对于顺序模型来说并不是一件简单的事情。
- en: Let's suppose a new model for the MNIST digit classification is invented, and
    it's called the **Y-Network,** as shown in *Figure 2.1.1*. The Y-Network uses
    the same input twice, both on the left and right CNN branches. The network combines
    the results using `concatenate` layer. The merge operation `concatenate` is similar
    to stacking two tensors of the same shape along the concatenation axis to form
    one tensor. For example, concatenating two tensors of shape (3, 3, 16) along the
    last axis will result in a tensor of shape (3, 3, 32).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个新的 MNIST 数字分类模型被发明了，它被称为**Y-Network**，如 *图 2.1.1* 所示。Y-Network 使用相同的输入两次，分别在左右
    CNN 分支上。网络使用 `concatenate` 层合并结果。合并操作 `concatenate` 类似于沿着连接轴堆叠两个形状相同的张量以形成一个张量。例如，沿着最后一个轴连接两个形状为
    (3, 3, 16) 的张量将得到一个形状为 (3, 3, 32) 的张量。
- en: 'Everything else after the `concatenate` layer will remain the same as the previous
    CNN model. That is `Flatten-Dropout-Dense`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`concatenate` 层之后的其他内容与之前的 CNN 模型保持不变，即 `Flatten-Dropout-Dense`：'
- en: '![Creating a two-input and one-output model](img/B08956_02_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个两输入一输出的模型](img/B08956_02_01.jpg)'
- en: 'Figure 2.1.1: The Y-Network accepts the same input twice but processes the
    input in two branches of convolutional networks. The outputs of the branches are
    combined using the concatenate layer. The last layer prediction is going to be
    similar to the previous CNN example.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.1.1: Y-Network 接受相同的输入两次，但在两个卷积网络分支中处理输入。分支的输出通过 concatenate 层组合。最后一层的预测将与之前的
    CNN 示例类似。'
- en: To improve the performance of the model in *Listing* *2.1.1*, we can propose
    several changes. Firstly, the branches of the Y-Network are doubling the number
    of filters to compensate for the halving of the feature maps size after `MaxPooling2D()`.
    For example, if the output of the first convolution is (28, 28, 32), after max
    pooling the new shape is (14, 14, 32). The next convolution will have a filter
    size of 64 and output dimensions of (14, 14, 64).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要改进 *列表* *2.1.1* 中模型的性能，我们可以提出几个改变。首先，Y-Network 的分支正在将滤波器数量加倍，以补偿 `MaxPooling2D()`
    后特征映射大小的减半。例如，如果第一个卷积的输出为 (28, 28, 32)，经过最大池化后新形状为 (14, 14, 32)。接下来的卷积将有 64 个滤波器大小，并且输出尺寸为
    (14, 14, 64)。
- en: 'Second, although both branches have the same kernel size of 3, the right branch
    use a dilation rate of 2\. *Figure 2.1.2* shows the effect of different dilation
    rates on a kernel with size 3\. The idea is that by increasing the coverage of
    the kernel using dilation rate, the CNN will enable the right branch to learn
    different feature maps. We''ll use the option `padding=''same''` to ensure that
    we will not have negative tensor dimensions when the dilated CNN is used. By using
    `padding=''same''`, we''ll keep the dimensions of the input the same as the output
    feature maps. This is accomplished by padding the input with zeros to make sure
    that the output has the *same* size:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，尽管两个分支的卷积核尺寸都为 3，右分支使用了扩张率为 2。*图 2.1.2* 展示了在大小为 3 的卷积核上不同扩张率的效果。这个想法是通过增加扩张率增加卷积核的覆盖范围，CNN
    将使右分支能够学习不同的特征映射。我们将使用 `padding='same'` 选项确保在使用扩张 CNN 时不会出现负张量维度。通过使用 `padding='same'`，我们将保持输入的尺寸与输出特征映射的尺寸相同。这是通过填充输入以确保输出具有*相同*大小来完成的：
- en: '![Creating a two-input and one-output model](img/B08956_02_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个两输入一输出的模型](img/B08956_02_02.jpg)'
- en: 'Figure 2.1.2: By increasing the dilate rate from 1, the effective kernel coverage
    also increases'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.1.2: 通过增加扩张率从 1 开始，有效卷积核覆盖范围也会增加。'
- en: Following listing shows the implementation of Y-Network. The two branches are
    created by the two for loops. Both branches expect the same input shape. The two `for`
    loops will create two 3-layer stacks of `Conv2D-Dropout-MaxPooling2D`. While we
    used the `concatenate` layer to combine the outputs of the left and right branches,
    we could also utilize the other merge functions of Keras, such as `add`, `dot`,
    `multiply`. The choice of the merge function is not purely arbitrary but must
    be based on a sound model design decision.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下清单展示了 Y-网络的实现。两个分支是通过两个 `for` 循环创建的。两个分支期望相同的输入形状。这两个 `for` 循环将创建两个 3 层堆叠的
    `Conv2D-Dropout-MaxPooling2D`。尽管我们使用了 `concatenate` 层来组合左右分支的输出，但我们也可以使用 Keras
    的其他合并函数，例如 `add`、`dot`、`multiply`。合并函数的选择并非完全任意，而是必须基于合理的模型设计决策。
- en: 'In the Y-Network, `concatenate` will not discard any portion of the feature
    maps. Instead, we''ll let the `Dense` layer figure out what to do with the concatenated
    feature maps. Listing 2.1.2, `cnn-y-network-2.1.2.py` shows the Y-Network implementation
    using the Functional API:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Y-网络中，`concatenate` 不会丢弃特征图的任何部分。相反，我们会让 `Dense` 层来处理拼接后的特征图。清单 2.1.2，`cnn-y-network-2.1.2.py`
    展示了使用功能性 API 实现的 Y-网络：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Taking a step back, we can note that the Y-Network is expecting two inputs for
    training and validation. The inputs are identical, so `[x_train, x_train]` is
    supplied.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Y-网络需要两个输入进行训练和验证。这两个输入是相同的，因此提供了`[x_train, x_train]`。
- en: 'Over the course of the 20 epochs, the accuracy of the Y-Network ranges from
    99.4% to 99.5%. This is a slight improvement over the 3-stack CNN which achieved
    a range between 99.3% and 99.4% accuracy range. However, this was at the cost
    of both higher complexity and more than double the number of parameters. The following
    figure, *Figure 2.1.3,* shows the architecture of the Y-Network as understood
    by Keras and generated by the `plot_model()` function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 20 个周期中，Y-网络的准确率从 99.4% 上升到 99.5%。这相比于3层堆叠的 CNN（其准确率范围为 99.3% 至 99.4%）略有提高。然而，这也带来了更高的复杂性以及超过一倍的参数数量。以下图表，*图
    2.1.3*，展示了 Keras 理解并通过 `plot_model()` 函数生成的 Y-网络架构：
- en: '![Creating a two-input and one-output model](img/B08956_02_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个双输入单输出模型](img/B08956_02_03.jpg)'
- en: 'Figure 2.1.3: The CNN Y-Network as implemented in Listing 2.1.2'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1.3：在清单 2.1.2 中实现的 CNN Y-网络
- en: This concludes our look at the Functional API. We should take this time to remember
    that the focus of this chapter is building deep neural networks, specifically
    ResNet and DenseNet. Therefore, we're only covering the Functional API materials
    needed to build them, as to cover the entire API would be beyond the scope of
    this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对功能性 API 的介绍。我们应该记住，本章的重点是构建深度神经网络，特别是 ResNet 和 DenseNet。因此，我们仅覆盖了构建它们所需的功能性
    API 内容，因为覆盖整个 API 会超出本书的范围。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The reader is referred to visit [https://keras.io/](https://keras.io/) for additional
    information on functional API.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以访问[https://keras.io/](https://keras.io/)以获取有关功能性 API 的更多信息。
- en: Deep residual networks (ResNet)
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度残差网络（ResNet）
- en: One key advantage of deep networks is that they have a great ability to learn
    different levels of representations from both inputs and feature maps. In both
    classification, segmentation, detection and a number of other computer vision
    problems, learning different levels of features generally leads to better performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络的一个关键优势是它们能够从输入和特征图中学习不同层次的表示。在分类、分割、检测以及许多其他计算机视觉问题中，学习不同层次的特征通常能带来更好的性能。
- en: However, you'll find that it's not easy to train deep networks as a result of
    the gradient vanishes (or explodes) with depth in the shallow layers during backpropagation.
    *Figure 2.2.1* illustrates the problem of vanishing gradient. The network parameters
    are updated by backpropagation from the output layer to all previous layers. Since
    backpropagation is based on the chain rule, there is a tendency for gradients
    to diminish as they reach the shallow layers. This is due to the multiplication
    of small numbers, especially for the small absolute value of errors and parameters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您会发现训练深度网络并不容易，因为在反向传播过程中，浅层的梯度会随着深度的增加而消失（或爆炸）。*图 2.2.1* 说明了梯度消失的问题。网络参数通过从输出层到所有前一层的反向传播进行更新。由于反向传播是基于链式法则的，因此梯度在到达浅层时会逐渐减小。这是因为小数的乘积，特别是当误差和参数的绝对值很小时。
- en: The number of multiplication operations will be proportional to the depth of
    the network. It's also worth noting that if the gradient degrades, the parameters
    will not be updated appropriately.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法操作的次数将与网络的深度成正比。还值得注意的是，如果梯度退化，参数将无法得到适当的更新。
- en: 'Hence, the network will fail to improve its performance:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络将无法提升其性能：
- en: '![Deep residual networks (ResNet)](img/B08956_02_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_04.jpg)'
- en: 'Figure 2.2.1: A common problem in deep networks is that the gradient vanishes
    as it reaches the shallow layers during backpropagation.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2.1：深度网络中的一个常见问题是梯度在反向传播过程中传递到浅层时会消失。
- en: '![Deep residual networks (ResNet)](img/B08956_02_05.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_05.jpg)'
- en: 'Figure 2.2.2: A comparison between a block in a typical CNN and a block in
    ResNet. To prevent degradation in gradients during backpropagation, a shortcut
    connection is introduced.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2.2：典型 CNN 中的一个块与 ResNet 中的一个块的对比。为了防止反向传播过程中梯度退化，引入了快捷连接。
- en: To alleviate the degradation of the gradient in deep networks, ResNet introduced
    the concept of a deep residual learning framework. Let's analyze a block, a small
    segment of our deep network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解深度网络中梯度的退化问题，ResNet 引入了深度残差学习框架的概念。让我们分析一下一个块，一个深度网络的小段。
- en: The preceding figure shows a comparison between a typical CNN block and a ResNet
    residual block. The idea of ResNet is that in order to prevent the gradient from
    degrading, we'll let the information flow through the shortcut connections to reach
    the shallow layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了典型 CNN 块与 ResNet 残差块之间的对比。ResNet 的理念是，为了防止梯度退化，我们将让信息通过快捷连接流向浅层。
- en: Next, we're going to look at more details within the discussion of the differences
    between the two blocks. *Figure 2.2.3* shows more details of the CNN block of
    another commonly used deep network, VGG[3], and ResNet. We'll represent the layer
    feature maps as **x**. The feature maps at layer *l* are
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进一步探讨这两个块之间差异的更多细节。*图 2.2.3* 显示了另一种常用深度网络 VGG[3] 和 ResNet 的 CNN 块的更多细节。我们将层特征图表示为
    **x**。层 *l* 的特征图是
- en: '![Deep residual networks (ResNet)](img/B08956_02_001.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_001.jpg)'
- en: . The operations in the CNN layer are **Conv2D-Batch Normalization** (**BN**)-**ReLU**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 层中的操作是**Conv2D-批量归一化**（**BN**）-**ReLU**。
- en: 'Let''s suppose we represent this set of operations in the form of *H*() = Conv2D-Batch
    Normalization(BN)-ReLU, that will then mean that:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将这一组操作表示为 *H*() = Conv2D-批量归一化（BN）-ReLU，这将意味着：
- en: '![Deep residual networks (ResNet)](img/B08956_02_002.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_002.jpg)'
- en: (Equation 2.2.1)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 2.2.1）
- en: '![Deep residual networks (ResNet)](img/B08956_02_003.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_003.jpg)'
- en: (Equation 2.2.2)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 2.2.2）
- en: In other words, the feature maps at layer *l* - 2 are transformed to
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*l* - 2 层的特征图被转换为
- en: '![Deep residual networks (ResNet)](img/B08956_02_004.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_004.jpg)'
- en: by *H*() = Conv2D-Batch Normalization(BN)-ReLU. The same set of operations is
    applied to transform
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由 *H*() = Conv2D-批量归一化（BN）-ReLU。相同的一组操作应用于转换
- en: '![Deep residual networks (ResNet)](img/B08956_02_005.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_005.jpg)'
- en: to
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '![Deep residual networks (ResNet)](img/B08956_02_006.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_006.jpg)'
- en: . To put this another way, if we have an 18-layer VGG, then there are 18 *H*()
    operations before the input image is transformed to the 18^(th) layer feature
    maps.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们有一个 18 层的 VGG，那么在输入图像转化为第 18 层特征图之前，会进行 18 次 *H*() 操作。
- en: 'Generally speaking, we can observe that the layer *l* output feature maps are
    directly affected by the previous feature maps only. Meanwhile, for ResNet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以观察到，层 *l* 输出的特征图仅受前一层特征图的直接影响。同时，对于 ResNet：
- en: '![Deep residual networks (ResNet)](img/B08956_02_007.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_007.jpg)'
- en: (Equation 2.2.3)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 2.2.3）
- en: '![Deep residual networks (ResNet)](img/B08956_02_008.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_008.jpg)'
- en: (Equation 2.2.4)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 2.2.4）
- en: '![Deep residual networks (ResNet)](img/B08956_02_06.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_06.jpg)'
- en: 'Figure 2.2.3: A detailed layer operations for a plain CNN block and a Residual
    block'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2.3：普通 CNN 块和残差块的详细层操作
- en: '![Deep residual networks (ResNet)](img/B08956_02_009.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络 (ResNet)](img/B08956_02_009.jpg)'
- en: is made of `Conv2D-BN,` which is also known as the residual mapping. The **+**
    sign is tensor element-wise addition between the shortcut connection and the output
    of
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由`Conv2D-BN`构成，这也被称为残差映射。**+**符号表示快捷连接与输出的张量元素逐一相加
- en: '![Deep residual networks (ResNet)](img/B08956_02_010.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络（ResNet）](img/B08956_02_010.jpg)'
- en: . The shortcut connection doesn't add extra parameters nor extra computational
    complexity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 快捷连接不会增加额外的参数或计算复杂度。
- en: The add operation can be implemented in Keras by the `add()` merge function.
    However, both the
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，add操作可以通过`add()`合并函数来实现。然而，两者
- en: '![Deep residual networks (ResNet)](img/B08956_02_011.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络（ResNet）](img/B08956_02_011.jpg)'
- en: equation and **x** should have the same dimensions. If the dimensions are different,
    for example, when changing the feature maps size, we should perform a linear projection
    on **x** as to match the size of
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式和**x**应该具有相同的维度。如果维度不同，例如在更改特征图大小时，我们应该对**x**进行线性投影，以匹配
- en: '![Deep residual networks (ResNet)](img/B08956_02_012.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络（ResNet）](img/B08956_02_012.jpg)'
- en: . In the original paper, the linear projection for the case, when the feature
    maps size is halved, is done by a `Conv2D` with a 1 × 1 kernel and `strides=2`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，当特征图大小减半时，线性投影是通过一个1 × 1卷积核和`strides=2`的`Conv2D`来实现的。
- en: Back in [Chapter 1](ch01.html "Chapter 1. Introducing Advanced Deep Learning
    with Keras"), Introducing *Advanced Deep Learning with Keras*, we discussed that `stride
    > 1` is equivalent to skipping pixels during convolution. For example, if `strides=2`,
    we could skip every other pixel when we slide the kernel during the convolution
    process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第1章](ch01.html "第1章。介绍Keras中的高级深度学习")，在介绍*高级深度学习与Keras*时，我们讨论了`stride > 1`等价于在卷积过程中跳过像素。例如，如果`strides=2`，则在滑动卷积核时，每跳过一个像素。
- en: The preceding *Equations* *2.2.3* and *2.2.4*, both model ResNet residual block
    operations. They imply that if the deeper layers can be trained to have fewer
    errors, then there is no reason why the shallower layers should have higher errors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的*方程式* *2.2.3*和*2.2.4*都表示ResNet残差块的操作。它们暗示，如果更深层的网络可以训练得错误较少，那么浅层网络不应该有更多的错误。
- en: Knowing the basic building blocks of ResNet, we're able to design a deep residual
    network for image classification. This time, however, we're going to tackle a
    more challenging and advanced dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了ResNet的基本构建块后，我们可以设计一个用于图像分类的深度残差网络。然而，这次我们将处理一个更具挑战性和高级的数据集。
- en: 'In our examples, we''re going to consider CIFAR10, which was one of the datasets
    the original paper was validated. In this example, Keras provides an API to conveniently
    access the CIFAR10 dataset, as shown:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将考虑CIFAR10数据集，这是原始论文验证过的其中一个数据集。在此示例中，Keras提供了一个API，可以方便地访问CIFAR10数据集，如下所示：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Like MNIST, the CIFAR10 dataset has 10 categories. The dataset is a collection
    of small (32 × 32) RGB real-world images of an airplane, automobile, bird, cat,
    deer, dog, frog, horse, ship, and a truck corresponding to each of the 10 categories.
    *Figure 2.2.4* shows sample images from CIFAR10.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与MNIST类似，CIFAR10数据集包含10个类别。该数据集由小型（32 × 32）RGB真实世界图像组成，包含飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车，每个类别对应一个图像。*图2.2.4*显示了CIFAR10的示例图像。
- en: 'In the dataset, there are 50,000 labeled train images and 10,000 labeled test
    images for validation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在该数据集中，有50,000个标记的训练图像和10,000个标记的测试图像用于验证：
- en: '![Deep residual networks (ResNet)](img/B08956_02_07.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络（ResNet）](img/B08956_02_07.jpg)'
- en: 'Figure 2.2.4: Sample images from the CIFAR10 dataset. The full dataset has
    50,000 labeled train images and 10,000 labeled test images for validation.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2.4：CIFAR10数据集的示例图像。完整数据集包含50,000个标记的训练图像和10,000个标记的测试图像用于验证。
- en: For the CIFAR10 data, ResNet can be built using different network architectures
    as shown in *Table 2.2.1*. The values of both *n* and the corresponding architectures
    of ResNet were validated in *Table 2.2.2*. *Table 2.2.1* means we have three sets
    of residual blocks. Each set has *2n* layers corresponding to *n* residual blocks.
    The extra layer in 32 × 32 is the first layer for the input image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CIFAR10数据集，ResNet可以使用不同的网络架构来构建，如*表2.2.1*所示。*n*的值和ResNet的相应架构已在*表2.2.2*中验证过。*表2.2.1*表示我们有三组残差块。每组有*2n*层，对应于*n*个残差块。在32
    × 32的输入图像中，额外的层是第一层。
- en: The kernel size is 3, except for the transition between two feature maps with
    different sizes that implements a linear mapping. For example, a `Conv2D` with
    a kernel size of 1 and `strides=2`. For the sake of consistency with DenseNet,
    we'll use the term Transition layer when we join two residual blocks of different
    sizes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核大小为 3，除了两个不同大小特征图之间的过渡层，它实现了线性映射。例如，`Conv2D` 的卷积核大小为 1，`strides=2`。为了与 DenseNet
    保持一致，我们将在连接两个不同大小的残差块时使用过渡层（Transition Layer）一词。
- en: ResNet uses `kernel_initializer='he_normal'` in order to aid the convergence
    when backpropagation is taking place [1]. The last layer is made of `AveragePooling2D-Flatten-Dense`.
    It's worth noting at this point that ResNet does not use dropout. It also appears
    that the add merge operation and the 1 × 1 convolution have a self-regularizing
    effect. *Figure 2.2.4* shows the ResNet model architecture for the CIFAR10 dataset
    as described in *Table 2.2.1*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 使用 `kernel_initializer='he_normal'` 来帮助反向传播时的收敛[1]。最后一层由 `AveragePooling2D-Flatten-Dense`
    组成。值得注意的是，ResNet 不使用 dropout。并且看起来 add 合并操作和 1 × 1 卷积具有自我正则化效果。*图 2.2.4* 展示了 CIFAR10
    数据集的 ResNet 模型架构，如 *表 2.2.1* 中所述。
- en: 'The following listing shows the partial ResNet implementation within Keras.
    The code has been contributed to the Keras GitHub repository. From *Table 2.2.2*
    we can also see that by modifying the value of `n`, we''re able to increase the
    depth of the networks. For example, for `n = 18`, we already have ResNet110, a
    deep network with 110 layers. To build ResNet20, we use `n = 3`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了 Keras 中的部分 ResNet 实现。该代码已被贡献到 Keras GitHub 仓库。从 *表 2.2.2* 我们还可以看到，通过修改
    `n` 的值，我们能够增加网络的深度。例如，对于 `n = 18`，我们已经得到了 ResNet110，这是一种拥有 110 层的深度网络。为了构建 ResNet20，我们使用
    `n = 3`：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `resnet_v1()` method is a model builder for ResNet. It uses a utility function,
    `resnet_layer()` to help build the stack of `Conv2D-BN-ReLU`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`resnet_v1()` 方法是 ResNet 的模型构建器。它使用一个工具函数 `resnet_layer()` 来帮助构建 `Conv2D-BN-ReLU`
    堆栈。'
- en: It's referred to as version 1, as we will see in the next section, an improved
    ResNet was proposed, and that has been called ResNet version 2, or v2\. Over ResNet,
    ResNet v2 has an improved residual block design resulting in better performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为版本 1，正如我们在下一节中将看到的那样，提出了改进版的 ResNet，并称之为 ResNet 版本 2，或 v2。与 ResNet 相比，ResNet
    v2 在残差模块设计上有所改进，从而提高了性能。
- en: '| Layers | Output Size | Filter Size | Operations |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 层数 | 输出大小 | 卷积核大小 | 操作 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Convolution | 32 × 32 | 16 | ![Deep residual networks (ResNet)](img/B08956_02_013.jpg)
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 | 32 × 32 | 16 | ![深度残差网络（ResNet）](img/B08956_02_013.jpg) |'
- en: '| Residual Block(1) | 32 × 32 |   | ![Deep residual networks (ResNet)](img/B08956_02_014.jpg)
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 残差模块(1) | 32 × 32 |   | ![深度残差网络（ResNet）](img/B08956_02_014.jpg) |'
- en: '| Transition Layer(1) | 32 × 32 |   | ![Deep residual networks (ResNet)](img/B08956_02_015.jpg)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 过渡层(1) | 32 × 32 |   | ![深度残差网络（ResNet）](img/B08956_02_015.jpg) |'
- en: '| 16 × 16 |   |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 16 × 16 |   |'
- en: '| Residual Block(2) | 16 × 16 | 32 | ![Deep residual networks (ResNet)](img/B08956_02_016.jpg)
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 残差模块(2) | 16 × 16 | 32 | ![深度残差网络（ResNet）](img/B08956_02_016.jpg) |'
- en: '| Transition Layer(2) | 16 × 16 |   | ![Deep residual networks (ResNet)](img/B08956_02_017.jpg)
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 过渡层(2) | 16 × 16 |   | ![深度残差网络（ResNet）](img/B08956_02_017.jpg) |'
- en: '| 8 × 8 |   |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 8 × 8 |   |'
- en: '| Residual Block(3) | 8 × 8 | 64 | ![Deep residual networks (ResNet)](img/B08956_02_018.jpg)
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 残差模块(3) | 8 × 8 | 64 | ![深度残差网络（ResNet）](img/B08956_02_018.jpg) |'
- en: '| Average Pooling | 1 × 1 |   | ![Deep residual networks (ResNet)](img/B08956_02_019.jpg)
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 平均池化 | 1 × 1 |   | ![深度残差网络（ResNet）](img/B08956_02_019.jpg) |'
- en: 'Table 2.2.1: ResNet network architecture configuration'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表 2.2.1：ResNet 网络架构配置
- en: '![Deep residual networks (ResNet)](img/B08956_02_08.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络（ResNet）](img/B08956_02_08.jpg)'
- en: 'Figure 2.2.4: The model architecture of ResNet for the CIFAR10 dataset classification'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2.4：用于 CIFAR10 数据集分类的 ResNet 模型架构
- en: '| # Layers | n | % Accuracy on CIFAR10 (Original paper) | % Accuracy on CIFAR10
    (This book) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| # 层数 | n | CIFAR10 精度（原论文） | CIFAR10 精度（本书） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ResNet20 | 3 | 91.25 | 92.16 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ResNet20 | 3 | 91.25 | 92.16 |'
- en: '| ResNet32 | 5 | 92.49 | 92.46 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ResNet32 | 5 | 92.49 | 92.46 |'
- en: '| ResNet44 | 7 | 92.83 | 92.50 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ResNet44 | 7 | 92.83 | 92.50 |'
- en: '| ResNet56 | 9 | 93.03 | 92.71 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ResNet56 | 9 | 93.03 | 92.71 |'
- en: '| ResNet110 | 18 | 93.57 | 92.65 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ResNet110 | 18 | 93.57 | 92.65 |'
- en: 'Table 2.2.2: ResNet architectures validated with CIFAR10'
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表 2.2.2：用 CIFAR10 验证的 ResNet 架构
- en: 'The following listing shows the partial code of `resnet-cifar10-2.2.1.py`,
    which is the Keras model implementation of ResNet v1:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了 `resnet-cifar10-2.2.1.py` 的部分代码，这是 ResNet v1 的 Keras 实现：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are some minor differences from the original implementation of ResNet.
    In particular, we don't use SGD, and instead, we'll use Adam. This is because
    ResNet is easier to converge with Adam. We'll also use a learning rate (`lr`)
    scheduler, `lr_schedule()`, in order to schedule the decrease in `lr` at 80, 120,
    160, and 180 epochs from the default 1e-3\. The `lr_schedule()` function will
    be called after every epoch during training as part of the `callbacks` variable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的ResNet实现相比，有一些小的差异。特别是，我们不使用SGD，而是使用Adam。这是因为使用Adam时，ResNet更容易收敛。我们还将使用学习率（`lr`）调度器`lr_schedule()`，在80、120、160和180个epoch时从默认的1e-3开始逐步减少`lr`。`lr_schedule()`函数将在训练的每个epoch后作为`callbacks`变量的一部分被调用。
- en: The other callback saves the checkpoint every time there is progress made in
    the validation accuracy. When training deep networks, it is a good practice to
    save the model or weight checkpoint. This is because it takes a substantial amount
    of time to train deep networks. When you want to use your network, all you need
    to do is simply reload the checkpoint, and the trained model is restored. This
    can be accomplished by calling Keras `load_model()`. The `lr_reducer()` function
    is included. In case the metric has plateaued before the schedule reduction, this
    callback will reduce the learning rate by the factor if the validation loss has
    not improved after `patience=5` epochs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个回调函数会在验证精度有进展时每次保存检查点。在训练深度网络时，保存模型或权重检查点是一个好习惯。因为训练深度网络需要大量时间。当你想使用你的网络时，只需要重新加载检查点，训练好的模型就会被恢复。这可以通过调用Keras的`load_model()`来实现。`lr_reducer()`函数也被包含在内。如果在调度减少学习率之前，验证损失没有改善，该回调函数将在`patience=5`个epoch后通过某个因子减少学习率。
- en: The `callbacks` variable is supplied when the `model.fit()` method is called.
    Similar to the original paper, the Keras implementation uses data augmentation,
    `ImageDataGenerator()`, in order to provide additional training data as part of
    the regularization schemes. As the number of training data increases, generalization
    will improve.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`model.fit()`方法时，会提供`callbacks`变量。与原始论文相似，Keras实现使用数据增强`ImageDataGenerator()`来提供额外的训练数据，作为正则化方案的一部分。随着训练数据量的增加，泛化能力会有所提升。
- en: 'For example, a simple data augmentation is flipping the photo of the dog, as
    shown in following figure (`horizontal_flip=True`). If it is an image of a dog,
    then the flipped image is still an image of a dog. You can also perform other
    transformation, such as scaling, rotation, whitening, and so on, and the label
    will still remain the same:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个简单的数据增强是翻转狗的照片，如下图所示（`horizontal_flip=True`）。如果原图是狗的照片，那么翻转后的图像仍然是狗的照片。你也可以进行其他变换，如缩放、旋转、白化等，标签依然保持不变：
- en: '![Deep residual networks (ResNet)](img/B08956_02_09.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![深度残差网络（ResNet）](img/B08956_02_09.jpg)'
- en: 'Figure 2.2.5: A simple data augmentation is flipping the original image'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2.5：一个简单的数据增强是翻转原始图像
- en: It's often difficult to exactly duplicate the implementation of the original
    paper, especially in the optimizer used and data augmentation, as there are slight
    differences in the performance of the Keras ResNet implementation in this book and the
    model in the original paper.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 精确复制原始论文中的实现通常是困难的，特别是在使用的优化器和数据增强方面，因为本书中Keras实现的ResNet模型与原始论文中的模型在性能上存在轻微差异。
- en: ResNet v2
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet v2
- en: After the release of the second paper on ResNet [4], the original model presented
    in the previous section has been known as ResNet v1\. The improved ResNet is commonly
    called ResNet v2\. The improvement is mainly found in the arrangement of layers
    in the residual block as shown in following figure.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布了关于ResNet的第二篇论文[4]之后，前一部分中介绍的原始模型被称为ResNet v1。改进版的ResNet通常称为ResNet v2。该改进主要体现在残差块中层的排列，如下图所示。
- en: 'The prominent changes in ResNet v2 are:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet v2的显著变化包括：
- en: The use of a stack of 1 × 1 - 3 × 3 - 1 × 1 `BN-ReLU-Conv2D`
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用1 × 1 - 3 × 3 - 1 × 1 `BN-ReLU-Conv2D`堆叠
- en: Batch normalization and ReLU activation come before 2D convolution
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化和ReLU激活在2D卷积之前
- en: '![ResNet v2](img/B08956_02_10.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![ResNet v2](img/B08956_02_10.jpg)'
- en: 'Figure 2.3.1: A comparison of residual blocks between ResNet v1 and ResNet
    v2'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3.1：ResNet v1与ResNet v2之间残差块的对比
- en: 'ResNet v2 is also implemented in the same code as `resnet-cifar10-2.2.1.py`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet v2也在与`resnet-cifar10-2.2.1.py`相同的代码中实现：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'ResNet v2''s model builder is shown in the following code. For example, to
    build ResNet110 v2, we''ll use `n = 12`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet v2 的模型构建器在以下代码中展示。例如，为了构建 ResNet110 v2，我们将使用 `n = 12`：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The accuracy of ResNet v2 is shown in following table:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet v2 的准确度在以下表格中展示：
- en: '| # Layers | n | % Accuracy on CIFAR10 (Original paper) | % Accuracy on CIFAR10
    (This book) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| # 层数 | n | CIFAR10 精度（原文） | CIFAR10 精度（本书） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ResNet56 | 9 | NA | 93.01 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ResNet56 | 9 | 无 | 93.01 |'
- en: '| ResNet110 | 18 | 93.63 | 93.15 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ResNet110 | 18 | 93.63 | 93.15 |'
- en: In the Keras applications package, ResNet50 has been implemented as well with
    the corresponding checkpoint for reuse. This is an alternative implementation
    but tied to the 50-layer ResNet v1.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 应用包中，ResNet50 也已经实现，并配有相应的检查点以便重用。这是一种替代实现，但绑定于 50 层 ResNet v1。
- en: Densely connected convolutional networks (DenseNet)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集连接卷积网络 (DenseNet)
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_11.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_11.jpg)'
- en: 'Figure 2.4.1: A 4-layer Dense block in DenseNet. The input to each layer is
    made of all the previous feature maps.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4.1：DenseNet 中的 4 层 Dense 块。每一层的输入由所有前面生成的特征图组成。
- en: DenseNet attacks the problem of vanishing gradient using a different approach.
    Instead of using shortcut connections, all the previous feature maps will become
    the input of the next layer. The preceding figure, shows an example of a dense
    interconnection in one Dense block.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 通过一种不同的方法解决了梯度消失问题。与使用快捷连接不同，所有之前的特征图将作为下一层的输入。前面的图展示了 Dense 块中的密集互连示例。
- en: 'For simplicity, in this figure, we''ll only show four layers. Notice that the
    input to layer *l* is the concatenation of all previous feature maps. If we designate
    the `BN-ReLU-Conv2D` as the operation *H*(x), then the output of layer *l* is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为简便起见，在此图中我们只显示了四层。请注意，第 *l* 层的输入是所有先前特征图的拼接。如果我们将 `BN-ReLU-Conv2D` 视为操作 *H*(x)，则第
    *l* 层的输出为：
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_020.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_020.jpg)'
- en: (Equation 2.4.1)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (方程 2.4.1)
- en: '`Conv2D` uses a kernel of size 3\. The number of feature maps generated per
    layer is called the growth rate, *k*. Normally, *k* = 12, but *k* = 24 is also
    used in the paper, *Densely Connected Convolutional Networks*, Huang, and others,
    2017 [5]. Therefore, if the number of feature maps'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv2D` 使用 3×3 的卷积核。每层生成的特征图数量称为增长率，*k*。通常，*k* = 12，但在论文《Densely Connected
    Convolutional Networks》中，*Huang* 等人（2017）也使用 *k* = 24 [5]。因此，如果特征图数量为'
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_021.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_021.jpg)'
- en: is
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_022.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_022.jpg)'
- en: ', then the total number of feature maps at the end of the 4-layer Dense block
    in *Figure 2.4.1* will be'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ，那么在 *图 2.4.1* 的 4 层 Dense 块结束时，特征图的总数将是
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_023.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_023.jpg)'
- en: .
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: DenseNet also recommends that the Dense block is preceded by `BN-ReLU-Conv2D`,
    along with the number of feature maps twice the growth rate,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 还建议 Dense 块前面应使用 `BN-ReLU-Conv2D`，并且特征图的数量应为增长率的两倍，
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_024.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_024.jpg)'
- en: '. Therefore, at the end of the Dense block, the total number of feature maps
    will be 72\. We''ll also use the same kernel size, which is 3\. At the output
    layer, DenseNet suggests that we perform an average pooling before the `Dense()`
    and `softmax` classifier. If the data augmentation is not used, a dropout layer
    must follow the Dense block `Conv2D`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 。因此，在 Dense 块结束时，特征图的总数将是 72。我们还将使用相同的卷积核大小，即 3。在输出层，DenseNet 建议我们在 `Dense()`
    和 `softmax` 分类器之前执行一次平均池化。如果未使用数据增强，必须在 Dense 块的 `Conv2D` 后添加一个 dropout 层：
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_12.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络 (DenseNet)](img/B08956_02_12.jpg)'
- en: 'Figure 2.4.2: A layer in a Dense block of DenseNet, with and without the bottleneck
    layer BN-ReLU-Conv2D(1). We''ll include the kernel size as an argument of Conv2D
    for clarity.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4.2：DenseNet 中一个 Dense 块的层，包含和不包含瓶颈层 BN-ReLU-Conv2D(1)。为清晰起见，我们将卷积核大小作为
    Conv2D 的一个参数。
- en: As the network gets deeper, two new problems will occur. Firstly, since every
    layer contributes *k* feature maps, the number of inputs at layer *l* is
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络加深，两个新问题将出现。首先，由于每一层都贡献*k*个特征图，因此在层*l*的输入数量为
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_025.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络（DenseNet）](img/B08956_02_025.jpg)'
- en: . Therefore, the feature maps can grow rapidly within deep layers, resulting
    in the computation becoming slow. For example, for a 101-layer network this will
    be 1200 + 24 = 1224 for *k* = 12.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 。因此，特征图在深层中会迅速增长，导致计算变得缓慢。例如，对于一个101层的网络，当*k* = 12时，这个数值为1200 + 24 = 1224。
- en: Secondly, similar to ResNet, as the network gets deeper the feature maps size
    will be reduced to increase the coverage of the kernel. If DenseNet uses concatenation
    in the merge operation, it must reconcile the differences in size.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，与ResNet类似，随着网络深度的增加，特征图的尺寸会被缩小，以增加卷积核的覆盖范围。如果DenseNet在合并操作中使用了拼接，它必须解决尺寸不匹配的问题。
- en: To prevent the number of feature maps from increasing to the point of being
    computationally inefficient, DenseNet introduced the Bottleneck layer as shown
    in *Figure 2.4.2*. The idea is that after every concatenation; a 1 × 1 convolution
    with a filter size equal to 4*k* is now applied. This dimensionality reduction
    technique prevents the number of feature maps to be processed by `Conv2D(3)` from rapidly
    increasing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止特征图数量增加到计算效率低下的程度，DenseNet引入了瓶颈层，如*图2.4.2*所示。其思路是，每次拼接后，应用一个大小为4*k*的1 ×
    1卷积。这种维度减少技术可以防止由`Conv2D(3)`处理的特征图数量迅速增加。
- en: The Bottleneck layer then modifies the DenseNet layer as `BN-ReLU-Conv2D(1)-BN-ReLU-Conv2D(3)`,
    instead of just `BN-ReLU-Conv2D(3)`. We've included the kernel size as an argument
    of `Conv2D` for clarity. With the Bottleneck layer, every `Conv2D(3)` is processing
    just the 4*k* feature maps instead of
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，瓶颈层将DenseNet层修改为`BN-ReLU-Conv2D(1)-BN-ReLU-Conv2D(3)`，而不仅仅是`BN-ReLU-Conv2D(3)`。为了清晰起见，我们在`Conv2D`中包含了卷积核大小作为参数。使用瓶颈层后，每个`Conv2D(3)`只处理4*k*个特征图，而不是
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_026.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络（DenseNet）](img/B08956_02_026.jpg)'
- en: 'for layer *l*. For example, for the 101-layer network, the input to the last
    `Conv2D(3)` is still 48 feature maps for *k* = 12 instead of 1224 as computed
    previously:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于层*l*。例如，对于101层的网络，最后一个`Conv2D(3)`的输入仍然是48个特征图，当*k* = 12时，而不是之前计算的1224：
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_13.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络（DenseNet）](img/B08956_02_13.jpg)'
- en: 'Figure 2.4.3: The transition layer in between two Dense blocks'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4.3：两个Dense块之间的过渡层
- en: To solve the problem in feature maps size mismatch, DenseNet divides a deep
    network into multiple dense blocks that are joined together by transition layers
    as shown in the preceding figure. Within each dense block, the feature map size
    (that is, width and height) will remain constant.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决特征图尺寸不匹配的问题，DenseNet将深度网络划分为多个密集块，这些密集块通过过渡层连接，如前图所示。在每个密集块内，特征图的尺寸（即宽度和高度）保持不变。
- en: The role of the transition layer is to *transition* from one feature map size
    to a smaller feature map size between two dense blocks. The reduction in size
    is usually half. This is accomplished by the average pooling layer. For example,
    an `AveragePooling2D` with default `pool_size=2` reduces the size from (64, 64,
    256) to (32, 32, 256). The input to the transition layer is the output of the
    last concatenation layer in the previous dense block.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 过渡层的作用是*过渡*从一个特征图尺寸到另一个较小的特征图尺寸，位于两个密集块之间。尺寸的减小通常是原来的一半。这个过程是通过平均池化层完成的。例如，默认`pool_size=2`的`AveragePooling2D`将尺寸从(64,
    64, 256)减少到(32, 32, 256)。过渡层的输入是上一个密集块中最后一个拼接层的输出。
- en: However, before the feature maps are passed to average pooling, their number
    will be reduced by a certain compression factor,
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在将特征图传递给平均池化之前，它们的数量将通过一定的压缩因子减少，
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_027.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络（DenseNet）](img/B08956_02_027.jpg)'
- en: ', using `Conv2D(1)`. DenseNet uses'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ，使用`Conv2D(1)`。DenseNet使用
- en: '![Densely connected convolutional networks (DenseNet)](img/B08956_02_028.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接卷积网络（DenseNet）](img/B08956_02_028.jpg)'
- en: in their experiment. For example, if the output of the last concatenation of
    the previous dense block is (64, 64, 512), then after `Conv2D(1)` the new dimensions
    of the feature maps will be (64, 64, 256). When compression and dimensionality
    reduction are put together, the transition layer is made of `BN-Conv2D(1)-AveragePooling2D`
    layers. In practice, batch normalization precedes the convolutional layer.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中。例如，如果上一个密集块的最后一次连接的输出是（64, 64, 512），那么在`Conv2D(1)`后，特征图的新维度将是（64, 64,
    256）。当压缩和维度降低结合在一起时，过渡层由`BN-Conv2D(1)-AveragePooling2D`层组成。在实际中，批量归一化位于卷积层之前。
- en: Building a 100-layer DenseNet-BC for CIFAR10
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个100层DenseNet-BC用于CIFAR10
- en: We're now going to build a **DenseNet-BC** (**Bottleneck-Compression**) with
    100 layers for the CIFAR10 dataset, using the design principles that we discussed
    above.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将构建一个用于CIFAR10数据集的**DenseNet-BC**（**瓶颈压缩**）模型，具有100层，使用我们之前讨论的设计原则。
- en: Following table, shows the model configuration, while *Figure 2.4.3* shows the
    model architecture. *Listing* *2.4.1* shows us the partial Keras implementation
    of DenseNet-BC with 100 layers. We need to take note that we use `RMSprop` since
    it converges better than SGD or Adam when using DenseNet.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了模型配置，*图2.4.3*展示了模型架构。*列表* *2.4.1*展示了100层DenseNet-BC的部分Keras实现。我们需要注意的是，我们使用`RMSprop`，因为它在使用DenseNet时比SGD或Adam收敛得更好。
- en: '| Layers | Output Size | DenseNet-100 BC |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 输出大小 | DenseNet-100 BC |'
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Convolution | 32 x 32 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_029.jpg)
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 | 32 x 32 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_029.jpg) |'
- en: '| Dense Block(1) | 32 x 32 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_030.jpg)
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 密集块(1) | 32 x 32 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_030.jpg)
    |'
- en: '| Transition Layer(1) | 32 x 32 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_031.jpg)
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 过渡层(1) | 32 x 32 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_031.jpg)
    |'
- en: '| 16 x 16 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 16 x 16 |'
- en: '| Dense Block(2) | 16 x 16 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_032.jpg)
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 密集块(2) | 16 x 16 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_032.jpg)
    |'
- en: '| Transition Layer(2) | 16 x 16 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_033.jpg)
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 过渡层(2) | 16 x 16 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_033.jpg)
    |'
- en: '| 8 x 8 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 8 x 8 |'
- en: '| Dense Block(3) | 8 x 8 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_034.jpg)
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 密集块(3) | 8 x 8 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_034.jpg) |'
- en: '| Average Pooling | 1 x 1 | ![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_035.jpg)
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 平均池化 | 1 x 1 | ![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_035.jpg) |'
- en: '| Classification Layer |   | `Flatten-Dense(10)-softmax` |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 分类层 |   | `Flatten-Dense(10)-softmax` |'
- en: 'Table 2.4.1: DenseNet-BC with 100 layers for CIFAR10 classification'
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表2.4.1：用于CIFAR10分类的100层DenseNet-BC
- en: '![Building a 100-layer DenseNet-BC for CIFAR10](img/B08956_02_14.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个100层DenseNet-BC用于CIFAR10](img/B08956_02_14.jpg)'
- en: 'Figure 2.4.3: Model architecture of DenseNet-BC with 100 layers for CIFAR10
    classification'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4.3：用于CIFAR10分类的100层DenseNet-BC模型架构
- en: 'Listing 2.4.1, `densenet-cifar10-2.4.1.py`: Partial Keras implementation of DenseNet-BC
    with 100 layers as shown in *Table 2.4.1*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4.1，`densenet-cifar10-2.4.1.py`：如*表2.4.1*所示，100层DenseNet-BC的部分Keras实现：
- en: '[PRE8]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training the Keras implementation in *Listing* *2.4.1* for 200 epochs achieves
    a 93.74% accuracy vs. the 95.49% as reported in the paper. Data augmentation is
    used. We used the same callback functions in ResNet v1/v2 for DenseNet.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在*列表* *2.4.1*中的Keras实现训练200个epochs后，准确率为93.74%，而论文中报告的是95.49%。使用了数据增强。我们在DenseNet中使用了与ResNet
    v1/v2相同的回调函数。
- en: For the deeper layers, the `growth_rate` and `depth` variables must be changed
    using the table on the Python code. However, it will take a substantial amount
    of time to train the network at a depth of 250, or 190 as done in the paper. To
    give us an idea of training time, each epoch runs for about an hour on a 1060Ti
    GPU. Though there is also an implementation of DenseNet in the Keras applications
    module, it was trained on ImageNet.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更深的层，需要使用Python代码中的表格更改`growth_rate`和`depth`变量。然而，训练一个深度为250或190的网络将需要相当长的时间，正如论文中所做的那样。为了给我们一个训练时间的概念，每个epoch大约需要在1060Ti
    GPU上运行一个小时。尽管Keras应用程序模块中也有DenseNet的实现，但它是在ImageNet上训练的。
- en: Conclusion
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we've presented Functional API as an advanced method for building
    complex deep neural network models using Keras. We also demonstrated how the Functional
    API could be used to build the multi-input-single-output Y-Network. This network,
    when compared to a single branch CNN network, archives better accuracy. For the
    rest of the book, we'll find the Functional API indispensable in building more
    complex and advanced models. For example, in the next chapter, the Functional
    API will enable us to build a modular encoder, decoder, and autoencoder.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了作为构建复杂深度神经网络模型的高级方法——功能性API，并展示了如何使用功能性API构建多输入单输出的Y型网络。与单一分支CNN网络相比，该网络的精度更高。接下来，本书中的其他章节，我们将发现功能性API在构建更复杂和高级模型时是不可或缺的。例如，在下一章，功能性API将帮助我们构建模块化的编码器、解码器和自编码器。
- en: We also spent a significant time exploring two important deep networks, ResNet
    and DenseNet. Both of these networks have been used not only in classification
    but also in other areas, such as segmentation, detection, tracking, generation,
    and visual/semantic understanding. We need to remember that it's more important
    that we understand the model design decisions in ResNet and DenseNet more closely
    than just following the original implementation. In that manner, we'll be able
    to use the key concepts of ResNet and DenseNet for our purposes.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还花费了大量时间探索两个重要的深度网络——ResNet 和 DenseNet。这两个网络不仅在分类中应用广泛，还应用于其他领域，如分割、检测、跟踪、生成以及视觉/语义理解。我们需要记住，理解ResNet和DenseNet中的模型设计决策，比单纯跟随原始实现更为重要。通过这种方式，我们能够将ResNet和DenseNet的关键概念应用于我们的实际需求。
- en: References
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Kaiming He and others. *Delving Deep into Rectifiers: Surpassing Human-Level
    Performance on ImageNet Classification*. Proceedings of the IEEE international
    conference on computer vision, 2015 ([https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&file=He_Delving_Deep_into_ICCV_2015_paper.pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&file=He_Delving_Deep_into_ICCV_2015_paper.pdf)).'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He 和其他人. *深入探讨整流器：超越人类级别的ImageNet分类性能*。IEEE国际计算机视觉会议论文集，2015（[https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&file=He_Delving_Deep_into_ICCV_2015_paper.pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&file=He_Delving_Deep_into_ICCV_2015_paper.pdf)）。
- en: Kaiming He and others. *Deep Residual Learning for Image Recognition*. Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2016a([http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.)).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He 和其他人. *用于图像识别的深度残差学习*。IEEE计算机视觉与模式识别会议论文集，2016a（[http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)）。
- en: Karen Simonyan and Andrew Zisserman. *Very Deep Convolutional Networks for Large-Scale
    Image Recognition*. ICLR, 2015([https://arxiv.org/pdf/1409.1556/](https://arxiv.org/pdf/1409.15)).
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karen Simonyan 和 Andrew Zisserman. *用于大规模图像识别的非常深的卷积网络*。ICLR，2015（[https://arxiv.org/pdf/1409.1556/](https://arxiv.org/pdf/1409.15)）。
- en: Kaiming He and others. *Identity Mappings in Deep Residual Networks*. European
    Conference on Computer Vision. Springer International Publishing, 2016b([https://arxiv.org/pdf/1603.05027.pdf](https://arxiv.org/pdf/1603.05027.)).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He 和其他人. *深度残差网络中的身份映射*。欧洲计算机视觉会议。Springer国际出版，2016b（[https://arxiv.org/pdf/1603.05027.pdf](https://arxiv.org/pdf/1603.05027.pdf)）。
- en: Gao Huang and others. *Densely Connected Convolutional Networks*. Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2017([http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)).
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gao Huang 和其他人. *密集连接卷积网络*。IEEE计算机视觉与模式识别会议论文集，2017（[http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)）。
- en: Saining Xie and others. *Aggregated Residual Transformations for Deep Neural
    Networks*. Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference
    on. IEEE, 2017([http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)).
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谢赛宁（Saining Xie）等人。*深度神经网络的聚合残差变换*。计算机视觉与模式识别（CVPR），2017年IEEE会议。IEEE，2017（[http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)）。
- en: 'Gustav Larsson, Michael Maire and Gregory Shakhnarovich. *Fractalnet: Ultra-Deep
    Neural Networks Without Residuals*. arXiv preprint arXiv:1605.07648, 2016 ([https://arxiv.org/pdf/1605.07648.pdf](https://arxiv.org/pdf/1605.07648.pdf)).'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '古斯塔夫·拉尔松（Gustav Larsson）、迈克尔·梅尔（Michael Maire）和格雷戈里·沙赫纳罗维奇（Gregory Shakhnarovich）。*Fractalnet:
    无残差的超深神经网络*。arXiv预印本 arXiv:1605.07648，2016年（[https://arxiv.org/pdf/1605.07648.pdf](https://arxiv.org/pdf/1605.07648.pdf)）。'
