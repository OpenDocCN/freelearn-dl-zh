- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Graph Deep Learning for Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图深度学习与自然语言处理
- en: Language, by its very nature, is inherently structured and relational. Words
    form sentences, and sentences form documents, which contain concepts that interlink
    in complex ways to convey meaning. Graph structures provide an ideal framework
    to capture these intricate relationships, going beyond the traditional models.
    By representing text as graphs, we can leverage the rich expressiveness of graph
    theory and the computational power of deep learning to tackle challenging **natural
    language processing** ( **NLP** ) problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 语言本身天生就是结构化的和关系性的。词语组成句子，句子组成文档，这些文档包含了通过复杂方式相互关联的概念，以传达意义。图结构提供了一个理想的框架来捕捉这些错综复杂的关系，超越传统模型。通过将文本表示为图，我们可以利用图论的丰富表达力和深度学习的计算能力来解决挑战性的**自然语言处理**（**NLP**）问题。
- en: In this chapter, we will delve into the fundamental concepts of graph representations
    in NLP, exploring various types of linguistic graphs such as dependency trees,
    co-occurrence networks, and knowledge graphs. We’ll then build upon this foundation
    to examine the architectures and mechanisms of **graph neural networks** ( **GNNs**
    ) that have been specifically adapted for language tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨NLP中图表示的基本概念，探索各种类型的语言图，如依存树、共现网络和知识图。然后，我们将在此基础上，研究专门为语言任务适配的**图神经网络**（**GNNs**）的架构和机制。
- en: 'We’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Graph structures in NLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图结构在NLP中的应用
- en: Graph-based text summarization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图的文本摘要
- en: '**Information extraction** ( **IE** ) using GNNs'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GNN的**信息提取**（**IE**）
- en: Graph-based **dialogue systems**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图的**对话系统**
- en: Graph structures in NLP
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图结构在NLP中的应用
- en: NLP has seen significant advancements in recent years, with graph-based approaches
    emerging as a powerful paradigm for representing and processing linguistic information.
    In this section, we introduce the concept of graph structures in NLP, highlighting
    their importance and exploring various types of linguistic graphs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）近年来取得了显著进展，基于图的方法作为一种强大的范式，开始在表示和处理语言信息方面崭露头角。在这一部分，我们介绍了NLP中的图结构概念，强调它们的重要性，并探讨了各种类型的语言图。
- en: Importance of graph representations in language
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图表示在语言中的重要性
- en: 'Graph representations play a crucial role in capturing the inherent structure
    and relationships within language. They offer several advantages over traditional
    sequential or **bag-of-word** modeling, which is a simple text representation
    technique that converts a document into a vector by counting the frequency of
    each word, disregarding grammar and word order:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示在捕捉语言中的内在结构和关系方面起着至关重要的作用。它们相较于传统的顺序或**词袋模型**，提供了多个优势。词袋模型是一种简单的文本表示技术，它通过计算每个词的频率将文档转换为向量，忽略语法和词序：
- en: '**Structural information** : Graphs can explicitly represent the hierarchical
    and relational nature of language, preserving important linguistic structures
    that may be lost in linear representations.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构信息**：图结构可以明确表示语言的层级性和关系性，保留可能在线性表示中丢失的重要语言结构。'
- en: '**Contextual relationships** : By connecting related elements, graphs capture
    long-range dependencies and contextual information more effectively than sequential
    models.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文关系**：通过连接相关元素，图能够比顺序模型更有效地捕捉远程依赖和上下文信息。'
- en: '**Flexibility** : Graph structures can represent various levels of linguistic
    information, from word-level relationships to document-level connections and even
    cross-document links.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：图结构可以表示不同层次的语言信息，从词级关系到文档级连接，甚至跨文档的链接。'
- en: '**Interpretability** : Graph representations often align with human intuition
    about language structure, making them more interpretable and analyzable.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：图表示通常与人类对语言结构的直观理解相符合，使得它们更具可解释性和可分析性。'
- en: '**Integration of external knowledge** : Graphs facilitate the incorporation
    of external knowledge sources, such as ontologies or knowledge bases, into NLP
    models.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部知识的整合**：图结构有助于将外部知识源（如本体论或知识库）整合进NLP模型。'
- en: '**Multi-modal integration** : Graph structures can naturally represent relationships
    between different modalities, such as text, images, and speech.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态整合**：图结构可以自然地表示不同模态之间的关系，例如文本、图像和语音。'
- en: Types of linguistic graphs
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言图的类型
- en: 'Several types of graph structures are commonly used in NLP, each capturing
    different aspects of linguistic information:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，常用几种图结构，每种结构捕捉语言信息的不同方面：
- en: '**Dependency trees** represent grammatical relationships between words in a
    sentence:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖树**表示句子中词语之间的语法关系：'
- en: '**Nodes** are words and **edges** indicate syntactic dependencies.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**是词语，**边**表示句法依赖关系。'
- en: '**Example** : In “ *The cat chased the mouse* ,” “ *chased* ” would be the
    root, with “ *cat* ” and “ *mouse* ” as its dependents. *Figure 8.1* illustrates
    a dependency tree representation of this sentence and shows how dependency trees
    represent grammatical relationships between words in a sentence:'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例**：在“*The cat chased the mouse*”中，根动词是“*chased*”，而“*cat*”和“*mouse*”是它的依赖词。*图
    8.1* 展示了这个句子的依赖树表示，并说明了依赖树如何表示句子中词语之间的语法关系：'
- en: '![Figure 8.1 – Dependency tree representation of the sentence “The cat chased
    the mouse”](img/B22118_08_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 句子“The cat chased the mouse”的依赖树表示](img/B22118_08_01.jpg)'
- en: Figure 8.1 – Dependency tree representation of the sentence “The cat chased
    the mouse”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 句子“The cat chased the mouse”的依赖树表示
- en: The diagram depicts “ *chased* ” as the root verb, with arrows indicating syntactic
    dependencies to other words such as “ *cat* ” (subject) and “ *mouse* ” (object).
    It also includes part-of-speech tags for each word, such as **DET** (determiner),
    **NOUN** , and **VERB** .
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图示将“*chased*”作为根动词，箭头表示与其他词的句法依赖关系，如“*cat*”（主语）和“*mouse*”（宾语）。它还包括每个词的词性标注，如**DET**（限定词）、**NOUN**（名词）和**VERB**（动词）。
- en: '**Co-occurrence graphs** capture word associations based on their co-occurrence
    in a corpus. They are useful for tasks such as word sense disambiguation and semantic
    similarity.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共现图**捕捉基于词语在语料库中共同出现的关联。它们在词义消歧和语义相似度等任务中非常有用。'
- en: '**Nodes** represent words and **edges** indicate how often they appear together.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**表示词汇，**边**表示它们共同出现的频率。'
- en: '**Example** : Let’s say we have the following sentences:'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例**：假设我们有以下句子：'
- en: “ *The cat and* *dog play.* ”
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*The cat and* *dog play.*”
- en: “ *The dog chases* *the cat.* ”
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*The dog chases* *the cat.*”
- en: “ *The cat sleeps on* *the mat.* ”
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*The cat sleeps on* *the mat.*”
- en: 'In this example, we’ll create a co-occurrence graph where words that appear
    in the same sentence are connected:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个共现图，其中出现在同一句子中的词语之间建立连接：
- en: '![Figure 8.2 – Word co-occurrence graph](img/B22118_08_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 词汇共现图](img/B22118_08_02.jpg)'
- en: Figure 8.2 – Word co-occurrence graph
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 词汇共现图
- en: '**Knowledge graphs** represent factual information and relationships between
    entities:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识图谱**表示事实信息以及实体之间的关系：'
- en: '**Nodes** are entities (e.g., people, places, concepts) and **edges** are relationships.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**是实体（例如，人、地点、概念），**边**是关系。'
- en: '**Example** : Some examples include ConceptNet, Wikidata, and domain-specific
    ontologies.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例**：一些例子包括ConceptNet、Wikidata和特定领域的本体。'
- en: '![Figure 8.3 – Movie industry knowledge graph](img/B22118_08_03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 电影行业知识图谱](img/B22118_08_03.jpg)'
- en: Figure 8.3 – Movie industry knowledge graph
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 电影行业知识图谱
- en: '*Figure 8* *.3* illustrates relationships between various elements of the film
    industry, including directors, movies, genres, and awards. The *nodes* represent
    entities such as filmmakers (e.g., **Christopher Nolan** , **Quentin Tarantino**
    ), films (e.g., **Interstellar** , **Pulp Fiction** ), genres (e.g., **sci-fi**
    , **crime** ), and awards (e.g., **Oscar** , **BAFTA** ). The connections between
    these nodes demonstrate the complex interplay of creative talent, film categories,
    and industry recognition, offering insights into the multifaceted nature of cinema
    and its key players.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.3* 展示了电影行业中各种元素之间的关系，包括导演、电影、类型和奖项。*节点*表示实体，如电影制片人（例如，**克里斯托弗·诺兰**、**昆汀·塔伦蒂诺**）、电影（例如，**星际穿越**、**低俗小说**）、类型（例如，**科幻**、**犯罪**）和奖项（例如，**奥斯卡**、**英国电影和电视艺术学院奖**）。这些节点之间的连接展示了创意人才、电影类别和行业认可之间复杂的相互作用，为电影及其关键人物的多面性提供了见解。'
- en: '**Semantic graphs** represent the meaning of sentences or documents. They are
    used in tasks such as semantic parsing and abstract meaning representation. **Semantic
    parsing** in NLP is the task of converting natural language expressions into formal,
    structured representations of their meaning. It involves mapping words and phrases
    to concepts, identifying relationships, and generating logical forms or executable
    code that capture the underlying semantics of the input text. This process enables
    machines to understand and act upon human language in a more precise and actionable
    manner.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义图** 表示句子或文档的含义。它们用于语义分析和抽象意义表示等任务。在自然语言处理中，**语义解析** 是将自然语言表达转换为其含义的形式化、结构化表示的任务。它涉及将词语和短语映射到概念，识别关系，并生成捕捉输入文本潜在语义的逻辑形式或可执行代码。这一过程使得机器能够更准确和可操作地理解和处理人类语言。'
- en: '**Nodes** can be concepts, events, or propositions, with **edges** showing
    semantic relationships.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点** 可以是概念、事件或命题，**边** 显示语义关系。'
- en: '**Example** : The semantic graph in *Figure 8.4* illustrates how the sentence
    “ *John read a book in the library* ” can be broken down into its constituent
    parts and relationships. The graph consists of nodes representing concepts and
    edges showing the semantic relationships between them:'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例**：*图 8.4* 中的语义图说明了句子 “*John read a book in the library*” 如何被分解成其组成部分和关系。图中的节点表示概念，边显示它们之间的语义关系：'
- en: '![Figure 8.4 – Semantic graphical representation](img/B22118_08_04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 语义图形表示](img/B22118_08_04.jpg)'
- en: Figure 8.4 – Semantic graphical representation
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 语义图形表示
- en: 'Specifically, the graph shows the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，该图显示了以下内容：
- en: “ *John* ” is connected to “ *Read* ” with the label **agent_of** , indicating
    John is the one performing the action.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*John*” 与 “*Read*” 通过 **agent_of** 标签连接，指示John是执行动作的人。
- en: “ *Book* ” is connected to “ *Read* ” with the label **object_of** , showing
    that the book is what’s being read.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*Book*” 与 “*Read*” 通过 **object_of** 标签连接，显示正在阅读的书籍。
- en: “ *Library* ” is connected to “ *Read* ” with the label **location_of** , indicating
    where the reading took place.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*Library*” 与 “*Read*” 通过 **location_of** 标签连接，指示阅读发生的地点。
- en: “ *John* ” is also connected to “ *Person* ” with an **is_a** relationship,
    classifying John as a person.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*John*” 也与 “*Person*” 通过 **is_a** 关系连接，将John归类为一个人。
- en: '**Discourse graphs** represent the structure of longer texts or conversations
    and are used in tasks such as dialogue understanding and text coherence analysis.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话图** 表示更长文本或对话的结构，用于对话理解和文本连贯性分析等任务。'
- en: '**Nodes** can be sentences or utterances, with **edges** showing discourse
    relations.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点** 可以是句子或话语，**边** 显示话语关系。'
- en: '**Example** : *Figure 8.5* depicts a discourse graph representing a simple
    conversation. The graph consists of seven nodes ( **U0** to **U6** ), each representing
    an utterance in the conversation, connected by directed edges that show the flow
    and relationships between the utterances. The edges are labeled with discourse
    relations such as **greeting-response** , **acknowledgment** , **question** ,
    **response** , and **acknowledgment-farewell** . The conversation begins with
    a greeting ( **U0** ), followed by a response ( **U1** ), which then branches
    out to an acknowledgment ( **U2** ) and a question ( **U3** ). The dialogue continues
    with further responses and concludes with a farewell ( **U6** ). The legend provides
    brief snippets of each utterance, giving context to the conversation flow:'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例**：*图 8.5* 描述了一个简单对话的语篇图，表示为一个简单对话。图中有七个节点（**U0** 到 **U6**），每个节点代表对话中的一句话，通过有向边连接，显示了话语之间的流动和关系。边上标有诸如
    **greeting-response**、**acknowledgment**、**question**、**response** 和 **acknowledgment-farewell**
    等话语关系。对话从问候开始（**U0**），接着是回应（**U1**），然后分支到承认（**U2**）和问题（**U3**）。对话继续以进一步的回应为特点，并以告别结束（**U6**）。图例提供了每个话语的简要片段，为对话流程提供上下文：'
- en: '![Figure 8.5 – Discourse graph: simple conversation](img/B22118_08_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 对话图：简单对话](img/B22118_08_05.jpg)'
- en: 'Figure 8.5 – Discourse graph: simple conversation'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 对话图：简单对话
- en: This visual representation effectively illustrates how discourse graphs can
    be used to analyze the structure, coherence, and progression of a conversation,
    making it a valuable tool for tasks such as dialogue understanding and text coherence
    analysis.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化表示有效地展示了话语图如何用于分析对话的结构、一致性和进展，使其成为对话理解和文本一致性分析等任务的有价值工具。
- en: Now, let’s examine a few real-world use cases of graph learning in NLP.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看图学习在自然语言处理中的一些实际应用场景。
- en: Graph-based text summarization
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图的文本摘要
- en: 'Graph-based approaches have become increasingly popular in text summarization
    due to their ability to capture complex relationships between textual elements.
    Here, we will explore two main categories of graph-based summarization: extractive
    and abstractive.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的方法在文本摘要中变得越来越流行，因为它们能够捕捉文本元素之间的复杂关系。在这里，我们将探讨两大类基于图的摘要方法：抽取式和生成式。
- en: Extractive summarization using graph centrality
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用图中心性进行抽取式摘要
- en: '**Extractive summarization** involves selecting and arranging the most important
    sentences from the original text to form a concise summary. Graph-based methods
    for extractive summarization typically follow these steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽取式摘要** 涉及从原始文本中选择和排列最重要的句子，以形成一个简明的摘要。基于图的抽取式摘要方法通常遵循以下步骤：'
- en: Construct a graph representation of the text.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建文本的图表示。
- en: Apply centrality measures to identify important nodes (sentences).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用中心性度量来识别重要的节点（句子）。
- en: Extract top-ranked sentences to form the summary.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取排名靠前的句子来构成摘要。
- en: Graph construction
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图构建
- en: 'The text is represented as a graph where *nodes* are sentences and *edges*
    represent similarities between sentences. Common similarity measures include the
    following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 文本被表示为一个图，其中 *节点* 是句子，*边* 表示句子之间的相似性。常见的相似性度量包括以下几种：
- en: Cosine similarity of **term frequency-inverse document frequency** ( **TF-IDF**
    ) vectors
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）向量的余弦相似性'
- en: '**Jaccard similarity** of word sets'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jaccard 相似性** 的词集'
- en: '**Semantic similarity** using word embeddings'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义相似性** 使用词嵌入'
- en: Centrality measures
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中心性度量
- en: 'Several **graph centrality** measures can be used to rank the importance of
    sentences:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种 **图中心性** 度量来对句子的 importance 进行排序：
- en: '**Degree centrality** measures the number of connections a node has.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度数中心性** 衡量节点的连接数。'
- en: '**Eigenvector centrality** considers the importance of neighboring nodes.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量中心性** 考虑邻居节点的重要性。'
- en: '**PageRank** is a variant of eigenvector centrality, originally used by Google
    for ranking web pages.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PageRank** 是特征向量中心性的一种变体，最初由 Google 用于对网页进行排名。'
- en: '**Hyperlink-Induced Topic Search** ( **HITS** ) computes hub and authority
    scores for nodes.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超链接诱导主题搜索**（**HITS**）计算节点的中心和权威分数。'
- en: Example – TextRank algorithm
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 – TextRank 算法
- en: '**TextRank** , proposed by Mihalcea and Tarau in 2004 ( [https://aclanthology.org/W04-3252/](https://aclanthology.org/W04-3252/)
    ), is a popular graph-based extractive summarization method inspired by PageRank.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**TextRank**，由 Mihalcea 和 Tarau 在 2004 年提出（[https://aclanthology.org/W04-3252/](https://aclanthology.org/W04-3252/)），是一种受
    PageRank 启发的流行图-based 抽取式摘要方法。'
- en: 'Let’s look at a simplified Python implementation:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简化的 Python 实现：
- en: 'First, we import the necessary libraries – **NetworkX** for graph operations,
    **NumPy** for numerical computations, and **scikit-learn** for TF-IDF vectorization
    and cosine similarity calculation:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库——**NetworkX** 用于图操作，**NumPy** 用于数值计算，**scikit-learn** 用于 TF-IDF
    向量化和余弦相似性计算：
- en: '[PRE0]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we define the **textrank** function, which takes a list of sentences
    and the number of top sentences to return. It creates a TF-IDF matrix from the
    sentences and computes a similarity matrix using cosine similarity:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义 **textrank** 函数，它接受一个句子列表和要返回的前几个句子的数量。它从句子中创建一个 TF-IDF 矩阵，并使用余弦相似性计算相似性矩阵：
- en: '[PRE1]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We create a graph from the similarity matrix and compute **pagerank** on this
    graph. Each sentence is a *node* , and the similarities are *edge weights* :'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从相似性矩阵中创建一个图，并在该图上计算 **pagerank**。每个句子是一个 *节点*，而相似性是 *边的权重*：
- en: '[PRE2]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we sort the sentences based on their **pagerank** scores and return
    the top **n** sentences as the summary:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们根据 **pagerank** 分数对句子进行排序，并返回前 **n** 个句子作为摘要：
- en: '[PRE3]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is an example usage of the **textrank** function. In this case, we define
    a sample text, split it into sentences, apply the **textrank** algorithm, and
    print the summary:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**textrank**函数的示例用法。在此案例中，我们定义了一个示例文本，将其拆分为句子，应用**textrank**算法，并打印摘要：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Abstractive summarization with graph-to-sequence models
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图到序列模型的抽象总结
- en: '**Abstractive summarization** aims to generate new sentences that capture the
    essence of the original text. Graph-to-sequence models have shown promising results
    in this area by leveraging the structural information of the input text.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽象总结**旨在生成新的句子，以捕捉原始文本的精髓。图到序列模型通过利用输入文本的结构信息，在这方面取得了有前景的结果。'
- en: 'For abstractive summarization, graphs are often constructed to represent more
    fine-grained relationships:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抽象总结，图通常被构建来表示更细粒度的关系：
- en: '**Nodes** : Words, phrases, or concepts'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：单词、短语或概念'
- en: '**Edges** : Syntactic dependencies, semantic relations, or co-occurrence'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边**：句法依赖关系、语义关系或共现'
- en: 'A typical graph-to-sequence model for abstractive summarization consists of
    the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的图到序列模型用于抽象总结，通常包括以下内容：
- en: A **graph encoder** uses GNNs (e.g., **graph convolutional networks** ( **GCNs**
    ) or **graph attention network** ( **GATs** )) to encode the input graph.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图编码器**使用GNN（例如，**图卷积网络**（**GCNs**）或**图注意力网络**（**GATs**））来编码输入图。'
- en: A **sequence decoder** generates the summary text, often using attention mechanisms
    to focus on relevant parts of the encoded graph.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列解码器**生成摘要文本，通常使用注意力机制来关注编码图的相关部分。'
- en: Abstract meaning representation (AMR)-to-text summarization
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抽象意义表示（AMR）到文本的总结
- en: '**Abstract meaning representation** ( **AMR** ) is a semantic graph representation
    of text. **AMR-to-text summarization** is an example of graph-to-sequence abstractive
    summarization. Let’s consider a conceptual example using **PyTorch** :'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽象意义表示**（**AMR**）是文本的语义图表示。**AMR到文本总结**是图到序列抽象总结的一个例子。让我们考虑一个使用**PyTorch**的概念性示例：'
- en: 'We import the necessary libraries – **PyTorch** for deep learning operations,
    **PyTorch Geometric** for GNNs, and specific modules for neural network layers
    and functions:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入了必要的库——**PyTorch**用于深度学习操作，**PyTorch Geometric**用于GNN，以及用于神经网络层和功能的特定模块：
- en: '[PRE5]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we define the **AMRToTextSummarizer** class, which is a neural network
    module. It initializes a GCN layer, a **gated recurrent unit** ( **GRU** ) layer,
    and a fully connected layer:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了**AMRToTextSummarizer**类，这是一个神经网络模块。它初始化了一个GCN层，一个**门控循环单元**（**GRU**）层，以及一个全连接层：
- en: '[PRE6]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is the forward pass of the network. We first apply graph convolution to
    encode the graph structure, then use a GRU for sequence decoding, and finally
    apply a fully connected layer to produce the output:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是网络的前向传播。我们首先应用图卷积来编码图结构，然后使用GRU进行序列解码，最后应用全连接层生成输出：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We set up the model parameters and create a sample graph for demonstration.
    Here, we define the dimensions for the input, hidden layer, and output and create
    a graph with three nodes:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置模型参数并创建一个示例图以进行演示。在这里，我们定义了输入、隐藏层和输出的维度，并创建了一个包含三个节点的图：
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we instantiate the model, run a forward pass with the sample data,
    and print the shape of the output logits. The output shape would be (1, 3, 10000),
    representing a batch size of 1, 3 nodes, and logits over a vocabulary of 10,000
    words:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们实例化模型，使用示例数据进行前向传播，并打印输出logits的形状。输出形状为（1，3，10000），表示批次大小为1，3个节点，以及10,000个词汇的logits：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This example demonstrates the basic structure of a graph-to-sequence model for
    abstractive summarization. In practice, more sophisticated architectures, attention
    mechanisms, and training procedures would be employed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例演示了一个图到序列模型用于抽象总结的基本结构。实际上，将采用更复杂的架构、注意力机制和训练过程。
- en: Information extraction (IE) using GNNs
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GNN的文本信息抽取（IE）
- en: IE is a crucial task in NLP that involves automatically extracting structured
    information from unstructured text. GNNs have shown promising results in this
    domain, particularly in event extraction and open IE. In this section, we explore
    how GNNs are applied to these IE tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 信息抽取（IE）是NLP中的一项关键任务，涉及从非结构化文本中自动提取结构化信息。GNN在该领域取得了有前景的成果，特别是在事件抽取和开放信息抽取（Open
    IE）方面。在本节中，我们探讨了GNN如何应用于这些IE任务。
- en: Event extraction
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件抽取
- en: '**Event extraction** is the task of identifying and categorizing events mentioned
    in text, along with their participants and attributes. GNNs have proven effective
    in this task due to their ability to capture complex relationships between entities
    and events in a document.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件提取**是识别和分类文本中提到的事件的任务，并确定其参与者和属性。由于GNN能够捕捉文档中实体和事件之间的复杂关系，因此在此任务中已被证明非常有效。'
- en: Graph construction for event extraction
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于事件提取的图构建
- en: In event extraction, a document is typically represented as a graph where **nodes**
    represent entities, events, and tokens, while **edges** represent various relationships
    such as syntactic dependencies, coreference links, and temporal order.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件提取中，文档通常被表示为图，其中**节点**表示实体、事件和标记，而**边**表示各种关系，如句法依赖、共指链和时间顺序。
- en: 'Consider the following sentence: “ *John Smith resigned as CEO of TechCorp*
    *on Monday* .”'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下句子：“*约翰·史密斯于周一辞去TechCorp首席执行官职务*。”
- en: 'The graph representation might include the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示可能包括以下内容：
- en: '**Nodes** : John Smith (person), TechCorp (organization), CEO (role), Monday
    (time), Resignation (event)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**: 约翰·史密斯（人名）、TechCorp（公司名）、首席执行官（职位）、周一（时间）、辞职（事件）'
- en: '**Edges** : (John Smith) -[AGENT]-> (Resignation), (Resignation) -[ROLE]->
    (CEO), (Resignation) -[ORG]-> (TechCorp), (Resignation) -[ TIME]-> (Monday)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边**:（约翰·史密斯） -[AGENT]->（辞职），（辞职） -[ROLE]->（首席执行官），（辞职） -[ORG]->（TechCorp），（辞职）
    -[TIME]->（周一）'
- en: GNN-based event extraction models
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于GNN的事件提取模型
- en: 'GNN models for event extraction typically involve the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GNN的事件提取模型通常包括以下内容：
- en: Encoding the initial node features using pre-trained word embeddings or contextual
    embeddings.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的词嵌入或上下文嵌入对初始节点特征进行编码。
- en: Applying multiple layers of graph convolution to propagate information across
    the graph.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过应用多层图卷积来传播图中的信息。
- en: Using the final node representations to classify events and their arguments.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最终的节点表示来分类事件及其论据。
- en: '*Figure 8* *.6* shows an example architecture:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8* *.6* 显示了一个示例架构：'
- en: '![Figure 8.6 – GNN-based event extraction model architecture and process flow](img/B22118_08_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 基于GNN的事件提取模型架构和流程](img/B22118_08_06.jpg)'
- en: Figure 8.6 – GNN-based event extraction model architecture and process flow
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 基于GNN的事件提取模型架构和流程
- en: A recent study ( [https://aclanthology.org/2021.acl-long.274/](https://aclanthology.org/2021.acl-long.274/)
    ) has shown that GNN-based models can outperform traditional sequence-based models,
    especially in capturing long-range dependencies and handling multiple events in
    a single document. Specifically, graph-based methods showed significant improvements
    in handling cross-sentence events and multiple event scenarios through a heterogeneous
    graph interaction network that captured global context and a *Tracker* module
    that modeled interdependencies between events. The model’s effectiveness was particularly
    evident when extracting events that involved many scattered arguments across different
    sentences, validating GNNs’ superior ability to capture long-range dependencies
    compared to traditional sequence models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一项最新研究（[https://aclanthology.org/2021.acl-long.274/](https://aclanthology.org/2021.acl-long.274/)）表明，基于GNN的模型在超越传统的基于序列的模型方面表现优越，尤其在捕捉长程依赖关系和处理单一文档中的多个事件时表现突出。具体来说，基于图的的方法在处理跨句子事件和多事件场景方面取得了显著的改进，这得益于一个异构图交互网络，该网络捕捉了全局上下文，并且有一个*追踪器*模块，模型了事件之间的相互依赖关系。该模型的有效性在提取涉及多个散布在不同句子中的论据的事件时尤为明显，这验证了GNN在捕捉长程依赖关系方面相比传统序列模型的优越性。
- en: Open IE
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放信息提取
- en: '**Open information extraction** ( **OpenIE** ) aims to extract relational triples
    (subject, relation, object) from text without being confined to a predefined set
    of relations. GNNs have been successfully applied to this task, leveraging the
    inherent graph-like structure of sentences.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放信息提取**（**OpenIE**）旨在从文本中提取关系三元组（主语、关系、宾语），而不受预定义关系集的限制。GNN已成功应用于此任务，利用了句子的图结构特性。'
- en: Graph-based OpenIE approach
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于图的开放信息提取方法
- en: In a graph-based OpenIE system, sentences are typically converted into dependency
    parse trees, which are then used as the input graph for a GNN.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于图的开放信息提取（OpenIE）系统中，句子通常被转换为依赖解析树，然后作为GNN的输入图。
- en: 'For example, in the sentence “ *Einstein developed the theory of relativity*
    ,” the dependency parse might look like the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在句子“*爱因斯坦提出了相对论*”中，依赖解析可能如下所示：
- en: '![Figure 8.7 – Graph-based OpenIE approach using dependency parsing](img/B22118_08_07.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 使用依赖解析的基于图的 OpenIE 方法](img/B22118_08_07.jpg)'
- en: Figure 8.7 – Graph-based OpenIE approach using dependency parsing
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 使用依赖解析的基于图的 OpenIE 方法
- en: GNN processing for OpenIE
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GNN 处理 OpenIE
- en: 'The GNN processes this graph in several steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 通过多个步骤处理图形：
- en: '**Node encoding** : Each word is encoded using contextual embeddings.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**节点编码**：每个单词使用上下文嵌入进行编码。'
- en: '**Graph convolution** : Information is propagated along the dependency edges.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图卷积**：信息沿依赖边传播。'
- en: '**Relation prediction** : The model predicts potential relations between pairs
    of nodes.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**关系预测**：模型预测节点对之间的潜在关系。'
- en: '**Triple formation** : Valid subject-relation-object triples are constructed
    based on the predictions.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**三元组形成**：基于预测构建有效的主语-关系-宾语三元组。'
- en: 'Using our previous example, the output would look like this: (Einstein, developed,
    theory of relativity).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前的示例，输出将如下所示：（爱因斯坦，开发，相对论）。
- en: Advantages of GNN-based IE
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 GNN 的信息提取的优势
- en: 'GNN-based approaches to IE offer several advantages:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GNN 的信息提取方法提供了几个优势：
- en: Capturing long-range dependencies that may be missed by sequential models
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕捉可能被顺序模型遗漏的长程依赖
- en: Integrating various types of linguistic information (syntactic, semantic, coreference)
    into a unified framework
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将各种类型的语言信息（句法、语义、指代）整合到一个统一的框架中
- en: Handling documents with complex structures, such as scientific papers or legal
    documents
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理具有复杂结构的文档，如科学论文或法律文档
- en: Future research in this area may focus on combining GNNs with other deep learning
    architectures, such as transformers, to create hybrid models that leverage the
    strengths of both approaches for more accurate and comprehensive information extraction.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域未来的研究可能会集中在将 GNN 与其他深度学习架构（如 transformers）结合，以创建融合两种方法优点的混合模型，从而实现更准确和全面的信息提取。
- en: Graph-based dialogue systems
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图的对话系统
- en: Dialogue systems are sophisticated AI-powered applications designed to facilitate
    human-computer interaction through natural language. These systems employ various
    NLP techniques such as natural language understanding, dialogue management, and
    natural language generation to interpret user inputs, maintain context, and produce
    appropriate responses. Modern dialogue systems often integrate machine learning
    algorithms to improve their performance over time, adapting to user preferences
    and learning from past interactions. They find applications in diverse fields,
    including customer service, virtual assistants, educational tools, and interactive
    storytelling, continuously evolving to provide more natural and effective communication
    between humans and machines.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统是复杂的人工智能应用，旨在通过自然语言促进人机互动。这些系统采用各种 NLP 技术，如自然语言理解、对话管理和自然语言生成，来解释用户输入、维护上下文并生成适当的回应。现代对话系统通常结合机器学习算法，以提高其性能，并根据用户偏好和过去的互动进行自我学习和调整。它们在多个领域中都有应用，包括客户服务、虚拟助手、教育工具和互动故事讲述，不断发展以提供更自然和高效的人机沟通。
- en: Graph-based approaches have shown significant promise in enhancing the performance
    and capabilities of dialogue systems. By leveraging graph structures to represent
    dialogue context, knowledge, and semantic relationships, these systems can better
    understand user intents, track conversation states, and generate more coherent
    and contextually appropriate responses.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的方法在提升对话系统的性能和能力方面展现了显著的前景。通过利用图结构来表示对话上下文、知识和语义关系，这些系统能够更好地理解用户意图、跟踪对话状态，并生成更连贯、上下文适当的回应。
- en: Dialogue state tracking with GNNs
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GNN 进行对话状态跟踪
- en: '**Dialogue state tracking** ( **DST** ) is a crucial component of task-oriented
    dialogue systems, responsible for maintaining an up-to-date representation of
    the user’s goals and preferences throughout the conversation. GNNs have been successfully
    applied to improve the accuracy and robustness of DST.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**对话状态跟踪**（**DST**）是面向任务的对话系统中的一个关键组件，负责在整个对话过程中保持用户目标和偏好的最新表示。GNN 已成功应用于提高
    DST 的准确性和鲁棒性。'
- en: In a typical GNN-based DST approach, the dialogue history is represented as
    a graph, where *nodes* represent utterances, slots, and values, while *edges*
    capture the relationships between these elements. As the conversation progresses,
    the graph is dynamically updated, and GNN layers are applied to propagate information
    across the graph, enabling more accurate state predictions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的基于GNN的DST方法中，对话历史被表示为图形，其中*节点*代表话语、槽位和数值，而*边*则捕捉这些元素之间的关系。随着对话的进行，图形会动态更新，GNN层被应用于在图形中传播信息，从而实现更准确的状态预测。
- en: For example, the **graph state tracker** ( **GST** ) proposed by Chen et al.
    in 2020 ( [https://doi.org/10.1609/aaai.v34i05.6250](https://doi.org/10.1609/aaai.v34i05.6250)
    ) uses a GAT to model the dependencies between different dialogue elements. This
    approach has shown superior performance on benchmark datasets such as MultiWOZ,
    particularly in handling complex multi-domain conversations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Chen等人于2020年提出的**图状态追踪器**（**GST**）( [https://doi.org/10.1609/aaai.v34i05.6250](https://doi.org/10.1609/aaai.v34i05.6250)
    )使用GAT来建模不同对话元素之间的依赖关系。该方法在基准数据集如MultiWOZ上表现出了优越的性能，特别是在处理复杂的多领域对话时。
- en: Graph-enhanced response generation
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于图的响应生成
- en: Graph structures can also significantly improve the quality and relevance of
    generated responses in both task-oriented and open-domain dialogue systems. By
    incorporating knowledge graphs or conversation flow graphs, these systems can
    produce more informative, coherent, and contextually appropriate responses.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图结构还可以显著提高生成响应的质量和相关性，无论是在任务导向型还是开放领域的对话系统中。通过结合知识图谱或对话流图，这些系统可以生成更具信息性、连贯性和上下文适宜性的响应。
- en: One approach is to use graph-to-sequence models, where the input dialogue context
    is first converted into a graph representation, and then a graph-aware decoder
    generates the response. This allows the model to capture long-range dependencies
    and complex relationships within the conversation history.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用图到序列模型，其中输入的对话上下文首先被转换为图形表示，然后图感知解码器生成响应。这使得模型能够捕捉对话历史中的长程依赖关系和复杂关系。
- en: For instance, the **GraphDialog** model introduced by Yang et al. in 2021 (
    [https://doi.org/10.18653/v1/2020.emnlp-main.147](https://doi.org/10.18653/v1/2020.emnlp-main.147)
    ) constructs a dialogue graph that captures both the local context (recent utterances)
    and global context (overall conversation flow). The model then uses graph attention
    mechanisms to generate responses that are more consistent with the entire conversation
    history. This approach represents conversations as structured graphs where *nodes*
    represent utterances and *edges* capture various types of relationships between
    them, such as temporal sequence and semantic similarity. The graph structure allows
    the model to better understand long-range dependencies and thematic connections
    across the dialogue, moving beyond the limitations of traditional sequential models.
    Furthermore, the graph attention mechanism helps the model focus on relevant historical
    context when generating responses, even if it occurred many turns earlier in the
    conversation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Yang等人于2021年提出的**GraphDialog**模型( [https://doi.org/10.18653/v1/2020.emnlp-main.147](https://doi.org/10.18653/v1/2020.emnlp-main.147)
    )构建了一个对话图，捕捉了局部上下文（最近的话语）和全局上下文（整体对话流程）。该模型随后使用图注意力机制生成更符合整个对话历史的响应。该方法将对话表示为结构化的图，其中*节点*代表话语，*边*捕捉它们之间的各种关系，如时间序列和语义相似性。图结构使得模型能够更好地理解对话中的长程依赖关系和主题联系，超越了传统顺序模型的局限性。此外，图注意力机制帮助模型在生成响应时专注于相关的历史上下文，即使它发生在对话的许多轮之前。
- en: This architecture has shown particular effectiveness in maintaining coherence
    during extended conversations and handling complex multi-topic dialogues where
    context from different parts of the conversation needs to be integrated.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构在维持长时间对话中的连贯性以及处理需要整合对话不同部分上下文的复杂多主题对话中表现出了特别的效果。
- en: Knowledge-grounded conversations using graphs
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用图的知识驱动对话
- en: Incorporating external knowledge into dialogue systems is crucial for generating
    informative and engaging responses. Graph-based approaches offer an effective
    way to represent and utilize large-scale knowledge bases in conversation models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将外部知识纳入对话系统对于生成信息丰富且引人入胜的响应至关重要。基于图的方法提供了一种有效的方式，在对话模型中表示和利用大规模知识库。
- en: 'Knowledge graphs can be integrated into dialogue systems in several ways:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱可以通过多种方式集成到对话系统中：
- en: '**As a source of factual information** : The system can query the knowledge
    graph to retrieve relevant facts and incorporate them into responses.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为事实信息源**：系统可以查询知识图谱以检索相关事实，并将其融入响应中。'
- en: '**For entity linking and disambiguation** : Graph structures can help resolve
    ambiguities and link mentions in the conversation to specific entities in the
    knowledge base.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于实体链接和消歧**：图结构可以帮助解决歧义，并将对话中的提及链接到知识库中的特定实体。'
- en: '**To guide response generation** : The graph structure can inform the generation
    process, ensuring that the produced responses are consistent with the known facts
    and relationships.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引导响应生成**：图结构可以为生成过程提供指导，确保生成的响应与已知事实和关系一致。'
- en: An example of this approach is the **Knowledge-Aware Graph-Enhanced GPT-2**
    ( **KG-GPT2** ) model proposed by W Lin et al. ( [https://doi.org/10.48550/arXiv.2104.04466](https://doi.org/10.48550/arXiv.2104.04466)
    ). This model incorporates a knowledge graph into a pre-trained language model,
    allowing it to generate more informative and factually correct responses in open-domain
    conversations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个例子是**知识感知图增强GPT-2**（**KG-GPT2**）模型，该模型由W Lin等人提出（[https://doi.org/10.48550/arXiv.2104.04466](https://doi.org/10.48550/arXiv.2104.04466)）。该模型将知识图谱集成到预训练的语言模型中，使其能够在开放领域的对话中生成更有信息量且事实正确的响应。
- en: Imagine you’re using a virtual assistant to plan a trip to London. You start
    by asking about hotels, then restaurants, and finally transportation. A traditional
    GPT-2-based system might struggle to connect related information across these
    different domains. For instance, if you mention wanting a “ *luxury hotel in central
    London* ” and later ask about “ *restaurants near my hotel* ,” the system needs
    to understand that you’re looking for high-end restaurants in central London.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在使用虚拟助手规划伦敦之行。你首先询问关于酒店的信息，然后是餐厅，最后是交通工具。基于传统的GPT-2系统，可能难以在这些不同领域之间连接相关信息。例如，如果你提到想要“*伦敦市中心的豪华酒店*”，然后又询问“*我酒店附近的餐厅*”，系统需要理解你是在寻找伦敦市中心的高档餐厅。
- en: 'The proposed model in the aforementioned paper solves this by using graph networks
    to create connections between related information. It works in three steps:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 上述论文中提出的模型通过使用图网络在相关信息之间建立连接来解决这个问题。它分为三个步骤：
- en: First, it processes your conversation using GPT-2 to understand the context.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它通过GPT-2处理你的对话以理解上下文。
- en: Then, it uses GATs to connect related information (such as location, price range,
    etc.) across different services.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它使用图注意力网络（GATs）来连接不同服务之间的相关信息（如位置、价格范围等）。
- en: Finally, it uses this enhanced understanding to make better predictions about
    what you want.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，它利用这种增强的理解来更好地预测你想要的内容。
- en: The researchers found this approach particularly effective when dealing with
    limited training data. In real terms, this means the system could learn to make
    good recommendations even if it hasn’t seen many similar conversations before.
    For example, if it learns that people booking luxury hotels typically also book
    high-end restaurants and premium taxis, it can apply this pattern to new conversations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，当面对有限的训练数据时，这种方法尤其有效。换句话说，这意味着系统即使没有见过许多类似的对话，也能够学会做出良好的推荐。例如，如果它学到预订豪华酒店的人通常也会预订高档餐厅和高级出租车，它可以将这种模式应用到新的对话中。
- en: Their approach showed significant improvements over existing systems, especially
    in understanding relationships between different services (such as hotels and
    restaurants) and maintaining consistency throughout the conversation. This makes
    the system more natural and efficient for real-world applications such as travel
    booking or restaurant reservation systems.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的方法在现有系统上显示了显著的改进，尤其是在理解不同服务（如酒店和餐厅）之间的关系并保持对话的一致性方面。这使得该系统在实际应用中（如旅行预订或餐厅预定系统）更加自然和高效。
- en: Graph-based dialogue policy learning
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于图的对话策略学习
- en: In task-oriented dialogue systems, graph structures can also be leveraged to
    improve dialogue policy learning. By representing the dialogue state, action space,
    and task structure as a graph, reinforcement-learning algorithms can more effectively
    explore and exploit the action space.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务导向的对话系统中，图结构也可以被用来改善对话策略学习。通过将对话状态、动作空间和任务结构表示为图，强化学习算法能够更有效地探索和利用动作空间。
- en: For example, the **Graph-Based Dialogue Policy** ( **GDP** ) framework introduced
    by Chen et al. in 2021 ( [https://aclanthology.org/C18-1107](https://aclanthology.org/C18-1107)
    ) uses a GNN to model the relationships between different dialogue states and
    actions. This approach enables more efficient policy learning, especially in complex
    multi-domain scenarios.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2021年陈等人提出的**基于图的对话策略**（**GDP**）框架（[https://aclanthology.org/C18-1107](https://aclanthology.org/C18-1107)）使用GNN来建模不同对话状态和行动之间的关系。该方法可以更高效地进行策略学习，特别是在复杂的多领域场景中。
- en: There is an underlying scalability problem with using graphs for language understanding
    related to nonlinear memory complexity. The quadratic memory complexity issue
    in graph-based NLP arises because when converting text into a fully connected
    graph, each token/word needs to be connected to every other token, resulting in
    ![<mml:math  ><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>](img/215.png)
    connections where ![<mml:math  ><mml:mi>n</mml:mi></mml:math>](img/216.png) is
    the sequence length.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图形进行语言理解存在一个潜在的可扩展性问题，这与非线性记忆复杂度相关。基于图的自然语言处理中的二次记忆复杂度问题源于，当将文本转换为一个完全连接的图时，每个标记/词语都需要与其他每个标记连接，从而导致![<mml:math  ><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>](img/215.png)的连接，其中![<mml:math  ><mml:mi>n</mml:mi></mml:math>](img/216.png)是序列长度。
- en: For example, in a 1,000-word document, 1 million edges must be stored in memory.
    This becomes particularly problematic with transformer-like architectures where
    each connection also stores attention weights and edge features. Modern NLP tasks
    often deal with much longer sequences or multiple documents simultaneously, making
    this quadratic scaling unsustainable for both memory usage and computational resources.
    Common mitigation strategies include sparse attention mechanisms, hierarchical
    graph structures, and sliding window approaches, but these can potentially lose
    important long-range dependencies in the text. Please refer to [*Chapter 5*](B22118_05.xhtml#_idTextAnchor093)
    for a more in-depth discussion of approaches to the issue of scalability.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一份1,000字的文档中，必须在内存中存储100万个边。对于类似于Transformer的架构，这尤其成问题，因为每个连接还存储了注意力权重和边特征。现代自然语言处理任务通常需要处理更长的序列或同时处理多个文档，这使得这种二次扩展在内存使用和计算资源方面变得不可持续。常见的缓解策略包括稀疏注意力机制、层次图结构和滑动窗口方法，但这些方法可能会丧失文本中的重要长程依赖性。有关可扩展性问题的更深入讨论，请参见[*第5章*](B22118_05.xhtml#_idTextAnchor093)。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a wide range of topics, starting with the fundamental
    concepts of graph representations in NLP and progressing through various applications.
    These applications include graph-based text summarization, IE using GNNs, OpenIE,
    mapping natural language to logic, question answering over knowledge graphs, and
    graph-based dialogue systems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了广泛的主题，从自然语言处理中的图表示基础概念开始，逐步深入到各种应用。包括基于图的文本摘要、使用GNN的信息抽取（IE）、开放信息抽取（OpenIE）、将自然语言映射到逻辑、基于知识图谱的问答和基于图的对话系统。
- en: You learned that graph-based approaches offer powerful tools for enhancing various
    aspects of dialogue systems, from state tracking to response generation and policy
    learning. As research in this area continues to advance, we can expect to see
    even more sophisticated and capable dialogue systems that leverage the rich structural
    information provided by graph representations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，基于图的方法为增强对话系统的各个方面提供了强大的工具，从状态跟踪到响应生成和策略学习。随着该领域研究的不断深入，我们可以期待看到更多利用图表示提供的丰富结构信息的更复杂、更强大的对话系统。
- en: In the next chapter, we will go through some of the very common use cases of
    graph learning around recommendation systems.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨图学习在推荐系统中的一些非常常见的应用场景。
