- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Understanding Sentiment in Natural Language with BiLSTMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BiLSTM理解自然语言中的情感
- en: '**Natural Language Understanding** (**NLU**) is a significant subfield of **Natural
    Language Processing** (**NLP**). In the last decade, there has been a resurgence
    of interest in this field with the dramatic success of chatbots such as Amazon''s
    Alexa and Apple''s Siri. This chapter will introduce the broad area of NLU and
    its main applications.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言理解**（**NLU**）是**自然语言处理**（**NLP**）的一个重要子领域。在过去十年中，随着亚马逊Alexa和苹果Siri等聊天机器人取得戏剧性成功，这一领域的兴趣再次激增。本章将介绍NLU的广泛领域及其主要应用。'
- en: Specific model architectures called **Recurrent Neural Networks** (**RNNs**),
    with special units called **Long Short-Term Memory** (**LSTM**) units, have been
    developed to make the task of understanding natural language easier. LSTMs in
    NLP are analogous to convolution blocks in computer vision. We will take two examples
    to build models that can understand natural language. Our first example is understanding
    the sentiment of movie reviews. This will be the focus of this chapter. The other
    example is one of the fundamental building blocks of NLU, **Named Entity Recognition**
    (**NER**). That will be the main focus of the next chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特定的模型架构叫做**递归神经网络**（**RNN**），其中包含称为**长短期记忆**（**LSTM**）单元的特殊单元，旨在使自然语言理解任务更加容易。NLP中的LSTM类似于计算机视觉中的卷积块。我们将以两个例子来构建可以理解自然语言的模型。第一个例子是理解电影评论的情感，这将是本章的重点。另一个例子是自然语言理解的基本构建块之一，**命名实体识别**（**NER**）。这将是下一章的主要内容。
- en: 'Building models capable of understanding sentiments requires the use of **Bi-Directional
    LSTMs** (**BiLSTMs**) in addition to the use of techniques from *Chapter 1*, *Essentials
    of NLP*. Specifically, the following will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 构建能够理解情感的模型需要使用**双向LSTM**（**BiLSTM**），以及*第1章*中介绍的*自然语言处理基础*技术。具体来说，本章将涵盖以下内容：
- en: Overview of NLU and its applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言理解（NLU）及其应用概述
- en: Overview of RNNs and BiRNNS using LSTMs and BiLSTMS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM和BiLSTM的RNN和BiRNN的概述
- en: Analyzing the sentiment of movie reviews with LSTMs and BiLSTMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM和BiLSTM分析电影评论的情感
- en: Using `tf.data` and the TensorFlow Datasets package to manage the loading of
    data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tf.data`和TensorFlow Datasets包来管理数据加载
- en: Optimizing the performance of data loading for effective utilization of the
    CPU and GPU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数据加载的性能，以有效利用CPU和GPU
- en: We will start with a quick overview of NLU and then get right into BiLSTMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从自然语言理解（NLU）的快速概述开始，然后直接进入BiLSTM。
- en: Natural language understanding
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: 'NLU enables the processing of unstructured text and extracts meaning and critical
    pieces of information that are actionable. Enabling a computer to understand sentences
    of text is a very hard challenge. One aspect of NLU is understanding the meaning
    of sentences. Sentiment analysis of a sentence becomes possible after understanding
    the sentence. Another useful application is the classification of sentences to
    a topic. This topic classification can also help in the disambiguation of entities.
    Consider the following sentence: "A CNN helps improve the accuracy of object recognition."
    Without understanding that this sentence is about machine learning, an incorrect
    inference may be made about the entity CNN. It may be interpreted as the news
    organization as opposed to a deep learning architecture used in computer vision.
    An example of a sentiment analysis model is built using a specific RNN architecture
    called BiLSTMs later in this chapter.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NLU使得能够处理非结构化文本并提取有意义和可操作的关键信息。让计算机理解文本句子是一个非常困难的挑战。NLU的一个方面是理解句子的意义。在理解句子后，情感分析就变得可能了。另一个有用的应用是将句子分类到某个主题。这种主题分类还可以帮助消除实体的歧义。考虑以下句子：“CNN有助于提高物体识别的准确性。”如果不了解这个句子是关于机器学习的，可能会对实体CNN做出错误推断。它可能被解释为新闻组织，而不是计算机视觉中的深度学习架构。本章稍后将使用特定的RNN架构——BiLSTM，构建一个情感分析模型。
- en: Another aspect of NLU is to extract information or commands from free-form text.
    This text can be sourced from converting speech, as spoken to Amazon's Echo device,
    for example, into text. Rapid advances in speech recognition now allow considering
    speech as equivalent to text. Extracting commands from the text, like an object
    and an action to perform, allows control of devices through voice commands. Consider
    the example sentence "Lower the volume." Here, the object is "volume" and the
    action is "lower." After extraction from text, these actions can be matched to
    a list of available actions and executed. This capability enables advanced **human-computer
    interaction** (**HCI**), allowing control of home appliances through voice commands.
    NER is used for detecting key tokens in sentences.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NLU 的另一个方面是从自由文本中提取信息或命令。这些文本可以来自将语音转换成文本，例如将说话内容转换为 Amazon Echo 设备所说的文本。语音识别技术的快速进展现已使得语音被视为等同于文本。从文本中提取命令，比如对象和要执行的动作，能够通过语音命令控制设备。考虑示例句子
    "调低音量。" 在这里，"音量" 是对象，"调低" 是动作。从文本中提取这些内容后，可以将这些动作与可用的动作列表匹配并执行。这种能力使得先进的 **人机交互**
    (**HCI**) 成为可能，从而能够通过语音命令控制家用电器。NER 用于检测句子中的关键词。
- en: This technique is incredibly useful in building form filling or slot filling
    chatbots. NER also forms the basis of other NLU techniques that perform tasks
    such as relation extraction. Consider the sentence "Sundar Pichai is the CEO of
    Google." In this sentence, what is the relationship between the entities "Sundar
    Pichai" and "Google"? The right answer is CEO. This is an example of relation
    extraction, and NER was used to identify the entities in the sentence. The focus
    of the next chapter is on NER using a specific architecture that has been quite
    effective in this space.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在构建表单填写或槽填充聊天机器人时非常有用。命名实体识别（NER）也是其他自然语言理解（NLU）技术的基础，这些技术执行诸如关系提取等任务。考虑句子
    "Sundar Pichai 是 Google 的 CEO。" 在这个句子中，"Sundar Pichai" 和 "Google" 这两个实体之间的关系是什么？正确答案是
    CEO。这是关系提取的一个例子，NER 用于识别句子中的实体。下一章的重点是使用特定架构的 NER，这在这一领域中已证明非常有效。
- en: A common building block of both sentiment analysis and NER models is Bi-directional
    RNN models. The next section describes BiLSTMs, which is Bi-directional RNN using
    LSTM units, prior to building a sentiment analysis model with it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析和 NER 模型的常见构建块是双向 RNN 模型。下一节将描述 BiLSTM，它是使用 LSTM 单元的双向 RNN，然后我们将用它来构建情感分析模型。
- en: Bi-directional LSTMs – BiLSTMs
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向 LSTM – BiLSTM
- en: LSTMs are one of the styles of recurrent neural networks, or RNNs. RNNs are
    built to handle sequences and learn the structure of them. An RNN does that by
    using the output generated after processing the previous item in the sequence
    along with the current item to generate the next output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是递归神经网络（RNN）的一种类型。RNN 是为处理序列并学习其结构而构建的。RNN 通过使用在处理序列中的前一个项目后生成的输出，与当前项目一起生成下一个输出。
- en: 'Mathematically, this can be expressed like so:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，可以这样表达：
- en: '![](img/B16252_02_001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_001.png)'
- en: This equation says that to compute the output at time *t*, the output at *t-1*
    is used as an input along with the input data *x*[t] at the same time step. Along
    with this, a set of parameters or learned weights, represented by ![](img/B16252_02_002.png),
    are also used in computing the output. The objective of training an RNN is to
    learn these weights ![](img/B16252_02_003.png) This particular formulation of
    an RNN is unique. In previous examples, we have not used the output of a batch
    to determine the output of a future batch. While we focus on applications of RNNs
    on language where a sentence is modeled as a sequence of words appearing one after
    the other, RNNs can be applied to build general time-series models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表示，要计算在时间 *t* 时刻的输出，*t-1* 时刻的输出会作为输入，与同一时间步的输入数据 *x*[t] 一起使用。除此之外，一组参数或学习得到的权重，表示为
    ![](img/B16252_02_002.png)，也用于计算输出。训练一个 RNN 的目标是学习这些权重 ![](img/B16252_02_003.png)。这种特定形式的
    RNN 是独特的。在之前的例子中，我们没有使用一个批次的输出去决定未来批次的输出。虽然我们主要关注 RNN 在语言上的应用，其中一个句子被建模为一个接一个出现的词序列，但
    RNN 也可以应用于构建通用的时间序列模型。
- en: RNN building blocks
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 构建块
- en: The previous section outlined the basic mathematical intuition of a recursive
    function that is a simplification of the RNN building block. *Figure 2.1* represents
    a few time steps and also adds details to show different weights used for computation
    for a basic RNN building block or cell.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前一部分概述了递归函数的基本数学直觉，这是 RNN 构建块的简化版本。*图 2.1* 表示了几个时间步骤，并添加了细节，展示了用于基本 RNN 构建块或单元计算的不同权重。
- en: '![](img/B16252_02_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_01.png)'
- en: 'Figure 2.1: RNN unraveled'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：RNN 解构
- en: 'The basic cell is shown on the left. The input vector at a specific time or
    sequence step *t* is multiplied by a weight vector, represented in the diagram
    as *U*, to generate an activation in the middle part. The key part of this architecture
    is the loop in this activation part. The output of a previous step is multiplied
    by a weight vector, denoted by *V* in the figure, and added to the activation.
    This activation can be multiplied by another weight vector, represented by *W*,
    to produce the output of that step shown at the top. In terms of sequence or time
    steps, this network can be unrolled. This unrolling is virtual. However, it is
    represented on the right side of the figure. Mathematically, activation at time
    step *t* can be represented by:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基本单元如左图所示。特定时间或序列步骤 *t* 的输入向量与一个权重向量相乘，图中表示为 *U*，以生成中间部分的激活。该架构的关键部分是该激活部分中的循环。前一个步骤的输出与一个权重向量相乘，图中用
    *V* 表示，并加到激活中。该激活可以与另一个权重向量相乘，表示为 *W*，以产生该步骤的输出，如顶部所示。就序列或时间步骤而言，该网络可以展开。这种展开是虚拟的。然而，它在图的右侧得到了表示。从数学角度看，时间步骤
    *t* 的激活可以表示为：
- en: '![](img/B16252_02_004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_004.png)'
- en: 'Output at the same step can be computed like so:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同一步骤的输出可以这样计算：
- en: '![](img/B16252_02_005.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_005.png)'
- en: The mathematics of RNNs has been simplified to provide intuition about RNNs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的数学已经简化，以便提供对 RNN 的直观理解。
- en: Structurally, the network is very simple as it is a single unit. To exploit
    and learn the structure of inputs passing through, weight vectors *U*, *V*, and
    *W* are shared across time steps. The network does not have layers as seen in
    fully connected or convolutional networks. However, as it is unrolled over time
    steps, it can be thought of as having as many layers as steps in the input sequences.
    There are additional criteria that would need to be satisfied to make a Deep RNN.
    More on that later in this section. These networks are trained using backpropagation
    and stochastic gradient descent techniques. The key thing to note here is that
    backpropagation is happening through the sequence or time steps before backpropogating
    through layers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从结构上看，网络非常简单，因为它是一个单一单元。为了利用和学习通过的输入结构，权重向量 *U*、*V* 和 *W* 在时间步骤之间共享。该网络没有像全连接或卷积网络那样的层。然而，由于它在时间步骤上展开，可以认为它有与输入序列中步骤数量相等的层。要构建深度
    RNN，还需要满足其他标准。稍后在本节中会详细讨论。这个网络使用反向传播和随机梯度下降技术进行训练。这里需要注意的关键是，反向传播是在序列或时间步骤中发生的，而不是通过层进行反向传播。
- en: 'Having this structure enables processing sequences of arbitrary lengths. However,
    as the length of sequences increases, there are a couple of challenges that emerge:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种结构使得可以处理任意长度的序列。然而，随着序列长度的增加，会出现一些挑战：
- en: '**Vanishing and exploding gradients**: As the lengths of these sequences increase,
    the gradients going back will become smaller and smaller. This will cause the
    network to train slowly or not learn at all. This effect will be more pronounced
    as sequence lengths increase. In the previous chapter, we built a network of a
    handful of layers. Here, a sentence of 10 words would equate to a network of 10
    layers. A 1-minute audio sample of 10 ms would generate 6,000 steps! Conversely,
    gradients can also explode if the output is increasing. The simplest way to manage
    vanishing gradients is through the use of ReLUs. For managing exploding gradients,
    a technique called **gradient clipping** is used. This technique artificially
    clips gradients if their magnitude exceeds a threshold. This prevents gradients
    from becoming too large or exploding.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失与爆炸**：随着序列长度的增加，反向传播的梯度会越来越小。这会导致网络训练缓慢或完全不学习。随着序列长度的增加，这一效果会更加明显。在上一章中，我们构建了一个少量层的网络。在这里，10个单词的句子相当于10层网络。10毫秒的1分钟音频样本将生成6000个步骤！相反，如果输出值增加，梯度也可能爆炸。管理梯度消失的最简单方法是使用ReLU。管理梯度爆炸的技术称为**梯度裁剪**。这种技术会在梯度的大小超过阈值时将其裁剪。这样可以防止梯度过大或爆炸。'
- en: '**Inability to manage long-term dependencies**: Let''s say that the third word
    in an eleven-word sentence is highly informative. Here is a toy example: "I think
    soccer is the most popular game across the world." As the processing reaches the
    end of the sentence, the contribution of the words prior earlier in the sequence
    will become smaller and smaller due to repeated multiplication with the vector
    *V* as shown above.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法管理长期依赖性**：假设在一个11个单词的句子中，第3个单词信息量很大。以下是一个简单的例子：“我认为足球是全世界最受欢迎的运动。”当处理到句子的结尾时，序列中前面单词的贡献会越来越小，因为它们与向量*V*反复相乘，正如上面所示。'
- en: '**Two specific RNN cell designs mitigate these problems**: **Long-Short Term
    Memory** (**LSTM**) and **Gated Recurrent Unit** (**GRU**). These are described
    next. However, note that TensorFlow provides implementations of both types of
    cells out of the box. So, building RNNs with these cell types is almost trivial.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两种特定的RNN单元设计缓解了这些问题**：**长短期记忆**（**LSTM**）和**门控循环单元**（**GRU**）。这些将在接下来描述。然而，请注意，TensorFlow默认提供了这两种类型的单元的实现。因此，使用这些单元类型构建RNN几乎是小菜一碟。'
- en: Long short-term memory (LSTM) networks
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络
- en: LSTM networks were proposed in 1997 and improved upon and popularized by many
    researchers. They are widely used today for a variety of tasks and produce amazing
    results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络是在1997年提出的，并经过许多研究人员的改进和推广。它们今天广泛用于各种任务，并取得了惊人的成果。
- en: 'LSTM has four main parts:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM有四个主要部分：
- en: '**Cell value** or memory of the network, also referred to as the cell, which
    stores accumulated knowledge'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元值**或网络的记忆，也称为单元，存储着累积的知识'
- en: '**Input gate**, which controls how much of the input is used in computing the new
    cell value'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**，控制输入在计算新单元值时的使用量'
- en: '**Output gate**, which determines how much of the cell value is used in the
    output'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**，决定单元值中有多少用于输出'
- en: '**Forget gate**, which determines how much of the current cell value is used
    for updating the cell value'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**，决定当前单元值有多少用于更新单元值'
- en: 'These are shown in the figure below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示：
- en: '![](img/B16252_02_02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_02.png)'
- en: 'Figure 2.2: LSTM cell (Source: Madsen, "Visualizing memorization in RNNs,"
    Distill, 2019)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：LSTM单元（来源：Madsen，《在RNN中可视化记忆》，Distill，2019年）
- en: Training RNNs is a very complicated process fraught with many frustrations.
    Modern tools such as TensorFlow do a great job of managing the complexity and
    reducing the pain to a great extent. However, training RNNs still is a challenging
    task, especially without GPU support. But the rewards of getting it right are
    well worth it, especially in the field of NLP.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 训练RNN是一个非常复杂的过程，充满了许多挑战。现代工具如TensorFlow很好地管理了复杂性，并大大减少了痛苦。然而，训练RNN仍然是一个具有挑战性的任务，尤其是在没有GPU支持的情况下。但一旦做对了，所带来的回报是值得的，尤其是在NLP领域。
- en: After a quick introduction to GRUs, we will pick up on LSTMs, talk about BiLSTMs,
    and build a sentiment classification model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要介绍GRU之后，我们将继续讨论LSTM，讲解BiLSTM，并构建一个情感分类模型。
- en: Gated recurrent units (GRUs)
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）
- en: 'GRUs are another popular, and more recent, type of RNN unit. They were invented
    in 2014\. They are simpler than LSTMs:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GRU是另一种流行且较新的RNN单元类型。它们是在2014年发明的。相比LSTM，它们更加简洁：
- en: '![](img/B16252_02_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_03.png)'
- en: 'Figure 2.3: Gated recurrent unit (GRU) architecture'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：门控循环单元（GRU）架构
- en: Compared to the LSTM, it has fewer gates. Input and forget gates are combined
    into a single update gate. Some of the internal cell state and hidden state is
    merged together as well. This reduction in complexity makes it easier to train.
    It has shown great results in the speech and sound domains. However, in neural
    machine translation tasks, LSTMs have shown superior performance. In this chapter,
    we will focus on using LSTMs. Before we discuss BiLSTMs, let's take a sentiment
    classification problem and solve it with LSTMs. Then, we will try and improve
    the model with BiLSTMs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与LSTM相比，它的门控较少。输入门和遗忘门合并成一个更新门。部分内部单元状态和隐藏状态也合并在一起。这种复杂度的降低使得训练变得更加容易。它在语音和声音领域已表现出优秀的效果。然而，在神经机器翻译任务中，LSTM表现出了更优的性能。本章将重点讲解如何使用LSTM。在我们讨论BiLSTM之前，先让我们通过LSTM来解决一个情感分类问题。然后，我们将尝试使用BiLSTM改进该模型。
- en: Sentiment classification with LSTMs
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LSTM进行情感分类
- en: Sentiment classification is an oft-cited use case of NLP. Models that predict
    the movement of stock prices by using sentiment analysis features from tweets
    have shown promising results. Tweet sentiment is also used to determine customers'
    perceptions of brands. Another use case is processing user reviews for movies,
    or products on e-commerce or other websites. To see LSTMs in action, let's use
    a dataset of movie reviews from IMDb. This dataset was published at the ACL 2011
    conference in a paper titled *Learning Word Vectors for Sentiment Analysis*. This
    dataset has 25,000 review samples in the training set and another 25,000 in the
    test set.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分类是自然语言处理（NLP）中常被引用的应用场景。通过使用来自推文的情感分析特征来预测股票价格波动的模型已经显示出有希望的结果。推文情感也用于确定客户对品牌的看法。另一个应用场景是处理电影或电子商务网站上的产品用户评论。为了展示LSTM的实际应用，我们将使用IMDb的电影评论数据集。该数据集在ACL
    2011会议上发布，论文题目为*学习用于情感分析的词向量*。该数据集包含25,000个训练集样本和另外25,000个测试集样本。
- en: 'A local notebook will be used for the code for this example. *Chapter 10*,
    *Installation and Setup Instructions for Code*, provides detailed instructions
    on how to set up the development environment. In short, you will need Python 3.7.5
    and the following libraries to start:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中的代码将使用本地笔记本。*第10章*，*代码的安装与设置说明*，提供了如何设置开发环境的详细说明。简而言之，您需要Python 3.7.5及以下库才能开始：
- en: pandas 1.0.1
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 1.0.1
- en: NumPy 1.18.1
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 1.18.1
- en: TensorFlow 2.4 and the `tensorflow_datasets 3.2.1` package
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.4 和 `tensorflow_datasets 3.2.1` 包
- en: Jupyter notebook
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter笔记本
- en: We will follow the overall process outlined in *Chapter 1*, *Essentials of NLP*.
    We start by loading the data we need.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照*第1章*，*NLP基础*中概述的整体过程来进行。我们从加载所需的数据开始。
- en: Loading the data
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据
- en: In the previous chapter, we downloaded the data and loaded it with the `pandas`
    library. This approach loaded the entire dataset into memory. However, sometimes
    data can be quite large, or spread into multiple files. In such cases, it may
    be too large for loading and need lots of pre-processing. Making text data ready
    to be used in a model requires normalization and vectorization at the very least.
    Often, this needs to be done outside of the TensorFlow graph using Python functions.
    This may cause issues in the reproducibility of code. Further, it creates issues
    for data pipelines in production where there is a higher chance of breakage as
    different dependent stages are being executed separately.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用`pandas`库下载并加载了数据。这个方法将整个数据集加载到内存中。然而，有时数据量可能非常大，或者分布在多个文件中。在这种情况下，数据可能会过大，无法加载并需要大量的预处理。使文本数据准备好供模型使用，至少需要进行标准化和向量化处理。通常，这些处理需要在TensorFlow图之外使用Python函数来完成。这可能会导致代码的可重现性问题。此外，这还会给生产环境中的数据管道带来问题，因为不同的依赖阶段被分开执行时，出现故障的概率更高。
- en: TensorFlow provides a solution for the loading, transformation, and batching
    of data through the use of the `tf.data` package. In addition, a number of datasets
    are provided for download through the `tensorflow_datasets` package. We will use
    a combination of these to download the IMDb data, and perform the tokenization,
    encoding, and vectorization steps before training an LSTM model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow通过`tf.data`包提供了解决数据加载、转换和批处理的方案。此外，通过`tensorflow_datasets`包提供了多个可供下载的数据集。我们将结合这些工具来下载IMDb数据，并在训练LSTM模型之前进行分词、编码和向量化处理。
- en: All the code for the sentiment review example can be found in the GitHub repo
    under the `chapter2-nlu-sentiment-analysis-bilstm` folder. The code is in an IPython
    notebook called `IMDB Sentiment analysis.ipynb`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所有情感评论示例的代码都可以在 GitHub 仓库中的 `chapter2-nlu-sentiment-analysis-bilstm` 文件夹下找到。代码在名为
    `IMDB Sentiment analysis.ipynb` 的 IPython notebook 中。
- en: 'The first step is to install the appropriate packages and download the datasets:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装适当的包并下载数据集：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `tfds` package comes with a number of datasets in different domains such
    as images, audio, video, text, summarization, and so on. To see the datasets available:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`tfds` 包提供了许多不同领域的数据集，例如图像、音频、视频、文本、摘要等。要查看可用的数据集：'
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That is a list of 155 datasets. Details of the datasets can be obtained on the
    catalog page at [https://www.tensorflow.org/datasets/catalog/overview](https://www.tensorflow.org/datasets/catalog/overview).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含 155 个数据集的列表。有关数据集的详细信息可以在目录页面 [https://www.tensorflow.org/datasets/catalog/overview](https://www.tensorflow.org/datasets/catalog/overview)
    查看。
- en: 'IMDb data is provided in three splits – training, test, and unsupervised. The
    training and testing splits have 25,000 rows each, with two columns. The first
    column is the text of the review, and the second is the label. "0" represents
    a review with negative sentiment while "1" represents a review with positive sentiment.
    The following code loads the training and testing data splits:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb 数据提供了三个分割——训练集、测试集和无监督集。训练集和测试集各有 25,000 行数据，每行有两列。第一列是评论文本，第二列是标签。 "0"
    表示带有负面情绪的评论，而 "1" 表示带有正面情绪的评论。以下代码加载训练集和测试集数据：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that this command may take a little bit of time to execute as data is
    downloaded. `ds_info` contains information about the dataset. This is returned
    when the `with_info` parameter is supplied. Let''s see the information contained
    in `ds_info`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于数据正在下载，此命令可能需要一些时间才能执行。`ds_info` 包含有关数据集的信息。当提供 `with_info` 参数时，它会返回该信息。让我们来看一下
    `ds_info` 中包含的信息：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see that two keys, `text` and `label`, are available in the supervised
    mode. Using the `as_supervised` parameter is key to loading the dataset as a tuple
    of values. If this parameter is not specified, data is loaded and made available
    as dictionary keys. In cases where the data has multiple inputs, that may be preferable.
    To get a sense of the data that has been loaded:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在监督模式下，`text` 和 `label` 两个键是可用的。使用 `as_supervised` 参数是将数据集加载为一组值元组的关键。如果没有指定此参数，数据将作为字典键加载并提供。在数据有多个输入的情况下，这可能是首选。为了了解已加载的数据：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The above review is an example of a negative review. The next step is tokenization
    and vectorization of the reviews.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的评论是负面评论的示例。接下来的步骤是对评论进行分词和向量化处理。
- en: Normalization and vectorization
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化与向量化
- en: In *Chapter 1*, *Essentials of NLP*, we discussed a number of different normalization
    methods. Here, we are only going to tokenize the text into words and construct
    a vocabulary, and then encode the words using this vocabulary. This is a simplified
    approach. There can be a number of different approaches that can be used for building
    additional features. Using techniques discussed in the first chapter, such as
    POS tagging, a number of features can be built, but that is left as an exercise
    for the reader. In this example, our aim is to use the same set of features on
    an RNN with LSTMs followed by using the same set of features on an improved model
    with BiLSTMs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*NLP 基础*中，我们讨论了多种标准化方法。在这里，我们只会将文本分词为单词并构建词汇表，然后使用该词汇表对单词进行编码。这是一种简化的方法。构建额外特征的方法有很多，可以使用第一章中讨论的技术，如
    POS 标注，构建多个特征，但这留给读者作为练习。在本示例中，我们的目标是使用相同的特征集在 RNN 上进行 LSTM 训练，然后使用相同的特征集在改进的
    BiLSTM 模型上进行训练。
- en: A vocabulary of the tokens occurring in the data needs to be constructed prior
    to vectorization. Tokenization breaks up the words in the text into individual
    tokens. The set of all the tokens forms the vocabulary.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量化之前，需要构建一个包含数据中所有令牌的词汇表。分词将文本中的单词拆分为单个令牌。所有令牌的集合构成了词汇表。
- en: 'Normalization of the text, such as converting to lowercase, etc., is performed
    along with this tokenization step. `tfds` comes with a set of feature builders
    for text in the `tfds.features.text` package. First, a set of all the words in
    the training data needs to be created:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的标准化，例如转换为小写字母等，是在此分词步骤中执行的。`tfds` 提供了一组用于文本的特征构建器，位于 `tfds.features.text`
    包中。首先，需要创建一个包含训练数据中所有单词的集合：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'By iterating through the training examples, each review is tokenized and the
    words in the review are added to a set. These are added to a set to get unique
    words. Note that tokens or words have not been converted to lowercase. This means
    that the size of the vocabulary is going to be slightly larger. Using this vocabulary,
    an encoder can be created. `TokenTextEncoder` is one of three out-of-the-box encoders
    that are provided in `tfds`. Note how the list of tokens is converted into a set
    to ensure only unique tokens are retained in the vocabulary. The tokenizer used
    for generating the vocabulary is passed in, so that every successive call to encode
    a string can use the same tokenization scheme. This encoder expects that the tokenizer
    object provides a `tokenize()` and a `join()` method. If you want to use StanfordNLP
    or some other tokenizer as discussed in the previous chapter, all you need to
    do is to wrap the StanfordNLP interface in a custom object and implement methods
    to split the text into tokens and join the tokens back into a string:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遍历训练示例，对每个评论进行分词，将评论中的单词添加到一个集合中。将它们添加到集合中以获取唯一的单词。注意，令牌或单词并未转换为小写。这意味着词汇表的大小会稍微大一些。使用此词汇表，可以创建一个编码器。`TokenTextEncoder`
    是 `tfds` 中提供的三种现成编码器之一。注意如何将令牌列表转换为集合，以确保词汇表中只保留唯一的令牌。用于生成词汇表的分词器被传递进来，这样每次调用编码字符串时都能使用相同的分词方案。此编码器期望分词器对象提供
    `tokenize()` 和 `join()` 方法。如果您想使用 StanfordNLP 或前一章节中讨论的其他分词器，只需将 StanfordNLP 接口封装在自定义对象中，并实现方法来将文本拆分为令牌并将令牌合并回字符串：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The vocabulary has 93,931 tokens. The longest review has 2,525 tokens. That
    is one wordy review! Reviews are going to have different lengths. LSTMs expect
    sequences of equal length. Padding and truncating operations make reviews of equal
    length. Before we do that, let''s test whether the encoder works correctly:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表包含 93,931 个令牌。最长的评论有 2,525 个令牌。真是一个冗长的评论！评论的长度会有所不同。LSTM 期望输入的是长度相等的序列。填充和截断操作会使评论具有相同的长度。在执行此操作之前，我们先测试编码器是否正常工作：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that punctuation is removed from these reviews when they are reconstructed
    from the encoded representations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当从编码表示重新构建这些评论时，标点符号会被移除。
- en: 'One convenience feature provided by the encoder is persisting the vocabulary
    to disk. This enables a one-time computation of the vocabulary and distribution
    for production use cases. Even during development, computation of the vocabulary
    can be a resource intensive task prior to each run or restart of the notebook.
    Saving the vocabulary and the encoder to disk enables picking up coding and model
    building from anywhere after the vocabulary building step is complete. To save
    the encoder, use the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器提供的一个便利功能是将词汇表持久化到磁盘。这使得词汇表和分布的计算可以仅执行一次，以供生产用例使用。即使在开发过程中，每次运行或重启笔记本之前，计算词汇表也可能是一项资源密集型的任务。将词汇表和编码器保存到磁盘，可以在完成词汇表构建步骤后，从任何地方继续进行编码和模型构建。要保存编码器，可以使用以下命令：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To load the encoder from the file and test it, the following commands can be
    used:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要从文件加载编码器并进行测试，可以使用以下命令：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Tokenization and encoding were done for a small set of rows at a time. TensorFlow
    provides mechanisms to perform these actions in bulk over large datasets, which
    can be shuffled and loaded in batches. This allows very large datasets to be loaded
    without running out of memory during training. To enable this, a function needs
    to be defined that performs a transformation on a row of data. Note that multiple
    transformations can be chained one after the other. It is also possible to use
    a Python function in defining these transformations. For processing the review
    above, the following steps need to be performed:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对每次小批量的行进行分词和编码。TensorFlow 提供了执行这些操作的机制，允许在大数据集上批量执行，可以对数据集进行打乱并分批加载。这使得可以在训练过程中加载非常大的数据集，而不会因内存不足而中断。为了实现这一点，需要定义一个函数，该函数对数据行进行转换。注意，可以将多个转换链式执行。也可以在定义这些转换时使用
    Python 函数。处理上述评论时，需要执行以下步骤：
- en: '**Tokenization**: Reviews need to be tokenized into words.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：评论需要被分词为单词。'
- en: '**Encoding**: These words need to be mapped to integers using the vocabulary.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码**：这些单词需要使用词汇表映射为整数。'
- en: '**Padding**: Reviews can have variable lengths, but LSTMs expect vectors of
    the same length. So, a constant length is chosen. Reviews shorter than this length
    are padded with a specific vocabulary index, usually `0` in TensorFlow. Reviews
    longer than this length are truncated. Fortunately, TensorFlow provides such a
    function out of the box.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充**：评论的长度可以不同，但LSTM期望向量的长度相同。因此，选择了一个固定长度。短于这个长度的评论会用特定的词汇索引填充，通常是`0`，在TensorFlow中是这样处理的。长于这个长度的评论则会被截断。幸运的是，TensorFlow提供了这样一个开箱即用的功能。'
- en: 'The following functions perform this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数执行了此操作：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`encode_tf_fn` is called by the dataset API with one example at a time. This
    means a tuple of the review and its label. This function in turn calls another
    function, `encode_pad_transform`, which is wrapped in the `tf.py_function` call
    that performs the actual transformation. In this function, tokenization is performed
    first, followed by encoding, and finally padding and truncating. A maximum length
    of 150 tokens or words is chosen for padding/truncating sequences. Any Python
    logic can be used in this second function. For example, the StanfordNLP package
    could be used to perform POS tagging of the words, or stopwords could be removed
    as shown in the previous chapter. Here, we try to keep things simple for this
    example.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode_tf_fn`由数据集API调用，一次处理一个样本。这意味着它处理的是一个包含评论及其标签的元组。此函数会调用另一个函数`encode_pad_transform`，该函数包装在`tf.py_function`调用中，执行实际的转换操作。在这个函数中，首先进行分词，然后是编码，最后是填充和截断。选择了最大长度150个标记或词进行填充/截断序列。此第二个函数可以使用任何Python逻辑。例如，可以使用StanfordNLP包来执行词性标注，或者像上一章那样去除停用词。在这个示例中，我们尽量保持简单。'
- en: Padding is an important step as different layers in TensorFlow cannot handle
    tensors of different widths. Tensors of different widths are called **ragged tensors**.
    There is ongoing work to incorporate support for ragged tensors and the support
    is improving. However, the support for ragged tensors is not universal in TensorFlow.
    Consequently, ragged tensors are avoided in this text.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 填充是一个重要的步骤，因为TensorFlow中的不同层不能处理宽度不同的张量。宽度不同的张量被称为**不规则张量**（ragged tensors）。目前正在进行相关的工作来支持不规则张量，并且该支持正在不断改进。然而，在TensorFlow中不规则张量的支持并不是普遍存在的。因此，在本书中避免使用不规则张量。
- en: 'Transforming the data is quite trivial. Let''s try the code on a small sample
    of the data:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 转换数据是非常简单的。让我们在一个小样本数据上试试这段代码：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note the "0" at the end of the encoded tensor in the first part of the output.
    That is a consequence of padding to 150 words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意输出第一部分中编码张量末尾的"0"。这是填充到150个词后的结果。
- en: 'Running this map over the entire dataset can be done like so:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样在整个数据集上运行这个map操作：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This should execute really fast. When the training loop executes, the mapping
    will be executed at that time. Other commands that are available and useful in
    the `tf.data.DataSet` class, of which `imdb_train` and `imdb_test` are instances,
    are `filter(),` `shuffle()`, and `batch()`. `filter()` can remove certain types
    of data from the dataset. It can be used to filter out reviews above or below
    a certain length, or separate out positive and negative examples to construct
    a more balanced dataset. The second method shuffles the data between training
    epochs. The last one batches data for training. Note that different datasets will
    result if these methods are applied in a different sequence.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会非常快速地执行。当训练循环执行时，映射将在此时执行。在`tf.data.DataSet`类中，还提供了其他有用的命令，`imdb_train`和`imdb_test`都是该类的实例，这些命令包括`filter()`、`shuffle()`和`batch()`。`filter()`可以从数据集中删除某些类型的数据。它可以用来过滤掉长度过长或过短的评论，或者将正面和负面的例子分开，从而构建一个更平衡的数据集。第二个方法会在训练的每个时期之间打乱数据。最后一个方法则用于将数据批处理以进行训练。请注意，如果这些方法的应用顺序不同，将会产生不同的数据集。
- en: 'Performance optimization with `tf.data`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.data`进行性能优化：
- en: '![](img/B16252_02_04.png)Figure 2.4: Illustrative example of the time taken
    by sequential execution of the map function (Source: Better Performance with the
    tf.data API at tensorflow.org/guide/data_performance )'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_04.png)图2.4：通过顺序执行map函数所需时间的示意例子（来源：Better Performance with
    the tf.data API，访问链接：tensorflow.org/guide/data_performance）'
- en: 'As can be seen in the figure above, a number of operations contribute to the
    overall training time in an epoch. This example chart above shows the case where
    files need to be opened, as shown in the topmost row, data needs to be read in
    the row below, a map transformation needs to be executed on the data being read,
    and then training can happen. Since these steps are happening in sequence, it
    can make the overall training time longer. Instead, the mapping step can happen
    in parallel. This will result in shorter execution times overall. CPU power is
    used to prefetch, batch, and transform the data, while the GPU is used for training
    computation and operations such as gradient calculation and updating weights.
    This can be enabled by making a small change in the call to the `map` function
    above:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，多个操作共同影响一个 epoch 中的整体训练时间。上面这个示例图展示了需要打开文件（如最上面一行所示）、读取数据（如下一行所示）、对读取的数据执行映射转换，然后才能进行训练。由于这些步骤是按顺序进行的，这可能会使整体训练时间变长。相反，映射步骤可以并行执行，这将使得整体执行时间变短。CPU
    用于预取、批处理和转换数据，而 GPU 则用于训练计算和诸如梯度计算和更新权重等操作。通过对上述 `map` 函数调用做一个小的修改，可以启用此功能：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Passing the additional parameter enables TensorFlow to use multiple subprocesses
    to execute the transformation on.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 传递额外的参数使 TensorFlow 能够使用多个子进程来执行转换操作。
- en: 'This can result in a speedup as shown below:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以带来如下面所示的加速效果：
- en: '![](img/B16252_02_05.png)Figure 2.5: Illustrative example of a reduction in
    training time due to parallelization of map (Source: Better Performance with the
    tf.data API at tensorflow.org/guide/data_performance )'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_02_05.png)图 2.5: 由于映射的并行化而减少训练时间的示例（来源：在 tensorflow.org/guide/data_performance
    上的 tf.data API 提升性能）'
- en: While we have normalized and encoded the text of the reviews, we have not converted
    it into word vectors or embeddings. This step is performed along with the model
    training in the next step. So, we are ready to start building a basic RNN model
    using LSTM now.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经对评论文本进行了标准化和编码，但还没有将其转换为词向量或嵌入。这一步骤将在下一步模型训练中完成。因此，我们现在可以开始构建一个基本的 LSTM
    模型。
- en: LSTM model with embeddings
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带嵌入的 LSTM 模型
- en: 'TensorFlow and Keras make it trivial to instantiate an LSTM-based model. In
    fact, adding a layer of LSTMs is one line of code. The simplest form is shown
    below:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 使得实例化一个基于 LSTM 的模型变得非常简单。实际上，添加一个 LSTM 层只需一行代码。最简单的形式如下所示：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, the `rnn_units` parameter determines how many LSTMs are strung together
    in one layer. There are a number of other parameters that can be configured, but
    the defaults are fairly reasonable on them. The TensorFlow documentation details
    these options and possible values with examples quite well. However, the review
    text tokens cannot be fed as is into the LSTM layer. They need to be vectorized
    using an embedding scheme. There are a couple of different approaches that can
    be used. The first approach is to learn these embeddings as the model trains.
    This is the approach we're going to use, as it is the simplest approach. In cases
    where the text data you may have is unique to a domain, like medical transcriptions,
    this is also probably the best approach. This approach, however, requires significant
    amounts of data for training for the embeddings to learn the right relationships
    with the words. The second approach is to use pre-trained embeddings, like Word2vec
    or GloVe, as shown in the previous chapter, and use them to vectorize the text.
    This approach has really worked well in general-purpose text models and can even
    be adapted to work very well in specific domains. Working with transfer learning
    is the focus of *Chapter 4*, *Transfer Learning with BERT,* though.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`rnn_units` 参数决定了在一个层中连接多少个 LSTM。还有许多其他参数可以配置，但默认值相当合理。TensorFlow 文档很好地详细说明了这些选项及其可能的取值，并附有示例。然而，评论文本标记不能直接输入到
    LSTM 层中。它们需要通过嵌入方案进行向量化。有几种不同的方法可以使用。第一种方法是在模型训练时学习这些嵌入。这是我们将要使用的方法，因为它是最简单的方式。如果你所拥有的文本数据是某个特定领域的，如医学转录，这也是最好的方法。这个方法需要大量的数据来训练，以便嵌入能够学习到单词之间的正确关系。第二种方法是使用预训练的嵌入，如
    Word2vec 或 GloVe，正如上一章所示，使用它们来向量化文本。这种方法在通用文本模型中表现良好，甚至可以适应特定领域。转移学习是 *第 4 章*
    的重点内容，*使用 BERT 进行转移学习*。
- en: 'Coming back to learning embeddings, TensorFlow provides an embedding layer
    that can be added before the LSTM layer. Again, this layer has several options
    that are well documented. To complete the binary classification model, all that
    remains is a final dense layer with one unit for classification. A utility function
    that can build models with some configurable parameters can be configured like
    so:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 回到学习嵌入，TensorFlow 提供了一个嵌入层，可以在 LSTM 层之前添加。这个层有几个选项，文档也有很好的说明。为了完成二分类模型，剩下的就是一个最终的全连接层，该层有一个单元用于分类。一个可以构建具有一些可配置参数的模型的工具函数可以这样配置：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This function exposes a number of configurable parameters to allow trying out
    different architectures. In addition to these parameters, batch size is another
    important parameter. These can be configured as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数暴露了一些可配置的参数，以便尝试不同的架构。除了这些参数之外，批次大小是另一个重要的参数。可以按照以下方式配置：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With the exception of the vocabulary size, all other parameters can be changed
    around to see the impact on model performance. With these configurations set,
    the model can be constructed:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了词汇表大小外，所有其他参数都可以进行调整，以查看对模型性能的影响。在设置好这些配置后，可以构建模型：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Such a small model has over 6 million trainable parameters. It is easy to check
    the size of the embedding layer. The total number of tokens in the vocabulary
    was 93,931\. Each token is represented by a 64-dimensional embedding, which provides
    93,931 X 64 = 6,011,584 million parameters.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个小模型有超过 600 万个可训练参数。很容易检查嵌入层的大小。词汇表中的总标记数为 93,931。每个标记都由一个 64 维的嵌入表示，提供了
    93,931 X 64 = 6,011,584 个参数。
- en: 'This model is now ready to be compiled with the specification of the loss function,
    optimizer, and evaluation metrics. In this case, since there are only two labels,
    binary cross-entropy is used as the loss. The Adam optimizer is a very good choice
    with great defaults. Since we are doing binary classification, accuracy, precision,
    and recall are the metrics we would like to track during training. Then, the dataset
    needs to be batched and training can be started:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个模型已经准备好进行编译，指定损失函数、优化器和评估指标。在这种情况下，由于只有两个标签，所以使用二元交叉熵作为损失函数。Adam 优化器是一个非常好的选择，具有很好的默认设置。由于我们正在进行二分类，准确率、精确度和召回率是我们希望在训练过程中跟踪的指标。然后，需要对数据集进行批处理，并开始训练：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'That is a very good result! Let''s compare it to the test set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常好的结果！让我们将其与测试集进行比较：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The difference between the performance on the training and test set implies
    there is overfitting happening in the model. One way to manage overfitting is
    to introduce a dropout layer after the LSTM layer. This is left as an exercise
    to you.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集之间的性能差异表明模型发生了过拟合。管理过拟合的一种方法是在 LSTM 层后引入一个 dropout 层。这个部分留给你作为练习。
- en: The model above was trained using an NVIDIA RTX 2070 GPU. You may see longer
    times per epoch when training using a CPU only.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型是在 NVIDIA RTX 2070 GPU 上训练的。使用仅 CPU 时，每个 epoch 的时间可能会更长。
- en: Now, let's see how BiLSTMs would perform on this task.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 BiLSTM 在这个任务中的表现如何。
- en: BiLSTM model
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BiLSTM 模型
- en: 'Building BiLSTMs is easy in TensorFlow. All that is required is a one-line
    change in the model definition. In the `build_model_lstm()` function, the line
    that adds the LSTM layer needs to be modified. The new function would look like
    this, with the modified line highlighted:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中构建 BiLSTM 很容易。所需要的只是对模型定义进行一行的修改。在 `build_model_lstm()` 函数中，添加
    LSTM 层的那一行需要修改。修改后的新函数如下所示，修改的部分已突出显示：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'But first, let''s understand what a BiLSTM is:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们了解什么是 BiLSTM：
- en: '![A picture containing light  Description automatically generated](img/B16252_02_06.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing light  Description automatically generated](img/B16252_02_06.png)'
- en: 'Figure 2.6: LSTMs versus BiLSTMs'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：LSTM 与 BiLSTM 的对比
- en: In a regular LSTM network, tokens or words are fed in one direction. As an example,
    take the review "This movie was really good." Each token starting from the left
    is fed into the LSTM unit, marked as a hidden unit, one at a time. The diagram
    above shows a version unrolled in time. What this means is that each successive
    word is considered as occurring at a time increment from the previous word. Each
    step produces an output that may or may not be useful. That is dependent on the
    problem at hand. In the IMDb sentiment prediction case, only the final output
    is important as it is fed to the dense layer to make a decision on whether the
    review was positive or negative.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规的LSTM网络中，词语或令牌是单向输入的。以句子“This movie was really good.”为例，从左到右，每个词一个接一个地输入LSTM单元，标记为隐含单元。上面的图表显示了一个随时间展开的版本。这意味着每个接下来的词被认为是相对于前一个词的时间增量。每一步会生成一个输出，这个输出可能有用，也可能没有用。这取决于具体问题。在IMDb情感预测的例子中，只有最后的输出才是重要的，因为它会被送到密集层做出判断，判断评论是正面还是负面。
- en: If you are working with right-to-left languages such as Arabic and Hebrew, please
    feed the tokens right to left. It is important to understand the direction the
    next word or token comes from. If you are using a BiLSTM, then the direction may
    not matter as much.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在处理像阿拉伯语和希伯来语这样的从右到左书写的语言，请确保按从右到左的顺序输入词令。理解下一个词或词令来自哪个方向非常重要。如果你使用的是BiLSTM，那么方向可能没有那么重要。
- en: Due to this time unrolling, it may appear as if there are multiple hidden units.
    However, it is the same LSTM unit, as shown in *Figure 2.2* earlier in the chapter.
    The output of the unit is fed back into the same unit at the next time step. In
    the case of BiLSTM, there is a pair of hidden units. One set operates on the tokens
    from left to right, while the other set operates on the tokens from right to left.
    In other words, a forward LSTM model can only learn from tokens from the past
    time steps. A BiLSTM model can learn from tokens from **the past and the future**.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间展开，可能看起来有多个隐含单元。然而，它们实际上是同一个LSTM单元，正如本章前面提到的*图2.2*所示。单元的输出被送回到同一个单元，作为下一时间步的输入。在BiLSTM的情况下，有一对隐含单元。一个集群处理从左到右的词令，而另一个集群处理从右到左的词令。换句话说，正向LSTM模型只能从过去的时间步的词令中学习。而BiLSTM模型可以同时从**过去和未来**的词令中学习。
- en: 'This method allows the capturing of more dependencies between words and the
    structure of the sentence and improves the accuracy of the model. Suppose the
    task is to predict the next word in this sentence fragment:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法能够捕捉更多的词语间依赖关系和句子结构，进而提高模型的准确性。假设任务是预测这个句子片段中的下一个词：
- en: '*I jumped into the …*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*我跳入了……*'
- en: 'There are many possible completions to this sentence. Further, suppose that
    you had access to the words after the sentence. Think about these three possibilities:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子有许多可能的补全方式。再者，假设你可以访问句子后面的词，想一想这三种可能性：
- en: '*I jumped into the …. with only a small blade*'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我带着一把小刀跳入了……*'
- en: '*I jumped into the … and swam to the other shore*'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我跳入了……并游到了另一岸*'
- en: '*I jumped into the … from the 10m diving board*'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我从10米跳板上跳入了……*'
- en: '*Battle* or *fight* would be likely words in the first example, *river* for
    the second, and *swimming pool* for the last one. In each case, the beginning
    of the sentence was exactly the same but the words from the end helped disambiguate
    which word should fill in the blank. This illustrates the difference between LSTMs
    and BiLSTMs. An LSTM can only learn from the past tokens, while the BiLSTM can
    learn from both past and future tokens.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*战斗*或*打斗*可能是第一个例子的常见词，第二个是*河流*，最后一个是*游泳池*。在每种情况下，句子的开头完全相同，但结尾的词帮助消除歧义，确定应该填入空白的词。这说明了LSTM和BiLSTM之间的差异。LSTM只能从过去的词语中学习，而BiLSTM能够从过去和未来的词语中学习。'
- en: This new BiLSTM model has a little over 12M parameters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的BiLSTM模型有超过1200万个参数。
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If you run the model shown above with no other changes, you will see a boost
    in the accuracy and precision of the model:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照上面所示的模型运行，且没有其他更改，你会看到模型的准确性和精度有所提升：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that the model is severely overfitting. It is important to add some form
    of regularization to the model. Out of the box, with no feature engineering or
    use of the unsupervised data for learning better embeddings, the accuracy of the
    model is above 83.5%. The current state-of-the-art results on this data, published
    in August 2019, have an accuracy of 97.42%. Some ideas that can be tried to improve
    this model include stacking layers of LSTMs or BiLSTMs, with some dropout for
    regularization, using the unsupervised split of the dataset along with training
    and testing review text data to learn better embeddings and using those in the
    final network, adding more features such as word shapes, and POS tags, among others.
    We will pick up this example again in *Chapter 4*, *Transfer Learning with BERT*,
    when we discuss language models such as BERT. Maybe this example will be an inspiration
    for you to try your own model and publish a paper with your state-of-the-art results!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型目前存在严重的过拟合问题。给模型添加某种形式的正则化非常重要。开箱即用的模型，在没有进行特征工程或使用无监督数据来学习更好的嵌入的情况下，模型的准确率已超过83.5%。2019年8月发布的该数据集当前的最先进结果，准确率为97.42%。一些可以尝试改进该模型的想法包括：堆叠LSTM或BiLSTM层，使用一些dropout进行正则化，使用数据集的无监督拆分以及训练和测试审查文本数据来学习更好的嵌入，并将其应用到最终网络中，增加更多特征如词形和词性标签等。我们将在*第四章*，*使用BERT的迁移学习*中再次提到这个例子，讨论BERT等语言模型。也许这个例子能激发你尝试自己构建模型，并用你最先进的结果发布论文！
- en: Note that BiLSTMs, while powerful, may not be suitable for all applications.
    Using a BiLSTM architecture assumes that the entire text or sequence is available
    at the same time. This assumption may not be true in some cases.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然BiLSTM非常强大，但并不适用于所有应用。使用BiLSTM架构假设整个文本或序列可以同时获取。在某些情况下，这一假设可能并不成立。
- en: In the case of the speech recognition of commands in a chatbot, only the sounds
    spoken so far by the users are available. It is not known what words a user is
    going to utter in the future. In real-time time-series analytics, only data from
    the past is available. In such applications, BiLSTMs cannot be used. Also, note
    that RNNs really shine with very large amounts of data training over several epochs.
    The IMDb dataset with 25,000 training examples is on the smaller side for RNNs
    to show their power. You may find you achieve similar or better results using
    TF-IDF and logistic regression with some feature engineering.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天机器人的命令语音识别中，只能使用用户到目前为止说出的声音。我们无法知道用户未来将会说什么词。在实时时间序列分析中，只有过去的数据可用。在这种应用中，BiLSTM无法使用。此外，值得注意的是，RNN在大量数据训练和多个epoch上才真正能展现其优势。IMDb数据集包含25,000个训练示例，算是较小的数据集，无法充分发挥RNN的强大能力。你可能会发现，使用TF-IDF和逻辑回归，并结合一些特征工程，能取得相似或更好的结果。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This is a foundational chapter in our journey through advanced NLP problems.
    Many advanced models use building blocks such as BiRNNs. First, we used the TensorFlow
    Datasets package to load data. Our work of building a vocabulary, tokenizer, and
    encoder for vectorization was simplified through the use of this library. After
    understanding LSTMs and BiLSTMs, we built models to do sentiment analysis. Our
    work showed promise but was far away from the state-of-the-art results, which
    will be addressed in future chapters. However, we are now armed with the fundamental
    building blocks that will enable us to tackle more challenging problems.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在高级NLP问题探索中的基础性章节。许多高级模型使用像BiRNN这样的构建模块。首先，我们使用TensorFlow Datasets包来加载数据。通过使用这个库，我们的词汇构建、分词器和向量化编码工作变得简化了。在理解LSTM和BiLSTM后，我们构建了情感分析模型。我们的工作展示了潜力，但离最先进的结果还有很长的距离，这将在未来的章节中讨论。然而，我们现在已经掌握了构建更复杂模型所需的基本构件，可以帮助我们解决更具挑战性的问题。
- en: Armed with this knowledge of LSTMs, we are ready to build our first NER model
    using BiLSTMs in the next chapter. Once this model is built, we will try to improve
    it using CRFs and Viterbi decoding.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有了这些LSTM的知识后，我们准备在下一章中使用BiLSTM构建我们的第一个命名实体识别（NER）模型。一旦模型构建完成，我们将尝试使用CRF和维特比解码来改进它。
