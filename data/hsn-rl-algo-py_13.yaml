- en: Imitation Learning with the DAgger Algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DAgger 算法的模仿学习
- en: The ability of an algorithm to learn only from rewards is a very important characteristic
    that led us to develop reinforcement learning algorithms. This enables an agent
    to learn and improve its policy from scratch without additional supervision. Despite
    this, there are situations where other expert agents are already employed in a
    given environment. **Imitation learning** (**IL**) algorithms leverage the expert
    by imitating their actions and learning the policy from them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 算法仅通过奖励来学习的能力是一个非常重要的特性，这也是我们开发强化学习算法的原因之一。它使得代理可以从零开始学习并改进其策略，而无需额外的监督。尽管如此，在某些情况下，给定环境中可能已经有其他专家代理在工作。**模仿学习**（**IL**）算法通过模仿专家的行为并从中学习策略来利用专家。
- en: This chapter focuses on imitation learning. Although different to reinforcement
    learning, imitation learning offers great opportunities and capabilities, especially
    in environments with very large state spaces and sparse rewards. Obviously, imitation
    learning is possible only when a more expert agent to imitate is available.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讲解模仿学习。虽然与强化学习不同，模仿学习在某些环境中提供了巨大的机会和能力，尤其是在具有非常大状态空间和稀疏奖励的环境中。显然，模仿学习只有在可以模仿的更专家代理存在时才可能进行。
- en: The chapter will focus on the main concepts and features of imitation learning
    methods. We'll implement an imitation learning algorithm called DAgger, and teach
    an agent to play Flappy Bird. This will help you to master this new family of
    algorithms and appreciate their basic principles.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讲解模仿学习方法的主要概念和特性。我们将实现一个名为 DAgger 的模仿学习算法，并教一个代理玩 Flappy Bird。这将帮助你掌握这一新型算法，并理解其基本原理。
- en: In the last section of this chapter, we'll introduce **inverse reinforcement
    learning** (**IRL**). IRL is a method that extracts and learns the behaviors of
    another agent in terms of values and rewards; that is, IRL learns the reward function.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后部分，我们将介绍**逆强化学习**（**IRL**）。IRL 是一种通过值和奖励来提取并学习另一个代理行为的方法；也就是说，IRL 学习奖励函数。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The imitation approach
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿方法
- en: Playing with Flappy Bird
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩 Flappy Bird
- en: Understanding the dataset aggregation algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集聚合算法
- en: IRL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IRL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: After a brief theoretical introduction to grasp the core concepts behind the
    imitation learning algorithms, we'll implement a real IL algorithm. However, we'll
    provide only the main and most interesting parts. Thus, if you are interested
    in the full implementation, you can find it in the GitHub repository of this book: [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要的理论介绍后，我们将实现一个实际的 IL 算法，帮助理解模仿学习算法背后的核心概念。然而，我们只会提供主要和最有趣的部分。如果你对完整实现感兴趣，可以在本书的
    GitHub 仓库中找到： [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)。
- en: Installation of Flappy Bird
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Flappy Bird
- en: Later, we'll run our IL algorithm on a revisited version of a famous game called Flappy
    Bird ([https://en.wikipedia.org/wiki/Flappy_Bird](https://en.wikipedia.org/wiki/Flappy_Bird)).
    In this section, we'll give you all the commands needed to install it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在一个重新设计的著名游戏 Flappy Bird 上运行我们的 IL 算法（[https://en.wikipedia.org/wiki/Flappy_Bird](https://en.wikipedia.org/wiki/Flappy_Bird)）。在这一部分，我们将提供安装所需的所有命令。
- en: 'But before installing the environment of the game, we need to take care of
    a few additional libraries:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但在安装游戏环境之前，我们需要处理一些额外的库：
- en: 'In Ubuntu, the procedure is as follows:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Ubuntu 中，步骤如下：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you are a Mac user, you can install the libraries with the following commands:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是 Mac 用户，可以通过以下命令安装库：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, for both Ubuntu and Mac users, the procedure is the following:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，对于 Ubuntu 和 Mac 用户，步骤如下：
- en: 'First, you have to clone PLE. The cloning is done with the following line of
    code:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要克隆 PLE。克隆可以通过以下代码行完成：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: PLE is a set of environments that also includes Flappy Bird. Thus, by installing
    PLE, you'll obtain Flappy Bird.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PLE 是一套环境，也包括 Flappy Bird。因此，通过安装 PLE，你将获得 Flappy Bird。
- en: 'Then, you have to enter the `PyGame-Learning-Environment` folder:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你需要进入 `PyGame-Learning-Environment` 文件夹：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And finally, run the installation with the following command:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过以下命令运行安装：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, you should be able to use Flappy Bird.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该能够使用 Flappy Bird 了。
- en: The imitation approach
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿方法
- en: 'IL is the art of acquiring a new skill by emulating an expert. This property
    of learning from imitation is not strictly necessary for learning sequential decision-making
    policies but nowadays, it is essential in plenty of problems. Some tasks cannot
    be solved through mere reinforcement learning, and bootstrapping a policy from
    the enormous spaces of complex environments is a key factor. The following diagram
    represents a high-level view of the core components involved in the imitation
    learning process:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习（IL）是通过模仿专家来获得新技能的艺术。模仿学习这一特性对学习顺序决策策略来说并不是绝对必要的，但如今，它在许多问题中是不可或缺的。有些任务不能仅仅通过强化学习来解决，从复杂环境中的巨大空间中自举策略是一个关键因素。以下图表展示了模仿学习过程中涉及的核心组件的高层视图：
- en: '![](img/c59df7c9-96f1-4662-9cda-5da6e9588fce.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c59df7c9-96f1-4662-9cda-5da6e9588fce.png)'
- en: If intelligent agents (the experts) already exist in an environment, they can
    be used to provide a huge amount of information to a new agent (the learner) about
    the behaviors needed to accomplish the task and navigate the environment. In this
    situation, the newer agent can learn much faster without the need to learn from
    scratch. The expert agent can also be used as a teacher to instruct and feed back
    to the new agent on its performing. Note the difference here. The expert can be
    used both as a guide to follow and as a supervisor to correct the mistakes of
    the student.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能体（专家）已经存在于环境中，它们可以为新的智能体（学习者）提供大量关于完成任务和导航环境所需行为的信息。在这种情况下，新的智能体可以在不从零开始学习的情况下更快地学习。专家智能体还可以作为教师，指导并反馈给新的智能体其表现。注意这里的区别，专家既可以作为指导者来跟随，也可以作为监督者来纠正学生的错误。
- en: If either the model of the guide, or the supervisor, is available, an imitation
    learning algorithm can leverage them. You can now understand why imitation learning
    plays such an important role and why we cannot leave it out of this book.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果能够提供引导模型或监督者，模仿学习算法可以利用它们。现在你可以理解为什么模仿学习如此重要，也明白为什么我们不能将其排除在本书之外。
- en: The driving assistant example
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驾驶助手示例
- en: 'To grasp these key concepts better, we can use the example of a teenager learning
    to drive. Let''s assume that they have never been in a car, that this is the first
    time they are seeing one, and that they don''t have any knowledge of how it works.
    There are three approaches to learning:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些关键概念，我们可以用一个青少年学车的例子来说明。假设他们从未坐过车，这是他们第一次看到汽车，而且他们对汽车的工作原理一无所知。学习有三种方法：
- en: They are given the keys and have to learn all by themselves, with no supervision
    at all.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们拿到钥匙后必须完全独立学习，完全没有监督。
- en: Before being given the keys, they sit in the passenger seat for 100 hours and
    look at the expert driving in different weather conditions and on different roads.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在获得钥匙之前，他们需要坐在副驾驶座上观察专家驾驶100小时，了解在不同天气条件和道路上的驾驶情况。
- en: They observe the expert driving but, most importantly, they have sessions where
    the expert provides feedback while driving. For example, the expert can give real-time
    instructions on how to park the car, and give direct feedback on how to stay in
    a lane.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们观察专家驾驶，但更重要的是，他们有机会在专家驾驶时得到反馈。例如，专家可以实时指导如何停车，并对如何保持车道提出直接反馈。
- en: As you may have guessed, the first case is a reinforcement learning approach
    where the agent has only sparse rewards from not breaking the car, pedestrians
    not yelling at them, and so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，第一个案例是强化学习方法，在这种方法中，智能体只有在不撞车、行人不对其大喊大叫等情况下才会获得稀疏的奖励。
- en: Regarding the second case, this is a passive IL approach with the competence
    that is acquired from the pure reproduction of the expert's actions. Overall,
    it's very close to a supervised learning approach.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 至于第二个案例，这是一种被动的模仿学习方法，通过纯粹复制专家的行为来获得能力。总体而言，它与监督学习方法非常相似。
- en: The third and final case is an active IL approach that gives rise to a *real*
    imitation learning approach. In this case, it is required that, during the training
    phase, the expert instructs the learner on every move the learner makes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个也是最后一个案例是主动的模仿学习方法，形成了*真正的*模仿学习方法。在这种情况下，要求在训练阶段，专家对学习者的每一个动作进行指导。
- en: Comparing IL and RL
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较模仿学习（IL）和强化学习（RL）
- en: Let's go more in-depth with the IL approach by highlighting the differences
    vis-à-vis RL. This contrast is very important. In imitation learning, the learner
    is not aware of any reward. This constraint can have very big implications.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过强调模仿学习与强化学习之间的差异来更深入地了解模仿学习方法。这种对比非常重要。在模仿学习中，学习者并不意识到任何奖励。这一约束可能会带来非常大的影响。
- en: Going back to our example, the apprentice can only replicate the expert's moves
    as closely as possible, be it in a passive or an active way. Not having objective
    rewards from the environment, they are constrained to the subjective supervision
    of the expert. Thus, even if they wanted to, they aren't able to improve and understand
    the teacher's reasoning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，学徒只能尽可能地模仿专家的动作，无论是被动的还是主动的。由于没有来自环境的客观奖励，他们只能受制于专家的主观监督。因此，即使他们想要改进，也无法理解和掌握老师的推理过程。
- en: So, IL should be seen as a way to copy the moves of the expert but without knowing
    its main goal. In our example, it's as if the young driver assimilates the trajectories
    of the teacher very well but, still, they don't know the motivations that made
    the teacher choose them. Without being aware of the reward, an agent trained with
    imitation learning cannot maximize the total reward as executed in RL.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模仿学习应被视为一种模仿专家动作的方式，但并不理解其主要目标。在我们的例子中，年轻的司机可能很好地模仿了老师的驾驶轨迹，但仍然不知道老师选择这些轨迹背后的动机。没有奖励的意识，经过模仿学习训练的代理无法像强化学习中那样最大化总奖励。
- en: 'This highlights the main differences between IL and RL. The former lacks the
    understanding of the main objective, and thus cannot surpass the teacher. The
    latter instead lacks a direct supervision signal and, in most cases, has access only
    to a sparse reward. This situation is clearly depicted in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了模仿学习和强化学习之间的主要区别。前者缺乏对主要目标的理解，因此无法超越老师。而后者则缺乏直接的监督信号，在大多数情况下，只能获得稀疏的奖励。这个情况在以下图示中得到了清晰的展示：
- en: '![](img/09a97c8b-02fd-4770-94cd-ecb16c86a8c3.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09a97c8b-02fd-4770-94cd-ecb16c86a8c3.png)'
- en: The diagram on the left represents the usual RL cycle, while on the right, the
    imitation learning cycle is represented. Here, the learner doesn't receive any
    reward; just the state and action given by the expert.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图表示通常的强化学习（RL）循环，而右侧则表示模仿学习（IL）循环。在这里，学习者不会获得任何奖励，只有专家提供的状态和动作。
- en: The role of the expert in imitation learning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿学习中专家的角色
- en: The terms *expert*, *teacher*, and *supervisor* refer to the same concept when
    speaking of imitation learning algorithms. They express a figure from which the
    new agent (the learner) can learn.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论模仿学习算法时，*专家*、*老师*和*监督者*这几个术语指的是相同的概念。它们表示一种可以让新代理（学习者）学习的角色。
- en: Fundamentally, the expert can be of any form, from a real human expert to an
    expert system. The first case is more obvious and adopted. What you are doing
    is teaching an algorithm to perform a task that a human is already able to do.
    The advantages are evident and it can be employed in a vast number of tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，专家可以有各种形式，从真实的人类专家到专家系统。前者更为明显并且被广泛采用。你所做的事情是教算法执行一个人类已经能够做的任务。其优点显而易见，并且可以应用于大量的任务中。
- en: The second case may not be so common. One of the valid motivations behind choosing
    a new algorithm trained with IL can be attributed to a slow expert system that,
    due to technical limitations, cannot be improved. For example, the teacher could
    be an accurate, but slow, tree search algorithm that is not able to perform at
    a decent speed at inference time. A deep neural network could be employed in its
    place. The training of the neural network under the supervision of the tree search
    algorithm could take some time but, once trained, it could perform much faster
    during runtime.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况可能不太常见。选择新算法并用模仿学习进行训练的有效动机之一，可能是由于技术限制，一个缓慢的专家系统无法得到改进。例如，老师可能是一个准确但缓慢的树搜索算法，在推理时无法以合适的速度运行。这时，可以用深度神经网络来代替它。尽管在树搜索算法的监督下训练神经网络可能需要一些时间，但一旦训练完成，它在运行时的表现会更快。
- en: By now, it should be clear that the quality of the policy coming from the learner
    is largely due to the quality of the information provided by the expert. The performance
    of the teacher is an upper limit to the final performances of the scholar. A poor
    teacher will always provide bad data to the learner. Thus, the expert is a key
    component that sets the bar for the quality of the final agent. With a weak teacher,
    we cannot pretend to obtain good policies.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，应该很清楚，从学习者得到的策略质量在很大程度上取决于专家提供的信息质量。教师的表现是学者最终表现的上限。一个糟糕的老师总是会给学习者提供糟糕的数据。因此，专家是设定最终代理质量标准的关键组件。只有在教师强大的情况下，我们才能期望获得好的策略。
- en: The IL structure
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IL结构
- en: Now that all the ingredients of imitation learning have been tackled, we can
    elaborate on the algorithms and approaches that can be used in order to design
    a full imitation learning algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了模仿学习的所有要素，可以详细说明可以用于设计完整模仿学习算法的算法和方法。
- en: 'The most straightforward way to tackle the imitation problem is shown in the
    following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解决模仿问题的最直接方法如下图所示：
- en: '![](img/7d182866-bc4e-4b9d-84b4-b882be2af54f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d182866-bc4e-4b9d-84b4-b882be2af54f.png)'
- en: 'The preceding diagram can be summarized in two main steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示可以总结为两个主要步骤：
- en: An expert collects data from the environment.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专家从环境中收集数据。
- en: A policy is learned through supervised learning on the dataset.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过监督学习在数据集上学习一个策略。
- en: Unfortunately, despite supervised learning being the imitation algorithm for
    excellence, most of the time, it doesn't work.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管监督学习是模仿算法的典范，但大多数时候，它并不奏效。
- en: 'To understand why the supervised learning approach isn''t a good alternative,
    we have to recall the foundations of supervised learning. We are mostly interested
    in two basic principles: the training and test set should belong to the same distribution,
    and the data should be independent and identically distributed (i.i.d). However,
    a policy should be tolerant of different trajectories and be robust to eventual
    distribution shifts.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么监督学习方法不是一个好的替代方案，我们必须回顾监督学习的基础。我们主要关注两个基本原则：训练集和测试集应该属于相同的分布，并且数据应该是独立同分布的（i.i.d.）。然而，一个策略应该能够容忍不同的轨迹，并对最终的分布变化具有鲁棒性。
- en: If an agent is trained using only a supervised learning approach to drive a
    car, whenever it shifts a little bit from the expert trajectories, it will be
    in a new state never seen before, and that will create a distribution mismatch.
    In this new state, the agent will be uncertain about the next action to take.
    In a usual supervised learning problem, it doesn't matter too much. If a prediction
    is missed, this will not have an influence on the next prediction. However, in
    an imitation learning problem, the algorithm is learning a policy and the i.i.d
    property is no longer valid because subsequent actions are strictly correlated
    to each other. Thus, they will have consequences and a compounding effect on all
    the others.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个代理仅通过监督学习方法来训练驾驶汽车，每当它从专家的轨迹中稍微偏离时，它将处于一个前所未见的新状态，这将导致分布不匹配。在这个新状态下，代理对下一步动作会感到不确定。在通常的监督学习问题中，这不会太影响。如果错过了一个预测，这不会对下一个预测产生影响。然而，在模仿学习问题中，算法正在学习一个策略，i.i.d.属性不再成立，因为后续的动作是严格相关的。因此，它们会对所有其他动作产生后果，并且有累积效应。
- en: In our example of the self-driving car, once the distribution has changed from
    that of the expert, the correct path will be very difficult to recover, since
    bad actions will accumulate and lead to dramatic consequences. The longer the
    trajectory, the worse the effect of imitation learning. To clarify, supervised
    learning problems with i.i.d. data can be seen as having a trajectory of length
    1\. No consequences on the next actions are found. The paradigm we have just presented
    is what we referred to previously as *passive* learning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自驾车的例子中，一旦分布从专家的分布发生变化，正确的路径将变得非常难以恢复，因为错误的动作会积累并导致严重后果。轨迹越长，模仿学习的效果越差。为了更清楚地说明，具有i.i.d.数据的监督学习问题可以视为长度为1的轨迹。对下一步动作没有任何影响。我们刚才提出的范式就是我们之前提到的*被动*学习。
- en: 'To overcome the distributional shift that can have catastrophic effects on
    policies learned using *passive* imitation, different techniques can be adopted.
    Some are hacks*,* while others are more algorithmic variations. Two of these strategies
    that work well are the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服由于使用*被动*模仿而可能对策略造成灾难性影响的分布变化，可以采用不同的技术。有些是巧妙的*黑客*技术，而另一些则是更具算法变种的方式。以下是两种效果较好的策略：
- en: Learning a model that generalizes very well on the data without overfitting
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习一个在数据上能很好地泛化而不发生过拟合的模型
- en: Using an active imitation in addition to the passive one
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了被动模仿，还可以使用主动模仿
- en: Because the first is more of a broad challenge, we will concentrate on the second
    strategy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因为第一个是更广泛的挑战，我们将集中精力在第二个策略上。
- en: Comparing active with passive imitation
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较主动模仿和被动模仿
- en: We introduced the term *active imitation* in the previous example, with the
    teenager learning to drive a car. Specifically, we referred to it in the situation
    in which the learner was driving with additional feedback from the expert. In
    general, for active imitation, we mean learning from on-policy data with the actions
    assigned by the expert.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们介绍了*主动模仿*这个术语，通过一个青少年学习开车的例子。具体来说，我们指的是在学习者在专家的额外反馈下进行驾驶的情境。一般来说，主动模仿是指从专家分配的动作中，通过策略数据进行学习。
- en: Speaking in terms of input *s* (the state or observation) and output *a* (the
    action), in passive learning, s and a both come from the expert. In active learning,
    s is sampled from the learner, and a is the action that the expert would have
    taken in state s. The objective of the newbie agent is to learn a mapping, [![](img/6d071f93-e760-4783-ba8a-76a29b1a221d.png)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入*s*（状态或观察）和输出*a*（动作）的角度来看，在被动学习中，s和a都来自专家。在主动学习中，s是从学习者那里采样的，a是专家在状态s下应该采取的动作。新手代理的目标是学习一个映射，[![](img/6d071f93-e760-4783-ba8a-76a29b1a221d.png)]。
- en: Active learning with on-policy data allows the learner to fix small deviations
    from the expert trajectory that the learner wouldn't know how to correct with
    only passive imitation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有策略数据的主动学习可以让学习者修正那些仅靠被动模仿无法纠正的小偏差。
- en: Playing Flappy Bird
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩Flappy Bird
- en: Later in this chapter, we'll develop and test an IL algorithm called DAgger
    on a new environment. The environment named Flappy Bird emulates the famous Flappy
    Bird game. Here, our mission is to give you the tools needed to implement code
    using this environment, starting from the explanation of the interface.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后续部分，我们将开发并测试一个名为DAgger的IL算法，应用在一个新的环境中。这个环境名为Flappy Bird，模拟了著名的Flappy Bird游戏。在这里，我们的任务是为你提供所需的工具，帮助你使用这个环境实现代码，从接口的解释开始。
- en: Flappy Bird belongs to the **PyGame Learning Environment** (**PLE**), a set
    of environments that mimic the **Arcade Learning Environment** (**ALE**) interface.
    This is similar to the **Gym** interface, and later we'll see the differences,
    although it's simple to use.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Flappy Bird属于**PyGame学习环境**（**PLE**），这是一组模仿**街机学习环境**（**ALE**）接口的环境。它类似于**Gym**接口，虽然使用起来很简单，稍后我们会看到它们之间的差异。
- en: 'The goal of Flappy Bird is to make the bird fly through vertical pipes without
    hitting them. It is controlled by only one action that makes it flap its wings.
    If it doesn''t fly, it progresses in a decreasing trajectory determined by gravity.
    A screenshot of the environment is shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Flappy Bird的目标是让小鸟飞过垂直的管道而不撞到它们。它只通过一个动作来控制，即让小鸟拍打翅膀。如果小鸟不飞，它会按照重力作用沿着下降轨迹前进。下面是环境的截图：
- en: '![](img/dab636b8-a119-4ae9-91f0-4ca19952eccf.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dab636b8-a119-4ae9-91f0-4ca19952eccf.png)'
- en: How to use the environment
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用环境
- en: In the following steps, we will see how to use the environment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将看到如何使用这个环境。
- en: 'In order to use Flappy Bird in our Python scripts, firstly, we need to import
    PLE and Flappy Bird:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在Python脚本中使用Flappy Bird，首先，我们需要导入PLE和Flappy Bird：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we instance a `FlappyBird` object and pass it to `PLE` with a few parameters:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们实例化一个`FlappyBird`对象，并将其传递给`PLE`，并传递一些参数：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, with `display_screen`, you can choose whether to display the screen.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通过`display_screen`，你可以选择是否显示屏幕。
- en: 'The environment is initialized by calling the `init()` method:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`init()`方法初始化环境：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To interact and get the state of the environment, we primarily use four functions:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与环境交互并获得环境的状态，我们主要使用四个函数：
- en: '`p.act(act)`, to execute the `act` action in the game. `act(act)` returns the
    reward obtained from the action performed.'
  id: totrans-87
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p.act(act)`，用来在游戏中执行`act`动作。`act(act)`返回执行该动作后获得的奖励。'
- en: '`p.game_over()`, to check whether the game reached a final state.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p.game_over()`，用于检查游戏是否达到了最终状态。'
- en: '`p.reset_game()`, to reset the game to the initial conditions.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p.reset_game()`，将游戏重置为初始状态。'
- en: '`p.getGameState()`, to obtain the current state of the environment. We could
    also use `p.getScreenRGB()` if we want to obtain the RGB observations (that is,
    the full screen) of the environment.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p.getGameState()`，用于获取当前环境的状态。如果我们想获取环境的 RGB 观察值（即整个屏幕），也可以使用`p.getScreenRGB()`。'
- en: 'Putting everything together, a simple script that plays Flappy Bird for five
    games can be designed as in the following code snippet. Note that in order to
    make it work, you still have to define the `get_action(state)` function that returns
    an action given a state:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起，一个简单的脚本可以设计成如下代码片段，用于让 Flappy Bird 玩五局。请注意，为了使其工作，您仍然需要定义返回给定状态下动作的`get_action(state)`函数：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A couple of things to point out here are as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个要点需要指出：
- en: '`getGameState()` returns a dictionary with the position, velocity, and the
    distance of the player, as well as the position of the next pipe and the following
    one. Before giving the state to the policymaker that we represented here with
    the `get_action` function, the dictionary is converted to a NumPy array and normalized.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getGameState()` 返回一个字典，其中包含玩家的位置、速度和距离，以及下一根管道和下下根管道的位置。在将状态传递给我们在此用`get_action`函数表示的策略制定者之前，字典会被转换为
    NumPy 数组并进行标准化。'
- en: '`act(action)` expects `None` as input if no action has to be performed, or
    `119` if the bird has to flap its wings in order to fly higher.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act(action)` 如果不需要执行动作，则期望输入为`None`；如果鸟需要拍打翅膀飞得更高，则输入为`119`。'
- en: Understanding the dataset aggregation algorithm
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集聚合算法
- en: One of the most successful algorithms that learns from demonstrations is **Dataset
    Aggregation** (**DAgger**). This is an iterative policy meta-algorithm that performs
    well under the distribution of states induced. The most notable feature of DAgger
    is that it addresses the distribution mismatch by proposing an active method in
    which the expert teaches the learner how to recover from the learner's mistakes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集聚合**（**DAgger**）是从演示中学习的最成功算法之一。它是一个迭代的策略元算法，在诱发的状态分布下表现良好。DAgger 最显著的特点是，它通过提出一种主动方法来解决分布不匹配问题，在这种方法中，专家教导学习者如何从学习者的错误中恢复。'
- en: A classic IL algorithm learns a classifier that predicts expert behaviors. This
    means that the model fits a dataset consisting of training examples, observed
    by an expert. The inputs are the observations, and the actions are the desired
    output values. However, following the previous reasoning, the predictions of the
    learner affect the future state or observation visited, violating the i.i.d assumption.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的 IL 算法学习一个分类器，预测专家的行为。这意味着模型拟合一个由专家观察到的训练样本数据集。输入是观察值，输出是期望的动作。然而，根据之前的推理，学习者的预测会影响未来访问的状态或观察，违反了独立同分布（i.i.d.）假设。
- en: 'DAgger deals with the change in distribution by iterating a pipeline of aggregation
    of new data sampled from the learner multiple times, and training with the aggregated
    dataset. A simple diagram of the algorithm is shown here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DAgger 通过反复迭代从学习者中采样的新数据聚合管道，来处理分布的变化，并利用聚合的数据集进行训练。算法的简单示意图如下所示：
- en: '![](img/fe1faf16-a462-463c-a02b-3e0531dd25b7.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe1faf16-a462-463c-a02b-3e0531dd25b7.png)'
- en: The expert populates the dataset used by the classifier, but, depending on the
    iteration, the action performed in the environment may come from the expert or
    the learner.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 专家填充了分类器所使用的数据集，但根据迭代的不同，环境中执行的动作可能来自专家，也可能来自学习者。
- en: The DAgger algorithm
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DAgger 算法
- en: Specifically, DAgger proceeds by iterating the following procedure. At the first
    iteration, a dataset *D* of trajectories is created from the expert policy and
    used to train a first policy ![](img/dd21f2a8-7117-44ca-96d1-97e70f418dfc.png)
    that best fits those trajectories without overfitting them. Then, during iteration
    *i*, new trajectories are collected with the learned policy ![](img/f9898e60-eb25-48ed-a2df-4a73bbc2b431.png) and
    added to the dataset *D*. After that, the aggregated dataset *D* with the new
    and old trajectories is used to train a new policy, ![](img/34d844e4-4668-46b2-b95e-6621b653c169.png).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，DAgger通过迭代以下过程进行。在第一次迭代中，从专家策略创建一个轨迹数据集 *D*，并用它来训练一个最适合这些轨迹且不发生过拟合的初始策略
    ![](img/dd21f2a8-7117-44ca-96d1-97e70f418dfc.png)。然后，在迭代 *i* 中，使用学习到的策略 ![](img/f9898e60-eb25-48ed-a2df-4a73bbc2b431.png)
    收集新的轨迹，并将其添加到数据集 *D* 中。接着，使用包含新旧轨迹的聚合数据集 *D* 来训练一个新的策略，![](img/34d844e4-4668-46b2-b95e-6621b653c169.png)。
- en: As per the report in the Dagger paper ([https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)), there
    is an active on-policy learning that outperforms many other imitation learning
    algorithms, and it's also able to learn very complex policies with the help of
    deep neural networks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据《Dagger论文》中的报告（[https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)），有一种活跃的基于策略的学习方法，优于许多其他模仿学习算法，并且在深度神经网络的帮助下，它能够学习非常复杂的策略。
- en: Additionally, at iteration *i*, the policy can be modified so that the expert
    takes control of a number of actions. This technique better leverages the expert
    and lets the learner gradually assume control over the environment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在迭代 *i* 时，可以修改策略，使专家控制多个动作。该技术更好地利用了专家的能力，并让学习者逐渐掌控环境。
- en: 'The pseudocode of the algorithm can clarify this further:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的伪代码可以进一步澄清这一点：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Implementation of DAgger
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DAgger的实现
- en: 'The code is divided into three main parts:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 代码分为三个主要部分：
- en: Load the expert inference function to predict an action given a state.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载专家推理函数，以便根据状态预测动作。
- en: Create a computational graph for the learner.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为学习者创建计算图。
- en: Create the DAgger iterations to build the dataset and train the new policy.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建DAgger迭代以构建数据集并训练新策略。
- en: Here, we'll explain the most interesting parts, leaving the others for your
    personal interest. You can check the remaining code and the complete version in
    the book's GitHub repository.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将解释最有趣的部分，其他部分留给你个人兴趣。你可以在书籍的GitHub仓库中查看剩余的代码和完整版本。
- en: Loading the expert inference model
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载专家推理模型
- en: The expert should be a policy that takes a state as input and returns the best
    action. Despite this, it can be anything. In particular, for these experiments,
    we used an agent trained with Proximal Policy Optimization (PPO) as the expert.
    In principle, this doesn't make any sense, but we adopted this solution for academic
    purposes, to facilitate integration with the imitation learning algorithms.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 专家应该是一个以状态为输入并返回最佳动作的策略。尽管如此，它可以是任何东西。特别是，在这些实验中，我们使用了一个通过近端策略优化（PPO）训练的代理作为专家。从原则上讲，这没有什么意义，但我们为学术目的采用了这一解决方案，以便与模仿学习算法进行集成。
- en: 'The expert''s model trained with PPO has been saved on file so that we can easily
    restore it with its trained weights. Three steps are required to restore the graph
    and make it usable:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PPO训练的专家模型已保存在文件中，因此我们可以轻松地恢复它并使用其训练好的权重。恢复图并使其可用需要三步：
- en: Import the meta graph. The computational graph can be restored with `tf.train.import_meta_graph`.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入元图。可以通过`tf.train.import_meta_graph`恢复计算图。
- en: Restore the weights. Now, we have to load the pretrained weights on the computational
    graph we have just imported. The weights have been saved in the latest checkpoint
    and they can be restored with `tf.train.latest_checkpoint(session, checkpoint)`.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恢复权重。现在，我们需要将预训练的权重加载到刚刚导入的计算图中。权重已保存在最新的检查点中，可以通过`tf.train.latest_checkpoint(session,
    checkpoint)`恢复。
- en: Access the output tensors. The tensors of the restored graph are accessed with
    `graph.get_tensor_by_name(tensor_name)`, where `tensor_name` is the tensor's name
    in the graph.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问输出张量。恢复的图的张量可以通过`graph.get_tensor_by_name(tensor_name)`访问，其中`tensor_name`是图中张量的名称。
- en: 'The following lines of code summarize the entire process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行总结了整个过程：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, because we are only interested in a simple function that returns an expert
    action given a state, we can design the `expert` function in such a way that it
    returns that function. Thus, inside `expert()`, we define an inner function called `expert_policy(state)` and
    return it as output of `expert()`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，因为我们只关心一个简单的函数，它会根据状态返回专家动作，我们可以设计 `expert` 函数，使其返回该函数。因此，在 `expert()` 内部，我们定义一个名为
    `expert_policy(state)` 的内部函数，并将其作为 `expert()` 的输出返回：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating the learner's computational graph
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建学习者的计算图
- en: All the following code is located inside a function called `DAgger`, which takes some
    hyperparameters that we'll see throughout the code as arguments.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有代码都位于一个名为 `DAgger` 的函数内部，该函数接受一些超参数，我们将在代码中看到这些参数。
- en: The learner's computational graph is simple as its only goal is to build a classifier.
    In our case, there are only two actions to predict, one for doing nothing, and
    the other to make the bird flap its wings. We can instantiate two placeholders,
    one for the input state, and one for the *ground-truth* actions that are those
    of the expert. The actions are an integer corresponding to the action taken. In
    the case of two possible actions, they are just 0 (do nothing) or 1 (fly).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者的计算图非常简单，因为它的唯一目标是构建一个分类器。在我们的案例中，只有两个动作需要预测，一个是做不做动作，另一个是让小鸟拍翅膀。我们可以实例化两个占位符，一个用于输入状态，另一个用于*真实标签*，即专家的动作。动作是一个整数，表示所采取的动作。对于两个可能的动作，它们分别是
    0（什么也不做）或 1（飞行）。
- en: 'The steps to build such a computational graph are the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这样的计算图的步骤如下：
- en: Create a deep neural network, specifically, a fully connected multilayer perceptron
    with a ReLu activations function in the hidden layers and a linear function on
    the final layer.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个深度神经网络，具体来说，是一个具有 ReLU 激活函数的全连接多层感知器，在隐藏层使用 ReLU 激活函数，在最后一层使用线性激活函数。
- en: For every input state, take the action with the highest value. This is done
    using the `tf.math.argmax(tensor,axis)` function with `axis=1`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个输入状态，选择具有最高值的动作。这个操作通过 `tf.math.argmax(tensor,axis)` 函数完成，`axis=1`。
- en: Convert the action's placeholders in a one-hot tensor. This is needed because
    the logits and labels that we'll use in the loss function should have dimensions, `[batch_size,
    num_classes]`. However, our labels named `act_ph` have shapes, `[batch_size]`.
    Therefore, we convert them to the desired shape with one-hot encoding. `tf.one_hot` is
    the TensorFlow function that does just that.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将动作的占位符转换为 one-hot 张量。这是必要的，因为我们在损失函数中使用的 logits 和标签应该具有维度`[batch_size, num_classes]`。然而，我们的标签
    `act_ph` 的形状是`[batch_size]`。因此，我们通过 one-hot 编码将它们转换为所需的形状。`tf.one_hot` 是 TensorFlow
    用于执行这一操作的函数。
- en: Create the loss function. We use the softmax cross-entropy loss function. This
    is a standard loss function used for discrete classification with mutually exclusive
    classes, just like in our case. The loss function is computed using `softmax_cross_entropy_with_logits_v2(labels,
    logits)` between the logits and the labels.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建损失函数。我们使用 softmax 交叉熵损失函数。这是一个标准的损失函数，适用于具有互斥类别的离散分类问题，就像我们的情况一样。损失函数通过`softmax_cross_entropy_with_logits_v2(labels,
    logits)`在 logits 和标签之间进行计算。
- en: Lastly, the mean of the softmax cross-entropy is computed across the batch and
    minimized using Adam.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算 softmax 交叉熵的平均值，并使用 Adam 优化器进行最小化。
- en: 'These five steps are implemented in the following lines:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个步骤在接下来的代码行中实现。
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then initialize a session, the global variables, and define a function, `learner_policy(state)`.
    This function, given a state, returns the action with a higher probability chosen
    by the learner (this is the same thing we did for the expert):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以初始化会话、全局变量，并定义一个函数 `learner_policy(state)`。该函数根据给定状态返回学习者选择的具有更高概率的动作（这与我们为专家所做的相同）：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Creating a DAgger loop
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 DAgger 循环
- en: 'It''s now time to set up the core of the DAgger algorithm. The outline has
    already been defined in the pseudocode in *The DAgger algorithm* section, but
    let''s take a more in-depth look at how it works:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是设置 DAgger 算法核心的时候了。该概要已经在 *The DAgger algorithm* 部分的伪代码中定义，但让我们更深入地了解它是如何工作的：
- en: 'Initialize the dataset composed of two lists, `X` and `y`, where we''ll put
    the states visited and the expert target actions. We also initialize the environment:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化由两个列表组成的数据集，`X` 和 `y`，其中存储访问过的状态和专家的目标动作。我们还初始化环境：
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Iterate across all the DAgger iterations. At the beginning of every DAgger
    iteration, we have to reinitialize the learner computational graph (because we
    retrain the learner on every iteration on the new dataset), reset the environment,
    and run a number of random actions. At the start of each game, we run a few random
    actions to add a stochastic component to the deterministic environment. The result
    will be a more robust policy:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历所有DAgger迭代。在每次DAgger迭代的开始，我们必须重新初始化学习者的计算图（因为我们在每次迭代时都会在新的数据集上重新训练学习者），重置环境，并执行一系列随机动作。在每个游戏开始时，我们执行一些随机动作，以向确定性环境中添加随机成分。结果将是一个更强健的策略：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Collect new data by interacting with the environment. As we said previously,
    the first iteration contains the expert that has to choose the actions by calling
    `expert_policy`, but, in the following iterations, the learner progressively takes
    control. The learned policy is executed by the `learner_policy` function. The
    dataset is collected by appending to `X` (the input variable) the current state
    of the game, and by appending to `y` (the output variable) the actions that the
    expert would have taken in that state. When the game is over, the game is reset
    and `game_rew` is set to `0`. The code is as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过与环境互动收集新数据。正如我们之前所说，第一次迭代中包含了专家，专家必须通过调用`expert_policy`来选择动作，但在后续迭代中，学习者会逐渐接管控制权。学习到的策略由`learner_policy`函数执行。数据集通过将当前游戏状态附加到`X`（输入变量），并将专家在该状态下会采取的动作附加到`y`（输出变量）来收集。当游戏结束时，游戏将重置，并将`game_rew`设置为`0`。代码如下：
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the actions are performed twice. This is done to reduce the number
    of actions every second to 15 instead of 30, as required by the environment.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，动作被执行了两次。这是为了将每秒的动作数量从30减少到15，以符合环境的要求。
- en: 'Train the new policy on the aggregated dataset. The pipeline is standard. The
    dataset is shuffled and divided into mini-batches of length `batch_size`. Then,
    the optimization is repeated by running `p_opt` for a number of epochs equals
    to `train_epochs` on each mini-batch. This is done with the following code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在汇总数据集上训练新策略。该流程是标准的。数据集被打乱并分成`batch_size`长度的小批次。然后，通过在每个小批次上运行`p_opt`进行多个训练周期（等于`train_epochs`），重复优化过程。以下是代码：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`test_agent` tests `learner_policy` on a few games to understand how well the
    learner is performing.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_agent`在几局游戏中测试`learner_policy`，以了解学习者的表现如何。'
- en: Analyzing the results on Flappy Bird
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Flappy Bird上的结果分析
- en: Before showing the results of the imitation learning approach, we want to provide
    some numbers so that you can compare these with those of a reinforcement learning
    algorithm. We know that this is not a fair comparison (the two algorithms work
    on very different conditions), but nevertheless, they underline why imitation
    learning can be rewarding when an expert is available.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示模仿学习方法的结果之前，我们想提供一些数据，以便你能将这些与强化学习算法的结果进行比较。我们知道这不是一个公平的比较（这两种算法在非常不同的条件下工作），但无论如何，它们强调了为什么当有专家可用时，模仿学习是值得的。
- en: The expert has been trained with proximal policy optimization for about 2 million
    steps and, after about 400,000 steps, reached a plateau score of about 138.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 专家已经使用近端策略优化进行了大约200万步的训练，并且在大约40万步后，达到了约138的停滞分数。
- en: 'We tested DAgger on Flappy Bird with the following hyperparameters:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Flappy Bird上测试了DAgger，使用了以下超参数：
- en: '| **Hyperparameter** | **Variable name** | **Value** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **变量名** | **值** |'
- en: '| Learner hidden layers | hidden_sizes | 16,16 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 学习者隐藏层 | hidden_sizes | 16,16 |'
- en: '| DAgger iterations | dagger_iterations | 8 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| DAgger迭代 | dagger_iterations | 8 |'
- en: '| Learning rate | p_lr | 1e-4 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | p_lr | 1e-4 |'
- en: '| Number of steps for every DAgger iteration | step_iterations | 100 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 每次DAgger迭代的步数 | step_iterations | 100 |'
- en: '| Mini-batch size | batch_size | 50 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 小批次大小 | batch_size | 50 |'
- en: '| Training epochs | train_epochs | 2000 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 训练周期数 | train_epochs | 2000 |'
- en: 'The plot in the following screenshot shows the trend of the performance of
    DAgger with respect to the number of steps taken:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了DAgger性能随步数变化的趋势：
- en: '![](img/48524175-156e-43d5-aa99-ff195df2dec0.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48524175-156e-43d5-aa99-ff195df2dec0.png)'
- en: The horizontal line represents the average performance reached by the expert.
    From the results, we can see that a few hundred steps are sufficient to reach
    the performance of the expert. However, compared with the experience required
    by PPO to train the expert, this represents about a 100-fold increase in sample
    efficiency.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 横线代表专家所达到的平均表现。从结果来看，我们可以看到几百步就足以达到专家的表现。然而，与 PPO 训练专家所需的经验相比，这表示样本效率提高了大约 100
    倍。
- en: Again, this is not a fair comparison as the methods are in different contexts,
    but it highlights that whenever an expert is available, it is suggested that you
    use an imitation learning approach (perhaps at least to learn a starting policy).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这并不是一个公平的比较，因为方法处于不同的背景中，但它突出了一个事实：每当有专家时，建议你使用模仿学习方法（至少可以用来学习一个初始策略）。
- en: IRL
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IRL
- en: One of the biggest limitations of IL lies in its inability to learn other trajectories
    to reach a goal, except those learned from the expert. By imitating an expert,
    the learner is constrained to the range of behaviors of its teacher. They are
    not aware of the end goal that the expert is trying to reach. Thus, these methods
    are only useful when there's no intention to perform better than the teacher.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: IL 的最大限制之一在于它无法学习其他路径来达到目标，除了从专家那里学到的路径。通过模仿专家，学习者受到教师行为范围的限制。他们并不了解专家试图达成的最终目标。因此，这些方法只有在没有意图超越教师表现的情况下才有用。
- en: IRL is an RL algorithm, such as IL, that uses an expert to learn. The difference
    is that IRL uses the expert to learn its reward function. Therefore, instead of
    copying the demonstrations, as is done in imitation learning, IRL figures out
    the goal of the expert. Once the reward function is learned, the agent uses it
    to learn the policy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: IRL 是一种强化学习算法，类似于 IL，使用专家进行学习。不同之处在于 IRL 使用专家来学习其奖励函数。因此，IRL 并不是像模仿学习那样复制示范，而是弄清楚专家的目标。一旦奖励函数被学习，智能体便可以利用它来学习策略。
- en: With the demonstrations used only to understand the goal of the expert, the
    agent is not bound to the actions of the teacher and can finally learn better
    strategies. For example, a self-driving car that learns by IRL would understand
    that the goal is to go from point A to point B in the minimum amount of time,
    while reducing the damage to things and people. The car would then learn a policy
    by itself (for example, with an RL algorithm) that maximizes this reward function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于示范仅用于理解专家的目标，智能体不受教师动作的限制，最终可以学到更好的策略。例如，一个通过 IRL 学习的自动驾驶汽车会明白，目标是以最短的时间从
    A 点到达 B 点，同时减少对物体和人员的损害。然后，汽车会自行学习一个策略（例如，使用 RL 算法），以最大化这个奖励函数。
- en: However, IRL also has a number of challenges that limit its applicability. The
    expert's demonstration may not be optimal, and, as a result, the learner may not
    be able to achieve its full potential and may remain stuck in the wrong reward
    function. The other challenge lies in the evaluation of the learned reward function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，IRL 也存在许多挑战，这些挑战限制了其适用性。专家的示范可能并非最优，因此，学习者可能无法充分发挥其潜力，并可能陷入错误的奖励函数中。另一个挑战在于对学习到的奖励函数的评估。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a break from reinforcement learning algorithms and
    explored a new type of learning called imitation learning. The novelty of this
    new paradigm lies in the way in which the learning takes place; that is, the resulting
    policy imitates the behavior of an expert. This paradigm differentiates from reinforcement
    learning in the absence of a reward signal and in its ability to leverage the
    incredible source of information brought by the expert entity.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们暂时跳出了强化学习算法，探讨了一种新的学习方式——模仿学习。这一新范式的创新之处在于学习的方式；即结果策略模仿专家的行为。这个范式与强化学习的不同之处在于没有奖励信号，并且能够利用专家实体带来的丰富信息源。
- en: We saw that the dataset from which the learner learns can be expanded with additional
    state action pairs to increase the confidence of the learner in new situations.
    This process is called data aggregation. Moreover, new data could come from the
    new learned policy and, in this case, we talked about on-policy data (as it comes
    from the same policy learned). This integration of on-policy states with expert
    feedback is a very valuable approach that increases the quality of the learner.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，学习者所学习的数据集可以通过增加额外的状态-动作对来扩展，以增加学习者在新情况中的信心。这个过程叫做数据聚合。此外，新数据可以来自新学习的策略，在这种情况下，我们称之为基于策略的数据（因为它来自同一个已学习的策略）。这种将基于策略的状态与专家反馈结合的做法是一种非常有价值的方法，可以提高学习者的质量。
- en: We then explored and developed one of the most successful imitation learning
    algorithms, called DAgger, and applied it to learn the Flappy Bird game.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们探索并开发了最成功的模仿学习算法之一，名为DAgger，并将其应用于学习Flappy Bird游戏。
- en: However, because imitation learning algorithms only copy the behavior of an
    expert, these systems cannot do better than the expert. Therefore, we introduced
    inverse reinforcement learning, which overcomes this problem by inferring the
    reward function from the expert. In this way, the policy can be learned independently
    of the teacher.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于模仿学习算法只是复制专家的行为，这些系统无法做得比专家更好。因此，我们引入了逆向强化学习，它通过推断专家的奖励函数来克服这个问题。通过这种方式，策略可以独立于教师来学习。
- en: In the next chapter, we'll take a look at another set of algorithms for solving
    sequential tasks; namely, evolutionary algorithms. You'll learn the mechanisms
    and advantages of these black-box optimization algorithms so that you'll be able
    to adopt them in challenging environments. Furthermore, we'll delve into an evolutionary
    algorithm called evolution strategy in greater depth and implement it.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍一组用于解决顺序任务的算法；即进化算法。你将学习这些黑箱优化算法的机制和优势，从而能够在挑战性环境中采用它们。此外，我们将更深入地探讨一种名为进化策略的进化算法，并加以实现。
- en: Questions
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Is imitation learning considered a reinforcement learning technique?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模仿学习是否被认为是一种强化学习技术？
- en: Would you use imitation learning to build an unbitable agent in Go?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会使用模仿学习来构建一个在围棋中无法击败的智能体吗？
- en: What's the full name of DAgger?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAgger的全名是什么？
- en: What's the main strength of DAgger?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAgger的主要优点是什么？
- en: Where would you use IRL instead of IL?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在哪里你会使用逆向强化学习而不是模仿学习？
- en: Further reading
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To read the original paper that introduced DAgger, checkout the following paper, *A
    Reduction of Imitation Learning and Structured Prediction to No-Regret Online
    Learning*: [https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要阅读介绍DAgger的原始论文，请查看以下论文，*将模仿学习和结构化预测归约为无悔在线学习*：[https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)。
- en: To learn more about imitation learning algorithms, checkout the following paper,
    *Global Overview of Imitation Learning*: [https://arxiv.org/pdf/1801.06503.pdf](https://arxiv.org/pdf/1801.06503.pdf).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于模仿学习算法的信息，请查看以下论文，*模仿学习的全球概述*： [https://arxiv.org/pdf/1801.06503.pdf](https://arxiv.org/pdf/1801.06503.pdf)。
- en: 'To learn more about inverse reinforcement learning, checkout the following
    survey, *A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress*: [https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于逆向强化学习的信息，请查看以下调查，*逆向强化学习调查：挑战、方法与进展*：[https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf)。
