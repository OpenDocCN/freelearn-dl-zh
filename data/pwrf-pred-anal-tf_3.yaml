- en: Chapter 3. Clustering Your Data – Unsupervised Learning for Predictive Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 聚类你的数据 – 用于预测分析的无监督学习
- en: In this lesson, we will dig deeper into predictive analytics and find out how
    we can take advantage of it to cluster records belonging to a certain group or
    class for a dataset of unsupervised observations. We will provide some practical
    examples of unsupervised learning; in particular, clustering techniques using
    TensorFlow will be discussed with some hands-on examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本课中，我们将深入探讨预测分析，并了解如何利用它对无监督观察数据集中的记录进行分组，属于某个特定组或类别。我们将提供一些无监督学习的实际示例；特别是会讨论使用TensorFlow进行的聚类技术，并附带一些动手示例。
- en: 'The following topics will be covered in this lesson:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本课将涵盖以下主题：
- en: Unsupervised learning and clustering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习与聚类
- en: Using K-means for predicting neighborhood
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-means进行预测邻域
- en: Using K-means for clustering audio files
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-means进行音频文件聚类
- en: Using unsupervised **k-nearest neighborhood** (**kNN**) for predicting nearest
    neighbors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用无监督的**k-最近邻**（**kNN**）进行预测最近邻
- en: Unsupervised Learning and Clustering
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习与聚类
- en: In this section, we will provide a brief introduction to the unsupervised **machine
    learning** (**ML**) technique. Unsupervised learning is a type of ML algorithm
    used for grouping related data objects and finding hidden patterns by inferencing
    from unlabeled datasets, that is, a training set consisting of input data without
    labels.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将简要介绍无监督**机器学习**（**ML**）技术。无监督学习是一种机器学习算法，用于通过推断无标签数据集中的隐藏模式，将相关的数据对象进行分组，也就是一个由无标签的输入数据组成的训练集。
- en: Let's see a real-life example. Suppose you have a large collection of not-pirated-totally-legal
    MP3s in a crowded and massive folder on your hard drive. Now, what if you can
    build a predictive model that helps automatically group together similar songs
    and organize them into your favorite categories such as country, rap, and rock?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个现实生活中的例子。假设你有一个庞大的文件夹，其中包含大量完全合法的非盗版MP3文件，存放在你的硬盘中。现在，如果你能够建立一个预测模型，帮助自动将相似的歌曲分组，并将它们组织到你最喜欢的类别中，比如乡村、嘻哈和摇滚，这该多好？
- en: This is an act of assigning an item to a group so that an MP3 is added to the
    respective playlist in an unsupervised way. In [Lesson 1](ch01.html "Chapter 1. From
    Data to Decisions – Getting Started with TensorFlow"), *From Data to Decisions
    – Getting Started with TensorFlow, on classification*, we assumed that you're
    given a training dataset of correctly labeled data. Unfortunately, we don't always
    have that extravagance when we collect data in the real world. For example, suppose
    we would like to divide a huge collection of music into interesting playlists.
    How canwe possibly group together songs if we don't have direct access to their
    metadata? One possible approach is a mixture of various ML techniques, but clustering
    is often at the heart of the solution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将项目分配到某个组中的行为，以便在无监督的方式下将MP3添加到相应的播放列表中。在[第1课](ch01.html "第1章 从数据到决策 - 使用TensorFlow入门")，*从数据到决策
    – 使用TensorFlow入门，关于分类*中，我们假设你有一个标注正确的训练数据集。不幸的是，当我们在现实世界中收集数据时，通常无法享有这样的奢侈。例如，假设我们想将大量音乐集合分成有趣的播放列表。如果我们无法直接访问它们的元数据，我们该如何将歌曲分组呢？一种可能的方法是结合多种机器学习技术，但聚类往往是解决方案的核心。
- en: In other words, the main objective of the unsupervised learning algorithms is
    to explore the unknown/hidden patterns in the input data that are unlabeled. Unsupervised
    learning, however, also comprehends other techniques to explain the key features
    of the data in an exploratory way toward finding the hidden patterns. To overcome
    this challenge, clustering techniques are used widely to group unlabeled data
    points based on certain similarity measures in an unsupervised way.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，无监督学习算法的主要目标是探索输入数据中未知/隐藏的模式，而这些数据没有标签。然而，无监督学习还包括其他技术，通过探索性方式解释数据的关键特征，以发现隐藏的模式。为了克服这一挑战，聚类技术被广泛用于根据某些相似度度量将无标签数据点分组，且这一过程是无监督的。
- en: In a clustering task, an algorithm groups related features into categories by
    analyzing similarities between input examples, where similar features are clustered
    and marked using circles.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类任务中，算法通过分析输入示例之间的相似性，将相关特征分组到不同类别中，其中相似的特征被聚集并用圆圈标记。
- en: 'Clustering uses include but are not limited to the following points:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的应用包括但不限于以下几点：
- en: Anomaly detection for suspicious pattern finding in an unsupervised way
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无监督方式下进行异常检测，以发现可疑模式
- en: Text categorization for finding useful patterns in the tests for NLP
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，用于寻找自然语言处理中的有用模式
- en: Social network analysis for finding coherent groups
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交网络分析，用于寻找一致的群体
- en: Data center computing clusters for finding a way of putting related computers
    together
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心计算集群，用于将相关计算机组合在一起
- en: Real estate data analysis for identifying neighborhoods based on similar features
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于相似特征的房地产数据分析，用于识别社区
- en: 'Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus, a trivial definition
    of clustering can be thought of as the process of organizing objects into groups
    whose members are similar in some way, as shown in figure 1:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是将数据样本或数据点划分并放入相应的同质类别或簇中的过程。因此，聚类的简单定义可以看作是将对象组织成组，组内成员在某些方面是相似的，如图1所示：
- en: '![Unsupervised Learning and Clustering](img/03_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习与聚类](img/03_01.jpg)'
- en: 'Figure 1: A typical data pipelines for clustering raw data'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：典型的聚类原始数据管道
- en: A cluster is, therefore, a collection of objects that have a similarity between
    them and are dissimilar to the objects belonging to other clusters. If a collection
    of objects is provided, clustering algorithms put these objects into groups based
    on similarity. For example, a clustering algorithm such as K-means locates the
    centroid of the groups of data points.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，簇是指一组在某些方面彼此相似，并且与属于其他簇的对象不相似的对象集合。如果给定一组对象，聚类算法会根据相似性将这些对象分成不同的组。例如，像 K-means
    这样的聚类算法会定位数据点组的质心。
- en: 'However, to make clustering accurate and effective, the algorithm evaluates
    the distance between each point from the centroid of the cluster. Eventually,
    the goal of clustering is to determine intrinsic grouping in a set of unlabeled
    data. For example, the K-means algorithm tries to cluster related data points
    within the predefined *3* (that is *k = 3*) clusters, as shown in figure 2:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了使聚类更准确和有效，算法会评估每个点到簇质心的距离。最终，聚类的目标是确定一组无标签数据中的内在分组。例如，K-means 算法会尝试将相关的数据点聚类到预定的
    *3* 个簇（即 *k = 3*）中，如图2所示：
- en: '![Unsupervised Learning and Clustering](img/03_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习与聚类](img/03_02.jpg)'
- en: 'Figure 2: The results of a typical clustering algorithm and a representation
    of the cluster centers'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：典型聚类算法的结果以及簇中心的表示
- en: Clustering is a process of intelligently categorizing items in your dataset.
    The overall idea is that two items in the same cluster are closer to each other
    than items that belong to separate clusters. This is a general definition, leaving
    the interpretation of closeness open. For example, perhaps cheetahs and leopards
    belong to the same cluster, whereas elephants belong to another when closeness
    is measured by the similarity of two species in the hierarchy of biological classification
    (family, genus, and species).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是智能地对数据集中的项进行分类的过程。总体思路是，同一簇中的两个项比属于不同簇的项更接近。这是一个通用定义，具体的接近度解释可以灵活调整。例如，当接近度通过物种在生物分类（如科、属和种）的层次关系中的相似性来衡量时，猎豹和美洲豹可能属于同一簇，而大象则属于另一个簇。
- en: Using K-means for Predictive Analytics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 K-means 进行预测分析
- en: K-means is a clustering algorithm that tries to cluster related data points
    together. However, we should know its working principle and mathematical operations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是一种聚类算法，旨在将相关的数据点聚集在一起。然而，我们需要了解其工作原理和数学运算。
- en: How K-means Works
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 工作原理
- en: 'Suppose we have *n* data points, *xi*, *i = 1...n*, that need to be partitioned
    into *k* clusters. Now that the target here is to assign a cluster to each data
    point, K-means aims to find the positions, *μi*, *i=1...k*, of the clusters that
    minimize the distance from the data points to the cluster. Mathematically, the
    K-means algorithm tries to achieve the goal by solving an equation that is an
    optimization problem:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 *n* 个数据点，*xi*，*i = 1...n*，需要将它们划分成 *k* 个簇。现在目标是为每个数据点分配一个簇，K-means 的目标是找到簇的位置
    *μi*，*i=1...k*，使得数据点到簇的距离最小。从数学角度来看，K-means 算法通过解决一个优化问题的方程来实现这一目标：
- en: '![How K-means Works](img/03_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![K-means 工作原理](img/03_03.jpg)'
- en: In the previous equation, **ci** is a set of data points, which when assigned
    to cluster **i** and![How K-means Works](img/03_21.jpg)is the Euclidean distance
    to be calculated (we will explain why we should use this distance measurement
    shortly). Therefore, we can see that the overall clustering operation using K-means
    is not a trivial one, but a NP-hard optimization problem. This also means that
    the K-means algorithm not only tries to find the global minima but often gets
    stuck in different solutions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方程中，**ci** 是一组数据点，当这些数据点被分配到簇 **i** 时，![K-means 工作原理](img/03_21.jpg)是需要计算的欧几里得距离（我们稍后会解释为何需要使用这种距离度量）。因此，我们可以看到，使用
    K-means 进行整体聚类操作并非易事，而是一个 NP-难度的优化问题。这也意味着 K-means 算法不仅尝试找到全局最小值，而且常常会陷入不同的解中。
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to the centroids. With every pass of the algorithm, each point is assigned to
    its nearest centroid based on some distance metric, usually the Euclidean distance
    stated earlier.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 K-means 算法进行聚类时，从初始化所有坐标为质心开始。每次算法执行时，根据某种距离度量（通常是前面提到的欧几里得距离），每个点都会被分配到其最近的质心。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Distance calculation**: There are other ways to calculate the distance as
    well. For example, the Chebyshev distance can be used to measure the distance
    by considering only the most notable dimensions. The Hamming distance algorithm
    can identify the difference between two strings. Mahalanobis distance can be used
    to normalize the covariance matrix. The Manhattan distance is used to measure
    the distance by considering only axis-aligned directions. The Haversine distance
    is used to measure the great-circle distances between two points on a sphere from
    the location.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离计算**：当然，还有其他方法可以计算距离。例如，切比雪夫距离可以通过只考虑最显著的维度来测量距离。哈明距离算法可以识别两个字符串之间的差异。马氏距离可以用来规范化协方差矩阵。曼哈顿距离则是通过仅考虑轴对齐的方向来测量距离。哈弗辛距离用于测量球面上两点之间的大圆距离。'
- en: 'Considering these distance-measuring algorithms, it is clear that the Euclidean
    distance algorithm would be the most appropriate to solve our purpose of distance
    calculation in the K-means algorithm. The centroids are then updated to be the
    centers of all the points assigned to it in that iteration. This repeats until
    there is a minimal change in the centers. In short, the K-means algorithm is an
    iterative algorithm and works in two steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些距离度量算法，可以明确看出，欧几里得距离算法是最适合用于解决我们在 K-means 算法中距离计算的目的的。然后，质心会更新为该迭代中分配给它的所有点的中心。这个过程会不断重复，直到质心的变化最小。简而言之，K-means
    算法是一个迭代算法，并且分为两个步骤：
- en: '**Cluster assignment step**: K-means goes through each of the *n* data points
    in the dataset that is assigned to a cluster closest to the k centroids, then
    the least distant one is picked.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**簇分配步骤**：K-means 会遍历数据集中每一个 *n* 数据点，将其分配到最接近的 k 个质心的簇中，然后选择距离最小的一个。'
- en: '**Update step**: For each cluster, a new centroid is calculated for all the
    data points in the cluster. The overall workflow of K-means can be explained using
    a flowchart, as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新步骤**：对于每个簇，计算该簇内所有数据点的新质心。K-means 的整体工作流程可以通过以下流程图来解释：'
- en: '![How K-means Works](img/03_04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![K-means 工作原理](img/03_04.jpg)'
- en: 'Figure 4: Flowchart of the K-means algorithm (Elbow method is an optional but
    also an advanced option)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：K-means 算法的流程图（肘部法则是一个可选但也是高级的选项）
- en: Using K-means for Predicting Neighborhoods
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 K-means 预测邻域
- en: 'Now, to show an example of clustering using K-means, we will use the Saratoga
    NY Homes dataset downloaded from [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    as an unsupervised learning technique. The dataset contains several features of
    the houses located in the suburb of the New York City; for example, price, lot
    size, waterfront, age, land value, new construct, central air, fuel type, heat
    type, sewer type, living area, Pct.College, bedrooms, fireplaces, bathrooms, and
    the number of rooms. However, only a few features have been shown in **Table 1**:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了展示使用 K-means 进行聚类的例子，我们将使用从 [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    下载的 Saratoga NY Homes 数据集，作为一种无监督学习技术。该数据集包含位于纽约市郊的房屋的若干特征；例如，价格、地块面积、水front、年龄、土地价值、新建构、中央空调、燃料类型、供热类型、污水处理类型、生活面积、大学百分比、卧室、壁炉、浴室以及房间数。然而，**表
    1** 中仅展示了其中的一些特征：
- en: '![Using K-means for Predicting Neighborhoods](img/03_05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![使用K-means预测邻近区域](img/03_05.jpg)'
- en: 'Table 1: A sample data from the Saratoga NY Homes dataset'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：来自萨拉托加纽约房屋数据集的示例数据
- en: 'The target of this clustering technique is to show an exploratory analysis
    based on the features of each house in the city for finding possible neighborhoods''
    of the house located in the same area. Before performing the feature extraction,
    we need to load and parse the Saratoga NY Homes dataset. However, we will look
    at this example with step-by-step source codes for better understanding:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该聚类技术的目标是基于城市中每个房屋的特征进行探索性分析，目的是寻找位于相同区域的可能邻近房屋。在进行特征提取之前，我们需要加载并解析萨拉托加纽约房屋数据集。然而，为了更好地理解，我们将一步一步地查看这个示例的源代码：
- en: Loading required libraries and packages.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的库和软件包。
- en: 'We need some built-in Python libraries, such as os, random, NumPy, and Pandas,
    for data manipulation; PCA for dimensionality reduction; Matplotlib for plotting;
    and of course, TensorFlow:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要一些内置的Python库，例如os、random、NumPy和Pandas，用于数据处理；PCA用于降维；Matplotlib用于绘图；当然，还有TensorFlow：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading, parsing, and preparing a training set.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载、解析并准备训练集。
- en: 'Here, the first line is used to ensure the reproducibility of the result. The
    second line basically reads the dataset from your location and converts it into
    the Pandas data frame:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的第一行用于确保结果的可重现性。第二行基本上是从你的位置读取数据集并将其转换为Pandas数据框：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you now print the data frame (using print(train)), you should find the dataframe
    containing headers and data as shown in figure 3:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果现在打印数据框（使用print(train)），您应该会看到包含标题和数据的数据框，如图3所示：
- en: '![Using K-means for Predicting Neighborhoods](img/03_06.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用K-means预测邻近区域](img/03_06.jpg)'
- en: 'Figure 5: A snapshot of the Saratoga NY Homes dataset'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5：萨拉托加纽约房屋数据集的快照
- en: Well, we have managed to prepare the dataset. Now, the next task is to conceptualize
    our K-means and write a function/class for it.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好的，我们已经成功准备了数据集。现在，下一步是概念化我们的K-means并为其编写一个函数/类。
- en: Implementing K-means.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现K-means。
- en: 'The following is the source code of K-means, which is simple in a TensorFlow
    way:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是K-means的源代码，这在TensorFlow中非常简单：
- en: '[PRE2]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The previous code contains all the steps required to develop the K-means model,
    including the distance-based centroid calculation, centroid update, and training
    parameters required.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码包含了开发K-means模型所需的所有步骤，包括基于距离的质心计算、质心更新和训练所需的参数。
- en: Clustering the houses.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对房屋进行聚类。
- en: 'Now the previous function can be invoked with real values, for example, our
    housing dataset. Since there are many houses with their respective features, it
    would be difficult to plot the clusters along with all the properties. This is
    the **Principal Component Analysis** (**PCA**) that we discussed in the previous
    lessons:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在可以用实际值调用前面的函数，例如，我们的房屋数据集。由于有许多房屋及其相应的特征，绘制所有房屋的聚类图会很困难。这就是我们在之前的课程中讨论的**主成分分析**（**PCA**）：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Well, now we are all set. It would be even better to visualize the clusters
    as shown in figure 6\. For this, we will use mpl_toolkits.mplot3d for 3D projection,
    as follows:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好的，现在我们一切就绪。为了更好地可视化聚类，您可以参见图6所示。为此，我们将使用mpl_toolkits.mplot3d进行3D投影，如下所示：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Using K-means for Predicting Neighborhoods](img/03_07.jpg)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用K-means预测邻近区域](img/03_07.jpg)'
- en: 'Figure 6: Clustering the houses with similar properties, for example, price'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6：聚类具有相似属性的房屋，例如价格
- en: Here, we can see that most houses fall in **0** to **100,000** range. The second
    highest houses fall in the range of **100000** to **200000**. However, it's really
    difficult to separate them. Moreover, the number of predefined clusters that we
    used is 10, which might not be the most optimal one. Therefore, we need to tune
    this parameter.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到大多数房屋的价格位于**0**到**100,000**之间。第二多的房屋价格位于**100,000**到**200,000**之间。然而，要区分它们是非常困难的。此外，我们使用的预定义聚类数量是10，这可能不是最优的选择。因此，我们需要调整此参数。
- en: Fine tuning and finding the optimal number of clusters.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调并找到最优的聚类数目。
- en: Choosing the right number of clusters often depends on the task. For example,
    suppose you're planning an event for hundreds of people, both young and old. If
    you have a budget for only two entertainment options, then you can use the K-means
    clustering with **k = 2** to separate the guests into two age groups. Other times,
    it's not as obvious what the value of `k` should be. Automatically figuring out
    the value of `k` is a bit more complicated.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择正确的簇数通常取决于任务。例如，假设你正在为数百人（包括老年人和年轻人）策划一个活动。如果你的预算只允许提供两个娱乐选项，那么你可以使用K均值聚类，设置**k
    = 2**，将宾客分成两组按年龄分配。有时，`k`值的选择并不那么明显。自动确定`k`值则会复杂一些。
- en: As mentioned earlier, the K-means algorithm tries to minimize the sum of squares
    of the distance (that is, Euclidean distance), in terms of **Within-Cluster Sum
    of Squares** (**WCSS**).
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，K均值算法试图最小化距离的平方和（即欧几里得距离），以**簇内平方和**（**WCSS**）为准。
- en: Note
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: However, if you want to minimize the sum of squares of the distance between
    the points of each set manually or automatically, you would end up with a model
    where each cluster is its own cluster center; in this case, this measure would
    be 0, but it would hardly be a generic enough model.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，如果你想手动或自动最小化每个集合中点之间的平方和，你将得到一个模型，其中每个簇都是其自己的簇中心；在这种情况下，这个度量值将为0，但它几乎不会是一个通用的模型。
- en: Therefore, once you have trained your model by specifying the parameters, you
    can evaluate the result using WCSS. Technically, it is same as the sum of distances
    of each observation in each K cluster. The beauty of clustering algorithms as
    a K-means algorithm is that it does the clustering on the data with an unlimited
    number of features. It is a great tool to use when you have raw data and would
    like to know the patterns in that data.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，一旦你通过指定参数训练好模型，就可以使用WCSS来评估结果。从技术上讲，它与每个K簇中每个观测点的距离和相同。聚类算法，尤其是K均值算法的优点是，它能在具有无限特征的数据上进行聚类。当你拥有原始数据，并希望了解其中的模式时，它是一个非常有用的工具。
- en: 'However, deciding the number of clusters prior to conducting the experiment
    might not be successful but sometimes may lead to an overfitting problem or an
    under-fitting one. Also, informally, determining the number of clusters is a separate
    but an optimization problem to be solved. So, based on this, we can redesign our
    K-means considering the WCSS value computation, as follows:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，在进行实验之前决定簇的数量可能并不会成功，有时可能导致过拟合问题或欠拟合问题。此外，非正式地说，确定簇的数量是一个独立的优化问题需要解决。因此，基于此，我们可以重新设计我们的K均值算法，考虑到WCSS值的计算，如下所示：
- en: '[PRE5]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To fine tune the clustering performance, we can use a heuristic approach called
    Elbow method. We start from **K = 2**. Then, we run the K-means algorithm by increasing
    `K` and observe the value of the cost function (CF) using WCSS. At some point,
    we should experience a big drop with respect to CF. Nevertheless, the improvement
    then becomes marginal with an increasing value of `K`.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了优化聚类性能，我们可以使用一种启发式方法，称为肘部法则。我们从**K = 2**开始。然后，通过增加`K`值运行K均值算法，并观察使用WCSS的代价函数（CF）值。在某个时刻，我们应该会看到CF值出现大幅下降。然而，随着`K`值的增加，改进将变得微不足道。
- en: 'In summary, we can pick the `K` after the last big drop of WCSS as the optimal
    one. The K-means includes various parameters such as withiness and betweenness,
    analyzing which you can find out the performance of K-means:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结来说，我们可以在WCSS最后一次大幅下降之后选择`K`值作为最优值。K均值包括多个参数，如簇内性和中心性，分析这些参数可以帮助我们了解K均值的性能：
- en: '**Betweenness**: This is the between sum of squares, also called the intra-cluster
    similarity'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中心性**：这是平方和的中心性，也称为簇内相似度'
- en: '**Withiness**: This is the within sum of squares, also called the inter-cluster
    similarity'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇内性**：这是平方和的簇内性，也称为簇间相似度'
- en: '**Totwithiness**: This is the sum of all the withiness of all the clusters,
    also called the total intra-cluster similarity'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总簇内性**：这是所有簇的簇内性之和，也称为总的簇内相似度'
- en: 'Note that a robust and accurate clustering model will have a lower value of
    withiness and a higher value of betweenness. However, these values depend on the
    number of clusters that is `K`, which is chosen before building the model. Now,
    based on this, we will train the K-means model for different `K` values that are
    a number of predefined clusters. We will start **K = 2** to `10`, as follows:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，一个健壮且准确的聚类模型将具有较低的内聚度值和较高的中介度值。然而，这些值依赖于选择的簇的数量`K`，这个值是在建立模型之前确定的。现在，基于此，我们将为不同的`K`值训练K-means模型，这些`K`值是预定义的簇的数量。我们将从**K
    = 2**到`10`开始，具体如下：
- en: '[PRE6]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s discuss how we can take the advantage of the Elbow method for determining
    the number of clusters. We calculated the cost function WCSS as a function of
    a number of clusters for the K-means algorithm applied to home data based on all
    the features, as follows:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何利用肘部法则来确定簇的数量。我们计算了K-means算法应用于房屋数据时，WCSS随簇数量变化的成本函数，如下所示：
- en: '[PRE7]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Using K-means for Predicting Neighborhoods](img/03_08.jpg)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用K-means预测邻里](img/03_08.jpg)'
- en: 'Figure 7: Number of clusters as a function of WCSS'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7：簇的数量与WCSS的关系
- en: We will try to reuse this lesson in upcoming examples using K-means too. Now,
    it can be observed that a big drop occurs when **k = 5**. Therefore, we chose
    the number of clusters to be **5** as discussed in figure 7\. Basically, this
    is the one after the last big drop. This means that the optimal number of cluster
    for our dataset that we need to set before we start training the K-means model
    is **5**.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在接下来的示例中，继续使用K-means算法复用这个教训。现在可以观察到，当**k = 5**时，发生了一个大幅下降。因此，我们选择了**5**个簇，如图7所示。基本上，这是在最后一次大幅下降之后的结果。这意味着，在我们开始训练K-means模型之前，我们需要设置的最优簇数量是**5**。
- en: Clustering analysis.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类分析。
- en: 'From figure 8, it is clear that most houses fall in **cluster 3** (**4655 houses**)
    and then in **cluster 4** (**3356 houses**). The x-axis shows the price and the
    y-axis shows the lot size for each house. We can also observe that the **cluster
    1** has only a few houses and potentially in longer distances, but it is also
    expensive. So, it is most likely that you will not find a nearer neighborhood
    to interact with if you buy a house that falls in this cluster. However, if you
    like more human interaction and budget is a constraint, you should probably try
    buying a house from cluster 2, 3, 4, or 5:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图8可以看出，大多数房屋属于**簇3**（**4655套房屋**），其次是**簇4**（**3356套房屋**）。x轴表示价格，y轴表示每套房屋的地块大小。我们还可以观察到，**簇1**只有少量房屋，且可能分布在较远的距离，但价格较高。因此，如果你购买的房屋属于这一簇，最有可能的是你无法找到一个更近的邻里互动。然而，如果你喜欢更多的人际互动且预算有限，可能应该考虑购买簇2、3、4或5中的房屋：
- en: '![Using K-means for Predicting Neighborhoods](img/03_09.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用K-means预测邻里](img/03_09.jpg)'
- en: 'Figure 8: Clusters of neighborhoods, that is,. homogeneous houses fall in same
    clusters'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8：邻里簇，即相同簇中的房屋属于同质群体
- en: To make the analysis, we dumped the output in RStudio and generated the clusters
    shown in figure 6\. The R script can be found on my GitHub repositories at [https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics).
    Alternatively, you can write your own script and do the visualization accordingly.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了进行分析，我们将输出数据导入到RStudio中，并生成了图6所示的簇。R脚本可以在我的GitHub仓库中找到，链接为：[https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics)。或者，您也可以编写自己的脚本并相应地进行可视化。
- en: Predictive Models for Clustering Audio Files
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频文件聚类的预测模型
- en: 'For clustering music with audio data, the data points are the feature vectors
    from the audio files. If two points are close together, it means that their audio
    features are similar. We want to discover which audio files belong to the same
    neighborhood because these clusters will probably be a good way to organize your
    music files:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用音频数据进行聚类，数据点是音频文件的特征向量。如果两个点靠得很近，说明它们的音频特征相似。我们希望发现哪些音频文件属于同一个邻里，因为这些簇很可能是组织您的音乐文件的好方法：
- en: Loading audio files with TensorFlow and Python.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow和Python加载音频文件。
- en: Some common input types in ML algorithms are audio and image files. This shouldn't
    come as a surprise because sound recordings and photographs are raw, redundant,
    ab nd often noisy representations of semantic concepts. ML is a tool to help handle
    these complications. These data files have various implementations, for example,
    an audio file can be an MP3 or WAV.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ML 算法中的一些常见输入类型是音频和图像文件。这不应该令人惊讶，因为录音和照片是语义概念的原始、冗余并且常常是嘈杂的表示。ML 是一种帮助处理这些复杂情况的工具。这些数据文件有各种实现方式，例如，音频文件可以是
    MP3 或 WAV。
- en: Reading files from a disk isn't exactly a ML-specific ability. You can use a
    variety of Python libraries to load files onto the memory, such as Numpy or Scipy.
    Some developers like to treat the data preprocessing step separately from the
    ML step. However, I believe that this is also a part of the whole analytics process.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从磁盘读取文件并不完全是 ML 特有的能力。你可以使用多种 Python 库将文件加载到内存中，例如 Numpy 或 Scipy。一些开发者喜欢将数据预处理步骤与
    ML 步骤分开。然而，我认为这也是整个分析过程的一部分。
- en: 'Since this is a TensorFlow book, I will try to use something from the TensorFlow
    built-in operator to list files in a directory called `tf.train.match_filenames_once()`.
    We can then pass this information along to a `tf.train.string_input_producer()queue`
    operator. This way, we can access a filename one at a time, without loading everything
    at once. Here''s the structure of this method:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这是一本关于 TensorFlow 的书，我会尝试使用 TensorFlow 内置操作符中的 `tf.train.match_filenames_once()`
    来列出目录中的文件。然后，我们可以将此信息传递给 `tf.train.string_input_producer()` 队列操作符。通过这种方式，我们可以一次访问一个文件，而不是一次性加载所有文件。下面是该方法的结构：
- en: '[PRE8]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This method takes two parameters: `pattern` and `name`. `pattern` signifies
    a file pattern or 1D tensor of file patterns. The `name` is used to signify the
    name of the operations. However, this parameter is optional. Once invoked, this
    method saves the list of matching patterns, so as the name implies, it is only
    computed once.'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个方法接受两个参数：`pattern` 和 `name`。`pattern` 表示文件模式或文件模式的 1D 张量。`name` 用来表示操作的名称。然而，这个参数是可选的。一旦调用该方法，它会保存匹配模式的列表，顾名思义，它只会计算一次。
- en: 'Finally, a variable that is initialized to the list of files matching the pattern(s)
    is returned by this method. Once we have finished reading the metadata and the
    audio files, we can decode the file to retrieve usable data from the given filename.
    Now, let''s get started. First, we need to import necessary packages and Python
    modules, as follows:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，返回一个初始化为与模式匹配的文件列表的变量。完成读取元数据和音频文件后，我们可以解码文件，以从给定的文件名中检索可用的数据。现在，让我们开始。首先，我们需要导入必要的包和
    Python 模块，如下所示：
- en: '[PRE9]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we can start reading the audio files from the directory specified. First,
    we need to store filenames that match a pattern containing a particular file extension,
    for example, `.mp3`, `.wav`, and so on. Then, we need to set up a pipeline for
    retrieving filenames randomly. Now, the code natively reads a file in TensorFlow.
    Then, we run the reader to extract the file data. Use can use following code for
    this task:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们可以开始从指定的目录读取音频文件。首先，我们需要存储与包含特定文件扩展名的模式匹配的文件名，例如 `.mp3`、`.wav` 等。然后，我们需要设置一个管道，用于随机获取文件名。现在，代码本地读取
    TensorFlow 中的文件。然后，我们运行读取器来提取文件数据。你可以使用以下代码来完成这项任务：
- en: '[PRE10]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Well, once we have read the data and metadata about all the audio files, the
    next and immediate tasks are to capture the audio features that will be used by
    K-means for the clustering purpose.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们读取了所有音频文件的数据和元数据，接下来的直接任务就是捕捉音频特征，这些特征将用于 K-means 聚类。
- en: Extracting features and preparing feature vectors.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取特征并准备特征向量。
- en: ML algorithms are typically designed to use feature vectors as input; however,
    sound files are a very different format. We need a way to extract features from
    sound files to create feature vectors.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ML 算法通常设计为使用特征向量作为输入；然而，声音文件是一个完全不同的格式。我们需要一种方法从声音文件中提取特征，以便创建特征向量。
- en: It helps to understand how these files are represented. If you've ever seen
    a vinyl record, you've probably noticed the representation of audio as grooves
    indented in the disk. Our ears interpret audio from a series of vibrations through
    the air. By recording the vibration properties, our algorithm can store sound
    in a data format. The real world is continuous but computers store data in discrete
    values.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它有助于理解这些文件是如何表示的。如果你曾经看过黑胶唱片，你可能会注意到音频被表示为磁碟上的凹槽。我们的耳朵通过空气中的一系列振动来解读音频。通过记录振动特性，我们的算法能够将声音存储为数据格式。现实世界是连续的，但计算机将数据存储为离散值。
- en: The sound is digitalized into a discrete representation through an **Analog
    to Digital Converter** (**ADC**). You can think about sound as a fluctuation of
    a wave over time. However, this data is too noisy and difficult to comprehend.
    An equivalent way to represent a wave is by examining the frequencies that make
    it up at each time interval. This perspective is called the frequency domain.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声音通过**模拟到数字转换器**（**ADC**）被数字化为离散表示。你可以把声音看作是随时间波动的波。然而，这些数据噪声很大，难以理解。表示波的等效方式是检查每个时间间隔内构成它的频率。这种视角被称为频域。
- en: It's easy to convert between time domain and frequency domain using a mathematical
    operation called a discrete Fourier transform (commonly known as the Fast Fourier
    transform). We will use this technique to extract a feature vector out of our
    sound.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用一种叫做离散傅里叶变换的数学运算（通常称为快速傅里叶变换），可以轻松地在时域和频域之间转换。我们将使用这一技术从我们的声音中提取特征向量。
- en: A sound may produce 12 kinds of pitch. In music terminology, the 12 pitches
    are C, C#, D, D#, E, F, F#, G, G#, A, A#, and B. Figure 9 shows how to retrieve
    the contribution of each pitch in a 0.1-second interval, resulting in a matrix
    with 12 rows. The number of columns grows as the length of the audio file increases.
    Specifically, there will be **10*t** columns for a **t** second audio.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个声音可能产生12种音高。在音乐术语中，这12个音高分别是C、C#、D、D#、E、F、F#、G、G#、A、A#和B。图9显示了如何在0.1秒的间隔内获取每个音高的贡献，从而得到一个包含12行的矩阵。列数随着音频文件长度的增加而增加。具体而言，对于一个**t**秒的音频，将有**10*t**列。
- en: 'This matrix is also called a chromogram of the audio. But first, we need to
    have a placeholder for TensorFlow to hold the chromogram of the audio and the
    maximum frequency:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个矩阵也叫做音频的色谱图。但首先，我们需要为TensorFlow提供一个占位符，用来保存音频的色谱图和最大频率：
- en: '[PRE11]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next task that we can perform is that we can write a method that can extract
    these chromograms for the audio files. It can look as follows:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来的任务是我们可以编写一个方法，提取音频文件的这些色谱图。它可能如下所示：
- en: '[PRE12]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The workflow of the previous code is as follows:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码的工作流程如下：
- en: First, pass in the filename and use these parameters to describe 12 pitches
    every 0.1 seconds.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，传入文件名，并使用这些参数描述每0.1秒的12个音高。
- en: Finally, represent the values of a 12-dimensional vector 10 times a second.
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，以每秒10次的频率表示12维向量的值。
- en: 'The chromogram output that we extract using the previous method will be a matrix,
    as visualized in figure 10\. A sound clip can be read as a chromogram, and a chromogram
    is a recipe for generating a sound clip. Now, we have a way to convert audio and
    matrices. As you have learned, most ML algorithms accept feature vectors as a
    valid form of data. That being said, the first ML algorithm we''ll look at is
    K-means clustering:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用前面的方法提取的色谱图输出将是一个矩阵，如图10所示。一个声音片段可以被读取为色谱图，而色谱图则是生成声音片段的配方。现在，我们有了将音频和矩阵相互转换的方法。如你所学，绝大多数机器学习算法接受特征向量作为有效的数据形式。也就是说，我们将要看的第一个机器学习算法是K均值聚类：
- en: '![Predictive Models for Clustering Audio Files](img/03_10.jpg)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于音频文件聚类的预测模型](img/03_10.jpg)'
- en: 'Figure 9: The visualization of the chromogram matrix where the x-axis represents
    time and the y-axis represents pitch class. The green markings indicate a presence
    of that pitch at that time'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9：色谱矩阵的可视化，其中x轴表示时间，y轴表示音高类别。绿色标记表示在该时间点该音高的存在。
- en: 'To run the ML algorithms on our chromogram, we first need to decide how we''re
    going to represent a feature vector. One idea is to simplify the audio by only
    looking at the most significant pitch class per time interval, as shown in figure
    10:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了在我们的色谱图上运行机器学习算法，我们首先需要决定如何表示特征向量。一个想法是通过只关注每个时间间隔内最显著的音高类别来简化音频，如图10所示：
- en: '![Predictive Models for Clustering Audio Files](img/03_11.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于音频文件聚类的预测模型](img/03_11.jpg)'
- en: 'Figure 10: The most influential pitch at every time interval is highlighted.
    You can think of it as the loudest pitch at each time interval'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10：在每个时间间隔内，最有影响力的音高被突出显示。你可以把它看作是每个时间间隔内最响亮的音高
- en: 'Now, we will count the number of times each pitch shows up in the audio file.
    Figure 11 shows this data as a histogram, forming a 12-dimensional vector. If
    we normalize the vector so that all the counts add up to **1**, then we can easily
    compare audio of different lengths:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将统计每个音高在音频文件中出现的次数。图 11 显示了这些数据的直方图，形成了一个 12 维向量。如果我们将该向量归一化，使所有计数之和为**1**，那么我们就可以轻松比较不同长度的音频：
- en: '![Predictive Models for Clustering Audio Files](img/03_12.jpg)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于聚类音频文件的预测模型](img/03_12.jpg)'
- en: 'Figure 11: We count the frequency of the loudest pitches heard at each interval
    to generate this histogram, which acts as our feature vector'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11：我们统计了在每个时间间隔内听到的最响亮音高的频率，以生成这个直方图，作为我们的特征向量
- en: 'Now that we have the chromagram, we need to use it to extract the audio feature
    to construct a feature vector. You can use the following method for this:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 既然我们已经有了色谱图，我们需要利用它提取音频特征来构建特征向量。你可以使用以下方法来完成这个任务：
- en: '[PRE13]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The workflow of the previous code is as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述代码的工作流程如下：
- en: Create an operation to identify the pitch with the biggest contribution.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个操作来识别贡献最大的音高。
- en: Now, convert the chromogram into a feature vector.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，将色谱图转换为特征向量。
- en: After this, we will construct a matrix where each row is a data item.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们将构建一个矩阵，其中每一行都是一个数据项。
- en: Now, if you can hear the audio clip, you can imagine and differentiate between
    the different audio files. However, this is just intuition.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，如果你能听到音频片段，你可以想象并区分不同的音频文件。然而，这只是直觉。
- en: 'Therefore, we cannot rely on this, but we should inspect them visually. So,
    we will invoke the previous method to extract the feature vector from each audio
    file and plot the feature. The whole operation should look as follows:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们不能仅依赖于此，而应该进行视觉检查。所以，我们将调用先前的方法，从每个音频文件中提取特征向量并绘制特征图。整个操作应如下所示：
- en: '[PRE14]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The previous code should plot the audio features of each audio file in the
    histogram as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的代码应该将每个音频文件的音频特征绘制成直方图，如下所示：
- en: '![Predictive Models for Clustering Audio Files](img/03_13.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于聚类音频文件的预测模型](img/03_13.jpg)'
- en: 'Figure 12: The ride audio files show a similar histogram'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12：踩镲音频文件显示了相似的直方图
- en: 'You can see some examples of audio files that we are trying to cluster based
    on their audio features. As you can see, the two on the right appear to have similar
    histograms. The two on the left also have similar sound spectrums:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到一些音频文件的示例，我们正在根据它们的音频特征进行聚类。正如你所看到的，右侧的两个音频文件似乎有相似的直方图。左侧的两个音频文件也具有相似的声音频谱：
- en: '![Predictive Models for Clustering Audio Files](img/03_14.jpg)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于聚类音频文件的预测模型](img/03_14.jpg)'
- en: 'Figure 13: The crash cymbal audio files show a similar histogram'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13：撞钹音频文件显示了相似的直方图
- en: 'Now, the target is to develop K-means so that it is able to group these sounds
    together accurately. We will look at the high-level view of the cough audio files,
    as shown in the following figure:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，目标是开发 K-means，使其能够准确地将这些声音聚类在一起。我们将查看咳嗽音频文件的高级视图，如下图所示：
- en: '![Predictive Models for Clustering Audio Files](img/03_15.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于聚类音频文件的预测模型](img/03_15.jpg)'
- en: 'Figure 14: The cough audio files show a similar histogram'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14：咳嗽音频文件显示了相似的直方图
- en: 'Finally, we have the scream audio files that have a similar histogram and audio
    spectrum, but of course are different compared to others:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们得到了具有相似直方图和音频频谱的尖叫音频文件，但与其他音频文件相比，当然它们是不同的：
- en: '![Predictive Models for Clustering Audio Files](img/03_16.jpg)'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![用于聚类音频文件的预测模型](img/03_16.jpg)'
- en: 'Figure 15: The scream audio files show a similar histogram and audio spectrum'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 15：尖叫音频文件显示了相似的直方图和音频频谱
- en: Now, we can imagine our problem. We have the features ready for training the
    K-means model. Let's start doing it.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以想象我们的任务。我们已经准备好了特征来训练 K-means 模型。让我们开始进行训练。
- en: Training K-means model.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 K-means 模型。
- en: Now that the feature vector is ready, it's time to feed this to the K-means
    model for clustering the feature presented in figure 10\. The idea is that the
    midpoint of all the points in a cluster is called a centroid.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在特征向量已经准备好，是时候将其输入 K-means 模型进行聚类了，如图 10 所示。其核心思想是，聚类中所有点的中点称为质心。
- en: 'Depending on the audio features we choose to extract, a centroid can capture
    concepts such as loud sound, high-pitched sound, or saxophone-like sound. Therefore,
    it''s important to note that the K-means algorithm assigns non-descript labels,
    such as cluster 1, cluster 2, or cluster 3\. First, we can write a method that
    computes the initial cluster centroids as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们选择提取的音频特征，质心可以捕捉到一些概念，如大声的声音、高音调的声音或萨克斯管类似的声音。因此，需要注意的是，K-means 算法分配的是没有描述性标签的聚类，例如聚类
    1、聚类 2 或聚类 3。首先，我们可以编写一个方法，计算初始的聚类质心，如下所示：
- en: '[PRE15]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, the next task is to randomly assign the cluster number to each data point
    based on the initial cluster assignment. This time we can use another method:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，接下来的任务是根据初始聚类分配随机地将聚类编号分配给每个数据点。这次我们可以使用另一种方法：
- en: '[PRE16]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The previous method computes the minimum distance and WCSS for the clustering
    evaluation in the later steps. Then, we need to update the centroid to check and
    make sure if there are any changes that occur in the cluster assignment:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的方法计算了聚类评估中最小距离和 WCSS。然后，我们需要更新质心来检查并确保聚类分配中是否发生了任何变化：
- en: '[PRE17]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have defined many variables, it''s time to initialize them using
    `local_variable_initializer()`, as follows:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 既然我们已经定义了许多变量，现在是时候使用 `local_variable_initializer()` 来初始化它们，如下所示：
- en: '[PRE18]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can perform the training. Forthis, the `audioClusterin()` method
    takes the number of tentative clusters k and iterates the training up to the maximum
    iteration, as follows:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们可以执行训练。为此，`audioClusterin()` 方法接受初步聚类数 `k` 并将训练迭代直到最大迭代次数，如下所示：
- en: '[PRE19]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The previous method returns the cluster cost, WCSS, and also prints the cluster
    number against each audio file. So, we have been able to finish the training step.
    Now, the next task is to evaluate the K-means clustering quality.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的方法返回了聚类成本、WCSS，并打印了每个音频文件的聚类编号。因此，我们已经完成了训练步骤。接下来的任务是评估 K-means 聚类质量。
- en: Evaluating the model.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。
- en: 'Here, we will evaluate the clustering quality from two perspectives. First,
    we will observe the predicted cluster number. Secondly, we will also try to find
    the optimal value of `k` as a function of WCSS. So, we will iterate the training
    for **K = 2** to say **10** and observe the clustering result. However, first,
    let''s create two empty lists to hold the values of `K` and WCSS in each step:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将从两个角度评估聚类质量。首先，我们将观察预测的聚类数量。其次，我们还将尝试找出 `k` 的最优值，它是 WCSS 的一个函数。因此，我们将对
    **K = 2** 到 **10** 进行迭代训练，并观察聚类结果。然而，首先，让我们创建两个空列表来保存每一步中 `K` 和 WCSS 的值：
- en: '[PRE20]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let''s iterate the training using the `for` loop as follows:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `for` 循环来迭代训练，如下所示：
- en: '[PRE21]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This prints the following output:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下输出：
- en: '[PRE22]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'These values signify that each audio file is clustered and the cluster number
    has been assigned (the first bracket is the cluster number, the contents in the
    second bracket is the filename). However, it is difficult to judge the accuracy
    from this output. One naïve approach would be to compare each file with figure
    12 to figure 15\. Alternatively, let''s adopt a better approach that we used in
    the first example that is the elbow method. For this, I have created a dictionary
    using two lists that are `k_list` and `wcss_list` computed previously, as follows:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些值表示每个音频文件已被聚类，并且聚类编号已分配（第一个括号是聚类编号，第二个括号内是文件名）。然而，从这个输出中很难判断准确性。一种天真的方法是将每个文件与图
    12 至图 15 进行比较。或者，让我们采用一种更好的方法，那就是我们在第一个示例中使用的肘部法则。为此，我已经创建了一个字典，使用了两个列表 `k_list`
    和 `wcss_list`，它们是先前计算得出的，具体如下：
- en: '[PRE23]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The previous code produces the following output:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的代码产生了以下输出：
- en: '[PRE24]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'From the previous output, you can see a sharp drop in WCSS for **k = 4**, and
    this is generated in the third iteration. So, based on this minimum evaluation,
    we can take a decision about the following clustering assignment:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从之前的输出可以看到，在 **k = 4** 时 WCSS 出现了急剧下降，这是在第三次迭代中生成的。因此，基于这个最小评估，我们可以做出关于以下聚类分配的决策：
- en: '[PRE25]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we have seen two complete examples of using K-means, there is another
    example called kNN. This is typically a supervised ML algorithm. In the next section,
    we will see how we can train this algorithm in an unsupervised way for a regression
    task.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 既然我们已经看过了使用 K-means 的两个完整示例，接下来我们将介绍另一个示例，叫做 kNN。它通常是一个监督学习算法。在下一节中，我们将看到如何以无监督的方式训练这个算法来进行回归任务。
- en: Using kNN for Predictive Analytics
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 kNN 进行预测分析
- en: kNN is non-parametric and instance-based and is used in supervised learning.
    It is a robust and versatile classifier, frequently used as a benchmark for complex
    classifiers such as **Neural Networks** (**NNs**) and **Support Vector Machines**
    (**SVMs**). kNN is commonly used in economic forecasting, data compression, and
    genetics based on their expression profiling.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: kNN是非参数化和基于实例的，广泛用于监督学习。它是一个强大且多功能的分类器，常用作复杂分类器（如**神经网络**(**NNs**)和**支持向量机**(**SVMs**)）的基准。kNN常用于经济预测、数据压缩和基因组学，尤其是基于基因表达分析的应用。
- en: Working Principles of kNN
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kNN的工作原理
- en: 'The idea of kNN is that from a set of features **x** we try to predict the
    labels **y**. Thus, kNN falls in a supervised learning family of algorithms. Informally,
    this means that we are given a labeled dataset consisting of training observations
    (*x*, *y*). Now, the task is to model the relationship between *x* and *y* so
    that the function *f: X→Y* learns from the unseen observation *x*. The function
    *f(x)* can confidently predict the corresponding label y prediction on a point
    *z* by looking at a set of nearest neighbors.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 'kNN的思想是，从一组特征**x**中，我们尝试预测标签**y**。因此，kNN属于监督学习算法的范畴。非正式地说，这意味着我们有一个标记的数据集，由训练观测值(*x*，*y*)组成。现在，任务是建模*x*和*y*之间的关系，使得函数*f:
    X→Y*从未见过的观测值*x*中学习。通过观察一组最近邻点，函数*f(x)*可以自信地预测点*z*的相应标签y。'
- en: 'However, the actual method of prediction depends on whether or not we are doing
    regression (continuous) or classification (discrete). For discrete classification
    targets, the prediction may be given by a maximum voting scheme weighted by the
    distance to the prediction point:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际的预测方法取决于我们是进行回归（连续）还是分类（离散）。对于离散分类目标，预测可以通过一个最大投票方案给出，该方案根据距离预测点的远近进行加权：
- en: '![Working Principles of kNN](img/03_17.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![kNN的工作原理](img/03_17.jpg)'
- en: Here, our prediction, *f(z)*, is the maximum weighted value of all classes,
    *j*, where the weighted distance from the prediction point to the training point,
    *i*, is given by *φ (dij)*, where *d* indicates the distance between two points.
    On the other hand, *Iij* is just an indicator function if point *i* is in class
    *j*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的预测*f(z)*是所有类别*j*的最大加权值，其中预测点到训练点的加权距离由*φ(dij)*给出，其中*d*表示两点之间的距离。另一方面，*Iij*是一个指示函数，表示点*i*是否属于类别*j*。
- en: 'For continuous regression targets, the prediction is given by a weighted average
    of all *k* points nearest to the prediction:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续回归目标，预测值由与预测点最近的所有*k*点的加权平均值给出：
- en: '![Working Principles of kNN](img/03_18.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![kNN的工作原理](img/03_18.jpg)'
- en: 'From the previous two equations, it is clear that the prediction is heavily
    dependent on the choice of the distance metric, *d*. There are many different
    specifications of distance metrics such as L1 and L2 metrics can be used for the
    textual distances:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的两个方程可以看出，预测结果在很大程度上依赖于距离度量的选择*d*。有很多不同的距离度量规范，例如L1和L2度量可用于文本距离：
- en: A straightforward way to weigh the distances is by the distance itself. Points
    that are further away from our prediction should have less impact than the nearer
    points. The most common way to weigh is the normalized inverse of the distance.
    We will implement this method in the next section.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的加权方法是根据距离本身进行加权。距离预测点更远的点应该比离得更近的点影响小。最常见的加权方式是距离的归一化逆值。我们将在下一节中实现此方法。
- en: Implementing a kNN-Based Predictive Model
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基于kNN的预测模型
- en: 'To illustrate how making predictions with the nearest neighbors works in TensorFlow,
    we will use the 1970s Boston housing dataset, which is available through the UCI
    ML repository at [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data).
    The following table shows the basic description of the dataset:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何在TensorFlow中使用最近邻进行预测，我们将使用1970年代的波士顿住房数据集，该数据集可通过UCI ML仓库获取，网址为[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data)。下表显示了数据集的基本描述：
- en: '![Implementing a kNN-Based Predictive Model](img/03_19.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![实现基于kNN的预测模型](img/03_19.jpg)'
- en: 'Here, we will predict the median neighborhood housing value that is the last
    value named MEDV as a function of several features. Since we consider the training
    set the trained model, we will find kNNs to the prediction points and do a weighted
    average of the target value. Let''s get started:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将预测的是邻里住房中位数，即最后一个名为 MEDV 的值，作为若干特征的函数。由于我们将训练集视为训练好的模型，因此我们将找到预测点的 kNN，并对目标值进行加权平均。让我们开始吧：
- en: Loading required libraries and packages.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的库和包。
- en: 'As an entry point, we import necessary libraries and packages that will be
    needed to do predictive analytics using kNN with TensorFlow:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为入口，我们导入必要的库和包，这些将用于使用 TensorFlow 进行 kNN 预测分析：
- en: '[PRE26]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Resetting the default graph and disabling the TensorFlow warning.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置默认图并禁用 TensorFlow 警告。
- en: 'We need to reset the default TensorFlow graph using the `reset_default_graph()`
    function from TensorFlow. You must also disable all warnings due to the absence
    of GPU on your device:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要使用 TensorFlow 的 `reset_default_graph()` 函数重置默认的 TensorFlow 图。你还必须禁用所有警告，因为你的设备没有
    GPU：
- en: '[PRE27]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Loading and preprocessing the dataset.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载和预处理数据集。
- en: 'First, we will load and parse the dataset using the `get()` function from the
    requests package as follows:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们将使用 requests 包中的 `get()` 函数加载并解析数据集，如下所示：
- en: '[PRE28]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: For more information on how the previous code works, please see the documentation
    of requests package at [http://docs.python-requests.org/en/master/user/quickstart/](http://docs.python-requests.org/en/master/user/quickstart/).
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欲了解更多关于前面代码如何工作的详细信息，请参阅[requests 包的文档](http://docs.python-requests.org/en/master/user/quickstart/)。
- en: 'Then, we will separate features (predictor) from the labels:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将把特征（预测变量）和标签分开：
- en: '[PRE29]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, to get some idea of the features and labels, let''s print them as follows:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，为了了解特征和标签的大概情况，让我们按如下方式打印它们：
- en: '[PRE30]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'So, the labels are okay to work with, and these are also continuous values.
    Now, let''s see the features:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所以，标签可以正常使用，而且它们也是连续值。现在，让我们看看特征：
- en: '[PRE31]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Well, if you see these values, they are pretty unscaled to be fed to a predictive
    model. Thus, we need to apply the min-max scaling to get a better structure of
    the features so that an estimator scales and translates each feature individually,
    and it ensures that it is in the given range on the training set, that is, between
    zero and one. Since features are most important in predictive analytics, we should
    take special care of them. The following line of code does the min-max scaling:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好吧，如果你看到这些值，它们并没有经过缩放，不能直接输入到预测模型中。因此，我们需要应用最小-最大缩放来使特征结构更加合理，以便估算器能单独对每个特征进行缩放和转换，并确保其在训练集中的给定范围内，即在零到一之间。由于特征在预测分析中非常重要，我们应该特别关注它们。以下代码行完成了最小-最大缩放：
- en: '[PRE33]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now let''s print them again to check to make sure what''s changed:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在让我们再次打印它们，以检查确保发生了什么变化：
- en: '[PRE34]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Preparing the training and test set.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练集和测试集。
- en: 'Since our features are already scaled, now it''s time to split the data into
    train and test sets. Now, we split the x and y values into the train and test
    sets. We will create the training set by selecting about 75% of the rows at random
    and leave the remaining 25% for the test set:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们的特征已经进行了缩放，现在是时候将数据分成训练集和测试集了。现在，我们将 x 和 y 值分别拆分成训练集和测试集。我们将通过随机选择约 75%
    的行来创建训练集，剩余的 25% 用于测试集：
- en: '[PRE35]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Preparing the placeholders for the tensors.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备张量的占位符。
- en: 'First, we will declare the batch size. Ideally, the batch size should be equal
    to the size of features in the test set:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们将声明批量大小。理想情况下，批量大小应该等于测试集中特征的大小：
- en: '[PRE36]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we need to declare the placeholders for the TensorFlow tensors, as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们需要声明 TensorFlow 张量的占位符，如下所示：
- en: '[PRE37]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Defining the distance metrics.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义距离度量。
- en: 'For this example, we are going to use the L1 distance. The reason is that using
    L2 did not give a better result in my case:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用 L1 距离。原因是使用 L2 距离在我的案例中没有得到更好的结果：
- en: '[PRE38]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Implementing kNN.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 kNN。
- en: 'Now, it''s time to implement kNN. This will predict the nearest neighbors by
    getting the minimum distance index. The `kNN()` method does the trick. There are
    several steps for doing this, as follows:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，到了实现 kNN 的时候了。这将通过获取最小距离的索引来预测最近的邻居。`kNN()` 方法完成了这个任务。为此，我们需要按以下步骤操作：
- en: Get the minimum distance index.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最小距离的索引。
- en: Compute the prediction function. To do this, we will use the `top_k()`, function,
    which returns the values and indices of the largest values in a tensor. Since
    we want the indices of the smallest distances, we will instead find the k-biggest
    negative distances. Since we are predicting continuous values that is regression
    task, we also declare the predictions and **Mean Squared Error** (**MSE**) of
    the target values.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测函数。为此，我们将使用`top_k()`函数，它返回张量中最大值的值和索引。由于我们需要的是最小距离的索引，所以我们将查找k个最大的负距离。由于我们预测的是连续值，也就是回归任务，我们还声明了预测值和**均方误差**（**MSE**）的目标值。
- en: Calculate the number of loops over training data.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练数据的循环次数。
- en: Initialize the global variables.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化全局变量。
- en: Iterate the training over the number of loops calculated in step 3.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据步骤3计算的循环次数进行训练迭代。
- en: 'Now, here''s the function of kNN. It takes the number of initial neighbors
    and starts the computation. Note that although it is a widely used convention,
    here I will make it a variable to do some tuning, as shown in the following code:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，这是kNN的函数。它接受初始邻居的数量并开始计算。请注意，尽管这是一个广泛使用的约定，但在这里我将其做成一个变量以便进行一些调优，如下所示：
- en: '[PRE39]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Evaluating the classification/regression.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估分类/回归。
- en: 'Note that this function does not return the optimal `mse` value, that is, the
    lowest `mse` value, but varies over different `k` values, so this is a hyperparameter
    to be tuned. One potential technique would be to iterate the method for *k = 2*
    to, say, `11` and keeping track of the optimal `k` value that forces `kNN()` to
    produce the lowest `mse` value. First, we define a method that iterates several
    times from `2` to `11` and returns two separate lists for `mse` and `k` respectively:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，这个函数并没有返回最优的`mse`值，也就是最低的`mse`值，而是在不同的`k`值下变化，因此它是一个需要调优的超参数。一个可能的技术是将方法迭代从*k
    = 2* 到 `11`，并跟踪哪个最优的`k`值使得`kNN()`产生最低的`mse`值。首先，我们定义一个方法，迭代多次从`2`到`11`，并分别返回两个单独的列表，包含`mse`和`k`值：
- en: '[PRE40]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, it''s time to invoke the previous method and find the optimal `k` value
    for which the kNN produces the lowest `mse` value. Upon receiving the two lists,
    we create a dictionary and use the `min()`method to return the optimal `k` value,
    as follows:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，是时候调用前面的那个方法，找到使得kNN产生最低`mse`值的最优`k`值了。在收到这两个列表后，我们创建一个字典，并使用`min()`方法来返回最优的`k`值，代码如下：
- en: '[PRE41]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, let''s print `the Optimal k value` for which we get the lowest `mse` value:'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们打印出`最优的k值`，即能够获得最低`mse`值的`k`值：
- en: '[PRE42]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Running the best kNN.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行最佳kNN。
- en: 'Now we have the optimal `k`, so we will entertain calculating the nearest neighbor.
    This time we will try to return the matrices for the predicted and actual labels:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们有了最优的`k`值，我们将进行最近邻计算。这次，我们会尝试返回预测标签和实际标签的矩阵：
- en: '[PRE43]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test:
    x_batch, y_target_train: y_vals_train, y_target_test: y_batch})'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test:
    x_batch, y_target_train: y_vals_train, y_target_test: y_batch})'
- en: '[PRE44]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Evaluating the best kNN.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估最佳kNN。
- en: 'Now, we will invoke the `bestKNN()` method with the optimal value of `k` that
    was calculated in the previous step, as follows:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将调用`bestKNN()`方法，使用在前一步中计算出的最优`k`值，代码如下：
- en: '[PRE45]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, I would like to measure the prediction accuracy. Are you wondering why?
    I know the reason. You''re right. There is no significant reason for calculating
    the accuracy or precision since we are predicting the continuous values that is
    labels. Even so, I would like to show you whether it works or not:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我想要测量预测的准确性。你是不是在想为什么？我知道原因。你没错。实际上，计算准确率或精度并没有特别重要的理由，因为我们预测的是连续值，也就是标签。尽管如此，我还是想展示给你看是否有效：
- en: '[PRE46]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The previous `getAccuracy()` method computes the accuracy, which is quite low.
    This is obvious and there is no exertion. This also implies that the previous
    method is pointless. However, if you are about to predict discrete values, this
    method will obviously help you. Try it yourself with suitable data and combinations
    of the previous code.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的`getAccuracy()`方法计算了准确率，但结果很低。这是显而易见的，且没有什么困难。这也意味着前面的这个方法没有什么意义。然而，如果你即将预测的是离散值，这个方法显然会对你有所帮助。试试用适当的数据和前面代码的组合来进行测试。
- en: 'But do not to be disappointed; we have another way of looking at how our predictive
    model performs. We can still plot a histogram showing the predicted versus actual
    labels that are a prediction and actual distribution:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但不要失望；我们还有另一种方式来观察我们的预测模型表现。我们仍然可以绘制一个直方图，展示预测标签与实际标签之间的分布：
- en: '[PRE47]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Implementing a kNN-Based Predictive Model](img/03_20.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![基于kNN的预测模型实现](img/03_20.jpg)'
- en: 'Figure 16: Predicted versus actual median prices of the houses in $1,000s'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图16：房屋的预测价格与实际中位数价格（单位：千美元）
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this lesson, we have discussed unsupervised learning from a theoretical and
    practical perspective. We have seen how we can make use of predictive analytics
    and find out how we can take advantage of it to cluster records belonging to a
    certain group or class for a dataset of unsupervised observations. We have discussed
    unsupervised learning and clustering using K-means. In addition, we have seen
    how we can fine tune the clustering using the Elbow method for better predictive
    accuracy. We have also seen how to predict neighborhoods using K-means, and then,
    we have seen another example of clustering audio clips based on their audio features.
    Finally, we have seen how we can use unsupervised kNN for predicting the nearest
    neighbors.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本课中，我们从理论和实践的角度讨论了无监督学习。我们已经了解了如何利用预测分析，并发现如何利用它将属于某个组或类别的记录聚类，以便对无监督观测数据集进行分析。我们讨论了使用K均值进行无监督学习和聚类。此外，我们还了解了如何通过肘部法则来优化聚类，以提高预测准确性。我们还讨论了如何使用K均值预测邻近区域，接着，我们看到了另一个基于音频特征对音频片段进行聚类的例子。最后，我们看到了如何使用无监督kNN预测最近邻。
- en: In the next lesson, we will discuss the wonderful field of text analytics using
    TensorFlow. Text analytics is a wide area in **natural language processing** (**NLP**),
    and ML is useful in many use cases, such as sentiment analysis, chatbots, email
    spam detection, text mining, and natural language processing. You will learn how
    to use TensorFlow for text analytics with a focus on use cases of text classification
    from the unstructured spam prediction and movie review dataset. Based on the spam
    filtering dataset, we will develop predictive models using a LR algorithm with
    TensorFlow
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，我们将讨论使用TensorFlow的文本分析这一精彩领域。文本分析是**自然语言处理**（**NLP**）中的一个广泛领域，机器学习在许多应用场景中都非常有用，例如情感分析、聊天机器人、电子邮件垃圾邮件检测、文本挖掘和自然语言处理。你将学习如何使用TensorFlow进行文本分析，重点关注基于非结构化垃圾邮件预测和电影评论数据集的文本分类应用场景。在垃圾邮件过滤数据集的基础上，我们将使用LR算法和TensorFlow开发预测模型。
- en: Assessments
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: kNN falls in a ______ learning family of algorithms.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kNN属于______学习算法家族。
- en: 'State whether the following statement is True or False: A cluster is a collection
    of objects that have a similarity between them and are dissimilar to the objects
    belonging to other clusters. If a collection of objects is provided, clustering
    algorithms put these objects into groups based on similarity.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下陈述是对还是错：一个簇是具有相似性的对象集合，与属于其他簇的对象不相似。如果提供了一组对象，聚类算法会根据相似性将这些对象分组。
- en: What is the main objective of unsupervised learning?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督学习的主要目标是什么？
- en: 'State whether the following statement is True or False: In a clustering task,
    an algorithm groups related features into categories by analyzing similarities
    between input examples, where similar features are clustered and marked using
    circles.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下陈述是对还是错：在聚类任务中，算法通过分析输入示例之间的相似性，将相关特征分为类别，其中相似的特征被聚类并用圆圈标记。
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding ______ classes or clusters.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类分析是关于将数据样本或数据点划分并放入相应的______类别或簇中。
- en: Heterogynous
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 异质的
- en: Linear
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性的
- en: Homogeneous
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同质的
- en: Similar.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相似的。
