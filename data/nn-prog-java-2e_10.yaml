- en: Chapter 10. Current Trends in Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 神经网络当前趋势
- en: 'This final chapter shows the reader the most recent trends in neural networks.
    Although this book is introductory, it is always useful to be aware of the latest
    developments and where the science behind this theory is going to. Among the latest
    advancements is the so-called **deep learning**, a very popular research field
    for many data scientists; this type of network is briefly covered in this chapter.
    Convolutional and cognitive architectures are also in this trend and gaining popularity
    for multimedia data recognition. Hybrid systems that combine different architectures
    are a very interesting strategy for solving more complex problems, as well as
    applications that involve analytics, data visualization, and so on. Being more
    theoretical, there is no actual implementation of the architectures, although
    an example of implementation for a hybrid system is provided. Topics covered in
    this chapter include:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向读者展示了神经网络领域的最新趋势。尽管本书是入门级的，但了解最新的发展以及这一理论背后的科学走向总是很有用的。最新的进展之一是所谓的**深度学习**，这是许多数据科学家非常受欢迎的研究领域；这一类型的网络在本章中有所概述。卷积和认知架构也属于这一趋势，并在多媒体数据识别方面越来越受欢迎。结合不同架构的混合系统是解决更复杂问题以及涉及分析、数据可视化等应用的一种非常有趣的策略。由于更偏向理论，这些架构并没有实际的实现，尽管提供了一个混合系统实现的例子。本章涵盖的主题包括：
- en: Deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习
- en: Convolutional neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Long short term memory networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: Hybrid systems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合系统
- en: Neuro-Fuzzy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经模糊
- en: Neuro-Genetic
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经遗传学
- en: Implementation of a hybrid neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合神经网络的实现
- en: Deep learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: One of the latest advancements in neural networks is the so-called deep learning.
    Nowadays it is nearly impossible to talk about neural networks without mentioning
    deep learning, because the recent research on feature extraction, data representation,
    and transformation has found that many layers of processing information are able
    to abstract and produce better representations of data for learning. Throughout
    this book we have seen that neural networks require input data in numerical form,
    no matter if the original data is categorical or binary, neural networks cannot
    process non-numerical data directly. But it turns out that in the real world most
    of the data is non-numerical or is even unstructured, such as images, videos,
    audios, texts, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络领域最新的进展之一是所谓的深度学习。如今，如果不提及深度学习，几乎不可能谈论神经网络，因为最近在特征提取、数据表示和转换方面的研究已经发现，许多层的处理信息能够抽象并产生更好的数据表示以供学习。贯穿本书，我们看到了神经网络需要以数值形式输入数据，无论原始数据是分类的还是二元的，神经网络不能直接处理非数值数据。但现实世界中的大多数数据是非数值的，甚至是未结构化的，如图像、视频、音频、文本等。
- en: In this sense a deep network would have many layers that could act as data processing
    units to transform this data and provide it to the next layer for subsequent data
    processing. This is analogous to the process that happens in the brain, from the
    nerve endings to the cognitive core; in this long path the signals are processed
    by multiple layers before resulting in signals that control the human body. Currently,
    most of the research on deep learning has been on the processing of unstructured
    data, particularly image and sound recognition and natural language processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种意义上，一个深度网络将会有许多层，这些层可以作为数据处理单元来转换这些数据，并将其提供给下一层以进行后续的数据处理。这与大脑中发生的过程类似，从神经末梢到认知核心；在这个漫长的过程中，信号被多层处理，最终产生控制人体的信号。目前，大多数深度学习研究集中在非结构化数据处理上，尤其是图像和声音识别以及自然语言处理。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Deep learning is still under development and much has changed since 2012\. Big
    companies such as Google and Microsoft have teams for research on this field and
    much is likely to change in the next couple of years.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习仍在发展中，自2012年以来已经发生了很大的变化。像谷歌和微软这样的大公司都有研究这个领域的团队，未来几年可能会有很多变化。
- en: 'A scheme of a deep learning architecture is shown in the following figure:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图中展示了深度学习架构的方案：
- en: '![Deep learning](img/B5964_10_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习](img/B5964_10_01.jpg)'
- en: On the other hand, deep neural networks have some problems that need to be overcome.
    The main problem is overfitting. The many layers that produce new representations
    of data are very sensitive to the training data, because the deeper the signals
    reach in the neural layers, the more specific the transformation will be for the
    input data. Regularization methods and pruning are often applied to prevent overfitting.
    Computation time is another common issue in training deep networks. The standard
    backpropagation algorithm can take a very long time to train a deep neural network,
    although strategies such as selecting a smaller training dataset can speed up
    the training time. In addition, in order to train a deep neural network, it is
    often recommended to use a faster machine and parallelize the training as much
    as possible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，深度神经网络存在一些需要克服的问题。主要问题是过拟合。产生数据新表示的许多层对训练数据非常敏感，因为信号在神经网络层中的深度越深，对输入数据的转换就越具体。正则化方法和剪枝通常被用来防止过拟合。计算时间是训练深度网络时的另一个常见问题。标准的反向传播算法训练深度神经网络可能需要非常长的时间，尽管选择较小的训练数据集等策略可以加快训练时间。此外，为了训练深度神经网络，通常建议使用更快的机器并将训练尽可能并行化。
- en: Deep architectures
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度架构
- en: 'There is a great variety of deep neural architectures with both feedforward
    and feedback flows, although they are typically feedforward. Main architectures
    are, without limitation to:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们通常是前馈的，但存在各种具有前馈和反馈流的深度神经网络架构。主要架构包括但不限于：
- en: '**Convolutional neural network**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**'
- en: In this architecture, the layers may have multidimensional organization. Inspired
    by the visual cortex of animals, the typical dimensionality applied to the layers
    is three-dimensional. In **convolutional neural networks** (**CNNs**), part of
    the signals of a preceding layer is fed into another part of neurons in the following
    layer. This architecture is feedforward and is well applied for image and sound
    recognition. The main feature that distinguishes this architecture from Multilayer
    Perceptrons is the partial connectivity between layers. Considering the fact that
    not all neurons are relevant for a certain neuron in the next layer, the connectivity
    is local and respects the correlation between neurons. This prevents both long
    time training and overfitting, provided that a fully connected MLP blows up the
    number of weight as the dimension of images grows, for example. In addition, neurons
    in layers are arranged in dimensions, typically three, thereby staked in an array
    in width, height, and depth.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，层可能具有多维组织。受到动物视觉皮层的启发，应用于层的典型维度是三维的。在**卷积神经网络**（**CNNs**）中，前一层的部分信号被输入到下一层中的一些神经元。这种架构是前馈的，非常适合图像和声音识别。这种架构与多层感知器的主要区别在于层之间的部分连接性。考虑到并非所有神经元都对下一层的某个神经元相关，连接性是局部的，并尊重神经元之间的相关性。这既防止了长时间训练，也防止了过拟合，因为当图像的维度增长时，完全连接的MLP会爆炸性地增加权重的数量。此外，层的神经元在维度上排列，通常是三维的，因此以宽度、高度和深度堆叠成数组。
- en: '![Deep architectures](img/B5964_10_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![深度架构](img/B5964_10_02.jpg)'
- en: In this architecture, the layers may have multidimensional organization. Inspired
    by the visual cortex of animals, the typical dimensionality applied to the layers
    is three-dimensional. In **convolutional neural networks** (**CNNs**), part of
    the signals of a preceding layer is fed into another part of neurons in the following
    layer. This architecture is feedforward and is well applied for image and sound
    recognition. The main feature that distinguishes this architecture from multilayer
    perceptrons is the partial connectivity between layers. Considering the fact that
    not all neurons are relevant for a certain neuron in the next layer, the connectivity
    is local and respects the correlation between neurons. This prevents both long
    time training and overfitting, provided that a fully connected MLP blows up the
    number of weight as the dimension of images grows, for example. In addition, neurons
    in layers are arranged in dimensions, typically three, thereby staked in an array
    in width, height, and depth.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，层可能具有多维组织。受动物视觉皮层的启发，应用于层的典型维度是三维的。在**卷积神经网络**（CNNs）中，前一层的部分信号被输入到下一层中的一些神经元。这种架构是前馈的，并且适用于图像和声音识别。这种架构与多层感知器的主要区别在于层之间的部分连接性。考虑到并非所有神经元都对下一层的某个神经元都相关，连接是局部的，并尊重神经元之间的相关性。这防止了长时间训练和过拟合，因为如果是一个全连接的MLP，随着图像维度的增长，权重数量会爆炸。此外，层的神经元在维度上排列，通常是三维的，因此以宽度、高度和深度堆叠在数组中。
- en: '**Long short-term memory**: This is a recurrent type of neural network that
    takes into account always the last value of the hidden layer, exactly like a **hidden
    Markov model** (**HMM**). A **Long Short Time Memory network** (**LSTM**) has
    LSTM units instead of traditional neurons, and these units implement operations
    such as store and forget a value to control the flow in a deep network. This architecture
    is well applied to natural language processing, due to the capacity of retaining
    information for a long time while receiving completely unstructured data such
    as audio or text files. One way to train this type of network is the backpropagation
    through time (BPTT) algorithm, but there are also other algorithms such as reinforcement
    learning or evolution strategies.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**：这是一种循环神经网络，它始终考虑隐藏层的最后一个值，就像一个**隐藏马尔可夫模型**（HMM）。**长短期记忆网络**（LSTM）使用LSTM单元而不是传统神经元，这些单元执行存储和忘记值等操作，以控制深度网络中的流动。这种架构由于能够保留长时间的信息，同时接收完全无结构的音频或文本文件等数据，因此在自然语言处理中得到了很好的应用。训练这种类型网络的一种方法是时间反向传播（BPTT）算法，但还有其他算法，如强化学习或进化策略。'
- en: '![Deep architectures](img/B5964_10_03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![深度架构](img/B5964_10_03.jpg)'
- en: '**Deep belief network**: **Deep belief networks** (**DBN''s**) are probabilistic
    models where layers are classified into visible and hidden. This is also a type
    of recurrent neural network based on a **restricted Boltzmann machine** (**RBM**).
    It is typically used as a first step in the training of a **deep neural network**
    (**DNN**), which is further trained by other supervised algorithms such as backpropagation.
    In this architecture each layer acts like a feature detector, abstracting new
    representations of data. The visible layer acts both as an output and as an input,
    and the deepest hidden layer represents the highest level of abstraction. Applications
    of this architecture are typically the same as those of convolutional neural networks.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度信念网络**：**深度信念网络**（DBN''s）是一种概率模型，其中层被分类为可见和隐藏。这也是一种基于**受限玻尔兹曼机**（RBM）的循环神经网络。它通常用作**深度神经网络**（DNN）训练的第一步，然后通过其他监督算法如反向传播进一步训练。在这个架构中，每一层都像一个特征检测器，抽象出数据的新表示。可见层既作为输出也作为输入，最深层的隐藏层代表最高层次的抽象。这种架构的应用通常与卷积神经网络相同。'
- en: '![Deep architectures](img/B5964_10_04.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![深度架构](img/B5964_10_04.jpg)'
- en: How to implement deep learning in Java
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在Java中实现深度学习
- en: 'Because this book is introductory, we are not diving into further details on
    deep learning in this chapter. However, some recommendations of code for a deep
    architecture are provided. An example on how a convolutional neural network would
    be implemented is provided here. One needs to implement a class called `ConvolutionalLayer`
    to represent a multidimensional layer, and a `CNN` class for the convolutional
    neural network itself:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本书是入门级的，我们在这个章节中不会深入探讨深度学习的细节。然而，提供了一些关于深度架构的代码推荐。这里提供了一个关于卷积神经网络如何实现的例子。需要实现一个名为`ConvolutionalLayer`的类来表示多维层，以及一个名为`CNN`的类来表示卷积神经网络本身：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this class, the neurons are organized in dimensions and methods for pruning
    are used to make the connections between the layers. Please see the files `ConvolutionalLayer.java`
    and `CNN.java` for further details.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，神经元按维度和组织，并使用剪枝方法来使层之间的连接。请参阅文件`ConvolutionalLayer.java`和`CNN.java`以获取更多详细信息。
- en: Since the other architectures are recurrent and this book does not cover the
    recurrent neural networks (for simplicity purposes in an introductory book) they
    are provided only for the reader's information. We suggest the reader to take
    a look at the references provided to find out more on these architectures.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其他架构是循环的，而本书没有涵盖循环神经网络（为了入门书籍的简洁性），它们仅提供供读者参考。我们建议读者查看提供的参考文献，以了解更多关于这些架构的信息。
- en: Hybrid systems
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合系统
- en: In machine learning, or even in the artificial intelligence field, there are
    many other algorithms and techniques other than neural networks. Each technique
    has its strengths and drawbacks, and that inspires many researchers to combine
    them into a single structure. Neural networks are part of the connectionist approach
    for artificial intelligence, whereby operations are performed on numerical and
    continuous values; but there are other approaches that include cognitive (rule-based
    systems) and evolutionary computation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习或甚至人工智能领域，除了神经网络之外，还有许多其他算法和技术。每种技术都有其优势和劣势，这激励了许多研究人员将它们结合成一个单一的结构。神经网络是人工智能连接主义方法的一部分，其中操作是在数值和连续值上进行的；但还有其他方法，包括认知（基于规则的系统）和进化计算。
- en: '| Connectionist | Cognitive | Evolutionary |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 连接主义 | 认知 | 进化 |'
- en: '| --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Numerical processing | Symbolic processing | Numerical and symbolic processing
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 数值处理 | 符号处理 | 数值和符号处理 |'
- en: '| Large network structures | Large rule bases and premises | Large quantity
    of solutions |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 大型网络结构 | 大型规则库和前提 | 大量解决方案 |'
- en: '| Performance by statistics | Design by experts/statistics | Better solutions
    are produced every iteration |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 通过统计进行性能 | 通过专家/统计进行设计 | 每次迭代产生更好的解决方案 |'
- en: '| Highly sensitive to data | Highly sensitive to theory | Local minima proof
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 对数据高度敏感 | 对理论高度敏感 | 局部最小值证明 |'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Fuzzy logic is a type of rule-based processing, where every variable is converted
    to a symbolic value according to a membership function, and then the combination
    of all variables is queried against an *IF-THEN* rule database.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊逻辑是一种基于规则的加工方式，其中每个变量都根据隶属函数转换为符号值，然后所有变量的组合将针对一个*IF-THEN*规则数据库进行查询。
- en: '![Neuro-fuzzy](img/B5964_10_05.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![神经模糊](img/B5964_10_05.jpg)'
- en: A membership function usually has a Gaussian bell shape, which tells us how
    much a given value is a *member* of that class. Let's take, for example, temperature,
    which may take on three different classes (cold, normal, and warm). A membership
    value will be higher the more the temperature is closer to the bell shape centers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 隶属函数通常具有高斯钟形形状，这告诉我们给定值属于该类有多少程度。以温度为例，它可能具有三个不同的类别（冷、正常和暖）。隶属值将随着温度越接近钟形中心而越高。
- en: '![Neuro-fuzzy](img/B5964_10_06.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![神经模糊](img/B5964_10_06.jpg)'
- en: 'Furthermore, the fuzzy processing finds which rules are fired by every input
    record and which output values are produced. A neuro-fuzzy architecture treats
    each input differently, so the first hidden layer has a set of neurons for each
    input corresponding for each membership function:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模糊处理找出每个输入记录触发的规则以及产生的输出值。神经模糊架构对每个输入的处理方式不同，因此第一个隐藏层有一组神经元对应于每个隶属函数：
- en: '![Neuro-fuzzy](img/B5964_10_07.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![神经模糊](img/B5964_10_07.jpg)'
- en: Tip
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In this architecture, the training finds optimal weights for the rule processing
    and weighted sum of consequent parameters only, the first hidden layer has no
    adjustable weights.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，训练仅找到规则处理和后续参数加权的最优权重，第一隐藏层没有可调整的权重。
- en: In fuzzy logic architecture, the experts define a rule database that may become
    huge as the number of variables increase. The neuro-fuzzy architecture releases
    the designer from defining the rules, and lets this task be performed by the neural
    network. The training of a neuro-fuzzy can be performed by gradient type algorithms
    such as backpropagation or matrix algebra such as least squares, both in the supervised
    mode. Neuro-fuzzy systems are suitable for control of dynamic systems and diagnostics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在模糊逻辑架构中，专家定义了一个规则数据库，随着变量数量的增加，这个数据库可能会变得很大。神经模糊架构释放了设计者定义规则的需求，并让神经网络来完成这项任务。神经模糊的训练可以通过梯度类型算法，如反向传播或矩阵代数，如最小二乘法，在监督模式下进行。神经模糊系统适用于动态系统的控制和诊断。
- en: Neuro-genetic
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经遗传
- en: In the evolutionary artificial intelligence approach, one common strategy is
    genetic algorithms. This name is inspired by natural evolution, which states that
    beings more adapted to the environment are able to produce new generations of
    better adapted beings. In the computing intelligence field, the *beings* or *individuals*
    are candidate solutions or hypotheses that can solve an optimization problem.
    Supervised neural networks are used for optimization, since there is an error
    measure that we want to minimize by adjusting the neural weights. While the training
    algorithms are able to find better weights by gradient methods, they often fall
    in local minima. Although some mechanisms, such as regularization and momentum,
    may improve the results, once the weights fall in a local minimum, it is very
    unlikely that a better weight will be found, and in this context genetic algorithms
    are very good at it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在进化人工智能方法中，一种常见的策略是遗传算法。这个名字的灵感来源于自然进化，它表明更能适应环境的生物能够产生新一代更适应环境的生物。在计算智能领域，*生物*或*个体*是能够解决优化问题的候选解或假设。由于存在一个我们希望通过调整神经网络权重来最小化的错误度量，因此监督神经网络用于优化。虽然训练算法能够通过梯度方法找到更好的权重，但它们往往陷入局部最小值。尽管一些机制，如正则化和动量，可能会改善结果，但一旦权重陷入局部最小值，找到更好的权重的可能性非常小，在这种情况下，遗传算法在这方面非常擅长。
- en: Think of the neural weights as a genetic code (or DNA). If we could generate
    a finite number of random generated weight sets, and evaluate which produce the
    best results (smaller errors or other performance measurement), we would select
    a top N best weight, and then set and apply genetic operations on them, such as
    reproduction (interchange of weights) and mutation (random change of weights).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络权重想象成遗传代码（或DNA）。如果我们能够生成有限数量的随机生成的权重集，并评估哪些产生最佳结果（较小的错误或其他性能度量），我们将选择前N个最佳权重，然后对它们进行设置和应用遗传操作，如繁殖（权重交换）和变异（随机改变权重）。
- en: '![Neuro-genetic](img/B5964_10_08.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![神经遗传](img/B5964_10_08.jpg)'
- en: This process is repeated until some acceptable solution is found.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，直到找到某个可接受的解决方案。
- en: Another strategy is to use genetic operations on neural network parameters,
    such as number of neurons, learning rate, activation functions, and so on. Considering
    that, there is always a need to adjust parameters or train multiple times to ensure
    we've found a good solution. So, one may code all parameters in a genetic code
    (parameter set) and generate multiple neural networks for each parameter set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是对神经网络参数使用遗传操作，例如神经元数量、学习率、激活函数等。考虑到这一点，我们总是需要调整参数或多次训练以确保我们找到了一个好的解决方案。因此，可以将所有参数编码在遗传代码（参数集）中，并为每个参数集生成多个神经网络。
- en: 'The scheme of a genetic algorithm is shown in the following figure:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法的方案如下所示：
- en: '![Neuro-genetic](img/B5964_10_09.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![神经遗传](img/B5964_10_09.jpg)'
- en: Tip
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Genetic algorithms are broadly used for many optimization problems, but in this
    book we are sticking with these two classes of problems, weight and parameter
    optimization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法被广泛用于许多优化问题，但在这本书中，我们坚持使用这两类问题，即权重和参数优化。
- en: Implementing a hybrid neural network
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现混合神经网络
- en: 'Now, let''s implement a simple code that can be used in the neuro-fuzzy and
    neuro-genetic networks. First, we need to define Gaussian functions for activation
    that will be the membership functions:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个可以在神经模糊和神经遗传网络中使用的简单代码。首先，我们需要定义激活函数的高斯函数，这些函数将是隶属函数：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The fuzzy sets and rules need to be represented in a way that a neural network
    can understand and drive the execution. This representation includes the quantity
    of sets per input, therefore having the information on how the neurons are connected;
    and the membership functions for each set. A simple way to represent the quantity
    is an array. The array of sets just indicates how many sets there are for each
    variable; and the array of rules is a matrix, where each row represents a rule
    and each column represents a variable; each set can be assigned a numerical integer
    value for reference in the rule array. An example of three variables, each having
    three sets, is defined in the following snippet, along with the rules:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊集和规则需要以一种神经网络可以理解和驱动执行的方式表示。这种表示包括每个输入的集合数量，因此包含有关神经元如何连接的信息；以及每个集合的隶属函数。表示数量的简单方法是一个数组。集合数组仅指示每个变量有多少个集合；规则数组是一个矩阵，其中每一行代表一个规则，每一列代表一个变量；每个集合可以在规则数组中分配一个数值整数作为参考。以下是一个包含三个变量，每个变量有三个集合的示例，以及相应的规则：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The membership functions can be referenced in a serialized array:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 隶属函数可以参考一个序列化数组：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We need also to create classes for the layers of a neuro fuzzy architecture,
    such as `InputFuzzyLayer` and `RuleLayer`. They can be children of a `NeuroFuzzyLayer`
    superclass, which can inherit from `NeuralLayer`. These classes are necessary
    because they work differently from the already defined neural layer:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要创建神经模糊架构层的类，例如`InputFuzzyLayer`和`RuleLayer`。它们可以是`NeuroFuzzyLayer`超类的子类，该超类可以继承自`NeuralLayer`。这些类是必要的，因为它们的工作方式与已经定义的神经网络层不同：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A `NeuroFuzzy` class will inherit from `NeuralNet`, having references to the
    other fuzzy layer classes. The `calc()`methods of the `NeuroFuzzyLayer` will also
    be different, taking into account the membership functions centers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeuroFuzzy`类将继承自`NeuralNet`，并引用其他模糊层类。`NeuroFuzzyLayer`的`calc()`方法也将不同，考虑到隶属函数的中心：'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For more details, see the files in the `edu.packt.neuralnet.neurofuzzy` package.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节，请参阅`edu.packt.neuralnet.neurofuzzy`包中的文件。
- en: 'To code a neuro-genetic for weight sets, one needs to define the genetic operations.
    Let''s create a class called `NeuroGenetic` to implement reproduction and mutation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要为权重集编写神经遗传算法，需要定义遗传操作。让我们创建一个名为`NeuroGenetic`的类来实现繁殖和变异：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next step is to define the evaluation of each weight on each iteration:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义每次迭代的每个权重的评估：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can just call a neuro-genetic algorithm by using the following
    code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过以下代码调用神经遗传算法：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this final chapter, we gave the reader a glimpse of what to do next in this
    field. Being more theoretical, this chapter has focused more on the functionality
    and information than on practical implementation, because this would be very heavy
    for an introductory book. In every case, a simple code is provided to give a hint
    on how to further implement deep neural networks. The reader is then encouraged
    to modify the codes of the previous chapters, adapting them to the hybrid neural
    networks and comparing the results. Being a very dynamic and novel field of research,
    at every moment new approaches and algorithms are under development, and we provide
    in the references a list of publications to stay up to date on this subject.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章的最后，我们向读者展示了在这个领域下一步应该做什么。由于更偏向理论，这一章更多地关注功能和信息，而不是实际实现，因为这将对于一个入门书籍来说过于复杂。在每种情况下，都提供了一个简单的代码示例，以提示如何进一步实现深度神经网络。然后鼓励读者修改前几章的代码，将它们适应混合神经网络，并比较结果。作为一个非常动态和新兴的研究领域，在每一个时刻都有新的方法和算法在开发中，我们在参考文献中提供了一份出版物列表，以保持对这个主题的最新了解。
