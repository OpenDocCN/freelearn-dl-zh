- en: Natural Language Processing with TF-IDF
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 进行自然语言处理
- en: 'In this chapter, the following recipes will be covered:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下内容：
- en: Downloading the therapy bot session text dataset
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载治疗机器人会话文本数据集
- en: Analyzing the therapy bot session dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析治疗机器人会话数据集
- en: Visualizing word counts in the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据集中的词频
- en: Calculating sentiment analysis of text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算文本的情感分析
- en: Removing stop words from the text
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中去除停用词
- en: Training the TF-IDF model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 TF-IDF 模型
- en: Evaluating TF-IDF model performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 TF-IDF 模型性能
- en: Comparing model performance to a baseline score
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型性能与基准分数进行比较
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Natural language processing** (**NLP**) is all over the news lately, and
    if you ask five different people, you will get ten different definitions. Recently
    NLP has been used to help identify bots or trolls on the internet trying to spread
    fake news or, even worse, tactics such as cyberbullying. In fact, recently there
    was a case in Spain where a student at a school was getting cyberbullied through
    social media accounts and it was having such a serious effect on the health of
    the student that the teachers started to get involved. The school reached out
    to researchers who were able to help identify several potential sources for the
    trolls using NLP methods such as TF-IDF. Ultimately, the list of potential students
    was presented to the school and when confronted the actual suspect admitted to
    being the perpetrator. The story was published in a paper titled *Supervised Machine
    Learning for the Detection of Troll Profiles in Twitter Social Network: Application
    to a Real Case of Cyberbullying* by Patxi Galan-Garcıa, Jose Gaviria de la Puerta,
    Carlos Laorden Gomez, Igor Santos, and Pablo Garcıa Bringas.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）最近在新闻中频繁出现，如果你问五个人，你可能会得到十个不同的定义。最近，NLP 被用来帮助识别互联网上的机器人或水军，这些人试图传播假新闻，甚至更糟，像网络欺凌这样的战术。事实上，最近在西班牙发生了一起案件，一名学生在社交媒体账号上遭遇网络欺凌，这对学生的健康产生了严重影响，老师们开始介入。学校联系了研究人员，他们利用
    NLP 方法，如 TF-IDF，帮助识别出几个可能的网络水军来源。最终，潜在的学生名单被提交给学校，并且在对质时，实际嫌疑人承认了自己是施害者。这一故事被发表在题为《*监督式机器学习用于
    Twitter 社交网络中水军档案的检测：应用于一起真实的网络欺凌案例*》的论文中，作者为 Patxi Galan-García、Jose Gaviria
    de la Puerta、Carlos Laorden Gomez、Igor Santos 和 Pablo García Bringas。'
- en: 'This paper highlights the ability to utilize several varying methods to analyze
    text and develop human-like language processing. It is this methodology that incorporates
    NLP into machine learning, deep learning, and artificial intelligence. Having
    machines able to ingest text data and potentially make decisions from that same
    text data is the core of natural language processing. There are many algorithms
    that are used for NLP, such as the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点介绍了利用多种方法分析文本并开发类人语言处理的能力。正是这种方法论将自然语言处理（NLP）融入到机器学习、深度学习和人工智能中。让机器能够理解文本数据并从中做出可能的决策，是自然语言处理的核心。用于自然语言处理的算法有很多，如下所示：
- en: TF-IDF
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Word2Vec
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: N-grams
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-grams
- en: Latent Dirichlet allocation (LDA)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）
- en: Long short-term memory (LSTM)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: This chapter will focus on a dataset that contains conversations between an
    individual and a chatbot from an online therapy website. The purpose of the chatbot
    is to recognize conversations that need to be flagged for immediate attention
    to an individual rather than continued discussion with the chatbot. Ultimately,
    we will focus on using a TF-IDF algorithm to perform text analysis on the dataset
    to determine whether the chat conversation warrants a classification that needs
    to be escalated to an individual or not. **TF-IDF** stands for **Term Frequency-Inverse
    Document Frequency**. This is a technique commonly used in algorithms to identify
    the importance of a word in a document. Additionally, TF-IDF is easy to compute
    especially when dealing with high word counts in documents and has the ability
    to measure the uniqueness of a word. This comes in handy when dealing with a chatbot
    data. The main goal is to quickly identify a unique word that would trigger escalation
    to an individual to provide immediate support.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论一个包含个体与在线治疗网站上的聊天机器人对话的数据集。该聊天机器人的目的是识别需要立即转交给个体而不是继续与聊天机器人讨论的对话。最终，我们将重点使用TF-IDF算法对数据集进行文本分析，以确定聊天对话是否需要被分类为需要升级给个体处理的情况。**TF-IDF**代表**词频-逆文档频率**。这是一种常用于算法中的技术，用于识别一个单词在文档中的重要性。此外，TF-IDF特别容易计算，尤其是在处理文档中的高词汇量时，并且能够衡量一个词的独特性。在处理聊天机器人数据时，这一点非常有用。主要目标是快速识别出一个独特的词，触发升级到个体的处理，从而提供即时支持。
- en: Downloading the therapy bot session text dataset
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载治疗机器人会话文本数据集
- en: This section will focus on downloading and setting up the dataset that will
    be used for NLP in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点介绍下载和设置将用于本章自然语言处理（NLP）的数据集。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The dataset that we will use in this chapter is based on interactions between
    a therapy bot and visitors to an online therapy website. It contains 100 interactions
    and each interaction is tagged as either `escalate` or `do_not_escalate`. If the
    discussion warrants a more serious conversation, the bot will tag the discussion
    as `escalate` to an individual. Otherwise, the bot will continue the discussion
    with the user.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集基于治疗机器人与在线治疗网站访客之间的互动。它包含100次互动，每次互动都被标记为`escalate`或`do_not_escalate`。如果讨论需要更严肃的对话，机器人将标记讨论为`escalate`，转交给个体处理。否则，机器人将继续与用户讨论。
- en: How it works...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: This section walks through the steps for downloading the chatbot data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍下载聊天机器人数据集的步骤。
- en: Access the dataset from the following GitHub repository: [https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data](https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data)
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下GitHub仓库访问数据集：[https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data](https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data)
- en: 'Once you arrive at the repository, right-click on the file seen in the following
    screenshot:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到达仓库后，右键点击下图所示的文件：
- en: '![](img/5333b70f-7e8a-43ff-a537-87c7392b5af1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5333b70f-7e8a-43ff-a537-87c7392b5af1.png)'
- en: Download `TherapyBotSession.csv` and save to the same local directory as the
    Jupyter notebook `SparkSession`.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`TherapyBotSession.csv`并保存到与Jupyter笔记本`SparkSession`相同的本地目录中。
- en: 'Access the dataset through the Jupyter notebook using the following script
    to build the `SparkSession` called `spark`, as well as to assign the dataset to
    a dataframe in Spark, called `df`:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Jupyter笔记本使用以下脚本访问数据集，以构建名为`spark`的`SparkSession`，并将数据集分配给Spark中的一个数据框（dataframe），名为`df`：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: This section explains how the chatbot data makes its way into our Jupyter notebook.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了聊天机器人数据如何进入我们的Jupyter笔记本。
- en: 'The contents of the dataset can be seen by clicking on TherapyBotSession.csv
    on the repository as seen in the following screenshot:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过点击仓库中的TherapyBotSession.csv查看数据集的内容，如下图所示：
- en: '![](img/b237f63c-d351-495b-bb66-37dcc5dac0d7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b237f63c-d351-495b-bb66-37dcc5dac0d7.png)'
- en: 'Once the dataset is downloaded, it can be uploaded and converted into a dataframe,
    `df`. The dataframe can be viewed by executing `df.show()`, as seen in the following
    screenshot:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集下载完成，它就可以上传并转换成数据框`df`。可以通过执行`df.show()`来查看数据框，如下图所示：
- en: '![](img/b869c90a-6637-4115-b795-9a9ed5a4af32.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b869c90a-6637-4115-b795-9a9ed5a4af32.png)'
- en: 'There are 3 main fields that are of particular interest to us from the dataframe:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据框中有3个主要字段是我们特别关注的：
- en: '`id`: the unique id of each transaction between a visitor to the website and
    the chatbot.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`id`：每次访客与网站聊天机器人之间交易的唯一ID。'
- en: '`label`: since this is a supervised modeling approach where we know the outcome
    that we are trying to predict, each transaction has been classified as either
    `escalate` or `do_not_escalate`. This field will be used during the modeling process
    to train the text to identify words that would classify falling under one of these
    two scenarios.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`label`：由于这是一个监督学习方法，我们已经知道我们试图预测的结果，因此每笔交易已被分类为`escalate`或`do_not_escalate`。这个字段将在建模过程中使用，用来训练文本识别哪些单词会被归类为这两种情况之一。'
- en: '`chat`: lastly we have the `chat` text from the visitor on the website that
    our model will classify.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`chat`：最后，我们有网站访客的`chat`文本，模型将对其进行分类。'
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The dataframe, `df`, has some additional columns, `_c3`, `_c4`, `_c5`, and
    `_c6` that will not be used in the model and therefore, can be excluded from the
    dataset using the following script:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框`df`有一些额外的列，`_c3`、`_c4`、`_c5`和`_c6`，这些列在模型中不会使用，因此可以使用以下脚本将其从数据集中排除：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the script can be seen in the following screenshot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的输出可以在下图中看到：
- en: '![](img/ff0dad66-0172-447e-897f-7d74f9138643.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff0dad66-0172-447e-897f-7d74f9138643.png)'
- en: Analyzing the therapy bot session dataset
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析治疗机器人会话数据集
- en: It is always important to first analyze any dataset before applying models on
    that same dataset
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集应用模型之前，首先分析数据集总是很重要的。
- en: Getting ready
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section will require importing `functions` from `pyspark.sql` to be performed
    on our dataframe.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要从`pyspark.sql`导入`functions`，以便在我们的数据框上执行。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The following section walks through the steps to profile the text data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将演示如何对文本数据进行剖析。
- en: 'Execute the following script to group the `label` column and to generate a
    count distribution:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以对`label`列进行分组并生成计数分布：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add a new column, `word_count`, to the dataframe, `df`, using the following
    script:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将一个新列`word_count`添加到数据框`df`中：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Aggregate the average word count, `avg_word_count`, by `label` using the following
    script:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本按`label`聚合平均单词数，`avg_word_count`：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The following section explains the feedback obtained from analyzing the text
    data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了通过分析文本数据获得的反馈。
- en: 'It is useful to collect data across multiple rows and group the results by
    a dimension. In this case, the dimension is `label`. A `df.groupby()` function
    is used to measure the count of 100 therapy transactions online distributed by `label`.
    We can see that there is a `65`:`35` distribution of `do_not_escalate` to `escalate`
    as seen in the following screenshot:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集跨多行的数据并按维度对结果进行分组是非常有用的。在这种情况下，维度是`label`。使用`df.groupby()`函数来衡量按`label`分布的100次治疗交易的计数。我们可以看到，`do_not_escalate`与`escalate`的比例为`65`:`35`，如下图所示：
- en: '![](img/9c270c9f-e510-4548-878c-cdd70926792c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c270c9f-e510-4548-878c-cdd70926792c.png)'
- en: 'A new column, `word_count`, is created to calculate how many words are used
    in each of the 100 transactions between the chatbot and the online visitor. The
    newly created column, `word_count`, can be seen in the following screenshot:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个新的列，`word_count`，用于计算聊天机器人与在线访客之间100次交易中使用的单词数量。新创建的列`word_count`可以在下图中看到：
- en: '![](img/4d3b4867-b4cc-4174-b011-28f25f2622ac.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d3b4867-b4cc-4174-b011-28f25f2622ac.png)'
- en: 'Since the `word_count` is now added to the dataframe, it can be aggregated
    to calculate the average word count by `label`. Once this is performed, we can
    see that `escalate` conversations on average are more than twice as long as `do_not_escalate`
    conversations, as seen in the following screenshot:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`word_count`现已添加到数据框中，可以对其进行聚合，按`label`计算平均单词数。完成此操作后，我们可以看到，`escalate`会话的平均长度是`do_not_escalate`会话的两倍多，如下图所示：
- en: '![](img/a4bb5dfb-f9cf-4c26-b408-3ef612192169.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4bb5dfb-f9cf-4c26-b408-3ef612192169.png)'
- en: Visualizing word counts in the dataset
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化数据集中的单词计数
- en: A picture is worth a thousand words and this section will set out to prove that.
    Unfortunately, Spark does not have any inherent plotting capabilities as of version
    2.2\. In order to plot values in a dataframe, we must convert to `pandas`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图片胜过千言万语，本节将证明这一点。不幸的是，从版本2.2开始，Spark没有任何内建的绘图能力。为了在数据框中绘制值，我们必须转换为`pandas`。
- en: Getting ready
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require importing `matplotlib` for plotting:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要导入`matplotlib`用于绘图：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How to do it...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This section walks through the steps to convert the Spark dataframe into a visualization
    that can be seen in the Jupyter notebook.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分详细说明了如何将 Spark 数据框转换为可在 Jupyter notebook 中查看的可视化图表。
- en: 'Convert Spark dataframe to a `pandas` dataframe using the following script:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将 Spark 数据框转换为 `pandas` 数据框：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Plot the dataframe using the following script:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本绘制数据框：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works...
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how the Spark dataframe is converted to `pandas` and then
    plotted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分解释了如何将 Spark 数据框转换为 `pandas`，然后进行绘制。
- en: A subset of the Spark dataframe is collected and converted to `pandas` using
    the `toPandas()` method in Spark.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 数据框的一个子集被收集并使用 Spark 中的 `toPandas()` 方法转换为 `pandas`。
- en: 'That subset of data is then plotted using matplotlib setting the y-values to
    be `word_count` and the x-values to be the `id` as seen in the following screenshot:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用 matplotlib 绘制该数据子集，将 y 值设置为 `word_count`，将 x 值设置为 `id`，如下图所示：
- en: '![](img/07a5e35f-d769-4487-bd3f-84995ef9ace5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07a5e35f-d769-4487-bd3f-84995ef9ace5.png)'
- en: See also
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: There are other plotting capabilities in Python other than `matplotlib` such
    as `bokeh`, `plotly`, and `seaborn`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `matplotlib` 之外，Python 还有其他绘图库，如 `bokeh`、`plotly` 和 `seaborn`。
- en: 'To learn more about `bokeh`, visit the following website:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 `bokeh` 的信息，请访问以下网站：
- en: '[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)'
- en: 'To learn more about `plotly`, visit the following website:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 `plotly` 的信息，请访问以下网站：
- en: '[https://plot.ly/](https://plot.ly/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://plot.ly/](https://plot.ly/)'
- en: 'To learn more about `seaborn`, visit the following website:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 `seaborn` 的信息，请访问以下网站：
- en: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
- en: Calculating sentiment analysis of text
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算文本的情感分析
- en: Sentiment analysis is the ability to derive tone and feeling behind a word or
    series of words. This section will utilize techniques in python to calculate a
    sentiment analysis score from the 100 transactions in our dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是从单词或一系列单词中提取语气和情感的能力。本部分将利用 Python 中的技术计算数据集中 100 个事务的情感分析得分。
- en: Getting ready
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require using functions and data types within PySpark. Additionally,
    we well importing the `TextBlob` library for sentiment analysis. In order to use
    SQL and data type functions within PySpark, the following must be imported:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将需要使用 PySpark 中的函数和数据类型。此外，我们还将导入用于情感分析的 `TextBlob` 库。为了在 PySpark 中使用 SQL
    和数据类型函数，必须导入以下内容：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Additionally, in order to use `TextBlob`, the following library must be imported:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使用 `TextBlob`，必须导入以下库：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How to do it...
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: The following section walks through the steps to apply sentiment score to the
    dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将详细介绍如何将情感评分应用于数据集。
- en: 'Create a sentiment score function, `sentiment_score`, using the following script:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本创建一个情感评分函数 `sentiment_score`：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Apply `sentiment_score` to each conversation response in the dataframe using
    the following script:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将 `sentiment_score` 应用于数据框中的每个对话回复：
- en: 'Create a `lambda` function, called `sentiment_score_udf`, that maps `sentiment_score`
    into a user-defined function within Spark, `udf`, to each transaction and specifies
    the output type of `FloatType()` as seen in the following script:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `sentiment_score_udf` 的 `lambda` 函数，将 `sentiment_score` 映射到 Spark 中的用户定义函数
    `udf`，并指定输出类型为 `FloatType()`，如下脚本所示：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Apply the function, `sentiment_score_udf`, to each `chat` column in the dataframe
    as seen in the following script:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下脚本所示，将函数 `sentiment_score_udf` 应用于数据框中的每个 `chat` 列：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Calculate the average sentiment score, `avg_sentiment_score`, by `label` using
    the following script:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本按 `label` 计算平均情感得分 `avg_sentiment_score`：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how a Python function is converted into a user-defined
    function, `udf`, within Spark to apply a sentiment analysis score to each column
    in the dataframe.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分解释了如何将 Python 函数转换为 Spark 中的用户定义函数 `udf`，以便将情感分析得分应用于数据框中的每一列。
- en: '`Textblob` is a sentiment analysis library in Python. It can calculate the
    sentiment score from a method called `sentiment.polarity` that is scored from
    -1 (very negative) to +1 (very positive) with 0 being neutral. Additionally, `Textblob`
    can measure subjectivity from 0 (very objective) to 1 (very subjective); although,
    we will not be measuring subjectivity in this chapter.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Textblob` 是一个 Python 中的情感分析库。它可以通过一个名为 `sentiment.polarity` 的方法计算情感得分，得分范围为
    -1（非常负面）到 +1（非常正面），0 则表示中立。此外，`Textblob` 还可以衡量主观性，范围从 0（非常客观）到 1（非常主观）；不过，本章不会涉及主观性测量。'
- en: 'There are a couple of steps to applying a Python function to Spark dataframe:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Python 函数应用到 Spark 数据框有几个步骤：
- en: '`Textblob` is imported and a function called `sentiment_score` is applied to
    the `chat` column to generate the sentiment polarity of each bot conversation
    in a new column, also called `sentiment_score`.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Textblob` 被导入，并且对 `chat` 列应用了一个名为 `sentiment_score` 的函数，以生成每个聊天对话的情感极性，结果会生成一个新的列，亦称为
    `sentiment_score`。'
- en: A Python function cannot be directly applied to a Spark dataframe without first
    going through a user-defined function transformation, `udf`, within Spark.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 数据框上直接应用 Python 函数之前，必须首先通过 Spark 中的用户自定义函数转换 `udf`。
- en: Additionally, the output of the function must also be explicitly stated, whether
    it be an integer or float data type. In our situation, we explicitly state that
    the output of the function will be using the `FloatType() from pyspark.sql.types`.
    Finally, the sentiment is applied across each row using a `lambda` function within
    the `udf` sentiment score function, called `sentiment_score_udf`.
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，函数的输出也必须明确声明，无论是整数还是浮动数据类型。在我们的例子中，我们明确声明函数的输出将使用来自 `pyspark.sql.types` 的
    `FloatType()`。最后，情感得分会通过 `lambda` 函数应用到每一行数据中，这个 `lambda` 函数是在 `udf` 中调用的，称为 `sentiment_score_udf`。
- en: 'The updated dataframe with the newly created field,`sentiment score`, can be
    seen by executing `df.show()`, as shown in the following screenshot:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过执行 `df.show()` 来查看更新后的数据框，其中包含新创建的字段 `sentiment_score`，如下所示的屏幕截图：
- en: '![](img/c71a115d-071e-44ba-a103-39370ba0b7d1.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c71a115d-071e-44ba-a103-39370ba0b7d1.png)'
- en: 'Now that the `sentiment_score` is calculated for each response from the chat
    conversation, we can denote a value range of -1 (very negative polarity) to +1
    (very positive polarity) for each row. Just as we did with counts and average
    word count, we can compare whether `escalate` conversations are more positive
    or negative in sentiment than `do_not_escalate` conversations on average. We can
    calculate an average sentiment score, `avg_sentiment_score`, by `label` as seen
    in the following screenshot:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在已经计算出了每个聊天对话的 `sentiment_score`，我们可以为每一行的情感极性指定一个从 -1（非常负面）到 +1（非常正面）的数值范围。就像我们处理词汇计数和平均词数一样，我们可以比较
    `escalate` 对话和 `do_not_escalate` 对话的情感得分，看看它们是否有显著差异。我们可以按 `label` 计算一个平均情感得分
    `avg_sentiment_score`，如下所示的屏幕截图：
- en: '![](img/c0f5585d-bb43-490e-9d4f-b73cb674d47e.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0f5585d-bb43-490e-9d4f-b73cb674d47e.png)'
- en: Initially, it would make sense to assume that `escalate` conversations would
    be more negative from a polarity score than `do_not_escalate`. We actually find
    that `escalate` is slightly more positive in polarity than `do_not_escalate`;
    however, both are pretty neutral as they are close to 0.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初步推测，`escalate` 的对话情感极性得分应该比 `do_not_escalate` 更负面。但实际上我们发现，`escalate` 的情感极性比
    `do_not_escalate` 稍微正面一些；然而，二者的情感极性都相当中立，接近 0。
- en: See also
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the `TextBlob` library, visit the following website:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 `TextBlob` 库的信息，请访问以下网站：
- en: '[http://textblob.readthedocs.io/en/dev/](http://textblob.readthedocs.io/en/dev/)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://textblob.readthedocs.io/en/dev/](http://textblob.readthedocs.io/en/dev/)'
- en: Removing stop words from the text
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本中去除停用词
- en: A stop word is a very common word used in the English language and is often
    removed from common NLP techniques because they can be distracting. Common stop
    word would be words such as *the* or *and*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是英语中常见的单词，通常在常见的自然语言处理（NLP）技术中被移除，因为它们可能会干扰分析。常见的停用词包括如 *the* 或 *and* 这样的词。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section requires importing the following libraries:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要导入以下库：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This section walks through the steps to remove stop words.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将演示如何去除停用词。
- en: 'Execute the following script to extract each word in `chat` into a string within
    an array:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本，将 `chat` 中的每个词提取到一个数组中的字符串：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Assign a list of common words to a variable, `stop_words`, that will be considered
    stop words using the following script:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一组常见的词语分配给一个变量 `stop_words`，并使用以下脚本将其视为停用词：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Execute the following script to import the `StopWordsRemover` function from
    PySpark and configure the input and output columns, `words` and `word without
    stop`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来从 PySpark 导入 `StopWordsRemover` 函数，并配置输入和输出列 `words` 和 `word without
    stop`：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Execute the following script to import Pipeline and define the `stages` for
    the stop word transformation process that will be applied to the dataframe:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本导入管道并定义停用词转换过程的 `阶段`，这些将应用到数据框：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, apply the stop word removal transformation, `pipelineFitRemoveStopWords`,
    to the dataframe, `df`, using the following script:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下脚本将停用词移除转换 `pipelineFitRemoveStopWords` 应用到数据框 `df`：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how to remove stop words from the text.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何从文本中删除停用词。
- en: Just as we did by applying some analysis when profiling and exploring the `chat` data,
    we can also tweak the text of the `chat` conversation and break up each word into
    a separate array. This will be used to isolate stop words and remove them.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在对 `chat` 数据进行分析和探索时所做的那样，我们也可以调整 `chat` 对话的文本，将每个词分割成一个单独的数组。这将用于隔离停用词并将其移除。
- en: 'The new column with each word extracted as a string is called `words` and can
    be seen in the following screenshot:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取的每个词作为字符串的新列称为 `words`，可以在以下截图中看到：
- en: '![](img/10f69edb-5e82-4230-9d8d-0356a7b7df3f.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10f69edb-5e82-4230-9d8d-0356a7b7df3f.png)'
- en: There are many ways to assign a group of words to a stop word list. Some of
    these words can be automatically downloaded and updated using a proper Python
    library called `nltk`, which stands for natural language toolkit. For our purposes,
    we will utilize a common list of 124 stop words to generate our own list. Additional
    words can be easily added or removed from the list manually.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有多种方法可以将一组词汇分配给停用词列表。这些词汇中的一些可以通过一个名为 `nltk` 的 Python 库自动下载和更新，`nltk` 代表自然语言工具包。为了我们的目的，我们将利用一个包含124个常见停用词的列表来生成我们自己的列表。额外的词汇可以轻松地手动添加或删除。
- en: Stop words do not add any value to the text and will be removed from the newly
    created column by specifying `outputCol="words without stop"`. Additionally, the
    column that will serve as the source for the transformation is set by specifying `inputCol
    = "words"`.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停用词对文本没有任何价值，因此会通过指定 `outputCol="words without stop"` 从新创建的列中删除。此外，通过指定 `inputCol
    = "words"` 来设置作为转换源的列。
- en: We create a pipeline, `stopWordRemovalPipeline`, to define the sequence of steps
    or `stages` that will transform the data. In this situation, the only stage that
    will be used to transform the data is the feature, `stopwordsRemover`.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个管道 `stopWordRemovalPipeline`，用来定义将转换数据的步骤或 `阶段` 序列。在这种情况下，用于转换数据的唯一阶段是特征
    `stopwordsRemover`。
- en: 'Each stage in a pipeline can have a transforming role and an estimator role.
    The estimator role, `pipeline.fit(df)`, is called on to produce a transformer
    function called `pipelineFitRemoveStopWords`. Finally, the `transform(df)` function
    is called on the dataframe to produce an updated dataframe with a new column called `words
    without stop`. We can compare both columns side by side to examine the differences
    as seen in the following screenshot:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道中的每个阶段都可以有一个转换角色和一个估算角色。估算角色 `pipeline.fit(df)` 被调用以生成一个名为 `pipelineFitRemoveStopWords`
    的转换器函数。最后，调用 `transform(df)` 函数对数据框进行转换，以生成一个新的数据框，其中包含一个名为 `words without stop`
    的新列。我们可以将两个列并排比较，检查它们之间的差异，如以下截图所示：
- en: '![](img/164af41e-b68a-4e6d-aaa4-ba4236e2f1d3.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/164af41e-b68a-4e6d-aaa4-ba4236e2f1d3.png)'
- en: The new column, `words without stop`, contains none of the strings that are
    considered stop words from the original column, `words`.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新列 `words without stop` 不包含原始列 `words` 中被视为停用词的任何字符串。
- en: See also
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见：
- en: 'To learn more about stop words from `nltk`, visit the following website:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 `nltk` 停用词的信息，请访问以下网站：
- en: '[https://www.nltk.org/data.html](https://www.nltk.org/data.html)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.nltk.org/data.html](https://www.nltk.org/data.html)'
- en: 'To learn more about Spark machine learning pipelines, visit the following website:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Spark 机器学习管道的信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/ml-pipeline.html](https://spark.apache.org/docs/2.2.0/ml-pipeline.html)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/ml-pipeline.html](https://spark.apache.org/docs/2.2.0/ml-pipeline.html)'
- en: 'To learn more about the `StopWordsRemover` feature in PySpark, visit the following
    website:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 PySpark 中 `StopWordsRemover` 特征的信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)'
- en: Training the TF-IDF model
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 TF-IDF 模型
- en: We are now ready to train our TF-IDF NLP model and see if we can classify these
    transactions as either `escalate` or `do_not_escalate`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的 TF-IDF 自然语言处理（NLP）模型，并看看我们是否能将这些事务分类为 `escalate` 或 `do_not_escalate`。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section will require importing from `spark.ml.feature` and `spark.ml.classification`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要从 `spark.ml.feature` 和 `spark.ml.classification` 导入内容。
- en: How to do it...
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The following section walks through the steps to train the TF-IDF model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将逐步讲解训练 TF-IDF 模型的步骤。
- en: 'Create a new user-defined function, `udf`, to define numerical values for the `label`
    column using the following script:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的用户定义函数 `udf`，用以下脚本为 `label` 列定义数值：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Execute the following script to set the TF and IDF columns for the vectorization
    of the words:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本，以设置词向量化的 TF 和 IDF 列：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Set up a pipeline, `pipelineTFIDF`, to set the sequence of stages for `TF_` and `IDF_` using
    the following script:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置管道 `pipelineTFIDF`，按以下脚本设置 `TF_` 和 `IDF_` 阶段的顺序：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit and transform the IDF estimator onto the dataframe, `df`, using the following
    script:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本，将 IDF 估算器拟合并转换为数据框 `df`：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Split the dataframe into a 75:25 split for model evaluation purposes using
    the following script:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将数据框按 75:25 的比例拆分，以进行模型评估：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import and configure a classification model, `LogisticRegression`, using the
    following script:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本导入并配置分类模型 `LogisticRegression`：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit the logistic regression model, `logreg`, onto the training dataframe, `trainingDF.` A
    new dataframe, `predictionDF`, is created based on the `transform()` method from
    the logistic regression model, as seen in the following script:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将逻辑回归模型 `logreg` 拟合到训练数据框 `trainingDF` 上。基于逻辑回归模型的 `transform()` 方法，会创建一个新的数据框
    `predictionDF`，如下脚本所示：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The following section explains to effectively train a TF-IDF NLP model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了如何有效地训练一个 TF-IDF 自然语言处理模型。
- en: 'It is ideal to have labels in a numerical format rather than a categorical
    form as the model is able to interpret numerical values while classifying outputs
    between 0 and 1. Therefore, all labels under the `label` column are converted
    to a numerical `label` of 0.0 or 1.0, as seen in the following screenshot:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理想情况下，标签应以数字格式而非类别形式出现，因为模型能够解释数字值，并在 0 和 1 之间进行输出分类。因此，`label` 列下的所有标签都会转换为数值形式的
    `label`，其值为 0.0 或 1.0，如下图所示：
- en: '![](img/41d1c54e-e6c8-43c9-9027-0d4cad7e3fcc.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41d1c54e-e6c8-43c9-9027-0d4cad7e3fcc.png)'
- en: TF-IDF models require a two-step approach by importing both `HashingTF` and
    `IDF` from `pyspark.ml.feature` to handle separate tasks. The first task merely
    involves importing both `HashingTF` and `IDF` and assigning values for the input
    and subsequent output columns. The `numfeatures` parameter is set to 100,000 to
    ensure that it is larger than the distinct number of words in the dataframe. If
    `numfeatures` were to be than the distinct word count, the model would be inaccurate.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF-IDF 模型需要采用两步法，通过从 `pyspark.ml.feature` 导入 `HashingTF` 和 `IDF` 来处理不同的任务。第一步仅涉及导入
    `HashingTF` 和 `IDF`，并为输入和输出列分配相应的值。`numfeatures` 参数被设置为 100,000，以确保它大于数据框中不同词汇的数量。如果
    `numfeatures` 小于词汇的不同数量，模型将不准确。
- en: As stated earlier, each step of the pipeline contains a transformation process
    and an estimator process. The pipeline, `pipelineTFIDF`, is configured to order
    the sequence of steps where `IDF` will follow `HashingTF`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，管道的每个步骤都包含转换过程和估算过程。管道 `pipelineTFIDF` 被配置为顺序排列各个步骤，其中 `IDF` 将紧随 `HashingTF`。
- en: '`HashingTF` is used to transform the `words without stop` into vectors within
    a new column called `rawFeatures`. Subsequently, `rawFeatures` will then be consumed
    by `IDF` to estimate the size and fit the dataframe to produce a new column called `features`,
    as seen in the following screenshot:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`HashingTF` 用于将“去除停用词后的词汇”转换为向量，存储在一个新的列 `rawFeatures` 中。随后，`rawFeatures` 会被
    `IDF` 处理，以估算大小并拟合数据框，生成一个新的列 `features`，如下图所示：'
- en: '![](img/988894fc-443a-47ba-a01b-b3154e088e6a.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/988894fc-443a-47ba-a01b-b3154e088e6a.png)'
- en: For training purposes, our dataframe will be conservatively split into a `75`:`25`
    ratio with a random seed set at `1234`.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进行训练，我们的数据框将以 `75`:`25` 的比例保守地拆分，并且随机种子设置为 `1234`。
- en: Since our main goal is to classify each conversation as either `escalate` for
    escalation or `do_not_escalate` for continued bot chat, we can use a traditional
    classification algorithm such as a logistic regression model from the PySpark
    library. The logistic regression model is configured with a regularization parameter,
    `regParam`, of 0.025\. We use the parameter to slightly improve the model by minimizing
    overfitting at the expense of a little bias.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的主要目标是将每个对话分类为 `escalate`（升级）或 `do_not_escalate`（继续与机器人对话），因此我们可以使用传统的分类算法，如来自
    PySpark 库的逻辑回归模型。逻辑回归模型配置了一个正则化参数 `regParam`，其值为 0.025。我们使用该参数通过在稍微增加偏差的情况下最小化过拟合，从而稍微改善模型。
- en: 'The logistic regression model is trained and fitted on `trainingDF`, and then
    a new dataframe, `predictionDF`, is created with the newly transformed field, `prediction`,
    as seen in the following screenshot:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑回归模型在 `trainingDF` 上训练并拟合，然后创建一个新的数据框 `predictionDF`，其中包含新转换的字段 `prediction`，如以下截图所示：
- en: '![](img/180abef6-2eba-44ba-9da6-f2688e4996f6.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/180abef6-2eba-44ba-9da6-f2688e4996f6.png)'
- en: There's more...
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While we did use the user-defined function, `udf`, to manually create a numerical
    label column, we also could have used a built-in feature from PySpark called `StringIndexer`
    to assign numerical values to categorical labels. To see `StringIndexer` in action,
    visit [Chapter 5](9cc3bf45-b46d-4c37-920c-87d6fdab58c2.xhtml), *Predicting Fire
    Department Calls with Spark ML*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们确实使用了用户定义函数 `udf` 来手动创建数值标签列，但我们也可以使用 PySpark 中的内置特性 `StringIndexer` 来为分类标签分配数值。要查看
    `StringIndexer` 的实际操作，请访问 [第 5 章](9cc3bf45-b46d-4c37-920c-87d6fdab58c2.xhtml)，*使用
    Spark ML 预测消防部门电话*。
- en: See also
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the TF-IDF model within PySpark, visit the following website:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 PySpark 中的 TF-IDF 模型，请访问以下网站：
- en: '[https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf)'
- en: Evaluating TF-IDF model performance
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 TF-IDF 模型性能
- en: At this point, we are ready to evaluate our model's performance
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们准备好评估模型的性能
- en: Getting ready
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require importing the following libraries:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要导入以下库：
- en: '`metrics` from `sklearn`'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` 来自 `sklearn`'
- en: '`BinaryClassificationEvaluator` from `pyspark.ml.evaluation`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 `pyspark.ml.evaluation` 的 `BinaryClassificationEvaluator`
- en: How to do it...
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: This section walks through the steps to evaluate the TF-IDF NLP model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了评估 TF-IDF NLP 模型的步骤。
- en: 'Create a confusion matrix using the following script:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本创建混淆矩阵：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Evaluate the model using `metrics` from sklearn with the following script:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自 sklearn 的 `metrics` 评估模型，使用以下脚本：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Calculate the ROC score using the following script:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算 ROC 分数：
- en: '[PRE30]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how we use the evaluation calculations to determine the
    accuracy of our model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了我们如何使用评估计算来确定模型的准确性。
- en: 'A confusion matrix is helpful to quickly summarize the accuracy numbers between
    actual results and predicted results. Since we had a 75:25 split, we should see
    25 predictions from our training dataset. We can build a build a confusion matric
    using the following script: `predictionDF.crosstab(''label'', ''prediction'').show()`.
    The output of the script can be seen in the following screenshot:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵有助于快速总结实际结果和预测结果之间的准确性。由于我们使用了 75:25 的拆分，因此应该从我们的训练数据集中看到 25 个预测结果。我们可以使用以下脚本来构建混淆矩阵：`predictionDF.crosstab('label',
    'prediction').show()`。脚本的输出可以在以下截图中看到：
- en: '![](img/7ec9d6e3-2748-475a-a432-bc4427fcc349.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ec9d6e3-2748-475a-a432-bc4427fcc349.png)'
- en: We are now at the stage of evaluating the accuracy of the model by comparing
    the `prediction` values against the actual `label` values. `sklearn.metrics` intakes
    two parameters, the `actual` values tied to the `label` column, as well as the
    `predicted` values derived from the logistic regression model.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在处于评估模型准确性的阶段，通过将 `prediction` 值与实际的 `label` 值进行比较。`sklearn.metrics` 接受两个参数，分别是与
    `label` 列关联的 `actual`（实际）值和从逻辑回归模型中得出的 `predicted`（预测）值。
- en: Please note that once again we are converting the column values from Spark dataframes
    to pandas dataframes using the `toPandas()` method.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们再次使用 `toPandas()` 方法将 Spark 数据框的列值转换为 Pandas 数据框。
- en: 'Two variables are created, `actual` and `predicted`, and an accuracy score
    of 91.7% is calculated using the `metrics.accuracy_score()` function, as seen
    in the following screenshot:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了两个变量，`actual`和`predicted`，并使用`metrics.accuracy_score()`函数计算了91.7%的准确率，如下截图所示：
- en: '![](img/39d21f82-8cb9-4106-ba4a-aee671c529b8.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39d21f82-8cb9-4106-ba4a-aee671c529b8.png)'
- en: 'The ROC (Receiver Operating Characteristic) is often associated with a curve
    measuring the true positive rate against the false positive rate. The greater
    the area under the curve, the better. The ROC score associated with the curve
    is another indicator that can be used to measure the performance of the model.
    We can calculate the `ROC` using the `BinaryClassificationEvaluator` as seen in
    the following screenshot:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ROC（接收者操作特征）通常与测量真阳性率与假阳性率之间的曲线相关。曲线下的面积越大越好。与曲线相关的ROC得分是另一个可以用来衡量模型性能的指标。我们可以使用`BinaryClassificationEvaluator`计算`ROC`，如以下截图所示：
- en: '![](img/18d393de-bf20-448d-a5c3-2a7d35018680.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18d393de-bf20-448d-a5c3-2a7d35018680.png)'
- en: See also
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the `BinaryClassificationEvaluator` from PySpark, visit
    the following website:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于PySpark中`BinaryClassificationEvaluator`的信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html](https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html](https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html)'
- en: Comparing model performance to a baseline score
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型性能与基准得分进行比较
- en: While it is great that we have a high accuracy score from our model of 91.7
    percent, it is also important to compare this to a baseline score. We dig deeper
    into this concept in this section.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的模型准确率为91.7%，这已经是很棒的结果，但与基准得分进行比较同样重要。本节将深入探讨这个概念。
- en: How to do it...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: This section walks through the steps to calculate the baseline accuracy.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了计算基准准确率的步骤。
- en: 'Execute the following script to retrieve the mean value from the `describe()`
    method:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以从`describe()`方法中检索平均值：
- en: '[PRE31]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Subtract `1- mean value score` to calculate baseline accuracy.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去`1 - 平均值得分`来计算基准准确率。
- en: How it works...
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains the concept behind the baseline accuracy and how we can
    use it to understand the effectiveness of our model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了基准准确率背后的概念，以及如何利用它来理解我们模型的效果。
- en: What if every `chat` conversation was flagged for `do_not_escalate` or vice
    versa. Would we have a baseline accuracy higher than 91.7 percent? The easiest
    way to figure this out is to run the `describe()` method on the `label` column
    from `predictionDF` using the following script: `predictionDF.describe('label').show()`
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果每个`chat`对话都被标记为`do_not_escalate`，反之亦然，会不会使我们的基准准确率超过91.7%？找出这个问题最简单的方法是运行`predictionDF`的`label`列上的`describe()`方法，使用以下脚本：`predictionDF.describe('label').show()`
- en: 'The output of the script can be seen in the following screenshot:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本的输出可以在下方的截图中看到：
- en: '![](img/ae9f9a3b-b15f-4c4d-8e6c-8b49a096f6a9.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae9f9a3b-b15f-4c4d-8e6c-8b49a096f6a9.png)'
- en: The mean of `label` is at 0.2083 or ~21%, which means that a `label` of 1 occurs
    only 21% of the time. Therefore, if we labeled each conversation as `do_not_escalate`,
    we would be correct ~79% of the time, which is less than our model accuracy of
    91.7%.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`label`的平均值为0.2083，约为21%，这意味着`label`为1的情况仅占21%。因此，如果我们将每个对话标记为`do_not_escalate`，我们将大约79%的时间是正确的，这比我们模型的准确率91.7%还低。'
- en: Therefore, we can say that our model performs better than a blind baseline performance
    model.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们可以说我们的模型表现优于盲目基准性能模型。
- en: See also
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the `describe()` method in a PySpark dataframe, visit the
    following website:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关PySpark数据框中`describe()`方法的更多信息，请访问以下网站：
- en: '[http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)'
