- en: Image Classification using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像分类
- en: So far, we haven't developed any **machine learning** (**ML**) projects for
    image processing tasks. Linear ML models and other regular **deep neural network** (**DNN**)
    models, such as **Multilayer Perceptrons** (**MLPs**) or **Deep Belief Networks**
    (**DBNs**), cannot learn or model non-linear features from images.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有开发过任何用于图像处理任务的 **机器学习**（**ML**）项目。线性机器学习模型和其他常规 **深度神经网络**（**DNN**）模型，例如
    **多层感知器**（**MLPs**）或 **深度置信网络**（**DBNs**），无法从图像中学习或建模非线性特征。
- en: On the other hand, a **convolutional neural network** (**CNN**) is a type of
    feedforward neural network in which the connectivity pattern between its neurons
    is inspired by the animal visual cortex. In the last few years, CNNs have demonstrated
    superhuman performance in complex visual tasks such as image search services,
    self-driving cars, automatic video classification, voice recognition, and **natural
    language processing **(**NLP**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**卷积神经网络**（**CNN**）是一种前馈神经网络，其中神经元之间的连接模式受到动物视觉皮层的启发。在过去的几年里，CNN 在复杂视觉任务中展现了超越人类的表现，例如图像搜索服务、自动驾驶汽车、自动视频分类、语音识别和
    **自然语言处理**（**NLP**）。
- en: In this chapter, we will see how to develop an end-to-end project for handling
    multi-label (that is, each entity can belong to multiple classes) image classification
    problems using CNN based on the Scala and **Deeplearning4j** (**DL4j**) framework
    with real **Yelp** image datasets. We will also discuss some theoretical aspects
    of CNNs and how to tune hyperparameters for better classification results before
    getting started.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何基于 Scala 和 **Deeplearning4j**（**DL4j**）框架，使用真实的 **Yelp** 图像数据集开发一个端到端的多标签（即每个实体可以属于多个类别）图像分类项目。我们还将讨论一些
    CNN 的理论方面，以及如何调整超参数以获得更好的分类结果，之后再开始实际操作。
- en: 'In a nutshell, we will learn the following topics throughout this end-to-end
    project:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这个端到端项目中，我们将学习以下主题：
- en: The drawbacks of regular DNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规 DNN 的缺点
- en: 'CNN architectures: convolution operations and pooling layers'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 架构：卷积操作和池化层
- en: Image classification using CNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行图像分类
- en: Tuning CNN hyperparameters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整 CNN 超参数
- en: Image classification and drawbacks of DNNs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类和 DNN 的缺点
- en: Before we start developing the end-to-end project for image classification using
    CNN, we need some background studies, such as the drawbacks of regular DNNs, suitability
    of CNNs over DNNs for image classification, CNN constructions, CNN's different
    operations, and so on. Although regular DNNs work fine for small images (for example,
    MNIST, CIFAR-10), it breaks down for larger images because of the huge number
    of parameters it requires. For example, a 100 x 100 image has 10,000 pixels, and
    if the first layer has just 1,000 neurons (which already severely restricts the
    amount of information transmitted to the next layer), this means a total of 10
    million connections. And that's just for the first layer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始开发基于 CNN 的图像分类端到端项目之前，我们需要进行一些背景学习，比如常规 DNN 的缺点、CNN 相较于 DNN 在图像分类中的适用性、CNN
    的构建方式、CNN 的不同操作等。虽然常规 DNN 对于小尺寸图像（例如 MNIST、CIFAR-10）效果良好，但对于更大尺寸的图像，它会因为所需的巨大参数数量而崩溃。例如，一张
    100 x 100 的图像有 10,000 个像素，如果第一层只有 1,000 个神经元（这已经极大限制了传递到下一层的信息量），那么就意味着总共有 1,000
    万个连接。而这仅仅是第一层的情况。
- en: CNNs solve this problem using partially connected layers. Because consecutive
    layers are only partially connected and because it heavily reuses its weights,
    a CNN has far fewer parameters than a fully connected DNN, which makes it much
    faster to train, reduces the risk of overfitting, and requires much less training
    data. Moreover, when a CNN has learned a kernel that can detect a particular feature,
    it can detect that feature anywhere on the image. In contrast, when a DNN learns
    a feature in one location, it can detect it only in that particular location.
    Since images typically have very repetitive features, CNNs are able to generalize
    much better than DNNs for image processing tasks, such as classification, using
    fewer training examples.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 通过使用部分连接层解决了这个问题。由于连续层之间仅部分连接，并且由于其权重的高度重用，CNN 的参数远少于完全连接的 DNN，这使得它训练速度更快，减少了过拟合的风险，并且需要的训练数据量更少。此外，当
    CNN 学会了能够检测某一特征的卷积核时，它可以在图像的任何位置检测到该特征。相比之下，当 DNN 在某个位置学到一个特征时，它只能在该位置检测到该特征。由于图像通常具有非常重复的特征，CNN
    在图像处理任务（如分类）中能够比 DNN 更好地进行泛化，并且只需较少的训练样本。
- en: 'Importantly, DNN has no prior knowledge of how pixels are organized; it does
    not know that nearby pixels are close. A CNN''s architecture embeds this prior
    knowledge. Lower layers typically identify features in small areas of the images,
    while higher layers combine the lower-level features into larger features. This
    works well with most natural images, giving CNNs a decisive head start compared
    to DNNs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，DNN并不事先了解像素是如何组织的；它不知道相邻的像素是彼此接近的。而CNN的架构则将这种先验知识嵌入其中。较低层通常识别图像的小区域中的特征，而较高层则将低级特征组合成更大的特征。这在大多数自然图像中效果很好，使CNN相比DNN具有决定性的优势：
- en: '![](img/045f23a6-f77a-48ab-8620-85fcdc44fe31.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/045f23a6-f77a-48ab-8620-85fcdc44fe31.png)'
- en: 'Figure 1: Regular DNN versus CNN'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：普通DNN与CNN
- en: For example, in *Figure 1*, on the left, you can see a regular three-layer neural
    network. On the right, a ConvNet arranges its neurons in three dimensions (width,
    height, and depth), as visualized in one of the layers. Every layer of a ConvNet
    transforms the 3D input volume to a 3D output volume of neuron activations. The
    red input layer holds the image, so its width and height would be the dimensions
    of the image, and the depth would be three (red, green, and blue channels).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*图1*中，左侧显示了一个普通的三层神经网络。右侧，ConvNet将它的神经元以三维（宽度、高度和深度）进行排列，如其中一层的可视化所示。ConvNet的每一层将3D输入体积转换为3D输出体积的神经元激活。红色输入层承载着图像，因此它的宽度和高度就是图像的维度，而深度则是三（红色、绿色和蓝色通道）。
- en: So, all the multilayer neural networks we looked at had layers composed of a
    long line of neurons, and we had to flatten input images or data to 1D before
    feeding them to the neural network. However, what happens once you try to feed
    them a 2D image directly? The answer is that, in CNN, each layer is represented
    in 2D, which makes it easier to match neurons with their corresponding inputs.
    We will see examples of it in upcoming sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们之前看到的所有多层神经网络的层都是由一长串神经元组成的，在将输入图像或数据传递给神经网络之前，我们需要将其展平成1D。但是，当你尝试直接将2D图像输入时会发生什么呢？答案是，在CNN中，每一层都是以2D形式表示的，这使得将神经元与它们对应的输入进行匹配变得更加容易。我们将在接下来的部分看到相关示例。
- en: Another important fact is all the neurons in a feature map share the same parameters,
    so it dramatically reduces the number of parameters in the model, but, more importantly,
    it means that once the CNN has learned to recognize a pattern in one location,
    it can recognize it in any other location. In contrast, once a regular DNN has
    learned to recognize a pattern in one location, it can recognize it only in that
    particular location.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的事实是，特征图中的所有神经元共享相同的参数，因此它显著减少了模型中的参数数量，但更重要的是，它意味着一旦CNN学会在某个位置识别某个模式，它就能够在任何其他位置识别该模式。相比之下，一旦普通的DNN学会在某个位置识别某个模式，它只能在那个特定位置识别该模式。
- en: CNN architecture
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN架构
- en: In multilayer networks, such as MLP or DBN, the outputs of all neurons of the
    input layer are connected to each neuron in the hidden layer, so the output will
    again act as the input to the fully-connected layer. In CNN networks, the connection
    scheme that defines the convolutional layer is significantly different. The convolutional
    layer is the main type of layer in CNN, where each neuron is connected to a certain
    region of the input area called the **receptive field**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层网络中，例如MLP或DBN，输入层所有神经元的输出都连接到隐藏层中的每个神经元，因此输出将再次作为输入传递给全连接层。而在CNN网络中，定义卷积层的连接方式有显著不同。卷积层是CNN中的主要层类型，每个神经元都连接到输入区域的某个区域，这个区域称为**感受野**。
- en: In a typical CNN architecture, a few convolutional layers are connected in a
    cascade style, where each layer is followed by a **rectified linear unit** (**ReLU**)
    layer, then a pooling layer, then a few more convolutional layers (+ReLU), then
    another pooling layer, and so on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的CNN架构中，几个卷积层以级联样式连接，每一层后面跟着一个**整流线性单元**（**ReLU**）层，然后是一个池化层，再接几个卷积层（+ReLU），再接另一个池化层，如此循环。
- en: 'The output from each convolution layer is a set of objects called **feature
    maps** that are generated by a single kernel filter. The feature maps can then
    be used to define a new input to the next layer. Each neuron in a CNN network
    produces an output followed by an activation threshold, which is proportional
    to the input and not bound:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层的输出是一组由单个核过滤器生成的对象，称为**特征图**。这些特征图可以用来定义下一个层的输入。CNN网络中的每个神经元都会生成一个输出，之后是一个激活阈值，该阈值与输入成比例，并且没有限制：
- en: '![](img/86c20f01-65a5-4544-a6ee-7fc6355e20e8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86c20f01-65a5-4544-a6ee-7fc6355e20e8.png)'
- en: 'Figure 2: A conceptual architecture of CNN'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：CNN的概念架构
- en: As you can see in *Figure 2*, the pooling layers are usually placed after the
    convolutional layers. The convolutional region is then divided by a pooling layer
    into sub-regions. Then, a single representative value is selected using either
    a max-pooling or average pooling technique to reduce the computational time of
    subsequent layers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2*所示，池化层通常被放置在卷积层之后。然后，卷积区域会被池化层划分为子区域。接着，使用最大池化或平均池化技术选择一个代表性值，从而减少后续层的计算时间。
- en: This way, the robustness of the feature with respect to its spatial position
    gets increased too. To be more specific, when the image properties, as feature
    maps, pass through the image, they get smaller and smaller as they progress through
    the network, but they also typically get deeper and deeper, since more feature
    maps will be added. At the top of the stack, a regular feedforward neural network
    is added, just like an MLP, which might compose of a few fully connected layers
    (+ReLUs), and the final layer outputs the prediction, for example, a softmax layer
    that outputs estimated class probabilities for a multiclass classification.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，特征相对于其空间位置的鲁棒性也得到了增强。更具体地说，当图像特性（作为特征图）通过网络时，它们随着网络的推进变得越来越小，但通常会变得更深，因为更多的特征图将被添加到网络中。在堆叠的顶部，加入了一个常规的前馈神经网络，就像MLP一样，可能由几层全连接层（+ReLUs）组成，最后一层输出预测结果，例如，softmax层输出多类分类的估计类别概率。
- en: Convolutional operations
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积操作
- en: A convolution is a mathematical operation that slides one function over another
    and measures the integral of their pointwise multiplication. It has deep connections
    with the Fourier transform and the Laplace transform and is heavily used in signal
    processing. Convolutional layers actually use cross-correlations, which are very
    similar to convolutions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一个数学运算，它将一个函数滑过另一个函数，并测量它们逐点相乘的积分。它与傅里叶变换和拉普拉斯变换有着深刻的联系，并广泛应用于信号处理。卷积层实际上使用的是互相关，它与卷积非常相似。
- en: 'Thus, the most important building block of a CNN is the convolutional layer:
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (as they were in previous chapters), but only to pixels in
    their receptive fields—see *Figure 3*. In turn, each neuron in the second convolutional
    layer is connected only to neurons located within a small rectangle in the first
    layer:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CNN的最重要组成部分是卷积层：第一卷积层中的神经元并不是与输入图像中的每一个像素相连接（如同前几章所述），而只是与它们感受野中的像素相连接——见*图3*。反过来，第二卷积层中的每个神经元仅与第一层中位于小矩形区域内的神经元相连接：
- en: '![](img/b2d6b3bc-542d-42ce-9287-6c21eff983ed.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2d6b3bc-542d-42ce-9287-6c21eff983ed.png)'
- en: 'Figure 3: CNN layers with rectangular local receptive fields'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：具有矩形局部感受野的CNN层
- en: This architecture allows the network to concentrate on low-level features in
    the first hidden layer, and then assemble them into higher-level features in the
    next hidden layer, and so on. This hierarchical structure is common in real-world
    images, which is one of the reasons why CNNs work so well for image recognition.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构使得网络能够在第一隐藏层中集中关注低级特征，然后在接下来的隐藏层中将它们组装成更高级的特征，依此类推。这种层次结构在现实世界的图像中很常见，这也是CNN在图像识别中表现如此出色的原因之一。
- en: Pooling layer and padding operations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层和填充操作
- en: 'Once you understand how convolutional layers work, the pooling layers are quite
    easy to grasp. A pooling layer typically works on every input channel independently,
    so the output depth is the same as the input depth. You may alternatively pool
    over the depth dimension, as we will see next, in which case the image''s spatial
    dimensions (height and width) remain unchanged, but the number of channels is
    reduced. Let''s see a formal definition of pooling layers from a well-known TensorFlow
    website:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了卷积层的工作原理，池化层就非常容易理解了。池化层通常会独立地处理每一个输入通道，因此输出深度与输入深度相同。你也可以选择在深度维度上进行池化，正如我们接下来将看到的那样，在这种情况下，图像的空间维度（高度和宽度）保持不变，但通道的数量会减少。让我们看一下来自一个知名TensorFlow网站的池化层的正式定义：
- en: '"The pooling ops sweep a rectangular window over the input tensor, computing
    a reduction operation for each window (average, max, or max with argmax). Each
    pooling op uses rectangular windows of size called ksize separated by offset strides.
    For example, if strides are all ones, every window is used, if strides are all
    twos, every other window is used in each dimension, and so on."'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “池化操作在输入张量上扫过一个矩形窗口，为每个窗口计算一个归约操作（平均、最大值或带有argmax的最大值）。每个池化操作使用称为ksize的矩形窗口，窗口间隔由偏移步幅定义。例如，如果步幅全为1，则每个窗口都会被使用；如果步幅全为2，则每个维度中的每隔一个窗口就会被使用，依此类推。”
- en: Therefore, just like in convolutional layers, each neuron in a pooling layer
    is connected to the outputs of a limited number of neurons in the previous layer,
    located within a small rectangular receptive field. You must define its size,
    the stride, and the padding type, just like before. However, a pooling neuron
    has no weights; all it does is aggregate the inputs using an aggregation function
    such as the max or mean.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，就像在卷积层中一样，池化层中的每个神经元与前一层中有限数量的神经元的输出相连接，这些神经元位于一个小的矩形感受野内。你必须像之前一样定义其大小、步幅和填充类型。然而，池化神经元没有权重；它所做的只是使用聚合函数（如最大值或均值）对输入进行聚合。
- en: 'Well, the goal of using pooling is to subsample the input image in order to
    reduce the computational load, the memory usage, and the number of parameters.
    This helps to avoid overfitting in the training stage. Reducing the input image
    size also makes the neural network tolerate a little bit of image shift. In the
    following example, we use a 2 x 2 pooling kernel and a stride of 2 with no padding.
    Only the max input value in each kernel makes it to the next layer since the other
    inputs are dropped:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目标使用池化是为了对子输入图像进行子采样，以减少计算负荷、内存使用和参数数量。这有助于在训练阶段避免过拟合。减少输入图像的大小还使得神经网络能够容忍一定程度的图像位移。在以下示例中，我们使用了2
    x 2的池化核和步幅为2的设置，并且没有填充。只有每个池化核中的最大输入值会传递到下一层，因为其他输入会被丢弃：
- en: '![](img/5c2fcbec-0188-4c45-8a7c-54f19f36d0d9.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c2fcbec-0188-4c45-8a7c-54f19f36d0d9.png)'
- en: 'Figure 4: An example using max pooling, that is, subsampling'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用最大池化的示例，即子采样
- en: Usually, *(stride_length)* x + filter_size <= input_layer_size* is recommended
    for most CNN-based network development.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*（stride_length）* x + filter_size <= input_layer_size* 是大多数基于CNN的网络开发中推荐的。
- en: Subsampling operations
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子采样操作
- en: As stated earlier, a neuron located in a given layer is connected to the outputs
    of the neurons in the previous layer. Now, in order for a layer to have the same
    height and width as the previous layer, it is common to add zeros around the inputs,
    as shown in the diagram. This is called **SAME** or **zero padding**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，位于给定层中的神经元与前一层中神经元的输出相连接。现在，为了使一个层具有与前一层相同的高度和宽度，通常会在输入周围添加零，如图所示。这称为**SAME**或**零填充**。
- en: 'The term SAME means that the output feature map has the same spatial dimensions
    as the input feature map. Zero padding is introduced to make the shapes match
    as needed, equally on every side of the input map. On the other hand, VALID means
    no padding and only drops the right-most columns (or bottom-most rows):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '"SAME"一词意味着输出特征图具有与输入特征图相同的空间维度。零填充被引入，以便在需要时使形状匹配，并且在输入图的每一侧填充相等。另一方面，"VALID"意味着没有填充，只是丢弃最右侧的列（或最下方的行）：'
- en: '![](img/dfdd4a04-457d-444c-9299-ae5649690df6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfdd4a04-457d-444c-9299-ae5649690df6.png)'
- en: 'Figure 5: SAME versus VALID padding with CNN'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：CNN中的SAME与VALID填充
- en: Now we have the minimum theoretical knowledge about CNNs and their architectures,
    it's time to do some hands-on work and create convolutional, pooling, and subsampling
    operations using Deeplearning4j (aka. DL4j), which is one of the first commercial-grade distributed open
    source deep-learning libraries written for Java and Scala. It also provides integrated
    support for Hadoop and Spark. DL4j is designed to be used in business environments
    on distributed GPUs and CPUs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了关于CNN及其架构的最基本理论知识，是时候动手实践了，使用Deeplearning4j（简称DL4j）创建卷积、池化和子采样操作。DL4j是最早的商业级分布式开源深度学习库之一，专为Java和Scala编写。它还提供对Hadoop和Spark的集成支持。DL4j旨在用于商业环境中的分布式GPU和CPU。
- en: Convolutional and subsampling operations in DL4j
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL4j中的卷积和子采样操作
- en: Before getting started, setting up our programming environment is a prerequisite.
    So let's do that first.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，设置我们的编程环境是一个先决条件。所以我们先做这个。
- en: Configuring DL4j, ND4s, and ND4j
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置DL4j、ND4s和ND4j
- en: 'The following libraries can be integrated with DL4j. They will make your JVM
    experience easier, whether you''re developing your ML application in Java or Scala:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下库可以与DL4j集成。无论你是在Java还是Scala中开发机器学习应用程序，它们都会使你的JVM体验更加顺畅：
- en: '**DL4j**: Neural net platform'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DL4j**：神经网络平台'
- en: '**ND4J**: NumPy for the JVM'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4J**：JVM上的NumPy'
- en: '**DataVec**: Tool for ML ETL operations'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataVec**：机器学习ETL操作工具'
- en: '**JavaCPP**: The bridge between Java and native C++'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaCPP**：Java与本地C++之间的桥梁'
- en: '**Arbiter**: Evaluation tool for ML algorithms'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arbiter**：机器学习算法评估工具'
- en: '**RL4J**: Deep reinforcement learning for the JVM'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL4J**：JVM上的深度强化学习'
- en: ND4j is just like NumPy for JVM. It comes with some basic operations of linear
    algebra, such as matrix creation, addition, and multiplication. ND4S, on the other
    hand, is a scientific computing library for linear algebra and matrix manipulation.
    Basically, it supports n-dimensional arrays for JVM-based languages.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ND4j就像JVM上的NumPy。它提供了一些线性代数的基本操作，例如矩阵创建、加法和乘法。另一方面，ND4S是一个科学计算库，专注于线性代数和矩阵操作。基本上，它支持JVM语言的n维数组。
- en: 'If you are using Maven on Eclipse (or any other editor—that is, IntelliJ IDEA),
    use the following dependencies in the `pom.xml` file (inside the `<dependencies>`
    tag) for dependency resolution for DL4j, ND4s, and ND4j:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Eclipse（或任何其他编辑器，如IntelliJ IDEA）中使用Maven，请在`pom.xml`文件（位于`<dependencies>`标签内）中使用以下依赖项来解决DL4j、ND4s和ND4j的依赖问题：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I used old versions since I was facing some compatibility issues, and it is
    still under active development. But feel free to adopt the latest upgrades. I
    believe readers can do it with minimal efforts.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的是旧版本，因为遇到了一些兼容性问题，但它仍在积极开发中。不过你可以自由地采用最新版本。我相信读者可以轻松完成这一点。
- en: 'Additionally, if a native system BLAS is not configured on your machine, ND4j''s
    performance will be reduced. You will experience a warning once you execute simple
    code written in Scala:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你的系统没有配置本地的BLAS，ND4j的性能将会下降。一旦你运行简单的Scala代码，就会看到警告：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, installing and configuring BLAS such as OpenBLAS or IntelMKL is not
    that difficult; you can invest some time and do it. Refer to the following URL
    for details: [http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open).
    It is also to be noted that the following are prerequisites when working with
    DL4j:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，安装和配置BLAS（如OpenBLAS或IntelMKL）并不难，你可以投入一些时间去完成它。详情请参阅以下网址：[http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open)。还需要注意的是，在使用DL4j时，以下是必备条件：
- en: Java (developer version) 1.8+ (only 64-bit versions supported)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java（开发者版本）1.8+（仅支持64位版本）
- en: Apache Maven for automated build and dependency manager
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Maven：用于自动构建和依赖管理
- en: IntelliJ IDEA or Eclipse
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IntelliJ IDEA 或 Eclipse
- en: Git
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Git
- en: Well done! Our programming environment is ready for a simple deep learning application
    development. Now it's time to get your hands dirty with some sample code. Let's
    see how to construct and train a simple CNN, using the CIFAR-10 dataset. CIFAR-10
    is one of the most popular benchmark datasets and has thousands of labelled images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们的编程环境已经准备好进行简单的深度学习应用开发。现在是时候动手编写一些示例代码了。让我们看看如何使用CIFAR-10数据集构建和训练一个简单的CNN。CIFAR-10是最受欢迎的基准数据集之一，包含成千上万的标注图像。
- en: Convolutional and subsampling operations in DL4j
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL4j中的卷积和子采样操作
- en: In this subsection, we will see an example of how to construct a CNN for MNIST
    data classification. The network will have two convolutional layers, two subsampling
    layers, one dense layer, and the output layer as the fully connected layer. The
    first layer is a convolutional layer followed by a subsampling layer, which is
    again followed by another convolutional layer. Then, a subsampling layer is followed
    by a dense layer, which is followed by an output layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将展示如何构建一个用于MNIST数据分类的CNN示例。该网络将包含两个卷积层、两个子采样层、一个全连接层和一个输出层。第一层是卷积层，接着是子采样层，随后是另一个卷积层。然后是子采样层，接着是全连接层，最后是输出层。
- en: 'Let''s see how these layers would look like using DL4j. The first convolution
    layer with ReLU as activation function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些层在使用DL4j时的表现。第一个卷积层，使用ReLU作为激活函数：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following activation functions are currently supported in DL4j:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: DL4j当前支持以下激活函数：
- en: ReLU
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU
- en: Leaky ReLU
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Tanh
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: Sigmoid
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Hard Tanh
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hard Tanh
- en: Softmax
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax
- en: Identity
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Identity
- en: '**ELU** (**Exponential Linear Units**)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ELU**（**指数线性单元**）'
- en: Softsign
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softsign
- en: Softplus
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softplus
- en: 'The second layer (that is, the first subsampling layer) is a subsampling layer
    with pooling type `MAX`, with kernel size 2 x 2 and stride size of 2 x 2 but no
    activation function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层（即第一个子采样层）是一个子采样层，池化类型为`MAX`，卷积核大小为2 x 2，步幅为2 x 2，但没有激活函数：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The third layer (2nd convolution layer) is a convolutional layer with ReLU
    as activation function, 1*1 stride:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第三层（第二个卷积层）是一个卷积层，使用ReLU作为激活函数，步幅为1*1：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The fourth layer (that is, the second subsampling layer) is a subsampling layer
    with pooling type `MAX`, with kernel size 2 x 2, and stride size of 2 x 2 but
    no activation function:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第四层（即第二个子采样层）是一个子采样层，池化类型为`MAX`，卷积核大小为2 x 2，步幅为2 x 2，但没有激活函数：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The fifth layer is a dense layer with ReLU as an activation function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第五层是一个全连接层，使用ReLU作为激活函数：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The sixth (that is, the final and fully connected layer) has Softmax as the
    activation function with the number of classes to be predicted (that is, 10):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第六层（即最后一层全连接层）使用Softmax作为激活函数，类别数量为待预测的类别数（即10）：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the layers are constructed, the next task is to construct and build the
    CNN by chaining all the layers. Using DL4j, it goes as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦各层构建完成，接下来的任务是通过链接所有的层来构建CNN。使用DL4j，操作如下：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we set up all the convolutional layers and initialize the network
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设置所有的卷积层并初始化网络，如下所示：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Conventionally, to train a CNN, all images need to be the same shape and size.
    So I placed the dimension as 28 x 28 in the preceding lines for simplicity. Now,
    you may be thinking, how do we train such a network? Well, now we will see this
    but, before that, we need to prepare the MNIST dataset, using the `MnistDataSetIterator
    ()` method, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，要训练一个CNN，所有的图像需要具有相同的形状和大小。所以我在前面的代码中将尺寸设置为28 x 28，便于说明。现在，你可能会想，我们如何训练这样的网络呢？好吧，接下来我们就会看到这一点，但在此之前，我们需要准备MNIST数据集，使用`MnistDataSetIterator()`方法，如下所示：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s start training the CNN, using the train set and iterate for each
    epoch:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始训练CNN，使用训练集并为每个周期进行迭代：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have trained the CNN, the next task is to evaluate the model on the
    test set, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练好了CNN，接下来的任务是评估模型在测试集上的表现，如下所示：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we compute some performance matrices, such as `Accuracy`, `Precision`,
    `Recall`, and `F1 measure`, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算一些性能矩阵，如`Accuracy`、`Precision`、`Recall`和`F1 measure`，如下所示：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For your ease, here I provided the full source code for this simple image classifier:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便你，我在这里提供了这个简单图像分类器的完整源代码：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Large-scale image classification using CNN
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN进行大规模图像分类
- en: In this section, we will show a step-by-step example of developing a real-life
    ML project for image classification. However, we need to know the problem description
    first, to learn what sort of image classification needs to be done. Moreover,
    learning about the dataset is a mandate before getting started.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示一个逐步开发实际ML项目（用于图像分类）的示例。然而，我们首先需要了解问题描述，以便知道需要进行什么样的图像分类。此外，在开始之前，了解数据集是必须的。
- en: Problem description
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题描述
- en: 'Nowadays, food selfies and photo-centric social storytelling are becoming social
    trends. Food lovers willingly upload an enormous amount of selfies taken with
    food and a picture of the restaurant to social media and respective websites.
    And, of course, they also provide a written review that can significantly boost
    the popularity of the restaurant:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，食物自拍和以照片为中心的社交故事讲述正在成为社交趋势。美食爱好者愿意将大量与食物合影的自拍和餐厅的照片上传到社交媒体和相应的网站。当然，他们还会提供书面评论，这能显著提升餐厅的知名度：
- en: '![](img/454a5472-e1c8-4bfe-823d-5161d6510316.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/454a5472-e1c8-4bfe-823d-5161d6510316.png)'
- en: 'Figure 6: Mining some insights about business from Yelp dataset'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：从Yelp数据集中挖掘一些商业洞察
- en: For example, millions of unique visitors visit Yelp and have written more than
    135 million reviews. There are lots of photos and lots of users who are uploading
    photos. Business owners can post photos and message their customers. This way,
    Yelp makes money by **selling ads** to those local businesses. An interesting
    fact is that these photos provide rich local business information across categories.
    Thus, training a computer to understand the context of these photos is not a trivial
    one and also not an easy task (see *Figure 6* to get an insight).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数百万独立访问者访问 Yelp 并撰写了超过 1.35 亿条评论。平台上有大量照片和上传照片的用户。商家可以发布照片并与顾客互动。通过这种方式，Yelp
    通过向这些本地商家 **出售广告** 来赚钱。一个有趣的事实是，这些照片提供了丰富的本地商业信息，涵盖了多个类别。因此，训练计算机理解这些照片的上下文并非一件简单的事，也不是一项容易的任务（参考
    *图 6* 获取更多的见解）。
- en: 'Now, the idea of this project is a challenging one: how can we turn those pictures
    into words? Let''s give it a try. More specifically, you are given photos that
    belong to a business. Now we need to build a model so that it can tags restaurants
    with multiple labels of the user-submitted photos automatically—that is, to predict
    the business attributes.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个项目的理念是充满挑战的：我们如何将这些图片转化为文字？让我们试试看。更具体地说，你将获得属于某个商家的照片。现在我们需要建立一个模型，使其能够自动为餐馆标记多个用户提交的照片标签——也就是说，预测商家的属性。
- en: Description of the image dataset
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像数据集的描述
- en: For such a challenge we need to have a real dataset. Don't worry, there are
    several platforms where such datasets are publicly available or can be downloaded
    with some terms and conditions. One such platform is **Kaggle**, which provides
    a platform for data analytics and ML practitioners to try ML challenges and win
    prizes. The Yelp dataset and the description can be found at: [https://www.kaggle.com/c/yelp-restaurant-photo-classification](https://www.kaggle.com/c/yelp-restaurant-photo-classification).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的挑战，我们需要一个真实的数据集。别担心，有多个平台提供公开的数据集，或者可以在一定的条款和条件下下载。一种这样的平台是 **Kaggle**，它为数据分析和机器学习实践者提供了一个平台，参与机器学习挑战并赢取奖品。Yelp
    数据集及其描述可以在以下网址找到：[https://www.kaggle.com/c/yelp-restaurant-photo-classification](https://www.kaggle.com/c/yelp-restaurant-photo-classification)。
- en: 'The labels of the restaurants are manually selected by Yelp users when they
    submit a review. There are nine different labels annotated by the Yelp community
    associated in the dataset:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 餐馆的标签是由 Yelp 用户在提交评论时手动选择的。数据集中包含 Yelp 社区标注的九种不同标签：
- en: '`0: good_for_lunch`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0: good_for_lunch`'
- en: '`1: good_for_dinner`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1: good_for_dinner`'
- en: '`2: takes_reservations`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2: takes_reservations`'
- en: '`3: outdoor_seating`'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3: outdoor_seating`'
- en: '`4: restaurant_is_expensive`'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4: restaurant_is_expensive`'
- en: '`5: has_alcohol`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`5: has_alcohol`'
- en: '`6: has_table_service`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`6: has_table_service`'
- en: '`7: ambience_is_classy`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`7: ambience_is_classy`'
- en: '`8: good_for_kids`'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`8: good_for_kids`'
- en: 'So we need to predict these labels as accurately as possible. One thing to
    be noted is that since Yelp is a community-driven website, there are duplicated
    images in the dataset for several reasons. For example, users can accidentally
    upload the same photo to the same business more than once, or chain businesses
    can upload the same photo to different branches. There are six files in the dataset,
    as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要尽可能准确地预测这些标签。需要注意的一点是，由于 Yelp 是一个社区驱动的网站，数据集中存在重复的图片，原因有很多。例如，用户可能会不小心将同一张照片上传到同一商家多次，或者连锁商家可能会将相同的照片上传到不同的分店。数据集包含以下六个文件：
- en: '`train_photos.tgz`: Photos to be used as the training set (234,545 images)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_photos.tgz`: 用作训练集的照片（234,545 张图片）'
- en: '`test_photos.tgz`: Photos to be used as the test set (500 images)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_photos.tgz`: 用作测试集的照片（500 张图片）'
- en: '`train_photo_to_biz_ids.csv`: Provides the mapping between the photo ID to
    business ID (234,545 rows)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_photo_to_biz_ids.csv`: 提供照片 ID 和商家 ID 之间的映射（234,545 行）'
- en: '`test_photo_to_biz_ids.csv`: Provides the mapping between the photo ID to business
    ID ( 500 rows)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_photo_to_biz_ids.csv`: 提供照片 ID 和商家 ID 之间的映射（500 行）'
- en: '`train.csv`: This is the main training dataset including business IDs, and
    their corresponding labels (1996 rows)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.csv`: 这是主要的训练数据集，包含商家 ID 和它们对应的标签（1996 行）'
- en: '`sample_submission.csv`: A sample submission—reference correct format for your
    predictions including `business_id` and the corresponding predicted labels'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_submission.csv`: 一个示例提交文件——参考正确的格式来提交你的预测结果，包括 `business_id` 和对应的预测标签'
- en: Workflow of the overall project
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整个项目的工作流程
- en: 'In this project, we will see how to read images from `.jpg` format into a matrix
    representation in Scala. Then, we will further process and prepare those images
    feedable by the CNNs. We will see several image manipulations, such as squaring
    all the images and resizing every image to the same dimensions, before we apply
    a grayscale filter to the image:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将展示如何将`.jpg`格式的图像读取为Scala中的矩阵表示。接着，我们将进一步处理并准备这些图像，以便CNN能够接收。我们将看到几种图像操作，比如将所有图像转换为正方形，并将每个图像调整为相同的尺寸，然后对图像应用灰度滤镜：
- en: '![](img/1501815a-0ebd-4b08-a671-19a7990e0bc4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1501815a-0ebd-4b08-a671-19a7990e0bc4.png)'
- en: 'Figure 7: A conceptualize view of a CNN for image classification'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：用于图像分类的 CNN 概念视图
- en: Then we train nine CNNs on training data for each class. Once the training is
    completed, we save the trained model, CNN configurations and parameters, so that
    they can be restored later on, and then we apply a simple aggregate function to
    assign classes to each restaurant, where each one has multiple images associated
    with it, each with its own vector of probabilities for each of the nine classes.
    Then we score test data and finally, evaluate the model using test images.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在训练数据上训练九个CNN，每个分类一个。一旦训练完成，我们保存训练好的模型、CNN配置和参数，以便以后恢复，接着我们应用一个简单的聚合函数来为每个餐厅分配类别，每个餐厅都有多个与之相关联的图像，每个图像都有其对应的九个类别的概率向量。然后我们对测试数据打分，最后，使用测试图像评估模型。
- en: Now let's see the structure of each CNN. Each network will have two convolutional
    layers, two subsampling layers, one dense layer, and the output layer as the fully
    connected layer. The first layer is a convolutional layer followed by a subsampling
    layer, which is again followed by another convolutional layer, then a subsampling
    layer, then a dense layer, which is followed by an output layer. We will see each
    layer's structure later on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看每个CNN的结构。每个网络将有两个卷积层，两个子采样层，一个密集层，以及作为完全连接层的输出层。第一层是卷积层，接着是子采样层，再之后是另一个卷积层，然后是子采样层，再接着是一个密集层，最后是输出层。我们稍后会看到每一层的结构。
- en: Implementing CNNs for image classification
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 CNN 用于图像分类
- en: 'The Scala object containing the `main()` method has the following workflow:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 包含`main()`方法的 Scala 对象有以下工作流：
- en: We read all the business labels from the `train.csv` file
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`train.csv`文件中读取所有的业务标签
- en: We read and create a map from image ID to business ID of form `imageID` → `busID`
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取并创建一个从图像ID到业务ID的映射，格式为`imageID` → `busID`
- en: We get a list of images from the `photoDir` directory to load and process and,
    finally, get the image IDs of 10,000 images (feel free to set the range)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`photoDir`目录获取图像列表，加载和处理图像，最后获取10,000张图像的图像ID（可以自由设置范围）
- en: We then read and process images into a `photoID` → vector map
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们读取并处理图像，形成`photoID` → 向量映射
- en: We chain the output of *step 3* and *step 4* to align the business feature,
    image IDs, and label IDs to get the feature extracted for the CNN
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将*步骤 3*和*步骤 4*的输出链接起来，对齐业务特征、图像ID和标签ID，以提取CNN所需的特征
- en: We construct nine CNNs.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建了九个CNN。
- en: We train all the CNNs and specify the model savings locations
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练所有的CNN并指定模型保存的位置
- en: We then repeat *step 2* to *step 6* to extract the features from the test set
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们重复*步骤 2*到*步骤 6*来从测试集提取特征
- en: Finally, we evaluate the model and save the prediction in a CSV file
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们评估模型并将预测结果保存到 CSV 文件中
- en: 'Now let''s see how the preceding steps would look in a high-level diagram:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看前述步骤在高层次的图示中是如何表示的：
- en: '![](img/ffa6d1ab-e72d-4d69-b4b2-a358679a7ed5.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffa6d1ab-e72d-4d69-b4b2-a358679a7ed5.png)'
- en: 'Figure 8: DL4j image processing pipeline for image classification'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：DL4j 图像处理管道用于图像分类
- en: 'Programmatically, the preceding steps can be represented as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序的角度来看，前述步骤可以表示如下：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Too much of a mouthful? Don't worry, we will now see each step in detail. If
    you look at the preceding steps carefully, you'll see *steps 1* to *step 5* are
    basically image processing and feature constructions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 觉得太复杂了吗？别担心，我们现在将详细查看每一步。如果仔细看前面的步骤，你会发现*步骤 1*到*步骤 5*基本上是图像处理和特征构建。
- en: Image processing
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: 'When I tried to develop this application, I found that the photos are of different
    size and shape: some images are tall, some of them are wide, some of them are
    outside, some images are inside, and most of them are pictures of food. However,
    some are other, random things too. Another important aspect is, while training
    images varied in portrait/landscape and the number of pixels, most were roughly
    square, and many of them were exactly 500 x 375:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我试图开发这个应用程序时，我发现照片的大小和形状各不相同：一些图像是高的，一些是宽的，一些在外面，一些在里面，大多数是食物图片。然而，还有一些其他的随机物品。另一个重要的方面是，尽管训练图像在纵向/横向和像素数量上有所不同，大多数都是大致正方形的，许多都是确切的500
    x 375：
- en: '![](img/40a41e7c-baad-4e63-b049-12851134ce52.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40a41e7c-baad-4e63-b049-12851134ce52.png)'
- en: 'Figure 9: Resized figure (left the original and tall one, right the squared
    one)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：调整大小后的图像（左边为原始和高大的图像，右边为正方形的图像）
- en: As we have already seen, CNN cannot work with images with a heterogeneous size
    and shape. There are many robust and efficient image processing techniques to
    extract only the **region of interest** (**ROI**). But, honestly, I am not an
    image processing expert, so I decided to keep this resizing step simpler.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，CNN无法处理尺寸和形状异质的图像。有许多强大且有效的图像处理技术可以仅提取感兴趣区域（ROI）。但是，老实说，我不是图像处理专家，所以决定简化这个调整大小的步骤。
- en: CNN has a serious limitation as it cannot address the orientational and relative
    spatial relationships. Therefore, these components are not very important to a
    CNN. In short, CNN is not that suitable for images having heterogeneous shape
    and orientation. For why, people are now talking about the Capsule Networks. See
    more at the original paper at [https://arxiv.org/pdf/1710.09829v1.pdf](https://arxiv.org/pdf/1710.09829v1.pdf)
    and [https://openreview.net/pdf?id=HJWLfGWRb](https://openreview.net/pdf?id=HJWLfGWRb).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）有一个严重的限制，即不能处理方向和相对空间关系。因此，这些组成部分对CNN来说并不重要。简而言之，CNN不太适合具有异构形状和方向的图像。因此，现在人们开始讨论胶囊网络。详细内容请参见原始论文：[https://arxiv.org/pdf/1710.09829v1.pdf](https://arxiv.org/pdf/1710.09829v1.pdf)
    和 [https://openreview.net/pdf?id=HJWLfGWRb](https://openreview.net/pdf?id=HJWLfGWRb)。
- en: Naively, I made all the images square, but still, I tried to preserve the quality.
    The ROIs are centered in most cases, so capturing only the center-most square
    of each image is not that trivial. Nevertheless, we also need to convert each
    image to a grayscale image. Let's make irregularly shaped images square. Take
    a look at the following image, where the original one is on the left and the right
    is the square one (see *Figure 9)*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地说，我将所有图像都制作成了正方形，但仍然努力保持质量。在大多数情况下，ROI是居中的，因此仅捕获每个图像的中心最方正的部分并不那么简单。尽管如此，我们还需要将每个图像转换为灰度图像。让我们把不规则形状的图像变成正方形。请看下面的图像，左边是原始图像，右边是正方形图像（见*图9*）。
- en: 'Now we have generated a square one, how did we achieve this? Well, I checked
    first if the height and the width are the same, if so, no resizing takes place.
    In the other two cases, I cropped the center region. The following method does
    the trick (but feel free to execute the `SquaringImage.scala` script to see the
    output):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一个正方形图像，我们是如何做到这一点的呢？好吧，我首先检查高度和宽度是否相同，如果是，则不进行调整大小。在另外两种情况下，我裁剪了中心区域。以下方法可以达到效果（但随意执行`SquaringImage.scala`脚本以查看输出）：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Well done! Now that all of our training images are square, the next import
    preprocessing task is to resize them all. I decided to make all the images 128
    x 128 in size. Let''s see how the previous (the original) one looks after resizing:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在我们所有的训练图像都是正方形的，下一个重要的预处理任务是将它们全部调整大小。我决定将所有图像都调整为128 x 128的大小。让我们看看之前（原始的）调整大小后的图像如何看起来：
- en: '![](img/a529084c-1479-4008-ac6b-37c1de1bebb7.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a529084c-1479-4008-ac6b-37c1de1bebb7.png)'
- en: 'Figure 10: Image resizing (256 x 256, 128 x 128, 64 x 64 and 32 x 32 respectively)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：图像调整（256 x 256, 128 x 128, 64 x 64 和 32 x 32 分别）
- en: 'The following method did the trick (but feel free to execute the `ImageResize.scala`
    script to see a demo):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法可以达到效果（但随意执行`ImageResize.scala`脚本以查看演示）：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'By the way, for the image resizing and squaring, I used some built-in packages
    for image reading and some third-party packages for processing:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，为了图像调整和制作正方形，我使用了一些内置的图像读取包和一些第三方处理包：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To use the preceding packages, add the following dependencies in a Maven-friendly
    `pom.xml` file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用上述包，请在Maven友好的`pom.xml`文件中添加以下依赖项：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Although DL4j-based CNNs can handle color images, it's better to simplify the
    computation with grayscale images. Although color images are more exciting and
    effective, this way we can make the overall representation simpler and space efficient.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于DL4j的卷积神经网络（CNN）可以处理彩色图像，但使用灰度图像可以简化计算。虽然彩色图像更具吸引力且效果更好，但通过这种方式，我们可以使整体表示更简单，并且节省空间。
- en: 'Let''s give an example of our previous step. We resized each image to a 256
    x 256 pixel image represented by 16,384 features, rather than 16,384 x 3 for a
    color image having three RGB channels (execute `GrayscaleConverter.scala` to see
    a demo). Let''s see how the converted image would look:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个之前步骤的例子。我们将每个图像调整为256 x 256像素的图像，表示为16,384个特征，而不是为一个有三个RGB通道的彩色图像表示为16,384
    x 3（执行`GrayscaleConverter.scala`来查看演示）。让我们看看转换后的图像效果：
- en: '![](img/5c109324-5f47-4bf4-9aec-c9007d184095.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c109324-5f47-4bf4-9aec-c9007d184095.png)'
- en: 'Figure 11: Left - original image, right the grayscale one RGB averaging'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：左 - 原始图像，右 - 灰度图像RGB平均化
- en: 'The preceding conversion is done using two methods called `pixels2Gray()` and
    `makeGray()`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 上述转换是使用名为`pixels2Gray()`和`makeGray()`的两个方法完成的：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So what happens under the hood? We chain the preceding three steps: make all
    the images square, then convert all of them to 25 x 256, and finally convert the
    resized image into a grayscale one:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，幕后发生了什么呢？我们将前面提到的三个步骤串联起来：首先将所有图像调整为正方形，然后将它们转换为25 x 256，最后将调整大小后的图像转换为灰度图像：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, in summary, we now have all the images in gray after squaring and resizing.
    The following image gives some sense of the conversion step:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，现在我们已经对所有图像进行了正方形化和调整大小，图像已经变为灰度。以下图像展示了转换步骤的一些效果：
- en: '![](img/4f8c2de5-bfa6-4582-871b-690811daae45.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f8c2de5-bfa6-4582-871b-690811daae45.png)'
- en: 'Figure 12: Resized figure (left the original and tall one, right the squared
    one)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：调整大小后的图像（左侧为原始高图，右侧为调整后的正方形图像）
- en: 'The following chaining also comes with some additional effort. Now we put these
    three steps together in the code, and we can finally prepare all of our images:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的步骤链式操作也需要额外的努力。现在我们将这三个步骤放在代码中，最终准备好所有的图像：
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Extracting image metadata
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取图像元数据
- en: Up too this point, we have loaded and pre-processed raw images, but we have
    no idea about the image metadata that we need to make our CNNs learn. Thus, it's
    time to load the CSV files that contain metadata about each image.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经加载并预处理了原始图像，但我们还不知道需要哪些图像元数据来让我们的CNN进行学习。因此，现在是时候加载包含每个图像元数据的CSV文件了。
- en: 'I wrote a method to read such metadata in CSV format, called `readMetadata()`,
    which is used later on by two other methods called `readBusinessLabels` and `readBusinessToImageLabels`.
    These three methods are defined in the `CSVImageMetadataReader.scala` script.
    Here''s the signature of the `readMetadata()` method:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一个方法来读取这种CSV格式的元数据，叫做`readMetadata()`，稍后两个方法`readBusinessLabels`和`readBusinessToImageLabels`也会使用它。这三个方法定义在`CSVImageMetadataReader.scala`脚本中。下面是`readMetadata()`方法的签名：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `readBusinessLabels()` method maps from business ID to labels of the form
    `businessID` → Set (labels):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`readBusinessLabels()`方法将商业ID映射到标签，格式为 `businessID` → Set (标签)：'
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `readBusinessToImageLabels ()` method maps from image ID to business ID
    of the form `imageID` → `businessID`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`readBusinessToImageLabels()`方法将图像ID映射到商业ID，格式为 `imageID` → `businessID`：'
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Image feature extraction
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征提取
- en: So far we have seen how to preprocess images so that features from those images
    can be extracted and fed into CNNs. Additionally, we have seen how to extract
    and map metadata and link it with the original images. Now it's time to extract
    features from those preprocessed images.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何预处理图像，以便从中提取特征并将其输入到CNN中。此外，我们还看到了如何提取和映射元数据并将其与原始图像链接。现在是时候从这些预处理过的图像中提取特征了。
- en: 'We also need to keep in mind the provenance of the metadata of each image.
    As you can guess, we need three map operations for feature extractions. Essentially,
    we have three maps. For details see the `imageFeatureExtractor.scala` script:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要记住每个图像元数据的来源。正如你所猜的那样，我们需要三次映射操作来提取特征。基本上，我们有三个映射。详情请参见`imageFeatureExtractor.scala`脚本：
- en: Business mapping with the form `imageID` → `businessID`
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 商业映射形式为 `imageID` → `businessID`
- en: Data map of the form `imageID` → image data
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据映射形式为 `imageID` → 图像数据
- en: Label map of the form `businessID` → labels
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签映射形式为 `businessID` → 标签
- en: 'We first define a regular expression pattern to extract the `.jpg` name from
    the CSV `ImageMetadataReader` class, which is used to match against training labels:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个正则表达式模式，从CSV `ImageMetadataReader`类中提取`.jpg`名称，该类用于与训练标签匹配：
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then we extract all the image IDs associated with their respective business
    ID:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取出所有与相应业务ID关联的图像ID：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we need to load and process all the images that are already preprocessed
    to extract the image IDs by mapping with the IDs extracted from the business IDs,
    as shown earlier:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要加载并处理所有已经预处理过的图像，通过与从业务ID提取的ID进行映射，如前所示，来提取图像ID：
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So far, we have been able to extract all the image IDs that are somehow associated
    with at least one business. The next move would be to read and process these images
    into an `imageID` → vector map:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经能够提取出所有与至少一个业务相关的图像ID。下一步是读取并处理这些图像，形成`imageID` → 向量映射：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Well done! We are just one step behind to extract required to train our CNNs.
    The final step in feature extraction is to extract the pixel data:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！我们只差一步，就能提取出训练CNN所需的数据。特征提取的最后一步是提取像素数据：
- en: '![](img/7bee7abb-8627-4ec1-84b4-66d95a1b5bb4.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bee7abb-8627-4ec1-84b4-66d95a1b5bb4.png)'
- en: 'Figure 13: Image data representation'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：图像数据表示
- en: 'In summary, we need to keep track of four pieces of the object for each image—that
    is, `imageID`, `businessID`, labels, and pixel data. Thus, as shown in the preceding
    figure, the primary data structure is constructed with four data types (four tuples)—`imgID`,
    `businessID`, pixel data vector, and labels:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，我们需要为每个图像跟踪四个对象的组成部分——即`imageID`、`businessID`、标签和像素数据。因此，如前图所示，主要数据结构由四种数据类型（四元组）构成——`imgID`、`businessID`、像素数据向量和标签：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Thus, we should have a class containing all pieces of these objects. Don''t
    worry, everything we need is defined in the `featureAndDataAligner.scala` script.
    Once we have instantiated an instance of `featureAndDataAligner` using the following
    line of code in the `Main.scala` script (under the `main` method), the `businessMap`,
    `dataMap`, and `labMap` are provided:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该有一个包含这些对象所有部分的类。别担心，我们需要的所有内容都已在`featureAndDataAligner.scala`脚本中定义。一旦我们在`Main.scala`脚本中的`main`方法下，通过以下代码行实例化`featureAndDataAligner`的实例，就可以提供`businessMap`、`dataMap`和`labMap`：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here, the option type for `labMap` is used since we don''t have this information
    when we score on test data—that is, `None` is used for that invocation:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`labMap`的选项类型被使用，因为在对测试数据评分时我们没有这个信息——即，对于该调用使用`None`：
- en: '[PRE32]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Excellent! Up to this point, we have managed to extract the features to train
    our CNNs. However, the feature in current form is still not suitable to feed into
    the CNNs, because we only have the feature vectors without labels. Thus, we need
    an intermediate conversion.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！到目前为止，我们已经成功提取了用于训练CNN的特征。然而，目前形式下的特征仍然不适合输入到CNN中，因为我们只有特征向量而没有标签。因此，我们需要进行中间转换。
- en: Preparing the ND4j dataset
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备ND4j数据集
- en: 'As I said, we need an intermediate conversion and pre-process to get the training
    set to contain feature vectors as well as labels. The conversion is pretty straightforward:
    we need the feature vector and the business labels.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所说，我们需要进行中间转换和预处理，以便将训练集包含特征向量以及标签。这个转换过程非常直接：我们需要特征向量和业务标签。
- en: 'For this, we have the `makeND4jDataSets` class (see `makeND4jDataSets.scala`
    for details). The class creates a ND4j dataset object from the data structure
    from the `alignLables` function in the form of `List[(imgID, bizID, labels, pixelVector)]`.
    First, we prepare the dataset using the `makeDataSet()` method:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们有`makeND4jDataSets`类（详见`makeND4jDataSets.scala`）。该类通过`alignLables`函数中的数据结构（以`List[(imgID,
    bizID, labels, pixelVector)]`的形式）创建ND4j数据集对象。首先，我们使用`makeDataSet()`方法准备数据集：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then we need to convert the preceding data structure further, into `INDArray`,
    which can then be consumed by the CNNs:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要进一步转换前面的数据结构，转换为`INDArray`，这样CNN就可以使用：
- en: '[PRE34]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Training the CNNs and saving the trained models
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练CNN并保存训练好的模型
- en: 'So far, we have seen how to prepare the training set; now we have a challenge
    ahead. We have to train 234,545 images. Although the testing phase would be less
    exhaustive with only 500 images, it''s better to train each CNN using batch-mode
    using DL4j''s `MultipleEpochsIterator`. Here''s a list of important hyperparameters
    and their details:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何准备训练集；现在我们面临一个挑战。我们必须训练234,545张图片。尽管测试阶段只用500张图片会轻松一些，但最好还是使用批处理模式，通过DL4j的`MultipleEpochsIterator`来训练每个CNN。以下是一些重要的超参数及其详细信息：
- en: '**Layers**: As we have already observed with our simple 5 layers MNIST, we
    received outstanding classification accuracy, which is very promising. Here I
    will try to construct a similar network having.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层数**：正如我们在简单的5层MNIST网络中已经观察到的，我们获得了卓越的分类精度，这非常有前景。在这里，我将尝试构建一个类似的网络。'
- en: '**The number of samples**: If you''re training all the images, it will take
    too long. If you train using the CPU, not the GPU, it will take days. When I tried
    with 50,000 images, it took one whole day for a machine with an i7 processor and
    32 GB of RAM. Now you can imagine how long it would take for the whole dataset.
    In addition, it will require at least 256 GB of RAM even if you do the training
    in the batch model.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本数量**：如果你训练所有图片，可能需要很长时间。如果你使用CPU而不是GPU进行训练，那将需要数天时间。当我尝试使用50,000张图片时，一台配置为i7处理器和32
    GB内存的机器用了整整一天。现在你可以想象，如果使用整个数据集会需要多长时间。此外，即使你使用批处理模式进行训练，它也至少需要256 GB的RAM。'
- en: '**Number of epochs**: This is the number of iterations through all the training
    records.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮次**：这是遍历所有训练记录的次数。'
- en: '**Number of output feature maps (that is, nOut)**: This is the number of feature
    maps. Take a closer look at other examples in the DL4j GitHub repository.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出特征图的数量（即nOut）**：这是特征图的数量。可以仔细查看DL4j GitHub仓库中的其他示例。'
- en: '**Learning Rate**: From the TensorFlow-like framework, I got some insights.
    In my opinion, it would be great to set a learning rate of 0.01 and 0.001.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：从类似TensorFlow的框架中，我获得了一些启示。在我看来，设置学习率为0.01和0.001会非常合适。'
- en: '**Number of batch**: This is the number of records in each batch—32, 64, 128,
    and so on. I used 128.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批次数量**：这是每个批次中的记录数量——32、64、128，依此类推。我使用了128。'
- en: Now, with the preceding hyperparameters, we can start training our CNNs. The
    following code does the trick. At first, we prepare the training set, then we
    define the required hyperparameters, then we normalize the dataset so the ND4j
    data frame is encoded and any labels that are considered true are 1s and the rest
    are zeros. Then we shuffle both the rows and labels of the encoded dataset.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用前面的超参数，我们可以开始训练我们的CNN。以下代码实现了这一功能。首先，我们准备训练集，然后定义所需的超参数，接着我们对数据集进行归一化，使ND4j数据框架被编码，且任何被认为为真实的标签是1，其余为0。然后我们对编码后的数据集的行和标签进行洗牌。
- en: 'Now we need to create epochs for the dataset iterator using the `ListDataSetIterator`
    and `MultipleEpochsIterator` respectively. Once the datasets are converted into
    batch-model, we are then ready to train the constructed CNNs:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要使用`ListDataSetIterator`和`MultipleEpochsIterator`分别为数据集迭代器创建epoch。将数据集转换为批次模型后，我们就可以开始训练构建的CNN：
- en: '[PRE35]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the preceding code, we also save a `.json` file containing all the network
    configurations and a `.bin` file for holding all the weights and parameters of
    all the CNNs. This is done by two methods; namely, `saveNN()` and `loadNN()` that
    are defined in the `NeuralNetwok.scala` script. First, let''s see the signature
    of the `saveNN()` method that goes as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们还保存了一个`.json`文件，包含所有网络配置，以及一个`.bin`文件，用于存储所有CNN的权重和参数。这是通过两个方法完成的；即在`NeuralNetwok.scala`脚本中定义的`saveNN()`和`loadNN()`。首先，让我们看看`saveNN()`方法的签名，代码如下：
- en: '[PRE36]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The idea is visionary as well as important since, as I said, you wouldn''t
    train your whole network for the second time to evaluate a new test set—that is,
    suppose you want to test just a single image. We also have another method named
    `loadNN()` that reads back the `.json` and `.bin` file we created earlier to a
    `MultiLayerNetwork` and is used to score new test data. The method goes as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法既有远见又很重要，因为，正如我所说，你不会为了评估一个新的测试集而第二次训练整个网络——也就是说，假设你只想测试一张图片。我们还有另一种方法叫做`loadNN()`，它将之前创建的`.json`和`.bin`文件读取回`MultiLayerNetwork`并用于评分新的测试数据。方法如下：
- en: '[PRE37]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Evaluating the model
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'The scoring approach that we''re going to use is pretty simple. It assigns
    business-level labels by averaging the image-level predictions. I know I did it
    naively, but you can try a better approach. What I have done is assign a business
    with label `0` if the average of the probabilities across all of its images that
    it belongs to class `0` is greater than 0.5:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的评分方法非常简单。它通过平均图像级别的预测来分配业务级别的标签。我知道我做得比较简单，但你可以尝试更好的方法。我做的是，如果某个业务的所有图像属于类别`0`的概率平均值大于0.5，则为该业务分配标签`0`：
- en: '[PRE38]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then we collect the model predictions from the `scoreModel()` method and merge
    with `alignedData`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从`scoreModel()`方法收集模型预测，并与`alignedData`合并：
- en: '[PRE39]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Finally, we can restore the trained and saved models, restore them back, and
    generate the submission file for Kaggle. The thing is that we need to aggregate
    image predictions to business scores for each model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以恢复训练并保存的模型，恢复它们，并生成Kaggle的提交文件。关键是我们需要将图像预测聚合成每个模型的业务分数。
- en: Wrapping up by executing the main() method
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过执行main()方法进行总结
- en: 'Let''s wrap up the overall discussion by watching the performance of our model.
    The following code is an overall glimpse:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看模型的性能来总结整体讨论。以下代码是一个总体概览：
- en: '[PRE40]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So, what's your impression? It's true that we haven't received outstanding classification
    accuracy. But we can still try with tuned hyperparameters. The next section provides
    some insight.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你的印象如何？确实，我们没有得到优秀的分类准确度。但我们仍然可以尝试调整超参数。下一部分提供了一些见解。
- en: Tuning and optimizing CNN hyperparameters
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整和优化CNN超参数
- en: The following hyperparameters are very important and must be tuned to achieve
    optimized results.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 以下超参数非常重要，必须调整以获得优化结果。
- en: '**Dropout**: Used for random omission of feature detectors to prevent overfitting'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃法（Dropout）**：用于随机省略特征检测器，以防止过拟合'
- en: '**Sparsity**: Used to force activations of sparse/rare inputs'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏性**：用于强制激活稀疏/罕见输入'
- en: '**Adagrad**: Used for feature-specific learning-rate optimization'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应梯度法（Adagrad）**：用于特征特定的学习率优化'
- en: '**Regularization**: L1 and L2 regularization'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：L1和L2正则化'
- en: '**Weight transforms**: Useful for deep autoencoders'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重转换**：对深度自编码器有用'
- en: '**Probability distribution manipulation**: Used for initial weight generation'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率分布操控**：用于初始权重生成'
- en: Gradient normalization and clipping
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度归一化和裁剪
- en: 'Another important question is: when do you want to add a max pooling layer
    rather than a convolutional layer with the same stride? A max pooling layer has
    no parameters at all, whereas a convolutional layer has quite a few. Sometimes,
    adding a local response normalization layer that makes the neurons that most strongly
    activate inhibit neurons at the same location but in neighboring feature maps,
    encourages different feature maps to specialize and pushes them apart, forcing
    them to explore a wider range of features. It is typically used in the lower layers
    to have a larger pool of low-level features that the upper layers can build upon.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的问题是：你什么时候想要添加一个最大池化层，而不是具有相同步幅的卷积层？最大池化层根本没有参数，而卷积层有很多。有时，添加一个局部响应归一化层，可以让最强激活的神经元抑制同一位置但邻近特征图中的神经元，鼓励不同的特征图进行专门化，并将它们分开，迫使它们探索更广泛的特征。通常用于较低层，以便拥有更多低级特征供上层构建。
- en: 'One of the main advantages observed during the training of large neural networks
    is overfitting, that is, generating very good approximations for the training
    data but emitting noise for the zones between single points. In the case of overfitting,
    the model is specifically adjusted to the training dataset, so it will not be
    used for generalization. Therefore, although it performs well on the training
    set, its performance on the test dataset and subsequent tests is poor because
    it lacks the generalization property:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练大规模神经网络时观察到的主要问题之一是过拟合，即为训练数据生成非常好的逼近，但在单个点之间的区域产生噪声。在过拟合的情况下，模型专门针对训练数据集进行调整，因此不能用于泛化。因此，尽管在训练集上表现良好，但在测试集和后续测试中的表现较差，因为它缺乏泛化能力：
- en: '![](img/e7c1d031-2428-4f36-b4d1-a703de85c86e.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7c1d031-2428-4f36-b4d1-a703de85c86e.png)'
- en: 'Figure 14: Dropout versus without dropout'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：丢弃法与不丢弃法的对比
- en: The main advantage of this method is that it avoids all neurons in a layer to
    synchronously optimize their weights. This adaptation made in random groups avoids
    all the neurons converging on the same goals, thus de-correlating the adapted
    weights. A second property discovered in the dropout application is that the activation
    of the hidden units becomes sparse, which is also a desirable characteristic.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要优点是避免了同一层的所有神经元同步优化它们的权重。这种在随机组中进行的适应，避免了所有神经元收敛到相同的目标，从而使得适应的权重不相关。应用dropout时发现的第二个特性是，隐藏单元的激活变得稀疏，这也是一种理想的特性。
- en: 'Since in CNN, one of the objective functions is to minimize the evaluated cost,
    we must define an optimizer. The following optimizers are supported by DL4j:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在CNN中，目标函数之一是最小化计算出的代价，我们必须定义一个优化器。DL4j支持以下优化器：
- en: SGD (learning rate only)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD（仅学习率）
- en: Nesterovs momentum
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov的动量
- en: Adagrad
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: RMSProp
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSProp
- en: Adam
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam
- en: AdaDelta
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaDelta
- en: In most of the cases, we can adopt the implemented RMSProp, which is an advanced
    form of gradient descent, if the performance is not satisfactory. RMSProp performs
    better because it divides the learning rate by an exponentially decaying average
    of squared gradients. The suggested setting value of the decay parameter is 0.9,
    while a good default value for the learning rate is 0.001.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，如果性能不满意，我们可以采用已实现的RMSProp，它是梯度下降的高级形式。RMSProp表现更好，因为它将学习率除以平方梯度的指数衰减平均值。建议的衰减参数值为0.9，而学习率的一个良好默认值为0.001。
- en: More technically, by using the most common optimizer, such as **Stochastic Gradient
    Descent** (**SGD**), the learning rates must scale with 1/T to get convergence,
    where T is the number of iterations. RMSProp tries to overcome this limitation
    automatically by adjusting the step size so that the step is on the same scale
    as the gradients. So, if you're training a neural network, but computing the gradients
    is mandatory, using RMSProp would be the faster way of learning in a mini-batch
    setting. Researchers also recommend using a Momentum optimizer while training
    a deep CNN or DNN.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地说，通过使用最常见的优化器，如**随机梯度下降**（**SGD**），学习率必须按1/T的比例进行缩放才能收敛，其中T是迭代次数。RMSProp尝试通过自动调整步长来克服这一限制，使步长与梯度处于相同的尺度。因此，如果你正在训练神经网络，但计算梯度是必须的，使用RMSProp将是小批量训练中更快的学习方式。研究人员还建议在训练深度CNN或DNN时使用动量优化器。
- en: From the layering architecture's perspective, CNN is different compared to DNN;
    it has a different requirement as well as tuning criteria. Another problem with
    CNNs is that the convolutional layers require a huge amount of RAM, especially
    during training, because the reverse pass of backpropagation requires all the
    intermediate values computed during the forward pass. During inference (that is,
    when making a prediction for a new instance), the RAM occupied by one layer can
    be released as soon as the next layer has been computed, so you only need as much
    RAM as required by two consecutive layers.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 从分层架构的角度来看，CNN与DNN不同；它有不同的需求和调优标准。CNN的另一个问题是卷积层需要大量的RAM，尤其是在训练过程中，因为反向传播的反向传递需要保留前向传播过程中计算的所有中间值。在推理过程中（即对新实例进行预测时），一个层占用的RAM可以在下一个层计算完毕后释放，因此你只需要两个连续层所需的内存。
- en: 'However, during training, everything computed during the forward pass needs
    to be preserved for the reverse pass, so the amount of RAM needed is (at least)
    the total amount of RAM required by all layers. If your GPU runs out of memory
    while training a CNN, here are five things you could try to solve the problem
    (other than purchasing a GPU with more RAM):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在训练过程中，前向传播过程中计算的所有内容都需要在反向传播时保留下来，因此所需的内存量至少是所有层所需的总内存量。如果你的GPU在训练CNN时内存不足，这里有五个解决问题的建议（除了购买更大内存的GPU）：
- en: Reduce the mini-batch size
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减小小批量的大小
- en: Reduce dimensionality using a larger stride in one or more layers
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用较大的步幅在一层或多层中减少维度
- en: Remove one or more layers
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除一层或多层
- en: Use 16-bit floats instead of 32-bit floats
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用16位浮点数代替32位浮点数
- en: Distribute the CNN across multiple devices
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将CNN分布到多个设备上
- en: Summary
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how to use and build real-life applications using
    CNNs, which are a type of feedforward artificial neural network in which the connectivity
    pattern between neurons is inspired by the organization of the animal visual cortex.
    Our image classifier application using CNN can classify real-life images with
    an acceptable level of accuracy, although we did not achieve higher accuracy.
    However, readers are encouraged to tune hyperparameters in the code and also try
    the same approach with another dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到如何使用和构建基于卷积神经网络（CNN）的现实应用，CNN是一种前馈人工神经网络，其神经元之间的连接模式受到动物视觉皮层组织的启发。我们使用CNN构建的图像分类应用可以以可接受的准确度对现实世界中的图像进行分类，尽管我们没有达到更高的准确度。然而，鼓励读者在代码中调整超参数，并尝试使用其他数据集采用相同的方法。
- en: Nevertheless, and importantly since the internal data representation of a convolutional
    neural network does not take into account important spatial hierarchies between
    simple and complex objects, CNN has some serious drawbacks and limitation for
    certain instances. Therefore, I would suggest you take a look at the recent activities
    around capsule networks on GitHub at [https://github.com/topics/capsule-network](https://github.com/topics/capsule-network).
    Hopefully, you can get something useful out from there
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是，由于卷积神经网络的内部数据表示没有考虑到简单和复杂物体之间的重要空间层级，因此CNN在某些实例中有一些严重的缺点和限制。因此，我建议你查看GitHub上关于胶囊网络的最新活动：[https://github.com/topics/capsule-network](https://github.com/topics/capsule-network)。希望你能从中获得一些有用的信息。
- en: This is, more-or-less, the end of our little journey in developing ML projects
    using Scala and different open source frameworks. Throughout the chapters, I tried
    to provide you with several examples of how to use these wonderful technologies
    efficiently for developing ML projects. During the writing of this book, I had
    to keep many constraints in my mind, for example, the page count, API availability,
    and my expertise. But I tried to make the book more-or-less simple, and I also
    tried to avoid details on the theory, as you can read about that in many books,
    blogs, and websites on Apache Spark, DL4j, and H2O itself.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是我们使用Scala和不同开源框架开发机器学习项目的小旅程的结束。在各章中，我尝试为你提供了多个示例，展示如何有效地使用这些出色的技术来开发机器学习项目。在写这本书的过程中，我必须考虑到许多限制条件，例如页面数量、API可用性和我的专业知识。但我尽量使书籍保持简洁，并且避免了过多的理论细节，因为关于Apache
    Spark、DL4j和H2O的理论内容，你可以在许多书籍、博客和网站上找到。
- en: 'I will also keep the code of this book updated on my GitHub repo at: [https://github.com/PacktPublishing/Scala-Machine-Learning-Projects](https://github.com/PacktPublishing/Scala-Machine-Learning-Projects).
    Feel free to open a new issue or any pull request for improving this book and
    stay tuned.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我还会在我的GitHub仓库上更新这本书的代码：[https://github.com/PacktPublishing/Scala-Machine-Learning-Projects](https://github.com/PacktPublishing/Scala-Machine-Learning-Projects)。随时欢迎提出新问题或提交任何拉取请求，以改善这本书，并保持关注。
- en: Finally, I did not write this book to earn money but a major portion of the
    royalties will be spent for the child education in the rural areas of my home
    district in Bangladesh. I would like to say thanks and express my sincere gratitude
    for buying and enjoying this book!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我写这本书并不是为了赚钱，但大部分版税将用于资助孟加拉国我家乡地区的儿童教育。我想感谢并对购买并享受这本书的读者表示衷心的感谢！
