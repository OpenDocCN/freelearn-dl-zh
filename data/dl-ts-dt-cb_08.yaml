- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Deep Learning for Time Series Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在时间序列分类中的应用
- en: In this chapter, we’ll tackle **time series classification** (**TSC**) problems
    using deep learning. As the name implies, TSC is a classification task involving
    time series data. The dataset contains several time series, and each of these
    has an associated categorical label. This problem is similar to a standard classification
    task, but the input explanatory variables are time series. We’ll explore how to
    approach this problem using different approaches. Besides using the **K-nearest
    neighbors** model to tackle this task, we’ll also develop different neural networks,
    such as a **residual neural network** (**ResNet**) and a convolutional neural
    network.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用深度学习解决 **时间序列分类** (**TSC**) 问题。顾名思义，TSC 是一个涉及时间序列数据的分类任务。数据集包含多个时间序列，每个序列都有一个相关的分类标签。这个问题类似于标准的分类任务，但输入的解释变量是时间序列。我们将探索如何使用不同的方法来处理这个问题。除了使用
    **K-最近邻** 模型来处理这个任务外，我们还将开发不同的神经网络，比如 **残差神经网络** (**ResNet**) 和卷积神经网络。
- en: By the end of this chapter, you’ll be able to set up a TSC task using a PyTorch
    Lightning data module and solve it with different models. You’ll also learn how
    to use the `sktime` Python library to solve this problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够使用 PyTorch Lightning 数据模块设置 TSC 任务，并用不同的模型解决它。你还将学习如何使用 `sktime`
    Python 库来解决这个问题。
- en: 'This chapter contains the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下食谱：
- en: Tackling TSC with K-nearest neighbors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 K-最近邻解决 TSC 问题
- en: Building a `DataModule` class for TSC
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 TSC 构建一个 `DataModule` 类
- en: Convolutional neural networks for TSC
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列分类的卷积神经网络
- en: ResNets for TSC
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ResNet 解决 TSC 问题
- en: Tackling TSC problems with `sktime`
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sktime`解决时间序列分类问题
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll focus on the PyTorch Lightning ecosystem to build deep learning models.
    Besides that, we’ll also use scikit-learn to create a baseline. Overall, the list
    of libraries used in the package is the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于 PyTorch Lightning 生态系统来构建深度学习模型。此外，我们还将使用 scikit-learn 来创建基准模型。总体而言，本包中使用的库列表如下：
- en: scikit-learn (1.3.2)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn (1.3.2)
- en: '`pandas` (2.1.3)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` (2.1.3)'
- en: NumPy (1.26.2)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy (1.26.2)
- en: Torch (2.1.1)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torch (2.1.1)
- en: PyTorch Lightning (2.1.2)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Lightning (2.1.2)
- en: '`sktime` (0.24.1)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sktime` (0.24.1)'
- en: '`keras-self-attention` (0.51.0)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras-self-attention` (0.51.0)'
- en: 'As an example, we’ll use the `Car` dataset from the repository available at
    the following link: [https://www.timeseriesclassification.com](https://www.timeseriesclassification.com).
    You can learn more about the dataset in the following work:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将使用以下链接中提供的`Car`数据集：[https://www.timeseriesclassification.com](https://www.timeseriesclassification.com)。你可以在以下文献中了解更多关于该数据集的信息：
- en: Thakoor, Ninad, and Jean Gao. *Shape classifier based on generalized probabilistic
    descent method with hidden Markov descriptor*. Tenth IEEE **International Conference
    on Computer Vision** (**ICCV**’05) Volume 1\. Vol. 1\. IEEE, 2005.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Thakoor, Ninad, 和 Jean Gao. *基于广义概率下降方法和隐马尔可夫描述符的形状分类器*。第十届 IEEE **计算机视觉国际会议**
    (**ICCV**’05) 第1卷。IEEE，2005年。
- en: 'The code and datasets used in this chapter can be found at the following GitHub
    URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码和数据集可以在以下 GitHub 链接中找到：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。
- en: Tackling TSC with K-nearest neighbors
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 K-最近邻解决 TSC 问题
- en: In this recipe, we’ll show you how to tackle TSC tasks using a popular method
    called K-nearest neighbors. The goal of this recipe is to show you how standard
    machine-learning models can be used to solve this problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将展示如何使用一种叫做 K-最近邻的流行方法来处理 TSC 任务。本食谱的目标是向你展示如何使用标准的机器学习模型来解决这个问题。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, let’s start by loading the data using `pandas`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 `pandas` 来加载数据：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset is already split into a training and testing set, so we read them
    separately. Now, let’s see how to build a K-nearest neighbor model using this
    dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经分为训练集和测试集，因此我们会分别读取它们。现在，让我们看看如何使用这个数据集构建一个 K-最近邻模型。
- en: How to do it…
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Here, we describe the steps necessary for building a time series classifier
    using scikit-learn:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们描述了使用 scikit-learn 构建时间序列分类器所需的步骤：
- en: 'Let’s start by splitting the target variable from the explanatory variables:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将目标变量从解释变量中分离出来：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first column of each dataset (index `0`) contains the target variable, which
    we assign to the `y_train` and `y_test` objects for the training and testing sets,
    respectively. The `X_train` and `X_test` objects contain the input explanatory
    time series for the corresponding datasets.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个数据集的第一列（索引 `0`）包含目标变量，我们将其分配给 `y_train` 和 `y_test` 对象，用于训练集和测试集。`X_train`
    和 `X_test` 对象包含相应数据集的输入解释时间序列。
- en: 'This particular dataset contains four different classes. Here’s what the distribution
    looks like:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个特定的数据集包含四个不同的类别。以下是它们的分布情况：
- en: '![Figure 8.1: Distribution of the four classes in the dataset](img/B21145_08_001.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1: 数据集中四个类别的分布](img/B21145_08_001.jpg)'
- en: 'Figure 8.1: Distribution of the four classes in the dataset'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.1: 数据集中四个类别的分布'
- en: 'Afterward, we need to normalize the time series. We accomplish this using the
    `MinMaxScaler` method from scikit-learn, which brings all values into a range
    between `0` and `1`:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要对时间序列进行标准化。我们使用 scikit-learn 中的 `MinMaxScaler` 方法来实现这一点，它将所有值压缩到 `0`
    和 `1` 之间：
- en: '[PRE2]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we fit the scaler using the training set, and then use
    it to transform the data in both datasets.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用训练集拟合标准化器，然后用它来转换两个数据集中的数据。
- en: 'Finally, we’re ready to create a K-nearest neighbors classification model:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们准备创建一个 K-最近邻分类模型：
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, we create a `KNeighborsTimeSeriesClassifier` instance
    that implements K-nearest neighbors and fits it using the training set. Then,
    we apply this model to the testing set by calling the `predict``()` method.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了一个 `KNeighborsTimeSeriesClassifier` 实例，它实现了 K-最近邻算法，并使用训练集对其进行拟合。然后，我们通过调用
    `predict()` 方法将此模型应用于测试集。
- en: 'The following figure shows the confusion matrix concerning the predictions
    of the K-nearest neighbor model:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图显示了 K-最近邻模型预测的混淆矩阵：
- en: '![Figure 8.2: Confusion matrix for the predictions of the K-nearest neighbor
    model](img/B21145_08_002.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2: K-最近邻模型预测的混淆矩阵](img/B21145_08_002.jpg)'
- en: 'Figure 8.2: Confusion matrix for the predictions of the K-nearest neighbor
    model'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.2: K-最近邻模型预测的混淆矩阵'
- en: How it works…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: TSC problems are like standard classification tasks where the input explanatory
    variables are time series. So, the process for tackling the problem is similar.
    After splitting the explanatory variables (`X`) from the target variable (`y`),
    we prepare the explanatory variables using operators such as normalization functions.
    Then, we can use any classifier to solve this task. In this recipe, we use the
    K-nearest neighbor model, which is well-known for being a simple yet effective
    approach for this task.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分类（TSC）问题类似于标准分类任务，其中输入的解释变量是时间序列。因此，解决该问题的过程类似。在将解释变量（`X`）与目标变量（`y`）分开后，我们使用标准化函数等操作符对解释变量进行预处理。然后，我们可以使用任何分类器来解决这个任务。在本节中，我们使用了K-最近邻模型，这是一种简单而有效的方法。
- en: Note that the normalization step using `MinMaxScaler` is important to bring
    all observations into a common value range.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用 `MinMaxScaler` 进行标准化步骤是非常重要的，能够将所有观测值映射到一个共同的值范围内。
- en: There’s more…
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'The `sktime` Python library provides several methods for tackling TSC problems.
    Here’s the link to the documentation: [https://www.sktime.net/en/stable/examples/02_classification.html](https://www.sktime.net/en/stable/examples/02_classification.html).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`sktime` Python 库提供了多种解决 TSC 问题的方法。这里是文档的链接：[https://www.sktime.net/en/stable/examples/02_classification.html](https://www.sktime.net/en/stable/examples/02_classification.html)。'
- en: In this recipe, we used the K-nearest neighbors model using default parameters.
    For example, we used the Minkowski metric, which may not be the best one. For
    time series, distance metrics such as dynamic time warping are usually better
    approaches.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了默认参数的 K-最近邻模型。例如，我们使用了明科夫斯基度量（Minkowski metric），但这可能不是最好的选择。对于时间序列，像动态时间规整（dynamic
    time warping）这样的距离度量通常是更好的选择。
- en: Building a DataModule class for TSC
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 TSC 构建 DataModule 类
- en: In this recipe, we return to the PyTorch Lightning framework. We’ll build a
    `DataModule` class to encapsulate the data preprocessing and the passing of observations
    to models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回到 PyTorch Lightning 框架。我们将构建一个 `DataModule` 类来封装数据预处理和将观测值传递给模型的过程。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Let’s load the dataset from the previous recipe:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从前面的食谱中加载数据集：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we’ll build a `DataModule` class to handle this dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个 `DataModule` 类来处理这个数据集。
- en: How to do it…
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'In the previous chapters, we used `TimeSeriesDataSet` from PyTorch Forecasting
    to handle the data preparation for us. This class managed several steps. These
    include normalization and transformation of the data for supervised learning.
    However, in TSC, an observation uses the entire time series as input:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用了PyTorch Forecasting中的`TimeSeriesDataSet`来处理数据准备工作。这个类管理了多个步骤，包括数据的归一化和转化，供监督学习使用。然而，在TSC中，一个观察值使用整个时间序列作为输入：
- en: 'We’ll start creating a simpler variant of `TimeSeriesDataSet` to handle the
    passing of observations to the model:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将开始创建一个更简化的`TimeSeriesDataSet`变体，以便将观察值传递给模型：
- en: '[PRE5]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `__getitem__``()` method is used internally to get observations from the
    dataset and pass them to the model, while the `__len__``()` method outputs the
    size of the dataset.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`__getitem__``()`方法在内部用于从数据集中获取观察值并将其传递给模型，而`__len__``()`方法则输出数据集的大小。'
- en: 'Then, we’re ready to build our `LightningDataModule` class. Here’s the constructor:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们准备好构建`LightningDataModule`类了。这里是构造函数：
- en: '[PRE6]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`TSCDataModule` contains the `self.train`, `self.validation`, and `self.test`
    dataset attributes that will be filled with the `setup``()` method. Besides that,
    the constructor also sets up the normalization method based on `MinMaxScaler`
    and the one-hot encoder called `OneHotEncoder`. We use the one-hot encoder to
    transform the target variable into a set of binary variables. This process is
    necessary for the training of neural networks.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`TSCDataModule`包含了`self.train`、`self.validation`和`self.test`数据集属性，这些属性将在`setup``()`方法中填充。此外，构造函数还会基于`MinMaxScaler`设置归一化方法，并使用名为`OneHotEncoder`的独热编码器。我们使用独热编码器将目标变量转换为一组二元变量。这一过程是训练神经网络所必需的。'
- en: 'Then, the `setup``()` method is implemented as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`setup``()`方法的实现如下：
- en: '[PRE7]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, we use the encoder to transform the target variable and
    do the same using the normalization method with the explanatory variables. Then,
    we create a validation set based on the training instances. We also cast the data
    objects as torch data structures using the `torch.tensor()` method. Finally, we
    create the `TSCDataset` instances based on the training, validation, and testing
    sets.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用编码器转换目标变量，并使用归一化方法对解释变量进行相同的转换。然后，我们基于训练实例创建验证集。我们还使用`torch.tensor()`方法将数据对象转换为torch数据结构。最后，我们基于训练、验证和测试集创建`TSCDataset`实例。
- en: 'We create the data loader methods using the `DataLoader` class directly on
    the respective dataset:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们直接在相应数据集上使用`DataLoader`类创建数据加载方法：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, here’s an example of how to get an observation using this data module:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这里是如何使用这个数据模块获取一个观察值的示例：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the next recipe, we’ll learn how to build a classification model using this
    data module.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个教程中，我们将学习如何使用这个数据模块构建一个分类模型。
- en: How it works…
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We created a `DataModule` class tailored for TSC classification problems. We
    encapsulated the data logic within the `setup``()` method, thus enabling us to
    use the PyTorch Lightning ecosystem to build deep learning models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个针对TSC分类问题定制的`DataModule`类。我们将数据逻辑封装在`setup``()`方法中，从而使我们能够使用PyTorch Lightning生态系统来构建深度学习模型。
- en: In this case, TSC problems do not involve autoregression. So, we created a simple
    variant of the `TimeSeriesDataSet` class to handle the process of passing the
    data to models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，TSC问题不涉及自回归。因此，我们创建了一个简单变体的`TimeSeriesDataSet`类，用于处理将数据传递给模型的过程。
- en: Convolutional neural networks for TSC
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于TSC的卷积神经网络
- en: In this recipe, we’ll walk you through building a convolutional neural network
    to tackle TSC problems. We’ll use the `DataModule` class created in the previous
    recipe to do this.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将带你构建一个卷积神经网络来解决TSC问题。我们将使用前面教程中创建的`DataModule`类来实现这一目标。
- en: Getting ready
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We start again by importing the dataset used in the previous recipe:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先再次导入前面教程中使用的数据集：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We also create an instance of the `TSCDataModule` data module we defined in
    the previous recipe. Let’s see how to create a convolutional neural network classifier
    to handle this task.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个`TSCDataModule`数据模块的实例，它是在前面的教程中定义的。让我们看看如何创建一个卷积神经网络分类器来处理这个任务。
- en: How to do it…
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Here, we will walk you through the steps of building a convolutional neural
    network for TSC problems using PyTorch:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将通过以下步骤为TSC问题构建一个卷积神经网络，使用PyTorch实现：
- en: 'Let’s start by creating the neural network based on PyTorch:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们先从基于PyTorch创建神经网络开始：
- en: '[PRE11]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then wrap this model within a `LightningModule` class from PyTorch Lightning
    called `TSCCnnModel`:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将该模型封装在一个来自 PyTorch Lightning 的 `LightningModule` 类 `TSCCnnModel` 中：
- en: '[PRE12]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This module also contains the usual training, validation, and testing steps.
    These are implemented as follows:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模块还包含了常见的训练、验证和测试步骤，具体实现如下：
- en: '[PRE13]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These steps are similar to those developed for forecasting problems, but in
    this case, we use cross entropy as the `loss``()` function.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些步骤与为预测问题开发的步骤类似，但在这种情况下，我们使用交叉熵作为 `loss()` 函数。
- en: 'We are now ready to train the model, which we do with a `Trainer` instance
    as follows:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练模型，接下来我们通过 `Trainer` 实例进行训练，如下所示：
- en: '[PRE14]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we set the output dimension to `4` when creating an instance
    of the `TSCCnnModel` class, which represents the number of classes in the dataset.
    We also set up an early stopping callback to drive the training process of the
    network.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们在创建 `TSCCnnModel` 类的实例时，将输出维度设置为 `4`，这代表了数据集中的类别数。我们还设置了一个提前停止回调，用以推动网络的训练过程。
- en: How it works…
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Convolutional neural networks have been successfully applied to TSC problems.
    In this recipe, we explored developing a classifier based on PyTorch Lightning’s
    `LightningModule`. We create a `ConvolutionalTSC` class that extends the `nn.Module`
    class. In the constructor of this class, we define the layers of the network:
    three convolutional layers (`conv1`, `conv2`, and `conv3`), and two densely connected
    layers (`fc1` and `fc2`). The `forward()` method details how these layers are
    composed together. Then, the convolutional layers stack on top of each other,
    and a max pooling operation (`MaxPool1d`) is applied after each convolution. Finally,
    we stack two densely connected layers, where the last one is the output layer.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络已成功应用于时间序列分类（TSC）问题。在本示例中，我们探索了基于 PyTorch Lightning 的 `LightningModule`
    开发分类器。我们创建了一个扩展自 `nn.Module` 类的 `ConvolutionalTSC` 类。在该类的构造函数中，我们定义了网络的层次：三个卷积层（`conv1`、`conv2`
    和 `conv3`），以及两个全连接层（`fc1` 和 `fc2`）。`forward()` 方法详细说明了这些层是如何组合在一起的。然后，卷积层依次堆叠，并在每个卷积后应用最大池化操作（`MaxPool1d`）。最后，我们堆叠了两个全连接层，其中最后一层是输出层。
- en: 'The following figure shows the confusion matrix for the convolution neural
    network:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了卷积神经网络的混淆矩阵：
- en: '![Figure 8.3: Confusion matrix for the convolutional neural network](img/B21145_08_003.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3：卷积神经网络的混淆矩阵](img/B21145_08_003.jpg)'
- en: 'Figure 8.3: Confusion matrix for the convolutional neural network'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：卷积神经网络的混淆矩阵
- en: The results obtained with the neural network are better than those when using
    the K-nearest neighbor model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络得到的结果优于使用 K 最近邻（K-NN）模型的结果。
- en: The workflow of this model follows the same logic as other recipes based on
    PyTorch Lightning. The main difference to take into account is that we’re dealing
    with a classification problem. So, we need to set a loss function that deals with
    this problem. Cross-entropy is the usual go-to function to train neural networks
    for classification tasks. The output dimension of the neural network is also set
    according to the number of classes. Essentially, there’s an output unit for each
    class in the dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的工作流与基于 PyTorch Lightning 的其他示例遵循相同的逻辑。需要注意的主要区别是我们正在处理分类问题。因此，我们需要设置一个处理该问题的损失函数。交叉熵是训练神经网络用于分类任务时通常使用的函数。神经网络的输出维度也根据类别的数量进行设置。基本上，每个数据集中的类别都会有一个输出单元。
- en: ResNets for TSC
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 TSC 的 ResNet
- en: This recipe shows you how to train a ResNet for TSC tasks. ResNets are a type
    of deep neural network architecture widely used in computer vision problems, such
    as image classification or object detection. Here, you’ll learn how to use them
    for modeling time series data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例演示了如何训练一个用于 TSC 任务的 ResNet。ResNet 是一种广泛用于计算机视觉问题（如图像分类或目标检测）的深度神经网络架构。在这里，你将学习如何将其应用于时间序列数据建模。
- en: Getting ready
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We’ll continue with the same dataset and data module as in the previous recipe:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用与前一个示例中相同的数据集和数据模块：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s see how to build a ResNet and train it with PyTorch Lightning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 PyTorch Lightning 构建并训练 ResNet。
- en: How to do it…
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'In this section, we describe the process of creating a ResNet for TSC tasks:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们描述了为 TSC 任务创建 ResNet 的过程：
- en: 'Let’s start by creating a ResNet using `nn.Module` from the `torch` library:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用 `torch` 库中的 `nn.Module` 创建一个 ResNet：
- en: '[PRE16]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we wrap `ResidualNeuralNetworkModel` with `pl.LightningModule`:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们将 `ResidualNeuralNetworkModel` 封装在 `pl.LightningModule` 中：
- en: '[PRE17]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The training, validation, and testing steps are implemented identically to
    in the previous recipe:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练、验证和测试步骤的实现与前面食谱中的实现完全相同：
- en: '[PRE18]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we transform the predictions and the actual values into a `torch.FloatTensor`
    structure for computing the `loss``()` function. We set cross-entropy as the `loss``()`
    function, which is typically used in classification. In the testing stage, we
    also evaluate the accuracy of the model.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们将预测值和实际值转换为 `torch.FloatTensor` 结构，以计算 `loss` `()` 函数。我们将交叉熵设置为 `loss`
    `()` 函数，这是分类问题中常用的函数。在测试阶段，我们还评估模型的准确性。
- en: 'Finally, here’s the workflow for training and testing the model based on PyTorch
    Lightning’s `Trainer`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这是基于 PyTorch Lightning 的 `Trainer` 进行模型训练和测试的工作流程：
- en: '[PRE19]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Essentially, after defining the model, the training and testing process with
    PyTorch Lightning’s `Trainer` is similar to that shown in the previous recipe.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本质上，在定义模型之后，使用 PyTorch Lightning 的 `Trainer` 进行训练和测试的过程与前面食谱中展示的类似。
- en: How it works…
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'ResNets have shown promising performance in TSC problems. The idea is to learn
    the difference (residual) between the original input and a transformed output
    obtained from a convolutional layer. In the preceding code, we create a neural
    network that takes as input three parameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 在 TSC 问题中表现出了良好的效果。其思想是学习原始输入和通过卷积层得到的转换输出之间的差异（残差）。在前面的代码中，我们创建了一个神经网络，该网络以三个参数作为输入：
- en: '`in_channels`: The number of input channels, which is equal to 1 because our
    time series are univariate'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`：输入通道的数量，由于我们的时间序列是单变量的，因此等于1。'
- en: '`out_channels`: The number of output channels from each residual block'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels`：每个残差块的输出通道数量'
- en: '`num_classes`: The number of classes in the dataset'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_classes`：数据集中类别的数量'
- en: 'The layers of the neural network are composed of three residual blocks named
    `ResNNBlock`. Residual blocks are the cornerstone of ResNets and are designed
    to solve the vanishing gradient problem. You can check the implementation of the
    residual blocks named `ResNNBlock` at the following URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).
    The output from the residual blocks is passed on to a linear layer.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的层由三个名为 `ResNNBlock` 的残差块组成。残差块是 ResNet 的基石，旨在解决梯度消失问题。你可以在以下网址查看名为 `ResNNBlock`
    的残差块实现：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。残差块的输出传递给一个线性层。
- en: 'The following figure shows the confusion matrix for the ResNet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 ResNet 的混淆矩阵：
- en: '![Figure 8.4: Confusion matrix for the ResNet](img/B21145_08_004.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4：ResNet 的混淆矩阵](img/B21145_08_004.jpg)'
- en: 'Figure 8.4: Confusion matrix for the ResNet'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：ResNet 的混淆矩阵
- en: Like in the previous recipe, we wrap the implementation of the ResNet with the
    PyTorch Lightning framework. In the testing stage of this recipe, we also include
    the accuracy metric, which tells us the percentage of cases the model gets right.
    This metric is commonly used in TSC problems, though it might not be very informative
    for datasets with an imbalanced target distribution.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个食谱类似，我们将 ResNet 的实现与 PyTorch Lightning 框架结合。在此食谱的测试阶段，我们还包含了准确率度量，告诉我们模型正确预测的案例百分比。这个度量通常用于
    TSC 问题，尽管对于目标分布不平衡的数据集，它可能不是很有意义。
- en: Tackling TSC problems with sktime
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sktime 解决 TSC 问题
- en: In this recipe, we explore an alternative approach to PyTorch for TSC problems,
    which is `sktime`. `sktime` is a Python library devoted to time series modeling,
    which includes several neural network models for TSC.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们探索了 PyTorch 的替代方法——`sktime`。`sktime` 是一个专门用于时间序列建模的 Python 库，其中包括几个用于
    TSC 的神经网络模型。
- en: Getting ready
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You can install `sktime` using `pip`. You’ll also need the `keras-self-attention`
    library, which includes self-attention methods necessary for running some of the
    methods in `sktime`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `pip` 安装 `sktime`。你还需要安装 `keras-self-attention` 库，其中包含运行 `sktime` 中某些方法所需的自注意力方法：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The trailing `dl` tag in squared brackets when installing `sktime` means you
    want to include the optional deep learning models available in the library.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 `sktime` 时，方括号中的 `dl` 标签表示你希望包括该库中提供的可选深度学习模型。
- en: In this recipe, we’ll use an example dataset available in `sktime`. We’ll load
    it in the next section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用 `sktime` 中提供的一个示例数据集。我们将在下一节加载它。
- en: How to do it…
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: As the name implies, the `sktime` library follows a design pattern similar to
    scikit-learn. So, our approach to building a deep learning model using `sktime`
    will be similar to the workflow described in the *Tackling TSC with K-nearest*
    *neighbors* recipe.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，`sktime`库遵循类似于scikit-learn的设计模式。因此，我们使用`sktime`构建深度学习模型的方法将类似于*使用K近邻解决TSC问题*方案中描述的工作流程。
- en: 'Let’s start by loading the dataset:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding code, we load a dataset concerning energy demand in Italy.
    You can check the following link for more information about this dataset: [https://www.timeseriesclassification.com/description.php?Dataset=ItalyPowerDemand](https://www.timeseriesclassification.com/description.php?Dataset=ItalyPowerDemand).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们加载了关于意大利能源需求的数据集。你可以查看以下链接以获取更多关于此数据集的信息：[https://www.timeseriesclassification.com/description.php?Dataset=ItalyPowerDemand](https://www.timeseriesclassification.com/description.php?Dataset=ItalyPowerDemand)。
- en: 'The data was originally used in the following work:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据最初用于以下工作：
- en: 'Keogh, Eamonn, et al. *Intelligent icons: Integrating lite-weight data mining
    and visualization into GUI operating systems*. Sixth **International Conference
    on Data** Mining (**ICDM**’06). IEEE, 2006.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Keogh, Eamonn等人。*智能图标：将轻量级数据挖掘与可视化集成到GUI操作系统中*。第六届**国际数据挖掘会议**（**ICDM**'06）。IEEE，2006年。
- en: We use `load_italy_power_demand` to load the train and test sets as `numpy`
    data structures.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`load_italy_power_demand`来加载训练集和测试集，作为`numpy`数据结构。
- en: Now, let’s see how to build different types of neural networks using this dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用这个数据集构建不同类型的神经网络。
- en: Fully connected neural network
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全连接神经网络
- en: 'We start by training a fully connected neural network. The configuration of
    this network, including `loss``()` and `activation``()` functions, is passed as
    arguments to the `FCNClassifier` instance:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练一个全连接神经网络开始。该网络的配置，包括`loss``()`和`activation``()`函数，作为参数传递给`FCNClassifier`实例：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The training and inference steps are done using the `fit``()` and `predict``()`
    methods, respectively. If you’re used to scikit-learn methods, this approach should
    be familiar to you.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和推断步骤分别使用`fit``()`和`predict``()`方法完成。如果你熟悉scikit-learn的方法，这种方式应该对你来说很熟悉。
- en: Convolutional neural network
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'As we learned in the *Convolutional neural networks for TSC* recipe, convolutional
    models can be an effective approach to classifying time series. Here’s the implementation
    available on `sktime` with `CNNClassifier`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*卷积神经网络用于TSC*方案中学到的，卷积模型可以是分类时间序列的有效方法。这里是`sktime`中使用`CNNClassifier`的实现：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can also set other parameters concerning convolutions, such as `avg_pool_size`
    or `n_conv_layers`. Check the documentation for a complete list of parameters
    at the following link: [https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以设置与卷积相关的其他参数，如`avg_pool_size`或`n_conv_layers`。查看文档以获取完整的参数列表，链接如下：[https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html)。
- en: LSTM-FCN neural network
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM-FCN神经网络
- en: 'Recurrent neural networks can also be useful for this problem. Here’s a combination
    of an LSTM with a fully connected layer that is available in `LSTMFCNClassifier`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络对这个问题也很有用。这里是一个将LSTM与全连接层结合的模型，能够在`LSTMFCNClassifier`中使用：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This method also includes an attention mechanism that improves classification
    accuracy significantly.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法还包含一个注意力机制，显著提高了分类准确性。
- en: TapNet model
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TapNet模型
- en: 'The `sktime`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`sktime`：'
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This model can manage a low-dimensional space (small number of features), and
    work well under a semi-supervised setting – that is, when there’s a large number
    of unlabeled observations available.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以处理低维空间（特征数较少），并且在半监督设置下表现良好——即，当有大量未标记的观察数据时。
- en: InceptionTime model
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: InceptionTime模型
- en: '`InceptionTime` is a state-of-the-art deep learning method for TSC problems.
    In practice, `InceptionTime` is an ensemble of deep convolutional neural networks
    that is inspired by the inception architecture created for computer vision tasks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`InceptionTime`是解决TSC问题的最先进深度学习方法。实际上，`InceptionTime`是一个深度卷积神经网络的集成，灵感来自于为计算机视觉任务创建的Inception架构：'
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The model also includes optional residual connections, which, in the preceding
    code, we use by setting the `use_residual` parameter to `True`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还包括可选的残差连接，在前面的代码中，我们通过将 `use_residual` 参数设置为 `True` 来使用它。
- en: Evaluation
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: 'We can use standard classification metrics to evaluate the performance of TSC
    models. Here’s how to compute the accuracy of the models we trained in this recipe:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准的分类指标来评估 TSC 模型的性能。以下是如何计算我们在这个食谱中训练的模型的准确性：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The results are shown in the following figure:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下面的图所示：
- en: '![Figure 8.5: Accuracy of the models](img/B21145_08_005.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5：模型的准确性](img/B21145_08_005.jpg)'
- en: 'Figure 8.5: Accuracy of the models'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：模型的准确性
- en: Overall, `InceptionTime` appears to be the best approach for this particular
    problem.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，`InceptionTime` 似乎是解决这个特定问题的最佳方法。
- en: How it works…
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理……
- en: In this recipe, we use the `sktime` Python library to build deep-learning models
    for TSC. While you can use PyTorch as we’ve shown in the other recipes of this
    chapter, `sktime` provides an extensive toolkit for tackling TSC tasks. Since
    `sktime` follows the philosophy of scikit-learn, most of the work is done using
    the `fit``()` and `predict``()` methods of the respective class.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用 `sktime` Python 库来构建深度学习模型以解决 TSC 问题。虽然你可以像我们在本章的其他食谱中展示的那样使用 PyTorch，`sktime`
    提供了一个广泛的工具包来处理 TSC 任务。由于 `sktime` 遵循 scikit-learn 的理念，大部分工作是通过相应类的 `fit` 和 `predict`
    方法完成的。
- en: There’s more…
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'You can check the documentation of `sktime` for other models, including some
    that are not based on deep learning. Here’s the link: [https://www.sktime.net/en/stable/users.html](https://www.sktime.net/en/stable/users.html).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看 `sktime` 的文档，了解其他模型，包括一些非深度学习的模型。这里是链接：[https://www.sktime.net/en/stable/users.html](https://www.sktime.net/en/stable/users.html)。
- en: In most of the models we used in this recipe, we set the parameters to their
    default values. But, you can create a validation set and optimize the configuration
    of the models for better performance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用的这个食谱中的大多数模型中，我们将参数设置为默认值。但是，你可以创建一个验证集并优化模型的配置，以获得更好的性能。
