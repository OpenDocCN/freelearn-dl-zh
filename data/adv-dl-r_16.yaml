- en: Text Classification Using Convolutional Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积递归神经网络进行文本分类
- en: '**Convolutional neural networks** (**CNNs**) have been found to be useful in
    capturing high-level local features from data. On the other hand, **recurrent
    neural networks** (**RNNs**), such as **long short-term memory** (**LSTM**), have
    been found to be useful in capturing long-term dependencies in data involving
    sequences such as text. When we use CNNs and RNNs in the same model architecture,
    it gives rise to what''s called **convolutional recurrent neural networks** (**CRNNs**).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）被发现能够有效捕捉数据中的高层次局部特征。另一方面，**递归神经网络**（**RNNs**），例如**长短期记忆**（**LSTM**），则在捕捉涉及文本等序列数据的长期依赖性方面表现出色。当我们在同一个模型架构中同时使用CNN和RNN时，就会产生所谓的**卷积递归神经网络**（**CRNNs**）。'
- en: This chapter illustrates how to apply convolutional recurrent neural networks
    to text classification problems by combining the advantages of RNNs and CNNs networks.
    The steps that are involved in this process include text data preparation, defining
    a convolutional recurrent network model, training the model, and model assessment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章说明了如何通过结合RNN和CNN网络的优点，将卷积递归神经网络应用于文本分类问题。这个过程包括文本数据准备、定义卷积递归网络模型、训练模型以及模型评估等步骤。
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，本章将涵盖以下内容：
- en: Working with the reuter_50_50 dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 reuter_50_50 数据集
- en: Preparing the data for model building
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备模型构建的数据
- en: Developing the model architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发模型架构
- en: Compiling and fitting the model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译和拟合模型
- en: Evaluating the model and predicting classes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型并预测类别
- en: Performance optimization tips and best practices
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化建议和最佳实践
- en: Working with the reuter_50_50 dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 reuter_50_50 数据集
- en: In the previous chapters, when dealing with text data, we made use of data that
    had already been converted into a sequence of integers for developing deep network
    models. In this chapter, we will use text data that needs to be converted into
    a sequence of integers. We will start by reading the data that we will use to
    illustrate how to develop a text classification deep network model. We will also
    explore the dataset that we'll use so that we have a better understanding of it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，当处理文本数据时，我们使用了已经转换为整数序列的数据来开发深度网络模型。本章中，我们将使用需要转换为整数序列的文本数据。我们将从读取将用于展示如何开发文本分类深度网络模型的数据开始。我们还将探索我们将使用的数据集，以便更好地理解它。
- en: 'In this chapter, we will make use of the `keras`, `deepviz`, and `readtext`
    libraries, as shown in the following code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用`keras`、`deepviz`和`readtext`库，如下所示的代码：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For illustrating the steps involved in developing a convolutional recurrent
    network model, we will make use of the `reuter_50_50` text dataset, which is available
    from the UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明开发卷积递归网络模型的步骤，我们将使用来自UCI机器学习库的`reuter_50_50`文本数据集：[https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#)。
- en: 'This dataset contains text files in two folders, with one folder for the training
    data and another for the test data:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含两个文件夹中的文本文件，一个文件夹用于训练数据，另一个用于测试数据：
- en: The folder containing the training data has 2,500 text files with 50 articles
    each from 50 authors.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存放训练数据的文件夹有2,500个文本文件，每个文件包含50篇来自50位作者的文章。
- en: Similarly, the folder containing the test data also has 2,500 text files with
    50 articles each from the same 50 authors.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，存放测试数据的文件夹也包含2,500个文本文件，每个文件包含50篇来自同样50位作者的文章。
- en: Reading the training data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取训练数据
- en: 'We can access the `reuter_50_50` dataset by going to `Data` folder from the
    link that we provided for the UCI Machine Learning Repository. From here, we can
    download the `C50.zip` folder. When unzipped, it contains a `C50` folder containing
    `C50train` and `C50test` folders. First, we will read the text files from the
    `C50train` folder using the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过访问我们提供的UCI机器学习库链接中的`Data`文件夹来访问`reuter_50_50`数据集。在这里，我们可以下载`C50.zip`文件夹。解压后，它包含一个名为`C50`的文件夹，其中包含`C50train`和`C50test`文件夹。首先，我们将使用以下代码读取`C50train`文件夹中的文本文件：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With the help of the preceding code, we can read data on 2,500 articles from
    the `C50train` folder into `trainx` and also save information about the author's
    names into `trainy`. We start by setting the working directory to the `C50train`
    folder using the `setwd` function. The `C50train` folder contains 50 folders named
    after 50 authors, and each folder contains 50 articles written by the corresponding
    author. We assign a value of 1 to k and initiate `tr`, `trainx`, and `trainy`
    as a list. Then, we create a loop so that the author's name is stored in `trainy`,
    which contains the author's names for each article, and so that `trainx` contains
    the corresponding articles written by the authors. Note that, after reading data
    on these 2,500 text files, `trainx` also contains information about file names.
    Using the last line of code, we retain data on only 2,500 texts and remove information
    about the file names that we will not need.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码帮助下，我们可以将来自`C50train`文件夹的2,500篇文章数据读取到`trainx`中，并将作者的名字信息保存到`trainy`中。我们首先通过`setwd`函数将工作目录设置为`C50train`文件夹。`C50train`文件夹包含50个以作者名字命名的文件夹，每个文件夹里有50篇该作者撰写的文章。我们将k的值设为1，并将`tr`、`trainx`和`trainy`初始化为列表。然后，我们创建一个循环，使得每篇文章的作者名字存储在`trainy`中，而`trainx`中存储相应的文章。需要注意的是，在读取这2,500个文本文件之后，`trainx`也包含了文件名信息。通过最后一行代码，我们只保留2,500篇文本的数据，并移除不需要的文件名信息。
- en: 'Now, let''s look at the content of text file 901 from the train data using
    the following code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过以下代码查看`train`数据中的文本文件901的内容：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the preceding code and output, we can make the following observations:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码和输出中，我们可以做出以下观察：
- en: The test file 901 in `trainx` contains certain news items about drug trials
    by the Chiroscience Group
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainx`中的测试文件901包含关于Chiroscience集团的药物试验的某些新闻项目'
- en: The author of this short article is Jonathan Birt
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇短文的作者是Jonathan Birt
- en: Having read the text files and author names for the training data, we can repeat
    this process for the test data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取了训练数据的文本文件和作者名字之后，我们可以对测试数据重复这一过程。
- en: Reading the test data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取测试数据
- en: 'Now, we will read the text files from the `C50test` folder located within the
    `C50` folder. We will use the following code to do so:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将读取位于`C50`文件夹内的`C50test`文件夹中的文本文件。我们将使用以下代码来实现：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we can see that the only change in this code is that we are creating `testx`
    and `testy` based on the test data located within the `C50test` folder. We read
    2,500 articles from the `C50test` folder into `testx` and save information about
    the author's names into `testy`. Once again, we use the last line of code to retain
    data on only 2,500 texts from the test data and remove information on file names,
    which isn't required for our analysis.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到这段代码中唯一的变化是，我们根据位于`C50test`文件夹中的测试数据创建了`testx`和`testy`。我们从`C50test`文件夹中读取了2,500篇文章到`testx`，并将作者的名字信息保存到`testy`中。再次使用最后一行代码，我们只保留了2,500篇来自测试数据的文本，并删除了不需要的文件名信息，这些信息对于我们的分析没有用处。
- en: Now that we've created the training and test data, we will carry out data preprocessing
    so that we can develop an author classification model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了训练数据和测试数据，我们将进行数据预处理，以便能够开发一个作者分类模型。
- en: Preparing the data for model building
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: In this section, we will prepare some data so that we can develop an author
    classification model. We will start by using tokens to convert text data that
    is available in the form of articles into a sequence of integers. We will also
    make changes to identify each author by unique integers. Subsequently, we will
    use padding and truncation to arrive at the same length for the sequence of integers
    that represent the articles by 50 authors. We will end this section by partitioning
    the training data into train and validation datasets and then carrying out one-hot
    encoding on the response variables.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将准备一些数据，以便能够开发一个作者分类模型。我们将从使用标记将以文章形式存在的文本数据转换为整数序列开始。我们还将进行更改，以通过唯一的整数来识别每个作者。随后，我们将使用填充和截断方法，使表示50个作者文章的整数序列达到相同的长度。最后，我们将通过将训练数据划分为训练集和验证集，并对响应变量进行独热编码，结束这一部分。
- en: Tokenization and converting text into a sequence of integers
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化和将文本转换为整数序列
- en: 'We will start by carrying out tokenization and then converting the articles,
    which are in text form, into a sequence of integers. To do this, we can use the
    following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从进行标记化开始，然后将文本形式的文章转换为整数序列。为此，我们可以使用以下代码：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the preceding code and output, we can observe the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码和输出中，我们可以观察到以下内容：
- en: For tokenization, we specify `num_words` as 500, indicating that we will use
    the 500 most frequent words from the text in the training data.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分词，我们将`num_words`指定为500，表示我们将使用训练数据中最频繁的500个单词。
- en: Note that using `fit_text_tokenizer` automatically converts text into lowercase
    and removes any punctuation that can be observed in the articles containing text
    data. Converting text into lowercase helps us avoid duplicates of words, where
    one may contain lowercase alphabetical characters and another may have uppercase
    alphabetical characters. Punctuation is removed since it doesn't add value when
    developing the author classification model with text as input.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，使用`fit_text_tokenizer`会自动将文本转换为小写，并去除任何可以在包含文本数据的文章中看到的标点符号。将文本转换为小写有助于避免单词重复的问题，其中一个可能包含小写字母字符，而另一个则可能包含大写字母字符。标点符号被去除，因为在使用文本作为输入开发作者分类模型时，它不会提供额外的价值。
- en: We use `texts_to_sequences` to convert the most frequent words in the text into
    a sequence of integers. The reason for doing this is to convert the unstructured
    data so that it has a structured format, which is required by deep learning models.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`texts_to_sequences`将文本中最频繁出现的单词转换为整数序列。这样做的原因是将非结构化数据转换为结构化格式，这是深度学习模型所要求的。
- en: The output of text file 7 shows a total of 314 integers that are between 1 and
    497.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本文件7的输出显示，共有314个整数，范围在1到497之间。
- en: Looking at the output for text file 901 (the same example in the training data
    that we reviewed earlier), we can see that it consists of 48 integers between
    1 and 470\. The original text consists of over 80 words and those words that do
    not belong to the 500 most frequent words are not represented in this sequence
    of integers.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看一下文本文件901的输出（这是我们之前回顾过的训练数据中的相同示例），可以看到它包含48个介于1到470之间的整数。原始文本包含超过80个单词，而那些不属于最频繁的500个单词的词汇没有出现在这个整数序列中。
- en: The first five integers, that is, 74, 356, 7, 9, and 199, correspond to the
    words `group`, `plc`, `said`, `on`, and `monday`, respectively. Other words at
    the beginning of the text that haven't been converted into integers do not belong
    to the top 500 most frequent words in the articles.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前五个整数，即74、356、7、9和199，分别对应于单词`group`、`plc`、`said`、`on`和`monday`。文本开头的其他单词没有被转换为整数，因为它们不属于文章中最频繁的500个单词。
- en: 'Now, let''s look at the number of integers per article in the training and
    test data. We can do this with the following code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下训练数据和测试数据中每篇文章包含的整数数量。我们可以使用以下代码来实现：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'From the preceding summary, we can make the following observations:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的总结中，我们可以得出以下观察结果：
- en: The number of integers per article in the training data ranges from 31 to 918,
    with a median of about 326 words.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据中每篇文章包含的整数数量范围从31到918，中位数约为326个单词。
- en: Similarly, the integers per article range from 39 to 1001 for the test data,
    with a median of about 331.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，测试数据中每篇文章包含的整数数量范围从39到1001，且中位数约为331。
- en: If the number of most frequent words is increased from 500 to a higher value,
    the median number of words is also expected to increase. This may lead to suitable
    changes needing to be made in the model architecture and parameter values. As
    an example, an increase in the number of words per article may call for more neurons
    in the deep network.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果将最频繁出现的单词数量从500增加到更高的值，则单词的中位数也预计会增加。这可能会导致模型架构和参数值需要做适当的调整。例如，每篇文章的单词数量增加可能需要在深度网络中增加更多的神经元。
- en: 'A histogram of the number of integers per text file for the training data is
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中每个文本文件包含整数的直方图如下：
- en: '![](img/5c45c3a5-2685-4d49-acbd-0be7edf681d6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c45c3a5-2685-4d49-acbd-0be7edf681d6.png)'
- en: The preceding histogram of integers per text file for the training data shows
    the overall pattern, with a mean and median of about 326\. The tail of this histogram
    is slightly longer toward the higher value, giving it a moderately right-skewed
    or positively-skewed pattern.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中文本文件的整数分布直方图显示了整体模式，平均值和中位数大约为326。该直方图的尾部稍微向较高的值延伸，呈现出轻度右偏或正偏的模式。
- en: Now that we've converted the text data into a sequence of integers, we will
    change the labels for the train and text data into integers as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已将文本数据转换为整数序列，我们还将把训练数据和测试数据的标签转换为整数。
- en: Changing labels into integers
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将标签转换为整数
- en: 'When developing deep learning networks for classification problems, we always
    use responses or labels in the form of integers. Author names for the train and
    test text data are stored in `trainy` and `testy`, respectively. Both `trainy`
    and `testy` are lists of 2,500 items that contain the names of 50 authors. To
    change the labels into integers, we can use the following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在为分类问题开发深度学习网络时，我们总是使用整数形式的响应或标签。训练集和测试集文本数据的作者名称分别存储在 `trainy` 和 `testy` 中。`trainy`
    和 `testy` 都是包含 50 个作者名称的 2,500 项列表。为了将标签转换为整数，我们可以使用以下代码：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, to convert labels containing author names into integers, we need
    to unlist them and then use integers from 0 to 49 to represent the 50 authors.
    We can also use `trainy_org` and `testy_org` to save these original integer labels
    for later use.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，要将包含作者名称的标签转换为整数，我们需要将其转化为列表，然后使用从 0 到 49 的整数来表示 50 位作者。我们还可以使用 `trainy_org`
    和 `testy_org` 来保存这些原始整数标签，以备后用。
- en: Next, we will carry out padding and truncation to make the data on a sequence
    of integers have an equal length for each article.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行填充和截断，以确保每篇文章的整数序列长度相等。
- en: Padding and truncation of sequences
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列的填充和截断
- en: 'When developing the author classification model, the number of integers for
    each training and test text data need to be of equal length. We can achieve this
    by padding and truncating the sequence of integers, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发作者分类模型时，每个训练和测试文本数据的整数数量需要相等。我们可以通过填充和截断整数序列来实现这一点，如下所示：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we are specifying the maximum length of all the sequences, that is, `maxlen`,
    to be 300\. This will truncate any sequences that are longer than 300 integers
    in an article and add zeroes to sequences that are shorter than 300 integers in
    an article. Note that for padding and truncation, a default setting of "pre" has
    been used and is not specifically indicated in the code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定所有序列的最大长度，即 `maxlen`，为 300。这将截断任何超过 300 个整数的文章序列，并向短于 300 个整数的序列添加零。注意，对于填充和截断，默认设置为
    "pre"，并且代码中没有特别指明。
- en: This means that for truncation and padding, the integers at the beginning of
    the sequence of integers are impacted. For padding and/or truncation toward the
    end of the sequence of integers, we can make use of `padding = "post"` and/or
    `truncation = "post"` within the code. We can also see that the dimensions of
    `trainx` show a 2,500 x 300 matrix.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，对于截断和填充，整数序列开头的整数会受到影响。对于序列末尾的填充和/或截断，我们可以在代码中使用 `padding = "post"` 和/或
    `truncation = "post"`。我们还可以看到，`trainx` 的维度显示为一个 2,500 x 300 的矩阵。
- en: 'Let''s look at the output from text files 7 and 901 in the train data, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下训练数据中文本文件 7 和 901 的输出，如下所示：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'From the preceding output, we can make the following observations:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以得出以下观察结果：
- en: Text file 7, which had 314 integers, has been reduced to 300 integers. Note
    that this step removed 14 integers at the beginning of the sequence.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本文件 7，原本有 314 个整数，已经减少到 300 个整数。请注意，这一步移除了序列开头的 14 个整数。
- en: Text file 901, which had 48 integers, now has 300 integers, which has been achieved
    by adding zeros at the beginning of the sequence to artificially make the total
    number of integers 300.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本文件 901，原本有 48 个整数，现在有 300 个整数，这通过在序列的开头添加零来人工使整数总数达到 300 来实现。
- en: Next, we will partition the training data into train and validation data, which
    will be required for training and assessing the network at the time of fitting
    the model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把训练数据划分为训练数据和验证数据，这将在拟合模型时用于训练和评估网络。
- en: Data partitioning
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'At the time of training the model, we use `validation_split`, which uses a
    specified percentage of training data to assess validation errors. The training
    data in this example contains data of the first 50 articles from the first author,
    followed by 50 articles from the second author, and so on. If we specify `validation_split`
    as 0.2, the model will be trained based on the first 80% (or 2,000) articles from
    the first 40 authors, and the last 20% (or 500) articles written by the last 10
    authors will be used for assessing validation errors. This will cause no input
    from the last 10 authors to be used in the model training. To overcome this problem,
    we will randomly partition the training data into train and validation data using
    the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，我们使用`validation_split`，该参数使用指定比例的训练数据来评估验证误差。此示例中的训练数据包含来自第一作者的前50篇文章，接着是第二作者的50篇文章，以此类推。如果我们将`validation_split`指定为0.2，模型将基于前40位作者的前80%（或2000篇）文章进行训练，而最后10位作者的最后20%（或500篇）文章将用于评估验证误差。这样，模型训练时将不会使用最后10位作者的输入。为了克服这个问题，我们将使用以下代码随机划分训练数据为训练集和验证集：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see, to partition the data into train and validation data, we have
    used an 80:20 split. We also used the `set.seed` function for repeatability purposes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，为了将数据划分为训练集和验证集，我们使用了80:20的划分比例。我们还使用了`set.seed`函数以确保结果的可重复性。
- en: After partitioning the train data, we will carry out one-hot encoding on the
    labels, which helps us represent the correct author with a value of one, and all
    the other authors with a value of zero.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在划分训练数据后，我们将对标签进行一热编码，这有助于我们用值为1表示正确的作者，用值为0表示所有其他作者。
- en: One-hot encoding the labels
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对标签进行一热编码
- en: 'To carry out one-hot encoding on the labels, we will use the following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对标签进行一热编码，我们将使用以下代码：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we have used the `to_categorical` function to one-hot encode the response
    variable. We used 50 to indicate the presence of 50 classes since the articles
    have been written by 50 authors that we plan to classify, using articles that
    have been written by them as input.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`to_categorical`函数对响应变量进行一热编码。我们使用50表示50个类别的存在，因为这些文章是由50位作者所写，我们计划对其进行分类，并使用他们写的文章作为输入。
- en: Now, the data is ready for developing the convolutional recurrent network model
    for author classification based on the articles they have written.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据已准备好用于基于作者所写文章的作者分类卷积递归网络模型的开发。
- en: Developing the model architecture
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发模型架构
- en: 'In this section, we will make use of convolutional and LSTM layers in the same
    network. The convolutional recurrent network architecture can be captured in the
    form of a simple flowchart:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将在同一网络中使用卷积层和LSTM层。卷积递归网络架构可以用一个简单的流程图来表示：
- en: '![](img/92ef946a-77e5-4756-a3a1-a2617ba2b4ff.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92ef946a-77e5-4756-a3a1-a2617ba2b4ff.png)'
- en: Here, we can see that the flowchart contains embedding, convolutional 1D, maximum
    pooling, LSTM, and dense layers. Note that the embedding layer is always the first
    layer in the network and is commonly used for applications involving text data.
    The main purpose of the embedding layer is to find a mapping of each unique word,
    which in our example is 500, and turn it into a vector that is smaller in size,
    which we will specify using `output_dim`. In the convolutional layer, we will
    use the `relu` activation function. Similarly, the activation functions that will
    be used for the LSTM and dense layers will be `tanh` and `softmax`, respectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到流程图包含了嵌入层、卷积1D层、最大池化层、LSTM层和全连接层。请注意，嵌入层始终是网络中的第一层，并且常用于涉及文本数据的应用。嵌入层的主要目的是为每个唯一的单词找到一个映射，在我们的例子中是500，并将其转换为一个较小的向量大小，这个大小我们将使用`output_dim`指定。在卷积层中，我们将使用`relu`激活函数。类似地，LSTM层和全连接层将分别使用`tanh`和`softmax`激活函数。
- en: 'We can use the following code to develop the model architecture. This also
    includes the output of the model summary:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来开发模型架构。这个代码还包括模型总结的输出：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'From the preceding code, we can make the following observations:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以得出以下结论：
- en: We have specified `input_dim` as 500, which was used as the number of most frequent
    words during data preparation.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已将`input_dim`指定为500，这在数据准备过程中作为最常见词的数量使用。
- en: For `output_dim`, we are using 32, which represents the size of the embedding
    vector. However, note that other numbers can also be explored and we will do so
    later in this chapter, at the time of performance optimization.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`output_dim`，我们使用了32，表示嵌入向量的大小。不过需要注意的是，其他数字也可以进行探索，我们将在本章后续的性能优化时进行探索。
- en: For `input_length`, we have specified 300, which is the number of integers in
    each sequence.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`input_length`，我们指定了300，这是每个序列中整数的数量。
- en: 'After the embedding layer, we have added a 1D convolutional layer with 32 filters.
    In the previous chapters, we used a 2D convolutional layer when working on image
    classification problems. In this example, we have data involving sequences and,
    in such situations, a 1D convolutional layer is more appropriate. For this layer,
    we have specified the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入层之后，我们添加了一个具有32个滤波器的1D卷积层。在之前的章节中，我们在处理图像分类问题时使用了2D卷积层。在这个例子中，我们的数据涉及到序列，在这种情况下，1D卷积层更为合适。对于这个层，我们指定了以下内容：
- en: The length of the 1D convolutional window is specified as 5 using `kernel_size`.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1D卷积窗口的长度通过`kernel_size`指定为5。
- en: We use `valid` for padding to indicate that no padding is required.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`valid`进行填充，表示不需要任何填充。
- en: We have specified the activation function as `relu`.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们指定了激活函数为`relu`。
- en: The strides of the convolution have been specified at 1.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积的步幅已被指定为1。
- en: 'The convolutional layer is followed by a pooling layer. The following are some
    of the comments for pooling and the subsequent layer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层后接池化层。以下是一些关于池化和后续层的注释：
- en: The convolutional layer helps us extract features, while the pooling layer after
    the convolutional layer helps us carry out downsampling and detect important features.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层帮助我们提取特征，而卷积层后的池化层帮助我们进行下采样并检测重要特征。
- en: In this example, we have specified a pooling size of 4, which means that the
    size of the output (74) is one-fourth of the input (296). This can also be seen
    in the model summary.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们指定了池化大小为4，这意味着输出的大小（74）是输入的四分之一（296）。这一点在模型总结中也可以看到。
- en: The next layer is the LSTM with 32 units.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个层是一个具有32个单元的LSTM层。
- en: The last layer is a dense layer with 50 units for the 50 authors, along with
    the `softmax` activation function.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层是一个具有50个单元的密集层，代表50个作者，并使用`softmax`激活函数。
- en: The `softmax` activation function makes all 50 outputs have a total value of
    one and thus allows them to be used as probabilities for each of the 50 authors.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`softmax`激活函数使得所有50个输出的总和为1，从而可以作为每个作者的概率值。'
- en: As we can see from the summary of the model, the total number of parameters
    in this network is 31,122.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从模型的总结中可以看出，该网络的总参数量为31,122。
- en: Next, we will compile the model, followed by training it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译模型，并开始训练。
- en: Compiling and fitting the model
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译并拟合模型
- en: In this section, we will compile the model and then train the model using the
    `fit` function using the training and validation dataset. We will also plot the
    loss and accuracy values that were obtained while training the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将编译模型，然后使用`fit`函数在训练和验证数据集上训练模型。我们还将绘制在训练过程中获得的损失和准确性值。
- en: Compiling the model
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'For compiling the model, we will use the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型时，我们将使用以下代码：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we've specified the `adam` optimizer. We're using `categorical_crossentropy`
    as the loss function since the labels are based on 50 authors. For the metrics,
    we've specified the accuracy of the author's classification.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了`adam`优化器。由于标签基于50个作者，我们使用`categorical_crossentropy`作为损失函数。对于评估指标，我们指定了作者分类的准确性。
- en: Now that the model has been compiled, it's ready for training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经被编译好，准备进行训练。
- en: Fitting the model
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'We will train the model using the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码训练模型：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we're training the model using `trainx` as input and `trainy` as output.
    The model's training is carried out for 30 epochs with a batch size of 32\. For
    assessing the validation loss and validation accuracy for each epoch during the
    training process, we make use of `validx` and `validy`, which we created earlier
    by taking approximately a 20% random sample from the training data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`trainx`作为输入，`trainy`作为输出来训练模型。模型将训练30个epoch，批次大小为32。在训练过程中，为了评估每个epoch的验证损失和验证准确性，我们使用了先前通过从训练数据中随机抽取大约20%的数据生成的`validx`和`validy`。
- en: 'The loss and accuracy values based on the train and validation data for each
    of the 30 epochs are stored in `model_one`. The following is a plot of this data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练数据和验证数据的损失值与准确度值，在每个30个epoch中被存储在`model_one`中。以下是这些数据的图示：
- en: '![](img/2647d6db-d60f-434a-a1e5-ecec9ff2e0e2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2647d6db-d60f-434a-a1e5-ecec9ff2e0e2.png)'
- en: 'From the preceding plot, we can make the following observations:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图表中，我们可以得出以下观察结果：
- en: The loss values for the training and validation data reduce as we go from 1
    to 30 epochs. However, the loss values for the validation data reduce at a slower
    pace compared to those for the training data as the training proceeds.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的损失值在从第1个到第30个epoch的过程中逐渐减小。然而，验证数据的损失值相较于训练数据的损失值，随着训练进展的过程中减小的速度较慢。
- en: The accuracy values for the training and validation data show a similar pattern
    in the opposite direction.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的准确度值呈现出相似的趋势，但方向相反。
- en: Increasing the number of epochs during training is likely to improve the loss
    and accuracy values; however, divergence between the curves is also expected to
    increase, with this potentially leading to an overfitting situation.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加训练过程中epoch的数量可能会改善损失值和准确度值；然而，曲线之间的发散度也可能增加，这可能导致过拟合问题。
- en: Next, we will evaluate `model_one` and make predictions using training and test
    data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将评估`model_one`并使用训练数据和测试数据进行预测。
- en: Evaluating the model and predicting classes
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型并预测类别
- en: In this section, we will evaluate the model based on our training and test data.
    We will obtain accuracy by correctly classifying each author using a confusion
    matrix for the training and test data to gain further insights. We will also use
    bar plots to visualize the accuracy of identifying each author.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将基于训练数据和测试数据评估模型。我们将通过使用训练数据和测试数据的混淆矩阵，正确分类每个作者来获得准确度，并进一步洞察。我们还将使用条形图来可视化每个作者的准确度。
- en: Model evaluation with training data
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练数据进行模型评估
- en: 'First, we will evaluate the model''s performance using training data. Then,
    we will use the model to predict the class representing each of the 50 authors.
    The code for evaluating the model is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用训练数据评估模型的表现。然后，我们将使用模型预测表示50个作者每个类别的标签。评估模型的代码如下：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, we can see that, by using the training data, we obtain a loss value of
    about 1.457 and an accuracy of about 0.535\. Next, we use the model to make a
    prediction about the classes for the articles in the training data. These predictions
    are then used to arrive at an accuracy reading for each of the 50 classes representing
    50 authors. The code that''s used to achieve this is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，通过使用训练数据，我们获得了大约1.457的损失值和大约0.535的准确度。接下来，我们使用模型对训练数据中的文章类别进行预测。然后，我们使用这些预测来计算每个代表50个作者的类别的准确度。实现这一目标的代码如下：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, to conserve space, we haven''t printed the output of
    the confusion matrix since it will be a 50 x 50 matrix. However, we have used
    information in the confusion matrix to arrive at the model''s accuracy by correctly
    predicting each author based on the articles they have written. The output that
    we''ve obtained is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，为了节省空间，我们没有打印混淆矩阵的输出，因为它将是一个50 x 50的矩阵。然而，我们已经利用混淆矩阵中的信息，通过正确预测每个作者基于其写的文章来得出模型的准确度。我们得到的输出如下：
- en: '![](img/5d97e23e-30f6-4829-afd0-3995073166e8.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d97e23e-30f6-4829-afd0-3995073166e8.png)'
- en: 'The preceding bar plot provides further insight into the model''s performance
    with respect to each author:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的条形图提供了更多关于模型在每个作者上的表现的洞察：
- en: The accuracy of correctly classifying an author has the highest value of 90%
    for author 15.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类作者的准确率在第15位作者上达到了最高值90%。
- en: The accuracy of correctly classifying an author has the lowest value of 0% for
    author 43.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类一个作者的准确率对于第43位作者来说，达到了最低值0%。
- en: This model struggles to correctly classify articles from certain authors, such
    as those labeled 3, 8, 18, 31, 43, 45, and 48.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型在正确分类某些作者的文章时存在困难，比如标记为3、8、18、31、43、45和48的作者。
- en: Having assessed the model using training data, we will repeat this process with
    the test data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用训练数据评估了模型之后，我们将使用测试数据重复此过程。
- en: Model evaluation with test data
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用测试数据进行模型评估
- en: 'We will use the model to obtain the loss and accuracy values from the test
    data using the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码，通过模型从测试数据中获取损失值和准确度值：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From the preceding code, we can see that the loss and accuracy values based
    on the test data are 2.461 and 0.251, respectively. Both of these results are
    inferior to the ones we obtained based on the training data, which is usually
    expected. Predicting the classes and calculating the accuracy of classification
    for each author, as shown in the following code, would help provide further insights:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到，基于测试数据的损失和准确率值分别为2.461和0.251。这两个结果都不如我们基于训练数据得到的结果，通常这是可以预期的。预测每个作者的类别并计算分类的准确性，如以下代码所示，将有助于提供进一步的见解：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The information in the confusion matrix is stored in `tab1`, which is used
    for arriving at the accuracy of correctly classifying articles from each author.
    The results are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵中的信息存储在`tab1`中，用于计算每个作者的文章正确分类的准确率。结果如下：
- en: '![](img/1292f3a5-02ee-4259-bbc8-eaf82fbc1b23.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1292f3a5-02ee-4259-bbc8-eaf82fbc1b23.png)'
- en: 'An overall accuracy of about 25% for the test data already suggested significantly
    inferior performance based on the test data. This can also be seen in the preceding
    bar chart. Let''s take a look at some of the observations we can make from this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据的整体准确率大约为25%，这已经表明基于测试数据的性能明显较差。这一点也可以从前面的柱状图中看到。让我们来看一下我们从中可以得出的观察结果：
- en: For the authors labeled 31, 43, 45, and 48, none of the 50 articles written
    by each author were correctly classified.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标签为31、43、45和48的作者，每位作者的50篇文章中没有一篇被正确分类。
- en: More than 80% of the articles from the authors labeled 15 and 38 were correctly
    classified.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自标签为15和38的作者的文章中，超过80%被正确分类。
- en: From this initial example, we can see that our model classification performance
    needs further improvement. The differences in performance that we have observed
    between the training and test data also indicate the presence of an overfitting
    problem. Thus, we need to make changes to the model architecture to obtain a model
    that not only provides higher accuracy in classification performance but also
    shows consistent performance between the training and test data. We will explore
    this in the next section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个初步的例子中，我们可以看到我们的模型分类性能需要进一步改善。我们在训练数据和测试数据中观察到的性能差异也表明了过拟合问题的存在。因此，我们需要对模型架构进行修改，以获得一个不仅能在分类性能上提供更高准确率，而且在训练数据和测试数据之间表现一致的模型。我们将在下一部分中探讨这一点。
- en: Performance optimization tips and best practices
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: In this section, we will explore changes we can make to the model architecture
    and other settings to improve author classification performance. We will carry
    out two experiments, and, for both of these two experiments, we will increase
    the number of most frequent words from 500 to 1,500 and increase the length of
    the sequences of integers from 300 to 400\. For both experiments, we will also
    add a dropout layer after the pooling layer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨可以对模型架构和其他设置进行的修改，以提高作者分类性能。我们将进行两个实验，在这两个实验中，我们将最常见词汇的数量从500增加到1,500，并将整数序列的长度从300增加到400。对于这两个实验，我们还将在池化层之后添加一个dropout层。
- en: Experimenting with reduced batch size
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试减少批量大小
- en: 'The code that we''ll be using for this experiment is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于此实验的代码如下：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From the preceding code, we can make the following observations:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以得出以下观察结论：
- en: We will update the model architecture by specifying `input_dim` as 1,500 and
    `input_length` as 400.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将通过指定`input_dim`为1,500和`input_length`为400来更新模型架构。
- en: We will reduce the batch size that's used at the time of fitting the model from
    32 to 16.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将把在训练模型时使用的批量大小从32减少到16。
- en: To address the overfitting problem, we have added a dropout layer with a rate
    of 25%.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了应对过拟合问题，我们已经添加了一个dropout层，丢弃率为25%。
- en: We have kept all other settings the same as those we had used for the previous
    model.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保持了所有其他设置与之前模型所用设置相同。
- en: 'The loss and accuracy values based on the training and validation data for
    each of the 30 epochs is stored in `model_two`. The results can be seen in the
    following plot:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练和验证数据的损失和准确率值，针对30个训练周期，每个周期的结果存储在`model_two`中。结果可以在以下图表中查看：
- en: '![](img/d33b921e-8107-4abb-b084-e851906e486d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d33b921e-8107-4abb-b084-e851906e486d.png)'
- en: 'The preceding plot indicates that the loss and accuracy values for the validation
    data stay flat for the last few epochs. However, they do not deteriorate. Next,
    we will obtain the loss and accuracy values based on the training and test data
    using the `evaluate` function, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表表明，验证数据的损失和准确率值在最后几个周期保持平稳。然而，它们并没有恶化。接下来，我们将基于训练数据和测试数据，使用`evaluate`函数来获取损失和准确率值，如下所示：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: From the preceding code and output, we can observe that the loss and accuracy
    values for the training data show better results compared to the previous model.
    However, for the test data, although the accuracy value is better, the loss value
    is slightly worse.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码和输出中，我们可以观察到训练数据的损失和准确率值相较于前一个模型有了更好的结果。然而，对于测试数据，尽管准确率有所提升，损失值却略微变差。
- en: 'The accuracy that was obtained by correctly classifying the articles in the
    testing data from each author can be seen in the following bar plot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个作者的测试数据中正确分类文章所获得的准确率，可以在以下条形图中看到：
- en: '![](img/698c0af5-4bf0-4171-aea2-62ad35dc4d4f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/698c0af5-4bf0-4171-aea2-62ad35dc4d4f.png)'
- en: 'From the preceding bar plot, we can make the following observations:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的条形图中，我们可以得出以下观察结果：
- en: The bar plot visually shows improvements compared to the previous model.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条形图清晰地展示了与之前的模型相比的改进。
- en: In the previous model, for the test data, we had four authors with no articles
    classified correctly. However, now, we don't have any authors with no correct
    classification.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前一个模型中，对于测试数据，我们有四个作者的文章没有被正确分类。然而，现在我们没有任何作者的文章是错误分类的。
- en: In the next experiment, we will look at more changes we can make in an effort
    to improve the author's classification performance even further.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将查看更多的改动，努力进一步提升作者分类性能。
- en: Experimenting with batch size, kernel size, and filters in CNNs
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在CNN中尝试批量大小、卷积核大小和滤波器
- en: 'The code that will be used for this experiment is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将用于此次实验的代码如下：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'From the preceding code, we can make the following observations:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以得出以下观察结果：
- en: We have reduced the kernel size from 5 to 4.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将卷积核的大小从5减小到了4。
- en: We have increased the number of filters for the convolutional layer from 32
    to 64.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将卷积层的滤波器数量从32增加到了64。
- en: We have reduced the batch size from 16 to 8 while training the model.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在训练模型时将批量大小从16减小到8。
- en: We have kept all other settings the same as what was used for the previous model.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保持了与之前的模型相同的其他设置。
- en: 'The loss and accuracy values based on the training and validation data for
    each of the 30 epochs are stored in `model_three`. A plot of this data is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基于每个30个周期的训练和验证数据的损失和准确率值被存储在`model_three`中。该数据的图表如下：
- en: '![](img/cfc795b3-827b-4043-b66f-968f6e21af1d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfc795b3-827b-4043-b66f-968f6e21af1d.png)'
- en: 'The plot for the loss and accuracy shows the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 损失和准确率的图表显示了以下内容：
- en: The accuracy values for the validation data remain flat for the last few epochs,
    whereas it increases at a relatively slower pace in the last few epochs for the
    training data.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据的准确率在最后几个周期保持平稳，而训练数据的准确率在最后几个周期以较慢的速度增加。
- en: The loss values based on the validation data start to increase during the last
    few epochs and continue to decrease for the training data.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于验证数据的损失值在最后几个周期开始上升，并且训练数据的损失继续下降。
- en: 'Now, we will obtain the loss and accuracy values based on the train and test
    data using the `evaluate` function, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将基于训练数据和测试数据，使用`evaluate`函数来获得损失和准确率的值，如下所示：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From the preceding code and output, we can observe the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码和输出中，我们可以观察到以下几点：
- en: The loss and accuracy values based on the training data show an improvement
    compared to the previous two models.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于训练数据的损失和准确率值相比前两个模型有所改进。
- en: For the test data, although the loss value is higher compared to the first two
    models, an accuracy value of about 34% shows better accuracy in classifying author
    articles.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试数据，尽管与前两个模型相比，损失值更高，但准确率约为34%，在分类作者文章方面表现出更好的准确性。
- en: 'The following bar plot shows the accuracy of correctly classifying the authors
    of articles in the test data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下条形图显示了正确分类测试数据中作者文章的准确率：
- en: '![](img/eb04c5b6-ee46-46b9-9145-30b64a20acfb.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb04c5b6-ee46-46b9-9145-30b64a20acfb.png)'
- en: 'From the preceding bar plot, we can observe the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的条形图中，我们可以观察到以下几点：
- en: The accuracy of correctly classifying articles from each author shows better
    performance compared to the previous two models since we don't have any authors
    with zero accuracy.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类每个作者文章的准确度显示出比之前的两个模型更好的表现，因为我们没有任何作者的准确率为零。
- en: When comparing the three models that we've used so far using test data, we can
    see that the first model has four authors classified with 50% or higher accuracy.
    However, for the second and third models, the number of authors classified with
    50% or higher accuracy increases to 8 and 9, respectively.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们使用测试数据比较迄今为止使用的三个模型时，可以看到第一个模型有四个作者的分类准确率达到50%或更高。然而，对于第二个和第三个模型，分类准确率达到50%或更高的作者数量分别增加到8和9。
- en: In this section, we carried out two experiments that showed that the author
    classification performance of the model can be improved further.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行了两项实验，结果表明，模型的作者分类性能可以进一步提高。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we illustrated the steps for developing a convolutional recurrent
    neural network for author classification based on articles that they have written.
    Convolutional recurrent neural networks combine the advantages of two networks
    into one network. On one hand, convolutional networks can capture high-level local
    features from the data, while, on the other hand, recurrent networks can capture
    long-term dependencies in the data involving sequences.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们展示了基于作者所写文章开发卷积递归神经网络进行作者分类的步骤。卷积递归神经网络将两种网络的优点结合成一个网络。一方面，卷积网络能够从数据中捕捉到高级局部特征；另一方面，递归网络能够捕捉到数据中涉及序列的长期依赖关系。
- en: First, convolutional recurrent neural networks extract features using a one-dimensional
    convolutional layer. These extracted features are then passed to the LSTM recurrent
    layer to obtain hidden long-term dependencies, which are then passed to a fully
    connected dense layer. This dense layer obtains the probability of the correct
    classification of each author based on the data in the articles. Although we used
    a convolutional recurrent neural network for the author classification problem,
    this type of deep network can be applied to other types of data involving sequences,
    such as natural language processing, speech, and video-related problems.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，卷积递归神经网络通过一维卷积层提取特征。然后，这些提取的特征传递给LSTM递归层，以获取隐藏的长期依赖关系，接着传递给全连接的密集层。该密集层根据文章中的数据获得正确分类每个作者的概率。尽管我们在作者分类问题中使用了卷积递归神经网络，但这种深度网络也可以应用于其他类型的涉及序列的数据，如自然语言处理、语音和视频相关问题。
- en: The next chapter will be the last chapter of this book and will go over tips,
    tricks, and the road ahead. Developing deep learning networks for different types
    of data is both art and science. Every application brings new challenges, as well
    as an opportunity for us to learn and improve our skills. In the next chapter,
    we will summarize some such experiences that can turn out to be very useful in
    certain applications and help save a significant amount of time in arriving at
    models that perform well.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将是本书的最后一章，主要讲解一些技巧、窍门以及未来的发展方向。为不同类型数据开发深度学习网络既是艺术也是科学。每一个应用都带来了新的挑战，同时也是我们学习和提升技能的机会。在下一章中，我们将总结一些经验，这些经验在某些应用中非常有用，并能帮助节省大量时间，以便更快地开发出表现优异的模型。
