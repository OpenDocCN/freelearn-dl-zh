- en: Audio Generation with NSynth and GANSynth
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NSynth和GANSynth进行音频生成
- en: In this chapter, we'll be looking into audio generation. We'll first provide
    an overview of WaveNet, an existing model for audio generation, especially efficient
    in text-to-speech applications. In Magenta, we'll use NSynth, a WaveNet autoencoder
    model, to generate small audio clips that can serve as instruments for a backing
    MIDI score. NSynth also enables audio transformations such as scaling, time stretching,
    and interpolation. We'll also use GANSynth, a faster approach based on **Generative
    Adversarial Network** (**GAN**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨音频生成。我们将首先概述WaveNet，这是一种现有的音频生成模型，尤其在语音合成应用中效率较高。在Magenta中，我们将使用NSynth，这是一个WaveNet自编码器模型，用于生成可以作为伴奏MIDI乐谱的音频片段。NSynth还支持音频变换，如缩放、时间拉伸和插值。我们还将使用GANSynth，这是基于**生成对抗网络**（**GAN**）的更快速方法。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Learning about WaveNet and temporal structures for music
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解WaveNet和音乐的时间结构
- en: Neural audio synthesis with NSynth
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NSynth进行神经音频合成
- en: Using GANSynth as a generative instrument
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GANSynth作为生成乐器
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll use the following tools:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下工具：
- en: The **command line** or **Bash** to launch Magenta from the Terminal
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**命令行**或**Bash**从终端启动Magenta
- en: '**Python** and its libraries to write music generation code using Magenta'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Python**及其库编写音乐生成代码，利用Magenta
- en: '**Magenta** to generate audio clips'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Magenta**生成音频片段
- en: '**Audacity** to edit audio clips'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Audacity**编辑音频片段
- en: Any media player to listen to the generated WAV files
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用任何媒体播放器播放生成的WAV文件
- en: In Magenta, we'll make the use of the **NSynth** and **GANSynth** models. We'll
    be explaining these models in depth, but if you feel like you need more information,
    the models' README in Magenta's source code ([github.com/tensorflow/magenta/tree/master/magenta/models](https://github.com/tensorflow/magenta/tree/master/magenta/models))
    is a good place to start. You can also take a look at Magenta's code, which is
    well documented. We also provide additional content in the *Further reading* section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在Magenta中，我们将使用**NSynth**和**GANSynth**模型。我们会深入解释这些模型，但如果你觉得需要更多信息，可以查看Magenta源代码中的模型README文件（[github.com/tensorflow/magenta/tree/master/magenta/models](https://github.com/tensorflow/magenta/tree/master/magenta/models)），这是一个很好的起点。你还可以查看Magenta的代码，该代码文档完善。此外，我们还在*进一步阅读*部分提供了额外的内容。
- en: The code for this chapter is in this book's GitHub repository in the `Chapter05`
    folder, located at [github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05).
    The examples and code snippets will suppose you are located in this chapter's
    folder. For this chapter, you should do `cd Chapter05` before you start.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于本书的GitHub仓库中的`Chapter05`文件夹，地址为[github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05)。示例和代码片段假设你已经位于本章的文件夹中。为了开始本章的内容，请先执行`cd
    Chapter05`。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请观看以下视频，查看代码示范：
- en: '[http://bit.ly/37QgQsI](http://bit.ly/37QgQsI)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/37QgQsI](http://bit.ly/37QgQsI)'
- en: Learning about WaveNet and temporal structures for music
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解WaveNet和音乐的时间结构
- en: In the previous chapters, we've been generating symbolic content such as MIDI.
    In this chapter, we'll be looking at generating sub-symbolic content, such as **raw
    audio**. We'll be using the Waveform Audio File Format (WAVE or WAV, stored in
    a `.wav` file), a format containing uncompressed audio content, usable on pretty
    much every platform and device. See [Chapter 1](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml),
    *Introduction on Magenta and Generative Art*, for more information on waveforms
    in general.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们一直在生成符号内容，如MIDI。在本章中，我们将探讨生成非符号内容，如**原始音频**。我们将使用波形音频文件格式（WAVE或WAV，存储在`.wav`文件中），这是一种包含未压缩音频内容的格式，可以在几乎所有平台和设备上使用。有关波形的更多信息，请参见[第1章](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml)，*Magenta与生成艺术简介*。
- en: Generating raw audio using neural nets is a rather recent feat, following the
    2016 WaveNet paper, *A Generative Model For Raw Audio*. Other network architectures
    also perform well in audio generation, such as SampleRNN, also released in 2016
    and used since to produce music tracks and albums (see databots for an example).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络生成原始音频是近年来的一个成就，源于2016年发布的WaveNet论文，*A Generative Model For Raw Audio*。其他网络架构在音频生成中也表现良好，比如SampleRNN，2016年同样发布并被用于制作音乐曲目和专辑（见databots的示例）。
- en: As stated in [Chapter 2](b60deee5-c58f-45eb-88a2-23718802e580.xhtml), *Generating
    Drum Sequences with DrumsRNN*, convolutional architectures are rather rare in
    music generation, given their shortcomings in handling sequential data. WaveNet
    uses a stack of causal convolution layers to address these problems, somewhat
    analogous to recurrent layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第2章](b60deee5-c58f-45eb-88a2-23718802e580.xhtml)中所述，*使用DrumsRNN生成鼓声序列*，卷积架构在音乐生成中相对罕见，因为它们在处理序列数据时存在不足。WaveNet使用了一堆因果卷积层来解决这些问题，这在某种程度上类似于递归层。
- en: Modeling raw audio is hard—you have to handle 16,000 samples per second (at
    least) and keep track of the general structure at a bigger time scale. WaveNet's
    implementation is optimized to handle such data with the use of dilated convolution,
    where the convolution filter is applied over a large area by skipping input values
    with a certain step, enabling the network to preserve the input resolution throughout
    the network by using just a few layers. During training, the predictions can be
    made in parallel, while during generation, the predictions have to be made sequentially
    or **sample by sample**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 建模原始音频是困难的——你需要处理每秒16,000个样本（至少），并在更大的时间尺度上跟踪整体结构。WaveNet的实现经过优化，能够处理这些数据，通过使用膨胀卷积，卷积滤波器通过跳过输入值一定的步长应用于一个较大的区域，从而使网络能够在整个网络中通过仅使用少数几层来保持输入分辨率。在训练过程中，预测可以并行进行，而在生成过程中，预测必须按顺序进行，或者**逐个样本**地生成。
- en: The WaveNet architecture has been used with excellent performance in text-to-speech
    application and recently in music generation but is computationally very expensive.
    Magenta's NSynth model is a **WaveNet autoregressive model**, an approach used
    to attain consistent long-term structure. Let's have a look into NSynth and its
    importance in generating music.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet架构在语音合成应用中表现优异，并且最近在音乐生成中也取得了很好的效果，但其计算开销非常大。Magenta的NSynth模型是一个**WaveNet自回归模型**，这种方法用于保持一致的长期结构。让我们来看看NSynth及其在生成音乐中的重要性。
- en: Looking at NSynth and WaveNet autoencoders
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看NSynth和WaveNet自编码器
- en: The NSynth model can be seen as a neural synthesizer—instead of having a synthesizer
    where you can define envelopes and specify the oscillator wave, pitch, and velocity,
    you have a model that generates new, realistic, instrument sounds. NSynth is **instrument-oriented**,
    or note-oriented, meaning it can be used to generate single notes of a generated
    instrument.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: NSynth模型可以看作是一个神经合成器——与其拥有一个可以定义包络线并指定振荡器波形、音高和力度的合成器，不如拥有一个生成新的、真实的乐器声音的模型。NSynth是**以乐器为导向**的，或者说是以音符为导向的，这意味着它可以用来生成某个生成乐器的单个音符。
- en: NSynth is a WaveNet-style autoencoder that learns the input data's temporal
    embedding. To understand the WaveNet Autoencoder (AE) network, you can refer to
    concepts explained in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with MusicVAE*, since both networks are AEs. You'll
    see here many of the concepts we've previously shown, such as encoding, latent
    space, and interpolation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: NSynth是一个WaveNet风格的自编码器，它学习输入数据的时间嵌入。要理解WaveNet自编码器（AE）网络，可以参考[第4章](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml)中解释的概念，*使用MusicVAE进行潜在空间插值*，因为这两个网络都是自编码器。在这里，你会看到我们之前展示的许多概念，比如编码、潜在空间和插值。
- en: 'Here is a simplified view of the WaveNet AE network:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是WaveNet AE网络的简化视图：
- en: '![](img/af142458-0126-4d26-a16a-906ee26021d0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af142458-0126-4d26-a16a-906ee26021d0.png)'
- en: First, the encoder sees the whole input, which is the whole mono waveform (in
    the `.wav` format), and after 30 layers of computation, calculates an average
    pooling to create a temporal embedding (***z*** in the diagram) of 16 dimensions
    for every 512 samples, which is a dimensionality reduction of 32 times. For example,
    a single audio input, with 16,000 samples (1 second of audio with a sample rate
    of 16,000), once encoded, will have a shape of 16 for the latent vector and 16,000/512
    for the time (see the next section, *Encoding the WAV files*, for an example of
    this). Then, the WaveNet decoder will upsample the embedding to its original time
    resolution using a 1x1 convolution, trying to reproduce as closely as possible
    the input sound.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，编码器看到整个输入，即整个单声道波形（`.wav`格式），经过30层计算后，通过平均池化计算出一个时域嵌入（图中的***z***），该嵌入具有16个维度，每512个样本计算一次，这是32倍的维度压缩。例如，一个包含16,000个样本（1秒的音频，采样率为16,000）的音频输入，经过编码后，其潜在向量的维度为16，时间维度为16,000/512（参见下一节*编码WAV文件*，其中有示例）。然后，WaveNet解码器将使用1x1卷积将嵌入上采样至其原始时间分辨率，尽可能精确地重现输入的声音。
- en: You can see WaveNet's implementation in the `Config` class of the `magenta.models.nsynth.wavenet.h512_bo16`
    module. The fastgen implementation used for the `synthesize` method is in the
    `FastGenerationConfig` class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`magenta.models.nsynth.wavenet.h512_bo16`模块的`Config`类中看到WaveNet的实现。用于`synthesize`方法的fastgen实现位于`FastGenerationConfig`类中。
- en: The *z* representation, or latent vector, has similar properties to what we
    saw in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml), *Latent Space
    Interpolation with MusicVAE—*similar sounds have similar *z* representations,
    and mixing or interpolation between two latent vectors is possible. This creates
    endless possibilities in terms of sound exploration. While traditional audio mixing
    revolves around the action of changing the volume of two audio clips to hear both
    at the same time, mixing two encodings together is about creating a sound that
    is a **hybrid of two original sounds.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*表示，或潜在向量，具有与我们在[第4章](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml)中看到的类似的特性，*Latent
    Space Interpolation with MusicVAE—*相似的声音具有相似的*z*表示，且可以在两个潜在向量之间进行混合或插值。这为声音探索创造了无尽的可能性。传统的音频混音围绕着改变两个音频片段的音量，使它们同时播放，而将两个编码混合在一起则是创造一个**由两种原始声音混合而成的声音。**'
- en: During this chapter, you'll get to listen to a lot of generated sounds, which
    we recommend you do instead of just looking at spectrograms. You'll probably notice
    that the sounds have a grainy or lo-fi texture. This is because the model works
    on mu-law encoded 8-bit 16 kHz sounds, which are of lower quality than what you
    typically listen to and is necessary for computational reasons.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，你将听到很多生成的声音，我们建议你多听这些声音，而不仅仅是查看声谱图。你可能会注意到这些声音有一种颗粒感或低保真的质感。这是因为模型使用的是经过μ-law编码的8位16
    kHz声音，这些声音的质量低于你通常听到的声音，这是出于计算原因的需要。
- en: Due to its training, the model might sometimes fall short while reconstructing
    the audio, which leads to additional harmonics, approximations, or crazy sounds.
    While surprising, these results give an interesting twist to the generated audio.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练的原因，模型在重建音频时有时可能会出现不足，导致额外的谐波、近似或奇怪的声音。虽然这些结果令人惊讶，但它们为生成的音频增添了一种有趣的转折。
- en: In this chapter, we'll be generating audio clips using NSynth, which we can
    then sequence using a previously generated MIDI sequence, for example. We'll listen
    to the sound of the interpolation between a cat sound and a bass sound by adding
    the encodings of both clips and synthesizing the result. We'll be generating a
    handful of sound combinations so we get a feel of what is possible in terms of
    audio interpolation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将使用NSynth生成音频片段，然后可以用之前生成的MIDI序列进行排列。例如，我们将听到猫声和低音声之间的插值声音，通过将两段音频的编码相加并合成结果。我们会生成一些音频组合，以便感受音频插值的可能性。
- en: Visualizing audio using a constant-Q transform spectrogram
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用常数-Q变换谱图可视化音频
- en: 'Before we start, we''ll introduce an audio visualization plot called the **Constant-Q**
    **Transform** (**CQT**) spectrogram. We provide more information about plotting
    audio signals and CQT in the last section, *Further reading*. In the previous
    chapters, we''ve been representing MIDI as a pianoroll plot, and the representations
    are simple and easy to understand. Audio, on the other hand, is hard to represent:
    two spectrograms looking almost the same might sound different.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将介绍一种音频可视化图谱，称为**Constant-Q** **Transform**（**CQT**）频谱图。我们将在最后一节*进一步阅读*中提供更多关于绘制音频信号和CQT的资料。在前几章中，我们一直用钢琴卷轴图来表示MIDI，且这些表示方式简单易懂。另一方面，音频的表示较为复杂：两幅几乎相同的频谱图可能听起来却不同。
- en: In [Chapter 1](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml), *Introduction to
    Magenta and Generative Art*, in the *Representing music with a spectrogram* section,
    we've shown how a spectrogram is a plot of time and frequency. In this chapter,
    we'll be looking at a CQT spectrogram, which is a spectrogram displayed with the
    magnitude represented by the intensity and the instantaneous frequency by color.
    The colors represent the 16 different dimensions of the embeddings. The intensity
    of lines is proportional to the log magnitude of the power spectrum, and the colors
    are given by the derivative of the phase, making the phase visible as rainbow
    colors, hence the nickname "rainbowgrams" given by the Magenta team.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml)，*Magenta与生成艺术简介*的*用频谱图表示音乐*部分中，我们展示了频谱图是时间与频率的图示。在本章中，我们将查看CQT频谱图，这是一种通过强度表示幅度、通过颜色表示瞬时频率的频谱图。颜色代表嵌入的16个不同维度。线条的强度与功率谱的对数幅度成比例，颜色则由相位的导数决定，使相位以彩虹色的形式呈现，因此Magenta团队将其称为“彩虹图”。
- en: 'For this section, we are providing four audio samples that we''ll use for our
    example and show as a rainbowgram. As always, the figures are not a replacement
    for listening to the audio content. Those samples are shown in the following screenshot:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分，我们提供了四个音频样本，用于我们的示例，并以彩虹图形式展示。像往常一样，这些图形无法替代聆听音频内容。这些样本显示在下图中：
- en: '![](img/cf087c72-9faa-454a-860f-c3b9cf946c89.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf087c72-9faa-454a-860f-c3b9cf946c89.png)'
- en: In the screenshot, you can notice a couple of things. First, the flute and the
    bass plots have a pretty well-defined harmonic series. Second, the metal plot,
    however, is more confused since it is a metal plate being struck. You can clearly
    see the attack of the sound and the following noise spanning the whole frequency
    range.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在截图中，你可以注意到几点。首先，长笛和低音的频谱图有着相当清晰的谐波系列。其次，金属的频谱图则显得更加混乱，因为它是金属板被敲击的声音。你可以清晰地看到声音的攻击部分，以及随后的噪声覆盖整个频率范围。
- en: For our example, we'll be combining each pair of those sounds, for example,
    metal and cat, and cat and flute.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将组合这些声音的每一对，例如金属和猫咪，猫咪和长笛。
- en: The NSynth dataset
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSynth数据集
- en: 'Before we start, we''ll have a brief look at the NSynth dataset, which was
    used to train the NSynth model. It is available at [magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth) and
    is a high-quality and large-scale dataset, an order of magnitude larger than other
    similar datasets. Even if it is difficult to use for training with NSynth, it
    is interesting to look at for its content: over 300,000 musical notes that are
    classified by source, family, and quality. It can also serve as content for producing
    audio clips.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们先简要了解一下用于训练NSynth模型的NSynth数据集。该数据集可以在[magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth)找到，是一个高质量且大规模的数据集，比其他类似数据集大一个数量级。即使它在使用NSynth进行训练时可能有些困难，但从其内容来看非常有趣：超过30万个按来源、家族和质量分类的音符。它也可以作为生成音频片段的内容。
- en: The audio clips are all 4 seconds long (the note was held for 3 seconds and
    given 1 second for the release) and represent a single note of different instruments.
    Each note has been recorded at every pitch of the standard MIDI piano range of
    21 to 108 at five different velocities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 音频片段的长度均为4秒（音符持续了3秒，释放音符用了1秒），并代表了不同乐器的单一音符。每个音符在标准MIDI钢琴范围的21到108之间的每个音高上都已录制，且以五种不同的力度进行了录制。
- en: Since the instruments are classified by source, which is the method of sound
    production (such as acoustic, electronic, or synthetic), the dataset can be split
    for training on a specific instrument source. For example, the pre-trained GANSynth
    model we are going to use, `acoustic_only`, is useful for generating a more classical
    type of sound because the instruments in the training set are varied. The instruments
    are also classified by family, such as piano and bass, and qualities such as bright,
    dark, and percussive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乐器是按声音来源分类的，即声音的产生方式（例如声学、电子或合成），因此可以将数据集拆分，以便针对特定的乐器来源进行训练。例如，我们将要使用的预训练GANSynth模型，`acoustic_only`，对于生成更经典类型的声音非常有用，因为训练集中的乐器种类繁多。乐器还按家族分类，如钢琴和低音，以及按音质分类，如明亮、阴暗和打击音。
- en: Interestingly, a dataset oriented on single notes like the NSynth dataset is
    really useful for producing neural audio synthesis of notes, which can in turn
    be sequenced with the other models in Magenta. In that sense, the NSynth model
    fits well in the Magenta ecosystem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，像NSynth数据集这样专注于单个音符的数据集在生成音符的神经音频合成中非常有用，这些音符又可以与Magenta中的其他模型一起进行排序。从这个角度看，NSynth模型非常适合Magenta生态系统。
- en: Neural audio synthesis with NSynth
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NSynth进行神经音频合成
- en: In this section, we'll be combining different audio clips together. We'll learn
    to encode the audio, optionally saving the resulting encodings on disk, mix (add)
    them, and then decode the added encodings to retrieve a sound clip.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将把不同的音频片段结合在一起。我们将学习如何对音频进行编码，并可选择将结果编码保存到磁盘，再对其进行混合（添加），然后解码添加后的编码以检索音频片段。
- en: 'We''ll be handling 1-second audio clips only. There are two reasons for this:
    first, **handling audio is costly**, and second, we want to **generate instrument
    notes** in the form of short audio clips. The latter is interesting for us because
    we can then sequence the audio clips using MIDI generated by the models we''ve
    been using in the previous chapters. In that sense, you can view NSynth as a generative
    instrument, and the previous models, such as MusicVAE or Melody RNN, as a generative
    score (partition) composer. With both elements, we can generate full tracks, with
    audio and structure.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只处理1秒钟的音频片段。这样做有两个原因：首先，**处理音频的成本很高**，其次，我们想要**生成乐器音符**，以短音频片段的形式呈现。后者对我们很有趣，因为我们可以使用我们在前几章中使用的模型生成的MIDI来对音频片段进行排序。从这个角度看，你可以将NSynth视为一个生成性乐器，而将之前的模型，如MusicVAE或Melody
    RNN，视为生成性乐谱（曲谱）作曲器。结合这两个元素，我们可以生成完整的音轨，包含音频和结构。
- en: To generate sound clips, we'll be using the `fastgen` module, an external contribution
    to Magenta, which now resides in NSynth's code, implementing optimizations for
    faster sound generation with an easy-to-use API.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成音频片段，我们将使用`fastgen`模块，这是Magenta的一个外部贡献，目前已集成到NSynth的代码中，优化了通过易于使用的API快速生成音频的功能。
- en: Choosing the WaveNet model
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择WaveNet模型
- en: Magenta provides two pre-trained NSynth models with included weights. We're
    going to be using the WaveNet pre-trained model. This model is very expensive
    to train, taking around 10 days on 32 K40 GPUs, so we won't be talking about training
    here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Magenta提供了两个包含权重的预训练NSynth模型。我们将使用WaveNet预训练模型。这个模型的训练非常昂贵，使用32个K40 GPU需要大约10天时间，因此我们在这里不讨论训练。
- en: You can follow this example in the `chapter_05_example_01.py` file in the source
    code of this chapter. There are more comments and content in the source code,
    so you should go check it out.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本章源代码中的`chapter_05_example_01.py`文件中参考这个示例。源代码中有更多的注释和内容，所以你应该去查看它。
- en: This chapter also contains audio clips in the `sounds` folders that you can
    use for this section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还包含`sounds`文件夹中的音频片段，你可以在这一部分使用它们。
- en: 'To download the pre-trained model, use the following method, which will download
    the model and extract it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载预训练模型，使用以下方法，它会下载并提取模型：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code downloads the proper checkpoint and then extracts it to the destination
    directory. This is similar to `download_checkpoint`, which we've written in the
    previous chapter. Using the `wavenet-ckpt` checkpoint name, the resulting checkpoint
    will be usable via the `checkpoints/wavenet-ckpt/model.ckpt-200000` path.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码下载合适的检查点并将其提取到目标目录。这类似于我们在上一章中编写的`download_checkpoint`。使用`wavenet-ckpt`检查点名称，得到的检查点可以通过`checkpoints/wavenet-ckpt/model.ckpt-200000`路径使用。
- en: Note that this method might download rather big files (the pre-trained models
    in this chapter are big), so it will look as if the program is stuck for a while.
    It only means that the file is being downloaded (once only) locally.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种方法可能会下载较大的文件（本章的预训练模型较大），所以程序看起来可能会卡住一段时间。这只是意味着文件正在本地下载（仅下载一次）。
- en: Encoding the WAV files
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码 WAV 文件
- en: 'First, we''ll encode the WAV files using the `fastgen` library. Let''s define
    the `encode` method and load the audio:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `fastgen` 库来编码 WAV 文件。我们定义 `encode` 方法并加载音频：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we first load the audio for each file in the `wav_filenames`
    parameter, using the `load_audio` method from the `magenta.models.nsynth.utils`
    module. To load the audio, two parameters are important, `sample_length` and `sample_rate`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先通过 `magenta.models.nsynth.utils` 模块中的 `load_audio` 方法加载 `wav_filenames`
    参数中的每个音频文件。要加载音频，两个参数非常重要，`sample_length` 和 `sample_rate`：
- en: The sample rate is set at `16000`, which is the sample rate used by the underlying
    model. Remember, the sample rate is the number of discrete samples for 1 second
    of audio.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样率设置为 `16000`，这是底层模型使用的采样率。记住，采样率是指每秒钟音频的离散采样数。
- en: The sample length can be calculated by multiplying the desired number of seconds
    by the sample rate. For our example, we'll be using audio clips of 1 second for
    a sample length of 16,000.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样长度可以通过将所需的秒数与采样率相乘来计算。以我们的示例为例，我们将使用 1 秒钟的音频片段，采样长度为 16,000。
- en: We first convert the `audios` list into `ndarray` for the `encode` method, which
    has a shape of (4, 16000), because we have 4 samples of 16,000 samples each. The
    `encode` method from `magenta.models.nsynth.wavenet` returns the encodings for
    the provided audio clips. The returned encodings has a shape of (4, 31,16), with
    a length of 4 representing the number of elements, 31 representing the time, and
    16 representing the size of the latent vector, *z*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将 `audios` 列表转换为 `ndarray`，然后传递给 `encode` 方法，形状为 (4, 16000)，因为我们有 4 个样本，每个样本
    16,000 个采样点。来自 `magenta.models.nsynth.wavenet` 的 `encode` 方法返回提供的音频片段的编码。返回的编码形状为
    (4, 31, 16)，其中 4 表示元素数量，31 表示时间，16 表示潜在向量的大小，*z*。
- en: You might be wondering why we have a length of 31 for the time in `encodings`.
    Remember our model reduces every 512 samples to 16 (see the *Looking at NSynth
    and WaveNet autoencoders* section), but our number of samples, 16,000, is not
    divisible by 512, so we end up with 31.25. This also impacts the decoding, which
    will result in WAV files of 0.992 seconds long.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么 `encodings` 中的时间长度为 31。记住，我们的模型将每 512 个采样点减少为 16 个（参见 *Looking at
    NSynth and WaveNet autoencoders* 部分），但我们的采样数 16,000 并不能被 512 整除，所以最后得到的是 31.25。这也会影响解码，导致生成的
    WAV 文件长度为 0.992 秒。
- en: Another important point here is that all the encodings are calculated at once
    in the same batch (the batch size is defined in the encode method by taking `audios.shape[0]`),
    which will be faster than doing them one by one.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的重要点是，所有编码都是一次性计算的，采用相同的批量（批量大小通过在 `encode` 方法中取 `audios.shape[0]` 来定义），这比逐个计算要更快。
- en: Visualizing the encodings
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化编码
- en: 'The encodings can be visualized by plotting them, with the abscissa being the
    time and the ordinate being the encoded value. Each curve in the figure represents
    a z dimension, with 16 different colors. Here is a diagram for each of the encoded
    sounds of our example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 编码可以通过绘制图表进行可视化，横坐标为时间，纵坐标为编码值。图中的每一条曲线代表一个 z 维度，具有 16 种不同的颜色。以下是我们示例中每个编码声音的示意图：
- en: '![](img/9b853181-8f38-41df-accf-f90fe26b0688.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b853181-8f38-41df-accf-f90fe26b0688.png)'
- en: You can use the `save_encoding_plot` plot method in the `audio_utils.py` file
    from this chapter's code to produce the encodings plot.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用本章代码中的 `audio_utils.py` 文件中的 `save_encoding_plot` 绘图方法来生成编码图。
- en: Saving the encodings for later use
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存编码以便后续使用
- en: Saving and loading the encodings once they have been calculated is a good practice
    since it will speed up your program, even if the longer part of it is still the
    synthesizing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出编码，保存和加载它们是一个好习惯，因为这将加快程序的运行速度，即使程序中较长的部分仍然是合成部分。
- en: You can find this code in the `audio_utils.py` file in the source code of this
    chapter. There are more comments and content in the source code, so you should
    go check it out.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本章的源代码中的 `audio_utils.py` 文件中找到这段代码。源代码中有更多的注释和内容，你可以去查看。
- en: 'To save the encodings, we''re using NumPy `.npy` files as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保存编码，我们使用 NumPy `.npy` 文件，如下所示：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see here we are using the `save` method from the `numpy` module. And
    we''re retrieving the encodings from the files using the `load` method as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们在这里使用了`numpy`模块中的`save`方法。我们通过以下方式使用`load`方法从文件中获取编码：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then use the returned encodings instead of calling `fastgen.encode(...)`.
    Now that we have our encodings ready, we'll see how to mix them together.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用返回的编码，而不是调用`fastgen.encode(...)`。现在我们已经准备好了编码，接下来我们将看到如何将它们混合在一起。
- en: Mixing encodings together by moving in the latent space
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过在潜在空间中移动混合编码
- en: Now that we have the encodings of our sound files, we can mix them. The term
    mixing is common in audio production and usually refers to superposing two sounds
    and adjusting their volume so that both can be heard properly. This is not what
    we are doing here; we are effectively **adding** the sounds together, resulting
    in a new sound that is more than their mere superposition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了音频文件的编码，可以将它们混合在一起。混合这个术语在音频制作中很常见，通常指的是将两种声音叠加并调整音量，使两者都能清晰地听到。但在这里，我们做的不是这种操作；我们实际上是在**相加**这些声音，产生一个新的声音，而不仅仅是它们的简单叠加。
- en: 'Let''s define a `mix_encoding_pairs` method for that purpose:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们定义一个`mix_encoding_pairs`方法：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The important bit is `encoding1 + encoding2 / 2.0`, where we add the encodings
    together, producing a new encoding that we'll later synthesize. In the rest of
    the method, we iterate on the encodings two by two, producing a new encoded mix
    for each pair without computing the mix of a sample with itself, resulting in
    12 elements in the method's return.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关键部分是`encoding1 + encoding2 / 2.0`，在这里我们将两个编码相加，产生一个新的编码，稍后我们会合成这个编码。在方法的其余部分，我们对编码进行两两迭代，为每对编码产生一个新的混合编码，且不计算样本与自身的混合，最终方法返回12个元素。
- en: We also keep the name prefix in the `<encoding-prefix-1>_<encoding-prefix-2>` format to
    better identify them when we save the WAV on disk (we're splitting using the `_`
    character because the samples from Freesound have them to split with the unique
    ID).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还保持名称前缀为`<encoding-prefix-1>_<encoding-prefix-2>`格式，以便在保存WAV文件时更好地识别它们（我们使用`_`字符进行分割，因为Freesound中的样本有这个唯一ID分隔符）。
- en: Finally, we return `ndarray` containing the mixed encodings as well a corresponding
    list of names for the encodings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回包含混合编码的`ndarray`以及与编码对应的名称列表。
- en: Synthesizing the mixed encodings to WAV
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将混合编码合成WAV格式
- en: 'Finally, we''ll now define the `synth` method that takes the encodings and
    turns them into sound:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义`synth`方法，它接收编码并将其转换为声音：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Basically, all that method is doing is calling the `synthesize` method on the
    `magenta.models.nsynth.wavenet.fastgen` module. The `encodings_mix` shape is (12,
    31, 16), where 12 is our `batch_size` (the number of final output audio clips),
    `31` is the time, and `16` is the dimensionality of the latent space.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个方法所做的就是在`magenta.models.nsynth.wavenet.fastgen`模块中调用`synthesize`方法。`encodings_mix`的形状为(12,
    31, 16)，其中12是我们的`batch_size`（最终输出音频片段的数量），`31`是时间，`16`是潜在空间的维度。
- en: 'To understand what the `synthesize` method is doing, take a look at this excerpt:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解`synthesize`方法的作用，看看这个摘录：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, `total_length` is 15,872, just short of our 16,000 sample length for a
    time of 1 second, because the length is calculated by multiplying the time (31)
    by the hop length (512). See the information box in the previous section, *Encoding
    the WAV files*, for more information. This will result in audio files that won't
    be exactly 1 second long.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`total_length`是15,872，略低于我们设定的16,000样本长度（对应1秒的时间），因为长度是通过将时间（31）乘以步幅长度（512）来计算的。有关更多信息，请参见前一部分“*编码WAV文件*”的信息框。这将导致音频文件的时长不会完全为1秒。
- en: 'The other thing to notice here is that the process **generates one sample at
    a time**. This might seem inefficient, and that''s because it is: the model is
    really good at reconstructing audio, but also painfully slow. You can see here
    that the bulk of the operation is executed in a serial loop in Python, not in
    parallel on the GPU.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的地方是这个过程**一次生成一个样本**。这可能看起来效率低下，实际上确实如此：模型在重建音频方面非常出色，但速度却极慢。你可以看到，这个过程的大部分操作是在Python中以串行方式执行的，而不是在GPU上并行执行的。
- en: In the following section, *Using GANSynth as a generative instrument*, we look
    at a similar, but faster, model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分“*将GANSynth用作生成乐器*”中，我们将介绍一个类似但速度更快的模型。
- en: Putting it all together
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 把所有东西组合起来
- en: Now that we have our three methods, `encode`, `mix`, and `synth`, we can call
    them to create new sounds and textures.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了三个方法：`encode`、`mix`和`synth`，我们可以调用它们来创造新的声音和质感。
- en: Preparing the audio clips
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备音频剪辑
- en: For this example, we provided some audio clips in the `sounds` folder that you
    can use. While we recommend you experiment with your own sound, you can test your
    method first with those and experiment with yours later.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们在`sounds`文件夹中提供了一些音频剪辑供您使用。虽然我们建议您尝试使用自己的声音，但您可以先用这些进行测试，稍后再尝试您自己的方法。
- en: 'There are many places you can find audio clips from:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从许多地方找到音频剪辑：
- en: Make your own! It is as easy as opening your mic and hitting a plate with a
    stick (see the following list for how to record with Audacity).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作您自己的音频！只需打开麦克风，用棍子敲击盘子即可（请参阅以下列表，了解如何使用Audacity录制）。
- en: The Freesound website, [freesound.org](https://freesound.org/), is an amazing
    community that's passionate about sharing audio clips. Freesound is a website
    for sharing copyright-free audio clips (most are under CC0 1.0 Universal (CC0
    1.0) Public Domain Dedication).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freesound网站，[freesound.org](https://freesound.org/)，是一个热衷于分享音频剪辑的令人惊叹的社区。Freesound是一个分享无版权音频剪辑的网站（大多数属于CC0
    1.0通用（CC0 1.0）公共领域贡献）。
- en: There's also the NSynth dataset, [magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有NSynth数据集，[magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth)。
- en: You can use any sample you want, but we recommend keeping it short (1 or 2 seconds)
    since this is a time-consuming process.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用任何您想要的样本，但我们建议保持较短（1或2秒），因为这是一个耗时的过程。
- en: Whatever source you choose, having a simple digital audio editor and recording
    application software will help you a lot with cutting, normalizing, and handling
    your sounds. As stated in the introduction, Audacity is amazing open source cross-platform
    (Windows, Linux, and macOS) software for that purpose.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪个来源，拥有简单的数字音频编辑器和录音应用程序软件都将帮助您大大简化切割、归一化和处理声音。正如介绍中所述，Audacity是一个出色的开源跨平台（Windows、Linux和macOS）软件。
- en: 'For example, if you downloaded your audio clips from Freesound, they might
    not all be the same length and volume, or they might be badly aligned. Audacity
    is easy to use for such tasks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您从Freesound下载了音频剪辑，它们的长度和音量可能不一致，或者它们可能对齐不良。Audacity非常适合处理这类任务：
- en: '![](img/316c1906-b98a-4f74-9a75-12751fe7931b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/316c1906-b98a-4f74-9a75-12751fe7931b.png)'
- en: 'In this screenshot, we see each line corresponds to an audio clip. All of them
    are cropped to 1 second, ready for our example. Here are some pointers for using
    Audacity proficiently:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个截图中，我们看到每一行对应一个音频剪辑。它们都被裁剪为1秒钟，准备用于我们的示例。以下是熟练使用Audacity的一些提示：
- en: To **record** your own sounds, click first on the **Click to Start Monitoring**
    option. If you see red bars, you're good. Then, click on the red record button
    on the left.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要**录制**你自己的声音，请首先点击**点击开始监控**选项。如果你看到红条，就表示一切正常。然后，点击左侧的红色录制按钮。
- en: To **cut** your recording, use the **Selection Tool** (*F1*) at the top, select
    a part, and press the *Delete* key to remove that part. You can use the audio
    position at the bottom for a precise selection of 1 second, for example.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要**剪切**您的录音，请在顶部使用**选择工具**（*F1*），选择一个部分，然后按下*删除*键删除该部分。您可以使用底部的音频位置来精确选择1秒钟的部分。
- en: To **shift** the content of the audio (move a sharp noise to the beginning of
    the clip, for example), use the **Time Shift Tool** (*F5*) at the top, select
    a part, and drag and drop the part you want to move.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要**调整**音频内容（例如将尖锐的噪音移到剪辑的开头），请在顶部使用**时间移动工具**（*F5*），选择一个部分，然后拖放您想要移动的部分。
- en: You will want your tracks to be in **mono** for this chapter (a single channel
    instead of two channels). If you see two wave lines for a single file, your audio
    is in stereo. On the left, click on the filename. In the dropdown menu, use **Split
    stereo track**, then remove the left or right track. You'll also need to put the
    track panning in the center between **L** and **R**.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于本章，您将希望将音轨设置为**单声道**（而不是两个声道）。如果您看到单个文件有两条波形线，则说明您的音频是立体声的。在左侧，单击文件名。在下拉菜单中，使用**分离立体声轨道**，然后删除左侧或右侧轨道。您还需要将轨道声道设置在**L**和**R**之间的中心位置。
- en: '**Normalizing** is the act of making something louder or quieter without modifying
    the content of the audio, which might be useful if you have a sample that doesn''t
    have the proper volume. To do that, select the whole track and use **Effect**
    > **Normalize**, then change the maximum amplitude to what you want.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**是在不修改音频内容的情况下使音量更大或更小的操作，这在您的样本音量不合适时可能很有用。要执行此操作，请选择整个轨道，然后使用**效果**
    > **归一化**，然后将最大振幅更改为您想要的值。'
- en: To **export** in WAV, using the **File > Export > Export as WAV** menu. You
    can use the **Solo** button on the left if you have multiple tracks because if
    you don't, they'll be mixed together.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要以WAV格式**导出**，请使用**文件 > 导出 > 导出为WAV**菜单。如果你有多个轨道，可以使用左侧的**独奏**按钮，因为如果不使用，它们会混合在一起。
- en: Now that we know how to produce our audio clips, let's write the code to use
    them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何生成音频片段了，接下来让我们编写代码来使用它们。
- en: Generating new instruments
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成新的乐器
- en: We are now ready to render audio clips by mixing the pairs of audio clips. This
    part is an expensive process, depending on the speed of your PC and whether you
    have a GPU or not; this might vary greatly. At the time of writing, a moderately
    powerful i7 laptop will take 20 minutes to compute all of the 12 samples, and
    a PC with an entry-level GPU such as an NVIDIA RTX 2060 will do it in 4 minutes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备通过混合音频片段的配对来渲染音频片段。这个过程相对耗时，取决于你的电脑速度以及是否有GPU；这可能差异很大。编写本文时，一台中等性能的i7笔记本电脑需要20分钟来计算所有12个样本，而一台配有入门级GPU（例如NVIDIA
    RTX 2060）的PC只需4分钟。
- en: You can start by taking only two samples from `WAV_FILENAMES` if you find the
    generation takes too long. We'll see in a later section, *Using GANSynth as a
    generative instrument*, that there are faster alternatives.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现生成过程太长，可以从`WAV_FILENAMES`中仅选择两个样本开始。稍后我们将在*使用GANSynth作为生成乐器*一节中看到有更快的替代方法。
- en: 'Finally, let''s call our `encode`, `mix`, and `synth` methods:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们调用我们的`encode`、`mix`和`synth`方法：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you decide to use your own sound, make sure they are in the `sounds` folder.
    Also, you can prefix the sound filenames with an identifier before an underscore
    character; the resulting clips will keep those identifier pairs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定使用自己的声音，确保它们位于`sounds`文件夹中。此外，你可以在声音文件名和下划线字符之间加上一个标识符；生成的音频片段将保留这些标识符对。
- en: The resulting output will sit in the `output/nsynth` folder as WAV files; you
    should have one per unique input pair, so 12 WAV clips if you used 4 input clips.
    Go ahead and listen to them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出将作为WAV文件保存在`output/nsynth`文件夹中；每一对唯一的输入对应该有一个文件，如果你使用了4个输入片段，应该会有12个WAV片段。去听一听它们吧。
- en: Visualizing and listening to our results
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化和聆听我们的结果
- en: Now that we have our clips generated, we can also look at the rainbowgrams.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了片段，我们也可以查看彩虹图了。
- en: You can find the code for the spectrograms and rainbowgrams in the `audio_utils.py` file in
    the source code of this chapter. There are more comments and content in the source
    code, so you should go check it out.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本章的源代码中的`audio_utils.py`文件中找到频谱图和彩虹图的代码。源代码中有更多的注释和内容，你应该去查看一下。
- en: 'To generate the rainbowgrams for all of the generated audio files for our example,
    let''s call the `save_rainbowgram_plot` method:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要为我们示例中的所有生成音频文件生成彩虹图，让我们调用`save_rainbowgram_plot`方法：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code outputs the following plots in `output/nsynth/plots`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将在`output/nsynth/plots`中输出以下图形：
- en: '![](img/a94ef627-1121-47b8-98d8-6ccd8724a1e9.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a94ef627-1121-47b8-98d8-6ccd8724a1e9.png)'
- en: 'There are some things to note about the generated audio files and their corresponding
    spectrograms:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成的音频文件及其对应的频谱图，有几点需要注意：
- en: First, the three metal-generated sounds on the left are interesting because
    they show that the generated sound kept the note's envelope since the original
    metal sound had a strong attack. The generated audio sounds like something is
    struck, like the original, but now with better harmonics.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，左边的三种金属生成声音很有趣，因为它们展示了生成的声音保留了音符的包络，因为原始的金属声音有很强的攻击性。生成的音频听起来像是有什么东西被击打了，类似于原始的声音，但现在的谐波更好。
- en: Then, the three cat generated sounds are also interesting. In them, the cute
    cat meow becomes an otherworldly growl. Since the NSynth model was trained on
    instrument notes and the cat sound is so different, the model has to guess, which
    results in an interesting sound. Experiment with sounds outside the training dataset,
    such as percussion; it's interesting to see what the model comes up with.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，三种猫咪生成的声音也很有趣。在这些声音中，萌萌的猫咪叫声变成了外星般的咆哮。由于NSynth模型是在乐器音符上训练的，而猫咪的声音差异如此之大，模型不得不进行猜测，这就产生了一个有趣的声音。尝试使用训练数据集外的声音，比如打击乐；看看模型会生成什么很有趣。
- en: In some of the clips, such as the flute + bass clip, we can hear some clicks
    in the generated audio. This happens when the model samples an extreme value and
    then corrects itself.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些音频片段中，比如长笛+低音片段，我们可以听到生成的音频中有一些点击声。这是因为模型采样了一个极端值后进行自我修正时发生的。
- en: You should try and experiment with different sound combinations and durations.
    We've been using rather short samples to speed up the process, but you can use
    samples as long as you want. Just remember that the NSynth dataset contains only
    single notes that are 4 seconds long, meaning the generation of multiple consequent
    notes for longer samples will result in the model guessing the transition between
    them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该尝试并实验不同的声音组合和持续时间。我们使用的是较短的样本，以加快处理速度，但你可以使用任意长度的样本。只需要记住，NSynth数据集中仅包含4秒长的单个音符，意味着生成多个连续音符的长样本将导致模型猜测它们之间的过渡。
- en: Using NSynth generated samples as instrument notes
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NSynth生成的样本作为乐器音符
- en: Now that we have a bunch of generated samples from NSynth, we can sequence them
    using MIDI. The easiest way to do this is to use a **Digital Audio Workstation**
    (**DAW**). Since this requires writing specific code to make Magenta send MIDI
    to the DAW, we'll be dedicating a section on this topic in [Chapter 9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml),
    *Making Magenta Interact with Music Applications*. If you want to try that now,
    you can skip ahead and return here later.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从NSynth生成了一些样本，我们可以使用MIDI对它们进行排序。最简单的方式是使用**数字音频工作站**（**DAW**）。由于这需要编写特定代码让Magenta将MIDI发送到DAW，因此我们将在[第9章](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml)中专门讨论这个话题，*让Magenta与音乐应用互动*。如果你现在想尝试，可以跳到后面，再回来查看这里的内容。
- en: Using the command line
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用命令行
- en: 'The command line is limited for NSynth, but you can still generate audio clips.
    If you haven''t done already, you''ll need to download and extract the checkpoint
    in the `checkpoints/wavenet-ckpt` folder. While in this chapter''s code, use the
    following command to generate audio from the audio clips in the `sounds` folder
    (warning: this process takes a long time):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行对于NSynth的使用有一定的限制，但你仍然可以生成音频片段。如果你还没有这么做，你需要下载并解压`checkpoints/wavenet-ckpt`文件夹中的检查点。在本章的代码中，使用以下命令从`sounds`文件夹中的音频片段生成音频（警告：这个过程会耗时较长）：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By using `batch_size=4` and `sample_length=16000`, you make sure that code runs
    as fast as possible. The resulting files will be in the `output/nsynth` folder,
    with names in the `gen_FILENAME.wav` format, where `FILENAME` is the source filename.
    You'll see a generated audio clip for each source sound, resulting in four audio
    clips.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`batch_size=4`和`sample_length=16000`，你确保代码运行尽可能快。生成的文件将保存在`output/nsynth`文件夹中，文件名格式为`gen_FILENAME.wav`，其中`FILENAME`是源文件名。你将看到每个源声音生成一个音频片段，共四个音频片段。
- en: 'The generated clips were produced by encoding the audio and then synthesizing
    it. Compare them with the original audio: it will give you a feel of the NSynth
    sound.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的音频片段是通过对音频进行编码后再合成的。将它们与原始音频进行比较：这将帮助你感受NSynth的声音特性。
- en: More of NSynth
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关于NSynth的内容
- en: There is more to NSynth than we've shown here, such as more advanced use of
    interpolation and mixing, time stretching, and more. NSynth has produced interesting
    projects, such as a mobile application (mSynth) and physical hardware (NSynth
    Super). Refer to the *Further reading* section for more information on NSynth.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: NSynth的功能远不止我们展示的这些，还包括更高级的插值和混合使用、时间拉伸等。NSynth已经产生了一些有趣的项目，比如移动应用（mSynth）和物理硬件（NSynth
    Super）。更多关于NSynth的信息，请参考*进一步阅读*部分。
- en: Using GANSynth as a generative instrument
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GANSynth作为生成乐器
- en: In the previous section, we used NSynth to generate new sound samples by combining
    existing sounds. You may have noticed that the audio synthesis process is very
    time-consuming. This is because autoregressive models, such as WaveNet, focus
    on a single audio sample, which makes the resulting reconstruction of the waveform
    really slow because it has to process them iteratively.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们使用NSynth通过组合现有的声音生成了新的声音样本。你可能已经注意到，音频合成过程非常耗时。这是因为自回归模型（如WaveNet）专注于单个音频样本，这使得波形的重建过程非常缓慢，因为它必须迭代地处理这些样本。
- en: GANSynth, on the other hand, uses upsampling convolutions, making the training
    and generation processing in parallel possible for the entire audio sample. This
    is a major advantage over autoregressive models such as NSynth since those algorithms
    tend to be I/O bound on GPU hardware.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GANSynth使用上采样卷积，使得整个音频样本的训练和生成过程可以并行进行。这是它相对于自回归模型（如NSynth）的一个重大优势，因为这些算法在GPU硬件上往往会受到I/O限制。
- en: 'The results of GANSynth are impressive:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: GANSynth的结果令人印象深刻：
- en: '**Training** on the NSynth dataset converges in ~3-4 days on a single V100
    GPU. For comparison, the NSynth WaveNet model converges in 10 days on 32 K40 GPUs.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个V100 GPU上，NSynth数据集的**训练**需要大约3-4天时间才能收敛。相比之下，NSynth WaveNet模型在32个K40 GPU上需要10天才能收敛。
- en: '**Synthesizing** a 4-second audio sample takes 20 milliseconds in GANSynth
    on a TitanX GPU. For comparison, the WaveNet baseline takes 1,077 seconds, which
    is 50,000 times slower.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TitanX GPU上，通过GANSynth合成4秒音频样本只需要20毫秒。相比之下，WaveNet基线需要1,077秒，慢了50,000倍。
- en: Another important implication of GANs is that the model has a spherical Gaussian
    prior, which is decoded to produce the entire sound, making the interpolations
    between two samples smoother and without additional artifacts, unlike WaveNet
    interpolation. This is because WaveNet autoencoders such as NSynth have limited
    scope when learning local latent codes that control generation on the scale of
    milliseconds.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的另一个重要含义是，该模型具有球形高斯先验，解码后产生整个声音，使得两个样本之间的插值更加平滑，没有额外的工件，不像WaveNet插值那样。这是因为像NSynth这样的WaveNet自编码器在学习控制毫秒级生成的局部潜在编码时，有限的范围。
- en: In this section, we'll be making use of GANSynth to generate a 30-second audio
    clip by taking a MIDI file and playing it using random instruments sampled from
    the model's latent space. Each instrument will be played for a limited amount
    of time over the course of the audio track, for example, for 5 seconds each, blending
    between one another when the instrument changes happen.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用GANSynth生成一个30秒的音频片段，通过从模型的潜在空间中随机选择乐器样本，并在音频轨道中的一段有限时间内播放每个乐器，例如每个乐器播放5秒，并在乐器变换时进行混合。
- en: Choosing the acoustic model
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择声学模型
- en: Magenta provides two pre-trained GANSynth models: `acoustic_only`, where the
    model was trained only on acoustic instruments, and  `all_instruments`, where
    the model was trained on the whole NSynth dataset (see the previous section, *The
    NSynth dataset*, for more information on the dataset).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Magenta提供了两个预训练的GANSynth模型：`acoustic_only`，该模型仅在声学乐器上进行训练，以及`all_instruments`，该模型在整个NSynth数据集上进行训练（有关数据集的更多信息，请参见上一节，*NSynth数据集*）。
- en: We're going to use the `acoustic_only` dataset for our example since the resulting
    audio track of a Bach score will sound more natural in terms of instrument choices.
    If you want to produce a wider generation, use the `all_instruments` model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在示例中使用`acoustic_only`数据集，因为对于巴赫乐谱的生成，生成的音频轨道在乐器选择方面会更自然。如果您想生成更广泛的音频生成，请使用`all_instruments`模型。
- en: You can follow this example in the `chapter_05_example_02.py` file in the source
    code of this chapter. There are more comments and content in the source code,
    so you should go check it out.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章的源代码`chapter_05_example_02.py`文件中查看此示例。源代码中有更多的注释和内容，所以您应该去查看一下。
- en: This chapter also contains a MIDI clip in the `midi` folder that we'll use for
    this section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还在`midi`文件夹中包含一个MIDI片段，我们将在本节中使用它。
- en: 'To download the model, use the following method, which will download the model
    and extract it:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载模型，请使用以下方法，该方法将下载并提取模型：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using the `acoustic_only` checkpoint name, the resulting checkpoint will be
    usable using the `checkpoints/acoustic_only` path.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`acoustic_only`检查点名称，生成的检查点可通过`checkpoints/acoustic_only`路径使用。
- en: Getting the notes information
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取音符信息
- en: To start this example, we'll be loading a MIDI file that will serve as the backing
    score for the audio generation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始这个示例，我们将加载一个MIDI文件，作为音频生成的背景乐谱。
- en: 'First, we load the MIDI file using the `load_midi` method in the `magenta.models.gansynth.lib.generate_util`
    module:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`magenta.models.gansynth.lib.generate_util`模块中的`load_midi`方法加载MIDI文件：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We've provided a MIDI file in the `midi` folder, but you can also provide a
    MIDI file you like, for example, a generation from a previous chapter. The `load_midi`
    method then returns a dictionary of information about the notes in the MIDI file,
    such as a list of pitches, velocities, and start and end times.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`midi`文件夹中提供了一个MIDI文件，但您也可以提供自己喜欢的MIDI文件，例如从前几章生成的文件。然后，`load_midi`方法返回关于MIDI文件中音符的信息字典，例如音调、速度以及开始和结束时间的列表。
- en: 'The provided `cs1-1pre-short.mid` file looks like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的`cs1-1pre-short.mid`文件如下所示：
- en: '![](img/a83f379a-bade-42e6-968f-ecc1ede516b4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a83f379a-bade-42e6-968f-ecc1ede516b4.png)'
- en: You can see the MIDI file is 28 seconds long (14 bars at 120 QPM) and contains
    two instruments.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到MIDI文件的长度为28秒（每分钟120拍，共14小节），包含两种乐器。
- en: Gradually sampling from the latent space
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从潜在空间逐步采样
- en: Now that we have the information about the MIDI file (in the `notes` variable),
    we can generate the audio from it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有MIDI文件的信息（在`notes`变量中），我们可以从中生成音频。
- en: 'Let''s define the `generate_audio` method:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义`generate_audio`方法：
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method has four important parts, which we'll explain in the following three
    subsections—getting the random instrument, getting the latent vectors, generating
    the samples from the latent vectors, then combining the notes into a full audio
    clip.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法有四个重要部分，我们将在接下来的三个子节中解释——获取随机乐器、获取潜在向量、从潜在向量生成样本，然后将音符组合成完整的音频片段。
- en: Generating random instruments
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成随机乐器
- en: 'The `get_random_instruments` method from `magenta.models.gansynth.lib.generate_util`
    looks like this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`magenta.models.gansynth.lib.generate_util`中的`get_random_instruments`方法如下所示：'
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using a sample of 28 seconds with 5 seconds per instrument gives us `n_instruments`
    of `5`, then the latent vectors get initialized by the model''s `generate_z` method, which
    is a sampling of the normal distribution:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用28秒的示例，每个乐器5秒钟，得到的`n_instruments`为`5`，然后模型通过`generate_z`方法初始化潜在向量，这是从正态分布中采样的：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This results in a `z_instruments` shape of (5, 256), 5 being the number of instruments
    and 256 the size of the latent vector. Finally, we take five steps of equal distance
    between the start and end time of the sequence in `t_instruments`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这会得到一个形状为(5, 256)的`z_instruments`，5表示乐器的数量，256表示潜在向量的大小。最后，我们在`t_instruments`的开始和结束时间之间采取五个相等的步长。
- en: Getting the latent vectors
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取潜在向量
- en: 'The `get_z_notes` method from `magenta.models.gansynth.lib.generate_util` looks
    like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`magenta.models.gansynth.lib.generate_util`中的`get_z_notes`方法如下所示：'
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This method takes each start note times and finds which instrument (previous
    instrument, `t_left`, and the next instrument, `t_right`) should be used for it.
    It then finds at what position the note is between the two instruments, in `interp`,
    to call the `slerp` method, which will find the proper latent vector that corresponds
    to the instrument between the two nearest vectors. This enables smooth transition
    from one instrument to the other.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法获取每个起始音符的时间，并查找应该使用哪个乐器（前一个乐器，`t_left`，以及下一个乐器，`t_right`）。然后，它查找音符在这两个乐器之间的位置，在`interp`中调用`slerp`方法，该方法会找到对应于这两个最接近向量之间的乐器的潜在向量。这使得从一个乐器平滑过渡到另一个乐器成为可能。
- en: Generating the samples from the encoding
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从编码中生成样本
- en: 'We won''t be looking into the details of the `generate_samples_from_z` method
    from `magenta.models.gansynth.lib.model`. We''ll just use this code snippet to
    illustrate what we introduced in the *Using GANSynth as a generative instrument *section about
    the model generating the audio clip as a whole:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨`magenta.models.gansynth.lib.model`中的`generate_samples_from_z`方法的细节。我们只使用这段代码来说明我们在*使用GANSynth作为生成乐器*部分中介绍的内容，即模型整体生成音频片段：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For our example, this method will iterate 27 times to process all of the labels
    by chunks of 8 `labels` and `z` at the same time (our `batch_size` is 8). The
    bigger the batch size, the more waves it can generate in parallel. You can see
    that, contrary to NSynth, the audio samples are not generated one by one.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，该方法将迭代27次，以每次处理8个`labels`和`z`的块（我们的`batch_size`为8）。批量大小越大，可以并行生成更多的波形。可以看到，与NSynth不同，音频样本并不是逐个生成的。
- en: Finally, once all of the audio chunks are generated, `combine_notes` from the
    `magenta.models.gansynth.lib.generate_util` module will generate the audio using
    the audio clip and the MIDI notes. Basically, what the method does is calculate
    an envelope for each MIDI note that will let the proper portion of the audio clip
    be heard when a note is triggered.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦所有音频片段生成完毕，`magenta.models.gansynth.lib.generate_util`模块中的`combine_notes`将利用音频片段和MIDI音符生成音频。基本上，该方法计算每个MIDI音符的包络线，当音符触发时，能听到音频片段的正确部分。
- en: Putting it all together
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有部分整合起来
- en: 'Now that we''ve defined and explained the different parts of the code, let''s
    call the corresponding method to generate the audio clip from the MIDI file using
    gradually interpolated instruments:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义并解释了代码的不同部分，接下来我们调用相应的方法，从MIDI文件生成音频片段，使用逐步插值的乐器：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The generated rainbowgram looks like this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的彩虹图如下所示：
- en: '![](img/43de3426-6e39-4415-a519-21020de7f544.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43de3426-6e39-4415-a519-21020de7f544.png)'
- en: The diagram doesn't tell us much about the sound, apart from seeing the progression
    of the notes in the whole audio clip. Go and listen to the generated clip. Multiple
    generations will introduce different instruments; make sure you test it multiple
    times and with multiple MIDI files to get a feel of the possibilities in terms
    of instrument generation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表没有给我们提供太多关于声音的信息，除了看到整个音频片段中音符的进展。去听一听生成的片段。多次生成将引入不同的乐器；确保多次测试并使用多个 MIDI
    文件，以便体验乐器生成的各种可能性。
- en: Using the command line
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用命令行
- en: 'The GANSynth command-line utility makes it possible to generate an audio clip
    from a MIDI file, like we''ve done with the Python code. If you haven''t done
    already, you''ll need to download and extract the checkpoint into the `checkpoints/wavenet-ckpt` folder. While
    in this chapter''s code, use the following command and an audio clip from the
    MIDI file from the `midi` folder (warning: this process takes a long time):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: GANSynth 命令行工具使得从 MIDI 文件生成音频片段成为可能，就像我们在 Python 代码中所做的那样。如果你还没有完成此操作，你需要下载并解压检查点到
    `checkpoints/wavenet-ckpt` 文件夹中。在本章的代码中，使用以下命令和 `midi` 文件夹中的 MIDI 文件生成音频片段（警告：此过程需要较长时间）：
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The resulting file will be in the `output/gansynth` folder and will be called
    `generated_clip.wav`. As in our example, the generated clip contains multiple
    instruments that are gradually blending together. You can use the `secs_per_instrument`
    parameter to change the time each instrument will play.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文件将位于 `output/gansynth` 文件夹中，并命名为 `generated_clip.wav`。像我们示例中的生成片段一样，生成的片段包含多个乐器，它们逐渐融合在一起。你可以使用
    `secs_per_instrument` 参数来调整每个乐器播放的时间。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we looked at audio generation using two models, NSynth and
    GANSynth, and produced many audio clips by interpolating samples and generating
    new instruments. We started by explaining what WaveNet models are and why they
    are used in audio generation, particularly in text-to-speech applications. We
    also introduced WaveNet autoencoders, an encoder and decoder network capable of
    learning its own temporal embedding. We talked about audio visualization using
    the reduced dimension of the latent space in rainbowgrams.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用两个模型（NSynth 和 GANSynth）进行音频生成，并通过插值样本和生成新乐器生成了许多音频片段。我们从解释 WaveNet
    模型是什么以及为什么它们在音频生成中（尤其是在语音合成应用中）被使用开始。我们还介绍了 WaveNet 自编码器，它是一种能够学习自身时间嵌入的编码器和解码器网络。我们讨论了如何使用降低维度的潜在空间进行音频可视化，并展示了彩虹图的应用。
- en: Then, we showed the NSynth dataset and the NSynth neural instrument. By showing
    an example of combining pairs of sounds, we learned how to mix two different encodings
    together in order to then synthesize the result into new sounds. Finally, we looked
    at the GANSynth model, a more performant model for audio generation. We showed
    the example of generating random instruments and smoothly interpolating between
    them.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们展示了 NSynth 数据集和 NSynth 神经网络乐器。通过展示组合不同声音的示例，我们学会了如何将两种不同的编码混合在一起，从而合成出新的声音。最后，我们看到了
    GANSynth 模型，它是一个性能更强的音频生成模型。我们展示了生成随机乐器并在它们之间平滑插值的示例。
- en: This chapter marks the end of the music generation content of this book—you
    can now generate a full song using MIDI as the backing score and neural instruments
    as the audio. During the course of the previous chapters, we've been using pre-trained
    models to show that the models in Magenta are ready to use and quite powerful.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本章标志着本书音乐生成内容的结束——你现在可以使用 MIDI 作为伴奏谱、神经网络乐器作为音频生成完整的歌曲。在前几章中，我们一直在使用预训练模型，展示了
    Magenta 中的模型已准备就绪且功能强大。
- en: Nonetheless, there are many reasons to train your own models, as we'll see in
    the following chapters. In [Chapter 6](1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml),
    *Data Preparation for Training*, we'll be looking into preparing datasets for
    specific genres of music and for specific instruments. In [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml),
    *Training Magenta Models*, we'll be using those datasets to train our own models
    that we can then use to generate new genres and instruments.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，训练你自己的模型仍有很多理由，正如我们将在接下来的章节中看到的那样。在 [第 6 章](1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml)，*训练数据准备*
    中，我们将探讨如何为特定的音乐类型和乐器准备数据集。在 [第 7 章](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml)，*训练
    Magenta 模型* 中，我们将使用这些数据集训练我们自己的模型，然后用它们生成新的音乐类型和乐器。
- en: Questions
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why is generating audio hard?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么生成音频这么难？
- en: What makes the WaveNet autoencoder interesting?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是什么让 WaveNet 自编码器变得有趣？
- en: What are the different colors in a rainbowgram? How many are there?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 彩虹图中有哪些不同的颜色？一共有多少种？
- en: How would you timestretch an audio clip, slowing it down by 2 seconds, using
    NSynth?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用NSynth将音频片段的播放速度降低2秒？
- en: Why is GANSynth faster that NSynth?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么GANSynth比NSynth更快？
- en: What code is required to sample 10 instruments from GANSynth latent space?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要什么代码来从GANSynth潜在空间中采样10种乐器？
- en: Further reading
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '**Audio Signals in Python:** An article on plotting audio signals in Python,
    explaining how to create a CQT plot ([myinspirationinformation.com/uncategorized/audio-signals-in-python/](http://myinspirationinformation.com/uncategorized/audio-signals-in-python/))'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python中的音频信号**：一篇关于在Python中绘制音频信号的文章，解释如何创建CQT图（[myinspirationinformation.com/uncategorized/audio-signals-in-python/](http://myinspirationinformation.com/uncategorized/audio-signals-in-python/)）'
- en: '**Constant-Q transform toolbox for music processing:** A paper (2010) on implementing
    CQTs for music ([www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing](https://www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing))'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Constant-Q变换工具箱用于音乐处理**：一篇关于实现CQT用于音乐处理的论文（2010）（[www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing](https://www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing)）'
- en: '**WaveNet: A generative model for raw audio:** A DeepMind article on WaveNet
    models for raw audio ([deepmind.com/blog/article/wavenet-generative-model-raw-audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio))'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WaveNet：一种用于原始音频的生成模型**：DeepMind关于WaveNet模型在原始音频中的应用的文章（[deepmind.com/blog/article/wavenet-generative-model-raw-audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)）'
- en: '**WaveNet: A Generative Model for Raw Audio:** A WaveNet paper (2016) ([arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499))'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WaveNet：原始音频的生成模型**：一篇关于WaveNet的论文（2016）（[arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)）'
- en: '**SampleRNN**: An article explaining the differences between WaveNet and SampleRNN
    ([deepsound.io/samplernn_first.html](http://deepsound.io/samplernn_first.html))'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SampleRNN**：一篇解释WaveNet与SampleRNN区别的文章（[deepsound.io/samplernn_first.html](http://deepsound.io/samplernn_first.html)）'
- en: '**NSynth: Neural Audio Synthesis:** A Magenta article on the NSynth model ([magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth))'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSynth：神经音频合成**：Magenta关于NSynth模型的文章（[magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth)）'
- en: '**Making a Neural Synthesizer Instrument:** More ideas on sound combinations
    and modifications ([magenta.tensorflow.org/nsynth-instrument](https://magenta.tensorflow.org/nsynth-instrument))'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制作神经合成器乐器**：关于声音组合和修改的更多想法（[magenta.tensorflow.org/nsynth-instrument](https://magenta.tensorflow.org/nsynth-instrument)）'
- en: '**Generate your own sounds with NSynth:** An article on fastgen and examples
    of timestretching and mixing ([magenta.tensorflow.org/nsynth-fastgen](https://magenta.tensorflow.org/nsynth-fastgen))'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用NSynth生成你自己的声音**：一篇关于fastgen的文章，并提供了时伸缩和混音的示例（[magenta.tensorflow.org/nsynth-fastgen](https://magenta.tensorflow.org/nsynth-fastgen)）'
- en: '**Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders:** An NSynth
    paper (2017) ([arxiv.org/abs/1704.01279](https://arxiv.org/abs/1704.01279))'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WaveNet自动编码器的神经音频合成**：一篇关于NSynth的论文（2017）（[arxiv.org/abs/1704.01279](https://arxiv.org/abs/1704.01279)）'
- en: '**GANSynth: Making music with GANs:** Magenta article on GANSynth ([magenta.tensorflow.org/gansynth](https://magenta.tensorflow.org/gansynth))'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GANSynth：用GANs制作音乐**：Magenta关于GANSynth的文章（[magenta.tensorflow.org/gansynth](https://magenta.tensorflow.org/gansynth)）'
- en: '**GANSynth: Adversarial Neural Audio Synthesis:** A GANSynth paper (2019) ([openreview.net/forum?id=H1xQVn09FX](https://openreview.net/forum?id=H1xQVn09FX))'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GANSynth：对抗性神经音频合成**：一篇关于GANSynth的论文（2019）（[openreview.net/forum?id=H1xQVn09FX](https://openreview.net/forum?id=H1xQVn09FX)）'
- en: '**Using NSynth to win the Outside Hacks Music Hackathon 2017:** A Magenta article
    on mSynth ([magenta.tensorflow.org/blog/2017/09/12/outside-hacks/](https://magenta.tensorflow.org/blog/2017/09/12/outside-hacks/))'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用NSynth赢得2017年Outside Hacks音乐黑客马拉松**：Magenta关于mSynth的文章（[magenta.tensorflow.org/blog/2017/09/12/outside-hacks/](https://magenta.tensorflow.org/blog/2017/09/12/outside-hacks/)）'
- en: '**What is NSynth Super?:** An article on NSynth Super, the NSynth hardware
    synthesizer ([nsynthsuper.withgoogle.com/](https://nsynthsuper.withgoogle.com/))'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**什么是NSynth Super？**：一篇关于NSynth Super——NSynth硬件合成器的文章（[nsynthsuper.withgoogle.com/](https://nsynthsuper.withgoogle.com/)）'
