- en: The Next Level in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的下一层级
- en: 'We have almost come to the end of our journey in deep learning with R. This
    chapter is a bit of a mixed bag of topics. We will begin this chapter by revisiting
    an image classification task and building a complete image classification solution
    image files rather than tabular data. We will then move on to explaining transfer
    learning, where you can use an existing model on a new dataset. Next we discuss
    an important consideration in any machine learning project - how will your model
    be used in deployment, that is, production? We will show how to create a REST
    API that allows any programming language to call a deep learning model in R to
    predict on new data. We will then move on to briefly discussing two other deep
    learning topics: Generative Adversarial Networks and reinforcement learning. Finally,
    we will close this chapter and the book by providing a few other resources that
    you may be interested in.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度学习之旅即将结束。本章内容涉及多个主题。我们将从重新回顾一个图像分类任务开始，构建一个完整的图像分类解决方案，使用图像文件而不是表格数据。然后，我们将解释迁移学习，您可以将现有模型应用于新的数据集。接下来，我们讨论在任何机器学习项目中的一个重要考虑因素——您的模型将在部署中如何使用，也就是在生产环境中如何使用？我们将展示如何创建一个
    REST API，允许任何编程语言调用 R 中的深度学习模型对新数据进行预测。然后，我们将简要讨论另外两个深度学习主题：生成对抗网络和强化学习。最后，我们将通过提供一些您可能感兴趣的其他资源，结束本章和本书的内容。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Building a complete image classification solution
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建完整的图像分类解决方案
- en: The ImageNet dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageNet 数据集
- en: Transfer learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Deploying TensorFlow models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 TensorFlow 模型
- en: Generative adversarial networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Reinforcement learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Additional deep learning resources
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他深度学习资源
- en: Image classification models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类模型
- en: We covered image classification in Chapter 5,* Image Classification Using Convolutional
    Neural Networks*. In that chapter, we described convolutional and pooling layers
    that are essential for deep learning tasks involving images. We also built a number
    of models on a simple dataset, the MNIST dataset. Here, we are going to look at
    some advanced topics in image classification. First, we will build a complete
    image classification model using image files as input. We will look at callbacks,
    which are a great aid in building complex deep learning models. A call-back function will
    be used to persist (save) a model to file, which will be loaded back later. We
    then use this model in our next example, which is transfer learning. This is where
    you use some of the layers in a pre-trained model on new data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 5 章中介绍了图像分类，*使用卷积神经网络进行图像分类*。在那一章中，我们描述了对于涉及图像的深度学习任务至关重要的卷积层和池化层。我们还在一个简单的数据集——MNIST
    数据集上构建了多个模型。在这里，我们将探讨一些高级的图像分类主题。首先，我们将构建一个完整的图像分类模型，使用图像文件作为输入。我们将了解回调函数，这是构建复杂深度学习模型时的得力助手。一个回调函数将被用来将模型持久化（保存）到文件中，并在之后加载。接下来，我们将使用这个模型进行迁移学习。在迁移学习中，您会使用预训练模型中的一些层来处理新的数据。
- en: Building a complete image classification solution
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建完整的图像分类解决方案
- en: We have built a few image classification models, but they used the MNIST dataset
    that was loaded from Keras or from CSV files. The data was always in tabular format.
    Obviously that is not how images are stored in most situations. This section looks
    at how to build an image classification model using a collection of image files. The
    first task is to acquire a set of image files. We are going to load the `CIFAR10`
    data that is included in Keras and save the data as image files. We will then
    use those files to build a deep learning model. After this exercise, you will
    know how to create a deep learning image classification task with your own image
    files.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了一些图像分类模型，但它们使用的是从 Keras 或 CSV 文件加载的 MNIST 数据集。数据始终是表格格式的。显然，这不是大多数情况下图像存储的方式。本节内容将讨论如何使用一组图像文件构建图像分类模型。第一个任务是获取一组图像文件。我们将加载包含在
    Keras 中的 `CIFAR10` 数据，并将其保存为图像文件。然后，我们将使用这些文件构建深度学习模型。完成这项练习后，您将学会如何使用自己的图像文件创建深度学习图像分类任务。
- en: The deep learning model in this chapter is not a complex model. The focus is
    to show how the data pipeline for an image classification task is structured.
    We look at how to arrange the image files, how to use data augmentation and how
    callbacks can be used during training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的深度学习模型并不是一个复杂的模型。重点是展示图像分类任务的数据管道结构。我们会看看如何安排图像文件，如何使用数据增强，以及如何在训练过程中使用回调。
- en: Creating the image data
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建图像数据
- en: 'The first step is to create the image files. The code for this section is in
    the `Chapter11/gen_cifar10_data.R` folder. We will load the `CIFAR10` data and
    save the image files in the data directory. The first step is to create the directory
    structure. There are 10 classes in the `CIFAR10` dataset: we will save 8 classes
    for building a model and we will use 2 classes in a later section (*Transfer learning*).
    The following code creates the following directories under `data`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建图像文件。此部分的代码位于 `Chapter11/gen_cifar10_data.R` 文件夹中。我们将加载 `CIFAR10` 数据并将图像文件保存在数据目录中。第一步是创建目录结构。`CIFAR10`
    数据集包含 10 个类别：我们将为构建模型保存 8 个类别，并将在后面的部分中使用 2 个类别（*迁移学习*）。以下代码将在 `data` 下创建以下目录：
- en: '`cifar_10_images`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images`'
- en: '`cifar_10_images/data1`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images/data1`'
- en: '`cifar_10_images/data2`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images/data2`'
- en: '`cifar_10_images/data1/train`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images/data1/train`'
- en: '`cifar_10_images/data1/valid`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images/data1/valid`'
- en: '`cifar_10_images/data2/train`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images/data2/train`'
- en: '`cifar_10_images/data2/valid`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_10_images/data2/valid`'
- en: 'This is the structure that Keras expects image data to be stored in. If you
    use this structure, then the images can be used to train a model in Keras. In
    the first part of the code, we create these directories:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Keras 期望图像数据存储的结构。如果您使用此结构，则可以将图像用于在 Keras 中训练模型。在代码的第一部分，我们创建了这些目录：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Under each of the train and valid directories, a separate directory is used
    for each category. We save the images for 8 classes under the `data1` folder,
    and save the images for 2 classes under the `data2` folder:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个 train 和 valid 目录下，都为每个类别使用一个单独的目录。我们将 8 个类别的图像保存在 `data1` 文件夹下，将 2 个类别的图像保存在
    `data2` 文件夹下：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we have created the directories, the next step is to save the images in
    the correct directories, which we will do in the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了目录，下一步就是将图像保存到正确的目录中，我们将在以下代码中完成此操作：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, as we have done previously, we will do a validation check to ensure
    that our images are correct. Let''s load in 9 images from one category. We want
    to check that the images display correctly and that they are from the same class:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，像之前做的那样，我们将进行验证检查，确保我们的图像是正确的。让我们加载 9 张来自同一类别的图像。我们想检查这些图像是否正确显示，并且它们是否来自同一类别：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following plot:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/eca7b405-c020-4861-8ca1-966e7b3653b0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eca7b405-c020-4861-8ca1-966e7b3653b0.png)'
- en: 'Figure 11.1: Sample CIFAR10 images'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：样本 CIFAR10 图像
- en: This looks good! The images display correctly and we can see that these images
    all appear to be of the same class, which is cars. The images are out of focus,
    but that is because they are only thumbnail images of size 32 x 32.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来不错！图像显示正确，我们可以看到这些图像似乎都属于同一个类别，即汽车。图像有些模糊，但那是因为它们仅是 32 x 32 的缩略图。
- en: Building the deep learning model
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度学习模型
- en: 'Once you have run the script from the preceding section, you should have 40,000
    images for training in the `cifar_10_images/data1/train` directory and 8,000 images
    for validation in the `cifar_10_images/data1/valid` directory. We will train a
    model with this data. The code for this section is in the `Chapter11/build_cifar10_model.R`
    folder. The first section creates the model definition, which should be familiar
    to you:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你运行了前面部分的脚本，你应该会在 `cifar_10_images/data1/train` 目录中拥有 40,000 张训练图像，在 `cifar_10_images/data1/valid`
    目录中拥有 8,000 张验证图像。我们将使用这些数据训练一个模型。此部分的代码位于 `Chapter11/build_cifar10_model.R` 文件夹中。第一部分创建了模型定义，您应该已经很熟悉：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The model definition was adapted from the VGG16 architecture, which we will
    see later. I used a smaller number of blocks and fewer nodes. Note that the final
    dense layer must have 8 nodes, because there are only 8, not 10 classes in the
    `data1` folder.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义是基于 VGG16 架构进行修改的，我们将在后面看到它。我使用了更少的块和节点。请注意，最终的密集层必须有 8 个节点，因为 `data1` 文件夹中只有
    8 个类别，而不是 10 个。
- en: 'The next part sets up a data generator; the purpose of this is to load batches
    of image files into the model as it is being trained. We can also apply data augmentation
    to the train dataset in the data generator. We will select to create artificial
    data by randomly flipping the images horizontally, shifting images horizontally/vertically,
    and rotating the images by up to 15 degrees. We saw in Chapter 6, *Tuning and
    Optimizing Models,* that data augmentation can significantly improve existing
    models:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分设置了数据生成器；其目的是在模型训练时将图像文件批次加载到模型中。我们还可以在数据生成器中对训练数据集应用数据增强。我们将选择通过随机水平翻转图像、水平/垂直平移图像和旋转图像最多15度来创建人工数据。我们在第6章
    *模型调优与优化* 中看到，数据增强可以显著改善现有模型：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once the data generators have been set up, we will also use two callback functions.
    Callback functions allow you to run custom code after a specific number of batches
    / epochs have executed. You can write your own callbacks or use some predefined
    callback functions. Previously, we used callbacks for logging metrics, but here
    the callbacks will implement model checkpointing and early stopping, which are
    are often used when building complex deep learning models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据生成器设置完毕，我们还将使用两个回调函数。回调函数允许你在执行特定数量的批次/周期后运行自定义代码。你可以编写自己的回调函数，或者使用一些预定义的回调函数。之前我们使用回调函数来记录指标，但在这里，回调函数将实现模型检查点和提前停止，这些通常在构建复杂深度学习模型时使用。
- en: Model checkpointing is used to save the model weights to disk. You can then
    load the model from disk into memory and use it for predicting new data, or you
    can continue training the model from the point it was saved to disk. You can save
    the weights after every epoch, this might be useful if you are using cloud resources
    and are worried about the machine terminating suddenly. Here, we use it to keep
    the best model we have seen so far in training. After every epoch, it checks the
    validation loss, and if it is lower than the validation loss in the existing file,
    it saves the model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 模型检查点用于将模型权重保存到磁盘。然后，你可以从磁盘加载模型到内存中，使用它来预测新的数据，或者你可以从保存到磁盘的地方继续训练模型。你可以在每个训练周期后保存权重，这在使用云资源并担心机器突然终止时可能非常有用。在这里，我们用它来保存到目前为止在训练过程中看到的最佳模型。每个训练周期后，它会检查验证损失，如果验证损失低于现有文件中的验证损失，则保存模型。
- en: 'Early stopping allows you to stop training a model when the performance no
    longer improves. Some people refer to it as a form of regularization because early
    stopping can prevent a model from overfitting. While it can avoid overfitting,
    it works very differently to the regularization techniques, such as L1, L2, weight
    decay, and dropout, that we saw in [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)*,
    Deep Learning Fundamentals*. When using early stopping, you usually would allow
    the model to continue for a few epochs even if performance is no longer improving,
    the number of epochs allowed before stopping training is known as *patience* in
    Keras. Here we set it to 10, that is, if we have 10 epochs where the model has
    failed to improve, we stop training. Here is the code to create the callbacks
    that we will use in our model:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止允许你在模型的性能不再提高时停止训练。有些人称之为一种正则化方法，因为提前停止可以防止模型过拟合。尽管它可以避免过拟合，但它与我们在[第3章](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)*深度学习基础*中看到的正则化技术（如L1、L2、权重衰减和丢弃法）非常不同。使用提前停止时，你通常会允许模型继续训练几个周期，即使性能不再提升，停止训练前允许的周期数在Keras中被称为*耐心*。在这里，我们将其设置为10，也就是说，如果模型在10个周期内没有提升，我们将停止训练。以下是我们将在模型中使用的回调函数代码：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the code to train the model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练模型的代码：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: One thing to note here is that we have to manage the steps per epoch for the
    train and validation generators. When you set up a generator, you don't know how
    much data is actually there, so we need to set the number of steps for each epoch.
    This is simply the number of records divided by the batch size.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点需要注意的是，我们必须管理训练和验证生成器的每个周期的步数。当你设置一个生成器时，你并不知道实际有多少数据，所以我们需要为每个周期设置步数。这只是记录数除以批次大小的结果。
- en: 'This model should take less than an hour to train on a GPU and significantly
    longer if training on a CPU. As the model is training, the best model is saved
    in `cifar_model.h5`. The best result on my machine was after epoch 64, when the
    validation accuracy was about 0.80\. The model continued to train for another
    10 epochs after this but failed to improve. Here is a plot of the training metrics:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在 GPU 上训练应少于一小时，而在 CPU 上训练则会显著更长。随着模型的训练，最佳模型会保存在`cifar_model.h5`文件中。我机器上最佳的结果是在第
    64 轮时，验证准确率约为 0.80。此后，模型继续训练了另外 10 个轮次，但未能提升性能。以下是训练指标的图表：
- en: '![](img/649c0029-c744-46ad-a55b-c07d56855380.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/649c0029-c744-46ad-a55b-c07d56855380.png)'
- en: 'Figure 11.2: Output metrics during model training'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：模型训练期间的输出指标
- en: Using the saved deep learning model
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用已保存的深度学习模型
- en: 'Now that we have built our deep learning model, we can restart R and reload
    the model from disk. The code for this section is in the `Chapter11/use_cifar10_model.R`
    folder. We will load the model that was created in the previous section by using
    the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了深度学习模型，可以重新启动 R 并从磁盘重新加载模型。本节的代码位于`Chapter11/use_cifar10_model.R`文件夹中。我们将使用以下代码加载上一节中创建的模型：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will use this model to generate a prediction for an image file from the
    validation set. We will pick the first directory in the validation folder and
    then pick the 7th file from that folder. We load the image and apply the same
    preprocessing to it as we did when preprocessing the images during training, which
    is to normalize the data by dividing the pixel values by 255.0\. Here is the code
    that loads the image and generates the prediction:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用该模型为验证集中的图像文件生成预测。我们将选择验证文件夹中的第一个目录，然后从该文件夹中选择第7个文件。我们加载图像并对其进行与训练期间相同的预处理，即通过将像素值除以
    255.0 来对数据进行归一化处理。以下是加载图像并生成预测的代码：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The model predicts that the input is from the first class with 99.7% certainty.
    Since we chose the first directory in the validation set, the prediction is correct.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测输入来自第一类，且置信度为 99.7%。由于我们选择了验证集中的第一个目录，预测是正确的。
- en: 'The final thing we will do with our model is to evaluate it on a directory
    of image files. We will also show how to generate predictions for an entire directory
    of image files. This code loads the images from the directories using data generators,
    similar to how we trained the model. Here is the code that evaluates and predicts
    categories by using the model for the validation images we saved to disk:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用模型做的最后一件事是评估它在图像文件目录上的表现。我们还将展示如何为整个图像文件目录生成预测。该代码通过数据生成器加载来自目录的图像，类似于我们训练模型的方式。以下是评估并使用模型对我们保存到磁盘的验证图像进行类别预测的代码：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The validation accuracy is `80.86%`, which is similar to what we observed during
    model training, this confirms that the model was saved correctly to disk. Here
    is the code that generates predictions for all 8,000 validation images:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 验证准确率为`80.86%`，这与我们在模型训练过程中观察到的相似，确认了模型已正确保存到磁盘。以下是为所有 8,000 个验证图像生成预测的代码：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that our prediction output has 8,000 rows and 8 columns, so for each
    validation image, there is a probability for each category. We can see that the
    sum for each row is 1.0 and that there is usually one class that has a significant
    probability. For example, the model is predicting that the first image is in class
    7 with a probability of 99.9%.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，预测输出有 8,000 行和 8 列，因此对于每个验证图像，每个类别都有一个概率。我们可以看到每行的总和为 1.0，并且通常会有一个类别的概率显著较大。例如，模型预测第一张图像属于第
    7 类，概率为 99.9%。
- en: We have now built a complete image classification solution using image files.
    This template can be reused for other tasks once the image data is stored in the
    same directory structure. If the new task had a different number of categories,
    then all you would need to change is the number of nodes in the last dense layer
    and possibly the softmax activation. However, if you did have a new image classification
    task that involved real-life images, then you probably would get better results
    by using an existing model and using transfer learning. Before I explain how to
    do that, I will provide some background on the ImageNet dataset, which is often
    used to train complex models which are then used in transfer learning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经建立了一个完整的图像分类解决方案，使用图像文件。只要图像数据存储在相同的目录结构中，这个模板就可以重复用于其他任务。如果新的任务有不同数量的类别，那么您只需要更改最后一个全连接层中的节点数，可能还需要修改
    softmax 激活函数。然而，如果您有一个新的图像分类任务，涉及现实生活中的图像，那么使用现有模型并进行迁移学习可能会获得更好的结果。在我解释如何操作之前，我将提供一些关于
    ImageNet 数据集的背景信息，它通常用于训练复杂的模型，然后这些模型可以用于迁移学习。
- en: The ImageNet dataset
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ImageNet 数据集
- en: 'From 2010, an annual image classification competition has been run called the
    **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**). The image
    set consists of over 14 million images that have been labelled with over 1,000
    categories. Without this dataset, there would not be the huge interest in deep
    learning that there is today. It has provided the stimulus for research in deep
    learning through the competition. The models and weights learnt on the Imagenet
    dataset have then been used in thousands of other deep learning models through
    transfer learning. The actual history of ImageNet is an interesting story. The
    following link ([https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/))
    explains how the project originally got little attention, but that changed with
    a number of linked events:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2010 年开始，举办了一年一度的图像分类比赛，名为 **ImageNet 大规模视觉识别挑战**（**ILSVRC**）。该图像集包含超过 1400
    万张已标记了 1000 多个类别的图像。如果没有这个数据集，今天深度学习的巨大关注度将不会出现。它通过竞赛激发了深度学习领域的研究。然后，在 ImageNet
    数据集上学习到的模型和权重通过迁移学习被应用于成千上万的其他深度学习模型。ImageNet 的实际历史是一个有趣的故事。以下链接（[https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/)）解释了这个项目最初并未受到关注，但随着一系列相关事件的发展，情况发生了变化：
- en: The ILSVRC became the benchmark for image classification for researchers.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ILSVRC 成为了研究人员图像分类的基准。
- en: NVIDIA had released libraries that allowed access to **graphical processing
    units** (**GPUs**). GPUs are designed to do massive parallel matrix operations,
    which is exactly what is needed to create deep neural networks.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA 发布了允许访问**图形处理单元**（**GPU**）的库。GPU 设计用于执行大规模并行矩阵运算，而这正是构建深度神经网络所需的。
- en: Geoffrey Hinton, Ilya Sutskever, and Alex Krizhevsky from the University of
    Toronto created a deep convolutional neural network architecture called **AlexNet**
    that won the competition in 2012\. Although this was not the first use of convolutional
    neural networks, their submission beat the next approach by a huge margin.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多伦多大学的Geoffrey Hinton、Ilya Sutskever 和 Alex Krizhevsky 创建了一个名为 **AlexNet** 的深度卷积神经网络架构，并在
    2012 年赢得了比赛。尽管这不是卷积神经网络的首次应用，但他们的提交大大超过了下一个方法。
- en: Researchers noticed that when they trained models using the ImageNet dataset,
    they could use them on other classification tasks. They almost always got much
    better performance from using the ImageNet model and then using transfer learning than
    just training a model from scratch on the original dataset.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究人员注意到，当他们使用 ImageNet 数据集训练模型时，能够将其应用于其他分类任务。他们几乎总是发现，使用 ImageNet 模型并进行迁移学习，比从头开始在原始数据集上训练模型要获得更好的性能。
- en: 'The advances in image classification can be tracked with some notable entries
    in the ILSVRC competition:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的进展可以通过 ILSVRC 竞赛中的一些重要条目进行追踪：
- en: '| **Team** | **Year** | **Error rate** |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **团队** | **年份** | **错误率** |'
- en: '| 2011 ILSVRC winner (not deep learning) | 2011 | 25.8% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2011 年 ILSVRC 冠军（非深度学习） | 2011 | 25.8% |'
- en: '| AlexNet (7 layers) | 2012 | 15.3% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet（7 层） | 2012 | 15.3% |'
- en: '| VGG Net (16 layers) | 2014 | 7.32% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| VGG Net（16 层） | 2014 | 7.32% |'
- en: '| GoogLeNet / Inception (19 layers) | 2014 | 6.67% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| GoogLeNet / Inception（19 层） | 2014 | 6.67% |'
- en: '| ResNet (152 layers) | 2015 | 3.57% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ResNet（152 层） | 2015 | 3.57% |'
- en: VGGNet, Inception, and Resnet are all available in Keras. A complete list of
    available networks can be found at [https://keras.rstudio.com/reference/index.html#section-applications](https://keras.rstudio.com/reference/index.html#section-applications).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet、Inception和Resnet都可以在Keras中使用。可以在[https://keras.rstudio.com/reference/index.html#section-applications](https://keras.rstudio.com/reference/index.html#section-applications)找到可用网络的完整列表。
- en: The models for these networks can be loaded in Keras and used to classify a
    new image into one of the 1,000 categories in ImageNet. We will look at this next.
    If you have a new classification task with a different set of images, then you
    can also use these networks and then use transfer learning, which we will look
    at later in this chapter. The number of categories can be different; you do not
    need to have 1,000 categories for your task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络的模型可以在Keras中加载，并用来将新图像分类到ImageNet的1,000个类别之一。我们接下来会讨论这个问题。如果你有一个新的分类任务，并且使用不同的图像集，你也可以使用这些网络，然后使用迁移学习，我们将在本章后面讨论迁移学习。类别的数量可以不同；你不需要为你的任务拥有1,000个类别。
- en: Perhaps the simplest model to begin with is VGGNet, as it is not that different
    to what we saw in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image
    Classification Using Convolutional Neural Networks*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最简单的模型是VGGNet，因为它与我们在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中看到的*使用卷积神经网络进行图像分类*并没有太大区别。
- en: Loading an existing model
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载现有模型
- en: 'In this section, we will load an existing model (VGGNet) in Keras and use it
    to classify a new image. The code for this section can be found in the `Chapter11/vgg_model.R`.
    We will begin by loading the model and looking at its architecture:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将加载一个现有的模型（VGGNet）并用它来分类一张新图片。本节的代码可以在`Chapter11/vgg_model.R`中找到。我们将从加载模型并查看其架构开始：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This model looks complicated, but when you look at it in detail, there is nothing
    that we haven't seen before. There are two blocks with two convolutional layers,
    which are then followed by a max pooling layer. These are followed by three blocks
    with three convolutional layers, which are then followed by a max pooling layer.
    Finally, we have a flatten layer and three dense layers. The last dense layer
    has 1,000 nodes, which is the number of categories in the ImageNet dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型看起来很复杂，但当你仔细看时，实际上没有什么是我们之前没有见过的。它有两个包含两个卷积层的模块，后面跟着一个最大池化层。然后是三个包含三个卷积层的模块，后面也跟着一个最大池化层。最后，我们有一个扁平层和三个全连接层。最后一个全连接层有1,000个节点，这是ImageNet数据集中类别的数量。
- en: 'Let''s use this model to make a prediction for a new image. This image is a
    bicycle, albeit an unusual one – it is a time trial bicycle:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个模型对一张新图像进行预测。这张图像是一辆自行车，尽管它有些不寻常——它是一辆计时赛自行车：
- en: '![](img/6c765554-eb64-4046-9290-bd8fbd86f830.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c765554-eb64-4046-9290-bd8fbd86f830.jpg)'
- en: 'Figure 11.3: Test image for classification'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：分类测试图像
- en: 'The following code block processes the image into a suitable format to use
    in the VGG model. It loads the image and resizes it to the dimensions of the images
    used to train the model `(224, 224)`. We then have to preprocess the image data
    before calling the `predict` function. Finally, there is a helper function in
    Keras called `imagenet_decode_predictions` that we can use to get the prediction
    categories and the probabilities:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块将图像处理成适合在VGG模型中使用的格式。它加载图像并将其调整为训练模型时使用的图像尺寸`(224, 224)`。然后，我们需要在调用`predict`函数之前对图像数据进行预处理。最后，Keras中有一个名为`imagenet_decode_predictions`的辅助函数，我们可以用它来获取预测类别和概率：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The top prediction is `bicycle-built-for-two` at just over 30%, and the second
    best prediction is `mountain_bike` at 16.5%. ImageNet has a category for a tricycle
    and unicycle (and even a `Model_T` car!), but does not seem to have a category
    for a bicycle, so this prediction is a not a bad result. However, `mountain_bike` is
    probably a more accurate category for this image as it definitely is not a bicycle
    for two people!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的预测是`bicycle-built-for-two`，概率略高于30%，第二好的预测是`mountain_bike`，概率为16.5%。ImageNet有三轮车和独轮车（甚至还有`Model_T`汽车！）的类别，但似乎没有自行车的类别，因此这个预测结果不算差。不过，`mountain_bike`可能是这个图像的更准确类别，因为它显然不是一辆双人自行车！
- en: Transfer learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: One of the few disadvantages deep learning has over traditional machine learning
    is that it requires lots of data. Transfer learning is one way to overcome this,
    by using the weights of a previously trained model (usually trained on ImageNet
    data) and then applying them to a new problem set.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习相较于传统机器学习的一个少数缺点是它需要大量数据。迁移学习是克服这一问题的一种方式，方法是使用一个之前训练好的模型的权重（通常是训练在ImageNet数据上的）并将其应用到新的问题集上。
- en: The ImageNet dataset consists of 15 million images in 1,000 classes. Since we
    can reuse parts of a model that has been trained on this amount of data, it may
    be possible to train the new model with just a few hundred images per category.
    This would depend on the images being somewhat related to the data used in the
    original model. For example, trying to use transfer learning from ImageNet models
    (which is trained on photographs) on data from other domains (for example, satellite
    or medical scans), would be more difficult and would require much more data. Some
    of the concerns we raised in [Chapter 6](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml), *Tuning
    and Optimizing Models*, about different data sources also applies. If the data
    is from a different type of data distribution, for example, mobile images, off-center
    photos, different lighting conditions, and so on, this will also matter. This
    is where creating more synthetic data through data augmentation can make a big
    difference.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet数据集包含1,500万张图片，分为1,000个类别。由于我们可以复用已在如此大量数据上训练过的模型的部分内容，可能只需每个类别几百张图片就能训练新模型。这取决于新数据与原始模型训练数据的相关性。例如，尝试将ImageNet模型（其训练数据为照片）上的迁移学习应用到其他领域的数据（例如，卫星图像或医学扫描）上，将会更加困难，并且需要更多的数据。我们在[第6章](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml)中关于不同数据源的讨论，也同样适用。如果数据来自不同类型的数据分布，例如，移动设备拍摄的图像、偏离中心的照片、不同的光照条件等，这也会产生影响。这时，通过数据增强创建更多合成数据将会起到很大的作用。
- en: 'We will now apply transfer learning using the model we built in the *Building
    the deep learning model* section. Recall that we only used 8/10 of the classes
    in building and evaluating this model. We will now build a new model using transfer
    learning that will differentiate between the 2 remaining classes. The code for
    this section can be found in the `Chapter11/cifar_txr.R` folder:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将应用迁移学习，使用在*构建深度学习模型*部分中构建的模型。回想一下，在构建和评估该模型时，我们仅使用了8/10的类别。现在我们将使用迁移学习构建一个新模型，用于区分剩下的2个类别。本部分的代码可以在`Chapter11/cifar_txr.R`文件夹中找到：
- en: 'We will use the model we built in the previous section and load it using the
    following code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用前一部分中构建的模型，并通过以下代码加载它：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, we will call `trainable_weights` on the model object to get the number
    of trainable layers. This will count all the non-activation layers in our model.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将调用模型对象上的`trainable_weights`来获取可训练层的数量。这将计算模型中所有非激活层的数量。
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, we freeze the early layers in our model. Freezing the layers in a model
    means that the weights will not be updated during back-propagation. We freeze
    the convolutional blocks, but do not freeze the dense layers at the end of the
    model. We use the names we set in the model definition to set the first and last
    layers to freeze.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将冻结模型中的早期层。冻结模型中的层意味着在反向传播过程中这些层的权重不会更新。我们冻结卷积块，但不冻结模型末尾的全连接层。我们使用在模型定义中设置的名称来指定冻结的第一层和最后一层。
- en: 'We then call `trainable_weights` on the model once more to confirm that the
    number changed from the preceding value, `14`, to `6`. Here is the code for freezing the
    layers:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们再次调用模型的`trainable_weights`，以确认数量已从之前的`14`变为`6`。这是冻结层的代码：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will remove the last dense layer and last activation layer from our
    model by calling the `pop_layer` function twice in the following code. We need
    to do this because our new task has 2 classes and not 8:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过在以下代码中调用`pop_layer`函数两次，从模型中移除最后的全连接层和激活层。我们需要这么做，因为我们的新任务有2个类别，而不是8个：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can add a new layer with 2 nodes (because we have 2 classes in the new
    task) by using the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下代码添加一个包含2个节点的新层（因为在新任务中我们有2个类别）：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following code block compiles the model again and sets up the generators
    to load the data. This is similar to what we saw when we built the model. One
    difference is that we do not use data augmentation here:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块会重新编译模型并设置生成器以加载数据。这与我们在构建模型时所看到的类似。不同之处在于这里没有使用数据增强：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can train the model by using the following code:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过以下代码训练模型：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The best accuracy was on epoch 9 when we got `96.55%` accuracy. This is significantly
    better than what we got on the multi-classification model (approximately 81%),
    but binary classification tasks are much easier than multi-classification tasks.
    We can also see that the model was very quick to train, because it only had to
    update the weights in the last few layers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的准确率出现在第 9 个训练周期（epoch），当时我们得到了 `96.55%` 的准确率。这比我们在多分类模型中得到的准确率（大约 81%）要高得多，但二分类任务比多分类任务要容易得多。我们还可以看到，模型的训练速度非常快，因为它只需要更新最后几层的权重。
- en: Deploying TensorFlow models
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 TensorFlow 模型
- en: Historically, one of the perceived disadvantages of using R for data science
    projects was the difficulty in deploying machine learning models built in R. This often
    meant that companies used R mainly as a prototyping tool to build models which
    were then rewritten in another language, such as Java and .NET. It is also one
    of the main reasons cited for companies switching to Python for data science as
    Python has more *glue code*, which allows it to interface with other programming
    languages.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，使用 R 进行数据科学项目的一个被认为的缺点是，部署在 R 中构建的机器学习模型的难度。这通常意味着公司主要将 R 用作原型设计工具，先构建模型，然后将其用
    Java 或 .NET 等其他语言重写。这也是公司转向使用 Python 进行数据科学的主要原因之一，因为 Python 具有更多的*粘合代码*，使得它可以与其他编程语言进行接口对接。
- en: Thankfully, this is changing. One interesting new product from RStudio, called RStudio Connect,
    allows companies to create a platform for sharing R-Shiny applications, reports
    in R Markdown, dashboards, and models. This allows companies to serve machine
    learning models using a REST interface.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这种情况正在发生变化。RStudio 发布了一个名为 RStudio Connect 的有趣新产品，它允许公司创建一个平台，用于共享 R-Shiny
    应用程序、R Markdown 报告、仪表盘和模型。这使得公司能够通过 REST 接口提供机器学习模型。
- en: The TensorFlow (and Keras) models we have created in this book can be deployed
    without any runtime dependency on either R or Python. One way of doing this is TensorFlow
    Serving, which is an open source software library for serving TensorFlow models.
    Another option is to use the Google CloudML interface that we saw in [Chapter
    10](2ea4d422-70f7-47af-a330-f0901f6f5fd3.xhtml)*, Running Deep Learning Models
    in the Cloud*. This allows you to create a publicly available REST API that can
    be called from your applications. TensorFlow models can also be deployed to iPhones
    and Android mobile phones.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中创建的 TensorFlow（以及 Keras）模型可以部署，而不依赖于 R 或 Python 的任何运行时环境。一种方式是使用 TensorFlow
    Serving，它是一个开源软件库，用于为 TensorFlow 模型提供服务。另一种选择是使用我们在[第 10 章](2ea4d422-70f7-47af-a330-f0901f6f5fd3.xhtml)*“在云端运行深度学习模型”*中看到的
    Google CloudML 接口。这使你可以创建一个公开的 REST API，供你的应用程序调用。TensorFlow 模型也可以部署到 iPhone 和
    Android 手机上。
- en: 'There are two basic options for scoring models in production:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中有两种基本的评分模型选项：
- en: '**Batch mode**: In batch mode, a set of data is scored offline and the prediction
    results are stored and used elsewhere'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理模式**：在批处理模式下，一组数据在离线状态下进行评分，预测结果会被存储并在其他地方使用。'
- en: '**Real-time mode**: In real-time mode, the data is scored immediately, usually
    a record at a time, and the results are immediately used.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时模式**：在实时模式下，数据会立即进行评分，通常是一次处理一条记录，并且结果会立即使用。'
- en: For a lot of applications, batch mode is more than adequate. You should carefully consider
    if you really need a real-time prediction system as it is requires more resources
    and needs constant monitoring. It is much more efficient to score records in a
    batch rather than individually. Another advantage of batch mode is that you know
    the demand on the application beforehand and can plan resources accordingly. With
    real-time systems, a spike in demand or a denial of service attack can cause problems
    with your prediction model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用程序来说，批处理模式已经足够。你应该仔细考虑自己是否真的需要实时预测系统，因为它需要更多的资源并且需要持续监控。批量处理记录比分别处理要高效得多。批处理模式的另一个优势是你事先知道应用程序的需求，可以相应地规划资源。而实时系统中，需求激增或拒绝服务攻击可能会导致预测模型出现问题。
- en: 'We have already seen batch mode for a saved model in the *Using the saved deep
    learning model* section in this chapter. So, let''s look at how we can build a
    REST interface to get a prediction on new data from a deep learning model in real-time.
    This will use the `tfdeploy` package. The code for this section can be found in
    the `Chapter11/deploy_model.R`. We are going to build a simple model based on
    the MNIST dataset and then create a web interface where we can submit a new image
    for classification. Here is the first part of the code that builds the model and
    prints out the predictions for the first 5 rows in the test set:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在本章的*使用已保存的深度学习模型*部分看到过批量模式的应用。因此，让我们看看如何构建一个REST接口，用于实时从深度学习模型获取新数据的预测。这将使用`tfdeploy`包。本节的代码可以在`Chapter11/deploy_model.R`中找到。我们将基于MNIST数据集构建一个简单的模型，然后创建一个web接口，我们可以在其中提交一张新的图片进行分类。以下是构建模型并打印测试集前5行预测结果的代码第一部分：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'There is nothing new about this code. Next, we will create a JSON file for
    one image file in the test set. JSON stands for JavaScript Object Notation, and
    is the accepted standard for serializing and sending data over a network connection.
    If HTML is the language for computers-to-human web communication, JSON is the
    language for computers-to-computers web communication. It is heavily used in microservice
    architecture, which is a framework for building a complex web ecosystem from lots
    of small web services. The data in the JSON file must have the same preprocessing
    applied as what was done during training – since we normalized the training data,
    we must also normalize the test data. The following code creates a JSON file with
    the values for the first instance in the testset and saves the file to `json_image.json`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码没有什么新颖之处。接下来，我们将为测试集中的一张图片文件创建一个JSON文件。JSON代表JavaScript对象表示法，是用于序列化和通过网络连接发送数据的公认标准。如果HTML是计算机与人类之间的网页通信语言，那么JSON则是计算机之间的网页通信语言。它在微服务架构中被广泛使用，微服务架构是一种通过许多小型web服务构建复杂web生态系统的框架。JSON文件中的数据必须应用与训练过程中相同的预处理方式——因为我们对训练数据进行了归一化处理，因此也必须对测试数据进行归一化处理。以下代码会创建一个包含测试集第一条数据值的JSON文件，并将文件保存为`json_image.json`：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now that we have a JSON file, let''s create a REST web interface for our model:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个JSON文件，让我们为我们的模型创建一个REST web接口：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once you do this, a new web page should pop up that is similar to the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，应该会弹出一个类似于以下内容的新网页：
- en: '![](img/a936938e-a122-4b20-ba84-3ca27b978e20.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a936938e-a122-4b20-ba84-3ca27b978e20.png)'
- en: 'Figure 11.4: Swagger UI for the TensorFlow model REST web service'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：TensorFlow模型REST web服务的Swagger UI
- en: 'This is a Swagger UI page showing the RESTful web services for the TensorFlow
    model. This allows us to test our API. While we could try to use this interface,
    it is easier to use the JSON file we just created. Open up Command Prompt on your
    machine, browse to the `Chapter11` code directory, and run the following command:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个显示TensorFlow模型RESTful web服务的Swagger UI页面。这允许我们测试API。虽然我们可以尝试使用这个接口，但使用我们刚刚创建的JSON文件会更为简便。打开你机器上的命令提示符，浏览到`Chapter11`代码目录，并运行以下命令：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should get the following response:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下响应：
- en: '![](img/67891d57-0caf-4506-956a-8f71e9589c86.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67891d57-0caf-4506-956a-8f71e9589c86.png)'
- en: The REST web interface returns another JSON string with these results. We can
    see that the 8th entry in the list is 1.0 and that all the other numbers are extremely
    small. This matches the prediction for the first row that we saw in the code at
    the start of this section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: REST web接口会返回另一个包含这些结果的JSON字符串。我们可以看到列表中的第8项为1.0，其他所有数字都非常小。这与我们在本节开始时在代码中看到的第一行的预测结果相匹配。
- en: I imagine that half of the people reading this are very excited about this and
    the other half couldn't care less! The half that really like this can see how
    R can be used to serve model predictions that interface with web applications.
    This opens up huge possibilities for using R, where beforehand it was believed
    that you either had to use Python or you had to redevelop models in other languages.
    The half that couldn't care less probably never had to deal with these issues
    with R, but in time they will see how important this is as well!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我想阅读这篇文章的人一半会非常激动，另一半则毫不关心！那一半真正感兴趣的人可以看到如何使用R来提供与web应用程序接口的模型预测。这为使用R打开了巨大的可能性，而之前人们认为你必须使用Python，或者必须用其他语言重新开发模型。那一半不感兴趣的人可能从未与这些R相关问题打过交道，但随着时间的推移，他们也会意识到这有多重要！
- en: Other deep learning topics
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他深度学习话题
- en: Two topics that get a lot of attention in deep learning are **Generative Adversarial
    Networks (GANs)** and reinforcement learning. We only briefly introduce both topics,
    there is no code for this section for a couple of reasons. Firstly both topics
    are very advanced and trying to create a use-case that is non-trivial would require
    a few chapters for each topic. Secondly, reinforcement learning is not well supported
    in R, so creating an example would be difficult. Despite this, I include both
    of these topics in the book because I believe they are important emerging areas
    in deep learning that you should definitely be aware of.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中有两个备受关注的话题，**生成对抗网络（GANs）**和强化学习。我们这里只简要介绍这两个话题，本节没有代码，原因有两个。首先，这两个话题都非常高级，想要创建一个非平凡的应用场景需要为每个话题写上几章。其次，强化学习在R中的支持不够好，因此创建一个示例会很困难。尽管如此，我还是将这两个话题纳入本书，因为我认为它们是深度学习中重要的前沿领域，你绝对应该了解它们。
- en: Generative adversarial networks
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Generative Adversarial Networks have been called *the coolest thing since sliced
    bread* by Yann LeCunn, one of the most prominent people in deep learning. If he
    believes that, then we should all take notice!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络被深度学习领域最杰出的人物之一Yann LeCunn称为*自从面包被切片以来最酷的东西*。如果他这么认为，那我们都应该引起注意！
- en: Most of our models in this book have been discriminative models, that is, we
    try to differentiate one class from another. However in [Chapter 9](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml),
    *Anomaly Detection and Recommendation Systems* we created a generative model in
    the anomaly detection use-case. This model could create new data, albeit a different
    representation of the input data. Creating complex generative models is a very
    hot research topic in deep learning. Many believe that generative models can solve
    many problems in deep learning, including one of the biggest, which is the lack
    of correctly labelled data. However before GANs, it was difficult to judge how
    good a generative model actually was. A group of researchers led by Ian Goodfellow
    proposed Generative Adversarial Networks (GANs) (Goodfellow, Ian, et al. *Generative
    adversarial nets.* Advances in neural information processing systems. 2014) that
    could be used to create realistic artificial data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数模型都是判别模型，也就是说，我们试图将一种类别与另一种类别区分开来。然而，在[第9章](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml)《异常检测与推荐系统》中，我们在异常检测的应用场景中创建了一个生成模型。这个模型可以生成新的数据，尽管是输入数据的不同表示形式。创建复杂的生成模型是深度学习中的一个非常热门的研究话题。许多人认为生成模型能够解决深度学习中的许多问题，其中之一就是缺乏正确标注的数据。然而，在生成对抗网络（GANs）出现之前，很难判断一个生成模型到底有多好。由Ian
    Goodfellow领导的一组研究人员提出了生成对抗网络（GANs）（Goodfellow, Ian, et al. *Generative adversarial
    nets.* Advances in neural information processing systems. 2014），它可以用来生成逼真的人工数据。
- en: In GANs, two models are trained together, the first is a generative model G
    that creates new data. The second model is a discriminative model D that tries
    to predict if an example is from the real dataset, or has been created by the
    generative model G. The basic GAN idea is for the generative model to try to fool the
    discriminative model, while the discriminative model must try to tell the differences
    from fake data and real data. The generator keeps creating new data and refining
    its process until the discriminative model can no longer tell the difference between
    the generated data and the real training data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在GANs中，两个模型一起训练，第一个是生成模型G，它生成新的数据。第二个模型是判别模型D，它尝试预测一个示例是否来自真实数据集，或者是由生成模型G生成的。基本的GAN思想是，生成模型试图“欺骗”判别模型，而判别模型则必须尝试区分假数据和真实数据。生成器不断生成新数据并改进其过程，直到判别模型无法再区分生成的数据和真实的训练数据。
- en: In the paper, the process is compared to a team of counterfeiters creating fake
    currency (the generative model) and the police who are trying to detect the counterfeit
    currency (the discriminative model). Both models improve incrementally until it
    is impossible to differentiate between the counterfeit currency and the real currency.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，这个过程被比作一群伪钞制造者在创造假币（生成模型），以及试图检测伪币的警察（判别模型）。这两个模型逐步改进，直到无法区分假币和真钱。
- en: 'GANs are notoriously hard to train. One paper that documented a working approach
    to training GANs on image data called their approach deep convolutional generative adversarial
    networks (Radford, Alec, Luke Metz, and Soumith Chintala. *Unsupervised representation
    learning with deep convolutional generative adversarial networks*. arXiv preprint
    arXiv:1511.06434 (2015)). In this paper, they recommended a number of guidelines
    to train stable deep convolutional generative adversarial networks (DCGANs):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的训练通常非常困难。一篇记录了在图像数据上训练GAN的有效方法的论文，将他们的方法称为深度卷积生成对抗网络（Radford, Alec, Luke
    Metz, 和 Soumith Chintala. *无监督表示学习与深度卷积生成对抗网络*。arXiv预印本 arXiv:1511.06434（2015））。在这篇论文中，他们推荐了一些训练稳定深度卷积生成对抗网络（DCGAN）的指导原则：
- en: Replace any pooling layers with strided convolutions (discriminator) and fractional-strided
    convolutions (generator).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有池化层替换为步幅卷积（判别器）和分数步幅卷积（生成器）。
- en: Use batchnorm for both models.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对两个模型都使用批归一化（batchnorm）。
- en: Remove fully connected hidden layers for deep architectures.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除深度架构中的全连接隐藏层。
- en: For the generator, use tanh activation in the output layer and ReLU elsewhere.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成器来说，在输出层使用tanh激活函数，其他地方使用ReLU。
- en: For the discriminator, use LeakyReLU activation for all layers.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于判别器，所有层使用LeakyReLU激活函数。
- en: 'Training DCGANs is an iterative process, the following steps are repeated:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 训练DCGAN是一个迭代过程，以下步骤会重复进行：
- en: First the generator creates some new examples.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，生成器创建一些新的示例。
- en: The discriminator is trained using real data and generated data.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器使用真实数据和生成数据进行训练。
- en: After the discriminator has been trained, both models are trained together.
    The discriminator's weights are frozen, but its gradients are used in the generator
    model so that the generator can update it's weights.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在判别器训练完成后，两个模型一起训练。判别器的权重被冻结，但其梯度会在生成器模型中使用，以便生成器可以更新它的权重。
- en: During this loop, it is vital that one model does not dominate the other model,
    they should both improve together. If the discriminator is too smart and is very
    confident that the instances from the generator are fakes, then there is no signal
    passed back to the generator and it can no longer improve. Similarly, if the generator
    finds a clever trick to fool the discriminator, it may generate images that are
    too similar, or of only one input category and the GAN again fails to improve.
    This shows the difficulty in training any GAN's, you have to find a set of parameters
    that works for the data and keeps two models synchronized. A good reference from
    one of the authors of the DCGAN paper on advice to make GANs work is [https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，至关重要的是一个模型不能主导另一个模型，它们应该一起改进。如果判别器过于聪明，并且非常确信生成器的实例是假的，那么就不会有信号传递回生成器，它将无法再改进。同样，如果生成器找到了一个聪明的技巧来欺骗判别器，它可能会生成过于相似的图像，或者仅属于一个输入类别，GAN将再次无法改进。这展示了训练任何GAN的难度，你必须找到一组适用于数据的参数，并保持两个模型同步。来自DCGAN论文作者之一的关于如何使GAN工作的一条良好参考是[https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)。
- en: GANs have many potential use-cases including being able to train with less data.
    They also could be used to predict missing data, e.g. add definition to blurred
    images / videos. They in reinforcement learning, which we discuss in the next
    section.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: GAN有许多潜在的应用场景，包括能够用更少的数据进行训练。它们还可以用来预测缺失的数据，例如给模糊的图像/视频添加定义。它们也可以在强化学习中应用，我们将在下一节中讨论。
- en: Reinforcement learning
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Reinforcement learning has a deceptively simple definition: an agent interacts
    with its environment and changes its behaviors based on the the consequences of
    its actions. This is actually how humans and animals behave in the real world
    and is why many people believe that reinforcement learning is the key to achieving
    artificial general intelligence (AGI).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习有一个看似简单的定义：一个智能体与环境进行交互，并根据其行为的后果改变自己的行为。这实际上就是人类和动物在现实世界中的行为方式，也正是许多人认为强化学习是实现人工通用智能（AGI）的关键原因。
- en: 'Artificial general intelligence (AGI) will be achieved if and when computers
    can perform complex tasks as well as people. This also requires that computers
    be able to adapt their current knowledge to new problems, just as humans do. Experts
    disagree on whether AGI is even possible. If we take the very first image from
    [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)*, Getting Started with
    Deep Learning*, we can see that the definition of artificial intelligence (*...
    performing functions that require intelligence when performed by people*) closely
    resembles the definition of reinforcement learning:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算机能够像人类一样执行复杂任务，那么人工通用智能（AGI）将实现。这也要求计算机能够像人类一样，将当前的知识应用于新问题。专家们对AGI是否可能实现存在分歧。如果我们参考[第1章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)*，《深度学习入门》*中的第一张图片，我们可以看到，人工智能的定义（*...当由人类执行时，表现出需要智能的功能*）与强化学习的定义非常相似：
- en: '![](img/7c6cc2bc-45b6-48ae-ac40-0278d97af008.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c6cc2bc-45b6-48ae-ac40-0278d97af008.png)'
- en: Figure 11.5: The relationship between artificial intelligence, machine learning,
    and deep learning
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：人工智能、机器学习和深度学习之间的关系
- en: 'David Silver, one of the most prominent people in reinforcement learning and
    one of the main people involved in AlphaGo, coined the following formula:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·西尔弗（David Silver），强化学习领域最杰出的人物之一，以及参与AlphaGo的主要人物之一，提出了以下公式：
- en: '*Artificial intelligence = Reinforcement learning + Deep learning*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能 = 强化学习 + 深度学习*'
- en: One well-known example of reinforcement learning is an algorithm that can play
    a number of Atari 2600 video games better than most people by just using image
    pixels as input to the algorithm. The reinforcement learning algorithm learns
    by playing the game thousands, maybe millions of times, and learns what actions
    it needs to take to achieve rewards, which could be to collect points or to keep
    its avatar alive as long as possible. Perhaps the best known example in reinforcement
    learning is AlphaGo, which defeated one of the best players in the world in Go.
    AlphaGo is a hybrid artificial system that was composed of neural networks, reinforcement
    learning, and heuristic search algorithms. It is much harder to program a computer
    to win in a game of Go than other games, such as chess, because brute-force approaches
    are not feasible. An additional problem in Go is the difficulty in evaluating
    the current position.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个著名例子是一个算法，它仅通过将图像像素作为输入，能够比大多数人玩多个Atari 2600视频游戏更好。这个强化学习算法通过玩游戏数千次，甚至可能是数百万次，学习它需要采取什么行动来获得奖励，奖励可能是收集积分或尽可能长时间保持角色的生命。强化学习中最著名的例子或许就是AlphaGo，它击败了世界上最强的围棋选手之一。AlphaGo是一个混合人工系统，由神经网络、强化学习和启发式搜索算法组成。编程让计算机在围棋中获胜比在其他游戏（如国际象棋）中更难，因为暴力破解方法不可行。围棋的另一个问题是评估当前局势的困难。
- en: A formal definition of reinforcement learning is where an agent observes a state
    *s[t]* at timestep *t*. When in state *s[t]*, the agent interacts with its environment
    by taking action, which means that the agent transitions to the new state *s[t+1]*.
    The movement into a new state is linked with a reward, and the goal of the agent
    is to learn a policy that maximizes the expected rewards. The rewards could be
    cumulative and/or discounted; for example, near-time rewards are worth more than
    far-off returns. The value function is the prediction of the future reward. If
    the new state *s[t+1]* is dependent only on the previous state *s[t]* and the
    action *a[t]*, then it becomes a Markov process. However, one of the major problems
    in reinforcement learning is that rewards may be sparse and that there may a long
    delay between an action and achieving the reward. There is also the problem where
    an immediate reward might cause the agent to go down a path that could ultimately
    be destructive. For example, in a computer game, the agent could take an immediate
    step of trying to maximize a score, but this ultimately means that the character
    *dies* sooner. In a more real-life scenario, for example, a self-driving car,
    if the goal is to get to a location quickly, then the agent might decide to drive
    dangerously, putting passengers and other road users at risk.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的正式定义是，代理在时间步长 *t* 观察到状态 *s[t]*。当处于状态 *s[t]* 时，代理通过采取行动与环境进行交互，这意味着代理将过渡到新的状态
    *s[t+1]*。进入新状态与奖励相关联，代理的目标是学习一个能够最大化期望奖励的策略。奖励可以是累积的和/或折扣的；例如，近时间的奖励比远期的回报更有价值。价值函数是对未来奖励的预测。如果新状态
    *s[t+1]* 仅依赖于先前的状态 *s[t]* 和动作 *a[t]*，那么它就成为了马尔可夫过程。然而，强化学习中的一个主要问题是奖励可能是稀疏的，并且可能在采取行动和获得奖励之间存在较长的延迟。还有一个问题是，直接的奖励可能会导致代理走上一条最终会导致破坏的路径。例如，在一款电子游戏中，代理可能会采取直接的步骤来最大化得分，但这最终意味着角色会更早*死亡*。在更接近现实生活的场景中，例如自动驾驶汽车，如果目标是快速到达某个地点，那么代理可能决定采取危险驾驶，进而把乘客和其他道路使用者置于风险之中。
- en: 'The core elements in reinforcement learning include the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的核心元素包括以下内容：
- en: Rewards are the gains that an agent can achieve in the near-term.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是代理在短期内能够获得的收益。
- en: A value function is the expected reward an agent can expect to achieve from
    the current state. The value function looks at the long-term rewards / goals,
    so this may mean taking actions that do not maximize rewards in the short-term.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值函数是代理从当前状态中能够期望获得的奖励。价值函数关注长期的奖励/目标，因此这可能意味着采取一些在短期内不最大化奖励的行动。
- en: A policy guides the actions that an agent can take, it maps the states to possible
    actions from that state.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略指导代理可以采取的行动，它将状态映射到从该状态可能采取的动作。
- en: The model is encapsulation of the environment that the agent interacts with.
    As such it is an incomplete representation of the physical world, but as long
    as it can accurately simulate the next step given an action, and calculate the
    reward, then it is an adequate representation that can be used for reinforcement
    learning.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是代理与环境交互的封装。因此，它是对物理世界的一个不完整表示，但只要它能够在给定某个动作后准确地模拟下一步，并计算奖励，那么它就是一个足以用于强化学习的充分表示。
- en: Other important mechanisms in reinforcement learning include multi-label classification,
    memory, unsupervised learning, knowledge transfer (using knowledge learned from
    one problem to solve related problems), search (to select the next best action
    by looking at all possible permutations *x* moves ahead), multi-agent RL, and
    learning to learn. We will not go into detail on these tasks, some may already
    be familiar to you. However, this list does highlight the complexity involved
    in reinforcement learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的其他重要机制包括多标签分类、记忆、无监督学习、知识转移（使用从一个问题中学到的知识来解决相关问题）、搜索（通过查看所有可能的排列，*x*步后选择最佳的下一步行动）、多代理强化学习（multi-agent
    RL）和学习如何学习（learning to learn）。我们不会深入讨论这些任务，因为其中一些你可能已经熟悉。然而，这个清单确实突显了强化学习所涉及的复杂性。
- en: Deep learning can be used as a component in reinforcement learning to work on
    subtasks, such as object detection, speech recognition, NLP, and so on. Deep learning
    can also be an integral part of reinforcement learning when it is used in the
    key components of reinforcement learning, which are the value function, policy,
    and the environmental model. This is called deep reinforcement learning (deep
    RL). For example, by using recurrent connections between hidden units, Hausknecht
    and Stone built a deep recurrent Q-network (DRQN) that could predict the speed
    of the ball in the computer game **Pong**. Another research area in linking deep
    learning with RL is for imitation learning. In imitation learning, an agent learns
    by observing an *expert*. It is especially useful where there are delayed rewards
    and evaluating the current position is hard. But imitation learning can be costly,
    so one approach is to use GANs to produce artificial data to be used in reinforcement
    learning.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以作为强化学习中的一个组成部分，处理子任务，如物体检测、语音识别、自然语言处理等。深度学习也可以是强化学习的关键组成部分，特别是当它用于强化学习中的核心要素——价值函数、策略和环境模型时。这就是所谓的深度强化学习（deep
    RL）。例如，通过在隐藏单元之间使用递归连接，Hausknecht和Stone建立了一个深度递归Q网络（DRQN），能够预测计算机游戏**Pong**中球的速度。将深度学习与强化学习联系起来的另一个研究领域是模仿学习。在模仿学习中，智能体通过观察*专家*来学习。它尤其在奖励延迟且当前状态难以评估的情况下非常有用。但模仿学习可能代价高昂，因此一种方法是使用生成对抗网络（GANs）生成人工数据，用于强化学习。
- en: Even though AlphaGo managed to beat the world champion in Go, it is nowhere
    near solving the problem of artificial general intelligence. DeepMind are a dedicated
    artificial intelligence company who combined experts in reinforcement learning,
    supervised learning and tree search functions and huge hardware resources to solve
    a single problem. AlphaGo was trained on a dataset of 30 million game states and
    simulated millions of games. The version that beat one of the best players in
    the world in Go used almost 2,000 CPUs and 300 GPUs. Before it could beat the
    world champion, it was coached by the European champion, although the early version
    did beat him first. However, AlphaGo solves only one problem, it cannot even generalize
    to other board games. Therefore, it does not come anywhere near solving artificial
    general intelligence.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AlphaGo成功击败了围棋世界冠军，但它距离解决人工通用智能问题还相差甚远。DeepMind是一个专注于人工智能的公司，汇集了强化学习、监督学习、树搜索功能的专家，以及巨大的硬件资源来解决一个单一问题。AlphaGo的训练数据集包含了3000万个游戏状态，并模拟了数百万场游戏。击败世界顶级围棋选手的版本几乎使用了2000个CPU和300个GPU。它在击败世界冠军之前，曾接受过欧洲冠军的指导，尽管早期版本确实先击败了他。然而，AlphaGo只解决了一个问题，甚至不能推广到其他棋类游戏。因此，它离解决人工通用智能问题还远远不够。
- en: 'One of the more honest appraisals of AlphaGo is from Andrej Karpathy, who is
    a distinguished researcher in deep learning and currently is director of artificial
    intelligence at Tesla. He posted a blog called **AlphaGo, in context** ([https://medium.com/@karpathy/alphago-in-context-c47718cb95a5](https://medium.com/@karpathy/alphago-in-context-c47718cb95a5))
    after AlphaGo defeated the number one ranked player in 2017\. Karpathy listed
    the following limitations of Go compared to other artificial intelligence tasks:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对AlphaGo的较为诚实的评价来自Andrej Karpathy，他是深度学习领域的杰出研究员，现为特斯拉人工智能部门的总监。在AlphaGo于2017年击败世界排名第一的选手后，他发布了一篇名为**AlphaGo,
    in context**的博客([https://medium.com/@karpathy/alphago-in-context-c47718cb95a5](https://medium.com/@karpathy/alphago-in-context-c47718cb95a5))。Karpathy列出了围棋与其他人工智能任务相比的以下局限性：
- en: The game is fully deterministic, that is, rules are fixed and fully known beforehand.
    In comparison, most real-world problems are not
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该游戏是完全确定性的，即规则是固定的，且事先完全已知。相比之下，大多数现实世界的问题并非如此。
- en: The game is fully observable, that is, complete information is known to all
    parties, there are no hidden variables or states.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该游戏是完全可观察的，即所有参与方都知道完整信息，且没有隐藏变量或状态。
- en: The games has a discrete action space, that is, there is a fixed number of allowable
    actions
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该游戏具有离散的动作空间，即有固定数量的可允许动作。
- en: A perfect simulator exists, that is, you can model millions of examples in a
    safe space. Real-life artificial intelligence does not have this.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在完美的模拟器，即可以在安全空间中模拟数百万个例子。现实生活中的人工智能没有这种能力。
- en: The game is relatively short.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该游戏相对较短。
- en: There are historical datasets from previous games
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在历史数据集，来自以往的游戏
- en: If we consider self-driving cars as an artificial intelligence task, it probably
    does not match any of these properties.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将自动驾驶汽车视为一个人工智能任务，那么它可能并不符合这些特性中的任何一个。
- en: One unusual quirk in AlphaGo games with the world champion is that it sometimes
    passed on moves that would have captured board space. As humans, when we play
    games, we sometimes crave immediate feedback and therefore make moves to achieve
    short-term rewards. AlphaGo was programmed to win the game, regardless of the
    margin, so was quite content to pass on making such moves during the games. It
    is interesting that some expert Go players believe that they can improve by studying
    the strategies of AlphaGo. We have come full circle – humans trying to imitate
    the actions of computers, which in turn are modeled on the actions of humans.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo与世界冠军对弈时的一个不寻常的特点是，有时它会跳过那些原本会占据棋盘空间的走法。作为人类，我们在玩游戏时，有时会渴望即时反馈，因此会做出一些走法来获得短期奖励。而AlphaGo被编程为赢得比赛，无论胜负差距如何，因此它在比赛中非常乐意跳过这些走法。令人有趣的是，一些围棋高手认为，通过研究AlphaGo的策略，他们可以提高自己的水平。我们已经走到了一个循环的尽头——人类试图模仿计算机的行为，而计算机的行为反过来又是根据人类的行为模型来设计的。
- en: Additional deep learning resources
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的深度学习资源
- en: This section consists of some recommendations if you wish to continue your development
    in deep learning. My first recommendation is that you ensure that you have run
    all the code examples in this book. You are getting less than 50% of the benefit
    if you just read this book without running the code. Go through the examples,
    change the code to try and beat the results I got, re-write the MXNet code to
    Keras code, and so on.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容包含了一些建议，帮助你继续在深度学习领域的发展。我的第一个建议是确保你已经运行了本书中的所有代码示例。如果你只是读这本书而没有运行代码，你得到的收益将不到50%。去完成这些示例，修改代码，试图超越我得到的结果，将MXNet代码改写为Keras代码，等等。
- en: 'I strongly recommend *Deep Learning Specialization* on Coursera by Andrew Ng
    ([https://www.coursera.org/specializations/deep-learning](https://www.coursera.org/specializations/deep-learning)).
    Unfortunately, it is in Python, but it still is a great resource. Also in Python
    are two excellent courses on fast.ai ([http://www.fast.ai/](http://www.fast.ai/))
    from Jeremy Howard. These two options take opposite approaches: the *Deep Learning*
    specialization on Coursera takes a bottom-up approach that goes from theory to
    practice, and the fast.ai courses show you practical examples from the start and
    only afterwards shows you the theory.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐Andrew Ng在Coursera上的*深度学习专攻*（[https://www.coursera.org/specializations/deep-learning](https://www.coursera.org/specializations/deep-learning)）。不幸的是，它是用Python编写的，但它仍然是一个很好的资源。在Python方面，还有Jeremy
    Howard在fast.ai上的两个优秀课程（[http://www.fast.ai/](http://www.fast.ai/)）。这两个选择采取了截然不同的
    approach：Coursera上的*深度学习*专攻采用的是自下而上的方法，从理论到实践，而fast.ai的课程从一开始就展示实际例子，之后才讲解理论。
- en: Another excellent resource is Kaggle ([https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)).
    Kaggle hosts competitions where data scientists compete to get the best score
    in machine learning competitions. Many of these tasks are computer vision tasks.
    I am not a huge fan of the competitions, because I think that they ignore a lot
    of the work in preparing and acquiring datasets and also ignore how models are
    deployed. However, two notable features in Kaggle are its Kernels and forums/blogs.
    Kernels are Python or R scripts from other people. These scripts often have very
    interesting approaches to machine learning tasks. It is well worth following a
    competition and just looking at how other competitors approach these problems.
    The second notable feature is the forums/blogs on Kaggle. Again, some interesting
    approaches are discussed on the competition forums, and after every competition,
    there's usually a blog post from one of the winning competitions discussing their
    approach in winning the competition.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优秀的资源是Kaggle（[https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)）。Kaggle举办竞赛，数据科学家们在机器学习竞赛中竞相争夺最佳成绩。这些任务中的许多都是计算机视觉任务。我对这些竞赛不是特别感兴趣，因为我认为它们忽略了数据集的准备和获取工作，也忽视了模型的部署方式。然而，Kaggle有两个值得注意的特点：其Kernels和论坛/博客。Kernels是其他人编写的Python或R脚本。这些脚本通常对机器学习任务有非常有趣的处理方式。非常值得关注一个竞赛，看看其他竞争者是如何解决这些问题的。第二个值得注意的特点是Kaggle的论坛/博客。在竞赛论坛上也会讨论一些有趣的方法，在每个竞赛结束后，通常会有一篇获胜者的博客，讨论他们获胜的策略。
- en: 'Going back to R, another fantastic resource is the RStudio website. These guys
    do fantastic work in keeping R relevant to data science and machine learning.
    RStudio put a lot of output back into the R ecosystem; for example, the excellent
    work by Hadley Wickham, their Chief Scientist. The founder of RStudio (J.J. Allaire)
    is the author of the R API''s to TensorFlow and Keras. We have used some of their
    excellent tools in this book, including RStudio IDE, RShiny, RMarkdown, the tidy
    universe of packages, and so on. Here are some links with examples that you should
    check out:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 回到R，另一个非常棒的资源是RStudio网站。这些人做了很多出色的工作，保持R在数据科学和机器学习中的相关性。RStudio把大量成果回馈到R生态系统中；例如，他们的首席科学家Hadley
    Wickham的优秀工作。RStudio的创始人（J.J. Allaire）是TensorFlow和Keras的R API的作者。我们在本书中使用了他们的一些优秀工具，包括RStudio
    IDE、RShiny、RMarkdown、tidy universe包等等。以下是一些包含示例的链接，值得你查看：
- en: '[https://keras.rstudio.com/](https://keras.rstudio.com/)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.rstudio.com/](https://keras.rstudio.com/)'
- en: '[https://keras.rstudio.com/articles/examples/index.html](https://keras.rstudio.com/articles/examples/index.html)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.rstudio.com/articles/examples/index.html](https://keras.rstudio.com/articles/examples/index.html)'
- en: '[https://tensorflow.rstudio.com/](https://tensorflow.rstudio.com/)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/](https://tensorflow.rstudio.com/)'
- en: '[https://tensorflow.rstudio.com/tfestimators/](https://tensorflow.rstudio.com/tfestimators/)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/tfestimators/](https://tensorflow.rstudio.com/tfestimators/)'
- en: '[https://tensorflow.rstudio.com/tensorflow/](https://tensorflow.rstudio.com/tensorflow/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/tensorflow/](https://tensorflow.rstudio.com/tensorflow/)'
- en: '[https://tensorflow.rstudio.com/tools/](https://tensorflow.rstudio.com/tools/)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/tools/](https://tensorflow.rstudio.com/tools/)'
- en: '[https://tensorflow.rstudio.com/learn/resources.html](https://tensorflow.rstudio.com/learn/resources.html)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tensorflow.rstudio.com/learn/resources.html](https://tensorflow.rstudio.com/learn/resources.html)'
- en: 'My final suggestion is looking at research papers. Here are a number of good
    papers to begin with:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我的最终建议是阅读研究论文。以下是一些可以开始的优秀论文：
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. *ImageNet Classification
    with Deep Convolutional Neural Networks*. Advances in neural information processing
    systems. 2012.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky, Alex, Ilya Sutskever, 和 Geoffrey E. Hinton. *使用深度卷积神经网络进行ImageNet分类*.
    神经信息处理系统进展. 2012.
- en: Szegedy, Christian, et al. *Going Deeper with Convolutions*. Cvpr, 2015.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy, Christian, 等人. *深入卷积网络*. Cvpr, 2015.
- en: 'LeCun, Yann, et al. *Learning Algorithms for Classification: A Comparison on
    Handwritten Digit Recognition*. Neural networks: the statistical mechanics perspective
    261 (1995): 276.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann, 等人. *分类学习算法：手写数字识别比较*. 神经网络：统计力学视角 261 (1995): 276.'
- en: Zeiler, Matthew D., and Rob Fergus. *Visualizing and Understanding Convolutional
    Networks*. European conference on computer vision. Springer, Cham, 2014.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler, Matthew D., 和 Rob Fergus. *可视化与理解卷积网络*. 欧洲计算机视觉会议. Springer, Cham, 2014.
- en: 'Srivastava, Nitish, et al. *Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting*. The Journal of Machine Learning Research 15.1 (2014): 1929-1958.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srivastava, Nitish, 等人. *Dropout：防止神经网络过拟合的简单方法*. 机器学习研究期刊 15.1 (2014): 1929-1958.'
- en: Simonyan, Karen, and Andrew Zisserman. *Very deep convolutional networks for
    large-scale image recognition*. arXiv preprint arXiv:1409.1556 (2014).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan, Karen, 和 Andrew Zisserman. *用于大规模图像识别的超深卷积神经网络*. arXiv预印本 arXiv:1409.1556
    (2014).
- en: Szegedy, Christian, et al. *Going deeper with convolutions*. Proceedings of
    the IEEE conference on computer vision and pattern recognition. 2015.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy, Christian, 等人. *深入卷积网络*. IEEE计算机视觉与模式识别会议论文集. 2015.
- en: He, Kaiming, et al. *Deep residual learning for image recognition*. Proceedings
    of the IEEE conference on computer vision and pattern recognition. 2016.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, Kaiming, 等人. *用于图像识别的深度残差学习*. IEEE计算机视觉与模式识别会议论文集. 2016.
- en: Goodfellow, Ian, et al. *Generative adversarial nets*. Advances in neural information
    processing systems. 2014.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, Ian, 等人. *生成对抗网络*. 神经信息处理系统进展. 2014.
- en: 'LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature 521.7553
    (2015): 436.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann, Yoshua Bengio, 和 Geoffrey Hinton. 深度学习. nature 521.7553 (2015):
    436.'
- en: 'Goldberg, Yoav. *A primer on neural network models for natural language processing*. Journal
    of Artificial Intelligence Research 57 (2016): 345-420.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goldberg, Yoav. *自然语言处理中的神经网络模型简介*. 人工智能研究期刊 57 (2016): 345-420.'
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, the reader has seen some advanced deep learning techniques.
    First, we looked at some image classification models and looked at some historical
    models. Next, we loaded an existing model with pre-trained weights into R and
    used it to classify a new image. We looked at transfer learning, which allows
    us to reuse an existing model as a base on which to build a deep learning model
    for new data. We built an image classifier model that could train on image files.
    This model also showed us how to use data augmentation and callbacks, which are
    used in many deep learning models. Finally, we demonstrated how we can build a
    model in R and create a REST endpoint for a prediction API that can be used from
    other applications or across the web.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，读者已经了解了一些先进的深度学习技术。首先，我们研究了一些图像分类模型，并回顾了一些历史模型。接下来，我们将一个已有的、带有预训练权重的模型加载到
    R 中，并使用它来分类一张新的图像。我们还研究了迁移学习，它允许我们将已有的模型作为基础，基于此构建一个用于新数据的深度学习模型。我们构建了一个图像分类器模型，可以在图像文件上进行训练。这个模型还向我们展示了如何使用数据增强和回调，这些技术在许多深度学习模型中都有使用。最后，我们演示了如何在
    R 中构建一个模型，并创建一个 REST 端点来提供预测 API，供其他应用程序或通过网络访问。
- en: We have come to the end of the book, and I really hope it was useful to you.
    R is a great language for data science and I believe it is easier to use and allows
    you to develop machine learning prototypes faster than the main alternative, Python.
    Now that it has support for some excellent deep learning frameworks in MXNet,
    Keras and TensorFlow, I believe that R will continue to be an excellent choice
    for data scientists and machine learning practioners.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本书已经到了结尾，我真的希望它对你有所帮助。R 是一门很适合数据科学的语言，我相信它比主要的替代品 Python 更易于使用，并且能让你更快地开发出机器学习原型。现在，它已经支持
    MXNet、Keras 和 TensorFlow 等一些优秀的深度学习框架，我相信 R 将继续成为数据科学家和机器学习从业者的一个优秀选择。
