- en: Solving Problems with Dynamic Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用动态规划解决问题
- en: The purposes of this chapter are manifold. We will introduce many topics that
    are essential to the understanding of reinforcement problems and the first algorithms
    that are used to solve them. Whereas, in the previous chapters, we talked about
    **reinforcement learning** (**RL**) from a broad and non-technical point of view,
    here, we will formalize this understanding to develop the first algorithms to
    solve a simple game.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是多方面的。我们将介绍许多对理解强化学习问题及其解决算法至关重要的主题。与前几章从广义和非技术角度讨论 **强化学习** (**RL**) 不同，这里我们将正式化这一理解，以开发解决简单游戏的第一批算法。
- en: The RL problem can be formulated as a **Markov decision process** (**MDP**),
    a framework that provides a formalization of the key elements of RL, such as value
    functions and the expected reward. RL algorithms can then be created using these
    mathematical components. They differ from each other by how these components are
    combined and on the assumptions made while designing them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题可以被表述为 **马尔可夫决策过程** (**MDP**)，这是一个提供强化学习关键元素（如价值函数和期望奖励）形式化的框架。然后，可以使用这些数学组件创建强化学习算法。它们之间的不同在于这些组件是如何组合的，以及在设计时所做的假设。
- en: For this reason, as we'll see in this chapter, RL algorithms can be categorized
    into three main categories that can overlap each other. This is because some algorithms
    can unify characteristics from more than one category. Once these pivotal concepts
    have been explained, we'll present the first type of algorithm, called dynamic
    programming, which can solve problems when given complete information about the
    environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，正如我们将在本章中看到的，强化学习算法可以分为三大类，这些类别之间可以相互重叠。这是因为某些算法可以将来自多个类别的特征结合在一起。解释完这些关键概念后，我们将介绍第一种类型的算法，称为动态规划，它可以在获得环境的完全信息时解决问题。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: MDP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDP
- en: Categorizing RL algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习算法分类
- en: Dynamic programming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划
- en: MDP
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MDP
- en: An MDP expresses the problem of sequential decision-making, where actions influence
    the next states and the results. MDPs are general and flexible enough to provide
    a formalization of the problem of learning a goal through interactions, the same
    problem that is addressed with RL. Thus we can express and reason with RL problems
    in terms of MDPs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 表达了一个顺序决策问题，其中动作会影响下一个状态及其结果。MDP 足够通用和灵活，可以为通过交互学习目标的问题提供形式化，这正是强化学习所要解决的问题。因此，我们可以用
    MDP 的语言来表述和推理强化学习问题。
- en: 'An MDP is four-tuple (S,A,P,R):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 是四元组 (S,A,P,R)：
- en: '*S* is the state space, with a finite set of states.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* 是状态空间，包含一个有限的状态集。'
- en: '*A* is the action space, with a finite set of actions.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* 是动作空间，包含一个有限的动作集。'
- en: '*P* is a transition function, which defines the probability of reaching a state,
    *s′*, from *s* through an action, *a*. In *P(s′, s, a) = p(s′| s, a)*, the transition
    function is equal to the conditional probability of *s′* given *s* and *a*.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P* 是转移函数，它定义了通过一个动作 *a* 从状态 *s* 转移到状态 *s′* 的概率。在 *P(s′, s, a) = p(s′| s, a)*
    中，转移函数等于 *s′* 在给定 *s* 和 *a* 下的条件概率。'
- en: '*R* is the reward function, which determines the value received for transitioning
    to state *s′* after taking action *a* from state *s*.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R* 是奖励函数，它决定了在从状态 *s* 采取动作 *a* 后，转移到状态 *s′* 时所获得的值。'
- en: 'An illustration of an MDP is given in the following diagram. The arrows represent
    the transitions between two states, with the transition probabilities attached
    to the tail of the arrows and the rewards on the body of the arrows. For their
    properties, the transition probabilities of a state must add up to 1\. In this
    example, the final state is represented with a square (state *S[5]*) and for simplicity,
    we have represented an MDP with a single action:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了一个 MDP 的示例。箭头表示两个状态之间的转移，箭头尾部附带转移概率，箭头主体则标注奖励。在其性质上，一个状态的转移概率之和必须等于 1。在此示例中，最终状态用一个方框表示（状态
    *S[5]*），为了简便，我们将一个 MDP 表示为只有一个动作的情况：
- en: '![](img/aacec67b-f1bb-4c5b-b003-716c51e2b48e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aacec67b-f1bb-4c5b-b003-716c51e2b48e.png)'
- en: Figure 3.1 Example of an MDP with five states and one action
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 示例：具有五个状态和一个动作的 MDP
- en: The MDP is controlled by a sequence of discrete time steps that create a trajectory
    of states and actions (*S[0], A[0], S[1], A[1], ...*), where the states follow
    the dynamics of the MDP, namely the state transition function, *p(s′|s, a)*. In
    this way, the transition function fully characterizes the environment's dynamics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 由一系列离散的时间步骤控制，这些时间步骤创建了状态和动作的轨迹（*S[0], A[0], S[1], A[1], ...*），其中状态遵循 MDP
    的动态，即状态转移函数 *p(s′|s, a)*。通过这种方式，转移函数完全表征了环境的动态。
- en: By definition, the transition function and the reward function are determined
    only by the current state, and not from the sequence of the previous states visited.
    This property is called the **Markov property**, which means that the process
    is memory-less and the future state depends only on the current one, and not on
    its history. Thus, a state holds all the information. A system with such a property
    is called **fully observable**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，转移函数和奖励函数仅由当前状态决定，而不依赖于访问过的前一个状态序列。这个性质叫做**马尔可夫性质**，意味着该过程是无记忆的，未来状态只依赖于当前状态，而不依赖于其历史。因此，一个状态包含了所有信息。具有这种性质的系统称为**完全可观察的**。
- en: 'In many practical RL cases, the Markov property does not hold up, and for practicality,
    we can get around the problem by assuming it is an MDP and using a finite number
    of previous states (a finite history): *S[t]*, *S[t-1]*, *S[t-2]*, ..., *S*[*t-k*]. In
    this case, the system is **partially observable** and the states are called **observations**.
    We''ll use this strategy in the Atari games, where we''ll use row pixels as the
    input of the agent. This is because the single frame is static and does not carry
    information about the speed or direction of the objects. Instead, these values
    can be retrieved using the previous three or four frames (it is still an approximation).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际的强化学习（RL）案例中，马尔可夫性质并不成立，为了实用，我们可以假设它是一个马尔可夫决策过程（MDP），并使用有限数量的前一个状态（有限历史）：*S[t]*、*S[t-1]*、*S[t-2]*、...、*S*[*t-k*]。在这种情况下，系统是**部分可观察的**，状态称为**观测值**。我们将在
    Atari 游戏中使用这种策略，其中我们将使用行像素作为智能体的输入。这是因为单帧图像是静态的，并不包含关于物体速度或方向的信息。相反，这些值可以通过使用前三到四帧来获取（这仍然是一种近似）。
- en: The final objective of an MDP is to find a policy, π, that maximizes the cumulative
    reward, [![](img/b27acbaf-de95-4dc7-b6e9-bd508392ce23.png)], where *R[π]* is the
    reward obtained at each step by following the policy, π. A solution of an MDP
    is found when a policy takes the best possible action in each state of the MDP.
    This policy is known as the **optimal** **policy**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 的最终目标是找到一个策略，π，最大化累计奖励，[![](img/b27acbaf-de95-4dc7-b6e9-bd508392ce23.png)]，其中
    *R[π]* 是按照策略 π 每一步获得的奖励。当一个策略在每个 MDP 状态中采取最佳可能的动作时，就找到了 MDP 的解。这种策略被称为**最优策略**。
- en: Policy
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: The policy chooses the actions to be taken in a given situation and can be categorized
    as deterministic or stochastic.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 策略选择在给定情况下采取的动作，并可以分为确定性或随机性。
- en: A deterministic policy is denoted as *a[t] = µ(st)*, while a stochastic policy
    can be denoted as *a[t] ~ π(.|s[t])*, where the tilde symbol (~) means **has distribution**.
    Stochastic policies are used when it is better to consider an action distribution;
    for example, when it is preferable to inject a noisy action into the system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性策略表示为 *a[t] = µ(st)*，而随机策略可以表示为 *a[t] ~ π(.|s[t])*，其中波浪号符号（~）表示**有分布**。当考虑一个动作分布时使用随机策略；例如，当需要向系统中注入噪声动作时。
- en: Generally, stochastic policies can be categorical or Gaussian. The former case
    is similar to a classification problem and is computed as a softmax function across
    the categories. In the latter case, the actions are sampled from a Gaussian distribution,
    described by a mean and a standard deviation (or variance). These parameters can
    also be functions of states.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，随机策略可以是分类的或高斯的。前者类似于分类问题，并通过在类别之间应用 softmax 函数进行计算。在后者中，动作是从高斯分布中采样的，通过均值和标准差（或方差）来描述。这些参数也可以是状态的函数。
- en: When using parameterized policies, we'll define them with the letter *θ*. For
    example, in the case of a deterministic policy, it would be written as *µ[θ] (s[t])*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用参数化策略时，我们将用字母 *θ* 来定义它们。例如，在确定性策略的情况下，它可以写作 *µ[θ] (s[t])*。
- en: Policy, decision-maker, and agent are three terms that express the same concept,
    so, in this book, we'll use these terms interchangeably.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 策略、决策者和智能体是表达相同概念的三个术语，因此在本书中，我们将交替使用这些术语。
- en: Return
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回报
- en: 'When running a policy in an MDP, the sequence of state and action (*S[0]*, *A[0]*, *S[1]*, *A[1]*,
    ...) is called **trajectory** or **rollout***,* and is denoted by ![](img/7e13ad77-456c-493a-8343-9ffc8a8bf955.png). In
    each trajectory, a sequence of rewards will be collected as a result of the actions.
    A function of these rewards is called **return** and in its most simplified version,
    it is defined as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP中运行策略时，状态和动作的序列（*S[0]*， *A[0]*， *S[1]*， *A[1]*，...）被称为**轨迹**或**展开**，并用 ![](img/7e13ad77-456c-493a-8343-9ffc8a8bf955.png)表示。在每个轨迹中，动作的结果会收集一系列奖励。这些奖励的函数称为**回报**，在其最简化的版本中，定义如下：
- en: '![](img/263dd699-3fe4-4a42-a6ec-3c5ee28e18e4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/263dd699-3fe4-4a42-a6ec-3c5ee28e18e4.png)'
- en: 'At this point, the return can be analyzed separately for trajectories with
    infinite and finite horizons. This distinction is needed because in the case of
    interactions within an environment that do not terminate, the sum previously presented
    will always have an infinite value. This situation is dangerous because it doesn''t
    provide any information. Such tasks are called continuing tasks and need another
    formulation of the reward. The best solution is to give more weight to the short-term
    rewards while giving less importance to those in the distant future. This is accomplished
    by using a value between 0 and 1 called the **discount factor** denoted with the
    symbol λ*. *Thus, the return **G** can be reformulated as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，回报可以单独分析无限和有限时间范围的轨迹。之所以需要这种区分，是因为在环境中进行的交互如果没有结束，之前展示的和将始终具有无限值的总和。这种情况是危险的，因为它并没有提供任何有用的信息。此类任务被称为持续任务，需要对奖励进行另一种表述。最好的解决方案是对短期奖励赋予更大权重，同时对远期奖励赋予较少的关注。这个过程可以通过使用一个介于0和1之间的值来实现，这个值称为**折扣因子**，通常用符号 λ*表示。因此，回报**G**可以重新表述如下：
- en: '![](img/e72838f1-d23d-4c2d-a455-08853a791dc4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e72838f1-d23d-4c2d-a455-08853a791dc4.png)'
- en: This formula can be viewed as a way to prefer actions that are closer in time
    with respect to those that will be encountered in the distant future. Take this
    example—you win the lottery and you can decide when you would like to collect
    the prize. You would probably prefer to collect it within a few days rather than
    in a few years. ![](img/f64405b5-36bf-47d8-8d63-38e0da4636d5.png) is the value
    that defines how long you are willing to wait to collect the prize. If ![](img/15638a94-11a5-4f72-8968-90fede169fa4.png), that
    means that you are not bothered about when you collect the prize. If [![](img/aa898182-0ce4-47e4-82fc-51be617d1fe8.png)],
    that means that you want it immediately.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式可以看作是倾向于选择那些与远期奖励相较较近的动作的一种方式。举个例子——假设你中了彩票，你可以决定何时领取奖金。你可能更愿意在几天内领取，而不是几年后领取。![](img/f64405b5-36bf-47d8-8d63-38e0da4636d5.png)
    就是定义你愿意等待多长时间领取奖金的值。如果 ![](img/15638a94-11a5-4f72-8968-90fede169fa4.png)，那意味着你不在乎何时领取奖金。如果 [![](img/aa898182-0ce4-47e4-82fc-51be617d1fe8.png)]，那就意味着你希望立即领取。
- en: 'In cases of trajectories with a finite horizon, meaning trajectories with a
    natural ending, tasks are called **episodic** (it derives from the term episode,
    which is another word for trajectory). In episodic tasks, the original formula
    (1) works, but nevertheless, it is preferred to have a variation of it with the
    discount factor:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限时间范围的轨迹中，即有自然结束的轨迹，任务被称为**回合式**任务（这个术语来源于“episode”，即轨迹的另一种说法）。在回合式任务中，原始公式（1）依然适用，但通常更偏好对其进行带有折扣因子的变体处理：
- en: '![](img/a2ced63d-7d0b-4d81-8688-db95ece7a973.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2ced63d-7d0b-4d81-8688-db95ece7a973.png)'
- en: With a finite but long horizon, the use of a discount factor increases the stability
    of algorithms, considering that long future rewards are only partially considered. In
    practice, discount factor values between 0.9 and 0.999 are used.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限但较长的时间范围内，使用折扣因子可以增加算法的稳定性，因为远期奖励仅部分考虑。实际上，折扣因子的值通常在0.9到0.999之间使用。
- en: 'A trivial but very useful decomposition of formula (3) is the definition of
    return in terms of the return at timestep *t + 1*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对公式（3）的一个简单但非常有用的分解是将回报定义为在时间步* t + 1 *的回报：
- en: '![](img/32c5321a-1dee-4421-ad5c-b9c7c7efc629.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32c5321a-1dee-4421-ad5c-b9c7c7efc629.png)'
- en: 'When simplifying the notation, it becomes the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简化符号后，它变为以下形式：
- en: '![](img/c1de6332-c6af-457a-b6cd-f721ea802575.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1de6332-c6af-457a-b6cd-f721ea802575.png)'
- en: Then, using the return notation, we can define the goal of RL to find an optimal
    policy, ![](img/14497f73-ee08-44a6-b1e7-74e633c68357.png), that maximizes the
    expected return as [![](img/36d54163-a38b-424c-84d1-5bb6dccabbec.png)], where [![](img/abb94ffb-0ccf-4754-b31e-eed001662724.png)] is
    the expected value of a random variable.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用回报符号，我们可以将强化学习的目标定义为找到一个最优政策，![](img/14497f73-ee08-44a6-b1e7-74e633c68357.png)，使其最大化预期回报，表示为[![](img/36d54163-a38b-424c-84d1-5bb6dccabbec.png)]，其中[![](img/abb94ffb-0ccf-4754-b31e-eed001662724.png)]是随机变量的期望值。
- en: Value functions
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值函数
- en: 'The return [![](img/e9046fe7-5a5d-46e6-90d1-c519d768a125.png)] provides a good
    insight into the trajectory''s value, but still, it doesn''t give any indication
    of the quality of the single states visited. This quality indicator is important
    because it can be used by the policy to choose the next best action. The policy
    has to just choose the action that will result in the next state with the highest
    quality. The **value function** does exactly this: it estimates the **quality**
    in terms of the expected return from a state following a policy. Formally, the
    value function is defined as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 回报[![](img/e9046fe7-5a5d-46e6-90d1-c519d768a125.png)]提供了关于轨迹值的良好洞察，但仍然没有提供关于单个访问状态质量的任何指示。这个质量指标非常重要，因为它可以被策略用来选择下一个最佳动作。策略只需选择一个能够导致下一个质量最高状态的动作。**值函数**正是这样做的：它根据政策从某一状态开始的预期回报来估计**质量**。形式上，值函数定义如下：
- en: '![](img/803e2f8b-f844-4f77-976a-56019065ffbf.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/803e2f8b-f844-4f77-976a-56019065ffbf.png)'
- en: 'The **action-value function**, similar to the value function, is the expected
    return from a state but is also conditioned on the first action. It is defined
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作值函数**类似于值函数，它是从某个状态出发的预期回报，但也依赖于第一个动作。其定义如下：'
- en: '![](img/6a4ec12a-7d62-4c8c-b3b5-8c19daafaddc.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a4ec12a-7d62-4c8c-b3b5-8c19daafaddc.png)'
- en: 'The value function and action-value function are also called the **V-function**
    and **Q-function **respectively, and are strictly correlated with each other since
    the value function can also be defined in terms of the action-value function:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数和动作值函数分别也被称为**V-函数**和**Q-函数**，它们是严格相关的，因为值函数也可以通过动作值函数来定义：
- en: '![](img/488cf9ca-f963-4c79-a1e5-5523c6c50d75.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/488cf9ca-f963-4c79-a1e5-5523c6c50d75.png)'
- en: 'Knowing the optimal [![](img/8a9c0fe1-da0e-43e5-9e66-e7b81af625a9.png)], the
    optimal value function is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 知道了最优[![](img/8a9c0fe1-da0e-43e5-9e66-e7b81af625a9.png)]，最优值函数如下：
- en: '![](img/538c8450-86a3-4138-a125-112d7dd8b047.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/538c8450-86a3-4138-a125-112d7dd8b047.png)'
- en: That's because the optimal action is [![](img/9e78caff-8401-4f9a-a736-3076e1ec787a.png)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为最优动作是[![](img/9e78caff-8401-4f9a-a736-3076e1ec787a.png)]。
- en: Bellman equation
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: '**V** and **Q** can be estimated by running trajectories that follow the policy, ![](img/28b05043-d213-4654-8fb0-f170fdcdb514.png),
    and then averaging the values obtained. This technique is effective and is used
    in many contexts, but is very expensive considering that the return requires the
    rewards from the full trajectory.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**V**和**Q**可以通过运行遵循策略的轨迹，![](img/28b05043-d213-4654-8fb0-f170fdcdb514.png)，然后对获得的值进行平均来估计。这种技术有效，且在许多场合中得到应用，但考虑到回报需要整个轨迹中的奖励，它是非常昂贵的。'
- en: 'Luckily, the Bellman equation defines the action-value function and the value
    function recursively, enabling their estimations from subsequent states. The Bellman
    equation does that by using the reward obtained in the present state and the value
    of its successor state. We already saw the recursive formulation of the return
    (in formula (5)) and we can apply it to the state value:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，贝尔曼方程递归地定义了动作值函数和值函数，使得它们能够从后续状态中进行估计。贝尔曼方程通过使用当前状态获得的奖励和其后继状态的值来实现这一点。我们已经看到了回报的递归公式（在公式（5）中），并且可以将其应用于状态值：
- en: '![](img/11c116e3-4fc4-42f3-830a-e709ef88eb8e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11c116e3-4fc4-42f3-830a-e709ef88eb8e.png)'
- en: 'Similarly, we can adapt the Bellman equation for the action-value function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以将贝尔曼方程应用于动作值函数：
- en: '![](img/fd00dc43-1da2-4a23-9d9d-224e4e9d5294.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd00dc43-1da2-4a23-9d9d-224e4e9d5294.png)'
- en: Now, with (6) and (7), [![](img/14df8976-b491-4874-bf26-1879dfdc30fb.png)] and [![](img/825e2f3a-c490-4ee0-9c32-aaed59534c36.png)]
    are updated only with the values of the successive states, without the need to
    unroll the trajectory to the end, as required in the old definition.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用（6）和（7），[![](img/14df8976-b491-4874-bf26-1879dfdc30fb.png)]和[![](img/825e2f3a-c490-4ee0-9c32-aaed59534c36.png)]仅根据连续状态的值进行更新，而不需要像旧定义中那样展开整个轨迹到终点。
- en: Categorizing RL algorithms
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习算法分类
- en: Before deep diving into the first RL algorithm that solves the optimal Bellman
    equation, we want to give a broad but detailed overview of RL algorithms. We need
    to do this because their distinctions can be quite confusing. There are many parts
    involved in the design of algorithms, and many characteristics have to be considered
    before deciding which algorithm best fits the actual needs of the user. The scope
    of this overview presents the big picture of RL so that in the next chapters,
    where we'll give a comprehensive theoretical and practical view of these algorithms,
    you will already see the general objective and have a clear idea of their location
    in the map of RL algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论解决最优贝尔曼方程的第一个强化学习算法之前，我们想先提供一个广泛但详细的强化学习算法概述。我们需要这样做，因为它们之间的区别可能会让人感到困惑。算法的设计涉及多个部分，且在决定哪种算法最适合用户的实际需求之前，需要考虑许多特性。本概述的范围呈现了强化学习的宏观图景，以便在接下来的章节中，我们将提供这些算法的全面理论和实践视角时，你将已经掌握它们的总体目标，并清晰地了解它们在强化学习算法地图中的位置。
- en: The first distinction is between model-based and model-free algorithms. As the
    name suggests, the first requires a model of the environment, while the second
    is free from this condition. The model of the environment is highly valuable because
    it carries precious information that can be used to find the desired policies;
    however, in most cases, the model is almost impossible to obtain. For example,
    it can be quite easy to model the game tic-tac-toe, while it can be difficult
    to model the waves of the sea. To this end, model-free algorithms can learn information
    without any assumptions about the environment. A representation of the categories
    of RL algorithms is visible in figure 3.2.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个区别是在基于模型和无模型算法之间。顾名思义，基于模型的算法需要一个环境模型，而无模型算法则不依赖于此条件。环境模型非常有价值，因为它包含了可以用来找到期望策略的宝贵信息；然而，在大多数情况下，模型几乎无法获取。例如，模拟井字游戏很容易，而模拟海浪则相当困难。为此，无模型算法可以在不假设环境的情况下学习信息。强化学习算法的类别在图3.2中有所展示。
- en: 'Here the distinction is shown between model-based and model-free, and two widely
    known model-free approaches, namely policy gradient and value-based. Also, as
    we''ll see in later chapters, a combination of those is possible:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了基于模型和无模型之间的区别，以及两种广为人知的无模型方法，即策略梯度和基于价值的方法。此外，正如我们将在后续章节中看到的，这些方法的结合是可能的：
- en: '![](img/a612c49f-f471-4f07-9d64-dc5dbe0377ca.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a612c49f-f471-4f07-9d64-dc5dbe0377ca.png)'
- en: Figure 3.2\. Categorization of RL algorithms
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2. 强化学习算法的分类
- en: The first distinction is between model-free and model-based. Model-free RL algorithms
    can be further decomposed in policy gradient and value-based algorithms. Hybrids
    are methods that combine important characteristics of both methods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个区别是在无模型和基于模型之间。无模型强化学习算法可以进一步细分为策略梯度算法和基于价值的算法。混合方法是结合了两者重要特性的算法。
- en: Model-free algorithms
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无模型算法
- en: 'In the absence of a model, **model-free** (**MF**) algorithms run trajectories
    within a given policy to gain experience and to improve the agent. MF algorithms
    are made up of three main steps that are repeated until a good policy is created:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有模型的情况下，**无模型**（**MF**）算法在给定的策略下运行轨迹以获取经验并改进智能体。MF算法由三个主要步骤组成，直到创建出一个良好的策略，这三个步骤会不断重复：
- en: The generation of new samples by running the policy in the environment. The
    trajectories are run until a final state is reached or for a fixed number of steps.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在环境中运行策略生成新样本。轨迹会一直运行直到达到最终状态，或运行固定的步数。
- en: The estimation of the return function.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回报函数的估计。
- en: The improvement of the policy using the samples collected, and the estimation
    done in step 2.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用收集的样本和步骤2中完成的估计来改进策略。
- en: These three components are at the heart of this type of algorithm, but based
    on how each step is performed, they generate different algorithms. Value-based
    algorithms and policy gradient algorithms are two such examples. They seem to
    be very different, but they are based on similar principles and both use the three-step
    approach.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个组件是此类算法的核心，但根据每个步骤的执行方式，它们会生成不同的算法。基于价值的算法和策略梯度算法就是两个这样的例子。它们看起来非常不同，但它们基于相似的原则，且都采用了三步法。
- en: Value-based algorithms
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于价值的算法
- en: Value-based algorithms, also known as **value function algorithms**, use a paradigm
    that's very similar to the one we saw in the previous section. That is, they use
    the Bellman equation to learn the Q-function, which in turn is used to learn a
    policy. In the most common setting, they use deep neural networks as a function
    approximator and other tricks to deal with high variance and general instabilities.
    To a certain degree, value-based algorithms are closer to supervised regression
    algorithms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的算法，也称为**价值函数算法**，使用与我们在前一部分中看到的非常相似的范式。也就是说，它们使用贝尔曼方程来学习Q函数，进而用于学习策略。在最常见的设置中，它们使用深度神经网络作为函数逼近器，并采用其他技巧来处理高方差和一般的不稳定性。在某种程度上，基于价值的算法更接近监督回归算法。
- en: Typically, these algorithms are off-policy, meaning they are not required to
    optimize the same policy that was used to generate the data. This means that these
    methods can learn from previous experience, as they can store the sampled data
    in a replay buffer. The ability to use previous samples makes the value function
    more sample-efficient than other model-free algorithms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些算法是离策略的，意味着它们不需要优化用于生成数据的相同策略。这意味着这些方法可以从以前的经验中学习，因为它们可以将采样数据存储在回放缓冲区中。使用以前样本的能力使得价值函数比其他无模型算法更具样本效率。
- en: Policy gradient algorithms
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度算法
- en: The other family of MF algorithms is that of the policy gradient methods (or
    policy optimization methods). They have a more direct and obvious interpretation
    of the RL problem, as they learn directly from a parametric policy by updating
    the parameters in the direction of the improvements. It's based on the RL principle
    that good actions should be encouraged (by boosting the gradient of the policy
    upward) while discouraging bad actions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类无模型（MF）算法是**策略梯度**方法（或策略优化方法）。它们对强化学习问题有更直接和明显的解释，因为它们通过更新参数以改进的方向直接从一个参数化策略中学习。其基于强化学习原理，即应当鼓励好的行动（通过提高策略的梯度），并且应当抑制坏的行动。
- en: Contrary to value function algorithms, policy optimization mainly requires on-policy
    data, making these algorithms more sample inefficient. Policy optimization methods
    can be quite unstable due to the fact that taking the steepest ascent in the presence
    of surfaces with high curvature can easily result in moving too far in any given
    direction, falling down into a bad region. To address this problem, many algorithms
    have been proposed, such as optimizing the policy only within a trust region,
    or optimizing a surrogate clipped objective function to limit changes to the policy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与价值函数算法相反，策略优化主要依赖于策略数据，这使得这些算法在样本效率上较低。策略优化方法可能会由于在存在高曲率的表面上采取最陡的上升路径而变得非常不稳定，这容易导致在某一方向上走得太远，最终跌入不好的区域。为了解决这个问题，提出了许多算法，例如仅在信任区域内优化策略，或者优化一个代理裁剪目标函数以限制策略的变化。
- en: A major advantage of policy gradient methods is that they easily handle environments
    with continuous action spaces. This is a very difficult thing to approach with
    value function algorithms as they learn Q-values for discrete pairs of states
    and actions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法的一个主要优点是它们能够轻松处理具有连续动作空间的环境。这对于基于价值的算法来说是一个非常困难的问题，因为它们为离散的状态和动作对学习Q值。
- en: Actor-Critic algorithms
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论家算法
- en: '**Actor-Critic** (**AC**) algorithms are on-policy policy gradient algorithms
    that also learn a value function (generally a Q-function) called a critic to provide
    feedback to the policy, the actor. Imagine that you, the actor, want to go to
    the supermarket via a new route. Unfortunately, before arriving at the destination,
    your boss calls you requiring you to go back to work. Because you didn''t reach
    the supermarket, you don''t know if the new road is actually faster than the old
    one. But if you reached a familiar location, you can estimate the time you''ll
    need to go from there to the supermarket and calculate whether the new path is
    preferable. This estimate is what the critic does. In this way, you can improve
    the actor even though you didn''t reach the final goal.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**演员-评论家**（**AC**）算法是离策略的策略梯度算法，它们还学习一个价值函数（通常是Q函数），这个函数叫做评论家，用来给策略（演员）提供反馈。想象一下，你，作为演员，想要通过一条新路线去超市。不幸的是，在到达目的地之前，你的老板打电话要求你回去工作。因为你没有到达超市，所以你不知道新路是否比旧路更快。但如果你到达了一个熟悉的地方，你可以估算从那里到超市所需的时间，并计算是否新路更优。这种估算就是评论家的作用。通过这种方式，即使你没有到达最终目标，你也可以改进演员的表现。'
- en: Combining a critic with an actor has been shown to be very effective and is
    commonly used in policy gradient algorithms. This technique can also be combined
    with other ideas used in policy optimization, such as trust-region algorithms.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将评论者与行动者结合已被证明非常有效，并且在策略梯度算法中被广泛使用。这项技术还可以与其他用于策略优化的思想相结合，例如信任区域算法。
- en: Hybrid algorithms
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合算法
- en: Advantages of both value functions and policy gradient algorithms can be merged,
    creating hybrid algorithms that can be more sample efficient and robust.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数和策略梯度算法的优势可以结合，创造出混合算法，这些算法可能更高效并且更加稳健。
- en: Hybrid approaches combine Q-functions and policy gradients to symbiotically
    and mutually improve each other. These methods estimate the expected Q-function
    of deterministic actions to directly improve the policy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法将Q函数和策略梯度结合在一起，互相促进和改善。这些方法估计确定性动作的期望Q函数，以直接改善策略。
- en: Be aware that because AC algorithms learn and use a value function, they are
    categorized as policy gradients and not as hybrid algorithms. This is because
    the main underlying objective is that of policy gradient methods. The value function
    is only an upgrade to provide additional information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于AC算法学习并使用价值函数，因此它们被归类为策略梯度，而不是混合算法。这是因为其主要目标是策略梯度方法，价值函数只是为了提供额外信息的一种升级。
- en: Model-based RL
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于模型的RL
- en: Having a model of the environment means that the state transitions and the rewards
    can be predicted for each state-action tuple (without any interaction with the
    real environment). As we already mentioned, the model is known only in limited
    cases, but when it is known, it can be used in many different ways. The most obvious
    application of the model is to use it to plan future actions. Planning is a concept
    used to express the organization of future moves when the consequences of the
    next actions are already known. For example, if you know exactly what moves your
    enemy will make, you can think ahead and plan all your actions before executing
    the first one. As a downside, planning can be very expensive and isn't a trivial
    process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有环境模型意味着每个状态-动作元组的状态转换和奖励可以预测（无需与真实环境交互）。正如我们之前提到的，模型在有限的情况下是已知的，但当它已知时，可以以多种方式使用。模型最明显的应用是用于规划未来的动作。规划是一个用于表达组织未来移动的概念，当下一步动作的后果已经知道时。例如，如果你完全知道敌人将采取哪些行动，你可以提前思考并在执行第一步之前规划所有的动作。缺点是，规划可能非常昂贵，而且不是一个简单的过程。
- en: A model can also be learned through interactions with the environment, assimilating
    the consequences (both in terms of the states and rewards) of an action. This
    solution is not always the best one because teaching a model could be terribly
    expensive in the real world. Moreover, if only a rough approximation of the environment
    is understood by the model, it could lead to disastrous results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与环境的交互，模型也可以通过吸收动作的后果（包括状态和奖励）来学习。这种解决方案并不总是最优的，因为在现实世界中，教一个模型可能非常昂贵。而且，如果模型只对环境有一个粗略的近似理解，可能会导致灾难性的结果。
- en: A model, whether known or learned, can be used both to plan and to improve the
    policy, and can be integrated into different phases of an RL algorithm. Well-known
    cases of model-based RL involve pure planning, embedded planning to improve the
    policy, and generated samples from an approximate model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是已知的还是通过学习得到的模型，都可以用来进行规划和改进策略，并可以集成到RL算法的不同阶段。基于模型的RL的著名案例包括纯规划、嵌入式规划以改进策略以及从近似模型中生成的样本。
- en: A set of algorithms that use a model to estimate a value function is called
    **dynamic programming** (**DP**) and will be studied later in this chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一组使用模型估计价值函数的算法称为**动态规划**（**DP**），将在本章稍后进行研究。
- en: Algorithm diversity
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法多样性
- en: Why are there so many types of RL algorithms? This is because there isn't one
    that is better than all the others in every context. Each one is designed for
    different needs and to take care of different aspects. The most notable differences
    are stability, sample efficiency, and wall clock time (training time). These will
    be more clear as we progress through the book but as a rule of thumb, policy gradient
    algorithms are more stable and reliable than value function algorithms. On the
    other hand, value function methods are more sample efficient as they are off-policy
    and can use prior experience. In turn, model-based algorithms are more sample
    efficient than Q-learning algorithms but their computational cost is much higher
    and they are slower.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么有那么多种强化学习算法？这是因为没有一种算法在所有情况下都比其他算法更好。每种算法都是为不同的需求设计的，旨在处理不同的方面。最显著的差异包括稳定性、样本效率和墙时（训练时间）。随着我们逐步深入本书，这些差异会更加明确，但作为经验法则，策略梯度算法比值函数算法更加稳定和可靠。另一方面，值函数方法更具样本效率，因为它们是离策略的，并且可以利用先前的经验。反过来，基于模型的算法比Q学习算法更具样本效率，但它们的计算成本更高，且速度更慢。
- en: Besides the ones just presented, there are other trade-offs that have to be
    taken into consideration while designing and deploying an algorithm (such as ease
    of use and robustness), which is not a trivial process.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了刚才提到的那些，还有其他一些权衡需要在设计和部署算法时考虑（例如易用性和鲁棒性），这不是一个简单的过程。
- en: Dynamic programming
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划
- en: DP is a general algorithmic paradigm that breaks up a problem into smaller chunks
    of overlapping subproblems, and then finds the solution to the original problem
    by combining the solutions of the subproblems.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划（DP）是一种通用的算法范式，它将一个问题分解为多个重叠的子问题，然后通过结合子问题的解决方案来找到原始问题的解。
- en: DP can be used in reinforcement learning and is among one of the simplest approaches.
    It is suited to computing optimal policies by being provided with a perfect model
    of the environment.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DP可以用于强化学习，并且是最简单的方式之一。它通过提供环境的完美模型来计算最优策略。
- en: DP is an important stepping stone in the history of RL algorithms and provides
    the foundation for the next generation of algorithms, but it is computationally
    very expensive. DP works with MDPs with a limited number of states and actions
    as it has to update the value of each state (or action-value), taking into consideration
    all the other possible states. Moreover, DP algorithms store value functions in
    an array or in a table. This way of storing information is effective and fast
    as there isn't any loss of information, but it does require the storage of large
    tables. Since DP algorithms use tables to store value functions, it is called
    tabular learning. This is opposed to approximated learning, which uses approximated
    value functions to store the values in a fixed size function, such as an artificial
    neural network.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划（DP）是强化学习算法历史中的一个重要里程碑，并为下一代算法奠定了基础，但其计算成本非常高。DP适用于具有有限状态和动作的马尔可夫决策过程（MDP），因为它必须更新每个状态（或动作值）的值，同时考虑到所有其他可能的状态。此外，DP算法将价值函数存储在数组或表格中。这种存储信息的方式是有效且快速的，因为没有信息丢失，但它确实需要存储大量的表格。由于DP算法使用表格存储价值函数，因此被称为表格学习。这与近似学习相对，后者使用近似价值函数将值存储在固定大小的函数中，例如人工神经网络。
- en: 'DP uses **bootstrapping**, meaning that it improves the estimation value of
    a state by using the expected value of the following states. As we have already
    seen, bootstrapping is used in the Bellman equation. Indeed, DP applies the Bellman
    equations, (6) and (7), to estimate ![](img/d1e5a85d-b3db-40e3-8673-6e550b0970cc.png) and/or
    ![](img/c3f1b6e5-7143-4c81-a8e4-e18a3b34132f.png). This can be done using the
    following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DP使用**自举**，意味着它通过使用后续状态的期望值来改进状态的估计值。正如我们之前看到的，自举在贝尔曼方程中被使用。实际上，DP应用了贝尔曼方程（6）和（7）来估算![](img/d1e5a85d-b3db-40e3-8673-6e550b0970cc.png)和/或![](img/c3f1b6e5-7143-4c81-a8e4-e18a3b34132f.png)。这可以通过以下方式完成：
- en: '![](img/b748b0d7-08fe-47f8-8b6f-3f2d8d554826.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b748b0d7-08fe-47f8-8b6f-3f2d8d554826.png)'
- en: 'Or by using the Q-function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过使用Q函数：
- en: '![](img/ae17b27a-87c2-4b74-9d79-82d604815cae.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae17b27a-87c2-4b74-9d79-82d604815cae.png)'
- en: Then, once the optimal value and action-value function are found, the optimal
    policy can be found by just taking the actions that maximize the expectation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦找到最优的价值函数和动作价值函数，就可以通过采取最大化期望的行动来找到最优策略。
- en: Policy evaluation and policy improvement
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略评估与策略改进
- en: 'To find the optimal policy, you first need to find the optimal value function.
    An iterative procedure that does this is called **policy evaluation**—it creates
    a ![](img/e066f1bb-6f23-4534-a641-d2c5b5744803.png) sequence that iteratively
    improves the value function for a policy, ![](img/efe619a7-adea-4fca-8f5c-0526379caee6.png),
    using the state value transition of the model, the expectation of the next state,
    and the immediate reward. Therefore, it creates a sequence of improving value
    functions using the Bellman equation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最优策略，你首先需要找到最优的价值函数。一个执行这一过程的迭代方法称为**策略评估**——它通过模型的状态值转移、下一个状态的期望以及即时奖励，创建一个![](img/e066f1bb-6f23-4534-a641-d2c5b5744803.png)序列，逐步改进一个策略的价值函数，![](img/efe619a7-adea-4fca-8f5c-0526379caee6.png)。因此，它使用贝尔曼方程创建一个不断改进的价值函数序列：
- en: '![](img/35ce6c80-4531-418a-89ee-d95018a5eb01.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35ce6c80-4531-418a-89ee-d95018a5eb01.png)'
- en: 'This sequence will converge to the optimal value as ![](img/2de8443b-3c7c-4562-81a4-8c8909dc7721.png).
    Figure 3.3 shows the update of ![](img/fcc876d2-ff8c-43bd-b4ab-94ca788e93c7.png) using
    the successive state values:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列将随着![](img/2de8443b-3c7c-4562-81a4-8c8909dc7721.png)的变化而收敛到最优值。图3.3展示了使用连续状态值更新![](img/fcc876d2-ff8c-43bd-b4ab-94ca788e93c7.png)：
- en: '![](img/750b984e-76e9-4fb4-8565-bbf6e0dc1e0f.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/750b984e-76e9-4fb4-8565-bbf6e0dc1e0f.png)'
- en: Figure 3.3\. The update of ![](img/772727b6-3e7b-41bf-a9c3-2b329fc37397.png) using formula
    (8)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3\. 使用公式（8）更新![](img/772727b6-3e7b-41bf-a9c3-2b329fc37397.png)
- en: The value function (8) can be updated only if the state transition function, `p`,
    and the reward function, `r`, for every state and action are known, so only if
    the model of the environment is completely known.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在知道每个状态和动作的状态转移函数`p`和奖励函数`r`时，价值函数（8）才可以更新，因此只有在环境的模型完全已知时才能更新。
- en: Note that the first summation of the actions in (8) is needed for stochastic
    policies because the policy outputs a probability for each action. For simplicity
    from now on, we'll consider only deterministic policies.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在（8）中的第一个动作求和是针对随机策略所必需的，因为该策略为每个动作输出一个概率。为了简便起见，从现在开始我们只考虑确定性策略。
- en: 'Once the value functions are improved, it can be used to find a better policy.
    This procedure is called *policy improvement* and is about finding a policy, ![](img/9c7a1dd0-2b3e-4a41-98fd-25aafdd44903.png), as
    follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦价值函数得到改进，它就可以用来找到更好的策略。这个过程称为*策略改进*，其目的是找到一个策略，![](img/9c7a1dd0-2b3e-4a41-98fd-25aafdd44903.png)，如下所示：
- en: '![](img/76d1e369-d645-4f9f-86df-577aa539974b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76d1e369-d645-4f9f-86df-577aa539974b.png)'
- en: It creates a policy, ![](img/bc4324c4-27a9-4444-9262-f5739c2b6eae.png), from
    the value function, ![](img/54caf5aa-e5a1-4022-a5ed-3e801909c297.png), of the
    original policy, ![](img/b07eeebf-18a3-489d-9769-5c9736bbb58c.png). As can be
    formally demonstrated, the new policy, ![](img/e5fc8c16-f6c2-4c49-a973-0fbc3d372902.png), is
    always better than ![](img/a999e245-6d4c-4c0f-8ee9-5b1020f8eb35.png), and the
    policy is optimal if and only if ![](img/46dec1a9-9029-46fb-87ff-c75694ccf873.png) is
    optimal. The combination of policy evaluation and policy improvement gives rise
    to two algorithms to compute the optimal policy. One is called **policy iteration**
    and the other is called **value iteration**. Both use policy evaluation to monotonically improve
    the value function and policy improvement to estimate the new policy. The only
    difference is that policy iteration executes the two phases cyclically, while
    value iteration combines them in a single update.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它从原始策略的价值函数，![](img/54caf5aa-e5a1-4022-a5ed-3e801909c297.png)，创建一个策略，![](img/bc4324c4-27a9-4444-9262-f5739c2b6eae.png)。如正式证明，新策略，![](img/e5fc8c16-f6c2-4c49-a973-0fbc3d372902.png)，总是比![](img/a999e245-6d4c-4c0f-8ee9-5b1020f8eb35.png)更好，并且当且仅当![](img/46dec1a9-9029-46fb-87ff-c75694ccf873.png)是最优时，策略才是最优的。策略评估和策略改进的结合产生了两种计算最优策略的算法。一种叫做**策略迭代**，另一种叫做**价值迭代**。两者都使用策略评估来单调地改进价值函数，并使用策略改进来估计新的策略。唯一的区别在于，策略迭代按循环方式执行这两个阶段，而价值迭代将它们合并在单一的更新中。
- en: Policy iteration
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略迭代
- en: Policy iteration cycles between policy evaluation, which updates ![](img/385c7c7e-e70d-4c66-b996-ce801f533505.png) under
    the current policy, ![](img/147da112-7f02-4562-8164-d6d642b64073.png), using formula (8),
    and policy improvement (9), which computes ![](img/1fefbb96-584a-47d3-8820-6b078fc78d85.png) using
    the improved value function, ![](img/2c92dcf9-5506-484f-8ee1-6e799304cca7.png). Eventually,
    after ![](img/61d2c68a-4610-4ead-923f-9ea65c7f0c41.png) cycles, the algorithm
    will result in an optimal policy, ![](img/a65f1839-d684-4a3e-85bb-741fb82da1cc.png).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代在策略评估和策略改进之间循环，策略评估使用公式(8)在当前策略下更新![](img/385c7c7e-e70d-4c66-b996-ce801f533505.png)，而策略改进(9)则利用改进后的价值函数计算![](img/1fefbb96-584a-47d3-8820-6b078fc78d85.png)，该函数由![](img/2c92dcf9-5506-484f-8ee1-6e799304cca7.png)表示。最终，经过![](img/61d2c68a-4610-4ead-923f-9ea65c7f0c41.png)次循环，算法将得到一个最优策略![](img/a65f1839-d684-4a3e-85bb-741fb82da1cc.png)。
- en: 'The pseudocode is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码如下：
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After an initialization phase, the outer loop iterates through policy evaluation
    and policy iteration until a stable policy is found. On each of these iterations,
    policy evaluation evaluates the policy found during the preceding policy improvement
    steps, which in turn use the estimated value function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化阶段之后，外部循环会在策略评估和策略迭代之间迭代，直到找到一个稳定的策略。在每次迭代中，策略评估会评估前一步策略改进步骤中找到的策略，而这些步骤则使用估算的价值函数。
- en: Policy iteration applied to FrozenLake
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用策略迭代到FrozenLake
- en: 'To consolidate the ideas behind policy iteration, we''ll apply it to a game
    called FrozenLake. Here, the environment consists of a 4 x 4 grid. Using four
    actions that correspond to the directions (0 is left, 1 is down, 2 is right, and
    3 is up), the agent has to move to the opposite side of the grid without falling
    in the holes. Moreover, movement is uncertain, and the agent has the possibility
    of movement in other directions. So, in such a situation, it could be beneficial
    not to move in the intended direction. A reward of +1 is assigned when the end
    goal is reached. The map of the game is shown in figure 3.4\. S is the start position,
    the star is the end position, and the spirals are the holes:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固策略迭代背后的思想，我们将其应用于一个名为FrozenLake的游戏。在这里，环境由一个4 x 4的网格组成。通过四个动作对应四个方向（0表示左，1表示下，2表示右，3表示上），代理需要移动到网格的另一端，而不能掉入洞中。此外，移动是不确定的，代理有可能朝其他方向移动。因此，在这种情况下，可能会有益于不沿着预定的方向移动。当达到目标时，奖励为+1。游戏地图如图3.4所示，S是起始位置，星星是目标位置，螺旋是洞：
- en: '![](img/c9d98d17-74ab-42f6-80eb-d542848be9d0.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9d98d17-74ab-42f6-80eb-d542848be9d0.png)'
- en: Figure 3.4 Map of the FrozenLake game
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 FrozenLake游戏地图
- en: With all the tools needed, let's see how to solve it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好所有所需工具后，让我们看看如何解决这个问题。
- en: 'All the code explained in this chapter is available on the GitHub repository
    of this book, using the following link: [https://https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中解释的所有代码都可以在本书的GitHub仓库中找到，链接如下：[https://https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)
- en: 'First, we have to create the environment, initializing the value function and
    the policy:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建环境，初始化价值函数和策略：
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we have to create the main cycle that does one step of policy evaluation
    and one step of policy improvement. This cycle finishes whenever the policy is
    stable. To do this, use the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个主循环，执行一次策略评估和一次策略改进。该循环会在策略稳定时结束。为此，请使用以下代码：
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the end, we can print the number of iterations completed, the value function,
    the policy, and the score reached running some test games:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印出完成的迭代次数、价值函数、策略以及通过运行一些测试游戏所达到的得分：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, before defining `policy_evaluation`, we can create a function to evaluate
    the expected action-value that will also be used in `policy_improvement`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在定义`policy_evaluation`之前，我们可以创建一个函数来评估预期的动作值，这个函数也将被用于`policy_improvement`：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `env.P` is a dictionary that contains all the information about the dynamics
    of the environment.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`env.P`是一个字典，包含了关于环境动态的所有信息。
- en: '`gamma` is the discount factor, with 0.99 being a standard value to use for
    simple and medium difficulty problems. The higher it is, the more difficult it
    is for the agent to predict the value of a state because it should look further
    into the future.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`gamma`是折扣因子，0.99是一个标准值，适用于简单和中等难度的问题。它越高，代理预测一个状态的值就越困难，因为它需要展望更远的未来。'
- en: 'Next, we can define the `policy_evaluation` function. `policy_evaluation` has
    to calculate formula (8) under the current policy for every state until it reaches steady
    values. Because the policy is deterministic, we only evaluate one action:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以定义`policy_evaluation`函数。`policy_evaluation`需要根据当前策略计算公式（8），并对每个状态进行计算，直到达到稳定值。因为策略是确定性的，所以我们只需要评估一个动作：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We consider the value function stable whenever `delta` is lower than the threshold, `eps`.
    When these conditions are met, the `while` loop statement is stopped.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当`delta`小于阈值`eps`时，我们认为值函数稳定。当这些条件满足时，`while`循环语句被停止。
- en: '`policy_improvement` takes the value function and the policy and iterates them
    across all of the states to update the policy based on the new value function:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`policy_improvement` 接收值函数和策略，并对所有状态进行迭代，基于新的值函数更新策略：'
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`policy_improvement(V, policy)` returns `False` until the policy changes. That''s
    because it means that the policy isn''t stable yet.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`policy_improvement(V, policy)`在策略发生变化之前返回`False`。这是因为它意味着策略尚未稳定。'
- en: 'The final snippet of code runs some games to test the new policy and prints
    the number of games won:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一段代码运行一些游戏来测试新策略，并打印赢得的游戏次数：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That's it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。
- en: 'It converges in about 7 iterations and wins approximately 85% of games:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 它大约在7次迭代后收敛，并且赢得了大约85%的游戏：
- en: '![](img/7d88776b-8ea4-4c76-abdf-e61bc3a217f1.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d88776b-8ea4-4c76-abdf-e61bc3a217f1.png)'
- en: Figure 3.5 Results of the FrozenLake game. The optimal policy is on the left
    and the optimal state values are on the right
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 冰湖游戏的结果。最优策略在左侧，最优状态值在右侧。
- en: The policy resulting from the code is shown on the left of figure 3.5\. You
    can see that it takes strange directions, but it's only because it follows the
    dynamics of the environment. On the right of figure 3.5, the final state's values
    are presented.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码得到的策略显示在图3.5的左侧。你可以看到它走的是一些奇怪的路线，但这只是因为它遵循了环境的动态。在图3.5的右侧，展示了最终状态的值。
- en: Value iteration
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'Value iteration is the other dynamic programming algorithm to find optimal
    values in an MDP, but unlike policy iterations that execute policy evaluations
    and policy iterations in a loop, value iteration combines the two methods in a
    single update. In particular, it updates the value of a state by selecting the
    best action immediately:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代是另一种用于在MDP中寻找最优值的动态规划算法，但与执行策略评估和策略迭代循环的策略迭代不同，值迭代将这两种方法合并为一次更新。特别地，它通过立即选择最佳动作来更新状态的值：
- en: '![](img/4ac987cf-3e3e-4ae7-9158-2de7a6806d5a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac987cf-3e3e-4ae7-9158-2de7a6806d5a.png)'
- en: 'The code for value iteration is even simpler than the policy iteration code,
    summarized in the following pseudocode:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代的代码比策略迭代的代码更简单，以下是总结的伪代码：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The only difference is in the new value estimation update and in the absence
    of a proper policy iteration module. The resulting optimal policy is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于新的值估计更新和没有适当的策略迭代模块。最终得到的最优策略如下：
- en: '![](img/63edbf60-b522-4259-a519-9a0c94167b59.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63edbf60-b522-4259-a519-9a0c94167b59.png)'
- en: Value iteration applied to FrozenLake
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用值迭代到冰湖
- en: We can now apply value iteration to the FrozenLake game in order to compare
    the two DP algorithms and to see whether they converge to the same policy and
    value function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将值迭代应用于冰湖游戏，以便比较这两种动态规划算法，并查看它们是否会收敛到相同的策略和值函数。
- en: 'Let''s define `eval_state_action` as before to estimate the action state value
    for a state-action pair:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像之前一样定义`eval_state_action`，用来估算状态-动作对的动作状态值：
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we create the main body of the value iteration algorithm:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建值迭代算法的主体部分：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It loops until it reaches a steady value function (determined by the threshold, `eps`)
    and for each iteration, it updates the value of each state using formula (10).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它会循环直到达到稳定的值函数（由阈值`eps`决定），并且在每次迭代中，使用公式（10）更新每个状态的值。
- en: 'As for the policy iteration, `run_episodes` executes some games to test the
    policy. The only difference is that in this case, the policy is determined at
    the same time that `run_episodes` is executed (for policy iteration, we defined
    the action for every state beforehand):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 至于策略迭代，`run_episodes`执行一些游戏来测试策略。唯一的不同之处在于，在这种情况下，策略是在执行`run_episodes`时同时确定的（对于策略迭代，我们提前为每个状态定义了动作）：
- en: '[PRE11]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can create the environment, unwrap it, run the value iteration,
    and execute some test games:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建环境，解开它，运行值迭代，并执行一些测试游戏：
- en: '[PRE12]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will be similar to the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下内容：
- en: '[PRE13]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The value iteration algorithm converges after 130 iterations. The resulting
    value function and policy are the same as the policy iteration algorithm.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代算法在130次迭代后收敛。得到的值函数和策略与策略迭代算法相同。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: An RL problem can be formalized as an MDP, providing an abstract framework for
    learning goal-based problems. An MDP is defined by a set of states, actions, rewards,
    and transition probabilities, and solving an MDP means finding a policy that maximizes
    the expected reward in each state. The Markov property is intrinsic to the MDP
    and ensures that the future states depend only on the current one, not on its
    history.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RL问题可以被形式化为一个MDP，为学习目标驱动问题提供了一个抽象框架。MDP由一组状态、动作、奖励和转移概率定义，解决MDP意味着找到一个在每个状态下最大化期望奖励的策略。马尔可夫性质是MDP的内在特性，它确保未来的状态仅依赖于当前状态，而不依赖于历史。
- en: Using the definition of MDP, we formulated the concepts of policy, return function,
    expected return, action-value function, and value function. The latter two can
    be defined in terms of the values of the subsequent states, and the equations
    are called Bellman equations. These equations are useful because they provide
    a method to compute value functions in an iterative way. The optimal value functions
    can then be used to find the optimal policy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MDP的定义，我们提出了策略、回报函数、期望回报、动作-值函数和值函数的概念。后两者可以通过后续状态的值来定义，这些方程被称为贝尔曼方程。这些方程非常有用，因为它们提供了一种迭代计算值函数的方法。然后，最优值函数可以用来找到最优策略。
- en: RL algorithms can be categorized as model-based or model-free. While the former
    requires a model of the environment to plan the next actions, the latter is independent
    of the model and can learn by direct interaction with the environment. Model-free
    algorithms can be further divided into policy gradient and value function algorithms.
    Policy gradient algorithms learn directly from the policy through gradient ascent
    and are typically on-policy. Value function algorithms are usually off-policy,
    and learn an action-value function or value function in order to create the policy.
    These two methods can be brought together to give rise to methods that combine
    the advantages of both worlds.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: RL算法可以分为基于模型的和无模型的。前者需要环境的模型来规划下一步动作，而后者独立于模型，可以通过与环境的直接交互来学习。无模型算法可以进一步分为策略梯度算法和价值函数算法。策略梯度算法通过梯度上升直接从策略中学习，通常是基于策略的。价值函数算法通常是离策略的，它们学习动作-值函数或值函数来创建策略。这两种方法可以结合在一起，产生兼具两者优点的方法。
- en: 'DP is the first set of model-based algorithms that we looked at in depth. It
    is used whenever the full model of the environment is known and when it is constituted
    by a limited number of states and actions. DP algorithms use bootstrapping to
    estimate the value of a state and they learn the optimal policy through two processes:
    policy evaluation and policy improvement. Policy evaluation computes the state
    value function for an arbitrary policy, while policy improvement improves the
    policy using the value function obtained from the policy evaluation process.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: DP是我们深入研究的第一组基于模型的算法。它在已知环境完整模型且由有限数量的状态和动作构成时使用。DP算法通过自举方法估计状态的值，并通过两个过程学习最优策略：策略评估和策略改进。策略评估计算任意策略的状态值函数，而策略改进则通过使用策略评估过程得到的值函数来改进策略。
- en: By combining policy improvement and policy evaluation, the policy iteration
    algorithm and the value iteration algorithm can be created. The main difference
    between the two is that while policy iteration runs iteratively of policy evaluation
    and policy improvement, value iteration combines the two processes in a single
    update.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合策略改进和策略评估，可以创建策略迭代算法和值迭代算法。两者的主要区别在于，策略迭代是通过策略评估和策略改进的迭代过程来运行的，而值迭代则将这两个过程合并为一次更新。
- en: Though DP suffers from the curse of dimensionality (the complexity grows exponentially
    with the number of states), the ideas behind policy evaluation and policy iteration
    are key in almost all RL algorithms because they use a generalized version of
    them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管动态规划受到维度灾难的困扰（复杂度随着状态数量呈指数增长），但策略评估和策略迭代背后的思想几乎在所有强化学习算法中都是关键，因为它们使用了这些思想的广义版本。
- en: Another disadvantage of DP is that it requires the exact model of the environment,
    limiting its applicability to many other problems.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划的另一个缺点是它需要环境的精确模型，这限制了它在许多其他问题中的应用。
- en: In the next chapter, you'll see how V-functions and Q-functions can be used
    to learn a policy, using problems where the model is unknown by sampling directly
    from the environment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将看到如何使用V函数和Q函数来学习策略，利用模型未知的情况下，直接从环境中采样的问题。
- en: Questions
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What's an MDP?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是马尔可夫决策过程（MDP）？
- en: What's a stochastic policy?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是随机策略？
- en: How can a return function be defined in terms of the return at the next time
    step?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何用下一个时间步的回报来定义回报函数？
- en: Why is the Bellman equation so important?
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么贝尔曼方程如此重要？
- en: What are the limiting factors of DP algorithms?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态规划（DP）算法的限制因素是什么？
- en: What is policy evaluation?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是策略评估？
- en: How do policy iteration and value iteration differ?
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略迭代和值迭代有何不同？
- en: Further reading
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Sutton and Barto, *Reinforcement Learning*, Chapters 3 and 4
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto，《强化学习》，第 3 章和第 4 章
