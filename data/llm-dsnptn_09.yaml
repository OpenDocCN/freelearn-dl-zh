- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Regularization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: '**Regularization** is a set of methods that constrain or modify the learning
    process to prevent the model from memorizing training data too precisely, encouraging
    it to learn more robust and generalizable patterns instead.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**是一组方法，它约束或修改学习过程，以防止模型过于精确地记住训练数据，鼓励它学习更稳健和可泛化的模式。'
- en: Regularization is a crucial aspect of training LLMs to prevent overfitting and
    improve generalization. Overfitting is detrimental because it causes a model to
    perform exceptionally well on training data while failing miserably on new, unseen
    data. When a model overfits, it essentially memorizes the noise and peculiarities
    of the training dataset, rather than learning generalizable patterns and relationships.
    This creates an illusion of high accuracy during development but leads to poor
    real-world performance, rendering the model ineffective for its intended purpose
    of making accurate predictions on novel inputs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是训练LLMs（大型语言模型）的一个关键方面，用于防止过拟合并提高泛化能力。过拟合是有害的，因为它会导致模型在训练数据上表现异常出色，而在新的、未见过的数据上却表现糟糕。当模型过拟合时，它实际上记住了训练数据集中的噪声和特殊性，而不是学习可泛化的模式和关系。这会在开发阶段产生高准确率的错觉，但会导致现实世界的表现不佳，使模型无法有效地用于其旨在对新颖输入进行准确预测的目的。
- en: In this chapter, you’ll learn about different regularization techniques specifically
    tailored to LLMs. We’ll explore methods such as layer-wise adaptive regularization,
    regularization in fine-tuning, and the combination of multiple techniques. You’ll
    gain insights into implementing these strategies and understanding their impact
    on model performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解针对LLMs特别定制的不同正则化技术。我们将探讨分层自适应正则化、微调中的正则化和多种技术的组合等方法。您将深入了解这些策略的实施及其对模型性能的影响。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: L2 regularization (Ridge regression)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化（岭回归）
- en: Dropout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Layer-wise adaptive regularization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层自适应正则化
- en: Gradient clipping and noise injection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度裁剪和噪声注入
- en: Regularization in transfer learning and fine-tuning scenarios
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迁移学习和微调场景中的正则化
- en: Emerging regularization techniques for next-generation LLMs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对下一代LLMs的新兴正则化技术
- en: L2 regularization (Ridge regression)
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2正则化（岭回归）
- en: '**L2 regularization**, also known as ridge regression or weight decay, is a
    technique used to prevent overfitting in machine learning models. It works by
    adding a penalty term to the loss function, which is proportional to the square
    of the model’s weights. This penalty term discourages the model from assigning
    large weights to individual features, leading to a simpler and more generalized
    model. By minimizing the combined loss function, which includes both the original
    loss and the penalty term, the model finds a balance between fitting the training
    data well and keeping the weights small, ultimately improving its ability to generalize
    to new, unseen data'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2正则化**，也称为岭回归或权重衰减，是一种用于防止机器学习模型过拟合的技术。它通过向损失函数添加一个惩罚项来实现，该惩罚项与模型权重的平方成正比。这个惩罚项阻止模型将大权重分配给单个特征，从而得到一个更简单、更通用的模型。通过最小化包含原始损失和惩罚项的合并损失函数，模型在拟合训练数据的同时保持权重较小，最终提高其泛化到新、未见过的数据的能力。'
- en: 'Here’s how to use it:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何使用它的：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this implementation, we use the AdamW optimizer that we discussed in [*Chapter
    7*](B31249_07.xhtml#_idTextAnchor108), which correctly implements weight decay.
    The `weight_decay` parameter controls the strength of regularization. A typical
    value is `0.01`, but you may need to adjust this based on your specific model
    and dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们使用了我们在[*第七章*](B31249_07.xhtml#_idTextAnchor108)中讨论的AdamW优化器，它正确地实现了权重衰减。`weight_decay`参数控制正则化的强度。一个典型的值是`0.01`，但您可能需要根据您的特定模型和数据集进行调整。
- en: Dropout
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: '**Dropout** is another powerful regularization technique that randomly “drops
    out” a portion of neurons during training.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout**是另一种强大的正则化技术，它在训练过程中随机“丢弃”一部分神经元。'
- en: Dropout helps combat overfitting by randomly deactivating a fraction of neurons
    during each training iteration, forcing the network to develop redundant pathways
    for information flow. This technique prevents neurons from becoming overly dependent
    on each other by creating a form of ensemble learning within a single network,
    where different subnetworks handle similar tasks. The result is a more robust
    model that relies on distributed representations rather than memorizing specific
    patterns, ultimately improving generalization to unseen data when all neurons
    are active during inference.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout通过在每次训练迭代中随机停用一部分神经元来帮助对抗过拟合。这迫使网络发展冗余的信息流路径。这种技术通过在单个网络内创建一种集成学习形式，防止神经元过度依赖彼此，其中不同的子网络处理类似任务。结果是，一个更健壮的模型，它依赖于分布式表示而不是记忆特定模式，最终在推理期间所有神经元都活跃时，提高了对未见数据的泛化能力。
- en: 'It’s particularly effective in large neural networks such as LLMs. Here’s how
    to implement dropout in a transformer-based LLM:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它在大型神经网络（如LLM）中特别有效。以下是如何在基于变换器的LLM中实现dropout的方法：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this implementation, dropout is applied after the embedding layer and within
    each transformer layer. The dropout rate of `0.1` is typical, but you may need
    to adjust this based on your specific use case.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，dropout在嵌入层之后以及每个变换器层内应用。`0.1`的dropout率是典型的，但根据您的具体用例，您可能需要调整这个值。
- en: Keep in mind that dropout is only applied during training, not during inference
    (when the model is being used to make predictions).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，dropout仅在训练期间应用，不在推理期间（当模型被用于做出预测时）应用。
- en: During training, neurons are randomly “dropped” (deactivated) with a specified
    probability (e.g., `0.5` means each neuron has a 50% chance of being turned off
    for that training batch). This forces the network to learn more robust features
    since it can’t rely on any single neuron always being present.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，神经元以指定的概率随机“停用”（失活）（例如，`0.5`表示每个神经元有50%的机会在该训练批次中被关闭）。这迫使网络学习更鲁棒的特征，因为它不能依赖于任何单个神经元始终存在。
- en: During inference (testing, evaluation, or deployment), dropout is disabled and
    all neurons are active. However, the weights are typically scaled by the dropout
    rate to account for the fact that more neurons are active than during training.
    This scaling ensures the expected output magnitude remains consistent.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理（测试、评估或部署）期间，dropout被禁用，所有神经元都是活跃的。然而，权重通常按dropout率进行缩放，以考虑到训练期间活跃的神经元比推理期间更多。这种缩放确保了期望的输出幅度保持一致。
- en: This training-only application of dropout is a key part of what makes it effective
    as a regularization technique – it creates a form of ensemble learning during
    training while still allowing for full network capacity during actual use.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种仅在训练中应用的dropout是使其作为正则化技术有效的关键部分——它在训练期间创建了一种集成学习的形式，同时在实际使用时仍允许网络发挥全部能力。
- en: Layer-wise adaptive regularization
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层级自适应正则化
- en: Layer-wise adaptive regularization involves applying different regularization
    strengths to different layers of the model. This can be particularly effective
    for LLMs, where lower layers may benefit from less regularization to capture fundamental
    patterns, while higher layers might need stronger regularization to prevent overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 层级自适应正则化涉及对模型的不同层应用不同的正则化强度。这对于LLM尤其有效，其中较低层可能从较少的正则化中受益以捕捉基本模式，而较高层可能需要更强的正则化以防止过拟合。
- en: 'The following Python code defines a `LayerwiseAdaptiveRegularization` class,
    which is a PyTorch `nn.Module` designed to wrap a base transformer model and apply
    a dropout rate that increases linearly with the depth of the model’s layers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码定义了一个`LayerwiseAdaptiveRegularization`类，这是一个PyTorch `nn.Module`，旨在封装一个基础变换器模型并应用一个随着模型层深度线性增加的dropout率：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `LayerwiseAdaptiveRegularization` class initializes with a base model, the
    number of layers, a starting dropout probability, and an increment for each subsequent
    layer. It then configures the dropout probabilities within the attention and MLP
    sub-layers of the transformer blocks. Finally, its forward method simply passes
    the input through the wrapped base model. An example of its usage is shown by
    wrapping a `create_lm_model()` with this layer-wise dropout regularization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`LayerwiseAdaptiveRegularization`类使用基础模型、层数、起始dropout概率以及后续每层的增量进行初始化。然后，它配置变换器块中注意力和MLP子层的dropout概率。最后，其前向方法简单地通过封装的基础模型传递输入。其使用示例是通过将`create_lm_model()`与这个层级dropout正则化封装来展示的。'
- en: This implementation wraps a base GPT-2 model and applies increasing dropout
    rates to higher layers. The base dropout rate is `0.1`, and it increases by `0.02`
    for each subsequent layer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现包装了一个基本的GPT-2模型，并将递增的dropout率应用于更高层。基本dropout率是`0.1`，后续每层增加`0.02`。
- en: Gradient clipping and noise injection
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度裁剪和噪声注入
- en: Gradient clipping and noise injection are techniques used to improve the training
    stability and generalization of LLMs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度裁剪和噪声注入是用于提高大型语言模型（LLMs）训练稳定性和泛化的技术。
- en: Gradient clipping, while primarily employed for optimization stability (see
    [*Chapter 7*](B31249_07.xhtml#_idTextAnchor108)), can indirectly contribute to
    regularization. By limiting the magnitude of gradients, it can constrain the updates
    to model parameters, potentially leading to a smoother optimization path and preventing
    overfitting. In some cases, gradient clipping can effectively reduce the impact
    of certain parameters, especially when gradients for those parameters are consistently
    clipped. This can lead to a form of implicit sparsity, where less important parameters
    are effectively downweighted.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度裁剪，虽然主要用于优化稳定性（参见[*第7章*](B31249_07.xhtml#_idTextAnchor108)），但可以间接地促进正则化。通过限制梯度的幅度，它可以约束模型参数的更新，可能带来更平滑的优化路径并防止过拟合。在某些情况下，梯度裁剪可以有效地减少某些参数的影响，尤其是当这些参数的梯度持续被裁剪时。这可能导致一种隐式稀疏性，即不那么重要的参数被有效地降低权重。
- en: 'Noise injection is a regularization technique commonly used to improve the
    generalization of machine learning models. By adding a small amount of noise to
    the input data, weights, or activation functions, noise injection helps prevent
    overfitting. The technique forces the model to be less reliant on specific patterns
    in the training data, encouraging it to learn more robust, general features that
    apply across different datasets. This approach is particularly useful in neural
    networks, where noise such as the following can be injected at various stages:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声注入是一种常用的正则化技术，用于提高机器学习模型的泛化能力。通过向输入数据、权重或激活函数添加少量噪声，噪声注入有助于防止过拟合。该技术迫使模型对训练数据中的特定模式依赖性降低，鼓励它学习更稳健、更通用的特征，这些特征适用于不同的数据集。这种方法在神经网络中特别有用，以下噪声可以在各个阶段注入：
- en: '**Input noise**: Adds noise directly to the input data, helping the model become
    more robust to variations in the input'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入噪声**：直接向输入数据添加噪声，帮助模型对输入的变异性更加鲁棒'
- en: '**Weight noise**: Perturbs the weights during training, encouraging the model
    to generalize better'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重噪声**：在训练过程中扰动权重，鼓励模型更好地泛化'
- en: '**Activation noise**: Adds noise to the activation functions, leading to smoother
    decision boundaries and reducing overfitting'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活噪声**：向激活函数添加噪声，导致决策边界更加平滑并减少过拟合'
- en: These methods help prevent overfitting, reduce the impact of outliers, and encourage
    the model to explore a wider range of solutions, ultimately leading to more robust
    and reliable language models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法有助于防止过拟合，减少异常值的影响，并鼓励模型探索更广泛的解决方案，最终导致更稳健和可靠的语言模型。
- en: 'Here’s how to implement gradient clipping and noise injection:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何实现梯度裁剪和噪声注入的方法：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This implementation applies gradient clipping to prevent exploding gradients
    and adds small amounts of noise to the input to improve robustness. `noise_factor`
    controls the amount of noise added; you may need to adjust this based on your
    specific use case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现应用梯度裁剪以防止梯度爆炸，并向输入添加少量噪声以提高鲁棒性。`noise_factor`控制添加噪声的量；您可能需要根据您的特定用例进行调整。
- en: The function initializes an **AdamW optimizer** and iterates over the dataset
    for a specified number of epochs. During each training step, it clears old gradients,
    adds noise to input tokens (ensuring values remain within the vocabulary range),
    and feeds the noisy input into the model for forward and backward passes. **Gradient
    clipping** prevents exploding gradients, ensuring stable training. The optimizer
    updates the model parameters, and the loss is tracked to monitor progress. Finally,
    the function prints the average loss per epoch.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 函数初始化一个**AdamW优化器**，并对数据集进行指定数量的轮次迭代。在每次训练步骤中，它清除旧梯度，向输入标记添加噪声（确保值保持在词汇范围内），并将带噪声的输入送入模型进行正向和反向传播。**梯度裁剪**防止梯度爆炸，确保稳定训练。优化器更新模型参数，并跟踪损失以监控进度。最后，函数打印每轮的平均损失。
- en: Next, let us explore regularization in transfer learning and fine-tuning scenarios.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨转移学习和微调场景中的规范化。
- en: Regularization in transfer learning and fine-tuning scenarios
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转移学习和微调场景中的规范化
- en: 'When fine-tuning pre-trained LLMs, it’s important to carefully adjust regularization
    to avoid hindering task-specific adaptation while still preventing overfitting.
    Here’s an approach to fine-tuning with adaptive regularization:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调预训练的LLM时，仔细调整规范化以避免阻碍特定任务的适应同时仍然防止过拟合是很重要的。以下是一种使用自适应规范化的微调方法：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This implementation starts with a higher dropout rate and gradually decreases
    it over the course of fine-tuning. This allows the model to adapt to the new task
    while still maintaining some regularization to prevent overfitting. This approach
    is also called adaptive dropout.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现从更高的dropout率开始，并在微调过程中逐渐降低它。这允许模型适应新任务，同时仍然保持一些规范化以防止过拟合。这种方法也称为自适应dropout。
- en: Adaptive dropout works well because it dynamically adjusts dropout rates based
    on neuron importance, rather than applying uniform dropout across the network.
    By selectively dropping less critical neurons more frequently while preserving
    important feature detectors, adaptive dropout creates an optimal balance between
    regularization and information preservation. This targeted approach prevents overfitting
    more efficiently than standard dropout, as it maintains the network’s capacity
    to learn complex patterns through important neurons while aggressively regularizing
    redundant or noise-sensitive parts, resulting in models that generalize better
    with less performance sacrifice on critical features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应dropout之所以有效，是因为它根据神经元的重要性动态调整dropout率，而不是在整个网络中应用均匀的dropout。通过选择性更频繁地丢弃不那么关键的神经元，同时保留重要的特征检测器，自适应dropout在规范化和信息保留之间创造了一个最佳平衡。这种有针对性的方法比标准dropout更有效地防止过拟合，因为它通过重要的神经元保持网络的复杂模式学习能力，同时积极规范化冗余或噪声敏感的部分，从而产生泛化能力更强且在关键特征上性能损失较小的模型。
- en: Emerging regularization techniques
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 出现的规范化技术
- en: Recent years have seen the emergence of sophisticated techniques that address
    the complex challenges of modern deep learning architectures. These new approaches
    go beyond simply preventing overfitting – they aim to improve model robustness,
    find better optima in the loss landscape, and enhance generalization through innovative
    training strategies. From geometrically motivated methods such as **sharpness-aware
    minimization** (**SAM**) to advanced optimization strategies such as **stochastic
    weight averaging** (**SWA**), these emerging regularization techniques are reshaping
    how we approach model training and generalization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，出现了解决现代深度学习架构复杂挑战的复杂技术。这些新方法不仅超越了简单地防止过拟合，它们旨在提高模型鲁棒性，在损失景观中找到更好的极值，并通过创新的训练策略增强泛化。从几何启发方法如**削弱度感知最小化**（**SAM**）到高级优化策略如**随机权重平均**（**SWA**），这些新兴的规范化技术正在重塑我们处理模型训练和泛化的方式。
- en: Stochastic weight averaging
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机权重平均
- en: SWA is a technique that improves neural network generalization by averaging
    weights from multiple points along the optimization trajectory, effectively finding
    flatter, more robust minima that perform better on unseen data than the typically
    sharp minima found by conventional optimization methods. **Stochastic gradient
    descent** (**SGD**) is a fundamental optimization algorithm that updates model
    parameters by following the negative gradient of the loss function computed on
    randomly selected small batches of training data, enabling efficient training
    of large models such as neural networks by approximating the full gradient computation
    while introducing beneficial noise that helps escape poor local minima.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SWA是一种通过平均优化轨迹上的多个点的权重来提高神经网络泛化能力的技巧，有效地找到更平坦、更稳健的极小值，这些极小值在未见过的数据上的表现优于传统优化方法通常找到的尖锐极小值。**随机梯度下降**（**SGD**）是一种基本的优化算法，通过跟踪在随机选择的训练数据小批次上计算的损失函数的负梯度来更新模型参数，通过近似全梯度计算同时引入有益的噪声来帮助逃离不良局部极小值，从而实现大型模型（如神经网络）的高效训练。
- en: 'WA involves averaging multiple points along the trajectory of SGD with a modified
    learning rate schedule. It improves generalization by finding broader optima.
    Here is a code example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: WA涉及使用修改后的学习率计划对SGD轨迹上的多个点进行平均。它通过找到更广泛的极值来提高泛化能力。以下是一个代码示例：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sharpness-aware minimization
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 削弱度感知最小化
- en: 'SAM seeks parameters that lie in neighborhoods with uniformly low loss values,
    leading to better generalization. Its key features are the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SAM寻求位于具有均匀低损失值邻域中的参数，从而实现更好的泛化。其关键特性如下：
- en: Looks for “flat” minima instead of sharp ones
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找“平坦”的极小值而不是尖锐的极小值
- en: Improves robustness against input perturbations
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高了对输入扰动的鲁棒性
- en: Generally provides better generalization than standard SGD
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常比标准的SGD提供更好的泛化能力
- en: 'Let us implement the `SAM` class in the following Python code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下面的Python代码中实现`SAM`类：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Differential privacy-based regularization
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于差分隐私的正则化
- en: '**Differential privacy** (**DP**) is a technique that adds carefully calibrated
    noise to data or computations to protect individual privacy while still allowing
    useful insights, ensuring that the inclusion or exclusion of any single data point
    does not significantly affect model performance.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**差分隐私**（**DP**）是一种技术，通过对数据或计算添加精心校准的噪声来保护个人隐私，同时仍然允许有用的见解，确保任何单个数据点的包含或排除都不会显著影响模型性能。'
- en: DP-based regularization is a technique used to enhance model privacy by adding
    noise to the model’s training process, which protects individual data points from
    being exposed in model outputs or learned representations. By introducing controlled
    randomness, DP-based regularization limits the model’s reliance on any specific
    data sample, thereby reducing the risk of overfitting and making the model less
    sensitive to variations in individual data points. This method is particularly
    valuable in privacy-sensitive applications, as it ensures that models can learn
    generalizable patterns without revealing specific information about the training
    data, making it useful in healthcare, finance, and other areas requiring data
    confidentiality.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DP的正则化是一种技术，通过向模型的训练过程中添加噪声来增强模型隐私，从而保护个人数据点不被暴露在模型输出或学习表示中。通过引入受控的随机性，基于DP的正则化限制了模型对任何特定数据样本的依赖，从而降低了过拟合的风险，并使模型对个人数据点的变化不那么敏感。这种方法在需要数据保密的应用中特别有价值，因为它确保模型可以在不泄露训练数据具体信息的情况下学习可泛化的模式，使其在医疗保健、金融和其他需要数据保密的领域非常有用。
- en: 'The following code snippet implements the `DPOptimizer` class:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段实现了`DPOptimizer`类：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Fast gradient sign method
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速梯度符号方法
- en: The **fast gradient sign method** (**FGSM**) is a technique for creating adversarial
    examples by adding a small, targeted perturbation to input data, pushing the model
    to misclassify. It works by calculating the gradient of the loss function with
    respect to the input and applying a slight adjustment in the direction that maximizes
    the model’s error. The input data is slightly changed by a small amount, controlled
    by a factor called ϵ, to create an “adversarial example” that can fool a machine
    learning model. FGSM is commonly used to test model robustness and for adversarial
    training, where models are trained on adversarial examples to enhance security.
    However, FGSM’s one-step nature makes it fast but less effective against strong
    defenses, unlike iterative methods that achieve higher attack success.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速梯度符号方法**（**FGSM**）是一种通过向输入数据添加小的、有针对性的扰动来创建对抗性样本的技术，使模型误分类。它通过计算损失函数相对于输入的梯度并应用一个轻微调整，以最大化模型错误的方向来实现。输入数据通过一个称为ϵ的因子控制的微小量进行轻微改变，以创建一个可以欺骗机器学习模型的“对抗性示例”。FGSM通常用于测试模型鲁棒性和对抗性训练，其中模型在对抗性示例上进行训练以增强安全性。然而，FGSM的单步特性使其快速但对抗强大防御的效果较差，与实现更高攻击成功率的迭代方法不同。'
- en: 'Let us see how it is implemented here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何在这里实现的：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Lookahead optimizer
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 看前优化器
- en: 'The lookahead optimizer is an innovative optimization technique that enhances
    the training stability and convergence of traditional optimizers, such as Adam
    or SGD, by maintaining two sets of parameters: fast weights and slow weights.
    The fast weights are updated frequently using a standard optimizer, while the
    slow weights are updated less frequently by synchronizing them with the fast weights.
    This approach allows for better exploration of the loss landscape, as the optimizer
    can escape local minima and smooth out oscillations in the optimization trajectory.
    By leveraging the strengths of both the base optimizer and the lookahead mechanism,
    this optimizer leads to faster convergence and improved generalization, making
    it a valuable addition to deep learning model training.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 预视优化器是一种创新的优化技术，通过维持两组参数：快速权重和慢速权重，来增强传统优化器（如Adam或SGD）的训练稳定性和收敛性。快速权重通过使用标准优化器频繁更新，而慢速权重通过与其同步更新来较少地更新。这种方法允许优化器更好地探索损失景观，因为优化器可以逃离局部最小值并平滑优化轨迹中的振荡。通过利用基础优化器和预视机制的优势，这种优化器实现了更快的收敛和更好的泛化，使其成为深度学习模型训练中的一个宝贵补充。
- en: 'The following code snippet shows how the lookahead optimizer can be implemented:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何实现预视优化器：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered fundamental concepts such as weight decay and L2
    regularization, dropout methods, layer-wise adaptive regularization, and combining
    multiple regularization approaches. We also discussed regularization strategies
    for transfer learning and fine-tuning scenarios, as well as techniques for enhancing
    model stability, such as gradient clipping and noise injection. Additionally,
    we introduced various emerging regularization methods.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了诸如权重衰减和L2正则化、dropout方法、逐层自适应正则化以及结合多种正则化方法等基本概念。我们还讨论了迁移学习和微调场景下的正则化策略，以及增强模型稳定性的技术，例如梯度裁剪和噪声注入。此外，我们还介绍了各种新兴的正则化方法。
- en: In the next chapter, we’ll explore checkpointing and recovery and investigate
    why these techniques are essential for managing long-running training processes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨检查点和恢复技术，并研究为什么这些技术对于管理长时间运行的训练过程至关重要。
