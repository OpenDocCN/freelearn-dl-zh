- en: Generative Adversarial Networks Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络基础
- en: '**Generative Adversarial Networks** (**GANs**) have brought about a revolutionary
    storm in the **machine learning** (**ML**) community. They, to some extent, have
    changed the way people solve practical problems in **Computer Vision** (**CV**)
    and **Natural Language Processing** (**NLP**). Before we dive right into the storm,
    let''s prepare you with the fundamental insights of GANs.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）在 **机器学习**（**ML**）社区中掀起了一场革命性的风暴。它们在一定程度上改变了人们解决 **计算机视觉**（**CV**）和
    **自然语言处理**（**NLP**）实际问题的方式。在我们深入探讨这场风暴之前，让我们先为你准备一些关于 GAN 的基础知识。'
- en: In this chapter, you will understand the idea behind adversarial learning and
    the basic components of a GAN model. You will also get a brief understanding on
    how GANs work and how it can be built with NumPy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将理解对抗学习背后的理念和 GAN 模型的基本组成部分。你还将简要了解 GAN 的工作原理，以及如何使用 NumPy 构建 GAN。
- en: Before we start exploiting the new features in PyTorch, we will first learn
    to build a simple GAN with NumPy to generate sine signals so that you may have
    a profound understanding of the mechanism beneath GANs. By the end of this chapter,
    you may relax a little as we walk you through many showcases about how GANs are
    used to address practical problems in CV and NLP fields.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始探索 PyTorch 中的新特性之前，我们将首先学习如何用 NumPy 构建一个简单的 GAN 来生成正弦信号，以便你能深刻理解 GAN 的机制。在本章结束时，你可以稍微放松一下，我们将通过多个实例展示
    GAN 如何在 CV 和 NLP 领域解决实际问题。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Fundamentals of machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: Generator and discriminator networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器和判别器网络
- en: What GAN we do?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN 的作用是什么？
- en: References and a useful reading list
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考资料和有用的阅读清单
- en: Fundamentals of machine learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: 'To introduce how GANs work, let''s use an analogy:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍 GAN 的工作原理，我们可以用一个类比：
- en: A long, long time ago, there were two neighboring kingdoms on an island. One
    was called Netland, and the other was called Ganland. Both kingdoms produced fine
    wine, armor, and weapons. In Netland, the king demanded that the blacksmiths who
    specialized in making armor worked at the east corner of the castle, while those
    who made swords worked at the west side so that the lords and knights could choose
    the best equipment the kingdom had to offer. The king of Ganland, on the other
    hand, put all of the blacksmiths in the same corner and demanded that the armor
    makers and sword makers should test their work against each other every day. If
    a sword broke through the armor, the sword would sell at a good price and the
    armor would be melted and reforged. If it didn't, the sword would be remade and
    men would strive to buy the armor. One day, the two kings were arguing over which
    kingdom made better wine until the quarrel escalated into war. Though outnumbered,
    the soldiers of Ganland wore the armor and swords that had been improved for years
    in the daily adversarial tests, and the Netland soldiers could not break their
    strong armor nor withstand their sharp swords. In the end, the defeated king of
    Netland, however reluctant he was, agreed that Ganland had better wine and blacksmiths.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 很久很久以前，在一个岛屿上有两个相邻的王国，一个叫做 Netland，另一个叫做 Ganland。两个王国都生产优质的酒、盔甲和武器。在 Netland，国王要求专门制造盔甲的铁匠在城堡的东角工作，而制造剑的铁匠则在西侧工作，以便贵族和骑士可以选择王国提供的最佳装备。而
    Ganland 的国王则把所有铁匠安排在同一个角落，并要求盔甲制造者和剑匠每天互相测试他们的作品。如果剑能突破盔甲，那么这把剑就会以好价钱卖出，而盔甲则会被熔化重新铸造。如果不能，剑将被重做，人们也会争相购买盔甲。一天，两个国王因哪一个王国酿造的酒更好而争论不休，直到争吵升级为战争。尽管
    Ganland 的士兵数量较少，但他们身穿经过多年日常对抗测试改进的盔甲，手持锐利的剑，而 Netland 的士兵既无法突破他们强大的盔甲，也无法抵挡他们锋利的剑。最终，尽管
    Netland 的国王不情愿，还是承认 Ganland 的酒和铁匠技艺更胜一筹。
- en: Machine learning – classification and generation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习 – 分类与生成
- en: '**ML** is the study of recognizing patterns from data without hardcoded rules
    given by humans. The recognizing of patterns (**Pattern Recognition** or **PR**)
    is the automatic discovering of the similarities and differences among raw data,
    which is an essential way to realize **Artificial Intelligence** (**AI**) that
    only exists in novels and movies. Although it is hard to tell when exactly real
    AI will come to birth in the future, the development of ML has given us much confidence
    in recent years. ML has already been vastly used in many fields, such as CV, NLP,
    recommendation systems, **Intelligent Transportation Systems** (**ITS**), medical
    diagnoses, robotics, and advertising.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习（ML）**是研究如何从数据中识别模式，而无需人为硬编码规则。模式识别（**Pattern Recognition**或**PR**）是自动发现原始数据中的相似性和差异性，这是实现**人工智能（AI）**的关键方式，人工智能这一概念最初只存在于小说和电影中。虽然我们很难确切预测未来何时会出现真正的人工智能，但近年来机器学习的发展已经让我们充满信心。机器学习已经在许多领域得到了广泛应用，如计算机视觉（CV）、自然语言处理（NLP）、推荐系统、**智能交通系统（ITS）**、医疗诊断、机器人技术和广告。'
- en: A ML model is typically described as a system that takes in data and gives certain
    outputs based on the parameters it contains. The **learning** of the model is
    actually adjusting the parameters to get better outputs. As illustrated in the
    following diagram, we feed training data into the model and get a certain output.
    We then use one or several criteria to measure the output, to tell how well our
    model performs. In this step, a set of desired outputs (or ground truth) with
    respect to the training data would be very helpful. If ground truth data is used
    in training, this process is often called **supervised learning**. If not, it
    is often regarded as **unsupervised learning**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习（ML）模型通常被描述为一个接受数据并根据其包含的参数给出某些输出的系统。模型的**学习**实际上是调整参数以获得更好的输出。如下面的图所示，我们将训练数据输入模型，得到一个输出。然后，我们使用一个或多个标准来衡量输出，以判断我们的模型表现如何。在这一步，关于训练数据的一组期望输出（或实际结果）将非常有帮助。如果训练中使用了实际结果数据，这个过程通常被称为**监督学习**。如果没有使用，则通常被视为**无监督学习**。
- en: 'We constantly adjust the model''s parameters based on its performance (in other
    words, whether it gives us the results we want) so that it yields better results
    in the future. This process is called **model training**. The training of a model
    takes as long as it pleases us. Typically, we stop the training after a certain
    number of iterations or when the performance is good enough. When the training
    process has finished, we apply the trained model to predict on new data (testing
    data). This process is called **model testing**. Sometimes, people use different
    sets of data for training and testing to see how well the model performs on samples
    it never meets, which is called the **generalization** capability. Sometimes an
    additional step called **model** **evaluation **is involved, when the parameters
    of the model are so complicated that we need another set of data to see whether
    our model or training process has been designed well:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据模型的表现（换句话说，是否给出我们想要的结果）不断调整模型的参数，以便未来能够产生更好的结果。这个过程叫做**模型训练**。模型的训练时间由我们决定。通常，我们在经过一定数量的迭代后，或者当模型的表现足够好时，就会停止训练。训练过程完成后，我们将训练好的模型应用于对新数据（测试数据）的预测，这个过程叫做**模型测试**。有时，人们会使用不同的数据集进行训练和测试，以观察模型在遇到从未见过的样本时的表现，这被称为**泛化**能力。有时，模型评估（**model
    evaluation**）是一个额外的步骤，当模型的参数过于复杂，我们需要使用另一组数据来验证我们的模型或训练过程是否设计得当。
- en: '![](img/8d437726-177b-4779-8b14-90d062d9a810.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d437726-177b-4779-8b14-90d062d9a810.png)'
- en: A typical machine learning system, with model training and testing
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习系统，包括模型训练和测试
- en: What types of problems this model can solve is essentially determined by the
    types of input and output data we want. For example, a classification model takes
    an input of any number of dimensions (audio, text, image, or video) and gives
    a 1-dimension output (single values indicating the predicted labels). A generative
    model typically takes a 1-dimension input (a latent vector) and generates high-dimension
    outputs (images, videos, or 3D models). It maps low-dimensional data to high-dimensional
    data, at the same time, trying to make the output samples look as convincing as
    possible. However, it is worth pointing out that we'll meet generative models
    that don't obey this rule in the future chapters. Until [Chapter 5](685b2621-6dbb-4157-a258-f3cf2825728c.xhtml),
    *Generating Images Based on Label Information*, it's a simple rule to bear in
    mind.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以解决哪些类型的问题，实质上取决于我们想要的输入和输出数据的类型。例如，分类模型接受任意维度的数据输入（如音频、文本、图像或视频），并给出一维的输出（表示预测标签的单一值）。而生成模型通常接受一维输入（潜在向量），并生成高维的输出（图像、视频或3D模型）。它将低维数据映射到高维数据，同时尽力使输出样本看起来尽可能真实。然而，值得指出的是，在后续章节中，我们将遇到不遵循这一规则的生成模型。直到[第5章](685b2621-6dbb-4157-a258-f3cf2825728c.xhtml)，*基于标签信息生成图像*，这是一个需要牢记的简单规则。
- en: When it comes to AI, there are two groups of believers in the community. The
    symbolists acknowledge the necessity of human experience and knowledge. They believe
    the low-level patterns constitute high-level decisions based on explicit rules
    given by humans. The connectionists believe that AI can be realized by an analogous
    network similar to human neural systems and adjusting the connections between
    simple neurons is the key to this system. Apparently, the exploding development
    of deep learning adds a score to the connectionists' side. What is your opinion?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 说到AI，社区里有两派信徒。符号主义者承认人类经验和知识的必要性。他们认为低级模式构成了基于人类明确规则的高级决策。连接主义者则认为，AI可以通过类似于人类神经系统的类比网络实现，调整简单神经元之间的连接是这个系统的关键。显然，深度学习的爆炸性发展为连接主义者的一方加分。你怎么看？
- en: Introducing adversarial learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入对抗学习
- en: Traditionally, generative problems are solved by statistics-based methods such
    as a **Boltzmann machine**, **Markov chain**, or **variational encoder**. As mathematically
    profound as they are, the generated samples are as of yet far from perfect. A
    classification model maps high-dimension data to low-dimension, while a generative
    model often maps low-dimension data to high-dimension ones. People in both fields
    have been working hard to improve their models. Let's look back to the little
    made-up opening story. Can we get the two different models to work against each
    other and improve themselves at the same time? If we take the output of a generative
    model as the input of the classification model, we can measure the performance
    of the generative model (the armor) with the classification model (the sword).
    At the same time, we can improve the classification model (the sword) by feeding
    generated samples (the armor) along with real samples, since we can agree that
    more data is often better for the training of ML models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，生成问题是通过基于统计的方法解决的，如**玻尔兹曼机**、**马尔可夫链**或**变分编码器**。尽管这些方法在数学上非常深奥，但生成的样本还远未完美。分类模型将高维数据映射到低维数据，而生成模型通常将低维数据映射到高维数据。两者领域的人们一直在努力改进他们的模型。让我们回顾一下那个虚构的开场故事。我们能让这两种不同的模型相互对抗并同时提升自己吗？如果我们把生成模型的输出作为分类模型的输入，就可以用分类模型（剑）来衡量生成模型（盔甲）的性能。与此同时，我们也可以通过将生成样本（盔甲）与真实样本一起输入，来改善分类模型（剑），因为我们都同意更多的数据通常有利于机器学习模型的训练。
- en: 'The training process where the two models try to weaken each other and, as
    a result, improve each other is called **adversarial learning**. As demonstrated
    in the following diagram, the models, A and B, have totally opposite agendas (for
    example, classification and generation). However, during each step of the training,
    the output of Model A improves Model B, and the output of Model B improves Model
    A:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型相互削弱并因此相互提升的训练过程被称为**对抗学习**。如下面的图所示，模型A和B有完全相反的目标（例如分类和生成）。然而，在每一步训练过程中，模型A的输出提升了模型B，而模型B的输出又提升了模型A：
- en: '![](img/d722cae9-5f24-4757-a699-30cda072c9e3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d722cae9-5f24-4757-a699-30cda072c9e3.png)'
- en: A typical adversarial learning system
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的对抗学习系统
- en: '**GANs** are designed based on this very idea, which was proposed by Goodfellow,
    Pouget-Abadie, Mirza, et al in 2014\. Now, GANs have become the most thriving
    and popular method to synthesize audio, text, images, video, and 3D models in
    the ML community. In this book, we will walk you through the basic components
    and mechanisms of different types of GANs and learn how to use them to address
    various practical problems. In the next section, we will introduce the basic structure
    of GANs to show you how and why they work so well.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络（GANs）** 是基于这一理念设计的，该理念由Goodfellow、Pouget-Abadie、Mirza等人在2014年提出。如今，生成对抗网络已成为机器学习领域中最为繁荣和流行的方法，用于合成音频、文本、图像、视频和3D模型。在本书中，我们将带您了解不同类型生成对抗网络的基本组成部分和机制，并学习如何使用它们解决各种实际问题。在接下来的部分，我们将介绍生成对抗网络的基本结构，以便向您展示它们是如何以及为何能如此有效地工作的。'
- en: Generator and discriminator networks
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器和判别器网络
- en: 'Here, we will show you the basic components of GANs and explain how they work
    with/against each other to achieve our goal to generate realistic samples. A typical
    structure of a GAN is shown in the following diagram. It contains two different
    networks: a generator network and a discriminator network. The **generator** network
    typically takes random noises as input and generates fake samples. Our goal is
    to let the fake samples be as close to the real samples as possible. That''s where
    the discriminator comes in. The **discriminator** is, in fact, a classification
    network, whose job is to tell whether a given sample is fake or real. The generator
    tries its best to trick and confuse the discriminator to make the wrong decision,
    while the discriminator tries its best to distinguish the fake samples from the
    real ones.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将展示生成对抗网络的基本组件，并解释它们如何相互作用或对立，以实现我们生成真实样本的目标。以下是生成对抗网络的典型结构示意图。它包含两个不同的网络：生成器网络和判别器网络。**生成器**
    网络通常以随机噪声为输入，并生成假样本。我们的目标是让这些假样本尽可能接近真实样本。此时，判别器就发挥了作用。**判别器** 实际上是一个分类网络，它的任务是判断给定的样本是假的还是现实的。生成器尽力欺骗并混淆判别器，使其做出错误的判断，而判别器则尽力区分真假样本。
- en: 'In this process, the differences between fake and real samples are used to
    improve the generator. Therefore, the generator gets better at generating realistic-looking
    samples while the discriminator gets better at picking them out. Since real samples
    are used to train the discriminator, the training process is therefore supervised.
    Even though the generator always gives fake samples without the knowledge of ground
    truth, the overall training of GAN is still **supervised**:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，通过利用真假样本之间的差异来改进生成器。因此，生成器在生成看起来更真实的样本方面会变得越来越好，而判别器在识别这些样本时也会变得越来越强。由于使用真实样本来训练判别器，因此这个训练过程是有监督的。尽管生成器在没有真实标签的情况下总是生成假样本，生成对抗网络的整体训练仍然是**有监督的**：
- en: '![](img/e656f056-7176-4841-8ab2-84e6d1b0207e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e656f056-7176-4841-8ab2-84e6d1b0207e.png)'
- en: Basic process of a GAN
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络的基本过程
- en: Mathematical background of GANs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络的数学背景
- en: 'Let''s take a look at the math behind this process to get a better understanding
    of the mechanism. Let ![](img/e4fdcaef-f6c7-4251-a263-378e85581a51.png) and ![](img/a0f5ff27-298f-4194-b834-47e74a4de2b5.png) represent
    the generator and discriminator networks, respectively. Let ![](img/5358da90-7104-466b-be88-de95b9cb655c.png) represent
    the performance criterion of the system. The optimization objective is described
    as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个过程背后的数学原理，以便更好地理解其机制。假设 ![](img/e4fdcaef-f6c7-4251-a263-378e85581a51.png)
    和 ![](img/a0f5ff27-298f-4194-b834-47e74a4de2b5.png) 分别代表生成器和判别器网络。假设 ![](img/5358da90-7104-466b-be88-de95b9cb655c.png)
    代表系统的性能标准。优化目标描述如下：
- en: '![](img/7629f5ad-ff01-49bf-9ad6-352d182d819a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7629f5ad-ff01-49bf-9ad6-352d182d819a.png)'
- en: In this equation, ![](img/c7c4bd1a-b0be-4fb7-afb3-bde5f3740804.png) is the real
    sample, ![](img/84f608eb-69dc-45b5-826f-6ff15659158b.png) is the generated sample,
    and ![](img/a97b65f1-02ea-45fb-b1d4-5388d6d06533.png) is the random noise that ![](img/5e2f7c11-0834-46eb-9358-5696a9e2ab65.png) uses
    to generate fake samples. ![](img/9df998ed-dd13-4c2e-97bf-21c29d75270d.png) is
    the expectation over ![](img/2264cd61-a60b-4641-92ab-0ac367b897ee.png), which
    means the average value of any function, ![](img/a5f718a9-f8bc-4a3f-b73b-ac07079acc1e.png), over
    all samples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![](img/c7c4bd1a-b0be-4fb7-afb3-bde5f3740804.png)是实际样本，![](img/84f608eb-69dc-45b5-826f-6ff15659158b.png)是生成的样本，![](img/a97b65f1-02ea-45fb-b1d4-5388d6d06533.png)是![](img/5e2f7c11-0834-46eb-9358-5696a9e2ab65.png)用来生成假样本的随机噪声。![](img/9df998ed-dd13-4c2e-97bf-21c29d75270d.png)是对![](img/2264cd61-a60b-4641-92ab-0ac367b897ee.png)的期望，表示对所有样本中任何函数![](img/a5f718a9-f8bc-4a3f-b73b-ac07079acc1e.png)的平均值。
- en: 'As mentioned before, the goal of the discriminator, ![](img/c8f7d82c-acd3-44cc-b621-1349279a4cfb.png), is
    to maximize the prediction confidence of real samples. Therefore, ![](img/757e3f28-fb7d-427c-b8e1-fc4ee1c135cd.png) needs
    to be trained with **gradient ascent** (the ![](img/7be72e8f-bfb8-4247-8c3b-3d61c34bbbff.png) operator
    in the objective). The update rule for, ![](img/ef174eed-8739-4cb1-87ee-1d477315c13e.png), is
    as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，判别器的目标，![](img/c8f7d82c-acd3-44cc-b621-1349279a4cfb.png)，是最大化真实样本的预测置信度。因此，![](img/757e3f28-fb7d-427c-b8e1-fc4ee1c135cd.png)
    需要通过**梯度上升**（目标中的![](img/7be72e8f-bfb8-4247-8c3b-3d61c34bbbff.png)算子）进行训练。更新规则如下：
- en: '![](img/26319362-d37f-4fee-80bc-20ea6964f938.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26319362-d37f-4fee-80bc-20ea6964f938.png)'
- en: In this formula, ![](img/2a2c4476-f6b1-4742-823c-987aea9ca695.png) is the parameter
    of ![](img/4126c5f6-8a46-49d0-9773-29c51ddb4bc4.png) (such as convolution kernels
    and weights in fully-connected layers), ![](img/52e8bf3e-64f4-4da7-85a4-24aa6a5f0a18.png) is
    the size of the mini batch (or batch size for short), and ![](img/e24446fd-41d9-41c5-b767-89690d10ec97.png) is
    the index of the sample in the mini-batch. Here, we assume that we are using mini-batches
    to feed the training data, which is fairly reasonable since it's the most commonly
    used and empirically effective strategy. Therefore, the gradients need to be averaged
    over ![](img/d9812252-2aa9-4ef4-b0b6-5ec52c08c78b.png) samples.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/2a2c4476-f6b1-4742-823c-987aea9ca695.png)是![](img/4126c5f6-8a46-49d0-9773-29c51ddb4bc4.png)的参数（例如卷积核和全连接层中的权重），![](img/52e8bf3e-64f4-4da7-85a4-24aa6a5f0a18.png)是小批量的大小（简称批量大小），![](img/e24446fd-41d9-41c5-b767-89690d10ec97.png)是小批量中样本的索引。这里，我们假设使用小批量来输入训练数据，这是一种相当合理的假设，因为这是最常用且经验上有效的策略。因此，梯度需要在![](img/d9812252-2aa9-4ef4-b0b6-5ec52c08c78b.png)样本上进行平均。
- en: 'There are 3 different ways to feed training data into models: (1) one sample
    at a time, which is often referred to as **stochastic** (for example, **Stochastic
    Gradient Descent** or** SGD**); (2) a handful of samples at a time, which is called
    **mini**-**batch**; and (3) all samples at one time, which is, in fact, called
    **batch**. The stochastic way introduces too much randomness so that one bad sample
    could jeopardize the good work of several previous training steps. The full batch
    requires too much memory to calculate. Therefore, we feed data to all of the models
    by mini-batch in this book, even though we might slothfully refer to it as just
    batch.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 向模型提供训练数据有三种不同的方式：（1）一次一个样本，这通常称为**随机**（例如，**随机梯度下降**或**SGD**）；（2）一次几个样本，这称为**小批量**；（3）一次所有样本，这实际上称为**批量**。随机方式引入了过多的随机性，导致一个坏样本可能会危及之前几个训练步骤的良好效果。全批量需要过多的内存来计算。因此，在本书中，我们通过小批量将数据提供给所有模型，尽管我们可能懒散地称其为批量。
- en: 'The goal of the generator network, ![](img/99821001-761d-489e-b507-b024acb31baf.png), is
    to fool the discriminator, ![](img/1d1ef16d-f059-4fa9-ad79-5f3bc703dd35.png), and
    let ![](img/0f2bdefe-7641-4334-8ad1-845340116f44.png) believe that the generated
    samples are real. Therefore, the training of ![](img/b23e0a77-9872-4650-b680-cc43b69a698f.png) is
    to maximize ![](img/3e009fdd-496e-490d-96ec-b4e9dd4f056e.png) or minimize ![](img/87430d00-7be9-43a2-9378-602ed797e643.png).
    Therefore, ![](img/71036e04-30c2-49b9-be85-0064aae009be.png) needs to be trained
    with **gradient descent** (the ![](img/32d44fa3-3ea0-44ce-9054-56489723c54d.png) operator
    in the objective). The update rule for ![](img/251e6d73-6aad-4e01-8d2b-fa4f4e755424.png) is
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络的目标，![](img/99821001-761d-489e-b507-b024acb31baf.png)，是欺骗判别器，![](img/1d1ef16d-f059-4fa9-ad79-5f3bc703dd35.png)，让
    ![](img/0f2bdefe-7641-4334-8ad1-845340116f44.png) 认为生成的样本是真实的。因此，![](img/b23e0a77-9872-4650-b680-cc43b69a698f.png)
    的训练就是最大化 ![](img/3e009fdd-496e-490d-96ec-b4e9dd4f056e.png) 或最小化 ![](img/87430d00-7be9-43a2-9378-602ed797e643.png)。因此，![](img/71036e04-30c2-49b9-be85-0064aae009be.png)
    需要通过 **梯度下降** 进行训练（目标函数中的 ![](img/32d44fa3-3ea0-44ce-9054-56489723c54d.png) 操作符）。![](img/251e6d73-6aad-4e01-8d2b-fa4f4e755424.png)
    的更新规则如下：
- en: '![](img/33b9dec3-6746-48c0-8fae-5996ceeedb2a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33b9dec3-6746-48c0-8fae-5996ceeedb2a.png)'
- en: In this formula, ![](img/4e9f2bc5-1dca-4d79-8165-8ea31c605113.png) is the parameters
    of ![](img/a3e44616-38e4-4983-9b89-275d94dc1edf.png)<q>,</q> ![](img/3417277d-b9e1-4f8a-bbfb-3caa4be5f150.png) is
    the size of the mini-batch, and ![](img/21482551-6613-41e5-aa29-1313c5155c0a.png) is
    the index of the sample in the mini-batch.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/4e9f2bc5-1dca-4d79-8165-8ea31c605113.png) 是 ![](img/a3e44616-38e4-4983-9b89-275d94dc1edf.png)<q>，</q>
    的参数，![](img/3417277d-b9e1-4f8a-bbfb-3caa4be5f150.png) 是迷你批次的大小，![](img/21482551-6613-41e5-aa29-1313c5155c0a.png)
    是迷你批次中样本的索引。
- en: If you are unfamiliar with the concept of GD, think of it as a little boy kicking
    a sticky ball on bumpy terrain. The boy wants the ball to be at the bottom of
    the lowest pit so that he can call it a day and go home. The ball is sticky so
    it doesn't roll after it hits the ground, even on a slope. Therefore, where the
    ball will hit is determined by which direction and how hard the boy kicks it.
    The amount of force the boy kicks the ball with is described by the step size
    (or the **learning rate**). The direction of kicking is determined by the characteristics
    of the terrain under his feet. An efficient choice would be the downhill direction,
    which is the negative gradient of the loss function with respect to the parameters.
    Therefore, we often use gradient descent to minimize an objective function. However,
    the boy is so obsessed with the ball that he only stares at the ball and refuses
    to look up to find the lowest pit in a wider range. Therefore, the GD method is
    sometimes inefficient because it takes a very long time to reach the bottom. We
    will introduce several tips on how to improve the efficiency of GD in [Chapter
    3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml), *Best Practices for Model Design
    and Training*. The **gradient ascent** is the opposite of gradient descent, which
    is to find the highest peak.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉梯度下降（GD）的概念，可以把它想象成一个小男孩在崎岖的地形上踢一个粘乎乎的球。小男孩希望球停在最低的坑里，这样他就可以结束今天的任务回家了。球是粘的，所以它在落地后不会滚动，即使是在斜坡上。因此，球停在哪里是由小男孩踢球的方向和力度决定的。小男孩踢球的力度由步长（或
    **学习率**）来描述。踢球的方向则由他脚下地形的特征决定。一个有效的选择是下坡方向，即损失函数相对于参数的负梯度。因此，我们通常使用梯度下降来最小化目标函数。然而，男孩太专注于球了，他只盯着球看，拒绝抬头去寻找更广范围内的最低坑。因此，梯度下降方法有时效率不高，因为它可能需要很长时间才能到达底部。我们将在
    [第3章](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml) *模型设计和训练的最佳实践* 中介绍一些如何提高梯度下降效率的技巧。**梯度上升**
    是梯度下降的相反操作，用于寻找最高峰。
- en: Using NumPy to train a sine signal generator
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NumPy 训练正弦信号生成器
- en: Maybe math is even more confusing than a big chunk of code to some. Now, let's
    look at some code to digest the equations we've thrown at you. Here, we will use
    Python to implement a very simple adversarial learning example to generate sine
    (sin) signals.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对一些人来说，数学可能比一大段代码还要让人困惑。现在，让我们看一些代码来消化我们刚才给你们抛出的方程式。在这里，我们将使用 Python 实现一个非常简单的对抗学习示例，用来生成正弦（sin）信号。
- en: In the following example, we will be only using NumPy, a powerful linear algebra
    Python library to implement a GAN model. We will need to calculate the gradients
    by ourselves so that you can have an in-depth understanding of what might be happening beneath
    the popular deep learning toolkits such as PyTorch. Rest assured that we won't
    do this in future chapters because we can use the powerful computational graph
    provided by PyTorch to calculate the gradients for us!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将仅使用 NumPy，一个强大的线性代数 Python 库来实现 GAN 模型。我们需要自己计算梯度，以便你能深入理解在流行的深度学习工具包（如
    PyTorch）背后可能发生的事情。放心，未来章节我们不再需要手动计算梯度，因为我们可以使用 PyTorch 提供的强大计算图来自动为我们计算梯度！
- en: Designing the network architectures
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计网络架构
- en: 'The architecture of the generator network is described in the following diagram.
    It takes a 1-dimension random value as input and gives a 10-dimension vector as
    output. It has 2 hidden layers with each containing 10 neurons. The calculation
    in each layer is a matrix multiplication. Therefore, the network is, in fact,
    a **Multilayer Perceptron** (**MLP**):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络的架构如以下图所示。它以一个一维随机值作为输入，并给出一个十维向量作为输出。它有两个隐藏层，每个隐藏层包含10个神经元。每一层的计算都是矩阵乘法。因此，该网络实际上是一个**多层感知器**（**MLP**）：
- en: '![](img/df51118f-249b-4bbd-be3a-df084d96438d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df51118f-249b-4bbd-be3a-df084d96438d.png)'
- en: Structure of the generator network
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络的结构
- en: 'The architecture of the discriminator network is described in the following
    diagram. It takes a 10-dimension vector as input and gives a 1-dimension value
    as output. The output is the prediction label (real or fake) of the input sample.
    The discriminator network is also an MLP with two hidden layers and each containing
    10 neurons:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的架构如以下图所示。它以一个十维向量作为输入，并给出一个一维值作为输出。该输出是对输入样本的预测标签（真实或假）。判别器网络同样是一个 MLP，具有两个隐藏层，每个隐藏层包含10个神经元：
- en: '![](img/c299314f-b7d9-44f0-aa27-754e7f165ab5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c299314f-b7d9-44f0-aa27-754e7f165ab5.png)'
- en: Structure of the discriminator network
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的结构
- en: Defining activation functions and the loss function
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义激活函数和损失函数
- en: We will be only using NumPy ([http://www.numpy.org](http://www.numpy.org)) to
    calculate and train our GAN model (and optionally using Matplotlib ([https://matplotlib.org](https://matplotlib.org)) to
    visualize the signals). If you don't already have the Python environment on your
    machine, please refer to [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml), *Getting
    Started with PyTorch 1.3*, to learn how to set up a working Python environment.
    If your Python environment is properly set up, let's move on to the actual code.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用 NumPy （[http://www.numpy.org](http://www.numpy.org)）来计算和训练我们的 GAN 模型（可选地使用
    Matplotlib（[https://matplotlib.org](https://matplotlib.org)）来可视化信号）。如果你的机器上还没有
    Python 环境，请参考[第2章](4459c703-9610-43e7-9eda-496d63a45924.xhtml)，*PyTorch 1.3 入门*，了解如何设置
    Python 环境。如果你的 Python 环境已正确设置，让我们开始实际代码吧。
- en: 'All of the following code can be placed in a `simple*.*py` file (such as `simple_gan.py`).
    Let''s look at the code step by step:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有代码可以放入一个 `simple*.*py` 文件中（例如 `simple_gan.py`）。我们将一步一步地查看代码：
- en: 'Import the `NumPy` library:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `NumPy` 库：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define a few constant variables that are needed in our model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型中所需的一些常量变量：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the real sine samples (with `numpy.sin`) that we want to estimate:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们想要估计的真实正弦样本（使用 `numpy.sin`）：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the previous snippet, we use a `bool` variable, `random`, to introduce randomness
    into the real samples, as real-life data has. The real samples look like this
    (50 samples with `random=True`):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们使用了一个 `bool` 变量 `random` 来引入真实样本中的随机性，因为现实生活中的数据具有这种特性。真实样本如下所示（50个样本，`random=True`）：
- en: '![](img/cb9a644e-7dcc-44bf-b223-b2d3a6945b06.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb9a644e-7dcc-44bf-b223-b2d3a6945b06.png)'
- en: The real sine samples
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 真实正弦样本
- en: 'Define the activation functions and their derivatives. If you are not familiar
    with the concept of activation functions, just remember that their jobs are to
    adjust the outputs of a layer so that its next layer can have a better understanding
    of these output values:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义激活函数及其导数。如果你不熟悉激活函数的概念，记住它们的作用是调整一层的输出，以便其下一层能够更好地理解这些输出值：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define a `helper` function to initialize the layer parameters:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `helper` 函数来初始化层的参数：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the `loss` function (both forward and backward):'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `loss` 函数（包括前向和反向）：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is called **binary cross-entropy**, which is typically used in binary classification
    problems (in which a sample either belongs to class A or class B). Sometimes,
    one of the networks is trained too well so that the `sigmoid` output of the discriminator
    might be either too close to 0 or 1\. Both of the scenarios lead to numerical
    errors of the `log` function. Therefore, we need to restrain the maximum and minimum
    values of the output value.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**二元交叉熵**，通常用于二分类问题（即一个样本要么属于类别A，要么属于类别B）。有时，某个网络训练得过好，以至于判别器的`sigmoid`输出可能过于接近0或1。两种情况都会导致`log`函数的数值错误。因此，我们需要限制输出值的最大值和最小值。
- en: Working on forward pass and backpropagation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行前向传播和反向传播
- en: 'Now, let''s create our generator and discriminator networks. We put the code
    in the same `simple_gan.py` file as well:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建生成器和判别器网络。我们将代码放在与`simple_gan.py`相同的文件中：
- en: 'Define the parameters of the generator network:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器网络的参数：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We keep track of the inputs and outputs of all the layers because we need them
    to calculate the derivatives to update the parameters later.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们跟踪所有层的输入和输出，因为我们需要它们来计算导数，以便稍后更新参数。
- en: 'Define the forward calculation (to generate samples based on random noise):'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义前向计算（根据随机噪声生成样本）：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It''s basically the same calculation process repeated 3 times. Each layer calculates
    its output according to this formula:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上是重复相同的计算过程三次。每一层都按照这个公式计算它的输出：
- en: '![](img/5b6b6174-39a7-4f27-be1c-2bda2ab5e28d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b6b6174-39a7-4f27-be1c-2bda2ab5e28d.png)'
- en: In this equation, ![](img/5daabc50-fcc2-47c3-8f4f-fee2d1f48ec9.png) represents
    the output value of a layer, <q>f</q> represents the activation function, and
    subscript <q>l</q> represents the index of the layer. Here, we use `ReLU` in the
    hidden layers and `Tanh` in the output layer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![](img/5daabc50-fcc2-47c3-8f4f-fee2d1f48ec9.png)表示某一层的输出值，<q>f</q>表示激活函数，下标<q>l</q>表示层的索引。这里我们在隐藏层使用`ReLU`，在输出层使用`Tanh`。
- en: 'Now it''s time to define the backward calculation for the generator network
    (to calculate the derivatives and update the parameters). This part of the code
    is a bit long. It''s really repeating the same process 3 times:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是定义生成器网络的反向计算的时候了（计算导数并更新参数）。这部分代码有点长，实际上是重复同样的过程三次：
- en: Calculate the derivatives of loss with respect to the output of this layer (for
    example, the derivative with respect to `output` or `x2`).
  id: totrans-83
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失相对于此层输出的导数（例如，相对于`output`或`x2`的导数）。
- en: Calculate the derivatives of loss with respect to the parameters (for example,
    the derivative with respect to `w3` or `b3`).
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失相对于参数的导数（例如，相对于`w3`或`b3`的导数）。
- en: Update the parameters with the derivatives.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用导数更新参数。
- en: 'Passing the gradients to the preceding layer. The derivatives are calculated
    as follows:'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将梯度传递给前一层。导数计算如下：
- en: '![](img/1c4e523a-2f8a-4000-8d35-8e079df29fd3.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c4e523a-2f8a-4000-8d35-8e079df29fd3.png)'
- en: '![](img/04542316-3e43-4141-944c-723a441a651e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04542316-3e43-4141-944c-723a441a651e.png)'
- en: '![](img/73bb6f13-fc16-4422-98ad-a4b39b10b11a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73bb6f13-fc16-4422-98ad-a4b39b10b11a.png)'
- en: 'In this process, the derivative of the loss with respect to the output, which
    is denoted by `delta` in the code, is the key to propagate the gradients from
    layer <q>l+1</q> to layer <q>l</q>. Therefore, this process is called **backpropagation**.
    The propagation from layer <q>l+1</q> to layer <q>l</q> is described as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，损失相对于输出的导数，在代码中表示为`delta`，是关键，它用于从层<q>l+1</q>传播梯度到层<q>l</q>。因此，这个过程被称为**反向传播**。从层<q>l+1</q>到层<q>l</q>的传播过程描述如下：
- en: '![](img/75e53cd1-5243-4db0-82ed-aad2eba959fa.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75e53cd1-5243-4db0-82ed-aad2eba959fa.png)'
- en: 'Calculate the derivatives with respect to the output:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相对于输出的导数：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Calculate the derivatives with respect to the parameters in the third layer:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 计算相对于第三层参数的导数：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Pass the gradients to the second layer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将梯度传递给第二层：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And update the parameters of the third layer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 并且更新第三层的参数：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Update the parameters in the second layer and pass the gradients to the first
    layer:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新第二层的参数并将梯度传递给第一层：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Update the parameters in the first layer:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新第一层的参数：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will notice that the following code looks similar to the preceding code.
    It is only mentioned here to point out that these lines help to keep the data
    from becoming unstable. You don''t have to add these three lines:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到以下代码与前面的代码相似。这里只是为了指出这些代码行有助于保持数据的稳定性。你不必添加这三行：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This code is included because the training of GANs can be very unstable and
    we need to clip the gradients and the parameters to ensure a stable training process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 包括这段代码是因为 GAN 的训练可能非常不稳定，我们需要裁剪梯度和参数以确保稳定的训练过程。
- en: We will elaborate on the topics of activation functions, loss functions, weight
    initialization, gradient clipping, weight clipping, and more in [Chapter 3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml),
    *Best Practices for Model Design and Training.* These are extremely useful for
    stabilizing and improving the training of GANs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第 3 章](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml)中详细阐述激活函数、损失函数、权重初始化、梯度裁剪、权重裁剪等话题，*模型设计和训练的最佳实践*。这些对于稳定和提高
    GAN 的训练非常有用。
- en: 'Now, let''s define the discriminator network:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义判别器网络：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And now define its forward calculation (to predict the label based on the input
    sample):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义它的前向计算（根据输入样本预测标签）：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we use LeakyReLU as the activation function for hidden layers and sigmoid
    for the output layer. Now, let''s define the backward calculation for the discriminator
    network (to calculate the derivatives and update the parameters):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用 LeakyReLU 作为隐藏层的激活函数，输出层使用 sigmoid 激活函数。现在，让我们定义判别器网络的反向计算（计算导数并更新参数）：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Please note that the main difference in the backward calculation of the discriminator
    is that it''s trained with gradient ascent. Therefore, to update its parameters,
    we need to add the gradients. So, in the preceding code, you will see lines like
    this that take care of it for us:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，判别器反向计算的主要区别在于它是通过梯度上升进行训练的。因此，为了更新它的参数，我们需要加上梯度。因此，在前面的代码中，你会看到类似这样的代码行，它为我们处理了这一过程：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Training our GAN model
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的 GAN 模型
- en: 'Now that all the necessary components are defined, we can begin the training
    of our GAN model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有必要的组件都已定义，我们可以开始训练我们的 GAN 模型：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As you can see from the preceding code, the training of the GAN model mainly
    has 3 steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，GAN 模型的训练主要分为 3 个步骤：
- en: Train the discriminator with real data (and recognize it as real).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用真实数据训练判别器（并将其识别为真实数据）。
- en: Train the discriminator with fake data (and recognize it as fake).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用虚假数据训练判别器（并将其识别为虚假数据）。
- en: Train the generator with fake data (and recognize it as real).
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用虚假数据训练生成器（并将其识别为真实数据）。
- en: The first two steps teach the discriminator how to tell the difference between
    real and fake data. The third step teaches the generator how to fool the discriminator
    by generating fake data that is similar to real data. This is the core idea of
    adversarial learning and the reason why GANs can generate relatively realistic
    audio, text, images, and videos.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前两步教判别器如何区分真实数据和虚假数据。第三步教生成器如何通过生成与真实数据相似的虚假数据来欺骗判别器。这是对抗学习的核心思想，也是 GAN 能够生成相对逼真的音频、文本、图像和视频的原因。
- en: Here, we use SGD to train the model for 50,000 iterations. If you are interested,
    feel free to implement a mini-batch GD to see whether it produces better results
    in a shorter time. You are also welcome to change the network architectures (for
    example, the number of layers, the number of neurons in each layer, and the data
    dimension, `X_DIM`) to see how results change with the hyperparameters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用 SGD 训练模型 50,000 次。如果你感兴趣，欢迎实现 mini-batch GD，看看它是否能在更短的时间内产生更好的结果。你也可以更改网络架构（例如层数、每层的神经元数量以及数据维度
    `X_DIM`），看看超参数变化对结果的影响。
- en: 'Finally, let''s use Matplotlib to visualize the generated samples:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用 Matplotlib 来可视化生成的样本：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It may take a few seconds to finish the training, depending on how powerful
    your CPU is. When the training is finished, the samples generated by the generator
    network may look like this (50 samples):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要几秒钟，具体取决于你的 CPU 性能。训练完成后，生成器网络生成的样本可能看起来像这样（50 个样本）：
- en: '![](img/d3c5a92f-6bb4-4362-adc7-aa78d6eb8184.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3c5a92f-6bb4-4362-adc7-aa78d6eb8184.png)'
- en: The generated sine samples
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的正弦样本
- en: Pretty convincing, right? It's amazing to see how it captures the peaks and
    valleys of the original sine waves. Imagine what GANs are capable of with much
    more complex structures!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 相当有说服力吧？看到它如何捕捉原始正弦波的峰值和谷值真是令人惊叹。想象一下，GAN 在更复杂结构下能够做什么！
- en: What GAN we do?
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN 能做什么？
- en: GANs can do a lot more than generating sine signals. We can apply GANs to address
    many different practical problems by altering the input and output dimensions
    of the generator and combining them with other methods. For example, we can generate
    text and audio (1-dimension), images (2-dimension), video, and 3D models (3-dimension)
    based on random input. If we keep the same dimension of input and output, we can
    perform denoising and translation on these types of data. We can feed real data
    into the generator and let it output data with larger dimensions, for example,
    image super-resolution. We can also feed one type of data and let it give another
    type of data, for example, generate audio based on text, generate images based
    on text, and so on.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 能做的事情远不止生成正弦信号。通过改变生成器的输入和输出维度并结合其他方法，我们可以将 GAN 应用到许多不同的实际问题中。例如，我们可以基于随机输入生成文本和音频（一维），图像（二维），视频和
    3D 模型（三维）。如果我们保持输入和输出的维度相同，就可以对这些类型的数据进行去噪和翻译。我们还可以将真实数据输入生成器，让它输出更大维度的数据，例如图像超分辨率。我们也可以输入一种类型的数据，让它输出另一种类型的数据，例如基于文本生成音频、基于文本生成图像等等。
- en: Even though it has only been 4 years since GANs first came out (at the time
    of writing), people have kept working on improving GANs and new GAN models are
    coming out almost weekly. If you take a look at [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo),
    you can see that there have been at least 500 different GAN models. It's nearly
    impossible for us to learn and evaluate each one of them. You'll be amazed to
    see that it is actually common to find several models sharing the same name! Therefore,
    in this book, we won't even try to introduce you to most of the GAN models out
    there. We will, however, help you to get familiar with the most typical GAN models
    in different applications and learn how to use them to address practical problems.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GAN 首次提出仅仅过了 4 年（截至写作时），人们一直在努力改进 GAN，新的 GAN 模型几乎每周都会发布。如果你查看 [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)，你会发现至少有
    500 种不同的 GAN 模型。要想学习并评估每一个模型几乎是不可能的。你会惊讶地发现，实际上有很多模型共享相同的名字！因此，在本书中，我们不会尝试向你介绍大多数现有的
    GAN 模型。然而，我们将帮助你熟悉在不同应用中最典型的 GAN 模型，并学习如何使用它们来解决实际问题。
- en: We will also introduce you to some useful tricks and techniques to improve the
    performance of GANs. We hope that, by the time you finish this book, you have
    a wide yet in-depth understanding of the mechanisms of various GAN models so that
    you will feel confident to design your own GANs to creatively solve the problems
    you may encounter in the future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍一些实用的技巧和技术，以提高 GAN 的性能。我们希望，当你完成本书时，能够对各种 GAN 模型的机制有广泛而深入的理解，从而让你有信心设计自己的
    GAN 来创造性地解决未来可能遇到的问题。
- en: 'Let''s take a look at what GANs are capable of and what their advantages are
    compared to traditional approaches in these fields: image processing, NLP, and
    3D modeling.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 GAN 能做些什么，以及它们在这些领域（图像处理、自然语言处理和 3D 建模）与传统方法相比有哪些优势。
- en: Image processing
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: In the field of image processing, GANs are applied to many applications, including
    image synthesis, image translation, and image restoration. These topics are the
    most common in the study and application of GANs and make up most of the content
    in this book. Images are one of the easiest to show and spread media form on the
    internet; therefore, any latest breakthrough in the image-wise application of
    GANs would receive overwhelming attention in the deep learning community.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像处理领域，GAN 被应用于许多应用场景，包括图像合成、图像翻译和图像修复。这些话题是 GAN 学习和应用中最常见的内容，也是本书的主要内容。图像是互联网上最容易展示和传播的媒体形式之一；因此，GAN
    在图像应用方面的任何最新突破都会在深度学习社区引起极大的关注。
- en: Image synthesis
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像合成
- en: 'Image synthesis is, in short, the creation of new images. Early in 2015, **DCGANs** (**Deep
    Convolutional Generative Adversarial Networks**) came out. It was one of the first
    well-performing and stable approaches to address the hard-to-train issues presented
    in earlier GAN models. It generates 64 x 64 images based on a random vector with
    a length of 100\. Some images generated by DCGANs are shown in the following screenshot.
    You may notice that some of the images are far from being realistic because of
    the blocky appearance of the pixels. In the paper by Radford, Metz, and Chintala
    (2015), they present many interesting and inspiring visual experiments and reveal
    even more potential of GANs. We will talk about the architecture and training
    procedure of DCGANs later in [Chapter 4](3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml),
    *Building Your First GAN with PyTorch*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，图像合成就是创造新的图像。早在2015年，**DCGANs**（**深度卷积生成对抗网络**）就问世了。这是最早解决了之前GAN模型中难以训练问题的稳定且表现良好的方法之一。它基于一个长度为100的随机向量生成64
    x 64的图像。以下截图展示了部分DCGAN生成的图像。你可能会注意到，由于像素的方块状外观，有些图像看起来远没有那么逼真。在Radford、Metz和Chintala（2015）的论文中，他们展示了许多有趣且启发性的视觉实验，揭示了GAN更大的潜力。我们将在[第4章](3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml)中讨论DCGAN的架构和训练过程，*用PyTorch构建你的第一个GAN*。
- en: '![](img/69a09d37-57b3-4a02-aa33-b0a2560dee60.png)![](img/d68c77e3-9e2c-498f-942a-86d8a822b817.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69a09d37-57b3-4a02-aa33-b0a2560dee60.png)![](img/d68c77e3-9e2c-498f-942a-86d8a822b817.png)'
- en: 'Images generated by a DCGAN (left: human faces; right: bedroom)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN生成的图像（左：人脸；右：卧室）
- en: Now, GANs perform extraordinarily in image synthesis. Take BigGAN, for example.
    It was proposed in a paper submitted to ICLR 2019 (*7^(th) International Conference
    on Learning Representations*) by Brock, Donahue, and Simonyan. It received a lot
    of attention on social media even during the open review process. It's capable
    of generating images as large as 512 x 512 with high quality.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，GAN在图像合成方面表现异常出色。以BigGAN为例，它是在ICLR 2019（*第7届国际学习表示会议*）上，Brock、Donahue和Simonyan提交的论文中提出的。即使在开放评审过程中，它就受到了社交媒体的广泛关注。它能够生成高质量、最大达到512
    x 512尺寸的图像。
- en: In future chapters, we will also take a look at GAN models that look further
    into attributes of images, rather than just class conditions. We will talk about
    Conditional GANs, which allow you to generate images interactively, and Age-cGAN,
    which generates human faces of any age of your desire. We will also look into
    how to use GANs to generate adversarial examples that even the best classifiers
    cannot correctly recognize in [Chapter 8](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml),
    *Training Your GANs to Break Different Models*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们还将探讨一些GAN模型，这些模型不仅关注图像的类别条件，还深入探讨图像的其他属性。我们将讨论条件GAN（Conditional GANs），它允许你互动式地生成图像，以及Age-cGAN，它能够根据你的需求生成任意年龄段的人脸。我们还将研究如何利用GAN生成对抗样本，这些样本连最好的分类器也无法正确识别，相关内容请见[第8章](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml)，*训练你的GAN以突破不同的模型*。
- en: Image translation
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像翻译
- en: If we describe image synthesis as a process where we expect the outputs to be
    2-dimension images when feeding 1-dimension vector into the models (again, note
    that there are exceptions since you can generate images based on other types of
    data if you want), image translation (more precisely, image-to-image translation)
    would be the process where we feed 2-dimension images into models that also give
    2-dimension data as outputs. A lot of interesting things can be done with image
    translation. For example, pix2pix (Isola, Zhu, Zhou, et al, 2016) transforms label
    maps into images, including turning edge sketches into colorized images, generating
    street view photos based on semantic segmentation information, transferring image
    styles, and so on. We will get to an upgraded version of pix2pix, pix2pixHD, in
    [Chapter 6](209b2357-05d7-48d4-9c91-e061eccf8344.xhtml), *Image-to-Image Translation
    and Its Applications*, along with other image-to-image translation methods such
    as CycleGAN and DiscoGAN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将图像合成描述为一个过程，即我们期望将1维向量输入模型后，输出为2维图像（再次强调，这里有例外，因为如果你愿意，也可以基于其他类型的数据生成图像），那么图像翻译（更准确地说，是图像到图像的翻译）就是将2维图像输入模型，并且模型输出仍为2维数据的过程。通过图像翻译，可以做很多有趣的事情。例如，pix2pix（Isola,
    Zhu, Zhou等人，2016）可以将标签图转换为图像，包括将边缘草图转为彩色图像、根据语义分割信息生成街景照片、进行图像风格迁移等。我们将在[第6章](209b2357-05d7-48d4-9c91-e061eccf8344.xhtml)中详细探讨pix2pix的升级版pix2pixHD，*图像到图像翻译及其应用*，并介绍其他图像到图像翻译方法，如CycleGAN和DiscoGAN。
- en: Image-to-image translation can be used in other computer vision applications
    and address more traditional problems, such as image restoration, image in-painting,
    and super-resolution. Image restoration is one of the most important research
    areas in computer vision. Mathematicians and computer scientists have been trying
    to figure out how to remove the annoying noises off photos or reveal more information
    out of blur images for decades. Traditionally, these problems are solved by iterative
    numerical calculations, which often require profound mathematical backgrounds
    to master. Now, with GANs at hand, these problems can be solved by image-to-image
    translation. For example, SRGAN (Ledig, Theis, Huszar, et al, 2016) can upscale
    images to 4X of size with high quality, which we will talk about in detail in
    [Chapter 7](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml), *Image Restoration with
    GANs*. Yes, Chen, Lim, et al (2016) proposed using a DCGAN-like model to address
    human face inpainting problems. More recently, Yu, Lin, Yang, et al (2018) designed
    a GAN model that fills in an arbitrary shape of blank holes in images and the
    generated pixels are quite convincing as well.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像的翻译可以应用于其他计算机视觉应用，并解决更传统的问题，比如图像恢复、图像修补和超分辨率。图像恢复是计算机视觉中最重要的研究领域之一。数学家和计算机科学家们几十年来一直在努力解决如何消除照片上的令人讨厌的噪音，或者从模糊图像中揭示更多信息的问题。传统上，这些问题通过迭代数值计算来解决，通常需要深厚的数学背景才能掌握。现在，有了生成对抗网络（GANs），这些问题可以通过图像到图像的翻译来解决。例如，SRGAN（Ledig,
    Theis, Huszar等人，2016年）可以高质量地将图像放大至原始尺寸的4倍，我们将在[第7章](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml)中详细讨论，*使用GAN进行图像恢复*。是的，Chen,
    Lim等人（2016年）提出使用类似DCGAN的模型来解决人脸修补问题。更近期，Yu, Lin, Yang等人（2018年）设计了一个GAN模型，可以填补图像中任意形状的空白区域，生成的像素也非常令人信服。
- en: Text-to-image translation is also a good application of GANs, in which new images
    are generated based on the description text. Reed, Akata, Yan, et al (2016) came
    up with a procedure that extracts distinguish features from detailed description
    text and uses the information to generate flower or bird images that match perfectly
    to the description. Months later, Zhang, Xu, Li, et al (2016) proposed StackGAN
    to generate 256 x 256 images with high fidelity based on description text. We
    will talk about text-to-image translation in [Chapter 9](464e6361-6a52-4de2-960a-4fa0576f42c7.xhtml),
    *Image Generation from Description Text*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像的翻译也是GAN的一个良好应用，根据描述文本生成新的图像。Reed, Akata, Yan等人（2016年）提出了一种从详细描述文本中提取区分特征并利用这些信息生成与描述完美匹配的花卉或鸟类图像的过程。几个月后，Zhang,
    Xu, Li等人（2016年）提出了StackGAN，根据描述文本生成高保真度的256 x 256图像。我们将在[第9章](464e6361-6a52-4de2-960a-4fa0576f42c7.xhtml)中详细讨论，*从描述文本生成图像*。
- en: Video synthesis and translation
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频合成与翻译
- en: A video is a sequence of images. Therefore, most of the image translation methods
    can be directly applied to video. However, a crucial performance criterion of
    video synthesis or translation is the calculation speed. For example, if we want
    to develop a camera application with different image styles for mobile devices,
    our users would certainly hope that they can see the processed results in real-time.
    Take video surveillance systems as another example. It is completely feasible
    to use GANs to denoise and enhance the video signals (provided that your clients
    trust your models without reservation). A fast model that processes each frame
    in milliseconds to keep up with the frame rate would certainly be worth considering.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 视频是一系列图像。因此，大多数图像翻译方法可以直接应用于视频。然而，视频合成或翻译的一个关键性能指标是计算速度。例如，如果我们希望为移动设备开发具有不同图像风格的相机应用程序，我们的用户肯定希望能实时看到处理后的结果。以视频监控系统为例。完全可以使用GAN来去噪和增强视频信号（前提是您的客户完全信任您的模型）。一个快速处理每帧图像的模型，以毫秒级速度保持帧率，无疑是值得考虑的。
- en: We'd like to point out an interesting gesture transfer project called **Everybody
    Dance Now**. It extracts the movements of the dancer from a source video, then
    maps the same movements to the person in the target video by image-to-image translation.
    This way, anyone can use this model to make dancing videos of their own!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想指出一个有趣的手势转移项目，名为**Everybody Dance Now**。它从源视频中提取舞者的动作，然后通过图像到图像的翻译将相同的动作映射到目标视频中的人物。这样，任何人都可以使用这个模型制作自己的舞蹈视频！
- en: NLP
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP
- en: NLP is the study of how to use computers to process and analyze natural human
    languages. Other than generating images, GANs can also be used to generate sequential
    and time-dependent data, such as text and audio. SeqGAN, proposed by Yu, Zhang,
    Wang, et al (2016), is designed to generate sequential signals, like poems and
    music. Shortly after, Mogren (2016) proposed C-RNN-GAN, which is designed to generate
    classical music under acoustic restraints. In 2017, Dong, Hsiao, Yang, et al designed
    MuseGAN to generate polyphonic music of multiple instruments, including bass,
    drums, guitar, piano, and strings. Feel free to visit the following web^(10) site
    to enjoy the generated music!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 是研究如何使用计算机处理和分析自然人类语言的学科。除了生成图像外，GANs 还可以用于生成序列和时间相关的数据，如文本和音频。SeqGAN，由
    Yu、Zhang、Wang 等人（2016）提出，旨在生成序列信号，如诗歌和音乐。随后，Mogren（2016）提出了 C-RNN-GAN，旨在生成受声学约束的古典音乐。2017
    年，Dong、Hsiao、Yang 等人设计了 MuseGAN，用于生成多乐器的复调音乐，包括贝斯、鼓、吉他、钢琴和弦乐。可以随时访问以下网站^(10) 享受生成的音乐！
- en: Speech enhancement is one of the main research areas in audio signal processing.
    Traditionally, people use spectral subtraction, Wiener filtering, subspace approaches,
    and more to remove the noises in audio or speech signals. However, the performances
    of these methods have only been satisfactory under certain circumstances. Pascual,
    Bonafonte, and Serrà (2017) designed SEGAN to address this problem and achieved
    impressive results^(11). We will talk about the applications of GANs in the field
    of NLP in [Chapter 10](e05f7dcc-b1fe-4b9b-a893-124e67718cac.xhtml), *Sequence
    Synthesis with GANs*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 语音增强是音频信号处理的主要研究领域之一。传统上，人们使用频谱减法、维纳滤波、子空间方法等来去除音频或语音信号中的噪声。然而，这些方法的性能只有在特定情况下才令人满意。Pascual、Bonafonte
    和 Serrà（2017）设计了 SEGAN 来解决这个问题，并取得了令人印象深刻的结果^(11)。我们将在 [第 10 章](e05f7dcc-b1fe-4b9b-a893-124e67718cac.xhtml)，*使用
    GAN 进行序列合成* 中讨论 GAN 在 NLP 领域的应用。
- en: 3D modeling
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D 建模
- en: Now that we know GANs can generate 2D data based on 1D inputs, it's only natural
    to consider leveling this up to generate 3D data based on 1D or 2D signals with
    GANs. 3D-GAN (Wu, Zhang, Xue, et al, 2016) is designed exactly for this purpose.
    It learns the mapping between the latent vector and 3D models to generate 3D objects
    based on a 1D vector. It is also completely feasible to use GANs to predict 3D
    models based on 2D silhouettes. Gadelha, Maji, and Wang (2016) designed PrGAN
    to generate 3D objects based on binary silhouette images from any viewpoint. We
    will discuss how to generate 3D objects with GANs in detail in [Chapter 11](09d087ce-5d7e-4bd4-af48-693ac63d891c.xhtml),
    *Reconstructing 3D Models with GANs*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道 GANs 可以基于 1D 输入生成 2D 数据，那么考虑将其升级到基于 1D 或 2D 信号生成 3D 数据也是自然而然的事情。3D-GAN（Wu、Zhang、Xue
    等人，2016）正是为此目的设计的。它学习潜在向量与 3D 模型之间的映射，从而基于 1D 向量生成 3D 对象。使用 GANs 基于 2D 轮廓预测 3D
    模型也是完全可行的。Gadelha、Maji 和 Wang（2016）设计了 PrGAN，用于基于任何视角的二进制轮廓图像生成 3D 对象。我们将在 [第
    11 章](09d087ce-5d7e-4bd4-af48-693ac63d891c.xhtml)，*使用 GAN 重建 3D 模型* 中详细讨论如何使用
    GAN 生成 3D 对象。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We've covered a tremendous amount of information just in this first chapter.
    You've seen how GANs came about and have a basic grasp of the roles of generators
    and discriminators. You've even seen a few examples of some of the things that
    GANs can do. We've even created a GAN program using just NumPy. Not to mention
    we now know why Ganland has better blacksmiths and wine.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一章中已经涵盖了大量信息。你已经了解了 GANs 的起源，并且对生成器和判别器的角色有了基本的理解。你甚至已经看到了 GANs 可以做的一些示例。我们甚至使用
    NumPy 创建了一个 GAN 程序。更不用说，现在我们知道为什么 Ganland 的铁匠和酒更好。
- en: Next, we'll dive into the wondrous world of PyTorch, what it is, and how to
    install it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探索 PyTorch 的神奇世界，了解它是什么以及如何安装它。
- en: The following is a list of references and other helpful links.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是参考文献和其他有用链接的列表。
- en: References and useful reading list
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和有用阅读列表
- en: Goodfellow I, Pouget-Abadie J, Mirza M, et al. (2014). Generative adversarial
    nets. NIPS, 2672-2680.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow I，Pouget-Abadie J，Mirza M 等（2014）。生成对抗网络。NIPS，2672-2680。
- en: 'Wang, J. (2017, Dec 23). *Symbolism vs. Connectionism: A Closing Gap in Artificial
    Intelligence*, retrieved from [https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence](https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence).'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang, J.（2017 年 12 月 23 日）。*符号主义与联结主义：人工智能中的一个逐渐弥合的鸿沟*，摘自 [https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence](https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence)。
- en: Radford A, Metz L, Chintala S. (2015). *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*. arXiv preprint arXiv:1511.06434.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford A, Metz L, Chintala S.（2015）。*无监督表示学习与深度卷积生成对抗网络*。arXiv预印本 arXiv:1511.06434。
- en: '"Dev Nag". (2017, Feb 11). **Generative Adversarial Networks** (**GANs**) in
    50 lines of code (PyTorch), retrieved from [https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f).'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '"Dev Nag"。（2017年2月11日）。**生成对抗网络**（**GANs**）在50行代码中的实现（PyTorch），检索自[https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)。'
- en: Brock A, Donahue J, Simonyan K. (2018). *Large Scale GAN Training for High Fidelity
    Natural Image Synthesis*. arXiv preprint arXiv:1809.11096.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brock A, Donahue J, Simonyan K. (2018). *大规模GAN训练用于高保真自然图像合成*。arXiv预印本 arXiv:1809.11096。
- en: Isola P, Zhu J Y, Zhou T, Efros A. (2016). *Image-to-Image Translation with
    Conditional Adversarial Networks*. arXiv preprint arXiv:1611.07004.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Isola P, Zhu J Y, Zhou T, Efros A. (2016). *基于条件对抗网络的图像到图像转换*。arXiv预印本 arXiv:1611.07004。
- en: Ledig C, Theis L, Huszar F, et al (2016). *Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network*. arXiv preprint arXiv:1609.04802.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ledig C, Theis L, Huszar F, et al (2016). *使用生成对抗网络的照片真实单图像超分辨率*。arXiv预印本 arXiv:1609.04802。
- en: Yeh R A, Chen C, Lim T Y, et al (2016). *Semantic Image Inpainting with Deep
    Generative Models*. arXiv preprint arXiv:1607.07539.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yeh R A, Chen C, Lim T Y, et al (2016). *基于深度生成模型的语义图像修复*。arXiv预印本 arXiv:1607.07539。
- en: Yu J, Lin Z, Yang J, et al (2018). *Free-Form Image Inpainting with Gated Convolution*. arXiv
    preprint arXiv:1806.03589.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yu J, Lin Z, Yang J, et al (2018). *自由形式图像修复与门控卷积*。arXiv预印本 arXiv:1806.03589。
- en: Reed S, Akata Z, Yan X, et al (2016). *Generative Adversarial Text to Image
    Synthesis*. arXiv preprint arXiv:1605.05396.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reed S, Akata Z, Yan X, et al (2016). *生成对抗文本到图像合成*。arXiv预印本 arXiv:1605.05396。
- en: 'Zhang H, Xu T, Li H, et al (2016). *StackGAN: Text to Photo-realistic Image
    Synthesis with Stacked Generative Adversarial Networks*. arXiv preprint arXiv:1612.03242.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang H, Xu T, Li H, et al (2016). *StackGAN：基于堆叠生成对抗网络的文本到照片真实图像合成*。arXiv预印本
    arXiv:1612.03242。
- en: 'Yu L, Zhang W, Wang J, et al (2016). *SeqGAN: Sequence Generative Adversarial
    Nets with Policy Gradient*. arXiv preprint arXiv:1609.05473.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yu L, Zhang W, Wang J, et al (2016). *SeqGAN：具有策略梯度的序列生成对抗网络*。arXiv预印本 arXiv:1609.05473。
- en: 'Mogren O. (2016). *C-RNN-GAN: Continuous recurrent neural networks with adversarial
    training*. arXiv preprint arXiv:1611.09904.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mogren O. (2016). *C-RNN-GAN：具有对抗训练的连续递归神经网络*。arXiv预印本 arXiv:1611.09904。
- en: 'Dong H W, Hsiao W Y, Yang L C, et al (2017). *MuseGAN: Multi-track Sequential
    Generative Adversarial Networks for Symbolic Music Generation and Accompaniment*. arXiv
    preprint arXiv:1709.06298.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dong H W, Hsiao W Y, Yang L C, et al (2017). *MuseGAN：用于符号音乐生成与伴奏的多轨序列生成对抗网络*。arXiv预印本
    arXiv:1709.06298。
- en: 'Pascual S, Bonafonte A, Serrà J. (2017). *SEGAN: Speech Enhancement Generative
    Adversarial Network*. arXiv preprint arXiv:1703.09452.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pascual S, Bonafonte A, Serrà J. (2017). *SEGAN：语音增强生成对抗网络*。arXiv预印本 arXiv:1703.09452。
- en: Wu J, Zhang C, Xue T, et al (2016). *Learning a Probabilistic Latent Space of
    Object Shapes via 3D Generative-Adversarial Modeling*. arXiv preprint arXiv:1610.07584.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wu J, Zhang C, Xue T, et al (2016). *通过3D生成对抗建模学习物体形状的概率潜在空间*。arXiv预印本 arXiv:1610.07584。
- en: Gadelha M, Maji S, Wang R. (2016). *3D Shape Induction from 2D Views of Multiple
    Objects*. arXiv preprint arXiv:1612.05872.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gadelha M, Maji S, Wang R. (2016). *从多个物体的2D视图推导3D形状*。arXiv预印本 arXiv:1612.05872。
