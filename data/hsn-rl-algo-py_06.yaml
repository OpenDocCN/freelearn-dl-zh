- en: Q-Learning and SARSA Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-Learning 和 SARSA 应用
- en: '**Dynamic programming** (**DP**) algorithms are effective for solving **reinforcement
    learning** (**RL**) problems, but they require two strong assumptions. The first
    is that the model of the environment has to be known, and the second is that the
    state space has to be small enough so that it does not suffer from the curse of
    dimensionality problem.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**（**DP**）算法对于解决 **强化学习**（**RL**）问题非常有效，但它们需要两个强假设。第一个假设是必须知道环境的模型，第二个假设是状态空间必须足够小，以至于不会受到维度灾难问题的影响。'
- en: In this chapter, we'll develop a class of algorithms that get rid of the first
    assumption. In addition, it is a class of algorithms that aren't affected by the
    problem of the curse of dimensionality of DP algorithms. These algorithms learn
    directly from the environment and from the experience, estimating the value function
    based on many returns, and do not compute the expectation of the state values
    using the model, in contrast with DP algorithms. In this new setting, we'll talk
    about experience as a way to learn value functions. We'll take a look at the problems
    that arise from learning a policy through mere interactions with the environment
    and the techniques that can be used to solve them. After a brief introduction
    to this new approach, you'll learn about **temporal difference** (**TD**) learning,
    a powerful way to learn optimal policies from experience. TD learning uses ideas
    from DP algorithms while using only information gained from interactions with
    the environment. Two temporal difference learning algorithms are SARSA and Q-learning.
    Though they are very similar and both guarantee convergence in tabular cases,
    they have interesting differences that are worth acknowledging. Q-learning is
    a key algorithm, and many state-of-the-art RL algorithms combined with other techniques use
    this method, as we will see in later chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开发一类摆脱第一个假设的算法。此外，这是一类不受动态规划（DP）算法维度灾难问题影响的算法。这些算法直接从环境和经验中学习，根据许多回报估计价值函数，而不是像
    DP 算法那样使用模型计算状态值的期望。在这种新设置下，我们将讨论经验作为学习价值函数的方式。我们将研究通过与环境的互动来学习策略时出现的问题，以及可以用来解决这些问题的技术。在简要介绍这一新方法后，你将了解
    **时序差分**（**TD**）学习，这是一种通过经验学习最优策略的强大方法。TD 学习借鉴了 DP 算法的思想，同时只使用从与环境互动中获得的信息。两种时序差分学习算法是
    SARSA 和 Q-learning。虽然它们非常相似，并且都能在表格化的情况下保证收敛，但它们有一些有趣的差异，值得我们注意。Q-learning 是一个关键算法，许多最先进的强化学习（RL）算法结合其他技术时使用这种方法，正如我们将在后续章节中看到的那样。
- en: To gain a better grasp on TD learning and to understand how to move from theory
    to practice, you'll implement Q-learning and SARSA in a new game. Then, we'll
    elaborate on the difference between the two algorithms, both in terms of their
    performance and use.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 TD 学习并了解如何从理论过渡到实践，你将实现 Q-learning 和 SARSA 在一个新游戏中的应用。然后，我们将详细阐述这两种算法的区别，既包括它们的表现，也包括它们的使用。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Learning without a model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模型学习
- en: TD learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD 学习
- en: SARSA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSA
- en: Applying SARSA to Taxi-v2
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 SARSA 应用到 Taxi-v2
- en: Q-learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning
- en: Applying Q-learning to Taxi-v2
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Q-learning 应用到 Taxi-v2
- en: Learning without a model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无模型学习
- en: 'By definition, the value function of a policy is the expected return (that
    is, the sum of discounted rewards) of that policy starting from a given state:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，策略的价值函数是从给定状态开始该策略的期望回报（即折扣奖励的总和）：
- en: '![](img/bd09e7fe-1faf-4516-99a2-705c097e11b4.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd09e7fe-1faf-4516-99a2-705c097e11b4.png)'
- en: 'Following the reasoning of [Chapter 3](f2414b11-976a-4410-92d8-89ee54745d99.xhtml),
    *Solving Problems with Dynamic Programming*, DP algorithms update state values
    by computing expectations for all the next states of their values:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[第 3 章](f2414b11-976a-4410-92d8-89ee54745d99.xhtml)《使用动态规划解决问题》的推理，DP 算法通过计算所有下一状态的期望来更新状态值：
- en: '![](img/afefe32e-0b3f-4c54-86af-5c6dc47d9456.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afefe32e-0b3f-4c54-86af-5c6dc47d9456.png)'
- en: Unfortunately, computing the value function means that you need to know the
    state transition probabilities. In fact, DP algorithms use the model of the environment
    to obtain those probabilities. But the major concern is what to do when it's not
    available. The best answer is to gain all the information by interacting with
    the environment. If done well, it works because by sampling from the environment
    a substantial number of times, you should able to approximate the expectation
    and have a good estimation of the value function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，计算价值函数意味着你需要知道状态转移概率。事实上，动态规划（DP）算法使用环境模型来获得这些概率。但主要的问题是，当这些信息不可用时该怎么办。最好的答案是通过与环境交互来获取所有信息。如果做得好，这种方法有效，因为通过从环境中采样多次，你应该能够近似期望值，并对价值函数有一个良好的估计。
- en: User experience
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户体验
- en: 'Now, the first thing we need to clarify is how to sample from the environment,
    and how to interact with it to get usable information about its dynamics:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要首先澄清的是如何从环境中采样，以及如何与环境互动以获取有关其动态的可用信息：
- en: '![](img/891bc12d-6d45-4ff1-9cbd-9967c18c2063.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/891bc12d-6d45-4ff1-9cbd-9967c18c2063.png)'
- en: Figure 4.1\. A trajectory that starts from state [![](img/d2f04276-d89b-4c09-be66-5c3e046bdcc1.png)]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1\. 从状态[![](img/d2f04276-d89b-4c09-be66-5c3e046bdcc1.png)]开始的轨迹
- en: The simple way to do this is to execute the current policy until the end of
    the episode. You would end up with a trajectory as shown in figure 4.1\. Once
    the episode terminates, the return values can be computed for each state by backpropagating
    upward the sum of the rewards, [![](img/530430fd-e98a-4631-92d1-6d399c977298.png)].
    Repeating this process multiple times (that is, running multiple trajectories)
    for every state would have multiple return values. The return values are then averaged
    for each state to compute the expected returns. The expected returns computed
    in such a way is an approximated value function. The execution of a policy until
    a terminal state is called a trajectory or an episode. The more trajectories are
    run, the more returns are observed and by the law of large numbers, the average
    of these estimations will converge to the expected value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事的简单方法是执行当前策略直到本回合结束。你最终会得到如图4.1所示的轨迹。一旦回合结束，可以通过向上反向传播奖励总和来为每个状态计算回报值，[![](img/530430fd-e98a-4631-92d1-6d399c977298.png)]。为每个状态重复这个过程多次（即运行多个轨迹），就会得到多个回报值。然后对这些回报值进行平均，以计算期望回报。以这种方式计算的期望回报是一个近似的价值函数。执行策略直到终止状态称为轨迹或回合。运行更多的轨迹会观察到更多的回报，根据大数法则，这些估计的平均值将会收敛到期望值。
- en: Like DP, the algorithms that learn a policy by direct interaction with the environment
    rely on the concepts of policy evaluation and policy improvement. Policy evaluation
    is the act of estimating the value function of a policy, while policy improvement
    uses the estimates made in the previous phase to improve the policy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与动态规划类似，通过与环境直接交互来学习策略的算法依赖于策略评估和策略改进的概念。策略评估是估算策略的价值函数，而策略改进则利用上一阶段的估算来改进策略。
- en: Policy evaluation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略评估
- en: 'We just saw how using real experience to estimate the value function is an
    easy process. It is about running the policy in an environment until a final state
    is reached, then computing the return value and averaging the sampled return,
    as can be seen in equation (1):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到，使用真实经验来估算价值函数是一个简单的过程。这涉及到在环境中执行策略直到达到最终状态，然后计算回报值并对采样回报进行平均，如方程（1）所示：
- en: '![](img/91e0d32e-f26f-40f4-9bbc-596c6b72bb96.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91e0d32e-f26f-40f4-9bbc-596c6b72bb96.png)'
- en: Thus the expected return of a state can be approximated from the experience
    by averaging the sampling episodes from that state. The methods that estimate
    the return function using (1) are called **Monte Carlo methods**. Until all of
    the state-action pairs are visited and enough trajectory has been sampled, Monte
    Carlo methods guarantee convergence to the optimal policy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态的期望回报可以通过对来自该状态的采样经历进行平均来近似。使用（1）估算回报函数的方法称为**蒙特卡洛方法**。直到所有状态-动作对都被访问并且足够的轨迹被采样，蒙特卡洛方法才能保证收敛到最优策略。
- en: The exploration problem
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索问题
- en: How can we guarantee that every action of each state is chosen? And why is that
    so important? We will first answer the latter question, and then show how we can
    (at least in theory) explore the environment to reach every possible state.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何保证每个状态的每个动作都会被选择？为什么这如此重要？我们将首先回答后一个问题，然后展示我们如何（至少在理论上）探索环境以到达每一个可能的状态。
- en: Why explore?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要探索？
- en: The trajectories are sampled following a policy that can be stochastic or deterministic.
    In the case of a deterministic policy, each time a trajectory is sampled, the
    visited states will always be the same, and the update of the value function will
    take into account only this limited set of states. This will considerably limit
    your knowledge about the environment. It is like learning from a teacher that
    never changes their opinion on a subject—you will be stuck with those ideas without
    learning about others.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹是通过一个策略进行采样的，这个策略可以是随机的也可以是确定性的。在确定性策略的情况下，每次采样轨迹时，访问的状态将始终是相同的，值函数的更新只会考虑这一有限的状态集。这将大大限制你对环境的了解。就像从一位永远不会改变自己观点的老师那里学习——你将停留在这些想法中，无法了解其他的知识。
- en: Thus the exploration of the environment is crucial if you want to achieve good
    results, and it ensures that there are no better policies that could be found.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，环境的探索至关重要，如果你想获得好的结果，它确保了没有更好的策略可以被发现。
- en: On the other hand, if a policy is designed in such a way that it explores the
    environment constantly without taking into consideration what has already been
    learned, the achievement of a good policy is very difficult, perhaps even impossible.
    This balance between exploration and exploitation (behaving according to the best
    policy currently available) is called the exploration-exploitation dilemma and
    will be considered in greater detail in [Chapter 12](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml),
    *Developing an ESBAS Algorithm*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果一个策略被设计成不断探索环境，而不考虑已经学到的东西，那么实现一个好的策略将变得非常困难，甚至可能是不可能的。探索与利用（按照当前最好的策略行事）之间的平衡被称为探索-利用困境，且将在[第12章](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml)中详细讨论，*开发一个ESBAS算法*。
- en: How to explore
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行探索
- en: A very effective method that can be used when dealing with such situations is
    called ![](img/a4dfaa2a-9c4c-42df-ac65-5686eb8e422f.png)-greedy exploration. It
    is about acting randomly with probability ![](img/3f6cb003-0dad-4616-bbd6-4186cb5d92ce.png)
    while acting greedily (that means choosing the best action) with probability ![](img/0955e0dc-b44d-4d6a-95a6-8d3ecbadf807.png).
    For example, if ![](img/0cb992a9-ec1e-469a-a0c1-83068847eafa.png), on average,
    for every 10 actions, the agent will act randomly 8 times.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理此类情况时，一个非常有效的方法叫做![](img/a4dfaa2a-9c4c-42df-ac65-5686eb8e422f.png)-贪心探索。它是在以概率![](img/3f6cb003-0dad-4616-bbd6-4186cb5d92ce.png)随机行动的同时，以概率![](img/0955e0dc-b44d-4d6a-95a6-8d3ecbadf807.png)采取贪心行动（即选择最佳行动）。例如，如果![](img/0cb992a9-ec1e-469a-a0c1-83068847eafa.png)，平均而言，每10次行动中，智能体将随机行动8次。
- en: To avoid exploring too much in later stages when the agent is confident about
    its knowledge, ![](img/855b7b6e-158b-473e-903e-3bf0ac18deb6.png) can decrease over
    time. This strategy is called **epsilon-decay**. With this variation, an initial
    stochastic policy will gradually converge to a deterministic and, hopefully, optimal
    policy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在智能体对自己的知识充满信心时，过多地进行探索，![](img/855b7b6e-158b-473e-903e-3bf0ac18deb6.png)可以随着时间的推移逐渐减少。这个策略被称为**epsilon衰减**。通过这种变化，最初的随机策略将逐渐收敛到一个确定性的，并且希望是最优的策略。
- en: There are many other exploration techniques (such as Boltzmann exploration)
    that are more accurate, but they are also quite complicated, and for the purpose
    of this chapter, ![](img/85b6b1ad-0cdd-4a7c-b81e-ed1522a3c7ea.png)-greedy is a
    perfect choice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的探索技术（如Boltzmann探索），它们更为精确，但也相当复杂，对于本章而言，![](img/85b6b1ad-0cdd-4a7c-b81e-ed1522a3c7ea.png)-贪心策略是一个完美的选择。
- en: TD learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD学习
- en: Monte Carlo methods are a powerful way to learn directly by sampling from the
    environment, but they have a big drawback—they rely on the full trajectory. They
    have to wait until the end of the episode, and only then can they update the state
    values. Therefore, a crucial factor is knowing what happens when the trajectory
    has no end, or if it's very long. The answer is that it will produce terrifying
    results. A similar solution to this problem has already come up in DP algorithms,
    where the state values are updated at each step, without waiting until the end.
    Instead of using the complete return accumulated during the trajectory, it just
    uses the immediate reward and the estimate of the next state value. A visual example
    of this update is given in figure 4.2 and shows the parts involved in a single
    step of learning. This technique is called **bootstrapping**, and it is not only
    useful for long or potentially infinite episodes, but for episodes of any length.
    The first reason for this is that it helps to decrease the variance of the expected
    return. The variance is decreased because the state values depend only on the
    immediate next reward and not on all the rewards of the trajectory. The second
    reason is that the learning process takes place at every step, making these algorithms
    learn online. For this reason, it is called one-step learning. In contrast, Monte
    Carlo methods are offline as they use the information only after the conclusion
    of the episode. Methods that learn online using bootstrapping are called TD learning
    methods*. *
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法是一种强大的直接通过从环境中采样来学习的方法，但它们有一个很大的缺点——它们依赖于完整的轨迹。它们必须等到整个回合结束，才能更新状态值。因此，一个关键问题是，当轨迹没有结束时，或者如果轨迹非常长时，会发生什么情况。答案是它将产生可怕的结果。针对这个问题，动态规划算法已经提出了类似的解决方案，状态值在每一步更新，而不是等到结束后才更新。它并不是使用在轨迹过程中累积的完整回报，而是使用即时奖励和下一状态值的估计。这种更新的可视化示例在图4.2中给出，展示了学习过程中的单步涉及的部分。这个技术叫做**自举**，它不仅对长回合或可能无限的回合有用，对于任何长度的回合同样有效。其第一个原因是，它有助于减少期望回报的方差。方差减少的原因是，状态值只依赖于即时的下一个奖励，而不依赖于整个轨迹的所有奖励。第二个原因是，学习过程在每一步都进行，使这些算法能够在线学习。因此，这被称为一步学习。相反，蒙特卡罗方法是离线的，因为它们只在回合结束后才使用信息。使用自举进行在线学习的方法称为TD学习方法*。
- en: '![](img/045ce027-d975-43f6-aff7-b051a802bfd3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/045ce027-d975-43f6-aff7-b051a802bfd3.png)'
- en: Figure 4.2\. One-step learning update with bootstrapping
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 一步学习更新与自举
- en: TD learning can be viewed as a combination of Monte Carlo methods and DP because
    they use the idea of sampling from the former and the idea of bootstrapping from
    the latter. TD learning is widely used all across RL algorithms, and it constitutes
    the core of many of these algorithms. The algorithms that will be presented later in
    this chapter (namely SARSA and Q-learning) are all one-step, tabular, model-free
    (meaning that they don't use the model of the environment) TD methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TD学习可以看作是蒙特卡罗方法和动态规划（DP）方法的结合，因为它们借鉴了前者的采样思想和后者的自举（bootstrapping）思想。TD学习广泛应用于强化学习（RL）算法中，构成了许多算法的核心。本章稍后介绍的算法（即SARSA和Q-learning）都是一步、表格型、无模型（意味着它们不使用环境的模型）TD方法。
- en: TD update
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD更新
- en: 'From the previous chapter, *Solving Problems with Dynamic Programming *we know
    the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一章*使用动态规划解决问题*中，我们知道以下内容：
- en: '![](img/3af8fde4-f56e-4c59-ad79-9a7249a2cf3d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3af8fde4-f56e-4c59-ad79-9a7249a2cf3d.png)'
- en: 'Empirically, the Monte Carlo update estimates this value by averaging returns
    from multiple full trajectories. Developing the equation further, we obtain the
    following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过经验方法，蒙特卡罗更新通过对多个完整轨迹的回报进行平均来估算这个值。进一步推导该方程，我们得到如下结果：
- en: '![](img/41d4a96c-3ed0-4d61-b5f2-41318dc3ce17.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41d4a96c-3ed0-4d61-b5f2-41318dc3ce17.png)'
- en: 'The preceding equation is approximated by the DP algorithms. The difference
    is that TD algorithms estimate the expected value instead of computing it. The
    estimate is done in the same way as Monte Carlo methods do, by averaging:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程通过动态规划算法进行了逼近。不同之处在于，TD算法通过估算期望值而不是计算它。估算的方式与蒙特卡罗方法相同，即通过平均来进行：
- en: '![](img/e6bc2f84-563b-4d4b-b067-b80d1ca84f39.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6bc2f84-563b-4d4b-b067-b80d1ca84f39.png)'
- en: 'In practice, instead of calculating the average, the TD update is carried out
    by improving the state value by a small amount toward the optimal value:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，TD更新不是计算平均值，而是通过向最优值小幅度地改善状态值来完成更新：
- en: '![](img/b8fbcf7b-0605-489c-b7da-68fa840e2313.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8fbcf7b-0605-489c-b7da-68fa840e2313.png)'
- en: '![](img/fbe576ca-39b1-4b16-807b-3b836f07ae75.png) is a constant that establishes
    how much the state value should change at each update. If ![](img/c03ec553-2338-4dab-b54d-35c70afb5d9f.png),
    then the state value will not change at all. Instead, if ![](img/5fbde5cc-9a40-4bae-9c18-97184ceb61b9.png),
    the state value will be equal to ![](img/0d9b1ea1-fa41-433e-84c2-b1aaf4dcaaf3.png)
    (called the **TD target**) and it will completely forget the previous value. In
    practice, we don''t want these extreme cases, and usually ![](img/eb159765-c1ac-48f2-a063-faf81fadd941.png) ranges
    from 0.5 to 0.001.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/fbe576ca-39b1-4b16-807b-3b836f07ae75.png) 是一个常数，用来确定每次更新时状态值应有多少变化。如果
    ![](img/c03ec553-2338-4dab-b54d-35c70afb5d9f.png)，那么状态值将完全不变。相反，如果 ![](img/5fbde5cc-9a40-4bae-9c18-97184ceb61b9.png)，状态值将等于
    ![](img/0d9b1ea1-fa41-433e-84c2-b1aaf4dcaaf3.png)（称为 **TD 目标**），并且完全忘记之前的值。在实践中，我们不希望出现这些极端情况，通常
    ![](img/eb159765-c1ac-48f2-a063-faf81fadd941.png) 的范围从 0.5 到 0.001。'
- en: Policy improvement
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策策改进
- en: TD learning converges to the optimal condition as long as each action of every
    state has a probability of greater than zero of being chosen. To satisfy this
    requirement, TD methods, as we saw in the previous section, have to explore the
    environment. Indeed, the exploration can be carried out using an ![](img/5c54d0da-f5a4-4ee8-a1a2-73705e530baf.png)-greedy
    policy. It makes sure that both greedy actions and random actions are chosen in
    order to ensure both the exploitation and exploration of the environment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TD 学习在每个状态的每个动作都有大于零的选择概率时会收敛到最优条件。为了满足这一要求，正如我们在前一部分看到的，TD 方法必须进行环境探索。事实上，探索可以通过使用
    ![](img/5c54d0da-f5a4-4ee8-a1a2-73705e530baf.png)-贪心策略来进行。它确保在选择动作时既有贪心动作，也有随机动作，从而确保了环境的开发和探索。
- en: Comparing Monte Carlo and TD
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较蒙特卡洛和 TD
- en: An important of both Monte Carlo TD methods is that they converge to an optimal
    solution as long as they deal with tabular cases (meaning that state values are
    stored in tables or arrays) and have an exploratory strategy. Nonetheless, they
    differ in the way they update the value function. Overall, TD learning has lower
    variance but suffers from a higher bias than Monte Carlo learning. In addition
    to this, TD methods are generally faster in practice and are preferred to Monte
    Carlo methods.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛 TD 方法的一个重要特点是，只要它们处理的是表格型问题（即状态值存储在表格或数组中）并且具有探索性策略，它们就能收敛到最优解。然而，它们在更新值函数的方式上有所不同。总体而言，TD
    学习的方差较低，但偏差高于蒙特卡洛学习。除此之外，TD 方法通常在实践中更快，因此更受偏好，相较于蒙特卡洛方法。
- en: SARSA
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA
- en: So far, we have presented TD learning as a general way to estimate a value function
    for a given policy. In practice, TD cannot be used as it is because it lacks the
    primary component to actually improve the policy. SARSA and Q-learning are two
    one-step, tabular TD algorithms that both estimate the value functions and optimize
    the policy, and that can actually be used in a great variety of RL problems. In
    this section, we will use SARSA to learn an optimal policy for a given MDP. Then,
    we'll introduce Q-learning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将 TD 学习呈现为一种估算给定策略的值函数的通用方法。在实践中，TD 方法无法直接使用，因为它缺少实际改进策略的主要组件。SARSA
    和 Q-learning 是两种一阶、表格型的 TD 算法，它们都可以估算值函数并优化策略，能够广泛应用于各种强化学习问题。在本节中，我们将使用 SARSA
    学习给定马尔可夫决策过程（MDP）的最优策略。然后，我们将介绍 Q-learning。
- en: A concern with TD learning is that it estimates the value of a state. Think
    about that. In a given state, how can you choose the action with the highest next
    state value? Earlier, we said that you should pick the action that will move the
    agent to the state with the highest value. However, without a model of the environment
    that provides a list of the possible next states, you cannot know which action
    will move the agent to that state. SARSA, instead of learning the value function,
    learns and applies the state-action function, ![](img/b734e28e-01c0-40bc-8583-569ba97c58e5.png).
    ![](img/8e2818a4-9f52-45f7-b551-b14f14b6e446.png) tells the value of a state, ![](img/fdbded2a-a552-42a3-80be-a42bff79adf1.png),
    if the action, ![](img/229a340d-a599-4777-b3b5-df6ee0b24519.png), is taken.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TD 学习的一个问题是它估算的是状态的价值。想一想，在一个给定的状态下，你如何选择具有最高下一个状态值的动作？我们之前说过，你应该选择那个能把智能体移动到具有最高值的状态的动作。然而，在没有环境模型的情况下，它无法提供可能的下一个状态列表，你无法知道哪个动作能将智能体移动到那个状态。SARSA
    不是学习值函数，而是学习并应用状态-动作函数，![](img/b734e28e-01c0-40bc-8583-569ba97c58e5.png)。![](img/8e2818a4-9f52-45f7-b551-b14f14b6e446.png)
    告诉我们一个状态的价值，![](img/fdbded2a-a552-42a3-80be-a42bff79adf1.png)，如果采取的动作是 ![](img/229a340d-a599-4777-b3b5-df6ee0b24519.png)。
- en: The algorithm
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: 'Basically, all the observations we have done for the TD update are also valid
    for SARSA. Once we apply them to the definition of Q-function, we obtain the SARSA
    update:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们对 TD 更新所做的所有观察对于 SARSA 也是有效的。一旦我们将其应用于 Q 函数的定义，就得到了 SARSA 更新：
- en: '![](img/d0cd69ef-50ea-4131-88d5-a05ab1c1ffa3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0cd69ef-50ea-4131-88d5-a05ab1c1ffa3.png)'
- en: '![](img/1a685bc4-b99e-4c43-aea0-89e8c7f11fb3.png) is a coefficient that determines
    how much the action value has been updated. ![](img/320ab0ac-4203-4ead-b5dc-10eba45f5849.png) is
    the discount factor, a coefficient between 0 and 1 used to give less importance
    to the values that come from distant future decisions (short-term actions are
    preferred to long-term ones). A visual interpretation of the SARSA update is given
    in figure 4.3.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/1a685bc4-b99e-4c43-aea0-89e8c7f11fb3.png) 是一个系数，用于确定动作值更新了多少。![](img/320ab0ac-4203-4ead-b5dc-10eba45f5849.png)
    是折扣因子，一个介于 0 和 1 之间的系数，用于降低来自遥远未来决策的值的重要性（短期动作比长期动作更受偏好）。SARSA 更新的视觉解释见图 4.3。'
- en: The name SARSA comes from the update that is based on the state, ![](img/a4ccb68c-970d-490b-9c86-874d84aeb271.png);
    the
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA 这个名字来源于基于状态的更新，![](img/a4ccb68c-970d-490b-9c86-874d84aeb271.png)；
- en: 'action, ![](img/29d4bf6c-98ba-41ed-914d-549566d03369.png), the reward, ![](img/9653ec88-0bed-4aa5-8821-cd19a7c573f5.png); the
    next state, ![](img/e80bc89b-3fc7-4d59-825e-4c15a483c2f6.png); and finally, the
    next action, ![](img/337c2089-dcd8-4b8b-ac27-404cc365ce00.png). Putting everything
    together, it forms ![](img/083544e5-5496-4797-bdfb-134e99081692.png), as can be
    seen in figure 4.3:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 动作，![](img/29d4bf6c-98ba-41ed-914d-549566d03369.png)，奖励，![](img/9653ec88-0bed-4aa5-8821-cd19a7c573f5.png)；下一个状态，![](img/e80bc89b-3fc7-4d59-825e-4c15a483c2f6.png)；最后，下一个动作，![](img/337c2089-dcd8-4b8b-ac27-404cc365ce00.png)。将所有内容结合起来，形成 ![](img/083544e5-5496-4797-bdfb-134e99081692.png)，如图
    4.3 所示：
- en: '![](img/91e24270-8c44-4ff7-9fe7-cb70af24364f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91e24270-8c44-4ff7-9fe7-cb70af24364f.png)'
- en: Figure 4.3 SARSA update
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 SARSA 更新
- en: SARSA is an on-policy algorithm. On-policy means that the policy that is used
    to collect experience through interaction with the environment (called a behavior
    policy) is the same policy that is updated. The on-policy nature of the method
    is due to the use of the current policy to select the next action, ![](img/92e66d09-e556-4192-ba60-e709abab7396.png), to
    estimate ![](img/2c2e535d-bf82-4c70-8702-d6c76a9abec4.png), and the assumption
    that in the following action it will follow the same policy (that is, it acts
    according to action ![](img/cd7f94cc-71ec-4804-8769-2e82a835cb00.png)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA 是一种在策略算法。所谓“在策略”，意味着通过与环境互动收集经验的策略（称为行为策略）就是被更新的策略。该方法的在策略特性来自于使用当前策略来选择下一个动作，![](img/92e66d09-e556-4192-ba60-e709abab7396.png)，以估计 ![](img/2c2e535d-bf82-4c70-8702-d6c76a9abec4.png)，并假设在随后的动作中将遵循相同的策略（即根据动作 ![](img/cd7f94cc-71ec-4804-8769-2e82a835cb00.png)
    执行）。
- en: 'On-policy algorithms are usually easier than off-policy algorithms, but they
    are less powerful and usually require more data to learn. Despite this, as for
    TD learning, SARSA is guaranteed to converge to the optimal policy if it visits
    every state-action an infinite number of times and the policy, over time, becomes
    a deterministic one. Practical algorithms use an ![](img/9abb1d02-9ae0-492d-9681-7a41fb1bba29.png)-greedy
    policy with a decay that tends to be zero, or a value close to it. The pseudocode
    of SARSA is summarized in the following code block. In the pseudocode, we used
    an ![](img/9abb1d02-9ae0-492d-9681-7a41fb1bba29.png)-greedy policy, but any strategy that
    encourages exploration can be used:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略算法通常比离策略算法更简单，但它们的表现力较弱，通常需要更多的数据来学习。尽管如此，就像 TD 学习一样，如果 SARSA 每次都访问每个状态-动作无限次，并且随着时间推移，策略变得确定性，那么它是可以保证收敛到最优策略的。实际算法通常使用 ![](img/9abb1d02-9ae0-492d-9681-7a41fb1bba29.png)-贪心策略，并伴有逐渐趋近于零的衰减。SARSA
    的伪代码总结如下。在伪代码中，我们使用了 ![](img/9abb1d02-9ae0-492d-9681-7a41fb1bba29.png)-贪心策略，但可以使用任何鼓励探索的策略：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/4d600891-0329-46b9-bbf7-3b597dffc849.png) is a function that implements
    the ![](img/1bf5501b-78b9-49b7-81a6-c8135dfddedf.png) strategy. Note that SARSA
    executes the same action that has been selected and used in the previous step
    to update the state-action value.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/4d600891-0329-46b9-bbf7-3b597dffc849.png) 是一个实现 ![](img/1bf5501b-78b9-49b7-81a6-c8135dfddedf.png)
    策略的函数。请注意，SARSA 执行与前一步骤中选择并使用的相同动作来更新状态-动作值。'
- en: Applying SARSA to Taxi-v2
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 SARSA 应用于 Taxi-v2
- en: After a more theoretical view of TD learning and particularly of SARSA, we are
    finally able to implement SARSA to solve problems of interest. As we saw previously,
    SARSA can be applied to environments with unknown models and dynamics, but as
    it is a tabular algorithm with scalability constraints, it can only be applied
    to environments with small and discrete action and state spaces. So, we choose
    to apply SARSA to a gym environment called Taxi-v2 that satisfies all the requirements
    and is a good test bed for these kinds of algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 TD 学习特别是 SARSA 进行更理论化的探讨之后，我们终于能够实现 SARSA 来解决感兴趣的问题。正如我们之前所看到的，SARSA 可以应用于具有未知模型和动态的环境，但由于它是一个具有可扩展性限制的表格化算法，因此只能应用于具有小规模离散动作和状态空间的环境。因此，我们选择将
    SARSA 应用于一个名为 Taxi-v2 的 gym 环境，它满足所有要求，并且是这类算法的良好测试平台。
- en: Taxi-v2 is a game that was introduced to study hierarchical reinforcement learning
    (a type of RL algorithm that creates a hierarchy of policies, each with the goal
    of solving a subtask) where the aim is to pick up a passenger and drop them at
    a precise location. A reward of +20 is earned when the taxi performs a successful
    drop-off, but a penalty of -10 is incurred for illegal pickup or drop-off. Moreover,
    a point is lost for every timestep. The render of the game is given in figure
    4.4\. There are six legal moves corresponding to the four directions, the pickup,
    and the drop-off actions. In figure 4.4, the `:` symbol represents an empty location;
    the `|` symbol represents a wall that the taxi can't travel through; and `R,G,Y,B`
    are the four locations. The taxi, the yellow rectangle in the diagram, has to
    pick up a person in the location identified by the light blue color and drop them
    off in the location identified by the color violet.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Taxi-v2 是一个用于研究层次化强化学习（这是一种创建策略层次结构的 RL 算法，每个策略的目标是解决一个子任务）游戏，其目标是接载一个乘客并将其送到指定位置。当出租车成功完成送客任务时，获得
    +20 的奖励，但如果非法接送或送达，将受到 -10 的惩罚。此外，每经过一个时间步，都会失去 1 分。游戏的渲染效果如图 4.4 所示。该环境有六个合法动作，分别对应四个方向、接载和送客动作。在图
    4.4 中，`:` 符号代表空位置，`|` 符号代表出租车无法穿越的墙壁，`R, G, Y, B` 是四个位置。出租车（图中的黄色矩形）必须在浅蓝色标识的地点接载乘客，并将其送到紫色标识的地点。
- en: '![](img/f8f30f96-62a1-40e1-9564-76b3a549628b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8f30f96-62a1-40e1-9564-76b3a549628b.png)'
- en: Figure 4.4 Start state of the Taxi-v2 environment
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 Taxi-v2 环境的起始状态
- en: The implementation is fairly straightforward and follows the pseudocode given
    in the previous section. Though we explain and show all the code here, it is also
    available on the GitHub repository of the book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实现过程相当直接，遵循前一节中给出的伪代码。尽管我们在这里解释并展示了所有代码，但也可以在本书的 GitHub 仓库中找到。
- en: Let's first implement the main function, `SARSA(..)`, of the SARSA algorithm,
    which does most of the work. After this, we'll implement a couple of auxiliary
    functions that perform simple but essential tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先实现 SARSA 算法的主函数 `SARSA(..)`，它完成了大部分工作。之后，我们将实现一些辅助函数，执行简单但重要的任务。
- en: '`SARSA` needs an environment and a few other hyperparameters as arguments to
    work:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`SARSA` 需要一个环境和一些其他超参数作为参数才能正常工作：'
- en: A learning rate, `lr`, previously called ![](img/130995d6-a1de-4526-bd0b-cecb56cc7b94.png),
    that controls the amount of learning at each update.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 `lr`，以前称为 ![](img/130995d6-a1de-4526-bd0b-cecb56cc7b94.png)，控制每次更新时的学习量。
- en: '`num_episodes` speaks for itself because it is the number of episodes that
    SARSA will execute before terminating.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_episodes` 一目了然，因为它是 SARSA 执行的总回合数，直到终止。'
- en: '`eps` is the initial value of the randomness of the ![](img/62d758d1-d853-4663-b013-9e0dacc1380f.png)-greedy
    policy.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` 是 ![](img/62d758d1-d853-4663-b013-9e0dacc1380f.png)贪心策略的随机性初始值。'
- en: '`gamma` is the discount factor used to give less importance to actions more
    in the future.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma` 是折扣因子，用来对未来的动作赋予较小的权重。'
- en: '`eps_decay` is the linear decrement of `eps` across episodes.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps_decay` 是 `eps` 在各回合之间的线性递减。'
- en: 'The first lines of code are as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的前几行如下：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, some variables are initialized. `nA` and `nS` are the numbers of actions
    and observations respectively of the environment, `Q` is the matrix that will
    contain the Q-values of each state-action pair, and `test_rewards` and `games_rewards`
    are lists used later to hold information about the scores of the games.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，初始化了一些变量。`nA` 和 `nS` 分别是环境中的动作数和观察数，`Q` 是将包含每个状态-动作对的 Q 值的矩阵，`test_rewards`
    和 `games_rewards` 是稍后用来存储游戏分数信息的列表。
- en: 'Next, we can implement the main loop that learns the Q-values:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以实现学习Q值的主循环：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Line 2 in the preceding code block resets the environment on each new episode
    and stores the current state of the environment. Line 3 initializes a Boolean
    variable that will be set to `True` when the environment is in a terminal state.
    The following two lines update the `eps` variable until it has a value higher
    than 0.01\. We set this threshold to keep, in the long run, a minimum rate of
    exploration of the environment. The last line chooses an ![](img/c438fb76-8f61-4cb6-a685-8b6b774c4a4e.png)-greedy
    action based on the current state and the Q-matrix. We'll define this function
    later.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块中的第2行在每个新回合时重置环境并存储当前的环境状态。第3行初始化了一个布尔变量，当环境处于终止状态时，该变量将被设置为`True`。接下来的两行更新了`eps`变量，直到其值大于0.01。我们设置此阈值是为了保持长期内环境探索的最低速率。最后一行根据当前状态和Q矩阵选择一个*贪婪*动作。我们稍后会定义这个函数。
- en: 'Now that we have taken care of the initialization needed at the start of each
    episode and have chosen the first action, we can loop until the episode (the game)
    ends. The following piece of code samples from the environment and updates the
    following Q-function, as per formula (5):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经处理了每个回合开始时需要的初始化，并选择了第一个动作，我们可以开始循环，直到回合（游戏）结束。以下这段代码从环境中获取样本并根据公式（5）更新Q函数：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`done` holds a Boolean value that indicates whether the agent is still interacting
    with the environment, as can be seen in line 2\. Therefore, to loop for a complete
    episode is the same as iterating as long as `done` is `False` (the first line
    of the code). Then, as usual, `env.step` returns the next state, the reward, the
    done flag, and an information string. In the next line, `eps_greedy` chooses the
    next action based on the `next_state` and the Q-values. The heart of the SARSA
    algorithm is contained in the subsequent line, which performs the update as per
    formula (5). Besides the learning rate and the gamma coefficient, it uses the
    reward obtained in the last step and the values held in the `Q` array.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`done` 保存一个布尔值，表示智能体是否仍在与环境交互，正如第2行所示。因此，循环直到完整回合结束就相当于只要`done`为`False`就继续迭代（代码的第一行）。然后，像往常一样，`env.step`
    返回下一个状态、奖励、done 标志和一个信息字符串。在接下来的行中，`eps_greedy` 根据`next_state`和Q值选择下一个动作。SARSA算法的核心在于随后的这一行，它根据公式（5）执行更新。除了学习率和折扣因子gamma外，还使用了上一阶段获得的奖励以及`Q`数组中的值。'
- en: The final lines set the state and action as the previous one, adds the reward
    to the total reward of the game, and if the environment is in a final state, the
    sum of the rewards is appended to `games_reward` and the inner cycle terminates.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行设置当前状态和动作与之前相同，将奖励加到游戏的总奖励中，并且如果环境处于终止状态，则将奖励的总和附加到`games_reward`，然后内循环终止。
- en: 'In the last lines of the `SARSA` function, every 300 epochs, we run 1,000 test
    games and print information such as the epoch, the `eps` value, and the mean of
    the test rewards. Moreover, we return the `Q` array:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SARSA`函数的最后几行中，每300个回合，我们运行1,000次测试游戏并打印信息，如回合数、`eps`值和测试奖励的平均值。此外，我们返回`Q`数组：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now implement the `eps_greedy` function, which chooses a random action
    from those that are allowed with probability, `eps`. To do this, it just samples
    a uniform number between 0 and 1, and if this is smaller than `eps`, it selects
    a random action. Otherwise, it selects a greedy action:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以实现`eps_greedy`函数，它以`eps`的概率从允许的动作中选择一个随机动作。为此，它只需在0和1之间采样一个均匀分布的数字，如果这个值小于`eps`，则选择一个随机动作。否则，它选择一个贪婪动作：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The greedy policy is implemented by returning the index that corresponds to
    the maximum Q value in state `s`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪策略通过返回对应于状态`s`中最大Q值的索引来实现：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The last function to implement is `run_episodes`, which runs a few episodes
    to test the policy. The policy used to select the actions is the greedy policy.
    That''s because we don''t want to explore while testing. Overall, the function
    is almost identical to the one implemented in the previous chapter for the dynamic
    programming algorithms:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个要实现的函数是`run_episodes`，它运行若干回合来测试策略。用于选择动作的策略是贪婪策略。这是因为在测试时我们不希望进行探索。总体而言，该函数与前一章中为动态规划算法实现的函数几乎相同：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Great!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！
- en: 'Now we''re almost done. The last part involves only creating and resetting
    the environment and the call to the `SARSA` function, passing the environment
    along with all the hyperparameters:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们快完成了。最后的部分仅涉及创建和重置环境，以及调用 `SARSA` 函数，传入环境和所有超参数：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you can see, we start with an `eps` of `0.4`. This means that the first
    actions will be random with a probability of 0.4 and because of the decay, it
    will decrease until it reaches the minimum value of 0.01 (that is, the threshold
    we set in the code):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们从 `eps` 为 `0.4` 开始。这意味着前几次动作将以 0.4 的概率是随机的，并且由于衰减，`eps` 会逐渐减少，直到达到最小值
    0.01（即我们在代码中设置的阈值）。
- en: '![](img/8dc67ed8-30a7-4d56-9d77-75086fc3ab67.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dc67ed8-30a7-4d56-9d77-75086fc3ab67.png)'
- en: Figure 4.5 Results of the SARSA algorithm on Taxi-v2
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 SARSA 算法在 Taxi-v2 上的结果
- en: The performance plot of the test games' cumulative rewards is shown in figure
    4.5\. Moreover, figure 4.6 shows a complete episode run with the final policy.
    It has to be read from left to right and from top to bottom. We can see that the
    taxi (highlighted in yellow first, and green later) has driven along an optimal
    path in both directions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 测试游戏累计奖励的性能图如图 4.5 所示。此外，图 4.6 显示了使用最终策略的完整回合运行。该图应从左到右、从上到下阅读。我们可以看到，出租车（最初用黄色高亮，之后用绿色高亮）在两个方向上都沿着最优路径行驶。
- en: '![](img/96385f67-a4db-409a-ad56-a599607f285f.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96385f67-a4db-409a-ad56-a599607f285f.png)'
- en: Figure 4.6 Render of the Taxi game. The policy derives from the Q-values trained
    with SARSA
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 渲染的出租车游戏。策略来自使用 SARSA 训练的 Q 值。
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章中提到的所有颜色参考，请参考颜色图像包，下载链接为：[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)。
- en: To have a better view of the algorithm and all the hyperparameters, we suggest
    you play with them, change them, and observe the results. You can also try to
    use an exponential ![](img/5e164cd1-0f13-42ad-b36f-e831f4067a4d.png)-decay rate
    instead of a linear one. You learn by doing just as RL algorithms do, by trial
    and error.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解算法和所有超参数，我们建议你进行尝试，改变它们并观察结果。你还可以尝试使用指数 ![](img/5e164cd1-0f13-42ad-b36f-e831f4067a4d.png)-衰减率，而不是线性衰减。你就像强化学习算法一样，通过试错学习。
- en: Q-learning
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is another TD algorithm with some very useful and distinct features
    from SARSA. Q-learning inherits from TD learning all the characteristics of one-step
    learning (from TD learning, that is, the ability of learning at each step) and
    the characteristic to learn from experience without a proper model of the environment.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是另一种 TD 算法，具有一些与 SARSA 非常不同且非常有用的特性。Q-learning 继承了 TD 学习的一步学习特性（即在每一步进行学习的能力）和从经验中学习而不需要环境的完整模型的特性。
- en: The most distinctive feature about Q-learning compared to SARSA is that it's
    an off-policy algorithm. As a reminder, off-policy means that the update can be
    made independently from whichever policy has gathered the experience. This means
    that off-policy algorithms can use old experiences to improve the policy. To distinguish
    between the policy that interacts with the environment and the one that actually
    improves, we call the former a behavior policy and the latter a target policy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SARSA 相比，Q-learning 最显著的特点是它是一个脱离策略（off-policy）算法。提醒一下，脱离策略意味着更新可以独立于收集经验的任何策略进行。这意味着脱离策略的算法可以利用旧经验来改善策略。为了区分与环境交互的策略和实际改进的策略，我们称前者为行为策略，后者为目标策略。
- en: Here, we'll explain the more primitive version of the algorithm that copes with
    tabular cases, but it can easily be adapted to work with function approximators
    such as artificial neural networks. In fact, in the next chapter, we'll implement
    a more sophisticated version of this algorithm that is able to use deep neural
    networks and that also uses previous experiences to exploit the full capabilities
    of the off-policy algorithms.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将解释处理表格案例的更原始版本的算法，但它可以很容易地适应使用如人工神经网络等函数逼近器。实际上，在下一章中，我们将实现这个算法的一个更复杂的版本，该版本能够使用深度神经网络，并且利用以前的经验来充分发挥脱离策略算法的能力。
- en: But first, let's see how Q-learning works, formalize the update rule, and create
    a pseudocode version of it to unify all the components.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们看看 Q-learning 如何工作，形式化更新规则，并创建一个伪代码版本来统一所有组件。
- en: Theory
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论
- en: 'The idea of Q-learning is to approximate the Q-function by using the current
    optimal action value. The Q-learning update is very similar to the update done
    in SARSA, with the exception that it takes the maximum state-action value:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 的核心思想是通过使用当前的最优动作值来近似 Q 函数。Q-learning 更新与 SARSA 中的更新非常相似，唯一的区别是它采用了最大状态-动作值：
- en: '![](img/fb0b7f8a-8518-444a-80a5-59a167b198e1.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb0b7f8a-8518-444a-80a5-59a167b198e1.png)'
- en: '![](img/d962bd58-7a56-43e8-b739-270a615d2d4c.png) is the usual learning rate
    and ![](img/44419044-3cd5-432f-ad02-ed49ccd71a98.png) is the discount factor.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/d962bd58-7a56-43e8-b739-270a615d2d4c.png) 是常用的学习率，![](img/44419044-3cd5-432f-ad02-ed49ccd71a98.png)
    是折扣因子。'
- en: While the SARSA update is done on the behavior policy (like a ![](img/f70f0377-f506-45eb-804f-c31684795df5.png)-greedy
    policy), the Q-update is done on the greedy target policy that results from the
    maximum action value. If this concept is not clear yet, take a look at figure
    4.7\. While in SARSA we had figure 4.3, where both actions ![](img/b231d43d-ef10-4d2f-88d2-33f8005762e7.png) and ![](img/5266e9e4-152f-4cfa-9cc8-fef3471a0317.png) come
    from the same policy, in Q-learning, action ![](img/46e4ade2-5c1e-4c82-8025-cb0992bdc14f.png) is
    chosen based on the next maximum state-action value. Because an update in Q-learning
    is not more dependent on the behavior policy (which is used only for sampling
    from the environment), it becomes an off-policy algorithm.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当 SARSA 更新是在行为策略上进行时（如 ![](img/f70f0377-f506-45eb-804f-c31684795df5.png)-贪婪策略），Q
    更新则是在通过最大动作值得到的贪婪目标策略上进行的。如果这个概念还不清楚，可以查看图 4.7。虽然在 SARSA 中我们有图 4.3，其中两个动作 ![](img/b231d43d-ef10-4d2f-88d2-33f8005762e7.png)
    和 ![](img/5266e9e4-152f-4cfa-9cc8-fef3471a0317.png) 来自同一策略，但在 Q-learning 中，动作
    ![](img/46e4ade2-5c1e-4c82-8025-cb0992bdc14f.png) 是根据下一个最大状态-动作值选择的。由于 Q-learning
    的更新不再过多依赖于行为策略（该策略仅用于从环境中采样），因此它成为了一种离策略算法。
- en: '![](img/33d09a14-c300-4a4f-a24e-06664799d4e2.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33d09a14-c300-4a4f-a24e-06664799d4e2.png)'
- en: Figure 4.7\. Q-learning update
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7. Q-learning 更新
- en: The algorithm
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: As Q-learning is a TD method, it needs a behavior policy that, as time passes,
    will converge to a deterministic policy. A good strategy is to use an ![](img/20394dda-420a-4364-b4ba-b70ace279a17.png)-greedy
    policy with linear or exponential decay (as has been done for SARSA).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Q-learning 是一种时序差分（TD）方法，它需要一个行为策略，随着时间的推移，该策略将收敛为一个确定性策略。一种好的策略是使用带有线性或指数衰减的
    ![](img/20394dda-420a-4364-b4ba-b70ace279a17.png)-贪婪策略（正如在 SARSA 中所做的那样）。
- en: 'To recap, the Q-learning algorithm uses the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，Q-learning 算法使用以下内容：
- en: A target greedy policy that constantly improves
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不断改进的目标贪婪策略
- en: A behavior ![](img/31f67e90-d151-47d5-be3b-1fef83212307.png)-greedy policy to
    interact with and explore the environment
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用来与环境交互并探索的行为 ![](img/31f67e90-d151-47d5-be3b-1fef83212307.png)-贪婪策略
- en: 'After these conclusive observations, we can finally come up with the following
    pseudocode for the Q-learning algorithm:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些总结性的观察之后，我们最终可以得出以下 Q-learning 算法的伪代码：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In practice, ![](img/6e1c9159-3acf-48e1-b9aa-5fe753a24ff7.png) usually has values
    between 0.5 and 0.001 and ![](img/bacc1adc-cabe-4814-84d7-b21beb1cd5e3.png) ranges
    from 0.9 to 0.999.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，![](img/6e1c9159-3acf-48e1-b9aa-5fe753a24ff7.png) 通常在 0.5 和 0.001 之间，而
    ![](img/bacc1adc-cabe-4814-84d7-b21beb1cd5e3.png) 的范围是从 0.9 到 0.999。
- en: Applying Q-learning to Taxi-v2
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Q-learning 应用于 Taxi-v2
- en: In general, Q-learning can be used to solve the same kinds of problems that
    can be tackled with SARSA, and because they both come from the same family (TD
    learning), they generally have similar performances. Nevertheless, in some specific
    problems, one approach can be preferred to the other. So it's useful to also know
    how Q-learning is implemented.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Q-learning 可以用来解决与 SARSA 相同类型的问题，且由于它们都来自同一个家族（TD 学习），因此它们通常具有类似的表现。然而，在某些特定问题中，可能会偏好其中一种方法。因此，了解
    Q-learning 的实现也是非常有用的。
- en: For this reason, here we'll implement Q-learning to solve Taxi-v2, the same
    environment that was used for SARSA. But be aware that with just a few adaptations,
    it can be used with every other environment with the correct characteristics.
    Having the results from both Q-learning and SARSA from the same environment we'll
    have the opportunity to compare their performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这里我们将实现 Q-learning 来解决 Taxi-v2，这与 SARSA 使用的环境相同。但请注意，经过一些调整后，它也可以应用于其他具有正确特征的环境。通过比较
    Q-learning 和 SARSA 在同一环境下的结果，我们将有机会对它们的性能进行比较。
- en: 'To be as consistent as possible, we kept some functions unchanged from the
    SARSA implementation. These are as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持尽可能一致，我们保留了一些来自SARSA实现的函数不变。它们如下：
- en: '`eps_greedy(Q,s,eps)` is the ![](img/680d9c2c-3eff-46ca-ab5c-aa21b40ae4d9.png)-greedy
    policy that takes a `Q` matrix, a state `s`, and the `eps` value. It returns an
    action.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps_greedy(Q,s,eps)` 是 ![](img/680d9c2c-3eff-46ca-ab5c-aa21b40ae4d9.png)-贪心策略，接受一个
    `Q` 矩阵、状态 `s` 和 `eps` 值。它返回一个动作。'
- en: '`greedy(Q,s)` is the greedy policy that takes a `Q` matrix and a state `s`.
    It returns the action associated with the maximum Q-value in the state `s`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`greedy(Q,s)` 是一个贪心策略，接受一个 `Q` 矩阵和一个状态 `s`。它返回与状态 `s` 中最大Q值相关联的动作。'
- en: '`run_episodes(env,Q,num_episodes,to_print)` is a function that runs `num_episodes`
    games to test the greedy policy associated with the `Q` matrix. If `to_print`
    is `True` it prints the results. Otherwise, it returns the mean of the rewards.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run_episodes(env,Q,num_episodes,to_print)` 是一个函数，用于运行 `num_episodes` 场游戏来测试与
    `Q` 矩阵相关的贪心策略。如果 `to_print` 为 `True`，则打印结果。否则，返回奖励的平均值。'
- en: To see the implementation of those functions, you can refer to the *SARSA applied
    to Taxi-v2* section or the GitHub repository of the book, which can be found at [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 若要查看这些函数的实现，可以参考 *SARSA应用于Taxi-v2* 部分或该书的GitHub仓库，网址为 [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)。
- en: 'The main function that executes the Q-learning algorithm takes an environment, `env`;
    a learning rate, `lr` (the ![](img/938bd970-d732-4e79-9f7a-004484d24ea3.png) variable
    used in (6)); the number of episodes to train the algorithm, `num_episodes`; the
    initial ![](img/af85e4b0-f91e-433d-b793-84d243e166ac.png) value, `eps`, used by
    the ![](img/d506f91c-1270-406d-90ae-df116f43c808.png)-greedy policy; the decay
    rate, `eps_decay`; and the discount factor, `gamma`, as arguments:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 执行Q学习算法的主函数接受一个环境 `env`；学习率 `lr`（公式（6）中使用的 ![](img/938bd970-d732-4e79-9f7a-004484d24ea3.png)
    变量）；训练算法的回合数 `num_episodes`；初始 ![](img/af85e4b0-f91e-433d-b793-84d243e166ac.png)
    值 `eps`，用于 ![](img/d506f91c-1270-406d-90ae-df116f43c808.png)-贪心策略；衰减率 `eps_decay`；和折扣因子
    `gamma`，作为参数：
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first lines of the function initialize the variables with the dimensions
    of the action and observation space, initialize the array `Q` that contains the
    Q-value of each state-action pair, and create empty lists used to keep track of
    the progress of the algorithm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的前几行初始化了动作和观测空间的维度，初始化了包含每个状态-动作对的Q值的数组 `Q`，并创建了空列表用于跟踪算法的进展。
- en: 'Then, we can implement the cycle that iterates `num_episodes` times:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以实现一个循环，该循环迭代 `num_episodes` 次：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Each iteration (that is, each episode) starts by resetting the environment,
    initializing the `done` and `tot_rew` variables, and decreasing `eps` linearly.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代（即每个回合）都从重置环境开始，初始化 `done` 和 `tot_rew` 变量，并线性地减小 `eps`。
- en: 'Then, we have to iterate across all of the timesteps of an episode (that correspond
    to an episode) because that is where the Q-learning update takes place:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要遍历一个回合的所有时间步（对应一个回合），因为Q学习的更新发生在这里：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is the main body of the algorithm. The flow is fairly standard:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是算法的主体部分，流程相当标准：
- en: The action is chosen following the ![](img/16c14ae5-c0ba-41bc-a4ca-0d0cb955060d.png)-greedy
    policy (the behavior policy).
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作是根据 ![](img/16c14ae5-c0ba-41bc-a4ca-0d0cb955060d.png)-贪心策略（行为策略）选择的。
- en: The action is executed in the environment, which returns the next state, a reward,
    and the done flag.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作在环境中执行，环境返回下一个状态、奖励和完成标志。
- en: The action-state value is updated based on formula (6).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于公式（6）更新状态-动作值。
- en: '`next_state` is assigned to the `state` variable.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`next_state` 被赋值给 `state` 变量。'
- en: The reward of the last step is added up to the cumulative reward of the episode.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步的奖励加到该回合的累计奖励中。
- en: If it was the final step, the reward is stored in `games_reward` and the cycle
    terminates.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这是最后一步，则奖励存储在 `games_reward` 中，循环终止。
- en: 'In the end, every 300 iterations of the outer cycle, we can run 1,000 games
    to test the agent, print some useful information, and return the `Q` array:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每经过外循环的300次迭代，我们可以运行1,000场游戏来测试代理，打印一些有用的信息，并返回 `Q` 数组。
- en: '[PRE13]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That''s everything. As a final step, in the `main` function, we can create
    the environment and run the algorithm:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容。最后，在 `main` 函数中，我们可以创建环境并运行算法：
- en: '[PRE14]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The algorithm reaches steady results after about 3,000 episodes, as can be
    deduced from figure 4.8\. This plot can be created by plotting `test_rewards`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从图4.8可以推断，算法在约3,000个回合后达到了稳定结果。该图可以通过绘制`test_rewards`来生成：
- en: '![](img/a274ced2-3c3c-40df-bec8-099457bd2e54.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a274ced2-3c3c-40df-bec8-099457bd2e54.png)'
- en: Figure 4.8 The results of Q-learning on Taxi-v2
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 Q学习在Taxi-v2上的结果
- en: As usual, we suggest that you tune the hyperparameters and play with the implementation
    to gain better insight into the algorithm.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们建议你调整超参数，并通过实践来深入理解算法。
- en: Overall, the algorithm has found a policy similar to the one found by the SARSA
    algorithm. To find it by yourself, you can render some episodes or print the greedy
    action resulting from the `Q` array.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，算法找到了一个类似于SARSA算法所找到的策略。若要自行寻找，可以渲染一些回合或打印出基于`Q`数组的贪心动作。
- en: Comparing SARSA and Q-learning
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较SARSA和Q学习
- en: 'We will now look at a quick comparison of the two algorithms. In figure 4.9,
    the performance of Q-learning and SARSA in the Taxi-v2 environment is plotted
    as the episode progresses. We can see that both are converging to the same value
    (and to the same policy) with comparable speed. While doing these comparisons,
    you have to consider that the environment and the algorithms are stochastic and
    they may produce different results. We can also see from plot 4.9 that Q-learning
    has a more regular shape. This is due to the fact that it is more robust and less
    sensitive to change:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下这两种算法的快速比较。在图4.9中，展示了Q学习和SARSA在Taxi-v2环境中随着回合数的增加而逐步演化的表现。我们可以看到，两者都以相似的速度收敛到相同的值（以及相同的策略）。在进行这些比较时，你必须考虑到环境和算法是随机的，可能会产生不同的结果。从图4.9中我们还可以看到，Q学习的曲线形状更加规则。这是因为Q学习更具鲁棒性，并且对变化不那么敏感：
- en: '![](img/95b84c0e-2eea-4232-ab65-f58a52018e3a.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95b84c0e-2eea-4232-ab65-f58a52018e3a.png)'
- en: Figure 4.9 Comparison of the results between SARSA and Q-learning on Taxi-v2
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 SARSA与Q学习在Taxi-v2上的结果比较
- en: So, is it better to use Q-learning? Overall, the answer is yes, and in most
    cases, Q-learning outperforms the other algorithms, but there are some environments
    in which SARSA works better. The choice between the two is dependent on the environment
    and the task.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是否更好使用Q学习？总体来说，答案是肯定的，在大多数情况下，Q学习优于其他算法，但也有一些环境中SARSA表现得更好。两者的选择取决于环境和任务。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a new family of RL algorithms that learn from
    experience by interacting with the environment. These methods differ from dynamic
    programming in their ability to learn a value function and consequently a policy
    without relying on the model of the environment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一类新的强化学习（RL）算法，它们通过与环境互动从经验中学习。这些方法与动态规划的不同之处在于，它们能够在不依赖环境模型的情况下学习价值函数，从而学习策略。
- en: Initially, we saw that Monte Carlo methods are a simple way to sample from the
    environment but because they need the full trajectory before starting to learn,
    they are not applicable in many real problems. To overcome these drawbacks, bootstrapping
    can be combined with Monte Carlo methods, giving rise to so-called temporal difference
    (TD) learning. Thanks to the bootstrapping technique, these algorithms can learn
    online (one-step learning) and reduce the variance while still converging to optimal
    policies. Then, we learned two one-step, tabular, model-free TD methods, namely
    SARSA and Q-learning. SARSA is on-policy because it updates a state value by choosing
    the action based on the current policy (the behavior policy). Q-learning, instead,
    is off-policy because it estimates the state value of a greedy policy while collecting
    experience using a different policy (the behavior policy). This difference between
    SARSA and Q-learning makes the latter slightly more robust and efficient than
    the former.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们看到蒙特卡罗方法是一种从环境中采样的简单方法，但由于它们需要完整的轨迹才能开始学习，因此在许多实际问题中并不适用。为了克服这些缺点，蒙特卡罗方法可以与引导技术结合，产生所谓的时间差分（TD）学习。得益于引导技术，这些算法可以在线学习（一步学习），并且在收敛到最优策略时减少方差。随后，我们学习了两种一步的、基于表格的、无模型的TD方法，即SARSA和Q学习。SARSA是基于策略的，因为它通过选择当前策略（行为策略）来更新状态值。Q学习则是脱离策略的，因为它在使用不同策略（行为策略）收集经验时，估计贪心策略的状态值。SARSA和Q学习之间的这种区别使得后者比前者更具鲁棒性和效率。
- en: Every TD method needs to explore the environment in order to know it well and
    find the optimal policies. The exploration of the environment is in the hands
    of the behavior policy, which occasionally has to act non-greedily, for example,
    by following an ![](img/00dff209-29c8-4bf8-b170-49e428beccd1.png)-greedy policy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 每个TD方法都需要探索环境，以便更好地了解环境并找到最优策略。环境的探索由行为策略控制，行为策略偶尔需要采取非贪婪行动，例如，遵循一个*非贪婪*策略。
- en: We implemented both SARSA and Q-learning and applied them to a tabular game
    called Taxi. We saw that both converge to the optimal policy with similar results.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了SARSA和Q学习，并将它们应用到一个叫做Taxi的表格游戏中。我们看到两者都收敛到了最优策略，并且结果相似。
- en: The Q-learning algorithm is key in RL because of its qualities. Moreover, through careful
    design, it can be adapted to work with very complex and high-dimensional games.
    All of this is possible thanks to the use of function approximations such as deep
    neural networks. In the next chapter, we'll elaborate on this, and introduce a
    deep Q-network that can learn to play Atari games directly from pixels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法在强化学习中至关重要，原因在于其特点。此外，通过精心设计，它可以适应非常复杂和高维的游戏。所有这一切都得益于函数逼近方法的应用，如深度神经网络。在下一章中，我们将详细介绍这一点，并介绍一个深度Q网络，它可以直接从像素学习玩Atari游戏。
- en: Questions
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What's the main property of the Monte Carlo method used in RL?
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在强化学习中，蒙特卡洛方法的主要特性是什么？
- en: Why are Monte Carlo methods offline?
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么蒙特卡洛方法是离线的？
- en: What are the two main ideas of TD learning?
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD学习的两个主要思想是什么？
- en: What are the differences between Monte Carlo and TD?
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法和TD方法有什么区别？
- en: Why is exploration important in TD learning?
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在TD学习中探索是重要的？
- en: Why is Q-learning off-policy?
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么Q学习是离策略的？
