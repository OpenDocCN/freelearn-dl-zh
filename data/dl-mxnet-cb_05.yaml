- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Analyzing Images with Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用计算机视觉分析图像
- en: Computer vision is one of the fields in which deep learning has progressed enormously,
    surpassing human-level performance in several tasks such as image classification
    and object recognition. Furthermore, the field has moved from academia to real-world
    applications, and the industry is recognizing its practitioners as adding high
    value to businesses.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是深度学习取得巨大进展的领域之一，在多个任务中超过了人类水平的表现，例如图像分类和物体识别。此外，计算机视觉已从学术领域走向现实世界的应用，行业也开始认识到其从业者对企业的高价值贡献。
- en: In this chapter, we will learn how to use GluonCV, a MXNet Gluon library specific
    to computer vision, how to build our own networks, and how to use GluonCV’s model
    zoo to use pretrained models for several applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用GluonCV，这是一个专门用于计算机视觉的MXNet Gluon库，如何构建我们自己的网络，并如何使用GluonCV的模型库来使用预训练模型进行多个应用。
- en: 'Specifically, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将讨论以下主题：
- en: Understanding convolutional neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解卷积神经网络
- en: Classifying images with AlexNet and ResNet
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AlexNet和ResNet对图像进行分类
- en: Detecting objects with Faster R-CNN and YOLO
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Faster R-CNN和YOLO检测物体
- en: Segmenting objects in images with PSPNet and DeepLab-v3
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PSPNet和DeepLab-v3对图像中的物体进行分割
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在*前言*中指定的技术要求外，本章还适用以下技术要求：
- en: Ensure that you have completed *Installing MXNet, Gluon, GluonCV and GluonNLP*,
    the first recipe from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and
    Running* *with MXNet*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请确保您已完成[*安装MXNet、Gluon、GluonCV和GluonNLP*](B16591_01.xhtml#_idTextAnchor016)，这是[*第1章*](B16591_01.xhtml#_idTextAnchor016)中的第一个食谱，*使用MXNet快速上手*
- en: 'Ensure that you have completed *A toy dataset for regression – load, manage,
    and visualize a house sales dataset*, the first recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请确保您已完成[*回归的玩具数据集 - 加载、管理和可视化房屋销售数据集*](B16591_02.xhtml#_idTextAnchor029)，这是[*第2章*](B16591_02.xhtml#_idTextAnchor029)中的第一个食谱，*使用MXNet并可视化数据集：Gluon*和*DataLoader*
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub网址找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05)。
- en: 'Furthermore, you can access each recipe directly from Google Colab – for example,
    for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以直接从Google Colab访问每个食谱——例如，本章的第一个食谱：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb)。
- en: Understanding convolutional neural networks
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解卷积神经网络
- en: In the previous chapters, we have used *fully connected* **Multi-Layer Perceptron**
    (**MLP**) networks to solve our regression and classification problem. However,
    as we will see, these networks are not optimal for solving image-related problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用了**全连接**的**多层感知器**（**MLP**）网络来解决回归和分类问题。然而，正如我们将看到的，这些网络并不是解决图像相关问题的最优选择。
- en: Images are highly dimensional entities – for example, each pixel in a color
    image has three features (red, green, and blue values), and a 1,024x1,024 image
    has more than 1 million pixels (a 1 megapixel image) and, therefore, more than
    3 million features (3 * 106). If we connect all these points in the input layer,
    to a second layer of 100 neurons for a *fully connected* network, we will require
    more than 108 parameters, and that would be only for the first layer. Processing
    images is, therefore, a time-intensive operation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是高维实体——例如，彩色图像中的每个像素有三个特征（红色、绿色和蓝色值），而一张1,024x1,024的图像有超过100万个像素（即1百万像素图像），因此具有超过300万个特征（3
    * 10^6）。如果我们将这些输入层的所有点连接到第二层100个神经元的*全连接*网络中，我们将需要超过10^8个参数，而这仅仅是第一层的需求。因此，处理图像是一个时间密集型操作。
- en: Furthermore, imagine that we are trying to detect eyes in faces; if a pixel
    belongs to an eye, the likelihood of nearby pixels belonging to the eye is very
    high (think of the pixels that make up the iris, for example). When inputting
    all our pixels directly into our network, all the information connected to pixel
    location is lost.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设我们正在尝试在人脸中检测眼睛；如果一个像素属于眼睛，那么附近的像素属于眼睛的可能性非常高（例如，考虑构成虹膜的像素）。当我们将所有像素直接输入到网络中时，所有与像素位置相关的信息都丢失了。
- en: An architecture called a **Convolutional Neural Network** (**CNN**) was developed
    to tackle these problems, and we will analyze the most important features of CNNs
    and how to implement them in this recipe to solve image-related problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为**卷积神经网络**（**CNN**）的架构被开发出来以解决这些问题，我们将在本教程中分析CNN的最重要特性，并了解如何在图像相关问题中实现它们。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As with the previous chapters, in this recipe, we will use a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在这个教程中，我们将使用一些矩阵运算和线性代数，但不会太难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will take the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将执行以下步骤：
- en: Introduce convolutional layer equations.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入卷积层方程。
- en: Understand the convolution parameters and receptive field.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解卷积参数和感受野。
- en: Run a convolutional layer example with MXNet.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MXNet运行卷积层示例。
- en: Introduce pooling layer equations.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入池化层方程。
- en: Run a pooling layer example with MXNet.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MXNet运行池化层示例。
- en: Summarize CNNs.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结卷积神经网络（CNN）。
- en: Introducing convolutional layer equations
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入卷积层方程
- en: The location problems described in the recipe introduction are formally known
    as **translation invariance** and **locality**. In CNNs, these problems is solved
    by using the convolution/cross-correlation operation in the so-called convolutional
    layers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程介绍的定位问题在正式术语中称为**平移不变性**和**局部性**。在卷积神经网络（CNN）中，这些问题通过在所谓的卷积层中使用卷积/互相关操作来解决。
- en: 'In the convolution, we have an input image (or **feature map** – see the following
    *Important note* relating to convolutional layer equations) that is combined with
    a **kernel**, which are the learnable parameters of this layer. The simplest way
    to see how this operation works is with an example – if we had a 3x3 input and
    we wanted to combine it with a 2x2 kernel, it would look like a sliding window,
    as shown in the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积操作中，我们有一个输入图像（或**特征图** – 请参见下面与卷积层方程相关的*重要说明*），它与**卷积核**结合，卷积核是该层的可学习参数。查看这个操作如何工作的最简单方式是通过一个示例——如果我们有一个3x3的输入，想将其与一个2x2的卷积核结合，那么它看起来就像一个滑动窗口，如下图所示：
- en: '![Figure 5.1 – The convolutional layer](img/B16591_05_1.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 卷积层](img/B16591_05_1.jpg)'
- en: Figure 5.1 – The convolutional layer
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 卷积层
- en: As shown in *Figure 5**.1*, to compute 1 pixel of the output, we can intuitively
    place the kernel over the input and compute multiplications, and then add all
    these values to obtain the final result. This helps a network learn features from
    the image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.1*所示，为了计算输出的一个像素，我们可以直观地将卷积核放置在输入图像上，进行乘法计算，然后将这些值加起来得到最终结果。这有助于网络从图像中学习特征。
- en: Furthermore, this operation is less computing intensive than the *fully connected*
    layers, addressing the computing problem we identified.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个操作的计算量比*全连接*层要小，解决了我们所识别的计算问题。
- en: Important note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: When using a convolution layer as the first step (typical in CNNs), the input
    is the full image. Moreover, the output can be understood as another image of
    a lower dimension with certain properties, given by the kernel. As kernels are
    learned to highlight certain features of the image, these outputs are known as
    *feature maps*. In the layer close to the input, each pixel of these feature maps
    is a combination of a small number of pixels of the image (for example, horizontal
    or vertical lines). As data travels through convolutional layers, these feature
    maps represent higher levels of abstraction (for example, eyes on a face).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当卷积层作为第一步使用时（在CNN中通常如此），输入是完整的图像。此外，输出可以理解为具有某些特性的低维度图像，这些特性由卷积核给出。随着卷积核学习突出图像的某些特征，这些输出被称为*特征图*。在靠近输入的层中，这些特征图的每个像素都是图像中少量像素的组合（例如，水平或垂直线条）。随着数据通过卷积层流动，这些特征图代表更高层次的抽象（例如，脸上的眼睛）。
- en: Understanding the convolution parameters and receptive field
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解卷积参数和感受野
- en: The example shown in *Figure 5**.1* is quite simple, for illustrative purposes;
    the input size is 6x6 and the kernel size is 3x3\. These sizes are variable and
    depend on the network architecture. However, there are three parameters that are
    very important to calculate the output size; these are padding, stride, and dilation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.1*中展示的例子非常简单，仅用于说明；输入大小是 6x6，卷积核大小是 3x3。这些大小是可变的，取决于网络架构。然而，有三个参数对于计算输出大小非常重要；它们是填充、步幅和膨胀。
- en: Padding is the number of zero-valued pixels (rows/columns) that are added to
    the input. The larger the padding, the larger the output, and effectively, this
    increases the input size. In the example, padding is `1` (represented as *p*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 填充是指添加到输入中的零值像素（行/列）的数量。填充越大，输出越大，实际上这会增加输入的大小。在这个例子中，填充是 `1`（表示为 *p*）。
- en: As mentioned, we can intuitively place the kernel over the input, compute multiplications,
    and then add all these results to obtain the final value. For the next value,
    we need to move the kernel to a different position, as shown in the following
    diagram.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以直观地将卷积核放置在输入上，计算乘积，然后将所有这些结果相加，得到最终的值。对于下一个值，我们需要将卷积核移到不同的位置，如下图所示。
- en: '![Figure 5.2 – The stride parameter](img/B16591_05_2.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 步幅参数](img/B16591_05_2.jpg)'
- en: Figure 5.2 – The stride parameter
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 步幅参数
- en: The number of spaces the kernel moves is defined by the stride. In *Figure 5**.2*,
    we can see an example of the stride being 2, with each 3x3 kernel separated by
    2 values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核移动的步长由步幅定义。在*图 5.2*中，我们可以看到步幅为 2 的例子，每个 3x3 的卷积核之间相隔 2 个值。
- en: Dilation is defined by how separated each input value that is used in the convolution
    with the kernel is.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀定义了在与卷积核进行卷积时，每个输入值之间的间隔。
- en: '![Figure 5.3 – The dilation parameter](img/B16591_05_3.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 膨胀参数](img/B16591_05_3.jpg)'
- en: Figure 5.3 – The dilation parameter
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 膨胀参数
- en: As we can see in *Figure 5**.3*, different dilation parameters combine elements
    of the input.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图 5.3*中看到的，不同的膨胀参数组合了输入的元素。
- en: These parameters define what is known as the **receptive field**, which is the
    size of the region of the input that produces an activation. It is an important
    parameter because only a feature present in the receptive field of our model will
    be represented in the output.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数定义了所谓的**感受野**，即输入区域的大小，该区域生成激活。它是一个重要的参数，因为只有出现在我们模型感受野中的特征才能在输出中得到表示。
- en: In the example in *Figure 5**.1*, a 6x6 input, combined with a 3x3 kernel, with
    a stride of 2, padding of 1, and dilation of 1, yields a 3x3 output and has a
    receptive field of the full input (all pixels in the input are at least used once).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.1*中的例子中，一个 6x6 的输入，结合 3x3 的卷积核，步幅为 2，填充为 1，膨胀为 1，得到一个 3x3 的输出，并且其感受野覆盖了整个输入（输入中的所有像素至少使用一次）。
- en: 'Another very interesting property of these parameters is that given the right
    combinations, you can have an output that is the same size as the input. The equation
    that gives the dimensions of the output is as follows (the equation needs to be
    applied to the height and the width, respectively):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数的另一个非常有趣的性质是，给定正确的组合，您可以获得与输入相同大小的输出。给出输出维度的公式如下（该公式分别应用于高度和宽度）：
- en: o = [i + 2 * p − k − (k − 1)*(d − 1)] / s + 1
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: o = [i + 2 * p − k − (k − 1)*(d − 1)] / s + 1
- en: In the preceding equation, *o* is the output dimension (height or width if working
    with 2D images), *i* is the input dimension (height/width), *p* is padding, *k*
    is the kernel size, *d* is dilation, and *s* is stride. There are different combinations
    that will give the input and the output the same size, one of which is *p = 1,
    k = 3, d = 1, s =* *1*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*o* 是输出维度（如果处理 2D 图像，则为高度或宽度），*i* 是输入维度（高度/宽度），*p* 是填充，*k* 是卷积核大小，*d*
    是膨胀，*s* 是步幅。有多种组合可以使输入和输出的大小相同，其中之一是 *p = 1, k = 3, d = 1, s = 1*。
- en: Running a convolutional layer example with MXNet
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 MXNet 的卷积层示例
- en: 'We can implement the following example using MXNet capabilities (note that
    padding is 0, stride is 1, and dilation is 1):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 MXNet 的功能实现以下示例（注意填充为 0，步幅为 1，膨胀为 1）：
- en: '![Figure 5.4 – A convolution example](img/B16591_05_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 卷积示例](img/B16591_05_4.jpg)'
- en: Figure 5.4 – A convolution example
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 卷积示例
- en: 'If we want to follow the example depicted in *Figure 5**.4*, applying the convolution
    operation to a 3x3 matrix with a 2x2 kernel, we can use the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要按照*图5.4*中所示的示例，将卷积操作应用于一个3x3的矩阵，并使用一个2x2的卷积核，我们可以使用以下代码：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These operations give the following as a result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作的结果如下：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is the expected result for the defined convolution step (taking into account
    the given padding, stride, and dilation parameters).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是定义的卷积步长的预期结果（考虑给定的填充、步幅和扩张参数）。
- en: Introducing the pooling layer equations
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入池化层方程
- en: As described earlier, a desirable property of neural network models when working
    with images is that as we traverse a network, we can increasingly process higher-level
    features, or equivalently, each pixel in the deep feature maps has a higher receptive
    field from the input.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当处理图像时，神经网络模型的一个理想特性是，随着我们遍历网络，我们可以处理更高层次的特征，或者等效地说，深层特征图中的每个像素都具有来自输入的更大感受野。
- en: The operation performed in these layers is similar to the convolutional layers,
    in the sense that we take a kernel of constant dimensions and apply a sliding
    window. However, in this case, the kernel parameters are constant and, therefore,
    not learned during the training of the network. This kernel is seen as an operation
    (a function) and is typically either the max function (the **max pooling layer**)
    or the average function (the **average** **pooling layer**).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层执行的操作类似于卷积层，从某种意义上来说，我们采用具有恒定维度的卷积核并应用滑动窗口。然而，在这种情况下，卷积核的参数是恒定的，因此在训练网络时不会被学习。这个卷积核被视为一种操作（一个函数），通常是最大值函数（**最大池化层**）或平均值函数（**平均池化层**）。
- en: '![Figure 5.5 – The max pooling layer](img/B16591_05_5.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 最大池化层](img/B16591_05_5.jpg)'
- en: Figure 5.5 – The max pooling layer
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 最大池化层
- en: Furthermore, by combining pixels that are nearby, we also achieve local invariance,
    another desirable property to process images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过组合邻近的像素，我们还实现了局部不变性，这是处理图像时的另一个理想特性。
- en: Running a pooling layer example with MXNet
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MXNet运行池化层示例
- en: 'We can implement the example shown in *Figure 5**.5* using MXNet''s capabilities,
    as shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MXNet的功能实现*图5.5*中显示的示例，如下所示：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These operations give the following as a result:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作的结果如下：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the expected result for the defined 2x2 max pooling step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是定义的2x2最大池化步长的预期结果。
- en: Summarizing CNNs
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结CNN
- en: 'A typical CNN for **image classification** has, therefore, two different parts:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个典型的CNN用于**图像分类**，它有两个不同的部分：
- en: '**Feature extraction**: This is also known as the network backbone. It is built
    with combinations of the convolutional and pooling layers seen in this recipe.
    The input to each layer is feature maps (an image and all its channels in the
    input layer), and the output is feature maps of reduced dimensions but with a
    larger number of channels. Layers are typically stacked – a convolutional layer,
    an activation function (typically, **Rectified Linear Unit** (**ReLU**)), and
    a max pooling layer.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这也被称为网络的骨干。它是由本教程中看到的卷积层和池化层的组合构建的。每一层的输入是特征图（输入层中的图像及其所有通道），输出是减少维度但具有更多通道的特征图。层通常是堆叠的—一个卷积层，一个激活函数（通常是**修正线性单元**（**ReLU**）），和一个最大池化层。'
- en: '`softmax` function as the activation. The number of layers in the classifier
    depends on the specific problem.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`softmax`函数作为激活函数。分类器中的层数取决于具体问题。'
- en: 'Therefore, a CNN architecture can be the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个CNN架构可以是以下形式：
- en: '![Figure 5.6 – CNN architecture](img/B16591_05_6.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – CNN架构](img/B16591_05_6.jpg)'
- en: Figure 5.6 – CNN architecture
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – CNN架构
- en: In *Figure 5**.6*, we can see a CNN architecture for *image classification*,
    composed of two stages for feature extraction, each stage combining a convolution
    layer (with the ReLU activation function) and a max pooling layer. Then, for the
    classifier, the remaining feature map is flattened into a vector and passed through
    a *fully connected* layer to finally provide the output with the *softmax* activation
    function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.6*中，我们可以看到一个用于*图像分类*的CNN架构，包含两个特征提取阶段，每个阶段结合了一个卷积层（具有ReLU激活函数）和一个最大池化层。然后，分类器将剩余的特征图展平为一个向量，并通过一个*全连接*层，最终通过*softmax*激活函数提供输出。
- en: How it works…
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we have introduced CNNs. This architecture has been developed
    since early the 2000s and is responsible for the revolution in **computer vision**
    applications, and making **deep learning** become the spotlight in most data-oriented
    tasks including **natural language processing**, **speech recognition**, **image
    generation**, and so on, reaching state-of-the-art in all of them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了CNNs。这种架构自2000年代初期开始发展，并推动了**计算机视觉**应用的革命，使得**深度学习**成为大多数数据驱动任务中的焦点，包括**自然语言处理**、**语音识别**、**图像生成**等，并在所有领域达到了最先进的水平。
- en: We have understood how CNNs work internally, exploring the concepts of *feature
    maps*, *receptive field*, and the mathematical concepts behind the main layers
    of these architectures, the *convolutional* and *max pooling* layers, and how
    they are combined to build a complete CNN model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了CNNs的内部工作原理，探索了*特征图*、*感受野*以及这些架构主要层（*卷积*层和*最大池化*层）背后的数学概念，并了解了它们如何组合来构建一个完整的CNN模型。
- en: There’s more...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'CNNs have evolved rapidly; in 1998, one of the first CNNs was published, solving
    a practical problem: [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 发展迅速；1998年，首个CNN之一被发布，解决了一个实际问题：[http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)。
- en: 'After that, it was not until 2012, with AlexNet, that CNNs gained worldwide
    attention, and since then, progress quickly developed, until they surpassed human-level
    performance. For more information on the history of CNNs, refer to this article:
    [https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f](https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，直到2012年，随着AlexNet的问世，CNNs才获得了全球关注，从那时起，进展迅速发展，直到它们超过了人类的表现。欲了解CNNs历史的更多信息，请参阅这篇文章：[https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f](https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f)。
- en: We briefly touched on the topics of translation invariance and locality. For
    more information on these topics, visit [https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html](https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要讨论了平移不变性和局部性的主题。欲了解更多信息，请访问 [https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html](https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html)。
- en: 'The relationship between convolution and cross-correlation is discussed here:
    [https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5](https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论了卷积与互相关的关系：[https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5](https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5)。
- en: 'For a better understanding of matrix dimensions, padding, stride, dilation,
    and receptive field, a good explanation is provided here: [https://theaisummer.com/receptive-field/](https://theaisummer.com/receptive-field/).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解矩阵维度、填充、步幅、扩张和感受野，这里提供了一个很好的解释：[https://theaisummer.com/receptive-field/](https://theaisummer.com/receptive-field/)。
- en: 'CNNs have been state-of-the-art for image classification until quite recently;
    in October 2020, **Transformers** were applied to computer vision tasks by Google
    Brain with **ViT**: [https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 在图像分类领域一直处于最前沿，直到最近；2020年10月，**Transformers** 被谷歌大脑应用于计算机视觉任务，并推出了**ViT**：[https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html)。
- en: In a nutshell, instead of forcing a network with the locality principle, Transformer
    architectures allow the same model to decide which features matter most at any
    layer, local or global. This behavior is called **self-attention**. Transformers
    are state-of-the-art for image classification at the time of writing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，与强制网络遵循局部性原则不同，Transformer架构允许相同的模型在任何层决定哪些特征最重要，无论是局部的还是全局的。这种行为被称为**自注意力**。在本文写作时，Transformers
    已经是图像分类领域的最先进技术。
- en: In this chapter, we are going to analyze in detail the following tasks – **image
    classification**, **object detection**, and **image segmentation**. However, *MXNet
    GluonCV Model Zoo* contains lots of models pre-trained for a large number of tasks.
    You are encouraged to explore the different examples provided at [https://cv.gluon.ai/model_zoo/index.html](https://cv.gluon.ai/model_zoo/index.html).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细分析以下任务——**图像分类**、**目标检测**和**图像分割**。然而，*MXNet GluonCV 模型库*包含了大量预训练模型，涵盖了许多任务。鼓励您探索在[https://cv.gluon.ai/model_zoo/index.html](https://cv.gluon.ai/model_zoo/index.html)提供的不同示例。
- en: Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MXNet 进行图像分类——GluonCV 模型库、AlexNet 和 ResNet。
- en: MXNet provides a variety of tools to compose custom deep learning models. In
    this recipe, we will see how to use MXNet to build a model from scratch, train
    it, and use it to classify images from a dataset. We will also see that although
    this approach works fine, it is time-consuming.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 提供了多种工具来构建自定义深度学习模型。在本节中，我们将看到如何使用 MXNet 从零开始构建模型、训练它，并使用它对数据集中的图像进行分类。我们还将看到，尽管这种方法有效，但它是耗时的。
- en: Another option, and one of the highest value features that MXNet and GluonCV
    provide, is their **Model Zoo**. GluonCV Model Zoo is a set of pre-trained, ready-to-go
    models, for use with your own applications. We will see how to use Model Zoo with
    two very important models for image classification – **AlexNet** and **ResNet**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择，也是 MXNet 和 GluonCV 提供的最有价值的功能之一，就是它们的**模型库**。GluonCV 模型库是一个预训练的、即插即用的模型集合，可以用于您的应用程序。我们将看到如何使用模型库，特别是用于图像分类的两个非常重要的模型——**AlexNet**
    和 **ResNet**。
- en: In this recipe, we will analyze and compare these approaches to classify images
    on a reduced version of the *Dogs vs.* *Cats* dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析并比较这些方法，在简化版的 *Dogs vs.* *Cats* 数据集上进行图像分类。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作。
- en: As with previous chapters, in this recipe, we will use a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在本节中，我们将使用一些矩阵运算和线性代数，但不会太困难。
- en: 'Furthermore, we will be classifying image datasets; therefore, we will revisit
    some concepts that we''ve already seen:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将对图像数据集进行分类，因此我们将重新审视一些我们之前已经学习过的概念。
- en: '*Understanding image datasets- load, manage, and visualize the Fashion MNIST
    dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解图像数据集——加载、管理和可视化 Fashion MNIST 数据集*，这是[*第 2 章*](B16591_02.xhtml#_idTextAnchor029)的第三个食谱，*使用
    MXNet 和可视化数据集：Gluon* *和 DataLoader*。'
- en: '[*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification
    Problems*'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 4 章*](B16591_04.xhtml#_idTextAnchor075)，*解决* *分类问题*。'
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will take the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将采取以下步骤：
- en: Explore a reduced version of the *Dogs vs.* *Cats* dataset.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索简化版的 *Dogs vs.* *Cats* 数据集。
- en: Create an AlexNet custom model from scratch.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始创建一个 AlexNet 自定义模型。
- en: Train an *AlexNet* custom model.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 *AlexNet* 自定义模型。
- en: Evaluate an *AlexNet* custom model.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估 *AlexNet* 自定义模型。
- en: Introduce Model Zoo.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍模型库。
- en: Introduce ImageNet pre-trained models.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 ImageNet 预训练模型。
- en: Load an *AlexNet* pre-trained model from *Model Zoo*.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Model Zoo* 加载 *AlexNet* 预训练模型。
- en: Evaluate an *AlexNet* pre-trained model from *Model Zoo*.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估来自 *Model Zoo* 的 *AlexNet* 预训练模型。
- en: Load a ResNet pre-trained model from *Model Zoo*.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Model Zoo* 加载 ResNet 预训练模型。
- en: Evaluate a *ResNet* pre-trained model from *Model Zoo*.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估来自 *Model Zoo* 的 *ResNet* 预训练模型。
- en: Exploring the reduced version of the dataset Dogs vs. Cats
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索简化版的 Dogs vs. Cats 数据集。
- en: For our image classification experiments, we will work with a new dataset, *Dogs
    vs. Cats*. This is a Kaggle dataset ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats))
    that can be downloaded manually. In this recipe, we will work with a reduced version
    of this dataset that can be downloaded from **Zenodo** ([https://zenodo.org/records/5226945](https://zenodo.org/records/5226945))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的图像分类实验，我们将使用一个新的数据集，*Dogs vs. Cats*。这是一个 Kaggle 数据集（[https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)），可以手动下载。在本节中，我们将使用该数据集的简化版本，该版本可以从**Zenodo**（[https://zenodo.org/records/5226945](https://zenodo.org/records/5226945)）下载。
- en: From the set of images in the dataset (either a cat or a dog is depicted), our
    model will need to correctly classify these images. In the first step, as we saw
    in previous chapters, we are going to do some **Exploratory Data** **Analysis**
    (**EDA**).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中的一组图像中（无论是猫还是狗），我们的模型需要正确地对这些图像进行分类。在第一步，正如我们在前几章中看到的，我们将进行一些**探索性数据**
    **分析**（**EDA**）。
- en: '![Figure 5.7 – The Dogs vs. Cats dataset](img/B16591_05_7.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 狗与猫数据集](img/B16591_05_7.jpg)'
- en: Figure 5.7 – The Dogs vs. Cats dataset
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 狗与猫数据集
- en: As can be seen in *Figure 5**.7*, each image in the dataset is in color, and
    they are resized to 224 px by 224 px (width and height). There are 1,000 images
    in the training and validation set and 400 images in the test set.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*图 5.7*中所示，数据集中的每张图像都是彩色的，并且它们的大小被调整为224 px * 224 px（宽度和高度）大小。训练集和验证集共有1,000张图像，测试集有400张图像。
- en: 'As we did in *Understanding image datasets – load, manage, and visualize the
    Fashion MNIST dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we can compute
    some visualizations using the dimensionality reduction techniques – **Principal
    Component Analysis** (**PCA**), **t-distributed stochastic neighbor embedding**
    (**t-SNE**), and **Uniform Manifold Approximation and** **Projection** (**UMAP**):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第2章*](B16591_02.xhtml#_idTextAnchor029)中所做的那样，*理解图像数据集——加载、管理和可视化Fashion
    MNIST数据集*，第三个食谱，*使用MXNet和可视化数据集：Gluon和DataLoader*，我们可以使用降维技术来计算一些可视化结果——**主成分分析**（**PCA**）、**t-分布随机邻域嵌入**（**t-SNE**）和**均匀流形近似与投影**（**UMAP**）：
- en: '![Figure 5.8 – Dogs versus cats visualizations – PCA, t-SNE, and UMAP](img/1.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 狗与猫的可视化 – PCA, t-SNE 和 UMAP](img/1.jpg)'
- en: Figure 5.8 – Dogs versus cats visualizations – PCA, t-SNE, and UMAP
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 狗与猫的可视化 – PCA, t-SNE 和 UMAP
- en: In *Figure 5**.8*, there is no clear boundary region to separate dogs versus
    cats. However, as we will see in the following sections, an architecture introduced
    in the previous chapter, CNNs, will achieve very good results on the task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.8*中，并没有明确的边界区域来区分狗与猫。然而，正如我们将在接下来的章节中看到的，上一章介绍的架构——卷积神经网络（CNNs），将在这个任务中取得非常好的结果。
- en: Creating an AlexNet custom model from scratch
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始创建一个AlexNet自定义模型
- en: AlexNet was a deep neural network that was developed by Alex Krizhevsky, Ilya
    Sutskever, and Geoffrey Hinton in 2012\. It was designed to compete in the **ImageNet
    Large Scale Visual Recognition Challenge** (**ILSVRC**) in 2012 and was the first
    CNN-based model to win this competition.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton于2012年开发的深度神经网络。它是为参加2012年的**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）而设计的，并且是第一个基于CNN的模型，赢得了这一竞赛。
- en: '![Figure 5.9 – AlexNet](img/B16591_05_9.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – AlexNet](img/B16591_05_9.jpg)'
- en: Figure 5.9 – AlexNet
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – AlexNet
- en: The network uses five convolutional layers and three *fully connected* layers.
    The activation function used is the ReLU, and it contains about 63 million trainable
    parameters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用了五个卷积层和三个*全连接*层。所使用的激活函数是ReLU，包含大约6300万个可训练参数。
- en: 'To generate this network from scratch with MXNet, we can use the following
    code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从头开始使用MXNet生成此网络，我们可以使用以下代码：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code uses MXNet functions to add the corresponding 2D *convolutional*,
    *max-pooling*, and *fully connected* layers with their corresponding activation
    functions, generating an AlexNet architecture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用MXNet函数添加相应的2D*卷积*、*最大池化*和*全连接*层，并附加其相应的激活函数，生成一个AlexNet架构。
- en: Training an AlexNet custom model
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练AlexNet自定义模型
- en: The task we are dealing with is an image classification task, which is a classification
    problem where input data is images, and therefore, we can use the training loop
    we saw in [*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving Classification
    Problems* – slightly modified for this task.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理的任务是图像分类任务，这是一种分类问题，输入数据是图像，因此我们可以使用在[*第4章*](B16591_04.xhtml#_idTextAnchor075)中看到的训练循环——稍作修改以适应此任务。
- en: 'The parameters chosen are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的参数如下：
- en: '**Number of** **epochs**: 20'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期数**: 20'
- en: '**Batch size**: 16 samples'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**: 16个样本'
- en: '**Optimizer**: Adam'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**: Adam'
- en: '**Learning** **rate**: 0.0001'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习** **率**: 0.0001'
- en: 'With these parameters, we obtain the following results (the best model was
    achieved on epoch 11):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们获得了以下结果（最佳模型是在第11个周期达到的）：
- en: '**Training** **loss**: 0.36'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练** **损失**: 0.36'
- en: '**Training** **accuracy**: 0.83'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练** **准确率**: 0.83'
- en: '**Validation** **loss**: 0.55'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证** **损失**: 0.55'
- en: '**Validation** **accuracy**: 0.785'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证** **准确率**：0.785'
- en: Evaluating an AlexNet custom model
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估一个自定义的AlexNet模型
- en: 'The accuracy obtained with the best model (in this case, the one corresponding
    to the last training iteration) on the test set is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最佳模型（在此情况下为最后一次训练迭代对应的模型）在测试集上获得的准确率如下：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That’s quite a decent number for just five epochs of training.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅经过五次训练周期而言，这个结果相当不错。
- en: 'Moreover, the confusion matrix computed is shown in the following figure (class
    **0** corresponds to cats and **1** to dogs):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，计算出的混淆矩阵如下图所示（**0**类对应猫，**1**类对应狗）：
- en: '![Figure 5.10 – A trained custom AlexNet confusion matrix](img/B16591_05_10.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 一个训练过的自定义AlexNet混淆矩阵](img/B16591_05_10.jpg)'
- en: Figure 5.10 – A trained custom AlexNet confusion matrix
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 一个训练过的自定义AlexNet混淆矩阵
- en: 'As shown in *Figure 5**.10*, the model mostly predicts accurately the expected
    classes, with the following per-class errors:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.10*所示，该模型大多能够准确预测期望的类别，以下是每个类别的错误率：
- en: '**Cats detected as dogs**: **85**/200 (43% of cat images were misclassified)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**猫被误识为狗**：**85**/200（43%的猫图像被错误分类）'
- en: '**Dogs detected as cats**: **24**/200 (12% of dog images were misclassified)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**狗被误识为猫**：**24**/200（12%的狗图像被错误分类）'
- en: Let’s move on to the next heading.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一部分内容。
- en: Introducing Model Zoo
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍模型库
- en: One of the best features that MXNet and GluonCV provide is their large pool
    of pre-trained models, readily available for its users to use and deploy in their
    own applications. This model library is called **Model Zoo**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet和GluonCV提供的最优秀的功能之一是它们庞大的预训练模型库，用户可以方便地在自己的应用中使用和部署这些模型。这个模型库被称为**模型库**。
- en: 'Moreover, depending on the task at hand, MXNet has some very interesting charts
    that compare the different pre-trained models optimized for tasks. For image classification
    (based on ImageNet), we have the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据手头的任务，MXNet提供了一些非常有趣的图表，用于比较针对不同任务优化的预训练模型。在图像分类（基于ImageNet）方面，我们有以下内容：
- en: '![Figure 5.11 – Model Zoo for image classification (ImageNet)](img/B16591_05_11.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 用于图像分类的模型库（ImageNet）](img/B16591_05_11.jpg)'
- en: Figure 5.11 – Model Zoo for image classification (ImageNet)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 用于图像分类的模型库（ImageNet）
- en: Note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'Source: [https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html)
- en: '*Figure 5**.11* displays the most important pre-trained models in **GluonCV
    Model Zoo**, according to accuracy (the vertical axis) and inference performance
    (the samples per second and horizontal axis). There are no models (yet) in the
    top-right quadrant, meaning that, currently, we need to balance these characteristics.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.11* 显示了**GluonCV模型库**中最重要的预训练模型，根据准确率（纵轴）和推理性能（每秒样本数与横轴）。目前，右上角的象限没有模型，这意味着我们目前需要平衡这些特性。'
- en: Using models from GluonCV Model Zoo can be done with just a couple of lines
    of code, and we will explore this path to solve our reduced *Dogs vs. Cats* dataset
    in the following steps.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自GluonCV模型库的模型仅需几行代码，我们将在接下来的步骤中探索此路径，以解决我们简化的*狗与猫*数据集。
- en: ImageNet pre-trained models
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageNet预训练模型
- en: Models from the **GluonCV Model Zoo** for **image classification** tasks have
    been pre-trained in the *ImageNet* dataset. This dataset is one of the most well-known
    datasets in computer vision. It was the first large-scale image dataset and was
    part of the deep learning revolution when, in 2012, AlexNet won the ILSVRC.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**GluonCV模型库**中的模型已经在*ImageNet*数据集上进行了预训练，用于**图像分类**任务。这个数据集是计算机视觉领域最著名的数据集之一。它是第一个大规模的图像数据集，并且在2012年AlexNet赢得ILSVRC时，成为深度学习革命的一部分。'
- en: 'This dataset has two variants:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集有两个变体：
- en: '**Full dataset**: More than 20,000 classes in about 14 million images'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整数据集**：超过20,000个类别，约1,400万张图像'
- en: '**ImageNet-1k**: 1,000 classes in about 1 million images'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ImageNet-1k**：大约1000个类别，约100万张图像'
- en: Due to the size and large number of classes, the full dataset is rarely used
    on benchmarks, with *ImageNet1k* the de facto ImageNet dataset (unless otherwise
    noted in the research papers, articles, and so on). Images in the dataset are
    in color and have a size of 224px by 224px (width and height).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集的庞大和类别的数量，完整的数据集在基准测试中很少使用，通常*ImageNet1k*被视为标准的ImageNet数据集（除非在研究论文、文章等中另有说明）。数据集中的图像为彩色图像，尺寸为224px×224px（宽×高）。
- en: All image classification pre-trained models in GluonCV Model Zoo have been pre-trained
    with ImageNet-1k, and therefore, they have 1,000 outputs. The outputs will be
    post-processed so that all ImageNet classes corresponding to cats point to class
    0, all ImageNet classes corresponding to dogs point to class 1, and all other
    outputs point to class 2, which we will consider as unknown.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GluonCV模型库中的所有图像分类预训练模型都已经在ImageNet-1k上进行过预训练，因此它们有1,000个输出。输出将经过后处理，使得所有与猫相关的ImageNet类别指向类别0，所有与狗相关的ImageNet类别指向类别1，所有其他输出指向类别2，我们将其视为未知类别。
- en: Loading an AlexNet pre-trained model from Model Zoo
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从模型库加载预训练的AlexNet模型
- en: To compare the advantages and disadvantages of using a custom-trained model
    and a pre-trained model from Model Zoo, in the following sections, we will work
    with a version of the AlexNet architecture that has been pre-trained on the *ImageNet*
    dataset, acquired from GluonCV Model Zoo.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较使用自定义训练模型和来自模型库的预训练模型的优缺点，在接下来的部分中，我们将使用一个在*ImageNet*数据集上预训练的AlexNet架构版本，该模型来自GluonCV模型库。
- en: 'Loading a pre-trained model is very easy and can be done with a single line
    of code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 加载预训练模型非常简单，可以通过一行代码完成：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `get_model` GluonCV function receives three parameters:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_model` GluonCV函数接收三个参数：'
- en: '`alexnet`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alexnet`'
- en: '`False`, only the uninitialized architecture will be loaded)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`，只会加载未初始化的架构'
- en: '`mx.cpu()` or `mx.gpu()`, if available'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mx.cpu()` 或 `mx.gpu()`，如果可用'
- en: This call will download the chosen model and, if required, its pre-trained weights
    and biases.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此调用将下载所选模型，并在需要时下载其预训练的权重和偏差。
- en: Evaluating an AlexNet pre-trained model from Model Zoo
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估来自模型库的预训练AlexNet模型
- en: 'With the loaded model from the previous section, we can now evaluate and compare
    our previous results, such as for `accuracy`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一节加载的模型，我们现在可以评估并比较之前的结果，比如 `accuracy`：
- en: '[PRE7]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, this number is slightly lower than the accuracy we achieved with
    our custom-trained AlexNet model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个数字略低于我们使用自定义训练的AlexNet模型时获得的准确率。
- en: 'After computing the confusion matrix, we obtain the following values:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 计算混淆矩阵后，我们得到以下值：
- en: '![Figure 5.12 – A pre-trained AlexNet Confusion Matrix](img/B16591_05_12.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 预训练的AlexNet混淆矩阵](img/B16591_05_12.jpg)'
- en: Figure 5.12 – A pre-trained AlexNet Confusion Matrix
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 预训练的AlexNet混淆矩阵
- en: When we analyze *Figure 5**.12*, the most significant difference is that our
    previous confusion matrix was a 2x2 matrix (with the options of **0** or **1**
    for true labels and **0** or **1** for predicted labels). However, with our pre-trained
    model, we have obtained a 3x3 confusion matrix. This is because, as mentioned
    previously, pre-trained models have been trained in ImageNet, and they output
    1,000 classes (instead of the two required for our dataset). These outputs have
    been post-processed, so all *ImageNet* classes corresponding to cats point to
    class **0**, all ImageNet classes corresponding to dogs point to class **1**,
    and all other outputs point to class **2**, which we will consider as unknown.
    When taking into account this unknown class **2**, the 3x3 matrix is computed.
    Please note how there are no images that produce a true label of **2**; the last
    row is all zeros.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们分析*图 5.12*时，最显著的区别是我们之前的混淆矩阵是一个2x2矩阵（对于真实标签和预测标签，选项为**0**或**1**）。然而，使用我们预训练的模型时，我们得到了一个3x3的混淆矩阵。这是因为，正如前面所提到的，预训练模型是基于ImageNet训练的，它们输出1,000个类别（而不是我们数据集所需的两个类别）。这些输出经过后处理，因此所有与猫相关的*ImageNet*类别指向类别**0**，所有与狗相关的ImageNet类别指向类别**1**，所有其他输出指向类别**2**，我们将其视为未知类别。考虑到这个未知类别**2**，就得出了3x3的矩阵。请注意，结果中没有产生真实标签为**2**的图像；最后一行全是零。
- en: 'The model mostly behaves accurately with the expected classes, with the following
    per-class errors (we need to add numbers from the two wrong columns):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型大部分时间能够准确地预测预期类别，以下是每个类别的错误（我们需要将两个错误列的数字相加）：
- en: '**Cats not detected as cats**: **96**/200 (48% of cats images were misclassified)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未检测为猫的猫**：**96**/200（48%的猫图像被误分类）'
- en: '**Dogs not detected as dogs**: **14**/200 (7% of dogs images were misclassified)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未检测为狗的狗**：**14**/200（7%的狗图像被误分类）'
- en: There is a significant difference in the per-class results, and this is due
    to the pre-trained dataset used, *ImageNet*, because it has a large number of
    classes associated with dog breeds and, therefore, has been trained more extensively
    on dog images.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在每类结果中存在显著差异，这主要是由于使用了预训练的数据集 *ImageNet*，因为它包含了大量与狗品种相关的类别，因此在狗图像上进行了更广泛的训练。
- en: Loading a ResNet pre-trained model from Model Zoo
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从模型库加载预训练的 ResNet 模型
- en: Looking at AlexNet and later models with a higher depth, such as VGGNet, it
    became clear that deeper layers could help when classifying images. However, when
    training these deep networks, by using *backpropagation* and the *chain rule*,
    the training algorithm starts computing smaller and smaller values for the gradients
    because of the large number of multiplications of small numbers (the activation
    function outputs are in the [0, 1] range), and therefore, when the gradients for
    the early layers are computed, the updated weights are seldom modified. This is
    known as the **vanishing gradient** problem, and different **ResNet** architectures
    were developed to avoid it. Specifically, ResNet models use residual blocks that
    add direct lines to connect layers, providing shortcuts that can be leveraged
    during training to avoid vanishing gradients.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看 AlexNet 及其后续更深的模型，如 VGGNet，逐渐清晰地认识到，深层网络在分类图像时确实能够起到帮助作用。然而，当训练这些深层网络时，利用
    *反向传播* 和 *链式法则*，训练算法开始计算越来越小的梯度值，因为大量的小数相乘（激活函数的输出位于 [0, 1] 范围内），因此，当计算早期层的梯度时，更新的权重很少发生变化。这就是著名的
    **梯度消失** 问题，而不同的 **ResNet** 架构则是为了解决这一问题而开发的。具体而言，ResNet 模型使用残差块，在层与层之间加入直通连接，为训练提供了可以利用的捷径，从而避免了梯度消失问题。
- en: '![Figure 5.13 – A ResNet residual block](img/B16591_05_13.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – ResNet 残差块](img/B16591_05_13.jpg)'
- en: Figure 5.13 – A ResNet residual block
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – ResNet 残差块
- en: The architecture shown in *Figure 5**.13* allows you to stack layers in a more
    scalable way, with known architectures with 18, 50, 101, and 152 layers. This
    approach proved very successful, and ResNet152 won the ILSVRC in 2015.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 5.13* 所示的架构使得可以更具扩展性地堆叠层，并且已有 18、50、101 和 152 层的已知架构。这种方法非常成功，ResNet152
    在 2015 年赢得了 ILSVRC。
- en: 'In this case, we will load the `v1d` version of `resNet50`, with a single line
    of code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将加载 `resNet50` 的 `v1d` 版本，只需一行代码：
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model then downloads successfully.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 模型随后成功下载。
- en: Evaluating a ResNet pre-trained model from Model Zoo
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估来自模型库的预训练 ResNet 模型
- en: 'With the loaded model from the previous section, we can now evaluate and compare
    with our previous results, such as for `accuracy`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一节加载的模型，我们现在可以评估并与之前的结果进行比较，例如 `准确率`：
- en: '[PRE9]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see, this number is significantly higher than previous models.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个数字明显高于之前的模型。
- en: 'After computing the confusion matrix, we obtain the following values:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算混淆矩阵后，我们得到了以下值：
- en: "![Figure 5.14 – A pre-trained ResNet \uFEFFconfusion \uFEFFmatrix](img/B16591_05_14.jpg)"
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 预训练的 ResNet 混淆矩阵](img/B16591_05_14.jpg)'
- en: Figure 5.14 – A pre-trained ResNet confusion matrix
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 预训练的 ResNet 混淆矩阵
- en: 'According to *Figure 5**.14*, the per-class error rate is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 *图 5.14*，每类的错误率如下：
- en: '**Cats not detected as cats**: **29**/200 (14.5% of cat images were misclassified)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未检测为猫的猫**：**29**/200（14.5%的猫图像被错误分类）'
- en: '**Dogs not detected as dogs**: **1**/200 (0.5% of dog images were misclassified)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未检测为狗的狗**：**1**/200（0.5%的狗图像被错误分类）'
- en: There is a significant difference in the per-class results, and this is again
    due to the pre-trained dataset used, *ImageNet*, because it has a large number
    of classes associated with dog breeds and, therefore, has been trained more extensively
    on dog images.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在每类结果中存在显著差异，这再次是由于使用了预训练的数据集 *ImageNet*，因为它包含了大量与狗品种相关的类别，因此在狗图像上进行了更广泛的训练。
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we compared two approaches to using computer vision models
    for image classification:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们比较了两种使用计算机视觉模型进行图像分类的方法：
- en: Training a custom model from scratch
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练自定义模型
- en: Using pre-trained models from GluonCV Model Zoo
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自 GluonCV 模型库的预训练模型
- en: We applied both approaches with AlexNet architecture and compared the results
    with the **ResNet-101 Model** **Zoo** version.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将两种方法应用于 AlexNet 架构，并将结果与 **ResNet-101 模型库** 版本进行了比较。
- en: Both approaches have advantages and disadvantages. Training from scratch provides
    us direct control on the number of output classes, and we can fully handle the
    training process and the evolution of loss and accuracy for both the training
    and validation datasets.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法都有优缺点。从零开始训练可以让我们直接控制输出类别的数量，并且可以完全掌控训练过程以及训练和验证数据集上损失和准确率的变化。
- en: However, in order to train a model, we need sufficient data, and that might
    not be always available. Furthermore, adjusting the training hyperparameters (epochs,
    batch size, optimizer, and learning rate) and the training itself are time-consuming
    processes that, if not done properly, can yield suboptimal accuracy values (or
    other metrics).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了训练一个模型，我们需要足够的数据，但这些数据可能并不总是可用的。此外，调整训练超参数（训练轮次、批次大小、优化器和学习率）以及实际训练过程是非常耗时的过程，如果做得不当，可能会导致准确度（或其他指标）不理想。
- en: In our examples, we used a reduced version of the *Dogs vs. Cats* dataset from
    Kaggle, and we used pre-trained models on *ImageNet*. The Kaggle dataset contains
    25,000 images (more than 10 times more) and the reader is encouraged to try the
    proposed solution on that dataset (all helper functions have also been tested
    with the full dataset).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用了Kaggle的*Dogs vs. Cats*数据集的简化版本，并使用了在*ImageNet*上预训练的模型。Kaggle数据集包含25,000张图像（比原始数据集多10倍以上），我们鼓励读者在该数据集上尝试所提出的解决方案（所有辅助函数也已经在完整数据集上进行了测试）。
- en: Furthermore, the choice of the *ImageNet* dataset was not casual; *ImageNet*
    has dog classes and cat classes, and therefore, the expectation was that these
    pre-trained models would perform well, as they had already seen images from the
    `dataset` class. However, when this is not possible, and we apply pre-trained
    models from a dataset to another dataset, the data probability distribution will
    typically be very different; hence, the accuracy obtained can be very low. This
    is known as the **domain gap** or **domain adaptation problem** between the source
    dataset (the data the model has been pre-trained on) and the target dataset (the
    data the model is evaluated on).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，选择*ImageNet*数据集并非随意决定；*ImageNet*拥有狗类和猫类的分类，因此预期这些预训练模型会表现良好，因为它们已经看过来自`dataset`类别的图像。然而，当无法做到这一点时，我们将一个数据集的预训练模型应用到另一个数据集上，数据的概率分布通常会非常不同；因此，得到的准确率可能非常低。这被称为**领域间差距**或**领域适应问题**，即源数据集（模型已在该数据集上预训练）与目标数据集（模型在该数据集上进行评估）之间的差距。
- en: One way to tackle these issues for supervised learning problems is fine-tuning.
    This approach is explored in detail in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 解决监督学习问题的一个方法是微调。该方法在[*第7章*](B16591_07.xhtml#_idTextAnchor148)中有详细探讨。
- en: We finalized the recipe evaluating our two pre-trained models, *AlexNet* and
    *ResNet*, and saw how CNN models have evolved through the years, increasing the
    accuracy obtained.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过评估两种预训练模型——*AlexNet*和*ResNet*，来完成这个食谱，并了解了CNN模型如何随着时间的推移演变，从而提高了获得的准确率。
- en: There’s more...
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In this recipe, we used *ImageNet* pre-trained models; for more information
    about *ImageNet*, and the ILSVRC, I suggest this article: [https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/](https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们使用了*ImageNet*预训练模型；有关*ImageNet*和ILSVRC的更多信息，我建议阅读这篇文章：[https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/](https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/)。
- en: Although primarily about the ILSVRC, this previous link also includes some history
    regarding CNNs, including *AlexNet*, VGGNet, and *ResNet*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这篇文章主要讲的是ILSVRC，但前面的链接也包括了一些关于CNN的历史，包括*AlexNet*、VGGNet和*ResNet*。
- en: 'However, computer vision datasets have been under strict scrutiny recently
    regarding data quality, and ImageNet is no exception, as this article describes:
    [https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/](https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最近计算机视觉数据集在数据质量方面受到了严格审查，*ImageNet*也不例外，正如本文所描述：[https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/](https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/)。
- en: '*Figure 5**.11* shows a static image corresponding to the accuracy versus samples
    per second graph for the Model Zoo for image classification (on *ImageNet*). A
    snapshot of a dynamic version is available at this link and is worth taking a
    look at: [https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.11*展示了一个静态图像，展示了模型库在图像分类（在*ImageNet*上）中的准确度与每秒样本数的关系图。一个动态版本的快照可以在这个链接查看，值得一看：[https://cv.gluon.ai/model_zoo/classification.html](https://cv.gluon.ai/model_zoo/classification.html)。'
- en: At this link, results from different models available in GluonCV Model Zoo are
    included, and it is suggested that you reproduce these results, as it is an interesting
    exercise.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个链接中，包含了GluonCV模型库中不同模型的结果，建议你复现这些结果，因为这是一个有趣的练习。
- en: Apart from ImageNet, GluonCV Model Zoo provides models pre-trained on **CIFAR10**.
    A list of these models can be found at [https://cv.gluon.ai/model_zoo/classification.html#cifar10.](https://cv.gluon.ai/model_zoo/classification.html#cifar10)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ImageNet，GluonCV模型库还提供了预训练的**CIFAR10**模型。这些模型的列表可以在[https://cv.gluon.ai/model_zoo/classification.html#cifar10](https://cv.gluon.ai/model_zoo/classification.html#cifar10)找到。
- en: 'For a deeper explanation of the vanishing gradient problem, Wikipedia provides
    a good start: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入理解梯度消失问题，维基百科提供了一个很好的起点：[https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。
- en: 'Lastly, regarding ResNet and its current research relevance, in a recently
    published paper, it was shown that ResNet can still achieve **State-of-the-Art**
    (**SOTA**) results when the latest researched training techniques are applied
    to it, highlighting the importance of datasets and a training algorithm (versus
    optimization only for model architecture): [https://gdude.de/blog/2021-03-15/Revisiting-Resnets](https://gdude.de/blog/2021-03-15/Revisiting-Resnets).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于ResNet及其当前的研究意义，在一篇最近发表的论文中，显示了当应用最新研究的训练技术时，ResNet仍然能够达到**最先进的**（**SOTA**）结果，突出了数据集和训练算法（而不仅仅是优化模型架构）的重要性：[https://gdude.de/blog/2021-03-15/Revisiting-Resnets](https://gdude.de/blog/2021-03-15/Revisiting-Resnets)。
- en: Detecting objects with MXNet – Faster R-CNN and YOLO
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MXNet进行目标检测——Faster R-CNN和YOLO。
- en: In this recipe, we will see how to use MXNet and GluonCV on a pre-trained model
    to detect objects from a dataset. We will see how to use GluonCV Model Zoo with
    two very important models for **object detection** – **Faster R-CNN** and **YOLOv3**.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将看到如何使用MXNet和GluonCV在一个预训练模型上检测数据集中的物体。我们将看到如何使用GluonCV模型库中的两个非常重要的**目标检测**模型——**Faster
    R-CNN**和**YOLOv3**。
- en: In this recipe, we will compare the performance of these two pre-trained models
    to detect objects on the *Penn-Fudan* *Pedestrians* dataset.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将比较这两个预训练模型在*Penn-Fudan* *行人*数据集上检测物体的性能。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As for previous chapters, in this recipe, we will be using a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 和前几章一样，在本食谱中，我们将使用一些矩阵运算和线性代数，但不会太难。
- en: 'As we will unpack in this recipe, object detection combines classification
    and regression, and therefore, chapters and recipes where we explored the foundations
    of these topics are recommended to revisit. Furthermore, we will be detecting
    objects on image datasets. This recipe will combine what we learned in the following
    chapters:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本食谱中详细介绍的，目标检测结合了分类和回归，因此，建议重新阅读我们在前几章和食谱中探讨这些主题基础的部分。此外，我们将在图像数据集上检测物体。本食谱将结合我们在以下章节中学到的内容：
- en: '*Understanding image datasets: load, manage, and visualize the Fashion MNIST
    dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解图像数据集：加载、管理和可视化Fashion MNIST数据集*，这是[*第2章*](B16591_02.xhtml#_idTextAnchor029)的第三个食谱，*使用MXNet并可视化数据集：Gluon*
    *和DataLoader*。'
- en: '[*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving* *Regression Problems*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B16591_03.xhtml#_idTextAnchor052)，*解决回归问题*'
- en: '[*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification
    Problems*'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B16591_04.xhtml#_idTextAnchor075)，*解决分类问题*'
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行……
- en: 'In this recipe, we will take the following steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将采取以下步骤：
- en: Introduce object detection.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍目标检测。
- en: Evaluate object detectors.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估目标检测器。
- en: Compare *Single-stage* and *Two-stage* object detectors.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较*单阶段*和*双阶段*目标检测器。
- en: Explore the *Penn-Fudan* *Pedestrians* dataset.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索*Penn-Fudan* *行人*数据集。
- en: Introduce the Object Detection Model Zoo.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍目标检测模型库。
- en: Worke with *MS COCO* pre-trained models.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*MS COCO*预训练模型。
- en: Load a Faster R-CNN pre-trained model from *Model Zoo*.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*Model Zoo*加载一个预训练的Faster R-CNN模型。
- en: Evaluate a *Faster R-CNN* pre-trained model from *Model Zoo*.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估一个*Faster R-CNN*预训练模型来自*Model Zoo*。
- en: Load a YOLOv3 pre-trained model from *Model Zoo*.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*Model Zoo*加载一个预训练的YOLOv3模型。
- en: Evaluate a *YOLOv3* pre-trained model from *Model Zoo*.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估一个*YOLOv3*预训练模型来自*Model Zoo*。
- en: Conclude what we have learned.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结我们学到的内容。
- en: Introducing object detection
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍目标检测
- en: 'In some of the previous chapters and recipes, we analyzed image classification
    problems, where the task of our models was to take an image and define the class
    most likely associated with it. In **object detection**, however, there can be
    multiple objects per image, corresponding to different classes, and in different
    locations of the image, and therefore, the output is now two lists, one providing
    the most likely class of each detected object and another indicating the estimated
    location of the object. The class output can be modeled as a classification problem,
    and the bounding box output can be modeled as a regression problem. Typically,
    locations are represented with what is called a bounding box. An example of bounding
    boxes is shown as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章和配方中，我们分析了图像分类问题，模型的任务是接收一张图像并定义与之最相关的类别。然而，在**目标检测**中，每张图像中可能有多个物体，对应不同的类别，且位于图像的不同位置，因此输出现在是两个列表，一个提供每个检测物体最可能的类别，另一个表示物体的估计位置。类别输出可以建模为一个分类问题，而边界框输出可以建模为回归问题。通常，位置用所谓的边界框来表示。以下是边界框的示例：
- en: '![Figure 5.15 – Bounding box examples](img/B16591_05_15.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 边界框示例](img/B16591_05_15.jpg)'
- en: Figure 5.15 – Bounding box examples
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 边界框示例
- en: In *Figure 5**.15*, we can see two examples of bounding boxes for two different
    classes – `person` and `dog`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.15*中，我们可以看到两个不同类别的边界框示例——`person`和`dog`。
- en: Evaluating object detectors
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估目标检测器
- en: In image classification, we defined a correct classification if the class identified
    in one image was the right one. However, in object detection, there are two parameters
    – the class and the bounding box. Intuitively, we can define a correct classification
    if, for each object that should be detected, there is a *similar enough* bounding
    box that has been classified properly. To define what *similar enough* means,
    we compute **Intersection over Union** (**IoU**), the ratio of the intersection
    of the area of the bounding boxes over the area of the union of the bounding boxes.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类中，我们定义一个正确的分类是指图像中识别出的类别是正确的。然而，在目标检测中，有两个参数——类别和边界框。直观地，我们可以定义一个正确的分类，如果对于每个应该检测到的物体，都有一个*足够相似*的边界框被正确分类。为了定义什么是*足够相似*，我们计算**交并比**(**IoU**)，即边界框交集区域与边界框并集区域的比值。
- en: '![Figure 5.16 – IoU](img/B16591_05_16.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – IoU](img/B16591_05_16.jpg)'
- en: Figure 5.16 – IoU
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – IoU
- en: A graphical interpretation of IoU can be seen in *Figure 5**.16*. When the IoU
    is above a determined threshold, the bounding boxes are said to match. By using
    IoU and its threshold (typically 0.5), a detection can be classified as correct
    (given the object was also correctly classified), and metrics such as accuracy,
    precision, and recall can be computed (per class).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: IoU的图形解释可以在*图 5.16*中看到。当IoU超过某个确定的阈值时，边界框被认为匹配。通过使用IoU及其阈值（通常为0.5），可以将检测分类为正确（前提是物体也被正确分类），并可以计算准确率、精度和召回率等指标（按类别计算）。
- en: 'Furthermore, in [*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving Classification
    Problems*, we discussed several options to evaluate *classification* problems.
    We introduced **Area Under the Curve**(**AUC**), and we saw how changing the threshold
    had an influence on **precision** and **recall**. When plotting precision and
    recall together (the **PR curve**), we can see the effect of the threshold, in
    the same way as we did for AUC. If we calculate the area covered between the curve
    (the *x* axis, *y = 0 axis*, and *y = 1 axis*), we obtain a parameter that is
    not dependent on the threshold value; it defines the performance of our model
    with the given data:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[*第 4 章*](B16591_04.xhtml#_idTextAnchor075)《解决分类问题》中，我们讨论了评估*分类*问题的几种选择。我们介绍了**曲线下面积**（**AUC**），并且我们看到了改变阈值对**精确度**和**召回率**的影响。当我们将精确度和召回率一起绘制（**PR
    曲线**）时，我们可以看到阈值的影响，就像我们在 AUC 中所做的那样。如果我们计算曲线下的面积（*x* 轴、*y = 0 轴* 和 *y = 1 轴* 之间的面积），我们得到一个不依赖于阈值的参数，它定义了我们模型在给定数据下的性能：
- en: '![Figure 5.17 – The PR curve](img/B16591_05_17.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – PR 曲线](img/B16591_05_17.jpg)'
- en: Figure 5.17 – The PR curve
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – PR 曲线
- en: One of the characteristics of this curve that we can see clearly in *Figure
    5**.17* is its zig-zag pattern. As we decrease the threshold, the curve goes down
    with false positives and goes up again with true positives.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 5.17*中清楚地看到这条曲线的一个特点，它呈现锯齿形。当我们降低阈值时，曲线因为假阳性而下降，然后随着真正的阳性再次上升。
- en: 'However, in order to be able to compare different models easily, instead of
    comparing the PR curves for each class, a single number metric was developed,
    the **mean Average Precision** (**mAP**). In short, it is the mean of all the
    areas under the PR curves for a model:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了能够轻松比较不同的模型，开发了一个单一的数字指标，而不是为每个类别比较 PR 曲线，这个指标就是**平均精度均值**（**mAP**）。简而言之，它是一个模型所有
    PR 曲线下面积的平均值：
- en: mAP =  1 _ N  ∑ i=1 N A P i
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: mAP = 1/N ∑ i=1 N AP i
- en: To compute *mAP*, the first step is to calculate the average precision for each
    class, which is the area under the PR curve and then compute its arithmetic mean.
    This value provides a single number where object detection models evaluated on
    the same dataset can be compared.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 *mAP*，第一步是计算每个类别的平均精度，这是 PR 曲线下面积，然后计算其算术平均值。这个值提供了一个单一的数字，可以用来比较在同一数据集上评估的物体检测模型。
- en: Comparing single-stage and two-stage object detectors
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较单阶段和二阶段物体检测器
- en: 'We can think of object detectors as cropping a specific part of an image and
    passing that cropped image through an image classifier, similar to what we did
    with full images in the previous recipe. Using this approach, our object detector
    will have two steps:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将物体检测器看作是裁剪图像的特定部分，并将该裁剪后的图像传递给图像分类器，这类似于我们在前面的步骤中对完整图像所做的操作。采用这种方法，我们的物体检测器将有两个步骤：
- en: '**Region proposal network**: This is the module that will indicate the regions
    where an object could be located.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**区域提议网络**：这是一个模块，用于指示可能包含物体的区域。'
- en: '**Object classifier**: The regions will be classified by the model, with the
    regions previously cropped and resized to match the model input constraints.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**物体分类器**：模型将对区域进行分类，区域之前已经裁剪并调整大小，以匹配模型输入的约束条件。'
- en: This approach is known as **two-stage object detection**, and its most important
    characteristic is its accuracy, although it is slow due to its complex architecture,
    and fully accurate (non-approximate) training cannot be done end to end.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**二阶段物体检测**，其最重要的特点是准确性，尽管由于其复杂的架构，速度较慢，并且无法进行完全精确（非近似）的端到端训练。
- en: Faster **Region-based Convolutional Neural Network** (**R-CNN**) is one of the
    models that follow this approach. One of the most important differences from previous
    versions of the model (**R-CNN** and **Fast R-CNN**) is that in order to provide
    faster computations, it uses pre-computed bounding boxes called *anchor boxes*,
    where the scale and the aspect ratio of the bounding box are pre-defined. This
    approach allows the networks to be modeled to compute the *offset* related to
    an anchor box, instead of the full bounding box coordinates, which simplifies
    the regression problem.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 更快的**基于区域的卷积神经网络**（**R-CNN**）是遵循这种方法的模型之一。与该模型的先前版本（**R-CNN** 和 **Fast R-CNN**）相比，最重要的区别是为了提供更快的计算，它使用预计算的边界框，称为*锚框*，其中边界框的尺度和长宽比是预定义的。这种方法允许网络模型计算与锚框相关的*偏移量*，而不是完整的边界框坐标，从而简化了回归问题。
- en: Another algorithm to improve computation times is **Non-Maximum Suppression**
    (**NMS**). Typically, thousands of regions will be proposed for the next step
    of the object detection pipeline. Many of these regions overlap with each other,
    and NMS is the algorithm that takes into account the confidence of the prediction,
    removing all overlapping regions over an IoU threshold.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提高计算时间的算法是**非极大值抑制**（**NMS**）。通常会对目标检测流程的下一步提出成千上万个区域建议。这些区域中许多重叠，而NMS算法考虑预测的置信度，移除所有IoU阈值以上重叠的区域。
- en: 'Another approach for object detectors is to design architectures that make
    predictions of bounding boxes and class probabilities together, allowing end-to-end
    training in one step. Architectures following this approach are known as **single-stage
    object detectors**. These architectures also make use of anchor boxes and NMS
    to improve the regression task. The two most famous architectures using this approach
    are as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种目标检测器的方法是设计可以同时预测边界框和类别概率的架构，使得可以进行端到端的一步训练。遵循这种方法的架构被称为**单阶段目标检测器**。这些架构还利用锚框和NMS来改进回归任务。使用这种方法的两个最著名的架构如下：
- en: '**You Only Look Once (YOLO**): The image is processed just once using a custom
    CNN architecture (a combination of convolutional and max-pooling layers), ending
    with two fully connected layers. These architectures have been developed continuously,
    with *YOLOv3* being one of the most popular.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**You Only Look Once (YOLO)**：图像仅一次处理，使用自定义的CNN架构（卷积层和最大池化层的组合），最终以两个全连接层结束。这些架构已经不断发展，*YOLOv3*是其中最流行的之一。'
- en: '**Single Shot Detector (SSD)**: The image is processed using a CNN backbone
    architecture (such as VGG-16) to compute feature maps, and the generated multi-scale
    feature maps are then classified. SSD512 (using *VGG-16* as the backbone) is one
    of the models that follow this architecture.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单发多框检测器（SSD）**：图像使用CNN主干架构（如VGG-16）进行处理以计算特征图，并且生成的多尺度特征图然后被分类。SSD512（使用*VGG-16*作为主干）是遵循这种架构的模型之一。'
- en: YOLOv3 model is the fastest and yields reasonable accuracy metrics, the SSD512
    model is a good trade-off between speed and accuracy, and the Faster R-CNN model
    has the highest accuracy and is the slowest of the three.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv3模型速度最快，并且产生了合理的准确度指标；SSD512模型在速度和准确度之间取得了良好的平衡；而Faster R-CNN模型具有最高的准确度，但速度是三者中最慢的。
- en: Exploring the Penn-Fudan Pedestrians dataset
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索Penn-Fudan行人数据集
- en: For our object detection experiments, we will work with a new dataset – *Penn-Fudan
    Pedestrians*. This is a publicly available dataset ([https://www.cis.upenn.edu/~jshi/ped_html/](https://www.cis.upenn.edu/~jshi/ped_html/))
    and is a collaboration between the universities of Pennsylvania and Fudan, and
    it must be downloaded manually.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目标检测实验，我们将使用一个新的数据集 – *Penn-Fudan行人*。这是一个公开可用的数据集（[https://www.cis.upenn.edu/~jshi/ped_html/](https://www.cis.upenn.edu/~jshi/ped_html/)），是宾夕法尼亚大学和复旦大学之间的合作项目，需要手动下载。
- en: The dataset has 423 pedestrians annotated from 170 images; 345 pedestrians were
    annotated for the release of the dataset (2007) and 78 pedestrians were added
    later, as the previous ones were either small or occluded.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有423个行人在170张图像中被标注；在数据集发布时（2007年），标注了345名行人，后来增加了78名行人，因为之前的行人要么很小要么被遮挡。
- en: From the set of images of the datasets, our models will need to correctly detect
    the pedestrians present in the images and localize them, using bounding boxes.
    To understand the problem better, as we saw in previous chapters, we are going
    to do some EDA.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集的图像集合中，我们的模型将需要正确检测图像中的行人并定位它们，使用边界框。为了更好地理解问题，正如我们在之前章节中所看到的，我们将进行一些探索性数据分析（EDA）。
- en: '![Figure 5.18 – The Penn-Fudan Pedestrians dataset](img/B16591_05_18.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图5.18 – Penn-Fudan行人数据集](img/B16591_05_18.jpg)'
- en: Figure 5.18 – The Penn-Fudan Pedestrians dataset
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 – Penn-Fudan行人数据集
- en: 'As we can see in *Figure 5**.18*, each image can have one or more pedestrians,
    with their corresponding bounding boxes. Each image in the dataset is in color,
    and they have a variable width and height, which are later resized depending on
    the model requirements. This figure was computed using the GluonCV visualization
    utils package (the `plot_bbox` function):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图5**.18*中所看到的，每个图像可能包含一个或多个行人，以及它们对应的边界框。数据集中的每个图像都是彩色的，它们具有可变的宽度和高度，这些后来会根据模型的需求进行调整。此图是使用GluonCV的可视化工具包（`plot_bbox`函数）生成的：
- en: '[PRE10]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As for this datase,t there are no different classes to be classified, and no
    further visualizations are computed.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，没有需要分类的不同类别，也没有计算进一步的可视化。
- en: Introducing object detection Model Zoo
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入物体检测模型库
- en: 'GluonCV also provides pre-trained models for object detection in its Model
    Zoo. For the *MS COCO* dataset, this is the accuracy (mAP) versus performance
    (samples per second) chart:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: GluonCV 还提供了用于物体检测的预训练模型，位于其模型库中。对于 *MS COCO* 数据集，这是准确度（mAP）与性能（每秒样本数）图表：
- en: '![Figure 5.19 – Model Zoo for object detection (MS COCO)](img/B16591_05_19.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – 物体检测模型库（MS COCO）](img/B16591_05_19.jpg)'
- en: Figure 5.19 – Model Zoo for object detection (MS COCO)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – 物体检测模型库（MS COCO）
- en: Note
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Image adapted from the following source: [https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自以下来源：[https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)
- en: '*Figure 5**.19* displays the most important pre-trained models in GluonCV Model
    Zoo, comparing accuracy (mAP on the vertical axis) and inference performance (samples
    per second on the horizontal axis). There are no models (yet) in the top-right
    quadrant, meaning that, currently, we need to balance between both characteristics.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.19* 展示了 GluonCV 模型库中最重要的预训练模型，比较了准确度（垂直轴上的 mAP）和推理性能（水平轴上的每秒样本数）。目前，在右上象限没有模型，意味着目前我们需要在这两者之间进行平衡。'
- en: Using models from GluonCV Model Zoo can be done with just a couple of lines
    of code, and we will explore this path to solve our *Penn-Fudan Pedestrians* dataset
    in the following steps.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自 GluonCV 模型库的模型只需几行代码，我们将在接下来的步骤中探讨使用这些模型来解决我们的 *Penn-Fudan Pedestrians*
    数据集问题。
- en: Working with MS COCO pre-trained models
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 MS COCO 预训练模型
- en: 'Models from the GluonCV Model Zoo for object detection tasks have been pre-trained
    in the *MS COCO* dataset. This dataset is one of the most popular object detection
    datasets in computer vision. It was developed by Microsoft in 2015 and was updated
    until 2017\. In its most recent update, it contains 80 classes (plus background)
    and is composed of the following:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: GluonCV 模型库中的物体检测任务模型已在 *MS COCO* 数据集上进行预训练。该数据集是计算机视觉中最受欢迎的物体检测数据集之一。它由微软于
    2015 年开发，并持续更新至 2017 年。在其最新的更新中，它包含 80 类（加上背景），包括以下内容：
- en: '**Training/validation sets**: 118,000/5,000 images'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练/验证集**：118,000/5,000 张图片'
- en: '**A test set**: 41,000 images'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个测试集**：41,000 张图片'
- en: 'Several object detection pre-trained models in GluonCV Model Zoo have been
    pre-trained with *MS COCO*, and therefore, each object detected will be classified
    among 80 classes. As there can be several objects in each image, in MXNet GluonCV
    implementations, the outputs of an object detection model are structured as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: GluonCV 模型库中的多个物体检测预训练模型已使用 *MS COCO* 数据集进行预训练，因此，每个检测到的物体将被分类为 80 类之一。由于每张图片可能包含多个物体，在
    MXNet GluonCV 实现中，物体检测模型的输出结构如下：
- en: '**An array of indices**: For each object detected, this array gives the index
    of the class of the detected object. The shape of this array is *BxNx1*, where
    *B* is the batch size and *N* is the number of objects detected per image (depending
    on the model).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一组索引**：对于每个检测到的物体，这个数组给出了该物体所属类别的索引。该数组的形状是 *BxNx1*，其中 *B* 是批量大小，*N* 是每张图片检测到的物体数量（取决于模型）。'
- en: '**An array of probabilities**: For each object detected, this array gives the
    probability associated with the detected object of being the detected class in
    the **array of indices**. The shape of this array is BxNx1, where B is the batch
    size and N is the number of objects detected per image (depending on the model).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一组概率**：对于每个检测到的物体，这个数组给出了该物体属于 **索引数组** 中检测到类别的概率。该数组的形状是 BxNx1，其中 B 是批量大小，N
    是每张图片检测到的物体数量（取决于模型）。'
- en: '**An array of bounding boxes**: For each object detected, this array gives
    the bounding box coordinates associated with the detected object. The shape of
    this array is *BxNx4*, where *B* is the batch size, N is the number of objects
    detected per image (depending on the model), and 4 is the coordinates in the format
    *[x-min, y-min,* *x-max, y-max]*.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一组边界框**：对于每个检测到的物体，这个数组给出了与该物体关联的边界框坐标。该数组的形状是 *BxNx4*，其中 *B* 是批量大小，*N* 是每张图片检测到的物体数量（取决于模型），4
    是坐标，格式为 *[x-min, y-min, x-max, y-max]*。'
- en: Looking at *Figure 5**.19*, two separated groups can be seen – the Faster R-CNN
    family, which has the highest accuracy but is slow, and the YOLO family, which
    is very fast but has a lower accuracy. For our experiments, we have selected two
    of the most popular models, each of them corresponding to a different Faster R-CNN
    (the backbone of ResNet-101 with the FPN version) and *YOLOv3* (the backbone of
    Darknet53, a 53 CNN).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 看着*图 5.19*，可以看到两个分离的组——Faster R-CNN系列，它具有最高的准确性但速度较慢，和YOLO系列，它非常快速但准确性较低。对于我们的实验，我们选择了两个最受欢迎的模型，每个模型对应一个不同的Faster
    R-CNN（ResNet-101骨干网络与FPN版本）和*YOLOv3*（Darknet53骨干网络，一个53层的CNN）。
- en: Furthermore, MS COCO contains the `person` class, and therefore, models pre-trained
    in MS COCO are well suited for the *Penn-Fudan* *Pedestrian* dataset.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MS COCO数据集包含了`person`类，因此，预训练于MS COCO的数据集非常适合用于*Penn-Fudan* *行人*数据集。
- en: Loading a Faster R-CNN pre-trained model from Model Zoo
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从模型库加载一个预训练的Faster R-CNN模型
- en: Faster R-CNN is a two-stage object detection architecture, meaning that, first,
    it provides regions where objects could be located, and second, by analyzing those
    regions, it provides the classes and locations of the detected objects. It was
    developed by Ren et al. (Microsoft Research) in 2014.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN是一个两阶段的物体检测架构，这意味着，首先，它提供了可能存在物体的区域，接着，通过分析这些区域，它提供了检测到的物体的类别和位置。它由Ren等人（微软研究院）于2014年开发。
- en: It is the third iteration of a series of architectures that have evolved from
    each other – R-CNN, Fast R-CNN, and Faster R-CNN.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个系列架构的第三次迭代，这些架构彼此演变——R-CNN、Fast R-CNN和Faster R-CNN。
- en: 'From the research papers (There''s more section), we can see an R-CNN architecture:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 从研究论文（更多部分）中，我们可以看到一个R-CNN架构：
- en: '![Figure 5.20 – The high-level architecture of R-CNN](img/B16591_05_20.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20 – R-CNN的高级架构](img/B16591_05_20.jpg)'
- en: Figure 5.20 – The high-level architecture of R-CNN
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 – R-CNN的高级架构
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'Source: The car image has the following source: *azerbaijan_stockers* on Freepik:
    [https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：这张车的图片来源于*azerbaijan_stockers*，来自Freepik：[https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)
- en: 'Secondly, we can see the Fast R-CNN architecture:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以看到Fast R-CNN架构：
- en: '![Figure 5.21 – The high-level architecture of Fast R-CNN](img/B16591_05_21.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.21 – Fast R-CNN的高级架构](img/B16591_05_21.jpg)'
- en: Figure 5.21 – The high-level architecture of Fast R-CNN
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 – Fast R-CNN的高级架构
- en: Note
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'Source: The car image has the following source: *azerbaijan_stockers* on Freepik:
    [https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：这张车的图片来源于*azerbaijan_stockers*，来自Freepik：[https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)
- en: 'Finally, we can see the Faster R-CNN architecture:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到Faster R-CNN架构：
- en: '![Figure 5.22 – The high-level architecture of Faster R-CNN](img/B16591_05_22.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.22 – Faster R-CNN的高级架构](img/B16591_05_22.jpg)'
- en: Figure 5.22 – The high-level architecture of Faster R-CNN
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22 – Faster R-CNN的高级架构
- en: Note
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'Source: The car image has the following source: *azerbaijan_stockers* on Freepik:
    [https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：这张汽车图片的来源为 *azerbaijan_stockers*，来自 Freepik：[https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2](https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&position=48&from_view=search&track=sph&uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2)
- en: 'The different architectures have similarities and differences, namely the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的架构有相似之处也有不同之处，具体如下：
- en: '**R-CNN**: This uses the selective search algorithm to provide 2,000 region
    proposals. Each of these regions is then fed into a CNN, which generates a feature
    vector for each object of 4,096 features and the four-coordinate’s bounding box.
    These feature vectors are the input to the **Support Vector Machine** (**SVM**)
    classifier.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R-CNN**：该方法使用选择性搜索算法提供 2,000 个区域提议。每个区域都被输入到 CNN 中，CNN 为每个物体生成一个 4,096 特征的特征向量以及四个坐标的边界框。这些特征向量作为
    **支持向量机** (**SVM**) 分类器的输入。'
- en: '**Fast R-CNN**: In this iteration, instead of feeding each region to a CNN,
    the whole image is fed once, and a single feature map of the full image is computed,
    accelerating the process. Then, **Regions Of Interest** (**ROIs**) are computed
    over this feature map (instead of the image), obtained similarly with the *selective
    search* algorithm. These are then passed through an *ROI pooling* layer, where
    each object in a proposed region is assigned a feature map of the same shape.
    This is an efficient method in which the output can now be fed into the two networks
    for the regressor and classifier, which provide the location and class of each
    object respectively. These two networks are based on fully connected layers. The
    regressor computes offsets from the ROIs, and for the classifier, the output activation
    function is *softmax*.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fast R-CNN**：在这一版本中，不再将每个区域输入到 CNN 中，而是将整个图像一次性输入，计算整张图像的单一特征图，从而加速了处理过程。然后，**感兴趣区域**
    (**ROIs**) 在该特征图（而非图像）上计算，计算方法与 *选择性搜索* 算法类似。接着，这些 ROIs 会通过 *ROI 池化* 层，每个在提议区域中的物体会被分配一个相同形状的特征图。这是一种高效的方法，其中输出可以被传递到回归器和分类器两个网络中，分别提供每个物体的位置和类别。这两个网络基于全连接层，回归器计算
    ROIs 的偏移量，分类器的输出激活函数为 *softmax*。'
- en: '**Faster R-CNN**: In this last iteration, three changes are introduced. Firstly,
    a backbone CNN is also used to compute the feature map of the image; however,
    instead of using the selective search algorithm to propose regions, some *fully
    convolutional* layers are added on top of the backbone CNN called a **Region Proposal
    Network** (**RPN**), yielding a much smaller computation time. Secondly, these
    region proposals are computed as offsets associated with anchor boxes. Lastly,
    to reduce the number of regions to process, **Non-Maximum Suppression** (**NMS**)
    is used. These three changes provide faster inference and higher accuracy.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Faster R-CNN**：在这一最后的迭代中，提出了三项改进。首先，使用骨干 CNN 来计算图像的特征图；但是，不再使用选择性搜索算法来提议区域，而是在骨干
    CNN 上方添加一些叫做 **区域提议网络** (**RPN**) 的 *全卷积* 层，从而大大缩短计算时间。其次，这些区域提议被计算为与锚框相关的偏移量。最后，为了减少需要处理的区域数量，使用
    **非极大值抑制** (**NMS**) 方法。这三项改进提供了更快的推理速度和更高的准确率。'
- en: 'For our experiments, we will use as the backbone the `v1d` version of weights
    from a ResNet-101 network, with a **feature pyramid network** as the RPN. We can
    load the model with a single line of code:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将使用来自 ResNet-101 网络的 `v1d` 版本权重作为骨干网络，并使用 **特征金字塔网络** 作为 RPN。我们可以通过一行代码加载模型：
- en: '[PRE11]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The model then downloads successfully.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 模型随后成功下载。
- en: The GluonCV implementation of this model is capable of detecting 80,000 distinct
    objects.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: GluonCV 实现的这个模型能够检测 80,000 个不同的物体。
- en: Evaluating a Faster R-CNN pre-trained model from Model Zoo
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估来自模型库的预训练 Faster R-CNN 模型
- en: Using the *Penn-Fudan Pedestrian* dataset, we can now perform qualitative and
    quantitative evaluation of the loaded model from the previous section.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *Penn-Fudan 行人* 数据集，我们现在可以对前一部分加载的模型进行定性和定量评估。
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们可以从数据集中选择一张图像，并将模型的输出与数据集中的真实标签输出进行比较：
- en: '![Figure 5.23 – Comparing predictions and ground-truth for Faster R-CNN](img/B16591_05_23.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.23 – 比较Faster R-CNN的预测和真实标签](img/B16591_05_23.jpg)'
- en: Figure 5.23 – Comparing predictions and ground-truth for Faster R-CNN
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23 – 比较Faster R-CNN的预测和真实标签
- en: From *Figure 5**.23*, we can see a very strong correlation between the predicted
    segmentation masks and the expected results from the ground-truth, as well as
    strong confidence (+99%) and perfect class accuracy from the model. This figure
    was computed using the GluonCV visualization `utils` package (the `plot_bbox`
    function).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 5.23*中，我们可以看到预测的分割掩码与真实标签之间有非常强的相关性，同时模型的置信度很高（+99%）且类别准确率完美。此图是使用GluonCV可视化`utils`包（`plot_bbox`函数）计算得出的。
- en: 'Quantitatively, we can perform a mAP evaluation and the runtime spent computing
    this metric:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量角度来看，我们可以进行mAP评估，并计算计算此指标所花费的运行时间：
- en: '[PRE12]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The computed mAP for this model for *MS COCO* (see the object detection model
    zoo) is 40.7; therefore, given the value of 0.67 for our model, we can conclude
    our model performs the task accurately. However, it did take some time to complete
    (~250 seconds), which is expected for Faster R-CNN architectures.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在*MS COCO*（参见目标检测模型库）上的mAP计算值为40.7；因此，考虑到我们模型的值为0.67，我们可以得出结论，模型执行任务准确无误。然而，计算完成确实花费了一些时间（大约250秒），这对于Faster
    R-CNN架构来说是预期的。
- en: Loading a YOLOv3 pre-trained model from Model Zoo
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从Model Zoo加载一个YOLOv3预训练模型
- en: '**You Only Look Once Version 3** (**YOLOv3**) is a single-stage object detection
    architecture, meaning it uses an end-to-end approach that makes predictions of
    bounding boxes and class probabilities in a single step. It was developed by Redmon
    et al. (at the University of Washington) from 2016 (YOLO) to 2018 (YOLOv3).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**You Only Look Once Version 3**（**YOLOv3**）是一个单阶段目标检测架构，这意味着它使用端到端的方法，在一个步骤中进行边界框和类别概率的预测。它由Redmon等人（华盛顿大学）开发，从2016年的YOLO到2018年的YOLOv3。'
- en: It is the third iteration of a series of architectures that have evolved from
    each other – YOLO, YOLOv2, and YOLOv3.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个系列架构的第三次迭代，它们是从彼此演化而来的——YOLO、YOLOv2和YOLOv3。
- en: 'From the research papers, we can see the YOLO architecture:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 从研究论文中，我们可以看到YOLO架构：
- en: '![Figure 5.24 – The architecture of YOLOv1](img/B16591_05_24.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.24 – YOLOv1架构](img/B16591_05_24.jpg)'
- en: Figure 5.24 – The architecture of YOLOv1
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 – YOLOv1架构
- en: 'Secondly, we can see the YOLOv2 architecture:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以看到YOLOv2架构：
- en: '![Figure 5.25 – The architecture of YOLOv2](img/B16591_05_25.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.25 – YOLOv2架构](img/B16591_05_25.jpg)'
- en: Figure 5.25 – The architecture of YOLOv2
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25 – YOLOv2架构
- en: 'And finally, we can see the YOLOv3 architecture:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到YOLOv3架构：
- en: '![Figure 5.26 – The architecture of YOLOv3](img/B16591_05_26.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.26 – YOLOv3架构](img/B16591_05_26.jpg)'
- en: Figure 5.26 – The architecture of YOLOv3
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26 – YOLOv3架构
- en: 'The different architectures have similarities and differences, namely the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 不同架构之间有相似之处和不同之处，具体如下：
- en: '**YOLO**: The initial model decomposes each image into grid cells of equal
    size. Each cell is responsible for detecting an object if the location of the
    center of the object is within the cell. Each cell can predict two bounding boxes,
    their class, and their confidence score, but only one object (with a different
    size and location). All the predictions are made simultaneously, using a CNN composed
    of 24 convolutional layers and 2 fully connected layers. There is a large number
    of overlapping bounding boxes, and NMS is used to reach the final output.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLO**：初始模型将每张图像分解为相同大小的网格单元。每个单元负责检测对象，如果对象中心的位置位于该单元内。每个单元可以预测两个边界框、它们的类别和置信度得分，但每个单元只能包含一个对象（具有不同的大小和位置）。所有的预测是同时进行的，使用一个由24个卷积层和2个全连接层组成的CNN。存在大量重叠的边界框，使用NMS（非极大值抑制）来得到最终输出。'
- en: '**YOLOv2**: YOLO architecture struggles to detect small objects in groups.
    To solve this issue, on this iteration, a number of changes are introduced – batch
    normalization to improve training accuracy, anchor boxes for regression, increased
    detection capabilities to five bounding boxes per cell, and a new backbone network,
    DarkNet-19, with 19 convolutional layers and 5 max pooling layers, with 11 more
    layers for detection.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLOv2**：YOLO架构在检测小物体群体时存在困难。为了解决这个问题，在此迭代中引入了多项改动——批量归一化以提高训练准确率、回归的锚框、增加到每个单元格五个边界框的检测能力，以及一个新的骨干网络DarkNet-19，具有19个卷积层和5个最大池化层，另外还有11个用于检测的层。'
- en: '**YOLOv3**: In this last iteration, three changes are introduced. Firstly,
    to increase further the detection accuracy of small objects, the backbone network
    is updated to DarkNet-53, with 53 convolutional layers and 53 layers for the detection
    head, allowing for predictions on three different scales. Secondly, the number
    of bounding boxes per cell is reduced from five to three; however, taking into
    account the three different scales, this provides nine anchor boxes.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLOv3**：在最后一次迭代中，引入了三项改动。首先，为了进一步提高小物体的检测准确率，骨干网络更新为DarkNet-53，具有53个卷积层和53个用于检测头的层，允许在三个不同的尺度上进行预测。其次，每个单元格的边界框数量从五个减少到三个；然而，考虑到三个不同的尺度，这提供了九个锚框。'
- en: 'For our experiments, we will use DarkNet-53 as a backbone. We can load the
    model with a single line of code:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将使用DarkNet-53作为骨干网络。我们可以通过一行代码加载模型：
- en: '[PRE13]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The model then downloads successfully.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 模型随后成功下载。
- en: The GluonCV implementation of this model is capable of detecting 100 distinct
    objects.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的GluonCV实现能够检测100种不同的物体。
- en: Evaluating a YOLOv3 pre-trained model from Model Zoo
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估来自模型库的YOLOv3预训练模型
- en: Using the *Penn-Fudan Pedestrian* dataset, we can now perform qualitative and
    quantitative evaluation of the loaded model from the previous section.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*Penn-Fudan行人*数据集，我们现在可以对上一节加载的模型进行定性和定量评估。
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们可以选择数据集中的一张图片，将模型的输出与数据集中的真实标签输出进行比较：
- en: '![Figure 5.27 – Comparing predictions and ground-truth for YOLOv3](img/B16591_05_27.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.27 – 比较YOLOv3的预测与真实标签](img/B16591_05_27.jpg)'
- en: Figure 5.27 – Comparing predictions and ground-truth for YOLOv3
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27 – 比较YOLOv3的预测与真实标签
- en: From *Figure 5**.27*, we can see a very strong correlation between the bounding
    boxes of the expected results from the ground-truth and the actual outputs of
    the model, as well as strong confidence (+95%) and perfect class accuracy from
    the model. This figure was computed using the GluonCV visualization utils package
    (the `plot_bbox` function).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 5.27*中，我们可以看到预期结果的边界框与实际模型输出之间有非常强的相关性，以及模型的强信心度（+95%）和完美的分类准确率。该图是使用GluonCV可视化工具包（`plot_bbox`函数）计算得出的。
- en: 'Quantitatively, we can perform a mAP evaluation and the runtime spent computing
    this metric:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量角度来看，我们可以进行mAP评估，并计算此度量所花费的运行时间：
- en: '[PRE14]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The computed mAP for this model for *MS COCO* (see the object detection model
    zoo) is 36.0; therefore, given the value of `0.53` for our model, we can conclude
    that it is performing the task accurately. It took `113` seconds to complete.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在*MS COCO*上的计算mAP（见物体检测模型库）为36.0；因此，考虑到我们模型的`0.53`值，我们可以得出结论：它正在准确执行任务。完成此任务花费了`113`秒。
- en: How it works...
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we tackled the object detection problem. We analyzed the differences
    between image classification problem and object detection problem for evaluation,
    network architectures, and datasets.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解决了物体检测问题。我们分析了图像分类问题与物体检测问题在评估、网络架构和数据集方面的差异。
- en: In our examples, we used a publicly available dataset, the *Penn-Fudan Pedestrians*
    dataset, and we used pre-trained models on MS COCO. This choice was not casual;
    *MS COCO* has a `person` class, and therefore, the expectation was that these
    pre-trained models would perform well, as they had already seen images from the
    dataset class. However, as mentioned in the previous recipe, when this is not
    possible, and we apply pre-trained models from a dataset to another dataset, the
    data probability distribution will typically be very different; hence, the accuracy
    obtained can be very low. This is known as the domain gap or domain adaptation
    problem between the source dataset (the images that a model has been pre-trained
    on) and the target dataset (the images that the model is evaluated on).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用了一个公开的*Penn-Fudan Pedestrians*数据集，并在MS COCO上使用了预训练模型。这个选择并非随意；*MS
    COCO*有一个`person`类别，因此，预期这些预训练模型能表现良好，因为它们已经看过该数据集类别的图像。然而，如前面所提到的，当无法实现这一点时，我们将一个数据集的预训练模型应用于另一个数据集时，数据的概率分布通常会有很大的不同；因此，得到的准确率可能会非常低。这被称为源数据集（模型预训练时使用的图像）和目标数据集（模型评估时使用的图像）之间的领域间隙或领域适应问题。
- en: One way to tackle these issues for supervised learning problems is fine-tuning.
    This approach is explored in detail in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148)*.*
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 解决监督学习问题的一个方法是微调。这一方法在[*第7章*](B16591_07.xhtml#_idTextAnchor148)中有详细探讨。
- en: We ended the recipe by evaluating our two pre-trained models, Faster R-CNN and
    YOLOv3, and we were able to confirm that our Faster R-CNN model was very accurate
    but slow, while YOLOv3 was much faster (2x) with a slightly lower accuracy (~20%
    decrease).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过评估两个预训练模型，Faster R-CNN和YOLOv3，来结束这部分内容，并确认我们的Faster R-CNN模型非常精确但较慢，而YOLOv3则更快（速度提高了2倍），但精度稍低（大约下降了20%）。
- en: There’s more...
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: '*Figure 5**.19* showed a static image corresponding to the mAP versus samples
    per second graph for the Model Zoo for object detection (on *MS COCO*). There
    is also a dynamic version at this link that is worth taking a look at: [https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html).
    On this page, results from different models available in *GluonCV Model Zoo* are
    included; I suggest that you reproduce these result,s as it is an interesting
    exercise.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5**.19*展示了与目标检测模型库（在*MS COCO*上的表现）对应的mAP与每秒样本数的静态图像。还有一个动态版本，值得一看：[https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)。在这个页面上，展示了*GluonCV模型库*中不同模型的结果；我建议你复现这些结果，这将是一个有趣的练习。'
- en: To understand more about the *MS COCO* dataset, the original paper is available
    at https://arxiv.org/pdf/1405.0312.pdf.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于*MS COCO*数据集的信息，原始论文可以通过此链接查看：https://arxiv.org/pdf/1405.0312.pdf。
- en: 'Furthermore, it is very interesting to read the original research papers and
    see how object detectors have evolved from an academic point of view:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，阅读原始研究论文并了解目标检测器是如何从学术角度演变的，非常有趣：
- en: '**R-CNN**: [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R-CNN**: [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)'
- en: '**Fast** **R-CNN**: [https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fast** **R-CNN**: [https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf)'
- en: '**Faster** **R-CNN**: [https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Faster** **R-CNN**: [https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)'
- en: '**YOLO**: [https://arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLO**: [https://arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf)'
- en: '**YOLOv2**: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLOv2**: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf)'
- en: '**YOLOv3**: [https://arxiv.org/pdf/1804.02767.pdf](https://arxiv.org/pdf/1804.02767.pdf)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YOLOv3**: [https://arxiv.org/pdf/1804.02767.pdf](https://arxiv.org/pdf/1804.02767.pdf)'
- en: In this recipe, we have seen how Faster R-CNN was more accurate but slower,
    and YOLOv3 was faster but less accurate. To balance the trade-off between accuracy
    and inference time for object detection problems, there are different possibilities.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们看到Faster R-CNN更精确但较慢，而YOLOv3较快但准确度较低。为了平衡物体检测问题中的准确度与推理时间之间的权衡，有不同的选择。
- en: 'One option is to estimate the difficulty of an image and apply a different
    object detector. If it is a challenging image, use the Faster R-CNN family; if
    it is simpler, use YOLOv3\. This approach is explored in detail in this paper:
    [https://arxiv.org/pdf/1803.08707.pdf](https://arxiv.org/pdf/1803.08707.pdf);
    however, *fine-tuning* a fast model such as YOLOv3 is recommended as an initial
    approach to this issue.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是估计图像的难度并应用不同的物体检测器。如果是挑战性的图像，使用Faster R-CNN系列；如果比较简单，使用YOLOv3。这种方法在这篇论文中有详细探讨：[https://arxiv.org/pdf/1803.08707.pdf](https://arxiv.org/pdf/1803.08707.pdf)；然而，*微调*像YOLOv3这样的快速模型作为初步方案是推荐的做法。
- en: Segmenting objects in images with MXNet – PSPNet and DeepLab-v3
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MXNet对图像中的物体进行分割 —— PSPNet和DeepLab-v3
- en: In this recipe, we will see how to use MXNet and GluonCV on a pre-trained model,
    segmenting objects in images from a dataset. This means that we will be able to
    split objects into different classes, such as `person`, `cat`, and `dog`. When
    framing the problem as segmentation, the expected output is an image of the same
    size as the input image, with each pixel value being the classified label (we
    will analyze how this works in the following sections). We will see how to use
    GluonCV Model Zoo with two very important models for **semantic segmentation**
    – **PSPNet** and **DeepLab-v3**.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将学习如何使用MXNet和GluonCV在一个预训练模型上对来自数据集的图像中的物体进行分割。这意味着我们将能够将物体分割成不同的类别，如`人`、`猫`和`狗`。当将问题表述为分割时，预期的输出是与输入图像大小相同的图像，每个像素值都是分类标签（我们将在接下来的部分分析这一过程）。我们将看到如何使用GluonCV模型库中的两个非常重要的模型进行**语义分割**
    —— **PSPNet**和**DeepLab-v3**。
- en: In this recipe, we will compare the performance of these two pre-trained models
    to segment objects semantically on the dataset introduced in the previous chapter,
    *Penn-Fudan Pedestrians*, as its ground-truth also includes segmentation masks.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将比较这两个预训练模型在上一章介绍的*Penn-Fudan行人*数据集上进行语义物体分割的表现，因为其地面真实值也包括分割掩码。
- en: Getting ready
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As with previous chapters, in this recipe, we will use a few matrix operations
    and linear algebra, but it will not be too difficult.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如同前几章一样，在本食谱中，我们将使用一些矩阵运算和线性代数，但不会太难。
- en: 'As we will unpack in this recipe, semantic segmentation is similar to classification
    and object detection problems, and therefore, chapters and recipes where we explored
    the foundations of these topics are recommended to revisit. Furthermore, we will
    be working on image datasets. This recipe will combine what we learned in the
    following chapters:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这个食谱中将要展开的，语义分割与分类和物体检测问题相似，因此，建议回顾我们探讨这些主题基础的章节和食谱。此外，我们将处理图像数据集。本食谱将结合我们在以下章节中学到的内容：
- en: '*Understanding image datasets: load, manage, and visualize the Fashion MNIST
    dataset*, the third recipe from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解图像数据集：加载、管理和可视化Fashion MNIST数据集*，来自[*第2章*](B16591_02.xhtml#_idTextAnchor029)的第三个食谱，*使用MXNet并可视化数据集：Gluon*和DataLoader'
- en: '[*Chapter 4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification
    Problems*'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B16591_04.xhtml#_idTextAnchor075)，*解决分类问题*'
- en: How to do it...
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will take the following steps:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将采取以下步骤：
- en: Introduce semantic segmentation.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍语义分割。
- en: Evaluate segmentation models.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估分割模型。
- en: Compare network architectures for semantic segmentation.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较语义分割的网络架构。
- en: Explore the *Penn-Fudan Pedestrians* dataset with segmentation ground-truth.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索带有分割地面真实值的*Penn-Fudan行人*数据集。
- en: Introduce Semantic segmentation Model Zoo.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍语义分割模型库。
- en: Load a PSPNet pre-trained model from Model Zoo.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型库加载预训练的PSPNet模型。
- en: Evaluate a PSPNet pre-trained model from Model Zoo.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估来自模型库的预训练PSPNet模型。
- en: Load a DeepLab-v3 pre-trained model from Model Zoo.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型库加载一个预训练的DeepLab-v3模型。
- en: Evaluate a DeepLab-v3 pre-trained model from Model Zoo.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估来自模型库的预训练DeepLab-v3模型。
- en: Introducing semantic segmentation
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍语义分割
- en: 'In some of the previous chapters and recipes, we analyzed image classification
    problems where the task of our models was to take an image and define the class
    most likely associated with it. In semantic segmentation, however, there can be
    multiple objects per image, corresponding to different classes, and in different
    locations of the image. In object detection, the generated output to solve this
    problem was two lists, one providing the most likely class of each detected object
    and another indicating the estimated location of the object. For semantic segmentation,
    the output for each image is a set of binary images, one per class expected to
    be detected (dataset classes), where each pixel can have a value of 1 (active)
    if that pixel has been classified with that label, or 0 (inactive) otherwise.
    Each of these images is a **binary** **segmentation mask**:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一些章节和示例中，我们分析了图像分类问题，在这些问题中，我们的模型任务是接收图像并定义最可能与之相关的类别。然而，在语义分割中，每张图像可能包含多个物体，分别对应不同的类别，并位于图像的不同位置。在目标检测中，解决这个问题的输出是两个列表，一个提供每个检测到的物体最可能的类别，另一个指示物体的估计位置。而在语义分割中，每张图像的输出是一组二值图像，每个类别预计都会被检测到（数据集中的类别），其中每个像素如果被分类为该标签，则像素值为
    1（活动状态），否则为 0（非活动状态）。每一张这样的图像都是一个**二值** **分割掩码**：
- en: '![](img/B16591_05_28(a).jpg)![](img/B16591_05_28(b).jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16591_05_28(a).jpg)![](img/B16591_05_28(b).jpg)'
- en: Figure 5.28 – Binary segmentation masks
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.28 – 二值分割掩码
- en: Note
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The person''s image has been taken as an example here from the following source:
    *azerbaijan_stockers* on Freepik: [https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm](https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm)#query=person%20in%20the%20street&position=13'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 此人图像来源于以下出处：*azerbaijan_stockers* 在 Freepik 上：[https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm](https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm)#query=person%20in%20the%20street&position=13
- en: '&from_view=search&track=ais&uuid=c3458125-63b6-4899-96e5-df07c307fb46The *masks*
    shown in *Figure 5**.28* can be seen as one-hot embeddings of the classes, and
    they can be combined by associating each class with a different number (its class
    index, for example) to form a new image:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&from_view=search&track=ais&uuid=c3458125-63b6-4899-96e5-df07c307fb46*图 5.28*
    显示的*masks*可以看作是类别的 one-hot 嵌入，可以通过将每个类别与一个不同的数字（例如其类别索引）关联来组合，形成一张新图像：'
- en: '![Figure 5.29 – Semantic segmentation](img/B16591_05_29.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.29 – 语义分割](img/B16591_05_29.jpg)'
- en: Figure 5.29 – Semantic segmentation
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29 – 语义分割
- en: The output of a semantic segmentation model is, therefore, the different binary
    segmentation masks (see an example in *Figure 5**.29*), and the number depends
    on the number of classes that the model has been trained on. Therefore, for each
    image input to the model with the shape *[H, W]* (*H* being the height and *W*
    being the *width*), the output array will have a shape of *[N, H, W]* (with *N*
    being the number of classes).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，语义分割模型的输出是不同的二值分割掩码（参见*图 5.29*的示例），数量取决于模型已训练的类别数。因此，对于输入到模型的每个图像，其形状为 *[H,
    W]*（*H* 为高度，*W* 为宽度），输出数组的形状将为 *[N, H, W]*（*N* 为类别数）。
- en: Evaluating segmentation models
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估分割模型
- en: An intuitive approach to evaluate models for a semantic segmentation task is
    to report the percentage of pixels that have been correctly classified. This metric
    is commonly used and is known as *pixel accuracy*.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 评估语义分割任务模型的一种直观方法是报告被正确分类的像素的百分比。这个指标通常被使用，称为 *像素准确率*。
- en: 'Pixel Accuracy =  #TP + #TN  __________________  #TP + #TN + #FP + #FN'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '像素准确率 =  #TP + #TN  __________________  #TP + #TN + #FP + #FN'
- en: However, pixel accuracy has a problem; when objects to be segmented are small
    in comparison to the image, this metric emphasizes the large number of pixels
    that have been correctly classified as not being the object (the inactive detection).
    For example, in a 1,000x1,000 image, we have a 100x100 object, and our model classifies
    the image as the background for all the pixels, with a pixel accuracy of 99%.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像素准确率存在一个问题；当待分割的物体相对于图像较小时，这个指标会强调那些被正确分类为非物体的像素数量（即非活动检测）。例如，在一个 1,000x1,000
    的图像中，假设我们有一个 100x100 的物体，且我们的模型将图像中的所有像素都分类为背景，那么像素准确率将为 99%。
- en: To fix these issues, we can use another metric to evaluate semantic segmentation
    models, **mean Intersection over** **Union** (**mIoU**).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们可以使用另一个指标来评估语义分割模型，**平均交并比**（**mIoU**）。
- en: '![Figure 5.30 – IoU for segmentation masks](img/B16591_05_30.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![图5.30 – 分割掩膜的IoU](img/B16591_05_30.jpg)'
- en: Figure 5.30 – IoU for segmentation masks
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.30 – 分割掩膜的IoU
- en: The computation of this metric is similar to the IoU we saw in the previous
    recipe for object detection, in *Figure 5**.16*. However, for object detection,
    the analysis was based on bounding boxes, whereas for semantic segmentation, as
    shown in *Figure 5**.30*, it evaluates the number of pixels common (the intersection)
    between the target and the predicted masks, divided by the total number of pixels
    present across both masks (the union), and then its arithmetic mean is computed
    for all classes in the dataset.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量的计算方式与我们在前一个实验中为对象检测看到的IoU类似，在*图5.16*中可以看到。然而，对于对象检测，分析是基于边界框的，而对于语义分割，如*图5.30*所示，它评估的是目标和预测掩膜之间的公共像素数（交集），除以两个掩膜中所有像素的总数（并集），然后计算所有数据集类别的算术平均值。
- en: Comparing network architectures for semantic segmentation
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较语义分割的网络架构
- en: 'Semantic segmentation models’ output is the different segmentation masks, which
    are the same size as the image input. To reach this objective, several architectures
    have been proposed, with the main difference with CNNs being that there are no
    fully connected layers. Appropriately, this network architecture is named **Fully
    Connected Networks** (**FCNs**). Several models evolved from this initial architecture
    and were state-of-the-art when they were first proposed:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割模型的输出是不同的分割掩膜，大小与图像输入相同。为了实现这个目标，提出了几种架构，主要区别在于没有全连接层。恰当地，这种网络架构被称为**全连接网络**（**FCNs**）。几个模型从这一初始架构演化而来，并在首次提出时达到了当时的最先进水平：
- en: '**Encoder–decoder**: The computation of feature maps by the convolutional and
    max pooling layers of the CNNs can be seen as an encoder, as image information
    is encoded in a multidimensional entity (feature maps). In this architecture,
    after the CNN feature maps, a series of upsampling layers (the decoder) are cascaded
    until images of the same size are computed. **U-Net** is an example of this architecture.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器–解码器**：CNN的卷积和最大池化层计算特征图可以看作是编码器，因为图像信息被编码为多维实体（特征图）。在这种架构中，经过CNN特征图后，一系列上采样层（解码器）被级联，直到计算出与原始图像大小相同的图像。**U-Net**就是这种架构的一个例子。'
- en: '**Spatial pyramid pooling**: One problem with FCNs is that the encoder does
    not provide enough global scene cues to the downstream layers, resulting in objects
    being misclassified due to a lack of global context (for example, boats labeled
    as cars in water-based images, where boats are expected and not cars). In this
    type of architecture, the feature map is aggregated to the output, along with
    feature maps at different grid scales computed by different modules. **PSPNet**
    is an example of this architecture.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间金字塔池化**：FCN的一个问题是编码器未能向下游层提供足够的全局场景线索，导致对象因缺乏全局上下文而被误分类（例如，在水面图像中将船只标记为汽车，而预期应为船只而非汽车）。在这种架构中，特征图被聚合到输出中，同时不同模块计算的不同网格尺度的特征图也被结合在一起。**PSPNet**就是这种架构的一个例子。'
- en: '**Context modules**: Another option to capture multi-scale information is to
    add extra modules cascaded on top of the original network. **DeepLab-v3** can
    be seen as a combination of this type of network and spatial pyramid pooling networks.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文模块**：捕捉多尺度信息的另一种选择是将额外的模块级联到原始网络的顶部。**DeepLab-v3**可以看作是这种类型的网络和空间金字塔池化网络的结合。'
- en: '![Figure 5.31 – Network architecture for semantic segmentation](img/B16591_05_31.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![图5.31 – 语义分割的网络架构](img/B16591_05_31.jpg)'
- en: Figure 5.31 – Network architecture for semantic segmentation
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31 – 语义分割的网络架构
- en: For our experiments in this recipe, we will use pre-trained versions of PSPNet
    and DeepLab-v3.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们将使用预训练版本的PSPNet和DeepLab-v3。
- en: Exploring the Penn-Fudan Pedestrians dataset with segmentation ground-truth
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用分割地面真值探索Penn-Fudan行人数据集
- en: This dataset is the one that we worked with in the previous recipe. However,
    the ground-truth we will use in this recipe is not the bounding boxes required
    for object detection but, instead, the masks required for semantic segmentation.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是我们在前一个实验中使用的那个数据集。然而，在本实验中，我们将使用的地面真值不是对象检测所需的边界框，而是语义分割所需的掩膜。
- en: '![Figure 5.32 – The Penn-Fudan Pedestrians dataset with mask ground-truth](img/B16591_05_32.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![图5.32 – Penn-Fudan行人数据集与掩膜地面真值](img/B16591_05_32.jpg)'
- en: Figure 5.32 – The Penn-Fudan Pedestrians dataset with mask ground-truth
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.32 – Penn-Fudan行人数据集及掩码真实值
- en: As we can see in *Figure 5**.32*, each image can have one or more pedestrians,
    with their corresponding masks. This figure was computed using the GluonCV visualization
    utils package (the `plot_mask` function).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图5.32*所见，每个图像可能包含一个或多个行人及其对应的掩码。该图像是通过GluonCV可视化工具包（`plot_mask`函数）计算得到的。
- en: In this dataset, there are no different classes to be classified, and no further
    visualizations are computed.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，没有需要分类的不同类别，也没有进一步的可视化结果。
- en: Introducing Semantic Segmentation Model Zoo
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍语义分割模型库
- en: 'GluonCV also provides pre-trained models for semantic segmentation in its Model
    Zoo. For the *MS COCO* dataset, this is the accuracy (mIoU) versus performance
    (samples per second) chart:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: GluonCV还提供了用于语义分割的预训练模型，在其模型库中。对于*MS COCO*数据集，以下是精度（mIoU）与性能（每秒样本数）的对比图：
- en: '![Figure 5.33 – Model Zoo for semantic segmentation (MS COCO)](img/B16591_05_33.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![图5.33 – 用于语义分割的模型库（MS COCO）](img/B16591_05_33.jpg)'
- en: Figure 5.33 – Model Zoo for semantic segmentation (MS COCO)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.33 – 用于语义分割的模型库（MS COCO）
- en: Note
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Source: [https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://cv.gluon.ai/model_zoo/detection.html](https://cv.gluon.ai/model_zoo/detection.html)
- en: '*Figure 5**.33* displays the most important pre-trained models in GluonCV Model
    Zoo, comparing accuracy (mIoU on the vertical axis) and inference performance
    (samples per second on the horizontal axis). There are no models (yet) in the
    top-right quadrant, meaning that, currently, we need to balance between both characteristics.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.33*展示了GluonCV模型库中最重要的预训练模型，比较了精度（纵轴上的mIoU）和推理性能（横轴上的每秒样本数）。目前，右上方象限没有模型，意味着目前我们需要在这两个特性之间找到平衡。'
- en: Using models from GluonCV Model Zoo can be done with just a couple of lines
    of code, and we will explore this path to solve our *Penn-Fudan Pedestrians* dataset
    for the segmentation task in the following steps.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GluonCV模型库中的模型只需几行代码，我们将探索这一路径来解决*Penn-Fudan Pedestrians*数据集的分割任务，具体步骤如下。
- en: Loading PSPNet pre-trained model from Model Zoo
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从模型库加载PSPNet预训练模型
- en: '**Pyramid Scene Parsing Network** (**PSPNet**) is a spatial pyramid pooling
    semantic segmentation architecture, which means that a pyramid pooling module
    is added to provide global cues. It was developed by Zhao et al. (the Chinese
    University of Hong Kong) in 2016\. It achieved first place in the 2016 **ILSVRC
    Scene** **Parsing Challenge**.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**金字塔场景解析网络**（**PSPNet**）是一种空间金字塔池化语义分割架构，这意味着添加了一个金字塔池化模块来提供全局线索。该模型由赵等人（香港中文大学）于2016年开发，并在2016年**ILSVRC场景解析挑战赛**中获得第一名。'
- en: Apart from the global pooling module, it differentiates from FCNs by using dilated
    convolutions.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 除了全局池化模块，它与FCNs的不同之处在于使用了扩张卷积。
- en: '![Figure 5.34 – The PSPNet architecture](img/B16591_05_34.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![图5.34 – PSPNet架构](img/B16591_05_34.jpg)'
- en: Figure 5.34 – The PSPNet architecture
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.34 – PSPNet架构
- en: Note
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Source: [https://github.com/hszhao/PSPNet/issues/101](https://github.com/hszhao/PSPNet/issues/101)'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://github.com/hszhao/PSPNet/issues/101](https://github.com/hszhao/PSPNet/issues/101)
- en: 'In *Figure 5**.34*, we can see the overall architecture of PSPNet:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.34*中，我们可以看到PSPNet的整体架构：
- en: '**Feature Map**: A ResNet-based CNN architecture, with dilated convolutions,
    is used to compute a feature map with 1/8 the size of the original image.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征图**：基于ResNet的CNN架构，使用扩张卷积来计算一个尺寸为原始图像1/8的特征图。'
- en: '**Pyramid Pooling Module**: Using a four-level pyramid (whole, half, quarter,
    and eighth), global context is computed. It is concatenated with the original
    feature map.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金字塔池化模块**：通过四级金字塔（整图、半图、四分之一图和八分之一图），计算全局上下文信息。它与原始特征图拼接在一起。'
- en: '**Final Prediction**: A final computation using a convolutional layer is done
    to generate the final predictions.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终预测**：通过卷积层进行最终计算，以生成最终的预测结果。'
- en: 'For our experiments, we will use a ResNet-101 network as a backbone. We can
    load the model with a single line of code:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将使用ResNet-101网络作为骨干网络。我们可以通过一行代码加载模型：
- en: '[PRE15]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The model then downloads successfully.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 模型成功下载。
- en: Evaluating a PSPNet pre-trained model from Model Zoo
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估从模型库（Model Zoo）中预训练的PSPNet模型
- en: By using the *Penn-Fudan Pedestrian* dataset for a segmentation task, we can
    now perform qualitative and quantitative evaluation of the loaded model from the
    previous section.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用*Penn-Fudan Pedestrian*数据集进行分割任务，我们现在可以对上一节加载的模型进行定性和定量评估。
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 从质量上讲，我们可以从数据集中选择一张图片，并将模型的输出与数据集中的真实结果进行比较：
- en: '![Figure 5.35 – Comparing predicted masks and ground-truth for PSPNet](img/B16591_05_35.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![图5.35 – 比较PSPNet的预测掩码和真实掩码](img/B16591_05_35.jpg)'
- en: Figure 5.35 – Comparing predicted masks and ground-truth for PSPNet
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.35 – 比较PSPNet的预测掩码和真实掩码
- en: From *Figure 5**.35*, we can see a very strong correlation between the predicted
    segmentation masks and the expected results from the ground-truth. This figure
    was computed using the GluonCV visualization utils package (the `plot_mask` function).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图5.35*中，我们可以看到预测的分割掩码与真实掩码之间有很强的相关性。该图是使用GluonCV可视化工具包（`plot_mask`函数）计算得出的。
- en: 'Quantitatively, we can perform an mIoU evaluation and the runtime spent computing
    this metric:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 从数量上讲，我们可以进行mIoU评估以及计算此指标所花费的运行时间：
- en: '[PRE16]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The computed mIoU for this model for *MS COCO* (see *Semantic Segmentation Model
    Zoo*) is `0.70`; therefore, given the value of `0.56` for our model, we can conclude
    that our model performs the task accurately. However, it did take some time to
    complete it (~340 seconds).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该模型在*MS COCO*上的计算mIoU（参见*语义分割模型库*）为`0.70`；因此，考虑到我们的模型mIoU为`0.56`，我们可以得出结论，模型执行任务的准确性较高。然而，完成该任务确实花了一些时间（大约340秒）。
- en: Loading a DeepLab-v3 pre-trained model from Model Zoo
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从Model Zoo加载DeepLab-v3预训练模型
- en: '**DeepLab-v3** is a semantic segmentation architecture that combines both *encoder-decoder*
    and **spatial pyramid pooling** architectures. It was developed by Chen et al.
    (Google) from 2015 (**DeepLab-v1**) to 2017 (**DeepLab-v3**).'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepLab-v3**是一种语义分割架构，它结合了*编码器-解码器*和**空间金字塔池化**架构。该架构由Chen等人（Google）于2015年（**DeepLab-v1**）至2017年（**DeepLab-v3**）开发。'
- en: It is the third iteration of a series of architectures that have evolved from
    each other – DeepLab-v1, DeepLab-v2, and DeepLab-v3.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从一系列相互演化的架构中来的第三个版本——DeepLab-v1、DeepLab-v2和DeepLab-v3。
- en: 'From the research papers, also included in the There’s more section we can
    see the DeepLab-v1 architecture:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 从研究论文中，也可以在“更多内容”部分看到DeepLab-v1架构：
- en: '![](img/B16591_05_36.jpg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16591_05_36.jpg)'
- en: Figure 5.36 – The architecture of DeepLab-v1
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.36 – DeepLab-v1架构
- en: 'Secondly, we can see the DeepLab- v2 architecture:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以看到DeepLab-v2架构：
- en: '![](img/B16591_05_37.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16591_05_37.jpg)'
- en: Figure 5.37 – The architecture of DeepLab-v2
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.37 – DeepLab-v2架构
- en: 'And finally, we can see the DeepLab-v3+ architecture:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到DeepLab-v3+架构：
- en: '![](img/B16591_05_38.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16591_05_38.jpg)'
- en: Figure 5.38 – The architecture of DeepLab-v3+
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.38 – DeepLab-v3+架构
- en: 'The different architectures have similarities and differences, namely the following:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的架构有相似之处也有不同之处，具体如下：
- en: '**DeepLab-v1**: This architecture evolves from the original FCN and uses a
    VGG-16 backbone network as well. The most important innovation is the usage of
    atrous or diluted convolutions instead of standard convolutions. We discussed
    this convolution parameter when we first introduced *convolutional layers* in
    [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression Problems*,
    showing how it increased the receptive field. This architecture also uses fully
    connected **Conditional Random Fields** (**CRFs**) for post-processing to polish
    the final segmentation masks, although it is very slow and cannot be trained end
    to end.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v1**：该架构来源于原始的FCN，并使用了VGG-16作为主干网络。最重要的创新是使用了空洞卷积或稀疏卷积，而不是标准卷积。我们在最初介绍*卷积层*时讨论过这个卷积参数，在[*第3章*](B16591_03.xhtml#_idTextAnchor052)《解决回归问题》中展示了它如何增加感受野。该架构还使用了全连接**条件随机场**（**CRFs**）进行后处理，以优化最终的分割掩码，尽管它非常慢且无法端到端训练。'
- en: '**DeepLab-v2**: The most important innovation for this model is a new module
    called **Atrous Spatial Pyramid Pooling** (**ASPP**). With this module, on one
    hand, the network can encode multi-scale features into a fixed-size feature map
    (which is flexible to different input sizes), and on the ther hand, by using atrous
    convolutions, it increases the receptive field, optimizing the computation cost.
    In the original implementation, 4 to 6 scales were used. Furthermore, it uses
    ResNet as the backbone network.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v2**：该模型最重要的创新是一个名为**Atrous Spatial Pyramid Pooling**（**ASPP**）的新模块。有了这个模块，一方面，网络可以将多尺度特征编码到一个固定大小的特征图中（适应不同的输入大小），另一方面，通过使用空洞卷积，增加了感受野，优化了计算成本。在原始实现中，使用了4到6个尺度。此外，它使用ResNet作为主干网络。'
- en: '**DeepLab-v3**: In this last iteration, several changes are introduced. Firstly,
    the network is modified to use batch normalization and dropout. Secondly, the
    ASPP module is modified to add a new scale in a separate channel for global image
    pooling, to use fine-grained details. Lastly, the multi-scale part of the training
    is removed and is only applied to inference. A by-product of these small improvements
    is that the CRF step is no longer needed, providing much faster results.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v3**：在这一最后的迭代中，引入了若干变化。首先，网络修改为使用批归一化和丢弃法。其次，ASPP模块进行了修改，增加了一个新的尺度，并在单独的通道中进行全局图像池化，以便使用细粒度的细节。最后，训练中的多尺度部分被移除，仅应用于推理。通过这些小改进的副产品是，CRF步骤不再需要，从而提供了更快的结果。'
- en: 'For our experiments, we will use a ResNet-152 network as a backbone. We can
    load the model with a single line of code:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将使用ResNet-152网络作为骨干网络。我们可以通过一行代码加载模型：
- en: '[PRE17]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The model then downloads successfully.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 模型成功下载。
- en: Evaluating a DeepLab-v3 pre-trained model from Model Zoo
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估来自Model Zoo的DeepLab-v3预训练模型
- en: Using the *Penn-Fudan Pedestrian* dataset for the segmentation task, we can
    now perform qualitative and quantitative evaluation of the loaded model from the
    previous section.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*Penn-Fudan Pedestrian*数据集进行分割任务后，我们现在可以对上一节加载的模型进行定性和定量评估。
- en: 'Qualitatively, we can choose an image from the dataset and compare the output
    of the model with the ground-truth output from the dataset:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在定性方面，我们可以从数据集中选择一张图像，并将模型的输出与数据集的地面真实输出进行比较：
- en: '![](img/B16591_05_39.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16591_05_39.jpg)'
- en: Figure 5.39 – Comparing predicted masks and ground-truth for DeepLab-v3
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.39 – 比较DeepLab-v3预测的掩码与地面真实值
- en: From *Figure 5**.39*, we can see a very strong correlation between the predicted
    segmentation masks and the expected results from the ground-truth. This figure
    was computed using the GluonCV visualization utils package (the `plot_mask` function).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 5.39*中，我们可以看到预测的分割掩码与地面真实值的预期结果之间有非常强的相关性。这个图是使用GluonCV可视化工具包（`plot_mask`函数）计算得到的。
- en: 'Quantitatively, we can perform mIoU evaluation and the runtime spent computing
    this metric:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量上讲，我们可以执行mIoU评估，并计算出用于计算此指标所花费的运行时间：
- en: '[PRE18]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The computed mIoU for this model for *MS COCO* (see *Semantic Segmentation Model
    Zoo*) is `0.715`; therefore, given the value of `0.56` for our model, we can conclude
    our model performs the task accurately, with a similar value to PSPNet. However,
    it is much faster (~70 secs).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该模型在*MS COCO*上的计算得到的mIoU（见*语义分割模型库*）是`0.715`；因此，考虑到我们模型的`0.56`值，我们可以得出结论，我们的模型执行任务时准确度良好，且与PSPNet的值相似。然而，它的速度要快得多（约70秒）。
- en: How it works...
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we tackled the semantic segmentation problem. We analyzed the
    differences between image classification and object detection for evaluation,
    network architectures, and datasets.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们解决了语义分割问题。我们分析了图像分类和目标检测在评估、网络架构和数据集上的差异。
- en: In our examples, we used a publicly available dataset, the *Penn-Fudan Pedestrians*
    dataset, and we used pre-trained models on *MS COCO*. This choice was not casual;
    *MS COCO* has a `person` class, and therefore, the expectation was that these
    pre-trained models would perform well, as they had already seen images from the
    dataset class.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用了一个公开可用的数据集——*Penn-Fudan Pedestrians*数据集，并使用了在*MS COCO*上预训练的模型。这个选择并非随意；*MS
    COCO*有一个`person`类别，因此，预期这些预训练模型会表现得很好，因为它们已经见过该数据集类别的图像。
- en: However, as mentioned in the previous recipe, when this is not possible, and
    we apply pre-trained models from a dataset to another dataset, the data probability
    distribution will typically be very different; hence, the accuracy obtained can
    be very low. This is known as the **domain gap** or **domain adaptation problem**
    between the source dataset (the images that the model has been pre-trained on)
    and the target dataset (the images that the model is evaluated on).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如前面食谱中提到的，当这不可行时，且我们将从一个数据集应用预训练模型到另一个数据集时，数据的概率分布通常会非常不同；因此，获得的准确度可能会很低。这就是**领域差距**或**领域适应问题**，它发生在源数据集（模型已预训练的图像）和目标数据集（模型评估的图像）之间。
- en: One way to tackle these issues for supervised learning problems is fine-tuning.
    This approach is explored in detail in a later chapter.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些问题，一种解决方案是微调。这种方法将在后面的章节中详细探讨。
- en: We ended the recipe by evaluating our two pre-trained models, PSPNet and DeepLab-v3,
    and we were able to compare qualitatively and quantitatively their results for
    accuracy and computation speed, verifying how both models yield a similar pixel
    accuracy (`0.46)` and mIoU (`0.56`), although DeepLab-v3 was faster (`~4.5x`).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过评估两个预训练模型PSPNet和DeepLab-v3来结束了本篇文章，并能够从定性和定量上比较它们在精度和计算速度方面的结果，验证了这两个模型的像素准确度（`0.46`）和mIoU（`0.56`）相似，尽管DeepLab-v3的速度更快（`~4.5x`）。
- en: There’s more...
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '*Figure 5**.33* showed a static image corresponding to the mIoU versus samples
    per second graph for the Model Zoo for Semantic Segmentation (on MS COCO); there
    is a dynamic version at this link that is worth taking a look at: https://cv.gluon.ai/model_zoo/segmentation.html.
    On this page, results from different models available in GluonCV Model Zoo are
    included; I suggest that you reproduce these results, as it is an interesting
    exercise.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.33* 显示了对应于模型库中语义分割（在MS COCO上的样本每秒数与mIoU图）的静态图像；该链接有一个动态版本，值得一看：[https://cv.gluon.ai/model_zoo/segmentation.html](https://cv.gluon.ai/model_zoo/segmentation.html)。在这个页面上，包含了来自GluonCV模型库的不同模型的结果；我建议你重现这些结果，因为这是一个有趣的练习。'
- en: 'During our discussion of the evolution of network architectures, most notably
    DeepLab, we mentioned how dilated/atrous convolutions help with multi-scale context
    aggregation. This research paper explores this topic in depth: [https://arxiv.org/pdf/1511.07122.pdf](https://arxiv.org/pdf/1511.07122.pdf).'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论网络架构演变时，尤其是DeepLab，我们提到了膨胀卷积（dilated/atrous convolutions）如何帮助多尺度上下文聚合。该研究论文深入探讨了这一主题：[https://arxiv.org/pdf/1511.07122.pdf](https://arxiv.org/pdf/1511.07122.pdf)。
- en: 'Furthermore, it is very interesting to read the original research papers and
    see how the segmentation task has evolved from an academic point of view:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，阅读原始研究论文非常有趣，可以看到从学术角度来看，语义分割任务是如何发展的：
- en: '**FCN**: [https://arxiv.org/pdf/1605.06211v1.pdf](https://arxiv.org/pdf/1605.06211v1.pdf)'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FCN**：[https://arxiv.org/pdf/1605.06211v1.pdf](https://arxiv.org/pdf/1605.06211v1.pdf)'
- en: '**U-Net**: [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U-Net**：[https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)'
- en: '**PSPNet**: [https://arxiv.org/pdf/1612.01105.pdf](https://arxiv.org/pdf/1612.01105.pdf)'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PSPNet**：[https://arxiv.org/pdf/1612.01105.pdf](https://arxiv.org/pdf/1612.01105.pdf)'
- en: '**DeepLab-v1**: [https://arxiv.org/pdf/1412.7062.pdf](https://arxiv.org/pdf/1412.7062.pdf)'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v1**：[https://arxiv.org/pdf/1412.7062.pdf](https://arxiv.org/pdf/1412.7062.pdf)'
- en: '**DeepLab-v2**: [https://arxiv.org/pdf/1606.00915v2.pdf](https://arxiv.org/pdf/1606.00915v2.pdf)'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v2**：[https://arxiv.org/pdf/1606.00915v2.pdf](https://arxiv.org/pdf/1606.00915v2.pdf)'
- en: '**DeepLab-v3**: [https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf)'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v3**：[https://arxiv.org/pdf/1706.05587.pdf](https://arxiv.org/pdf/1706.05587.pdf)'
- en: 'Semantic segmentation is an active area of research and, as such, is constantly
    evolving, with new networks appearing and redefining the state of the art, such
    as the following:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割是一个活跃的研究领域，因此它不断发展，新的网络不断出现并重新定义了技术的前沿，例如以下几种：
- en: '**DeepLab-v3+**: The next step from DeepLab-v3, developed by Chen et al. (Google,
    2018): [https://arxiv.org/pdf/1802.02611.pdf](https://arxiv.org/pdf/1802.02611.pdf)'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab-v3+**：DeepLab-v3的下一步，由Chen等人（谷歌，2018年）开发：[https://arxiv.org/pdf/1802.02611.pdf](https://arxiv.org/pdf/1802.02611.pdf)'
- en: '**Swin-V2G**: Using Transformers, developed by Liu et al. (Microsoft, 2021):
    [https://arxiv.org/pdf/2111.09883v1.pdf](https://arxiv.org/pdf/2111.09883v1.pdf)'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swin-V2G**：使用Transformers，由Liu等人（微软，2021年）开发：[https://arxiv.org/pdf/2111.09883v1.pdf](https://arxiv.org/pdf/2111.09883v1.pdf)'
