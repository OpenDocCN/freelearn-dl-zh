- en: Audio Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频分析
- en: 'In the previous chapters, we learned about dealing with sequential text data.
    Audio can also be considered sequential data, with varying amplitudes over time.
    In this chapter, we will be learning about the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何处理顺序文本数据。音频也可以看作是顺序数据，其振幅随时间变化。在这一章中，我们将学习以下内容：
- en: Classifying a song by genre
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按类型分类歌曲
- en: Music generation using deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习生成音乐
- en: Transcribing audio into text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频转录为文本
- en: Classifying a song by genre
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按类型分类歌曲
- en: In this case study, we will be classifying a song into one of 10 possible genres.
    Imagine a scenario where we are tasked to automatically classify the genre of
    a song without manually listening to it. This way, we can potentially minimize
    operational overload as far as possible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将把一首歌分类为10种可能的类型之一。想象一下一个场景：我们被要求自动分类一首歌的类型，而不需要手动听它。这样，我们就能尽可能减少操作负担。
- en: Getting ready
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The strategy we''ll adopt is as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略如下：
- en: Download a dataset of various audio recordings and the genre they fit into.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一个包含各种音频录音及其对应类型的数据集。
- en: Visualize and contrast a spectrogram of the audio signal for various genres.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化并对比不同类型音频信号的频谱图。
- en: 'Perform CNN operations on top of a spectrogram:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在频谱图上执行CNN操作：
- en: Note that we will be performing a CNN 1D operation on a spectrogram, as the
    concept of translation does not apply in the case of audio recordings
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，我们将在频谱图上执行CNN 1D操作，因为音频录音的情况下，翻译概念并不适用。
- en: Extract features from the CNN after multiple convolution and pooling operations.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从CNN中提取特征，经过多次卷积和池化操作后。
- en: Flatten the output and pass it through a dense layer that has 10 possible classes
    in an output layer.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展平输出并通过一个具有10个可能类别的输出层的密集层。
- en: Minimize categorical cross-entropy to classify the audio recording to one of
    the 10 possible classes.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化类别交叉熵，以将音频录音分类为10个可能类别之一。
- en: Once we classify the audio, we'll plot the embeddings of each audio input so
    that similar audio recordings are grouped together. This way, we will be in a
    position to identify the genre of a new song without listening to it, thus automatically
    classifying the audio input into a genre.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对音频进行分类，我们将绘制每个音频输入的嵌入图，以便将相似的音频录音归为一类。这样，我们就能在不听歌的情况下识别新歌的类型，从而自动将音频输入分类到一个类型中。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Genre_classification.ipynb` in GitHub):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略的代码如下（代码文件在GitHub中名为`Genre_classification.ipynb`）：
- en: 'Download the dataset and import the relevant packages:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集并导入相关包：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Loop through the audio files to extract the  `mel spectrogram` input features
    of the input audio, and store the output genre for the audio input:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环遍历音频文件，提取输入音频的`mel频谱图`特征，并存储音频输入的输出类型：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we are loading the audio file and extracting its features.
    Additionally, we are extracting the `melspectrogram` features of the signal. Finally,
    we are storing the `mel` features as the input array and the genre as the output
    array.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们加载音频文件并提取其特征。此外，我们还提取了信号的`mel频谱图`特征。最后，我们将`mel`特征存储为输入数组，将类型存储为输出数组。
- en: 'Visualize the spectrogram:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化频谱图：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output of the preceding code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/1d90d784-81f5-45d9-bbae-80908b53091b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d90d784-81f5-45d9-bbae-80908b53091b.png)'
- en: You can see that there is a distinct difference between the classical audio spectrogram and
    the rock audio spectrogram.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到古典音频频谱图和摇滚音频频谱图之间有明显的区别。
- en: 'Define the input and output arrays:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入和输出数组：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Convert the output classes into one-hot encoded versions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将输出类别转换为独热编码版本：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create train-and-test datasets:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Build and compile the method:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译方法：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that the `Conv1D` method in the preceding code works in a manner very
    similar to that of `Conv2D`; however, it is a one-dimensional filter in `Conv1D` and
    a two-dimensional one in `Conv2D`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述代码中的`Conv1D`方法与`Conv2D`非常相似；然而，`Conv1D`是一个一维滤波器，而`Conv2D`是一个二维滤波器：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Fit the model:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From the preceding code, we can see that the model classifies with an accuracy
    of ~60% on the test dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述代码中我们可以看到，模型在测试数据集上的分类准确率约为60%。
- en: 'Extract the output from the pre-final layer of the model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型的倒数第二层提取输出：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code produces output at the pre-final layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码在倒数第二层产生输出。
- en: 'Reduce the dimensions of the embeddings to 2, using `t-SNE` so that we can
    now plot our work on a chart:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`t-SNE`将嵌入的维度减少到2，这样我们就可以在图表上绘制我们的工作：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Plot the `t-SNE` output:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`t-SNE`输出：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is the chart for the preceding code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的图表：
- en: '![](img/f0e76348-4592-4fb2-b376-87ec1d393faf.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0e76348-4592-4fb2-b376-87ec1d393faf.png)'
- en: From the preceding diagram, we can see that audio recordings for similar genres
    are located together. This way, we are now in a position to classify a new song
    into one of the possible genres automatically, without manual inspection. However,
    if the probability of an audio belonging to a certain genre is not very high,
    it will potentially go to a manual review so that misclassifications are uncommon.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到相似类型的音频记录聚集在一起。这样，我们现在可以自动地将一首新歌分类到可能的某个类型中，而无需人工检查。然而，如果音频属于某个类型的概率不高，它可能会被送去人工复审，以确保错误分类的可能性较小。
- en: Generating music using deep learning
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习生成音乐
- en: In the previous chapter, we learned about generating text by going through a
    novel. In this section, we will learn about generating audio from a sequence of
    audio notes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了通过阅读小说来生成文本。在本节中，我们将学习如何通过一系列音频音符生成音频。
- en: Getting ready
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: A MIDI file typically contains information about the notes and chords of the
    audio file, whereas the note object contains information about the pitch, octave,
    and offset of the notes. The chord object contains a set of notes that are played
    at the same time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个MIDI文件通常包含音频文件中音符和和弦的信息，而音符对象包含音符的音高、八度和偏移量的信息。和弦对象包含同时演奏的一组音符。
- en: 'The strategy that we''ll adopt to build a music generator is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的音乐生成策略如下：
- en: Extract the notes present in audio file
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取音频文件中的音符
- en: Assign a unique ID for each note.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个音符分配一个唯一的ID。
- en: Take a sequence of 100 historical notes, and the 101^(st) note shall be the
    output.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取100个历史音符的序列，第101个音符将是输出。
- en: Fit an LSTM model.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练LSTM模型。
- en: How to do it...
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Music_generation.ipynb` in GitHub) along with the recommended audio file:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论的策略编码如下（代码文件可以在GitHub上的`Music_generation.ipynb`中找到），并附带推荐的音频文件：
- en: 'Import the relevant packages and dataset:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Read the content of the file:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取文件内容：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding code reads a stream of scores.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码读取了一个分数流。
- en: 'Define a function that reads the stream of scores and extracts the notes from
    it (along with silence, if present in the audio file):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，读取分数流并提取其中的音符（如果音频文件中有静音，也会提取）：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we are obtaining the notes by looping through the elements
    and, depending on whether the element is a note, a chord, or a rest (which indicates
    silence), we extract the corresponding notes, append them, and return the appended
    list.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过遍历元素来获取音符，取决于元素是音符、和弦还是休止符（表示静音），我们提取相应的音符，附加它们，并返回附加后的列表。
- en: 'Extract the notes from the input audio file''s stream:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入音频文件的流中提取音符：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'A sample note output is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例音符输出如下：
- en: '![](img/b3f25f4f-d8de-4fc1-bcae-dee7545cfe57.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3f25f4f-d8de-4fc1-bcae-dee7545cfe57.png)'
- en: Note that the values starting with a `#` indicate silence (the duration is the
    same as the number adjacent to `#`).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，值以`#`开头表示静音（持续时间与紧邻`#`的数字相同）。
- en: 'Create the input and output dataset by creating a dictionary of the note''s
    ID and its corresponding name:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过创建音符ID及其对应名称的字典，来创建输入和输出数据集：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create a sequence of input and output arrays:'
  id: totrans-78
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数组的序列：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding step, we are taking a sequence of 100 notes as input and extracting
    the output at the 101st time step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们将100个音符的序列作为输入，并提取第101个时间步的输出。
- en: 'Additionally, we are also converting the note into its corresponding ID:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将音符转换为其对应的ID：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we are reshaping the input data so that it can then be
    fed into an LSTM layer (which requires the `batch_size` shape, time steps, and
    the number of features per time step).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在重新调整输入数据的形状，以便将其馈送到LSTM层（该层需要`batch_size`形状、时间步数和每个时间步的特征数）。
- en: Additionally, we are normalizing the input, and we are also converting the output
    into a one-hot encoded set of vectors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们正在对输入进行归一化，并将输出转换为一组独热编码的向量。
- en: 'Fit the model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following is the output of the preceding code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/3ee599ca-17c2-42a4-962e-f5fb91653bdd.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ee599ca-17c2-42a4-962e-f5fb91653bdd.png)'
- en: 'Generate predictions:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that, in the preceding code, we have chosen a random audio location, from
    where we'll sample a sequence that will be used as a seed for prediction in future
    time steps.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的代码中，我们选择了一个随机的音频位置，从那里我们将采样一个序列，用作未来时间步预测的种子。
- en: 'Generate predictions by taking a sequence of 100 notes at a time, generating
    the next prediction, appending it to the input sequence, and generating the next
    prediction (by taking the latest sequence of the last 100 notes):'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过一次处理100个音符的序列，生成下一个预测，将其附加到输入序列中，再生成下一个预测（通过获取最后100个音符的最新序列）：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that we are dividing the index (which is the predicted output of the model)
    by 49, as we did in the same exercise while building the model (divided by `np.max(network_input)`).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将索引（即模型的预测输出）除以49，就像在构建模型时一样（除以`np.max(network_input)`）。
- en: The preceding exercise is slightly different than the text generation exercise,
    where we performed embedding on top of input word IDs, as we are not performing
    embedding in this scenario. The model is still working without embedding in this
    scenario, potentially because there are fewer unique values in the input.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的练习与文本生成练习略有不同，后者是基于输入词ID进行嵌入操作，而在这种情况下，我们没有进行嵌入。模型仍然在没有嵌入的情况下运行，可能是因为输入中唯一的值较少。
- en: 'Create note values based on values generated by the model:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据模型生成的值创建音符值：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that, in the preceding code, we are offsetting each note by 0.5 seconds,
    so that the notes do not stack on top of one another while producing the output.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的代码中，我们将每个音符的时间偏移了0.5秒，这样在生成输出时音符不会重叠。
- en: 'Write the generated predictions into a music stream:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的预测写入音乐流：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, you should be able to listen to the music that's been generated by your
    model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该能够听到你的模型生成的音乐。
- en: Transcribing audio into text
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将音频转录为文本
- en: In [Chapter 14](1f989cf7-40b3-4ecd-9457-0dd648746922.xhtml), *End-to-End Learning*,
    we learned about transcribing handwritten text images into text. In this section,
    we will be leveraging a similar end-to-end model to transcribe voices into text.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第14章](1f989cf7-40b3-4ecd-9457-0dd648746922.xhtml)，*端到端学习*中，我们学习了如何将手写文本图像转录为文本。在这一部分，我们将利用类似的端到端模型将语音转录为文本。
- en: Getting ready
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: 'The strategy that we''ll adopt to transcribe voices is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的语音转录策略如下：
- en: Download a dataset that contains the audio file and its corresponding transcriptions
    (*ground truths*)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载一个包含音频文件及其对应转录（*实际结果*）的数据集。
- en: 'Specify a sampling rate while reading the audio files:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在读取音频文件时指定采样率：
- en: If the sampling rate is 16,000, we'll be extracting 16,000 data points per second
    of audio.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果采样率为16,000，我们将从每秒音频中提取16,000个数据点。
- en: 'Extract a Fast Fourier Transformation of the audio array:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取音频数组的快速傅里叶变换：
- en: An FFT ensures that we have only the most important features of a signal.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFT确保我们仅保留信号中最重要的特征。
- en: By default, the FFT gives us *n/2* number of data points, where *n* is the number
    of data points in the whole audio recording.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，FFT给我们提供*n/2*个数据点，其中*n*是整个音频录音中的数据点数量。
- en: Sample the FFT features of the audio where we extract 320 data points at a time;
    that is, we extract 20 milliseconds (320/16000 = 1/50 seconds) of audio data at
    a time
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对音频进行FFT特征采样，每次提取320个数据点；也就是说，我们每次提取20毫秒（320/16000 = 1/50秒）的音频数据。
- en: Additionally, we will sample 20 milliseconds of data at 10-millisecond intervals.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们将在10毫秒间隔处每次采样20毫秒的数据。
- en: For this exercise, we'll be working on an audio recording where the audio duration
    is, at most, 10 seconds
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将处理一个音频录音，其最大时长为10秒。
- en: 'We will store the 20 milliseconds of audio data into an array:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将把20毫秒的音频数据存储到一个数组中：
- en: We have already seen that we sample 20 milliseconds of data for every 10 milliseconds.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经看到，每10毫秒采样20毫秒的数据。
- en: Thus, for a one-second audio clip, we will have 100 x 320 data points, and for
    a 10- second audio clip, we'll have 1,000 x 320 = 320,000 data points.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，对于一个一秒钟的音频片段，我们将有100 x 320个数据点，而对于一个10秒钟的音频片段，我们将有1,000 x 320 = 320,000个数据点。
- en: We will initialize an empty array of 160,000 data points and overwrite the values
    with the FFT values—as we have already learned that the FFT values are one half
    of the original number of data points
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将初始化一个160,000个数据点的空数组，并用FFT值覆盖这些值——因为我们已经知道FFT值是原始数据点数量的一半
- en: For each array of 1,000 x 320 data points, we'll store the corresponding transcriptions
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个1,000 x 320的数据点数组，我们将存储对应的转录文本
- en: We'll assign an index for each character and then convert the output into a
    list of indices
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将为每个字符分配一个索引，然后将输出转换为索引列表
- en: Additionally, we'll also be storing the input length (which is the predefined
    number of time steps) and the label lengths (which are the actual number of characters
    present in the output)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们还将存储输入长度（即预定义的时间步数）和标签长度（即输出中实际的字符数量）
- en: Furthermore, we will define the CTC loss function that is based on the actual
    output, the predicted output, the number of time steps (input length), and the
    label length (the number of characters in the output)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们将定义基于实际输出、预测输出、时间步数（输入长度）和标签长度（输出字符数）的CTC损失函数
- en: We'll define the model that is a combination of `conv1D` (as this is audio data)
    and GRU
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将定义一个模型，结合使用`conv1D`（因为这是音频数据）和GRU
- en: Furthermore, we will ensure that we normalize data using batch normalization
    so that the gradients do not vanish
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们将确保通过批量归一化来规范化数据，以防止梯度消失
- en: We'll run the model on batches of data, where we randomly sample batches of
    data and feed them to the model that tries to minimize the CTC loss
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在数据批次上运行模型，在此过程中我们随机采样数据批次并将其输入模型，模型试图最小化CTC损失
- en: Finally, we will decode the model predictions on a new data point by using the
    `ctc_decode` method
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将通过使用`ctc_decode`方法解码模型在新数据点上的预测
- en: How to do it...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Voice transcription.ipynb` in GitHub):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略的代码实现如下（代码文件可在GitHub的`Voice transcription.ipynb`中找到）：
- en: 'Download the dataset and import the relevant packages:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集并导入相关的包：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Read all the file names and their corresponding transcriptions and turn them
    into separate lists:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取所有文件名及其对应的转录文本，并将它们转换为单独的列表：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Store the length of the transcription into a list so that we can understand
    the maximum transcription length:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转录文本的长度存储到一个列表中，这样我们就能理解最大转录文本的长度：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For this exercise, to be in a position to train a model on a single GPU, we''ll
    perform this exercise on the first 2,000 audio files whose transcriptions are
    fewer than 100 characters in length:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本次练习，为了能够在单个GPU上训练模型，我们将对前2,000个音频文件进行操作，这些音频文件的转录文本长度少于100个字符：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding code, we are storing the audio name and the corresponding audio
    transcription for only those audio recordings that have a transcription length
    of fewer than 100 characters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们仅为那些转录文本长度少于100个字符的音频录音存储音频名称和对应的转录文本
- en: 'Store the inputs as a 2D array and the corresponding outputs of only those
    audio files that have a duration of fewer than 10 seconds:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入存储为二维数组，并仅对那些时长少于10秒的音频文件存储对应的输出：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create an index for each unique character in the data:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据中的每个唯一字符创建一个索引：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create the input and the label lengths:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和标签长度：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that we are creating an input length that is 243, as the output of the
    model (which we are going to build in a later step) has 243 time steps.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在创建一个243的输入长度，因为模型的输出（我们将在后续步骤中构建）具有243个时间步。
- en: 'Define the CTC loss function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义CTC损失函数：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the model:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the input and the output parameters of the CTC loss function:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义CTC损失函数的输入和输出参数：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Build and compile the model:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A summary of the model is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/4cdf1be1-cff4-4eb0-8b4b-1f5ce3b93a33.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cdf1be1-cff4-4eb0-8b4b-1f5ce3b93a33.png)'
- en: 'Fit the model on batches of data that are sampled from the input:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在从输入中采样的数据批次上拟合模型：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the preceding code, we are looping through and extracting batches of data
    2,500 times, normalizing the input data, and fitting the model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在循环并提取2,500次数据批次，规范化输入数据，并拟合模型
- en: Also, we are performing a high number of epochs, as the CTC loss decreases slowly
    for this particular dataset and model combination.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们进行了大量的训练周期，因为对于这个特定的数据集和模型组合，CTC损失下降得很慢。
- en: 'Predict the test audio:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测测试音频：
- en: '[PRE36]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding code, we are specifying a model (`model2`) that takes the input
    test array and extracts the model prediction in each of the 243 time steps.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们指定了一个模型（`model2`），它接受输入的测试数组并提取每个243个时间步骤中的模型预测结果。
- en: Additionally, we are extracting the prediction for the 12^(th) element from
    the last of the input array (note that we have excluded the last 25 input data
    points from being considered while training the model). Furthermore, we have also
    pre-processed it in the same way that we did before, by passing the input data
    to the model-training process.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们从输入数组的最后提取了第12^(th)个元素的预测（请注意，我们在训练模型时排除了最后25个输入数据点）。此外，我们还像之前一样对其进行了预处理，将输入数据传递给模型训练过程。
- en: 'Decode the predictions on the new data point:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码新数据点上的预测：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the preceding code, we were decoding the prediction using the `ctc_decode`
    method. Alternatively, we could have decoded the prediction in the same way as
    we extracted the prediction in the handwritten image transcription. Finally, we
    print out the predictions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`ctc_decode`方法解码了预测结果。或者，我们也可以像提取手写图像转录中的预测那样解码预测。最后，我们打印出了预测结果。
- en: 'We''ll be in a position to decode the predictions by calling the previously
    defined function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将能够通过调用之前定义的函数来解码预测结果：
- en: '[PRE38]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of one of the predictions is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个预测的输出如下：
- en: '![](img/06e5ae31-fcc7-4a45-893e-13d568a5ed0e.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06e5ae31-fcc7-4a45-893e-13d568a5ed0e.png)'
- en: 'While the preceding output looks as if it''s gibberish, it sounds phonetically
    similar to the actual audio, which is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的输出看起来像是胡言乱语，但它在语音上与实际的音频相似，具体如下：
- en: '![](img/cd63b2fd-7610-4ec3-99d0-17b98214b6d8.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd63b2fd-7610-4ec3-99d0-17b98214b6d8.png)'
- en: There's more...
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Some ways in which we can further improve the accuracy of our transcriptions
    are as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步提高转录准确性的一些方法如下：
- en: Train on more data points
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更多数据点上进行训练
- en: Incorporate a language model to perform fuzzy matching on the output so that
    we correct the predicted output
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融入语言模型对输出进行模糊匹配，以便我们能纠正预测结果。
