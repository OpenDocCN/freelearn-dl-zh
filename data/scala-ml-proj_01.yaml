- en: Analyzing Insurance Severity Claims
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析保险赔付严重度
- en: Predicting the cost, and hence the severity, of claims in an insurance company
    is a real-life problem that needs to be solved in an accurate way. In this chapter,
    we will show you how to develop a predictive model for analyzing insurance severity
    claims using some of the most widely used regression algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测保险公司理赔的成本，从而预测赔付严重度，是一个需要准确解决的现实问题。在本章中，我们将向您展示如何使用一些最广泛使用的回归算法，开发一个用于分析保险赔付严重度的预测模型。
- en: We will start with simple **linear regression** (**LR**) and we will see how
    to improve the performance using some ensemble techniques, such as **gradient
    boosted tree** (**GBT**) regressors. Then we will look at how to boost the performance
    with Random Forest regressors. Finally, we will show you how to choose the best
    model and deploy it for a production-ready environment. Also, we will provide
    some background studies on machine learning workflow, hyperparameter tuning, and
    cross-validation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简单的**线性回归**（**LR**）开始，看看如何通过一些集成技术（如**梯度提升树**（**GBT**）回归器）提高性能。接着我们将研究如何使用随机森林回归器提升性能。最后，我们将展示如何选择最佳模型并将其部署到生产环境中。此外，我们还将提供一些关于机器学习工作流程、超参数调优和交叉验证的背景知识。
- en: 'For the implementation, we will use **Spark ML** API for faster computation
    and massive scalability. In a nutshell, we will learn the following topics throughout
    this end-to-end project:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实现，我们将使用**Spark ML** API，以实现更快的计算和大规模扩展。简而言之，在整个端到端项目中，我们将学习以下主题：
- en: Machine learning and learning workflow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习与学习工作流程
- en: Hyperparameter tuning and cross-validation of ML models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数调优与交叉验证
- en: LR for analyzing insurance severity claims
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分析保险赔付严重度的线性回归（LR）
- en: Improving performance with gradient boosted regressors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度提升回归器提高性能
- en: Boosting the performance with random forest regressors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林回归器提升性能
- en: Model deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署
- en: Machine learning and learning workflow
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与学习工作流程
- en: '**Machine learning** (**ML**) is about using a set of statistical and mathematical
    algorithms to perform tasks such as concept learning, predictive modeling, clustering,
    and mining useful patterns can be performed. The ultimate goal is to improve the
    learning in such a way that it becomes automatic, so that no more human interactions
    are needed, or to reduce the level of human interaction as much as possible.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是使用一组统计学和数学算法来执行任务，如概念学习、预测建模、聚类和挖掘有用模式。最终目标是以一种自动化的方式改进学习，使得不再需要人工干预，或者尽可能减少人工干预的程度。'
- en: 'We now refer to a famous definition of ML by **Tom M. Mitchell** (*Machine
    Learning*, *Tom Mitchell*, McGraw Hill, 1997**)**, where he explained what learning
    really means from a computer science perspective:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在引用**Tom M. Mitchell**在《*机器学习*》一书中的著名定义（*Tom Mitchell, McGraw Hill, 1997**），他从计算机科学的角度解释了什么是学习：
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “计算机程序被称为从经验E中学习，针对某类任务T和性能度量P，如果它在T类任务中的表现，通过P衡量，在经验E下有所提高。”
- en: 'Based on the preceding definition, we can conclude that a computer program
    or machine can do the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的定义，我们可以得出结论：计算机程序或机器可以执行以下操作：
- en: Learn from data and histories
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据和历史中学习
- en: Be improved with experience
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过经验得到改善
- en: Interactively enhance a model that can be used to predict an outcome
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互式地增强一个可以用来预测结果的模型
- en: 'A typical ML function can be formulated as a convex optimization problem for
    finding a minimizer of a convex function *f* that depends on a variable vector
    *w* (weights), which has *d* records. Formally, we can write this as the following
    optimization problem:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习（ML）函数可以被表示为一个凸优化问题，目的是找到一个凸函数*f*的最小值，该函数依赖于一个变量向量*w*（权重），并且包含*d*条记录。形式上，我们可以将其写为以下优化问题：
- en: '![](img/87f40bc8-35a2-4bab-ae37-9de1886ef0d4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87f40bc8-35a2-4bab-ae37-9de1886ef0d4.png)'
- en: 'Here, the objective function is of the form:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，目标函数的形式为：
- en: '![](img/5050b364-5c61-4106-b65d-5594a34ca97f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5050b364-5c61-4106-b65d-5594a34ca97f.png)'
- en: Here, the vectors ![](img/1bc24f84-8db5-4cbd-a0c3-546b033d7e06.png)are the training
    data points for *1≤i≤n*, and are their corresponding labels that we want to predict
    eventually. We call the method *linear* if *L(w;x,y)* can be expressed as a function
    of *wTx* and *y*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，向量 ![](img/1bc24f84-8db5-4cbd-a0c3-546b033d7e06.png) 是 *1≤i≤n* 的训练数据点，它们是我们最终想要预测的相应标签。如果
    *L(w;x,y)* 可以表示为 *wTx* 和 *y* 的函数，我们称该方法为*线性*。
- en: 'The objective function *f* has two components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数 *f* 有两个组成部分：
- en: A regularizer that controls the complexity of the model
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制模型复杂度的正则化器
- en: The loss that measures the error of the model on the training data
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量模型在训练数据上误差的损失函数
- en: The loss function *L(w;)* is typically a convex function in *w*. The fixed regularization
    parameter *λ≥*0 defines the trade-off between the two goals of minimizing the
    loss on the training error and minimizing model complexity to avoid overfitting.
    Throughout the chapters, we will learn in details on different learning types
    and algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数 *L(w;)* 通常是 *w* 的凸函数。固定的正则化参数 *λ≥*0 定义了训练误差最小化和模型复杂度最小化之间的权衡，以避免过拟合。在各章节中，我们将详细学习不同的学习类型和算法。
- en: On the other hand, **deep neural networks** (**DNN**) form the core of **deep
    learning** (**DL**) by providing algorithms to model complex and high-level abstractions
    in data and can better exploit large-scale datasets to build complex models
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**深度神经网络**（**DNN**）是**深度学习**（**DL**）的核心，它通过提供建模复杂和高级数据抽象的算法，能够更好地利用大规模数据集来构建复杂的模型。
- en: 'There are some widely used deep learning architectures based on artificial
    neural networks: DNNs, Capsule Networks, Restricted Boltzmann Machines, deep belief
    networks, factorization machines and recurrent neural networks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些广泛使用的基于人工神经网络的深度学习架构：DNN、胶囊网络、限制玻尔兹曼机、深度信念网络、矩阵分解机和递归神经网络。
- en: These architectures have been widely used in computer vision, speech recognition,
    natural language processing, audio recognition, social network filtering, machine
    translation, bioinformatics and drug design. Throughout the chapters, we will
    see several real-life examples using these architectures to achieve state-of-the
    art predictive accuracy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构已广泛应用于计算机视觉、语音识别、自然语言处理、音频识别、社交网络过滤、机器翻译、生物信息学和药物设计等领域。在各章节中，我们将看到多个使用这些架构的实际案例，以实现最先进的预测精度。
- en: Typical machine learning workflow
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的机器学习工作流程
- en: 'A typical ML application involves several processing steps, from the input
    to the output, forming a scientific workflow as shown in *Figure 1, ML workflow*.
    The following steps are involved in a typical ML application:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的机器学习应用涉及多个处理步骤，从输入到输出，形成一个科学工作流程，如*图 1，机器学习工作流程*所示。一个典型的机器学习应用包括以下步骤：
- en: Load the data
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据
- en: Parse the data into the input format for the algorithm
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据解析成算法所需的输入格式
- en: Pre-process the data and handle the missing values
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行预处理并处理缺失值
- en: Split the data into three sets, for training, testing, and validation (train
    set and validation set respectively) and one for testing the model (test dataset)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为三个集合，分别用于训练、测试和验证（训练集和验证集），以及一个用于测试模型（测试数据集）
- en: Run the algorithm to build and train your ML model
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行算法来构建和训练你的机器学习模型
- en: Make predictions with the training data and observe the results
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据进行预测并观察结果
- en: Test and evaluate the model with the test data or alternatively validate the
    model using some cross-validator technique using the third dataset called a **validation
    dataset**
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试数据测试并评估模型，或者使用交叉验证技术通过第三个数据集（称为**验证数据集**）来验证模型。
- en: Tune the model for better performance and accuracy
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整模型以提高性能和准确性
- en: Scale up the model so that it can handle massive datasets in future
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展模型，使其能够处理未来的大规模数据集
- en: 'Deploy the ML model in production:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产环境中部署机器学习模型：
- en: '![](img/a91968fa-0d1b-4233-a91f-81b0c24a6953.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a91968fa-0d1b-4233-a91f-81b0c24a6953.png)'
- en: 'Figure 1: ML workflow'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：机器学习工作流程
- en: 'The preceding workflow is represent a few steps to solve ML problems. Where,
    ML tasks can be broadly categorized into supervised, unsupervised, semi-supervised,
    reinforcement, and recommendation systems. The following *Figure 2, Supervised
    learning in action*, shows the schematic diagram of supervised learning. After
    the algorithm has found the required patterns, those patterns can be used to make
    predictions for unlabeled test data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工作流程表示了解决机器学习问题的几个步骤，其中，机器学习任务可以大致分为监督学习、无监督学习、半监督学习、强化学习和推荐系统。下面的*图 2，监督学习的应用*显示了监督学习的示意图。当算法找到了所需的模式后，这些模式可以用于对未标记的测试数据进行预测：
- en: '![](img/893ef2af-4c84-4f42-af07-2f3eb3cabd97.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/893ef2af-4c84-4f42-af07-2f3eb3cabd97.png)'
- en: 'Figure 2: Supervised learning in action'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：监督学习的应用
- en: Examples include classification and regression for solving supervised learning
    problems so that predictive models can be built for predictive analytics based
    on them. Throughout the upcoming chapters, we will provide several examples of
    supervised learning, such as LR, logistic regression, random forest, decision
    trees, Naive Bayes, multilayer perceptron, and so on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括用于解决监督学习问题的分类和回归，从而可以基于这些问题构建预测分析的预测模型。在接下来的章节中，我们将提供多个监督学习的示例，如 LR、逻辑回归、随机森林、决策树、朴素贝叶斯、多层感知机等。
- en: 'A regression algorithm is meant to produce continuous output. The input is
    allowed to be either discrete or continuous:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 回归算法旨在产生连续的输出。输入可以是离散的也可以是连续的：
- en: '![](img/6b7d7c39-737e-4c4a-8774-faf7797f90cc.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b7d7c39-737e-4c4a-8774-faf7797f90cc.png)'
- en: 'Figure 3: A regression algorithm is meant to produce continuous output'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：回归算法旨在产生连续输出
- en: 'A classification algorithm, on the other hand, is meant to produce discrete
    output from an input of a set of discrete or continuous values. This distinction
    is important to know because discrete-valued outputs are handled better by classification,
    which will be discussed in upcoming chapters:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 而分类算法则旨在从一组离散或连续的输入值中产生离散的输出。这一区别很重要，因为离散值的输出更适合由分类处理，这将在后续章节中讨论：
- en: '![](img/99801fcd-6736-48e4-b691-e2af528dfe29.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99801fcd-6736-48e4-b691-e2af528dfe29.png)'
- en: 'Figure 4: A classification algorithm is meant to produce discrete output'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：分类算法旨在产生离散输出
- en: In this chapter, we will mainly focus on the supervised regression algorithms.
    We will start with describing the problem statement and then we move on to the
    very simple LR algorithm. Often, performance of these ML models is optimized using
    hyperparameter tuning and cross-validation techniques. So knowing them, in brief,
    is mandatory so that we can easily use them in future chapters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要关注监督回归算法。我们将从描述问题陈述开始，然后介绍非常简单的 LR 算法。通常，这些机器学习模型的性能通过超参数调整和交叉验证技术进行优化。因此，简要了解它们是必要的，这样我们才能在后续章节中轻松使用它们。
- en: Hyperparameter tuning and cross-validation
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整和交叉验证
- en: Tuning an algorithm is simply a process that one goes through in order to enable
    the algorithm to perform optimally in terms of runtime and memory usage. In Bayesian
    statistics, a hyperparameter is a parameter of a prior distribution. In terms
    of ML, the term hyperparameter refers to those parameters that cannot be directly
    learned from the regular training process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法简单来说是一个过程，通过这个过程可以使算法在运行时间和内存使用方面达到最佳表现。在贝叶斯统计学中，超参数是先验分布的一个参数。在机器学习中，超参数指的是那些无法通过常规训练过程直接学习到的参数。
- en: 'Hyperparameters are usually fixed before the actual training process begins.
    This is done by setting different values for those hyperparameters, training different
    models, and deciding which ones work best by testing them. Here are some typical
    examples of such parameters:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数通常在实际训练过程开始之前就已固定。通过为这些超参数设置不同的值，训练不同的模型，然后通过测试它们来决定哪些效果最好。以下是一些典型的此类参数示例：
- en: Number of leaves, bins, or depth of a tree
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的叶子数、箱数或深度
- en: Number of iterations
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数
- en: Number of latent factors in a matrix factorization
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解中的潜在因子数量
- en: Learning rate
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Number of hidden layers in a deep neural network
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络中的隐藏层数量
- en: The number of clusters in k-means clustering and so on
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 聚类中的簇数量等等
- en: 'In short, hyperparameter tuning is a technique for choosing the right combination
    of hyperparameters based on the performance of presented data. It is one of the
    fundamental requirements for obtaining meaningful and accurate results from ML
    algorithms in practice. The following figure shows the model tuning process, things
    to consider, and workflow:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，超参数调优是一种根据所呈现数据的表现选择合适的超参数组合的技术。它是从机器学习算法中获取有意义和准确结果的基本要求之一。下图展示了模型调优过程、需要考虑的事项以及工作流程：
- en: '![](img/91d93133-43aa-4cc8-9d2f-45e076ca4033.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91d93133-43aa-4cc8-9d2f-45e076ca4033.png)'
- en: 'Figure 5: Model tuning process'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：模型调优过程
- en: Cross-validation (also known as **rotation estimation**) is a model validation
    technique for assessing the quality of the statistical analysis and results. The
    target is to make the model generalized toward an independent test set. It will
    help if you want to estimate how a predictive model will perform accurately in
    practice when you deploy it as an ML application. During the cross-validation
    process, a model is usually trained with a dataset of a known type.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（也称为**旋转估计**）是一种用于评估统计分析和结果质量的模型验证技术。其目标是使模型对独立的测试集具有较强的泛化能力。如果你希望估计预测模型在实践中部署为机器学习应用时的表现，交叉验证会有所帮助。在交叉验证过程中，通常会使用已知类型的数据集训练模型。
- en: 'Conversely, it is tested using a dataset of an unknown type. In this regard,
    cross-validation helps to describe a dataset to test the model in the training
    phase using the validation set. There are two types of cross-validation that can
    be typed as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，它是使用一个未知类型的数据集进行测试。在这方面，交叉验证有助于通过使用验证集在训练阶段描述数据集，以测试模型。有两种类型的交叉验证，具体如下：
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷尽性交叉验证**：包括留 p 交叉验证和留一交叉验证'
- en: '**Non-exhaustive cross-validation**: This includes K-fold cross-validation
    and repeated random subsampling cross-validation'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非穷尽性交叉验证**：包括 K 折交叉验证和重复随机子抽样交叉验证'
- en: In most cases, the researcher/data scientist/data engineer uses 10-fold cross-validation
    instead of testing on a validation set (see more in *Figure 6*, *10-fold cross-validation
    technique*). This is the most widely used cross-validation technique across all
    use cases and problem types, as explained by the following figure.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，研究人员/数据科学家/数据工程师使用 10 折交叉验证，而不是在验证集上进行测试（见 *图 6*，*10 折交叉验证技术*）。正如下图所示，这种交叉验证技术是所有使用案例和问题类型中最广泛使用的。
- en: 'Basically, using this technique, your complete training data is split into
    a number of folds. This parameter can be specified. Then the whole pipeline is
    run once for every fold and one ML model is trained for each fold. Finally, the
    different ML models obtained are joined by a voting scheme for classifiers or
    by averaging for regression:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，使用该技术时，您的完整训练数据会被分割成若干个折叠。这个参数是可以指定的。然后，整个流程会针对每个折叠运行一次，并为每个折叠训练一个机器学习模型。最后，通过分类器的投票机制或回归的平均值将获得的不同机器学习模型结合起来：
- en: '![](img/5b0e92fa-300e-4609-be4d-3a658d7e9779.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b0e92fa-300e-4609-be4d-3a658d7e9779.png)'
- en: 'Figure 6: 10-fold cross-validation technique'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：10 折交叉验证技术
- en: Moreover, to reduce the variability, multiple iterations of cross-validation
    are performed using different partitions; finally, the validation results are
    averaged over the rounds.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了减少变异性，交叉验证会进行多次迭代，使用不同的数据分割；最后，验证结果会在各轮次中进行平均。
- en: Analyzing and predicting insurance severity claims
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析和预测保险索赔的严重性
- en: Predicting the cost, and hence the severity, of claims in an insurance company
    is a real-life problem that needs to be solved in a more accurate and automated
    way. We will do something similar in this example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 预测保险公司索赔的费用，从而推测其严重性，是一个需要以更精确和自动化的方式解决的现实问题。在本示例中，我们将做类似的事情。
- en: We will start with simple logistic regression and will learn how to improve
    the performance using some ensemble techniques, such as an random forest regressor.
    Then we will look at how to boost the performance with a gradient boosted regressor.
    Finally, we will show how to choose the best model and deploy it for a production-ready
    environment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简单的逻辑回归开始，并学习如何使用一些集成技术（如随机森林回归器）来提高性能。接着，我们将看看如何使用梯度提升回归器来进一步提升性能。最后，我们将展示如何选择最佳模型并将其部署到生产环境中。
- en: Motivation
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: When someone is devastated by a serious car accident, his focus is on his life,
    family, child, friends, and loved ones. However, once a file is submitted for
    the insurance claim, the overall paper-based process to calculate the severity
    claim is a tedious task to be completed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个人遭遇严重的车祸时，他的关注点在于自己的生命、家人、孩子、朋友和亲人。然而，一旦提交了保险索赔文件，计算索赔严重程度的整个纸质流程就成了一项繁琐的任务。
- en: This is why insurance companies are continually seeking fresh ideas to improve
    their claims service for their clients in an automated way. Therefore, predictive
    analytics is a viable solution to predicting the cost, and hence severity, of
    claims on the available and historical data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么保险公司不断寻求创新思路，自动化改进客户索赔服务的原因。因此，预测分析是预测索赔费用，从而预测其严重程度的可行解决方案，基于现有和历史数据。
- en: Description of the dataset
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: A dataset from the **Allstate Insurance company** will be used, which consists
    of more than 300,000 examples with masked and anonymous data and consisting of
    more than 100 categorical and numerical attributes, thus being compliant with
    confidentiality constraints, more than enough for building and evaluating a variety
    of ML techniques.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用来自**Allstate 保险公司**的数据集，该数据集包含超过30万个示例，数据是经过掩码处理和匿名化的，并包含超过100个分类和数值属性，符合保密性约束，足够用于构建和评估各种机器学习技术。
- en: The dataset is downloaded from the Kaggle website at [https://www.kaggle.com/c/allstate-claims-severity/data](https://www.kaggle.com/c/allstate-claims-severity/data).
    Each row in the dataset represents an insurance claim. Now, the task is to predict
    the value for the `loss` column. Variables prefaced with `cat` are categorical,
    while those prefaced with `cont` are continuous.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是从 Kaggle 网站下载的，网址是 [https://www.kaggle.com/c/allstate-claims-severity/data](https://www.kaggle.com/c/allstate-claims-severity/data)。数据集中的每一行代表一次保险索赔。现在，任务是预测`loss`列的值。以`cat`开头的变量是分类变量，而以`cont`开头的变量是连续变量。
- en: It is to be noted that the Allstate Corporation is the second largest insurance
    company in the United States, founded in 1931\. We are trying to make the whole
    thing automated, to predict the cost, and hence the severity, of accident and
    damage claims.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Allstate 公司是美国第二大保险公司，成立于1931年。我们正在努力使整个过程自动化，预测事故和损坏索赔的费用，从而预测其严重程度。
- en: Exploratory analysis of the dataset
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集的探索性分析
- en: 'Let''s look at some data properties (use the `EDA.scala` file for this). At
    first, we need to read the training set to see the available properties. To begin
    with, let''s place your training set in your project directory or somewhere else
    and point to it accordingly:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些数据属性（可以使用`EDA.scala`文件）。首先，我们需要读取训练集，以查看可用的属性。首先，将你的训练集放在项目目录或其他位置，并相应地指向它：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I hope you have Java, Scala and Spark installed and configured on your machine.
    If not, please do so. Anyway, I''m assuming they are. So let''s create an active
    Spark session, which is the gateway for any Spark application:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你已经在机器上安装并配置了 Java、Scala 和 Spark。如果没有，请先完成安装。无论如何，我假设它们已经安装好了。那么，让我们创建一个活跃的
    Spark 会话，这是任何 Spark 应用程序的入口：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Spark session alias on Scala REPL**:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala REPL 中的 Spark 会话别名**：'
- en: If you are inside Scala REPL, the Spark session alias `spark` is already defined,
    so just get going.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 Scala REPL 中，Spark 会话别名`spark`已经定义好了，所以可以直接开始。
- en: 'Here, I have a method called `createSession()` under the class `SparkSessionCreate`
    that goes as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我有一个名为`createSession()`的方法，它位于`SparkSessionCreate`类中，代码如下：
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since this will be used frequently throughout this book, I decided to create
    a dedicated method. So, let's load, parse, and create a DataFrame using the `read.csv`
    method but in Databricks `.csv` format (as known as `com.databricks.spark.csv`)
    since our dataset comes with `.csv` format.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在本书中会频繁使用此功能，我决定创建一个专门的方法。因此，我们使用`read.csv`方法加载、解析并创建 DataFrame，但使用 Databricks
    `.csv` 格式（也称为`com.databricks.spark.csv`），因为我们的数据集是以`.csv`格式提供的。
- en: At this point, I have to interrupt you to inform something very useful. Since
    we will be using Spark MLlib and ML APIs in upcoming chapters too. Therefore,
    it would be worth fixing some issues in prior. If you're a Windows user then let
    me tell you a very weired issue that you will be experiencing while working with
    Spark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我必须打断一下，告诉你一个非常有用的信息。由于我们将在接下来的章节中使用 Spark MLlib 和 ML API，因此，提前解决一些问题是值得的。如果你是
    Windows 用户，那么我得告诉你一个很奇怪的问题，你在使用 Spark 时可能会遇到。
- en: Well, the thing is that Spark works on **Windows**, **Mac OS**, and **Linux**.
    While using `Eclipse` or `IntelliJ IDEA` to develop your Spark applications (or
    through Spark local job sumit) on Windows, you might face an I/O exception error
    and consequently your application might not compile successfully or may be interrupted.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，事情是这样的，Spark可以在**Windows**、**Mac OS**和**Linux**上运行。当你在Windows上使用`Eclipse`或`IntelliJ
    IDEA`开发Spark应用程序（或者通过Spark本地作业提交）时，你可能会遇到I/O异常错误，导致应用程序无法成功编译或被中断。
- en: 'The reason is that Spark expects that there is a runtime environment for `Hadoop`
    on Windows. Unfortunately, the **binary** distribution of **Spark ** (**v2.2.0
    for example**) release does not contain some Windows native components (example, `winutils.exe`,
    `hadoop.dll`, and so on). However, these are required (not optional) to run `Hadoop`
    on Windows. Therefore, if you cannot ensure the runtime environment, an I/O exception
    saying the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于Spark期望在Windows上有一个`Hadoop`的运行环境。不幸的是，**Spark**的**二进制**发布版（例如**v2.2.0**）不包含一些Windows本地组件（例如，`winutils.exe`，`hadoop.dll`等）。然而，这些是运行`Hadoop`在Windows上所必需的（而不是可选的）。因此，如果你无法确保运行环境，就会出现类似以下的I/O异常：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now there are two ways to tackale this issue on Windows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有两种方法来解决这个问题，针对Windows系统：
- en: '**From IDE such as Eclipse and IntelliJ IDEA**: Download the `winutls.exe`
    from [https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin/](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin/).
    Then download and copy it inside the `bin` folder in the Spark distribution—example, `spark-2.2.0-bin-hadoop2.7/bin/`
    . Then select the project | Run Configurations... |  Environment | New | create
    a variable named `HADOOP_HOME` and put the path in the value field—example, `c:/spark-2.2.0-bin-hadoop2.7/bin/` | OK
    | Apply | Run. Then you''re done!'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**来自IDE，如Eclipse和IntelliJ IDEA**：从[https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin/](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin/)下载`winutils.exe`。然后下载并将其复制到Spark分发版中的`bin`文件夹——例如，`spark-2.2.0-bin-hadoop2.7/bin/`。然后选择项目
    | 运行配置... | 环境 | 新建 | 创建一个名为`HADOOP_HOME`的变量，并在值字段中填入路径——例如，`c:/spark-2.2.0-bin-hadoop2.7/bin/`
    | 确定 | 应用 | 运行。这样就完成了！'
- en: '**With local Spark** **job submit**: Add the `winutils.exe` file path to the
    hadoop home directory using System set properties—example, in the Spark code `System.setProperty("hadoop.home.dir",
    "c:\\\spark-2.2.0-bin-hadoop2.7\\\bin\winutils.exe")`'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用本地Spark** **作业提交**：将`winutils.exe`文件路径添加到hadoop主目录，使用System设置属性——例如，在Spark代码中`System.setProperty("hadoop.home.dir",
    "c:\\\spark-2.2.0-bin-hadoop2.7\\\bin\winutils.exe")`'
- en: 'Alright, let''s come to your original discussion. If you see the preceding
    code block then we set to read the header of the CSV file, which is directly applied
    to the column names of the DataFrame created, and the `inferSchema` property is
    set to `true`. If you don''t specify the `inferSchema` configuration explicitly,
    the float values will be treated as `strings`*.* This might cause `VectorAssembler`
    to raise an exception such as `java.lang.IllegalArgumentException: Data type StringType
    is not supported`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '好的，让我们回到你原始的讨论。如果你看到上面的代码块，我们设置了读取CSV文件的头部，它直接应用于创建的DataFrame的列名，并且`inferSchema`属性被设置为`true`。如果你没有明确指定`inferSchema`配置，浮动值将被视为`strings`*.*
    这可能导致`VectorAssembler`抛出像`java.lang.IllegalArgumentException: Data type StringType
    is not supported`的异常：'
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let''s print the schema of the DataFrame we just created. I have abridged
    the output and shown only a few columns:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印一下我们刚才创建的DataFrame的schema。我已经简化了输出，只显示了几个列：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can see that there are 116 categorical columns for categorical features.
    Also, there are 14 numerical feature columns. Now let''s see how many rows there
    are in the dataset using the `count()` method:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到有116个分类列用于分类特征。还有14个数值特征列。现在让我们使用`count()`方法看看数据集中有多少行：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding number is pretty high for training an ML model. Alright, now
    let''s see a snapshot of the dataset using the `show()` method but with only some
    selected columns so that it makes more sense. Feel free to use `df.show()` to
    see all columns:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的数字对于训练ML模型来说相当高。好的，现在让我们通过`show()`方法查看数据集的快照，但只选取一些列，以便更有意义。你可以使用`df.show()`来查看所有列：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/dc972f3b-871f-4080-8a6c-e85e23ab79e8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc972f3b-871f-4080-8a6c-e85e23ab79e8.png)'
- en: 'Nevertheless, if you look at all the rows using `df.show()`, you will see some
    categorical columns containing too many categories. To be more specific, category
    columns `cat109` to `cat116` contain too many categories, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用`df.show()`查看所有行，你会看到一些分类列包含了过多的类别。更具体地说，`cat109`到`cat116`这些分类列包含了过多的类别，具体如下：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/55f4b79f-1af8-438a-8f99-4baedbed98cf.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55f4b79f-1af8-438a-8f99-4baedbed98cf.png)'
- en: In later stages, it would be worth dropping these columns to remove the skewness
    in the dataset. It is to be noted that in statistics, skewness is a measure of
    the asymmetry of the probability distribution of a real-valued random variable
    with respect to the mean.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续阶段，值得删除这些列，以去除数据集中的偏斜性。需要注意的是，在统计学中，偏斜度是衡量一个实值随机变量的概率分布相对于均值的非对称性的一种度量。
- en: 'Now that we have seen a snapshot of the dataset, it is worth seeing some other
    statistics such as average claim or loss, minimum, maximum loss, and many more,
    using Spark SQL. But before that, let''s rename the last column from `loss` to
    `label` since the ML model will complain about it. Even after using the `setLabelCol`
    on the regression model, it still looks for a column called `label`. This results
    in a disgusting error saying `org.apache.spark.sql.AnalysisException: cannot resolve
    ''label'' given input columns`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们已经看到了数据集的快照，接下来值得查看一些其他统计信息，比如平均索赔或损失、最小值、最大损失等等，使用Spark SQL来进行计算。但在此之前，我们先将最后一列的`loss`重命名为`label`，因为ML模型会对此产生警告。即使在回归模型中使用`setLabelCol`，它仍然会查找名为`label`的列。这会导致一个令人烦恼的错误，提示`org.apache.spark.sql.AnalysisException:
    cannot resolve ''label'' given input columns`：'
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, since we want to execute an SQL query, we need to create a temporary view
    so that the operation can be performed in-memory:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们想要执行SQL查询，我们需要创建一个临时视图，以便操作可以在内存中执行：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s average the damage claimed by the clients:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算客户声明的平均损失：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Similarly, let''s see the lowest claim made so far:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，让我们看看到目前为止的最低索赔：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And let''s see the highest claim made so far:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看到目前为止的最高索赔：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Since Scala or Java does not come with a handy visualization library, I could
    not something else but now let's focus on the data preprocessing before we prepare
    our training set.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Scala或Java没有自带便捷的可视化库，我暂时无法做其他处理，但现在我们集中精力在数据预处理上，准备训练集之前进行清理。
- en: Data preprocessing
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Now that we have looked at some data properties, the next task is to do some
    preprocessing, such as cleaning, before getting the training set. For this part,
    use the `Preprocessing.scala` file. For this part, the following imports are required:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经查看了一些数据属性，接下来的任务是进行一些预处理，如清理数据，然后再准备训练集。对于这一部分，请使用`Preprocessing.scala`文件。对于这部分，需要以下导入：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we load both the training and the test set as shown in the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载训练集和测试集，如以下代码所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The next task is to prepare the training and test set for our ML model to be
    learned. In the preceding DataFrame out of the training dataset, we renamed the
    `loss` to `label`. Then the content of `train.csv` was split into training and
    (cross) validation data, 75% and 25%, respectively.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步任务是为我们的ML模型准备训练集和测试集。在之前的训练数据框中，我们将`loss`重命名为`label`。接着，将`train.csv`的内容分割为训练数据和（交叉）验证数据，分别为75%和25%。
- en: 'The content of `test.csv` is used for evaluating the ML model. Both original
    DataFrames are also sampled, which is particularly useful for running fast executions
    on your local machine:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`test.csv`的内容用于评估ML模型。两个原始数据框也进行了采样，这对在本地机器上运行快速执行非常有用：'
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We also should do null checking. Here, I have used a naïve approach. The thing
    is that if the training DataFrame contains any null values, we completely drop
    those rows. This makes sense since a few rows out of 188,318 do no harm. However,
    feel free to adopt another approach such as null value imputation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该进行空值检查。这里，我采用了一种简单的方法。因为如果训练数据框架中包含任何空值，我们就会完全删除这些行。这是有意义的，因为在188,318行数据中，删除少数几行并不会造成太大问题。不过，你也可以采取其他方法，如空值插补：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then we cache both the sets for faster in-memory access:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们缓存这两个数据集，以便更快速地进行内存访问：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Additionally, we should perform the sampling of the test set that will be required
    in the evaluation step:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还应该对测试集进行采样，这是评估步骤中所需要的：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since the training set contains both the numerical and categorical values,
    we need to identify and treat them separately. First, let''s identify only the
    categorical column:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练集包含了数值型和分类值，我们需要分别识别并处理它们。首先，让我们只识别分类列：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, the following method is used to remove categorical columns with too many
    categories, which we already discussed in the preceding section:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下方法删除类别过多的列，这是我们在前一节中已经讨论过的：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now the following method is used to select only feature columns. So essentially,
    we should remove the ID (since the ID is just the identification number of the
    clients, it does not carry any non-trivial information) and the label column:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来使用以下方法只选择特征列。所以本质上，我们应该删除ID列（因为ID只是客户的识别号码，不包含任何非平凡的信息）和标签列：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Well, so far we have treated some bad columns that are either trivial or not
    needed at all. Now the next task is to construct the definitive set of feature
    columns:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，到目前为止，我们已经处理了一些无关或不需要的列。现在下一步任务是构建最终的特征列集：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`StringIndexer` encodes a given string column of labels to a column of label
    indices. If the input column is numeric in nature, we cast it to string using
    the `StringIndexer` and index the string values. When downstream pipeline components
    such as Estimator or Transformer make use of this string-indexed label, you must
    set the input column of the component to this string-indexed column name. In many
    cases, you can set the input column with `setInputCol`.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`StringIndexer`将给定的字符串标签列编码为标签索引列。如果输入列是数值类型的，我们使用`StringIndexer`将其转换为字符串，并对字符串值进行索引。当下游管道组件（如Estimator或Transformer）使用这个字符串索引标签时，必须将该组件的输入列设置为该字符串索引列名。在许多情况下，你可以通过`setInputCol`来设置输入列。'
- en: 'Now we need to use the `StringIndexer()` for categorical columns:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要使用`StringIndexer()`来处理类别列：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that this is not an efficient approach. An alternative approach would be
    using a OneHotEncoder estimator.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是一种高效的方法。另一种替代方法是使用OneHotEncoder估算器。
- en: OneHotEncoder maps a column of label indices to a column of binary vectors,
    with a single one-value at most. This encoding permits algorithms that expect
    continuous features, such as logistic regression, to utilize categorical features.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: OneHotEncoder将标签索引列映射到二进制向量列，每个向量最多只有一个值为1。该编码允许期望连续特征的算法（如逻辑回归）利用类别特征。
- en: 'Now let''s use the `VectorAssembler()` to transform a given list of columns
    into a single vector column:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`VectorAssembler()`将给定的列列表转换为单一的向量列：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`VectorAssembler` is a transformer. It combines a given list of columns into
    a single vector column. It is useful for combining the raw features and features
    generated by different feature transformers into one feature vector, in order
    to train ML models such as logistic regression and decision trees.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorAssembler`是一个转换器。它将给定的列列表合并为单一的向量列。它对于将原始特征和由不同特征转换器生成的特征合并为一个特征向量非常有用，以便训练机器学习模型，如逻辑回归和决策树。'
- en: That's all we need before we start training the regression models. First, we
    start training the LR model and evaluate the performance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练回归模型之前，这就是我们需要做的全部。首先，我们开始训练LR模型并评估其性能。
- en: LR for predicting insurance severity claims
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用LR预测保险赔偿的严重程度
- en: As you have already seen, the loss to be predicted contains continuous values,
    that is, it will be a regression task. So in using regression analysis here, the
    goal is to predict a continuous target variable, whereas another area called classification
    predicts a label from a finite set.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，预测的损失包含连续值，也就是说，这是一个回归任务。因此，在此使用回归分析时，目标是预测一个连续的目标变量，而另一个领域——分类，则预测从有限集合中选择一个标签。
- en: '**Logistic regression** (**LR**) belongs to the family of regression algorithms.
    The goal of regression is to find relationships and dependencies between variables.
    It models the relationship between a continuous scalar dependent variable *y*
    (that is, label or target) and one or more (a D-dimensional vector) explanatory
    variable (also independent variables, input variables, features, observed data,
    observations, attributes, dimensions, and data points) denoted as *x* using a
    linear function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**（**LR**）属于回归算法家族。回归的目标是寻找变量之间的关系和依赖性。它通过线性函数建模连续标量因变量*y*（即标签或目标）与一个或多个（D维向量）解释变量（也称为自变量、输入变量、特征、观察数据、观测值、属性、维度和数据点）*x*之间的关系：'
- en: '![](img/6e405005-952c-4f9c-92c8-cbf4e839e297.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e405005-952c-4f9c-92c8-cbf4e839e297.png)'
- en: 'Figure 9: A regression graph separates data points (in red dots) and the blue
    line is regression'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：回归图将数据点（红色圆点）分开，蓝线为回归线
- en: 'LR models the relationship between a dependent variable *y,* which involves
    a linear combination of interdependent variables *x[i]*. The letters *A* and *B*
    represent constants that describe the *y* axis intercept and the slope of the
    line respectively:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LR模型描述了因变量 *y* 与一组相互依赖的自变量 *x[i]* 之间的关系。字母 *A* 和 *B* 分别表示描述 *y* 轴截距和回归线斜率的常数：
- en: '*y = A+Bx*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = A+Bx*'
- en: '*Figure 9*, *Regression graph separates data points (in red dots) and the blue
    line is regression* shows an example of simple LR with one independent variable—that
    is, a set of data points and a **best fit** line, which is the result of the regression
    analysis itself. It can be observed that the line does not actually pass through
    all of the points.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9*，*回归图将数据点（红色点）与回归线（蓝色线）分开*，显示了一个简单的LR示例，只有一个自变量——即一组数据点和一个**最佳拟合**线，这是回归分析的结果。可以观察到，这条线并不完全通过所有数据点。'
- en: 'The distance between any data points (measured) and the line (predicted) is
    called the regression error. Smaller errors contribute to more accurate results
    in predicting unknown values. When the errors are reduced to their smallest levels
    possible, the line of best fit is created for the final regression error. Note
    that there are no single metrics in terms of regression errors; there are several
    as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据点（实际测量值）与回归线（预测值）之间的距离称为回归误差。较小的误差有助于更准确地预测未知值。当误差被减少到最小水平时，最终的回归误差会生成最佳拟合线。请注意，在回归误差方面没有单一的度量标准，以下是几种常见的度量：
- en: '**Mean Squared Error** (**MSE**): It is a measure of how close a fitted line
    is to data points. The smaller the MSE, the closer the fit is to the data.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）：它是衡量拟合线与数据点接近程度的指标。MSE越小，拟合程度越接近数据。'
- en: '**Root Mean Squared Error** (**RMSE**): It is the square root of the MSE but
    probably the most easily interpreted statistic, since it has the same units as
    the quantity plotted on the vertical axis.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）：它是均方误差（MSE）的平方根，但可能是最容易解释的统计量，因为它与纵轴上绘制的量具有相同的单位。'
- en: '**R-squared**: R-squared is a statistical measure of how close the data is
    to the fitted regression line. R-squared is always between 0 and 100%. The higher
    the R-squared, the better the model fits your data.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R平方**：R平方是衡量数据与拟合回归线之间接近程度的统计量。R平方总是介于0和100%之间。R平方越高，模型越能拟合数据。'
- en: '**Mean Absolute Error** (**MAE**): MAE measures the average magnitude of the
    errors in a set of predictions without considering their direction. It''s the
    average over the test sample of the absolute differences between prediction and
    actual observation where all individual differences have equal weight.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）：MAE衡量一组预测中误差的平均幅度，而不考虑其方向。它是测试样本中预测值与实际观察值之间的绝对差异的平均值，其中所有个体差异具有相同的权重。'
- en: '**Explained variance**:In statistics, **explained** variation measures the
    proportion to which a mathematical model accounts for the variation of a given
    dataset.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释方差**：在统计学中，**解释**方差衡量数学模型在多大程度上能够解释给定数据集的变化。'
- en: Developing insurance severity claims predictive model using LR
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LR开发保险赔偿严重性预测模型
- en: 'In this sub-section, we will develop a predictive analytics model for predicting
    accidental loss against the severity claim by clients. We start with importing
    required libraries:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将开发一个预测分析模型，用于预测客户在事故损失中的赔偿严重性。我们从导入所需的库开始：
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Then we create an active Spark session as the entry point to the application.
    In addition, importing `implicits__` required for implicit conversions like converting
    RDDs to DataFrames.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个活动的Spark会话，作为应用程序的入口点。此外，导入 `implicits__`，这是隐式转换所需的，如将RDD转换为DataFrame。
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we define some hyperparameters, such as the number of folds for cross-validation,
    the number of maximum iterations, the value of the regression parameter, the value
    of tolerance, and elastic network parameters, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一些超参数，如交叉验证的折数、最大迭代次数、回归参数的值、容差值以及弹性网络参数，如下所示：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Well, now we create an LR estimator:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个LR估计器：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now let''s build a pipeline estimator by chaining the transformer and the LR
    estimator:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过连接变换器和LR估计器来构建一个管道估计器：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Spark ML pipelines have the following components:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML管道包含以下组件：
- en: '**DataFrame**: Used as the central data store where all the original data and
    intermediate results are stored.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据框**：用作中央数据存储，所有原始数据和中间结果都存储在这里。'
- en: '**Transformer**: A transformer transforms one DataFrame into another by adding
    additional feature columns. Transformers are stateless, meaning that they don''t
    have any internal memory and behave exactly the same each time they are used.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：转换器通过添加额外的特征列将一个DataFrame转换成另一个DataFrame。转换器是无状态的，意味着它们没有内部记忆，每次使用时的行为都完全相同。'
- en: '**Estimator**: An estimator is some sort of ML model. In contrast to a transformer,
    an estimator contains an internal state representation and is highly dependent
    on the history of the data that it has already seen.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估算器**：估算器是一种机器学习模型。与转换器不同，估算器包含内部状态表示，并且高度依赖于它已经见过的数据历史。'
- en: '**Pipeline**: Chains the preceding components, DataFrame, Transformer, and
    Estimator together.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：将前面的组件、DataFrame、转换器和估算器连接在一起。'
- en: '**Parameter**: ML algorithms have many knobs to tweak. These are called **hyperparameters**,
    and the values learned by a ML algorithm to fit data are called **parameters**.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：机器学习算法有许多可调整的参数。这些称为**超参数**，而机器学习算法通过学习数据来拟合模型的值称为**参数**。'
- en: 'Before we start performing the cross-validation, we need to have a paramgrid.
    So let''s start creating the paramgrid by specifying the number of maximum iterations,
    the value of the regression parameter, the value of tolerance, and Elastic network
    parameters as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始执行交叉验证之前，我们需要有一个参数网格（paramgrid）。所以让我们通过指定最大迭代次数、回归参数值、公差值和弹性网络参数来创建参数网格，如下所示：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, for a better and stable performance, let''s prepare the K-fold cross-validation
    and grid search as a part of model tuning. As you can probably guess, I am going
    to perform 10-fold cross-validation. Feel free to adjust the number of folds based
    on your settings and dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得更好且稳定的性能，让我们准备K折交叉验证和网格搜索作为模型调优的一部分。正如你们可能猜到的，我将进行10折交叉验证。根据你的设置和数据集，可以自由调整折数：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Fantastic - we have created the cross-validation estimator. Now it''s time
    to train the LR model:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了——我们已经创建了交叉验证估算器。现在是训练LR模型的时候了：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that we have the fitted model, that means it is now capable of making predictions.
    So let''s start evaluating the model on the train and validation set and calculating
    RMSE, MSE, MAE, R-squared, and many more:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了拟合的模型，这意味着它现在能够进行预测。所以，让我们开始在训练集和验证集上评估模型，并计算RMSE、MSE、MAE、R平方等指标：
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    set. Let''s hunt for the best model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经成功计算了训练集和测试集的原始预测结果。接下来，让我们寻找最佳模型：
- en: '[PRE35]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once we have the best fitted and cross-validated model, we can expect good
    prediction accuracy. Now let''s observe the results on the train and the validation
    set:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳拟合并且通过交叉验证的模型，我们可以期望得到良好的预测准确性。现在，让我们观察训练集和验证集上的结果：
- en: '[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we print the preceding results as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将打印前面的结果，如下所示：
- en: '[PRE37]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So our predictive model shows an MAE of about `1356.419484419514` for both the
    training and test set. However, the MAE is much lower on the Kaggle public and
    private leaderboard (go to: [https://www.kaggle.com/c/allstate-claims-severity/leaderboard](https://www.kaggle.com/c/allstate-claims-severity/leaderboard))
    with an MAE of 1096.92532 and 1109.70772 respectively.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的预测模型在训练集和测试集上的MAE约为`1356.419484419514`。然而，在Kaggle的公共和私人排行榜上，MAE要低得多（请访问：[https://www.kaggle.com/c/allstate-claims-severity/leaderboard](https://www.kaggle.com/c/allstate-claims-severity/leaderboard)），公共和私人的MAE分别为1096.92532和1109.70772。
- en: 'Wait! We are not done yet. We still need to make a prediction on the test set:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！我们还没有完成。我们仍然需要在测试集上进行预测：
- en: '[PRE38]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding code should generate a CSV file named `result_LR.csv`. If we open
    the file, we should observe the loss against each ID, that is, claim. We will
    see the contents for both LR, RF, and GBT at the end of this chapter. Nevertheless,
    it is always a good idea to stop the Spark session by invoking the `spark.stop()`
    method.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应生成一个名为`result_LR.csv`的CSV文件。如果我们打开文件，我们应该能够看到每个ID（即索赔）对应的损失。我们将在本章结束时查看LR、RF和GBT的内容。尽管如此，结束Spark会话时，调用`spark.stop()`方法总是个好主意。
- en: An ensemble method is a learning algorithm that creates a model that is composed
    of a set of other base models. Spark ML supports two major ensemble algorithms
    called GBT and random forest based on decision trees. We will now see if we can
    improve the prediction accuracy by reducing the MAE error significantly using
    GBT.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是一种学习算法，它创建一个由其他基础模型组成的模型。Spark ML支持两种主要的集成算法，分别是基于决策树的GBT和随机森林。接下来，我们将看看是否可以通过显著减少MAE误差来提高预测准确度，使用GBT。
- en: GBT regressor for predicting insurance severity claims
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于预测保险赔付严重性的GBT回归器
- en: In order to minimize a `loss` function, **Gradient Boosting Trees** (**GBTs**)
    iteratively train many decision trees. On each iteration, the algorithm uses the
    current ensemble to predict the label of each training instance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化`loss`函数，**梯度提升树**（**GBT**）通过迭代训练多棵决策树。在每次迭代中，算法使用当前的集成模型来预测每个训练实例的标签。
- en: Then the raw predictions are compared with the true labels. Thus, in the next
    iteration, the decision tree will help correct previous mistakes if the dataset
    is re-labeled to put more emphasis on training instances with poor predictions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，原始预测与真实标签进行比较。因此，在下一次迭代中，决策树将帮助纠正之前的错误，如果数据集被重新标记以强调对预测不准确的训练实例进行训练。
- en: 'Since we are talking about regression, it would be more meaningful to discuss
    the regression strength of GBTs and its losses computation. Suppose we have the
    following settings:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们讨论的是回归，那么讨论GBT的回归能力及其损失计算会更有意义。假设我们有以下设置：
- en: '*N* data instances'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 数据实例'
- en: '*y[i]* = label of instance *i*'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y[i]* = 实例*i*的标签'
- en: '*x[i]* = features of instance *i*'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[i]* = 实例*i*的特征'
- en: 'Then the *F(x[i])* function is the model''s predicted label; for instance,
    it tries to minimize the error, that is, loss:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*F(x[i])*函数是模型的预测标签；例如，它试图最小化误差，即损失：
- en: '![](img/a116bede-a15a-490c-b5a4-cc9b15c285a1.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a116bede-a15a-490c-b5a4-cc9b15c285a1.png)'
- en: 'Now, similar to decision trees, GBTs also:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与决策树类似，GBT也会：
- en: Handle categorical features (and of course numerical features too)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理类别特征（当然也包括数值特征）
- en: Extend to the multiclass classification setting
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展到多类分类设置
- en: Perform both the binary classification and regression (multiclass classification
    is not yet supported)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行二分类和回归（目前尚不支持多类分类）
- en: Do not require feature scaling
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要特征缩放
- en: Capture non-linearity and feature interactions, which are greatly missing in
    LR, such as linear models
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕捉线性模型中（如线性回归）严重缺失的非线性和特征交互
- en: '**Validation while training**: Gradient boosting can overfit, especially when
    you have trained your model with more trees. In order to prevent this issue, it
    is useful to validate while carrying out the training.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练过程中的验证**：梯度提升可能会过拟合，尤其是在你使用更多树训练模型时。为了防止这个问题，训练过程中进行验证是非常有用的。'
- en: 'Since we have already prepared our dataset, we can directly jump into implementing
    a GBT-based predictive model for predicting insurance severity claims. Let''s
    start with importing the necessary packages and libraries:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经准备好了数据集，我们可以直接进入实现基于GBT的预测模型来预测保险赔付严重性。让我们从导入必要的包和库开始：
- en: '[PRE39]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now let''s define and initialize the hyperparameters needed to train the GBTs,
    such as the number of trees, number of max bins, number of folds to be used during
    cross-validation, number of maximum iterations to iterate the training, and finally
    max tree depth:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义并初始化训练GBT所需的超参数，例如树的数量、最大分箱数、交叉验证中使用的折数、训练的最大迭代次数，最后是最大树深度：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, again we instantiate a Spark session and implicits as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们再次实例化一个Spark会话并启用隐式转换，如下所示：
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now that we care an estimator algorithm, that is, GBT:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们关心的是一个估算器算法，即GBT：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we build the pipeline by chaining the transformations and predictor together
    as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过将变换和预测器串联在一起构建管道，如下所示：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Before we start performing the cross-validation, we need to have a paramgrid.
    So let''s start creating the paramgrid by specifying the number of maximum iteration,
    max tree depth, and max bins as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始执行交叉验证之前，我们需要一个参数网格。接下来，我们通过指定最大迭代次数、最大树深度和最大分箱数来开始创建参数网格：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, for a better and stable performance, let''s prepare the K-fold cross-validation
    and grid search as a part of model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust the number of folds based on you
    settings and dataset:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得更好且稳定的性能，让我们准备 K-fold 交叉验证和网格搜索作为模型调优的一部分。如你所料，我将进行 10-fold 交叉验证。根据你的设置和数据集，你可以自由调整折数：
- en: '[PRE45]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Fantastic, we have created the cross-validation estimator. Now it''s time to
    train the GBT model:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒，我们已经创建了交叉验证估算器。现在是时候训练 GBT 模型了：
- en: '[PRE46]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now that we have the fitted model, that means it is now capable of making predictions.
    So let''s start evaluating the model on the train and validation set, and calculating
    RMSE, MSE, MAE, R-squared, and so on:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了拟合的模型，这意味着它现在能够进行预测。所以让我们开始在训练集和验证集上评估模型，并计算 RMSE、MSE、MAE、R-squared
    等指标：
- en: '[PRE47]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    set. Let''s hunt for the best model:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！我们已经成功计算了训练集和测试集的原始预测值。让我们开始寻找最佳模型：
- en: '[PRE48]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As already stated, by using GBT it is possible to measure feature importance
    so that at a later stage we can decide which features are to be used and which
    ones are to be dropped from the DataFrame. Let''s find the feature importance
    of the best model we just created previously, for all features in ascending order
    as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用 GBT 可以衡量特征重要性，这样在后续阶段我们可以决定哪些特征要使用，哪些要从 DataFrame 中删除。让我们找到之前创建的最佳模型的特征重要性，并按升序列出所有特征，如下所示：
- en: '[PRE49]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once we have the best fitted and cross-validated model, we can expect good
    prediction accuracy. Now let''s observe the results on the train and the validation
    set:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳拟合且交叉验证的模型，就可以期待良好的预测精度。现在让我们观察训练集和验证集上的结果：
- en: '[PRE50]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, we print the preceding results as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们按如下方式打印之前的结果：
- en: '[PRE51]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: So our predictive model shows an MAE of about `1126.582534126603` and `1289.9808960385383`
    for the training and test sets respectively. The last result is important for
    understanding the feature importance (the preceding list is abridged to save space
    but you should receive the full list). Especially, we can see that the first three
    features are not important at all so we can safely drop them from the DataFrame.
    We will provide more insight in the next section.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的预测模型显示训练集和测试集的 MAE 分别为 `1126.582534126603` 和 `1289.9808960385383`。最后一个结果对理解特征重要性至关重要（前面的列表已经简化以节省空间，但你应该收到完整的列表）。特别是，我们可以看到前三个特征完全不重要，因此我们可以安全地将它们从
    DataFrame 中删除。在下一节中我们会提供更多的见解。
- en: 'Now finally, let us run the prediction over the test set and generate the predicted
    loss for each claim from the clients:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在测试集上运行预测，并为每个客户的理赔生成预测损失：
- en: '[PRE52]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The preceding code should generate a CSV file named `result_GBT.csv`. If we
    open the file, we should observe the loss against each ID, that is, claim. We
    will see the contents for both LR, RF, and GBT at the end of this chapter. Nevertheless,
    it is always a good idea to stop the Spark session by invoking the `spark.stop()`
    method.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该生成一个名为 `result_GBT.csv` 的 CSV 文件。如果我们打开文件，我们应该能看到每个 ID 对应的损失，也就是理赔。我们将在本章末尾查看
    LR、RF 和 GBT 的内容。不过，结束时调用 `spark.stop()` 方法停止 Spark 会话总是一个好主意。
- en: Boosting the performance using random forest regressor
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林回归器提升性能
- en: In the previous sections, we did not experience the expected MAE value although
    we got predictions of the severity loss in each instance. In this section, we
    will develop a more robust predictive analytics model for the same purpose but
    use an random forest regressor. However, before diving into its formal implementation,
    a short overview of the random forest algorithm is needed.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，尽管我们对每个实例的损失严重性做出了预测，但并没有得到预期的 MAE 值。在本节中，我们将开发一个更稳健的预测分析模型，目的是相同的，但使用随机森林回归器。不过，在正式实现之前，我们需要简要了解一下随机森林算法。
- en: Random Forest for classification and regression
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林用于分类和回归
- en: Random Forest is an ensemble learning technique used for solving supervised
    learning tasks, such as classification and regression. An advantageous feature
    of Random Forest is that it can overcome the overfitting problem across its training
    dataset. A forest in Random Forest usually consists of hundreds of thousands of
    trees. These trees are actually trained on different parts of the same training
    set.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成学习技术，用于解决监督学习任务，如分类和回归。随机森林的一个优势特性是它能够克服训练数据集上的过拟合问题。随机森林中的一片“森林”通常由数百到数千棵树组成。这些树实际上是在同一训练集的不同部分上训练的。
- en: More technically, an individual tree that grows very deep tends to learn from
    highly unpredictable patterns. This creates overfitting problems on the training
    sets. Moreover, low biases make the classifier a low performer even if your dataset
    quality is good in terms of the features presented. On the other hand, an Random
    Forest helps to average multiple decision trees together with the goal of reducing
    the variance to ensure consistency by computing proximities between pairs of cases.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地说，单棵树如果长得很深，往往会从高度不可预测的模式中学习。这会导致训练集上的过拟合问题。此外，较低的偏差会使分类器表现较差，即使你的数据集在特征呈现方面质量很好。另一方面，随机森林通过将多棵决策树进行平均，目的是减少方差，确保一致性，通过计算案例对之间的接近度来实现。
- en: '**GBT** or **Random Forest**? Although both GBT and Random Forest are ensembles
    of trees, the training processes are different. There are several practical trade-offs
    that exist, which often poses the dilemma of which one to choose. However, Random
    Forest would be the winner in most cases. Here are some justifications:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**GBT**还是**随机森林**？虽然GBT和随机森林都是树的集成方法，但它们的训练过程不同。两者之间存在一些实际的权衡，这常常会带来选择困难。然而，在大多数情况下，随机森林是更优的选择。以下是一些理由：'
- en: GBTs train one tree at a time, but Random Forest can train multiple trees in
    parallel. So the training time is lower for RF. However, in some special cases,
    training and using a smaller number of trees with GBTs is easier and quicker.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GBT每次训练一棵树，而随机森林则可以并行训练多棵树。所以随机森林的训练时间较短。然而，在某些特殊情况下，使用较少数量的树进行GBT训练更简单且速度更快。
- en: RFs are less prone to overfitting in most cases, so it reduces the likelihood
    of overfitting. In other words, Random Forest reduces variance with more trees,
    but GBTs reduce bias with more trees.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，随机森林不易过拟合，因此降低了过拟合的可能性。换句话说，随机森林通过增加树的数量来减少方差，而GBT通过增加树的数量来减少偏差。
- en: Finally, Random Forest can be easier to tune since performance improves monotonically
    with the number of trees, but GBT performs badly with an increased number of trees.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，随机森林相对更容易调优，因为性能会随着树的数量单调提升，但GBT随着树的数量增加表现较差。
- en: 'However, this slightly increases bias and makes it harder to interpret the
    results. But eventually, the performance of the final model increases dramatically.
    While using the Random Forest as a classifier, there are some parameter settings:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这会略微增加偏差，并使得结果更难以解释。但最终，最终模型的性能会显著提高。在使用随机森林作为分类器时，有一些参数设置：
- en: If the number of trees is 1, then no bootstrapping is used at all; however,
    if the number of trees is > 1, then bootstrapping is needed. The supported values
    are `auto`, `all`, `sqrt`, `log2`, and `onethird`.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果树的数量是1，则完全不使用自助抽样；但如果树的数量大于1，则需要使用自助抽样。支持的值有`auto`、`all`、`sqrt`、`log2`和`onethird`。
- en: The supported numerical values are *(0.0-1.0)* and *[1-n]*. However, if `featureSubsetStrategy`
    is chosen as `auto`, the algorithm chooses the best feature subset strategy automatically.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的数值范围是*(0.0-1.0)*和*[1-n]*。但是，如果选择`featureSubsetStrategy`为`auto`，算法将自动选择最佳的特征子集策略。
- en: If the `numTrees == 1`, the `featureSubsetStrategy` is set to be `all`. However,
    if the `numTrees > 1` (that is, forest), the `featureSubsetStrategy` is set to
    be `sqrt` for classification.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`numTrees == 1`，则`featureSubsetStrategy`设置为`all`。但是，如果`numTrees > 1`（即森林），则`featureSubsetStrategy`将设置为`sqrt`用于分类。
- en: Moreover, if a real value `n` is set in the range of *(0, 1.0)*, `n*number_of_features`
    will be used. However, if an integer value `n` is in the range (1, the number
    of features) is set, only `n` features are used alternatively.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，如果设置了一个实数值`n`，并且`n`的范围在*(0, 1.0)*之间，则将使用`n*number_of_features`。但是，如果设置了一个整数值`n`，并且`n`的范围在(1，特征数)之间，则仅交替使用`n`个特征。
- en: 'The parameter `categoricalFeaturesInfo` is a map used for storing arbitrary
    or of categorical features. An entry *(n -> k)* indicates that feature `n` is
    categorical with I categories indexed from *0: (0, 1,...,k-1)*.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '参数 `categoricalFeaturesInfo` 是一个映射，用于存储任意或分类特征。一个条目 *(n -> k)* 表示特征 `n` 是分类的，有
    I 个类别，索引从 *0: (0, 1,...,k-1)*。'
- en: The impurity criterion is used for information gain calculation. The supported
    values are `gini` and `variance` for classification and regression respectively.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纯度标准用于信息增益计算。支持的值分别为分类和回归中的 `gini` 和 `variance`。
- en: The `maxDepth` is the maximum depth of the tree (for example, depth 0 means
    one leaf node, depth 1 means one internal node plus two leaf nodes).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth` 是树的最大深度（例如，深度为 0 表示一个叶节点，深度为 1 表示一个内部节点加上两个叶节点）。'
- en: The `maxBins` signifies the maximum number of bins used for splitting the features,
    where the suggested value is 100 to get better results.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins` 表示用于拆分特征的最大桶数，建议的值是 100，以获得更好的结果。'
- en: Finally, the random seed is used for bootstrapping and choosing feature subsets
    to avoid the random nature of the results.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，随机种子用于自助抽样和选择特征子集，以避免结果的随机性。
- en: As already mentioned, since Random Forest is fast and scalable enough for a
    large-scale dataset, Spark is a suitable technology to implement the RF, and to
    implement this massive scalability. However, if the proximities are calculated,
    storage requirements also grow exponentially.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，由于随机森林足够快速且可扩展，适合处理大规模数据集，因此 Spark 是实现 RF 的合适技术，并实现这种大规模的可扩展性。然而，如果计算了邻近性，存储需求也会呈指数增长。
- en: 'Well, that''s enough about RF. Now it''s time to get our hands dirty, so let''s
    get started. We begin with importing required libraries:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，关于 RF 就讲到这里。现在是时候动手实践了，开始吧。我们从导入所需的库开始：
- en: '[PRE53]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Then we create an active Spark session and import implicits:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个活动的 Spark 会话并导入隐式转换：
- en: '[PRE54]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then we define some hyperparameters, such as the number of folds for cross-validation,
    number of maximum iterations, the value of regression parameters, value of tolerance,
    and elastic network parameters, as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一些超参数，比如交叉验证的折数、最大迭代次数、回归参数的值、公差值以及弹性网络参数，如下所示：
- en: '[PRE55]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Note that for an Random Forest based on a decision tree, we require `maxBins`
    to be at least as large as the number of values in each categorical feature. In
    our dataset, we have 110 categorical features with 23 distinct values. Considering
    this, we have to set `MaxBins` to at least 23\. Nevertheless, feel free to play
    with the previous parameters too. Alright, now it''s time to create an LR estimator:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于基于决策树的随机森林，我们要求 `maxBins` 至少与每个分类特征中的值的数量一样大。在我们的数据集中，我们有 110 个分类特征，其中包含
    23 个不同的值。因此，我们必须将 `MaxBins` 设置为至少 23。然而，还是可以根据需要调整之前的参数。好了，现在是时候创建 LR 估计器了：
- en: '[PRE56]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now let''s build a pipeline estimator by chaining the transformer and the LR
    estimator:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过将变换器和 LR 估计器连接起来，构建一个管道估计器：
- en: '[PRE57]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Before we start performing the cross-validation, we need to have a paramgrid.
    So let''s start creating the paramgrid by specifying the number of trees, a number
    for maximum tree depth, and the number of maximum bins parameters, as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始执行交叉验证之前，需要有一个参数网格。所以让我们通过指定树的数量、最大树深度的数字和最大桶数参数来创建参数网格，如下所示：
- en: '[PRE58]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, for better and stable performance, let''s prepare the K-fold cross-validation
    and grid search as a part of model tuning. As you can probably guess, I am going
    to perform 10-fold cross-validation. Feel free to adjust the number of folds based
    on your settings and dataset:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得更好且稳定的性能，让我们准备 K 折交叉验证和网格搜索作为模型调优的一部分。正如你可能猜到的，我将执行 10 折交叉验证。根据你的设置和数据集，随时调整折数：
- en: '[PRE59]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Fantastic, we have created the cross-validation estimator. Now it''s time to
    train the LR model:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，我们已经创建了交叉验证估计器。现在是训练 LR 模型的时候了：
- en: '[PRE60]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now that we have the fitted model, that means it is now capable of making predictions.
    So let''s start evaluating the model on the train and validation set, and calculating
    RMSE, MSE, MAE, R-squared, and many more:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了拟合的模型，这意味着它现在能够进行预测。那么让我们开始在训练集和验证集上评估模型，并计算 RMSE、MSE、MAE、R-squared
    等指标：
- en: '[PRE61]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    set. Let''s hunt for the best model:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们已经成功地计算了训练集和测试集的原始预测结果。接下来，让我们寻找最佳模型：
- en: '[PRE62]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'As already stated, by using RF, it is possible to measure the feature importance
    so that at a later stage, we can decide which features should be used and which
    ones are to be dropped from the DataFrame. Let''s find the feature importance
    from the best model we just created for all features in ascending order, as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，通过使用RF（随机森林），可以衡量特征的重要性，以便在后续阶段决定哪些特征应该保留，哪些特征应从DataFrame中删除。接下来，让我们按升序查找刚刚为所有特征创建的最佳模型的特征重要性，如下所示：
- en: '[PRE63]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once we have the best fitted and cross-validated model, we can expect a good
    prediction accuracy. Now let''s observe the results on the train and the validation
    set:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了最佳拟合并经过交叉验证的模型，就可以期待较好的预测准确性。现在，让我们观察训练集和验证集的结果：
- en: '[PRE64]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we print the preceding results as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们按如下方式打印前面的结果：
- en: '[PRE65]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: So our predictive model shows an MAE of about `809.5917285994619` and `1273.0714382935894`
    for the training and test set respectively. The last result is important for understanding
    the feature importance (the preceding list is abridged to save space but you should
    receive the full list).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的预测模型在训练集和测试集上分别显示出MAE（平均绝对误差）为`809.5917285994619`和`1273.0714382935894`。最后的结果对于理解特征重要性非常重要（前面的列表已简化以节省空间，但您应该会收到完整的列表）。
- en: 'I have drawn both the categorical and continuous features, and their respective
    importance in Python, so I will not show the code here but only the graph. Let''s
    see the categorical features showing feature importance as well as the corresponding
    feature number:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在Python中绘制了类别特征和连续特征及其相应的重要性，因此这里不再展示代码，只展示图表。让我们看看类别特征的特征重要性，以及对应的特征编号：
- en: '![](img/ca636833-3cff-4307-a496-220d051b6786.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca636833-3cff-4307-a496-220d051b6786.png)'
- en: 'Figure 11: Random Forest categorical feature importance'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：随机森林类别特征重要性
- en: From the preceding graph, it is clear that categorical features `cat20`, `cat64`,
    `cat47`, and `cat69` are less important. Therefore, it would make sense to drop
    these features and retrain the Random Forest model to observe better performance.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以清楚地看出，类别特征`cat20`、`cat64`、`cat47`和`cat69`的重要性较低。因此，删除这些特征并重新训练随机森林模型，以观察更好的表现是有意义的。
- en: 'Now let''s see how the continuous features are correlated and contribute to
    the loss column. From the following figure, we can see that all continuous features
    are positively correlated with the loss column. This also signifies that these
    continuous features are not that important compared to the categorical ones we
    have seen in the preceding figure:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看连续特征与损失列的相关性及其贡献。从下图中可以看到，所有连续特征与损失列之间都有正相关关系。这也意味着，这些连续特征与我们在前面图中看到的类别特征相比并不那么重要：
- en: '![](img/3e81f309-b2f2-4737-9047-cf674a9bf9d8.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e81f309-b2f2-4737-9047-cf674a9bf9d8.png)'
- en: 'Figure 12: Correlations between the continuous features and the label'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：连续特征与标签之间的相关性
- en: 'What we can learn from these two analyses is that we can naively drop some
    unimportant columns and train the Random Forest model to observe if there is any
    reduction in the MAE value for both the training and validation set. Finally,
    let''s make a prediction on the test set:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两个分析中我们可以得出结论：我们可以简单地删除一些不重要的列，并训练随机森林模型，观察训练集和验证集的MAE值是否有所减少。最后，让我们对测试集进行预测：
- en: '[PRE66]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Also, similar to LR, you can stop the Spark session by invoking the `stop()`
    method. Now the generated `result_RF.csv` file should contain the loss against
    each ID, that is, claim.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与LR（逻辑回归）类似，您可以通过调用`stop()`方法停止Spark会话。现在生成的`result_RF.csv`文件应该包含每个ID（即索赔）的损失。
- en: Comparative analysis and model deployment
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较分析与模型部署
- en: 'You have already seen that the LR model is much easier to train for a small
    training dataset. However, we haven''t experienced better accuracy compared to
    GBT and Random Forest models. However, the simplicity of the LR model is a very
    good starting point. On the other hand, we already argued that Random Forest would
    be the winner over GBT for several reasons, of course. Let''s see the results
    in a table:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到，LR模型对于小型训练数据集来说训练起来要容易得多。然而，与GBT（梯度提升树）和随机森林模型相比，我们并没有看到更好的准确性。然而，LR模型的简洁性是一个非常好的起点。另一方面，我们已经讨论过，随机森林在许多方面都会胜过GBT。让我们在表格中查看结果：
- en: '![](img/2c90a1fe-89d1-4300-8946-1e2c1e877daf.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c90a1fe-89d1-4300-8946-1e2c1e877daf.png)'
- en: 'Now let''s see how the predictions went for each model for 20 accidents or
    damage claims:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看每个模型对于20起事故或损害索赔的预测情况：
- en: '![](img/f13ba518-733c-4670-aea7-a02cf25050d7.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f13ba518-733c-4670-aea7-a02cf25050d7.png)'
- en: 'Figure 13: Loss prediction by i) LR, ii) GBT, and iii) Random Forest models'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：i) 线性回归（LR）、ii) 梯度提升树（GBT）和 iii) 随机森林模型的损失预测
- en: Therefore, based on table 2, it is clear that we should go with the Random Forest
    regressor to not only predict the insurance claim loss but also its production.
    Now we will see a quick overview of how to take our best model, that is, an Random
    Forest regressor into production. The idea is, as a data scientist, you may have
    produced an ML model and handed it over to an engineering team in your company
    for deployment in a production-ready environment.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据表2，我们可以清楚地看到，我们应该选择随机森林回归模型（Random Forest regressor）来预测保险理赔损失以及其生产情况。现在我们将简要概述如何将我们最好的模型，即随机森林回归模型投入生产。这个想法是，作为数据科学家，你可能已经训练了一个机器学习模型，并将其交给公司中的工程团队进行部署，以便在生产环境中使用。
- en: 'Here, I provide a naïve approach, though IT companies must have their own way
    to deploy the models. Nevertheless, there will be a dedicated section at the end
    of this topic. This scenario can easily become a reality by using model persistence—the
    ability to save and load models that come with Spark. Using Spark, you can either:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我提供了一种简单的方法，尽管IT公司肯定有自己的模型部署方式。尽管如此，本文的最后会有专门的章节。通过使用模型持久化功能——即 Spark 提供的保存和加载模型的能力，这种场景完全可以变为现实。通过
    Spark，你可以选择：
- en: Save and load a single model
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载单一模型
- en: Save and load a full pipeline
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存并加载整个工作流
- en: 'A single model is pretty simple, but less effective and mainly works on Spark
    MLlib-based model persistence. Since we are more interested in saving the best
    model, that is, the Random Forest regressor model, at first we will fit an Random
    Forest regressor using Scala, save it, and then load the same model back using
    Scala:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 单一模型相对简单，但效果较差，主要适用于基于 Spark MLlib 的模型持久化。由于我们更关心保存最好的模型，也就是随机森林回归模型，我们首先将使用
    Scala 拟合一个随机森林回归模型，保存它，然后再使用 Scala 加载相同的模型：
- en: '[PRE67]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We can now simply call the `write.overwrite().save()` method to save this model
    to local storage, HDFS, or S3, and the load method to load it right back for future
    use:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以简单地调用`write.overwrite().save()`方法将该模型保存到本地存储、HDFS 或 S3，并使用加载方法将其重新加载以便将来使用：
- en: '[PRE68]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now the thing that we need to know is how to use the restored model for making
    predictions. Here''s the answer:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要知道的是如何使用恢复后的模型进行预测。答案如下：
- en: '[PRE69]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '![](img/149cecc1-17f7-4375-b90d-263bf5166ed9.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/149cecc1-17f7-4375-b90d-263bf5166ed9.png)'
- en: 'Figure 14: Spark model deployment for production'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：Spark模型在生产中的部署
- en: So far, we have only looked at saving and loading a single ML model but not
    a tuned or stable one. It might even provide you with many wrong predictions.
    Therefore, now the second approach might be more effective.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看过如何保存和加载单一的机器学习模型，但没有涉及调优或稳定的模型。这个模型可能甚至会给你很多错误的预测。因此，现在第二种方法可能更有效。
- en: The reality is that, in practice, ML workflows consist of many stages, from
    feature extraction and transformation to model fitting and tuning. Spark ML provides
    pipelines to help users construct these workflows. Similarly, a pipeline with
    the cross-validated model can be saved and restored back the same way as we did
    in the first approach.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现实情况是，在实际操作中，机器学习工作流包括多个阶段，从特征提取和转换到模型拟合和调优。Spark ML 提供了工作流帮助工具，以帮助用户构建这些工作流。类似地，带有交叉验证模型的工作流也可以像我们在第一种方法中做的那样保存和恢复。
- en: 'We fit the cross-validated model with the training set:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用训练集对交叉验证后的模型进行拟合：
- en: '[PRE70]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then we save the workflow/pipeline:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们保存工作流/流水线：
- en: '[PRE71]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Note that the preceding line of code will save the model in your preferred
    location with the following directory structure:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的代码行将把模型保存在你选择的位置，并具有以下目录结构：
- en: '![](img/18125bde-d6a1-45e6-92f1-18fa933f1365.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18125bde-d6a1-45e6-92f1-18fa933f1365.png)'
- en: 'Figure 15: Saved model directory structure'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：保存的模型目录结构
- en: '[PRE72]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Spark-based model deployment for large-scale dataset
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Spark的模型部署用于大规模数据集
- en: In a production ready environment, we often need to deploy a pretrained models
    in scale. Especially, if we need to handle a massive amount of data. So our ML
    model has to face this scalability issue to perform continiously and with faster
    response. To overcome this issue, one of the main big data paradigms that Spark
    has brought for us is the introduction of in-memory computing (it supports dis
    based operation, though) and caching abstraction.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们常常需要以规模化的方式部署预训练模型。尤其是当我们需要处理大量数据时，我们的 ML 模型必须解决这个可扩展性问题，以便持续执行并提供更快速的响应。为了克服这个问题，Spark
    为我们带来的一大大数据范式就是引入了内存计算（尽管它支持磁盘操作），以及缓存抽象。
- en: 'This makes Spark ideal for large-scale data processing and enables the computing
    nodes to perform multiple operations by accessing the same input data across multiple
    nodes in a computing cluster or cloud computing infrastructures (example, Amazon
    AWS, DigitalOcean, Microsoft Azure, or Google Cloud). For doing so, Spark supports
    four cluster managers (the last one is still experimental, though):'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 Spark 非常适合大规模数据处理，并使得计算节点能够通过访问多个计算节点上的相同输入数据，执行多个操作，无论是在计算集群还是云计算基础设施中（例如，Amazon
    AWS、DigitalOcean、Microsoft Azure 或 Google Cloud）。为此，Spark 支持四种集群管理器（不过最后一个仍然处于实验阶段）：
- en: '**Standalone**: A simple cluster manager included with Spark that makes it
    easy to set up a cluster.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Standalone**: Spark 附带的简单集群管理器，使得设置集群变得更加容易。'
- en: '**Apache Mesos**: A general cluster manager that can also run Hadoop MapReduce
    and service applications.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos**: 一个通用的集群管理器，也可以运行 Hadoop MapReduce 和服务应用程序。'
- en: '**Hadoop YARN**: The resource manager in Hadoop 2.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**: Hadoop 2 中的资源管理器。'
- en: '**Kubernetes (experimental)**: In addition to the above, there is experimental
    support for Kubernetes. Kubernetes is an open-source platform for providing container-centric
    infrastructure. See more at [https://spark.apache.org/docs/latest/cluster-overview.html](https://spark.apache.org/docs/latest/cluster-overview.html).'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes（实验性）**: 除了上述内容外，还支持 Kubernetes 的实验性功能。Kubernetes 是一个开源平台，用于提供容器为中心的基础设施。更多信息请见
    [https://spark.apache.org/docs/latest/cluster-overview.html](https://spark.apache.org/docs/latest/cluster-overview.html)。'
- en: You can upload your input dataset on **Hadoop Distributed File System** (**HDFS**)
    or **S3** storage for efficient computing and storing big data cheaply. Then the
    `spark-submit` script in Spark’s bin directory is used to launch applications
    on any of those cluster modes. It can use all of the cluster managers through
    a uniform interface so you don’t have to configure your application specially
    for each one.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将输入数据集上传到 **Hadoop 分布式文件系统**（**HDFS**）或 **S3** 存储中，以实现高效计算和低成本存储大数据。然后，Spark
    的 bin 目录中的 `spark-submit` 脚本将用于在任意集群模式下启动应用程序。它可以通过统一的接口使用所有集群管理器，因此你不需要为每个集群专门配置应用程序。
- en: However, if your code depends on other projects, you will need to package them
    alongside your application in order to distribute the code to a Spark cluster.
    To do this, create an assembly jar (also called `fat` or `uber` jar) containing
    your code and its dependencies. Then ship the code where the data resides and
    execute your Spark jobs. Both the `SBT` and `Maven` have assembly plugins that
    should help you to prepare the jars.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你的代码依赖于其他项目，那么你需要将它们与应用程序一起打包，以便将代码分发到 Spark 集群中。为此，创建一个包含你的代码及其依赖项的 assembly
    jar 文件（也称为 `fat` 或 `uber` jar）。然后将代码分发到数据所在的地方，并执行 Spark 作业。`SBT` 和 `Maven` 都有
    assembly 插件，可以帮助你准备这些 jar 文件。
- en: 'When creating assembly jars, list Spark and Hadoop as dependencies as well.
    These need not be bundled since they are provided by the cluster manager at runtime.
    Once you have an assembled jar, you can call the script by passing your jar as
    follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 assembly jar 文件时，也需要将 Spark 和 Hadoop 列为依赖项。这些依赖项不需要打包，因为它们会在运行时由集群管理器提供。创建了合并的
    jar 文件后，可以通过以下方式传递 jar 来调用脚本：
- en: '[PRE73]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'In the preceding command, some of the commonly used options are listed down
    as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的命令中，列出了以下一些常用的选项：
- en: '`--class`: The entry point for your application (example, `org.apache.spark.examples.SparkPi`).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--class`: 应用程序的入口点（例如，`org.apache.spark.examples.SparkPi`）。'
- en: '`--master`: The master URL for the cluster (example, `spark://23.195.26.187:7077`).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--master`: 集群的主 URL（例如，`spark://23.195.26.187:7077`）。'
- en: '`--deploy-mode`: Whether to deploy your driver on the worker nodes (cluster)
    or locally as an external client.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--deploy-mode`: 是否将驱动程序部署在工作节点（集群）上，还是作为外部客户端在本地部署。'
- en: '`--conf`: Arbitrary Spark configuration property in key=value format.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conf`: 任意的 Spark 配置属性，采用 key=value 格式。'
- en: '`application-jar`: Path to a bundled jar including your application and all
    dependencies. The URL must be globally visible inside of your cluster, for instance,
    an `hdfs://` path or a `file://` path that is present on all nodes.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`application-jar`：包含你的应用程序和所有依赖项的捆绑 jar 文件的路径。URL 必须在你的集群中全局可见，例如，`hdfs://`
    路径或在所有节点上都存在的 `file://` 路径。'
- en: '`application-arguments`: Arguments passed to the main method of your main class,
    if any.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`application-arguments`：传递给主类主方法的参数（如果有的话）。'
- en: 'For example, you can run the `AllstateClaimsSeverityRandomForestRegressor`
    script on a Spark standalone cluster in client deploy mode as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以在客户端部署模式下，在 Spark 独立集群上运行 `AllstateClaimsSeverityRandomForestRegressor`
    脚本，如下所示：
- en: '[PRE74]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'For more info see Spark website at [https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html).
    Nevertheless, you can find useful information from online blogs or books. By the
    way, I discussed this topic in details in one of my recently published books:
    Md. Rezaul Karim, Sridhar Alla, **Scala and Spark for Big Data Analytics**, Packt
    Publishing Ltd. 2017\. See more at [https://www.packtpub.com/big-data-and-business-intelligence/scala-and-spark-big-data-analytics](https://www.packtpub.com/big-data-and-business-intelligence/scala-and-spark-big-data-analytics).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参见 Spark 网站：[https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html)。不过，你也可以从在线博客或书籍中找到有用的信息。顺便提一下，我在我最近出版的一本书中详细讨论了这个话题：Md.
    Rezaul Karim, Sridhar Alla, **Scala 和 Spark 在大数据分析中的应用**，Packt Publishing Ltd.
    2017。更多信息请见：[https://www.packtpub.com/big-data-and-business-intelligence/scala-and-spark-big-data-analytics](https://www.packtpub.com/big-data-and-business-intelligence/scala-and-spark-big-data-analytics)。
- en: Anyway, we will learn more on deploying ML models in production in upcoming
    chapters. Therefore, that's all I have to write for this chapter.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们将在接下来的章节中学习更多关于如何在生产环境中部署 ML 模型的内容。因此，这一章就写到这里。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how to develop a predictive model for analyzing
    insurance severity claims using some of the most widely used regression algorithms.
    We started with simple LR. Then we saw how we can improve performance using a
    GBT regressor. Then we experienced improved performance using ensemble techniques,
    such as the Random Forest regressor. Finally, we looked at performance comparative
    analysis between these models and chose the best model to deploy for production-ready
    environment.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经学习了如何使用一些最广泛使用的回归算法开发用于分析保险严重性索赔的预测模型。我们从简单的线性回归（LR）开始。然后我们看到如何通过使用
    GBT 回归器来提升性能。接着，我们体验了使用集成技术（如随机森林回归器）来提高性能。最后，我们进行了这些模型之间的性能对比分析，并选择了最佳模型来部署到生产环境中。
- en: In the next chapter, we will look at a new end-to-end project called *Analyzing
    and Predicting Telecommunication Churn*. Churn prediction is essential for businesses
    as it helps you detect customers who are likely to cancel a subscription, product,
    or service. It also minimizes customer defection. It does so by predicting which
    customers are more likely to cancel a subscription to a service.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一个新的端到端项目，名为 *电信客户流失分析与预测*。流失预测对于企业至关重要，因为它可以帮助你发现那些可能取消订阅、产品或服务的客户。它还可以最大限度地减少客户流失。通过预测哪些客户更有可能取消服务订阅，达到了这一目的。
