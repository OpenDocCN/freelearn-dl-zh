- en: Object Detection and Instance Segmentation with CNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于CNN的物体检测和实例分割
- en: 'Until now, in this book, we have been mostly using **convolutional neural networks**
    (**CNNs**) for classification. Classification classifies the whole image into
    one of the classes with respect to the entity having the maximum probability of
    detection in the image. But what if there is not one, but multiple entities of
    interest and we want to have the image associated with all of them? One way to
    do this is to use tags instead of classes, where these tags are all classes of
    the penultimate Softmax classification layer with probability above a given threshold.
    However, the probability of detection here varies widely by size and placement
    of entity, and from the following image, we can actually say, *How confident is
    the model that the identified entity is the one that is claimed?* What if we are
    very confident that there is an entity, say a dog, in the image, but its scale
    and position in the image is not as prominent as that of its owner, a *Person* entity?
    So, a *Multi-Class Tag* is a valid way but not the best for this purpose:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们主要使用**卷积神经网络**（**CNNs**）进行分类。分类将整张图像分类为具有最高检测概率的实体的类别。但如果图像中不仅有一个实体，而是多个感兴趣的实体，我们希望能够关联所有这些实体的图像该怎么办？一种方法是使用标签而不是类别，这些标签是倒数第二个Softmax分类层中具有高于给定阈值的概率的所有类别。然而，这里检测概率会根据实体的大小和位置有很大的差异，从下图中我们实际上可以问，*模型有多自信，认为识别出的实体确实是声明的那个？*假设我们非常自信，图像中确实有一个实体，比如狗，但它在图像中的尺度和位置不如它的主人，*人类*实体那样显眼。那么，*多类别标签*是一个有效的方式，但并不是最好的选择：
- en: '![](img/6abbc29c-4951-427c-bc3e-f85cbefac003.jpeg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6abbc29c-4951-427c-bc3e-f85cbefac003.jpeg)'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The differences between object detection and image classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体检测和图像分类的区别
- en: Traditional, non-CNN approaches for object detection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的非CNN物体检测方法
- en: Region-based CNN and its features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于区域的CNN及其特点
- en: Fast R-CNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fast R-CNN
- en: Faster R-CNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faster R-CNN
- en: Mask R-CNN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mask R-CNN
- en: The differences between object detection and image classification
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测和图像分类的区别
- en: Let's take another example. You are watching the movie *101 Dalmatians*, and
    you want to know how many Dalmatians you can actually count in a given movie scene
    from that movie. Image Classification could, at best, tell you that there is at
    least one dog or one *Dalmatian* (depending upon which level you have trained
    your classifier for), but not exactly how many of them there are.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举另一个例子。假设你正在观看电影《*101忠犬*》，你想知道在电影的某个场景中，实际能数到多少只达尔马提亚犬。图像分类在最佳情况下可以告诉你至少有一只狗或者一只*达尔马提亚犬*（具体取决于你为分类器训练的级别），但无法准确告诉你它们有多少只。
- en: 'Another issue with classification-based models is that they do not tell you
    where the identified entity in the image is. Many times, this is very important.
    Say, for example, you saw your neighbor''s dogplaying with him (*Person*) and
    his cat. You took a snap of them and wanted to extract the image of the dog from
    there to search on the web for its breed or similar dogs like it. The only problem
    here is that searching the whole image might not work, and without identifying
    individual objects from the image, you have to do the cut-extract-search job manually
    for this task, as shown in the following image:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分类的模型的另一个问题是，它们无法告诉你图像中识别出的实体在哪里。很多时候，这一点非常重要。举个例子，假设你看到邻居的狗正在和他（*人类*）以及他的猫玩耍。你拍了一张他们的照片，想从中提取出狗的图像，以便在网上搜索它的品种或类似的狗。然而，问题是，搜索整张图像可能无法成功，而且如果没有从图像中识别出单独的对象，你就不得不手动进行裁剪-提取-搜索的工作，如下图所示：
- en: '![](img/8d289e70-4022-4ba7-b696-a5b2e1017574.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d289e70-4022-4ba7-b696-a5b2e1017574.jpeg)'
- en: So, you essentially need a technique that not only identifies the entities in
    an image but also tells you their placement in the image. This is what is called
    **object detection**. Object detection gives you bounding boxes and class labels
    (along with the probability of detection) of all the entities identified in an
    image. The output of this system can be used to empower multiple advanced use
    cases that work on the specific class of the objects detected.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你实际上需要一种技术，它不仅能识别图像中的实体，还能告诉你它们在图像中的位置。这就是所谓的**物体检测**。物体检测为图像中所有识别出的实体提供边界框和类别标签（以及检测的概率）。该系统的输出可以用于支持多个高级用例，这些用例依赖于特定类别的检测对象。
- en: 'Take, for example, the Facial Recognition feature that you have in Facebook,
    Google Photos, and many other similar apps. In it, before you identify *who is* there
    in an image taken in at a party, you need to detect all the faces in that image;
    then you can pass these faces through your face recognition/classification module
    to get/classify their names. So, the Object nomenclature in object detection is
    not limited to linguistic entities but includes anything that has specific boundaries
    and enough data to train the system, as shown in the following image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，像Facebook、Google Photos以及许多其他类似应用中的面部识别功能。在这些功能中，在你识别图像中*是谁*参加了派对之前，你需要先检测图像中的所有面部；然后，你可以将这些面部通过你的面部识别/分类模块来获取/分类出他们的名字。所以，物体检测中的“物体”命名不仅限于语言实体，还包括任何具有明确边界并且有足够数据来训练系统的事物，如下图所示：
- en: '![](img/ea2c10b7-68bc-4eee-8901-c33a84738f3a.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea2c10b7-68bc-4eee-8901-c33a84738f3a.jpeg)'
- en: Now, if you want to find out how many of the guests present at your party were
    actually **enjoying** it, you can even run an object detection for **Smiling Faces**
    or a **Smile Detector**. There are very powerful and efficient trained models
    of object detectors available for most of the detectable human body parts (eye,
    face, upper body, and so on), popular human expressions (such as a smile), and
    many other general objects as well. So, the next time you use the **Smile Shutter**
    on your smartphone (a feature made to automatically click the image when most
    of the faces in the scene are detected as smiling), you know what is powering
    this feature.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想知道参加你派对的客人中有多少人实际上在**享受**派对，你甚至可以进行一个**微笑脸**的物体检测，或者使用一个**微笑检测器**。目前有非常强大和高效的物体检测模型，适用于大多数可检测的人体部位（眼睛、面部、上半身等）、常见的人的表情（例如微笑）以及许多其他常见物体。所以，下次你使用智能手机上的**微笑快门**功能时（该功能会在场景中大多数面孔被检测为微笑时自动拍照），你就知道是什么驱动了这个功能。
- en: Why is object detection much more challenging than image classification?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么物体检测比图像分类更加具有挑战性？
- en: From our understanding of CNN and image classification so far, let's try to
    understand how we can approach the object detection problem, and that should logically
    lead us to the discovery of the underlying complexity and challenges. Assume we
    are dealing with monochromatic images for simplicity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们目前对CNN和图像分类的理解，让我们尝试理解如何解决物体检测问题，这应该会逻辑地引导我们发现其潜在的复杂性和挑战。假设我们为了简化问题，处理的是单色图像。
- en: 'Any object detection at a high level may be considered a combination of two
    tasks (we will refute this later):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 任何高级的物体检测都可以被视为两个任务的组合（我们稍后会反驳这一点）：
- en: Getting the right bounding boxes (or as many of them to filter later)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取正确的边界框（或者获取足够多的边界框以便后续过滤）
- en: Classifying the object in that bounding box (while returning the classification
    effectiveness for filtering)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在该边界框内对物体进行分类（同时返回分类效果用于过滤）
- en: So, object detection not only has to cater to all the challenges of image classification
    (second objective), but also faces new challenges of finding the right, or as
    many as possible, bounding boxes. As we already know how to use CNNs for the purpose
    of image classification, and the associated challenges, we can now concentrate
    on our first task and explore how effective (classification accuracy) and efficient
    (computational complexity) our approach is—or rather how challenging this task
    is going to be.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，物体检测不仅要解决图像分类（第二个目标）的所有挑战，还面临着寻找正确或尽可能多的边界框这一新的挑战。我们已经知道如何使用CNN进行图像分类，以及相关的挑战，现在我们可以集中精力处理第一个任务，并探讨我们的方案在效果（分类精度）和效率（计算复杂度）方面的有效性——或者更确切地说，探讨这一任务将会有多么具有挑战性。
- en: 'So, we start with randomly generating bounding boxes from the image. Even if
    we do not worry about the computational load of generating so many candidate boxes,
    technically termed as **Region Proposals** (regions that we send as proposals
    for classifying objects), we still need to have some mechanism for finding the
    best values for the following parameters:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们从随机生成图像中的边界框开始。即使我们不担心生成这么多候选框的计算负载，技术上称为**区域提议**（我们发送作为分类物体的提议的区域），我们仍然需要有某种机制来寻找以下参数的最佳值：
- en: Starting (or center) coordinates to extract/draw the candidate bounding box
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于提取/绘制候选边界框的起始（或中心）坐标
- en: Length of the candidate bounding box
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选边界框的长度
- en: Width of the candidate bounding box
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选边界框的宽度
- en: Stride across each axis (distance from one starting location to another in the
    *x*-horizontal axis and *y*-vertical axis)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨越每个轴（从一个起始位置到另一个位置的* x *-水平轴和 * y *-垂直轴的距离）
- en: Let's assume that we can generate such an algorithm that can give us the most
    optimal value of these parameters. Still, will one value for these parameters
    work in most of the cases, or in fact, in some general cases? From our experience,
    we know that each object will have a different scale, so we know that one fixed
    value for *L* and *W* for these boxes will not work. Also, we can understand that
    the same object, say Dog, may be present in varying proportions/scales and positions
    in different images, as in some of our earlier examples. So this confirms our
    belief that we need boxes of not only different scales but also different sizes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以生成一个算法，给出这些参数的最优值。那么，这些参数的一个值在大多数情况下适用吗？实际上，在某些一般情况下适用吗？根据我们的经验，我们知道每个物体的尺度不同，因此我们知道，对于这些框，*L*
    和 *W* 的一个固定值是行不通的。此外，我们还可以理解，相同的物体，比如狗，在不同的图像中可能以不同的比例/尺度和位置出现，正如我们之前的一些例子所示。所以这证实了我们的信念：我们需要的是不同尺度而且不同大小的框。
- en: 'Let''s assume that, correcting from the previous analogy, we want to extract
    *N* number of candidate boxes per starting coordinate in the image, where *N*
    encompasses most of the sizes/scales that may fit our classification problem.
    Although that seems to be a rather challenging job in itself, let''s assume we
    have that magic number and it is far from a combination of *L[1,l-image] x W[1,w-image]*
    (all combinations of *L* and *W* where length is a set of all integers between
    1 and the length of the actual image and breadth is from 1 to the breadth of the
    image); that will lead us to *l*w* boxes per coordinate:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设从前面的类比中修正过来，我们希望从图像中的每个起始坐标提取 *N* 个候选框，其中 *N* 包含大多数可能适合我们分类问题的尺寸/尺度。尽管这似乎是一个相当具有挑战性的任务，但假设我们已经有了那个魔法数字，而它远不是
    *L[1,l-image] x W[1,w-image]*（所有 *L* 和 *W* 的组合，其中长度是实际图像的所有整数集合，宽度是从1到图像宽度）；这将导致每个坐标有
    *l*w* 个框：
- en: '![](img/c76e332d-472c-46bd-8e0d-4f51af7b7126.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c76e332d-472c-46bd-8e0d-4f51af7b7126.jpeg)'
- en: Then, another question is about how many starting coordinates we need to visit
    in our image from where we will extract these *N* boxes each, or the Stride. Using
    a very big stride will lead us to extract sub-images in themselves, instead of
    a single homogeneous object that can be classified effectively and used for the
    purpose of achieving some of the objectives in our earlier examples. Conversely,
    too short a stride (say, 1 pixel in each direction) may mean a lot of candidate
    boxes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，另一个问题是我们需要在图像中访问多少个起始坐标，从这些位置提取每个 *N* 个框，或者说步长。使用一个非常大的步长会导致我们提取到的子图像本身，而不是可以有效分类并用于实现我们之前示例中某些目标的单一同质物体。相反，步长过短（比如每个方向上1个像素）可能意味着会有大量候选框。
- en: From the preceding illustration, we can understand that even after hypothetically
    relaxing most of the constraints, we are nowhere close to making a system that
    we can fit in our Smartphones to detect smiling selfies or even bright faces in
    real time (even after an hour in fact). Nor can it have our robots and self-driving
    cars identify objects as they move (and navigate their way by avoiding them).
    This intuition should help us appreciate the advancements in the field of object
    detection and why it is such an impactful area of work.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的示例中，我们可以理解，即使假设放宽大部分约束条件，我们仍然无法制造出可以装进智能手机、实时检测微笑自拍或甚至明亮面孔的系统（实际上即便是一个小时也不行）。我们的机器人和自动驾驶汽车也无法在移动时识别物体（并通过避开它们来导航）。这种直觉应该帮助我们理解物体检测领域的进展，以及为什么这是一个如此具有影响力的工作领域。
- en: Traditional, nonCNN approaches to object detection
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统的非CNN物体检测方法
- en: Libraries such as OpenCV and some others saw rapid inclusion in the software
    bundles for Smartphones, Robotic projects, and many others, to provide detection
    capabilities of specific objects (face, smile, and so on), and Computer Vision
    like benefits, though with some constraints even before the prolific adoption
    of CNN.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 像 OpenCV 这样的库以及其他一些库在智能手机、机器人项目和许多其他软件包中迅速被纳入，以提供特定物体（如面部、微笑等）的检测能力，以及计算机视觉等相关功能，尽管即便是在CNN广泛采用之前，也存在一些约束。
- en: CNN-based research in this area of object detection and Instance Segmentation
    provided many advancements and performance enhancements to this field, not only
    enabling large-scale deployment of these systems but also opening avenues for
    many new solutions. But before we plan to jump into CNN based advancements, it
    will be a good idea to understand how the challenges cited in the earlier section
    were answered to make object detection possible in the first place (even with
    all the constraints), and then we will logically start our discussion about the
    different researchers and the application of CNN to solve other problems that
    still persist with the use of traditional approaches.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的目标检测和实例分割领域的研究为该领域提供了许多进展和性能提升，不仅使这些系统的大规模部署成为可能，还为许多新解决方案开辟了道路。但在我们计划深入探讨基于CNN的进展之前，了解在前一部分中提到的挑战是如何得到解决的，以使得目标检测在各种限制条件下仍能得以实现，将是一个不错的主意。接着，我们将按逻辑开始讨论不同的研究人员以及如何将CNN应用于解决传统方法仍然存在的其他问题。
- en: Haar features, cascading classifiers, and the Viola-Jones algorithm
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Haar特征、级联分类器和Viola-Jones算法
- en: Unlike CNN, or the deepest learning for that matter, which is known for its
    capability of generating higher conceptual features automatically, which in-turn
    gives a major boost to the classifier, in case of traditional machine learning
    applications, such features need to be hand crafted by SMEs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与CNN或深度学习不同，后者以能够自动生成更高层次概念特征而著称，这些特征反过来可以大大提升分类器的性能，在传统的机器学习应用中，这些特征需要由领域专家手工设计。
- en: As we may also understand from our experience working on CPU-based machine learning
    classifiers, their performance is affected by high dimensionality in data and
    the availability of too many features to apply to the model, especially with some
    of the very popular and sophisticated classifiers such as **Support Vector Machines**
    (**SVM**), which used to be considered state-of-the-art until some time ago.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从使用基于CPU的机器学习分类器的经验中也可以理解的那样，它们的性能受数据高维度和可应用于模型的特征数量过多的影响，尤其是对于一些非常流行且复杂的分类器，如**支持向量机**（**SVM**），它曾被认为是最先进的技术，直到不久前。
- en: In this section, we will understand some of the innovative ideas drawing inspirations
    from different fields of science and mathematics that led to the resolution of
    some of the cited challenges above, to fructify the concept of real-time object
    detection in non-CNN systems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解一些创新的想法，这些想法来自不同科学和数学领域的启发，最终解决了上面提到的一些挑战，从而使得非CNN系统中的实时目标检测得以实现。
- en: Haar Features
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Haar特征
- en: Haar or Haar-like features are formations of rectangles with varying pixel density.
    Haar features sum up the pixel intensity in the adjacent rectangular regions at
    specific locations in the detection region. Based on the difference between the
    sums of pixel intensities across regions, they categorize the different subsections
    of the image.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Haar或类似Haar特征是具有不同像素密度的矩形形式。Haar特征通过在检测区域内特定位置的相邻矩形区域中求和像素强度，根据各区域像素强度总和之间的差异，分类图像的不同子区域。
- en: Haar-like features have their name attributed to the mathematics term of Haar
    wavelet, which is a sequence of rescaled square-shaped functions that together
    form a wavelet family or basis.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类Haar特征的名称源自数学术语Haar小波，它是一系列重新缩放的方形函数，这些函数共同形成了一个小波族或基底。
- en: Because Haar-like features work on the difference between pixel intensities
    across regions, they work best with monochrome images. This is also the reason
    the images used earlier and in also this section are monochrome for better intuition.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Haar-like特征是基于区域间像素强度差异工作的，因此它们在单色图像上效果最佳。这也是为什么前面使用的图像以及本节中的图像是单色的，以便更好地直观理解。
- en: 'These categories can be grouped into three major groups, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别可以分为三个主要组别，如下所示：
- en: Two rectangle features
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个矩形特征
- en: Three rectangle features
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个矩形特征
- en: Four rectangle features
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个矩形特征
- en: '![](img/b4eecaef-6d1f-4db9-b0f1-59ca9198ba09.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4eecaef-6d1f-4db9-b0f1-59ca9198ba09.png)'
- en: Haar-like Features
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类Haar特征
- en: With some easy tricks, the computation of varying intensities across the image
    becomes very efficient and can be processed at a very high rate in real time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些简单的技巧，图像中不同区域的像素强度计算变得非常高效，并可以实时以非常高的速度处理。
- en: Cascading classifiers
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 级联分类器
- en: Even if we can extract Haar features from a particular region very quickly,
    it does not solve the problem of extracting such features from a lot of different
    places in the image; this is where the concept of cascading features comes in
    to help. It was observed that only 1 in 10,000 sub-regions turns positive for
    faces in classification, but we have to extract all features and run the whole
    classifier across all regions. Further, it was observed that by using just a few
    of the features (two in the first layer of the cascade), the classifier could
    eliminate a very high proportion of the regions (50% in the first region of the
    cascade). Also, if the sample consists of just these reduced region samples, then
    only slightly more features (10 features in the second layer of the cascade) are
    required for a classifier that can weed out a lot more cases, and so on. So we
    do classification in layers, starting with a classifier that requires very low
    computational power to weed out most of the subregions, gradually increasing the
    computation load required for the remaining subset, and so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们能够非常快速地从特定区域提取Haar特征，这也不能解决从图像中许多不同位置提取这些特征的问题；这时，级联特征的概念就能发挥作用。观察到在分类中，只有1/10,000的子区域会被判定为面部，但我们必须提取所有特征并在所有区域运行整个分类器。进一步观察到，通过仅使用少数几个特征（级联第一层中的两个特征），分类器可以消除大量区域（级联第一层中50%的区域）。此外，如果样本仅包含这些减少的区域样本，那么只需稍多的特征（级联第二层中的10个特征）就能让分类器排除更多的情况，以此类推。因此，我们按层次进行分类，从需要非常低计算能力的分类器开始，逐步增加剩余子集所需的计算负载，依此类推。
- en: The Viola-Jones algorithm
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Viola-Jones算法
- en: In 2001, Paul Viola and Michael Jones proposed a solution that could work well
    to answer some of the preceding challenges, but with some constraints. Though
    it is an almost two decades old algorithm, some of the most popular computer vision
    software to date, or at least till recently, used to embed it in some form or
    another. This fact makes it very important to understand this very simple, yet
    powerful, algorithm before we move on to CNN-based approaches for Region Proposal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在2001年，Paul Viola 和 Michael Jones 提出了一个解决方案，可以很好地应对一些前述挑战，但也有一些约束。尽管这是一个近二十年前的算法，但到目前为止，甚至直到最近，许多流行的计算机视觉软件仍然以某种形式将其嵌入其中。这一事实使得在我们转向基于CNN的区域提议方法之前，理解这个非常简单而强大的算法变得非常重要。
- en: OpenCV, one of the most popular software libraries for computer vision, uses
    cascading classifiers as the predominant mode for object detection, and Haar-featuring-like
    Cascade classifier is very popular with OpenCV. A lot of pretrained Haar classifiers
    are available for this for multiple types of general objects.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 是最流行的计算机视觉软件库之一，使用级联分类器作为物体检测的主要模式，Haar特征类似的级联分类器在OpenCV中非常流行。为此，已有许多预训练的Haar分类器可供使用，涵盖多种类型的常见物体。
- en: This algorithm is not only capable of delivering detections with high **TPRs**
    (**True Positive Rates**) and low **FPRs** (**False Positive Rates**), it can
    also work in real time (process at least two frames per second).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法不仅能够提供高**TPR**（**真正率**）和低**FPR**（**假正率**）的检测结果，还能在实时条件下工作（每秒至少处理两帧）。
- en: High TPR combined with Low FPR is a very important criterion for determining
    the robustness of an algorithm.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 高TPR结合低FPR是确定算法鲁棒性的一个非常重要的标准。
- en: 'The constraints of their proposed algorithm were the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出的算法的约束条件如下：
- en: It could work only for detecting, not recognizing faces (they proposed the algorithm
    for faces, though the same could be used for many other objects).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法仅适用于检测面部，而非识别面部（尽管他们提出的算法是针对面部的，但同样可以用于许多其他物体）。
- en: The faces had to be present in the image as a frontal view. No other view could
    be detected.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部必须出现在图像中并且是正面视角，其他视角无法被检测到。
- en: 'At the heart of this algorithm are the Haar (like) Features and Cascading Classifiers.
    Haar Features are described later in a subsection. The Viola-Jones algorithm uses
    a subset of Haar features to determine general features on a face such as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的核心是Haar（类似）特征和级联分类器。Haar特征将在后面的一个小节中介绍。Viola-Jones算法使用Haar特征的一个子集来确定面部的一般特征，例如：
- en: Eyes (determined by a two-rectangle feature (horizontal), with a dark horizontal
    rectangle above the eye forming the brow, followed by a lighter rectangle below)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 眼睛（通过一个由两个矩形特征（水平）确定，其中一个较暗的水平矩形在眼睛上方形成眉毛，接着是一个较亮的矩形位于下方）
- en: Nose (three-rectangle feature (vertical), with the nose as the center light
    rectangle and one darker rectangle on either side on the nose, forming the temple),
    and so on
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼻子（三个矩形特征（垂直），鼻子中心为浅矩形，两侧各有一个较暗的矩形，形成太阳穴），等等。
- en: These fast-to-extract features can then be used to make a classifier to detect
    (distinguish) faces (from non-faces).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些快速提取的特征可以用于创建一个分类器，以检测（区分）人脸（与非人脸）。
- en: Haar features, with some tricks, are very fast to compute.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Haar特征通过一些技巧，计算速度非常快。
- en: '![](img/8614f6b9-ba3d-4666-9c6a-d7a7540fcdd6.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8614f6b9-ba3d-4666-9c6a-d7a7540fcdd6.jpg)'
- en: Viola-Jones algorithm and Haar-like Features for detecting faces
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Viola-Jones算法和Haar-like特征用于检测人脸
- en: These Haar-like features are then used in the cascading classifiers to expedite
    the detection problem without losing the robustness of detection.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些Haar-like特征随后被用于级联分类器，以加快检测问题的处理速度，同时不失检测的鲁棒性。
- en: The Haar Features and cascading classifiers thus led to some of the very robust,
    effective, and fast individual object detectors of the previous generation. But
    still, the training of these cascades for a new object was very time consuming,
    and they had a lot of constraints, as mentioned before. That is where the new
    generation CNN-based object detectors come to the rescue.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Haar特征和级联分类器促成了前一代一些非常鲁棒、高效且快速的单目标检测器。然而，训练这些级联以适应新的目标仍然非常耗时，并且存在许多限制，正如前面提到的。这就是新一代基于CNN的物体检测器发挥作用的地方。
- en: In this chapter, we have covered only the basis of Haar-Cascades or Haar features
    (in the non-CNN category) as they remained predominant for a long time and were
    the basis of many new types. Readers are encouraged to also explore some of the
    later and much effective SIFT and HOG-based features/cascades (associated papers
    are given in the *References* section).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仅介绍了Haar级联或Haar特征的基础（属于非CNN范畴），因为它们长期占据主导地位，并且是许多新类型的基础。我们鼓励读者也探索一些后来更有效的基于SIFT和HOG的特征/级联（相关论文见*参考文献*部分）。
- en: R-CNN – Regions with CNN features
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R-CNN - 带CNN特征的区域
- en: In the 'Why is object detection much more challenging than image classification?'
    section, we used a non-CNN method to draw region proposals and CNN for classification,
    and we realized that this is not going to work well because the regions generated
    and fed into CNN were not optimal. R-CNN or regions with CNN features, as the
    name suggests, flips that example completely and use CNN to generate features
    that are classified using a (non-CNN) technique called **SVM** (**Support Vector
    Machines**)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在“为什么物体检测比图像分类更具挑战性？”这一节中，我们使用了非CNN方法来生成区域提议，并用CNN进行分类，我们意识到这样做效果不佳，因为生成的区域并没有经过优化，且输入到CNN中的区域并不理想。R-CNN或带CNN特征的区域，顾名思义，完全颠覆了这个示例，使用CNN来生成特征，然后采用一种叫做**支持向量机（SVM）**的非CNN技术进行分类。
- en: 'R-CNN uses the sliding window method (much like we discussed earlier, taking
    some *L x W* and stride) to generate around 2,000 regions of interest, and then
    it converts them into features for classification using CNN. Remember what we
    discussed in the transfer learning chapter—the last flattened layer (before the
    classification or softmax layer) can be extracted to transfer learning from models
    trained on generalistic data, and further train them (often requiring much less
    data as compared to a model with similar performance that has been trained from
    scratch using domain-specific data) to model domain-specific models. R-CNNs also
    use a similar mechanism to improve their effectiveness on specific object detection:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN使用滑动窗口方法（就像我们之前讨论的那样，选择一些*L x W*的窗口和步幅）生成约2000个感兴趣区域，然后将它们转换为CNN特征进行分类。记得我们在迁移学习章节中讨论的内容——最后一层平展层（分类或Softmax层之前）可以提取出来，进行迁移学习，从在通用数据上训练的模型中进行学习，并进一步训练它们（与使用领域特定数据从头开始训练的相似性能模型相比，通常需要的数据量要少得多），从而构建领域特定的模型。R-CNN也使用类似的机制来提高其在特定目标检测上的效果：
- en: '![](img/6173be31-0eb7-4bce-9345-27e63b442d91.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6173be31-0eb7-4bce-9345-27e63b442d91.png)'
- en: R-CNN – Working
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN - 工作原理
- en: The original paper on R-CNN claims that on a PASCAL VOC 2012 dataset, it has
    improved the **mean average precision** (**mAP**) by more than 30% relative to
    the previous best result on that data while achieving a mAP of 53.3%.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN原始论文声称，在PASCAL VOC 2012数据集上，它相较于以前的最佳结果提高了**平均精度（mAP）**超过30%，并且达到了53.3%的mAP。
- en: We saw very high precision figures for the image classification exercise (using
    CNN) over the ImageNet data. Do not use that figure with the comparison statistics
    given here, as not only are the datasets used different (and hence not comparable),
    but also the tasks in hand (classification versus object detection) are quite
    different, and object detection is much more challenging a task than image classification.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在对ImageNet数据集进行图像分类练习（使用CNN）时，得到了非常高精度的结果。不要将该结果与此处给出的对比统计数据一起使用，因为使用的数据集不仅不同（因此无法进行比较），而且任务本身（分类与目标检测）也完全不同，目标检测比图像分类要更具挑战性。
- en: 'PASCAL **VOC** (**Visual Object Challenge**): Every area of research requires
    some sort of standardized dataset and standard KPIs to compare results across
    different studies and algorithms. Imagenet, the dataset we used for image classification,
    cannot be used as a standardized dataset for object detection, as object-detection
    requires (train, test, and validation set) data labeled with not only the object
    class but also its position. ImageNet does not provide this. Therefore, in most
    object detection studies, we may see the use of a standardized object-detection
    dataset, such as PASCAL VOC. The PASCAL VOC dataset has 4 variants so far, VOC2007,
    VOC2009, VOC2010, and VOC2012\. VOC2012 is the latest (and richest) of them all.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL **VOC**（**视觉目标挑战**）：每个研究领域都需要某种标准化的数据集和标准的KPI，以便在不同的研究和算法之间进行结果比较。我们用于图像分类的数据集ImageNet不能作为目标检测的标准化数据集，因为目标检测不仅需要对对象类别进行标注，还需要标注对象的位置。ImageNet并未提供这种数据。因此，在大多数目标检测研究中，我们可能会看到使用标准化的目标检测数据集，如PASCAL
    VOC。PASCAL VOC数据集目前有4个版本：VOC2007、VOC2009、VOC2010和VOC2012。VOC2012是其中最新（也是最丰富）的版本。
- en: Another place we stumbled at was the differing scales (and location) of the
    regions of interest, *recognition using region*. This is what is called the **localization**
    challenge; it is solved in R-CNN by using a varying range of receptive fields,
    starting from as high a region with 195 x 195 pixels and 32 x 32 strides, to lesser
    downwards.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们遇到的难点是兴趣区域的不同尺度（和位置），*使用区域进行识别*。这就是所谓的**定位**挑战；在R-CNN中，它通过使用不同范围的感受野来解决这个问题，从高达195
    x 195像素和32 x 32步长的区域开始，到较小的区域逐渐减小。
- en: This approach is called **recognition using region**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**使用区域进行识别**。
- en: Wait a minute! Does that ring a bell? We said that we will use CNN to generate
    features from this region, but CNN uses a constant-size input to produce a fixed-size
    flattened layer. We do require fixed-size features (flattened vector size) as
    input to our SVMs, but here the input region size is changing. So how does that
    work? R-CNN uses a popular technique called **Affine Image Warping** to compute
    a fixed-size CNN input from each region proposal, regardless of the region's shape.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下！这是不是让你想起了什么？我们曾说过将使用CNN从这个区域生成特征，但CNN使用的是固定大小的输入来生成固定大小的平坦层。我们确实需要固定大小的特征（扁平化的向量大小）作为SVM的输入，但这里输入区域的大小是变化的。那么这如何实现呢？R-CNN使用了一种流行的技术，叫做**仿射图像变换**，通过这种技术，可以从每个区域提议中计算出固定大小的CNN输入，无论区域的形状如何。
- en: In geometry, an affine transformation is the name given to a transformation
    function between affine spaces that preserves points, straight lines, and planes.
    Affine spaces are structures that generalize the properties of Euclidian spaces
    while preserving only the properties related to parallelism and respective scale.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在几何学中，仿射变换是指在仿射空间之间的变换函数，这种变换保持点、直线和平面的不变。仿射空间是一种结构，它在保留与平行性和相应尺度相关的性质的同时，推广了欧几里得空间的性质。
- en: Besides the challenges that we have covered, there exists another challenge
    that is worth mentioning. The candidate regions that we generated in the first
    step (on which we performed classification in the second step) were not very accurate,
    or they were lacking tight boundaries around the object identified. So we include
    a third stage in this method, which improves the accuracy of the bounding boxes
    by running a regression function (called **bounding-box regressors**) to identify
    the boundaries of separation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们已经讨论过的挑战，还有一个值得提到的挑战。我们在第一步中生成的候选区域（在第二步中进行分类）并不非常准确，或者它们缺乏围绕识别对象的紧密边界。因此，我们在这种方法中加入了第三阶段，通过运行回归函数（称为**边界框回归器**）来提高边界框的准确性，以识别分隔的边界。
- en: R-CNN proved to be very successful when compared to the earlier end-to-end non-CNN
    approaches. But it uses CNN only for converting regions to features. As we understand,
    CNNs are very powerful for image classifications as well, but because our CNN
    will work only on input region images and not on flattened region features, we
    cannot use it here directly. In the next section, we will see how to overcome
    this obstacle.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期的端到端非CNN方法相比，R-CNN证明非常成功。但它仅使用CNN将区域转换为特征。如我们所知，CNN在图像分类中也非常强大，但由于我们的CNN仅对输入的区域图像工作，而不是对展平后的区域特征进行操作，因此无法直接使用它。在下一节中，我们将看到如何克服这一障碍。
- en: R-CNN is very important to cover from the perspective of understanding the background
    use of CNN in object detection as it has been a giant leap from all non-CNN-based
    approaches. But because of further improvements in CNN-based object detection,
    as we will discuss next, R-CNN is not actively worked upon now and the code is
    not maintained any longer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从理解CNN在目标检测中的背景使用角度来看，R-CNN非常重要，因为它是从所有非CNN方法中迈出的巨大一步。但由于CNN在目标检测中的进一步改进，正如我们接下来会讨论的那样，R-CNN现在不再被积极开发，代码也不再维护。
- en: Fast R-CNN – fast region-based CNN
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fast R-CNN – 快速区域卷积神经网络
- en: 'Fast R-CNN, or Fast Region-based CNN method, is an improvement over the previously
    covered R-CNN. To be precise about the improvement statistics, as compared to
    R-CNN, it is:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN，或称快速区域卷积神经网络方法，是对先前的R-CNN的改进。具体来说，与R-CNN相比，它的改进统计数据如下：
- en: 9x faster in training
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度提升9倍
- en: 213x faster at scoring/servicing/testing (0.3s per image processing), ignoring
    the time spent on region proposals
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评分/服务/测试时快213倍（每张图片处理0.3秒），不包括区域提议所花费的时间
- en: Has higher mAP of 66% on the PASCAL VOC 2012 dataset
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PASCAL VOC 2012数据集上具有更高的mAP，达66%
- en: 'Where R-CNN uses a smaller (five-layer) CNN, Fast R-CNN uses the deeper VGG16
    network, which accounts for its improved accuracy. Also, R-CNN is slow because
    it performs a ConvNet forward pass for each object proposal without sharing computation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在R-CNN使用较小的（五层）CNN时，Fast R-CNN使用更深的VGG16网络，这也提高了其准确性。此外，R-CNN之所以慢，是因为它对每个对象提议执行一次卷积神经网络的前向传播，而没有共享计算：
- en: '![](img/a12c5041-0ca5-492b-8a7f-12be6edd2eb9.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a12c5041-0ca5-492b-8a7f-12be6edd2eb9.png)'
- en: 'Fast R-CNN: Working'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN：工作原理
- en: 'In Fast R-CNN, the deep VGG16 CNN provides essential computations for all the
    stages, namely:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fast R-CNN中，深度VGG16 CNN为所有阶段提供了必要的计算，即：
- en: '**Region of Interest** (**RoI**) computation'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感兴趣区域**（**RoI**）计算'
- en: Classification Objects (or background) for the region contents
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对区域内容进行分类（对象或背景）
- en: Regression for enhancing the bounding box
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归以增强边界框
- en: The input to the CNN, in this case, is not raw (candidate) regions from the
    image, but the (complete) actual image itself; the output is not the last flattened
    layer but the convolution (map) layer before that. From the so-generated convolution
    map, a the RoI pooling layer (a variant of max-pooling) is used to generate the
    flattened fixed-length RoI corresponding to each object proposal are generated,
    which are then passed through some **fully connected** (**FC**) layers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CNN的输入不是来自图像的原始（候选）区域，而是完整的实际图像；输出不是最后的展平层，而是之前的卷积（映射）层。从生成的卷积映射中，使用RoI池化层（最大池化的变体）来生成对应每个目标提议的展平固定长度RoI，这些RoI随后会通过一些**全连接**（**FC**）层。
- en: The RoI pooling is a variant of max pooling (that we used in our initial chapters
    in this book), in which output size is fixed and input rectangle is a parameter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: RoI池化是最大池化的一种变体（我们在本书的初始章节中使用过），其中输出大小是固定的，输入矩形是一个参数。
- en: The RoI pooling layer uses max pooling to convert the features inside any valid
    region of interest into a small feature map with a fixed spatial extent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: RoI池化层使用最大池化，将任何有效感兴趣区域中的特征转换为一个具有固定空间范围的小特征图。
- en: 'The output from the penultimate FC layer is then used for both:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 来自倒数第二个全连接层的输出将用于以下两项：
- en: Classification (SoftMax layer) with as many classes as object proposals, +1
    additional class for the background (none of the classes found in the region)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类（SoftMax层），类别数与目标提议的数量相同，额外+1个类别用于背景（区域中未找到的任何类别）
- en: Sets of regressors that produce the four numbers (two numbers denoting the x,
    y coordinates of the upper-left corner for the box for that object, and the next
    two numbers corresponding to the height and width of that object found in that
    region) for each object-proposal that is required to make bounding boxes precise
    for that particular object
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组回归器，产生四个数字（两个数字表示该物体框的左上角的x、y坐标，接下来的两个数字对应于该区域内物体的高度和宽度），这些数字对于每个物体提议都是必需的，以便为该物体提供精确的边界框。
- en: The result achieved with Fast R-CNN is great. What is even greater is the use
    of a powerful CNN network to provide very effective features for all three challenges
    that we need to overcome. But there are still some drawbacks, and there is scope
    for further improvements as we will understand in our next section on Faster R-CNN.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Fast R-CNN所取得的结果非常出色。更为出色的是，利用强大的CNN网络为我们需要克服的所有三个挑战提供了非常有效的特征。但仍然存在一些缺点，正如我们在下一节关于Faster
    R-CNN的内容中将了解的那样，仍然有进一步改进的空间。
- en: Faster R-CNN – faster region proposal network-based CNN
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN – 基于更快区域提议网络的CNN
- en: We saw in the earlier section that Fast R-CNN brought down the time required
    for scoring (testing) images drastically, but the reduction ignored the time required
    for generating Region Proposals, which use a separate mechanism (though pulling
    from the convolution map from CNN) and continue proving a bottleneck. Also, we
    observed that though all three challenges were resolved using the common features
    from convolution-map in Fast R-CNN, they were using different mechanisms/models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中看到，Fast R-CNN大幅减少了评分（测试）图像所需的时间，但这种减少忽略了生成区域提议所需的时间，这一过程使用了一个独立的机制（尽管是从CNN的卷积图中提取的），并继续形成瓶颈。此外，我们观察到，尽管所有三个挑战在Fast
    R-CNN中都使用了来自卷积图的共同特征来解决，但它们使用了不同的机制/模型。
- en: Faster R-CNN improves upon these drawbacks and proposes the concept of **Region
    Proposal Networks** (**RPNs**), bringing down the scoring (testing) time to 0.2
    seconds per image, even including time for Region Proposals.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN改进了这些缺点，并提出了**区域提议网络**（**RPNs**）的概念，将评分（测试）时间减少到每张图像0.2秒，即使包括了区域提议的时间。
- en: Fast R-CNN was doing the scoring (testing) in 0.3 seconds per image, that too
    excluding the time required for the process equivalent to Region Proposal.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN在每张图像的评分（测试）上用了0.3秒，这还不包括区域提议过程所需的时间。
- en: '![](img/f947c085-62de-43b0-bde7-f7a27bb31127.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f947c085-62de-43b0-bde7-f7a27bb31127.png)'
- en: 'Faster R-CNN: Working - The Region Proposal Networking acting as Attention
    Mechanism'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'Faster R-CNN: 工作原理 - 区域提议网络作为注意力机制'
- en: 'As shown in the earlier figure, a VGG16 (or another) CNN works directly on
    the image, producing a convolutional map (similar to what was done in Fast R-CNN).
    Things differ from here, where now there are two branches, one feeding into the
    RPN and the other into the detection Network. This is again an extension of the
    same CNN for prediction, leading to a **Fully Convolutional Network** (**FCN**).
    The RPN acts as an Attention Mechanism and also shares full-image convolutional
    features with the detection network. Also, now because all the parts in the network
    can use efficient GPU-based computation, it thus reduces the overall time required:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，VGG16（或其他）CNN直接作用于图像，生成一个卷积图（类似于在Fast R-CNN中所做的）。从这里开始有所不同，现在有两个分支，一个进入RPN，另一个进入检测网络。这再次是相同CNN的扩展，用于预测，形成了**全卷积网络**（**FCN**）。RPN作为注意力机制并且与检测网络共享完整图像的卷积特征。此外，现在由于网络中的所有部分都可以使用高效的基于GPU的计算，因此减少了总体所需的时间：
- en: '![](img/1600a405-9d97-4b14-aeaa-b7b322fb5d71.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1600a405-9d97-4b14-aeaa-b7b322fb5d71.jpg)'
- en: 'Faster R-CNN: Working - The Region Proposal Networking acting as Attention
    Mechanism'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'Faster R-CNN: 工作原理 - 区域提议网络作为注意力机制'
- en: For a greater understanding of the Attention Mechanism, refer to the chapter
    on Attention Mechanisms for CNN in this book.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解注意力机制，请参考本书中关于CNN的注意力机制章节。
- en: The RPN works in a sliding window mechanism, where a window slides (much like
    CNN filters) across the last convolution map from the shared convolutional layer.
    With each slide, the sliding window produces *k (k=N[Scale] × N[Size])* number
    of Anchor Boxes (similar to Candidate Boxes), where *N[Scale]* is the number of
    (pyramid like) scales per *size* of the *N[Size ]*sized (aspect ratio) box extracted
    from the center of the sliding window, much like the following figure.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: RPN通过滑动窗口机制工作，其中一个窗口（类似CNN滤波器）在共享卷积层的最后一个卷积图上滑动。每次滑动时，滑动窗口会产生*k (k=N[Scale]
    × N[Size])*个锚框（类似候选框），其中*N[Scale]*是每个*size*的*N[Size]*大小（长宽比）框的尺度数，这些框从滑动窗口的中心提取，就像下图所示。
- en: The RPN leads into a flattened, FC layer. This, in turn, leads into two networks,
    one for predicting the four numbers for each of the *k* boxes (determining the
    coordinates, length and width of the box as in Fast R-CNN), and another into a
    binomial classification model that determines the objectness or probability of
    finding any of the given objects in that box. The output from the RPN leads into
    the detection network, which detects which particular class of object is in each
    of the k boxes given the position of the box and its objectness.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: RPN输出进入一个展平的全连接（FC）层。然后，输出进入两个网络，一个用于预测每个*k*框的四个数字（确定框的坐标、长宽，如同Fast R-CNN中一样），另一个进入一个二项分类模型，确定该框内是否包含目标物体的可能性。来自RPN的输出进入检测网络，检测每个k框中所包含的具体物体类别，给定框的位置及其物体性。
- en: '![](img/ce65a2bc-2168-4e83-be93-176926aef7e0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce65a2bc-2168-4e83-be93-176926aef7e0.png)'
- en: 'Faster R-CNN: Working - extracting different scales and sizes'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN：工作原理 - 提取不同尺度和大小
- en: One problem in this architecture is the training of the two networks, namely
    the Region Proposal and detection network. We learned that CNN is trained using backpropagating
    across all layers while reducing the losses layers with every iteration. But because
    of the split into two different networks, we could at a time backpropagate across
    only one network. To resolve this issue, the training is done iteratively across
    each network, while keeping the weights of the other network constant. This helps
    in converging both the networks quickly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构中的一个问题是两个网络的训练，分别是区域提议网络（Region Proposal）和检测网络。我们了解到，CNN是通过反向传播训练的，反向传播遍历所有层，并在每次迭代时减少每层的损失。但由于架构分成了两个不同的网络，我们每次只能对一个网络进行反向传播。为了解决这个问题，训练是通过在每个网络中迭代进行的，同时保持另一个网络的权重不变。这有助于两个网络快速收敛。
- en: An important feature of the RPN architecture is that it has translation invariance
    with respect to both the functions, one that is producing the anchors, and another
    that is producing the attributes (its coordinate and objectness) for the anchors.
    Because of translation invariance, a reverse operation, or producing the portion
    of the image given a vector map of an anchor map is feasible.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RPN架构的一个重要特性是，它对两个函数具有平移不变性，一个生成锚点，另一个为锚点生成属性（其坐标和物体性）。由于平移不变性，反向操作或根据锚点图的向量图生成图像的部分是可行的。
- en: Owing to Translational Invariance, we can move in either direction in a CNN,
    that is from image to (region) proposals, and from the proposals to the corresponding
    portion of the image.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于平移不变性，我们可以在CNN中任意方向移动，即从图像到（区域）提议，从提议到图像的相应部分。
- en: Mask R-CNN – Instance segmentation with CNN
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mask R-CNN - 使用CNN进行实例分割
- en: Faster R-CNN is state-of-the-art stuff in object detection today. But there
    are problems overlapping the area of object detection that Faster R-CNN cannot
    solve effectively, which is where Mask R-CNN, an evolution of Faster R-CNN can
    help.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN是目前目标检测领域的最先进技术。但在目标检测的相关领域，Faster R-CNN无法有效解决一些问题，这就是Mask R-CNN——Faster
    R-CNN的进化版本——能够提供帮助的地方。
- en: This section introduces the concept of instance segmentation, which is a combination
    of the standard object detection problem as described in this chapter, and the
    challenge of semantic segmentation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了实例分割的概念，它结合了本章描述的标准目标检测问题与语义分割的挑战。
- en: In semantic segmentation, as applied to images, the goal is to classify each
    pixel into a fixed set of categories without differentiating object instances.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义分割中，应用于图像时，目标是将每个像素分类到一个固定的类别集中，而不区分物体实例。
- en: Remember our example of counting the number of dogs in the image in the intuition
    section? We were able to count the number of dogs easily, because they were very
    much apart, with no overlap, so essentially just counting the number of objects
    did the job. Now, take the following image, for instance, and count the number
    of tomatoes using object detection. It will be a daunting task because the Bounding
    Boxes will have so much of an overlap that it will be difficult to distinguish
    the Instances of tomatoes from the boxes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们在直觉部分中提到的计算图像中狗的数量的例子吗？我们能够轻松地数出狗的数量，因为它们彼此相隔很远，没有重叠，因此基本上只需数对象的数量即可完成任务。现在，以以下这张图片为例，使用目标检测数番茄的数量。这将是一项艰巨的任务，因为边界框（Bounding
    Boxes）重叠严重，很难区分番茄实例与框的关系。
- en: So, essentially, we need to go further, beyond bounding boxes and into pixels
    to get that level separation and identification. Like we use to classify bounding
    boxes with object names in object detection, in Instance Segment, we segment/
    classify, each pixel with not only the specific object name but also the object-instance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，本质上，我们需要进一步深入，超越边界框，进入像素层面，以便获得这种级别的分离和识别。就像我们在目标检测中用物体名称来分类边界框一样，在实例分割中，我们对每个像素进行分割/分类，不仅标出具体的物体名称，还要标出物体实例。
- en: The object detection and Instance Segmentation could be treated as two different
    tasks, one logically leading to another, much like we discovered the tasks of
    finding Region Proposals and Classification in the case of object detection. But
    as in the case of object detection, and especially with techniques like Fast/Faster
    R-CNN, we discovered that it would be much effective if we have a mechanism to
    do them simultaneously, while also utilizing much of the computation and network
    to do so, to make the tasks seamless.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测和实例分割可以被视为两个不同的任务，一个逻辑上引导另一个，正如我们在目标检测中发现的那样，任务是查找区域提议和分类。但是，正如在目标检测中，尤其是使用像
    Fast/Faster R-CNN 这样的技术时，我们发现如果能够同时进行这些任务，同时还能够利用大量计算和网络资源来完成任务，这将更加高效，从而使这些任务无缝衔接。
- en: '![](img/d580f08c-98db-4d19-9f25-fcffec41326e.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d580f08c-98db-4d19-9f25-fcffec41326e.jpeg)'
- en: Instance Segmentation – Intuition
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割 – 直觉
- en: Mask R-CNN is an extension of Faster R-CNN covered in the earlier network, and
    uses all the techniques used in Faster R-CNN, with one addition—an additional
    path in the network to generate a Segmentation Mask (or Object Mask) for each
    detected Object Instance in parallel. Also, because of this approach of using
    most of the existing network, it adds only a minimal overhead to the entire processing
    and has a scoring (test) time almost equivalent to that of Faster R-CNN. It has
    one of the best accuracies across all single-model solutions as applied to the
    COCO2016 challenge (using the COCO2015 dataset).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 是 Faster R-CNN 的一种扩展，前者在之前的网络中已经覆盖，并且使用了 Faster R-CNN 中的所有技术，唯一的新增部分是——在网络中增加了一个额外的路径，用于并行生成每个检测到的对象实例的分割掩码（或对象掩码）。此外，由于这种方法主要利用现有网络，因此它对整个处理过程仅增加了最小的开销，并且其评分（测试）时间几乎等同于
    Faster R-CNN。它在所有单模型解决方案中，尤其是在应用于 COCO2016 挑战（使用 COCO2015 数据集）时，具有最好的准确度之一。
- en: Like, PASCAL VOC, COCO is another large-scale standard (series of) dataset (from
    Microsoft). Besides object detection, COCO is also used for segmentation and captioning.
    COCO is more extensive than many other datasets and much of the recent comparison
    on object detection is done on this for comparison purposes. The COCO dataset
    comes in three variants, namely COCO 2014, COCO 2015, and COCO 2017.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 PASCAL VOC，COCO 是另一个大规模的标准数据集（由微软提供）。除了目标检测，COCO 还用于分割和图像描述。COCO 比许多其他数据集更为广泛，最近在目标检测方面的很多比较都是基于
    COCO 数据集进行的。COCO 数据集有三个版本，分别是 COCO 2014、COCO 2015 和 COCO 2017。
- en: In Mask R-CNN, besides having the two branches that generate the objectness
    and localization for each anchor box or RoI, there also exists a third FCN that
    takes in the RoI and predicts a segmentation mask in a pixel-to-pixel manner for
    the given anchor box.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mask R-CNN 中，除了有两个分支分别生成每个锚框或 RoI 的目标性（objectness）和定位信息外，还有一个第三个全卷积网络（FCN），它接受
    RoI 并以逐像素的方式为给定的锚框预测一个分割掩码。
- en: 'But there still remain some challenges. Though Faster R-CNN does demonstrate
    transformational invariance (that is, we could trace from the convolutional map
    of the RPN to the pixel map of the actual image), the convolutional map has a
    different structure from that of the actual image pixels. So, there is no pixel-to-pixel
    alignment between network inputs and outputs, which is important for our purpose
    of providing pixel-to-pixel masking using this network. To solve this challenge,
    Mask R-CNN uses a quantization-free layer (named RoIAlign in the original paper)
    that helps align the exact spatial locations. This layer not only provides exact
    alignment but also helps in improving the accuracy to a great extent, because
    of which Mask R-CNN is able to outperform many other networks:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 但是仍然存在一些挑战。尽管 Faster R-CNN 确实展示了变换不变性（也就是说，我们可以从 RPN 的卷积图追踪到实际图像的像素图），但卷积图的结构与实际图像像素的结构不同。因此，网络输入和输出之间没有像素级的对齐，这对于我们通过该网络提供像素到像素的遮罩非常重要。为了解决这个问题，Mask
    R-CNN 使用了一个无量化层（在原文中称为 RoIAlign），它有助于对齐精确的空间位置。这个层不仅提供了精确的对齐，还大大提高了精度，因此 Mask
    R-CNN 能够超越许多其他网络：
- en: '![](img/50bfd31a-acb9-4003-8b02-bcdf236793f3.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50bfd31a-acb9-4003-8b02-bcdf236793f3.jpg)'
- en: Mask R-CNN – Instance Segmentation Mask (illustrative output)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN – 实例分割遮罩（示例输出）
- en: The concept of instance segmentation is very powerful and can lead to realizing
    a lot of very impactful use cases that were not possible with object detection
    alone.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割的概念非常强大，它能够实现很多使用物体检测单独无法完成的有影响力的应用场景。
- en: We can even use instance segmentation to estimate human poses in the same framework
    and eliminate them.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用实例分割来估计同一框架中的人体姿势并将其消除。
- en: Instance segmentation in code
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码中的实例分割
- en: It's now time to put the things that we've learned into practice. We'll use
    the COCO dataset and its API for the data, and use Facebook Research's Detectron
    project (link in References), which provides the Python implementation of many
    of the previously discussed techniques under an Apache 2.0 license. The code works
    with Python2 and Caffe2, so we'll need a virtual environment with the given configuration.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将我们学到的内容付诸实践了。我们将使用 COCO 数据集及其 API 来获取数据，并使用 Facebook Research 的 Detectron
    项目（链接见参考文献），该项目提供了许多前面讨论的技术的 Python 实现，遵循 Apache 2.0 许可协议。该代码适用于 Python2 和 Caffe2，因此我们需要一个带有指定配置的虚拟环境。
- en: Creating the environment
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建环境
- en: The virtual environment, with Caffe2 installation, can be created as per the
    `caffe2` installation instructions on the Caffe2 repository link in the *References*
    Section. Next, we will install the dependencies.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按照 *References* 部分中 Caffe2 仓库链接中的 `caffe2` 安装说明来创建带有 Caffe2 安装的虚拟环境。接下来，我们将安装依赖。
- en: Installing Python dependencies (Python2 environment)
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Python 依赖（Python2 环境）
- en: 'We can install the Python dependencies as shown in the following code block:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下代码块中的方式安装 Python 依赖：
- en: Python 2X and Python 3X are two different flavors of Python (or more precisely
    CPython), and not a conventional upgrade of version, therefore the libraries for
    one variant might not be compatible with another. Use Python 2X for this section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Python 2X 和 Python 3X 是 Python 的两种不同版本（或者更准确地说是 CPython），并不是一个传统意义上的版本升级，因此一个版本的库可能与另一个版本不兼容。在这一部分中，请使用
    Python 2X。
- en: '*When we refer to the (interpreted) programming language Python, we need to
    refer to it with the specific interpreter (since it is an interpreted language
    as opposed to a compiled one like Java). The interpreter that we implicitly refer
    to as the Python interpreter (like the one you download from Python.org or the
    one that comes bundled with Anaconda) is technically called CPython, on which
    is the default byte-code interpreter of Python, which is written in C. But there
    are other Python interpreters also like Jython (build on Java), PyPy (written
    in Python itself - not so intuitive, right?), IronPython (.NET implementation
    of Python). *'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我们提到（解释型的）编程语言 Python 时，我们需要使用特定的解释器（因为它是解释型语言，而不是像 Java 那样的编译型语言）。我们通常所提到的
    Python 解释器（例如从 Python.org 下载的版本或与 Anaconda 捆绑的版本）技术上叫做 CPython，它是 Python 的默认字节码解释器，使用
    C 语言编写。但是，也有其他 Python 解释器，例如 Jython（基于 Java 构建）、PyPy（用 Python 本身编写——有点不直观吧？）、IronPython（.NET
    实现的 Python）。*'
- en: '[PRE0]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Downloading and installing the COCO API and detectron library (OS shell commands)
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并安装 COCO API 和 detectron 库（操作系统命令行命令）
- en: 'We will then download and  install the Python dependencies as shown in the
    following code block:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将下载并安装Python依赖项，如以下代码块所示：
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Alternatively, we can download and use the Docker image of the environment
    (requires Nvidia GPU support):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以下载并使用该环境的Docker镜像（需要Nvidia GPU支持）：
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Preparing the COCO dataset folder structure
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备COCO数据集文件夹结构
- en: 'Now we will see the code to prepare the COCO dataset folder structure as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到准备COCO数据集文件夹结构的代码，如下所示：
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running the pre-trained model on the COCO dataset
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在COCO数据集上运行预训练模型
- en: 'We can now implement the pre-trained model on the COCO dataset as shown in
    the following code snippet:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在COCO数据集上实现预训练模型，如以下代码片段所示：
- en: '[PRE4]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: References
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Paul Viola and Michael Jones, Rapid object detection using a boosted cascade
    of simple features, *Conference on Computer Vision and Pattern Recognition*, 2001.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Paul Viola 和 Michael Jones，*《使用增强级联简单特征进行快速目标检测》*，计算机视觉与模式识别会议，2001年。
- en: Paul Viola and Michael Jones, Robust Real-time object detection, *International
    Journal of Computer Vision*, 2001.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Paul Viola 和 Michael Jones，*《鲁棒的实时目标检测》*，国际计算机视觉杂志，2001年。
- en: Itseez2015opencv, OpenCV, *Open Source Computer Vision Library*, Itseez, 2015.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Itseez2015opencv，OpenCV，*《开源计算机视觉库》*，Itseez，2015年。
- en: Ross B. Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, *Rich feature
    hierarchies for accurate object detection and semantic segmentation*, CoRR, arXiv:1311.2524,
    2013.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ross B. Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik，*《准确目标检测和语义分割的丰富特征层次》*，CoRR，arXiv:1311.2524，2013年。
- en: Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, *Rich feature hierarchies
    for accurate object detection and semantic segmentation*, Computer Vision and
    Pattern Recognition, 2014.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik，*《准确目标检测和语义分割的丰富特征层次》*，计算机视觉与模式识别，2014年。
- en: M. Everingham, L. VanGool, C. K. I. Williams, J. Winn, A. Zisserman, *The PASCAL
    Visual Object Classes Challenge 2012*, VOC2012, Results.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Everingham, L. VanGool, C. K. I. Williams, J. Winn, A. Zisserman，*《PASCAL视觉目标类别挑战赛2012》*，VOC2012，结果。
- en: D. Lowe. *Distinctive image features from scale-invariant keypoints*, IJCV,
    2004.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Lowe，*《基于尺度不变关键点的独特图像特征》*，IJCV，2004年。
- en: N. Dalal and B. Triggs. *Histograms of oriented gradients for human detection*.
    In CVPR, 2005.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N. Dalal 和 B. Triggs，*《用于人类检测的方向梯度直方图》*，CVPR，2005年。
- en: Ross B. Girshick, Fast R-CNN, CoRR, arXiv:1504.08083, 2015.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ross B. Girshick，Fast R-CNN，CoRR，arXiv:1504.08083，2015年。
- en: Rbgirshick, fast-rcnn, GitHub, [https://github.com/rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn),
    Feb-2018.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rbgirshick，fast-rcnn，GitHub，[https://github.com/rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn)，2018年2月。
- en: Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun, Faster R-CNN: *Towards
    Real-Time Object Detection with Region Proposal Networks*, CoRR, arXiv:1506.01497,
    2015.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun, Faster R-CNN: *《基于区域提议网络的实时目标检测》*，CoRR，arXiv:1506.01497，2015年。'
- en: Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun, Faster R-CNN: *Towards
    Real-Time Object Detection with Region Proposal Networks*, Advances in **Neural
    Information Processing Systems** (**NIPS**), 2015.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shaoqing Ren 和 Kaiming He 和 Ross Girshick 和 Jian Sun，Faster R-CNN：*《基于区域提议网络的实时目标检测》*，**神经信息处理系统**
    (**NIPS**)，2015年。
- en: Rbgirshick, py-faster-rcnn, GitHub, [https://github.com/rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn),
    Feb-2018.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rbgirshick，py-faster-rcnn，GitHub，[https://github.com/rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)，2018年2月。
- en: Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollar, Kaiming He,
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ross Girshick，Ilija Radosavovic，Georgia Gkioxari，Piotr Dollar，Kaiming He，
- en: Detectron, GitHub, [https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron),
    Feb-2018.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Detectron，GitHub，[https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron)，2018年2月。
- en: 'Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B.
    Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, C. Lawrence Zitnick, *Microsoft
    COCO: Common Objects in Context*, CoRR, arXiv:1405.0312, 2014.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B.
    Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, C. Lawrence Zitnick，*《Microsoft
    COCO：上下文中的常见物体》*，CoRR，arXiv:1405.0312，2014年。
- en: Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross B. Girshick, Mask R-CNN, CoRR,
    arXiv:1703.06870, 2017.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross B. Girshick，Mask R-CNN，CoRR，arXiv:1703.06870，2017年。
- en: Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng
    Wang, Hartwig Adam, MaskLab: *Instance Segmentation by Refining Object Detection
    with Semantic and Direction Features*, CoRR, arXiv:1712.04837, 2017.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng
    Wang, Hartwig Adam, MaskLab: *通过语义和方向特征优化目标检测的实例分割*，CoRR，arXiv:1712.04837，2017。'
- en: Anurag Arnab, Philip H. S. Torr, *Pixelwise Instance Segmentation with a Dynamically
    Instantiated Network*, CoRR, arXiv:1704.02386, 2017.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Anurag Arnab, Philip H. S. Torr，*使用动态实例化网络的像素级实例分割*，CoRR，arXiv:1704.02386，2017。
- en: Matterport, Mask_RCNN, GitHub, [https://github.com/matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN),
    Feb-2018.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Matterport，Mask_RCNN，GitHub，[https://github.com/matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN)，2018年2月。
- en: CharlesShang, FastMaskRCNN, GitHub, [https://github.com/CharlesShang/FastMaskRCNN](https://github.com/CharlesShang/FastMaskRCNN),
    Feb-2018.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CharlesShang, FastMaskRCNN，GitHub，[https://github.com/CharlesShang/FastMaskRCNN](https://github.com/CharlesShang/FastMaskRCNN)，2018年2月。
- en: Caffe2, Caffe2, GitHub, [https://github.com/caffe2/caffe2](https://github.com/caffe2/caffe2),
    Feb-2018.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Caffe2，Caffe2，GitHub，[https://github.com/caffe2/caffe2](https://github.com/caffe2/caffe2)，2018年2月。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started from the very simple intuition behind the task of
    object detection and then proceeded to very advanced concepts, such as Instance
    Segmentation, which is a contemporary research area. Object detection is at the
    heart of a lot of innovation in the field of Retail, Media, Social Media, Mobility,
    and Security; there is a lot of potential for using these technologies to create
    very impactful and profitable features for both enterprise and social consumption.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从目标检测任务背后的简单直觉入手，然后逐步介绍了更为先进的概念，例如实例分割，这是当今的研究热点。目标检测在零售、媒体、社交媒体、移动性和安全领域的创新中占据核心地位；这些技术具有巨大的潜力，可以为企业和社会消费创造具有深远影响和盈利潜力的功能。
- en: From the Algorithms perspective, this chapter started with the legendary Viola-Jones
    algorithm and its underlying mechanisms, such as Haar Features and Cascading Classifiers.
    Using that intuition, we started exploring the world of CNN for object detection
    with algorithms, such as R-CNN, Fast R-CNN, up to the very state-of-the-art Faster
    R-CNN.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从算法的角度来看，本章从传奇的Viola-Jones算法及其底层机制开始，诸如Haar特征和级联分类器。基于这些直觉，我们开始探索卷积神经网络（CNN）在目标检测中的应用，涉及的算法包括R-CNN、Fast
    R-CNN，一直到最先进的Faster R-CNN。
- en: In this chapter, we also laid the foundations and introduced a very recent and
    impactful field of research called **instance segmentation**. We also covered
    some state-of-the-art Deep CNNs based on methods, such as Mask R-CNN, for easy
    and performant implementation of instance segmentation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还奠定了基础，并介绍了一个非常新颖且具有深远影响的研究领域——**实例分割**。我们还讨论了基于Mask R-CNN等方法的先进深度CNN，用于实现实例分割的简便且高效的实现。
