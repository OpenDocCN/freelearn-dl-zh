- en: Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像生成
- en: In the previous chapters, we learned about predicting the class of an image
    and detecting where the object is located in the whole image. If we work backwards,
    we should be in a position to generate an image if we are given a class. Generative
    networks come in handy in this scenario, where we try to create new images that
    look very similar to the original image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何预测图像的类别并检测物体在图像中的位置。如果我们反向操作，给定一个类别后，我们应该能够生成一张图像。在这种情况下，生成网络非常有用，我们尝试创建看起来与原始图像非常相似的新图像。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下几种方法：
- en: Generating images that can fool a neural network using an adversarial attack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对抗性攻击生成能够欺骗神经网络的图像
- en: DeepDream algorithm to generate images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DeepDream算法生成图像
- en: Neural style transfer between images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像之间的神经风格迁移
- en: Generating images of digits using Generative Adversarial Networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成对抗网络生成数字图像
- en: Generating images of digits using a Deep Convolutional GAN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度卷积生成对抗网络生成数字图像
- en: Face generation using a Deep Convolutional GAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度卷积生成对抗网络（Deep Convolutional GAN）生成面部
- en: Face transition from one to another
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部从一个到另一个的过渡
- en: Performing vector arithmetic on generated images
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成的图像进行向量算术运算
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapters, we identified the optimal weights that result in
    classifying an image into the right class. The output class of an image can be
    changed by varying the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们确定了分类图像到正确类别的最优权重。通过改变以下内容，可以改变图像的输出类别：
- en: The weights connecting the input to the output layer, while the input pixels
    remain constant
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接输入层和输出层的权重，输入像素保持恒定
- en: The input pixel values, while the weights remain constant
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入像素值保持不变时，权重保持恒定
- en: In this chapter, we will employ these two techniques to generate images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将采用这两种技术来生成图像。
- en: In the case studies of an adversarial attack, the neural style transfer and
    DeepDream will leverage the technique of changing the input pixel values. In the
    techniques involving a **Generative Adversarial Network** (**GAN**), we will leverage
    the technique of changing certain weights that connect input pixel values to the
    output.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗性攻击的案例研究中，神经风格迁移和DeepDream将利用改变输入像素值的技巧。而涉及**生成对抗网络**（**GAN**）的技术，则会利用改变连接输入像素值和输出的某些权重的技巧。
- en: The first three case studies in this chapter will leverage the technique of
    changing the input pixel values, while the rest leverage a change in weights that
    connect the input to the output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前三个案例研究将利用改变输入像素值的技巧，而其余的则利用改变连接输入和输出的权重。
- en: Generating images that can fool a neural network using adversarial attack
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过对抗性攻击生成能够欺骗神经网络的图像
- en: To understand how to perform an adversarial attack on an image, let's understand
    how regular predictions are made using transfer learning first and then we will
    figure out how to tweak the input image so that the image's class is completely
    different, even though we barely changed the input image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解如何对图像执行对抗性攻击，我们先了解如何使用迁移学习进行常规预测，然后我们将弄清楚如何调整输入图像，以便图像的类别完全不同，尽管我们几乎没有改变输入图像。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s go through an example where we will try to identify the class of the
    object within the image:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子，尝试识别图像中的物体类别：
- en: Read the image of a cat
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取一张猫的图像
- en: Preprocess the image so that it can then be passed to an inception network
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理图像，以便将其传递给Inception网络
- en: Import the pre-trained Inception v3 model
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入预训练的Inception v3模型
- en: Predict the class of the object present in the image
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测图像中物体的类别
- en: The image will be predicted as a persian cat as Inception v3 works well in predicting
    objects that belong to one of the ImageNet classes
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于Inception v3在预测属于ImageNet类之一的物体时表现良好，图像将被预测为波斯猫
- en: 'The task at hand is to change the image in such a way that it meets the following
    two criteria:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是以这样的方式改变图像，使其满足以下两个标准：
- en: The prediction of the new image using the same network should be an African
    elephant with a very high probability
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相同的网络对新图像进行预测时，应该以非常高的概率预测为非洲象
- en: The new image should be visually indistinguishable from the original image by
    a human
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新生成的图像应在人眼看来与原始图像无法区分
- en: 'To achieve this, we will follow this strategy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们将遵循以下策略：
- en: 'Define a loss function:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数：
- en: The loss is the probability of the image (of the persian cat) belonging to the
    African elephant class
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失是图像（波斯猫）属于非洲象类别的概率
- en: The higher the loss, the closer are we to our objective
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失越高，我们离目标就越近
- en: Hence, in this case, we would be maximizing our loss function
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，在此情况下，我们将最大化我们的损失函数
- en: 'Calculate the gradient of change in the loss with respect to the change in
    the input:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失变化相对于输入变化的梯度：
- en: This step helps in understanding the input pixels that move the output toward
    our objective
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一步有助于理解哪些输入像素将输出向我们的目标推进
- en: 'Update the input image based on the calculated gradients:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于计算出的梯度更新输入图像：
- en: Ensure that the pixel values in the original image is not translated by more
    than 3 pixels in the final image
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保原始图像中的像素值在最终图像中不会偏移超过3个像素
- en: This ensures that the resulting image is humanly indistinguishable from the
    original image
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这确保了生成的图像在人眼看来与原始图像无法区分
- en: Repeat steps 2 and step 3 until the prediction of the updated image is an African
    elephant with a confidence of at least 0.8
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和步骤3，直到更新后的图像被预测为非洲象，且置信度至少为0.8
- en: How to do it...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s go ahead and implement this strategy in code (The code file is available
    as `Adversarial_attack.ipynb` in GitHub):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在代码中实现这一策略（代码文件可在GitHub上的`Adversarial_attack.ipynb`找到）：
- en: 'Read the image of a cat:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取猫的图像：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The plot of the image looks as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的绘制如下所示：
- en: '![](img/33c46965-bc81-4b5e-aec1-f52dacb165dc.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33c46965-bc81-4b5e-aec1-f52dacb165dc.png)'
- en: 'Preprocess the image so that it can then be passed to an inception network:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理图像，以便将其传递到Inception网络：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Import the pre-trained model:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入预训练模型：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Predict the class of the object present in the image:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测图像中对象的类别：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code results in the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码结果如下：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the input and output:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入和输出：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`model_input_layer` is the input to the model and `model_output_layer` is the
    probability of various classes for the input image (the last layer with softmax
    activation).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_input_layer`是模型的输入，`model_output_layer`是输入图像的各种类别的概率（最后一层使用softmax激活）。'
- en: 'Set the limits of change for the original image:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置原始图像变化的限制：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, we are specifying the limits to which the original image
    can be changed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们指定了原始图像可以改变的限制。
- en: 'Initialize the cost function so that the object type to fake is an African
    elephant (386^(th) index value in the prediction vector):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化代价函数，使得要伪装的对象类型是非洲象（预测向量中第386个索引值）：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output of `model_output_layer` is the probability of various classes for
    the image of interest. In this instance, we are specifying that the cost function
    will be dictated by the index location of the object we are trying to fake our
    object into.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_output_layer`的输出是感兴趣图像的各种类别的概率。在此实例中，我们指定代价函数将由我们试图将对象伪装成的目标对象的索引位置来决定。'
- en: 'Initialize the gradient function of the cost with respect to the input:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化代价函数相对于输入的梯度：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code calculates the gradient of `cost_function` with respect to the change
    in `model_input_layer` (which is the input image).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码计算了`cost_function`相对于`model_input_layer`（即输入图像）变化的梯度。
- en: 'Map the cost and gradient functions with respect to the input:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 映射与输入相关的代价和梯度函数：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, we are calculating the values of `cost_function` (the
    probability of the image belonging to the African elephant class) and the gradients
    with respect to the input image.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在计算`cost_function`（图像属于非洲象类别的概率）和相对于输入图像的梯度。
- en: 'Keep updating the input image with respect to gradients until the probability
    of the resulting image being an African elephant is at least 80%:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一直更新输入图像，直到生成图像的非洲象概率至少达到80%：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, we are obtaining the cost and gradients that correspond
    to the input image (`hacked_image`). Additionally, we are updating the input image
    by the gradient (which is multiplied by the learning rate). Finally, if the hacked
    image crosses the threshold of the maximum changes of the input image, we'll clip
    it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们获取与输入图像（`hacked_image`）对应的代价和梯度。此外，我们通过梯度（与学习率相乘）更新输入图像。最后，如果被修改的图像超过了输入图像的最大变化阈值，我们将对其进行裁剪。
- en: Keep looping through these steps until you achieve a probability that the input
    image is at least 0.8.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不断循环这些步骤，直到你得到输入图像的概率至少为0.8。
- en: 'The variation of the probability of the image of persian cat being detected
    as the image of an African elephant over increasing epochs is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练轮次的增加，波斯猫图像被识别为非洲象图像的概率变化如下：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The variation of probability of the modified image belonging to the African
    elephant class is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的图像属于非洲象类别的概率变化如下：
- en: '![](img/a8c81887-f7bb-4364-8ea0-27f4321ada37.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8c81887-f7bb-4364-8ea0-27f4321ada37.png)'
- en: 'Predict the class of the updated image:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测更新图像的类别：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The output of the `predict` method, which provides the probability of the modified
    image belonging to African elephant class, is 0.804.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`方法的输出是修改后图像属于非洲象类别的概率，值为0.804。'
- en: 'De-process the updated input image (as it was pre-processed to scale it) so
    that it can be visualized:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对更新后的输入图像进行去处理（因为它在预处理时已经被缩放）以便可视化：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The combination of the original image, the modified (hacked) images and the
    difference between the two images is printed as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像、修改后的（被篡改的）图像以及两者之间的差异将如下打印出来：
- en: '![](img/1db1140a-75c9-4b31-9012-aeba89a7bb24.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1db1140a-75c9-4b31-9012-aeba89a7bb24.png)'
- en: Note that the output is now visually indistinguishable from the original image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出现在在视觉上与原始图像无法区分。
- en: It is interesting to note that with hardly any change in pixel values from the
    original image, we have fooled the neural network (the inception v3 model) so
    that it now predicts a different class. This is a great example of some of the
    security flaws that you could encounter if the algorithm that was used to come
    up with a prediction is exposed to users who could build images that can fool
    the system.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管像素值几乎没有变化，但我们成功地欺骗了神经网络（inception v3模型），使它预测了一个不同的类别。这是一个很好的例子，展示了如果用于预测的算法暴露给可以制作欺骗系统图像的用户，可能会遇到的安全漏洞。
- en: DeepDream algorithm to generate images
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DeepDream算法生成图像
- en: In the previous section, we tweaked the input image's pixels slightly. In this
    section, we will tweak the input image a little more so that we can come up with
    an image that is still of the same object, however a little more artistic than
    the original one. This algorithm forms the backbone of style-transfer techniques
    using neural networks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们稍微调整了输入图像的像素。在这一节中，我们将进一步调整输入图像，以便生成一张仍然是相同物体的图像，但比原图更具艺术感。该算法是使用神经网络进行风格迁移技术的核心。
- en: Let's go through the intuition of how DeepDream works.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下DeepDream如何工作的直觉。
- en: We will pass our image through a pre-trained model (VGG19, in this example).
    We already learned that, depending on the input image, certain filters in the
    pre-trained model activate the most and certain filters activate the least.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个预训练模型（例如VGG19）来处理我们的图像。我们已经了解到，根据输入图像，预训练模型中的某些滤波器激活得最多，而某些滤波器则激活得最少。
- en: We will supply the layers of neural network that we want to activate the most.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供我们希望激活的神经网络层。
- en: The neural network adjusts the input pixel values until we obtain the maximum
    value of the chosen layers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络会调整输入像素值，直到我们获得所选层的最大值。
- en: However, we will also ensure that the maximum possible activation does not exceed
    a certain value as the resultant image in that case could be very different from
    the original image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也会确保最大可能的激活值不超过某个值，因为如果激活值过高，结果图像可能会与原始图像有很大不同。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'With this intuition in place, let''s go through the steps of implementing the
    DeepDream algorithm:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 理解了这些直觉后，让我们来看看如何实现DeepDream算法的步骤：
- en: Choose the layers of neural network that you want to activate the most and assign
    weightage to the amount of contribution the layers can make towards overall loss
    calculation.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你想要最强激活的神经网络层，并为这些层对整体损失计算的贡献分配权重。
- en: 'Extract the output of the given layer when an image is passed through the layer
    and calculate the loss value at each layer:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取给定层的输出，当图像通过该层时，并计算每一层的损失值：
- en: An image activates the layer the most when the sum of squares of the output
    of the image in that layer is the highest
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当图像在某一层的输出平方和最大时，图像会最强地激活该层。
- en: Extract the gradient of change in the input pixel values with respect to the
    loss.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取输入像素值变化相对于损失的梯度。
- en: Update the input pixel values based on the gradient extracted in the previous
    step.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据上一阶段提取的梯度更新输入像素值。
- en: Extract the loss value (the sum of the squares of the activation) across all
    chosen layers for the updated input pixel values.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取更新后的输入像素值在所有选定层中的损失值（激活的平方和）。
- en: If the loss value (the weighted sum of the squared activation) is greater than
    a predefined threshold, stop updating the image.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果损失值（激活值的加权平方和）大于预定义的阈值，则停止更新图像。
- en: How to do it...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s implement these steps in code (The code file is available as `Deepdream.ipynb`
    in GitHub):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中实现这些步骤（代码文件可在GitHub的`Deepdream.ipynb`中找到）：
- en: 'Import the relevant packages and import the image:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包并导入图像：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Preprocess the image so that it can then be passed to the VGG19 model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像进行预处理，使其能够传递到VGG19模型：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Build a function that de-processes the processed image:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个去处理已处理图像的函数：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Preprocess the image:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the layers that contribute to the overall loss-value calculation:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义对整体损失值计算有贡献的层：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we are showing you that we will use the second and fifth
    pooling layers, and also assign the weights that these two layers will contribute
    to the overall loss value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们展示了将使用第二层和第五层池化层，并且分配这两层对整体损失值的贡献权重。
- en: 'Initialize the loss function:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化损失函数：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding step, we are initializing the loss value and a dictionary of
    the various layers in the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们初始化了损失值和模型中各个层的字典。
- en: 'Calculate the overall loss value of the activations:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 计算激活的整体损失值：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, we are looping through the layers that we are interested
    in ( `layer_contributions` ) and noting down the weights ( `coeff` ) that we have
    assigned to each layer. Additionally, we are calculating the output of the layers
    of interest ( `activation` ), and updating the loss value using the sum of squares
    of the activation values post scaling them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们遍历了感兴趣的层（`layer_contributions`），并记录了为每层分配的权重（`coeff`）。此外，我们还计算了感兴趣层的输出（`activation`），并通过对激活值进行缩放后求平方和来更新损失值。
- en: 'Initialize the gradient value:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化梯度值：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `K.gradients` method gives us the gradient of the loss with respect to the
    change in the input, `dream`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`K.gradients`方法给出了损失相对于输入变化（`dream`）的梯度。'
- en: 'Normalize the gradient values so that the change in the gradients is slow:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对梯度值进行归一化，以便梯度的变化速度较慢：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a function that maps the input image to the loss value and the gradient
    of the loss value with respect to the change in input pixel values (where the
    input image is `dream`):'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，将输入图像映射到损失值以及损失值相对于输入像素值变化的梯度（其中输入图像是`dream`）：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define a function that provides the loss and gradient values for a given input
    image:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，提供给定输入图像的损失值和梯度值：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Update the original image based on the obtained loss and gradient values over
    multiple iterations.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于获得的损失和梯度值，通过多次迭代更新原始图像。
- en: 'In the following code, we are looping through the image 100 times. We are defining
    the learning rate of changing the image and the maximum possible loss (change
    in the image) that can happen:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们遍历图像100次。我们定义了图像变化的学习率和图像可能发生的最大损失（变化）：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the following code, we are extracting the loss and gradient values of the
    image and then stopping the change in the image if the loss value is more than
    the defined threshold:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们提取了图像的损失值和梯度值，然后在损失值超过定义的阈值时停止图像的变化：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the following code, we are updating the image based on the gradient values
    and are de-processing the image and printing the image:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们根据梯度值更新图像，并进行去处理图像并打印图像：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code results in an image that looks as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成的图像如下所示：
- en: '![](img/d34a9836-cc7c-49dd-8afb-fe8cc5ac2545.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d34a9836-cc7c-49dd-8afb-fe8cc5ac2545.png)'
- en: Note that the wavy patterns in the preceding image are obtained potentially
    because these are the patterns that maximize the various network layers' activations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面图像中的波浪图案可能是因为这些是最大化各个网络层激活的模式。
- en: Here, we have seen another application of perturbing input pixels, which in
    this case resulted in a slightly more artistic image.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了扰动输入像素的另一种应用， 在这种情况下，结果是图像略显艺术感。
- en: Neural style transfer between images
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像之间的神经风格迁移
- en: In the previous recipe, the modified pixel values were trying to maximize the
    filter activations. However, it does not give us the flexibility of specifying
    the style of the image; neural style transfer comes in handy in this scenario.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的步骤中，修改的像素值试图最大化滤波器的激活值。然而，这并没有给我们提供指定图像风格的灵活性；此时，神经风格迁移派上了用场。
- en: In neural style transfer, we have a content image and a style image, and we
    try to combine these two images in such a way that the content in the content
    image is preserved while maintaining the style of the style image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经风格迁移中，我们有一个内容图像和一个风格图像，我们尝试以一种方式将这两张图像结合起来，既能保持内容图像中的内容，又能保持风格图像的风格。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: The intuition of neural style transfer is as follows.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移的直觉如下。
- en: We try to modify the original image in a similar way to the DeepDream algorithm.
    However, the additional step is that the loss value is split into content loss
    and style loss.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试以类似于DeepDream算法的方式修改原始图像。然而，额外的步骤是将损失值分为内容损失和风格损失。
- en: Content loss refers to how different the generated image is from the content
    image. Style loss refers to how correlated the style image is to the generated
    image.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失指的是生成图像与内容图像之间的差异。风格损失指的是风格图像与生成图像之间的相关性。
- en: While we mentioned that the loss is calculated based on the difference in images,
    in practice, we modify it slightly by ensuring that the loss is calculated using
    the activations from images and not the original images. For example, the content
    loss at layer 2 will be the squared difference between activations of the content
    image and the generated image when passed through the second layer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们提到损失是基于图像之间的差异来计算的，但在实践中，我们通过确保使用来自图像的激活值而不是原始图像来稍微修改它。例如，第二层的内容损失将是内容图像和生成图像在通过第二层时激活值之间的平方差。
- en: While calculating the content loss seems straightforward, let's try to understand
    how to calculate the similarity between the generated image and the style image.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算内容损失看起来很直接，但让我们尝试理解如何计算生成图像与风格图像之间的相似性。
- en: 'A technique called gram matrix comes into the picture. Gram matrix calculates
    the similarity between a generated image and a style image, and is calculated
    as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一种叫做gram矩阵的技术出现了。gram矩阵计算生成图像和风格图像之间的相似度，计算公式如下：
- en: '![](img/5754caec-464d-4b8e-a375-65df6ffdc898.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5754caec-464d-4b8e-a375-65df6ffdc898.png)'
- en: Where *GM(l)* is the gram matrix value at layer *l* for the style image, *S*,
    and the generated image, *G*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*GM(l)* 是风格图像 *S* 和生成图像 *G* 在层 *l* 处的gram矩阵值。
- en: A gram matrix results from multiplying a matrix with the transpose of itself.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: gram矩阵是通过将一个矩阵与其自身的转置相乘得到的。
- en: Now that we are in a position to calculate the style loss and content loss,
    the final modified input image is the image that minimizes the overall loss, that
    is, a weighted average of style and content loss.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算风格损失和内容损失了，最终的修改输入图像是最小化整体损失的图像，也就是风格损失和内容损失的加权平均值。
- en: 'Neural style transfer is implemented in the following steps:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移的实现步骤如下：
- en: Pass the image through a pre-trained model.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像通过一个预训练模型。
- en: Extract the layer values at a predefined layer.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取预定义层的层值。
- en: Initialize the generated image as the same as the content image.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像初始化为与内容图像相同。
- en: Pass the generated image through the model and extract its values at the exact
    same layer.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像通过模型并提取其在相同层的值。
- en: Calculate the content loss.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算内容损失。
- en: Pass the style image through multiple layers of the model and calculate the
    gram matrix values of the style image.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将风格图像通过模型的多个层，并计算风格图像的gram矩阵值。
- en: Pass the generated image through the same layers that the style image passed
    through and calculate its corresponding gram matrix values.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像通过与风格图像相同的层，并计算其对应的gram矩阵值。
- en: Extract the squared difference of the gram matrix values of the two images.
    This will be the style loss.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取两张图像的gram矩阵值之间的平方差。这将是风格损失。
- en: The overall loss will be the weighted average of the style loss and content
    loss.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整体损失将是风格损失和内容损失的加权平均值。
- en: The input image that minimizes the overall loss will be the final image of interest.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化整体损失的输入图像将是最终的目标图像。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Import the relevant packages and content, style images, that need to be combined
    to form an artistic image, as follows (The code file is available as `Neural_style_transfer.ipynb`
    in GitHub):'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和内容、样式图像，它们需要结合在一起形成艺术图像，如下所示（代码文件可在GitHub上的`Neural_style_transfer.ipynb`找到）：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The style and base images look as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 样式图像和基础图像如下所示：
- en: '![](img/6e087838-928d-476e-b7e8-a1153df3d213.png)![](img/470f0a29-7b89-40ca-a2ba-33d574883c08.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e087838-928d-476e-b7e8-a1153df3d213.png)![](img/470f0a29-7b89-40ca-a2ba-33d574883c08.png)'
- en: 'Initialize the `vgg19` model so that the images can be passed through its network:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`vgg19`模型，以便图像可以通过其网络传递：
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reshape the base image and extract the feature values at the `block3_conv4` layer
    of the VGG19 model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整基础图像并提取VGG19模型中`block3_conv4`层的特征值：
- en: '[PRE30]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, we are defining a function that takes the input image
    and extracts the output at the predefined layer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个函数，该函数获取输入图像并在预定义层中提取输出。
- en: 'Define the layers from which extractions need to be made to calculate the content
    and style losses as well as the corresponding weights that need to be assigned
    to each layer:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义需要提取内容和样式损失的层，以及需要分配给每个层的相应权重：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code, we are defining the layers from which the content and
    style loss are calculated, as well as assigning the weights associated with the
    loss arising from each of these layers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了计算内容和样式损失的层，并为这些层产生的损失分配了相应的权重。
- en: 'Define the gram matrix and style loss functions:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义Gram矩阵和样式损失函数：
- en: 'In the following code, we are defining a function that calculates the gram
    matrix output output as the dot product of features obtained by flattening the image:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们定义了一个函数，该函数计算作为通过扁平化图像获得的特征的点积的Gram矩阵输出：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the following code, we are calculating the style loss as defined in the
    style loss equation specified in *Getting ready* section:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在计算在*准备阶段*中定义的样式损失方程式中所指定的样式损失：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Initialize the loss value function:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化损失值函数：
- en: 'Calculating the content loss:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 计算内容损失：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding code, we are updating the loss value based on the loss at the
    layers that calculate the content loss. Note that `layer_output_base` is the output
    when we pass the original base image through the content layer (as defined in
    step 3).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们根据计算内容损失的层中的损失更新损失值。请注意，`layer_output_base`是通过内容层传递原始基础图像时的输出（如第3步所定义）。
- en: The greater the difference between the activation (which is based on the modified
    image) and `layer_output_base` (which is based on the original image), the greater
    the content loss associated with the image.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 激活（基于修改后的图像）和`layer_output_base`（基于原始图像）之间的差异越大，图像的内容损失就越大。
- en: 'Calculating the style loss:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 计算样式损失：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the preceding code, we are calculating style loss in the same manner as
    we calculated the content loss but on different layers and using a different custom
    function we built: `style_loss`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们以与计算内容损失相同的方式计算样式损失，但在不同的层上，并使用我们构建的不同自定义函数：`style_loss`。
- en: 'Build a function that maps the input image to the loss values and the corresponding
    gradient values:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个函数，将输入图像映射到损失值和相应的梯度值：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code fetches the loss and gradient values in a manner that is
    very similar to the *DeepDream algorithm to generate images* recipe.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码以与*DeepDream算法生成图像*食谱非常相似的方式获取损失和梯度值。
- en: 'Run the model for multiple epochs:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行模型多个周期：
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The preceding code results in an image that is a combination of the content
    and style images:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了一张将内容图像和样式图像相结合的图像：
- en: '![](img/03042fd3-6db8-40c1-bfe4-071359915494.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03042fd3-6db8-40c1-bfe4-071359915494.png)'
- en: With differing layers that are selected to calculate the content and style loss,
    and differing weights assigned to coefficients of layers in their respective style
    or content contributions, the resulting generated image could be different.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择不同的层来计算内容和样式损失，并为这些层在各自样式或内容贡献中分配不同的系数权重，最终生成的图像可能会有所不同。
- en: 'In the previous three case studies, we saw how we can generate new images by
    changing the input pixel values. In the rest of this chapter, we will take a different
    approach to generating new images: using GANs.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的三个案例研究中，我们看到如何通过改变输入像素值来生成新图像。在本章的其余部分，我们将采用一种不同的生成新图像的方法：使用**生成对抗网络**（GANs）。
- en: Generating images of digits using Generative Adversarial Networks
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成对抗网络生成数字图像
- en: A GAN uses a stack of neural networks to come up with a new image that looks
    very similar to the original set of images. It has a variety of applications in
    image generation, and the field of GAN research is progressing very quickly to
    come up with images that are very hard to distinguish from real ones. In this
    section, we will understand the basics of a GAN – how it works and the difference
    in the variations of GANs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一个生成对抗网络（GAN）使用一堆神经网络生成一张与原始图像集非常相似的新图像。它在图像生成中有着广泛的应用，并且GAN研究领域正在快速进展，旨在生成那些非常难以与真实图像区分的图像。在本节中，我们将理解GAN的基础知识——它是如何工作的，以及GAN变种之间的差异。
- en: 'A GAN comprises two networks: a generator and a discriminator. The generator
    tries to generate an image and the discriminator tries to determine whether the
    image it is given as an input is a real image or a generated (fake) image.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个GAN由两个网络组成：生成器和判别器。生成器尝试生成一张图像，判别器则尝试确定它收到的输入图像是真实的图像还是生成的（假的）图像。
- en: To gain further intuition, let's assume that a discriminator model tries to
    classify a picture into a human face image, or not a human face from a dataset
    that contains thousands of human face images and non-human face images.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解，假设判别器模型试图将一张图片分类为人脸图像或非人脸图像，数据集中包含了成千上万的人脸图像和非人脸图像。
- en: Once we train the model to classify human and non-human faces, if we show a
    new human face to the model, it would still classify it as a human face, while
    it learns to classify a non-human face as a non-human face.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练模型以区分人脸和非人脸，当我们向模型展示一张新的人脸时，模型仍然会将其分类为人脸，而它会学习将非人脸分类为非人脸。
- en: The task of the generator network is to generate images that look so similar
    to the original set of images that a discriminator can get fooled into thinking
    that the generated image actually came from the original dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络的任务是生成与原始图像集非常相似的图像，以至于判别器会被“欺骗”，认为生成的图像实际上来自原始数据集。
- en: Getting ready
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The strategy we''ll adopt to generate images is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的生成图像的策略如下：
- en: Generate a synthetic image using the generator network, which in the initial
    step is a noisy image that is generated by reshaping a set of noise values to
    the shape of our images.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成器网络生成合成图像，初始步骤是生成一张噪声图像，该图像是通过将一组噪声值重新塑形为我们图像的形状来生成的。
- en: 'Concatenate the generated image with the original set of images where the discriminator
    predicts whether each of the images is a generated image or an original image—this
    ensures that the discriminator is trained:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像与原始图像集合连接，并让判别器预测每个图像是生成的图像还是原始图像——这确保了判别器被训练：
- en: Note that the weights of a discriminator network are trained in this iteration
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，判别器网络的权重在这一迭代过程中得到了训练。
- en: The loss of a discriminator network is the binary cross-entropy of the prediction
    and actual values of an image
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器网络的损失是图像的预测值和实际值之间的二元交叉熵。
- en: The output value of the generated image will be fake (0) and the values of the
    original images will be real (1)
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成图像的输出值将是假的（0），而原始图像的值将是真实的（1）。
- en: 'Now that the discriminator has been trained for one iteration, train a generator
    network that modifies the input noise such that it looks more like a real image
    than a synthetic one – one that has the potential to fool the discriminator. This
    process goes through the following steps:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，判别器已经经过了一次迭代训练，接下来训练生成器网络，修改输入噪声，使其看起来更像真实图像而非合成图像——一个有可能欺骗判别器的图像。这个过程包括以下步骤：
- en: The input noise is passed through a generator network, which reshapes the input
    into an image.
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入噪声通过生成器网络传递，生成器将输入转化为图像。
- en: The image generated from the generator network is then passed through a discriminator
    network – however, note that the weights in the discriminator network are frozen
    in this iteration so that they are not trained in this iteration (because they
    were already trained in step 2).
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从生成器网络生成的图像会传递到判别器网络——但请注意，在这一迭代中判别器网络的权重是被冻结的，因此它们不会在这一迭代中被训练（因为它们已经在步骤2中进行了训练）。
- en: The value of the generated image's output from the discriminator will be real
    (1) as its task is to fool the discriminator.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从判别器得到的生成图像输出值将是真实的（1），因为它的任务是欺骗判别器。
- en: 'The loss of a generator network is the binary cross-entropy of the prediction
    from the input image and the actual value (which is 1 for all the generated images)—this
    ensures that the generator network weights are fine-tuned:'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器网络的损失是输入图像的预测与实际值之间的二进制交叉熵（对于所有生成的图像，实际值为1）——这确保了生成器网络的权重被微调：
- en: Note that the discriminator network weights are frozen in this step
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，在这一步中，判别器网络的权重已被冻结
- en: Freezing the discriminator ensures that the generator network learns from the
    feedback provided by the discriminator
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冻结判别器可以确保生成器网络从判别器提供的反馈中学习
- en: Repeat these steps multiple times until you generate realistic images.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这些步骤多次，直到生成真实的图像。
- en: How to do it...
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the *Adversarial attack to fool a neural network section*, we discussed
    our strategy of how to generate an image that looks very similar to the original
    images. In this section, we will implement the process of generating a digit''s
    image from the MNIST dataset (the code file is available as `Vanilla_and_DC_GAN.ipynb`
    in GitHub):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在*对抗性攻击欺骗神经网络*部分，我们讨论了如何生成一个与原始图像非常相似的图像的策略。在这一部分，我们将实现从MNIST数据集生成数字图像的过程（代码文件可在GitHub上的`Vanilla_and_DC_GAN.ipynb`中找到）：
- en: 'Import the relevant packages:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the parameters:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义参数：
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the generator and discriminator networks:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器和判别器网络：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'For the generator, we are building a model that takes a noise vector that is
    100 dimensions in shape and will be converting it into an image that is 28 x 28
    x 1 in shape. Note that we used `LeakyReLU` activation in the model. A summary
    of the generator network is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器，我们构建了一个模型，它接收一个形状为100维的噪声向量，并将其转换为一个形状为28 x 28 x 1的图像。注意，我们在模型中使用了`LeakyReLU`激活函数。生成器网络的摘要如下：
- en: '![](img/bee45c41-5ea7-4026-b15b-ef627ff7902a.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bee45c41-5ea7-4026-b15b-ef627ff7902a.png)'
- en: 'In the following code, we are building a discriminator model where we take
    an input image that is 28 x 28 x 1 in shape and produce an output that is either
    1 or 0, which indicates whether the input image is an original image or a fake
    image:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们构建了一个判别器模型，其中我们输入一个形状为28 x 28 x 1的图像，并输出一个值为1或0的结果，表示输入图像是原始图像还是伪造图像：
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'A summary of the discriminator network is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的摘要如下：
- en: '![](img/cf8f01e2-d615-49af-9cae-12915b038e85.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf8f01e2-d615-49af-9cae-12915b038e85.png)'
- en: 'Compile the generator and discriminator models:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 编译生成器和判别器模型：
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the stacked generator discriminator model that helps to optimize weights
    for the generator while freezing weights for the discriminator network. The stacked
    generator discriminator takes the random noise that we pass to the model as input
    and converts that noise into an image that is 28 x 28 in shape using the generator
    network. Furthermore, it determines whether the 28 x 28 image is a real or fake
    image:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义堆叠的生成器判别器模型，帮助优化生成器的权重，同时冻结判别器网络的权重。堆叠的生成器判别器接受我们传入模型的随机噪声作为输入，并使用生成器网络将噪声转换为一个28
    x 28的图像。此外，它还判断这个28 x 28的图像是真实的还是伪造的：
- en: '[PRE44]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define a function to plot the generated images:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来绘制生成的图像：
- en: '[PRE45]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Provide the input images:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供输入图像：
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We are discarding the `y_train` dataset, as we do not need the output labels,
    since our model generates new images based on the given set of images, that is `X_train`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们丢弃了`y_train`数据集，因为我们不需要输出标签，模型是基于给定的图像集合（即`X_train`）生成新图像的。
- en: 'Optimize the images by running them over multiple epochs:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多次训练周期优化图像：
- en: 'In the following code, we are obtaining the real images (`legit_images`) and
    generating the fake image (`synthetic_images`) data, which we will try to convert
    into a realistic image by modifying noise data (`gen_noise`) as the input, as
    follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在获取真实图像（`legit_images`）并生成假图像（`synthetic_images`）数据，我们将尝试通过修改噪声数据（`gen_noise`）作为输入，将其转换为逼真的图像，如下所示：
- en: '[PRE47]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In the following code, we are training the discriminator (using the `train_on_batch` method),
    where the real images are expected to have a value of 1 and the fake images are
    expected to have a value of zero in the output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在训练判别器（使用`train_on_batch`方法），其中真实图像应输出1，而假图像应输出0：
- en: '[PRE48]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In the following code, we are preparing a new set of data where `noise` is
    the input and `y_mislabeled` is the output to train the generator (note that the
    output is the exact opposite of what the output was when we were training the
    discriminator):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在准备一组新的数据，其中`noise`是输入，`y_mislabeled`是输出，用于训练生成器（请注意，输出与我们训练判别器时的输出正好相反）：
- en: '[PRE49]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In the following code, we are training the stacked combination of the generator
    and discriminator, where the discriminator weights are frozen while the generator''s
    weights get updated to minimize the loss value. The generator''s task is to generate images that
    can trick the discriminator to output a value of 1:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在训练生成器和判别器的堆叠组合，其中判别器的权重被冻结，而生成器的权重会更新，以最小化损失值。生成器的任务是生成能够欺骗判别器输出1的图像：
- en: '[PRE50]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In the following code, we are looking at the output of generator loss and discriminator
    loss across various epochs:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们观察生成器损失和判别器损失在不同周期的输出：
- en: '[PRE51]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/135e0b94-6745-46c3-8bd1-e9adfdcd07b4.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/135e0b94-6745-46c3-8bd1-e9adfdcd07b4.jpg)'
- en: 'The variation of discriminator and generator loss of increasing epochs is as
    follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器和生成器损失随着周期增加的变化如下：
- en: '![](img/37f6d676-e678-494b-a8e3-9856cbc10712.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37f6d676-e678-494b-a8e3-9856cbc10712.png)'
- en: Note that the preceding output has a lot of scope for improvement in terms of
    how realistic the generated images look.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的输出在生成图像的真实感方面还有很大的改进空间。
- en: There's more...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The output that we saw here is also a function of the model's architecture.
    For example, vary the activation function in various layers of model to tanh and
    see how the resulting output looks to get an idea of what the resulting generated
    images look like.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的输出也是模型架构的函数。例如，可以将模型各层的激活函数更改为tanh，看看生成的输出如何变化，从而大致了解生成图像的样子。
- en: Generating images using a Deep Convolutional GAN
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度卷积GAN生成图像
- en: In the previous section, we looked at generating digits using a vanilla generator
    and a discriminator network. However, we can have a scenario where the network
    can learn the features in an image much better by using the convolution architectures,
    as the filters in a CNN learn specific details within an image. **Deep Convolutional
    Generative Adversarial Networks** (**DCGANs**) take advantage of this phenomenon
    to come up with new images.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们研究了使用Vanilla生成器和判别器网络生成数字。然而，我们也可以遇到一种情况，即通过使用卷积架构，网络能更好地学习图像中的特征，因为CNN中的滤波器会学习图像中的特定细节。**深度卷积生成对抗网络**（**DCGANs**）利用这一现象生成新的图像。
- en: How to do it...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'While the intuition of how a DCGAN works is very similar to that of a GAN (which
    we worked with in the previous recipe), the major difference is in the architecture
    of the generator and discriminator of the DCGAN, which looks as follows (The code
    file is available as `Vanilla_and_DC_GAN.ipynb` in GitHub):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DCGAN的工作原理与GAN（我们在上一个示例中使用的模型）非常相似，但主要的区别在于DCGAN的生成器和判别器架构，其结构如下（代码文件可以在GitHub上找到，文件名为`Vanilla_and_DC_GAN.ipynb`）：
- en: '[PRE52]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note that, in the DCGAN, we performed multiple convolution and pooling operations
    on the input data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在DCGAN中，我们对输入数据执行了多次卷积和池化操作。
- en: 'If we rerun the exact same steps that we performed in the Vanilla GAN (the *Generative
    Adversarial Network to generate images* recipe), but this time using the models
    defined with a convolution and pooling architecture (and thus DCGAN), we get the
    following generated image:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新执行在Vanilla GAN（*生成对抗网络用于生成图像*）示例中执行的完全相同的步骤，但这次使用定义了卷积和池化架构的模型（即DCGAN），我们将得到以下生成的图像：
- en: '![](img/2875a289-9afe-430e-8502-c63a9c999541.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2875a289-9afe-430e-8502-c63a9c999541.png)'
- en: 'The variation of generator and discriminator loss values over increasing epochs
    is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代轮次增加，生成器和判别器的损失值变化如下：
- en: '![](img/9a924e72-de38-489a-93be-b1fac122e4ce.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a924e72-de38-489a-93be-b1fac122e4ce.png)'
- en: We can see that, while everything else remains the same and only the model architecture
    has changed, the resulting images through DCGAN are a lot more realistic than
    the results of a Vanilla GAN.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，尽管其他一切保持不变，仅模型架构发生了变化，但通过 DCGAN 生成的图像比 Vanilla GAN 的结果真实得多。
- en: Face generation using a Deep Convolutional GAN
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度卷积 GAN 生成面部
- en: So far, we have seen how to generate new images. In this section, we will learn
    how to generate a new set of faces from an existing dataset of faces.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了如何生成新图像。在本节中，我们将学习如何从现有的面部数据集中生成一组新的面部图像。
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The approach we will be adopting for this exercise will be very similar to
    what we adopted in the *Generating images using a* *Deep Convolutional GAN* recipe:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本次练习中采用的方案与我们在 *使用深度卷积 GAN 生成图像* 处方中的方法非常相似：
- en: Collect a dataset that contains multiple face images.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一个包含多个面部图像的数据集。
- en: Generate random images at the start.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始时生成随机图像。
- en: Train a discriminator by showing it a combination of faces and random images,
    where the discriminator is expected to differentiate between an actual face image
    and a generated face image.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过展示包含面部和随机图像的组合来训练判别器，判别器需要区分实际面部图像和生成的面部图像。
- en: Once the discriminator model is trained, freeze it and adjust the random images
    in such a way that the discriminator now assigns a higher probability of belonging
    to the original face images to the adjusted random images.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦判别器模型训练完成，将其冻结，并调整随机图像，使得判别器现在会给经过调整的随机图像分配更高的属于原始面部图像的概率。
- en: Repeat the preceding two steps through multiple iterations until the generator
    does not get trained any further.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前面两步，进行多次迭代，直到生成器不再继续训练。
- en: How to do it...
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Face generation is implemented in code as follows (the code file is available
    as `Face_generation.ipynb` in GitHub):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 面部生成的代码实现如下（代码文件在 GitHub 上可用，名为 `Face_generation.ipynb`）：
- en: 'Download the dataset. The recommended dataset to be downloaded and the associated
    code is provided in GitHub. A sample of images is as follows:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集。建议下载的数据集和相关代码已在 GitHub 上提供。以下是图像示例：
- en: '![](img/cba9b1ce-cca6-459e-b691-2a0b84628ee0.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cba9b1ce-cca6-459e-b691-2a0b84628ee0.png)'
- en: 'Define the model architecture:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: '[PRE53]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Note that the preceding code is the same as the generator we built in the *Deep
    convolutional generative adversarial networks* recipe:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述代码与我们在 *深度卷积生成对抗网络* 处方中构建的生成器相同：
- en: '[PRE54]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Note that the preceding architecture is the same as the one we built in the
    *Generating images using Deep Convolutional GAN* section:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述架构与我们在 *使用深度卷积 GAN 生成图像* 部分中构建的架构相同：
- en: '[PRE55]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Define the utility functions to load, preprocess, and de-process the image
    and also to plot the images:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于加载、预处理和反处理图像的实用函数，并绘制图像：
- en: '[PRE56]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Note that we are resizing our images to a smaller shape so that the the number
    of parameters that need to be tweaked through the model is minimal:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在将图像调整为较小的形状，以便通过模型调整的参数数量最小化：
- en: '[PRE57]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Import the dataset and preprocess it:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集并进行预处理：
- en: '[PRE58]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In the following code, we are creating the input dataset and converting it
    into an array:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在创建输入数据集并将其转换为数组：
- en: '[PRE59]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compile the generator, discriminator, and stacked generator discriminator models:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译生成器、判别器和堆叠的生成器-判别器模型：
- en: '[PRE60]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Run the model over multiple epochs in a manner that is very similar to what
    we employed in the *Deep Convolutional Generative Adversarial Networks* recipe:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以类似于我们在 *深度卷积生成对抗网络* 处方中使用的方式，运行模型多轮迭代：
- en: '[PRE61]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The preceding code generates images that look as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成的图像如下所示：
- en: '![](img/c8296899-5777-40c5-ac1d-5f259a7dae53.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8296899-5777-40c5-ac1d-5f259a7dae53.png)'
- en: Note that, while these images look very blurry, the picture is an original one
    that is not present in the original dataset. There is a lot of scope for improvement
    in this output by varying the model architecture and having deeper layers.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管这些图像看起来非常模糊，但这张图片是原始的，不存在于原始数据集中。通过改变模型架构并增加更深的层次，这个输出还有很大的提升空间。
- en: 'The variation of discriminator and generator loss values over increasing epochs
    looks as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练轮数增加，判别器和生成器损失值的变化如下所示：
- en: '![](img/807a0f3c-24e4-42f7-87fa-48fffa9fff7e.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/807a0f3c-24e4-42f7-87fa-48fffa9fff7e.png)'
- en: Note that, from the preceding diagram, we might want to train the model for
    a fewer number of epochs so that the generator loss is not very high.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从前面的图表中，我们可能希望训练模型的轮数少一些，以使生成器的损失值不那么高。
- en: Face transition from one to another
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一张人脸过渡到另一张人脸
- en: Now that we are in a position to generate faces, let's go ahead and perform
    some vector arithmetic on top of the generated images.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够生成面部图像了，接下来让我们在生成的图像上进行一些向量运算。
- en: For this exercise, we will perform the transition of face generation from one
    face to another.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将执行从一个人脸到另一个人脸的生成过渡。
- en: Getting ready
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: We'll continue from the image generation model that we built in the *Face generation
    using a Deep Convolutional GAN* section.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续从《*使用深度卷积GAN进行人脸生成*》部分构建的图像生成模型开始。
- en: Let's say we want to see the transition of one generated face image into another
    generated face image. This process is enabled by slowly varying the vector from
    the first vector (the vector of the first generated image) to the second vector
    (the vector of the second generated image). You can essentially think of each
    of the latent (vector) dimensions as representing a certain aspect about the image.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望看到一张生成的人脸图像逐渐过渡到另一张生成的人脸图像。这个过程是通过慢慢改变从第一个向量（第一张生成图像的向量）到第二个向量（第二张生成图像的向量）来实现的。你可以将每个潜在的（向量）维度看作是图像的某个特定方面。
- en: 'The strategy that we''ll adopt is as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略如下：
- en: Generate two images
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成两张图像
- en: Translate the first generated image in to the second generated image in 10 steps
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在10步中将第一张生成图像转换为第二张生成图像
- en: Assigning a weight of 1 to the first generated image and a weight of 0 to the
    second generated image in the first step
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步中，将第一张生成图像的权重设为1，第二张生成图像的权重设为0。
- en: In the second step, assign a weight of 0.9 to the first generated image and
    a weight of 0.1 to the second generated image
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二步中，将第一张生成图像的权重设为0.9，第二张生成图像的权重设为0.1。
- en: Repeat the preceding steps until we assign a weight of 0 to the first generated
    image and a weight of 1 to the second generated image
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，直到我们将第一张生成图像的权重设为0，第二张生成图像的权重设为1。
- en: How to do it...
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'We''ll code up the strategy that we laid out in the *Getting ready* section,
    as follows (The code file is available as `Face_generation.ipynb` in GitHub):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写在《准备开始》部分中概述的策略，代码如下（代码文件可以在GitHub上的`Face_generation.ipynb`中找到）：
- en: 'Generate the first image from random noise (note that we''ll continue from
    step 6 in *Face generation using a Deep Convolutional GAN* section):'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机噪声生成第一张图像（请注意，我们将从《*使用深度卷积GAN进行人脸生成*》部分的第6步继续）：
- en: '[PRE62]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The generated image looks as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像如下所示：
- en: '![](img/0f786cff-d1c3-4593-acf5-c2471595c8c2.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f786cff-d1c3-4593-acf5-c2471595c8c2.png)'
- en: 'Generate the second image from random noise:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机噪声生成第二张图像：
- en: '[PRE63]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The following is the output of the preceding code snippet:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码片段的输出：
- en: '![](img/f6450874-5966-4739-9766-ea1b6f6c4997.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6450874-5966-4739-9766-ea1b6f6c4997.png)'
- en: 'Generate the visualization of obtaining the second image from the first image:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成从第一张图像到第二张图像的可视化：
- en: '[PRE64]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We will get the following output:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下输出：
- en: '![](img/dfcd4d0b-296b-45d5-b28a-8ad685a06a23.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfcd4d0b-296b-45d5-b28a-8ad685a06a23.jpg)'
- en: Note that, in the preceding output, we have slowly transformed the first image
    into the second image.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的输出中，我们已经慢慢将第一张图像转换成了第二张图像。
- en: Performing vector arithmetic on generated images
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生成图像上执行向量运算
- en: Now that we understand that the latent vector representations play a key part
    in changing the outcome of the generated image, let's further build our intuition
    with images that have a certain face alignment.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了潜在向量表示在改变生成图像结果中的关键作用，接下来让我们用具有特定人脸对齐的图像进一步构建我们的直觉。
- en: Getting ready
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The strategy that we''ll adopt to perform vector arithmetic on a generated
    image is as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的向量运算策略如下：
- en: Generate three images that are based on the random noise of 100 vector values
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成三张基于100个向量值随机噪声的图像
- en: Ensure that two of the three images have generated faces that look to the left,
    and that one looks to the right
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保三张图像中有两张生成的面朝左，并且有一张面朝右。
- en: Calculate a new vector that is the sum of images that are aligned in the same
    direction, which is further subtracted from the image that is aligned in the opposite
    direction
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一个新的向量，它是对齐同一方向的图像之和，再从对齐在相反方向的图像中减去该向量。
- en: Generate the image from the resulting vector obtained in the previous step
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从上一步骤中获得的结果向量生成图像
- en: How to do it...
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We''ll code up the strategy that we listed out as follows (The code file is
    available as `Face_generation.ipynb` in GitHub). Note that we''ll continue from
    step 6 in *Face generation using a Deep Convolutional GAN* section):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下策略进行编程（代码文件在 GitHub 上可作为 `Face_generation.ipynb` 获取）。注意，我们将从 *使用深度卷积
    GAN 生成面孔* 部分的第 6 步继续：
- en: 'Generate three vectors (ensure that two images are aligned in one direction
    and that the other is aligned in the opposite direction by varying the generated
    noise):'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成三个向量（确保两幅图像对齐在一个方向上，另一幅图像则通过改变生成的噪声与之对立方向对齐）：
- en: '[PRE65]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Plot the generated images:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制生成的图像：
- en: '[PRE66]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The three generated images are as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 三个生成的图像如下：
- en: '![](img/424f17c6-4b45-4a97-a375-f043c7af965f.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/424f17c6-4b45-4a97-a375-f043c7af965f.png)'
- en: We can see that images 2 and 3 have the face looking to the right, while image
    1 has the face looking straight ahead.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到图像 2 和 3 的人脸朝向右侧，而图像 1 的人脸正面朝前。
- en: 'Perform a vector arithmetic of the vector representations of each of these
    images to see the outcome:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这些图像的每一个向量表示进行向量运算，以查看结果：
- en: '[PRE67]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The preceding code generates a face, as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了如下的面孔：
- en: '![](img/78804f8b-9305-493a-a5fb-fb140c532770.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78804f8b-9305-493a-a5fb-fb140c532770.png)'
- en: The preceding arithmetic shows that the vector arithmetic (vector of image 1
    + image 2 - image 3) generated an image has the face looking straight ahead and
    thus strengthening our intuition of the workings of latent vector representations.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 上述运算显示，向量运算（图像 1 + 图像 2 - 图像 3 的向量）生成的图像使得面孔朝前，从而增强了我们对潜在向量表示工作原理的直觉。
- en: There's more...
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We have merely touched on the basics of GAN; there are a variety of GAN-based
    techniques that are currently becoming popular. We will discuss the applications
    of a few of them:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅触及了 GAN 的基础；目前有多种基于 GAN 的技术正在变得流行。我们将讨论其中一些技术的应用：
- en: '**pix2pix**: Imagine a scenario where you doodle (sketch) the structure of
    an object and the object shapes up in a variety of forms. pix2pix is an algorithm
    that helps in enabling this.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pix2pix**：想象一个场景，你涂鸦（草图）一个物体的结构，然后这个物体以多种形式呈现。pix2pix 是一种帮助实现这一点的算法。'
- en: '**Cycle GAN**: Imagine a scenario where you want an object to look like a completely
    different object (for example, you want a horse object to look like a zebra and
    vice versa). You also want to ensure that every other aspect of the image remains
    the same, except the change in the object. Cycle GAN comes in handy in such a
    scenario.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cycle GAN**：想象一个场景，你希望一个物体看起来像完全不同的物体（例如，你希望一个马的物体看起来像一只斑马，反之亦然）。你还希望确保图像的其他所有部分保持不变，只有物体发生变化。在这种情况下，Cycle
    GAN 很有用。'
- en: '**BigGAN** is a recent development that comes up with extremely realistic-looking
    generated images.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BigGAN** 是最近的一项发展，它生成的图像看起来极为真实。'
