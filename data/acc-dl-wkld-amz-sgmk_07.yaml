- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Operationalizing Deep Learning Training
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习训练的运营化
- en: In [*Chapter 1*](B17519_01.xhtml#_idTextAnchor013), *Introducing Deep Learning
    with Amazon SageMaker*, we discussed how SageMaker integrates with CloudWatch
    Logs and Metrics to provide visibility into your training process by collecting
    training logs and metrics. However, **deep learning** (**DL**) training jobs are
    prone to multiple types of specific issues related to model architecture and training
    configuration. Specialized tools are required to monitor, detect, and react to
    these issues. Since many training jobs run for hours and days on large amounts
    of compute instances, the cost of errors is high.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B17519_01.xhtml#_idTextAnchor013)《使用Amazon SageMaker介绍深度学习》中，我们讨论了SageMaker如何与CloudWatch
    Logs和Metrics集成，通过收集训练日志和指标来提供对训练过程的可视化。然而，**深度学习**（**DL**）训练作业容易遇到与模型架构和训练配置相关的多种特定问题。需要专门的工具来监控、检测和应对这些问题。由于许多训练作业需要在大量计算实例上运行数小时甚至数天，因此错误的成本非常高。
- en: 'When running DL training jobs, you need to be aware of two types of issues:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行深度学习训练作业时，你需要意识到两种类型的问题：
- en: Issues with model and training configuration, which prevent the model from efficient
    learning during training. Examples of such issues include vanishing and exploding
    gradients, overfitting and underfitting, not decreasing loss, and others. The
    process of finding such errors is known as **debugging**.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型和训练配置的问题，阻碍了模型在训练过程中的高效学习。例如，梯度消失和爆炸、过拟合和欠拟合、损失未下降等问题。找出这些错误的过程被称为**调试**。
- en: Suboptimal model and training configuration, which doesn’t allow you to fully
    utilize available hardware resources. For instance, let’s say that the batch size
    is smaller than the optimal value and the GPU resources are underutilized. This
    leads to a slower than possible training speed. We call the process of finding
    such issues **profiling**.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 次优的模型和训练配置，未能充分利用可用的硬件资源。例如，假设批量大小小于最佳值，GPU资源未得到充分利用，这导致训练速度比可能的速度慢。我们称这种找出问题的过程为**分析**。
- en: In this chapter, we will review the available open source and SageMaker capabilities
    for training, debugging, and profiling. We will start with the popular open source
    tool for training monitoring and debugging called **TensorBoard** and review how
    it can be integrated with SageMaker’s training infrastructure. Then, we will compare
    it to the proprietary **SageMaker Debugger**, which provides advanced capabilities
    to help you automatically detect various types of issues and manage your training
    job accordingly. You will develop practical experience in using both tools.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾用于训练、调试和分析的开源工具和SageMaker功能。我们将从流行的开源训练监控和调试工具**TensorBoard**开始，回顾它如何与SageMaker的训练基础设施集成。然后，我们将其与专有的**SageMaker调试器**进行对比，后者提供了先进的功能，帮助你自动检测各种问题，并相应地管理训练作业。你将获得使用这两种工具的实际经验。
- en: Another type of problem you typically need to solve when operationalizing your
    DL models is establishing an efficient way to find optimal combinations of model
    hyperparameters. This process is known as **hyperparameter tuning**. It is especially
    relevant in the initial stages of model development and adoption when you need
    to establish a production-ready model baseline. SageMaker provides an automated
    way to tune your model using a feature called **Automatic Model Tuning**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在运营化深度学习模型时，你通常需要解决的另一类问题是建立一种高效的方法来寻找最佳的模型超参数组合。这个过程被称为**超参数调优**。它在模型开发和采用的初期阶段尤为重要，因为此时你需要建立一个可以投入生产的模型基线。SageMaker提供了一种自动化的方式，通过**自动模型调优**功能来调节你的模型。
- en: Finally, we will discuss how to reduce the cost of your training job and model
    tuning jobs by using **EC2 Spot Instances**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论如何通过使用**EC2 Spot实例**来降低训练作业和模型调优作业的成本。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Debugging training jobs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试训练作业
- en: Profiling your DL training
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析你的深度学习训练
- en: Hyperparameter optimization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Using EC2 Spot Instances
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EC2 Spot实例
- en: After reading this chapter, you will be able to establish profiling and debugging
    procedures for your large-scale DL training to minimize unnecessary costs and
    time to train. You will also know how to organize hyperparameter tuning and optimize
    it for cost using Spot Instances.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你将能够为大规模深度学习训练建立分析和调试程序，从而最小化不必要的成本和训练时间。你还将学会如何组织超参数调优，并利用Spot实例优化成本。
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will provide code samples so that you can develop practical
    skills. The full code examples are available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供代码示例，以便你能够培养实际技能。完整的代码示例可以在这里找到：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/)。
- en: 'To follow along with this code, you will need the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本代码进行操作，你将需要以下内容：
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有管理 Amazon SageMaker 资源权限的 AWS 账户和 IAM 用户。
- en: Have a SageMaker notebook, SageMaker Studio notebook, or local SageMaker compatible
    environment established.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置一个 SageMaker 笔记本、SageMaker Studio 笔记本，或建立一个本地的 SageMaker 兼容环境。
- en: Access to GPU training instances in your AWS account. Each example in this chapter
    will provide recommended instance types to use. You may need to increase your
    compute quota for **SageMaker Training Job** to have GPU instances enabled. In
    this case, please follow the instructions at [https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要访问 AWS 账户中的 GPU 训练实例。本章中的每个示例都会提供推荐的实例类型。你可能需要增加 **SageMaker 训练作业** 的计算配额，以启用
    GPU 实例。在这种情况下，请按照[https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml)中的说明操作。
- en: You must install the required Python libraries by running `pip install -r requirements.txt`.
    The file that contains the required libraries is located at the root of `chapter7`.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须通过运行`pip install -r requirements.txt`来安装所需的 Python 库。包含所需库的文件位于 `chapter7`
    目录的根目录下。
- en: Debugging training jobs
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试训练任务
- en: 'To effectively monitor and debug DL training jobs, we need to have access to
    the following information:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地监控和调试深度学习训练任务，我们需要访问以下信息：
- en: Scalar values such as accuracy and loss, which we use to measure the quality
    of the training process
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量值，如准确率和损失，用于衡量训练过程的质量
- en: Tensor values such as weights, biases, and gradients, which represent the internal
    state of the model and its optimizers
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量值，如权重、偏差和梯度，代表模型及其优化器的内部状态
- en: Both TensorBoard and SageMaker Debugger allow you to collect tensors and scalars,
    so both can be used to debug the model and training processes. However, unlike
    TensorBoard, which is primarily used for training visualizations, SageMaker Debugger
    provides functionality to react to changes in model states in near-real time.
    For example, it allows us to stop training jobs earlier if training loss hasn’t
    decreased for a certain period.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 和 SageMaker Debugger 都允许你收集张量和标量，因此两者都可以用于调试模型和训练过程。然而，不同于主要用于训练可视化的
    TensorBoard，SageMaker Debugger 提供了几乎实时响应模型状态变化的功能。例如，如果训练损失在一段时间内没有下降，它可以让我们提前停止训练任务。
- en: In this section, we will dive deep into how to use TensorBoard and SageMaker
    Debugger. We will review the features of both solutions in detail and then develop
    practical experiences of using both solutions to debug your training script.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨如何使用 TensorBoard 和 SageMaker Debugger。我们将详细回顾这两种解决方案的功能，然后开发使用这两种解决方案调试训练脚本的实际经验。
- en: Please note that we will use the same examples for both debugging and profiling
    tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将在调试和性能分析任务中使用相同的示例。
- en: Using TensorBoard with SageMaker
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker 中使用 TensorBoard
- en: 'TensorBoard is an open source tool developed originally for the TensorFlow
    framework, but it is now available for other DL frameworks, including PyTorch.
    TensorBoard supports the following features for visualizing and inspecting the
    training process:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是一个最初为 TensorFlow 框架开发的开源工具，但现在也支持其他深度学习框架，包括 PyTorch。TensorBoard
    支持以下功能，用于可视化和检查训练过程：
- en: Tracking scalar values (loss, accuracy, and others) over time.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时间追踪标量值（损失、准确率等）。
- en: Capturing tensors such as weights, biases, and gradients and how they change
    over time. This can be useful for visualizing weights and biases and verifying
    that they are changing expectedly.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获张量，如权重、偏差和梯度，以及它们随时间变化的情况。这对于可视化权重和偏差并验证它们是否按预期变化非常有用。
- en: Experiment tracking via a dashboard of hyperparameters.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过超参数仪表板进行实验追踪。
- en: Projecting high-dimensional embeddings to a lower-dimensionality space.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将高维嵌入投影到低维空间。
- en: Capturing images, audio, and text data.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获图像、音频和文本数据。
- en: Additionally, TensorBoard comes with native profiling capabilities for TensorFlow
    programs. Profiling is also available for PyTorch via an add-on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TensorBoard 还提供了针对 TensorFlow 程序的本地性能分析功能。通过附加组件，PyTorch 也支持性能分析。
- en: Debugging PyTorch training
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调试 PyTorch 训练
- en: 'Let’s review how TensorBoard can help you to get insights into your training
    process and debug it using a practical example. We will use a pre-trained ResNet
    model from the PyTorch model zoo and train it to recognize two classes: bees and
    ants.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 TensorBoard 如何帮助你深入了解训练过程，并通过实际示例调试它。我们将使用来自 PyTorch 模型库的预训练 ResNet
    模型，并训练它识别两种类别：蜜蜂和蚂蚁。
- en: 'We provide code highlights in this section. The full training code is available
    here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/1_TensorBoard_PyTorch.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/1_TensorBoard_PyTorch.ipynb).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中提供了代码亮点。完整的训练代码可以在这里查看：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/1_TensorBoard_PyTorch.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/1_TensorBoard_PyTorch.ipynb)。
- en: Modifying the training script
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 修改训练脚本
- en: 'To use TensorBoard, we need to make minimal changes to our training script.
    Follow these steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 TensorBoard，我们只需对训练脚本做最小的修改。请按照以下步骤操作：
- en: 'First, we must import and initialize TensorBoard’s `SummaryWriter` object.
    Here, we are using the S3 location to write TensorBoard summaries:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须导入并初始化 TensorBoard 的 `SummaryWriter` 对象。在这里，我们使用 S3 位置来写入 TensorBoard
    汇总：
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we must capture some training artifacts that won’t change during training
    – in our case, the model graph. Note that we need to execute the model’s forward
    path on the sample data to do so:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须捕获一些在训练过程中不会改变的训练文物——在我们这个案例中是模型图。请注意，我们需要在样本数据上执行模型的前向传播才能做到这一点：
- en: '[PRE1]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In our training loop, we capture the scalars and tensors that we wish to inspect.
    We use the epoch number as the time dimension. Let’s say that in our case, we
    wish to capture the following data:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的训练循环中，我们捕获了我们希望检查的标量和张量。我们使用 epoch 编号作为时间维度。假设在我们这个案例中，我们希望捕获以下数据：
- en: How accuracy and loss change every epoch for training and validation datasets
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 epoch 对于训练集和验证集的准确率和损失变化
- en: Distribution of gradients and weights on the first convolutional and last fully
    connected layers during the training phase
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练阶段，第一个卷积层和最后一个全连接层的梯度和权重分布
- en: The training hyperparameters and how they impact performance
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练超参数以及它们对性能的影响
- en: 'To capture these parameters, we must add the following code to our training
    loop:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕获这些参数，我们必须在训练循环中添加以下代码：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let’s review our training job configuration with debugging enabled.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下启用了调试的训练任务配置。
- en: Monitoring the training process
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监控训练过程
- en: 'To start the SageMaker training job, we need to provide the S3 location where
    TensorBoard summaries will be written. We can do this by setting the `tb-s3-url`
    hyperparameter, as shown here:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 SageMaker 训练任务，我们需要提供 TensorBoard 汇总文件将写入的 S3 位置。我们可以通过设置 `tb-s3-url` 超参数来实现，如下所示：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the training job has started, you can start your TensorBoard locally by
    running the following command in the terminal:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 训练任务开始后，你可以通过在终端中运行以下命令，启动本地的 TensorBoard：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note the following when using TensorBoard in cloud development environments:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在云开发环境中使用 TensorBoard 时，请注意以下事项：
- en: 'If you are using a SageMaker notebook instance, then TensorBoard will be available
    here: `https://YOUR_NOTEBOOK_DOMAIN/proxy/6006/`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用的是 SageMaker Notebook 实例，则可以通过以下地址访问 TensorBoard：`https://YOUR_NOTEBOOK_DOMAIN/proxy/6006/`
- en: 'If you are using SageMaker Studio, then TensorBoard will available here: `https://<YOUR_STUDIO_DOMAIN>/jupyter/default/proxy/6006/`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用的是 SageMaker Studio，则可以通过以下地址访问 TensorBoard：`https://<YOUR_STUDIO_DOMAIN>/jupyter/default/proxy/6006/`
- en: 'The TensorBoard data will be updated in near-real time as the training job
    progresses. Let’s review our training process in TensorBoard:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练任务的进展，TensorBoard 数据将实时更新。让我们在 TensorBoard 中回顾我们的训练过程：
- en: 'On the **Scalar** and **Time Series** tabs, you can find changes in scalar
    values over time. We use the epoch index as an indicator of time. *Figure 7.1*
    shows the training and validation accuracies at every epoch:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**标量（Scalar）**和**时间序列（Time Series）**标签页中，您可以看到标量值随时间变化的情况。我们使用 epoch 索引作为时间的指示器。*图
    7.1* 显示了每个 epoch 的训练和验证准确率：
- en: '![Figure 7.1 – Accuracies over time in TensorBoard ](img/B17519_07_001.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – TensorBoard 中随时间变化的准确率](img/B17519_07_001.jpg)'
- en: Figure 7.1 – Accuracies over time in TensorBoard
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – TensorBoard 中随时间变化的准确率
- en: On the **Graph** tab, you can see visual representations of the model and how
    data flows from inputs to outputs.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**图形（Graph）**标签页中，您可以看到模型的可视化表示，以及数据从输入到输出的流动方式。
- en: 'The `0`, which indicates that our model is learning and, hence, the absolute
    gradient values are decreasing:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`，表示我们的模型正在学习，因此绝对梯度值在下降：'
- en: '![Figure 7.2 – Histogram of model weights in TensorBoard ](img/B17519_07_002.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – TensorBoard 中模型权重的直方图](img/B17519_07_002.jpg)'
- en: Figure 7.2 – Histogram of model weights in TensorBoard
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – TensorBoard 中模型权重的直方图
- en: The **HParam** tab allows us to capture and compare hyperparameters side by
    side. This can be useful for tracking experiments during hyperparameter searches
    to identify optimal model and training job configuration.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HParam** 标签页使我们能够并排捕捉和比较超参数。这对于在超参数搜索过程中跟踪实验，识别最优的模型和训练任务配置非常有用。'
- en: Now that we understand how to use TensorBoard to visualize the training process,
    let’s see how we can use TensorBoard to profile our training job.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何使用 TensorBoard 可视化训练过程，让我们来看看如何使用 TensorBoard 对训练任务进行性能分析。
- en: Profiling PyTorch training
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能分析 PyTorch 训练
- en: TensorBoard provides out-of-the-box profiling capabilities for TensorFlow programs
    (including Keras). To profile PyTorch programs in TensorBoard, you can use the
    open source **torch_tb_profiler** plugin.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 为 TensorFlow 程序（包括 Keras）提供了开箱即用的性能分析功能。要在 TensorBoard 中对 PyTorch
    程序进行性能分析，您可以使用开源的**torch_tb_profiler**插件。
- en: 'When profiling the training process, we are usually interested in the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行训练过程的性能分析时，我们通常关心以下几个方面：
- en: How efficiently we utilize our resources (GPU and CPU) over time
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何高效地利用资源（GPU 和 CPU）随时间变化的情况
- en: What operations (DL operators, data loading, memory transfer, and so on) utilize
    what resources
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些操作（深度学习运算符、数据加载、内存传输等）使用了哪些资源
- en: In the case of distributed training, how efficient the communication is between
    nodes and individual training devices
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式训练的情况下，节点与各个训练设备之间的通信效率如何
- en: How to improve the overall resource utilization and increase training efficiency
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何提高整体资源利用率并提高训练效率
- en: Both the TensorFlow and PyTorch plugins for TensorBoard provide such capabilities
    for profiling. Let’s review how profiling works for the same task we did for debugging.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 PyTorch 插件都为 TensorBoard 提供了性能分析的功能。让我们回顾一下性能分析是如何与调试任务一起工作的。
- en: Modifying the training script
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 修改训练脚本
- en: 'To profile applications using `torch_tb_profiler`, we need to make minimal
    modifications to our training code. Specifically, we need to wrap our training
    loop with the plugin context manager, as shown in the following code block:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `torch_tb_profiler` 对应用程序进行性能分析，我们需要对训练代码进行最小的修改。具体来说，我们需要用插件上下文管理器包裹训练循环，如下代码块所示：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The parameters of the context manager that are passed at initialization time
    define what profiling data must be gathered and at what intervals. At the time
    of writing this book, the `torch_db_profiler` plugin doesn’t support writing to
    the S3 location. Hence, we must write the profiling data to the local output directory
    stored in the `"SM_OUTPUT_DATA_DIR"` environment variable. After training is done,
    SageMaker automatically archives and stores the content of this directory to the
    S3 location.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化时传递给上下文管理器的参数定义了必须收集哪些性能分析数据以及在什么时间间隔收集。在写这本书时，`torch_db_profiler` 插件不支持写入
    S3 位置。因此，我们必须将性能分析数据写入存储在 `"SM_OUTPUT_DATA_DIR"` 环境变量中的本地输出目录。训练完成后，SageMaker
    会自动将该目录的内容归档并存储到 S3 位置。
- en: Using TensorBoard Profiler
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 TensorBoard Profiler
- en: 'To review the output of TensorBoard Profiler, we need to download the data
    to the local environment:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看 TensorBoard Profiler 的输出，我们需要将数据下载到本地环境中：
- en: 'We will start by getting a path to the profiler data. For this, we can use
    the training job estimator instance:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从获取性能分析数据的路径开始。为此，我们可以使用训练任务估算器实例：
- en: '[PRE6]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, in your notebook or terminal window, you can run the following commands
    to unarchive the profiler data and start TensorBoard:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在你的笔记本或终端窗口中，你可以运行以下命令来解压分析器数据并启动 TensorBoard：
- en: '[PRE7]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Upon starting TensorBoard, you should be automatically redirected to the profiler
    summary. From there, you have access to several views that contain profiling information:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorBoard 后，你应该会自动跳转到分析器摘要页面。在这里，你可以访问几个包含分析信息的视图：
- en: 'The **Overview** tab provides a general summary of the device(s) used for training,
    their utilization over time, and the breakdown of operations. In our case, for
    example, the majority of the time is spent performing kernels that comprise forward
    and backward model passes. This is generally a good indicator that we utilize
    our GPU resources on the training model:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Overview** 标签页提供了用于训练的设备（设备）的概览，展示了它们的时间利用率和操作的拆解。例如，在我们的案例中，大部分时间都花费在执行包括前向和反向模型传递的内核上。这通常是我们在训练模型时充分利用
    GPU 资源的一个良好指标：'
- en: '![Figure 7.3 – The Overview tab of TensorBoard Profiler ](img/B17519_07_003.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – TensorBoard Profiler 的 Overview 标签页](img/B17519_07_003.jpg)'
- en: Figure 7.3 – The Overview tab of TensorBoard Profiler
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – TensorBoard Profiler 的 Overview 标签页
- en: 'The **Operators** tab gives you an idea of how much time specific operators
    consume (such as convolution or batch normalization). In the following screenshot,
    we can see, for instance, that the backward pass on convolution layers takes most
    of the GPU time:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Operators** 标签页让你了解特定操作符（如卷积或批量归一化）消耗了多少时间。在下图中，我们可以看到，例如，卷积层的反向传递占用了大部分
    GPU 时间：'
- en: '![Figure 7.4 – The Operators tab of TensorBoard Profiler ](img/B17519_07_004.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – TensorBoard Profiler 的 Operators 标签页](img/B17519_07_004.jpg)'
- en: Figure 7.4 – The Operators tab of TensorBoard Profiler
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – TensorBoard Profiler 的 Operators 标签页
- en: 'The **Kernel** tab breaks down the time spent performing on specific GPU kernels.
    For instance, in the following diagram, you can see that various **Single-Precision
    General Matrix Multiply** (**SGEMM**) kernels take most of the time:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kernel** 标签页显示了在特定 GPU 核心上执行的时间。比如在下图中，你可以看到各种 **单精度通用矩阵乘法**（**SGEMM**）内核占用了大部分时间：'
- en: '![Figure 7.5 – The Kernel tab of TensorBoard Profiler ](img/B17519_07_005.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – TensorBoard Profiler 的 Kernel 标签页](img/B17519_07_005.jpg)'
- en: Figure 7.5 – The Kernel tab of TensorBoard Profiler
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – TensorBoard Profiler 的 Kernel 标签页
- en: 'The **Trace** tab shows the timeline of the profiled operators and GPU kernels,
    as well as the handoff between CPU and GPU devices (for instance, transferring
    data inputs from CPU to GPU):'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Trace** 标签页显示了分析的操作符和 GPU 内核的时间线，以及 CPU 和 GPU 设备之间的交接（例如，将数据输入从 CPU 转移到 GPU）：'
- en: '![](img/B17519_07_006.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17519_07_006.jpg)'
- en: Figure 7.6 – The Trace tab of TensorBoard Profiler
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – TensorBoard Profiler 的 Trace 标签页
- en: 'The **Memory** tab provides memory utilization over time for a given device.
    In the following chart, for instance, you can see allocated memory (that is, memory
    used to store tensors) and total reserved memory:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Memory** 标签页提供了给定设备的内存利用情况。在下图中，你可以看到分配的内存（即用于存储张量的内存）和总保留内存：'
- en: '![Figure 7.7 – The Memory tab of TensorBoard Profiler ](img/B17519_07_007.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – TensorBoard Profiler 的 Memory 标签页](img/B17519_07_007.jpg)'
- en: Figure 7.7 – The Memory tab of TensorBoard Profiler
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – TensorBoard Profiler 的 Memory 标签页
- en: As you can see, TensorBoard is a great tool for monitoring, debugging, and profiling
    your training script. TensorBoard, when used with plugins, supports both TensorFlow
    and PyTorch frameworks. However, one of the drawbacks of TensorBoard is that it
    doesn’t provide any ways to react to suboptimal conditions, such as underutilization
    of GPU devices or slow or no convergences of your model during training. To stop
    training jobs earlier when such conditions happen, you will need to further instrumentalize
    your code via callbacks and custom logic.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，TensorBoard 是一个非常适合用于监控、调试和分析训练脚本的工具。当与插件一起使用时，TensorBoard 支持 TensorFlow
    和 PyTorch 框架。然而，TensorBoard 的一个缺点是，它没有提供任何方式来响应不理想的情况，比如 GPU 设备未充分利用，或模型在训练过程中出现收敛缓慢或无收敛的情况。为了在这种情况下提前停止训练作业，你需要通过回调和自定义逻辑进一步对代码进行工具化。
- en: SageMaker Debugger addresses these limitations by providing a generic mechanism
    to detect common training problems and take mitigation actions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger 通过提供一种通用机制来检测常见的训练问题并采取缓解措施，解决了这些局限性。
- en: Monitoring training with SageMaker Debugger
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Debugger 监控训练
- en: SageMaker Debugger is a comprehensive SageMaker capability that allows you to
    automatically monitor, debug, and profile DL training jobs running on SageMaker.
    SageMaker Debugger provides you with insights into your DL training by capturing
    the internal state of your training loop and instances metrics in near-real time.
    Debugger also allows you to automatically detect common issues happening during
    training and take appropriate actions when issues are detected. This allows you
    to automatically find issues in complex DL training jobs earlier and react accordingly.
    Additionally, SageMaker Debugger supports writing custom rules for scenarios not
    covered by built-in rules.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger是一个全面的SageMaker功能，允许你自动监控、调试和分析运行在SageMaker上的深度学习训练任务。SageMaker
    Debugger通过捕捉训练循环的内部状态和实例指标，提供近实时的深度学习训练洞察。Debugger还可以帮助你自动检测训练过程中常见问题，并在发现问题时采取适当的行动。这使得你能够在复杂的深度学习训练任务中更早地发现问题，并作出相应反应。此外，SageMaker
    Debugger还支持编写自定义规则，以应对内置规则未涵盖的场景。
- en: 'SageMaker has several key components:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker具有几个关键组件：
- en: The open source `smedebug` library (https://github.com/awslabs/sagemaker-debugger),
    which integrates with DL frameworks and Linux instances to persist debugging and
    profiling data to Amazon S3, as well as to retrieve and analyze it once the training
    job has been started
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源的`smedebug`库（https://github.com/awslabs/sagemaker-debugger），它与深度学习框架和Linux实例集成，将调试和分析数据持久化到Amazon
    S3，并在训练任务启动后检索和分析数据
- en: The SageMaker Python SDK, which allows you to configure the `smedebug` library
    with no or minimal code changes in your training script
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Python SDK，允许你在训练脚本中通过最小的代码变更来配置`smedebug`库
- en: Automatically provisioned processing jobs to validate output tensors and profiling
    data against rules
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化配置处理任务，以验证输出张量和分析数据是否符合规则
- en: SageMaker Debugger supports TensorFlow, PyTorch, and MXNet DL frameworks. The
    `smedebug` library is installed by default in SageMaker DL containers, so you
    can start using SageMaker Debugger without having to make any modifications to
    your training script. You can also install the `smdebug` library in a custom Docker
    container and use all the features of SageMaker Debugger.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger支持TensorFlow、PyTorch和MXNet深度学习框架。`smedebug`库默认安装在SageMaker深度学习容器中，因此你可以在不修改训练脚本的情况下开始使用SageMaker
    Debugger。你还可以在自定义Docker容器中安装`smdebug`库，并使用SageMaker Debugger的所有功能。
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that there are minor differences in the `smedebug` APIs for different
    DL frameworks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，不同深度学习框架的`smedebug`API可能会有细微的差异。
- en: The `smedebug` library provides a rich API for configuring, saving, and analyzing
    captured tensors. It uses a `hook` object to capture tensors and scalars by injecting
    them into the training process. `hook` allows you to group tensors and scalars
    into logical tensor `Trial` object allows you to query the stored tensors of a
    given training job for further analysis. You can run tensor queries in real time
    without waiting for the training job to be fully complete. SageMaker Debugger
    also supports emitting TensorBoard-compatible summary logs for easy visualization
    of output tensors and scalars.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`smedebug`库提供了丰富的API，用于配置、保存和分析捕获的张量。它通过在训练过程中注入`hook`对象来捕获张量和标量。`hook`允许你将张量和标量分组到逻辑张量`Trial`对象中，从而可以查询给定训练任务的存储张量进行进一步分析。你可以实时运行张量查询，而无需等待训练任务完全完成。SageMaker
    Debugger还支持生成兼容TensorBoard的摘要日志，方便可视化输出张量和标量。'
- en: Using SageMaker Debugger
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger
- en: 'Let’s apply these concepts to a practical task. We will instrumentalize the
    ResNet model and fine-tune it for a binary classification task. The full code
    is available here: https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些概念应用于一个实际任务。我们将对ResNet模型进行工具化，并针对二分类任务进行微调。完整代码请查看此处：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb)。
- en: Code instrumentalization
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码工具化
- en: 'The `smedebug` library requires minimal changes to capture tensors and scalars.
    First, you need to initiate the `hook` object outside of your training loop, as
    well as after model and optimizer initialization:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`smedebug`库只需要最少的修改即可捕获张量和标量。首先，你需要在训练循环外部初始化`hook`对象，以及在模型和优化器初始化之后：'
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we are using `.create_from_json_file()` to create our `hook` object.
    This method instantiates `hook` based on the hook configuration you provide in
    the SageMaker training object. Since we are adding both the `model` and `criterion`
    objects to `hook`, we should expect to see both model parameters (weights, biases,
    and others), as well as loss scalar.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用 `.create_from_json_file()` 方法来创建我们的 `hook` 对象。此方法基于您在 SageMaker 训练对象中提供的
    hook 配置实例化 `hook`。由于我们将 `model` 和 `criterion` 对象都添加到 `hook` 中，因此我们应该期望看到模型参数（权重、偏置等），以及损失标量。
- en: 'Inside our training loop, the only modification we need to make is to differentiate
    between the training and validation phases by switching between `smedebug.modes.Train`
    and `smedebug.modes.Eval`. This will allow `smedebug` to segregate the tensors
    that are captured in the training and evaluation phases:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练循环中，我们唯一需要修改的地方是通过在 `smedebug.modes.Train` 和 `smedebug.modes.Eval` 之间切换来区分训练阶段和验证阶段。这将使得
    `smedebug` 能够区分在训练和评估阶段捕获的张量：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s review how to configure `hook`, rules, actions, and tensor collections
    when running a SageMaker training job.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下在运行 SageMaker 训练任务时，如何配置 `hook`、规则、操作和张量集合。
- en: Training job configuration
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练任务配置
- en: 'As part of the SageMaker Python SDK, AWS provides the `sagemaker.debugger`
    library for Debugger configuration. Let’s take a look:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 SageMaker Python SDK 的一部分，AWS 提供了 `sagemaker.debugger` 库用于 Debugger 配置。让我们来看看：
- en: 'We will start by importing some Debugger entities:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入一些 Debugger 实体：
- en: '[PRE10]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we must define automatic actions and a set of rules. Here, we are using
    Debugger’s built-in rules to detect some common DL training issues. Note that
    we can assign different actions to different rules. In our case, we want to stop
    our training job immediately when the rule is triggered:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须定义自动化操作和一组规则。在这里，我们使用 Debugger 内置的规则来检测一些常见的深度学习训练问题。请注意，我们可以为不同的规则分配不同的操作。在我们的例子中，我们希望在触发规则时立即停止训练任务：
- en: '[PRE11]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we must configure the collection of tensors and how they will be persisted.
    Here, we will define that we want to persist the weights and losses collection.
    For weights, we will also save a histogram that can be further visualized in TensorBoard.
    We will also set a saving interval for the training and evaluation phases:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须配置张量的收集以及它们的持久化方式。在这里，我们将定义要持久化权重和损失的集合。对于权重，我们还将保存一个可以在 TensorBoard
    中进一步可视化的直方图。我们还将为训练和评估阶段设置保存间隔：
- en: '[PRE12]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we are ready to pass these objects to the SageMaker `Estimator` object:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备将这些对象传递给 SageMaker 的 `Estimator` 对象：
- en: '[PRE13]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, we are ready to start training the job using the `fit()` method. In the
    next section, we will learn how to retrieve and analyze SageMaker Debugger outputs.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备使用 `fit()` 方法开始训练任务。在下一节中，我们将学习如何检索和分析 SageMaker Debugger 输出。
- en: Reviewing the Debugger results
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 审查 Debugger 结果
- en: 'SageMaker Debugger provides functionality to retrieve and analyze collected
    tensors from training jobs as part of the `smedebug` library. In the following
    steps, we will highlight some key APIs:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger 提供了一个功能，可以从训练任务中检索和分析收集的张量，作为 `smedebug` 库的一部分。在接下来的步骤中，我们将介绍一些关键的
    API：
- en: 'In the following code block, we are creating a new trial object using the S3
    path where the tensors were persisted:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们使用保存张量的 S3 路径来创建一个新的 trial 对象：
- en: '[PRE14]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s output all the available tensors by running the following command:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过运行以下命令输出所有可用的张量：
- en: '[PRE15]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should be able to see multiple collections with many tensors, including
    biases, weights, losses, and gradients. Let’s access specific numeric values.
    Running the following command will return a list of associated scalar values:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该能够看到多个集合，其中包含许多张量，包括偏置、权重、损失和梯度。让我们访问具体的数值。运行以下命令将返回一组相关的标量值：
- en: '[PRE16]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using a simple plotting function (refer to the sources for its implementation),
    we can visualize loss for the training and evaluation phases. Running the following
    command will result in a 2D loss chart. Similarly, you can access and process
    tensors:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个简单的绘图函数（有关其实现，请参考源代码），我们可以可视化训练和评估阶段的损失。运行以下命令将生成一个 2D 损失图表。类似地，您可以访问和处理张量：
- en: '[PRE17]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following figure visualizes the training and validation losses:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下图可视化了训练和验证损失：
- en: '![Figure 7.8 – Training and validation losses ](img/B17519_07_008.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 训练和验证损失](img/B17519_07_008.jpg)'
- en: Figure 7.8 – Training and validation losses
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 训练和验证损失
- en: 'Now, let’s review if any rules were triggered during our training:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下在训练过程中是否触发了任何规则：
- en: '[PRE18]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This outputs all configured rules; their statuses are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出所有已配置的规则；它们的状态如下：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we can see, in our case, no rules were triggered, and our job was completed.
    You can experiment with rule settings. For instance, you can reset weights on
    one of the model layers. This will result in triggering the `PoorWeightInitiailization`
    rule and the training process being stopped.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在我们的案例中，没有触发任何规则，工作已经完成。你可以尝试不同的规则设置。例如，你可以重置模型层中的某一层权重，这将触发`PoorWeightInitiailization`规则，进而导致训练过程被停止。
- en: 'Lastly, let’s visually inspect the saved tensors using TensorBoard. For this,
    we simply need to start TensorBoard using the S3 path we supplied to the `Estimator`
    object earlier:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用TensorBoard对保存的张量进行可视化检查。为此，我们只需使用之前提供给`Estimator`对象的S3路径启动TensorBoard：
- en: '[PRE20]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Feel free to explore TensorBoard on your own. You should expect to find histograms
    of weights.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以随意探索TensorBoard。你应该会看到权重的直方图。
- en: 'In this section, we reviewed SageMaker Debugger’s key capabilities and learned
    how to use them. You may have already observed some benefits of SageMaker Debugger
    over TensorBoard:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了SageMaker Debugger的关键功能，并学习了如何使用它们。你可能已经注意到SageMaker Debugger相比于TensorBoard的一些优势：
- en: Zero or minimal effort in instrumentalizing your code for SageMaker Debugger
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在为SageMaker Debugger进行代码仪器化时几乎无需任何额外工作
- en: A rich API to process and analyze output tensors
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供强大的API来处理和分析输出张量
- en: A large number of built-in rules and actions with the ability to create custom
    rules and actions
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量内置规则和操作，支持创建自定义规则和操作
- en: TensorBoard functionality is supported out of the box
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard功能开箱即用
- en: With these capabilities, SageMaker Debugger allows you to improve the quality
    of your training jobs, accelerate experimentation, and reduce unnecessary costs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些功能，SageMaker Debugger允许你提高训练工作的质量，加速实验，并减少不必要的成本。
- en: Additionally, SageMaker Debugger provides profiling capabilities. We’ll review
    them next.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SageMaker Debugger还提供了性能分析功能。我们接下来将回顾它们。
- en: Profiling your DL training
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对你的深度学习训练进行性能分析
- en: SageMaker Debugger allows you to collect various types of advanced metrics from
    your training instances. Once these metrics have been collected, SageMaker generates
    detailed metrics visualizations, detects resource bottlenecks, and provides recommendations
    on how instance utilization can be improved.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger允许你从训练实例中收集各种类型的高级指标。一旦这些指标被收集，SageMaker将生成详细的指标可视化，检测资源瓶颈，并提供如何提高实例利用率的建议。
- en: 'SageMaker Debugger collects two types of metrics:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger收集两种类型的指标：
- en: '**System metrics**: These are the resource utilization metrics of training
    instances such as CPU, GPU, network, and I/O.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统指标**：这些是训练实例的资源利用率指标，如CPU、GPU、网络和I/O。'
- en: '**Framework metrics**: These are collected at the DL framework level. This
    includes metrics collected by native framework profiles (such as PyTorch profiler
    or TensorFlow Profiler), data loader metrics, and Python profiling metrics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架指标**：这些指标是在深度学习框架级别收集的，包括原生框架分析器（如PyTorch Profiler或TensorFlow Profiler）收集的指标、数据加载器指标和Python性能分析指标。'
- en: As in the case of debugging, you can define rules that will be automatically
    evaluated against collected metrics. If a rule is triggered, you can define one
    or several actions that will be taken. For example, you can send an email if the
    training job has GPU utilization below a certain threshold.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与调试相似，你可以定义一些规则，自动评估收集的指标。如果触发了某个规则，你可以定义一个或多个将采取的操作。例如，如果训练任务的GPU利用率低于某个阈值，你可以发送一封电子邮件。
- en: It’s time to profile our training code with SageMaker Debugger. You can find
    the full code in the *Profiling DL Training* section at https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用SageMaker Debugger对我们的训练代码进行性能分析了。你可以在https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb中的*Profiling
    DL Training*部分找到完整的代码。
- en: Configuring the training job for profiling
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置训练任务以进行性能分析
- en: 'We will start by defining what system and framework metrics we want to collect.
    For instance, we can provide a custom configuration for the framework, data loader,
    and Python. Note that system profiling is enabled by default:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义要收集的系统和框架指标。例如，我们可以为框架、数据加载器和Python提供自定义配置。请注意，系统性能分析默认是启用的：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, we must provide the profiling config to the SageMaker training job configuration:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须将分析配置提供给 SageMaker 训练作业配置：
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that we set `num-data-workers` to `8`, while `ml.p2.xlarge` has only 4
    CPU cores. Usually, it’s recommended to have the number of data workers equal
    to the number of CPUs. Let’s see if SageMaker Debugger will be able to detect
    this suboptimal configuration.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将 `num-data-workers` 设置为 `8`，而 `ml.p2.xlarge` 只有 4 个 CPU 核心。通常建议数据工作者的数量与
    CPU 数量相等。让我们看看 SageMaker Debugger 是否能够检测到这个次优配置。
- en: Reviewing profiling outcomes
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 审查分析结果
- en: 'You can start monitoring profiling outcomes in near-real time. We will use
    the `semdebug.profiler` API to process profiling outputs:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以开始实时监控分析结果。我们将使用 `semdebug.profiler` API 处理分析输出：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once the data is available, we can retrieve and visualize it. Running the following
    code will chart the CPU, GPU, and GPU memory utilization from system metrics:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据可用，我们可以提取并可视化它。运行以下代码将绘制来自系统指标的 CPU、GPU 和 GPU 内存利用率图：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similarly, you can visualize other collected metrics. SageMaker Debugger also
    generates a detailed profiling report that aggregates all visualizations, insights,
    and recommendations in one place. Once your training job has finished, you can
    download the profile report and all collected data by running the following command
    in your terminal:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您也可以可视化其他收集的指标。SageMaker Debugger 还会生成一份详细的分析报告，将所有可视化内容、见解和建议集中在一个地方。一旦您的训练作业完成，您可以通过在终端运行以下命令下载分析报告和所有收集的数据：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Once all the assets have been downloaded, open the `profiler-report.xhtml` file
    in your browser and review the generated information. Alternatively, you can open
    `profiler-report.ipynb`, which provides the same insights in the form of an executable
    Jupyter notebook.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有资产都已下载，请在浏览器中打开 `profiler-report.xhtml` 文件，并查看生成的信息。或者，您也可以打开 `profiler-report.ipynb`，它以可执行的
    Jupyter notebook 形式提供相同的见解。
- en: 'The report covers the following aspects:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 报告涵盖以下方面：
- en: System usage statistics
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统使用统计信息
- en: Framework metrics summary
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框架指标总结
- en: Summary of rules and their status
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则及其状态总结
- en: Training loop analysis and recommendations for optimizations
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环分析和优化建议
- en: Note that, in the *Dataloading analysis* section, you should see a recommendation
    to decrease the number of data workers according to our expectations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 *数据加载分析* 部分，您应该看到根据我们的预期减少数据工作者数量的建议。
- en: As you can see, SageMaker Debugger provides extensive profiling capabilities,
    including a recommendation to improve and automate rule validation with minimal
    development efforts. Similar to other Debugger capabilities, profiling is free
    of charge, so long as you are using built-in rules.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，SageMaker Debugger 提供了广泛的分析功能，包括建议改善和自动化规则验证，且开发工作量最小。与其他 Debugger 功能类似，只要使用内置规则，分析是免费的。
- en: Hyperparameter optimization
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: A SageMaker Automatic Model Tuning job allows you to run multiple training jobs
    with a unique combination of hyperparameters in parallel. In other words, a single
    tuning job creates multiple SageMaker training jobs. Hyperparameter tuning allows
    you to speed up your model development and optimization by trying many combinations
    of hyperparameters in parallel and iteratively moving toward more optimal combinations.
    However, it doesn’t guarantee that your model performance will always improve.
    For instance, if the chosen model architecture is not optimal for the task at
    hand or your dataset is too small for the chosen model, you are unlikely to see
    any improvements when running hyperparameter optimizations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 自动模型调优作业允许您并行运行多个训练作业，每个作业使用唯一的超参数组合。换句话说，一个调优作业会创建多个 SageMaker 训练作业。超参数调优通过并行尝试多个超参数组合并迭代地朝着更优的组合前进，可以加速模型开发和优化。然而，它并不保证您的模型性能总是会提高。例如，如果所选的模型架构不适合当前任务，或者您的数据集对于所选模型来说太小，那么在进行超参数优化时，您不太可能看到任何改进。
- en: 'When designing for your tuning job, you need to consider several key parameters
    of your tuning job, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计调优作业时，您需要考虑几个关键参数，如下所示：
- en: '**Search algorithm** (or **strategy**): This defines how SageMaker chooses
    the next combination of hyperparameters.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索算法**（或 **策略**）：这定义了 SageMaker 如何选择下一个超参数组合。'
- en: '**Hyperparameters with ranges**: The SageMaker search algorithm will pick hyperparameter
    values within user-defined ranges.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有范围的超参数**：SageMaker 搜索算法将在用户定义的范围内选择超参数值。'
- en: '**Objective metric**: This will be used to compare a combination of hyperparameters
    and define the best candidate. SageMaker doesn’t restrict you from choosing any
    arbitrary target metric.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标指标**：这将用于比较一组超参数并定义最佳候选值。SageMaker 不限制你选择任何任意的目标指标。'
- en: 'SageMaker supports two search strategies: **Bayesian** and **Random**. Random
    search selects the next combination of hyperparameters randomly within defined
    ranges. While it’s a simple strategy, it is considered a relatively efficient
    one. Because the next hyperparameter combination doesn’t depend on previously
    tried or currently running combinations, you can have a large number of training
    jobs running in parallel. Bayesian search selects the next combination of hyperparameters
    based on the outcomes of previous training jobs. Under the hood, SageMaker trains
    a regression model for this, which takes the results of previous jobs as input
    (hyperparameters and resulting target metrics) and outputs the candidate hyperparameter
    combination. Note that the Bayesian model may not converge. In such cases, it
    makes sense to review identified hyperparameter ranges.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 支持两种搜索策略：**贝叶斯**和**随机**。随机搜索在定义的范围内随机选择下一组超参数。尽管这是一种简单的策略，但被认为是相对高效的。因为下一组超参数的选择不依赖于之前尝试过的或当前运行的组合，所以你可以同时运行大量的训练作业。贝叶斯搜索则是基于之前训练作业的结果来选择下一组超参数。在后台，SageMaker
    为此训练了一个回归模型，该模型将之前作业的结果作为输入（超参数及其对应的目标指标），并输出候选的超参数组合。需要注意的是，贝叶斯模型可能不会收敛。在这种情况下，回顾已确定的超参数范围是有意义的。
- en: 'Choosing hyperparameters and their ranges significantly impacts your tuning
    job performance. SageMaker supports several types of hyperparameters – categorical,
    continuous, and integer. You can combine different types of hyperparameters. For
    instance, the following code defines the model architecture as a categorical hyperparameter,
    the learning rate scheduler step is defined as an integer parameter, and the learning
    rate is defined as a continuous parameter (in other words, a float type):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 选择超参数及其范围对调优作业的性能有显著影响。SageMaker 支持几种类型的超参数——分类的、连续的和整数型的。你可以组合不同类型的超参数。例如，以下代码将模型架构定义为分类超参数，学习率调度步长定义为整数型参数，而学习率定义为连续型参数（换句话说，浮动类型）：
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that for numeric hyperparameters, we also define `"Logarithmic"` scaling
    type for the learning rate parameter since its range spans multiple orders of
    magnitude. For the scheduler step size, we choose the `"Linear"` scaling type
    since its range is narrow.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于数值型超参数，我们还为学习率参数定义了“对数”缩放类型，因为它的范围跨越了多个数量级。对于调度步长，我们选择了“线性”缩放类型，因为它的范围较窄。
- en: 'You also need to define the objective metric for your hyperparameter tuning
    job. The objective metric is defined similarly to other metrics via a Regex pattern.
    Note that you need to ensure that your training script outputs your objective
    metric in the `stdout/stderr` streams. Follow these steps:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要为超参数调优作业定义目标指标。目标指标的定义类似于其他指标，通过正则表达式模式定义。请注意，你需要确保训练脚本将目标指标输出到 `stdout/stderr`
    流中。请按以下步骤操作：
- en: 'In the following code, we are defining four metrics that will be captured by
    SageMaker and then choosing `val_accuracy` as our objective metric to optimize
    for:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们定义了四个指标，这些指标将由 SageMaker 捕获，然后选择 `val_accuracy` 作为我们要优化的目标指标：
- en: '[PRE27]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we must define the parameters of the training job. Note that the hyperparameters
    that are provided as part of the training job configuration will be static and
    won’t be changed as part of the tuning job:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义训练作业的参数。请注意，作为训练作业配置一部分提供的超参数将是静态的，并且在调优作业中不会发生变化：
- en: '[PRE28]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we must combine our objective metric, metric definitions, and hyperparameter
    ranges in the `HyperParameterTuner` object, which will orchestrate the creation
    of child training jobs and track the overall status of your tuning. Additionally,
    we must provide the total max number of training jobs and the number of concurrent
    training jobs. These parameters will have an impact on how quickly the tuning
    job will run and its total cost:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须在 `HyperParameterTuner` 对象中结合我们的目标指标、指标定义和超参数范围，该对象将协调创建子训练作业并跟踪调优的整体状态。此外，我们还必须提供训练作业的最大总数和并发训练作业的数量。这些参数将影响调优作业的运行速度和总成本：
- en: '[PRE29]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Also, pay attention to the `objective_type` parameter, which defines whether
    the tuning job will try to maximize or minimize the objective metric. Since we
    chose `accuracy` as our objective metric, we want to maximize it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意 `objective_type` 参数，它定义了调优作业是要最大化还是最小化目标指标。由于我们选择了 `accuracy` 作为目标指标，因此我们希望最大化它。
- en: 'Once the tuner object has been instantiated, you can use the `.fit()` method
    to start training:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦调优对象被实例化，您可以使用 `.fit()` 方法开始训练：
- en: '[PRE30]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once the job is completed, you can analyze the outcomes of the tuning job.
    For this, you can navigate to the AWS console and inspect them visually. Alternatively,
    you can export tuner job results and statistics to a pandas DataFrame for further
    analysis, as shown here:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业完成后，您可以分析调优作业的结果。为此，您可以导航到 AWS 控制台并进行可视化检查。或者，您可以将调优作业结果和统计数据导出为 pandas DataFrame
    以进行进一步分析，如下所示：
- en: '[PRE31]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using this method, you can perform more advanced analytics, such as defining
    the correlation between various hyperparameters and objective metrics. This type
    of analysis, for instance, may uncover cases where your hyperparameter ranges
    need to be modified to further improve the target metric.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方法，您可以执行更高级的分析，例如定义各种超参数与目标指标之间的相关性。例如，这种类型的分析可能揭示出需要修改超参数范围的情况，从而进一步提升目标指标。
- en: Using EC2 Spot Instances
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 EC2 Spot 实例
- en: Running large training and model tuning jobs can be very expensive. One approach
    to minimize costs is to use EC2 Spot Instances from a pool of unused compute resources
    in a chosen AWS region. Thus, Spot Instances are considerably cheaper than regular
    on-demand instances (up to 90%). However, Spot Instances can be stopped with short
    notice if the spot capacity of the chosen instance type is exhausted in a given
    AWS region.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 运行大规模的训练和模型调优作业可能非常昂贵。最小化成本的一种方法是使用来自选定 AWS 区域的未使用计算资源池中的 EC2 Spot 实例。因此，Spot
    实例比常规按需实例便宜得多（最高可达 90%）。然而，如果所选实例类型在给定 AWS 区域的 Spot 容量耗尽，Spot 实例可能会被随时停止。
- en: SageMaker simplifies the provisioning of Spot Instances for training jobs and
    fully handles interruption and training job restarts when the spot capacity is
    available again. When the training job is interrupted and then restarted, we want
    to continue our training process rather than starting from scratch. To support
    this, your training script needs to be modified so that it can save and restart
    the training job.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 简化了为训练作业提供 Spot 实例的过程，并在 Spot 容量再次可用时完全处理中断和训练作业重启。当训练作业被中断并重新启动时，我们希望继续训练过程，而不是从头开始。为了支持这一点，您的训练脚本需要进行修改，以便能够保存并重新启动训练作业。
- en: 'To support spot training, your training script needs the following modifications:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持 Spot 训练，您的训练脚本需要以下修改：
- en: 'When loading the model for the first time, check if there is a model copy already
    available in the `/opt/ml/checkpoints` path. If the checkpointed model is available,
    this means that we trained this model previously. To continue training, we need
    to load the checkpointed model and proceed with training. If the checkpointed
    model is not available, we proceed with regular model loading:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当首次加载模型时，检查 `/opt/ml/checkpoints` 路径中是否已有模型副本。如果检查点模型已存在，说明我们之前训练过此模型。要继续训练，我们需要加载检查点模型并继续训练。如果检查点模型不存在，则继续常规的模型加载：
- en: '![Figure 7.9 – Uploading the checkpoint artifacts to S3 storage ](img/B17519_07_009.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 将检查点工件上传到 S3 存储](img/B17519_07_009.jpg)'
- en: Figure 7.9 – Uploading the checkpoint artifacts to S3 storage
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 将检查点工件上传到 S3 存储
- en: 'In your training script, you need to specify the checkpoint handler (refer
    to the DL framework documentation for this) and store your model checkpoints in
    the designated directory – that is, `/opt/ml/checkpoints`. In the case of Spot
    Instance interruption, SageMaker will automatically copy the content of this directory
    to S3\. When the Spot Instance is available again, SageMaker will copy your checkpoints
    from S3 back to the `/opt/ml/checkpoints` directory:'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的训练脚本中，你需要指定检查点处理程序（请参考深度学习框架的文档），并将模型检查点存储在指定的目录中——即 `/opt/ml/checkpoints`。如果
    Spot 实例被中断，SageMaker 将自动将该目录的内容复制到 S3。Spot 实例恢复可用后，SageMaker 会从 S3 将检查点复制回 `/opt/ml/checkpoints`
    目录：
- en: '![Figure 7.10 – Restoring the checkpoint artifacts from S3 storage ](img/B17519_07_010.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 从 S3 存储恢复检查点文件](img/B17519_07_010.jpg)'
- en: Figure 7.10 – Restoring the checkpoint artifacts from S3 storage
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 从 S3 存储恢复检查点文件
- en: When using Spot Instances, please be aware that using spot training may result
    in longer and unpredictable training times. Each Spot Instance interruption will
    result in additional startup time during restart. The amount of available spot
    capacity depends on the instance type and AWS region. GPU-based instance types
    in certain AWS regions may have very limited spot capacity. Note that spot capacity
    constantly fluctuates. You can use the **Amazon Spot Instance advisor** feature
    to determine available spot capacity for different EC2 instances, the chance of
    interruption, and cost savings compared to regular on-demand instances.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Spot 实例时，请注意，使用 Spot 训练可能会导致训练时间变长且不可预测。每次 Spot 实例中断都会导致重启时额外的启动时间。可用的 Spot
    容量取决于实例类型和 AWS 区域。在某些 AWS 区域，基于 GPU 的实例类型可能具有非常有限的 Spot 容量。请注意，Spot 容量会不断波动。你可以使用**Amazon
    Spot 实例顾问**功能，来确定不同 EC2 实例的可用 Spot 容量、中断的可能性以及与常规按需实例相比的成本节省。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter concludes *Part 2* of this book. In this and the two previous chapters,
    we discussed how to build and optimize large-scale training jobs. First, we reviewed
    the available specialized hardware for DL training and how to choose optimal instance
    types. Then, we discussed how to engineer distributed training using open source
    and Amazon proprietary solutions. In this chapter, we discussed how to efficiently
    operationalize your model training. We reviewed different issues that may occur
    during training and how to detect and mitigate them. We also discussed how to
    manage and optimize hyperparameter tuning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了本书的*第2部分*。在本章及前两章中，我们讨论了如何构建和优化大规模训练作业。首先，我们回顾了可用于深度学习训练的专业硬件，并介绍了如何选择最优的实例类型。接着，我们讨论了如何使用开源解决方案和
    Amazon 专有解决方案来进行分布式训练。在本章中，我们讨论了如何高效地将模型训练操作化。我们回顾了训练过程中可能遇到的不同问题，以及如何检测和缓解这些问题。我们还讨论了如何管理和优化超参数调优。
- en: In *Part 3*, *Serving Deep Learning Models*, we will dive deep into DL inference
    on Amazon SageMaker. We will discuss what hardware is available for inference
    and how to engineer your inference server. Then, we will review the operational
    aspects of model serving. In the next chapter, [*Chapter 8*](B17519_08.xhtml#_idTextAnchor121),
    *Considering Hardware for Inference*, we will review the available hardware accelerators
    suitable for inference workloads, discuss selection criteria, and explain how
    you can optimize your model for inference on specific hardware accelerators using
    model compilers and SageMaker Neo.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3部分*，*提供深度学习模型服务*中，我们将深入探讨在 Amazon SageMaker 上进行深度学习推理的过程。我们将讨论可用于推理的硬件，以及如何设计你的推理服务器。然后，我们将回顾模型服务的操作方面。在下一章，[*第8章*](B17519_08.xhtml#_idTextAnchor121)，*考虑推理硬件*中，我们将回顾适用于推理工作负载的硬件加速器，讨论选择标准，并解释如何使用模型编译器和
    SageMaker Neo 对你的模型进行优化，以便在特定硬件加速器上进行推理。
- en: 'Part 3: Serving Deep Learning Models'
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：提供深度学习模型服务
- en: In this chapter, we will focus on hosting trained models on Amazon SageMaker.
    We will review available software and hardware options and provide recommendations
    on what to choose and when.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点讨论如何在 Amazon SageMaker 上托管训练好的模型。我们将回顾可用的软件和硬件选项，并提供选择建议以及何时使用它们的指导。
- en: 'This section comprises the following chapters:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 8*](B17519_08.xhtml#_idTextAnchor121), *Considering Hardware for
    Inference*'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B17519_08.xhtml#_idTextAnchor121)，*考虑推理硬件*'
- en: '[*Chapter 9*](B17519_09.xhtml#_idTextAnchor137), *Implementing Model Servers*'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B17519_09.xhtml#_idTextAnchor137)，*实现模型服务器*'
- en: '[*Chapter 10*](B17519_10.xhtml#_idTextAnchor154), *Operationalizing Inference
    Workloads*'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B17519_10.xhtml#_idTextAnchor154)，*操作化推理工作负载*'
