- en: Deep Learning Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础
- en: In this chapter, I am going to introduce the core concepts of **Deep Learning**
    (**DL**), the relationship it has with **Machine Learning** (**ML**) and **Artificial
    Intelligence** (**AI**), the different types of multilayered neural networks,
    and a list of real-world practical applications. I will try to skip mathematical
    equations as much as possible and keep the description very high level, with no
    reference to code examples. The goal of this chapter is to make readers aware
    of what DL really is and what you can do with it, while the following chapters
    will go much more into the details of this, with lots of practical code examples
    in Scala and Python (where this programming language can be used).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将介绍**深度学习**（**DL**）的核心概念，它与**机器学习**（**ML**）和**人工智能**（**AI**）的关系，各种类型的多层神经网络，以及一些现实世界中的实际应用。我将尽量避免数学公式，并保持描述的高层次，不涉及代码示例。本章的目的是让读者了解深度学习的真正含义以及它能做什么，而接下来的章节将更详细地讲解这一内容，并提供大量Scala和Python中的实际代码示例（这些编程语言可以使用）。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: DL concepts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习概念
- en: '**Deep neural networks** (**DNNs**)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度神经网络** (**DNNs**)'
- en: Practical applications of DL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的实际应用
- en: Introducing DL
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度学习
- en: 'DL is a subset of ML that can solve particularly hard and large-scale problems
    in areas such as **Natural Language Processing** (**NLP**) and image classification.
    The expression DL is sometimes used in an interchangeable way with ML and AI,
    but both ML and DL are subsets of AI. AI is the broader concept that is implemented
    through ML. DL is a way of implementing ML, and involves neural network-based
    algorithms:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习（ML）的一个子集，可以解决特别困难和大规模的问题，应用领域包括**自然语言处理** (**NLP**)和图像分类。DL这个术语有时与ML和AI互换使用，但ML和DL都是AI的子集。AI是更广泛的概念，它通过ML来实现。DL是实现ML的一种方式，涉及基于神经网络的算法：
- en: '![](img/127ce50b-0d72-4411-84c9-51b9f69d9eca.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/127ce50b-0d72-4411-84c9-51b9f69d9eca.png)'
- en: Figure 2.1
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1
- en: AI is considered the ability of a machine (it could be any computer-controlled
    device or robot) to perform tasks that are typically associated with humans. It
    was introduced in the 1950s, with the goal of reducing human interaction, thereby
    making the machine do all the work. This concept is mainly applied to the development
    of systems that typically require human intellectual processes and/or the ability
    to learn from past experiences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）被认为是机器（它可以是任何计算机控制的设备或机器人）执行通常与人类相关的任务的能力。该概念于20世纪50年代提出，目的是减少人类的互动，从而让机器完成所有工作。这个概念主要应用于开发那些通常需要人类智力过程和/或从过去经验中学习的系统。
- en: 'ML is an approach that''s used to implement AI. It is a field of computer science
    that gives computer systems the ability to learn from data without being explicitly
    programmed. Basically, it uses algorithms to find patterns in data and then uses
    a model that recognizes those patterns to make predictions on new data. The following
    diagram shows the typical process that''s used to train and build a model:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是一种实现人工智能（AI）的方法。它是计算机科学的一个领域，使计算机系统能够从数据中学习，而不需要显式编程。基本上，它使用算法在数据中寻找模式，然后使用能够识别这些模式的模型对新数据进行预测。下图展示了训练和构建模型的典型过程：
- en: '![](img/889161c2-bb5f-492b-8a5b-3acdb5deb12d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/889161c2-bb5f-492b-8a5b-3acdb5deb12d.png)'
- en: Figure 2.2
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2
- en: 'ML can be classified into three types:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以分为三种类型：
- en: Supervised learning algorithms, which use labeled data
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习算法，使用标注数据
- en: Unsupervised learning algorithms, which find patterns, starting from unlabeled
    data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习算法，从未标注数据中发现模式
- en: Semi-supervised learning, which uses a mix of the two (labeled and unlabeled
    data)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习，使用两者的混合（标注数据和未标注数据）
- en: At the time of writing, supervised learning is the most common type of ML algorithm.
    Supervised learning can be divided into two groups – regression and classification
    problems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 截至写作时，有监督学习是最常见的机器学习算法类型。有监督学习可以分为两类——回归和分类问题。
- en: 'The following graph shows a simple regression problem:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个简单的回归问题：
- en: '![](img/3c36b426-c6d8-4403-b913-662050ffd368.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c36b426-c6d8-4403-b913-662050ffd368.png)'
- en: Figure 2.3
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3
- en: As you can see, there are two inputs (or features), **Size** and **Price**,
    which are used to generate a curve-fitting line and make subsequent predictions
    of the property price.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，图中有两个输入（或特征），**大小**和**价格**，它们被用来生成曲线拟合线，并对房产价格进行后续预测。
- en: 'The following graph shows an example of supervised classification:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了一个监督分类的示例：
- en: '![](img/acc798aa-87f2-4f2c-98e4-145c0bcb6373.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acc798aa-87f2-4f2c-98e4-145c0bcb6373.png)'
- en: Figure 2.4
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4
- en: The dataset is labeled with benign (circles) and malignant (crosses) tumors
    for breast cancer patients. A supervised classification algorithm attempts, by
    fitting a line through the data, to part the tumors into two different classifications.
    Future data would then be classified as benign or malignant based on that straight-line
    classification. The case in the preceding graph has only two discrete outputs,
    but there are cases where there could be more than two classifications as well
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集标记了良性（圆圈）和恶性（叉号）肿瘤，针对乳腺癌患者。一个监督分类算法通过拟合一条直线将数据分为两类。然后，基于该直线分类，未来的数据将被分类为良性或恶性。前述图表中的情况只有两个离散输出，但也有可能存在超过两种分类的情况。
- en: 'While in supervised learning, labeled datasets help the algorithm determine
    what the correct answer is, in unsupervised learning, an algorithm is provided
    with an unlabeled dataset and depends on the algorithm itself to uncover structures
    and patterns in the data. In the following graphs (the graph on the right can
    be found at [https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png)[Images/supervised_unsupervised.png](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png)),
    no information is provided about the meaning of each data point. We ask the algorithm
    to find a structure in the data in a way that is independent of supervision. An
    unsupervised learning algorithm could find that there are two distinct clusters
    and then perform straight-line classification between them:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，带标签的数据集帮助算法确定正确答案，而在无监督学习中，算法提供未标记的数据集，依赖于算法本身来发现数据中的结构和模式。在以下图表中（右侧的图表可以在 [https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png)[Images/supervised_unsupervised.png](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png) 查看），没有提供关于每个数据点的含义信息。我们要求算法以独立于监督的方式发现数据中的结构。一个无监督学习算法可能会发现数据中有两个不同的簇，然后在它们之间进行直线分类：
- en: '![](img/d705080d-f433-4b30-af21-182ca64e5d72.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d705080d-f433-4b30-af21-182ca64e5d72.png)'
- en: Figure 2.5
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5
- en: '**DL** is the name for multilayered neural networks, which are networks that
    are composed of several hidden layers of nodes between the input and output. DL
    is a refinement of **Artificial Neural Networks** (**ANNs**), which emulate how
    the human brain learns (even if not closely) and how it solves problems. ANNs
    consist of an interconnected group of neurons, similar to the way neurons work
    in the human brain. The following diagram represents the general model of ANNs:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习（DL）**是指多层神经网络的名称，这些网络由输入和输出之间的多个隐藏层节点组成。DL是**人工神经网络（ANNs）**的细化版，模拟了人类大脑的学习方式（尽管并不完全相同）以及解决问题的方法。ANNs由一个互联的神经元群体组成，类似于人脑中神经元的工作方式。以下图示表示ANN的通用模型：'
- en: '![](img/5f6589c0-3379-4137-aac8-dcb67a14a15b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f6589c0-3379-4137-aac8-dcb67a14a15b.png)'
- en: Figure 2.6
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6
- en: 'A neuron is the atomic unit of an ANN. It receives a given number of input
    (*x[i]*) before executing computation on it and finally sends the output to other
    neurons in the same network. The weights (*w[j]*), or *parameters*, represent
    the strength of the input connection – they can assume positive or negative values.
    The net input can be calculated as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是人工神经网络（ANN）的基本单元。它接收一定数量的输入（*x[i]*），对其进行计算，然后最终将输出发送到同一网络中的其他神经元。权重（*w[j]*），或*参数*，表示输入连接的强度——它们可以是正值或负值。网络输入可以按以下公式计算：
- en: '*y[in] = x[1] X w[1] + x[2] X w[2] + x[3] X w[3] + … + x[n] X w[n]*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*y[in] = x[1] X w[1] + x[2] X w[2] + x[3] X w[3] + … + x[n] X w[n]*'
- en: 'The output can be calculated by applying the activation function over the net
    input:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可以通过对网络输入应用激活函数来计算：
- en: '*y = f(y[in])*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = f(y[in])*'
- en: The activation function allows an ANN to model complex non-linear patterns that
    simpler models may not represent correctly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数使人工神经网络（ANN）能够建模复杂的非线性模式，而简单的模型可能无法正确表示这些模式。
- en: 'The following diagram represents a neural network:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示表示一个神经网络：
- en: '![](img/77e12f41-77b6-40ad-b0a9-6279e6220ca3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77e12f41-77b6-40ad-b0a9-6279e6220ca3.png)'
- en: Figure 2.7
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7
- en: The first layer is the input layer – this is where features are put into the
    network. The last one is the output layer. Any layer in between that is not an
    input or output layer is a hidden layer. The term DL is used because of the multiple
    levels of hidden layers in neural networks that are used to resolve complex non-linear
    problems. At each layer level, any single node receives input data and a weight,
    and will then output a confidence score to the nodes of the next layer. This process
    happens until the output layer is reached. The error of the score is calculated
    on that layer. The errors are then sent back and the weights of the network are
    adjusted to improve the model (this is called **backpropagation** and happens
    inside a process called **gradient descent**, which we will discuss in [Chapter
    6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml), *Recurrent Neural Networks*).
    There are many variations of neural networks – more about them in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是输入层——这是将特征输入网络的地方。最后一层是输出层。任何不属于输入层或输出层的中间层都是隐藏层。之所以称为DL，是因为神经网络中存在多个隐藏层，用来解决复杂的非线性问题。在每一层中，任何单个节点都会接收输入数据和一个权重，并将一个置信度分数输出给下一层的节点。这个过程会一直进行，直到到达输出层。这个分数的误差会在该层计算出来。然后，误差会被发送回去，调整网络的权重，从而改进模型（这被称为**反向传播**，并发生在一个叫做**梯度下降**的过程中，我们将在[第六章](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml)中讨论，*循环神经网络*）。神经网络有许多变种——更多内容将在下一部分介绍。
- en: 'Before moving on, a final observation. You''re probably wondering why most
    of the concepts behind AI, ML, and DL have been around for decades, but have only
    been hyped up in the past 4 or 5 years. There are several factors that accelerated
    their implementation and made it possible to move them from theory to real-world
    applications:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，最后一个观察点。你可能会想，为什么AI、ML和DL背后的大多数概念已经存在了几十年，但在过去的4到5年才被炒作起来？有几个因素加速了它们的实施，并使其从理论走向现实应用：
- en: '**Cheaper computation**: In the last few decades, hardware has been a constraining
    factor for AI/ML/DL. Recent advances in both hardware (coupled with improved tools
    and software frameworks) and new computational models (including those around
    GPUs) have accelerated AI/ML/DL adoption.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更便宜的计算**：在过去几十年中，硬件一直是AI/ML/DL的制约因素。近期硬件（结合改进的工具和软件框架）以及新计算模型（包括围绕GPU的模型）的进步，加速了AI/ML/DL的采用。'
- en: '**Greater data availability**: AI/ML/DL needs a huge amount of data to learn.
    The digital transformation of society is providing tons of raw material to move
    forward quickly. Big data now comes from diverse sources such as IoT sensors,
    social and mobile computing, smart cars, healthcare devices, and many others that
    are or will be used to train models.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更大的数据可用性**：AI/ML/DL需要大量的数据来进行学习。社会的数字化转型正在提供大量原始材料，推动快速发展。大数据如今来自多种来源，如物联网传感器、社交和移动计算、智能汽车、医疗设备等，这些数据已经或将被用于训练模型。'
- en: '**Cheaper storage**: The increased amount of available data means that more
    space is needed for storage. Advances in hardware, cost reduction, and improved
    performance have made the implementation of new storage systems possible, all
    without the typical limitations of relational databases.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更便宜的存储**：可用数据量的增加意味着需要更多的存储空间。硬件的进步、成本的降低和性能的提高使得新存储系统的实现成为可能，而这一切都没有传统关系型数据库的限制。'
- en: '**More advanced algorithms**: Less expensive computation and storage enable
    the development and training of more advanced algorithms that also have impressive
    accuracy when solving specific problems such as image classification and fraud
    detection.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更先进的算法**：更便宜的计算和存储使得更先进的算法得以开发和训练，这些算法在解决特定问题时，如图像分类和欺诈检测，展现了令人印象深刻的准确性。'
- en: '**More, and bigger, investments**: Last but not least, investment in AI is
    no longer confined to universities or research institutes, but comes from many
    other entities, such as tech giants, governments, start-ups, and large enterprises
    across almost every business area.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多、更大规模的投资**：最后但同样重要的是，人工智能的投资不再仅仅局限于大学或研究机构，而是来自许多其他实体，如科技巨头、政府、初创公司和各行各业的大型企业。'
- en: DNNs overview
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN概述
- en: As stated in the previous section, a DNN is an ANN with multiple hidden layers
    between the input and output layers. Typically, they are feedforward networks
    in which data flows from the input layer to the output layer without looping back,
    but there are different flavors of DNNs – among them, those with the most practical
    applications are **Convolutional Neural Networks** (**CNNs**) and **Recurrent
    Neural Networks** (**RNNs**).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，DNN 是一种在输入层和输出层之间具有多个隐藏层的人工神经网络（ANN）。通常，它们是前馈网络，其中数据从输入层流向输出层，不会回传，但
    DNN 有不同的变种——其中，最具实际应用的是**卷积神经网络**（**CNNs**）和**递归神经网络**（**RNNs**）。
- en: CNNs
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs
- en: The most common use case scenarios of CNNs are all to do with image processing,
    but are not restricted to other types of input, whether it be audio or video.
    A typical use case is image classification – the network is fed with images so
    that it can classify the data. For example, it outputs a lion if you give it a
    lion picture, a tiger when you give it a tiger picture, and so on. The reason
    why this kind of network is used for image classification is because it uses relatively
    little preprocessing compared to other algorithms in the same space – the network
    learns the filters that, in traditional algorithms, were hand-engineered.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 最常见的应用场景都与图像处理相关，但并不限于其他类型的输入，无论是音频还是视频。一个典型的应用场景是图像分类——网络接收图像输入，以便对数据进行分类。例如，当你给它一张狮子图片时，它输出狮子，当你给它一张老虎图片时，它输出老虎，依此类推。之所以使用这种网络进行图像分类，是因为它相对于同领域的其他算法来说，预处理工作量较小——网络学习到的滤波器，传统算法是人工设计的。
- en: Being a multilayered neural network, A CNN consists of an input and an output
    layer, as well as multiple hidden layers. The hidden layers can be convolutional,
    pooling, fully connected, and normalization layers. Convolutional layers apply
    a convolution operation ([https://en.wikipedia.org/wiki/Convolution](https://en.wikipedia.org/wiki/Convolution))
    to an input, before passing the result to the next layer. This operation emulates
    how the response of an individual physical neuron to a visual stimulus is generated.
    Each convolutional neuron processes only the data for its receptive field (which
    is the particular region of the sensory space of an individual sensory neuron
    in which a change in the environment will modify the firing of that neuron). Pooling
    layers are responsible for combining the outputs of clusters of neurons in a layer
    into a single neuron in the next layer. There are different implementations of
    poolings—max pooling, which uses the maximum value from each cluster from the
    prior layer; average pooling, which uses the average value from any cluster of
    neurons on the prior layer; and so on. Fully connected layers, instead, as you
    will clearly realize from their name, connect every neuron in a layer to every
    other neuron in another layer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个多层神经网络，CNN 由输入层、输出层以及多个隐藏层组成。隐藏层可以是卷积层、池化层、全连接层和归一化层。卷积层对输入进行卷积运算（[https://en.wikipedia.org/wiki/Convolution](https://en.wikipedia.org/wiki/Convolution)），然后将结果传递给下一个层。这个操作模拟了个体物理神经元对视觉刺激的响应生成方式。每个卷积神经元仅处理其感受野中的数据（感受野是指个体感官神经元的感官空间中，环境变化会改变该神经元的放电情况的特定区域）。池化层负责将一个层中神经元群的输出合并成下一层的单一神经元。池化有不同的实现方式——最大池化，使用来自前一层每个群体的最大值；平均池化，使用前一层任何神经元群的平均值；等等。全连接层则顾名思义，将一层中的每个神经元与另一层中的每个神经元连接起来。
- en: CNNs don't parse all the training data at once, but they usually start with
    a sort of input scanner. For example, consider an image of 200 x 200 pixels as
    input. In this case, the model doesn't have a layer with 40,000 nodes, but a scanning
    input layer of 20 x 20, which is fed using the first 20 x 20 pixels of the original
    image (usually, starting in the upper-left corner). Once we have passed that input
    (and possibly used it for training), we feed it using the next 20 x 20 pixels
    (this will be explained better and in a more detailed manner in [Chapter 5](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml),
    *Convolutional Neural Networks*; the process is similar to the movement of a scanner,
    one pixel to the right). Please note that the image isn't dissected into 20 x
    20 blocks, but the scanner moves over it. This input data is then fed through
    one or more convolutional layers. Each node of those layers only has to work with
    its close neighboring cells—not all of the nodes are connected to each other.
    The deeper a network becomes, the more its convolutional layers shrink, typically
    following a divisible factor of the input (if we started with a layer of 20, then,
    most probably, the next one would be a layer of 10 and the following a layer of
    5). Powers of two are commonly used as divisible factors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 并不会一次性解析所有训练数据，但它们通常从某种输入扫描器开始。例如，考虑一张 200 x 200 像素的图像作为输入。在这种情况下，模型没有一个包含
    40,000 个节点的层，而是一个 20 x 20 的扫描输入层，该层使用原始图像的前 20 x 20 像素（通常从左上角开始）。一旦我们处理完该输入（并可能用它进行训练），我们就会使用下一个
    20 x 20 像素输入（这一过程将在[第 5 章](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml)，*卷积神经网络*中更详细地解释；这个过程类似于扫描仪的移动，每次向右移动一个像素）。请注意，图像并不是被分解成
    20 x 20 的块，而是扫描仪在其上移动。然后，这些输入数据会通过一个或多个卷积层。每个卷积层的节点只需要与其邻近的节点工作——并不是所有的节点都互相连接。网络越深，它的卷积层越小，通常遵循输入的可分因子（如果我们从
    20 的层开始，那么下一个层很可能是 10，接下来是 5）。通常使用 2 的幂作为可分因子。
- en: 'The following diagram (by Aphex34—own work, CC BY-SA 4.0, [https://commons.wikimedia.org/w/index.php?curid=45679374](https://commons.wikimedia.org/w/index.php?curid=45679374))
    shows the typical architecture of a CNN:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图（由 Aphex34 制作，CC BY-SA 4.0，[https://commons.wikimedia.org/w/index.php?curid=45679374](https://commons.wikimedia.org/w/index.php?curid=45679374)）展示了
    CNN 的典型架构：
- en: '![](img/dccc596d-f722-451b-9a59-b5dac59e4f8f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dccc596d-f722-451b-9a59-b5dac59e4f8f.png)'
- en: Figure 2.8
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8
- en: RNNs
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）
- en: RNNs are primarily popular for many NLP tasks (even if they are currently being
    used in different scenarios, which we will talk about in [Chapter 6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml),
    *Recurrent Neural Networks*). What's different about RNNs? Their peculiarity is
    that the connections between units form a directed graph along a sequence. This
    means that an RNN can exhibit a dynamic temporal behavior for a given time sequence.
    Therefore, they can use their internal state (memory) to process sequences of
    inputs, while in a traditional neural network, we assume that all inputs and outputs
    are independent of each other. This makes RNNs suitable for cases such as those,
    for example, when we want to predict the next word in a sentence – it is definitely
    better to know which words came before it. Now, you can understand why they are
    called recurrent – the same task is performed for every element of a sequence,
    with the output being dependent on the previous computations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 主要因许多 NLP 任务而流行（即使它们目前也被用于不同的场景，我们将在[第 6 章](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml)，*循环神经网络*中讨论）。RNN
    的独特之处是什么？它们的特点是单元之间的连接形成一个沿着序列的有向图。这意味着 RNN 可以展示给定时间序列的动态时间行为。因此，它们可以使用内部状态（记忆）来处理输入序列，而在传统神经网络中，我们假设所有输入和输出彼此独立。这使得
    RNN 适用于某些场景，例如当我们想要预测句子中的下一个词时——知道它前面的词肯定更有帮助。现在，你可以理解为什么它们被称为“循环”——每个序列元素都执行相同的任务，且输出依赖于之前的计算。
- en: 'RNNs have loops in them, allowing information to persist, like so:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 中有循环结构，允许信息保持，如下所示：
- en: '![](img/c18d6c33-bc28-4b76-8804-5d5ccff8990a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c18d6c33-bc28-4b76-8804-5d5ccff8990a.png)'
- en: Figure 2.9
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9
- en: 'In the preceding diagram, a chunk of the neural network, **H**, receives some
    input, **x** and outputs a value, **o**. A loop allows information to be passed
    from one step of the network to the next. By unfolding the RNN in this diagram
    into a full network (as shown in the following diagram), it can be thought of
    as multiple copies of the same network, each passing information to a successor:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，神经网络的一部分，**H**，接收一些输入，**x**，并输出一个值，**o**。一个循环允许信息从网络的一个步骤传递到下一个步骤。通过展开图中的RNN，形成一个完整的网络（如以下图所示），它可以被看作是多个相同网络的副本，每个副本将信息传递给后续步骤：
- en: '![](img/582aac54-452c-4688-b416-f4fc1b84bce4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/582aac54-452c-4688-b416-f4fc1b84bce4.png)'
- en: Figure 2.10
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10
- en: Here, **x[t]** is the input at time step **t**, **H[t]** is the hidden state
    at time step **t** (and represents the memory of the network), and **o[t]** is
    the output at step **t**. The hidden states capture information about what happened
    in all the previous time steps. The output at a given step is calculated based
    only on the memory at time **t**. An RNN shares the same parameters across every
    step—that's because the same task is performed at each step; it just has different
    inputs—drastically reduces the total number of parameters it needs to learn. Outputs
    aren't necessary at each step, since this depends on the task at hand. Similarly,
    inputs aren't always needed at each time step.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**x[t]** 是时间步 **t** 的输入，**H[t]** 是时间步 **t** 的隐藏状态（代表网络的记忆），而 **o[t]** 是时间步
    **t** 的输出。隐藏状态捕捉了所有前一步骤中发生的事情的信息。给定步骤的输出仅基于时间 **t** 的记忆进行计算。RNN在每个步骤中共享相同的参数——这是因为每个步骤执行的是相同的任务，只是输入不同——大大减少了它需要学习的总参数数量。每个步骤的输出不是必需的，因为这取决于当前的任务。同样，并非每个时间步都需要输入。
- en: 'RNNs were first developed in the 1980s and only lately have they come in many
    new variants. Here''s a list of some of those architectures:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RNN最早在1980年代开发，直到最近才有了许多新的变种。以下是其中一些架构的列表：
- en: '**Fully recurrent**: Every element has a weighted one-way connection to every
    other element in the architecture and has a single feedback connection to itself.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全递归**：每个元素与架构中的每个其他元素都有一个加权的单向连接，并且与自身有一个单一的反馈连接。'
- en: '**Recursive**: The same set of weights is applied recursively over a structure,
    which resembles a graph structure. During this process, the structure is traversed
    in topological sorting ([https://en.wikipedia.org/wiki/Topological_sorting](https://en.wikipedia.org/wiki/Topological_sorting)).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归**：相同的权重集在结构中递归地应用，这种结构类似于图形结构。在此过程中，结构会按拓扑排序进行遍历（[https://en.wikipedia.org/wiki/Topological_sorting](https://en.wikipedia.org/wiki/Topological_sorting)）。'
- en: '**Hopfield**: All of the connections are symmetrical. This is not suitable
    in scenarios where sequences of patterns need to be processed, as it requires
    stationary inputs only.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**霍普菲尔德网络**：所有的连接都是对称的。这种网络不适用于需要处理模式序列的场景，因为它只需要静态输入。'
- en: '**Elman network**: This is a three-layer network, arranged horizontally, plus
    a set of so-called **context units**. The middle hidden layer is connected to
    all of them, with a fixed weight of 1\. What happens at each time step is that
    the input is fed forward and then a learning rule is applied. Because the back-connections
    are fixed, a copy of the previous values of the hidden units is saved in the context
    units. This is the way the network can maintain a state. For this reason, this
    kind of RNN allows you to perform tasks that are beyond the power of a standard
    multilayered neural network.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**埃尔曼网络**：这是一个三层网络，横向排列，外加一组所谓的**上下文单元**。中间的隐藏层与所有这些单元连接，权重固定为1。在每个时间步，输入被前馈，然后应用一个学习规则。由于反向连接是固定的，隐藏单元的前一值会被保存在上下文单元中。这样，网络就能保持状态。正因如此，这种类型的RNN允许你执行一些标准多层神经网络无法完成的任务。'
- en: '**Long short-term memory (LSTM)**: This is a DL that prevents back-propagated
    errors from vanishing or exploding gradients (this will be covered in more detail
    in [Chapter 6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml), *Recurrent Neural
    Networks*). Errors can flow backward through (in theory) an unlimited number of
    virtual layers unfolded in space. This means that an LSTM can learn tasks that
    require memories of events that could have happened several time steps earlier.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆（LSTM）**：这是一种深度学习方法，防止反向传播的错误消失或梯度爆炸（这一点将在[第6章](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml)，*递归神经网络*中详细讲解）。错误可以通过（理论上）无限数量的虚拟层向后流动。也就是说，LSTM可以学习需要记住可能发生在几个时间步之前的事件的任务。'
- en: '**Bi-directional**: By concatenating the outputs of two RNNs, it can predict
    each element of a finite sequence. The first RNN processes the sequence from left
    to right, while the second one does so in the opposite direction.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向**：通过连接两个RNN的输出，可以预测有限序列中的每个元素。第一个RNN从左到右处理序列，而第二个RNN则以相反的方向进行处理。'
- en: '**Recurrent multilayer perceptron network**: This consists of cascaded subnetworks,
    each containing multiple layers of nodes. Each subnetwork, except for the last
    layer (the only one that can have feedback connections), is feed-forward.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归多层感知器网络**：由级联子网络组成，每个子网络包含多个节点层。除最后一层（唯一可以有反馈连接的层）外，其他子网络都是前馈的。'
- en: '[Chapter 5](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml), *Convolutional Neural
    Networks*, and [Chapter 6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml), *Recurrent
    Neural Networks*, will go into more detail about CNNs and RNNs.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml)，*卷积神经网络*，以及[第6章](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml)，*递归神经网络*，将详细讲解CNN和RNN。'
- en: Practical applications of DL
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的实际应用
- en: 'The DL concepts and models that were illustrated in the previous two sections
    aren''t just pure theory – practical applications have been implemented from them.
    DL excels at identifying patterns in unstructured data; most use cases are related
    to media such as images, sound, video, and text. Nowadays, DL is applied in a
    number of use case scenarios across different business domains, such as the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 前两部分所介绍的深度学习概念和模型不仅仅是纯理论——实际上，已经有许多应用基于这些概念和模型得以实现。深度学习擅长识别非结构化数据中的模式；大多数应用场景与图像、声音、视频和文本等媒体相关。如今，深度学习已经应用于多个商业领域的众多场景，包括以下几种：
- en: '**Computer vision**: A number of applications in the automotive industry, facial
    recognition, motion detection, and real-time threat detection'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算机视觉**：在汽车工业、面部识别、动作检测和实时威胁检测等方面的应用。'
- en: '**NLP**: Sentiment analysis in social media, fraud detection in finance and
    insurance, augmented search, and log analysis'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理（NLP）**：社交媒体情感分析、金融和保险中的欺诈检测、增强搜索和日志分析。'
- en: '**Medical diagnosis**: Anomaly detection, pathology identification'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学诊断**：异常检测、病理识别。'
- en: '**Search engines**: Image searching'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索引擎**：图像搜索。'
- en: '**IoT**: Smart homes, predictive analysis using sensor data'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物联网（IoT）**：智能家居、基于传感器数据的预测分析。'
- en: '**Manufacturing**: Predictive maintenance'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制造业**：预测性维护。'
- en: '**Marketing**: Recommendation engines, automated target identification'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**营销**：推荐引擎、自动化目标识别。'
- en: '**Audio analysis**: Speech recognition, voice searching, and machine translation'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频分析**：语音识别、语音搜索和机器翻译。'
- en: There are many others that are yet to come.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多内容在后续章节中会进一步介绍。
- en: Summary
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, the basics of DL were introduced. This overview was kept very
    high-level to help readers who are new to this topic and prepare them to tackle
    the more detailed and hands-on topics that are covered in the following chapters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了深度学习（DL）的基础知识。这个概述保持了较高的层次，以帮助那些刚接触这一话题的读者，并为他们准备好迎接接下来章节中更详细和实践性的内容。
