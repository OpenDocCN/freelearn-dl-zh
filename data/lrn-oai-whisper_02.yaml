- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Understanding the Core Mechanisms of Whisper
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Whisper 的核心机制
- en: Welcome to [*Chapter 2*](B21020_02.xhtml#_idTextAnchor058) of our journey to
    mastering Whisper’s groundbreaking speech recognition capabilities. In the previous
    chapter, we explored the value propositions of production-grade speech recognition
    and why Whisper marks a pivotal advancement in conversational AI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们掌握 Whisper 开创性语音识别能力之旅的[*第二章*](B21020_02.xhtml#_idTextAnchor058)。在上一章，我们探讨了生产级语音识别的价值主张以及为什么
    Whisper 标志着对对话式 AI 的一次重要进展。
- en: Now, it’s time to demystify the technology under the hood. This chapter offers
    a comprehensive yet accessible overview of Whisper’s technical architecture and
    functions. Consider it your guidebook for navigating the ASR landscape as we dismantle
    Whisper piece by piece.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候揭开技术背后的面纱了。本章提供了 Whisper 技术架构和功能的全面而易懂的概述。把它当作你在 ASR 领域航行的指南，我们将一块一块地解剖
    Whisper。
- en: 'Our goals for this chapter are threefold:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标有三个：
- en: '**Develop literacy** in the critical components of modern ASR systems, including
    Whisper’s unique approach. We’ll survey the techniques and data flows fueling
    today’s speech recognition.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**培养现代 ASR 系统关键组件的识别能力**，包括 Whisper 独特的方法。我们将调查当前语音识别的技术和数据流。'
- en: '**Cultivate intuition** around the systemic interactions that enable translating
    speech into text and downstream natural language understanding. We’ll map the
    associations and data flows between crucial components, such as **acoustic models**,
    **language models**, and **decoders**, to reveal their intricate interdependence
    in the speech recognition pipeline. By tracing audio input through incremental
    processing steps and demonstrating how later stages rely on earlier ones, you’ll
    organically grasp the cumulative effect of these interconnected systems working
    in symphony.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**培养直觉**，理解将语音转化为文本以及后续自然语言理解所涉及的系统性交互。我们将描绘出 **声学模型**、**语言模型**和**解码器**等关键组件之间的关联和数据流，揭示它们在语音识别流程中的复杂相互依赖关系。通过追踪音频输入在逐步处理阶段的演变，并展示后续阶段如何依赖于前期步骤，你将自然地理解这些相互连接的系统协同工作的累积效果。'
- en: '**Enable optimization** by illuminating Whisper’s internal mechanisms. Grasping
    Whisper’s strengths, limitations, and trade-offs allows for the precise tuning
    of system configurations to achieve ideal accuracy, speed, and cost targets.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启用优化**，通过揭示 Whisper 的内部机制，帮助优化其性能。理解 Whisper 的优点、局限性和权衡取舍，可以精确调整系统配置，以实现理想的准确性、速度和成本目标。'
- en: We won’t dive into the complex math fueling innovations such as **recurrent
    neural networks** (**RNNs**) and **transformers**. Instead, we’ll focus on digestible
    conceptual frameworks so you can hit the ground running applying Whisper. With
    technology demystification comes informed strategy and impact.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨推动 **递归神经网络**（**RNNs**）和 **变压器**（**transformers**）等创新的复杂数学原理。相反，我们将专注于易于消化的概念框架，让你能够快速应用
    Whisper。随着技术的去神秘化，随之而来的是信息化的战略和影响。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Delving deeper into ASR systems
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨 ASR 系统
- en: Exploring the Whisper ASR system
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 Whisper ASR 系统
- en: Understanding Whisper’s components and functions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Whisper 的组件和功能
- en: Applying best practices for performance optimization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用最佳实践进行性能优化
- en: By the end of this chapter, you will understand the critical elements of Whisper’s
    ASR system, dissect its components and functions, and learn practical techniques
    for optimizing its performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解 Whisper ASR 系统的关键要素，分析其组件和功能，并学习优化性能的实用技巧。
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To harness the capabilities of OpenAI’s Whisper for advanced applications, this
    chapter leverages Python and Google Colab for ease of use and accessibility. The
    Python environment setup includes the Whisper library for transcription tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 OpenAI 的 Whisper 实现高级应用，本章采用 Python 和 Google Colab 来提高使用便捷性和可访问性。Python
    环境设置包括 Whisper 库，用于转录任务。
- en: '**Key requirements**:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要求**：'
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If the **T4 GPU** runtime type is available,
    select it for better performance.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab 笔记本**：这些笔记本已经设置好，以便用最低要求的内存和容量运行我们的 Python 代码。如果可以选择 **T4 GPU**
    运行类型，请选择它以获得更好的性能。'
- en: '**Python environment**: Each notebook contains directives to load the required
    Python libraries, including Whisper.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 环境**：每个笔记本中都包含指令来加载所需的 Python 库，包括 Whisper。'
- en: '**GitHub repository access**: All Python code, including examples, is available
    in the chapter’s GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter02)](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter02).
    These Colab notebooks are ready to run, providing a practical and hands-on approach
    to learning.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub库访问**：所有的Python代码，包括示例，已上传至本章的GitHub库([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter02)](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter02)。这些Colab笔记本已准备好运行，提供了一种实用的动手学习方法。'
- en: By meeting these technical requirements, you will be prepared to explore Whisper
    in different contexts while enjoying the streamlined experience of Google Colab
    and the comprehensive resources available on GitHub.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过满足这些技术要求，你将准备好在不同情境下探索Whisper，同时享受Google Colab的流畅体验和GitHub上丰富的资源。
- en: Delving deeper into ASR systems
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨自动语音识别（ASR）系统
- en: What exactly happens behind the scenes when we talk to Siri or Alexa? How does
    a computer transform the ambiguous sounds of natural language into correctly identified
    words and phrases? Well, that is where ASR systems come in.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们与Siri或Alexa对话时，幕后究竟发生了什么？计算机是如何将自然语言的模糊声音转化为准确的单词和短语的？这就是ASR系统的作用所在。
- en: ASR is playing an increasingly vital role in our daily lives. ASR powers many
    interactions with technology, from smart speakers to voice assistants on our phones.
    It facilitates hands-free control, enables voice search, and supports other voice-driven
    functionalities. The rise of conversational AI, including chatbots and virtual
    assistants, depends heavily on accurate and efficient speech recognition.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ASR在我们日常生活中的作用越来越重要。ASR支持与技术的许多互动，从智能音响到手机上的语音助手。它支持免提控制、语音搜索，并支持其他语音驱动的功能。对话式AI的崛起，包括聊天机器人和虚拟助手，极大依赖于准确且高效的语音识别。
- en: ASR systems can identify and process human speech and convert it into machine-readable
    text. In other words, they transcribe spoken audio into written words. This technology
    enables voice interfaces and verbal communication with computer systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ASR系统能够识别和处理人类的语音，并将其转换为机器可读的文本。换句话说，它们将口语音频转录为书面文字。这项技术使得语音接口和与计算机系统的口头交流成为可能。
- en: Definition and purpose of ASR systems
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASR系统的定义和目的
- en: On a basic level, ASR systems bridge the gap between human speech and machine
    understanding. Their role is to analyze an acoustic audio signal, identify the
    linguistic content, and output a textual translation that computers can process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本层面来看，ASR系统弥合了人类语言和机器理解之间的鸿沟。它们的作用是分析声学音频信号，识别语言内容，并输出计算机可以处理的文本翻译。
- en: 'More specifically, here are the key objectives ASR solutions aim to achieve:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，以下是ASR解决方案所要实现的关键目标：
- en: '**Convert audio to text**: The core purpose is transcribing spoken words into
    equivalent written text that software applications can intake. This text can then
    undergo further NLP.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将音频转换为文本**：核心目的是将口语转录为等效的书面文本，供软件应用程序处理。然后，这些文本可以进一步进行自然语言处理（NLP）。'
- en: '**Understand natural language**: Humans sometimes speak differently. We slur
    words, stutter, or talk over each other. ASR must handle these complexities and
    discern meaning from ambiguous audio.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解自然语言**：人类的语言表达有时是不同的。我们可能会吞音、结巴，或者互相打断。ASR必须处理这些复杂情况，并从模糊的音频中辨析出意义。'
- en: '**Enable voice interfaces**: ASR powers the voice **user interfaces** (**UIs**)
    that allow us to interact with technology through speech. These UIs include voice
    assistants, smart speakers, and dialogue systems.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启用语音接口**：ASR支持语音**用户界面**（**UIs**），让我们通过语音与技术互动。这些用户界面包括语音助手、智能音响和对话系统。'
- en: '**Improve accessibility**: For those with disabilities such as blindness or
    impaired motor function, ASR enables alternative input methods beyond keyboards
    or touchscreens. Voice control significantly expands accessibility.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改善可访问性**：对于有视力障碍或运动功能障碍的用户，ASR提供了超越键盘或触摸屏的替代输入方式。语音控制大大拓展了可访问性。'
- en: '**Drive efficiency**: Automating speech transcription relieves humans of tedious
    audio/video captioning and documentation. ASR saves massive time and effort.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高效率**：自动化语音转录解放了人类，免去繁琐的音频/视频字幕和文档工作。ASR节省了大量的时间和精力。'
- en: ASR delivers speech analytics that fuel **voice UIs** (**VUIs**), quantified
    self-applications, and other voice-driven interactions. As the demand for ubiquitous
    voice interfaces booms, improving ASR accuracy and capabilities remains imperative.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ASR 提供的语音分析推动了 **语音用户界面** (**VUIs**)、量化自我应用程序和其他语音驱动的交互。随着对无处不在的语音界面的需求激增，提高
    ASR 的准确性和能力仍然至关重要。
- en: ASR allows us to communicate with machines as we do with other humans when implemented
    harmoniously with complementary technologies such as **natural language understanding**
    (**NLU**), dialogue management, and **text-to-speech** (**TTS**). This natural
    interaction paradigm is essential to the vision of an intelligent assistant in
    every home.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ASR 与其他互补技术如 **自然语言理解** (**NLU**)、对话管理和 **语音合成** (**TTS**) 协同工作时，它使我们能够像与其他人类一样与机器进行沟通。这种自然交互范式对于每个家庭中智能助手的愿景至关重要。
- en: ASR in the real world
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界中的 ASR
- en: Automated speech recognition already enables many common hands-free interfaces
    through a paradigm known as VUIs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别已经通过一种被称为 VUI 的范式，实现了许多常见的免提界面。
- en: Voice user interfaces
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 语音用户界面
- en: VUIs allow people to interact with devices through conversational speech instead
    of touch, typing, or clicking. They comprise the speech recognition and NLU stacks,
    enabling systems such as Alexa and Siri to intake raw voice queries before responding
    intelligently. Effective VUIs combine ASR transcriptions with downstream dialogue
    systems to handle natural commands, questions, and instructions using only spoken
    utterances. This hands-free control paradigm powered by voice makes interacting
    with technology faster, easier, and more accessible.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: VUI 使人们能够通过对话式语音与设备互动，而不是触摸、打字或点击。它们包括语音识别和 NLU 技术栈，使系统如 Alexa 和 Siri 能够在响应之前接收原始语音查询。有效的
    VUI 将 ASR 转录与下游对话系统结合使用，通过语音指令处理自然的命令、问题和指示。这种由语音驱动的免提控制范式使得与技术互动变得更快、更简便、更易于访问。
- en: 'While mostly invisible to users, ASR already enables many common scenarios
    through VUIs:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于用户来说大多是隐形的，但 ASR 已经通过 VUI 实现了许多常见场景：
- en: '**Smart speakers** such as Amazon Echo and Google Home rely on ASR to understand
    and respond to voice commands, allowing hands-free music playback, household control
    via the **Internet of Things** (**IoT**), information queries, and more.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能音响**如 Amazon Echo 和 Google Home 依靠 ASR 理解并响应语音命令，支持免提播放音乐、通过 **物联网** (**IoT**)
    控制家庭设备、查询信息等功能。'
- en: '**Virtual assistants** such as Siri, Alexa, Cortana, and Google Assistant use
    ASR to transcribe user queries. After speech recognition, they execute commands,
    answer questions, or make recommendations using downstream natural language and
    dialogue processing.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟助手**如 Siri、Alexa、Cortana 和 Google Assistant 使用 ASR 来转录用户查询。在语音识别后，它们执行命令、回答问题或通过下游的自然语言和对话处理进行推荐。'
- en: '**Captioning and documentation** tools employ ASR to rapidly transcribe videos,
    podcasts, lectures, medical reports, legal proceedings, and more.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字幕和文档**工具利用 ASR 快速转录视频、播客、讲座、医疗报告、法律程序等内容。'
- en: '**Hands-free control** of smartphones, tablets, laptops, TVs, and in-vehicle
    infotainment happens through ASR **application programming interfaces** (**APIs**)
    that can navigate apps, input text, place calls, adjust volume, and so on via
    voice instructions rather than touchscreens.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 ASR **应用程序接口** (**API**)，**免提控制**智能手机、平板电脑、笔记本电脑、电视和车载娱乐系统，用户可以通过语音指令导航应用程序、输入文本、拨打电话、调整音量等，而不需要触摸屏。
- en: '**Speech analytics** solutions extract insights from customer call transcripts
    generated via ASR to understand sentiment, trends, compliance, agent performance,
    and other metrics.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音分析**解决方案通过 ASR 生成的客户通话转录提取见解，以了解情感、趋势、合规性、代理表现和其他指标。'
- en: ASR thus plays a profound role in human-computer interaction. Its accuracy and
    robustness directly impact the user experience of many popular intelligent products
    and services. Under the hood, ASR feeds the voice-driven revolution through speech
    transcription capabilities that enable verbal system control and analytics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ASR 在人机交互中发挥着深远的作用。其准确性和鲁棒性直接影响到许多流行智能产品和服务的用户体验。在背后，ASR 通过语音转录能力为语音驱动的革命提供动力，使得口头系统控制和分析成为可能。
- en: After seeing the profound real-world impacts of modern ASR systems such as virtual
    assistants and smart speakers, one may wonder how we got here. What seminal breakthroughs
    in algorithms, data, and compute architectures catalyzed today’s flexible and
    accurate speech recognition solutions? The following section will chart the rapid
    progression of core methodologies through pivotal eras that brought us to the
    cutting-edge innovations in the neural networks powering Whisper. Understanding
    this development arc provides context around ongoing challenges and remaining
    headroom as the field continues pushing further boundaries. Equipped with historical
    perspectives, we can better anticipate future directions amidst this technological
    Cambrian explosion.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 看到现代ASR系统如虚拟助手和智能音响在现实世界中的深远影响后，人们不禁会问，我们是如何走到今天的？在算法、数据和计算架构方面有哪些开创性的突破催生了今天灵活且精准的语音识别解决方案？接下来的章节将回顾核心方法学在关键时代的快速进展，带领我们走到驱动
    Whisper 的神经网络的尖端创新。了解这一发展历程，可以帮助我们更好地理解当前的挑战和剩余的发展空间，因为该领域正在不断推动新的边界。借助历史的视角，我们可以更好地预见未来的方向，在这一技术的寒武纪大爆发中找到立足之地。
- en: Brief history and evolution of ASR technology
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ASR技术的简史与演变
- en: The concepts behind automated speech recognition date back to the 1930s, when
    Bell Laboratories built machines to recognize digits spoken over the telephone.
    However, widespread commercial adoption of the technology we know today only occurred
    in the 1990s and 2000s.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别的概念可以追溯到20世纪30年代，当时贝尔实验室开发了能够识别通过电话传递的数字的机器。然而，今天我们所知的技术的广泛商业化应用，直到90年代和2000年代才得以实现。
- en: After nearly a century of innovation, speech recognition capabilities have advanced
    enormously thanks to transformative approaches in machine learning and the availability
    of big data. The accuracy and versatility of ASR continue to progress at a remarkable
    pace.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 经过近一个世纪的创新，语音识别技术在机器学习的变革性方法和大数据的可用性推动下，取得了巨大的进展。自动语音识别（ASR）的准确性和多样性持续以惊人的速度发展。
- en: The early days – Pattern recognition approaches
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期阶段——模式识别方法
- en: The first significant wave of innovation in ASR came during the 1950s at Bell
    Laboratories. Researchers focused on isolated word recognition using heuristic
    techniques to match acoustic patterns by examining audio waveforms and identifying
    distinguishable speech components.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ASR的第一次重大创新浪潮出现在1950年代的贝尔实验室。研究人员集中于使用启发式技术，通过分析音频波形并识别可区分的语音成分来匹配声学模式，从而实现孤立词识别。
- en: Bell Labs built specialized machines to interpret spoken digit sequences over
    the telephone. For example, callers could verbally provide a bank account number
    to route their request. These primitive Audrey systems represented early examples
    of pattern matching without modern machine learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔实验室开发了专用的机器，用来解读通过电话传输的数字序列。例如，用户可以口头提供银行账户号码，以便路由他们的请求。这些原始的 Audrey 系统代表了没有现代机器学习技术的早期模式匹配实例。
- en: Audrey systems
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Audrey 系统
- en: The Audrey systems developed by Bell Laboratories in the 1950s were pioneering
    speech recognition devices aimed at deciphering digits spoken over the telephone.
    They used analog circuits to match acoustic patterns to individual numbers, enabling
    the routing of calls based on verbally provided account or contact numbers. While
    limited in scope, these specialized machines represented some of the first attempts
    at ASR through template matching. Audrey marked an early milestone, though substantial
    innovation was still needed for more flexible systems that could handle continuous
    speech.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔实验室在1950年代开发的 Audrey 系统是开创性的语音识别设备，旨在解读通过电话传输的数字。它们使用模拟电路将声学模式与个别数字进行匹配，从而根据口头提供的账户或联系方式对电话进行路由。虽然功能有限，但这些专用机器代表了通过模板匹配进行ASR的早期尝试。Audrey
    标志着一个早期的里程碑，尽管为了实现能够处理连续语音的更灵活系统，仍然需要大量的创新。
- en: Over the following decades, researchers developed rule-based approaches using
    signal processing and acoustic fingerprinting. However, these methods struggled
    to fully accommodate the dynamic complexities of natural language and the variability
    of speech patterns across diverse speakers. They also were heavily burdened with
    extensive expert feature engineering, which was challenging to scale across languages.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几十年中，研究人员发展出了基于规则的方法，使用信号处理和声学指纹识别技术。然而，这些方法难以完全适应自然语言的动态复杂性以及不同说话者之间的语音模式变异性。同时，这些方法还依赖于大量的专家特征工程，难以在不同语言间进行规模化应用。
- en: This early progress was promising but needed more sophistication to handle continuous
    speech, diverse accents, environments, and vocabulary beyond a few words. More
    advanced techniques would be necessary to deliver today’s flexible ASR.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的进展很有前景，但需要更复杂的技术来处理连续语音、多样的口音、环境以及超出几个单词的词汇。为了实现今天灵活的自动语音识别（ASR），需要采用更先进的技术。
- en: Statistical approaches emerge – Hidden Markov models and *n*-gram models
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计方法的出现——隐马尔可夫模型与*n*-gram模型
- en: A paradigm shift occurred in the 1970s and 80s with the introduction of probabilistic
    modeling using **hidden Markov models** (**HMMs**) and *n***-gram** language models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在1970年代和1980年代，随着**隐马尔可夫模型**（**HMMs**）和*n*-gram语言模型的引入，概率建模技术发生了范式转变。
- en: Hidden Markov models
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: HMMs are statistical models that analyze sequences by modeling underlying states
    *hidden* from the observer. In ASR, they model generating speech sounds as transitions
    between hidden states over time, tracking the probability of particular phonemes
    or words occurring given previous acoustic cues. Rather than definitive rules,
    HMMs provide the computational framework for statistically handling speech ambiguities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: HMM是统计模型，通过对序列进行分析，建模观察者无法看到的潜在状态。在自动语音识别中，它们将生成语音的声音建模为隐藏状态之间的转移，跟踪在给定前一个声学提示的情况下特定音素或单词出现的概率。HMM不是确定性规则，而是提供了一种计算框架，用于统计地处理语音中的歧义。
- en: '*N*-gram language models'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*－gram 语言模型'
- en: '*N*-gram language models calculate conditional word probabilities by examining
    historical sequences of 1–3 previous words. For example, a 3-gram model estimates
    the likelihood of each possible next word following every unique consecutive word
    pair. Language models can use these probability distributions to predict and refine
    interim ASR transcriptions into more probable phrases. However, *n*-grams fail
    to model longer-range contexts.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*-gram语言模型通过分析前1–3个单词的历史序列，计算条件词概率。例如，3-gram模型估算每个可能的下一个单词在每对独特连续单词后的出现概率。语言模型可以利用这些概率分布来预测并改进临时的ASR转录，使其更加符合可能的短语。然而，*n*-gram无法建模长距离的上下文。'
- en: Rather than solely pattern matching, researchers adopted principles from *Bayesian
    statistics* to compute likelihood scores and make predictions under uncertainty.
    That approach enabled more graceful handling of the ambiguities and variances
    inherent to speech signals.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员不再仅仅依赖模式匹配，而是采用*贝叶斯统计*的原理来计算似然得分，并在不确定性下进行预测。这种方法使得能够更优雅地处理语音信号固有的歧义和变异。
- en: Using HMMs, researchers modeled speech components such as phonemes and words
    as Markov processes, allowing tracking of transitional probabilities from one
    sound to another. *N*-gram language models then predicted the following words
    based on previous word sequences. Combined with acoustic models, these key innovations
    could process continuous speech recognition for small vocabularies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HMM，研究人员将语音组件（如音素和单词）建模为马尔可夫过程，从而跟踪从一个声音到另一个声音的转移概率。*N*-gram语言模型则根据之前的单词序列预测后续单词。结合声学模型，这些关键创新可以处理小词汇量的连续语音识别。
- en: In the commercial realm, Dragon Systems launched its Dragon NaturallySpeaking
    dictation software in 1990 using HMM. That launch represented a significant milestone
    as one of the first large-vocabulary continuous speech recognition systems for
    **personal** **computers** (**PCs**).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业领域，Dragon Systems 于1990年推出了基于HMM的 Dragon NaturallySpeaking 语音输入软件。这一发布代表了一个重要的里程碑，是首批支持大词汇量连续语音识别的**个人**
    **计算机**（**PCs**）系统之一。
- en: However, successful adoption faced challenges such as limited accuracy, lack
    of environment robustness, extensive training requirements, and little language
    context. Significant improvements would come in the following decades with neural
    networks and increased computational power.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，成功的应用面临着诸如精度有限、缺乏环境鲁棒性、需要大量训练以及语言上下文不足等挑战。在接下来的几十年里，神经网络和计算能力的提高将带来显著的改进。
- en: The deep learning breakthrough
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习突破
- en: While HMM/*n*-gram systems represented significant progress, they relied heavily
    on manual feature engineering and limited model capacities. In contrast, deep
    learning approaches could automatically discover intricate representations and
    patterns from raw data at scale.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管HMM/*n*-gram系统代表了显著进展，但它们在很大程度上依赖手动特征工程，且模型能力有限。相比之下，深度学习方法可以在大规模数据中自动发现复杂的表示和模式。
- en: The late 2000s saw the introduction of **deep neural networks** (**DNNs**) to
    speech recognition, delivering exceptional boosts in accuracy. Deep feedforward
    and recurrent networks overcame previous limitations using multilayered artificial
    neurons.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2000年代末期，**深度神经网络**（**DNNs**）被引入语音识别，显著提高了准确度。深度前馈和递归网络通过多层人工神经元克服了以前的局限性。
- en: Then, in 2016, Microsoft achieved an industry milestone, reaching human parity
    in conversational speech recognition through extensive neural network training
    using their proprietary **Computational Network Toolkit** (**CNTK**) framework,
    now known as the Microsoft Cognitive Toolkit. The researchers reported a **word
    error rate** (**WER**) of 5.9 percent, equal to that of people who were asked
    to transcribe the same conversation. The key to Microsoft’s success was using
    **long short-term memory** (**LSTM**) acoustic models combined with a novel spatial
    smoothing method and **lattice-free maximum mutual information** (**LF-MMI**)
    acoustic training. They also employed multiple RNN language models and large amounts
    of data, including Bing voice search logs, to train their DNNs. This data-driven
    approach allowed the system to learn from the variations and nuances in speech,
    improving its ability to recognize and transcribe spoken words accurately. Microsoft’s
    breakthrough demonstrated capabilities on par with human listeners, unlocking
    new potential for commercial voice assistants and dialogue agents to reach new
    versatility and utility.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2016年，微软达成了一个行业里程碑，通过大量神经网络训练，使用他们的专有**计算网络工具包**（**CNTK**）框架（现在被称为微软认知工具包），在对话语音识别方面达到了人类水平。研究人员报告的**词错误率**（**WER**）为5.9%，与被要求转录同一对话的人类相当。微软成功的关键在于使用**长短期记忆**（**LSTM**）声学模型，结合新颖的空间平滑方法和**无格最大互信息**（**LF-MMI**）声学训练。他们还采用了多个RNN语言模型和大量数据，包括必应语音搜索日志，来训练他们的DNN。这种数据驱动的方法使系统能够从语音中的变化和细微差别中学习，从而提高了准确识别和转录口语的能力。微软的突破展示了与人类听众相当的能力，为商业语音助手和对话代理释放了新的潜力，赋予其更强的多功能性和实用性。
- en: Word error rate
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 词错误率
- en: 'WER is a standard metric used to measure the performance of a speech recognition
    or machine translation system. It is calculated as the ratio of errors in a transcript
    to the total words spoken. The errors are categorized into three types: substitutions
    (when a word is replaced with another), insertions (when a word that wasn’t originally
    spoken is added), and deletions (when a word is omitted). The formula for WER
    is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: WER是一个标准的度量指标，用于衡量语音识别或机器翻译系统的性能。它通过计算文字记录中的错误与总的发言单词数的比例来得出。错误分为三种类型：替换（当一个单词被另一个单词替代时）、插入（当一个未曾说出的单词被添加时）和删除（当一个单词被省略时）。WER的公式如下：
- en: '*Word Error Rate = (Substitutions + Insertions + Deletions)/Number of* *Words
    Spoken*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*词错误率 = (替换 + 插入 + 删除) / 发言的单词数*'
- en: For example, if there are 10 substitutions, 5 insertions, and 5 deletions in
    a transcript of 100 words, the WER would be 20%. A lower WER indicates better
    accuracy in recognizing speech. It’s important to note that while WER is a widely
    used metric, it is not the only measure of the effectiveness of a speech recognition
    system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果在100个单词的文字记录中有10个替换、5个插入和5个删除，则WER为20%。较低的WER表示语音识别的准确度更高。需要注意的是，虽然WER是广泛使用的度量标准，但它并不是衡量语音识别系统效果的唯一标准。
- en: Long short-term memory
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: LSTM is a type of RNN designed to remember information for extended periods.
    Unlike traditional RNNs, which struggle to maintain long-term dependencies due
    to the vanishing gradient problem, LSTMs can learn these dependencies, making
    them well-suited for tasks involving sequential data with long-term temporal dependencies.
    This includes language translation, speech recognition, and time series forecasting
    applications. An LSTM network includes memory blocks, which are recurrently connected
    and contain one or more memory cells along with three multiplicative units, allowing
    the network to regulate the flow of information.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是一种RNN，设计用于长期记忆信息。与传统的RNN不同，后者由于梯度消失问题难以维持长期依赖关系，LSTM可以学习这些依赖关系，使其非常适合处理涉及长期时间依赖性的序列数据任务。这些任务包括语言翻译、语音识别和时间序列预测应用。LSTM网络包含记忆块，这些记忆块是递归连接的，包含一个或多个记忆单元以及三个乘法单元，允许网络调节信息流。
- en: Lattice-free maximum mutual information
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 无格最大互信息
- en: LF-MMI is a method used for sequence-level training of speech recognition acoustic
    models. The MMI objective function aims to maximize the mutual information between
    the observed acoustic features and the corresponding word sequences in the training
    data. The *lattice-free* aspect refers to the fact that this method does not require
    the generation of lattices (a type of graph used in traditional speech recognition
    systems) during training, which makes it more efficient and suitable for GPU-based
    training. LF-MMI has been shown to achieve state-of-the-art results on many speech
    recognition tasks, and it is particularly effective for training DNNs used in
    ASR systems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LF-MMI 是一种用于语音识别声学模型序列级训练的方法。MMI 目标函数旨在最大化观察到的声学特征与训练数据中对应词序列之间的互信息。*无格*这一特点指的是该方法在训练过程中无需生成格图（传统语音识别系统使用的一种图形），这使得它更高效，且适合基于
    GPU 的训练。LF-MMI 已被证明在许多语音识别任务中取得了最先进的成果，并且特别适用于训练用于 ASR 系统的 DNN。
- en: 'State-of-the-art systems today use different neural architectures:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的最先进系统采用了不同的神经网络架构：
- en: Convolutional layers learn translation-invariant features directly from spectrograms.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层直接从频谱图中学习平移不变特征。
- en: Recurrent layers include LSTMs, model speech sequences, and long-range context.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环层包括 LSTM，建模语音序列和长距离上下文。
- en: Transformers capture global dependencies through self-attention, removing recurrence
    constraints.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers 通过自注意力机制捕捉全局依赖，去除了递归约束。
- en: In addition to excellent statistical foundations, these neural advances catalyzed
    the commercial success of virtual assistants and widespread voice interfaces.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了卓越的统计学基础，这些神经网络进展催化了虚拟助手的商业成功以及语音界面的广泛应用。
- en: Ongoing innovations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续创新
- en: 'ASR remains a highly active research area as we find new ways to improve flexibility,
    reduce latency, and enhance accuracy. Exciting innovations continue to emerge,
    such as the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别仍然是一个高度活跃的研究领域，我们不断探索新的方法来提高灵活性、减少延迟并增强准确性。令人兴奋的创新不断涌现，例如以下内容：
- en: '**End-to-end modeling**: Separate acoustic and language models have been replaced
    with single integrated networks, simplifying training and optimizing the entire
    pipeline.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端建模**：原先独立的声学和语言模型已被单一集成网络所替代，从而简化了训练并优化了整个流程。'
- en: '**Multimodal learning**: Audio, visual, and textual data can now be combined
    to improve robustness. Lip movements and other visual cues provide additional
    signals.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态学习**：音频、视觉和文本数据现在可以结合使用，以提高鲁棒性。嘴唇动作和其他视觉线索提供了额外的信号。'
- en: '**Personalization**: Models can be adapted to individual speakers’ voices and
    accents for tailored performance. Unique vocal profiles enhance recognition.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化**：模型可以根据个人发音和口音进行适配，提供定制化的表现。独特的声纹档案增强了识别能力。'
- en: '**Low-resource languages**: Progress in ASR for languages with limited training
    data is facilitated through *cross-lingual transfer learning*. That means high-resource
    languages (e.g., English) help bootstrap those languages with limited training
    data.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低资源语言**：通过*跨语言迁移学习*，在有限训练数据的语言上取得了语音识别的进展。这意味着高资源语言（例如英语）有助于启动那些训练数据有限的语言。'
- en: '**On-device deployment**: Thanks to model compression and acceleration hardware,
    real-time ASR is now available on mobile phones and edge devices.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备端部署**：得益于模型压缩和加速硬件，实时语音识别（ASR）现已可以在手机和边缘设备上实现。'
- en: Thanks to advancements in machine learning, ASR technology now delivers incredible
    utility after decades of iteration. With enhanced robustness to diverse speech
    and environments, broad language support, and scalable deployment, ASR promises
    to enable even more voice-driven experiences in the years to come. Whisper sits
    at the cutting edge, pushing boundaries ever further.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于机器学习的进展，经过几十年的迭代，ASR 技术如今提供了惊人的实用性。通过增强对不同语音和环境的鲁棒性，广泛的语言支持以及可扩展的部署，ASR 有望在未来几年推动更多基于语音的体验。Whisper
    位于技术前沿，持续突破边界。
- en: Most relevantly, Whisper represents a massive leap in applying state-of-the-art
    self-supervised learning to achieve human-level capabilities using only open datasets.
    This unprecedented accuracy and language coverage sets a new standard for production
    speech recognition systems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最相关的是，Whisper 在应用最先进的自监督学习方面取得了巨大的飞跃，使用仅限开放数据集实现了人类级别的能力。这种前所未有的准确性和语言覆盖范围为生产级语音识别系统设定了新的标准。
- en: Exploring the Whisper ASR system
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Whisper ASR 系统
- en: Now that we’ve surveyed the landscape and capabilities of automated speech recognition,
    it’s time to demystify Whisper’s technical inner workings. This section offers
    an accessible yet comprehensive overview of the algorithms, data pipelines, and
    innovations unlocking Whisper’s unprecedented transcription abilities.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了自动语音识别的全貌和能力，是时候揭开Whisper技术内在机制的面纱了。本节提供了一个既简明又全面的概述，介绍了Whisper如何通过其算法、数据管道和创新实现前所未有的转录能力。
- en: We’ll highlight approaches in acoustic modeling, self-supervised pre-training
    strategies, model architectures, and performance optimizations that set Whisper
    apart. Collectively, these techniques enable robust real-world speech recognition
    across languages, environments, and hardware.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点介绍声学建模、自监督预训练策略、模型架构和性能优化等方法，这些方法使Whisper与众不同。综合起来，这些技术实现了跨语言、环境和硬件的强大现实语音识别能力。
- en: 'While we won’t dig into granular mathematical equations, you’ll develop an
    intuitive grasp of Whisper’s competitive advantages, such as the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会深入探讨具体的数学公式，但你将直观地理解Whisper的竞争优势，例如以下几点：
- en: Handling fuzzy sound-to-symbol mapping with **connectionist temporal classification**
    (**CTC**) acoustic models
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**连接主义时序分类**（**CTC**）声学模型处理模糊的语音到符号映射
- en: Incorporating global language context via transformers
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过变压器（transformers）整合全球语言上下文
- en: Streamlining low-latency deployments across devices
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精简跨设备的低延迟部署
- en: Understanding Whisper’s mechanics empowers practical tuning for your targets
    around accuracy, speed, and cost. Architectural literacy breeds informed strategy.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Whisper的机制有助于你针对准确性、速度和成本等目标进行实际调优。架构知识能帮助制定明智的策略。
- en: Connectionist temporal classification
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 连接主义时序分类
- en: CTC acoustic models handle fuzzy sound-to-symbol mapping in speech recognition
    systems. These models are designed to handle the inherent uncertainty in aligning
    input sequences (such as audio frames) with output sequences (such as phonemes
    or words). This is particularly challenging in speech recognition because the
    boundaries between spoken words are unclear, and the same word can be pronounced
    differently depending on the context.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: CTC声学模型在语音识别系统中处理模糊的语音到符号映射。这些模型旨在处理将输入序列（如音频帧）与输出序列（如音素或单词）对齐时的固有不确定性。这在语音识别中尤为具有挑战性，因为口语单词之间的边界不明确，而且同一个单词的发音可能因上下文而不同。
- en: Let’s dive in! We’ll explore Whisper’s fusion with CTC and transformers, as
    well as the integration of linguistic knowledge, to understand how it blends end-to-end
    and hybrid techniques. These are not separate discussions but rather interconnected
    aspects of Whisper’s architecture.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨！我们将探索Whisper如何与CTC和变压器融合，以及如何整合语言学知识，了解它是如何将端到端和混合技术结合起来的。这些讨论不是独立的，而是Whisper架构中相互关联的各个方面。
- en: Understanding the trade-offs – End-to-end versus hybrid models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解权衡——端到端方法与混合模型
- en: 'When architecting an ASR system, architects face a pivotal decision: end-to-end
    versus hybrid modeling strategies. This initial fork impacts everything from accuracy
    and speed to adaptability. Before implementing Whisper or any production-grade
    speech platform, developers should consider the critical trade-offs between the
    two paradigms below the surface.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计ASR系统时，架构师面临一个关键决策：端到端方法还是混合建模策略。这一初步选择会影响到准确性、速度和适应性等方方面面。在实施Whisper或任何生产级语音平台之前，开发人员应考虑两种方法之间的深层权衡。
- en: Is Whisper an end-to-end or hybrid ASR system?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper是一个端到端还是混合型自动语音识别（ASR）系统？
- en: OpenAI’s Whisper is an ASR system that uses an end-to-end approach. The Whisper
    architecture is implemented as an encoder-decoder transformer, where input audio
    is processed in a two-step process. First, it generates a mathematical representation
    of the audio and then decodes this representation into a sequence of text tokens.
    This process is characteristic of an end-to-end approach, where the entire task
    – from raw audio input to text output – is handled by a single, integrated model.
    Therefore, Whisper can be classified as an end-to-end ASR system.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的Whisper是一个使用端到端方法的ASR系统。Whisper的架构采用编码器-解码器变压器（transformer）实现，输入的音频经过两步处理。首先，它生成音频的数学表示，然后将该表示解码为一系列文本标记。这一过程是端到端方法的典型特征，即整个任务——从原始音频输入到文本输出——都由单一集成的模型处理。因此，Whisper可以被归类为一个端到端的ASR系统。
- en: 'Some examples of hybrid ASR systems include the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些混合型ASR系统的例子包括：
- en: '- *Kaldi-based ASR systems*: Kaldi is an open source toolkit for speech recognition
    research. It supports both DNN-HMM hybrid models and end-to-end deep learning
    models. It often uses LF-MMI training for its hybrid models.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '- *基于 Kaldi 的 ASR 系统*：Kaldi 是一个用于语音识别研究的开源工具包。它支持 DNN-HMM 混合模型和端到端深度学习模型，通常为其混合模型使用
    LF-MMI 训练。'
- en: '- The *Vicomtech Speech Transcription Systems*: These systems were used for
    the *Albayzín-RTVE 2020 Speech to Text Transcription Challenge* and likely included
    hybrid ASR components.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '- *Vicomtech 语音转录系统*：这些系统曾用于 *Albayzín-RTVE 2020 语音到文本转录挑战*，并且很可能包含了混合 ASR
    组件。'
- en: '- *Wav2Vec 2.0*: This was developed by Facebook AI and is a self-supervised
    learning model for speech recognition used in end-to-end and hybrid ASR systems.
    In the context of hybrid ASR models, Wav2Vec 2.0 can generate high-quality acoustic
    representations that can be used as input to a traditional ASR system, such as
    an HMM or a DNN.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '- *Wav2Vec 2.0*：这是由 Facebook AI 开发的自监督学习模型，用于语音识别，可应用于端到端和混合 ASR 系统。在混合 ASR
    模型中，Wav2Vec 2.0 可以生成高质量的声学表示，作为传统 ASR 系统（如 HMM 或 DNN）的输入。'
- en: These hybrid systems are designed to effectively deal with the temporal variability
    in speech signals by leveraging neural networks’ discriminative training capabilities
    and deep feature extraction while utilizing HMMs’ robust sequence modeling.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些混合系统旨在通过利用神经网络的判别训练能力和深度特征提取，同时利用 HMM 的强大序列建模能力，来有效处理语音信号中的时间变异性。
- en: In theory, end-to-end modeling seems ideal. A unified model optimizes the complete
    mapping of acoustic signals to transcriptions. This elegantly sidesteps glue code
    and intermediate representations. However, as we explore in this section, hybrid
    architectures still dominate industry systems due to other constraints such as
    latency, customization, and robustness.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，端到端建模看起来是理想的。一个统一的模型可以优化声学信号到转录的完整映射，这巧妙地避免了胶水代码和中间表示。然而，正如我们在本节中探讨的，混合架构仍然主导着行业系统，因为存在延迟、定制性和鲁棒性等其他限制。
- en: '*Figure 2**.1* compares traditional hybrid-based and more recent end-to-end
    ASR systems. Both systems take air traffic controller (ATCO) voice communication
    as input and produce transcripts as output.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.1* 比较了传统的混合基础和更现代的端到端 ASR 系统。两个系统都以空中交通管制员（ATCO）的语音通信作为输入，并生成转录作为输出。'
- en: '![Figure 2.1 – Traditional hybrid-based and more recent end-to-end ASR systems
    (A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers - Scientific
    Figure on ResearchGate. Available from: https://www.researchgate.net/figure/Traditional-hybrid-based-and-more-recent-end-to-end-automatic-speech-recognition-systems_fig1_370961598)](img/B21020_02_1.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 传统的混合基础和更现代的端到端 ASR 系统（《虚拟仿真-飞行员代理用于空中交通管制员培训 - ResearchGate 上的科学图表》。可从以下网址获取：https://www.researchgate.net/figure/Traditional-hybrid-based-and-more-recent-end-to-end-automatic-speech-recognition-systems_fig1_370961598)](img/B21020_02_1.jpg)'
- en: 'Figure 2.1 – Traditional hybrid-based and more recent end-to-end ASR systems
    (A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers - Scientific
    Figure on ResearchGate. Available from: https://www.researchgate.net/figure/Traditional-hybrid-based-and-more-recent-end-to-end-automatic-speech-recognition-systems_fig1_370961598)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 传统的混合基础和更现代的端到端 ASR 系统（《虚拟仿真-飞行员代理用于空中交通管制员培训 - ResearchGate 上的科学图表》。可从以下网址获取：https://www.researchgate.net/figure/Traditional-hybrid-based-and-more-recent-end-to-end-automatic-speech-recognition-systems_fig1_370961598)
- en: In the traditional hybrid-based ASR system, the process begins with feature
    extraction from the input voice communication. This is followed by acoustic modeling,
    which maps the acoustic features to phonetic units. The next step is pronunciation
    modeling, which maps the phonetic units to words. Finally, language modeling is
    used to predict the probability of a sequence of words, producing the final transcript.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的混合基础 ASR 系统中，处理过程从输入语音通信的特征提取开始。接着进行声学建模，将声学特征映射到音位单元。下一步是发音建模，将音位单元映射到单词。最后，使用语言建模来预测单词序列的概率，生成最终的转录结果。
- en: On the other hand, the end-to-end ASR system directly maps the input voice communication
    to the output transcript, bypassing the intermediate steps of acoustic, pronunciation,
    and language modeling. This simplifies the system and can potentially improve
    performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，端到端 ASR 系统直接将输入的语音通信映射到输出的转录，跳过了声学、发音和语言建模的中间步骤。这简化了系统，并可能提高性能。
- en: The figure also shows optional modules (represented by dotted blocks) that can
    be added to increase the system’s overall performance. These include surveillance
    data or other data types such as sectors or waypoints. These additional data sources
    can provide context that helps the ASR system better understand and transcribe
    the ATCO voice communication.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该图还显示了可以添加的可选模块（以虚线块表示），以提高系统的整体性能。这些包括监视数据或其他数据类型，如部门或航点。这些附加数据源可以提供上下文，帮助ASR系统更好地理解和转录空中交通管制员（ATCO）的语音通信。
- en: In summary, *Figure 2**.1* illustrates the differences between traditional hybrid-based
    and end-to-end ASR systems and shows how additional data sources can enhance their
    performance in the context of air traffic control.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，*图2.1*展示了传统混合基础与端到端ASR系统之间的差异，并展示了如何通过额外的数据源来提升其在空中交通管制背景下的性能。
- en: There are no universally superior options for end-to-end versus hybrid-based
    ASR systems. The approach taken impacts adaptability, deployment requirements,
    and scalability. Let’s analyze the critical considerations when determining the
    right strategic direction.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在端到端与混合基础的ASR系统之间没有绝对优越的选择。所采取的方案会影响适应性、部署需求和可扩展性。让我们分析在确定正确战略方向时的关键考虑因素。
- en: Accuracy and output quality
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确性和输出质量
- en: 'For many applications, recognition precision is paramount. Architectural decisions
    significantly influence output quality:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用来说，识别精度至关重要。架构决策对输出质量有显著影响：
- en: '**End-to-end strengths**: Jointly trained components directly target the final
    objective: no suboptimal pipelines or disjoint errors.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端优势**：联合训练的组件直接针对最终目标：没有次优的流水线或割裂的错误。'
- en: '**Hybrid advantage**: Mix and match best-of-breed components (acoustic, linguistic).
    Customize the balance of errors.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合优势**：混合搭配最佳的声学和语言组件。定制错误的平衡。'
- en: In practice, hybrid models achieve state-of-the-art accuracy by combining an
    optimal acoustic model such as *Wav2Vec 2.0* with advanced transformer language
    models. Specialization beats generalization, but end-to-end models are quickly
    catching up.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，混合模型通过将最佳的声学模型如*Wav2Vec 2.0*与先进的变换器语言模型结合，实现了最先进的准确性。专门化优于泛化，但端到端模型正在迅速追赶。
- en: Output quality also depends on factors such as the volume of training data,
    model size, and personalization techniques. But hybrid systems edge out end-to-end
    at scale, partially thanks to their customizability.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出质量还取决于诸如训练数据量、模型大小和个性化技术等因素。但在规模上，混合系统超越端到端系统，部分原因在于其可定制性。
- en: Latency and throughput
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟和吞吐量
- en: 'Real-time recognition necessitates optimizing for low latency and streaming
    ASR calls for quick incremental outputs, not just batch offline decoding. End-to-end
    networks tend to be more extensive and computationally intensive without modular
    components. This introduces latency challenges for real-time systems:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 实时识别需要优化低延迟，而流式自动语音识别（ASR）要求快速增量输出，而不仅仅是批量离线解码。端到端网络通常更为庞大，且计算密集，没有模块化组件。这为实时系统引入了延迟挑战：
- en: '**End-to-end difficulty**: The joint model applies full sequence context. Outputs
    lag audio inputs.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端困难**：联合模型应用全序列上下文，输出滞后于音频输入。'
- en: '**Hybrid advantage**: Separate acoustic and language models enable streaming
    low-latency recognition.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合优势**：独立的声学和语言模型使流式低延迟识别成为可能。'
- en: That said, innovations such as convolutional neural architectures and model
    distillation continue improving end-to-end latency profiles. Cloud acceleration
    hardware mitigates computational constraints.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，卷积神经网络架构和模型蒸馏等创新仍在持续改善端到端的延迟表现。云加速硬件缓解了计算约束。
- en: Throughput is less concerning since modern systems handle high volumes of audio
    streams in parallel. Overall, hybrid approaches currently achieve faster incremental
    outputs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量问题较小，因为现代系统可以并行处理大量音频流。总体来说，混合方法目前能实现更快的增量输出。
- en: Customization and control
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定制与控制
- en: 'Customizability allows tailoring ASR capabilities to specific domains such
    as medicine, law, or customer support centers. This requires interfacing separate
    speech and language components. End-to-end systems offer less flexibility to substitute
    specialized modules or inject domain knowledge:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可定制性允许将ASR能力量身定制为特定领域，如医学、法律或客户支持中心。这需要接口化的独立语音和语言组件。端到端系统在替换专用模块或注入领域知识时，灵活性较低：
- en: '**End-to-end difficulty**: Entangled components limit modularity and custom
    inputs.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端困难**：纠缠的组件限制了模块化和自定义输入。'
- en: '**Hybrid advantage**: Mix and match acoustic, pronunciation, and language models.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合优势**：混合和匹配声学、发音和语言模型。'
- en: The ability to swap modules makes it simpler to bias models toward specific
    vocabularies or formats in hybrid systems. This greater control and specialization
    increase applicability for niche use cases.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 模块交换的能力使得在混合系统中将模型偏向特定词汇或格式变得更简单。这种更大的控制力和专业化提高了在特定应用场景中的适用性。
- en: Deployment requirements
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署要求
- en: 'Hardware constraints around memory, compute, and power consumption determine
    feasible deployment environments for ASR systems:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件限制（如内存、计算能力和功耗）决定了语音识别系统的可行部署环境。
- en: '**End-to-end difficulty**: Large, resource-intensive models strain edge devices.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端困难**：大型、资源密集型的模型给边缘设备带来了压力。'
- en: '**Hybrid advantage**: Distribute pipelines across systems and specialized devices.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合优势**：将管道分布到不同的系统和专业设备上。'
- en: For example, researchers execute acoustic models on low-powered end devices
    while offloading language models to the cloud, effectively allowing split processing.
    Compression techniques such as *quantization* can shrink models, but hybrid systems
    provide more deployment flexibility.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，研究人员在低功耗的终端设备上执行声学模型，同时将语言模型卸载到云端，有效地实现了分布式处理。诸如*量化*等压缩技术可以缩小模型，但混合系统提供了更多的部署灵活性。
- en: In many ways, end-to-end modeling represents a philosophically purer approach.
    But hybrid systems make practical trade-offs, unlocking modular, customizable
    architectures. This positions them to deliver greater accuracy, lower latency,
    and more flexible deployments across diverse speech recognition applications.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个方面来看，端到端建模代表了一种哲学上更纯粹的方法。但混合系统则做出了实际的折衷，解锁了模块化、可定制的架构。这使得它们能够在多种语音识别应用中提供更高的准确性、更低的延迟和更灵活的部署。
- en: Whisper charts an exciting path forward, unlocking many benefits of integrated
    end-to-end modeling while retaining a hybrid acoustic/language split. As research
    in conversational AI continues rapidly advancing, we may one day see end-to-end
    networks rivaling and even overtaking hybrid approaches thanks to sufficient data
    and computing. But for now, hybrid architectures rule production speech recognition
    thanks to their balance of quality, speed, and control.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper为未来铺设了一条激动人心的发展道路，在保留混合声学/语言分割的同时，解锁了端到端建模的诸多优势。随着对话式AI研究的快速发展，未来我们或许会看到端到端网络能够与混合方法相媲美，甚至超过它们，这得益于足够的数据和计算能力。但目前，混合架构在生产环境中的语音识别中占据主导地位，因为它们在质量、速度和控制之间达到了平衡。
- en: As we just explored, Whisper strikes an optimal balance by blending end-to-end
    and modular components. Let’s delve into the specific neural network architectures
    that Whisper unites, including CTC models and transformers, to create such a high-performing
    hybrid speech recognition pipeline.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚刚探讨的，Whisper通过融合端到端和模块化组件找到了最佳的平衡。让我们深入了解Whisper整合的具体神经网络架构，包括CTC模型和Transformer，以创建这样一个高性能的混合语音识别管道。
- en: Combining connectionist temporal classification and transformer models in Whisper
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Whisper中结合了连接时序分类（CTC）和Transformer模型。
- en: 'Behind the scenes, Whisper fuses two powerful neural network architectures
    to unlock robust speech recognition:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，Whisper融合了两种强大的神经网络架构，解锁了强大的语音识别能力：
- en: CTC, which excels at labeling unsegmented sequential data such as audio streams
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CTC擅长标注未分割的序列数据，如音频流。
- en: Transformers, which encode global dependencies in sequences via self-attention,
    capturing long-range linguistic context
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer通过自注意力机制编码序列中的全局依赖，捕捉长距离的语言上下文。
- en: This hybrid CTC/transformer scheme builds on decades of research into recurrent
    networks, computer vision, and NLP. The resulting system handles both aligned
    and unaligned speech with greater context. Let’s explore Whisper’s technical foundations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合CTC/Transformer方案基于几十年对递归网络、计算机视觉和自然语言处理（NLP）领域的研究。由此产生的系统能更好地处理对齐和未对齐的语音，具有更强的上下文理解能力。让我们来探讨一下Whisper的技术基础。
- en: Connectionist temporal classification
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接时序分类（CTC）
- en: The CTC algorithm, introduced in 2006, provides an elegant framework for transcribing
    unsegmented sequences. It can identify labels from raw streams without knowing
    alignment boundaries.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: CTC算法于2006年提出，为转录未分割的序列提供了一种优雅的框架。它可以在不知道对齐边界的情况下，从原始流中识别标签。
- en: Speech recognition must handle continuous inputs with fuzzy transitions between
    words and sounds. Unlike text, audio signals don’t come pre-split into semantic
    units. CTC learns to detect phonemes and transcripts directly from feature sequences.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别必须处理连续输入，其中单词和声音之间的过渡模糊不清。与文本不同，音频信号并没有预先按语义单元分割。CTC 学会从特征序列中直接检测音素和转录文本。
- en: CTC frames the labeling task to predict a probability distribution over all
    possible label sequences. An initial output layer emits label candidates for each
    timestep. Post-processing then consolidates these into the final, most probable
    transcript using dynamic programming.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: CTC 将标签任务框定为预测所有可能标签序列的概率分布。初始输出层为每个时间步发出标签候选项。后处理则使用动态规划将这些候选项整合为最终的、最可能的转录文本。
- en: 'For example, CTC’s initial network layer might output a messier sequence such
    as “*th**―**e c**―**a**―**t s**―**a**―**t.”* Repeated labels and blanks collapsed
    into the final prediction: “*the* *cat sat.”*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CTC 的初始网络层可能输出类似于“*th**―**e c**―**a**―**t s**―**a**―**t.”* 的杂乱序列。重复的标签和空白会被压缩成最终的预测结果：“*the*
    *cat sat.”*
- en: CTC’s algorithmic approach essentially *warps* predictions onto the correct
    ground truth sequence. Powerful acoustic models such as *Wav2Vec 2.0* now leverage
    CTC to frame speech recognition as a sequence transduction problem solved by deep
    learning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: CTC 的算法方法本质上是将预测“扭曲”到正确的真实标签序列上。像 *Wav2Vec 2.0* 这样的强大声学模型现在利用 CTC 将语音识别框定为一个通过深度学习解决的序列转导问题。
- en: Transformers and self-attention
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer 和自注意力机制
- en: Transformers were introduced in 2017 for machine translation. They deliver breakthrough
    results by modeling sequences in radically new ways. Rather than recurrence and
    convolutions, transformers process inputs using multiheaded self-attention.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 于 2017 年首次引入用于机器翻译。通过以全新的方式建模序列，它们实现了突破性的成果。与递归和卷积不同，Transformer
    使用多头自注意力机制处理输入。
- en: This mechanism calculates representations of sequence positions by relating
    them to every other element. Models learn which contextual relationships matter
    most to focused tasks such as translation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该机制通过将序列位置与每个其他元素关联，计算序列位置的表示。模型学会了哪些上下文关系对专注的任务（如翻译）最为重要。
- en: For example, a transformer translates a sentence by heavily weighting the essential
    self-attentions for generating the next output word based on the inputs. This
    gives it a global purview of long-range dependencies unavailable to RNNs and **convolutional
    neural** **networks** (**CNNs**).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Transformer 通过大量加权关键自注意力机制，基于输入生成下一个输出词。这使得它能够全面理解长距离依赖关系，而这些在 RNN 和 **卷积神经**
    **网络**（**CNNs**）中是无法实现的。
- en: Transformers now underpin state-of-the-art NLP across machine translation, question-answering,
    dialogue systems, and more. Models such as *GPT-4* reveal their excellent linguistic
    abilities.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Transformer 支撑着最先进的自然语言处理技术，涵盖了机器翻译、问答、对话系统等领域。像 *GPT-4* 这样的模型展现了它们出色的语言能力。
- en: Uniting CTC, transformers, and speech
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CTC、Transformer 和语音的结合
- en: Modern ASR systems blend these advanced neural architectures. CTC handles unlabeled
    audio streams with fuzzy sound alignments, and transformers encode robust language
    representations and output text corrections.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的 ASR 系统结合了这些先进的神经网络架构。CTC 处理带有模糊声音对齐的未标记音频流，Transformer 则编码稳健的语言表示并输出文本修正。
- en: 'Specifically, Whisper infuses transformers after the CTC acoustic model during
    decoding. This two-step pipeline maximizes both auditory and linguistic learning:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，Whisper 在解码过程中将 Transformer 融合到 CTC 声学模型之后。这个两步管道最大化了听觉和语言学习：
- en: '![Figure 2.2 – Two-step pipeline in the Whisper ASR system](img/B21020_02_2.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – Whisper ASR 系统中的两步管道](img/B21020_02_2.jpg)'
- en: Figure 2.2 – Two-step pipeline in the Whisper ASR system
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – Whisper ASR 系统中的两步管道
- en: The CTC model first generates label candidates from raw audio. This handles
    the fuzzy sound-to-symbol transduction challenges. Downstream transformers then
    refine and correct the initial CTC outputs by better incorporating language context.
    Human speech often deviates from formal textual language, so additional language-specific
    conditioning is needed to improve transcript accuracy.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: CTC 模型首先从原始音频生成标签候选项。这解决了模糊的声音到符号的转化挑战。下游的 Transformer 则通过更好地结合语言上下文，细化和纠正初始
    CTC 输出。由于人类语音通常偏离正式的文本语言，因此需要额外的语言特定条件化来提高转录准确性。
- en: Jointly optimized during training, this architecture fits language structure
    onto imperfect acoustic outputs. Whisper bridges the auditory and linguistic domains
    to handle real-world speech recognition.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中联合优化，该架构将语言结构适配到不完美的声学输出上。Whisper架起了听觉和语言领域的桥梁，处理现实世界中的语音识别。
- en: 'In summary, fusing CTC, transformers, and speech unlocks synergistic advantages:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，融合CTC、变换器和语音技术能够释放协同效应的优势：
- en: Robust acoustic modeling from CTC specialization
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自CTC专业化的强大声学建模
- en: Global language context from transformer self-attentions
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自变换器自注意力的全球语言上下文
- en: Joint optimization between all components
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有组件之间的联合优化
- en: Customization of separate modules
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分模块定制
- en: Together, CTC and attention mechanisms provide the best of both worlds. Whisper
    banked on their complementary superpowers to drive state-of-the-art capabilities.
    This technical combo meal fuels Whisper’s reliability and accuracy in recognizing
    speech in the wild. The all-neural design also simplifies training by co-optimizing
    the entire pipeline end-to-end.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: CTC 和注意力机制共同提供了二者的最佳优势。Whisper依靠它们互补的超级能力来推动最先进的功能。这一技术组合为Whisper在复杂环境中进行语音识别提供了可靠性和准确性。全神经网络的设计也通过端到端的联合优化简化了训练过程。
- en: Expect transformer architectures to dominate as foundations for advancing conversational
    AI. Combined with complementary specialization techniques, as shown in Whisper,
    their flexible modeling capacities unlock ever-improving language-aware speech
    recognition systems.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 预计变换器架构将在推动对话AI的基础上占主导地位。结合如Whisper所示的互补专业技术，它们灵活的建模能力解锁了不断改进的语言感知语音识别系统。
- en: Now that we have explored how Whisper combines the strengths of CTC and transformer
    models to handle the acoustic challenges of transcribing speech signals, we are
    ready to examine the other half of the equation – integrating linguistic knowledge
    for translating signals into coherent language. After all, accurate speech recognition
    requires more than precise acoustic signal decoding – outputs must conform to
    the constructs and conventions of natural languages such as English to be usable.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了Whisper如何结合CTC和变换器模型的优势，以应对转录语音信号的声学挑战，接下来我们准备研究方程的另一半——整合语言学知识，将信号转化为连贯的语言。毕竟，精确的语音识别不仅需要精准的声学信号解码——输出还必须符合自然语言（如英语）的构造和惯例，才能使用。
- en: The role of linguistic knowledge in Whisper
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言学知识在Whisper中的作用
- en: Robust speech recognition requires more than decoding audio signals. Systems
    must account for the complexities of human language to handle real-world variability
    and ambiguity. This is where integrating meaningful linguistic knowledge becomes
    critical.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的语音识别不仅仅需要解码音频信号。系统必须考虑到人类语言的复杂性，以应对现实世界中的变化和模糊性。这正是整合有意义的语言学知识变得至关重要的地方。
- en: State-of-the-art solutions such as Whisper enhance accuracy by combining acoustic
    predictions with **language-specific conditioning**. Language models provide the
    statistical probabilities of potential word or phoneme sequences based on the
    language’s constructs. This allows for selecting the most likely text transcript
    fitting the acoustic signal among multiple guess candidates.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的解决方案，如Whisper，通过将声学预测与**语言特定的调节**相结合，提升了准确性。语言模型提供了基于语言构造的潜在单词或音素序列的统计概率。这使得在多个候选结果中，能够选择最符合声学信号的最可能的文本转录。
- en: 'Language-specific conditioning then adapts the models to the characteristics
    and conventions of the target language, including the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 语言特定的调节随后将模型调整到目标语言的特征和惯例，包括以下内容：
- en: Vocabulary – the valid words and lexical constructs
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇表——有效的单词和词汇结构
- en: Grammar – how words fit together into phrase structures
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法——词语如何组合成短语结构
- en: Pronunciation modeling – the plausible speech sounds and patterns
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发音建模——合理的语音音素和模式
- en: Non-native pronunciation challenges – adapting to accents of second-language
    speakers
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非母语发音挑战——适应第二语言使用者的口音
- en: Dialects – handling different global dialects
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方言——处理全球不同方言
- en: By tailoring to these linguistic attributes, Whisper develops an enriched understanding
    that facilitates recognizing language elements robustly, from core sounds to semantics.
    The customization allows for the graceful handling of real-world speech complexity.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过量身定制这些语言学属性，Whisper开发了丰富的理解能力，有助于从核心音素到语义等方面强健地识别语言元素。这一定制使得能够优雅地处理现实世界中复杂的语音特征。
- en: Language-specific conditioning
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 语言特定条件化
- en: Language-specific conditioning makes transcriptions more precise by resolving
    acoustic uncertainties, such as reducing confusion between homophones such as
    “they’re,” “their,” and “there.” The ASR system models probability distributions
    from potential words and the context provided by language-specific conditioning.
    For example, **phonotactic constraints**, which describe the allowable combinations
    of phonemes in a particular language, can guide the ASR system, especially when
    acoustic cues are missing or distorted. **Semantic analysis** can also bias the
    speech recognizer toward sentences appropriate to a particular task or domain
    and away from meaningless sequences of words.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 语言特定条件化通过解决声学不确定性，使转录更加精确，例如减少同音异义词之间的混淆，如“they’re”，“their”和“there”。ASR 系统从潜在单词和语言特定条件化提供的上下文中建模概率分布。例如，**音系约束**描述了特定语言中音素的可组合性，可以在声学线索丢失或失真的情况下引导
    ASR 系统。**语义分析**还可以使语音识别系统偏向于适合特定任务或领域的句子，而远离没有意义的单词序列。
- en: Let’s explore the various facets of language-specific conditioning that empower
    Whisper’s supervised and semi-supervised training.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下赋能 Whisper 的监督和半监督训练的各种语言特定条件化方面。
- en: Vocabulary encoding
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词汇编码
- en: Recognizing speech requires mapping audio to semantic symbols such as words
    or subword units. Whisper encodes vocabulary from diverse textual datasets spanning
    web pages, books, code, and more.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别需要将音频映射到语义符号，如单词或子词单元。Whisper 从涵盖网页、书籍、代码等多种文本数据集对词汇进行编码。
- en: At the time of writing, Whisper’s latest iteration, called `large-v3`, was released
    on November 6, 2022\. The model was trained on 1 million hours of weakly labeled
    audio (weakly supervised pre-training) and 4 million hours of pseudo-labeled audio
    collected using `large-v2`. Whisper’s weakly supervised pre-training process engraves
    **parseable language constructs** into the model. These constructs are essentially
    patterns and structures in the language that the model learns to recognize and
    reproduce. This learning process is not as precise as fully supervised learning,
    but it provides enough information for the model to learn effectively. When the
    model is later fine-tuned on downstream speech recognition tasks, these learned
    language constructs transfer to the new tasks. This means the model can apply
    the patterns and structures discovered during pre-training to recognize and transcribe
    speech in the downstream tasks. In essence, the weakly supervised pre-training
    process allows Whisper to learn a broad understanding of language vocabulary from
    a large and diverse dataset and then apply this understanding of vocabulary to
    specific tasks during fine-tuning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，Whisper 的最新版本 `large-v3` 于 2022 年 11 月 6 日发布。该模型在 100 万小时的弱标签音频（弱监督预训练）和
    400 万小时使用 `large-v2` 收集的伪标签音频上进行了训练。Whisper 的弱监督预训练过程将**可解析的语言结构**刻录到模型中。这些结构本质上是语言中的模式和结构，模型学习识别并再现这些模式。这个学习过程不像完全监督学习那样精确，但它为模型提供了足够的信息，使其能够有效学习。当模型在下游语音识别任务中进一步微调时，这些学习到的语言结构会转移到新任务上。这意味着模型可以将预训练期间发现的模式和结构应用到下游任务中，从而识别和转录语音。实质上，弱监督预训练过程使得
    Whisper 能够从一个大型多样化的数据集学习语言词汇的广泛理解，并在微调时将这种词汇理解应用于特定任务。
- en: Whisper’s language-specific conditioning and vocabulary encoding are key to
    engraving nuanced statistical representations around lexical and phrasal *shapes*
    in the language model. By having encoded permissible linguistic forms and structures,
    Whisper can segment continuous speech signals and selectively surface plausible
    word candidates matching learned vocabulary patterns. This linguistic familiarity
    helps resolve uncertainty during acoustic decoding by restricting outputs to plausible
    lexical selections that align with the encoded vocabulary. In other words, by
    thoroughly modeling the shapes and contours of a language’s lexical norms, Whisper
    can smoothly map noisy speech signals to valid textual candidates that agree with
    its vocabulary knowledge.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的语言特定条件化和词汇编码是围绕词汇和短语的*形状*在语言模型中刻录细微统计表示的关键。通过编码允许的语言形式和结构，Whisper 能够对连续的语音信号进行分段，并选择性地呈现符合学习到的词汇模式的合理单词候选项。这种语言熟悉性有助于通过将输出限制为与编码词汇一致的合理词汇选择，来解决声学解码中的不确定性。换句话说，通过彻底建模语言词汇规范的形状和轮廓，Whisper
    能够将噪声语音信号平滑地映射到与其词汇知识一致的有效文本候选项。
- en: Grammars and structures
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语法和结构
- en: Beyond individual words, processing natural language requires encoding permissible
    grammatical patterns. Structures such as part-of-speech sequences (e.g., noun→verb→adverb)
    and multiword phrases (e.g., “on the other hand”) constitute allowable syntax.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单个单词外，处理自然语言还需要编码允许的语法模式。诸如词性顺序（例如，名词→动词→副词）和多词短语（例如，“on the other hand”）等结构构成了允许的句法。
- en: Whisper’s pre-training exposures ingest common English language text structures
    across genres such as news articles, literature, emails, code, etc. The diversity
    captures constructions that are both simple and complex.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的预训练暴露包含了跨多个领域的常见英文文本结构，如新闻文章、文学作品、电子邮件、代码等。这种多样性涵盖了简单和复杂的构造。
- en: Resulting language models steer outputs toward valid utterances. For example,
    grammatical knowledge informs the proper expansion of abbreviations and acronyms
    based on context (e.g., knowing NASA refers to the National Aeronautics and Space
    Administration). This goes beyond basic vocabulary familiarity.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 结果生成的语言模型会引导输出向有效的表达靠拢。例如，语法知识有助于根据上下文正确扩展缩写和首字母缩略词（例如，知道 NASA 指的是美国国家航空航天局）。这不仅仅是基础词汇的熟悉。
- en: Structured language representations reduce false positive transcripts that fail
    to conform to accepted grammar and phrasing conventions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化的语言表示减少了那些不符合接受的语法和表述规范的错误转录。
- en: Pronunciation modeling
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发音建模
- en: Humans pronounce words differently across regions and contexts. Modeling diverse
    accents, speech impediments, coarticulation, and other variabilities improves
    recognition.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在不同地区和情境下发音有所不同。建模多种口音、语音障碍、连读以及其他变异性有助于提高识别能力。
- en: Whisper demonstrates remarkable adaptation to unique pronunciation styles. Its
    self-supervised pre-training leverages audio narration data containing diverse
    voices.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 展现了对独特发音风格的显著适应能力。其自监督预训练利用了包含多种声音的音频叙述数据。
- en: Exposure to different speakers teaches the correlations between raw acoustic
    signals and their associated words, regardless of minor variations. Patterns still
    emerge around customary pronunciation deviations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与不同说话者的接触教会了原始声学信号与其相关单词之间的关联，不管是微小的变化。即便如此，关于习惯发音偏差的模式依然会显现。
- en: By ingesting many voices, Whisper builds acoustic-linguistic connections resilient
    to reasonable deviation. This gives decoding flexibility without excessive brittleness.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过吸收大量语音数据，Whisper 建立了抗合理偏差的声学语言连接。这赋予了解码灵活性，而不至于过于脆弱。
- en: Non-native pronunciation challenges
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非母语发音挑战
- en: However, modeling fluent non-native speech poses added challenges. Second-language
    speakers learn pronunciation patterns that can deviate more significantly than
    others from native norms.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，建模流利的非母语发音会带来更多的挑战。第二语言学习者的发音模式可能与母语规范有较大偏差。
- en: For example, Mandarin Chinese speakers consistently substitute */l/* for */r/*
    sounds when speaking English. Other syllabic mismatches trip up language learners
    in relatively systematic ways.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，普通话中文说话者在讲英语时会始终将*/l/*发音替代为*/r/*。其他音节不匹配会让语言学习者在相对有规律的方式下出错。
- en: Handling these non-native patterns requires even more diversity during training
    to capture a long tail of accents. Thankfully, Whisper’s self-supervised pre-training
    leverages English narration data from international sources.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些非母语的发音模式需要在训练过程中进行更多的多样化，以捕捉各种口音的长尾。幸运的是，Whisper 的自监督预训练利用了来自国际来源的英语叙述数据。
- en: The model encodes correlations between common second-language speech deviations
    and correct standard transcripts. This exposure teaches associations despite incorrectly
    pronounced words or misordered *visemes*.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型编码了常见的第二语言发音偏差与正确标准转录之间的关联。这种暴露帮助模型学习关联，即便是发音不准确的单词或错序的*视觉音素*。
- en: The result is a more globally relevant system forgiving non-native, accent-influenced
    speech. Whisper demonstrates marked gains in recognizing learners compared to
    previous benchmarks lacking sufficient dialectal range during training.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个更具全球相关性的系统，能够宽容地处理非母语、受口音影响的发音。与以前在训练中缺乏足够方言范围的基准相比，Whisper 在识别学习者方面表现出了显著的提升。
- en: Dialectal fluency
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方言流利度
- en: Beyond pronunciation, more considerable dialectal differences characterize groups
    speaking the same root language, whether regional British English or Singaporean
    English; supporting diverse dialects requires dialect-tuned modeling.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了发音外，同一种语言的群体之间往往有更显著的方言差异，无论是英式英语、还是新加坡英语；支持多种方言需要方言调优的建模。
- en: Whisper was fed English data spanning international publications, books, web
    articles, and more. This molded inclusive dialect fluency beyond solely American
    English. The vocabulary, grammar, and phrasing encapsulate diverse English dialects.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 接收了涵盖国际出版物、书籍、网络文章等的英语数据。这使得其语言能力不仅限于美式英语，涵盖了更多包容性方言。其词汇、语法和措辞体现了多样的英语方言。
- en: Exposure to this breadth allows it to adapt gracefully to users worldwide. Performance
    remains strong without solely overfitting to a single flavor of English. The model
    generalizes across dialects. This dialectal dexterity prevents fragmented accuracy
    across geos and usage domains. Whisper aims for dialect-agnostic fluency in its
    linguistic foundations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这种广泛的接触让它能够优雅地适应全球用户。性能依然强劲，而不仅仅是过度拟合到某一种英语口音。该模型能够在各个方言之间进行泛化。这种方言灵活性防止了在不同地区和使用领域的准确度碎片化。Whisper
    旨在其语言基础上实现方言无关的流畅性。
- en: Whisper can easily handle real-world speech complexity by fusing speech recognition
    with multifaceted language knowledge. Its unprecedented vocabulary capacity, dialectal
    range, and syntactic mastery enable the decoding of extraordinarily diverse audio
    with precision and recall across domains.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 通过将语音识别与多方面的语言知识结合，轻松应对现实世界中的语音复杂性。其前所未有的词汇容量、方言范围和句法掌握能力使其能够高精度和高召回地解码各个领域中极为多样的音频内容。
- en: You’ll soon understand everything from acoustic feature extraction through language
    model decoding and refinements. This systemic view connects dots across the modules
    powering Whisper’s end-to-end pipeline.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 很快你就会理解从声学特征提取到语言模型解码和优化的整个过程。这个系统性的视角将不同模块的工作方式联系起来，推动 Whisper 的端到端流程。
- en: Equipped with architectural knowledge, we’ll optimize configurations for your
    specific use case constraints around precision, latency, and cost. But first,
    let’s decompose Whisper into its critical sub-systems.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了架构知识后，我们将根据你具体的精度、延迟和成本等使用场景要求优化配置。但在此之前，让我们先将 Whisper 拆解成其关键子系统。
- en: Understanding Whisper’s components and functions
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Whisper 的组件和功能
- en: Now that we’ve demystified Whisper’s architecture and optimized design, it’s
    time to dive deeper into its functional components. This critical section dissects
    the modules powering Whisper’s speech recognition pipeline from audio ingestion
    to text output.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解密了 Whisper 的架构和优化设计，接下来是深入探讨其功能组件的时候了。本节内容将详细解析驱动 Whisper 语音识别流程的模块，从音频输入到文本输出。
- en: We’ll survey the processes involved in converting spoken utterances into machine-readable
    transcripts. We aim to develop systemic intuitions about how Whisper’s parts cooperate
    fluidly to handle real-world speech translation challenges at scale.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨将口语转化为机器可读转录的过程。我们的目标是通过系统性直觉，理解 Whisper 各部分如何流畅合作，处理大规模的现实世界语音翻译挑战。
- en: 'While mathematical complexities operate under the hood, you’ll gain accessible
    clarity around the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数学复杂性在幕后操作，但你将获得以下内容的清晰理解：
- en: Preprocessing of raw audio signals
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始音频信号的预处理
- en: Encoding of acoustic patterns
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声学模式的编码
- en: Modeling of language
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模
- en: Searching for output spaces
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索输出空间
- en: Refinement of transcripts
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转录文本的优化
- en: Understanding these functional pieces grants intuition for tweaking configurations
    and components toward your use case constraints. Architectural literacy breeds
    strategic optimization. Minor tuning adjustments may yield dramatic gains.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些功能模块能为调整配置和组件提供直觉，优化配置以适应你的使用场景要求。架构知识可以促进战略性优化，微调调整可能带来显著提升。
- en: Let’s start unfolding Whisper’s components like a complex Swiss watch.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像拆解一只复杂的瑞士手表一样，开始探索 Whisper 的组件。
- en: Audio input and preprocessing
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频输入与预处理
- en: The journey from speech to transcription begins with audio input. Whisper ingests
    raw waveform signals via microphones or other audio capture sources. This analog
    audio then undergoes frontend processing, including noise filtering and digitization,
    to extract clean features and encode the verbal content.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 从语音到转录的旅程始于音频输入。Whisper 通过麦克风或其他音频捕捉设备获取原始波形信号。这些模拟音频信号经过前端处理，包括噪声过滤和数字化，提取干净的特征并编码语言内容。
- en: Understanding Whisper’s audio handling stages is crucial for configuring suitable
    data collection pipelines. We must feed the system quality inputs, emphasizing
    linguistic essence rather than distracting characteristics.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Whisper的音频处理阶段对于配置合适的数据采集管道至关重要。我们必须为系统提供优质的输入，重点关注语言本质，而非干扰特征。
- en: Let’s explore the role of audio input hardware, preprocessing considerations,
    and Whisper’s feature extraction process, which sets the critical foundation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来探索音频输入硬件的作用、预处理考虑事项以及Whisper的特征提取过程，这些构成了关键的基础。
- en: Audio input sources
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频输入源
- en: 'High-performance speech recognition requires quality signals from the start.
    While ambient acoustic environments differ, ideal audio capture equipment for
    Whisper includes the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能语音识别要求从一开始就获取优质的信号。尽管环境中的噪声条件各异，但Whisper理想的音频捕捉设备包括以下内容：
- en: '**Microphones**: Dedicated microphone hardware with crisp, directional inputs
    ensures the capturing of clear speech from users. Consumer devices often have
    inadequate mic components that are unable to isolate voices. Prioritize lavalier
    microphones or mic arrays focusing on speaker voices over environmental noise.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**麦克风**：专用麦克风硬件具有清晰的定向输入，确保能够捕捉到用户的清晰语音。消费类设备通常配备的麦克风组件不足，无法有效隔离人声。优先选择领夹麦克风或麦克风阵列，专注于捕捉说话者的声音而非环境噪声。'
- en: '**Near-field sources**: When possible, minimize interference by positioning
    microphones near target speakers. This prevents contamination from far-field sounds.
    Have users speak directly into devices.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近场源**：尽可能将麦克风放置在目标说话者附近，以减少干扰。这可以防止远场声音的污染。让用户直接对着设备说话。'
- en: '**Low noise conditions**: Seek quiet indoor settings without disruptive background
    noise. Echoey rooms also distort audio – whenever possible, record speech in sound-dampened
    environments through acoustic paneling.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低噪声条件**：选择安静的室内环境，避免干扰性背景噪音。回音较大的房间也会扭曲音频——如果可能的话，尽量在经过声学处理的环境中录音，以减少噪声。'
- en: Preprocessing and filtering
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理和滤波
- en: 'Before analysis, raw audio requires preprocessing to improve signal quality
    and extract critical characteristics. Whisper applies the following crucial adjustments:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析之前，原始音频需要预处理，以提高信号质量并提取关键特征。Whisper会进行以下重要调整：
- en: '**Noise reduction**: Environmental sounds such as humming appliances degrade
    performance. Adaptive filters identify and subtract predictable background noise
    spectral profiles.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声减少**：环境噪音如电器的嗡嗡声会降低性能。自适应滤波器能够识别并减去可预测的背景噪音频谱。'
- en: '**Gain normalization**: Variations in recording volumes should get normalized
    to a standard intensity range – loudness alone conveys no linguistic meaning.
    Target -20 to -10 dBFS for clear but uncompressed speech peaks.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增益归一化**：录音音量的变化应归一化到标准强度范围——仅靠响度无法传达语言意义。目标是将音量控制在-20到-10 dBFS之间，以确保语音峰值清晰且未压缩。'
- en: '**Frequency equalization**: This balances relative energy distribution across
    low-, mid-, and high-frequency bands and sharpens acoustic signatures of speech
    components such as consonants and vowels for better perception by algorithms.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率均衡**：通过平衡低频、中频和高频段的相对能量分布，锐化语音成分如辅音和元音的声学特征，以便算法更好地识别。'
- en: Audio feature extraction
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频特征提取
- en: 'The final frontend step extracts informative numeric representations of the
    audio data through signal processing techniques before feeding Whisper models.
    Essential feature extraction methods include the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的前端步骤通过信号处理技术提取音频数据的有用数值表示，然后再输入到Whisper模型中。基本的特征提取方法包括以下内容：
- en: '**Spectrograms**: Visually map the signal energy across audio frequencies over
    time. Differences reveal speech components.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频谱图**：通过视觉方式映射音频频率随时间变化的信号能量差异，揭示语音成分。'
- en: '**Log-Mel filter banks**: Mimic the human auditory system’s frequency perception
    by compressing and smoothing critical bands.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数梅尔滤波器组**：通过压缩和平滑关键频段，模拟人类听觉系统的频率感知。'
- en: '**Mel-frequency cepstral coefficients** (**MFCCs**): Statistically compress
    frequency data into most variant latent dimensions via MFCC transformers.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梅尔频率倒谱系数**（**MFCCs**）：通过MFCC变换器，统计性地将频率数据压缩到最具变异性的潜在维度。'
- en: Together with pixel-like frame sequencing across time, these high-level features
    encode audio in model-consumable tensors while denoising. The resulting compact
    preprocessing captures core speech essence to inform acoustic modeling. Getting
    this front end right ensures Whisper gets clean data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 结合像像素一样的时间帧序列，这些高级特征以模型可消费的张量形式编码音频，同时进行去噪。由此产生的紧凑预处理捕捉了核心语音本质，以便为声学建模提供信息。确保前端处理正确，确保Whisper获得干净的数据。
- en: Next, we’ll see how Whisper leverages the outputs for decoding speech components
    into text.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到Whisper如何利用这些输出解码语音成分为文本。
- en: Acoustic modeling
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声学建模
- en: The next phase is acoustic modeling after preprocessing audio into informative
    feature representations. This converts low-level speech signals into higher-level
    linguistic units through statistical learning – the first step in translating
    sounds into language symbols.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是音频预处理后进行声学建模，将音频转换成有用的特征表示。这通过统计学习将低级语音信号转化为更高级的语言单元——这是将声音转化为语言符号的第一步。
- en: 'Acoustic modeling uncovers and encodes the correlations between raw speech
    audio patterns and corresponding textual artifacts such as words, phonemes, or
    characters. Models capture the systematic relationships between the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 声学建模揭示并编码原始语音音频模式与相应文本产物（如单词、音素或字符）之间的关联。模型捕捉以下内容的系统关系：
- en: Spoken sounds
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口语化声音
- en: Word spellings
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词拼写
- en: Language semantics
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言语义
- en: By mathematically representing these associations, systems such as Whisper decode
    speech waveforms into probable transcriptions, bridging the acoustic and linguistic
    domains.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数学表示这些关联，像Whisper这样的系统将语音波形解码为可能的转录文本，架起了声学与语言学领域之间的桥梁。
- en: Speech units for acoustic modeling
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于声学建模的语音单元
- en: 'Ideally, acoustic models would directly translate waveform signals into complete
    transcripts. However, reliably modeling such complex conditional probabilities
    requires massive datasets covering all variations. Instead, architects insert
    intermediate steps tying acoustics to smaller constructive units:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，声学模型应直接将波形信号转换为完整的转录文本。然而，可靠地建模如此复杂的条件概率需要涵盖所有变异的大规模数据集。因此，架构师插入中间步骤，将声学信号与更小的构建单元连接起来：
- en: '**Phoneme-level modeling**'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音素级建模**'
- en: HMMs historically decoded audio into constituent phonemes as an interim output
    for downstream processing. However, this requires preemptively segmenting speech
    signals, which relies on prior acoustic understanding.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HMM（隐马尔可夫模型）历史上将音频解码成构成音素，作为下游处理的过渡输出。然而，这需要预先对语音信号进行分段，这依赖于先前的声学理解。
- en: '**Character-level modeling**'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符级建模**'
- en: Modern end-to-end architectures such as Deep Speech translate acoustics straight
    into characters. But naively focusing solely on characters risks losing sensitivity
    to higher-level constructs such as words and phrases.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现代的端到端架构，如Deep Speech，直接将声学信号翻译成字符。但单纯聚焦于字符可能会失去对更高级结构（如单词和短语）的敏感性。
- en: Whisper leverages a middle ground—modeling customized subwords as the target
    acoustic conditioning labels. These data-driven lexical chunks strike a balance
    between atomic signals and complex phrases, allowing both sonic details and linguistic
    structures to shine through.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper采用了一种折中的方式——将定制的子词建模为目标声学调节标签。这些数据驱动的词汇单元在原子信号与复杂短语之间找到了平衡，使得声音细节与语言结构得以充分展现。
- en: Whisper’s acoustic model architecture
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Whisper的声学模型架构
- en: 'Using a CTC loss function, Whisper’s acoustic model fuses causal convolutional
    layers with recurrent transformers. This unique combination handles local audio
    patterns while learning longer-term dependencies:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CTC损失函数，Whisper的声学模型将因果卷积层与递归变换器融合。这种独特的组合处理局部音频模式，同时学习长期依赖关系：
- en: Convolutions detect localized acoustic patterns associated with character sequences,
    simultaneously operating on small raw audio windows. Different filters learn various
    speech attributes.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积操作检测与字符序列相关的局部声学模式，同时在小的原始音频窗口上进行操作。不同的滤波器学习不同的语音属性。
- en: Recurrent layers then contextualize the sequential convoluted representations
    over more considerable periods using transformers. Attention distributions relate
    current audio to previous chunks to handle contiguous signal dynamics from individual
    sounds to complete multiword utterances.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环层通过使用变换器对顺序卷积表示进行上下文处理，跨越更长时间段。注意力分布将当前音频与前一段联系起来，以处理从单个声音到完整多词语句的连续信号动态。
- en: The CTC loss function provides training supervision, bridging lower-level audio
    patterns with the target unit labels such as subwords. Alignments get handled
    implicitly.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CTC 损失函数提供了训练监督，将低级音频模式与目标单元标签（如子词）进行连接。对齐操作被隐式处理。
- en: Whisper’s acoustic model architecture provides a powerful deep neural map that
    directly converts acoustic signals into linguistic constructs for downstream interpretation.
    This forms the essential sonic-to-semantic foundation for speech recognition.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的声学模型架构提供了一个强大的深度神经网络映射，将声学信号直接转换为语言结构，用于下游解释。这构成了语音识别的基本声音到语义的基础。
- en: Language modeling
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模
- en: The acoustic model handles the first phase, translating speech audio into linguistic
    symbols. But making sense of those symbols requires understanding language rules
    around vocabulary, semantics, grammar, and more. Enter language models – Whisper’s
    context experts.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 声学模型处理第一阶段，将语音音频转化为语言符号。但理解这些符号需要理解词汇、语义、语法等语言规则。进入语言模型——Whisper 的上下文专家。
- en: While acoustic models decode audio signals, language models interpret symbol
    sequences, providing context around permissible utterances. They score and refine
    interim transcriptions from upstream acoustics using statistical patterns and
    innate rules seen during training.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当声学模型解码音频信号时，语言模型解释符号序列，提供关于允许语句的上下文。它们使用训练期间看到的统计模式和固有规则，对来自上游声学模型的临时转录进行评分和优化。
- en: This entails disambiguating words based on probable language structure. For
    example, language models know that “The clouds are in the ____” more likely fits
    “sky” than random gibberish – models narrow uncertainty by assessing plausibility.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着根据可能的语言结构消除歧义。例如，语言模型知道，“The clouds are in the ____”更可能填入“sky”而不是随机的无意义词语——模型通过评估合理性来缩小不确定性。
- en: Whisper’s transformer language model
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Whisper 的变换器语言模型
- en: Whisper employs a standalone *transformer-based language model* that operates
    on interim acoustic model outputs. The transformer contains learned representations
    around the statistical relationships between words and multiword lexical chunks
    in English. It models complex linguistic contexts using stacked self-attention
    layers relating current symbols to surrounding ones.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 采用了一个独立的*基于变换器的语言模型*，它处理声学模型的临时输出。该变换器包含了学习到的表示，描述了英语中单词与多词词组之间的统计关系。它通过堆叠的自注意力层建模复杂的语言上下文，将当前符号与周围符号相关联。
- en: Specifically, Whisper’s language model leverages **masked language modeling**
    (**MLM**) for predicting randomly hidden words within a given context. This approach
    allows the model to make inferences based on the visible context and learning
    patterns.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Whisper 的语言模型利用**掩码语言建模**（**MLM**）来预测给定上下文中随机隐藏的词。这种方法使得模型能够根据可见的上下文和学习到的模式进行推理。
- en: Masked language modeling
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言建模
- en: MLM is a training technique used in NLP where some portion of the input data
    is intentionally masked or hidden during training, and the model is tasked with
    predicting the masked words based on the context provided by the unmasked words.
    In the context of Whisper and other ASR systems, MLM can be used to train the
    model to better understand the structure and semantics of the language in which
    it is transcribing. In essence, MLM allows the model to learn the underlying structure
    of the language and improve its ability to transcribe speech accurately, even
    in challenging conditions such as noisy environments or when dealing with accents
    or technical language.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: MLM 是一种在自然语言处理中的训练技术，在训练过程中会故意掩盖或隐藏输入数据的部分内容，模型需要基于未掩盖的词语上下文来预测被隐藏的词语。在 Whisper
    和其他语音识别系统的上下文中，MLM 可以用来训练模型更好地理解语言结构和语义，尤其是在噪音环境、方言或技术性语言等复杂条件下。归根结底，MLM 使得模型能够学习语言的基本结构，提高其在困难条件下准确转录语音的能力。
- en: By assessing possible transcriptions from acoustics against this robust understanding
    of language conventions, the model rescores and refines outputs for coherence.
    Fluency and semantic precision improve dramatically.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将声学信号的可能转录与对语言约定的强大理解进行对比，模型重新评分并优化输出，以确保一致性。流利性和语义精度得到了显著提高。
- en: Advantages over *N*-grams
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于*N*-gram 的优势
- en: Historically, speech systems modeled language using simple historical *n-gram
    counts* – the probability of each word following observed sequences. For example,
    3-grams encodes the likelihood of every word given every unique preceding word
    pair.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，语音系统使用简单的历史*n-gram计数*来建模语言——即每个词后跟随观察到的序列的概率。例如，3-gram编码了每个词基于前一个唯一的词对出现的概率。
- en: However, these Markovian models fail to exploit longer-range context and structural
    intricacies. Human language has more complexity than truncated historical statistics.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些马尔可夫模型无法利用更长范围的上下文和结构复杂性。人类语言比截断的历史统计更为复杂。
- en: Conversely, transformer language models learn holistic representations where
    every symbol gets contextually related to surrounding ones. There are no independent
    assumptions. Transformers handle intricacies such as hierarchical phrase structures
    that *n*-grams miss.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，变换器语言模型学习整体表示，其中每个符号与周围的符号在上下文中相关联。没有独立假设。变换器能够处理*n-gram所忽视的复杂性，如层次短语结构。
- en: This gives Whisper an enriched awareness of language behavior when refining
    acoustic transcriptions into valid, coherent text results. Powerful modern language
    modeling handles complexity beyond surface statistics.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得Whisper在将声学转录精炼成有效的、连贯的文本结果时，能更加深入地理解语言行为。强大的现代语言建模处理了超出表面统计的复杂性。
- en: Next, we’ll explore how all the upstream components come together during the
    decoding phase to generate final speech recognition outputs.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨所有上游组件在解码阶段如何协同工作，生成最终的语音识别输出。
- en: Decoding
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码
- en: Let’s recap the previous pipeline stages before proceeding further into decoding.
    The **audio input and preprocessing** phase converts raw waveforms into informative
    numeric representations, extracting relevant audio features. Next, the **acoustic
    modeling** stage predicts linguistic label outputs such as characters or subwords
    from the audio features, creating a set of estimating label sequences. The next
    stage, **language modeling**, assesses and re-scores the acoustic model’s initial
    label sequence predictions for greater coherence by incorporating contextual knowledge,
    resulting in interim transcriptions.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续解码之前，让我们回顾一下之前的管道阶段。**音频输入与预处理**阶段将原始波形转换为信息丰富的数字表示，提取相关的音频特征。接下来，**声学建模**阶段从音频特征中预测语言标签输出，如字符或子词，生成一组估算标签序列。接下来的阶段是**语言建模**，通过融入上下文知识评估和重新评分声学模型的初始标签序列预测，确保更大的连贯性，从而得到中间转录。
- en: 'So, in summary:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，总结一下：
- en: '**Audio preprocessing** provides input features.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频预处理**提供输入特征。'
- en: '**Acoustic modeling** makes initial label sequence predictions.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声学建模**做出初步的标签序列预测。'
- en: '**Language modeling** rescores those interim outputs.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言建模**重新评分这些中间输出。'
- en: Now, let’s understand how the decoding stage searches for the optimal text transcription
    fitting both the acoustic and language guidance.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解解码阶段如何搜索最优的文本转录，既符合声学指导，又符合语言指导。
- en: After extracting speech features, estimating label sequences, and scoring interim
    transcriptions, the final phase generates optimal text outputs – a process called
    decoding. This inference stage combines all upstream components.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取语音特征、估算标签序列并评分中间转录后，最终阶段生成最优的文本输出——这一过程叫做解码。这个推理阶段结合了所有上游组件。
- en: The decoder takes acoustic model label predictions and finds the best corresponding
    word sequences based on language model guidance. Efficient search is critical
    for navigating the exponentially ample space of possible transcriptions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器采用声学模型标签预测，并根据语言模型的指导找到最合适的词序列。高效的搜索对于在可能的转录空间中导航至关重要。
- en: Let’s explore Whisper’s decoding approach.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索Whisper的解码方法。
- en: Beam search
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 束搜索
- en: OpenAI employs **beam search** – a fast heuristic algorithm that approximates
    the most likely sequences while pruning unlikely candidates. This focuses computations
    on the most promising decoded text results.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI采用了**束搜索**——一种快速的启发式算法，它近似最可能的序列，同时修剪掉不太可能的候选序列。这样可以将计算集中在最有前途的解码文本结果上。
- en: Beam search example
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索示例
- en: Here’s a simplified example of how beam search might work in Whisper.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简化的示例，展示了束搜索在Whisper中的工作原理。
- en: Let’s say we have an audio input that says “Hello, world” and we’re using a
    beam width of two (meaning we keep the two most likely sequences at each step).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个音频输入，内容是“Hello, world”，并且我们使用宽度为二的束搜索（意味着在每一步我们保留两个最可能的序列）。
- en: At the first step, the model might predict that the most likely first words
    are “Hello” and “Yellow” based on the acoustic features of the audio input.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，模型可能会根据音频输入的声学特征预测最可能的首个单词是“Hello”和“Yellow”。
- en: In the next step, the model considers both sequences’ extensions. It might predict
    that “Hello, world” and “Hello, word” are the most likely continuations of “Hello”
    and that “Yellow world” and “Yellow word” are the most likely continuations of
    “Yellow.” The model then compares these sequences and keeps the two most likely
    overall. Let’s say it keeps “Hello, world” and “Yellow world.”
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，模型会考虑两个序列的扩展。它可能预测“Hello, world”和“Hello, word”是“Hello”的最可能延续，而“Yellow
    world”和“Yellow word”是“Yellow”的最可能延续。然后，模型会比较这些序列，并保留两个最可能的整体。假设它保留了“Hello, world”和“Yellow
    world”。
- en: This process continues until a stopping condition is met. In the end, the model
    might output “Hello, world” as the most likely transcription of the audio input.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程会持续，直到满足停止条件。最终，模型可能会输出“Hello, world”作为音频输入的最可能转录结果。
- en: It’s important to note that this is a simplified example, and the actual process
    involves complex calculations of probabilities based on the model’s learned parameters.
    Also, the beam width can be adjusted to trade-off between computational efficiency
    and transcription accuracy.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这是一个简化的例子，实际过程涉及基于模型学习的参数进行复杂的概率计算。此外，束宽度可以调整，以在计算效率和转录精度之间进行权衡。
- en: Beam search incrementally builds up partial transcriptions one token at a time,
    retaining only the top candidates at each step based on conditional model scores.
    Words get added to active hypotheses in order of probability.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索逐步构建部分转录，每次添加一个标记，并根据条件模型分数保留每一步的最顶端候选项。单词会按概率顺序添加到活动假设中。
- en: By discarding lower-scoring chains that are unlikely to maximize the final objective,
    searches remain tractable without exhaustively analyzing all options. The beam
    width determines processing breadth. Wider beams improve accuracy at an efficiency
    cost.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过丢弃不太可能最大化最终目标的低评分链条，搜索仍然可控，而无需穷尽地分析所有选项。束宽度决定了处理的广度。更宽的束改善了准确性，但代价是效率下降。
- en: Whisper leverages dynamic beam pruning for optimal trade-offs. Beam sizes expand
    and contract based on interim confidence scores. More candidates get retained
    during uncertain segments before being narrowed as clarity increases.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper利用动态束搜索修剪来实现最佳的权衡。束宽度根据中间置信度分数扩展和收缩。在不确定的片段中，会保留更多候选项，随着清晰度的提高再进行缩小。
- en: Rescoring and re-ranking
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新评分和重排序
- en: 'After generating candidate transcripts, Whisper rescores outputs using heavier
    processing for further gains. Secondary evaluation better incorporates richer
    context missed initially. To further optimize accuracy, Whisper applies additional
    techniques such as the following:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成候选转录后，Whisper通过更重的处理重新评分输出，以进一步提升效果。二次评估更好地融入了最初被忽略的更丰富上下文。为了进一步优化准确性，Whisper采用了以下额外技术：
- en: '*N*-best list re-ranking, which takes the top hypotheses and reorders them
    after evaluating with the more prominent language model rather than the fast approximator
    used during beam search. This boosts precision.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*-最佳列表重排序，它会在通过更强大的语言模型评估后重新排列最顶端的假设，而不是在束搜索中使用的快速近似器。这提高了精度。'
- en: LSTM rescoring, which goes beyond re-ranking; this technique involves feeding
    acoustic outputs into auxiliary LSTM networks, which act as alternative decoders.
    This captures different speech nuances missed by the baseline models.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM重新评分，它超越了重排序；这种技术涉及将声学输出输入到辅助LSTM网络，这些网络充当替代解码器。这捕捉了基线模型未能捕捉到的不同语音细微差别。
- en: Combining distinct search, scoring, and decoding strategies allows Whisper’s
    overall pipeline to correct itself – a hallmark of deep learning system design.
    No single method holds a monopoly on performance.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 结合不同的搜索、评分和解码策略，使Whisper的整体流程能够自我纠正——这是深度学习系统设计的一个标志。没有单一的方法能够垄断性能。
- en: Next, we’ll look at the final phase, which is focused on postprocessing these
    decoded results to prepare cleaned machine-readable text for downstream usage.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到最终阶段，重点是对这些解码结果进行后处理，以准备清洁的机器可读文本供下游使用。
- en: Postprocessing
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后处理
- en: After decoding audio into final text transcripts, Whisper applies various postprocessing
    approaches to further polish and structure outputs – the final step before surfacing
    recognized speech.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在将音频解码为最终文本转录后，Whisper应用各种后处理方法进一步润色和结构化输出——这是在呈现识别的语音之前的最后一步。
- en: Postprocessors handle formatting, accuracy optimization, entity linking, and
    more. This critical yet often overlooked pipeline stage completes the speech-to-language
    transition, transforming rough decoded text into consumable, actionable information.
    Let’s explore some of Whisper’s key postprocessing capabilities.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理器处理格式、准确度优化、实体链接等。这个关键但常常被忽视的处理阶段完成了语音到语言的转换，将粗略的解码文本转变为可消费的、可操作的信息。让我们一起探索Whisper的一些关键后处理能力。
- en: Text normalization
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本规范化
- en: 'First, the raw decoded text gets normalized into properly written language
    conventions. Text normalization is crucial in converting raw decoded text into
    a format that adheres to appropriate written language conventions. Here are examples
    of how text normalization is applied:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，原始解码文本被规范化为符合书面语言惯例的格式。文本规范化在将原始解码文本转换为符合适当书面语言惯例的格式时至关重要。以下是文本规范化应用的一些示例：
- en: '**Numerals are expanded into words**: For instance, the numeral “2023” in the
    text would be expanded to “two thousand twenty-three” to make it more readable
    and understandable when converted to speech.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数字被扩展为单词**：例如，文本中的数字“2023”将被扩展为“两千二十-three”，以便在转换为语音时更易于阅读和理解。'
- en: '**Disfluencies such as filler utterances are removed**: Disfluencies such as
    “um” or “uh” might be present in a speech transcription. These would be removed
    during text normalization to create a cleaner, more fluent written representation
    of the spoken content.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除流畅性障碍，如填充词**：在语音转录中可能会出现“嗯”或“啊”之类的填充词，这些在文本规范化过程中会被去除，从而创建出更清晰、更流畅的书面表达。'
- en: '**Punctuation and capitalization are standardized**: Text normalization ensures
    that punctuation marks are correctly placed and words are appropriately capitalized
    according to the rules of written language. For example, the beginning of sentences
    would be capitalized, and periods or commas would be added where necessary to
    reflect the natural pauses and ends of sentences.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标点和大小写得到标准化**：文本规范化确保标点符号正确放置，单词按照书面语言规则恰当地大写。例如，句首的单词会大写，句末会加上句号或逗号，以反映自然的停顿和句子结束。'
- en: By transforming literal transcripts that reflect actual spoken cadences into
    a format with a natural reading flow, text normalization preserves the meaning
    while enhancing readability and the overall user experience.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将反映实际口音的字面转录转换为具有自然阅读流畅度的格式，文本规范化在保持意义的同时，增强了可读性和整体用户体验。
- en: Accuracy optimization
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确度优化
- en: 'Next, to further boost integrity, detected errors get automatically corrected
    using auxiliary models trained to identify and fix common mistakes:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了进一步增强准确性，检测到的错误会通过辅助模型自动修正，这些模型经过训练以识别和修正常见错误：
- en: Homophones such as “*they’re*”/“*there*”/“*their*” are rectified.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同音词如“*they’re*”（他们）/“*there*”（那里）/“*their*”（他们的）会得到纠正。
- en: Redundancies such as “*the the*” are fixed.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似“*the the*”这样的冗余问题会被修复。
- en: Common substitutions are handled through confusion matrices (e.g., correcting
    frequent “*pat*”/“*bat*” mixups).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的替换通过混淆矩阵处理（例如，纠正频繁出现的“*pat*”（拍）/“*bat*”（蝙蝠）混淆）。
- en: Together with a final grammar check, precision refinement networks learn correction
    patterns from human-edited transcripts.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 与最终的语法检查一起，精准度精炼网络通过学习人类编辑过的转录中的修正模式来不断提高修正精度。
- en: Entity linking
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实体链接
- en: Whisper’s ability to understand speech goes beyond mere text output. It involves
    the crucial step of entity linking, which connects the decoded words to their
    real-world references. Grounding words in data is the key to unlocking contextual
    understanding.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的语音理解能力不仅限于文字输出。它涉及到一个至关重要的步骤——实体链接，将解码后的单词与现实世界中的对应实体连接起来。将单词与数据关联是解锁上下文理解的关键。
- en: When Whisper encounters a brand name in the speech, it doesn’t just transcribe
    the words; it maps them to canonical IDs in a knowledge base. This linking lets
    Whisper understand the brand’s context, products, and marketplace. Similarly,
    when a person is mentioned, Whisper employs facial recognition to identify the
    individual, linking the name to a rich information profile.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当Whisper在语音中遇到品牌名称时，它不仅仅转录这些单词；它还将这些单词映射到知识库中的标准ID。这种链接让Whisper理解品牌的背景、产品和市场。同样，当提到某个人时，Whisper会利用人脸识别技术识别该个体，并将名字与丰富的个人信息档案关联起来。
- en: Geographic references are another area where entity linking shines. By connecting
    location names to geographic databases, Whisper can understand the spatial context
    of the speech and associate the mentioned place with its coordinates, population,
    and other relevant data points.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 地理参考是实体链接展现优势的另一个领域。通过将地点名称与地理数据库连接，Whisper能够理解语音的空间上下文，并将提到的地点与其坐标、人口以及其他相关数据点关联起来。
- en: This grounding of words in data empowers Whisper to comprehend speech as a sequence
    of words and a network of interconnected concepts. It can draw upon the linked
    information to interpret the meaning and context of the speech more accurately.
    Entity linking is thus a critical component in Whisper’s ability to bridge the
    gap between raw speech and true understanding.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将词语与数据结合，Whisper能够将语音理解为一连串的单词和一个互联的概念网络。它可以利用这些关联信息更准确地解读语音的意义和上下文。因此，实体链接是Whisper弥合原始语音和真正理解之间差距的关键组件。
- en: Structured output
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化输出
- en: 'Finally, Whisper structures recognized content using semantic schemas tailored
    to target use cases:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Whisper使用针对目标用例量身定制的语义架构来构建已识别的内容：
- en: '**Annotating questions for a conversational response**: Suppose a user asks
    a voice assistant, “What’s the weather like today?” Whisper can annotate this
    input as a question, allowing the voice assistant to generate a conversational
    response such as, “The weather today is sunny with a high of 75 degrees.”'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注问题以生成对话回复**：假设用户向语音助手询问：“今天的天气怎么样？”Whisper可以将这个输入标注为问题，从而允许语音助手生成类似于“今天的天气晴，最高气温75华氏度”的对话式回答。'
- en: '**Flagging commands to trigger actions**: If a user says, “Set an alarm for
    7 AM,” Whisper can flag this as a command. This flag tells the voice assistant
    to set an alarm rather than transcribing the speech.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记命令以触发动作**：如果用户说“设置早上7点的闹钟”，Whisper可以将其标记为命令。这个标记告诉语音助手设置闹钟，而不是将语音转录。'
- en: '**Marking keywords for content analytics**: In a business meeting, a participant
    might say, “Our Q1 revenue exceeded expectations, but we need to improve our marketing
    strategy for Q2.” Whisper can mark “Q1 revenue,” “exceeded expectations,” and
    “improve marketing strategy for Q2” as keywords. These keywords can then be used
    for content analytics, helping the business to track important topics and trends
    in their meetings.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记关键词以进行内容分析**：在一次商务会议中，某个参与者可能会说：“我们的Q1收入超出预期，但我们需要在Q2改善市场营销策略。”Whisper可以将“Q1收入”、“超出预期”和“改善Q2市场营销策略”标记为关键词。这些关键词随后可用于内容分析，帮助企业追踪会议中的重要话题和趋势。'
- en: This output framing eases downstream consumption, indexation, and learning.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这种输出框架简化了下游的消费、索引和学习。
- en: Getting the last mile of postprocessing right prepares Whisper’s speech recognition
    for real-world application. The decoder transcribes audio signals. Postprocessors
    transcode those signals into usable, accessible language.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正确完成最后一公里的后处理，为Whisper的语音识别做好实际应用准备。解码器将音频信号转录，后处理器将这些信号转码为可用的、易访问的语言。
- en: And with that final postprocessing phase, we’ve now covered the whole gamut
    of Whisper’s speech recognition pipeline – from audio input handlers to acoustic
    classifiers, language models, decoders, and output refiners.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最后的后处理阶段，我们已经涵盖了Whisper语音识别管道的全貌——从音频输入处理器到声学分类器、语言模型、解码器和输出优化器。
- en: When woven together, these components ingest spoken natural language and systematically
    translate signals into precise text transcripts consumable by downstream applications.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些组件结合在一起时，它们将口语自然语言吸收并系统地将信号转换为下游应用程序可处理的精确文本记录。
- en: We’ve built strong intuitions around the data flow across the modules, giving
    Whisper unprecedented accuracy and speed at scale. Understanding these mechanics
    opens optimization pathways.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经围绕模块之间的数据流构建了强大的直觉，使Whisper在大规模应用中具有前所未有的准确性和速度。理解这些机制为优化路径的开辟提供了可能。
- en: Now equipped with architectural knowledge, we’re ready to shift focus toward
    tailored configuration, troubleshooting, and advancement of Whisper implementations
    based on infrastructure constraints and use case targets.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，具备了架构知识，我们准备将重点转向针对基础设施限制和用例目标的Whisper实施的定制配置、故障排除和进展。
- en: Our next section will cover specialized guidelines around rightsizing and accelerating
    Whisper for your success scenario while upholding accuracy, availability, and
    efficiency standards.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将介绍有关权衡和加速Whisper以适应成功场景的专门指南，同时保持准确性、可用性和效率标准。
- en: Applying best practices for performance optimization
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用最佳实践进行性能优化
- en: Now equipped with a solid grasp of Whisper’s architecture and data flows, we’re
    ready to shift focus toward real-world deployment. This pivotal section distills
    fundamental guidelines, trade-offs, and operational wisdom, accelerating production
    success.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，凭借对 Whisper 架构和数据流的扎实理解，我们准备将重点转向实际部署。本节内容提炼了基础性指南、权衡和操作智慧，加速生产成功。
- en: We’ll cover specialized topics beyond basic setup – from strategically provisioning
    infrastructure to monitoring metrics, integrating downstream NLP, tuning configurations,
    and troubleshooting common incidents. Consider this your handbook for effectively
    scaling Whisper-based solutions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖超越基本设置的专题——从战略性配置基础设施到监控指标、集成下游 NLP、调整配置和排除常见故障。将其视为有效扩展 Whisper 基础解决方案的手册。
- en: 'While fundamentals provide strong bases, intricacies of the environment determine
    outcomes. By tailoring and streamlining system-wide stack configurations to your
    context, we unlock next-level reliability, efficiency, and **return on investment**
    (**ROI**). Specifically, this involves steps such as the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基础设施提供了坚实的基础，但环境的复杂性决定了最终结果。通过根据您的上下文量身定制和简化系统范围的堆栈配置，我们能够解锁更高层次的可靠性、效率和**投资回报**（**ROI**）。具体而言，这包括以下步骤：
- en: Optimally allocating cloud, edge, or on-device compute
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化分配云端、边缘或设备端的计算资源
- en: Balancing data pipelines without congestion
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不发生拥塞的情况下平衡数据管道
- en: Tuning accuracy and latency for use case needs
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整精度和延迟以满足用例需求
- en: Smoothly interoperating with adjacent workflows
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与相邻工作流程平滑互操作
- en: Rapidly addressing anomalies
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速处理异常情况
- en: Let’s prepare implementations for the demands of real-world conditions!
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为现实条件的需求准备实施方案！
- en: Understanding compute requirements
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解计算需求
- en: Compute provisioning proves foundational when deploying performant Whisper-powered
    applications. The allocated CPU, GPU, memory, and storage resources directly impact
    throughput, latency, and concurrency.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署高性能 Whisper 驱动的应用程序时，计算资源的配置至关重要。分配的 CPU、GPU、内存和存储资源直接影响吞吐量、延迟和并发性。
- en: Unfortunately, Whisper’s scale leads many to underestimate its production infrastructure
    needs. At over 500 million parameters, the model requires significant hardware
    acceleration to deliver real-time speech recognition across users.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Whisper 的规模使得许多人低估了其生产基础设施的需求。拥有超过 5 亿个参数，该模型需要大量硬件加速，才能在用户间提供实时语音识别。
- en: This section provides compute guidelines for streamlining Whisper deployments.
    We’ll demystify its architecture considerations from on-device endpoints to cloud-accelerated
    requests. Target the optimal infrastructure fit.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容提供了优化 Whisper 部署的计算指南。我们将揭示其架构的考虑因素，从设备端点到云加速请求，目标是找到最佳的基础设施匹配。
- en: 'Whisper’s core workload involves matrix multiplications during neural network
    inferencing. These operations stress different underlying hardware components
    such as the following:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的核心工作负载包括神经网络推理中的矩阵乘法。这些操作对不同的硬件组件产生压力，具体包括：
- en: '**GPUs** accelerate deep learning matrix calculations in parallel, handling
    hundreds of operations simultaneously – their thousands of cores suit ML numerical
    processing. For Whisper, GPUs drive faster acoustic model inferencing.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU** 加速深度学习矩阵计算并行处理，能够同时处理数百个操作——其成千上万的核心非常适合机器学习的数值处理。对于 Whisper 来说，GPU
    驱动着更快的声学模型推理。'
- en: '**CPUs** also Cprovide parallelization but are optimized for general-purpose
    branching logic over specialized math. Whisper relies on CPUs for audio decoding,
    beam search, and language model computations.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU** 也提供并行化能力，但它们更适合于通用分支逻辑的处理，而非专用数学运算。Whisper 依赖 CPU 进行音频解码、束搜索和语言模型计算。'
- en: '**Memory** fuels model parameters and audio inputs. Whisper demands GBs for
    state storage and data transfers between processing units. High bandwidth reduces
    transfer bottlenecks.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**为模型参数和音频输入提供支持。Whisper 需要 GB 级的存储空间和在处理单元之间进行数据传输。高带宽有助于减少传输瓶颈。'
- en: '**Storage** holds pre-trained weights and buffers prediction outputs. High
    throughput *NVMe* SSDs manage heavy read/write Whisper workloads.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**用于保存预训练的权重和缓冲预测输出。高吞吐量的*NVMe* SSD 处理着重的读写 Whisper 工作负载。'
- en: The complementary notebook `LOAIW_ch02_exploring_audio_data_workflows.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb))
    provides further details on compute considerations for Whisper, including architecture
    trade-offs for on-device, edge, and cloud deployment.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 补充笔记本 `LOAIW_ch02_exploring_audio_data_workflows.ipynb` （[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb)）提供了有关
    Whisper 的计算考虑事项的更多细节，包括本地设备、边缘和云部署的架构权衡。
- en: Balancing these resources prevents systemic bottlenecks that slow down performance.
    Carefully consider the entire stack.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡这些资源可以防止系统瓶颈，从而提高性能。要仔细考虑整个技术栈。
- en: Considering these resource demands and hardware capabilities, we now explore
    specialized optimization guidelines tailored to distinct endpoint targets.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些资源需求和硬件能力，我们现在探讨针对不同终端目标的专业化优化指南。
- en: Optimizing the deployment targets
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化部署目标
- en: Optimizing the deployment of OpenAI’s Whisper model across various environments
    requires a strategic approach that considers each target platform’s unique demands.
    Here’s an intermediate-level explanation of best practices for deploying Whisper
    in different contexts.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同环境中优化 OpenAI 的 Whisper 模型部署需要一种战略性的方法，考虑到每个目标平台的独特需求。以下是关于在不同环境中部署 Whisper
    的中级最佳实践说明。
- en: On-device deployment (phones and IoT devices)
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设备本地部署（手机和物联网设备）
- en: For deployment on mobile phones and IoT devices, the focus should be on efficiency
    due to the limited computational resources. Whisper models should be selected
    based on the balance between size and accuracy. The quantized 40 MB model on-device
    Whisper inference on Android mobile using `whisper.tflite` ([https://github.com/openai/whisper/discussions/506](https://github.com/openai/whisper/discussions/506))
    is an example of a lightweight model suitable for such devices. Leveraging platform-specific
    neural accelerators, such as Apple’s Neural Engine or Google’s Edge tensor processing
    unit (TPU), is crucial for maximizing performance. Memory management is also critical,
    as these devices have limited RAM, so developers must ensure that the Whisper
    model does not exhaust available memory.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动电话和物联网设备上的部署，重点应放在效率上，因为这些设备的计算资源有限。Whisper 模型的选择应基于大小与精度之间的平衡。使用 `whisper.tflite`
    在 Android 移动设备上进行的 40 MB 量化模型推理（[https://github.com/openai/whisper/discussions/506](https://github.com/openai/whisper/discussions/506)）是一个适合此类设备的轻量级模型示例。利用平台特定的神经加速器，如
    Apple 的神经引擎或 Google 的边缘张量处理单元（TPU），对提升性能至关重要。内存管理同样至关重要，因为这些设备的 RAM 限制，因此开发人员必须确保
    Whisper 模型不会耗尽可用内存。
- en: Edge server deployment
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘服务器部署
- en: 'Edge servers are private nodes that can offer model containment while providing
    scalability akin to cloud infrastructure. For edge deployment, it’s advisable
    to use high-core CPUs and deep memory buffers to handle the computational load.
    Load-balanced GPUs can accelerate inference tasks, and low-latency storage solutions
    such as NVMe SSD clusters can improve the system’s overall responsiveness. *Whispering*
    (*Whispering: Joint Service Offloading and Computation Reuse in Cloud-Edge Networks*
    - [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8528222/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8528222/))
    involves computation reuse at the edge, which can significantly reduce task completion
    times by avoiding redundant computations.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘服务器是私有节点，能够在提供类似云基础设施的可扩展性的同时提供模型容器化。对于边缘部署，建议使用多核 CPU 和深度内存缓冲区来处理计算负载。负载均衡的
    GPU 可以加速推理任务，低延迟的存储解决方案如 NVMe SSD 集群可以提升系统的整体响应速度。*Whispering*（*Whispering: Joint
    Service Offloading and Computation Reuse in Cloud-Edge Networks* - [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8528222/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8528222/)）涉及在边缘进行计算重用，通过避免冗余计算显著减少任务完成时间。'
- en: Cloud infrastructure deployment
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云基础设施部署
- en: In a cloud environment, virtual machines offer the flexibility of scaling resources
    as needed. For Whisper, selecting machine images optimized for machine learning,
    such as AWS’s *inf1* instances equipped with *Inferentia* chips, is beneficial,
    as they can provide cost-effective, high-performance inference. Autoscaling groups
    are essential for managing variability in demand, ensuring that resources are
    scaled up during peak times and scaled down when demand wanes to control costs.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在云环境中，虚拟机提供了按需扩展资源的灵活性。对于 Whisper，选择为机器学习优化的机器镜像（例如 AWS 的 *inf1* 实例，配备 *Inferentia*
    芯片）是有益的，因为它们能提供具有成本效益的高性能推理。自动扩展组对于管理需求波动至关重要，确保在需求高峰时扩展资源，在需求下降时缩减资源，从而控制成本。
- en: General considerations
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般考虑事项
- en: Regardless of the deployment environment, analyzing traffic, performance benchmarks,
    and budgets is essential for planning effectively. Overprovisioning leads to unnecessary
    expenses, while underprovisioning can degrade service quality. The balance between
    cost and performance is crucial. Monitoring tools and performance metrics should
    be in place to ensure that the deployment meets the required service levels and
    to facilitate scaling decisions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 无论部署环境如何，分析流量、性能基准和预算对于有效规划至关重要。过度配置会导致不必要的开支，而配置不足则可能影响服务质量。成本和性能之间的平衡至关重要。应配备监控工具和性能指标，以确保部署满足所需的服务水平，并便于做出扩展决策。
- en: In summary, the best practices for deploying Whisper across different environments
    involve selecting the right model size, leveraging specialized hardware accelerators,
    managing memory efficiently, and using cloud resources judiciously. It’s also
    important to consider the trade-offs between latency, cost, and performance metrics
    to ensure an optimized deployment that meets each environment’s specific needs.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在不同环境中部署 Whisper 的最佳实践包括选择合适的模型大小、利用专用硬件加速器、有效管理内存，并合理使用云资源。同时，还需要考虑延迟、成本和性能指标之间的权衡，以确保优化部署，满足各环境的特定需求。
- en: With the established infrastructure, we’ll explore specialized practices for
    effectively routing the torrents of data coursing through Whisper’s pipelines
    during inference.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立好基础设施后，我们将探讨有效地路由通过 Whisper 推理管道流动的数据流量的专业实践。
- en: Managing data flows
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理数据流
- en: Managing data flows in AI systems, particularly in the context of OpenAI’s Whisper,
    involves several best practices that ensure efficient and effective operation.
    These practices are crucial for handling the intricate queues, caches, buffers,
    micro-batches, parallel streams, and competitive resource scheduling that constitute
    the nervous system of AI data movement.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AI 系统中，尤其是在 OpenAI 的 Whisper 中，管理数据流涉及一些最佳实践，这些实践确保了系统的高效运行。它们对于处理复杂的队列、缓存、缓冲区、微批次、并行流和竞争资源调度至关重要，这些构成了
    AI 数据流动的神经系统。
- en: Let’s explore various techniques for optimizing these AI data flows, including
    strategic coordination, routing, and scaling.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索优化这些 AI 数据流的各种技术，包括战略协调、路由和扩展。
- en: Understanding the fundamental data types and flows
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解基本数据类型和数据流动
- en: 'Whisper’s core data transformations involve several key data types:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的核心数据转换涉及几种关键数据类型：
- en: '**Audio streams**: These are segmented into chunks from recording devices,
    with metadata attached for tracing across asynchronous stages.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频流**：这些音频流从录音设备中分段，并附带元数据用于在异步阶段之间追踪。'
- en: '**Features**: These encode audio frequencies and temporal qualities into numeric
    matrices.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：这些特征将音频频率和时间特性编码为数字矩阵。'
- en: '**Labels**: These attach interim phonetic and lexical representations during
    acoustic model inferencing.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：在声学模型推理过程中，这些标签会附加临时的语音和词汇表示。'
- en: '**Transcripts**: These constitute the final text outputs containing the decoded
    speech.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转录文本**：这些构成最终的文本输出，包含解码后的语音内容。'
- en: Understanding these data types allows for strategic optimization, such as prioritized
    routing and tailored storage for each type.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些数据类型可以帮助我们进行战略优化，例如为每种类型的数据进行优先路由和定制存储。
- en: Coordinating shared data stores
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协调共享数据存储
- en: 'Shared access to data in parallel movements requires careful coordination.
    Whisper leverages several tools for this:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 共享数据访问的并行处理需要精心协调。Whisper 使用了多个工具来实现这一点：
- en: '**Message queues**: These buffer and asynchronously process audio segments
    and transcripts across systems.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息队列**：这些队列缓冲并异步处理跨系统的音频片段和转录文本。'
- en: '**NoSQL stores**: These provide a low-latency lookup of large feature sets
    and audio batch metadata.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NoSQL存储**：这些提供对大型特征集和音频批处理元数据的低延迟查找。'
- en: '**In-memory data grids**: These cache expensive model outputs such as label
    sequences for fast reuse.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存数据网格**：这些缓存高开销的模型输出，如标签序列，以便快速重用。'
- en: Strategically routing data
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 战略性地路由数据
- en: 'Minimizing data transfers and replication is crucial for efficient operation.
    Whisper optimizes data flows through several strategies:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化数据传输和复制对于高效运行至关重要。Whisper通过几种策略优化数据流：
- en: '**Locality processing**: This involves processing operations within modules,
    such as language model rescoring, with the aim of constraining excessive movement
    or computation.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部处理**：这涉及在模块内处理操作，如语言模型重新评分，目的是限制过度的移动或计算。'
- en: '**Compression**: This reduces transferred bytes through encodings.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩**：这通过编码减少了传输的字节数。'
- en: '**Priority**: This allows fast-tracking audio segments over batch feature sets.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先级**：这允许在批量特征集之外优先处理音频片段。'
- en: '**Caching**: This facilitates reusing stored artifacts such as filter banks
    when possible.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存**：这有助于在可能的情况下重用存储的工件，例如滤波器组。'
- en: Next, refer to the complementary notebook. There you will find code samples
    for loading, visualizing, and processing of audio data with Python libraries such
    as `librosa`. The notebook covers audio data workflows relevant to ingestion in
    Whisper pipelines ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb)).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，参考补充笔记本。你将在那里找到使用Python库（如`librosa`）加载、可视化和处理音频数据的代码示例。该笔记本涵盖了与Whisper管道中数据摄取相关的音频数据工作流程（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter02/LOAIW_ch02_exploring_audio_data_workflows.ipynb)）。
- en: Scaling horizontally
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 横向扩展
- en: Strategic data flow balancing is crucial when demand surges, such as when a
    smart home leverages Whisper for multi-user voice control across rooms. This can
    involve caching audio features extracted by the acoustic model to avoid redundant
    computing, compressing data to reduce network bandwidth strain, prioritizing audio
    chunks from active speakers, and dynamically providing extra downstream containers
    to spread the load and prevent bottlenecks.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在需求激增时，战略性的数据流平衡至关重要，比如智能家居利用Whisper实现跨房间的多用户语音控制。这可能涉及缓存由声学模型提取的音频特征以避免冗余计算、压缩数据以减少网络带宽压力、优先处理来自活跃发言者的音频片段，并动态提供额外的下游容器来分担负载，防止瓶颈。
- en: In summary, managing data flows in AI systems such as Whisper involves understanding
    the fundamental data types and flows, coordinating shared data stores, strategically
    routing data, and scaling horizontally when necessary. These practices help to
    address scalability, stability, and efficiency challenges, ensuring that AI systems
    can deliver high-quality, real-time interaction speed for acceptable consumer
    experiences despite volatile user patterns.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，管理像Whisper这样的AI系统中的数据流，涉及理解基本的数据类型和流向，协调共享的数据存储，战略性地路由数据，并在必要时进行横向扩展。这些实践有助于解决可扩展性、稳定性和效率问题，确保AI系统能够在用户行为波动的情况下，提供高质量、实时的交互速度，从而为消费者提供可接受的体验。
- en: Next, we’ll explore monitoring critical channels and infrastructure for smooth
    operations.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索监控关键通道和基础设施，以确保平稳操作。
- en: Monitoring metrics and optimization
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控指标与优化
- en: Monitoring metrics and optimization are crucial for managing and improving OpenAI’s
    Whisper’s performance. This process involves tracking **key performance indicators**
    (**KPIs**) across various domains, including model performance, hardware utilization,
    and data flow health.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 监控指标与优化对于管理和改进OpenAI Whisper的性能至关重要。该过程涉及跟踪各个领域的**关键性能指标**（**KPI**），包括模型性能、硬件利用率和数据流健康。
- en: Model performance metrics
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型性能指标
- en: 'Model performance metrics ensure that the Whisper system accurately transcribes
    speech. These metrics include the following:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能指标确保Whisper系统准确转录语音。以下是这些指标：
- en: '**WER**: This is the primary benchmark for ASR systems. It quantifies the number
    of word mistakes by comparing the system’s output to the ground truth transcripts.
    A lower WER indicates better performance.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WER**：这是自动语音识别（ASR）系统的主要基准。它通过将系统输出与标准转录进行比较，量化单词错误的数量。较低的WER表示更好的性能。'
- en: '**Character error rate** (**CER**): This is more granular than WER and is particularly
    useful in contexts that require high precision, such as clinical or technical
    settings.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符错误率**（**CER**）：这一指标比WER更为细致，尤其在需要高精度的环境下（如临床或技术场景）非常有用。'
- en: '**Latency**: This refers to the time delay between the speech input and the
    final output. Monitoring latency at various process stages, such as during acoustic
    modeling and the entire pipeline, is essential.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟**：这是指语音输入到最终输出之间的时间延迟。在各个处理阶段，如声学建模和整个管道中，监控延迟是至关重要的。'
- en: '**Throughput**: This measures the number of transcripts processed per unit
    of time. It provides insights into scaling needs against request volumes and concurrent
    sessions.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：该指标衡量每单位时间处理的转录数量。它提供了关于请求量和并发会话的扩展需求的见解。'
- en: Monitoring infrastructure health
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控基础设施健康
- en: 'Monitoring the health of the underlying hardware is also crucial for maintaining
    the performance of the Whisper system. Key metrics in this domain include the
    following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 监控底层硬件的健康状况对保持Whisper系统的性能至关重要。该领域的关键指标包括：
- en: '**GPU/CPU utilization**: Monitoring GPU and CPU utilization can help identify
    saturated accelerators and balance the load across underutilized resources.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU/CPU利用率**：监控GPU和CPU的利用率可以帮助识别过载的加速器，并平衡负载，避免资源闲置。'
- en: '**RAM utilization**: Monitoring RAM utilization can help prevent exceeding
    limits that slow processing as memory swaps to disk.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAM利用率**：监控RAM利用率有助于防止超出限制，避免内存交换到磁盘导致处理速度变慢。'
- en: '**Bandwidth/throughput**: Monitoring network capacity can help identify whether
    data transfer is slowing down, indicating a need for network upgrades.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带宽/吞吐量**：监控网络容量可以帮助识别数据传输是否变慢，提示是否需要进行网络升级。'
- en: '**Storage latency**: Spikes in storage latency can indicate struggling disks
    that cannot feed data to models quickly enough.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储延迟**：存储延迟的峰值可能表明硬盘出现问题，无法快速将数据传输给模型。'
- en: Data pipeline analytics
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据管道分析
- en: 'Optimizing data flows between components is another critical aspect of Whisper
    optimization. Key metrics in this domain include the following:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 优化组件之间的数据流动是Whisper优化的另一个关键方面。该领域的关键指标包括：
- en: '**Queue depth**: A rising backlog can signal that downstream components struggle
    to keep up, indicating a need to address bottlenecks.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**队列深度**：队列积压增加可能表明下游组件跟不上进度，提示需要解决瓶颈问题。'
- en: '**Cache hit rate**: A lower-than-expected cache hit rate can indicate ineffective
    caching, which slows down data reuse.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存命中率**：低于预期的缓存命中率可能表明缓存效果不佳，从而导致数据重用变慢。'
- en: '**Data freshness**: This metric quantifies the latency for audio segments traversing
    multistage pipelines.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据新鲜度**：该指标量化了音频片段在多阶段管道中传输的延迟。'
- en: '**Errors**: Tracking pipeline failures and everyday recovery events can provide
    insights into the system’s fragility.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误**：跟踪管道故障和日常恢复事件可以提供系统脆弱性的见解。'
- en: We can industrialize models and unlock potential advanced capabilities by carefully
    monitoring these metrics. For example, a customer support chatbot that relies
    on Whisper to transcribe customer inquiries can maintain customer satisfaction
    during peak traffic by proactively addressing signals such as high GPU utilization,
    increased WER, slow data freshness, and low cache hit rates.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过仔细监控这些指标来实现模型的工业化，并释放潜在的先进能力。例如，依赖Whisper进行客户咨询转录的客户支持聊天机器人，可以通过主动监控如高GPU利用率、WER增加、数据新鲜度变慢以及低缓存命中率等信号，在流量高峰期间保持客户满意度。
- en: In conclusion, monitoring and optimization ensure the Whisper system’s performance
    and reliability. By tracking key metrics across model performance, hardware utilization,
    and data flow health, it’s possible to identify bottlenecks, make necessary adjustments,
    and improve the system’s performance and efficiency. Without monitoring and providing
    actionable insights around infrastructure and model metrics, upholding customer
    quality-of-service through data assets such as Whisper becomes impossible. Measurement
    enables progress.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 总结，监控和优化是确保Whisper系统性能和可靠性的关键。通过跟踪模型性能、硬件利用率和数据流健康的关键指标，可以识别瓶颈，进行必要的调整，并提高系统的性能和效率。如果没有对基础设施和模型指标进行监控并提供可操作的洞察，依靠数据资产如Whisper来维持客户服务质量将变得不可能。衡量是进步的关键。
- en: Summary
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we peeled back the layers shrouding Whisper’s exceptional speech
    recognition capabilities. Now that you are informed of internal processes from
    audio ingestion to language decoding, you can strategically fine-tune implementations
    for particular use case needs.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们揭开了 Whisper 卓越语音识别能力的面纱。现在，你已经了解了从音频输入到语言解码的内部过程，你可以战略性地为特定用例需求微调实现方案。
- en: We surveyed the technical landscape before exploring Whisper’s hybridized design,
    melding end-to-end optimization with modular customizability. You grasped CTC
    acoustic model handling of fuzzy sound alignments alongside transformer integration,
    which provides robust language representations.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在探索 Whisper 的混合设计之前，先调查了技术领域，融合了端到端优化与模块化定制化。你已经掌握了 CTC 声学模型处理模糊声音对齐以及变换器集成，这为语言表示提供了强大的支持。
- en: These building blocks enable the unlocking of performance gains, availability,
    and cost efficiencies through metrics monitoring, parameter tuning, de-bottlenecking,
    and more. In the future, accuracy improvements can be achieved through retraining
    processes that embed insights into model weights, thereby institutionalizing learning
    and refinement.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这些构件通过指标监控、参数调优、瓶颈解除等方式，解锁了性能提升、可用性和成本效益。在未来，精度提升可以通过再训练过程实现，将洞察融入模型权重，从而将学习和精炼制度化。
- en: Equipped with architectural comprehension, you can confidently sculpt deployments
    catering to latency constraints, precision thresholds, infrastructure realities,
    and budget limitations. Understanding the data flows and trade-offs breeds an
    informed strategy that optimizes business outcomes.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 具备架构理解后，你可以自信地雕刻部署方案，满足延迟约束、精度门限、基础设施现实和预算限制。理解数据流和权衡带来一个信息丰富的战略，从而优化商业成果。
- en: We’re now ready to advance technical mastery having achieved functional literacy.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了功能性素养后，我们现在准备提升技术精通度。
- en: In the next chapter, we’ll dig deeper, navigating the nuances within Whisper’s
    neural architecture, multitasking strategies, and weakly supervised training methodology,
    which fuels outstanding performance across languages and environments.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨 Whisper 的神经架构、多任务策略和弱监督训练方法，这些都推动了它在不同语言和环境中的卓越表现。
- en: We’ll dissect transformer mechanics for sequential data while explaining encoder-decoder
    attention patterns that learn linguistic relationships. We’ll also grasp techniques
    for handling language variation – whether vocabulary, pronunciation, dialect,
    or task. Finally, we’ll demystify how limited labeled data can steer sizable models
    via clever pre-training objectives.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将剖析变换器在处理顺序数据中的机制，同时解释学习语言关系的编码器-解码器注意力模式。我们还将掌握处理语言变异的技巧——无论是词汇、发音、方言还是任务。最后，我们将揭示如何通过巧妙的预训练目标，让有限的标注数据驱动大型模型。
- en: These advanced insights expand the possibilities of interoperating Whisper within
    innovative downstream applications – from multilingual customer support bots to
    fused video/speech analytics. Comprehension breeds creative integration.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这些先进的洞察扩展了 Whisper 在创新下游应用中的互操作可能性——从多语言客户支持机器人到融合的视频/语音分析。理解促使创造性整合。
- en: Let’s now level up with a closer examination of internal modeling techniques!
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过更深入地研究内部建模技术来提升我们的技术水平！
- en: 'Part 2: Underlying Architecture'
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：底层架构
- en: In this part, you will explore the technical backbone of OpenAI’s Whisper, exploring
    its architecture and the transformer model that drives its cutting-edge ASR capabilities.
    You will understand Whisper’s inner workings comprehensively, including its encoder-decoder
    mechanics, multitasking and multilingual capabilities, and training techniques
    using weak supervision on large-scale data. Additionally, you will learn how to
    fine-tune Whisper for specific domain and language needs, enabling you to customize
    and integrate it effectively into various applications.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将探索 OpenAI Whisper 的技术支柱，研究它的架构以及推动其前沿语音识别能力的变换器模型。你将全面理解 Whisper 的内部运作，包括它的编码器-解码器机制、多任务和多语言能力，以及使用弱监督对大规模数据进行训练的技巧。此外，你将学习如何为特定领域和语言需求微调
    Whisper，使其能够有效地定制并集成到各种应用中。
- en: 'This part includes the following chapters:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 3*](B21020_03.xhtml#_idTextAnchor088), *Diving into the Whisper Architecture*'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B21020_03.xhtml#_idTextAnchor088)，*深入探索 Whisper 架构*'
- en: '[*Chapter 4*](B21020_04.xhtml#_idTextAnchor113)*, Fine-Tuning Whisper for Domain
    and Language Specificity*'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B21020_04.xhtml#_idTextAnchor113)*，针对领域和语言特定性的微调 Whisper*'
