- en: <st c="0">10</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="0">10</st>
- en: <st c="3">Key RAG Components in LangChain</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="3">LangChain中的关键RAG组件</st>
- en: <st c="35">This chapter takes an in-depth look at the key technical components
    that we have been talking about as they relate to</st> **<st c="154">LangChain</st>**
    <st c="163">and</st> **<st c="168">retrieval-augmented generation</st>** <st c="198">(</st>**<st
    c="200">RAG)</st>**<st c="204">. As a refresher, the key technical components
    of our RAG system, in order of how they are used, are</st> **<st c="305">vector
    stores</st>**<st c="318">,</st> **<st c="320">retrievers</st>**<st c="330">, and</st>
    **<st c="336">large language models</st>** <st c="357">(</st>**<st c="359">LLMs</st>**<st
    c="363">).</st> <st c="367">We will step through the latest version of our code,
    last seen in</st> [*<st c="433">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="442">,</st> *<st c="444">Code lab 8.3</st>*<st c="456">. We will focus on each
    of these core components, and we will show the various options for each component
    using LangChain in the code.</st> <st c="591">Naturally, a lot of this discussion
    will highlight differences among each option and discuss the different scenarios
    in which one option might be better</st> <st c="744">over another.</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35">本章深入探讨了与**<st c="154">LangChain</st>** <st c="163">和**<st c="168">检索增强生成</st>**
    <st c="198">(**<st c="200">RAG</st>**)相关的关键技术组件。作为复习，我们RAG系统的关键技术组件，按照使用顺序排列，包括**<st
    c="305">向量存储</st>**<st c="318">、**<st c="320">检索器</st>**<st c="330">和**<st c="336">大型语言模型</st>**
    <st c="357">(**<st c="359">LLMs</st>**)。我们将逐步介绍我们代码的最新版本，这是在**[*<st c="433">第8章</st>**](B22475_08.xhtml#_idTextAnchor152)中最后看到的，*<st
    c="444">代码实验室8.3</st>*<st c="456">。我们将关注每个核心组件，并使用LangChain在代码中展示每个组件的各种选项。*<st
    c="591">自然地，这次讨论将突出每个选项之间的差异，并讨论在不同场景下某个选项可能比另一个选项更好的情况。</st>
- en: <st c="757">We start with a code lab outlining options for your</st> <st c="810">vector
    store.</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="757">我们从一份代码实验室开始，概述了您的</st> <st c="810">向量存储选项。</st>
- en: <st c="823">Technical requirements</st>
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="823">技术要求</st>
- en: <st c="846">The code for this chapter is placed in the following GitHub</st>
    <st c="907">repository:</st> [<st c="919">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="846">本章的代码放置在以下GitHub</st> <st c="907">仓库中：</st> [<st c="919">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10)
- en: <st c="1016">Code lab 10.1 – LangChain vector store</st>
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1016">代码实验室10.1 – LangChain向量存储</st>
- en: <st c="1055">The goal for all these code</st> <st c="1084">labs is to help you
    become more familiar with how the</st> <st c="1137">options for each key component
    offered within the LangChain platform can enhance your RAG system.</st> <st c="1236">We
    will dive deep into what each component does, available functions, parameters
    that make a difference, and ultimately, all of the options you can take advantage
    of for a better RAG implementation.</st> <st c="1435">Starting with</st> *<st
    c="1449">Code lab 8.3</st>*<st c="1461">, (skipping</st> [*<st c="1473">Chapter
    9</st>*](B22475_09.xhtml#_idTextAnchor184)<st c="1482">’s evaluation code), we
    will step through these elements in order of how they appear in code, starting
    with the vector stores.</st> <st c="1610">You can find this code in its entirety
    in the</st> [*<st c="1656">Chapter 10</st>*](B22475_10.xhtml#_idTextAnchor218)
    <st c="1666">code folder on GitHub also labeled</st> <st c="1702">as</st> `<st
    c="1705">10.1</st>`<st c="1709">.</st>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1055">所有这些代码</st> <st c="1084">实验室的目标是帮助您更熟悉LangChain平台内提供的每个关键组件选项如何增强您的RAG系统。</st>
    <st c="1236">我们将深入研究每个组件的功能、可用函数、影响参数，以及最终，您可以利用的所有更好的RAG实现选项。</st> <st c="1435">从**<st
    c="1449">代码实验室8.3</st>**<st c="1461">开始，（跳过**[*<st c="1473">第9章</st>**](B22475_09.xhtml#_idTextAnchor184)的评估代码），我们将按照代码中出现的顺序逐步介绍这些元素，从向量存储开始。</st>
    <st c="1610">您可以在GitHub上找到完整的代码，在**[*<st c="1656">第10章</st>**](B22475_10.xhtml#_idTextAnchor218)的代码文件夹中，也标记为**<st
    c="1702">10.1</st>`<st c="1709">。</st>
- en: <st c="1710">Vector stores, LangChain, and RAG</st>
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="1710">向量存储、LangChain和RAG</st>
- en: '**<st c="1744">Vector stores</st>** <st c="1758">play</st> <st c="1764">a crucial
    role in RAG systems by efficiently storing and</st> <st c="1821">indexing vector
    representations of the knowledge base documents.</st> <st c="1886">LangChain</st>
    <st c="1895">provides seamless</st> <st c="1914">integration with various vector
    store</st> <st c="1952">implementations, such as</st> **<st c="1977">Chroma</st>**<st
    c="1983">,</st> **<st c="1985">Weaviate</st>**<st c="1993">,</st> **<st c="1995">FAISS</st>**
    <st c="2000">(</st>**<st c="2002">Facebook AI Similarity Search</st>**<st c="2031">),</st>
    **<st c="2035">pgvector</st>**<st c="2043">, and</st> **<st c="2049">Pinecone</st>**<st
    c="2057">. For this</st> <st c="2068">code lab, we will show the code for adding</st>
    <st c="2110">your data to Chroma, Weaviate, and FAISS, laying the groundwork for
    you to be able to integrate any vector store among the many that LangChain offers.</st>
    <st c="2262">These vector stores offer high-performance similarity search capabilities,
    enabling fast retrieval of relevant documents based on the</st> <st c="2396">query
    vector.</st>'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="1744">向量存储</st>** <st c="1758">在RAG系统中起着至关重要的作用，通过高效地存储和</st> <st
    c="1821">索引知识库文档的向量表示。</st> <st c="1886">LangChain</st> <st c="1895">提供了与各种向量存储</st>
    <st c="1914">实现的无缝集成，例如</st> **<st c="1977">Chroma</st>**<st c="1983">,</st> **<st
    c="1985">Weaviate</st>**<st c="1993">,</st> **<st c="1995">FAISS</st>** <st c="2000">(</st>**<st
    c="2002">Facebook AI Similarity Search</st>**<st c="2031">),</st> **<st c="2035">pgvector</st>**<st
    c="2043">, 和</st> **<st c="2049">Pinecone</st>**<st c="2057">。对于这个</st> <st c="2068">代码实验室，我们将展示如何将数据添加到Chroma、Weaviate和FAISS中，为你能够集成LangChain提供的众多向量存储中的任何一个打下基础。</st>
    <st c="2262">这些向量存储提供了高性能的相似度搜索功能，能够根据</st> <st c="2396">查询向量快速检索相关文档。</st>'
- en: <st c="2409">LangChain’s vector store class serves as a unified interface for
    interacting with different vector store backends.</st> <st c="2525">It provides
    methods for adding documents to the vector store, performing similarity searches,
    and retrieving the stored documents.</st> <st c="2656">This abstraction allows
    developers to easily switch between vector store implementations without modifying
    the core</st> <st c="2772">retrieval logic.</st>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2409">LangChain的向量存储类作为与不同向量存储后端交互的统一接口。</st> <st c="2525">它提供了向向量存储添加文档、执行相似度搜索和检索存储文档的方法。</st>
    <st c="2656">这种抽象允许开发者轻松地在不同的向量存储实现之间切换，而无需修改核心</st> <st c="2772">检索逻辑。</st>
- en: <st c="2788">When building a RAG system with LangChain, you can leverage the
    vector store class to efficiently store and retrieve document vectors.</st> <st
    c="2924">The choice of vector store depends on factors such as scalability, search
    performance, and deployment requirements.</st> <st c="3040">Pinecone, for example,
    offers a fully managed vector database with high scalability and real-time search
    capabilities, making it suitable for production-grade RAG systems.</st> <st c="3212">On
    the other hand, FAISS provides an open source library for efficient similarity
    search, which can be used for local development and experimentation.</st> <st
    c="3363">Chroma is a popular place for developers to start when building their
    first RAG pipelines due to its ease of use and its effective integration</st>
    <st c="3506">with LangChain.</st>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2788">当使用LangChain构建RAG系统时，你可以利用向量存储类来高效地存储和检索文档向量。</st> <st c="2924">向量存储的选择取决于可扩展性、搜索性能和部署需求等因素。</st>
    <st c="3040">例如，Pinecone提供了一种具有高可扩展性和实时搜索能力的完全托管向量数据库，使其适用于生产级RAG系统。</st> <st
    c="3212">另一方面，FAISS提供了一个用于高效相似度搜索的开源库，可用于本地开发和实验。</st> <st c="3363">Chroma因其易用性和与LangChain的有效集成而成为开发者构建第一个RAG管道时的热门选择。</st>
- en: <st c="3521">If you look at the code we discussed in previous chapters, we are
    already using Chroma.</st> <st c="3610">Here is a snippet of that code showing
    our use of Chroma, which you can find in the code for this code lab</st> <st c="3717">as
    well:</st>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3521">如果你查看我们在前几章中讨论的代码，我们已经在使用Chroma。</st> <st c="3610">以下是该代码片段，展示了我们使用Chroma的方式，你可以在本代码实验室的代码中找到它：</st>
    <st c="3717">：</st>
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="3957">LangChain calls this an</st> **<st c="3982">integration</st>**
    <st c="3993">since it is integrating with a third party called Chroma.</st> <st
    c="4052">There are many other integrations available</st> <st c="4096">with LangChain.</st>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3957">LangChain称这为</st> **<st c="3982">集成</st>** <st c="3993">，因为它与第三方Chroma进行了集成。</st>
    <st c="4052">LangChain还有许多其他可用的集成。</st>
- en: <st c="4111">On the LangChain</st> <st c="4128">website, as it currently stands,
    there is an</st> **<st c="4174">Integrations</st>** <st c="4186">link in</st>
    <st c="4195">the main website navigation at the top of the website page.</st>
    <st c="4255">If you click on that, you will see a menu down the left side that
    stretches out pretty far and has the main categories of</st> **<st c="4377">Providers</st>**
    <st c="4386">and</st> **<st c="4391">Components</st>**<st c="4401">. As you might
    have guessed, this gives you the ability to view all the integrations by either
    providers or components.</st> <st c="4521">If you click on</st> **<st c="4537">Providers</st>**<st
    c="4546">, you will first see</st> **<st c="4567">Partner Packages</st>** <st
    c="4583">and</st> **<st c="4588">Featured Community Providers</st>**<st c="4616">.
    Chroma is not currently in either of these lists, but if you want to find out
    more about Chroma as a provider, click on the link near the end of the page that
    says</st> **<st c="4782">Click here to see all providers</st>**<st c="4813">.
    The list is in alphabetical order.</st> <st c="4850">Scroll down to the Cs and
    find Chroma.</st> <st c="4889">This will show you useful LangChain documentation
    related to Chroma, particularly in creating both the vector store and</st> <st
    c="5009">the retriever.</st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangChain**网站**上，目前有一个**<st c="4174">集成</st>** <st c="4186">链接位于网站页面顶部的**主网站导航**中。</st>
    <st c="4195">如果您点击它，您将看到一个沿着左侧伸展得很远的菜单，其中包括**<st c="4377">提供者</st>** <st c="4386">和**<st
    c="4391">组件</st>**<st c="4401">的主要类别。</st> <st c="4255">正如您可能已经猜到的，这使您能够通过提供者或组件来查看所有集成。</st>
    <st c="4521">如果您点击**<st c="4537">提供者</st>**<st c="4546">，您将首先看到**<st c="4567">合作伙伴包</st>**
    <st c="4583">和**<st c="4588">特色社区提供者</st>**<st c="4616">。</st> Chroma目前不在这两个列表中，但如果您想了解更多关于Chroma作为提供者的信息，请点击页面末尾的链接，该链接说**<st
    c="4782">点击此处查看所有提供者</st>**<st c="4813">。列表按字母顺序排列。</st> <st c="4850">向下滚动到C处找到Chroma。</st>
    <st c="4889">这将显示与Chroma相关的有用LangChain文档，尤其是在创建向量存储和**检索器**。</st>
- en: <st c="5023">Another useful approach is to click on</st> **<st c="5063">Vector
    stores</st>** <st c="5076">under</st> **<st c="5083">Components</st>**<st c="5093">.
    There are currently 49 vector store options!</st> <st c="5140">The current link
    is here for version 0.2.0, but keep an eye out for future versions</st> <st c="5224">as
    well:</st>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有用的方法是点击**<st c="5063">向量存储</st>** <st c="5076">下的</st> **<st c="5083">组件</st>**<st
    c="5093">。目前有49个向量存储选项！</st> <st c="5140">当前链接为版本0.2.0，但也要关注未来的版本</st> <st c="5224">：</st>
- en: '[<st c="5232">https://python.langchain.com/v0.2/docs/integrations/vectorstores/</st>](https://python.langchain.com/v0.2/docs/integrations/vectorstores/
    )'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<st c="5232">https://python.langchain.com/v0.2/docs/integrations/vectorstores/</st>](https://python.langchain.com/v0.2/docs/integrations/vectorstores/
    )'
- en: <st c="5298">Another area we highly recommend you review is the LangChain vector
    store</st> <st c="5373">documentation here:</st>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈推荐您查看的另一个领域是LangChain向量存储**文档**：</st> <st c="5373">
- en: '[<st c="5392">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores</st>](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores
    )'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[<st c="5392">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores</st>](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores
    )'
- en: <st c="5494">We have already talked about our current vector store and Chroma
    in general in-depth in past chapters, but let’s review Chroma and discuss where
    it is</st> <st c="5646">most useful.</st>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的章节中，我们已经深入讨论了我们的当前向量存储和Chroma，但让我们回顾一下Chroma并讨论它在哪里**最有用**。</st> <st c="5646">
- en: <st c="5658">Chroma</st>
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="5658">Chroma</st>
- en: '**<st c="5665">Chroma</st>** <st c="5672">is an open</st> <st c="5683">source
    AI-native vector database designed for developer productivity and</st> <st c="5756">ease
    of use.</st> <st c="5770">It offers fast search performance and seamless integration
    with LangChain through its Python SDK.</st> <st c="5868">Chroma supports various
    deployment modes, including in-memory, persistent storage, and containerized deployment</st>
    <st c="5980">using Docker.</st>'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="5665">Chroma</st>** <st c="5672">是一个开源的AI原生向量数据库，旨在提高开发者的生产力和**易用性**。</st>
    <st c="5756">它提供快速的搜索性能，并通过其Python SDK与LangChain无缝集成。</st> <st c="5770">Chroma支持多种部署模式，包括内存中、持久存储和Docker容器化部署</st>
    <st c="5980">。</st>'
- en: <st c="5993">One of the key advantages of Chroma is its simplicity and developer-friendly
    API.</st> <st c="6076">It provides straightforward methods for adding, updating,
    deleting, and querying documents in the vector store.</st> <st c="6188">Chroma
    also supports dynamic filtering of collections based on metadata, allowing for
    more targeted searches.</st> <st c="6298">Additionally, Chroma offers built-in
    functionality for document chunking and indexing, making it convenient to work
    with large</st> <st c="6425">text datasets.</st>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7910">Chroma的一个关键优势是其简单性和开发者友好的API。</st> <st c="6076">它提供了添加、更新、删除和查询向量存储中文档的直接方法。</st>
    <st c="6188">Chroma还支持基于元数据的动态过滤集合，从而可以进行更有针对性的搜索。</st> <st c="6298">此外，Chroma还提供了内置的文档分块和索引功能，使得处理大型文本数据集变得方便。</st>
    <st c="6425">文本数据集。</st>
- en: <st c="6439">When considering Chroma as a vector store for a RAG application,
    it’s important to evaluate its architecture and selection criteria.</st> <st c="6573">Chroma’s
    architecture consists of an indexing layer for fast vector retrieval, a storage
    layer for efficient data management, and a processing layer for real-time operations.</st>
    <st c="6748">Chroma integrates smoothly with LangChain, allowing developers to
    leverage its capabilities within the LangChain ecosystem.</st> <st c="6872">The
    Chroma client can be easily instantiated and passed to LangChain, enabling efficient
    storage and retrieval of document vectors.</st> <st c="7004">Chroma also supports
    advanced retrieval options, such</st> <st c="7058">as</st> **<st c="7061">maximum
    marginal relevance</st>** <st c="7087">(</st>**<st c="7089">MMR</st>**<st c="7092">)
    and metadata filtering to refine</st> <st c="7128">search results.</st>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6439">Chroma的架构由一个用于快速向量检索的索引层、一个用于高效数据管理的存储层和一个用于实时操作的处理层组成。</st> <st
    c="6573">Chroma与LangChain无缝集成，使开发者能够在LangChain生态系统中利用其功能。</st> <st c="6748">Chroma客户端可以轻松实例化并传递给LangChain，从而实现高效的文档向量存储和检索。</st>
    <st c="6872">Chroma还支持高级检索选项，例如**最大边际相关性**（**MMR**）和元数据过滤，以细化搜索结果。</st>
- en: <st c="7143">Overall, Chroma is</st> <st c="7162">a solid choice</st> <st c="7177">for
    developers seeking an open source, easy-to-use vector database that integrates
    well with LangChain.</st> <st c="7282">Its simplicity, fast search performance,
    and built-in document processing features make it an attractive option for building
    RAG applications.</st> <st c="7425">In fact, these are some of the reasons why
    we chose to feature Chroma in this book in several of the chapters.</st> <st c="7536">However,
    it’s important to assess your specific requirements and compare Chroma with other
    vector store alternatives to determine the best fit for your project.</st> <st
    c="7697">Let’s look at the code and discuss some of the other options available,
    starting</st> <st c="7778">with FAISS.</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7143">总的来说，Chroma是</st> <st c="7162">一个不错的选择</st> <st c="7177">，对于寻求一个开源、易于使用且与LangChain良好集成的向量数据库的开发者来说。</st>
    <st c="7282">其简单性、快速搜索性能和内置的文档处理功能使其成为构建RAG应用的吸引人选择。</st> <st c="7425">事实上，这也是我们为什么在本书的几个章节中突出介绍Chroma的原因之一。</st>
    <st c="7536">然而，评估您的具体需求并将Chroma与其他向量存储替代品进行比较，以确定最适合您项目的方案是很重要的。</st> <st c="7697">让我们查看代码并讨论一些其他可用的选项，从FAISS开始。</st>
    <st c="7778">考虑将Chroma作为RAG应用的向量存储时，评估其架构和选择标准很重要。</st>
- en: <st c="7789">FAISS</st>
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="7789">FAISS</st>
- en: <st c="7795">Let’s start with how our code</st> <st c="7826">would</st> <st
    c="7832">change if we wanted to use FAISS as our vector store.</st> <st c="7886">You
    would first need to</st> <st c="7910">install FAISS:</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7795">让我们从如何修改我们的代码</st> <st c="7826">开始，如果我们想使用FAISS作为我们的向量存储。</st>
    <st c="7832">您首先需要安装FAISS：</st>
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="7947">After you have restarted the kernel (since you installed a new
    package), run all the code down to the vector store-related cell, and replace
    the Chroma-related code with the FAISS vector</st> <st c="8135">store instantiation:</st>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7947">在您重新启动内核（因为您安装了新包）后，运行所有代码直到向量存储相关的单元格，并将与Chroma相关的代码替换为FAISS向量存储实例化：</st>
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="8300">The</st> `<st c="8305">Chroma.from_documents()</st>` <st c="8328">method
    call has been replaced with</st> `<st c="8364">FAISS.from_documents()</st>`<st
    c="8386">. The</st> `<st c="8392">collection_name</st>` <st c="8407">and</st>
    `<st c="8412">client</st>` <st c="8418">parameters are not applicable to FAISS,
    so they have been removed from the method call.</st> <st c="8507">We repeat some
    of the code that we saw with the Chroma vector store, such as the document generation,
    which allows us to show the exact equivalent in code between the two vector store
    options.</st> <st c="8700">With these changes, the code can now use FAISS as the
    vector store instead</st> <st c="8775">of Chroma.</st>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8300">`<st c="8305">Chroma.from_documents()</st>`</st> <st c="8328">方法调用已被替换为
    `<st c="8364">FAISS.from_documents()</st>`。</st> `<st c="8392">collection_name</st>`
    <st c="8407">和 `<st c="8412">client</st>` <st c="8418">参数对 FAISS 不适用，因此已从方法调用中删除。</st>
    `<st c="8507">我们重复了一些与 Chroma 向量存储相关的代码，例如文档生成，这使我们能够展示两种向量存储选项之间的代码精确等效。</st>`
    <st c="8700">通过这些更改，代码现在可以使用 FAISS 作为向量存储而不是 Chroma。</st>
- en: '**<st c="8785">FAISS</st>** <st c="8791">is an</st> <st c="8798">open source
    library developed by Facebook AI.</st> <st c="8844">FAISS offers high-performance
    search capabilities and can handle large datasets that may not fit entirely in
    memory.</st> <st c="8961">Much like other vector stores mentioned here, the architecture
    of FAISS consists of an indexing layer that organizes vectors for fast retrieval,
    a storage layer for efficient data management, and an optional processing layer
    for real-time operations.</st> <st c="9212">FAISS provides various indexing techniques,
    such as clustering and quantization, to optimize search performance and memory
    usage.</st> <st c="9342">It also supports GPU acceleration for even faster</st>
    <st c="9392">similarity search.</st>'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="8785">FAISS</st>** <st c="8791">是由 Facebook AI 开发的开源库。</st> <st c="8798">FAISS
    提供高性能的搜索功能，可以处理可能无法完全适应内存的大型数据集。</st> <st c="8961">与其他在此处提到的向量存储类似，FAISS 的架构包括一个索引层，用于组织向量以实现快速检索，一个存储层，用于高效的数据管理，以及一个可选的处理层，用于实时操作。</st>
    <st c="9212">FAISS 提供了各种索引技术，如聚类和量化，以优化搜索性能和内存使用。</st> <st c="9342">它还支持 GPU 加速，以实现更快的相似性搜索。</st>'
- en: <st c="9410">If you have GPUs available, you</st> <st c="9443">can install this
    package instead of the one we</st> <st c="9490">installed previously:</st>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9410">如果您有可用的 GPU，您</st> <st c="9443">可以安装此包而不是我们之前安装的包：</st>
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="9534">Using the GPU version of FAISS can significantly speed up the similarity
    search process, especially for large-scale datasets.</st> <st c="9661">GPUs can
    handle a large number of vector comparisons in parallel, enabling faster retrieval
    of relevant documents in a RAG application.</st> <st c="9797">If you are working
    in an environment that deals with massive amounts of data and requires a substantial
    performance boost compared to what we have already been working with (Chroma),
    you should definitely test FAISS GPU and see the impact it can have</st> <st c="10048">for
    you.</st>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9534">使用 FAISS 的 GPU 版本可以显著加快相似性搜索过程，特别是对于大规模数据集。</st> <st c="9661">GPU
    可以并行处理大量向量比较，从而在 RAG 应用中更快地检索相关文档。</st> <st c="9797">如果您在一个处理大量数据且需要比我们之前使用（Chroma）更高的性能的环境工作，您绝对应该测试
    FAISS GPU 并看看它对您的影响。</st> <st c="10048">。
- en: <st c="10056">The FAISS LangChain documentation provides detailed examples and
    guides on how to use FAISS within the LangChain framework.</st> <st c="10181">It
    covers topics such as ingesting documents, querying the vector store, saving and
    loading indexes, and performing advanced operations, such as filtering and merging.</st>
    <st c="10349">The documentation also highlights FAISS-specific features, such
    as similarity search with scores and serialization/deserialization</st> <st c="10480">of
    indexes.</st>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10056">FAISS LangChain 文档提供了如何在 LangChain 框架中使用 FAISS 的详细示例和指南。</st>
    <st c="10181">它涵盖了诸如摄取文档、查询向量存储、保存和加载索引以及执行高级操作（如过滤和合并）等主题。</st> <st c="10349">文档还突出了
    FAISS 特有的功能，例如带有分数的相似性搜索和索引的序列化/反序列化。</st> <st c="10480">。
- en: <st c="10491">Overall, FAISS is</st> <st c="10510">a powerful and efficient
    vector store option for building RAG applications with LangChain.</st> <st c="10601">Its
    high-performance search capabilities, scalability, and seamless integration with
    LangChain make it a compelling choice for developers seeking a robust and customizable
    solution for storing and retrieving</st> <st c="10809">document</st> <st c="10818">vectors.</st>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10491">总的来说，FAISS是</st> <st c="10510">一个强大且高效的向量存储选项，用于构建与LangChain结合的RAG应用。</st>
    <st c="10601">它的高性能搜索能力、可扩展性和与LangChain的无缝集成使其成为寻求强大且可定制解决方案的开发者的一个有吸引力的选择，用于存储和检索</st>
    <st c="10809">文档</st> <st c="10818">向量。</st>
- en: <st c="10826">Those are two powerful options for your vector store needs.</st>
    <st c="10887">Next, we will show and discuss the Weaviate vector</st> <st c="10938">store
    option.</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10826">这些是满足你的向量存储需求的有力选择。</st> <st c="10887">接下来，我们将展示并讨论Weaviate向量</st>
    <st c="10938">存储选项。</st>
- en: <st c="10951">Weaviate</st>
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="10951">Weaviate</st>
- en: <st c="10960">There are multiple options</st> <st c="10987">for how you want
    to use and access Weaviate.</st> <st c="11033">We are going to show the</st> <st
    c="11057">embedding version, which runs a Weaviate instance from your application
    code rather than from a standalone Weaviate</st> <st c="11174">server installation.</st>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10960">关于如何使用和访问Weaviate，有多种</st> <st c="10987">选择。</st> <st c="11033">我们将展示</st>
    <st c="11057">嵌入版本，它从你的应用程序代码而不是从独立的Weaviate</st> <st c="11174">服务器安装中运行Weaviate实例。</st>
- en: <st c="11194">When Embedded Weaviate starts for the first time, it creates a
    permanent datastore in the location set in</st> `<st c="11301">persistence_data_path</st>`<st
    c="11322">. When your client exits, the Embedded Weaviate instance also exits,
    but the data persists.</st> <st c="11414">The next time the client runs, it starts
    a new instance of Embedded Weaviate.</st> <st c="11492">New Embedded Weaviate
    instances use the data that is saved in</st> <st c="11554">the datastore.</st>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11194">当嵌入式Weaviate首次启动时，它会在</st> `<st c="11301">persistence_data_path</st>`<st
    c="11322">设置的路径中创建一个永久数据存储。</st> <st c="11414">当你的客户端退出时，嵌入式Weaviate实例也会退出，但数据会持续保留。</st>
    <st c="11492">下次客户端运行时，它将启动一个新的嵌入式Weaviate实例。</st> <st c="11554">新的嵌入式Weaviate实例使用存储在</st>
    <st c="11554">数据存储中的数据。</st>
- en: <st c="11568">If you are familiar with</st> **<st c="11594">GraphQL</st>**<st
    c="11601">, you may recognize the influence it has had on Weaviate when you start
    looking at the code.</st> <st c="11694">The query language and API are inspired
    by GraphQL, but</st> <st c="11750">Weaviate does not use GraphQL directly.</st>
    <st c="11790">Weaviate uses a RESTful API with a query language that resembles
    GraphQL in terms of its structure and functionality.</st> <st c="11908">Weaviate
    uses predefined data types for properties in the schema definition, similar to
    GraphQL’s scalar types.</st> <st c="12020">The available data types in Weaviate
    include string, int, number, Boolean, date,</st> <st c="12101">and more.</st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11568">如果你熟悉</st> **<st c="11594">GraphQL</st>**<st c="11601">，当你开始查看代码时，你可能认识到它对Weaviate产生的影响。</st>
    <st c="11694">查询语言和API受到了GraphQL的启发，但</st> <st c="11750">Weaviate并不直接使用GraphQL。</st>
    <st c="11790">Weaviate使用RESTful API，其查询语言在结构和功能上与GraphQL相似。</st> <st c="11908">Weaviate在模式定义中使用预定义的数据类型来表示属性，类似于GraphQL的标量类型。</st>
    <st c="12020">Weaviate中可用的数据类型包括字符串、整数、数字、布尔值、日期等。</st>
- en: <st c="12110">One strength of Weaviate is its support of batch operations for
    creating, updating, or deleting multiple data objects in a single request.</st>
    <st c="12250">This is similar to GraphQL’s mutation operations, where you can
    perform multiple changes in a single request.</st> <st c="12360">Weaviate uses
    the</st> `<st c="12378">client.batch</st>` <st c="12390">context manager to group
    multiple operations into a batch, which we will demonstrate in</st> <st c="12479">a
    moment.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12110">Weaviate的一个优势是它支持批量操作，可以在单个请求中创建、更新或删除多个数据对象。</st> <st c="12250">这与GraphQL的突变操作类似，你可以在单个请求中执行多个更改。</st>
    <st c="12360">Weaviate使用</st> `<st c="12378">client.batch</st>` <st c="12390">上下文管理器将多个操作组合成一个批次，我们将在</st>
    <st c="12479">稍后演示。</st>
- en: <st c="12488">Let’s start with how our code would change if we wanted to use
    Weaviate as our vector store.</st> <st c="12582">You will first need to</st> <st
    c="12605">install FAISS:</st>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12488">让我们从如何使用Weaviate作为我们的向量存储开始。</st> <st c="12582">你首先需要</st> <st
    c="12605">安装FAISS：</st>
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="12680">After you have restarted the kernel (since you installed a new
    package), you run all the code down to the vector store-related cell, and update
    the code with the FAISS vector</st> <st c="12856">store instantiation:</st>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12680">在你重启内核（因为你安装了新的包）之后，你运行所有与向量存储相关的代码，并更新代码以使用FAISS向量</st> <st c="12856">存储实例化：</st>
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="13068">As you can see, there are many additional packages to import for
    Weaviate.</st> <st c="13144">We also install</st> `<st c="13160">tqdm</st>`<st
    c="13164">, which is not specific to</st> <st c="13191">Weaviate, but it is required,
    as Weaviate uses</st> `<st c="13238">tqdm</st>` <st c="13242">to show progress
    bars when</st> <st c="13270">it loads.</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13068">正如你所见，为了使用Weaviate，你需要导入许多额外的包。</st> <st c="13144">我们还安装了</st>
    `<st c="13160">tqdm</st>`<st c="13164">，这不是Weaviate特有的，但它是必需的，因为Weaviate使用</st>
    `<st c="13238">tqdm</st>` <st c="13242">来显示加载时的进度条。</st>
- en: <st c="13279">We must first declare</st> `<st c="13302">weaviate_client</st>`
    <st c="13317">as the</st> <st c="13325">Weaviate client:</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13279">我们必须首先声明</st> `<st c="13302">weaviate_client</st>` <st c="13317">作为</st>
    <st c="13325">Weaviate客户端：</st>
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="13412">The differences between our original</st> <st c="13449">Chroma
    vector store code and using Weaviate are more complicated than other approaches
    we have taken so far.</st> <st c="13559">With Weaviate, we initialized with the</st>
    `<st c="13598">WeaviateClient</st>` <st c="13612">client and the embedding options
    to enable embedded mode, as you</st> <st c="13678">saw previously.</st>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13412">我们原始的</st> <st c="13449">Chroma向量存储代码和使用Weaviate之间的差异比我们迄今为止采取的其他方法更复杂。</st>
    <st c="13559">使用Weaviate，我们使用</st> `<st c="13598">WeaviateClient</st>` <st c="13612">客户端和嵌入选项初始化，以启用嵌入式模式，正如你</st>
    <st c="13678">之前所看到的。</st>
- en: <st c="13693">Before we proceed, we need to make sure there is not already an
    instance of the Weaviate client in place, or our code</st> <st c="13812">will
    fail:</st>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13693">在我们继续之前，我们需要确保已经没有Weaviate客户端的实例存在，否则我们的代码</st> <st c="13812">将会失败：</st>
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: <st c="13893">For</st> <st c="13898">Weaviate, you have to</st> <st c="13919">make
    sure you clear out any lingering schemas from past iterations since they can persist
    in</st> <st c="14013">the background.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13893">对于</st> <st c="13898">Weaviate，你必须</st> <st c="13919">确保清除过去迭代中遗留的任何模式，因为它们可能会在</st>
    <st c="14013">后台持续存在。</st>
- en: <st c="14028">We then use the</st> `<st c="14045">weaviate</st>` <st c="14053">client
    to establish our database using a GraphQL-like</st> <st c="14108">definition schema:</st>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14028">然后我们使用</st> `<st c="14045">weaviate</st>` <st c="14053">客户端，通过类似于GraphQL的</st>
    <st c="14108">定义模式来建立我们的数据库：</st>
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="14501">This provides a full schema class that you will later pass into
    the vector store definition as part of the</st> `<st c="14609">weviate_client</st>`
    <st c="14623">object.</st> <st c="14632">You need to define this schema for your
    collection using the</st> `<st c="14693">client.collections.create()</st>` <st
    c="14720">method.</st> <st c="14729">The schema definition includes specifying
    the class name, properties, and their data types.</st> <st c="14821">Properties
    can have different data types, such as string, integer, and Boolean.</st> <st
    c="14901">As you can see, Weaviate</st> <st c="14925">enforces a stricter schema
    validation compared to what we’ve used in previous labs</st> <st c="15009">with</st>
    <st c="15014">Chroma.</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14501">这提供了一个完整的模式类，你稍后将其作为</st> `<st c="14609">weviate_client</st>`
    <st c="14623">对象的一部分传递给向量存储定义。</st> <st c="14632">你需要使用</st> `<st c="14693">client.collections.create()</st>`
    <st c="14720">方法为你的集合定义此模式。</st> <st c="14729">模式定义包括指定类名、属性及其数据类型。</st> <st c="14821">属性可以有不同的数据类型，例如字符串、整数和布尔值。</st>
    <st c="14901">正如你所见，与我们在之前的实验室中使用Chroma所用的相比，Weaviate执行了更严格的模式验证。</st>
- en: <st c="15021">While this GraphQL-like schema adds some complexity to establishing
    your vector store, it also gives you more control of your database in helpful
    and powerful ways.</st> <st c="15187">In particular, you have more granular control
    over how to define</st> <st c="15252">your schema.</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15021">虽然这种类似于GraphQL的模式在建立你的向量存储时增加了一些复杂性，但它也以有用且强大的方式为你提供了更多对数据库的控制。</st>
    <st c="15187">特别是，你对自己的模式定义有了更细粒度的控制。</st>
- en: <st c="15264">You may recognize the next code, as it looks a lot like the</st>
    `<st c="15325">dense_documents</st>` <st c="15340">and</st> `<st c="15345">sparse_documents</st>`
    <st c="15361">variables we have defined in the past, but if you look closely,
    there is a slight difference that is important</st> <st c="15473">to Weaviate:</st>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15264">您可能认出下面的代码，因为它看起来很像我们之前定义的</st> `<st c="15325">dense_documents</st>`
    <st c="15340">和</st> `<st c="15345">sparse_documents</st>` <st c="15361">变量，但如果你仔细看，有一个重要的</st>
    <st c="15473">差异对Weaviate来说很重要：</st>
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="15745">There is a slight change to these definitions for Weaviate when
    we pre-process the documents with the metadata.</st> <st c="15858">We use</st>
    `<st c="15865">'doc_id'</st>` <st c="15873">rather than</st> `<st c="15886">'id'</st>`
    <st c="15890">for Weaviate.</st> <st c="15905">This is because</st> `<st c="15921">'id'</st>`
    <st c="15925">is used internally and is not available for our use.</st> <st c="15979">Later
    in the code, when you extract the ID from the metadata results, you will want
    to update that code to use</st> `<st c="16090">'doc_id'</st>` <st c="16098">as
    well.</st>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15745">当我们使用元数据预处理文档时，这些定义对Weaviate会有轻微的变化。</st> <st c="15858">我们使用</st>
    `<st c="15865">'doc_id'</st>` <st c="15873">而不是</st> `<st c="15886">'id'</st>`
    <st c="15890">作为Weaviate。</st> <st c="15905">这是因为</st> `<st c="15921">'id'</st>`
    <st c="15925">在内部使用，并且不可用于我们。</st> <st c="15979">在代码的后续部分，当您从元数据结果中提取ID时，您将需要更新该代码以使用</st>
    `<st c="16090">'doc_id'</st>` <st c="16098">。
- en: <st c="16107">Next, we define our vector store, similar to what we have done
    in the past with Chroma and FAISS, but with</st> <st c="16215">Weaviate-specific
    parameters:</st>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16107">接下来，我们定义我们的向量存储，类似于我们过去在Chroma和FAISS中做过的事情，但使用</st> <st c="16215">Weaviate特定的参数：</st>
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="16416">For the vector store initialization, Chroma uses the</st> `<st
    c="16470">from_documents</st>` <st c="16484">method to create the vector store
    directly from the documents, whereas, for Weaviate, we create the vector store
    and then add the documents after.</st> <st c="16632">Weaviate also requires additional
    configuration, such as</st> `<st c="16689">text_key</st>`<st c="16697">,</st>
    `<st c="16699">attributes</st>`<st c="16709">, and</st> `<st c="16715">by_text</st>`<st
    c="16722">. One major difference is Weaviate’s use of</st> <st c="16766">a schema.</st>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16416">对于向量存储初始化，Chroma使用</st> `<st c="16470">from_documents</st>` <st
    c="16484">方法直接从文档创建向量存储，而对于Weaviate，我们创建向量存储然后添加文档。</st> <st c="16632">Weaviate还需要额外的配置，例如</st>
    `<st c="16689">text_key</st>`<st c="16697">,</st> `<st c="16699">attributes</st>`<st
    c="16709">, 和</st> `<st c="16715">by_text</st>`<st c="16722">。一个主要区别是Weaviate使用</st>
    <st c="16766">一个模式。</st>
- en: <st c="16775">Lastly, we load up the</st> <st c="16798">Weaviate vector store
    instance with our actual content, which also applies</st> <st c="16874">the embedding
    function in</st> <st c="16900">the process:</st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将我们的实际内容加载到<st c="16775">Weaviate向量存储实例中，这也适用于</st> <st c="16798">过程中的嵌入函数：</st>
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="17319">In summary, Chroma offers a simpler and more flexible approach
    to data schema definition and focuses on embedding storage and retrieval.</st>
    <st c="17457">It can be easily embedded into your application.</st> <st c="17506">On
    the other hand, Weaviate</st> <st c="17533">provides a more structured and feature-rich
    vector database solution with explicit schema definition, multiple storage backends,
    and built-in support for various embedding models.</st> <st c="17714">It can be
    deployed as a standalone server or hosted in the cloud.</st> <st c="17780">The
    choice between Chroma, Weaviate, or any of the other vector stores depends on
    your specific requirements, such as the level of schema flexibility, deployment
    preferences, and the need for additional features beyond</st> <st c="17999">embedding</st>
    <st c="18009">storage.</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17319">总的来说，Chroma提供了一种更简单、更灵活的数据模式定义方法，并专注于嵌入存储和检索。</st> <st c="17457">它可以轻松地嵌入到您的应用程序中。</st>
    <st c="17506">另一方面，Weaviate</st> <st c="17533">提供了一个结构更清晰、功能更丰富的向量数据库解决方案，具有显式的模式定义、多个存储后端和内置对各种嵌入模型的支持。</st>
    <st c="17714">它可以作为独立服务器部署或托管在云中。</st> <st c="17780">Chroma、Weaviate或其他向量存储之间的选择取决于您的具体需求，例如模式灵活性的水平、部署偏好以及除嵌入存储之外需要额外功能的需求：</st>
    <st c="17999">嵌入</st> <st c="18009">存储。</st>
- en: <st c="18017">Note that you can use any one of these vector stores and the remaining
    code will work with the data loaded to them.</st> <st c="18134">This is a strength
    of using LangChain, which allows you to swap components in and out.</st> <st c="18221">This
    is particularly necessary in the world of generative AI, where new and dramatically
    improved technologies are launching all the time.</st> <st c="18360">Using this
    approach, if you come across a newer and better vector store technology that makes
    a difference in your RAG pipeline, you can make this change relatively quickly
    and easily.</st> <st c="18545">Let’s talk next about another key component in
    the LangChain arsenal that is at the center of a RAG application:</st> <st c="18658">the
    retriever.</st>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以使用这些向量存储中的任何一个，并且剩余的代码将适用于加载到它们中的数据。这是使用LangChain的一个优势，它允许您在组件之间进行交换。这在生成式AI的世界中尤其必要，因为新的和显著改进的技术不断推出。使用这种方法，如果您遇到一种新的、更好的向量存储技术，它会在您的RAG管道中产生差异，您可以相对快速且容易地进行这种更改。接下来，让我们谈谈LangChain武器库中的另一个关键组件，它是RAG应用程序的核心：**检索器**。
- en: <st c="18672">Code lab 10.2 – LangChain Retrievers</st>
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实验室10.2 – LangChain 检索器
- en: '<st c="18709">In this code lab, we will cover a few examples of the most important
    component in the retrieval process: the</st> **<st c="18819">LangChain retriever</st>**<st
    c="18838">. Like the</st> <st c="18848">LangChain vector store, there are too</st>
    <st c="18887">many options for LangChain retrievers to list here.</st> <st c="18939">We
    will focus on a few popular choices that are particularly applicable to RAG applications,
    and we encourage you to look at all the others to see if there are better options
    for your specific situation.</st> <st c="19143">Just like we discussed with the
    vector stores, there is ample documentation on the LangChain website that will
    help you find your best</st> <st c="19278">solution:</st> [<st c="19288">https://python.langchain.com/v0.2/docs/integrations/retrievers/</st>](https://python.langchain.com/v0.2/docs/integrations/retrievers/
    )'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码实验室中，我们将介绍检索过程中最重要的组件的一些示例：**LangChain 检索器**。与LangChain向量存储一样，这里列出的LangChain检索器选项太多。我们将关注一些特别适用于RAG应用程序的流行选择，并鼓励您查看所有其他选项，看看是否有更适合您特定情况的选项。就像我们讨论向量存储时一样，LangChain网站上有很多文档可以帮助您找到最佳解决方案：[https://python.langchain.com/v0.2/docs/integrations/retrievers/](https://python.langchain.com/v0.2/docs/integrations/retrievers/)
- en: <st c="19351">The documentation for the retriever package</st> <st c="19395">can
    be found</st> <st c="19409">here:</st> [<st c="19415">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers</st>](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers
    )
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器包的文档可以在以下位置找到：[https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers)
- en: <st c="19514">Now, let’s get started with coding</st> <st c="19550">for retrievers!</st>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始为检索器编写代码！
- en: <st c="19565">Retrievers, LangChain, and RAG</st>
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索器、LangChain和RAG
- en: '**<st c="19596">Retrievers</st>** <st c="19607">are</st> <st c="19611">responsible
    for querying the vector store and retrieving the most relevant documents based
    on the input query.</st> <st c="19723">LangChain offers a range of retriever implementations
    that can be used in conjunction with different vector stores and</st> <st c="19842">query</st>
    <st c="19847">encoders.</st>'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索器** 负责根据输入查询查询向量存储并检索最相关的文档。LangChain提供了一系列的检索器实现，这些实现可以与不同的向量存储和查询编码器一起使用。'
- en: <st c="19857">In our code so far, we have already seen three versions of the
    retriever; let’s review them first, as they relate to the original Chroma-based</st>
    <st c="20001">vector store.</st>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们目前的代码中，我们已经看到了检索器的三个版本；让我们首先回顾它们，因为它们与基于Chroma的原始向量存储相关联。
- en: <st c="20014">Basic retriever (dense embeddings)</st>
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本检索器（密集嵌入）
- en: <st c="20049">We start with</st> <st c="20064">the</st> **<st c="20068">dense
    retriever</st>**<st c="20083">. This is the code we have used in several of our
    code labs up to</st> <st c="20149">this</st> <st c="20153">point:</st>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20049">我们从</st> <st c="20064">密集检索器</st> <st c="20068">**<st c="20083">**</st>
    <st c="20083">开始。这是我们在此点之前在几个代码实验室中使用的代码：</st>
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="20229">The dense retriever is created using the</st> `<st c="20271">vectorstore.as_retriever</st>`
    <st c="20295">function, specifying the number of top results to retrieve (</st>`<st
    c="20356">k=10</st>`<st c="20361">).</st> <st c="20365">Under the hood of this
    retriever, Chroma uses dense vector representations of the documents and performs
    a similarity search using cosine distance or Euclidean distance to retrieve the
    most relevant documents based on the</st> <st c="20587">query embedding.</st>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20229">密集检索器是通过使用</st> `<st c="20271">vectorstore.as_retriever</st>`
    <st c="20295">函数创建的，指定要检索的顶部结果数量（</st>`<st c="20356">k=10</st>`<st c="20361">）。</st>
    <st c="20365">在这个检索器的底层，Chroma使用文档的密集向量表示，并使用余弦距离或欧几里得距离进行相似度搜索，根据查询嵌入检索最相关的文档。</st>
- en: <st c="20603">This is using the simplest type of retriever, the vector store
    retriever, which simply creates embeddings for each piece of text and uses those
    embeddings for retrieval.</st> <st c="20774">The retriever is essentially a wrapper
    around the vector store.</st> <st c="20838">Using this approach gives you access
    to the built-in retrieval/search functionality of the vector store, but in a way
    that integrates and interfaces in the LangChain ecosystem.</st> <st c="21015">It
    is a lightweight wrapper around the vector store class that gives you a consistent
    interface for all of the retriever options in LangChain.</st> <st c="21158">Because
    of this, once you construct a vector store, it’s very easy to construct a retriever.</st>
    <st c="21251">If you need to</st> <st c="21265">change your vector store or retriever,
    that is also very easy to do</st> <st c="21334">as</st> <st c="21337">well.</st>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20603">这是使用最简单的检索器类型，即向量存储检索器，它为每段文本创建嵌入，并使用这些嵌入进行检索。</st> <st c="20774">检索器本质上是对向量存储的包装。</st>
    <st c="20838">使用这种方法，您可以使用LangChain生态系统中的集成和接口访问向量存储的内置检索/搜索功能。</st> <st c="21015">它是对向量存储类的轻量级包装，为LangChain中的所有检索器选项提供了一致的接口。</st>
    <st c="21158">因此，一旦构建了向量存储，构建检索器就非常容易。</st> <st c="21251">如果您需要</st> <st c="21265">更改向量存储或检索器，这也同样容易做到</st>
    <st c="21334">。</st>
- en: '<st c="21342">There are two primary search capabilities that come from these
    types of retrievers, stemming directly from the vector stores that it wraps: similarity
    search</st> <st c="21501">and MMR.</st>'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21342">这些类型的检索器提供了两种主要的搜索功能，直接源于它所包装的向量存储：相似度搜索和MMR。</st>
- en: <st c="21509">Similarity score threshold retrieval</st>
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="21509">相似度分数阈值检索</st>
- en: <st c="21546">By default, retrievers use similarity</st> <st c="21584">search.</st>
    <st c="21593">If you want to set a threshold of similarity</st> <st c="21638">though,
    you simply need to set the search type to</st> `<st c="21688">similarity_score_threshold</st>`
    <st c="21714">and set that similarity score threshold within the</st> `<st c="21766">kwargs</st>`
    <st c="21772">function that you pass to the retriever object.</st> <st c="21821">The
    code looks</st> <st c="21836">like this:</st>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，检索器使用相似度搜索。<st c="21584">如果您想设置一个相似度阈值，则只需将搜索类型设置为</st> `<st c="21688">similarity_score_threshold</st>`
    <st c="21714">并在传递给检索器对象的</st> `<st c="21766">kwargs</st>` <st c="21772">函数中设置该相似度分数阈值。</st>
    <st c="21821">代码看起来像这样：</st>
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="21973">This is a useful upgrade to the default similarity search that
    can be useful in many RAG applications.</st> <st c="22077">However, similarity
    search is not the only type of search these retrievers can support; there is</st>
    <st c="22174">also MMR.</st>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21973">这是对默认相似度搜索的有用升级，在许多RAG应用中可能很有用。</st> <st c="22077">然而，相似度搜索并不是这些检索器支持的唯一搜索类型；还有MMR。</st>
- en: <st c="22183">MMR</st>
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="22183">MMR</st>
- en: '`<st c="22655">search_type="mmr"</st>` <st c="22672">as a parameter when you
    define the retriever,</st> <st c="22719">like this:</st>'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="22655">search_type="mmr"</st>` <st c="22672">作为定义检索器时的一个参数，如下所示：</st>'
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="22793">Adding this to any vector store-based retriever will cause it
    to utilize an MMR type</st> <st c="22879">of search.</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22793">将此添加到任何基于向量存储的检索器中，将使其利用MMR类型的搜索。</st>
- en: <st c="22889">Similarity search and</st> <st c="22911">MMR can be supported
    by any vector stores that also</st> <st c="22964">support those search techniques.</st>
    <st c="22997">Let’s next talk about the sparse search mechanism we introduced
    in</st> [*<st c="23064">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="23073">, the</st> <st c="23079">BM25 retriever.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22889">相似性搜索和</st> <st c="22911">MMR 可以由支持这些搜索技术的任何向量存储支持。</st> <st c="22964">接下来，让我们谈谈我们在</st>
    [*<st c="23064">第 8 章</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="23073">中引入的稀疏搜索机制，即</st>
    <st c="23079">BM25 检索器。</st>
- en: <st c="23094">BM25 retriever</st>
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="23094">BM25 检索器</st>
- en: '`<st c="23173">BM25Retriever</st>` <st c="23186">is the</st> <st c="23194">LangChain
    representation of BM25 that can be used for sparse text</st> <st c="23260">retrieval
    purposes.</st>'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="23173">BM25Retriever</st>` <st c="23186">是</st> <st c="23194">LangChain
    对 BM25 的表示，可用于稀疏文本</st> <st c="23260">检索目的。</st>'
- en: <st c="23279">You have seen this retriever as well, as we used it to turn our
    basic search into a hybrid search in</st> [*<st c="23381">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="23390">. We see this in our code with</st> <st c="23421">these settings:</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23279">您也见过这个检索器，因为我们曾用它将我们的基本搜索转换为混合搜索，在</st> [*<st c="23381">第 8 章</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="23390">。我们在代码中通过以下设置看到这一点：</st> <st c="23421">这些设置：</st>
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="23509">The</st> `<st c="23514">BM25Retriever.from_documents()</st>` <st
    c="23544">method is called to create a sparse retriever from the sparse documents,
    specifying the number of top results to</st> <st c="23658">retrieve (</st>`<st
    c="23668">k=10</st>`<st c="23673">).</st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23509">调用</st> `<st c="23514">BM25Retriever.from_documents()</st>` <st
    c="23544">方法从稀疏文档创建稀疏检索器，指定要检索的 top 结果数量（</st>`<st c="23668">k=10</st>`<st c="23673">）。</st>
- en: <st c="23676">BM25 works by calculating a relevance score for each document
    based on the query terms and the document’s</st> **<st c="23783">term frequencies
    and inverse document frequencies</st>** <st c="23832">(</st>**<st c="23834">TF-IDF</st>**<st
    c="23840">).</st> <st c="23844">It uses a</st> <st c="23854">probabilistic model
    to estimate the relevance of documents to a given query.</st> <st c="23931">The
    retriever returns the top-k documents with the highest</st> <st c="23990">BM25
    scores.</st>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23676">BM25 通过根据查询词和文档的</st> **<st c="23783">词频和逆文档频率</st>** <st c="23832">(</st>**<st
    c="23834">TF-IDF</st>**<st c="23840">) 计算每个文档的相关性得分。</st> <st c="23844">它使用一个</st>
    <st c="23854">概率模型来估计文档与给定查询的相关性。</st> <st c="23931">检索器返回具有最高</st> <st c="23990">BM25
    分数的 top-k 文档。</st>
- en: <st c="24002">Ensemble retriever</st>
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="24002">集成检索器</st>
- en: <st c="24021">An</st> **<st c="24025">ensemble retriever</st>** <st c="24043">combines</st>
    <st c="24052">multiple retrieval methods and uses an additional</st> <st c="24103">algorithm
    to combine their results into one set.</st> <st c="24152">An ideal use of this
    type of retriever is when you want to combine dense and sparse retrievers to support
    a hybrid retriever approach like what we created in</st> [*<st c="24310">Chapter
    8</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="24319">’s</st> *<st c="24323">Code</st>*
    *<st c="24328">lab 8.3</st>*<st c="24335">:</st>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24021">一个</st> **<st c="24025">集成检索器</st>** <st c="24043">结合</st> <st
    c="24052">多种检索方法，并使用一个额外的</st> <st c="24103">算法将它们的结果合并成一个集合。</st> <st c="24152">这种类型检索器的理想用途是在您想结合密集和稀疏检索器以支持混合检索方法时，例如我们在</st>
    [*<st c="24310">第 8 章</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="24319">的</st>
    *<st c="24323">代码</st> *<st c="24328">实验室 8.3</st> *<st c="24335">中创建的：</st>
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: <st c="24456">In our case, the ensemble retriever combines the Chroma dense
    retriever and the BM25 sparse retriever to achieve better retrieval performance.</st>
    <st c="24600">It is created using the</st> `<st c="24624">EnsembleRetriever</st>`
    <st c="24641">class, which takes the list of retrievers and their corresponding
    weights.</st> <st c="24717">In this case, the dense retriever and sparse retriever
    are passed with equal weights of</st> `<st c="24805">0.5</st>` <st c="24808">each.</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24456">在我们的案例中，集成检索器结合了 Chroma 密集检索器和 BM25 稀疏检索器，以实现更好的检索性能。</st> <st
    c="24600">它是使用</st> `<st c="24624">EnsembleRetriever</st>` <st c="24641">类创建的，该类接受检索器的列表及其相应的权重。</st>
    <st c="24717">在这种情况下，密集检索器和稀疏检索器以相等的权重</st> `<st c="24805">0.5</st>` <st c="24808">传递。</st>
- en: <st c="24814">The</st> `<st c="24819">c</st>` <st c="24820">parameter in the
    ensemble retriever is a reranking parameter that controls the balance between
    the original retrieval scores and the reranking scores.</st> <st c="24972">It
    is used to adjust the influence of the reranking step on the final retrieval results.</st>
    <st c="25061">In this case, the</st> `<st c="25079">c</st>` <st c="25080">parameter
    is set to</st> `<st c="25101">0</st>`<st c="25102">, which means no reranking
    is performed.</st> <st c="25143">When</st> `<st c="25148">c</st>` <st c="25149">is
    set to a non-zero value, the ensemble retriever performs an additional reranking
    step on the retrieved documents.</st> <st c="25267">The reranking step rescores
    the retrieved documents based on a separate reranking model or function.</st>
    <st c="25368">The reranking model can take into account additional features or
    criteria to assess the relevance of the documents to</st> <st c="25486">the query.</st>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24814">在集成检索器中，</st> `<st c="24819">c</st>` <st c="24820">参数是一个重新排序参数，它控制原始检索分数和重新排序分数之间的平衡。</st>
    <st c="24972">它用于调整重新排序步骤对最终检索结果的影响。</st> <st c="25061">在这种情况下，</st> `<st c="25079">c</st>`
    <st c="25080">参数设置为</st> `<st c="25101">0</st>`<st c="25102">，这意味着不执行重新排序。</st>
    <st c="25143">当</st> `<st c="25148">c</st>` <st c="25149">设置为非零值时，集成检索器对检索到的文档执行额外的重新排序步骤。</st>
    <st c="25267">重新排序步骤根据单独的重新排序模型或函数重新评分检索到的文档。</st> <st c="25368">重新排序模型可以考虑到额外的特征或标准来评估文档与</st>
    <st c="25486">查询的相关性。</st>
- en: <st c="25496">In RAG applications, the quality and relevance of the retrieved
    documents directly impact the generated output.</st> <st c="25609">By utilizing
    the</st> `<st c="25626">c</st>` <st c="25627">parameter and a suitable reranking
    model, you can enhance the retrieval results to better suit the specific requirements
    of your RAG application.</st> <st c="25774">For example, you can design a reranking
    model that takes into account factors such as document relevance, coherence with
    the query, or domain-specific criteria.</st> <st c="25935">By setting an appropriate
    value for</st> `<st c="25971">c</st>`<st c="25972">, you can strike a balance
    between the original retrieval scores and the reranking scores, giving more weight
    to the reranking model when needed.</st> <st c="26118">This can help prioritize
    documents that are more relevant and informative for the RAG task, leading to
    improved</st> <st c="26230">generated outputs.</st>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25496">在RAG应用中，检索到的文档的质量和相关性直接影响生成的输出。</st> <st c="25609">通过利用</st> `<st
    c="25626">c</st>` <st c="25627">参数和合适的重新排序模型，您可以增强检索结果以更好地满足您RAG应用的具体要求。</st>
    <st c="25774">例如，您可以设计一个重新排序模型，该模型考虑了诸如文档相关性、与查询的一致性或特定领域标准等因素。</st> <st c="25935">通过为</st>
    `<st c="25971">c</st>`<st c="25972">设置适当的值，您可以在原始检索分数和重新排序分数之间取得平衡，在需要时给予重新排序模型更多的权重。</st>
    <st c="26118">这可以帮助优先考虑对RAG任务更相关和更有信息量的文档，从而提高</st> <st c="26230">生成的输出质量。</st>
- en: <st c="26248">When a query is passed to the ensemble retriever, it sends the
    query to both the dense and sparse retrievers.</st> <st c="26359">The ensemble
    retriever then combines the results from both retrievers based on their assigned
    weights and returns the top-k documents.</st> <st c="26494">Under the hood, the
    ensemble retriever leverages the strengths of both dense and sparse retrieval
    methods.</st> <st c="26601">Dense retrieval captures semantic similarity using
    dense vector representations, while sparse retrieval relies on keyword matching
    and term frequencies.</st> <st c="26754">By combining their results, the ensemble
    retriever aims to provide more accurate and comprehensive</st> <st c="26853">search
    results.</st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26248">当查询传递给集成检索器时，它会将查询发送给密集和稀疏检索器。</st> <st c="26359">集成检索器随后根据分配的权重将两个检索器的结果结合起来，并返回前k个文档。</st>
    <st c="26494">在底层，集成检索器利用了密集和稀疏检索方法的优势。</st> <st c="26601">密集检索通过密集向量表示捕获语义相似性，而稀疏检索则依赖于关键词匹配和词频。</st>
    <st c="26754">通过结合它们的结果，集成检索器旨在提供更准确和全面的</st> <st c="26853">搜索结果。</st>
- en: <st c="26868">The specific classes and methods used in the code snippet may
    vary depending on the library or framework being used.</st> <st c="26986">However,
    the general concepts of dense retrieval using vector similarity search, sparse</st>
    <st c="27073">retrieval using BM25, and ensemble retrieval combining multiple
    retrievers</st> <st c="27149">remain</st> <st c="27156">the same.</st>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26868">代码片段中使用的特定类和方法可能因所使用的库或框架而异。</st> <st c="26986">然而，使用向量相似度搜索进行密集检索、使用BM25进行稀疏检索以及结合多个检索器的集成检索的一般概念</st>
    <st c="27073">仍然是相同的。</st>
- en: <st c="27165">That covers the retrievers we have already seen in previous code,
    all drawn from the data we accessed and processed during the indexing stage.</st>
    <st c="27309">There are numerous other retriever types that work with data you
    extract from your documents that you can explore on the LangChain website to suit
    your needs.</st> <st c="27468">However, not all retrievers are designed to pull
    from documents you are processing.</st> <st c="27552">Next, we will review an
    example of a retriever built off a public data</st> <st c="27623">source, Wikipedia.</st>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27165">这涵盖了我们在之前的代码中已经看到的检索器，所有这些都是从我们在索引阶段访问和处理的数据中提取的。</st> <st c="27309">还有许多其他类型的检索器可以与您从文档中提取的数据一起使用，您可以在LangChain网站上探索这些检索器以满足您的需求。</st>
    <st c="27468">然而，并非所有检索器都旨在从您正在处理的文档中提取数据。</st> <st c="27552">接下来，我们将回顾一个基于公开数据源（维基百科）构建的检索器的示例。</st>
- en: <st c="27641">Wikipedia retriever</st>
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="27641">维基百科检索器</st>
- en: <st c="27661">As described by the creators of the</st> <st c="27697">Wikipedia
    retriever</st> <st c="27717">on the LangChain</st> <st c="27734">website (</st>[<st
    c="27744">https://www.langchain.com/</st>](https://www.langchain.com/)<st c="27771">):</st>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27661">正如维基百科检索器</st> <st c="27697">的制作者在LangChain</st> <st c="27717">网站上所描述的（</st>[<st
    c="27744">https://www.langchain.com/</st>](https://www.langchain.com/)<st c="27771">）：</st>
- en: <st c="27774">Wikipedia is the largest and most-read reference work in history,
    acting as a multilingual free online encyclopedia written and maintained by a
    community of volunteers.</st>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27774">维基百科是历史上最大、最受欢迎的参考工具，它是一个由志愿者社区编写和维护的多语言免费在线百科全书。</st>
- en: <st c="27943">That sounds like a great resource to tap for useful knowledge
    in your RAG applications!</st> <st c="28032">We will add a new cell after our
    existing retriever cell where we will use this Wikipedia retriever to retrieve
    wiki pages from</st> `<st c="28160">wikipedia.org</st>` <st c="28173">into the</st>
    `<st c="28183">Document</st>` <st c="28191">format that is</st> <st c="28207">used
    downstream.</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27943">这听起来像是一个很好的资源，可以用来在你的RAG应用中获取有用的知识！</st> <st c="28032">我们将在现有的检索器单元之后添加一个新的单元，我们将使用这个维基百科检索器从</st>
    `<st c="28160">wikipedia.org</st>` <st c="28173">检索维基页面到</st> `<st c="28183">文档</st>`
    <st c="28191">格式，这是</st> <st c="28207">下游使用的。</st>
- en: <st c="28223">We first need to install a couple of</st> <st c="28261">new packages:</st>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28223">我们首先需要安装几个</st> <st c="28261">新的包：</st>
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: <st c="28343">As always, when you install new packages, don’t forget to restart</st>
    <st c="28410">your kernel!</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28343">一如既往，当你安装新的包时，别忘了重启</st> <st c="28410">你的内核！</st>
- en: <st c="28422">With</st> <st c="28427">the</st> `<st c="28432">WikipediaRetriever</st>`
    <st c="28450">retriever, we now have a mechanism</st> <st c="28485">that can fetch
    data from Wikipedia as it relates to the user query we pass to it, similar to
    the other retrievers we have used, but using the whole of Wikipedia data</st>
    <st c="28652">behind it:</st>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28422">有了</st> <st c="28427">WikipediaRetriever</st> <st c="28450">检索器，我们现在有一个机制</st>
    <st c="28485">可以从维基百科获取与我们所传递的用户查询相关的数据，类似于我们之前使用的其他检索器，但使用的是维基百科的全部数据</st> <st
    c="28652">（</st>：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="29245">In this code, we import the</st> `<st c="29274">WikipediaRetriever</st>`
    <st c="29292">class from the</st> `<st c="29308">langchain_community.retrievers</st>`
    <st c="29338">module.</st> `<st c="29347">WikipediaRetriever</st>` <st c="29365">is
    a retriever class specifically designed to retrieve relevant documents from Wikipedia
    based on a given query.</st> <st c="29479">We then instantiate an instance of
    this receiver using the</st> `<st c="29538">WikipediaRetriever</st>` <st c="29556">class
    and assign it to the variable retriever.</st> <st c="29604">The</st> `<st c="29608">load_max_docs</st>`
    <st c="29621">parameter is set to</st> `<st c="29642">10</st>`<st c="29644">,
    indicating that the retriever should load a maximum of 10 relevant documents.</st>
    <st c="29724">The user query here is</st> `<st c="29747">What defines the golden
    age of piracy in the Caribbean?</st>`<st c="29802">, and we can look at the response
    to see what Wikipedia articles are retrieved to help answer</st> <st c="29896">this
    question.</st>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29245">在此代码中，我们从</st> `<st c="29274">langchain_community.retrievers</st>`
    <st c="29292">模块中导入</st> `<st c="29274">WikipediaRetriever</st>` <st c="29292">类。</st>
    `<st c="29347">WikipediaRetriever</st>` <st c="29365">是一个专门设计用于根据给定查询从Wikipedia检索相关文档的检索器类。</st>
    <st c="29479">然后我们使用</st> `<st c="29538">WikipediaRetriever</st>` <st c="29556">类实例化一个接收器实例，并将其分配给变量retriever。</st>
    <st c="29604">`<st c="29608">load_max_docs</st>` <st c="29621">参数设置为</st> `<st
    c="29642">10</st>`<st c="29644">，表示检索器应加载最多10个相关文档。</st> <st c="29724">此处的用户查询是</st>
    `<st c="29747">什么是加勒比海盗黄金时代的定义？</st>`<st c="29802">，我们可以查看响应以了解Wikipedia文章是如何被检索出来以帮助回答</st>
    <st c="29896">此问题。</st>
- en: <st c="29910">We</st> <st c="29913">call the</st> `<st c="29923">get_relevant_documents</st>`
    <st c="29945">method of the retriever</st> <st c="29969">object, passing in a
    query string as an argument, and receive this as the first document in</st> <st
    c="30062">that response:</st>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29910">我们调用检索器对象的</st> `<st c="29923">get_relevant_documents</st>` <st
    c="29945">方法，传入一个查询字符串作为参数，并在响应中接收第一个文档：</st>
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="30474">You can see the matching content at</st> <st c="30511">this link:</st>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30474">您可以在以下链接中查看匹配的内容：</st> <st c="30511">此链接：</st>
- en: '[<st c="30521">https://en.wikipedia.org/wiki/Golden_Age_of_Piracy</st>](https://en.wikipedia.org/wiki/Golden_Age_of_Piracy
    )'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[海盗黄金时代](https://en.wikipedia.org/wiki/Golden_Age_of_Piracy)'
- en: <st c="30572">This link was provided as the source by</st> <st c="30613">the
    retriever.</st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30572">此链接是由</st> <st c="30613">检索器提供的来源。</st>
- en: <st c="30627">In summary, this code demonstrates how to use the</st> `<st c="30678">WikipediaRetriever</st>`
    <st c="30696">class from the</st> `<st c="30712">langchain_community.retrievers</st>`
    <st c="30742">module to retrieve relevant documents from Wikipedia based on a
    given query.</st> <st c="30820">It then extracts and prints specific metadata
    information (title, summary, source) and the content of the first</st> <st c="30932">retrieved
    document.</st>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30627">总的来说，此代码演示了如何使用</st> `<st c="30678">WikipediaRetriever</st>` <st
    c="30696">类从</st> `<st c="30712">langchain_community.retrievers</st>` <st c="30742">模块中检索基于给定查询的Wikipedia相关文档。</st>
    <st c="30820">然后它提取并打印特定的元数据信息（标题、摘要、来源）以及检索到的第一份文档的内容。</st>
- en: '`<st c="30951">WikipediaRetriever</st>` <st c="30970">internally handles the
    process of querying Wikipedia’s API or search functionality, retrieving the relevant
    documents, and returning them as a list of</st> `<st c="31122">Document</st>`
    <st c="31130">objects.</st> <st c="31140">Each</st> `<st c="31145">Document</st>`
    <st c="31153">object contains metadata and the actual page content, which can
    be accessed and utilized as needed.</st> <st c="31254">There are many other retrievers
    that can access public data sources similar to this but focused on specific domains.</st>
    <st c="31371">For scientific research, there is</st> `<st c="31405">PubMedRetriever</st>`<st
    c="31420">. For other fields of research, such as mathematics and computer science,
    there is</st> `<st c="31503">ArxivRetreiver</st>`<st c="31517">, which accesses
    data from the open-access archive of more than 2 million scholarly articles about
    these subjects.</st> <st c="31632">In the</st> <st c="31638">finance world, there
    is a retriever called</st> `<st c="31682">KayAiRetriever</st>` <st c="31696">that
    can access</st> **<st c="31713">Securities and Exchange Commission</st>** <st
    c="31747">(</st>**<st c="31749">SEC</st>**<st c="31752">) filings, which contain
    the financial statements that public companies are required to submit to the</st>
    <st c="31855">US SEC.</st>'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="30951">WikipediaRetriever</st>` <st c="30970">内部处理查询维基百科 API 或搜索功能的过程，检索相关文档，并将它们作为</st>
    `<st c="31122">Document</st>` <st c="31130">对象列表返回。</st> <st c="31140">每个</st>
    `<st c="31145">Document</st>` <st c="31153">对象都包含元数据和实际页面内容，可以根据需要访问和使用。</st>
    <st c="31254">还有许多其他检索器可以访问类似此类但专注于特定领域的公共数据源。</st> <st c="31371">对于科学研究，有</st>
    `<st c="31405">PubMedRetriever</st>`<st c="31420">。对于其他研究领域，如数学和计算机科学，有</st> `<st
    c="31503">ArxivRetreiver</st>`<st c="31517">，它可以从关于这些主题的超过 200 万篇开放获取档案的数据中获取数据。</st>
    <st c="31632">在</st> <st c="31638">金融领域，有一个名为</st> `<st c="31682">KayAiRetriever</st>`
    <st c="31696">的检索器，可以访问</st> **<st c="31713">证券交易委员会</st>** <st c="31747">(</st>**<st
    c="31749">SEC</st>**<st c="31752">) 的文件，这些文件包含上市公司必须提交给</st> <st c="31855">美国证券交易委员会</st>
    的财务报表。'
- en: '<st c="31862">For projects that deal with data that is not on a massive scale,
    we have one more retriever to highlight: the</st> <st c="31973">kNN retriever.</st>'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31862">对于处理非大规模数据的项目的检索器，我们还有一个要强调：</st> <st c="31973">kNN 检索器。</st>
- en: <st c="31987">kNN retriever</st>
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="31987">kNN 检索器</st>
- en: <st c="32001">The</st> <st c="32005">nearest-neighbor algorithms we have</st>
    <st c="32041">been working with up to this point, the ones responsible for finding
    the most closely related content to the user query, have been based</st> <st c="32178">on</st>
    **<st c="32182">approximate nearest neighbor</st>** <st c="32210">(</st>**<st
    c="32212">ANN</st>**<st c="32215">).</st> <st c="32219">There is a more</st> *<st
    c="32235">traditional</st>* <st c="32246">and</st> *<st c="32251">older</st>*
    <st c="32256">algorithm that serves as an alternative to ANN though, and this
    is</st> <st c="32323">the</st> **<st c="32328">k-nearest neighbor</st>** <st c="32346">(</st>**<st
    c="32348">kNN</st>**<st c="32351">).</st> <st c="32355">But kNN is based on an
    algorithm that dates back to 1951; why would we use this when we have a more sophisticated
    and powerful algorithm like ANN available?</st> <st c="32512">Because kNN is</st>
    *<st c="32527">still better</st>* <st c="32539">than anything that came after
    it.</st> <st c="32574">That is not a misprint.</st> <st c="32598">kNN is still
    the</st> *<st c="32615">most effective</st>* <st c="32629">way to find the nearest
    neighbors.</st> <st c="32665">It is better than ANN, which is touted as</st> *<st
    c="32707">the</st>* <st c="32710">solution by all of the database, vector database,
    and information retrieval companies that operate in this field.</st> <st c="32825">ANN
    can come close, but kNN is still</st> <st c="32862">considered better.</st>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32001">我们</st> <st c="32005">一直</st> <st c="32041">在使用的</st> <st c="32005">最近邻算法，负责找到与用户查询最相关的内容的算法，一直</st>
    <st c="32178">基于</st> **<st c="32182">近似最近邻</st>** <st c="32210">(**<st c="32212">ANN</st>**)。</st>
    <st c="32219">尽管有一个更*<st c="32235">传统</st>** <st c="32246">和*<st c="32251">古老</st>**
    <st c="32256">的算法可以作为ANN的替代方案，那就是</st> <st c="32323">k-最近邻</st> **<st c="32328">（kNN）</st>**。</st>
    <st c="32355">但kNN基于一个可以追溯到1951年的算法；为什么我们有像ANN这样更复杂、更强大的算法可用时还要使用这个呢？</st> <st
    c="32512">因为kNN</st> *<st c="32527">仍然</st> <st c="32539">比之后的任何东西都要好。</st> <st
    c="32574">这不是一个错误。</st> <st c="32598">kNN仍然是</st> *<st c="32615">最有效</st>** <st
    c="32629">的方法来找到最近邻。</st> <st c="32665">它比ANN更好，ANN被数据库、向量数据库和信息检索公司吹捧为</st> *<st
    c="32707">解决方案</st>** <st c="32710">。</st> <st c="32825">ANN可以接近，但kNN仍然被认为是更好的。</st>
- en: <st c="32880">Why is ANN touted as</st> *<st c="32902">the</st>* <st c="32905">solution
    then?</st> <st c="32921">Because kNN does not scale to the level you see in the
    large enterprises these vendors are targeting.</st> <st c="33023">But this is
    all relative.</st> <st c="33049">You may have a million data points, which sounds
    like a lot, with 1,536 dimension vectors, but that is still considered quite small
    on the global enterprise stage.</st> <st c="33213">kNN can handle that pretty
    easily!</st> <st c="33248">Many of the smaller projects that are using ANN in
    the field can probably benefit from using kNN instead.</st> <st c="33354">The
    theoretical limit of kNN is going to depend on many things, such as your development
    environment, your data, the dimensions of your data, internet connectivity if
    using APIs, and many more.</st> <st c="33548">So, we cannot give a specific number
    of data points.</st> <st c="33601">You will need to test this.</st> <st c="33629">But
    if it is smaller than the project I just described, 1 million data points with
    1,536 dimension vectors, in a relatively capable development environment, you
    should really consider kNN!</st> <st c="33818">At some point, you will notice
    a significant increase in processing time, and when the wait becomes too long
    for the usefulness of your application, switch to ANN.</st> <st c="33982">But
    in the meantime, be sure to take full advantage of the superior search capabilities</st>
    <st c="34070">of kNN.</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32880">为什么人工神经网络（ANN）被誉为</st> *<st c="32902">解决方案</st>* <st c="32905">呢？</st>
    <st c="32921">因为kNN无法扩展到这些供应商针对的大型企业所看到的水平。</st> <st c="33023">但这都是相对的。</st> <st
    c="33049">你可能有一百万个数据点，这听起来很多，但与1,536维向量相比，在全球企业舞台上仍然相当小。</st> <st c="33213">kNN可以轻松处理这一点！</st>
    <st c="33248">许多在领域内使用ANN的小型项目可能从使用kNN中受益。</st> <st c="33354">kNN的理论极限将取决于许多因素，例如你的开发环境、你的数据、数据的维度、如果使用API，则还取决于互联网连接，等等。</st>
    <st c="33548">因此，我们无法给出具体的数据点数量。</st> <st c="33601">你需要进行测试。</st> <st c="33629">但如果它小于我刚才描述的项目，即1百万个数据点，1,536维向量，在一个相对强大的开发环境中，你真的应该考虑kNN！</st>
    <st c="33818">在某个时候，你会注意到处理时间的显著增加，当等待时间变得过长以至于你的应用程序的实用性降低时，切换到ANN。</st> <st
    c="33982">但在此期间，务必充分利用kNN的优越搜索能力</st> <st c="34070">。</st>
- en: <st c="34077">Luckily, kNN is available in an easy-to-set-up retriever called</st>
    `<st c="34142">KNNRetriever</st>`<st c="34154">. This retriever will utilize the
    same dense embeddings we use with our other algorithms, and therefore, we will
    replace</st> `<st c="34275">dense_retriever</st>` <st c="34290">with the kNN-based</st>
    `<st c="34310">KNNRetriever</st>`<st c="34322">. Here is the code to implement
    that, fitting in nicely after we defined the previous version of our</st> `<st
    c="34423">dense_retriever</st>` <st c="34438">retriever object:</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34077">幸运的是，kNN可以通过一个易于设置的检索器**<st c="34142">KNNRetriever</st>**<st c="34154">获得。这个检索器将利用我们与其他算法一起使用的相同密集嵌入，因此我们将用基于kNN的**<st
    c="34310">KNNRetriever</st>**<st c="34322">替换**<st c="34275">dense_retriever</st>**<st
    c="34290">。以下是实现此功能的代码，在定义了之前版本的我们的**<st c="34423">dense_retriever</st>**<st c="34438">检索器对象之后很好地融入其中：</st>
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: <st c="34707">Run the</st> <st c="34715">remaining code in the code lab to see
    it take the place of our</st> <st c="34778">previous</st> `<st c="34788">dense_retriever</st>`
    <st c="34803">and perform in its place.</st> <st c="34830">In this particular
    situation, with a very limited dataset, it is difficult to evaluate if it is doing
    better than the ANN-based algorithm we were previously using.</st> <st c="34994">But,
    as your project scales, we highly recommend you take advantage of this approach
    until its scaling issues become too much of</st> <st c="35123">a burden.</st>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34707">运行代码实验室中的剩余代码，看看它如何取代我们之前的**<st c="34788">dense_retriever</st>**<st
    c="34803">并在此处执行。</st> <st c="34830">在这种情况下，由于数据集非常有限，很难评估它是否比我们之前使用的基于ANN的算法表现更好。</st>
    <st c="34994">但是，随着你的项目规模扩大，我们强烈建议你利用这种方法，直到其扩展问题变得过于沉重。</st>
- en: <st c="35132">This concludes our exploration of the retrievers that can support
    RAG.</st> <st c="35204">There are additional types of retrievers, as well as notable
    integrations with vector stores that support those retrievers that can be reviewed
    on the LangChain website.</st> <st c="35374">For example, there is a time-weighted
    vector store retriever that allows you to incorporate recency into the retrieval
    process.</st> <st c="35502">There is also a retriever called the Long-Context
    Reorder focused on improving results from long-context models that have difficulty
    paying attention to information in the middle of the retrieved documents.</st>
    <st c="35709">Be sure to take a look at what is available, as they have the potential
    to have a significant impact on your RAG application.</st> <st c="35835">We will
    now move on to talking about the</st> *<st c="35876">brains</st>* <st c="35882">of
    the operation and of the generation stage:</st> <st c="35929">the LLMs.</st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35132">这标志着我们对支持RAG的检索器的探索结束。</st> <st c="35204">还有其他类型的检索器，以及与支持这些检索器的向量存储的显著集成，可以在LangChain网站上查看。</st>
    <st c="35374">例如，有一个时间加权向量存储检索器，允许你在检索过程中结合近期性。</st> <st c="35502">还有一个名为Long-Context
    Reorder的检索器，专注于改进难以关注检索文档中间信息的长上下文模型的结果。</st> <st c="35709">务必查看可用的内容，因为它们可能对你的RAG应用产生重大影响。</st>
    <st c="35835">我们现在将转向讨论操作和生成阶段的**<st c="35876">大脑</st>**：<st c="35882">LLMs。</st>
    <st c="35929">LLMs。</st>
- en: <st c="35938">Code lab 10.3 – LangChain LLMs</st>
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="35938">代码实验室10.3 – LangChain LLMs</st>
- en: '<st c="35969">We now turn our attention</st> <st c="35996">to the last key
    component for RAG: the LLM.</st> <st c="36040">Just like the retriever in the
    retrieval stage, without the LLM for the generation stage, there is no RAG.</st>
    <st c="36147">The retrieval stage simply retrieves data from our data source,
    typically data the LLM does not know about.</st> <st c="36255">However, that does
    not mean that the LLM does not play a vital role in our RAG implementation.</st>
    <st c="36350">By providing the retrieved data to the LLM, we quickly catch that
    LLM up with what we want it to talk about, and this allows the LLM to do what
    it is really good at, providing a response based on that data to answer the original
    question posed by</st> <st c="36597">the user.</st>'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35969">我们现在将注意力转向RAG的最后一个关键组件：LLM。</st> <st c="35996">就像检索阶段中的检索器一样，如果没有生成阶段的LLM，就没有RAG。</st>
    <st c="36040">检索阶段只是从我们的数据源检索数据，通常是LLM不知道的数据。</st> <st c="36147">但这并不意味着LLM在我们的RAG实现中没有发挥至关重要的作用。</st>
    <st c="36255">通过向LLM提供检索到的数据，我们迅速让LLM了解我们希望它讨论的内容，这使得LLM能够发挥其真正擅长的能力，根据这些数据提供基于数据的响应来回答用户提出的原始问题。</st>
- en: <st c="36606">The synergy between LLMs and RAG systems stems from the complementary
    strengths of these two technologies.</st> <st c="36714">RAG systems enhance the
    capabilities of LLMs by incorporating external knowledge sources, enabling the
    generation of responses that are not only contextually relevant but also factually
    accurate and up to date.</st> <st c="36925">In turn, LLMs contribute to RAG by
    providing a sophisticated understanding of the query context, facilitating more
    effective retrieval of pertinent information from the knowledge base.</st> <st
    c="37110">This symbiotic relationship significantly improves the performance of
    AI systems in tasks that require both deep language understanding and access to
    a wide range of factual</st> <st c="37283">information, leveraging the strengths
    of each component to create a more powerful and</st> <st c="37370">versatile system.</st>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 和 RAG 系统之间的协同作用源于这两种技术的互补优势。<st c="36714">RAG 系统通过整合外部知识源来增强 LLMs 的能力，从而生成不仅与上下文相关，而且事实准确且最新的响应。</st>
    <st c="36925">反过来，LLMs 通过提供对查询上下文的复杂理解，促进从知识库中更有效地检索相关信息。</st> <st c="37110">这种共生关系显著提高了
    AI 系统在需要深度语言理解和广泛事实信息访问的任务中的性能，利用每个组件的优势，创建一个更强大和**多才多艺**的系统。</st>
- en: '<st c="37387">In this code lab, we will cover a few examples of the most important
    component in the generation stage: the</st> <st c="37496">LangChain LLM.</st>'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码实验室中，我们将介绍生成阶段最重要的组件之一：**LangChain LLM**。
- en: <st c="37510">LLMs, LangChain, and RAG</st>
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**LLMs**、LangChain 和 RAG'
- en: <st c="37535">As with the previous key components, we will first provide links
    to the LangChain documentation related to this major component, the</st> <st c="37669">LLMs:</st>
    [<st c="37675">https://python.langchain.com/v0.2/docs/integrations/llms/</st>](https://python.langchain.com/v0.2/docs/integrations/llms/
    )
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的关键组件一样，我们首先提供与这个主要组件相关的 LangChain 文档链接，即**LLMs**：<st c="37675">[https://python.langchain.com/v0.2/docs/integrations/llms/](https://python.langchain.com/v0.2/docs/integrations/llms/)</st>
- en: <st c="37732">Here is a second helpful source for information combining LLMs
    with LangChain is the API</st> <st c="37822">documentation:</st> [<st c="37837">https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms</st>](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms
    )
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是第二个有用的信息来源，它结合了 LLMs 和 LangChain 的 API 文档：<st c="37822">[https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms)</st>
- en: '<st c="37940">Let’s start with the API we have been using</st> <st c="37985">already:
    OpenAI.</st>'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们一直在使用的 API 开始：**OpenAI**。
- en: <st c="38001">OpenAI</st>
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**OpenAI**'
- en: <st c="38008">We already</st> <st c="38020">have this code in place, but</st>
    <st c="38048">let’s refresh the inner workings of this code by stepping through
    the key areas of our lab that make this</st> <st c="38155">component work:</st>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了这段代码，但让我们通过回顾实验室中使这个组件工作的关键区域来刷新这段代码的内部工作原理：
- en: <st c="38170">First, we must install the</st> `<st c="38198">langchain-openai</st>`
    <st c="38214">package:</st>
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须安装**langchain-openai**包：
- en: '[PRE21]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: <st c="38351">Next, we import the</st> `<st c="38372">openai</st>` <st c="38378">library,
    which is the official Python library for interacting with OpenAI’s API, and will
    be used in this code primarily to apply the API key to the model so that we can
    access the paid API.</st> <st c="38570">We then import the</st> `<st c="38589">ChatOpenAI</st>`
    <st c="38599">and</st> `<st c="38604">OpenAIEmbeddings</st>` <st c="38620">classes
    from the</st> `<st c="38638">langchain_openai</st>` <st c="38654">library:</st>
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们导入**openai**库，这是与 OpenAI API 交互的官方 Python 库，在本代码中主要用于将 API 密钥应用于模型，以便我们可以访问付费
    API。<st c="38570">然后，我们导入</st> `<st c="38589">ChatOpenAI</st>` <st c="38599">和</st>
    `<st c="38604">OpenAIEmbeddings</st>` <st c="38620">类，它们来自</st> `<st c="38638">langchain_openai</st>`
    <st c="38654">库：</st>
- en: '[PRE22]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <st c="38859">In the next line, we load the environment variables from a file
    named</st> `<st c="38930">env.txt</st>` <st c="38937">using the</st> `<st c="38948">load_dotenv</st>`
    <st c="38959">function:</st>
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一行，我们使用**load_dotenv**函数从名为**env.txt**的文件中加载环境变量：
- en: '[PRE24]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: <st c="39192">We then</st> <st c="39201">pass that API key into</st> <st c="39223">OpenAI
    using the</st> <st c="39241">following code:</st>
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="39192">然后</st> <st c="39201">我们将该API密钥通过以下代码传递给</st> <st c="39223">OpenAI：</st>
- en: '[PRE25]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: <st c="39681">Later in the code, we define the LLM we want</st> <st c="39727">to
    use:</st>
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="39681">在代码的后续部分，我们定义了我们想要</st> <st c="39727">使用的LLM：</st>
- en: '[PRE28]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: <st c="39792">This line creates an instance of the</st> `<st c="39830">ChatOpenAI</st>`
    <st c="39840">class, specifying the model name as</st> `<st c="39877">gpt-4o-mini</st>`
    <st c="39888">and setting the</st> `<st c="39905">temperature</st>` <st c="39916">variable
    to</st> `<st c="39929">0</st>`<st c="39930">. The temperature controls the randomness
    of the generated responses, with lower values producing more focused and deterministic
    outputs.</st> <st c="40068">Currently,</st> `<st c="40079">gpt-4o-mini</st>` <st
    c="40090">is the newest and most capable model while also being the most cost-effective
    of the GPT4 series.</st> <st c="40189">But even that model costs 10X more than</st>
    `<st c="40229">gpt-3.5-turbo</st>`<st c="40242">, which is actually a relatively</st>
    <st c="40275">capable model.</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39792">这一行创建了一个</st> `<st c="39830">ChatOpenAI</st>` <st c="39840">类的实例，指定模型名称为</st>
    `<st c="39877">gpt-4o-mini</st>` <st c="39888">，并将</st> `<st c="39905">温度</st>`
    <st c="39916">变量设置为</st> `<st c="39929">0</st>`<st c="39930">。温度控制生成响应的随机性，较低的值会产生更集中和确定性的输出。</st>
    <st c="40068">目前，</st> `<st c="40079">gpt-4o-mini</st>` <st c="40090">是最新且能力最强的模型，同时也是GPT4系列中最具成本效益的模型。</st>
    <st c="40189">但即使是这个模型，其成本也比</st> `<st c="40229">gpt-3.5-turbo</st>`<st c="40242">高10倍，而实际上</st>
    `<st c="40275">gpt-3.5-turbo</st>`是一个相对</st> <st c="40275">有能力的模型。</st>
- en: <st c="40289">The most expensive of OpenAI’s models,</st> `<st c="40329">gpt-4-32k</st>`<st
    c="40338">, is not as fast or capable as</st> `<st c="40369">gpt-4o-mini</st>`
    <st c="40380">and has a context window 4X its size.</st> <st c="40419">There will
    likely be newer models soon, including</st> `<st c="40469">gpt-5</st>`<st c="40474">,
    that may be even lower cost and more capable.</st> <st c="40522">What you can
    take away from all of this is that you shouldn’t just assume the latest model
    is going to be the most expensive, and there are alternative versions that can
    be more capable and even more cost-effective coming out all the time.</st> <st
    c="40762">Stay diligent in following the latest releases of the models, and for
    each release, weigh</st> <st c="40852">the benefits of cost, LLM capability, and
    any other related</st> <st c="40912">attributes to decide if a change</st> <st
    c="40945">is warranted.</st>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40289">OpenAI最昂贵的模型</st> `<st c="40329">gpt-4-32k</st>`<st c="40338">，其速度和能力并不如</st>
    `<st c="40369">gpt-4o-mini</st>` <st c="40380">，且其上下文窗口大小是其4倍。</st> <st c="40419">未来可能会出现更新的模型，包括</st>
    `<st c="40469">gpt-5</st>`<st c="40474">，这些模型可能成本更低且能力更强。</st> <st c="40522">从所有这些中，你可以得出的结论是，你不应该仅仅假设最新的模型将是成本最高的，而且总会有更强大且成本效益更高的替代版本不断推出。</st>
    <st c="40762">持续关注模型的最新发布，并对每个版本，权衡</st> <st c="40852">成本、LLM能力以及其他相关</st> <st
    c="40912">属性，以决定是否需要进行更改</st> <st c="40945">。</st>
- en: <st c="40958">But in this effort, you do not need to limit yourself to just
    OpenAI.</st> <st c="41029">Using LangChain makes it easy to switch LLMs and broaden
    your search for the best solution to all options within the LangChain community.</st>
    <st c="41167">Let’s step through some other options you</st> <st c="41209">may
    consider.</st>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40958">但在这一努力中，你不需要将自己限制在仅使用OpenAI。</st> <st c="41029">使用LangChain可以轻松切换LLM，并扩大你在LangChain社区内寻找最佳解决方案的搜索范围。</st>
    <st c="41167">让我们逐一探讨一些你可能考虑的其他选项。</st>
- en: <st c="41222">Together AI</st>
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="41222">Together AI</st>
- en: '**<st c="41234">Together AI</st>** <st c="41246">offers a</st> <st c="41255">developer-friendly
    set of services that give you access to numerous models.</st> <st c="41332">Their</st>
    <st c="41337">pricing for hosted LLMs is difficult to beat, and they often offer
    $5.00 in free credits to test out the</st> <st c="41443">different models.</st>'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="41234">Together AI</st>** <st c="41246">提供了一套面向开发者的服务，让你可以访问众多模型。</st>
    <st c="41332">他们</st> `<st c="41337">托管LLM的定价难以匹敌，并且经常提供5.00美元的免费信用额度来测试</st>
    `<st c="41443">不同的模型。</st>`'
- en: <st c="41460">If you are new to Together API, you can use this link to set up
    your API key and add it to your</st> `<st c="41557">env.txt</st>` <st c="41564">file
    just like we did in the past with the OpenAI API</st> <st c="41619">key:</st>
    [<st c="41624">https://api.together.ai/settings/api-keys</st>](https://api.together.ai/settings/api-keys
    )
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41460">如果您是Together API的新用户，可以使用此链接设置您的API密钥并将其添加到您的</st> `<st c="41557">env.txt</st>`
    <st c="41564">文件中，就像我们过去使用OpenAI API</st> <st c="41619">密钥一样：</st> [<st c="41624">https://api.together.ai/settings/api-keys</st>](https://api.together.ai/settings/api-keys
    )
- en: <st c="41665">As you arrive at this web page, it currently offers you a $5.00
    credit that will be in place after you click on the</st> **<st c="41782">Get started</st>**
    <st c="41793">button.</st> <st c="41802">You do not have to provide a credit card
    to access this $</st><st c="41859">5.00 credit.</st>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41665">当您到达这个网页时，它目前提供5.00美元的信用额度，点击</st> **<st c="41782">开始使用</st>**
    <st c="41793">按钮后即可使用。</st> <st c="41802">您无需提供信用卡即可访问这</st><st c="41859">5.00美元的信用额度。</st>
- en: <st c="41872">Be sure to add your new API key to your</st> `<st c="41913">env.txt</st>`
    <st c="41920">file</st> <st c="41926">as</st> `<st c="41929">TOGETHER_API_KEY</st>`<st
    c="41945">.</st>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41872">请确保将您的新API密钥添加到您的</st> `<st c="41913">env.txt</st>` <st c="41920">文件中</st>
    <st c="41926">作为</st> `<st c="41929">TOGETHER_API_KEY</st>`<st c="41945">。</st>
- en: <st c="41946">Once you are logged in, you can see the current costs for each
    LLM</st> <st c="42014">here:</st> [<st c="42020">https://api.together.ai/models</st>](https://api.together.ai/models
    )
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41946">一旦您登录，您就可以在这里看到每个LLM的当前成本：</st> <st c="42014">这里：</st> [<st c="42020">https://api.together.ai/models</st>](https://api.together.ai/models
    )
- en: <st c="42050">For example, Meta Llama 3 70B instruct (Llama-3-70b-chat-hf) is
    currently listed to cost $0.90 per 1 million tokens.</st> <st c="42168">This is
    a model that has been shown to rival ChatGPT 4, but Together AI, will</st> <st
    c="42245">run with significantly lower inference costs than what OpenAI</st> <st
    c="42308">charges.</st> <st c="42317">Another highly capable model, the Mixtral
    mixture of experts model costs $1.20 per 1 million Follow these steps to set up
    and use</st> <st c="42447">Together AI:</st>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42050">例如，Meta Llama 3 70B instruct (Llama-3-70b-chat-hf) 目前列出的成本为每100万个令牌0.90美元。</st>
    <st c="42168">这是一个已被证明可以与ChatGPT 4相媲美的模型，但Together AI将</st> <st c="42245">以比OpenAI</st>
    <st c="42308">收费显著低得多的推理成本运行。</st> <st c="42317">另一个高度强大的模型，Mixtral专家混合模型，每100万个令牌成本为1.20美元。按照以下步骤设置和使用</st>
    <st c="42447">Together AI：</st>
- en: <st c="42459">We start with installing the package we need to use the</st> <st
    c="42516">Together API:</st>
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="42459">我们首先安装使用</st> <st c="42516">Together API</st> <st c="42516">所需的包：</st>
- en: '[PRE30]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: <st c="42571">This prepares us to use the integration between the Together API</st>
    <st c="42637">and LangChain:</st>
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="42571">这使我们能够使用Together API和LangChain之间的集成：</st>
- en: '[PRE31]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: <st c="42911">Just like we did in the past with the OpenAI API key, we are going
    to pull in</st> `<st c="42990">TOGETHER_API_KEY</st>` <st c="43006">so that it
    can access</st> <st c="43029">your account:</st>
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="42911">就像我们过去使用OpenAI API密钥一样，我们将导入</st> `<st c="42990">TOGETHER_API_KEY</st>`
    <st c="43006">以便它可以访问</st> <st c="43029">您的账户：</st>
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: <st c="43106">We are going to use the Llama 3 Chat model and Mistral’s Mixtral
    8X22B Instruct model, but you can choose from 50+ models</st> <st c="43229">here:</st>
    [<st c="43235">https://docs.together.ai/docs/inference-models</st>](https://docs.together.ai/docs/inference-models)
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="43106">我们将使用Llama 3 Chat模型和Mistral的Mixtral 8X22B Instruct模型，但您可以从50多个模型中选择</st>
    <st c="43229">这里：</st> [<st c="43235">https://docs.together.ai/docs/inference-models</st>](https://docs.together.ai/docs/inference-models)
- en: <st c="43281">You may find a better model for your</st> <st c="43319">particular
    needs!</st>
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="43281">您可能会找到更适合您</st> <st c="43319">特定需求的模型！</st>
- en: <st c="43336">Here, we are defining</st> <st c="43359">the models:</st>
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="43336">在这里，我们正在定义</st> <st c="43359">模型：</st>
- en: '[PRE35]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '***   <st c="43758">Here, we</st> <st c="43767">have</st> <st c="43773">updated
    the final code for using the Llama</st> <st c="43816">3 model:</st>'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '***   <st c="43758">在这里，我们</st> <st c="43767">更新了使用Llama</st> <st c="43816">3模型的最终代码：</st>'
- en: '[PRE43]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: <st c="44280">This should look familiar, as it is the RAG chain we have used
    in the past, but running with the Llama</st> <st c="44383">3 LLM.</st>
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="44280">这应该看起来很熟悉，因为它是我们过去使用的RAG链，但现在运行的是Llama 3 LLM。</st> <st c="44383">3
    LLM。</st>
- en: '[PRE65]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: <st c="44549">This is the final RAG chain we use, updated with the previous
    Llama 3-focused</st> <st c="44628">RAG chain.</st>
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="44549">这是我们使用的最终RAG链，已更新为之前的以Llama 3为重点的</st> <st c="44628">RAG链。</st>
- en: <st c="44638">Next, we want</st> <st c="44652">to run similar code to what we
    have run in the past that</st> <st c="44709">invokes and runs the RAG pipeline
    with the Llama 3 LLM replacing the</st> <st c="44779">ChatGPT-4o-mini model:</st>
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="44638">接下来，我们希望</st> <st c="44652">运行与过去运行类似的代码，该代码调用并运行RAG管道，用Llama
    3 LLM替换ChatGPT-4o-mini模型：</st>
- en: '[PRE69]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Google''s environmental initiatives include:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谷歌的环境倡议包括：
- en: '[PRE84]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '1\. Empowering individuals to take action: Offering sustainability features
    in Google products, such as eco-friendly routing in Google Maps, energy efficiency
    features in Google Nest thermostats, and carbon emissions information in Google
    Flights…'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. 赋能个人采取行动：在谷歌产品中提供可持续性功能，例如谷歌地图中的环保路线，谷歌Nest恒温器中的节能功能，以及谷歌航班中的碳排放信息…
- en: '[PRE85]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[TRUNCATED]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[TRUNCATED]'
- en: '[PRE86]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '10\. Engagement with external targets and initiatives: Participating in industry-wide
    initiatives and partnerships to promote sustainability, such as the RE-Source
    Platform, iMasons Climate Accord, and World Business Council for Sustainable Development.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 10\. 与外部目标和倡议互动：参与行业范围内的倡议和伙伴关系，以促进可持续性，例如RE-Source平台、iMasons气候协定和世界可持续发展商业理事会。
- en: '[PRE87]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '***   <st c="45979">Let’s see what it looks like if we use the mixture of</st>
    <st c="46034">experts model:</st>'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '***   <st c="45979">让我们看看如果我们使用专家混合模型会是什么样子：</st> <st c="46034">专家模型：</st>'
- en: '[PRE88]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '****<st c="46526">Again, this</st> <st c="46538">should look familiar, as it
    is the RAG chain we have used in the past, but</st> <st c="46613">this time running
    with the mixture of</st> <st c="46651">experts LLM.</st>****'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '****<st c="46526">再次，这应该看起来很熟悉，因为我们过去使用的是RAG链，但</st> <st c="46538">这次运行的是专家LLM混合模型。</st>****'
- en: '[PRE108]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '****<st c="46839">Just as we did</st> <st c="46854">before, we update the final
    RAG pipeline with the previous</st> <st c="46914">mixture of experts-focused</st>
    <st c="46941">RAG chain.</st>****'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '****<st c="46839">就像我们之前做的那样：</st> <st c="46854">我们更新了最终的RAG管道，使用之前的</st> <st
    c="46914">专家混合模型重点的</st> <st c="46941">RAG链。</st>****'
- en: '****<st c="46951">This code will let us see the results of the mixture of experts
    replacing the</st> <st c="47030">ChatGPT-4o-mini model:</st>****'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '****<st c="46951">此代码将让我们看到用专家混合模型替换ChatGPT-4o-mini模型的结果：</st> <st c="47030">ChatGPT-4o-mini模型：</st>****'
- en: '[PRE111]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Google''s environmental initiatives are organized around three key pillars:
    empowering individuals to take action, working together with partners and customers,
    and operating their business sustainably.'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谷歌的环境倡议围绕三个关键支柱组织：赋能个人采取行动，与合作伙伴和客户合作，以及可持续运营其业务。
- en: '[PRE122]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '1\. Empowering individuals: Google provides sustainability features like eco-friendly
    routing in Google Maps, energy efficiency features in Google Nest thermostats,
    and carbon emissions information in Google Flights. Their goal is to help individuals,
    cities, and other partners collectively reduce 1 gigaton of carbon equivalent
    emissions annually by 2030.'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. 赋能个人：谷歌在谷歌地图中提供环保路线功能，在谷歌Nest恒温器中提供节能功能，在谷歌航班中提供碳排放信息。他们的目标是帮助个人、城市和其他合作伙伴到2030年共同减少10亿吨碳当量排放。
- en: '[PRE123]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[TRUNCATED]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[TRUNCATED]'
- en: '[PRE124]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Additionally, Google advocates for strong public policy action to create low-carbon
    economies, they work with the United Nations Framework Convention on Climate Change
    (UNFCCC) and support the Paris Agreement's goal to keep global temperature rise
    well below 2°C above pre-industrial levels. They also engage with coalitions and
    sustainability initiatives like the RE-Source Platform and the Google.org Impact
    Challenge on Climate Innovation.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，谷歌倡导采取强有力的公共政策行动以创造低碳经济，他们与联合国气候变化框架公约（UNFCCC）合作，支持巴黎协定目标，即保持全球温度上升幅度远低于工业化前水平2°C。他们还与联盟和可持续性倡议如RE-Source平台和谷歌.org气候创新挑战赛合作。
- en: '[PRE125]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: Google's environmental initiatives include empowering individuals to take action,
    working together with partners and customers, operating sustainably, achieving
    net-zero carbon emissions, focusing on water stewardship, engaging in a circular
    economy, and supporting sustainable consumption of public goods. They also engage
    with suppliers to reduce energy consumption and greenhouse gas emissions, report
    environmental data, and assess environmental criteria. Google is involved in various
    sustainability initiatives, such as the iMasons Climate Accord, ReFED, and supporting
    projects with The Nature Conservancy. They also work with coalitions like the
    RE-Source Platform and the World Business Council for Sustainable Development.
    Additionally, Google invests in breakthrough innovation and collaborates with
    startups to tackle sustainability challenges. They also focus on renewable energy
    and use data analytics tools to drive more intelligent supply chains.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谷歌的环境倡议包括赋权个人采取行动、与合作伙伴和客户合作、可持续运营、实现净零碳排放、关注水资源管理、参与循环经济，以及支持公共商品的可持续消费。他们还与供应商合作以减少能源消耗和温室气体排放、报告环境数据，并评估环境标准。谷歌参与了各种可持续性倡议，如iMasons气候协议、ReFED，以及与大自然保护协会支持的项目。他们还与像RE-Source平台和世界可持续发展商业理事会这样的联盟合作。此外，谷歌投资于突破性创新，并与初创公司合作应对可持续性挑战。他们还专注于可再生能源，并使用数据分析工具推动更智能的供应链。
- en: '[PRE126]****'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE126]****'
- en: '****<st c="49763">The new responses from Llama 3 and the mixture of experts
    models show expanded responses that seem to be similar, if not more robust, compared
    to the original responses we were able to achieve with OpenAI’s</st> `<st c="49971">gpt-4o-mini</st>`
    <st c="49982">model at considerably fewer costs than OpenAI’s more expensive but
    more</st> <st c="50055">capable models.</st>'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '****<st c="49763">Llama 3和专家混合模型的新响应显示了扩展的响应，与原始响应相比似乎更相似，如果不是更稳健，而且成本比OpenAI更昂贵但更</st>
    <st c="49971">gpt-4o-mini</st> <st c="49982">模型低得多。</st>'
- en: <st c="50070">Extending the LLM capabilities</st>
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="50070">扩展LLM功能</st>
- en: <st c="50101">There are aspects of these LLM</st> <st c="50132">objects that
    can be better utilized in your RAG application.</st> <st c="50194">As described
    in the LangChain LLM documentation (</st>[<st c="50243">https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/</st>](https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/)<st
    c="50319">):</st>
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50101">这些LLM</st> <st c="50132">对象的一些方面可以在你的RAG应用程序中得到更好的利用。</st> <st
    c="50194">如LangChain LLM文档（</st>[<st c="50243">https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/</st>](https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/)<st
    c="50319">）中所述：</st>
- en: <st c="50323">All LLMs implement the Runnable interface, which comes with default
    implementations of all methods, ie.</st> <st c="50427">ainvoke, batch, abatch,
    stream, astream.</st> <st c="50468">This gives all LLMs basic support for async,
    streaming and batch.</st>
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50323">所有大型语言模型（LLM）都实现了Runnable接口，该接口提供了所有方法的默认实现，即。</st> <st c="50427">ainvoke,
    batch, abatch, stream, astream。</st> <st c="50468">这为所有LLM提供了基本的异步、流式和批量支持。</st>
- en: <st c="50533">These are key features that can significantly speed up the processing
    of your RAG application, particularly if you are processing multiple LLM calls
    at once.</st> <st c="50692">In the following subsections, we will look at the
    key methods and how they can</st> <st c="50771">help you.</st>
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50533">这些是关键特性，可以显著加快你的RAG应用程序的处理速度，尤其是当你同时处理多个LLM调用时。</st> <st c="50692">在接下来的小节中，我们将探讨关键方法以及它们如何</st>
    <st c="50771">帮助你。</st>
- en: <st c="50780">Async</st>
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="50780">异步</st>
- en: <st c="50786">By default, async support</st> <st c="50812">runs the regular
    sync method in a separate thread.</st> <st c="50864">This allows other parts of
    your async program to keep running while the language model</st> <st c="50951">is
    working.</st>
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50786">默认情况下，异步支持</st> <st c="50812">在单独的线程中运行常规同步方法。</st> <st c="50864">这允许你的异步程序的其他部分在语言模型</st>
    <st c="50951">工作时继续运行。</st>
- en: <st c="50962">Stream</st>
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="50962">流</st>
- en: '<st c="50969">Streaming support</st> <st c="50987">typically returns</st> `<st
    c="51006">Iterator</st>` <st c="51014">(or</st> `<st c="51019">AsyncIterator</st>`
    <st c="51032">for async streaming) with just one item: the final result from the
    language model.</st> <st c="51116">This doesn’t provide word-by-word streaming,
    but it ensures your code can</st> <st c="51189">work with any of the LangChain
    language model integrations that expect a stream</st> <st c="51270">of tokens.</st>'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50969">流支持</st> <st c="50987">通常返回</st> `<st c="51006">迭代器</st>` <st
    c="51014">(或</st> `<st c="51019">异步迭代器</st>` <st c="51032">用于异步流) 仅包含一个项目：语言模型最终的结果。</st>
    <st c="51116">这并不提供逐词流，但它确保你的代码可以</st> <st c="51189">与任何期望流令牌的 LangChain 语言模型集成工作。</st>
    <st c="51270">流</st>
- en: <st c="51280">Batch</st>
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="51280">批量处理</st>
- en: <st c="51286">Batch support processes</st> <st c="51310">multiple inputs at
    the same time.</st> <st c="51345">For a sync batch, it uses multiple threads.</st>
    <st c="51389">For an async batch, it uses</st> `<st c="51417">asyncio.gather</st>`<st
    c="51431">. You can control how many tasks run at once using the</st> `<st c="51486">max_concurrency</st>`
    <st c="51501">setting</st> <st c="51510">in</st> `<st c="51513">RunnableConfig</st>`<st
    c="51527">.</st>
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51286">批量支持处理</st> <st c="51310">同时处理多个输入。</st> <st c="51345">对于同步批量，它使用多个线程。</st>
    <st c="51389">对于异步批量，它使用</st> `<st c="51417">asyncio.gather</st>`<st c="51431">。您可以使用
    `<st c="51486">max_concurrency</st>` <st c="51501">设置</st> <st c="51510">在</st>
    `<st c="51513">RunnableConfig</st>`<st c="51527">中</st> 控制 simultaneously running
    tasks.
- en: <st c="51528">Not all LLMs support all of these functions natively though.</st>
    <st c="51590">For the two implementations we have discussed, as well as many more,
    LangChain provides an in-depth chart that can be found</st> <st c="51714">here:</st>
    [<st c="51720">https://python.langchain.com/v0.2/docs/integrations/llms/</st>](https://python.langchain.com/v0.2/docs/integrations/llms/)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51528">尽管如此，并非所有 LLM 都原生支持所有这些功能。</st> <st c="51590">对于我们已经讨论的两个实现以及许多其他实现，LangChain
    提供了一个深入的图表，可以在以下位置找到：</st> [<st c="51720">https://python.langchain.com/v0.2/docs/integrations/llms/</st>](https://python.langchain.com/v0.2/docs/integrations/llms/)
- en: <st c="51777">Summary</st>
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="51777">摘要</st>
- en: '<st c="51785">This chapter explored the key technical components of RAG systems
    in the context of LangChain: vector stores, retrievers, and LLMs.</st> <st c="51918">It
    provided an in-depth look at the various options available for each component
    and discussed their strengths, weaknesses, and scenarios in which one option might
    be better</st> <st c="52092">than another.</st>'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51785">本章在 LangChain 的背景下探讨了 RAG 系统的关键技术组件：向量存储、检索器和 LLM。</st> <st c="51918">它深入探讨了每个组件的各种选项，并讨论了它们的优缺点以及在某些情况下一个选项可能比另一个选项更好的场景。</st>
    <st c="52092">尽管并非所有 LLM 都原生支持所有这些功能。</st>
- en: <st c="52105">The chapter started by examining vector stores, which play a crucial
    role in efficiently storing and indexing vector representations of knowledge base
    documents.</st> <st c="52268">LangChain integrates with various vector store implementations,
    such as Pinecone, Weaviate, FAISS, and PostgreSQL with vector extensions.</st>
    <st c="52406">The choice of vector store depends on factors such as scalability,
    search performance, and deployment requirements.</st> <st c="52522">The chapter
    then moved on to discuss retrievers, which are responsible for querying the vector
    store and retrieving the most relevant documents based on the input query.</st>
    <st c="52692">LangChain offers a range of retriever implementations, including
    dense retrievers, sparse retrievers (such as BM25), and ensemble retrievers that
    combine the results of</st> <st c="52861">multiple retrievers.</st>
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52105">本章首先检查了向量存储，这在高效存储和索引知识库文档的向量表示中起着至关重要的作用。</st> <st c="52268">LangChain
    与各种向量存储实现集成，例如 Pinecone、Weaviate、FAISS 和具有向量扩展的 PostgreSQL。</st> <st c="52406">向量存储的选择取决于可扩展性、搜索性能和部署要求等因素。</st>
    <st c="52522">然后，本章转向讨论检索器，它们负责查询向量存储并根据输入查询检索最相关的文档。</st> <st c="52692">LangChain
    提供了一系列检索器实现，包括密集检索器、稀疏检索器（如 BM25）和组合多个检索器结果的集成检索器。</st>
- en: <st c="52881">Finally, the chapter covered the role of LLMs in RAG systems.</st>
    <st c="52944">LLMs contribute to RAG by providing a sophisticated understanding
    of the query context and facilitating more effective retrieval of pertinent information
    from the knowledge base.</st> <st c="53123">The chapter showcased the integration
    of LangChain with various LLM providers, such as OpenAI and Together AI, and highlighted
    the capabilities and cost considerations of different models.</st> <st c="53312">It
    also discussed the extended capabilities of LLMs in LangChain, such as async,
    streaming, and batch support, and provided a comparison of the native implementations
    offered by different</st> <st c="53500">LLM integrations.</st>
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52881">最后，本章讨论了LLMs在RAG系统中的作用。</st> <st c="52944">LLMs通过提供对查询上下文的深入理解，并促进从知识库中更有效地检索相关信息，从而为RAG做出贡献。</st>
    <st c="53123">本章展示了LangChain与各种LLM提供商（如OpenAI和Together AI）的集成，并强调了不同模型的性能和成本考虑。</st>
    <st c="53312">它还讨论了LLMs在LangChain中的扩展功能，如异步、流式和批量支持，并提供了不同LLM集成提供的原生实现比较。</st>
    <st c="53500">LLM集成。</st>
- en: <st c="53517">In the next chapter, we will continue to talk about how LangChain
    can be utilized to build a capable RAG application, focusing now on the smaller
    components that can be used in support of the key components we just discussed
    in</st> <st c="53746">this chapter.</st>****
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53517">在下一章中，我们将继续讨论如何利用LangChain构建一个功能强大的RAG应用程序，现在重点关注可以支持我们刚才在本章中讨论的关键组件的较小组件。</st>
    <st c="53746">这一章。</st>****
