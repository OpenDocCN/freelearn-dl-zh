- en: '*Chapter 16*: Marketing, Personalization and Finance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第16章*：营销、个性化和金融'
- en: In this chapter, we discuss three areas in which RL (RL) is gaining significant
    traction. First, we describe how it can be used in personalization and recommendation
    systems. With that, we go beyond the single-step bandit approaches we covered
    in the earlier chapters. A related field that can also significantly benefit from
    RL is marketing. In addition to personalized marketing applications, RL can help
    in areas such as managing campaign budgets and reducing customer churn. Finally,
    we discuss the promise of RL in finance and related challenges. In doing so, we
    introduce TensorTrade, a Python framework for developing and testing RL-based
    trading algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了强化学习（RL）在三个领域中获得显著关注的情况。首先，我们描述了它如何用于个性化和推荐系统。通过这一点，我们超越了之前章节中介绍的单步强盗方法。一个相关领域也可以从RL中受益匪浅，那就是营销。除了个性化营销应用，RL还可以帮助管理活动预算和减少客户流失等领域。最后，我们讨论了RL在金融领域的前景和相关挑战。在这个过程中，我们介绍了TensorTrade，这是一个用于开发和测试基于RL的交易算法的Python框架。
- en: 'So, in this chapter, we cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将涵盖以下内容：
- en: Going beyond bandits for personalization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越强盗模型进行个性化
- en: Developing effective marketing strategies using RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用强化学习（RL）开发有效的营销策略
- en: Applying RL in finance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RL应用于金融
- en: Going beyond bandits for personalization
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越强盗模型进行个性化
- en: When we covered multi-armed and contextual bandit problems in the early chapters
    of the book, we presented a case study aimed at maximizing the **click-through
    rate (CTR)** of online ads. This is just one example of how bandit models can
    be used to provide users with personalized content and experience, a common challenge
    of almost all online (and offline) content providers, from e-retailers to social
    media platforms. In this section, we go beyond the bandit models and describe
    a multi-step RL approach to personalization. Let's first start by discussing where
    the bandit models fall short, and then how multi-step RL can address those issues.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本书的早期章节中讨论多臂强盗和上下文强盗问题时，我们提出了一个案例研究，旨在最大化**点击率（CTR）**的在线广告。这只是一个例子，展示了强盗模型如何用于为用户提供个性化内容和体验，这是几乎所有在线（和离线）内容提供商面临的共同挑战，从电子零售商到社交媒体平台。在这一部分，我们将超越强盗模型，描述一个多步强化学习方法来进行个性化。让我们首先讨论强盗模型的不足之处，然后再看看多步RL如何解决这些问题。
- en: Shortcomings of bandit models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强盗模型的缺点
- en: 'The goal in bandit problems is to maximize the immediate (single-step) return.
    In an online ad CTR maximization problem, this is usually a good way of thinking
    about the goal: an online ad is displayed, the user clicks, and voila! If not,
    it''s a miss.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强盗问题的目标是最大化即时（单步）回报。在一个在线广告点击率（CTR）最大化问题中，这通常是一个很好的思考目标：展示在线广告，用户点击，然后完成！如果没有点击，那就是错失。
- en: The relationship between the user and, let's say, YouTube or Amazon, is much
    more sophisticated than this. The user experience on such platforms is a journey,
    rather than an event. The platform recommends some content, and it is not a total
    miss if the user does not click on it. It could be the case that the user finds
    the presented content interesting and enticing and just continues browsing. Even
    if that user session does not result in a click or conversion, the user may come
    back soon knowing that the platform is serving some content relevant to the user's
    interests. Conversely, too many clicks in a session could very well mean that
    the user is not able to find what they are looking for, leading to a bad experience.
    This "journey-like" nature of the problem, the fact that the platform (agent)
    decisions have a downstream impact on the customer satisfaction and business value
    (reward), is what makes multi-step RL an appealing approach here.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 用户与平台之间的关系，比方说，YouTube或Amazon，远比这更复杂。在这些平台上的用户体验是一个过程，而不是一个事件。平台推荐一些内容，如果用户没有点击，也不完全是失败。可能用户发现了推荐的内容有趣且吸引人，只是继续浏览。即使该用户会话没有产生点击或转化，用户也可能很快回来，因为他们知道平台会推荐一些与其兴趣相关的内容。相反，如果会话中有太多的点击，很可能意味着用户没有找到他们想要的内容，从而导致了不好的体验。这种“过程性”的问题特性——即平台（代理）的决策对客户满意度和商业价值（奖励）有下游影响——正是使得多步强化学习成为这里吸引人的方法。
- en: Tip
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Although it is tempting to depart from a bandit model towards multi-step RL,
    think twice before doing so. Bandit algorithms are much easier to use and have
    well-understood theoretical properties, whereas it could be quite challenging
    to successfully train an agent in a multi-step RL setting. Note that many multi-step
    problems can be cast as a single-step problem by including a memory for the agent
    in the context and the expected future value of the action in the immediate reward,
    which then allows us to stay in the bandit framework.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从赌博模型转向多步强化学习是诱人的，但在这么做之前要三思而后行。赌博算法更容易使用，并且具有公认的理论特性，而在多步强化学习的设定中成功训练代理人可能会非常具有挑战性。需要注意的是，许多多步问题可以通过为代理人包含记忆并在当前奖励中考虑动作的期望未来价值，将其转化为单步问题，这样我们就可以留在赌博框架中。
- en: One of the successful implementations of multi-step deep RL for personalization
    is related to news recommendation, proposed by (Zheng et al. 2018). In the next
    section, we describe a similar approach to the problem inspired by this work,
    although our discussion will be at a higher level and broader than what the paper
    suggests.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习在个性化方面的一个成功实现与新闻推荐相关，由郑等人（2018）提出。在下一节中，我们将描述一种类似的解决方法，灵感来自这项工作，尽管我们的讨论会比论文中的内容更高层次和更广泛。
- en: Deep RL for news recommendation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习在新闻推荐中的应用
- en: 'When we go on our favorite news app, we expect to read some interesting and
    perhaps important content. Of course, what makes news interesting or important
    is different for everyone, so we have a personalization problem. As Zheng et al.
    (2018) mention, there are some additional challenges in news recommendation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打开最喜欢的新闻应用时，我们期待阅读一些有趣且可能重要的内容。当然，什么内容是有趣或重要的对每个人来说都不同，因此我们面临个性化问题。正如郑等人（2018）提到的，新闻推荐中有一些额外的挑战：
- en: 'The action space is not fixed. In fact, it is quite the opposite: There is
    so much news flowing in during a day, each with some unique characteristics, making
    it hard to think about the problem like a traditional Gym environment.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作空间并非固定的。实际上，情况恰恰相反：每天涌入大量新闻，每条新闻都有一些独特的特征，这使得很难将问题看作一个传统的Gym环境。
- en: 'User preferences are quite dynamic too: They change and evolve over time. A
    user who is more into politics this week can get bored and read about art next
    week, which is what Zheng et al. (2018) demonstrate with data.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户偏好也是相当动态的：它们随着时间变化而改变和演变。本周对政治感兴趣的用户，下周可能会感到厌倦，转而阅读关于艺术的内容，郑等人（2018）通过数据证明了这一点。
- en: As we mentioned, this is a truly multi-step problem. If there are two pieces
    of news that can be displayed, one about a disaster and the other about a sports
    game, showing the former could have a higher chance of getting clicked due to
    its sensational nature. It could also lead to the user leaving the platform early
    as their morale decreases, preventing more engagement on the platform.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们所提到的，这是一个真正的多步问题。如果有两条新闻可以展示，一条是关于灾难的，另一条是关于体育比赛的，展示前者可能因为其轰动效应而更容易被点击。然而，这也可能导致用户因情绪低落而提前离开平台，从而减少更多平台上的互动。
- en: What the agent observes about the user is so limited compared to all possible
    factors playing a role in the user's behavior. Therefore, the environment is partially
    observable.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理人对用户的观察相对于可能影响用户行为的所有因素来说是非常有限的。因此，环境是部分可观察的。
- en: So, the problem is to choose which news piece(s) from a dynamic inventory to
    display to a user whose interests, one, change over time, and two, are impacted
    by many factors that the agent does not fully observe.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题在于从动态库存中选择哪些新闻展示给用户，用户的兴趣一方面随着时间变化，另一方面受到许多代理人未完全观察到的因素的影响。
- en: Let's next describe the components of the RL problem here.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们描述一下强化学习问题的组成部分。
- en: The observation and action space
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察和动作空间
- en: 'Borrowing from Zheng et al. (2018), there are the following pieces of information
    that make up the observation and action space about a particular user and a news
    piece:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴郑等人（2018）的研究，关于特定用户和新闻内容的观察和动作空间包括以下信息：
- en: '**User features** related to the features of all the news pieces the user clicked
    in that session, that day, in the past week, how many times the user has come
    to the platform over various time horizons, and so on.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户特征**，与用户在该会话中、当天、过去一周点击的所有新闻的特征相关，用户在不同时间段内访问平台的次数等等。'
- en: '**Context features** are related to the information about the time of the day,
    the day of the week, whether it is a holiday, election day, and so on.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文特征**与一天中的时间、星期几、是否是节假日、选举日等信息相关。'
- en: '**User-news features** about how many times this particular news piece appeared
    in the feed of the particular user over the past hour, and similar statistics
    related to the entities, topics, and categories in the news.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户-新闻特征**，例如这篇特定新闻在过去一小时内出现在特定用户的新闻流中的次数，以及与新闻中的实体、主题和类别相关的类似统计数据。'
- en: '**News features** of this particular piece such as topic, category, provider,
    entity names, click counts over the past hour, the past 6 hours, past day, and
    so on.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇新闻的**新闻特征**，如主题、类别、提供者、实体名称、过去一小时、过去6小时、过去一天的点击次数等。
- en: 'Now, this makes up a different observation-action space than what we are used
    to:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这构成了一个与我们通常使用的观察-动作空间不同的空间：
- en: 'The first two sets of features are more like an observation: The user shows
    up, requests news content, and the agent observes the user and context-related
    features.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两组特征更像是一种观察：用户出现，请求新闻内容，代理观察用户和与上下文相关的特征。
- en: Then the agent needs to pick a news piece to display (or a set of news pieces,
    as in the paper). Therefore, the features related to the news and user-news correspond
    to an action.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，代理需要选择一个新闻条目进行展示（或者如论文所示，选择一组新闻条目）。因此，新闻相关的特征和用户-新闻特征对应着一个动作。
- en: What is interesting is that the available action set is dynamic, and it carries
    elements related to the user/observation (in user-news features).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣的是，可用的动作集是动态的，并且它包含了与用户/观察相关的元素（在用户-新闻特征中）。
- en: 'Although this is not a traditional setup, don''t let that scare you. We can
    still estimate, for example, the Q-value of a given observation-action pair: We
    simply need to feed all these features to a neural network that estimates the
    Q-value. To be specific, the paper uses two neural networks, one estimating the
    state-value and the other the advantage, to calculate the Q-value, although a
    single network can also be used. This is depicted in Figure 16.1:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这不是一种传统的设置，但不要让它吓到你。我们仍然可以估算，例如，给定观察-动作对的Q值：我们只需要将所有这些特征输入到一个神经网络中，该网络估算Q值。具体来说，论文使用了两个神经网络，一个估算状态值，另一个估算优势，以计算Q值，尽管也可以使用一个单独的网络。这个过程在图16.1中有说明：
- en: '![Figure 16.1 – Q-network of a news recommendation agent'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图16.1 – 新闻推荐代理的Q网络'
- en: '](img/B14160_16_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_16_01.jpg)'
- en: Figure 16.1 – Q-network of a news recommendation agent
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 – 新闻推荐代理的Q网络
- en: 'Now, compare this to a traditional Q-network:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，比较一下传统的Q网络：
- en: A regular Q-network would have multiple heads, one for each action (and plus
    one for the value estimation if the action heads are estimating advantages rather
    than Q values). Such a network outputs Q-value estimates for all of the actions
    in a fixed set for a given observation in a single forward pass.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常规的Q网络会有多个头，每个头对应一个动作（如果动作头估算的是优势而不是Q值，还会有一个额外的头用于价值估算）。这样的网络在一次前向传播中输出给定观察的所有固定动作集的Q值估算。
- en: In this setup, we need to make a separate forward pass over the network for
    each available news piece given the user, and then pick the one(s) with highest
    Q values.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种设置下，我们需要对每个可用的新闻条目进行单独的前向传播，基于用户的输入，然后选择具有最高Q值的新闻条目。
- en: This approach is actually similar to what we used in [*Chapter 3*](B14160_03_Final_SK_ePub.xhtml#_idTextAnchor059),
    *Contextual Bandits*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法实际上类似于我们在[*第3章*](B14160_03_Final_SK_ePub.xhtml#_idTextAnchor059)中使用的，*上下文型强盗*。
- en: Next, let's discuss an alternative modeling approach that we could use in this
    setup.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来讨论在这种设置中可以使用的另一种建模方法。
- en: Using action embeddings
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用动作嵌入
- en: When the action space is very large and/or it varies each time step, like in
    this problem, **action embeddings** can be used to select an action given an observation.
    An action embedding is a representation of the action as a fixed-sized array,
    usually obtained as an outcome of a neural network.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当动作空间非常大和/或在每个时间步骤中有所变化时，就像这个问题中那样，**动作嵌入**可以用来根据观察选择一个动作。动作嵌入是一个表示动作的固定大小数组，通常通过神经网络的输出得到。
- en: 'Here is how it works:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其工作原理如下：
- en: We use a policy network that outputs, rather than action values, an **intention
    vector**, a fixed-size array of numbers.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个策略网络，它输出的不是动作值，而是**意图向量**，这是一个固定大小的数字数组。
- en: The intention vector carries information about what the ideal action would look
    like given the observation.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意图向量携带关于在给定观察情况下理想动作的相关信息。
- en: 'In the context of the news recommendation problem, such an intention vector
    would mean something like: "Given these user and context features, this user wants
    to read about team sports from international leagues that are in finals."'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在新闻推荐问题中，这样的意图向量可能意味着：“根据这些用户和上下文特征，该用户想阅读来自国际联赛、正在进行决赛的团队运动新闻。”
- en: This intention vector is then compared to the available actions. The action
    that is "closest" to the "intention" is chosen. For example, a news piece about
    team sports in a foreign league, but not playing the finals.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，将这个意图向量与可用的动作进行比较。选择与“意图”最“接近”的动作。例如，一篇关于外国联赛团队运动的新闻，但不是正在进行决赛的新闻。
- en: A measure of the closeness is cosine similarity, which is the dot product of
    an intention and action embedding.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度的衡量标准是余弦相似度，它是意图和动作嵌入的点积。
- en: 'This setup is illustrated in Figure 16.2:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置如图 16.2 所示：
- en: '![Figure 16.2 – Using action embeddings for news recommendation'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 16.2 – 使用动作嵌入进行新闻推荐](img/B14160_16_02.jpg)'
- en: '](img/B14160_16_02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_16_02.jpg)'
- en: Figure 16.2 – Using action embeddings for news recommendation
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2 – 使用动作嵌入进行新闻推荐
- en: Info
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The use of embeddings in this way is something OpenAI introduced to deal with
    the huge action space in Dota 2\. A nice blog explaining their approach is at
    [https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/](https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/).
    This can also be implemented fairly easily in RLlib, which is explained at [https://bit.ly/2HQRkfx](https://bit.ly/2HQRkfx).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种使用嵌入的方法是 OpenAI 为了应对 Dota 2 中巨大的动作空间而引入的。一篇很好的博客解释了他们的方法，链接在这里：[https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/](https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/)。这种方法也可以很容易地在
    RLlib 中实现，具体内容请见：[https://bit.ly/2HQRkfx](https://bit.ly/2HQRkfx)。
- en: Next, let's discuss what the reward function looks like in the news recommendation
    problem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来讨论在新闻推荐问题中，奖励函数是什么样的。
- en: The reward function
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励函数
- en: In the online ad problem that we solved at the beginning of the book, maximizing
    the CTR was the only objective. In the news recommendation problem, we would like
    to maximize the long-term engagement with the user, while also increasing the
    likelihood of immediate clicks. This requires a measure for user activeness, for
    which the paper uses a survival model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在书本开始部分解决的在线广告问题中，最大化点击率（CTR）是唯一目标。而在新闻推荐问题中，我们希望最大化与用户的长期互动，同时也提高即时点击的可能性。这就需要一个衡量用户活跃度的标准，论文中使用了生存模型。
- en: Exploration using dueling bandit gradient descent
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用对抗式乐队梯度下降进行探索
- en: Exploration is an essential component of RL. When we train RL agents in simulation,
    we don't care about taking bad actions for the sake of learning, except it wastes
    some of the computation budget. If the RL agent is trained in a real environment,
    as it usually is in a setting such as news recommendation, then exploration will
    have consequences beyond computational inefficiency and could harm user satisfaction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 探索是强化学习（RL）的一个基本组成部分。当我们在模拟中训练 RL 智能体时，尽管我们不介意为了学习而采取不好的行动，只是这会浪费一些计算资源。但如果
    RL 智能体是在真实环境中训练的，例如新闻推荐这种场景，那么探索的后果不仅仅是计算效率低下，它可能还会损害用户满意度。
- en: If you think about the common ![](img/Formula_05_272.png)-greedy approach, during
    exploration, it takes an action uniformly at random over the entire action space,
    even though we know that some of the actions are really bad. For example, even
    though the agent knows that a reader is mainly interested in politics, it will
    randomly display news pieces about beauty, sports, celebrities, and so on, which
    the reader might find totally irrelevant.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑常见的ε-greedy方法，在探索过程中，它会从整个动作空间中随机选择一个动作，尽管我们知道有些动作其实是很糟糕的。例如，尽管智能体知道读者主要对政治感兴趣，它仍会随机显示一些关于美妆、体育、名人等的新闻，这些对读者来说可能完全没有关联。
- en: 'One way of overcoming this issue is to incrementally deviate from the greedy
    action to explore and update the policy if the agent receives a good reward after
    an exploratory action. Here is how we could do it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这个问题的一种方法是逐步偏离贪心策略进行探索，并在智能体通过探索性行动获得好奖励时更新策略。以下是我们可以如何操作：
- en: In addition to the regular ![](img/Formula_16_002.png) network, we use an **explore
    network** ![](img/Formula_16_003.png) to generate exploratory actions.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了常规的![](img/Formula_16_002.png)网络外，我们还使用**探索网络**![](img/Formula_16_003.png)来生成探索性动作。
- en: The weights of ![](img/Formula_16_004.png), denoted by ![](img/Formula_16_005.png),
    is obtained by perturbing the weights of ![](img/Formula_16_006.png), denoted
    by ![](img/Formula_16_007.png).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_16_004.png)的权重，由![](img/Formula_16_005.png)表示，通过扰动![](img/Formula_16_006.png)的权重得到，![](img/Formula_16_007.png)表示。'
- en: More formally, ![](img/Formula_16_008.png), where ![](img/Formula_16_009.png)
    is some coefficient to control the trade-off between exploration and exploitation,
    and ![](img/Formula_16_010.png) generates a random number between the inputs.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更正式地说，![](img/Formula_16_008.png)，其中![](img/Formula_16_009.png)是一个系数，用来控制探索与开发之间的权衡，而![](img/Formula_16_010.png)生成一个介于输入之间的随机数。
- en: To obtain a set of news pieces to display to a user, we first generate two sets
    of recommendations from ![](img/Formula_16_011.png) and ![](img/Formula_16_012.png),
    one from each, and then randomly select pieces from both to add to the display
    set.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获得一组展示给用户的新闻，我们首先从![](img/Formula_16_011.png)和![](img/Formula_16_012.png)生成两组推荐，每组一个，然后从两者中随机选择一些项来添加到展示集中。
- en: Once the display set is shown to the user, the agent collects the user feedback
    (reward). If the feedback for the items generated by ![](img/Formula_16_013.png)
    is better, ![](img/Formula_16_014.png) does not change. Otherwise, the weights
    are updated ![](img/Formula_16_015.png), where ![](img/Formula_16_016.png) is
    some step size.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦展示集展示给用户，代理就会收集用户反馈（奖励）。如果由![](img/Formula_16_013.png)生成的项的反馈更好，![](img/Formula_16_014.png)则不变化。否则，权重会更新为![](img/Formula_16_015.png)，其中![](img/Formula_16_016.png)是某个步长。
- en: Finally, let's discuss how this model can be trained and deployed to get effective
    outcomes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论一下如何训练和部署该模型以获得有效的结果。
- en: Model training and deployment
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练与部署
- en: We already mentioned the dynamic characteristics of user behavior and preferences.
    Zheng et al. (2018) overcome this by training and updating the model frequently
    throughout the day, so that the model captures the latest dynamics in the environment.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到用户行为和偏好的动态特性。Zheng等人（2018年）通过全天频繁训练和更新模型来克服这一问题，从而使得模型能够捕捉到环境中的最新动态。
- en: This concludes our discussion on personalization applications of RL, particularly
    in a news recommendation setting. Next, we shift gears towards a related area,
    marketing, which can also benefit from personalization, and do more using RL.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们关于RL个性化应用的讨论，特别是在新闻推荐环境中的应用。接下来，我们转向一个相关领域——市场营销，它也可以通过个性化受益，并通过RL做得更多。
- en: Developing effective marketing strategies using RL
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RL制定有效的市场营销策略
- en: RL can significantly improve marketing strategies in multiple areas. Let's now
    talk about some of them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: RL可以显著改善多个领域的市场营销策略。现在我们来谈谈其中的一些。
- en: Personalized marketing content
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个性化营销内容
- en: In relation to the previous section, there is always room for more personalization
    in marketing. Rather than sending the same email or flier to all customers, or
    having rough customer segmentation, RL can help to determine the best sequence
    of personalized marketing content to communicate to customers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节相关，市场营销中总是有更多个性化的空间。与其向所有客户发送相同的电子邮件或传单，或进行粗略的客户细分，不如利用RL来帮助确定向客户传达个性化营销内容的最佳顺序。
- en: Marketing resource allocation for customer acquisition
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 客户获取的市场营销资源分配
- en: Marketing departments often make decisions about where to spend the budget based
    on subjective judgment and/or simple models. RL can actually come up with pretty
    dynamic policies to allocate marketing resources while leveraging the information
    about the product, responses from different marketing channels, and context information
    such as the time of year and so on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 市场营销部门通常根据主观判断和/或简单模型来决定如何分配预算。RL实际上能够提出非常动态的政策来分配市场营销资源，同时利用有关产品的信息、来自不同营销渠道的反馈，以及季节性等上下文信息。
- en: Reducing the customer churn rate
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降低客户流失率
- en: Retailers have long studied predictive models to identify customers that are
    about to be lost. After they are identified, usually, discount coupons, promotion
    items, and so on are sent to the customer. But the sequence of taking such actions,
    given the type of the customer and the responses from the earlier actions, is
    underexplored. RL can effectively evaluate the value of each of these actions
    and reduce customer churn.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 零售商长期以来一直研究预测模型，以识别即将流失的客户。在识别后，通常会向客户发送折扣券、促销商品等。但考虑到客户类型及先前行动的反应，采取这些行动的顺序还没有得到充分探讨。强化学习可以有效评估这些行动的价值，减少客户流失。
- en: Winning back lost customers
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新赢回流失的客户
- en: If you ever subscribed to The Economist magazine, and then made the mistake
    of quitting your subscription, you probably received many phone calls, mail, emails,
    notifications, and so on. One cannot help but wonder whether such spamming is
    the best approach. It is probably not. An RL-based approach, on the other hand,
    can help determine which channels to use and when, along with the accompanying
    perks of maximizing the chances of winning the customers back while minimizing
    the cost of these efforts.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经订阅过《经济学人》杂志，然后做了取消订阅的错误决定，你可能收到了很多电话、邮件、通知等等。人们不禁会想，这种垃圾邮件式的做法真的是最佳方法吗？可能不是。而基于强化学习的方法，则可以帮助确定使用哪些渠道、何时使用，并通过最大化客户回流的机会，同时最小化这些努力的成本，来获得相应的好处。
- en: This list could go on. I suggest you take a moment or two to think about what
    marketing behavior of the companies you are a customer of you find disturbing,
    ineffective, irrelevant, and how RL could help with that.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表还可以继续下去。我建议你花一两分钟思考一下，作为某公司的客户，你觉得哪些营销行为令人不安、无效或无关紧要，并思考强化学习如何能够帮助改进这些行为。
- en: 'Next, our final area for this chapter: finance.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们本章的最后一个领域：金融。
- en: Applying RL in finance
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在金融中应用强化学习
- en: 'If we need to reiterate RL''s promise, it is to obtain policies for sequential
    decision making to maximize rewards under uncertainty. What is a better match
    than finance for such a tool! In finance, the following are true:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要重申强化学习的优势，那就是通过获取用于顺序决策的策略，在不确定性下最大化回报。金融领域岂不是与这种工具的最佳匹配？在金融中，以下几项是成立的：
- en: The goal is very much to maximize some monetary reward.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标确实是最大化某种货币奖励。
- en: Decisions made now will definitely have consequences down the road.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前做出的决策必定会在未来产生影响。
- en: Uncertainty is a defining factor.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性是一个决定性因素。
- en: As a result, RL is getting increasingly popular in the finance community.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强化学习在金融社区中变得越来越受欢迎。
- en: 'To be clear, this section will not include any examples of a winning trading
    strategy, well, for obvious reasons: First, the author does not know any; second,
    even if he did, he would not include them in a book (and no one would). In addition,
    there are challenges when it comes to using RL in finance. So, we start this section
    with a discussion on these challenges. Once we ground ourselves in reality, we
    will proceed to define some application areas and introduce some tools you can
    use in this area.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，这一部分将不会包含任何成功的交易策略示例，嗯，显然有两个原因：首先，作者并不知晓任何成功策略；其次，即使知道，也不会在书中提及（也没有人会这么做）。此外，在金融中使用强化学习也存在一些挑战。因此，我们将从讨论这些挑战开始。一旦我们站稳脚跟，就会继续定义一些应用领域，并介绍一些可以在该领域使用的工具。
- en: Challenges with using RL in finance
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在金融中使用强化学习（RL）面临的挑战
- en: How much time did it take for you to get a decent score in a video game when
    you first played it? An hour? Two? In any case, it is a tiny fraction of the experience
    that an RL agent would need, millions of game frames, if not billions, to reach
    that level. This is because the gradients an RL agent obtains in such environments
    are too noisy to quickly learn from for the algorithms we are using. And video
    games are not that difficult after all.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次玩一款视频游戏时，花了多长时间才能获得不错的成绩？一个小时？两个小时？无论如何，这只是强化学习智能体所需经验的微不足道一部分，如果不是数亿次游戏帧的话，可能需要数百万次。因为强化学习智能体在这样的环境中获得的梯度噪声太大，导致我们所使用的算法无法快速学习。而且，毕竟视频游戏并不那么难。
- en: Have you ever wondered what it takes to become a successful trader on the stock
    market? Years of financial experience, perhaps a Ph.D. in physics or math? Even
    then, it is difficult for a trader to beat market performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾想过，成为股市成功交易员需要具备什么条件？多年金融经验，或许还有物理学或数学的博士学位？即便如此，交易员也很难击败市场表现。
- en: 'This was perhaps too long of a statement to convince you that it is difficult
    to make money by trading equities for the following reasons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个过于冗长的声明，旨在让你相信通过股市交易赚钱很困难，原因如下：
- en: Financial markets are highly efficient (although not perfectly), and it is very
    difficult, if not practically impossible, to predict the market. In other words,
    the **signal** is very weak in the financial data, and it is almost completely
    **noise**.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金融市场是高度高效的（尽管不是完美的），并且预测市场是非常困难的，甚至几乎不可能。换句话说，**信号**在金融数据中非常微弱，几乎完全是**噪声**。
- en: Financial markets are dynamic. A trading strategy that is profitable today may
    not last long as others may discover the same strategy and trade on it. For example,
    if people knew that Bitcoin would trade at $100K at the first solar eclipse, the
    price would jump to that level now as people would not wait until that day to
    buy it.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金融市场是动态的。今天有利可图的交易策略可能不会持续太久，因为其他人可能会发现相同的策略并进行交易。例如，如果人们知道比特币会在第一次日全食时交易到 10
    万美元，那么价格现在就会跳到那个水平，因为人们不会等到那天才购买它。
- en: Unlike video games, financial markets are real-word processes. So, we need to
    create a simulation of them to be able to collect the huge amounts of data needed
    to train an RL agent for an environment much noisier than a video game. Remember
    that all simulations are imperfect, and what the agent learns is likely to be
    the quirks of the simulation model rather than a real signal, which won't be useful
    outside of the simulation.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与视频游戏不同，金融市场是现实世界中的过程。因此，我们需要创建一个模拟环境，以便能够收集大量数据，训练一个比视频游戏环境更嘈杂的 RL 代理。记住，所有模拟都是不完美的，代理学到的内容很可能是模拟模型的独特性，而不是实际信号，这些内容在模拟之外不会有用。
- en: The real-world data is big enough to easily detect a low signal in it.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界的数据足够大，可以轻松地检测到其中微弱的信号。
- en: So, this was a long, discouraging, yet necessary disclaimer we needed to put
    out there to let you know that it is far from low-hanging-fruit to train a trader
    agent. Having said that, RL is a tool, a powerful one, and its effective use is
    up to the capabilities of its user.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是一个冗长的、令人沮丧但必要的免责声明，我们需要告诉你，训练一个交易代理远不是轻而易举的事。话虽如此，RL 是一种工具，一种强大的工具，它的有效使用取决于用户的能力。
- en: Now, it is time to induce some optimism. There are some cool open source libraries
    out there for you to work towards creating your trader agent, and TensorTrade
    is one candidate. It could be helpful for educational purposes and to strategize
    some trading ideas before going into some more custom tooling.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候引发一些乐观情绪了。市面上有一些很酷的开源库，供你创建交易代理，而 TensorTrade 是其中之一。它可能对教育用途有所帮助，并且可以在进入更定制的工具之前，帮助你策划一些交易思路。
- en: Introducing TensorTrade
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 TensorTrade
- en: TensorTrade is designed to easily compose Gym-like environments for trading
    equities. It allows the user to define and glue together various kinds of data
    streams, feature extractions for observations, action spaces, reward structures,
    and other convenient utilities you might need to train a trader agent. Since the
    environment follows the Gym API, it can be easily hooked up with RL libraries,
    such as RLlib.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TensorTrade 旨在轻松构建类似 Gym 的股市交易环境。它允许用户定义并连接各种数据流、观察特征提取、动作空间、奖励结构以及训练交易代理所需的其他便捷工具。由于环境遵循
    Gym API，它可以轻松地与 RL 库（如 RLlib）连接。
- en: In this section, we give a quick introduction to TensorTrade and defer the details
    of what you can do with it to the documentation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍 TensorTrade，并将其详细用途留给文档进行说明。
- en: Info
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: You can find the TensorTrade documentation at [https://www.tensortrade.org/](https://www.tensortrade.org/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://www.tensortrade.org/](https://www.tensortrade.org/) 找到 TensorTrade
    的文档。
- en: Let's start with installation and then put together some TensorTrade components
    to create an environment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装开始，然后将一些 TensorTrade 组件组合起来创建一个环境。
- en: Installing TensorTrade
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 TensorTrade
- en: 'TensorTrade can be installed with a simple `pip` command as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TensorTrade 可以通过以下简单的 `pip` 命令进行安装：
- en: '[PRE0]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you would like to create a virtual environment to install TensorTrade in,
    don't forget to also install Ray RLlib.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个虚拟环境来安装 TensorTrade，请别忘了也安装 Ray RLlib。
- en: TensorTrade concepts and components
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorTrade 的概念和组件
- en: As we mentioned, TensorTrade environments can be composed of highly modular
    components. Let's put together a basic environment here.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，TensorTrade 环境可以由高度模块化的组件组成。让我们在这里搭建一个基本环境。
- en: Instrument
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工具
- en: 'Instruments in finance are assets that can be traded. We can define the `U.S.
    Dollar` and `TensorTrade Coin` instruments as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 金融中的工具是可以交易的资产。我们可以如下定义`美元`和`TensorTrade Coin`工具：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding integer arguments represent the precision of the quantities for
    those instruments.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的整数参数表示这些工具的数量精度。
- en: Stream
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流
- en: 'A stream simply refers to an object that streams data, such as price data from
    a market simulation. For example, we can create a simple sinusoidal USD-TTC price
    stream as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 流指的是一个传输数据的对象，例如来自市场模拟的价格数据。例如，我们可以创建一个简单的正弦波USD-TTC价格流，如下所示：
- en: '[PRE2]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Exchange and data feed
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交易所和数据流
- en: 'Next, we need to create an exchange to trade these instruments in. We put the
    price stream we just created in the exchange:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个交易所来交易这些工具。我们将刚刚创建的价格流放入交易所：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have a price stream defined, we can also define transformations
    for it and extract some features and indicators. All such features will also be
    streams and they will be all packed in a data feed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了价格流，我们还可以为其定义转换，并提取一些特征和指标。所有这些特征也将是流，它们将全部打包成数据流：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you are wondering what the last line is doing, it is the log ratio of prices
    in two consecutive time steps to assess the relative change.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想最后一行在做什么，它是两个连续时间步价格的对数比，用于评估相对变化。
- en: Wallet and portfolio
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 钱包和投资组合
- en: 'Now, it''s time to create some wealth for ourselves and put it in our wallets.
    Feel free to be generous to yourself:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候为自己创造一些财富并把它放进我们的钱包了。随意对自己慷慨一些：
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our wallets together make up our portfolio.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的钱包共同构成了我们的投资组合。
- en: Reward scheme
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励方案
- en: 'The reward scheme is simply the kind of reward function we want to incentivize
    our agent with. If you are thinking "there is only one goal, make profit!", well,
    there is more to it. You can use things like risk-adjusted returns or define your
    own goal. For now, let''s keep things simple and use the profit as the reward:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励方案就是我们希望用来激励代理的奖励函数。如果你在想“目标只有一个，赚取利润！”，其实还有更多内容。你可以使用类似风险调整回报的东西，或者定义你自己的目标。现在，让我们保持简单，使用利润作为奖励：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Action scheme
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作方案
- en: 'The action scheme defines the type of actions you want your agent to be able
    to take, such as a simple **buy**/**sell**/**hold** all assets (**BSH**), or fractional
    buys/sells and so on. We also put the cash and assets in it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 动作方案定义了你希望代理能够执行的动作类型，例如简单的**买入**/**卖出**/**持有**所有资产（**BSH**），或者是分数买入/卖出等。我们还将现金和资产放入其中：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Putting them all together in an environment
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将它们全部放在一个环境中
- en: 'Finally, these can be all put together to form an environment with some additional
    parameters:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些可以全部组合起来，形成一个具有一些附加参数的环境：
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So, that's the basics of creating an environment in TensorTrade. You can of
    course go much beyond this, but you get the idea. After this step, it can be easily
    plugged into RLlib, or virtually any library compatible with the Gym API.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在TensorTrade中创建环境的基本知识。你当然可以做得远不止这些，但你已经明白了大致的思路。完成这一步后，它可以轻松地插入RLlib，或者几乎任何与Gym
    API兼容的库。
- en: Training RLlib agents in TensorTrade
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在TensorTrade中训练RLlib代理
- en: 'As you may remember from the earlier chapters, one way of using custom environments
    in Gym is to put them in a function that returns the environment and register
    them. So, it looks something like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从前几章记得的那样，使用Gym中的自定义环境的一种方式是将它们放入一个返回环境的函数中并注册它们。所以，它看起来像这样：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The environment name can then be referred to inside the trainer config in RLlib.
    You can find the full code in `Chapter16/tt_example.py` on the GitHub repo.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 环境名称可以在RLlib的训练配置中引用。你可以在GitHub仓库的`Chapter16/tt_example.py`中找到完整的代码。
- en: Info
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: This example mostly follows what is in the TensorTrade documentation. For a
    more detailed Ray/RLlib tutorial, you can visit [https://www.tensortrade.org/en/latest/tutorials/ray.html](https://www.tensortrade.org/en/latest/tutorials/ray.html).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例主要遵循TensorTrade文档中的内容。要获取更详细的Ray/RLlib教程，你可以访问[https://www.tensortrade.org/en/latest/tutorials/ray.html](https://www.tensortrade.org/en/latest/tutorials/ray.html)。
- en: This concludes our discussion on TensorTrade. You can now go ahead and try a
    few trading ideas. When you are back, let's briefly talk about developing machine
    learning-based trading strategies and wrap up the chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于TensorTrade的讨论总结。现在你可以继续尝试一些交易想法。当你回来时，我们简要谈谈如何开发基于机器学习的交易策略，并结束这一章。
- en: Developing equity trading strategies
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发股票交易策略
- en: 'What is almost as noisy as the market itself is the information about how to
    trade them. How to develop effective trading strategies is well beyond the scope
    of our book. However, here is a blog post that can give you get a realistic idea
    about what to pay attention to when developing ML models for trading. As always,
    use your judgment and due diligence to decide on what to believe: [https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/](https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎与市场本身一样嘈杂的是关于如何交易它们的信息。如何制定有效的交易策略远超本书的范围。然而，下面有一篇博客文章，可以让你对在为交易开发机器学习模型时需要注意的事项有一个现实的了解。像往常一样，使用你的判断力和尽职调查来决定相信什么：[https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/](https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/)。
- en: With that, let's wrap up this chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以结束本章的内容。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered three important RL application areas: personalization,
    marketing, and finance. For personalization and marketing, this chapter went beyond
    the bandit applications that are commonly used in these areas and discussed the
    merits of multi-step RL. We also covered methods such as dueling bandit gradient
    descent, which helped us achieve more conservative exploration to avoid excessive
    reward losses, and action embeddings, which is helpful to deal with large action
    spaces. We concluded the chapter with a discussion on financial applications of
    RL, their challenges, and introduced the TensorTrade library.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了三个重要的强化学习（RL）应用领域：个性化、营销和金融。对于个性化和营销，本章不仅讨论了这些领域中常用的赌博算法应用，还讨论了多步骤强化学习的优点。我们还介绍了如对抗赌博梯度下降（dueling
    bandit gradient descent）等方法，这帮助我们实现了更保守的探索，以避免过度的奖励损失，以及行动嵌入（action embeddings），它有助于处理大型动作空间。最后，我们讨论了强化学习在金融中的应用及其面临的挑战，并介绍了TensorTrade库。
- en: The next chapter is the last application chapter of the book, in which we will
    focus on smart cities and cybersecurity.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章是本书的最后一个应用章节，我们将重点讨论智能城市和网络安全。
- en: References
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Zheng, G. et al. (2018). *DRN: A Deep RL Framework for News Recommendation*.
    WWW ''18: Proceedings of the 2018 World Wide Web Conference April 2018, Pages
    167–176, [https://doi.org/10.1145/3178876.3185994](https://doi.org/10.1145/3178876.3185994).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zheng, G. et al. (2018). *DRN: A Deep RL Framework for News Recommendation*.
    WWW ''18: 2018年全球互联网大会论文集，2018年4月，第167–176页， [https://doi.org/10.1145/3178876.3185994](https://doi.org/10.1145/3178876.3185994)。'
