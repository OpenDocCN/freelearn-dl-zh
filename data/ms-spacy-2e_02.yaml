- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Core Operations with spaCy
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy的核心操作
- en: In this chapter, you will learn about core operations with spaCy, such as creating
    a language pipeline, tokenizing the text, and breaking the text into its sentences.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习关于spaCy的核心操作，例如创建语言管道、分词文本以及将文本分解成句子。
- en: First, you’ll learn what a language processing pipeline is and also explore
    the pipeline components. We’ll continue with general spaCy conventions – important
    classes and class organization – to help you to better understand spaCy and develop
    a solid understanding of the library itself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将了解什么是语言处理管道，并探索管道组件。我们将继续介绍一般的spaCy约定——重要的类和类组织——以帮助你更好地理解spaCy并建立对该库的坚实基础。
- en: You will then learn about the first pipeline component – **Tokenizer** . You’ll
    also learn about an important linguistic concept – **lemmatization** – along with
    its applications in **Natural Language Understanding** ( **NLU** ). Following
    that, we will cover **container** **classes** and **spaCy data structures** in
    detail. We will finish the chapter with useful spaCy features that you’ll use
    in everyday NLP development.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将了解第一个管道组件——**分词器**。你还将了解一个重要的语言概念——**词形还原**——以及其在**自然语言理解**（**NLU**）中的应用。在此之后，我们将详细介绍**容器**类和**spaCy数据结构**。我们将以一些有用的spaCy特性结束本章，这些特性将在日常NLP开发中使用。
- en: 'We’re going to cover the following main topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Overview of spaCy conventions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy约定概述
- en: Introducing tokenization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍分词
- en: Understanding lemmatization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词形还原
- en: spaCy container objects
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy容器对象
- en: More spaCy token features
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多spaCy标记特性
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code of this chapter can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)找到。
- en: Overview of spaCy conventions
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy约定概述
- en: 'Calling **nlp** on our text makes spaCy run a pipeline consisting of many processing
    steps. The first one is the tokenization to produce a **Doc** object. Then, depending
    on the spaCy components we choose to add to our pipeline, the text can be further
    processed by components such as a **tagger** , a **parser** , and an **entity
    recognizer** . We call this a **language processing pipeline** . Each pipeline
    is built using **components** . Each component returns the processed **Doc** and
    then passes it to the next component. This process is showcased in the following
    diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的文本上调用**nlp**会使spaCy运行一个由许多处理步骤组成的管道。第一个步骤是分词，以生成**文档**对象。然后，根据我们添加到管道中的spaCy组件，文本可以进一步由如**词性标注器**、**解析器**和**实体识别器**等组件处理。我们称之为**语言处理管道**。每个管道都是使用**组件**构建的。每个组件返回处理后的**文档**，然后将其传递给下一个组件。这个过程在以下图中展示：
- en: '![Figure 2.1 – A high-level view of the processing pipeline](img/B22441_02_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 处理管道的高级视图](img/B22441_02_01.jpg)'
- en: Figure 2.1 – A high-level view of the processing pipeline
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 处理管道的高级视图
- en: 'A spaCy pipeline object is created when we load a language model. In the following
    code segment, we load an English model and initialize a pipeline:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们加载语言模型时创建spaCy管道对象。在以下代码段中，我们加载一个英语模型并初始化一个管道：
- en: 'First, we import spaCy and use **spacy.load** to return a **Language** class
    instance:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入spaCy并使用**spacy.load**返回一个**语言**类实例：
- en: '[PRE0]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can use this **Language** instance to get the **Doc** object:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个**语言**实例来获取**文档**对象：
- en: '[PRE1]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The **Language** class applies all the pipeline steps to the input sentence
    behind the scenes. After applying **nlp** to the sentence, the **Doc** object
    contains tokens that are tagged, lemmatized, and marked as entities if the **token**
    is an **entity** (we will go into detail about the entities in [*Chapter 5*](B22441_05.xhtml#_idTextAnchor074)
    , *Extracting Semantic Representations with* *spaCy Pipelines* ).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言**类在幕后将所有管道步骤应用于输入句子。在将**nlp**应用于句子后，**文档**对象包含标记，这些标记被标记、词形还原，如果**标记**是**实体**，则标记为实体（我们将在[*第5章*](B22441_05.xhtml#_idTextAnchor074)中详细介绍实体，*使用spaCy管道提取语义表示*)。'
- en: 'Each pipeline component has a well-defined task:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个管道组件都有一个明确的任务：
- en: '**Tokenizer (tokenizer)** : Segment text into tokens'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tokenizer (分词器)** : 将文本分割成标记'
- en: '**Tagger (tagger)** : Assign part-of-speech tags'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tagger (词性标注器)** : 分配词性标签'
- en: '**DependencyParser (parser)** : Assign dependency labels'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DependencyParser (解析器)** : 分配依存标签'
- en: '**EntityRecognizer (ner)** : Detect and label named entities'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体识别器 (ner)**：检测和标记命名实体'
- en: 'The spaCy language processing pipeline *always depends on the statistical model*
    that powers the component. Each component corresponds to a spaCy class. spaCy
    classes have self-explanatory names such as **Language** , **Doc** , and **Vocab**
    . You can see a list of all spaCy pipeline components here: [https://spacy.io/usage/spacy-101/#architecture-pipeline](https://spacy.io/usage/spacy-101/#architecture-pipeline)
    . There are also data structure classes to represent text and language data. We’re
    already familiar with the **Doc** class, but we also have these other container
    classes:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 语言处理管道 *始终依赖于* 驱动组件的统计模型。每个组件对应一个 spaCy 类。spaCy 类具有自解释的名称，例如 **Language**、**Doc**
    和 **Vocab**。您可以在以下位置查看所有 spaCy 管道组件的列表：[https://spacy.io/usage/spacy-101/#architecture-pipeline](https://spacy.io/usage/spacy-101/#architecture-pipeline)。还有用于表示文本和语言数据的数据结构类。我们已经熟悉了
    **Doc** 类，但我们还有这些其他容器类：
- en: '**Doc** : A container object in spaCy that represents the entire processed
    text, holding the structure of the document and its tokens.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Doc**：spaCy 中的一个容器对象，代表整个处理过的文本，持有文档的结构及其标记。'
- en: '**Token** : A single unit of text within a **Doc** object, such as a word,
    punctuation mark, or symbol.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记**：**Doc** 对象中的一个单一文本单元，如单词、标点符号或符号。'
- en: '**Span** : A continuous slice of tokens within a **Doc** object, representing
    a portion of the text, such as a phrase or named entity.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨度**：在 **Doc** 对象内部的一块连续标记，代表文本的一部分，如短语或命名实体。'
- en: '**Lexeme** : An object that stores lexical information about a word, such as
    its base form, spelling, and attributes, independent of its context in the text.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词元**：一个存储关于单词的词汇信息（如其基本形式、拼写和属性）的对象，独立于其在文本中的上下文。'
- en: Finally, spaCy provides helper classes for vectors, language vocabulary, and
    annotations. We’ll see the **Vocab** class often in this book. **Vocab** represents
    a language’s vocabulary. The spaCy library’s backbone data structures are **Doc**
    and **Vocab** . The **Doc** object abstracts the text by owning the sequence of
    tokens and all their properties. The **Vocab** object provides a centralized set
    of strings and lexical attributes to all the other classes. This way, spaCy avoids
    storing multiple copies of linguistic data. *Figure 2* *.2* shows how all spaCy
    containers work together.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，spaCy 提供了用于向量、语言词汇和注释的辅助类。在这本书中，我们将经常看到 **Vocab** 类。**Vocab** 代表一种语言的词汇。spaCy
    库的主干数据结构是 **Doc** 和 **Vocab**。**Doc** 对象通过拥有标记序列及其所有属性来抽象文本。**Vocab** 对象为所有其他类提供集中化的字符串和词汇属性。这样，spaCy
    避免存储多个语言数据的副本。*图 2.2* 展示了所有 spaCy 容器是如何协同工作的。
- en: '![Figure 2.2 – spaCy architecture](img/B22441_02_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – spaCy 架构](img/B22441_02_02.jpg)'
- en: Figure 2.2 – spaCy architecture
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – spaCy 架构
- en: spaCy does all the text processing operations for us behind the scenes, allowing
    us to concentrate on our own application’s development. Let’s start with the **Tokenizer**
    class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 在幕后为我们执行所有文本处理操作，使我们能够专注于我们自己的应用程序开发。让我们从 **Tokenizer** 类开始。
- en: Introducing Tokenization
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍分词
- en: We saw in *Figure 2* *.1* that the first step in a text processing pipeline
    is tokenization. **Tokenization** is always the first operation because all the
    other operations require tokens.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *图 2.1* 中看到，文本处理管道中的第一步是分词。**分词**始终是第一个操作，因为所有其他操作都需要标记。
- en: 'Tokenization simply means splitting the sentence into its tokens. You can think
    of a token as the smallest meaningful part of a piece of text. Tokens can be words,
    numbers, punctuation, currency symbols, and any other meaningful symbols that
    are the building blocks of a sentence. The following are examples of tokens:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 分词简单地说就是将句子分割成其标记。您可以将标记视为文本片段中最小的有意义的部分。标记可以是单词、数字、标点符号、货币符号以及构成句子的任何其他有意义的符号。以下是一些标记的示例：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Input to the spaCy tokenizer is Unicode text and the result is a **Doc** object.
    The following code shows the tokenization process:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 分词器的输入是 Unicode 文本，输出是一个 **Doc** 对象。以下代码展示了分词过程：
- en: 'First, we import the library and load the English language model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入库并加载英语语言模型：
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we apply the **nlp** object to a sentence to create a **Doc** object.
    The **Doc** object is the container for a sequence of **Token** objects. We then
    print the token texts:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将 **nlp** 对象应用于一个句子以创建一个 **Doc** 对象。**Doc** 对象是 **Token** 对象序列的容器。然后我们打印标记文本：
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Figure 2* *.3* shows the tokens we’ve split along with their indexes.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2* *.3* 展示了我们分割的分词及其索引。'
- en: '![Figure 2.3 – Tokenization of “I own a ginger cat.”](img/B22441_02_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – “I own a ginger cat.”的分词](img/B22441_02_03.jpg)'
- en: Figure 2.3 – Tokenization of “I own a ginger cat.”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – “I own a ginger cat.”的分词
- en: Tokenization can be tricky. There are many aspects we should pay attention to
    – punctuation, whitespaces, numbers, and so on. Splitting from the whitespaces
    with **text.split(" ")** might be tempting and looks like it works for the example
    sentence *I own a* *ginger cat* .
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分词可能很棘手。有许多方面我们应该注意 – 标点符号、空白、数字等等。使用**text.split(" ")**从空白处分割可能很有吸引力，看起来在示例句子*I
    own a* *ginger cat*中似乎有效。
- en: 'How about the sentence **"It''s been a crazy week!!!"** ? If we make a split(
    **" "** ), the resulting tokens would be **It''s** , **been** , **a** , **crazy**
    , **week!!!** , which is not what you want. First of all, **It''s** is not one
    token, it’s two tokens: **it** and **''s** . **week!!!** is not a valid token
    as the punctuation is not split correctly. Moreover, **!!!** should be tokenized
    per symbol and should generate three **!** ’s. This may not look like an important
    detail, but it is important for sentiment analysis. Let’s see what the spaCy tokenizer
    has generated:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关于句子**"It's been a crazy week!!!"**呢？如果我们进行分割( **" "** )，生成的分词将是**It's**、**been**、**a**、**crazy**、**week!!!**，这并不是您想要的。首先，**It's**不是一个分词，它是两个分词：**it**和**'s**。**week!!!**不是一个有效的分词，因为标点符号没有被正确分割。此外，**!!!**应该按符号分词，并生成三个**!**。这可能看起来不是一个重要的细节，但对于情感分析来说很重要。让我们看看spaCy分词器生成了什么：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 2* *.4* shows the tokens and their indexes.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2* *.4* 展示了分词及其索引。'
- en: '![Figure 2.4 – Tokenization of apostrophe and punctuation marks](img/B22441_02_04.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 引号和标点符号的分词](img/B22441_02_04.jpg)'
- en: Figure 2.4 – Tokenization of apostrophe and punctuation marks
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 引号和标点符号的分词
- en: 'How does spaCy know where to split the sentence? Unlike other parts of the
    pipeline, the tokenizer doesn’t need a statistical model. Tokenization is based
    on *language-specific rules* . You can see examples the language specific data
    here: [https://github.com/explosion/spaCy/tree/master/spacy/lang](https://github.com/explosion/spaCy/tree/master/spacy/lang)
    .'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy是如何知道在哪里分割句子的？与其他管道部分不同，分词器不需要统计模型。分词基于*语言特定的规则*。您可以在以下链接中看到语言特定数据的示例：[https://github.com/explosion/spaCy/tree/master/spacy/lang](https://github.com/explosion/spaCy/tree/master/spacy/lang)。
- en: 'Tokenizer exceptions define rules for exceptions, such as **it''s** , **don''t**
    , **won''t** , abbreviations, and so on. If you look at the rules for English
    ( [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py)
    ), you will see that rules look like **{ORTH: "n''t", LEMMA: "not"}** , which
    describes the splitting rule for **n''t** to the tokenizer.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '分词器异常定义了异常的规则，例如**it''s**、**don''t**、**won''t**、缩写等等。如果您查看英语的规则（[https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py)），您会看到规则看起来像**{ORTH:
    "n''t", LEMMA: "not"}**，这描述了分词器对**n''t**的分割规则。'
- en: 'The prefixes, suffixes, and infixes mostly describe how to deal with punctuation
    – for example, we split at a period if it is at the end of the sentence, otherwise,
    most probably it’s part of an abbreviation such as **N.Y.** and we shouldn’t touch
    it. Here, **ORTH** means the text and **LEMMA** means the base word forms without
    any inflections. *Figure 2* *.5* shows you the execution of the spaCy tokenization
    algorithm:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀、后缀和中缀主要描述了如何处理标点符号 – 例如，如果句尾有一个句号，我们就分割它，否则，它很可能是像**N.Y.**这样的缩写的一部分，我们不应该触碰它。在这里，**ORTH**表示文本，**LEMMA**表示不带任何屈折的词的基本形式。*图2*
    *.5* 展示了spaCy分词算法的执行过程：
- en: '![Figure 2.5 – spaCy performing tokenization with exception rules, source:  https://spacy.io/usage/linguistic-features#tokenization](img/B22441_02_05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – spaCy使用异常规则进行分词，来源：https://spacy.io/usage/linguistic-features#tokenization](img/B22441_02_05.jpg)'
- en: 'Figure 2.5 – spaCy performing tokenization with exception rules, source: [https://spacy.io/usage/linguistic-features#tokenization](https://spacy.io/usage/linguistic-features#tokenization)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – spaCy使用异常规则进行分词，来源：[https://spacy.io/usage/linguistic-features#tokenization](https://spacy.io/usage/linguistic-features#tokenization)
- en: Tokenization rules depend on the grammatical rules of the individual language.
    Punctuation rules such as splitting periods, commas, or exclamation marks are
    similar for many languages; however, some rules are specific to the individual
    language, such as abbreviation words and apostrophe usage. spaCy supports each
    language having its own specific rules by allowing hand-coded data and rules,
    as each language has its own subclass.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分词规则取决于个别语言的语法规则。如分割句号、逗号或感叹号之类的标点符号规则在许多语言中是相似的；然而，一些规则是针对个别语言的，例如缩写词和撇号的使用。spaCy
    通过允许手动编码的数据和规则来支持每个语言都有其特定的规则，因为每个语言都有自己的子类。
- en: Tip
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: spaCy provides non-destructive tokenization, which means that we always will
    be able to recover the original text from the tokens. Whitespace and punctuation
    information is preserved during tokenization, so the input text is preserved as
    it is.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 提供了非破坏性分词，这意味着我们总是能够从标记中恢复原始文本。在分词过程中，空白和标点信息被保留，因此输入文本保持原样。
- en: Every **Language** object contains a **Tokenizer** object. The **Tokenizer**
    class is the class that performs the tokenization. You don’t often call this class
    directly when you create a **Doc** class instance, while the **Tokenizer** class
    acts behind the scenes. When we want to customize the tokenization, we need to
    interact with this class. Let’s see how it is done.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 **Language** 对象都包含一个 **Tokenizer** 对象。**Tokenizer** 类是执行分词的类。当你创建 **Doc**
    类实例时，通常不会直接调用这个类，而 **Tokenizer** 类在幕后工作。当我们想要自定义分词时，我们需要与这个类进行交互。让我们看看它是如何完成的。
- en: Customizing the tokenizer
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义分词器
- en: 'When we work with a specific domain such as medicine, insurance, or finance,
    we often come across words, abbreviations, and entities that need special attention.
    Most domains that you’ll process have characteristic words and phrases that need
    custom tokenization rules. Here’s how to add a special case rule to an existing
    **Tokenizer** class instance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理特定领域，如医学、保险或金融时，我们经常会遇到需要特别关注的单词、缩写和实体。你将要处理的多数领域都有其特有的单词和短语，需要自定义分词规则。以下是向现有的
    **Tokenizer** 类实例添加特殊案例规则的方法：
- en: 'First, let’s import spaCy and the **ORTH** symbol, which means orthography:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 spaCy 和 **ORTH** 符号，它表示正字法：
- en: '[PRE6]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we instantiate the **Language** object as usual, process the **Doc** object,
    and print the tokens:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们像往常一样实例化 **Language** 对象，处理 **Doc** 对象，并打印标记：
- en: '[PRE7]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we can define the special case where the word **lemme** should be
    tokenized as two tokens, **lem** and **me** :'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以定义一个特殊情况，其中单词 **lemme** 应该被分词为两个标记，**lem** 和 **me**：
- en: '[PRE8]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When defining custom rules, the default punctuation splitting rules will still
    be applied. Even if the special case is surrounded by punctuation, it will still
    be recognized. The tokenizer will handle the punctuation step by step and apply
    the same process to the remaining substring, as in this example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义自定义规则时，默认的标点符号分割规则仍然会被应用。即使特殊情况被标点符号包围，它仍然会被识别。分词器会逐步处理标点符号，并将相同的处理过程应用于剩余的子字符串，如下例所示：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You should modify the tokenizer by adding new rules only if you really need
    to. Trust me, you can get quite unexpected results with custom rules. Especially
    if you have social media text, first feed some sentences into the spaCy NLP pipeline
    and see how the tokenization works out. Let’s see how to debug the tokenizer component.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在你确实需要的时候，才应该通过添加新规则来修改分词器。相信我，使用自定义规则可能会得到相当意外的结果。特别是如果你有社交媒体文本，首先将一些句子输入到
    spaCy NLP 管道中，看看分词是如何进行的。让我们看看如何调试分词器组件。
- en: Debugging the tokenizer
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试分词器
- en: 'spaCy has a tool for debugging: **nlp.tokenizer.explain(sentence)** . It returns
    tuples ( **tokenizer rule/pattern, token** ) to help us understand what happened
    exactly during the tokenization. Let’s see an example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 有一个用于调试的工具：**nlp.tokenizer.explain(sentence)**。它返回（**分词规则/模式，标记**）元组，帮助我们了解在分词过程中确切发生了什么。让我们看看一个例子：
- en: 'Let’s process the text as usual:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们像往常一样处理文本：
- en: '[PRE10]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can ask the **Tokenizer** class instance of the **Language** object
    for an explanation of the tokenization in this sentence:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以要求 **Language** 对象的 **Tokenizer** 类实例解释这个句子的分词：
- en: '[PRE11]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The **nlp.tokenizer.explain()** method explained the rules that the tokenizer
    used one by one. After splitting a sentence into its tokens, it’s time to split
    a text into its sentences.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**nlp.tokenizer.explain()** 方法逐一解释了分词器使用的规则。在将句子分割成其标记后，就到了将文本分割成其句子的时间了。'
- en: Sentence segmentation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子分割
- en: We saw that breaking a sentence into its tokens is not a straightforward task
    at all. How about breaking a text into sentences? It’s indeed a bit more complicated
    to mark where a sentence starts and ends due to the same reasons of punctuation,
    abbreviations, and so on.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，将一个句子分解为其标记并不是一个简单的任务。那么，将文本分解为句子呢？由于标点符号、缩写等原因，标记句子开始和结束的确要复杂一些。
- en: 'A **Doc** object’s sentences are available via the **doc.sents** property:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**Doc** 对象的句子可以通过 **doc.sents** 属性访问：'
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Determining sentence boundaries is a more complicated task than tokenization.
    As a result, spaCy uses the **dependency parser** to perform sentence segmentation.
    This is a unique feature of spaCy – no other library puts such a sophisticated
    idea into practice. The results are very accurate in general, unless you process
    text of a very specific genre, such as from the conversation domain, or social
    media text.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 确定句子边界比标记化更复杂。因此，spaCy 使用 **依存句法分析器** 来执行句子分割。这是 spaCy 的一个独特特性——没有其他库将如此复杂的思想付诸实践。一般来说，结果非常准确，除非你处理的是非常特定类型的文本，例如来自对话领域或社交媒体文本。
- en: Now that we know how to segment a text into sentences and tokenize the sentences,
    let’s start with lemmatization, a commonly used operation in semantics and sentiment
    analysis.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何将文本分割成句子并对句子进行标记化，让我们从词形还原开始，这是语义和情感分析中常用的操作。
- en: Understanding lemmatization
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词形还原
- en: 'A **lemma** is the base form of a token. You can think of a lemma as the form
    in which the token appears in a dictionary. For instance, the lemma of *eating*
    is *eat* ; the lemma of *eats* is *eat* ; ate similarly maps to eat. **Lemmatization**
    is the process of reducing word forms to their lemmas. The following code is a
    quick example of how to do lemmatization with spaCy:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**词元** 是一个标记的基本形式。你可以把词元看作是标记在词典中出现的形态。例如，*eating* 的词元是 *eat* ；*eats* 的词元也是
    *eat* ；*ate* 同样映射到 *eat*。**词形还原** 是将词形缩减到词元的过程。以下代码是使用 spaCy 进行词形还原的快速示例：'
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By now, you should be familiar with what the first three lines of the code do.
    In the **for** loop, we print each token, **text** and **lemma_** . Let’s see
    lemmatization in action with a real-world example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该熟悉代码的前三行做了什么。在 **for** 循环中，我们打印出每个标记，**text** 和 **lemma_** 。让我们通过一个现实世界的例子来看看词形还原的实际应用。
- en: Lemmatization in NLU
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLU 中的词形还原
- en: Lemmatization is an important step in NLU. We’ll go over an example in this
    subsection. Suppose that you design an NLP pipeline for a ticket booking system.
    Your application processes a customer’s sentence, extracts necessary information
    from it, and then passes it to the booking API.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是 NLU 中的一个重要步骤。我们将在本小节中通过一个例子来讲解。假设你为票务预订系统设计了一个 NLP 管道。你的应用程序处理客户的句子，从中提取必要的信息，然后将它传递给预订
    API。
- en: 'The NLP pipeline wants to extract the form of travel (a flight, bus, or train),
    the destination city, and the date. The first thing the application needs to verify
    is the means of travel:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 管道想要提取旅行的形式（航班、巴士或火车）、目的地城市和日期。应用程序需要验证的第一件事是旅行方式：
- en: Fly
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 飞行
- en: Flight
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 航班
- en: Airway
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 航路
- en: Airplane
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 飞机
- en: Plane
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 飞机
- en: Bus
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴士
- en: Railway
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铁路
- en: Train
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 火车
- en: 'We have this list of keywords, and we want to recognize the means of travel
    by searching the tokens in the keywords list. The most compact way of doing this
    search is by looking up the token’s **lemma** . Consider the following customer
    sentences:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有这个关键词列表，我们希望通过在关键词列表中搜索标记来识别旅行方式。进行这种搜索的最紧凑方式是查找标记的 **词元** 。考虑以下客户句子：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we don’t need to include all word forms of the verb *fly* ( *fly* , *flying*
    , *flies* , *flew* , and *flown* ) in the keywords list, and similar for the word
    **flight** ; we reduced all possible variants to the base forms – *fly* and *flight*
    . Don’t think of English only; languages such as Portuguese, German, and Finnish
    have many word forms from a single lemma as well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不需要在关键词列表中包含动词 *fly* 的所有词形（ *fly* , *flying* , *flies* , *flew* , 和 *flown*
    ），对于单词 **flight** 也是如此；我们将所有可能的变体都缩减到了基本形式 – *fly* 和 *flight* 。不要只考虑英语；像葡萄牙语、德语和芬兰语这样的语言也有许多来自单个词元的词形。
- en: Lemmatization also comes in handy when we want to recognize the destination
    city. There are many nicknames available for global cities, such as *Angeltown*
    for *Los Angeles* . The default tokenizer and lemmatizer won’t know the difference
    between the official name and the nickname. The **AttributeRuler** component lets
    us set custom token attributes using **Matcher** patterns (we will learn more
    about **Matcher** in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056) ).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要识别目的地城市时，词形化也很有用。全球城市有许多昵称，例如 *Angeltown* 对应于 *Los Angeles*。默认的标记化和词形化器不会区分官方名称和昵称。**AttributeRuler**
    组件允许我们使用 **Matcher** 模式设置自定义标记属性（我们将在 [*第4章*](B22441_04.xhtml#_idTextAnchor056)
    中了解更多关于 **Matcher** 的内容）。
- en: 'Let’s add a special rule to set the lemma of **Angeltown** as **Los Angeles**
    :'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一个特殊规则来设置 **Angeltown** 的词形为 **Los Angeles**：
- en: 'The **nlp.get_pipe()** method returns a pipeline component. We’ll get the **AttributeRuler**
    component and add the special rule:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nlp.get_pipe()** 方法返回一个管道组件。我们将获取 **AttributeRuler** 组件并添加特殊规则：'
- en: '[PRE15]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s see if this worked by processing the **doc** and printing the lemmas:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过处理 **doc** 并打印词形来查看这是否有效：
- en: '[PRE16]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that we’ve learned about the **tokenizer** and the **lemmatizer** components
    (usually the two components of a processing pipeline), let’s go ahead and learn
    more about spaCy **container objects** .
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 **tokenizer** 和 **lemmatizer** 组件（通常是一个处理管道的两个组件），让我们继续学习更多关于 spaCy
    **容器对象** 的内容。
- en: spaCy container objects
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy 容器对象
- en: At the beginning of this chapter, we saw a list of container objects including
    **Doc** , **Token** , **Span** , and **Lexeme** . In this section, we’ll see the
    properties of container objects in detail.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们看到了一个包括 **Doc**、**Token**、**Span** 和 **Lexeme** 在内的容器对象列表。在本节中，我们将详细查看容器对象的属性。
- en: Using container objects, we can access the linguistic properties that spaCy
    assigns to text. A **container object** is a logical representation of text units
    such as a document, a token, or a slice of a document.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用容器对象，我们可以访问 spaCy 分配给文本的语言学属性。一个 **容器对象** 是文本单元（如文档、标记或文档的一部分）的逻辑表示。
- en: 'Container objects in spaCy follow the natural structure of the text: a document
    is composed of sentences and sentences are composed of tokens. We most widely
    use **Doc** , **Token** , and **Span** objects in development, which represent
    a document, a single token, and a phrase, respectively. A container can contain
    other containers – for instance, a document contains tokens and spans.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 中的容器对象遵循文本的自然结构：一个文档由句子组成，句子由标记组成。我们在开发中最广泛使用 **Doc**、**Token** 和 **Span**
    对象，分别代表文档、单个标记和短语。一个容器可以包含其他容器——例如，一个文档包含标记和跨度。
- en: Let’s explore each class and its useful properties one by one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探索每个类及其有用的属性。
- en: Doc
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档
- en: 'We created **Doc** objects in our code to represent the text, so you might
    have already figured out that **Doc** represents a text. Here, **doc.text** returns
    a Unicode representation of the document text:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代码中创建了 **Doc** 对象来表示文本，所以你可能已经猜到 **Doc** 代表文本。在这里，**doc.text** 返回文档文本的 Unicode
    表示：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The building block of a **Doc** object is **Token** , hence when you iterate
    a **Doc** object, you get token objects as items:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Doc** 对象的构建块是 **Token**，因此当你迭代一个 **Doc** 对象时，你会得到作为项的 **Token** 对象：'
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The same logic applies to indexing:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的逻辑适用于索引：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The length of a **Doc** object is the number of tokens it includes:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**Doc** 对象的长度是它包含的标记数量：'
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As for the text’s sentences, **doc.sents** returns an iterator to the list
    of sentences. Each sentence is a **Span** object:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于文本的句子，**doc.sents** 返回一个句子列表的迭代器。每个句子都是一个 **Span** 对象：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The named entities in the text are provided by **doc.ents** . The result is
    a list of **Span** objects. We’ll look at named entities in detail later in the
    book – for now, think of them as proper nouns:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中的命名实体由 **doc.ents** 提供。结果是 **Span** 对象的列表。我们将在本书的后面部分详细讨论命名实体——现在，把它们看作专有名词：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Another syntactic property is **doc.noun_chunks** . It yields the noun phrases
    found in the text:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个句法属性是 **doc.noun_chunks**。它返回文本中找到的名词短语：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The property **doc.lang_** returns the language of the doc created:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 属性 **doc.lang_** 返回创建的文档的语言：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'A useful method for serialization is **doc.to_json** . This is how to convert
    a **Doc** object to **JSON** :'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的序列化方法是 **doc.to_json**。这是将 **Doc** 对象转换为 **JSON** 的方法：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Pro tip
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'You might have noticed that we call **doc.lang_** , not **doc.lang** . The
    **doc.lang** call returns the language ID, whereas **doc.lang_** returns the Unicode
    string of the language, that is, the name of the language. You can see the same
    convention with token features in the following: **token.lemma_** , **token.tag_**
    , and **token.pos_** .'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，我们调用的是 **doc.lang_**，而不是 **doc.lang**。**doc.lang** 调用返回语言 ID，而 **doc.lang_**
    返回语言的 Unicode 字符串，即语言名称。您可以在以下内容中看到相同约定：**token.lemma_**、**token.tag_** 和 **token.pos_**。
- en: The **Doc** object has very useful properties with which you can understand
    a sentence’s syntactic properties and use them in your own applications. Let’s
    move on to the **Token object** and see what it offers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Doc** 对象具有非常实用的属性，您可以使用这些属性来理解句子的句法属性，并在您自己的应用程序中使用它们。让我们继续了解 **Token** 对象，看看它提供了什么。'
- en: Token
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Token
- en: 'A **token** object represents a word. Token objects are the building blocks
    of **Doc** and **Span** objects. In this section, we will cover the following
    properties of the **Token** class:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**token** 对象代表一个单词。词元对象是 **Doc** 和 **Span** 对象的构建块。在本节中，我们将介绍 **Token** 类的以下属性：'
- en: '**token.text**'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.text**'
- en: '**token.text_with_ws**'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.text_with_ws**'
- en: '**token.i**'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.i**'
- en: '**token.idx**'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.idx**'
- en: '**token.doc**'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.doc**'
- en: '**token.sent**'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.sent**'
- en: '**token.is_sent_start**'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.is_sent_start**'
- en: '**token.ent_type**'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token.ent_type**'
- en: 'We usually don’t construct an object of the **Token** class directly, rather
    we construct a **Doc** object and then access its tokens:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不会直接构造 **Token** 类的对象，而是构造 **Doc** 对象，然后访问其词元：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The **token.text** property is similar to **doc.text** and provides the underlying
    Unicode string. **token.text_with_ws** is a similar property. It provides the
    text with a trailing whitespace if it’s present in the **doc** object:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**token.text** 属性类似于 **doc.text**，提供了底层的 Unicode 字符串。**token.text_with_ws**
    是一个类似的属性。如果它在 **doc** 对象中存在，它将提供带有尾随空白的文本：'
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finding the length of a token is similar to finding the length of a Python
    string:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 查找词元的长度与查找 Python 字符串的长度类似：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The **token.i** property gives the index of the token in the **doc** object:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**token.i** 属性提供了词元在 **doc** 对象中的索引：'
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The token’s character offset (the character position) in the **doc** object
    is provided by the **token.idx** :'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 词元的字符偏移量（字符位置）由 **token.idx** 提供：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Another cool property of tokens is that we can also access the **doc** object
    that created the token as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的词元属性是，我们还可以访问创建词元的 **doc** 对象，如下所示：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This also works for getting the sentence that the token belongs to:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于获取词元所属的句子：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Another useful property is **token.is_sent_start** ; it returns a Boolean indicating
    whether the token starts a sentence:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的属性是 **token.is_sent_start**；它返回一个布尔值，指示该词元是否是一个句子的开始：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'These are the basic properties of the **Token** object that you’ll use every
    day. There is another set of properties that are more related to syntax and semantics.
    We already saw how to calculate the token lemma in the previous section:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是您每天都会使用的 **Token** 对象的基本属性。还有另一组属性，它们与句法和语义更相关。我们已经在上一节中看到了如何计算词元词干：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You already learned that **doc.ents** property gives the named entities of
    the document. If you want to learn what sort of entity the token is, use **token.ent_type_**
    :'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解到 **doc.ents** 属性提供了文档中的命名实体。如果您想了解词元是哪种类型的实体，请使用 **token.ent_type_**：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Two syntactic features related to POS tagging are **token.pos_** and **token.tag**
    . We’ll learn what they are and how to use them in the next chapter. The **Token**
    object has a rich set of features, enabling us to process the text from head to
    toe. Let’s move on to the **Span** object and see what it offers us.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与 POS 标注相关的两个句法特征是 **token.pos_** 和 **token.tag**。我们将在下一章中学习它们是什么以及如何使用它们。**Token**
    对象具有丰富的功能集，使我们能够从头到尾处理文本。让我们继续了解 **Span** 对象，看看它提供了什么。
- en: Span
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Span
- en: '**Span** objects represent phrases or segments of the text. A **Span** object
    must be a contiguous sequence of tokens. We usually don’t initialize **Span**
    objects, rather we slice a **Doc** object:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**Span** 对象代表短语或文本的片段。**Span** 对象必须是词元的连续序列。我们通常不会初始化 **Span** 对象，而是从 **Doc**
    对象中切片：'
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Trying to slice an invalid index will raise an **IndexError** . Most indexing
    and slicing rules of Python strings are applicable to **Doc** slicing as well:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试切片无效索引将引发 **IndexError**。Python 字符串的大多数索引和切片规则也适用于 **Doc** 切片：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'There is one more way to create a **Span** object – we can make a character-level
    slice of a **Doc** object with **char_span([start_idx, end_idx])** :'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 创建**Span**对象还有另一种方法——我们可以使用**char_span([start_idx, end_idx])**从**Doc**对象中创建一个字符级别的切片：
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The building blocks of a **Span** object are **Token** objects. If you iterate
    over a **Span** object, you get **Token** objects:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**Span**对象的基本构建块是**Token**对象。如果你遍历一个**Span**对象，你会得到**Token**对象：'
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can think of the **Span** object as a junior **Doc** object. Hence, most
    of the features of **Doc** are applicable to **Span** as well. For instance, **len**
    is identical:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将**Span**对象视为一个初级**Doc**对象。因此，**Doc**的大多数特性也适用于**Span**。例如，**len**是相同的：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The **Span** object also supports indexing. The result of slicing a **Span**
    object is another **Span** object:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**Span**对象也支持索引。切片**Span**对象的结果是另一个**Span**对象：'
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Just like a **Token** knows the **Doc** object it’s created from, **Span**
    also knows the **Doc** object it’s created from:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如**Token**知道它是由哪个**Doc**对象创建的一样，**Span**也知道自己是由哪个**Doc**对象创建的：
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can also locate the **Span** object in the original **Doc** object:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在原始的**Doc**对象中定位**Span**对象：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you want a brand-new **Doc** object, you can call **span.as_doc()** . It
    copies the data into a new **Doc** object:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个新的**Doc**对象，你可以调用**span.as_doc()**。它会将数据复制到一个新的**Doc**对象中：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The properties **span.ents** , **span.sent** , **span.text** , and **span.text_with_ws**
    are similar to their corresponding **Doc** and **Token** methods. We’ll now go
    through a few more features and methods for more detailed text analysis in the
    next section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 属性**span.ents**、**span.sent**、**span.text**和**span.text_with_ws**与它们对应的**Doc**和**Token**方法类似。现在我们将介绍更多特性和方法，以便在下一节中进行更详细的分析文本。
- en: More spaCy Token features
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多spaCy标记特性
- en: 'Most NLP development is token and span oriented; that is, it processes tags,
    dependency relations, tokens themselves, and phrases. We apply transformations
    like eliminating small words and words without much meaning, processing URLs taking
    into account the protocol and subdomain parts, and so on. These actions sometimes
    depend on the *token shape* (for example, if the token is a short word or if the
    token looks like a URL string) or more semantical features (such as the token
    is an article, or the token is a conjunction). In this section, we will see these
    features of tokens with some examples. We’ll start with features related to the
    token shape:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数NLP开发都是基于标记和跨度进行的；也就是说，它处理标签、依存关系、标记本身和短语。我们应用一些转换，比如消除小词和意义不大的词，处理URL时考虑到协议和子域部分等。这些操作有时取决于*标记形状*（例如，如果标记是一个短词或如果标记看起来像URL字符串）或更语义化的特性（例如，标记是一个冠词，或标记是一个连词）。在本节中，我们将通过一些示例来查看这些标记特性。我们将从与标记形状相关的特性开始：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The **token.lower_** feature turns the token in lowercase. The return value
    is a Unicode string and this feature is equivalent to **token.text.lower()** .
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**token.lower_**特性将标记转换为小写。返回值是一个Unicode字符串，这个特性等同于**token.text.lower()**。'
- en: The features **is_lower** and **is_upper** are similar to their Python string
    method counterparts, **islower()** and **isupper()** . The **is_lower** feature
    returns **True** if all the characters are lowercase, while **is_upper** does
    the same with uppercase.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 特性**is_lower**和**is_upper**与它们的Python字符串方法对应，**islower()**和**isupper()**。**is_lower**特性如果所有字符都是小写则返回**True**，而**is_upper**则对大写执行相同的操作。
- en: 'The feature **is_alpha** returns **True** if all the characters of the token
    are alphabetic letters. Examples of nonalphabetic characters are numbers, punctuation,
    and whitespace:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**is_alpha**特性如果标记的所有字符都是字母则返回**True**。非字母字符的例子包括数字、标点和空白字符：'
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The **is_ascii** feature returns **True** if all the characters of the token
    are ASCII characters:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**is_ascii**特性如果标记的所有字符都是ASCII字符则返回**True**：'
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The **is_digit** feature returns **True** if all the characters of the token
    are numbers:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**is_digit**特性如果标记的所有字符都是数字则返回**True**：'
- en: '[PRE48]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The feature **is_punct** returns **True** if the token is a punctuation mark:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 特性**is_punct**如果标记是一个标点符号则返回**True**：
- en: '[PRE49]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The features **is_left_punct** and **is_right_punct** return **True** if the
    token is a left punctuation mark or right punctuation mark, respectively. A right
    punctuation mark can be any mark that closes a left punctuation mark, such as
    right brackets, **>** or **»** . Left punctuation marks are similar, with the
    left brackets **<** and **«** as some examples:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 **is_left_punct** 和 **is_right_punct** 分别在标记是左标点或右标点时返回 **True**。一个右标点可以是任何关闭左标点的标记，例如右括号、**>**
    或 **»**。左标点类似，左括号 **<** 和 **«** 是一些例子：
- en: '[PRE50]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The methods **like_url** , **like_num** , and **like_email** capture information
    about the token shape and return **True** if the token looks like a URL, a number,
    or an email, respectively. These methods are very handy when we want to process
    social media text and scraped web pages:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 **like_url**、**like_num** 和 **like_email** 捕获关于标记形状的信息，如果标记看起来像URL、数字或电子邮件，则分别返回
    **True**。当我们想要处理社交媒体文本和抓取的网页时，这些方法非常实用：
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**token.shape_** is an unusual feature – there is nothing similar in other
    NLP libraries. It returns a string that shows a token’s orthographic features.
    Numbers are replaced with **d** , uppercase letters are replaced with **X** ,
    and lowercase letters are replaced with **x** . You can use the result string
    as a feature in your machine learning algorithms, and token shapes can be correlated
    to text sentiment:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**token.shape_** 是一个不寻常的特性——在其他NLP库中没有类似的东西。它返回一个字符串，显示了标记的正字法特征。数字被替换为 **d**，大写字母被替换为
    **X**，小写字母被替换为 **x**。你可以将结果字符串用作机器学习算法中的特征，并且标记形状可以与文本情感相关联：'
- en: '[PRE52]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The feature **is_stop** is frequently used by machine learning algorithms.
    Often, we filter words that do not carry much meaning, such as *the* , *a* , *an*
    , *and* , *just* , *with* , and so on. Such words are called stop words. Each
    language has its own stop word list, and you can access English stop words at
    [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)
    :'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 **is_stop** 经常被机器学习算法使用。通常，我们会过滤掉那些意义不大的词，例如 *the*、*a*、*an*、*and*、*just*、*with*
    等等。这样的词被称为停用词。每种语言都有自己的停用词列表，你可以在 [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)
    查找英语停用词：
- en: '[PRE53]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We’re finally done with some of spaCy’s syntactic, semantic, and orthographic
    features. Many of those methods focused on the **Token** object as a token is
    the syntactic unit of a text.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于完成了spaCy的一些句法、语义和正字法特征的学习。许多方法都集中在 **Token** 对象上，因为一个标记是文本的句法单位。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter gave you a comprehensive picture of spaCy library classes and
    methods. We took a deep dive into language processing pipelines and learned about
    pipeline components. We also covered a basic yet important syntactic task: tokenization.
    We continued with the linguistic concept of lemmatization and you learned about
    a real-world application of this spaCy feature. We explored spaCy container classes
    in detail and finalized the chapter by looking at precise and useful spaCy features.
    In the next chapter, we will dive into spaCy’s full linguistic power. You’ll discover
    linguistic features including spaCy’s most used features: the POS tagger, dependency
    parser, and named entities.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为你全面介绍了spaCy库的类和方法。我们深入研究了语言处理管道，并了解了管道组件。我们还介绍了基本但重要的句法任务：分词。我们继续探讨了词形还原这一语言概念，并学习了spaCy这一特性的实际应用。我们详细探讨了spaCy容器类，并在本章最后部分审视了精确且有用的spaCy特性。在下一章中，我们将深入spaCy的全部语言能力。你将发现包括spaCy最常用的特性：词性标注器、依存句法分析器和命名实体。
- en: 'Part 2: Advanced Linguistic and Semantic Analysis'
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：高级语言和语义分析
- en: Building on the fundamentals, this section explores advanced techniques for
    analyzing and extracting information from text using spaCy’s linguistic and rule-based
    tools. You'll learn how to perform sophisticated parsing and matching and even
    develop your own custom components to handle complex tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础知识的基础上，本节探讨了使用spaCy的语言和基于规则的工具分析文本和提取信息的先进技术。你将学习如何进行复杂的解析和匹配，甚至开发自己的自定义组件来处理复杂任务。
- en: 'This part has the following chapters:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 3*](B22441_03.xhtml#_idTextAnchor045) , *Extracting Linguistic Features*'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B22441_03.xhtml#_idTextAnchor045) ，*提取语言特征*'
- en: '[*Chapter 4*](B22441_04.xhtml#_idTextAnchor056) , *Mastering Rule-Based Matching*'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B22441_04.xhtml#_idTextAnchor056) ，*基于规则的匹配精通*'
- en: '[*Chapter 5*](B22441_05.xhtml#_idTextAnchor074) , *Extracting Semantic Representations
    with spaCy Pipelines*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B22441_05.xhtml#_idTextAnchor074) ，*使用spaCy管道提取语义表示*'
- en: '[*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) , *Utilizing spaCy with Transformers*'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B22441_06.xhtml#_idTextAnchor087) ，*使用Transformers与spaCy*'
