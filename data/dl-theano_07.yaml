- en: Chapter 7. Classifying Images with Residual Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。使用残差网络分类图像
- en: This chapter presents state-of-the-art deep networks for image classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了用于图像分类的最先进的深度网络。
- en: Residual networks have become the latest architecture, with a huge improvement
    in accuracy and greater simplicity.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络已成为最新的架构，准确性大幅提高，并且更为简洁。
- en: Before residual networks, there had been a long history of architectures, such
    as **AlexNet**, **VGG**, **Inception** (**GoogLeNet**), **Inception v2,v3, and
    v4**. Researchers were searching for different concepts and discovered some underlying
    rules with which to design better architectures.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在残差网络之前，已经有很长时间的架构历史，比如**AlexNet**、**VGG**、**Inception**（**GoogLeNet**）、**Inception
    v2、v3 和 v4**。研究人员一直在寻找不同的概念，并发现了一些潜在的规律来设计更好的架构。
- en: 'This chapter will address the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Main datasets for image classification evaluation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类评估的主要数据集
- en: Network architectures for image classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类的网络架构
- en: Batch normalization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Global average pooling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局平均池化
- en: Residual connections
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Stochastic depth
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机深度
- en: Dense connections
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集连接
- en: Multi-GPU
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多 GPU
- en: Data augmentation techniques
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强技术
- en: Natural image datasets
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然图像数据集
- en: Image classification usually includes a wider range of objects and scenes than
    the MNIST handwritten digits. Most of them are natural images, meaning images
    that a human being would observe in the real world, such as landscapes, indoor
    scenes, roads, mountains, beaches, people, animals, and automobiles, as opposed
    to synthetic images or images generated by a computer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类通常包括比 MNIST 手写数字更广泛的物体和场景。它们大多数是自然图像，意味着人类在现实世界中观察到的图像，例如风景、室内场景、道路、山脉、海滩、人类、动物和汽车，而不是合成图像或计算机生成的图像。
- en: 'To evaluate the performance of image classification networks for natural images,
    three main datasets are usually used by researchers to compare performance:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估图像分类网络在自然图像上的表现，研究人员通常使用三个主要数据集来比较性能：
- en: 'Cifar-10, a dataset of 60,000 small images (32x32) regrouped into 10 classes
    only, which you can easily download:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cifar-10 数据集包含 60,000 张小图像（32x32），仅分为 10 类，您可以轻松下载：
- en: '[PRE0]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are some example images for each class:'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是每个类别的一些示例图像：
- en: '![Natural image datasets](img/00106.jpeg)'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![自然图像数据集](img/00106.jpeg)'
- en: Cifar 10 dataset classes with samples [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cifar 10 数据集类别及样本 [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
- en: Cifar-100, a dataset of 60,000 images, partitioned into 100 classes and 20 super-classes
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cifar-100 数据集包含 60,000 张图像，分为 100 类和 20 个超级类别
- en: 'ImageNet, a dataset of 1.2 million images, labeled with a wide range of classes
    (1,000). Since ImageNet is intended for non-commercial use only, it is possible
    to download Food 101, a dataset of 101 classes of meals, and 1,000 images per
    class:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageNet 数据集包含 120 万张图像，标注了广泛的类别（1,000 类）。由于 ImageNet 仅供非商业用途，您可以下载 Food 101
    数据集，该数据集包含 101 种餐食类别，每个类别有 1,000 张图像：
- en: '[PRE1]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before introducing residual architectures, let us discuss two methods to improve
    classification net accuracy: batch normalization, and global average pooling.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍残差架构之前，让我们讨论两种提高分类网络准确度的方法：批量归一化和全局平均池化。
- en: Batch normalization
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Deeper networks, with more than 100 layers can help image classification for
    a few hundred classes. The major issue with deep networks is to ensure that the
    flows of inputs, as well as the gradients, are well propagated from one end of
    the network to the other end.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更深的网络，超过 100 层，可以帮助图像分类多个类别。深度网络的主要问题是确保输入流以及梯度能够从网络的一端有效传播到另一端。
- en: Nevertheless, it is not unusual that nonlinearities in the network get saturated,
    and gradients become null. Moreover, each layer in the network has to adapt to
    constant changes in the distribution of its inputs, a phenomenon known as **internal
    covariate shift**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，网络中的非线性部分饱和，梯度变为零并不罕见。此外，网络中的每一层都必须适应其输入分布的持续变化，这一现象被称为**内部协变量偏移**。
- en: It is known that a network trains faster with input data linearly processed
    to have zero mean and unit variance (known as **network input normalization**),
    and normalizing each input feature independently, instead of jointly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 已知，网络训练时，输入数据经过线性处理以使均值为零、方差为单位（称为**网络输入归一化**）能加速训练，并且每个输入特征应独立归一化，而不是联合归一化。
- en: 'To normalize the input of every layer in a network, it is a bit more complicated:
    zeroing the mean of the input will ignore the learned bias of the previous layer,
    and the problem is even worse with unit variance. The parameters of the previous
    layer may grow infinitely, while the loss stays constant, when inputs of the layer
    are normalized.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要规范化网络中每一层的输入，稍微复杂一些：将输入的均值归零会忽略前一层学习到的偏置，而当方差为单位时，问题更加严重。当该层的输入被归一化时，前一层的参数可能会无限增长，而损失保持不变。
- en: 'So, for **layer input normalization**, a **batch normalization layer** relearns
    the scale and the bias after normalization:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于**层输入规范化**，**批量归一化层**在归一化后重新学习尺度和偏置：
- en: '![Batch normalization](img/00107.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![批量归一化](img/00107.jpeg)'
- en: Rather than using the entire dataset, it uses the batch to compute the statistics
    for normalization, with a moving average to get closer to entire dataset statistics
    while training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它不使用整个数据集，而是使用批次来计算归一化的统计量，并通过移动平均来接近整个数据集的统计信息，同时进行训练。
- en: 'A batch normalization layer has the following benefits:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个批量归一化层具有以下好处：
- en: It reduces the influence of bad initializations or too high learning rates
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了不良初始化或过高学习率的影响
- en: It increases the accuracy of the net by a margin
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提高了网络的准确性
- en: It accelerates the training
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它加速了训练
- en: It reduces overfitting, regularizing the model
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了过拟合，起到正则化模型的作用
- en: When introducing batch normalization layers, you can remove dropout, increase
    the learning rate, and reduce L2 weight normalization.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 引入批量归一化层时，可以移除 dropout，增加学习率，并减少 L2 权重规范化。
- en: 'Be careful to place nonlinearity after the BN layer, and to remove bias in
    the previous layer:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 小心将非线性激活放在 BN 层之后，并去除前一层的偏置：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Global average pooling
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局平均池化
- en: Traditionally, the two last layers of a classification net are a fully connected
    layer and a softmax layer. The fully connected layer outputs a number of features
    equal to the number of classes, and the softmax layer normalizes these values
    to probabilities that sum to 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，分类网络的最后两层是全连接层和 softmax 层。全连接层输出一个等于类别数量的特征数，softmax 层将这些值归一化为概率，且它们的和为
    1。
- en: 'Firstly, it is possible to replace max-pooling layers of stride 2 with new
    convolutional layers of stride 2: all-convolutional networks perform even better.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可以将步幅为 2 的最大池化层替换为步幅为 2 的新的卷积层：全卷积网络的表现更好。
- en: 'Secondly, removing the fully connected layer is also possible. If the number
    of featuremaps output by the last convolutional layer is chosen equal to the number
    of classes, a global spatial average reduces each featuremap to a scalar value,
    representing the score for the class averaged at the different *macro* spatial
    locations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，也可以移除全连接层。如果最后一个卷积层输出的特征图数量选择为类别数，全球空间平均池化将每个特征图缩减为一个标量值，表示在不同*宏观*空间位置上类的得分平均值：
- en: '![Global average pooling](img/00108.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![全局平均池化](img/00108.jpeg)'
- en: Residual connections
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差连接
- en: While very deep architectures (with many layers) perform better, they are harder
    to train, because the input signal decreases through the layers. Some have tried
    training the deep networks in multiple stages.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非常深的架构（具有许多层）表现更好，但它们更难训练，因为输入信号在层与层之间逐渐减弱。有人尝试在多个阶段训练深度网络。
- en: 'An alternative to this layer-wise training is to add a supplementary connection
    to shortcut a block of layers, named the **identity connection**, passing the
    signal without modification, in addition to the classic convolutional layers,
    named the **residuals**, forming a **residual block**, as shown in the following
    image:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逐层训练的替代方法是向网络中添加一个附加连接，跳过一个块的层，称为**恒等连接**，它将信号传递而不作任何修改，除了经典的卷积层，称为**残差**，形成一个**残差块**，如下图所示：
- en: '![Residual connections](img/00109.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![残差连接](img/00109.jpeg)'
- en: Such a residual block is composed of six layers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的残差块由六层组成。
- en: 'A residual network is a network composed of multiple residual blocks. Input
    is processed by a first convolution, followed by batch normalization and non-linearity:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络是由多个残差块组成的网络。输入经过第一层卷积处理，然后是批量归一化和非线性激活：
- en: '![Residual connections](img/00110.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![残差连接](img/00110.jpeg)'
- en: 'For example, for a residual net composed of two residual blocks, and eight
    featuremaps in the first convolution on an input image of size *28x28*, the layer
    output shapes will be the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于由两个残差块组成的残差网络，且第一卷积层有八个特征图，输入图像的大小为 *28x28*，层的输出形状如下：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The number of output featuremaps increase while the size of each output featuremap
    decreases: such a technique in funnel of **decreasing featuremap sizes/increasing
    the number of dimensions keeps the number** of parameters per layer constant which
    is a common best practice for building networks.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出特征图的数量增加，而每个输出特征图的大小减小：这种技术通过**减小特征图大小/增加维度的数量保持每层参数数量不变**，这是构建网络时常见的最佳实践。
- en: 'Three transitions to increase the number of dimensions occur, one before the
    first residual block, a second one after n residual blocks, and a third one after
    *2xn* residual blocks. Between each transition, the number of filters are defined
    in an array:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加维度，在三个不同位置进行了三次维度转换，第一次在第一个残差块之前，第二次在 n 个残差块之后，第三次在 *2xn* 个残差块之后。每个转换之间，过滤器的数量按数组定义：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The dimensional increase is performed by the first layer of the corresponding
    residual block. Since the input is not the same shape as the output, the simple
    identity connection cannot be concatenated with the output of layers of the block,
    and is replaced by a dimensional projection to reduce the size of the output to
    the dimension of the block output. Such a projection can be done with a convolution
    of kernel *1x1* with a stride of `2`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 维度增加是通过相应残差块的第一层进行的。由于输入的形状与输出不同，简单的恒等连接无法与块层的输出拼接，因此用维度投影代替，以将输出的大小调整为块输出的维度。这样的投影可以通过一个`1x1`的卷积核，步幅为`2`来实现：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Some variants of residual blocks have been invented as well.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些变种的残差块被发明出来。
- en: 'A wide version (Wide-ResNet) of the previous residual block simply consists
    of increasing the number of outputs per residual blocks by a factor as they come
    to the end:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个宽版（Wide-ResNet）残差块是通过增加每个残差块的输出数量来构建的，当它们到达末端时，这个增加是通过一个倍数来实现的：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A bottleneck version consists of reducing the number of parameters per layer,
    to create a bottleneck that has the effect of dimension reduction, implementing
    the Hebbian theory *Neurons that fire together wire together*, and to help residual
    blocks capture particular types of pattern in the signal:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个瓶颈版本通过减少每层的参数数量来创建一个瓶颈，它具有降维效果，实施赫布理论 *共同发放的神经元会相互连接*，并帮助残差块捕获信号中的特定模式：
- en: '![Residual connections](img/00111.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![残差连接](img/00111.jpeg)'
- en: 'Bottlenecks are reductions in both featuremap size and number of output at
    the same time, not keeping the number of parameters constant per layer as in the
    previous practice:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈是同时减少特征图大小和输出数量，而不是像之前的做法那样保持每层参数数量不变：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, the full network of three stacks of residual blocks is built with:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，完整的三堆残差块网络已经构建完成：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The command for a MNIST training:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 MNIST 训练的命令：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This gives a top-1 accuracy of 98%.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了98%的 top-1 精度。
- en: 'On Cifar 10, residual networks with more than a 100 layers require the batch
    size to be reduced to 64 to fit into the GPU''s memory:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Cifar 10 上，残差网络层数超过100层时，需要将批量大小减少到 64，以适应 GPU 的内存：
- en: 'For ResNet-110 (6 x 18 + 2):'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ResNet-110（6 x 18 + 2）：
- en: '[PRE10]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'ResNet-164 (6 x 27 + 2):'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-164（6 x 27 + 2）：
- en: '[PRE11]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Wide ResNet-110:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽版 ResNet-110：
- en: '[PRE12]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With ResNet-bottleneck-164:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ResNet-bottleneck-164：
- en: '[PRE13]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For Food-101, I reduce further the batch size for ResNet 110:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Food-101，我进一步减少了 ResNet 110 的批量大小：
- en: '[PRE14]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Stochastic depth
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机深度
- en: Since the propagation of the signal through the layers might be prone to errors
    in any of the residual blocks, the idea of stochastic depth is to train the network
    to robustness by randomly removing some of the residual blocks, and replacing
    them with an identity connection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于信号在层间传播时可能在任何一个残差块中出现错误，随机深度的想法是通过随机移除一些残差块并用恒等连接替代，来训练网络的鲁棒性。
- en: 'First, the training is much faster, since the number of parameters is lower.
    Second,in practice, the robustness is proven and it provides better classification
    results:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于参数数量较少，训练速度更快。其次，实践证明它具有鲁棒性，并且能提供更好的分类结果：
- en: '![Stochastic depth](img/00112.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![随机深度](img/00112.jpeg)'
- en: Dense connections
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集连接
- en: 'Stochastic depth skips some random layers by creating a direct connection.
    Going one step further, instead of removing some random layers, another way to
    do the same thing is to add an identity connection with previous layers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随机深度通过创建直接连接来跳过一些随机的层。更进一步地，除了移除一些随机层外，另一种实现相同功能的方法是为之前的层添加一个身份连接：
- en: '![Dense connections](img/00113.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接](img/00113.jpeg)'
- en: A dense block (densely connected convolutional networks)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个密集块（密集连接卷积网络）
- en: 'As for residual blocks, a densely connected convolutional network consists
    of repeating dense blocks to create a stack of layer blocks:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 至于残差块，一个密集连接的卷积网络由重复的密集块组成，以创建一堆层块：
- en: '![Dense connections](img/00114.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![密集连接](img/00114.jpeg)'
- en: A network with dense blocks (densely connected convolutional networks)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 具有密集块的网络（密集连接卷积网络）
- en: 'Such an architecture choice follows the same principles as those seen in [Chapter
    10](part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 10. Predicting
    Times Sequences with Advanced RNN"), *Predicting Times Sequence with Advanced
    RNN*, with highway networks: the identity connection helps the information to
    be correctly propagated and back-propagated through the network, reducing the
    effect of *exploding/vanishing gradients* when the number of layers is high.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构选择遵循了在[第10章](part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b
    "第10章 使用高级RNN预测时间序列")中看到的相同原则，*使用高级RNN预测时间序列*，带有高速公路网络：身份连接有助于信息在网络中正确传播和反向传播，从而减少了在层数较高时出现的*梯度爆炸/消失*问题。
- en: 'In Python, we replace our residual block with a densely connected block:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们将残差块替换为一个密集连接块：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note also that batch normalization is done feature by feature and, since the
    output of every block is already normalized, a second renormalization is not necessary.
    Replacing the batch normalization layer by a simple affine layer learning the
    scale and bias on the concated normalized features is sufficient:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另请注意，批量归一化是逐特征进行的，由于每个块的输出已经归一化，因此不需要第二次归一化。用一个简单的仿射层替代批量归一化层，学习连接归一化特征的尺度和偏置即可：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For training DenseNet-40:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练DenseNet-40：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Multi-GPU
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多GPU
- en: Cifar and MNIST images are still small, below 35x35 pixels. Training on natural
    images requires the preservation of details in the images. So, for example, a
    good input size is 224x224, which is 40 times more. When image classification
    nets with such input size have a few hundred layers, GPU memory limits the batch
    size to a dozen images and so training a batch takes a long time.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Cifar和MNIST图像仍然较小，低于35x35像素。自然图像的训练需要保留图像中的细节。例如，224x224的输入大小就非常合适，这比35x35大了40倍。当具有如此输入大小的图像分类网络有几百层时，GPU内存限制了批次大小，最多只能处理十几张图像，因此训练一个批次需要很长时间。
- en: 'To work in multi-GPU mode:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要在多GPU模式下工作：
- en: The model parameters are in a shared variable, meaning shared between CPU /
    GPU 1 / GPU 2 / GPU 3 / GPU 4, as in single GPU mode.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数是共享变量，意味着在CPU / GPU 1 / GPU 2 / GPU 3 / GPU 4之间共享，和单GPU模式一样。
- en: The batch is divided into four splits, and each split is sent to a different
    GPU for the computation. The network output is computed on the split, and the
    gradients retro-propagated to each weight. The GPU returns the gradient values
    for each weight.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批次被分成四个部分，每个部分被送到不同的GPU进行计算。网络输出在每个部分上计算，梯度被反向传播到每个权重。GPU返回每个权重的梯度值。
- en: The gradients for each weight are fetched back from the multiple GPU to the
    CPU and stacked together. The stacked gradients represent the gradient of the
    full initial batch.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个权重的梯度从多个GPU拉回到CPU并堆叠在一起。堆叠后的梯度代表了整个初始批次的梯度。
- en: The update rule applies to the batch gradients and updates the shared model
    weights.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新规则应用于批次梯度，并更新共享的模型权重。
- en: 'See the following figure:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见下图：
- en: '![Multi-GPU](img/00115.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![多GPU](img/00115.jpeg)'
- en: Theano stable version supports only one GPU per process, so use the first GPU
    in your main program and launch sub-processes for each GPU to train on. Note that
    the cycle described in the preceding image requires the synchronization of the
    update of the model to avoid each GPU training on unsynchronized models. Instead
    of reprogramming it yourself, a Platoon ([https://github.com/mila-udem/platoon](https://github.com/mila-udem/platoon))
    framework is dedicated to train your models across multiple GPUs inside one node.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Theano稳定版本仅支持每个进程一个GPU，因此在主程序中使用第一个GPU，并为每个GPU启动子进程进行训练。请注意，前述图像中的循环需要同步模型的更新，以避免每个GPU在不同步的模型上进行训练。与其自己重新编程，不如使用Platoon框架（[https://github.com/mila-udem/platoon](https://github.com/mila-udem/platoon)），该框架专门用于在一个节点内跨多个GPU训练模型。
- en: Note, too, that it would also be more accurate to synchronize the batch normalization
    mean and variance across multiple GPUs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，值得注意的是，将多个GPU上的批量归一化均值和方差同步会更加准确。
- en: Data augmentation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'Data augmentation is a very important technique to improve classification accuracy.
    Data augmentation consists of creating new samples from existing samples, by adding
    some jitters such as:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是提高分类精度的一个非常重要的技术。数据增强通过从现有样本创建新样本来实现，方法是添加一些抖动，例如：
- en: Random scale
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放
- en: Random sized crop
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机大小裁剪
- en: Horizontal flip
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平翻转
- en: Random rotation
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机旋转
- en: Lighting noise
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光照噪声
- en: Brightness jittering
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亮度抖动
- en: Saturation jittering
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 饱和度抖动
- en: Contrast jittering
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比抖动
- en: This will help the model to be more robust to different lighting conditions
    that are very common in real life.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助模型在现实生活中常见的不同光照条件下变得更加鲁棒。
- en: Instead of always seeing the same dataset, the model discovers different samples
    at each epoch.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 模型每一轮都会发现不同的样本，而不是始终看到相同的数据集。
- en: Note that input normalization is also important to get better results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入归一化对于获得更好的结果也很重要。
- en: Further reading
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'You can refer to the following titles for further insights:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下标题以获得更多见解：
- en: Densely Connected Convolutional Networks, by Gao Huang, Zhuang Liu, Kilian Q.
    Weinberger, and Laurens van der Maaten, Dec 2016
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集连接卷积网络，Gao Huang，Zhuang Liu，Kilian Q. Weinberger 和 Laurens van der Maaten，2016年12月
- en: 'Code has been inspired by the Lasagne repository:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码灵感来源于Lasagne仓库：
- en: '[https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py](https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py)'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py](https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py)'
- en: '[https://github.com/Lasagne/Recipes/tree/master/papers/densenet](https://github.com/Lasagne/Recipes/tree/master/papers/densenet)'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/Lasagne/Recipes/tree/master/papers/densenet](https://github.com/Lasagne/Recipes/tree/master/papers/densenet)'
- en: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,
    Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi, 2016
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception-v4，Inception-ResNet 和残差连接对学习的影响，Christian Szegedy，Sergey Ioffe，Vincent
    Vanhoucke 和 Alex Alemi，2016
- en: Deep Residual Learning for Image Recognition, Kaiming He, Xiangyu Zhang, and
    Shaoqing Ren, Jian Sun 2015
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别的深度残差学习，Kaiming He，Xiangyu Zhang 和 Shaoqing Ren，Jian Sun，2015
- en: Rethinking the Inception Architecture for Computer Vision, Christian Szegedy,
    Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, 2015
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新思考计算机视觉中的Inception架构，Christian Szegedy，Vincent Vanhoucke，Sergey Ioffe，Jonathon
    Shlens 和 Zbigniew Wojna，2015
- en: Wide Residual Networks, Sergey Zagoruyko, and Nikos Komodakis, 2016
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽残差网络，Sergey Zagoruyko 和 Nikos Komodakis，2016
- en: Identity Mappings in Deep Residual Networks, Kaiming He, Xiangyu Zhang, Shaoqing
    Ren, and Jian Sun, Jul 2016
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度残差网络中的恒等映射，Kaiming He，Xiangyu Zhang，Shaoqing Ren 和 Jian Sun，2016年7月
- en: Network In Network, Min Lin, Qiang Chen, Shuicheng Yan, 2013
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中的网络，Min Lin，Qiang Chen，Shuicheng Yan，2013
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: New techniques have been presented to achieve state-of-the-art classification
    results, such as batch normalization, global average pooling, residual connections,
    and dense blocks.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了新技术来实现最先进的分类结果，如批量归一化、全局平均池化、残差连接和密集块。
- en: These techniques have led to the building residual networks, and densely connected
    networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术推动了残差网络和密集连接网络的构建。
- en: The use of multiple GPUs helps training image classification networks, which
    have numerous convolutional layers, large reception fields, and for which the
    batched inputs of images are heavy in memory usage.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个GPU有助于训练图像分类网络，这些网络具有多个卷积层、大的感受野，并且批量输入的图像在内存使用上较重。
- en: Lastly, we looked at how data augmentation techniques will enable an increase
    of the size of the dataset, reducing the potential of model overfitting, and learning
    weights for more robust networks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们研究了数据增强技术如何增加数据集的大小，减少模型过拟合的可能性，并学习更稳健网络的权重。
- en: In the next chapter, we'll see how to use the early layers of these networks
    as features to build encoder networks, as well as how to reverse the convolutions
    to reconstruct an output image to perform pixel-wise predictions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何利用这些网络的早期层作为特征来构建编码器网络，以及如何反转卷积以重建输出图像，以进行像素级预测。
