- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Linear Regression with TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行线性回归
- en: In this chapter, we will cover the concept of linear regression and how we can
    implement it using TensorFlow. We will start by discussing what linear regression
    is, how it works, its underlying assumptions, and the type of problems that can
    be solved using it. Next, we will examine the various evaluation metrics used
    in regression modeling, such as mean squared error, mean absolute error, root
    mean squared error, and R-squared, and strive to understand how to interpret the
    results from these metrics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍线性回归的概念以及如何使用 TensorFlow 实现它。我们将首先讨论什么是线性回归，它是如何工作的，它的基本假设是什么，以及可以用它解决的各种问题类型。接下来，我们将探讨回归建模中使用的各种评估指标，例如均方误差、平均绝对误差、均方根误差和决定系数
    R²，并努力理解如何解释这些指标的结果。
- en: To get hands-on, we will implement linear regression by building a real-world
    use case where we predict employees’ salaries using various attributes. Here,
    we will learn in a hands-on fashion how to load and pre-process data, covering
    important ideas such as handling missing values, encoding categorical variables,
    and normalizing the data for modeling. Then, we will explore the process of building,
    compiling, and fitting a linear regression model with TensorFlow, as well as examine
    concepts such as underfitting and overfitting and their impact on our model’s
    performance. Before we close this chapter, you will also learn how to save and
    load a trained model to make predictions on unseen data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行实践操作，我们将通过构建一个实际案例来实现线性回归，在这个案例中，我们使用不同的属性预测员工的薪资。在这个过程中，我们将通过动手实践学习如何加载和预处理数据，涵盖处理缺失值、编码分类变量和标准化数据等重要概念。接着，我们将探索如何使用
    TensorFlow 构建、编译和拟合线性回归模型，并审视像欠拟合和过拟合这样的概念及其对模型性能的影响。在本章结束前，您还将学习如何保存和加载训练好的模型，以便对未见过的数据进行预测。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Linear regression with TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行线性回归
- en: Evaluating regression models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Salary prediction with TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 预测工资
- en: Saving and loading models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的保存与加载
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will use a `python >= 3.8.0`, along with the following packages, which can
    be installed using the `pip` `install` command:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `python >= 3.8.0`，并安装以下包，您可以通过 `pip install` 命令安装：
- en: '`tensorflow>=2.7.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow>=2.7.0`'
- en: '`tensorflow-datasets==4.4.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow-datasets==4.4.0`'
- en: '`pillow==8.4.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pillow==8.4.0`'
- en: '`pandas==1.3.4`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas==1.3.4`'
- en: '`numpy==1.21.4`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy==1.21.4`'
- en: '`scipy==1.7.3`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scipy==1.7.3`'
- en: Linear regression with TensorFlow
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行线性回归
- en: '**Linear regression** is a supervised machine learning technique that models
    the linear relationship between the predicted output variable (dependent variable)
    and one or more independent variables. When one independent variable can be used
    to effectively predict the output variable, we have a case of *simple linear regression*,
    which can be represented by the equation *y = wX + b*, where *y* is the target
    variable, *X* is the input variable, *w* is the weight of the feature(s), and
    *b* is the bias.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**是一种监督式机器学习技术，用于建模预测输出变量（因变量）与一个或多个自变量之间的线性关系。当一个自变量可以有效地预测输出变量时，我们就得到了*简单线性回归*的案例，其可以用方程
    *y = wX + b* 表示，其中 *y* 是目标变量，*X* 是输入变量，*w* 是特征的权重，*b* 是偏置。'
- en: '![Figure 3.1 – A plot showing simple linear regression](img/B18118_03_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 展示简单线性回归的图表](img/B18118_03_001.jpg)'
- en: Figure 3.1 – A plot showing simple linear regression
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 展示简单线性回归的图表
- en: In *Figure 3**.1*, the straight line, referred to as the regression line (the
    line of best fit), is the line that optimally models the relationship between
    *X* and *y*. Hence, we can use it to determine the dependent variable based on
    the current value of the independent variable at a certain point on the plot.
    The objective of linear regression is to find the best values of *w* and *b*,
    which model the underlying relationship between *X* and *y*. The closer the predicted
    value is to the ground truth, the smaller the error.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 3.1* 中，直线被称为回归线（最佳拟合线），它是最优地建模 *X* 与 *y* 之间关系的直线。因此，我们可以利用它根据图表中某一点上自变量的当前值来确定因变量。线性回归的目标是找到
    *w* 和 *b* 的最佳值，它们能够建模 *X* 和 *y* 之间的潜在关系。预测值越接近真实值，误差就越小。
- en: Conversely, when we have more than one input variable used to predict the output
    value, then we have a case of *multiple linear regression*, and we can represent
    it by the equation *y = b0 + b1X1 + b2X2 + .... + bnXn*, where *y* is the target
    variable, *X1*, *X2*, ... *Xn* are input variables, *b0* is the bias, and *b1*,
    *b2*, ... *bn* are the feature weights.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，当我们有多个输入变量来预测输出值时，就出现了*多重线性回归*的情况，我们可以用方程 *y = b0 + b1X1 + b2X2 + .... +
    bnXn* 来表示，其中 *y* 是目标变量，*X1*、*X2*、... *Xn* 是输入变量，*b0* 是偏差项，*b1*、*b2*、... *bn* 是特征权重。
- en: Simple linear and multiple linear regression have lots of real-world applications,
    as they are simple to implement and computationally cheap. Hence, they can be
    easily applied to large datasets. However, linear regression may fail when we
    try to model nonlinear relationships between *X* and *y*, or when there are many
    irrelevant features in our input data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归和多重线性回归有很多实际应用，因为它们易于实现且计算成本低。因此，它们可以很容易地应用于大规模数据集。然而，当我们试图建立 *X* 和 *y*
    之间的非线性关系模型，或者输入数据中包含大量无关特征时，线性回归可能会失效。
- en: Linear regression is widely used to solve a wide range of real-world problems
    across different domains. For example, we can apply linear regression to predict
    the price of a house using factors such as the size, number of bedrooms, location,
    and proximity to social amenities. Similarly, in the field of **human resource**
    (**HR**), we can use linear regression to predict the salary of new hires, based
    on factors such as years of experience of the candidate and their level of education.
    These are a few examples of what is possible using linear regression. Next, let
    us see how we can evaluate linear regression models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归广泛应用于解决各个领域的实际问题。例如，我们可以应用线性回归预测房屋价格，考虑因素如房屋大小、卧室数量、位置以及距离社会设施的远近。同样，在**人力资源**（**HR**）领域，我们可以利用线性回归预测新员工的薪资，考虑因素包括候选人的工作经验年限和教育水平。这些都是使用线性回归可以实现的一些例子。接下来，让我们看看如何评估线性回归模型。
- en: Evaluating regression models
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: In our *hello world* example from [*Chapter 2*](B18118_02.xhtml#_idTextAnchor045),
    *Introduction to TensorFlow*, we tried to predict a student’s test score when
    the student spent 38 hours studying during the term. Our study model arrived at
    81.07 marks, while the true value was 81\. So, we were close but not completely
    correct. When we subtract the difference between our model’s prediction and the
    ground truth, we get a residual of 0.07\. The residual value could be either positive
    or negative, depending on whether our model overestimates or underestimates the
    predicted result. When we take the absolute value of the residual, we eliminate
    any negative signs; hence, the absolute error will always be a positive value,
    irrespective of whether the residual is positive or negative.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们来自 [*第2章*](B18118_02.xhtml#_idTextAnchor045) 的 *hello world* 示例中，*TensorFlow简介*，我们尝试预测学生在学期内学习了38小时后，考试成绩是多少。我们的模型得出了81.07分，而真实值是81分。因此，我们很接近，但并不完全正确。当我们减去模型预测值和真实值之间的差异时，我们得到了一个残差0.07。残差值可以是正数也可以是负数，具体取决于我们的模型是高估还是低估了预测结果。当我们取残差的绝对值时，就消除了任何负号；因此，绝对误差总是一个正值，无论残差是正还是负。
- en: 'The formula for absolute error is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差的公式如下：
- en: Absolute error = |Y pred − Y true|
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差 = |Y pred − Y true|
- en: where Y pred = the predicted value and Y true = the ground truth.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Y pred = 预测值，Y true = 真实值。
- en: 'The **mean absolute error** (**MAE**) of a model is the average of all absolute
    errors of the data points under consideration. MAE measures the average of the
    residuals and can be represented using the following equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是模型的所有数据点的绝对误差的平均值。MAE 衡量的是残差的平均值，可以通过以下公式表示：'
- en: MAE =  1 _ n  ∑ i=1 n  |Y pred − Y true|
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MAE = 1 _ n ∑ i=1 n |Y pred − Y true|
- en: 'where:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*n* = the number of data points under consideration'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* = 所考虑的数据点数量'
- en: ∑ = summation of the absolute errors of all the observations
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∑ = 所有观测值的绝对误差之和
- en: '|Y pred − Y true| = Absolute value'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|Y pred − Y true| = 绝对值'
- en: If the MAE = 0, it means that Y pred = Y true. This means the model is 100 percent
    accurate; although this is an ideal scenario, it is highly unlikely. On the flip
    side, if MAE= ∞, this means the model is completely off, as it fails to capture
    any relationship between the input and output variables. The larger the error,
    the larger the value of the MAE. For performance evaluation, we aim for low values
    of MAE, but because MAE is a relative metric whose value depends on the scale
    of the data you work with, it is difficult to compare MAE results across different
    datasets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果MAE = 0，这意味着Y_pred = Y_true。这意味着模型的预测完全准确；尽管这是一个理想的情况，但极不可能发生。相反，如果MAE = ∞，则表示模型完全失败，因为它未能捕捉到输入与输出变量之间的任何关系。误差越大，MAE的值也越大。在性能评估中，我们希望MAE的值较低，但由于MAE是一个相对度量，其值取决于所处理数据的规模，因此很难在不同数据集之间比较MAE的结果。
- en: 'Another important evaluation metric is the **mean squared error** (**MSE**).
    MSE, in contrast to MAE, squares the residuals, thus removing any negative values
    in the residuals. MSE is represented using the following equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的评估指标是**均方误差**（**MSE**）。与MAE不同，MSE对残差进行了平方处理，从而去除了残差中的负值。MSE用以下公式表示：
- en: MSE =  1 _ N  ∑ i=1 N ( Y pred − Y true) 2
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MSE = 1 _ N ∑ i=1 N ( Y pred − Y true) 2
- en: Like MAE, when there are no residuals, we have a perfect model. So, the lower
    the MSE value, the better the performance of the model. Unlike MAE, where large
    or small errors have a proportional impact, MSE penalizes larger errors in comparison
    to smaller errors, and it has a higher order of units, since we square the residual
    in this instant.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与MAE类似，当没有残差时，我们有一个完美的模型。因此，MSE值越低，模型性能越好。与MAE不同，MAE中的大误差或小误差对结果有相同比例的影响，而MSE会对较大的误差进行惩罚，相较于小误差，它具有更高的单位阶数，因为我们在此情况下对残差进行了平方。
- en: 'Another useful metric in regression modeling is the **root mean square error**
    (**RMSE**). As the name suggests, it is the square root of MSE, as shown in the
    equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回归建模中另一个有用的指标是**均方根误差**（**RMSE**）。顾名思义，它是MSE的平方根，如下所示的公式所示：
- en: MSE =  1 _ N  ∑ i=1 N ( Y pred − Y true) 2
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MSE = 1 _ N ∑ i=1 N ( Y pred − Y true) 2
- en: RMSE = √ _ MSE  = √ ________________   1 _ N  ∑ i=1 N ( Y pred − Y true) 2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE = √_ MSE = √ ________________ 1 _ N ∑ i=1 N ( Y pred − Y true) 2
- en: 'Lastly, let us look at the **coefficient of determination** (**R squared**).
    R2 measures how well the dependent variable is explained by the independent variables
    in a regression modeling task. We can calculate R2 with this equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一下**决定系数**（**R平方**）。R²衡量回归建模任务中，因变量在多大程度上可以由自变量解释。我们可以通过以下公式计算R²：
- en: R 2 = 1 −  R res _ R tot
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: R 2 = 1 − R res _ R tot
- en: where Rres is the sum of the square of residuals and Rtot is the total sum of
    squares. The closer the value of R2 is to 1, the more accurate the model is, and
    the closer the R2 value of a model is to 0, the worse the model is. Also, it is
    possible for R² to take on a negative value. This happens when the model does
    not follow the trend of the data – in this instance, Rres is greater than Rtot.
    A negative R2 is a sign that our model requires significant improvement due to
    its poor performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Rres是残差平方和，Rtot是总平方和。R²值越接近1，模型越准确；R²值越接近0，模型越差。此外，R²值也可能是负数。这发生在模型未能遵循数据趋势的情况下——此时，Rres大于Rtot。负的R²是模型性能差的标志，说明我们的模型需要显著改进。
- en: We have looked at some regression evaluation metrics. The good news is that
    we will not work them out by hand; we will leverage the `tf.keras.metrics` module
    from TensorFlow to help us do the heavy lifting. We have breezed quickly through
    the theory at a high level. Now, let us examine a multiple linear regression case
    study to enable us to understand all the moving parts required to build a model
    with TensorFlow, as well as understand how to evaluate, save, load, and use our
    trained model to make predictions on new data. Let’s proceed to our case study.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了一些回归评估指标。好消息是，我们不需要手动计算它们；我们将利用TensorFlow中的`tf.keras.metrics`模块来帮助我们完成这些繁重的计算。我们已经从高层次快速浏览了理论。接下来，让我们通过一个多元线性回归的案例研究，帮助我们理解构建TensorFlow模型所需的各个部分，同时了解如何评估、保存、加载并使用训练好的模型来对新数据进行预测。让我们继续进行案例研究。
- en: Salary prediction with TensorFlow
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行薪资预测
- en: In this case study, you will assume the role of a new machine learning engineer
    at Tensor Limited, a rapidly growing start-up with over 200 employees. Now, the
    company wants to hire seven new employees, and the HR department is having a hard
    time coming up with the ideal salary based on varying qualifications, years of
    experience, the roles applied for, and the level of training of each of the potential
    new hires. Your job is to work with the HR unit to determine the optimal salary
    for each of these potential hires.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，你将假设自己是Tensor Limited（一个快速增长的初创公司，拥有200多名员工）的新晋机器学习工程师。目前，公司希望招聘7名新员工，人力资源部门难以根据不同的资质、工作经验、申请职位以及每个潜在新员工的培训水平来确定理想的薪资。你的任务是与人力资源部门合作，为这些潜在新员工确定最优薪资。
- en: Luckily, we went through the machine learning life cycle in [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014),
    *Introduction to Machine Learning,* built our hello world case study in [*Chapter
    2*](B18118_02.xhtml#_idTextAnchor045), *Introduction to TensorFlow,* and have
    already covered some key evaluation metrics required for regression modeling in
    this chapter. So, you are well equipped theoretically to carry out the task. You
    have had a productive discussion with the HR manager, and now you have a better
    understanding of the task and the requirements. You defined your task as a supervised
    learning task (regression). Also, the HR unit allowed you to download employee
    records and their corresponding salaries for this task. Now that you have the
    dataset, let us proceed to load the data into our notebook.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们已经在[*第1章*](B18118_01.xhtml#_idTextAnchor014)《机器学习简介》中讲解了机器学习生命周期，[*第2章*](B18118_02.xhtml#_idTextAnchor045)《TensorFlow简介》里也构建了我们的Hello
    World案例研究，并且在本章中已经涵盖了回归建模所需的一些关键评估指标。因此，从理论上讲，你已经充分准备好执行这个任务。你已经与人力资源经理进行了富有成效的讨论，现在对任务和要求有了更清晰的理解。你将任务定义为一个监督学习任务（回归）。同时，人力资源部门允许你下载员工记录和相应的薪资数据来完成这个任务。现在你已经拥有数据集，让我们继续将数据加载到笔记本中。
- en: Loading the data
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'Perform the following steps to load the dataset:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以加载数据集：
- en: 'Open the notebook called `Linear_Regression_with_TensorFlow.ipynb`. We will
    start by importing all the necessary libraries for this project:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开名为`Linear_Regression_with_TensorFlow.ipynb`的笔记本。我们将从导入所有必要的库开始：
- en: '[PRE0]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will run this code block. If everything goes well, we will get to see the
    version of TensorFlow we are using:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行这段代码块。如果一切顺利，我们将看到我们正在使用的TensorFlow版本：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will import some additional libraries that will help us simplify our
    workflow:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将导入一些额外的库，这些库将帮助我们简化工作流程：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will run this cell, and everything should work perfectly. NumPy is a scientific
    computing library in Python that is used to perform mathematical operations on
    arrays, while pandas is a built-in Python library for data analysis and manipulation.
    Matplotlib and Seaborn are used to visualize data, and we will use sklearn for
    data preprocessing and splitting our data. We will apply these libraries in this
    case study, and you will get to understand what they do and also be able to apply
    them in your exam and beyond.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行这个单元格，应该一切正常。NumPy是一个Python中的科学计算库，用于对数组执行数学运算，而pandas是Python内置的用于数据分析和处理的库。Matplotlib和Seaborn用于数据可视化，我们将使用sklearn进行数据预处理和数据拆分。在这个案例研究中，我们将应用这些库，你将了解它们的作用，并且能够在考试中以及之后的工作中应用它们。
- en: 'Now, we will proceed to load the dataset, which we got from the HR team for
    this project:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将继续加载数据集，这个数据集是我们从人力资源团队获取的，用于这个项目：
- en: '[PRE13]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will use pandas to generate a DataFrame that holds the record in a tabular
    format, and we will use `df.head()` to print the first five entries in the data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pandas生成一个DataFrame，以表格格式保存记录，并将使用`df.head()`打印数据中的前五个条目：
- en: '![Figure 3.2 – A DataFrame showing a snapshot of our dataset](img/B18118_03_002.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 显示数据集快照的DataFrame](img/B18118_03_002.jpg)'
- en: Figure 3.2 – A DataFrame showing a snapshot of our dataset
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 显示数据集快照的DataFrame
- en: We now have a sense of what data was collected, based on the details captured
    in each column. We will proceed to explore the data to see what we can learn and
    how we can effectively develop a solution to meet the business objective. Let
    us proceed by looking at data pre-processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每一列捕捉的细节，我们现在对收集的数据有了初步了解。接下来，我们将继续探索数据，看看我们能学到什么，并且如何有效地开发一个解决方案来实现业务目标。让我们通过查看数据预处理部分继续。
- en: Data preprocessing
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'To be able to model our data, we need to ensure it is in the right form (i.e.,
    numerical values). Also, we will need to deal with missing values and remove irrelevant
    features. In the real world, data preprocessing takes a long time. You will hear
    this repeatedly, and it is true. Without correctly shaping the data, we cannot
    model it. Let’s jump in and see how we can do this for our current task. From
    the DataFrame, we can immediately see that there are some irrelevant columns,
    and they hold personally identifiable information about employees. So, we will
    remove and also inform HR about this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够对数据进行建模，我们需要确保数据是正确的格式（即数值型数据）。此外，我们还需要处理缺失值并去除无关的特征。在实际应用中，数据预处理通常需要很长时间。你会反复听到这一点，且这是真的。如果数据没有正确地整理，我们就无法进行建模。让我们深入了解一下，看看如何为当前任务做这些操作。从DataFrame中，我们可以立即看到一些无关的列，它们包含员工的个人身份信息。因此，我们将删除这些列，并告知人力资源部门：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will use the `drop` function in pandas to drop the name, phone number, and
    date of birth columns. We will now display the DataFrame again using `df.head()`
    to show the first five rows of the data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pandas中的`drop`函数删除姓名、电话号码和出生日期列。接下来，我们将再次使用`df.head()`显示DataFrame，展示数据的前五行：
- en: '![Figure 3.3 – The first five rows of the DataFrame after dropping the columns](img/B18118_03_003.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 删除列后的DataFrame前五行](img/B18118_03_003.jpg)'
- en: Figure 3.3 – The first five rows of the DataFrame after dropping the columns
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 删除列后的DataFrame前五行
- en: 'We have successfully removed the irrelevant columns, so we can now proceed
    and check for missing values in our dataset using the `isnull()` function in pandas:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功删除了无关的列，现在可以继续使用pandas中的`isnull()`函数检查数据集中的缺失值：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When we run this code block, we can see that there are no missing values in
    the `University` and `Salary` columns. However, we have missing values for the
    `Role`, `Cert`, `Qualification`, and `Experience` columns:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个代码块时，可以看到`University`和`Salary`列没有缺失值。然而，`Role`、`Cert`、`Qualification`和`Experience`列存在缺失值：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'There are a number of ways to handle missing values – from simply asking HR
    to fix the omissions to simple imputations or replacements using mean, median,
    or mode. In this case study, we will drop the rows with missing values, since
    it’s a small subset of our data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值有多种方法——从简单地要求HR修复遗漏，到使用均值、中位数或众数进行简单的填补或替换。在这个案例研究中，我们将删除含有缺失值的行，因为它是我们数据的一个小子集：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We use the `dropna` function to drop all the missing values in the dataset,
    and then we save the new dataset in `df`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`dropna`函数删除数据集中的所有缺失值，然后将新的数据集保存为`df`。
- en: Note
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you want to learn more about how to handle missing values, check out this
    playlist by Data Scholar: [https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u](https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于如何处理缺失值的内容，可以查看Data Scholar的这个播放列表：[https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u](https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u)。
- en: 'Now, we need to check to ensure that there are no more missing values using
    the `isnull()` function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要检查确保没有更多的缺失值，使用`isnull()`函数：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the code, and let’s see whether there are any missing values:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码，看看是否还有缺失值：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that there are no missing values in our dataset anymore. Our model
    requires us to pass in numerical values for it to be able to model our data and
    predict the target variable, so let us look at the data types:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据集中不再有缺失值。我们的模型要求输入数值型数据，才能对数据进行建模并预测目标变量，因此让我们来看看数据类型：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we run the code, we get an output showing the different columns and their
    data types:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们会得到一个输出，显示不同的列及其数据类型：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'From the output, we can see that experience and salary are numeric values,
    since they are `float` and `int`, respectively, while `Qualification`, `University`,
    `Role`, and `Cert` are categorical values. This means we cannot train our model
    yet; we have to find a way to convert our categorical values to numerical values.
    Luckily, this is possible via a process called one-hot encoding. `get_dummies`
    function in pandas to achieve this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到`experience`和`salary`是数值型数据，因为它们分别是`float`和`int`类型，而`Qualification`、`University`、`Role`和`Cert`是分类数据。这意味着我们还不能训练模型；我们必须找到一种方法将分类数据转换为数值数据。幸运的是，这可以通过一个叫做独热编码（one-hot
    encoding）的过程来实现。我们可以使用pandas中的`get_dummies`函数来完成这一任务：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When we run the code, we will get a DataFrame like the one displayed in *Figure
    3**.4*. We use the `drop_first` argument to drop the first category.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们将得到一个类似于*图 3.4*中显示的DataFrame。我们使用`drop_first`参数来删除第一类。
- en: '![Figure 3.4 – A DataFrame showing numerical values](img/B18118_03_004.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 显示数值的 DataFrame](img/B18118_03_004.jpg)'
- en: Figure 3.4 – A DataFrame showing numerical values
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 显示数值的 DataFrame
- en: If you are confused as to why we dropped one of the categorical columns, let’s
    look at the `Cert` column, which was made up of yes or no. values If we performed
    one hot encoding, without dropping any columns, we would have two `Cert` columns,
    as displayed in *Figure 3**.5*. In the `Cert_No` column, if the employee has a
    relevant certification, the column gets a value of `0`, and when the employee
    does not have a relevant certification, the column gets a value of `1`. Looking
    at the `Cert_Yes` column, we can see that when an employee has a certificate,
    the column gets a value of `1`; otherwise, it gets `0`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不理解为什么我们删除了其中一个分类列，让我们来看一下`Cert`列，它由“是”或“否”值组成。如果我们进行了独热编码，但没有删除任何列，那么我们将有两个`Cert`列，如*图
    3.5*所示。在`Cert_No`列中，如果员工有相关证书，该列的值为`0`，如果员工没有相关证书，该列的值为`1`。查看`Cert_Yes`列，我们可以看到，当员工有证书时，该列的值为`1`；否则，该列的值为`0`。
- en: '![Figure 3.5 – The dummy variables from the Cert column](img/B18118_03_005.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 来自 Cert 列的虚拟变量](img/B18118_03_005.jpg)'
- en: Figure 3.5 – The dummy variables from the Cert column
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 来自 Cert 列的虚拟变量
- en: From *Figure 3**.5*, we can see that both columns can be used to show whether
    an employee has a certificate or not. Using both dummy columns generated from
    our certificate column will lead to the *dummy variable trap*. This occurs when
    our one-hot encoded columns are strongly related or correlated, where one column
    can effectively explain the other column. Hence, we say both columns are multicollinear,
    and *multicollinearity* can lead to the overfitting of our model. We will talk
    more about overfitting in [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105)*, Image
    Classification with* *Neural Networks*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 3.5*中，我们可以看到两列都可以用来显示员工是否有证书。使用从证书列生成的两个虚拟列会导致*虚拟变量陷阱*。当我们的独热编码列之间存在强相关性时，就会发生这种情况，其中一列可以有效地解释另一列。因此，我们说这两列是多重共线性的，而*多重共线性*可能导致模型过拟合。我们将在[*第五章*](B18118_05.xhtml#_idTextAnchor105)中讨论更多关于过拟合的内容，标题为“使用神经网络进行图像分类”。
- en: For now, it is good enough to know that overfitting is a situation where our
    model performs very well on training data but poorly on test data. To avoid the
    dummy variable trap, we will drop one of the columns in *Figure 3**.5*. If there
    are three categories, we only need two columns to capture all three categories;
    if we have four categories, we will only need three columns to capture four categories,
    and so on. Hence, we can drop the extra columns for all the other categorical
    columns as well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们知道过拟合是指我们的模型在训练数据上表现非常好，但在测试数据上表现不佳的情况。为了避免虚拟变量陷阱，我们将删除*图 3.5*中的一列。如果有三类，我们只需要两列就能捕捉到所有三类；如果有四类，我们只需要三列来捕捉所有四类，依此类推。因此，我们可以删除所有其他分类列的多余列。
- en: 'Now, we will use the `corr()` function to get the correlation of our refined
    dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`corr()`函数来获取我们精炼数据集的相关性：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can see that there is a strong correlation between salary and years of experience.
    Also, there is a strong correlation between `Role_Senior` and `Salary`, as shown
    in *Figure 3**.6*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，薪资与工作经验年数之间存在强相关性。同时，`Role_Senior`与`Salary`之间也有较强的相关性，如*图 3.6*所示。
- en: '![Figure 3.6 – The correlation values for our data](img/B18118_03_006.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 我们数据的相关性值](img/B18118_03_006.jpg)'
- en: Figure 3.6 – The correlation values for our data
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 我们数据的相关性值
- en: We have completed the preprocessing phase of our task, or at least for now.
    We have removed all irrelevant columns; we also removed the missing values by
    dropping rows with missing values and, finally, used one-hot encoding to convert
    our categorical values to numeric values. It is important to note that we are
    skipping some **exploratory data analysis** (**EDA**) steps here, such as visualizing
    the data; although that’s an essential step, our core focus for the exam is building
    models with TensorFlow. In our Colab notebook, you will find some additional EDA
    steps; although they are not directly relevant to the exams, they will give you
    a better understanding of your data and help you detect anomalies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了任务的预处理阶段，至少目前是这样。我们已移除所有不相关的列；还通过删除缺失值的行来去除缺失值，最后使用独热编码将分类值转换为数值。需要注意的是，我们在这里跳过了一些**探索性数据分析**（**EDA**）步骤，例如可视化数据；虽然这些步骤很重要，但我们在考试中的核心重点是使用TensorFlow构建模型。在我们的Colab笔记本中，你会找到一些额外的EDA步骤；虽然它们与考试内容无关，但它们将帮助你更好地理解数据，并帮助你检测异常值。
- en: Now, let us move on to the modeling phase.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入建模阶段。
- en: Model building
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型构建
- en: 'To build a model, we will have to sort our data into features (*X*) and the
    target (*y*). To do this, we will run this code block:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建模型，我们必须将数据分为特征（*X*）和目标（*y*）。为此，我们将运行以下代码块：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We will use the `drop()` function to drop the `Salary` column from the `X` variable,
    and we will make the `y` variable the `Salary` column alone, since this is our
    target.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`drop()`函数从`X`变量中删除`Salary`列，同时将`y`变量设为仅包含`Salary`列，因为这就是我们的目标变量。
- en: 'With our features and target variable well defined, we can proceed to split
    our data into training and test sets. This step is important, as it enables our
    model to learn patterns from our data to effectively predict employees’ salaries.
    To achieve this, we train our model using the training set and then evaluate the
    model’s efficacy on the hold-out test set. We discussed this at a high level in
    [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014)*, Introduction to Machine Learning,*
    when we talked about the ML life cycle. It is a very important process, as we
    will use the test set to evaluate our model’s generalization capability before
    we deploy it for real-world use. To split our data into training and testing sets,
    we will use the `sklearn` library:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征和目标变量定义清楚后，我们可以继续将数据分割为训练集和测试集。这一步很重要，因为它使我们的模型能够从数据中学习模式，从而有效地预测员工的薪资。为了实现这一点，我们使用训练集训练模型，然后在留出的测试集上评估模型的效果。我们在[*第1章*](B18118_01.xhtml#_idTextAnchor014)《机器学习导论》中讨论过这一点，介绍了机器学习的生命周期。这是一个非常重要的过程，因为我们将使用测试集来评估模型的泛化能力，然后再将其部署到实际应用中。为了将数据分割为训练集和测试集，我们将使用`sklearn`库：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Using the `train_test_split` function from the `sklearn` library, we split our
    data into training and testing datasets, with a test size of `0.2`. We set the
    `random_state =10` to ensure reproducibility so that every time you use the same
    `random_state` value, you’ll get the same split, even if you run the code multiple
    times. For instance, in our code, we set `random_state` to `10`, which means every
    time we run the code, we will get the same split. If we change this value from
    `10` to, say, `50`, we will get a different shuffled split for our training and
    test set. Setting the `random_state` argument when splitting our data into training
    and test sets is very useful, as it allows us to effectively compare different
    models, since our training set and test sets are the same across all the models
    we experiment with.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`库中的`train_test_split`函数，我们将数据分割为训练集和测试集，测试集大小为`0.2`。我们设置`random_state
    = 10`以确保可重复性，这样每次使用相同的`random_state`值时，即使多次运行代码，也能得到相同的分割。例如，在我们的代码中，我们将`random_state`设置为`10`，这意味着每次运行代码时，我们都会得到相同的分割。如果我们将这个值从`10`改为`50`，那么我们的训练集和测试集就会得到不同的随机分割。在将数据分割为训练集和测试集时，设置`random_state`参数非常有用，因为它可以确保我们有效地比较不同的模型，因为我们在所有实验模型中使用的是相同的训练集和测试集。
- en: When modeling our data in machine learning, we usually use 80 percent of the
    data to train the model and 20 percent of the data to test the model’s generalization
    capability. That’s why we set `test_size` to `0.2` for our dataset. Now that we
    have everything in place, we will start the modeling process in earnest. When
    it comes to building models with TensorFlow, there are three essential steps,
    as illustrated in *Figure 3**.7* – building the model, compiling the model, and
    fitting it to our data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中建模数据时，通常使用80%的数据来训练模型，剩下的20%用来测试模型的泛化能力。这就是为什么我们将`test_size`设置为`0.2`来处理我们的数据集。现在我们已经准备就绪，我们将认真开始建模过程。当涉及使用TensorFlow构建模型时，有三个关键步骤，如*图3**.7*所示
    – 构建模型、编译模型和将其适应我们的数据。
- en: '![Figure 3.7 – The three-step modeling process](img/B18118_03_007.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 三步建模过程](img/B18118_03_007.jpg)'
- en: Figure 3.7 – The three-step modeling process
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 三步建模过程
- en: 'Let us see how we can use this three-step approach to build our salary prediction
    model. We will begin by building our model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用这种三步方法来构建我们的薪资预测模型。我们将从构建我们的模型开始：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In *Figure 3**.8*, we can see the first line of code for our model. Here, we
    generated a single layer using the `Sequential` class as an array. The `Sequential`
    class is used for layer definition. The `Dense` function is used to generate a
    layer of fully connected neuron. In this case, we have just one unit. For our
    activation function here, we employ a linear activation function. Activation functions
    are used to determine the output of a neuron based on a given input or set of
    inputs. Here, the linear activation function simply outputs whatever the input
    is – that is, a direct relationship between the input and the output. Next, we
    pass in the input shape of our data, which in this case is 8, representing the
    features in data (columns) in `X_train`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3**.8*中，我们可以看到我们模型的第一行代码。在这里，我们使用`Sequential`类作为数组生成单个层。`Sequential`类用于定义层。`Dense`函数用于生成一个全连接神经元层。在这种情况下，我们只有一个单元。在这里，我们使用线性激活函数作为激活函数。激活函数用于根据给定的输入或一组输入确定神经元的输出。在线性激活函数中，输出与输入直接相关。接下来，我们传入我们数据的输入形状，在这种情况下是8，表示`X_train`中的数据特征（列）。
- en: '![Figure 3.8 – Building a model in TensorFlow](img/B18118_03_008.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 在TensorFlow中构建模型](img/B18118_03_008.jpg)'
- en: Figure 3.8 – Building a model in TensorFlow
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 在TensorFlow中构建模型
- en: In the first step of our three-step process, we designed the model structure.
    Now, we will move on to the model compilation step. This step is equally important
    as it determines how the model will learn. Here, we specify parameters such as
    the loss function, the optimizer, and the metrics we want to use to evaluate our
    model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们三步骤过程的第一步中，我们设计了模型结构。现在，我们将进入模型编译步骤。这一步同样重要，因为它决定了模型的学习方式。在这里，我们指定参数，如损失函数、优化器以及我们想要用来评估模型的指标。
- en: 'The optimizer determines how our model will update its internal parameters,
    based on the information it gathers from the loss function and the data. The job
    of the loss function is to measure how well our model does on our training data.
    We then use our metrics to monitor the model’s performance on the training step
    and test steps. Here, we use **stochastic gradient descent** (**SGD**) as our
    optimizer and MAE for our loss and evaluation metric:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器决定了我们的模型将如何根据损失函数和数据更新其内部参数。损失函数的作用是衡量模型在训练数据上的表现。然后，我们使用指标来监测模型在训练步骤和测试步骤上的性能。在这里，我们使用**随机梯度下降**
    (**SGD**) 作为我们的优化器，MAE作为我们的损失和评估指标：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, all we have to do is feed our model with training data and the corresponding
    labels, with which our model can learn to intelligently predict the target numerical
    values, which in our case is the expected salary. Every time the model makes a
    prediction, the loss function compares the difference between the model’s prediction
    and the ground truth. This information is passed to the optimizer, which uses
    the information to make an improved prediction until the model can fashion the
    right mathematical equation to accurately predict our employee’s salary.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需用训练数据和相应的标签来喂养我们的模型，我们的模型就能学会智能地预测目标数值，这在我们的情况下是预期的薪水。每次模型进行预测时，损失函数都会比较模型预测与实际值之间的差异。这些信息传递给优化器，优化器利用信息进行改进预测，直到模型能够制定正确的数学方程以准确预测我们员工的薪水。
- en: 'Now, let’s fit our training model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们适应我们的训练模型：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We use `model_1.fit` to fit our training data and labels and set the number
    of tries (epochs) to `50`. In just a few lines of code, we have generated a mini-brain
    that we can train over time to make sensible predictions. Let’s run the code and
    see what the output looks like:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`model_1.fit`来拟合我们的训练数据和标签，并将尝试的次数（代数）设置为`50`。只需几行代码，我们就生成了一个可以随着时间训练的迷你大脑，从而做出合理的预测。让我们运行代码，看看输出是什么样子的：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have displayed the last five tries (epochs `46`–`50`). The error drops gradually;
    however, we end up with a very large error after `50` epochs. Perhaps we can train
    our model for more epochs, as we did in [*Chapter 2*](B18118_02.xhtml#_idTextAnchor045)*,
    Introduction to TensorFlow*. Why not?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了最后五次尝试（`46`–`50`代）。误差逐渐下降；然而，在`50`代后，我们仍然得到了一个非常大的误差。或许我们可以像在[*第二章*](B18118_02.xhtml#_idTextAnchor045)*《TensorFlow简介》*中那样训练更多代数的模型，为什么不呢？
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we simply change the number of epochs to `500` using our single-layer
    model. The activation function, the loss, and optimizers are the same as our initial
    model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需将代数改为`500`，使用我们的单层模型。激活函数、损失函数和优化器与我们的初始模型相同：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'From the last five lines of our output, we can see that the loss is still quite
    high after `500` epochs. You may wish to experiment with the model for longer
    epochs to see how it will fare. It is also a good idea to visualize your model’s
    loss curve to see how it performs. A lower loss indicates a better-performing
    model. With this in mind, let us explore the loss curve for `model_2`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们输出的最后五行中，我们可以看到在`500`代后，损失仍然相当高。你可以尝试让模型训练更长时间，看看它会有什么表现。可视化模型的损失曲线也是一个好主意，这样你可以看到它的表现。较低的损失意味着模型表现更好。考虑到这一点，让我们来探讨`model_2`的损失曲线：
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We will generate a utility plotting function, `visualize_model`, which we will
    use in our experiments to plot the model’s loss over time as it trains. In this
    code, we generate a figure to plot the loss values stored in the `history` object.
    The `history` object is the output of the `fit` function in our three-step modeling
    process, and it holds the loss and metrics values at the end of each epoch.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一个实用的绘图函数`visualize_model`，并在我们的实验中使用它来绘制模型在训练过程中随时间变化的损失。在这段代码中，我们生成一个图形来绘制存储在`history`对象中的损失值。`history`对象是我们三步建模过程中`fit`函数的输出，它包含了每个代结束时的损失和度量值。
- en: 'To plot `model_2`, we simply call the function to visualize the plot and pass
    in `history_2`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制`model_2`，我们只需调用函数来可视化图表，并传入`history_2`：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'When we run the code, we get the plot shown in *Figure 3**.9*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，得到的图形如*图3.9*所示：
- en: '![Figure 3.9 – Model losses at 500](img/B18118_03_009.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 500代的模型损失](img/B18118_03_009.jpg)'
- en: Figure 3.9 – Model losses at 500
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 500代的模型损失
- en: From *Figure 3**.9*, we can see the loss falling, and the rate at which it falls
    is too slow, as it takes **500** epochs to fall from around **97400** to **97000**.
    In your spare time, you can try to train the model for 2,000 or more epochs. It
    will not be able to generalize well, as the model is too simple to handle the
    complexity of our data. In machine learning lingo, we say the model is *underfitting*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图3.9*中，我们可以看到损失在下降，但下降的速度太慢，因为它从大约**97400**下降到**97000**需要**500**代。在你有空时，可以尝试将模型训练2000代或更多。它将无法很好地泛化，因为这个模型太简单，无法处理我们数据的复杂性。在机器学习术语中，我们说这个模型是*欠拟合*的。
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注：
- en: 'There are primarily two ways to build models with TensorFlow – the sequential
    API and the functional API. The sequential API is a simple way of building models
    by using a stack of layers, where data flows in a single direction, from the input
    layer to the output layer. Conversely, the functional API in TensorFlow allows
    us to build more complex models – this includes models with multiple inputs or
    outputs and models with shared layers. Here, we use the Sequential API. For more
    information about building models with the sequential API, check out the documentation:
    [https://www.tensorflow.org/guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建模型主要有两种方式——顺序API和函数式API。顺序API通过使用一系列层来构建模型，数据单向流动，从输入层到输出层，这是一种简单的方式。相反，TensorFlow的函数式API允许我们构建更复杂的模型——这包括具有多个输入或输出的模型以及具有共享层的模型。在这里，我们使用的是顺序API。有关使用顺序API构建模型的更多信息，请查看文档：[https://www.tensorflow.org/guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model)。
- en: 'Hence, let us try to build a more complex model and see whether we can push
    the loss lower and quicker than our initial model:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们尝试构建一个更复杂的模型，看看能否比我们的初始模型更快地降低损失值：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here, we have generated a new model. We stack a 64-neuron layer on top of our
    single-unit layer. We also use a **Rectified Linear Unit** (**ReLU**) activation
    function for this layer; its job is to help our model learn more complex patterns
    in our data and improve computational efficiency. The second layer is our output
    layer, made up of a single neuron because we have a regression task (predicting
    a continuous value). Let’s run it for 500 epochs and see whether this will make
    any difference:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成了一个新模型。我们在单一神经元层上方堆叠了一个64神经元的层。我们还为这一层使用了**整流线性单元**（**ReLU**）激活函数；它的作用是帮助我们的模型学习数据中的更复杂模式，并提高计算效率。第二层是输出层，由一个神经元组成，因为我们在做回归任务（预测连续值）。让我们运行500个epoch，看看是否会有所不同：
- en: '[PRE37]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: From the last five lines of our output, we can see that there is a significant
    drop in our loss to around `3686`. Let’s also plot the loss curve to get a visual
    understanding as well.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出的最后五行来看，我们可以看到损失值显著下降，降到了大约`3686`。我们也来绘制一下损失曲线，以便更直观地理解。
- en: '![Figure 3.10 – Model losses after 500 epochs](img/B18118_03_010.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 500个epoch后的模型损失](img/B18118_03_010.jpg)'
- en: Figure 3.10 – Model losses after 500 epochs
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 500个epoch后的模型损失
- en: In *Figure 3**.10*, we can see that our model’s loss has fallen below our lowest
    recorded loss. This is a massive improvement in comparison to our previous model.
    However, this is not a desired result, nor does it look like the type of result
    we would like to present to the HR team. This is because, with this model, if
    an employee earns $50,000, the model could predict either around $46,300 as the
    employee’s salary, which would make them unhappy, or $53,700 as the employee’s
    salary, in which case the HR team will not be happy. So, we need to figure out
    how to improve our result.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 3**.10*中，我们可以看到，模型的损失已经降到了我们记录的最低损失值以下。这与我们之前的模型相比，取得了巨大的进步。然而，这并不是理想的结果，也看起来不像我们希望呈现给HR团队的结果。这是因为，使用这个模型，如果某个员工的年薪是$50,000，模型可能会预测出大约$46,300的薪水，这会让员工不高兴；或者预测出$53,700的薪水，这样HR团队也不会高兴。所以，我们需要想办法改进我们的结果。
- en: 'Let’s zoom into the plot to have a better understanding of what is happening
    with our model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大图表，更好地理解我们的模型发生了什么。
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: When we run the code, it returns the plot shown in *Figure 3**.11*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它返回了*图 3**.11*所示的图表。
- en: '![Figure 3.11 – Model losses after 500 epochs when we zoom into the plot](img/B18118_03_011.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11 – 当我们放大图表时，500个epoch后的模型损失](img/B18118_03_011.jpg)'
- en: Figure 3.11 – Model losses after 500 epochs when we zoom into the plot
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 当我们放大图表时，500个epoch后的模型损失
- en: From the plot in *Figure 3**.11*, we can see that the loss falls sharply and
    settles at around the 100th epoch, and nothing significant seems to happen afterward.
    Hence, training the model for longer just as we did in our previous model may
    not be the optimal solution. What can we do to improve our model?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 3**.11*中的图表，我们可以看到，损失值急剧下降并在第100个epoch左右稳定下来，之后似乎没有显著变化。因此，像我们之前的模型那样将模型训练得更长时间可能并不是最优的解决方案。那么，我们该如何改进我们的模型呢？
- en: 'Perhaps we can add another layer? Let’s do that and see what our results look
    like. As we initially pointed out, our job requires a lot of experimentation;
    only then can we learn how to do things better and faster:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们可以再添加一层？让我们试试，看看结果如何。正如我们最初指出的，我们的工作需要大量的实验；只有这样，我们才能学会如何做得更好、更快：
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here, we added another dense layer of `64` neurons. Note that we also use ReLU
    as the activation function here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了一个`64`神经元的密集层。请注意，我们在这里也使用了ReLU作为激活函数：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We display only the last five epochs, and we can see the loss is around `97384`,
    which is worse than the results achieved in `model_3`. So, how do we know how
    many layers to use in our modeling process? The answer is by experimenting. We
    use trial and error, backed by our understanding of what the results look like.
    We can decide whether we need to add more layers, as we did initially when the
    model was underfitting. And should the model get so complex that it masters the
    training data well but does not generalize well on our test (hold-out) data, it
    is said to be *overfitting* in machine learning lingo.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只展示最后五个训练周期，可以看到损失大约为`97384`，这比`model_3`中的结果要差。那么，我们如何知道在建模过程中使用多少层呢？答案是通过实验。我们通过试验和错误，并结合对结果的理解，来决定是否需要添加更多层，就像最初模型欠拟合时我们所做的那样。如果模型变得过于复杂，能够很好地掌握训练数据，但在我们的测试（留出）数据上无法很好地泛化，那么在机器学习术语中，它就被称为*过拟合*。
- en: Now that we have tried smaller and larger models, we cannot yet say we have
    achieved a suitable result, and the HR manager has checked in on us to find out
    how we are doing in terms of the prediction modeling task. So far, we did some
    research, as all ML engineers do, and we discovered a very important step that
    we can try out. What step? Let’s see.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经尝试了更小和更大的模型，但仍然无法说我们已经达到了合适的结果，而人力资源经理也来询问我们在预测建模任务上取得了什么进展。到目前为止，我们做了一些研究，正如所有的机器学习工程师所做的那样，发现了一个非常重要的步骤，我们可以尝试。是什么步骤呢？让我们看看。
- en: Normalization
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化
- en: '**Normalization** is a technique applied to input features to ensure they are
    of a consistent scale, usually between 0 and 1\. This process helps our model
    to converge faster and more accurately. It is worth noting that we should apply
    normalization after completing other data preprocessing steps, such as handling
    missing values.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化**是一种应用于输入特征的技术，确保它们具有一致的尺度，通常是在0和1之间。这个过程有助于我们的模型更快且更准确地收敛。值得注意的是，我们应该在完成其他数据预处理步骤后，如处理缺失值，才应用标准化。'
- en: 'It’s good practice to know that improving your model output can also rely strongly
    on your data preparation process. Hence, let us apply this here. We will take
    a step back from model building and look at our features after we converted all
    the columns to numerical values:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 了解改进模型输出的效果往往也强烈依赖于数据准备过程是一个很好的做法。因此，让我们在这里应用这一点。我们将暂时跳出模型构建的步骤，来看看在将所有列转换为数值后我们的特征数据：
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We will use the `describe` function to get vital statistics of our data. This
    information shows us that most of the columns have a minimum value of 0 and a
    maximum value of 1, but the experience column is of a different scale, as shown
    in *Figure 3**.12*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `describe` 函数来获取数据的关键信息。该信息显示，大多数列的最小值为0，最大值为1，但`Experience` 列的尺度不同，正如*图
    3.12*所示：
- en: '![Figure 3.12 – A statistical summary of the dataset (before normalization)](img/B18118_03_012.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – 数据集的统计摘要（标准化前）](img/B18118_03_012.jpg)'
- en: Figure 3.12 – A statistical summary of the dataset (before normalization)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – 数据集的统计摘要（标准化前）
- en: Why does this matter, you may ask? When the scale of our data is different,
    our model will arbitrarily attach more importance to columns with higher values,
    which could affect the model’s ability to predict our target correctly. To resolve
    this issue, we will use normalization to scale the data between 0 and 1 to bring
    all our features to the same scale, thereby giving every feature an equal chance
    when our model begins to learn how they relate to our target (*y*).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么这很重要？当我们数据的尺度不同，模型会不合理地给予数值较大的列更多的权重，这可能影响模型正确预测目标的能力。为了解决这个问题，我们将使用标准化方法，将数据缩放到0到1之间，从而使所有特征具有相同的尺度，给每个特征在模型学习它们与目标(*y*)之间关系时相等的机会。
- en: 'To normalize our data, we will use the following equation to scale it:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化我们的数据，我们将使用以下公式进行缩放：
- en: X norm =  X − X min _ X max − X min
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: X norm = X − X min _ X max − X min
- en: 'where *X* is our data, X min is the minimum value of *X*, and X max is the
    maximum value of *X*. In our case study, the minimum value of *X* in the `Experience`
    column is 1, and the maximum value of *X* in the `Experience` column is 7\. The
    good news is that we can easily implement this step using the `MinMaxScaler` function
    from the `sklearn` library. Let’s see how to scale our data next:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *X* 是我们的数据，X min 是 *X* 的最小值，X max 是 *X* 的最大值。在我们的案例中，`Experience` 列的 *X*
    最小值为 1，最大值为 7。好消息是，我们可以使用 `sklearn` 库中的 `MinMaxScaler` 函数轻松实现这一步骤。接下来，让我们看看如何缩放我们的数据：
- en: '[PRE42]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Let’s use the `describe()` function to view the key statistics again, as shown
    in *Figure 3**.13*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`describe()`函数再次查看关键统计数据，如*图3.13*所示。
- en: '![Figure 3.13 – A statistical summary of the dataset (after normalization)](img/B18118_03_013.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 数据集的统计摘要（标准化后）](img/B18118_03_013.jpg)'
- en: Figure 3.13 – A statistical summary of the dataset (after normalization)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 数据集的统计摘要（标准化后）
- en: Now, all our data is of the same scale. So, we have successfully implemented
    normalization of our data in just a few lines of code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们所有的数据都具有相同的尺度。因此，我们通过几行代码成功地对数据进行了标准化。
- en: 'Now, we split our data into training and testing sets, but this time, we use
    our normalized *X* (`X_norm`) in the code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据分为训练集和测试集，但这次我们在代码中使用了标准化后的*X*（`X_norm`）：
- en: '[PRE43]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, we use our best-performing model (`model_3`) from the initial experiments
    we have done so far. Let’s see how our model performs after normalization:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用从初始实验中获得的最佳模型（`model_3`）。让我们看看标准化后模型的表现：
- en: '[PRE44]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE45]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: From the results, we can see that MAE has reduced by more than half in comparison
    to the results we got without applying normalization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以看到与未应用标准化时的结果相比，MAE减少了一半以上。
- en: '![Figure 3.14 – The zoomed-in loss curve for model_5](img/B18118_03_014.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14 – model_5的放大损失曲线](img/B18118_03_014.jpg)'
- en: Figure 3.14 – The zoomed-in loss curve for model_5
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – model_5的放大损失曲线
- en: 'Also, if you look at the loss plot for `model_5` in *Figure 3**.14*, you can
    see the loss fails to drop significantly after around the 100th epoch. Instead
    of guessing how many epochs are ideal to train the model, how about we set a rule
    to stop training when the model fails to improve its performance? Also, we can
    see that `model_5` doesn’t give us the result we want; perhaps now is a good time
    to try out a bigger model, in which we train it for longer and set a rule to stop
    training once it fails to improve its performance on the training data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你查看`model_5`在*图3.14*中的损失曲线，你会发现损失在大约第100个epoch之后没有显著下降。与其猜测训练模型的理想epoch数，不如设置一个规则，当模型无法改进其性能时就停止训练。而且我们可以看到，`model_5`没有给出我们想要的结果；也许现在是尝试更大模型的好时机，我们可以训练更长时间，并设置一个规则，当模型无法改进其在训练数据上的性能时就停止训练：
- en: '[PRE46]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here, we use a three-layer model; the first two layers are made up of 64 neurons
    and the output layer has a single neuron. To set the rule to stop training, we
    use *early stopping*; this additional parameter is applied when we fit our model
    on the data to stop training when the model loss fails to improve after 10 epochs.
    This is achieved by specifying the metric to monitor loss and setting `patience`
    to `10`. Early stopping is also a great technique to prevent overfitting, as it
    stops training when the model fails to improve; we will discuss this further in
    [*Chapter 6*](B18118_06.xhtml#_idTextAnchor129)*, Improving the Model*. Let’s
    look at the result now:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了一个三层模型，前两层由64个神经元组成，输出层只有一个神经元。为了设置停止训练的规则，我们使用了*early stopping*；这个附加参数是在我们将模型拟合到数据时应用的，用来在模型损失在10个epoch后没有改善时停止训练。这是通过指定监控损失的度量并将`patience`设置为`10`来实现的。Early
    stopping也是一种防止过拟合的好技术，因为它在模型无法改进时停止训练；我们将在[*第6章*](B18118_06.xhtml#_idTextAnchor129)《改进模型》中进一步讨论这个问题。现在让我们看看结果：
- en: '[PRE47]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Although we set our training for `1000` epochs, our `Earlystopping` callback
    halted the training process on the 29th epoch because it observed no meaningful
    drop in the loss. Although the result here isn’t great, we have used `EarlyStopping`
    to save a considerable amount of computational resources and time. Perhaps now
    is a good time to try out a different optimizer. For this next experiment, let’s
    use the Adam optimizer. Adam is another popular optimizer that is used in deep
    learning, due to its ability to adaptively control the learning rate for each
    parameter in a model, which accelerates the model’s convergence:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将训练设置为`1000`个epoch，但由于`Earlystopping`回调在第29个epoch时停止了训练，因为它没有观察到损失值的显著下降。虽然这里的结果不是很好，但我们使用了`EarlyStopping`来节省了大量的计算资源和时间。也许现在是尝试不同优化器的好时机。对于下一个实验，我们使用Adam优化器。Adam是深度学习中另一种流行的优化器，因为它能够自适应地控制模型中每个参数的学习率，从而加速模型的收敛：
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Note we only changed the optimizer to Adam in our compile step. Let’s see the
    result of this change in the optimizer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在编译步骤中只更改了优化器为Adam。让我们看看这个优化器变化的结果：
- en: '[PRE49]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'By just changing the optimizer, we have recorded an incredible drop in loss.
    Also, note that we did not need the entire `1000` epochs, as training ended on
    `901` epochs. Let us add another layer; perhaps we will see an improved performance:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过更换优化器，我们记录到了损失值的惊人下降。同时，注意我们并未使用全部`1000`个epoch，因为训练在`901`个epoch时就结束了。让我们再添加一层，或许会看到更好的表现：
- en: '[PRE50]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here, we added an extra layer with `64` neurons, with ReLU as the activation
    function. Everything else is the same:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了一个额外的层，包含`64`个神经元，并使用ReLU作为激活函数。其他部分保持不变：
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Training stops at 270 epochs; although our model is more complex, it doesn’t
    perform better than `model_7` on training. We have tried out different ideas while
    experimenting; now, let us try out all eight models on the test set and evaluate
    them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在270个epoch后停止；尽管我们的模型更复杂，但在训练中它并未表现得比`model_7`更好。在实验过程中，我们尝试了不同的思路，现在让我们在测试集上尝试所有八个模型并进行评估。
- en: Model evaluation
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'To evaluate our models, we will write a function to apply the `evaluate` metrics
    to all eight models:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们将编写一个函数，将`evaluate`指标应用于所有八个模型：
- en: '[PRE52]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will generate an `eval_testing(model)` function that takes a model as an
    argument and uses the `evaluate` method to evaluate the performance of the model
    on our test dataset. Looping through the list of models, the code returns the
    loss and MAE values for all eight models for our test data:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一个`eval_testing(model)`函数，该函数以模型作为参数，并使用`evaluate`方法来评估模型在我们的测试数据集上的表现。遍历模型列表时，代码将返回所有八个模型在测试数据上的损失和MAE值：
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: After we evaluate the models, we can that see `model_7` has the lowest loss.
    Let’s see how it does on our test set by using it to make predictions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估完模型后，我们可以看到`model_7`的损失最小。让我们看看它在测试集上的表现，使用它来进行预测。
- en: Making predictions
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: 'Now that we are done with experimenting and have evaluated the models, let’s
    use `model_7` to predict our test set salaries and see how they compare with the
    ground truth. To do this, we will use the `predict()` function:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们完成了实验并评估了模型，让我们使用`model_7`来预测我们的测试集薪资，并查看它与真实值的对比。为此，我们将使用`predict()`函数：
- en: '[PRE54]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'After we run this code block, we get the output in an array, as shown here:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完这段代码后，我们会得到如下所示的数组输出：
- en: '[PRE55]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'For clarity, let’s build a DataFrame with the model’s prediction and ground
    truth. This should be fun and somewhat magical when you see how good our model
    has become:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们将构建一个包含模型预测值和真实值的DataFrame。当你看到我们的模型变得如此优秀时，这应该会非常有趣，甚至有些神奇：
- en: '[PRE56]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, we generate two columns and convert the model’s prediction from `float`
    to `int`, just to keep it in scope with the ground truth. Ready for the result?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成了两列，并将模型的预测值从`float`转换为`int`，以便与真实值对齐。准备好查看结果了吗？
- en: 'We will use the `head` function to print out the first 10 values of the test
    set:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`head`函数打印出测试集的前10个值：
- en: '[PRE57]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We then see our results, as shown in *Figure 3**.15*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们会看到如下所示的结果，如*图 3.15*所示：
- en: '![Figure 3.15 – A DataFrame showing the actual values, predictions made by
    the model, and the resulting residuals](img/B18118_03_015.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – 一个显示实际值、模型预测值和剩余值的DataFrame](img/B18118_03_015.jpg)'
- en: Figure 3.15 – A DataFrame showing the actual values, predictions made by the
    model, and the resulting residuals
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – 一个显示实际值、模型预测值和剩余值的DataFrame
- en: Our model has achieved something impressive; it is really close to the initial
    salaries in our test data. Now, you can show the HR manager your amazing result.
    We must save the model so that we can load it and make predictions anytime we
    want. Let’s learn how to do this next.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型取得了令人印象深刻的成果；它与测试数据中的初始薪资非常接近。现在，你可以向人力资源经理展示你的惊人成果了。我们必须保存模型，以便随时加载并进行预测。接下来，我们将学习如何做到这一点。
- en: Saving and loading models
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: 'The beauty of TensorFlow is the ease with which we can do complex stuff. To
    save a model, we just need one line of code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的魅力在于我们能够轻松完成复杂的任务。要保存模型，我们只需要一行代码：
- en: '[PRE58]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: You can save it as `your_model.h5` or `your_model`; either way works. TensorFlow
    recommends the `SavedModel` approach because it is language-agnostic, which makes
    it easy to deploy on various platforms. In this format, we can save the model
    and its individual components, such as the weights and variables. Conversely,
    the HDF5 format saves the complete model structure, its weights, and the training
    configurations as a single file. This approach gives us greater flexibility to
    share and distribute models; however, for deployment purposes, it’s not the preferred
    method. When we run the code, we can see the saved model on the left-hand panel
    in our Colab notebook, as shown in *Figure 3**.16*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将它保存为`your_model.h5`或`your_model`，两者都可以。TensorFlow 推荐使用`SavedModel`格式，因为它是语言无关的，这使得它可以轻松部署到各种平台上。在这种格式下，我们可以保存模型及其各个组件，如权重和变量。相反，HDF5
    格式将完整的模型结构、权重和训练配置保存为一个文件。这种方法给我们提供了更大的灵活性来共享和分发模型；然而，在部署方面，它不是首选方法。当我们运行代码时，我们可以在
    Colab 笔记本的左侧面板中看到保存的模型，如*图 3.16*所示。
- en: '![Figure 3.16 – A snapshot of our saved model](img/B18118_03_016.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.16 – 我们保存的模型快照](img/B18118_03_016.jpg)'
- en: Figure 3.16 – A snapshot of our saved model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16 – 我们保存的模型快照
- en: 'Now that we have saved the model, it is wise to test it out by reloading it
    and testing it. Let’s do that. Also, it’s just one line of code to load the model:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经保存了模型，最好通过重新加载它并进行测试来验证它。我们来做一下这个操作。而且，加载模型只需要一行代码：
- en: '[PRE59]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let’s try out our `saved_model` and see whether it will work as well as `model_7`.
    We will generate `y_pred` again and generate a DataFrame, using `y_test` and `y_pred`
    as we did earlier checking first the 10 random samples from our test data:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试我们的`saved_model`，看看它是否能像`model_7`一样有效。我们将重新生成`y_pred`并生成一个数据框，使用`y_test`和`y_pred`，就像我们之前做的那样，首先检查我们测试数据中的10个随机样本：
- en: '![Figure 3.17 – A DataFrame showing the actual values and predictions made
    by the saved model](img/B18118_03_017.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.17 – 显示实际值和保存的模型预测值的数据框](img/B18118_03_017.jpg)'
- en: Figure 3.17 – A DataFrame showing the actual values and predictions made by
    the saved model
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – 显示实际值和保存的模型预测值的数据框
- en: 'From the results in *Figure 3**.17*, we can see that our saved model performs
    at a high level. Now, you can deliver your result to the HR manager, and they
    should be excited about your results. Let’s imagine that the HR manager wants
    you to use your model to predict the salary of the new hires. Let’s do that next:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 3.17*的结果中，我们可以看到我们保存的模型表现得非常好。现在，你可以将结果交给人力资源经理，他们应该会对你的结果感到兴奋。假设人力资源经理希望你使用你的模型来预测新员工的薪资，我们接下来就来做这个：
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We generate a function using our saved model. We simply wrap all the steps
    we’ve covered so far into the function, and we return a DataFrame. Now, let’s
    read in the data of our new hires:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用保存的模型生成一个函数。我们只需将到目前为止所覆盖的所有步骤封装到这个函数中，然后返回一个数据框。现在，让我们读取新员工的数据：
- en: '[PRE61]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: When we run the code block, we can see their data in a DataFrame, as shown in
    *Figure 3**.18*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码块时，我们可以看到它们的数据框，如*图 3.18*所示。
- en: '![Figure 3.18 – A DataFrame showing the new hires](img/B18118_03_018.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.18 – 显示新员工的数据框](img/B18118_03_018.jpg)'
- en: Figure 3.18 – A DataFrame showing the new hires
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18 – 显示新员工的数据框
- en: 'Now, we pass the data into the function we generated to get the predicted salaries
    for our new hires:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据传入我们生成的函数，获取新员工的预测薪资：
- en: '[PRE62]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We pass `df_new` into the salary prediction function, and we get a new DataFrame,
    as shown in *Figure 3**.19*:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`df_new`传入薪资预测函数，然后得到一个新的数据框，如*图 3.19*所示：
- en: '![Figure 3.19 – A DataFrame showing the new hires with their predicted salaries](img/B18118_03_019.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.19 – 显示新员工及其预测薪资的数据框](img/B18118_03_019.jpg)'
- en: Figure 3.19 – A DataFrame showing the new hires with their predicted salaries
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – 显示新员工及其预测薪资的数据框
- en: Finally, we have achieved our goal. HR is happy, the new hires are happy, and
    everyone in the company thinks you are a magician. Perhaps a pay raise could be
    on the table, but while you bask in the euphoria around your first success, your
    manager returns with another task. This time, it is a classification task, which
    we will look at this in the next chapter. For now, good job!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们达成了目标。人力资源部门很高兴，新员工也很高兴，公司里的每个人都觉得你是个魔术师。也许加薪提案会摆上桌面，但就在你陶醉于第一次成功的喜悦时，经理带着另一个任务回来。这次是一个分类任务，我们将在下一章讨论这个任务。现在，干得好！
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a deeper dive into supervised learning, with a focus
    on regression modeling. Here, we discussed the difference between simple and multiple
    linear regression and looked at some important evaluation metrics for regression
    modeling. Then, we rolled up our sleeves on our case study, helping our company
    build a working regression model to predict the salaries of new employees. We
    carried out some data preprocessing steps and saw the importance of normalization
    in our modeling process.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入研究了监督学习，重点是回归建模。在这里，我们讨论了简单线性回归和多重线性回归的区别，并了解了一些重要的回归建模评估指标。接着，我们在案例研究中动手帮助公司构建了一个有效的回归模型，用于预测新员工的薪资。我们进行了数据预处理，并认识到在建模过程中标准化的重要性。
- en: At the end of the case study, we successfully built a salary prediction model,
    evaluated the model on our test set, and mastered how to save and load models
    for use at a later stage. Now, you can confidently build a regression model with
    TensorFlow.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在案例研究的最后，我们成功构建了一个薪资预测模型，并在测试集上评估了该模型，掌握了如何保存和加载模型以供后续使用。现在，您可以自信地使用 TensorFlow
    构建回归模型。
- en: In the next chapter, we’ll take a look at classification modeling.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨分类建模。
- en: Questions
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Let’s test what we learned in this chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下本章所学的内容。
- en: What is linear regression?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是线性回归？
- en: What is the difference between simple and multiple linear regression?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单线性回归和多重线性回归有什么区别？
- en: What evaluation metric penalizes large errors in regression modeling?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个评估指标会惩罚回归模型中的大误差？
- en: Use the salary dataset to forecast salaries.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用薪资数据集预测薪资。
- en: Further reading
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, you can check out the following resources:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多，您可以查看以下资源：
- en: 'Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*. [S.l.]: Packt Publishing.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amr, T., 2020\. *使用 scikit-learn 和科学 Python 工具包进行实践机器学习*。[S.l.]：Packt 出版社。
- en: Raschka, S. and Mirjalili, V., 2019\. *Python Machine Learning*. 3rd ed. Packt
    Publishing.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raschka, S. 和 Mirjalili, V., 2019\. *Python 机器学习*。 第3版。Packt 出版社。
- en: '*TensorFlow* *documen**t**ation*: [https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TensorFlow* *文档*：[https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide)。'
