- en: Web Scraping and Interactive Visualizations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬虫与交互式可视化
- en: So far in this book, we have focused on using Jupyter to build reproducible
    data analysis pipelines and predictive models. We'll continue to explore these
    topics in this chapter, but the main focus here is data acquisition. In particular,
    we will show you how data can be acquired from the web using HTTP requests. This
    will involve scraping web pages by requesting and parsing HTML. We will then wrap
    up this chapter by using interactive visualization techniques to explore the data
    we've collected.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书主要聚焦于使用Jupyter构建可重复的数据分析流程和预测模型。在本章中我们将继续探讨这些话题，但本章的主要重点是数据获取。特别是，我们将展示如何通过HTTP请求从网上获取数据。这将涉及通过请求和解析HTML来抓取网页。最后，我们将通过使用交互式可视化技术来探索我们收集的数据，作为本章的总结。
- en: The amount of data available online is huge and relatively easy to acquire.
    It's also continuously growing and becoming increasingly important. Part of this
    continual growth is the result of an ongoing global shift from newspapers, magazines,
    and TV to online content. With customized news feeds available all the time on
    cell phones, and live-news sources such as Facebook, Reddit, Twitter, and YouTube,
    it's difficult to imagine the historical alternatives being relevant much longer.
    Amazingly, this accounts for only some of the increasingly massive amounts of
    data available online.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 网上可获得的数据量庞大且相对容易获取，而且数据量还在持续增长，变得越来越重要。部分持续增长的原因，是全球从报纸、杂志和电视转向在线内容的趋势。随着手机上随时可用的定制化新闻源，以及Facebook、Reddit、Twitter和YouTube等实时新闻源的出现，已经很难想象历史上的其他选择还能在未来长久存在。令人惊讶的是，这还仅仅是线上数据不断增长的一部分。
- en: With this global shift toward consuming content using HTTP services (blogs,
    news sites, Netflix, and so on), there are plenty of opportunities to use data-driven
    analytics. For example, Netflix looks at the movies a user watches and predicts
    what they will like. This prediction is used to determine the suggested movies
    that appear. In this chapter, however, we won't be looking at "business-facing"
    data as such, but instead we will see how the client can leverage the internet
    as a database. Never before has this amount and variety of data been so easily
    accessible. We'll use web-scraping techniques to collect data, and then we'll
    explore it with interactive visualizations in Jupyter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着全球向通过HTTP服务（如博客、新闻网站、Netflix等）消费内容的转变，使用数据驱动的分析方法的机会大量增加。例如，Netflix会根据用户观看的电影，预测他们可能喜欢的电影，并基于此预测推荐相关电影。不过，在本章中，我们并不打算探讨这种“面向业务”的数据，而是将展示客户端如何将互联网作为数据库来利用。从未有过如此多样化且易于获取的数据。我们将使用网络爬虫技术来收集数据，然后利用Jupyter中的交互式可视化进行探索。
- en: Interactive visualization is a visual form of data representation, which helps
    users understand the data using graphs or charts. Interactive visualization helps
    a developer or analyst present data in a simple form, which can be understood
    by non-technical personnel too.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式可视化是一种数据表现形式，通过图表或图形帮助用户理解数据。交互式可视化帮助开发者或分析人员将数据呈现成简洁易懂的形式，非技术人员也能轻松理解。
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够：
- en: Analyze how HTTP requests work
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析HTTP请求如何工作
- en: Scrape tabular data from a web page
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网页中爬取表格数据
- en: Build and transform Pandas Data Frames
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建与转换Pandas数据框
- en: Create interactive visualizations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建交互式可视化
- en: Scraping Web Page Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取网页数据
- en: In the spirit of leveraging the internet as a database, we can think about acquiring
    data from web pages either by scraping content or by interfacing with web APIs.
    Generally, scraping content means getting the computer to read data that was intended
    to be displayed in a human-readable format. This is in contradistinction to web
    APIs, where data is delivered in machine-readable formats – the most common being
    JSON.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在将互联网作为数据库的理念下，我们可以考虑通过爬取网页内容或与Web API接口来获取数据。通常，爬取内容意味着让计算机读取原本是为人类呈现的可读格式的数据。与此不同，Web
    API则是通过机器可读的格式（最常见的是JSON）传递数据。
- en: In this topic, we will focus on web scraping. The exact process for doing this
    will depend on the page and desired content. However, as we will see, it's quite
    easy to scrape anything we need from an HTML page so long as we have an understanding
    of the underlying concepts and tools. In this topic, we'll use Wikipedia as an
    example and scrape tabular content from an article. Then, we'll apply the same
    techniques to scrape data from a page on an entirely separate domain. But fist,
    we'll take some time to introduce HTTP requests.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们将重点讨论网页抓取。进行抓取的具体过程取决于页面和所需的内容。不过，正如我们将看到的，只要我们了解底层的概念和工具，从HTML页面中抓取任何我们需要的内容是非常容易的。在本节中，我们将以维基百科为例，抓取文章中的表格内容。然后，我们将应用相同的技术，从一个完全不同域的页面抓取数据。但首先，我们将花一些时间来介绍HTTP请求。
- en: Introduction to HTTP Requests
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HTTP请求简介
- en: The Hypertext Transfer Protocol, or HTTP for short, is the foundation of data
    communication for the internet. It defies how a page should be requested and how
    the response should look. For example, a client can request an Amazon page of
    laptops for sale, a Google search of local restaurants, or their Facebook feed.
    Along with the URL, the request will contain the user agent and available browsing
    cookies among the contents of the **request header**. The user agent tells the
    server what browser and device the client is using, which is usually used to provide
    the most user-friendly version of the web page's response. Perhaps they have recently
    logged in to the web page; such information would be stored in a cookie that might
    be used to automatically log the user in.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 超文本传输协议，简称HTTP，是互联网数据通信的基础。它定义了如何请求一个页面以及响应应该是什么样子。例如，客户端可以请求一个销售笔记本电脑的亚马逊页面、一个本地餐馆的谷歌搜索结果，或者是他们的Facebook动态。除了URL，请求中还会包含用户代理和可用的浏览器Cookie，这些内容属于**请求头**。用户代理告诉服务器客户端正在使用的浏览器和设备，通常用于提供最适合用户的网页响应版本。也许他们最近登录了该网页；这些信息会存储在Cookie中，用于自动登录用户。
- en: These details of HTTP requests and responses are taken care of under the hood
    thanks to web browsers. Luckily for us, today the same is true when making requests
    with high level languages such as Python. For many purposes, the contents of request
    headers can be largely ignored.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些HTTP请求和响应的细节由Web浏览器在幕后处理。幸运的是，今天使用Python等高级语言进行请求时也是如此。对于许多用途，请求头的内容可以大体上忽略。
- en: Unless otherwise specified, these are automatically generated in Python when
    requesting a URL. Still, for the purposes of troubleshooting and understanding
    the responses yielded by our requests, it's useful to have a foundational understanding
    of HTTP.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则这些请求通常是在请求URL时由Python自动生成的。不过，为了排查问题并理解请求产生的响应，理解HTTP的基础知识是很有用的。
- en: There are many types of HTTP methods, such as GET, HEAD, POST, and PUT. The
    fist two are used for requesting that data be sent from the server to the client,
    whereas the last two are used for sending data to the server.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP方法有很多种，如GET、HEAD、POST和PUT。前两种用于请求从服务器发送数据到客户端，而后两种则用于将数据发送到服务器。
- en: 'These HTTP methods are summarized in the following table:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些HTTP方法总结在以下表格中：
- en: '| **HTTP method** | **Description** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **HTTP方法** | **描述** |'
- en: '| GET  | Retrieves the information from the specified URL |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| GET  | 从指定的URL获取信息 |'
- en: '| HEAD | Retrieves the meta information from the HTTP header of the specified URL
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| HEAD | 从指定URL的HTTP头部获取元信息 |'
- en: '| POST | Sends the attached information for appending to the resource(s) at
    the specified URL |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| POST | 将附加信息发送到指定URL的资源进行附加 |'
- en: '| PUT | Sends the attached information for replacing the resource(s) at the specified
    URL |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| PUT | 将附加信息发送到指定URL的资源以替换原有内容 |'
- en: A GET request is sent each time we type a web page address into our browser
    and press *Enter*. For web scraping, this is usually the only HTTP method we are
    interested in, and it's the only method we'll be using in this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们在浏览器中输入网页地址并按下*Enter*键时，都会发送一个GET请求。在网页抓取中，这通常是我们唯一关心的HTTP方法，也是本章将要使用的唯一方法。
- en: 'Once the request has been sent, a variety of response types can be returned
    from the server. These are labeled with 100-level to 500-level codes, where the
    fist digit in the code represents the response class. These can be described as
    follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求被发送，服务器可以返回各种类型的响应。这些响应使用从100级到500级的代码进行标记，其中代码中的第一位数字表示响应的类别。可以将其描述如下：
- en: '**1xx**: Informational response, for example, server is processing a request.
    It''s uncommon to see this.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1xx**: 信息响应，例如，服务器正在处理请求。这种情况很少见。'
- en: '**2xx**: Success, for example, page has loaded properly.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2xx**: 成功，例如，页面已正确加载。'
- en: '**3xx**: Redirection, for example, the requested resource has been moved and
    we were redirected to a new URL.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3xx**: 重定向，例如，请求的资源已移动，我们被重定向到一个新的URL。'
- en: '**4xx**: Client error, for example, the requested resource does not exist.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4xx**: 客户端错误，例如，请求的资源不存在。'
- en: '**5xx**: Server error, for example, the website server is receiving too much
    traffic and could not fulfill the request.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5xx**: 服务器错误，例如，网站服务器正在接收过多的流量，无法完成请求。'
- en: For the purposes of web scraping, we usually only care about the response class,
    that is, the fist digit of the response code. However, there exist subcategories
    of responses within each class that offer more granularity on what's going on.
    For example, a 401 code indicates an unauthorized response, whereas a 404 code
    indicates a *page not found* response.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络抓取的目的，我们通常只关心响应类别，即响应代码的第一个数字。但是，每个类别中还有子类别，提供了更详细的信息。例如，401代码表示未经授权的响应，而404代码表示*页面未找到*的响应。
- en: This distinction is noteworthy because a 404 would indicate we've requested
    a page that does not exist, whereas 401 tells us we need to log in to view the
    particular resource.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种区分很重要，因为404表示我们请求的页面不存在，而401告诉我们需要登录才能查看特定资源。
- en: Let's see how HTTP requests can be done in Python and explore some of these
    topics using the Jupyter Notebook.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中进行HTTP请求，并使用Jupyter Notebook探索这些主题。
- en: Making HTTP Requests in the Jupyter Notebook
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中进行HTTP请求
- en: Now that we've talked about how HTTP requests work and what type of responses
    we should expect, let's see how this can be done in Python. We'll use a library
    called **Requests**, which happens to be the most downloaded external library
    for Python. It's possible to use Python's built-in tools, such as `urllib`, for
    making HTTP requests, but Requests is far more intuitive, and in fact it's recommended
    over `urllib` in the official Python documentation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了HTTP请求的工作原理及其应该期望的响应类型，让我们看看如何在Python中实现这一点。我们将使用一个名为**Requests**的库，这恰好是Python中下载次数最多的外部库。可以使用Python的内置工具，如`urllib`，来进行HTTP请求，但Requests更加直观，并且在官方Python文档中推荐使用它而不是`urllib`。
- en: Requests is a great choice for making simple and advanced web requests. It allows
    for all sorts of customization with respect to headers, cookies, and authorization.
    It tracks redirects and provides methods for returning specific page content such
    as JSON. Furthermore, there's an extensive suite of advanced features. However,
    it does not allow JavaScript to be rendered.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Requests是进行简单和高级网络请求的一个很好的选择。它允许对头部、Cookie和授权进行各种自定义设置。它跟踪重定向并提供方法来返回特定页面内容，例如JSON。此外，还有一整套高级功能。但是，它不允许JavaScript渲染。
- en: Oftentimes, servers return HTML with JavaScript code snippets included, which
    are automatically run in the browser on load time. When requesting content with
    Python using Requests, this JavaScript code is visible, but it does not run. Therefore,
    any elements that would be altered or created by doing so are missing. Often,
    this does not affect the ability to get the desired information, but in some cases
    we may need to render the JavaScript in order to scrape the page properly. For
    doing this, we could use a library like Selenium. This has a similar API to the
    Requests library, but provides support for rendering JavaScript using web drivers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，服务器返回包含JavaScript代码片段的HTML，这些片段在加载时会自动在浏览器中运行。使用Python的Requests请求内容时，可以看到这些JavaScript代码，但不会运行。因此，任何由此创建或更改的元素均会缺失。通常情况下，这并不影响获取所需信息的能力，但在某些情况下，我们可能需要渲染JavaScript以正确地抓取页面。为此，我们可以使用类似Selenium这样的库。它与Requests库具有类似的API，但提供了使用Web驱动程序来渲染JavaScript的支持。
- en: Let's dive into the following section using the Requests library with Python
    in a Jupyter Notebook.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Requests库和Python在Jupyter Notebook中深入研究下一节。
- en: Handling HTTP requests with Python in a Jupyter Notebook
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中使用Python处理HTTP请求
- en: Start the `NotebookApp` from the project directory by executing `jupyter notebook`.
    Navigate to the `*Chapter-3*` directory and open up the `chapter-3-workbook.ipynb
    file`. Find the cell near the top where the packages are loaded and run it.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行`jupyter notebook`从项目目录启动`NotebookApp`。导航到`*Chapter-3*`目录并打开`chapter-3-workbook.ipynb`文件。找到顶部附近加载包的单元格并运行它。
- en: We are going to request a web page and then examine the response object. There
    are many different libraries for making requests and many choices for exactly
    how to do so with each. We'll only use the Requests library, as it provides excellent
    documentation, advanced features, and a simple API.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将请求一个网页，然后检查响应对象。市面上有很多不同的库可以用来发起请求，并且每个库在具体操作方式上也有很多选择。我们只会使用Requests库，因为它提供了出色的文档、强大的功能和简单的API。
- en: 'Scroll down to Subtopic `Introduction to HTTP requests` and run the fist cell
    in that section to import the Requests library. Then, prepare a request by running
    the cell containing the following code:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动到子主题`HTTP请求简介`，并运行该部分中的第一个单元格以导入Requests库。然后，运行包含以下代码的单元格以准备请求：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use the `Request class` to prepare a GET request to the jupyter.org homepage.
    By specifying the user agent as `Mozilla/5.0`, we are asking for a response that
    would be suitable for a standard desktop browser. Finally, we prepare the request.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Request类`来准备一个GET请求，向jupyter.org主页发送请求。通过指定用户代理为`Mozilla/5.0`，我们请求的是一个适合标准桌面浏览器的响应。最后，我们准备好请求。
- en: 'Print the docstring for the "**prepared request**" req, by running the cell
    containing `req?`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含`req?`的单元格来打印“**已准备好的请求**”req的文档字符串：
- en: '![](img/e77b864d-d18e-4a67-b60a-1c75504f962e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e77b864d-d18e-4a67-b60a-1c75504f962e.png)'
- en: Looking at its usage, we see how the request can be sent using a session. This
    is similar to opening a web browser (starting a session) and then requesting a
    URL.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查看它的用法，我们可以看到如何使用会话发送请求。这类似于打开一个网页浏览器（启动会话），然后请求一个URL。
- en: 'Make the request and store the response in a variable named page, by running
    the following code:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发起请求并将响应存储在一个名为page的变量中，运行以下代码：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code returns the HTTP response, as referenced by the page variable. By
    using the with statement, we initialize a session whose scope is limited to the
    indented code block. This means we do not have to worry about explicitly closing
    the session, as it is done automatically.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码返回HTTP响应，通过`page`变量引用。通过使用with语句，我们初始化了一个作用范围仅限于缩进代码块的会话。这意味着我们不必显式地关闭会话，因为它会自动完成。
- en: Run the next two cells in the notebook to investigate the response. The string representation
    of `page` should indicate a 200 status code response. This should agree with the
    `status_code` attribute.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中的接下来的两个单元格来检查响应。`page`的字符串表示应显示200状态码响应。这应该与`status_code`属性一致。
- en: 'Save the response text to the page_html variable and take a look at the head
    of the string with `page_html[:1000]` :'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将响应文本保存到page_html变量中，并通过`page_html[:1000]`查看字符串的头部：
- en: '![](img/76d13a1f-33ff-4387-8142-0a75ef6c31db.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76d13a1f-33ff-4387-8142-0a75ef6c31db.png)'
- en: As expected, the response is HTML. We can format this output better with the
    help of BeautifulSoup, a library which will be used extensively for HTML parsing
    later in this section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，响应是HTML。我们可以借助BeautifulSoup库更好地格式化这个输出，稍后我们将广泛使用它来解析HTML。
- en: 'Print the head of the formatted HTML by running the following:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码打印格式化HTML的头部：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We import BeautifulSoup and then print the pretty output, where newlines are indented
    depending on their hierarchy in the HTML structure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入BeautifulSoup，然后打印格式化输出，输出中的新行根据它们在HTML结构中的层级关系进行缩进。
- en: 'We can take this a step further and actually display the HTML in Jupyter by
    using the IPython display module. Do this by running the following code:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以进一步操作，实际上通过使用IPython显示模块在Jupyter中显示HTML。通过运行以下代码来实现：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/1a80ff73-c7ef-4df5-a3e9-0230db3808bd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a80ff73-c7ef-4df5-a3e9-0230db3808bd.png)'
- en: 'Here, we see the HTML rendered as well as possible, given that no JavaScript
    code has been run and no external resources have loaded. For example, the images
    that are hosted on the jupyter.org server are not rendered and we instead see
    the `alt text`: **circle of programming icons**, jupyter logo, and so on.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到HTML渲染的效果，考虑到没有执行JavaScript代码，也没有加载外部资源。例如，托管在jupyter.org服务器上的图片没有被渲染，我们看到的是`alt文本`：**编程图标的圆圈**、jupyter
    logo 等等。
- en: 'Let''s compare this to the live website, which can be opened in Jupyter using
    an IFrame. Do this by running the following code:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将其与实时网站进行比较，该网站可以通过 IFrame 在 Jupyter 中打开。通过运行以下代码来实现：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/8a8f9adc-9993-4134-acd7-85cce986b9cc.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a8f9adc-9993-4134-acd7-85cce986b9cc.png)'
- en: Here, we see the full site rendered, including JavaScript and external resources.
    In fact, we can even click on the hyperlinks and load those pages in the IFrame,
    just like a regular browsing session.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到整个网站被渲染出来，包括 JavaScript 和外部资源。事实上，我们甚至可以点击超链接，并在 IFrame 中加载那些页面，就像常规的浏览会话一样。
- en: It's good practice to close the IFrame after using it. This prevents it from
    eating up memory and processing power. It can be closed by selecting the cell
    and clicking **Current Outputs** | **Clear** from the **Cell** menu in the Jupyter
    Notebook.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭 IFrame 是一个良好的实践，这可以防止它占用过多的内存和处理能力。可以通过选择单元格并在 Jupyter Notebook 的 **Cell**
    菜单中点击 **Current Outputs** | **Clear** 来关闭它。
- en: Recall how we used a prepared request and session to request this content as
    a string in Python. This is often done using a shorthand method instead. The drawback
    is that we do not have as much customization of the request header, but that's
    usually fine.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们如何使用一个准备好的请求和会话来获取内容并将其作为字符串在 Python 中使用。通常，我们也可以使用一个简便的方法来实现这一点。缺点是我们对请求头部的定制化程度较低，但通常这并不是什么问题。
- en: 'Make a request to [http://www.python.org/](http://www.python.org/) by running
    the following code:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码向 [http://www.python.org/](http://www.python.org/) 发起请求：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The string representation of the page (as displayed beneath the cell) should
    indicate a 200 status code, indicating a successful response.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 页面字符串表示（显示在单元格下方）应该显示一个 200 状态码，表示响应成功。
- en: Run the next two cells. Here, we print the `url` and `history` attributes of
    our page.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的两个单元格。在这里，我们打印页面的 `url` 和 `history` 属性。
- en: The URL returned is not what we input; notice the difference? We were redirected
    from the input URL, [http://www.python.org/](http://www.python.org/), to the secured
    version of that page, [https://www.python.org/](https://www.python.org/). The
    difference is indicated by an additional s at the start of the URL, in the protocol.
    Any redirects are stored in the history attribute; in this case, we find one page
    in here with status code 301 (permanent redirect), corresponding to the original
    URL requested.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的 URL 并不是我们输入的那个；注意到差异了吗？我们从输入的 URL [http://www.python.org/](http://www.python.org/)
    被重定向到了该页面的安全版本 [https://www.python.org/](https://www.python.org/)。差异就在于协议中 URL
    开头多了一个 s。所有的重定向信息都保存在历史记录属性中；在这个案例中，我们在其中找到了一个状态码为 301（永久重定向）的页面，它对应的是我们最初请求的
    URL。
- en: Now that we're comfortable making requests, we'll turn our attention to parsing
    the HTML. This can be something of an art, as there are usually multiple ways
    to approach it, and the best method often depends on the details of the specific
    HTML in question.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经习惯了发起请求，现在我们将关注于解析 HTML。这有时是一门艺术，因为通常有多种方法可以处理它，而最佳方法往往取决于具体 HTML 的细节。
- en: Parsing HTML in the Jupyter Notebook
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中解析 HTML
- en: When scraping data from a web page, after making the request, we must extract
    the data from the response content. If the content is HTML, then the easiest way
    to do this is with a high-level parsing library such as Beautiful Soup. This is
    not to say it's the only way; in principle, it would be possible to pick out the
    data using regular expressions or Python string methods such as `split`, but pursuing
    either of these options would be an inefficient use of time and could easily lead
    to errors. Therefore, it's generally frowned upon and instead, the use of a trustworthy
    parsing tool is recommended.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在从网页抓取数据时，在发起请求后，我们必须从响应内容中提取数据。如果内容是 HTML，那么最简单的方法是使用一个高级解析库，比如 Beautiful Soup。并不是说这是唯一的方式；原则上，我们也可以使用正则表达式或
    Python 字符串方法，如 `split` 来提取数据，但采用这些方式会浪费时间，并且容易出错。因此，一般不推荐这样做，通常建议使用一个可信赖的解析工具。
- en: In order to understand how content can be extracted from HTML, it's important
    to know the fundamentals of HTML. For starters, HTML stands for **Hyper Text Markup
    Language**. Like Markdown or XML (**eXtensible Markup Language**), it's simply
    a language for marking up text.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何从 HTML 中提取内容，了解 HTML 的基础知识非常重要。首先，HTML 代表 **超文本标记语言**。像 Markdown 或 XML（**可扩展标记语言**）一样，它只是用于标记文本的语言。
- en: In HTML, the display text is contained within the content section of HTML elements,
    where element attributes specify how that element should appear on the page.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HTML 中，显示文本包含在 HTML 元素的内容部分，元素属性则指定该元素在页面上的显示方式。
- en: '![](img/756dbc5a-2843-4e9d-85bf-d9c189dfacec.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/756dbc5a-2843-4e9d-85bf-d9c189dfacec.png)'
- en: Looking at the anatomy of an HTML element, as seen in the preceding picture,
    we see the content enclosed between start and end tags. In this example, the tags
    are `<p>` for paragraph; other common tag types are `<div>` (text block), `<table>`
    (data table), `<h1>` (heading), `<img>` (image), and `<a>` (hyperlinks). Tags
    have attributes, which can hold important metadata. Most commonly, this metadata
    is used to specify how the element text should appear on the page. This is where
    CSS files come into play. The attributes can store other useful information, such
    as the hyperlink `href` in an `<a>` tag, which specifies a URL link, or the alternate
    alt label in an `<img>` tag, which specifies the text to display if the image
    resource cannot be loaded.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 HTML 元素的结构，如前图所示，我们可以看到内容被包裹在开始和结束标签之间。在这个例子中，标签是 `<p>`（段落）；其他常见的标签类型包括 `<div>`（文本块），`<table>`（数据表），`<h1>`（标题），`<img>`（图片），和
    `<a>`（超链接）。标签有属性，属性可以存储重要的元数据。最常见的情况下，这些元数据用于指定元素文本在页面上的显示方式。此时 CSS 文件发挥作用。属性还可以存储其他有用信息，如
    `<a>` 标签中的超链接 `href`，它指定了一个 URL 链接，或 `<img>` 标签中的备用 alt 标签，用于在图片资源无法加载时显示的文本。
- en: Now, let's turn our attention back to the Jupyter Notebook and parse some HTML!
    Although not necessary when following along with this section, it's very helpful
    in realworld situations to use the developer tools in Chrome or Firefox to help
    identify the HTML elements of interest. We'll include instructions for doing this
    with Chrome in the following section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把注意力转回到 Jupyter Notebook，解析一些 HTML！虽然在本节学习时不一定需要使用，但在实际应用中，使用 Chrome 或
    Firefox 的开发者工具来帮助识别感兴趣的 HTML 元素非常有用。接下来的章节我们会提供如何在 Chrome 中执行此操作的说明。
- en: Parsing HTML with Python in a Jupyter Notebook
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中使用 Python 解析 HTML
- en: In `chapter-3-workbook.ipynb file`, scroll to the top of Subtopic `Parsing HTML
    with Python`.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `chapter-3-workbook.ipynb` 文件中，滚动到 `Parsing HTML with Python` 子主题的顶部。
- en: In this section, we'll scrape the central bank interest rates for each country,
    as reported by Wikipedia. Before diving into the code, let's first open up the
    web page containing this data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将抓取每个国家的中央银行利率，这些数据来自 Wikipedia。在进入代码之前，让我们先打开包含这些数据的网页。
- en: Open up the [https://en.wikipedia.org/wiki/List_of_countries_by_ central_bank_interest_rates](https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates)
    URL in a web browser. Use Chrome, if possible, as later in this section we'll
    show you how to view and search the HTML with Chrome's developer tools.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开[https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates](https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates)这个链接，在浏览器中查看。尽可能使用
    Chrome，因为在本节中我们将展示如何使用 Chrome 的开发者工具查看和搜索 HTML。
- en: Looking at the page, we see very little content other than a big list of countries
    and their interest rates. This is the table we'll be scraping.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 看这个页面，我们除了大列表的国家和其利率之外，几乎没有其他内容。这就是我们要抓取的表格。
- en: 'Return to the Jupyter Notebook and load the HTML as a Beautiful Soup object
    so that it can be parsed. Do this by running the following code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 Jupyter Notebook，并将 HTML 加载为 Beautiful Soup 对象，以便进行解析。通过运行以下代码来完成：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We use Python's default `html.parser` as the parser, but third-party parsers
    such as `lxml` may be used instead, if desired.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Python 默认的 `html.parser` 作为解析器，但如果需要，也可以使用第三方解析器，如 `lxml`。
- en: Usually, when working with a new object like this Beautiful Soup one, it's a
    good idea to pull up the docstring by doing `soup?`. However, in this case, the
    docstring is not particularly informative. Another tool for exploring Python objects
    is `pdir`, which lists all of an object's attributes and methods (this can be
    installed with `pip install pdir2`). It's basically a formatted version of Python's
    built-in `dir` function.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在使用像 Beautiful Soup 这样的新对象时，查看文档字符串是个好主意，方法是输入 `soup?`。然而，在这种情况下，文档字符串并不特别有用。另一个用于探索
    Python 对象的工具是 `pdir`，它列出对象的所有属性和方法（可以通过 `pip install pdir2` 安装）。它本质上是 Python 内建
    `dir` 函数的格式化版本。
- en: 'Display the attributes and methods for the BeautifulSoup object by running
    the following code. This will run, regardless of whether or not the `pdir` external
    library is installed:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码显示BeautifulSoup对象的属性和方法。无论是否安装了`pdir`外部库，都会运行此代码：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we see a list of methods and attributes that can be called on `soup`.
    The most commonly used function is probably `find_all`, which returns a list of
    elements that match the given criteria.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到一个方法和属性的列表，这些方法和属性可以在`soup`上调用。最常用的函数可能是`find_all`，它返回符合给定条件的元素列表。
- en: 'Get the h1 heading for the page with the following code:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码获取页面的h1标题：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Usually, pages only have one H1 element, so it's obvious that we only find one
    here.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，页面上只有一个H1元素，因此很明显我们这里只找到一个。
- en: 'Run the next couple of cells. We redefine H1 to the fist (and only) list element
    with `h1 = h1[0]` , and then print out the HTML element attributes with `h1.attrs`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格。我们将H1重新定义为第一个（也是唯一的）列表元素，`h1 = h1[0]`，然后通过`h1.attrs`打印出HTML元素的属性：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We see the class and ID of this element, which can both be referenced by CSS
    code to define the style of this element.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到此元素的类和ID，二者都可以通过CSS代码引用，以定义该元素的样式。
- en: Get the HTML element content (that is, the visible text) by printing `h1.text`.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印`h1.text`获取HTML元素内容（即可见文本）。
- en: 'Get all the images on the page by running the following code:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取页面上的所有图片：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There are lots of images on the page. Most of these are for the country flags.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 页面上有很多图片，其中大多数是国家旗帜。
- en: 'Print the source of each image by running the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码打印每张图片的来源：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/740a50cf-9941-4f4a-a73b-745386209da4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/740a50cf-9941-4f4a-a73b-745386209da4.png)'
- en: We use a list comprehension to iterate through the elements, selecting the `src`
    attribute of each (so long as that attribute is actually available).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用列表推导式遍历元素，选择每个元素的`src`属性（前提是该属性实际存在）。
- en: Now, let's scrape the table. We'll use Chrome's developer tools to hunt down
    the element this is contained within.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们抓取表格。我们将使用Chrome的开发者工具来定位包含该元素的部分。
- en: If not already done, open the Wikipedia page we're looking at in Chrome. Then,
    in the browser, select Developer Tools from the `View menu`. A sidebar will open.
    The HTML is available to look at from the `Elements` tab in Developer Tools.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果还没有完成，打开我们正在查看的维基百科页面（在Chrome中）。然后，在浏览器中从`视图菜单`中选择开发者工具。一个侧边栏将会打开。HTML可以从开发者工具的`元素`标签中查看。
- en: 'Select the little arrow in the top left of the tools sidebar. This allows us
    to hover over the page and see where the HTML element is located, in the Elements
    section of the sidebar:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择工具侧边栏左上角的小箭头。这允许我们悬停在页面上，并查看HTML元素在侧边栏的“元素”部分的位置：
- en: '![](img/225749b6-435f-4729-9879-718d3ea1cba3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/225749b6-435f-4729-9879-718d3ea1cba3.png)'
- en: 'Hover over the body to see how the table is contained within the div that has `id="bodyContent"`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鼠标悬停在正文上，查看表格如何包含在具有`id="bodyContent"`的div中：
- en: '![](img/3ccc0aab-d3f5-43ca-ab7a-511cfda86a32.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ccc0aab-d3f5-43ca-ab7a-511cfda86a32.png)'
- en: 'Select that div by running the following code:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码选择该div：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can now seek out the table within this subset of the full HTML. Usually,
    tables are organized into headers `<th>`, rows `<tr>`, and data entries `<td>`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在这个HTML的子集内查找表格。通常，表格被组织为标题`<th>`、行`<tr>`和数据条目`<td>`。
- en: 'Get the table headers by running the following code:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取表格标题：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we see three headers. In the content of each is a break element `<br/>`,
    which will make the text a bit more difficult to cleanly parse.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到三个标题。每个标题的内容中都有一个`<br/>`换行元素，这会使得文本稍微难以干净地解析。
- en: 'Get the text by running the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取文本：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we get the content with the get_text method, and then run the replace
    string method to remove the newline resulting from the `<br/>` element. To get
    the data, we'll first perform some tests and then scrape all the data in a single cell.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`get_text`方法获取内容，然后运行`replace string`方法以移除由`<br/>`元素引起的换行符。为了获取数据，我们首先进行一些测试，然后在一个单元格中抓取所有数据。
- en: 'Get the data for each cell in the second <tr> (row) element by running the following
    code:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取第二个`<tr>`（行）元素中每个单元格的数据：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We find all the row elements, pick out the third one, and then find the three
    data elements inside that.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到所有的行元素，挑选出第三个，然后找出其中的三个数据元素。
- en: Let's look at the resulting data and see how to parse the text from each row.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果数据，看看如何解析每行的文本。
- en: 'Run the next couple of cells to print `d1` and its `text` attribute:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格以打印`d1`及其`text`属性：
- en: '![](img/753c4a22-175a-40c9-8557-35c2d040dd2b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/753c4a22-175a-40c9-8557-35c2d040dd2b.png)'
- en: We're getting some undesirable characters at the front. This can be solved by
    searching for only the text of the `<a>` tag.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面获取了一些不需要的字符。这可以通过仅搜索`<a>`标签的文本来解决。
- en: Run `d1.find('a').text` to return the properly cleaned data for that cell.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`d1.find('a').text`以返回该单元格的正确清理数据。
- en: Run the next couple of cells to print `d2` and its text. This data appears to
    be clean enough to convert directly into a flat.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格以打印`d2`及其文本。此数据看起来足够干净，可以直接转换为平面格式。
- en: 'Run the next couple of cells to print `d3` and its text:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格以打印`d3`及其文本：
- en: '![](img/0a3416ed-8371-407e-a654-8947074319f2.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a3416ed-8371-407e-a654-8947074319f2.png)'
- en: Similar to `d1`, we see that it would be better to get only the `span` element's
    text.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`d1`，我们看到最好只获取`span`元素的文本。
- en: 'Properly parse the date for this table entry by running the following code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，正确解析此表格条目的日期：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we''re ready to perform the full scrape by iterating over the row elements `<th>`.
    Run the following code:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备通过遍历行元素`<th>`来进行完整抓取。运行以下代码：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We iterate over the rows, ignoring any that contain more than three data elements.
    These rows will not correspond to data in the table we are interested in. Rows
    that do have three data elements are assumed to be in the table, and we parse
    the text from these as identified during the testing. T
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历这些行，忽略任何包含三个以上数据元素的行。这些行不对应我们感兴趣的表格数据。那些确实包含三个数据元素的行被认为是表格中的数据，我们按照测试时确定的方式解析这些文本。
- en: The text parsing is done inside a `try/except` statement, which will catch any
    errors and allow this row to be skipped without stopping the iteration. Any rows
    that raise errors due to this statement should be looked at. The data for these
    could be recorded manually or accounted for by altering the scraping loop and
    re-running it. In this case, we'll ignore any errors for the sake of time.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 文本解析在`try/except`语句中完成，这将捕获任何错误并允许跳过该行而不会停止迭代。任何由于此语句导致错误的行应该进行检查。可以手动记录这些数据，或通过修改抓取循环并重新运行它来考虑这些行。在此情况下，为了节省时间，我们将忽略任何错误。
- en: 'Print the head of the scraped data list by running print(data[:10]):'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`print(data[:10])`打印抓取数据列表的前10行：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We''ll visualize this data later in the chapter. For now, save the data to
    a CSV file by running the following code:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在本章稍后可视化这些数据。现在，先通过运行以下代码将数据保存为CSV文件：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we are using semicolons to separate the fields.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用分号来分隔字段。
- en: Activity:Web Scraping with Jupyter Notebooks
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：使用Jupyter Notebooks进行网页抓取
- en: We are going to get the population of each country. Then, in the next topic,
    this will be visualized along with the interest rate data scraped in the previous
    section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获取每个国家的人口数据。然后，在下一个主题中，这些数据将与上一节抓取的利率数据一起进行可视化。
- en: 'The page we look at in this activity is available here: [http://www.worldometers.info/world-population/population-by-country/](http://www.worldometers.info/world-population/population-by-country/).
    Now that we''ve seen the basics of web scraping, let''s apply the same techniques
    to a new web page and scrape some more data!'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本次活动中我们查看的页面可以在此访问：[http://www.worldometers.info/world-population/population-by-country/](http://www.worldometers.info/world-population/population-by-country/)。现在我们已经了解了网页抓取的基础知识，接下来我们将相同的技术应用到一个新网页并抓取更多数据！
- en: 'This page may have changed since this document was created. If this URL no
    longer leads to a table of country populations, please use this Wikipedia page
    instead: [https://en.wikipedia.org/wiki/List_of_countries_by_population (United_Nations)](https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations))
    .'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本文档创建时页面可能已发生变化，如果此URL不再指向国家人口表格，请使用此Wikipedia页面：[https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)](https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations))。
- en: 'For this page, the data can be scraped using the following code snippet:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于此页面，数据可以使用以下代码片段进行抓取：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the `chapter-3-workbook.ipynb` Jupyter Notebook, scroll to `Activity Web scraping
    with Python`.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`chapter-3-workbook.ipynb` Jupyter Notebook中，滚动至`Activity Web scraping with
    Python`部分。
- en: 'Set the `url` variable and load an IFrame of our page in the notebook by running
    the following code:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`url`变量并通过运行以下代码在笔记本中加载我们的页面的IFrame：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Close the IFrame by selecting the cell and clicking Current Outputs | Clear
    from the Cell menu in the Jupyter Notebook.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择单元格并点击Jupyter Notebook的Cell菜单中的“Current Outputs | Clear”，来关闭IFrame。
- en: 'Request the page and load it as a BeautifulSoup object by running the following code:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码请求页面并将其加载为BeautifulSoup对象：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We feed the page content to the `BeautifulSoup` constructor. Recall that previously,
    we used page.text here instead. The difference is that page.content returns the
    raw binary response content, whereas page.text returns the `UTF-8` decoded content.
    It's usually best practice to pass the bytes object and let `BeautifulSoup` decode
    it, rather than doing it with Requests using `page.text`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将页面内容传递给`BeautifulSoup`构造函数。回想一下，之前我们使用的是`page.text`。它们的区别在于，`page.content`返回的是原始的二进制响应内容，而`page.text`返回的是`UTF-8`解码后的内容。通常最好将字节对象传递给`BeautifulSoup`，让它来进行解码，而不是使用Requests中的`page.text`来处理。
- en: 'Print the `H1` for the page by running the following code:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码打印页面的`H1`：
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We'll scrape the table by searching for `<th>`, `<tr>`, and `<td>` elements,
    as in the previous section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过搜索`<th>`、`<tr>`和`<td>`元素来抓取表格数据，正如在前一部分中所做的那样。
- en: 'Get and print the table headings by running the following code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取并打印表头：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We are only interested in the fist three columns. Select these and parse the
    text with the following code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只对前三列感兴趣。选择这些列并使用以下代码解析文本：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After selecting the subset of table headers we want, we parse the text content
    from each and remove any newline characters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择我们想要的表头子集后，我们将从每个表头中解析文本内容，并去除任何换行符。
- en: Now, we'll get the data. Following the same prescription as the previous section, we'll
    test how to parse the data for a sample row.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来获取数据。按照前一部分的流程，我们将测试如何解析一行样本数据。
- en: 'Get the data for a sample row by running the following code:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取一行样本数据：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How many columns of data do we have? Print the length of `row_data` by running
    `print(len(row_data))` .
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有多少列数据？通过运行`print(len(row_data))`来打印`row_data`的长度。
- en: 'Print the fist elements by running `print(row_data[:4])` :'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`print(row_data[:4])`来打印前四个元素：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It's pretty obvious that we want to select list indices 1, 2, and 3\. The first
    data value can be ignored, as it's simply the index.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们需要选择索引1、2和3。第一个数据值可以忽略，因为它仅仅是索引。
- en: 'Select the data elements we''re interested in parsing by running the following
    code:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码选择我们感兴趣的解析数据元素：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Looking at the `row_data` output, we can find out how to correctly parse the
    data.We''ll want to select the content of the `<a>` element in the fist data element,
    and then simply get the text from the others. Test these assumptions by running
    the following code:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下`row_data`的输出，我们可以找出如何正确地解析数据。我们将选择第一个数据元素中的`<a>`元素的内容，然后简单地从其他元素中提取文本。通过运行以下代码来测试这些假设：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Excellent! This looks to be working well. Now, we're ready to scrape the entire table.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！看起来这运行得很顺利。现在，我们准备抓取整个表格。
- en: 'Scrape and parse the table data by running the following code:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码抓取并解析表格数据：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is quite similar to before, where we try to parse the text and skip the
    row if there's some error.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的情况非常相似，当我们尝试解析文本时，如果出现错误，我们会跳过这一行。
- en: 'Print the head of the scraped data by running print(data[:10]):'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`print(data[:10])`来打印抓取数据的前十条：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It looks like we have managed to scrape the data! Notice how similar the process
    was for this table compared to the Wikipedia one, even though this web page is
    completely different. Of course, it will not always be the case that data is contained
    within a table, but regardless, we can usually use `find_all` as the primary method
    for parsing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们已经成功地抓取了数据！请注意，尽管这个网页完全不同，但这个表格的抓取过程与维基百科的表格非常相似。当然，数据不一定总是以表格的形式存在，但无论如何，我们通常可以使用`find_all`作为主要的解析方法。
- en: 'Finally, save the data to a `CSV file` for later use. Do this by running the
    following code:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将数据保存为一个`CSV 文件`以供后续使用。通过运行以下代码实现：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: To summarize, we've seen how Jupyter Notebooks can be used for web scraping.
    We started this chapter by learning about HTTP methods and status codes. Then,
    we used the Requests library to actually perform HTTP requests with Python and
    saw how the Beautiful Soup library can be used to parse the HTML responses.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经了解了如何使用Jupyter Notebook进行网页抓取。我们从学习HTTP方法和状态码开始。然后，我们使用Requests库来实际执行HTTP请求，并看到了如何使用Beautiful
    Soup库来解析HTML响应。
- en: Our Jupyter Notebook turned out to be a great tool for this type of work. We
    were able to explore the results of our web requests and experiment with various
    HTML parsing techniques. We were also able to render the HTML and even load a
    live version of the web page inside the notebook!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Jupyter Notebook 证明是进行此类工作的一个很好的工具。我们能够探索网页请求的结果，并尝试各种 HTML 解析技术。我们还能够在笔记本中渲染
    HTML，甚至加载网页的实时版本！
- en: 'In the next topic of this chapter, we shift to a completely new topic: interactive
    visualizations. We''ll see how to create and display interactive charts right
    inside the notebook, and use these charts as a way to explore the data we''ve
    just collected.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一个主题中，我们将转向一个全新的话题：交互式可视化。我们将展示如何在笔记本中创建和显示交互式图表，并使用这些图表作为探索我们刚刚收集的数据的一种方式。
- en: Interactive Visualizations
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交互式可视化
- en: Visualizations are quite useful as a means of extracting information from a
    dataset. For example, with a bar graph it's very easy to distinguish the value
    distribution, compared to looking at the values in a table. Of course, as we have
    seen earlier in this book, they can be used to study patterns in the dataset that
    would otherwise be quite difficult to identify. Furthermore, they can be used
    to help explain a dataset to an unfamiliar party. If included in a blog post,
    for example, they can boost reader interest levels and be used to break up blocks
    of text.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是从数据集中提取信息的一个非常有用的手段。例如，通过条形图，比起查看表格中的数值，区分数值分布变得非常容易。当然，正如我们在本书前面所看到的，它们还可以用来研究数据集中一些否则很难识别的模式。此外，它们还可以帮助向不熟悉的人解释数据集。例如，如果包含在博客文章中，它们可以提升读者的兴趣，并用来打破一块块的文字内容。
- en: When thinking about interactive visualizations, the benefits are similar to
    static visualizations, but enhanced because they allow for active exploration
    on the viewer's part. Not only do they allow the viewer to answer questions they
    may have about the data, they also think of new questions while exploring. This
    can benefit a separate party such as a blog reader or co-worker, but also a creator,
    as it allows for easy adhoc exploration of the data in detail, without having
    to change any code.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑交互式可视化时，它的好处与静态可视化相似，但由于它们允许观众进行主动探索，因此更加增强。不仅允许观众回答他们可能对数据有的问题，还可以在探索过程中提出新的问题。这对像博客读者或同事这样的外部人员有益，对于创作者来说也同样重要，因为它允许他们在不修改任何代码的情况下，轻松地对数据进行详细的即兴探索。
- en: In this topic, we'll discuss and show how to use Bokeh to build interactive
    visualizations in Jupyter. Prior to this, however, we'll briefly revisit pandas
    Data Frames, which play an important role in doing data visualization with Python.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论并展示如何使用 Bokeh 在 Jupyter 中构建交互式可视化。但在此之前，我们将简要回顾 pandas 的 DataFrame，它在使用
    Python 进行数据可视化时发挥着重要作用。
- en: Building a DataFrame to Store and Organize Data
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个用于存储和组织数据的 DataFrame
- en: As we've seen time and time again in this book, pandas is an integral part of
    doing data science with Python and Jupyter Notebooks. DataFrames offer a way to
    organize and store labeled data, but more importantly, pandas provides time saving
    methods for transforming data within a DataFrame. Examples we have seen in this
    book include dropping duplicates, mapping dictionaries to columns, applying functions
    over columns, and filing in missing values.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中一再看到的，pandas 是使用 Python 和 Jupyter Notebooks 进行数据科学的一个重要组成部分。DataFrame
    提供了一种组织和存储带标签数据的方式，但更重要的是，pandas 提供了节省时间的方法，用于在 DataFrame 中转换数据。本书中我们看到的一些例子包括删除重复项、将字典映射到列、对列应用函数，以及填充缺失值。
- en: With respect to visualizations, DataFrames offer methods for creating all sorts
    of matplotlib graphs, including `df.plot.barh()` , `df.plot.hist()` , and more.
    The interactive visualization library Bokeh previously relied on pandas DataFrames
    for their *high-level* charts. These worked similar to Seaborn, as we saw earlier
    in the previous chapter, where a DataFrame is passed to the plotting function
    along with the specific columns to plot. The most recent version of Bokeh, however,
    has dropped support for this behavior. Instead, plots are now created in much
    the same way as matplotlib, where the data can be stored in simple lists or NumPy
    arrays. The point of this discussion is that DataFrames are not entirely necessary,
    but still very helpful for organizing and manipulating the data prior to visualization.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可视化，数据框提供了创建各种matplotlib图形的方法，包括`df.plot.barh()`、`df.plot.hist()`等。交互式可视化库Bokeh以前依赖Pandas数据框来生成其*高级*图表。这些图表的工作方式类似于Seaborn，正如我们在前一章看到的那样，其中数据框作为输入传递给绘图函数，并指定需要绘制的列。然而，Bokeh的最新版本已不再支持这种行为。现在，图形的创建方式与matplotlib类似，数据可以存储在简单的列表或NumPy数组中。讨论的要点是，数据框并非绝对必要，但仍然非常有助于在可视化之前组织和处理数据。
- en: Building and merging Pandas DataFrames
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和合并Pandas数据框
- en: Let's dive right into an exercise, where we'll continue working on the country
    data we scraped earlier. Recall that we extracted the central bank interest rates
    and populations of each country, and saved the results in CSV files. We'll load
    the data from these files and merge them into a DataFrame, which will then be
    used as the data source for the interactive visualizations to follow.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入一个练习，继续处理之前抓取的国家数据。回想一下，我们提取了每个国家的中央银行利率和人口，并将结果保存在CSV文件中。接下来，我们将从这些文件加载数据并将其合并成一个数据框，然后将作为后续交互式可视化的数据源。
- en: In the `chapter-3-workbook.ipynb` Jupyter Notebook, scroll to the Subtopic `Building
    a DataFrame to store and organize data` .
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`chapter-3-workbook.ipynb` Jupyter Notebook中，滚动到子主题`构建一个数据框以存储和组织数据`。
- en: We are first going to load the data from the `CSV files`, so that it's back
    to the state it was in after scraping. This will allow us to practice building
    DataFrames from Python objects, as opposed to using the `pd.read_csv function`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将从`CSV文件`加载数据，这样它就恢复到抓取后原始的状态。这将使我们能够练习从Python对象构建数据框，而不是使用`pd.read_csv`函数。
- en: When using `pd.read_csv`, the datatype for each column will be inferred from
    the string input. On the other hand, when using `pd.DataFrame` as we do here,
    the datatype is instead taken as the type of the input variables. In our case,
    as will be seen, we read the file and do not bother converting the variables to
    numeric or date-time until after instantiating the DataFrame.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.read_csv`时，每一列的数据类型将根据字符串输入自动推断。另一方面，当我们使用`pd.DataFrame`时，数据类型将根据输入变量的类型来决定。就我们来说，正如将要看到的，我们在读取文件后不会立即将变量转换为数字或日期时间类型，而是在实例化数据框后进行转换。
- en: 'Load the CSV files into lists by running the following code:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码将CSV文件加载到列表中：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Check what the resulting lists look like by running the next two cells. We
    should see an output similar to the following:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行接下来的两个单元格，检查结果列表的样子。我们应该能看到类似以下的输出：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, the data is in a standard Python list structure, just as it was after scraping
    from the web pages in the previous sections. We're now going to create two DataFrames
    and merge them, so that all of the data is organized within one object.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据处于标准的Python列表结构中，就像在前面章节从网页抓取后一样。接下来，我们将创建两个数据框并将它们合并，以便将所有数据组织到一个对象中。
- en: 'Use the standard DataFrame constructor to create the two DataFrames by running the
    following code:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，使用标准的数据框构造函数创建两个数据框：
- en: '[PRE35]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This isn't the first time we've used this function in this book. Here, we pass
    the lists of data (as seen previously) and the corresponding column names. The
    input data can also be of dictionary type, which can be useful when each column
    is contained in a separate list.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们第一次在本书中使用这个函数。这里，我们传递数据列表（如前所示）和相应的列名。输入数据也可以是字典类型，当每列数据分别存储在不同的列表中时，这种方式非常有用。
- en: Next, we're going to clean up each DataFrame. Starting with the interest rates
    one, let's print the head and tail, and list the data types.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将清理每个数据框。从利率数据框开始，让我们打印出前几行和后几行，并列出数据类型。
- en: 'When displaying the entire DataFrame, the default maximum number of rows is
    60 (for version 0.18.1). Let''s reduce this to 10 by running the following code:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示整个数据框时，默认的最大行数为60（适用于版本0.18.1）。让我们通过运行以下代码将其减少为10：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Display the head and tail of the interest rates DataFrame by running the following code:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码显示利率数据框的头部和尾部：
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/d1c43f2f-daf2-4866-9dbb-de64f5794494.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1c43f2f-daf2-4866-9dbb-de64f5794494.png)'
- en: 'Print the data types by running:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令打印数据类型：
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Pandas has assigned each column as a string datatype, which makes sense because
    the input variables were all strings. We'll want to change these to string, float,
    and date-time, respectively.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas已将每一列分配为字符串数据类型，这是合理的，因为输入变量都是字符串。我们将需要分别将这些转换为字符串、浮动和日期时间类型。
- en: 'Convert to the proper datatypes by running the following code:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码将数据转换为正确的类型：
- en: '[PRE39]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We use `astype` to cast the Interest Rate values as floats, setting `copy=False`
    to save memory. Since the date values are given in such an easy-to-read format,
    these can be converted simply by using `pd.to_datetime`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`astype`将利率值转换为浮动类型，并设置`copy=False`以节省内存。由于日期值的格式非常易读，因此可以通过使用`pd.to_datetime`轻松地将其转换。
- en: 'Check the new datatypes of each column by running the following code:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码检查每一列的新数据类型：
- en: '[PRE40]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As can be seen, everything is now in the proper format.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，现在所有数据都已正确格式化。
- en: 'Let''s apply the same procedure to the other DataFrame. Run the next few cells
    to repeat the preceding steps for `df_populations`:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对另一个数据框应用相同的过程。运行接下来的几个单元格来重复对`df_populations`执行前面的步骤：
- en: '[PRE41]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/f705ec84-e2f2-4a1a-b184-8ce45d7a1cdb.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f705ec84-e2f2-4a1a-b184-8ce45d7a1cdb.png)'
- en: 'Then, run this code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行此代码：
- en: '[PRE42]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: To cast the numeric columns as a float, we had to first apply some modifications
    to the strings in this case. We stripped away any commas from the populations
    and removed the percent sign from the Yearly Change column, using string methods.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数值列转换为浮动类型，我们必须首先对这些字符串做一些修改。我们使用字符串方法去掉了人口数据中的逗号，并去除了“年变化”列中的百分号。
- en: Now, we're going to merge the DataFrames on the country name for each row. Keep
    in mind that these are still the raw country names as scraped from the web, so
    there might be some work involved with matching the strings.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将根据每一行的国家名称合并数据框。请记住，这些仍然是从网络抓取的原始国家名称，因此在匹配字符串时可能需要进行一些工作。
- en: 'Merge the DataFrames by running the following code:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码合并数据框：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We pass the population data in the left DataFrame and the interest rates in
    the right one, performing an outer match on the country columns. This will result
    in `NaN values` where the two do not overlap.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将左侧数据框中的人口数据和右侧数据框中的利率数据传入，并在国家列上执行外连接匹配。这将导致在两者不重叠的地方出现`NaN值`。
- en: 'For the sake of time, let''s just look at the most populated countries to see
    whether we missed matching any. Ideally, we would want to check everything. Look
    at the most populous countries by running the following code:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了节省时间，让我们仅查看人口最多的国家，看看是否漏掉了任何匹配。理想情况下，我们应该检查所有内容。通过运行以下代码查看人口最多的国家：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/c00baec8-0c59-4528-8f78-228fd4503cf6.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c00baec8-0c59-4528-8f78-228fd4503cf6.png)'
- en: It looks like U.S. didn't match up. This is because it's listed as United States
    in the interest rates data. Let's remedy this.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来美国（U.S.）没有匹配上。这是因为在利率数据中它被列为“United States”。让我们来修复这个问题。
- en: 'Fix the label for U.S. in the populations table by running the following code:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码修正人口表中美国（U.S.）的标签：
- en: '[PRE45]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We rename the country for the populations DataFrame with the use of the `loc` method
    to locate that row. Now, let's merge the DataFrames properly.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`loc`方法定位该行，并重命名人口数据框中的国家名称。现在，让我们正确地合并数据框。
- en: 'Re-merge the DataFrames on the country names, but this time use an inner merge to
    remove the `NaN` values:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次根据国家名称合并数据框，但这次使用内部合并来删除`NaN`值：
- en: '[PRE46]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We are left with two identical columns in the merged DataFrame. Drop one of
    them by running the following code:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在合并后的数据框中留下了两列相同的内容。通过运行以下代码删除其中一列：
- en: '[PRE47]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Rename the columns by running the following code:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码重命名列：
- en: '[PRE48]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We are left with the following merged and cleaned DataFrame:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下合并和清理后的数据框：
- en: '![](img/609d17de-62a7-46c1-9385-af183c54d3a1.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/609d17de-62a7-46c1-9385-af183c54d3a1.png)'
- en: 'Now that we have all the data in a nicely organized table, we can move on to
    the fun part: visualizing it. Let''s save this table to a `CSV file` for later
    use, and then move on to discuss how visualizations can be created with Bokeh.
    Write the merged data to a `CSV file` for later use with the following code:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将所有数据整理成一个结构良好的表格，接下来进入有趣的部分：可视化。我们将把这个表格保存为`CSV文件`以供以后使用，然后讨论如何使用Bokeh创建可视化。使用以下代码将合并后的数据写入`CSV文件`以供以后使用：
- en: '[PRE49]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Introduction to Bokeh
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bokeh简介
- en: Bokeh is an interactive visualization library for Python. Its goal is to provide
    similar functionality to `D3`, the popular interactive visualization library for
    JavaScript. Bokeh functions very differently than `D3`, which is not surprising
    given the differences between Python and JavaScript. Overall, it's much simpler
    and it doesn't allow nearly as much customization as `D3` does. This works to
    its advantage though, as it's much easier to use, and it still boasts an excellent
    suite of features that we'll explore in this section.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Bokeh是一个用于Python的交互式可视化库。其目标是提供类似于流行的JavaScript交互式可视化库`D3`的功能。Bokeh的工作方式与`D3`截然不同，这并不令人惊讶，因为Python和JavaScript之间存在差异。总体而言，它更简单，且不像`D3`那样允许进行大量定制。然而，这正是它的优势所在，因为它更易于使用，并且仍然拥有一套出色的功能，我们将在本节中进行探讨。
- en: Let's dive right into a quick exercise with the Jupyter Notebook and introduce
    Bokeh by example.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过Jupyter笔记本的快速练习，直接通过示例介绍Bokeh。
- en: There is good documentation online for Bokeh, but much of it is outdated. Searching
    something like `Bokeh bar plot` in Google still tends to turn up documentation
    for legacy modules that no longer exist, for example, the high-level plotting
    tools that used to be available through `bokeh.charts` (prior to version 0.12.0).
    These are the ones that take pandas DataFrames as input in much the same way that
    Seaborn plotting functions do. Removing the high-level plotting tools module has
    simplified Bokeh, and will allow for more focused development going forward. Now,
    the plotting tools are largely grouped into the bokeh. `plotting module`, as will
    be seen in the next exercise and following activity.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Bokeh的在线文档很完善，但很多内容已经过时。例如，在Google中搜索`Bokeh条形图`通常会找到旧版模块的文档，这些模块已经不再存在，如以前可以通过`bokeh.charts`访问的高级绘图工具（版本0.12.0之前）。这些工具接收类似于Seaborn绘图函数的pandas
    DataFrame作为输入。删除这些高级绘图工具模块使得Bokeh变得更加简化，并且能够专注于更有针对性的开发。现在，绘图工具主要被归类到bokeh.`plotting模块`中，正如在接下来的练习和活动中所看到的那样。
- en: Introduction to interactive visualizations with Bokeh
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Bokeh进行交互式可视化简介
- en: We'll load the required Bokeh modules and show some simple interactive plots
    that can be made with Bokeh. Please note that the examples in this book have been
    designed using version 0.12.10 of Bokeh.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载所需的Bokeh模块，并展示一些简单的交互式图表，这些图表可以通过Bokeh创建。请注意，本书中的示例是使用Bokeh 0.12.10版本设计的。
- en: In the `chapter-3-workbook.ipynb` Jupyter notebook, scroll to Subtopic `Introduction
    to Bokeh`.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`chapter-3-workbook.ipynb` Jupyter 笔记本中，滚动到子主题`Bokeh简介`。
- en: 'Like scikit-learn, Bokeh modules are usually loaded in pieces (unlike pandas,
    for example, where the whole library is loaded at once). Import some basic plotting
    modules by running the following code:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与scikit-learn类似，Bokeh模块通常是分块加载的（与pandas不同，后者会一次性加载整个库）。通过运行以下代码来导入一些基本的绘图模块：
- en: '[PRE50]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We need to run `output_notebook()` in order to render the interactive visuals within
    the Jupyter notebook.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要运行`output_notebook()`，以便在Jupyter笔记本中渲染交互式可视化。
- en: 'Generate random data to plot by running the following code:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码生成随机数据进行绘图：
- en: '[PRE51]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The random data is generated using the cumulative sum of a random set of numbers
    that are distributed about zero. The effect is a trend that looks similar to a stock
    price time series, for example.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数据是通过累积求和一组随机数生成的，这些随机数分布在零附近。其效果类似于股票价格时间序列的趋势。
- en: 'Plot the data with a line plot in Bokeh by running the following code:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码在Bokeh中绘制数据的折线图：
- en: '[PRE52]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/f9f51bc7-d14c-435b-b8d2-a069d61cdc52.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9f51bc7-d14c-435b-b8d2-a069d61cdc52.png)'
- en: We instantiate the figure, as referenced by the variable p, and then plot a
    line. Running this in Jupyter yields an interactive figure with various options
    along the right-hand side.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了图形，如变量p所引用的那样，然后绘制了一条线。在Jupyter中运行这段代码会生成一个交互式图形，并在右侧显示各种选项。
- en: The top three options (as of version 0.12.10) are **Pan**, **Box Zoom**, and
    **Wheel Zoom**. Play around with these and experiment with how they work. Use
    the reset option to re-load the default plot limits.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个选项（截至版本0.12.10）是**平移**、**框缩放**和**滚轮缩放**。尝试使用这些选项并实验它们的工作方式。使用重置选项重新加载默认的图表限制。
- en: 'Other plots can be created with the alternative methods of `figure`. Draw a
    scatter plot by running the following code, where we replace `line` in the preceding
    code with `circle`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他图表可以通过`figure`的替代方法创建。通过运行以下代码绘制散点图，在该代码中，我们将前述代码中的`line`替换为`circle`：
- en: '[PRE53]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/fa778c9d-5325-479e-8cf2-e7bd42030acb.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa778c9d-5325-479e-8cf2-e7bd42030acb.png)'
- en: Here, we've specified the size of each circle using a random set of numbers.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一组随机数字来指定每个圆的大小。
- en: A very enticing feature of interactive visualizations is the tooltip. This is
    a hover tool that allows the user to get information about a point by hovering
    over it.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式可视化的一个非常诱人的功能是工具提示。这是一个悬停工具，允许用户通过悬停在某个点上来获取该点的信息。
- en: 'In order to add this tool, we''re going to use a slightly different method
    for creating the plot. This will require us to import a couple of new libraries.
    Run the following code:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了添加这个工具，我们将使用一种稍微不同的方法来创建图表。这将需要我们导入一些新的库。请运行以下代码：
- en: '[PRE54]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This time, we'll create a data source to pass to the plotting method. This can
    contain metadata, which can be included in the visualization via the hover tool.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将创建一个数据源并传递给绘图方法。它可以包含元数据，这些元数据可以通过悬停工具包含在可视化中。
- en: 'Create random labels and plot the interactive visualization with a hover tool
    by running the following code:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码创建随机标签并绘制带有悬停工具的交互式可视化：
- en: '[PRE55]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/ad807af2-a484-47dd-a750-662ac88e7a1e.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad807af2-a484-47dd-a750-662ac88e7a1e.png)'
- en: We define a data source for the plot by passing a dictionary of key/value pairs
    to the `ColumnDataSource` constructor. This source includes the *x* location,
    *y* location, and size of each point, along with the random letter *A*, *B*, or
    *C* for each point. These random letters are assigned as labels for the hover
    tool, which will also display the size of each point.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将一个键值对字典传递给`ColumnDataSource`构造函数来定义图表的数据源。该数据源包括每个点的*x*位置、*y*位置和大小，以及每个点的随机字母*A*、*B*或*C*。这些随机字母将作为悬停工具的标签，并显示每个点的大小。
- en: The **Hover Tool** is then added to the figure, and the data is retrieved from
    each element through the specific plotting method, which is circle in this case.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将**悬停工具**添加到图表中，通过特定的绘图方法从每个元素中提取数据，在此例中是圆形。
- en: The result is that we are now able to hover over the points and see the data
    we've selected for the **Hover Tool**!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们现在可以在点上悬停并查看我们为**悬停工具**选择的数据！
- en: We notice, by looking at the toolbar to the right of the plot, that by explicitly
    including the **Hover Tool**, the others have disappeared. These can be included
    by manually adding them to the list of tool objects that gets passed to `bokeh.
    plotting.figure`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，通过查看位于图表右侧的工具栏，可以发现通过显式包含**悬停工具**，其他工具已经消失。通过手动将它们添加到传递给`bokeh.plotting.figure`的工具对象列表中，可以重新显示这些工具。
- en: 'Add pan, zoom, and reset tools to the plot by running the following code:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，向图表添加平移、缩放和重置工具：
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This code is identical to what was previously shown except for the tools variable, which
    now references several new tools we've imported from the Bokeh library.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与之前展示的完全相同，唯一不同的是工具变量，它现在引用了我们从Bokeh库导入的多个新工具。
- en: We'll stop the introductory exercise here, but we'll continue creating and exploring
    plots in the following activity.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里停止介绍性练习，但将在接下来的活动中继续创建和探索图表。
- en: Activity:Exploring Data with Interactive Visualizations
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：通过交互式可视化探索数据
- en: We'll pick up using Bokeh right where we left off with the previous exercise,
    except instead of using the randomly generated data seen there, we'll instead
    use the data we scraped from the web in the fist part of this chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用Bokeh，从前一个练习中断的位置开始，除了不再使用随机生成的数据，而是使用我们在本章第一部分从网页抓取的数据。
- en: To use Bokeh to create interactive visualizations of our scraped data.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Bokeh来创建我们的抓取数据的交互式可视化。
- en: 'In the `chapter-3-workbook.ipynb` file, scroll to the `Activity: Interactive visualizations
    with Bokeh section`.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在`chapter-3-workbook.ipynb`文件中，滚动到`Activity: Interactive visualizations with
    Bokeh`部分。'
- en: 'Load the previously scraped, merged, and cleaned web page data by running the following
    code:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，加载先前抓取、合并和清理的网页数据：
- en: '[PRE57]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Recall what the data looks like by displaying the DataFrame:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过显示 DataFrame 来回顾数据的样子：
- en: '![](img/37124811-b721-4d65-80c3-5e483686f8c1.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37124811-b721-4d65-80c3-5e483686f8c1.png)'
- en: Whereas in the previous exercise we were interested in learning how Bokeh worked,
    now we are interested in what this data looks like. In order to explore this dataset,
    we are going to use interactive visualizations.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 而在前一个练习中，我们关注的是了解 Bokeh 的工作原理，现在我们关注的是这份数据的样子。为了探索这个数据集，我们将使用交互式可视化。
- en: 'Draw a scatter plot of the population as a function of the interest rate by
    running the following code:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码绘制人口与利率之间关系的散点图：
- en: '[PRE58]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](img/1f175063-00f7-4bf9-85b9-b7a19a8eda9d.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f175063-00f7-4bf9-85b9-b7a19a8eda9d.png)'
- en: This is quite similar to the final examples we looked at when introducing Bokeh
    in the previous exercise. We set up a customized data source with the x and y
    coordinates for each point, along with the country name. This country name is
    passed to the **Hover Tool**, so that it's visible when hovering the mouse over
    the dot. We pass this tool to the figure, along with a set of other useful tools.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在前一个练习中介绍 Bokeh 时看到的最终示例非常相似。我们设置了一个自定义数据源，其中包含每个点的 x 和 y 坐标，以及国家名称。这个国家名称会传递给**悬停工具**，使其在鼠标悬停在点上时可见。我们将此工具传递给图形，并添加了一组其他有用的工具。
- en: 'In the data, we see some clear outliers with high populations. Hover over these
    to see what they are:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据中，我们看到一些明显的人口高峰值。将鼠标悬停在这些点上，查看它们对应的国家：
- en: '![](img/5b5c8b32-c6f6-4abf-8771-4f082bf5c2d0.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b5c8b32-c6f6-4abf-8771-4f082bf5c2d0.png)'
- en: We see they belong to India and China. These countries have fairly average interest
    rates. Let's focus on the rest of the points by using the **Box Zoom** tool to
    modify the view window size.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到它们属于印度和中国。这些国家的利率相对平均。让我们使用**框选缩放**工具来调整视图窗口的大小，专注于其余的点。
- en: 'Select the Box Zoom tool and alter the viewing window to better see the majority
    of the data:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**框选缩放**工具，并调整视图窗口，以更好地查看大部分数据：
- en: '![](img/6031cccf-3e75-466f-8aff-97e113f9ca58.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6031cccf-3e75-466f-8aff-97e113f9ca58.png)'
- en: '![](img/98950e43-777f-492e-b146-7505210a94f5.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98950e43-777f-492e-b146-7505210a94f5.png)'
- en: 'Explore the points and see how the interest rates compare for various countries.What
    are the countries with the highest interest rates?:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 探索这些点，看看不同国家的利率是如何比较的。哪些国家的利率最高？
- en: '![](img/e1e76aa6-a9fc-4d7f-a2b1-bfd83f8d138e.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1e76aa6-a9fc-4d7f-a2b1-bfd83f8d138e.png)'
- en: 'Some of the lower population countries appear to have negative interest rates.
    Select the **Wheel Zoom** tool and use it to zoom in on this region. Use the **Pan**
    tool to re-center the plot, if needed, so that the negative interest rate samples
    are in view. Hover over some of these and see what countries they correspond to:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些人口较少的国家似乎出现了负利率。选择**滚轮缩放**工具并使用它来放大这一地区。如果需要，可以使用**平移**工具重新调整图表的中心，使负利率样本显示在视图中。将鼠标悬停在这些样本上，查看它们对应的国家：
- en: '![](img/349b1120-db5a-4ca3-9d4a-5ccfab136c7d.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/349b1120-db5a-4ca3-9d4a-5ccfab136c7d.png)'
- en: '![](img/c2f37280-5aa8-43cb-80e7-82efa4d97c50.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2f37280-5aa8-43cb-80e7-82efa4d97c50.png)'
- en: Let's re-plot this, adding a color based on the date of last interest rate change.
    This will be useful to search for relations between the date of last change and
    the interest rate or population size.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新绘制图形，基于最后利率变化的日期添加颜色。这对于搜索最后变动日期与利率或人口规模之间的关系非常有用。
- en: 'Add a Year of last change column to the DataFrame by running the following
    code:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码向 DataFrame 添加“最后变动年份”列：
- en: '[PRE59]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We first define a function to group the samples based on year of last change,
    and then apply that function to the **Date of last change** column. Next, we need
    to map these values to colors for the visualization.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个函数，按最后变动年份对样本进行分组，然后将该函数应用于**最后变动日期**列。接下来，我们需要将这些值映射到颜色，以便进行可视化。
- en: 'Create a map to group the last change date into color categories by running
    the following code:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码创建一个地图，将最后变动日期分组到不同颜色类别中：
- en: '[PRE60]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Once mapped to the `Year of last change` column, this will assign values to
    colors based on the available categories: 2018, 2017, 2016, and Other. The colors
    here are standard strings, but they could alternatively by represented by hexadecimal
    codes.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦映射到`最后变动年份`列，这将根据可用的类别（2018年、2017年、2016年和其他）将值分配给颜色。这里的颜色是标准字符串，但它们也可以用十六进制代码表示。
- en: 'Create the colored visualization by running the following code:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码创建彩色可视化：
- en: '[PRE61]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/4bcbaf09-268f-44ee-87d1-0eb714ff1705.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bcbaf09-268f-44ee-87d1-0eb714ff1705.png)'
- en: There are some technical details that are important here. First of all, we add
    the colors and labels for each point to the `ColumnDataSource`. These are then
    referenced when plotting the circles by setting the `fill_color` and legend arguments.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些重要的技术细节。首先，我们为每个点添加颜色和标签到`ColumnDataSource`中。然后在绘制圆圈时通过设置`fill_color`和legend参数引用这些信息。
- en: 'Looking for patterns, zoom in on the lower population countries:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找模式，放大低人口国家的数据：
- en: '![](img/be463b9f-63fc-40f4-991e-b9eb1c760eb6.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be463b9f-63fc-40f4-991e-b9eb1c760eb6.png)'
- en: We can see how the dark dots are more prevalent to the right-hand side of the
    plot. This indicates that countries that have higher interest rates are more likely
    to have been recently updated.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在绘图的右侧，黑色点更为显著。这表明利率较高的国家更有可能最近进行更新。
- en: The one data column we have not yet looked at is the year-over-year change in
    population. Let's visualize this compared to the interest rate and see if there
    is any trend. We'll also enhance the plot by setting the circle size based on
    the country population.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未查看的数据列是年度人口变化。让我们将其与利率进行比较，并查看是否存在任何趋势。我们还将通过基于国家人口设置圆圈大小来增强图表。
- en: 'Plot the interest rate as a function of the year-over-year population change
    by running the following code:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将利率作为年度人口变化的函数来绘制，运行以下代码：
- en: '[PRE62]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](img/2cae693f-0b3f-454a-9266-560a1d80be75.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cae693f-0b3f-454a-9266-560a1d80be75.png)'
- en: Here, we use the square root of the population for the radii, making sure to
    also scale down the result to a good size for the visualization.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用人口的平方根来作为半径，同时确保将结果缩小到适合可视化的尺寸。
- en: We see a strong correlation between the year-over-year population change and
    the interest rate. This correlation is especially strong when we take the population
    sizes into account, by looking primarily at the bigger circles. Let's add a line
    of best fit to the plot to illustrate this correlation.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到年度人口变化与利率之间存在很强的相关性。当我们考虑人口规模时，这种相关性尤为明显，主要观察较大的圆圈。让我们添加一条最佳拟合线到图表中来说明这种相关性。
- en: We'll use scikit-learn to create the line of best fit, using the country populations
    (as visualized in the preceding plot) as weights.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit-learn根据国家人口（如在前面的图表中可视化）创建最佳拟合线。
- en: 'Determine the line of best fit for the previously plotted relationship by running
    the following code:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码确定先前绘制的关系的最佳拟合线：
- en: '[PRE63]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The scikit-learn code should be familiar from earlier in this book. As promised,
    we are using the transformed populations, as seen in the previous plot, as the
    weights. The line of best fit is then calculated by predicting the linear model
    values for a range of *x* values.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn代码应该在本书的早些时候就已经很熟悉了。正如承诺的那样，我们使用转换后的人口数据，就像在前面的图表中所见，作为权重。然后通过预测线性模型值的一系列*x*值来计算最佳拟合线。
- en: To plot the line, we can reuse the preceding code, adding an extra call to the
    line module in Bokeh. We'll also have to set a new data source for this line.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制线条，我们可以重复前面的代码，并在Bokeh中的线模块中添加额外的调用。我们还需要为这条线设置一个新的数据源。
- en: 'Re-plot the preceding fiure, adding a line of best fi, by running the following code:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新绘制前面的图表，并通过运行以下代码添加最佳拟合线：
- en: '[PRE64]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![](img/c2c434ec-a839-45e3-874a-dd67617910cc.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2c434ec-a839-45e3-874a-dd67617910cc.png)'
- en: For the line source, `lm_source`, we include N/A as the country name and population,
    as these are not applicable values for the line of best fit. As can be seen by
    hovering over the line, they indeed appear in the tooltip.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线条源`lm_source`，我们将国家名称和人口设置为N/A，因为它们对于最佳拟合线不适用。当悬停在线条上时，可以看到它们确实出现在工具提示中。
- en: The interactive nature of this visualization gives us a unique opportunity to
    explore outliers in this dataset, for example, the tiny dot in the lower-right
    corner.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可交互的可视化方式让我们有机会探索数据集中的离群值，例如右下角的小点。
- en: 'Explore the plot by using the zoom tools and hovering over interesting samples.Note
    the following:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用缩放工具并悬停在有趣的样本上来探索这个图表。请注意以下内容：
- en: 'Ukraine has an unusually high interest rate, given the low year-over-year population
    change:'
  id: totrans-343
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乌克兰的利率异常高，考虑到低的年度人口变化：
- en: '![](img/b0800611-c3f6-4fa6-8d1f-c913c9b6f7eb.png)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/b0800611-c3f6-4fa6-8d1f-c913c9b6f7eb.png)'
- en: 'The small country of Bahrain has an unusually low interest rate, given the high
    year-over-year population change:'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一个小国家，巴林的利率异常低，考虑到高的年度人口变化：
- en: '![](img/7a573f2f-33a1-4e1b-8288-c6995b23a7f7.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a573f2f-33a1-4e1b-8288-c6995b23a7f7.png)'
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we scraped web page tables and then used interactive visualizations
    to study the data.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们抓取了网页表格，并利用交互式可视化来研究数据。
- en: We started by looking at how HTTP requests work, focusing on GET requests and
    their response status codes. Then, we went into the Jupyter Notebook and made
    HTTP requests with Python using the Requests library. We saw how Jupyter can be
    used to render HTML in the notebook, along with actual web pages that can be interacted
    with. After making requests, we saw how Beautiful Soup can be used to parse text
    from the HTML, and used this library to scrape tabular data.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先了解了 HTTP 请求的工作原理，重点关注了 GET 请求及其响应状态码。接着，我们进入了 Jupyter Notebook，使用 Python
    的 Requests 库发起了 HTTP 请求。我们看到，Jupyter 可以在笔记本中渲染 HTML，以及实际可以交互的网页。发起请求后，我们了解了如何使用
    Beautiful Soup 来解析 HTML 中的文本，并利用这个库来抓取表格数据。
- en: After scraping two tables of data, we stored them in pandas DataFrames. The
    fist table contained the central bank interest rates for each country and the
    second table contained the populations. We combined these into a single table
    that was then used to create interactive visualizations.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取了两个数据表后，我们将它们存储在 pandas DataFrame 中。第一个表包含了各国中央银行的利率，第二个表则包含了人口数据。我们将它们合并成一个单独的表格，并用这个表格来创建交互式可视化。
- en: Finally, we used Bokeh to render interactive visualizations in Jupyter. We saw
    how to use the Bokeh API to create various customized plots and made scatter plots
    with specific interactive abilities such as zoom, pan, and hover. In terms of
    customization, we explicitly showed how to set the point radius and color for
    each data sample. Furthermore, when using Bokeh to explore the scraped population
    data, the tooltip was utilized to show country names and associated data when
    hovering over the points.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 Bokeh 在 Jupyter 中渲染了交互式可视化。我们学习了如何使用 Bokeh API 创建各种定制的图表，并制作了带有缩放、平移和悬停等交互功能的散点图。在定制化方面，我们明确展示了如何为每个数据样本设置点的半径和颜色。此外，在使用
    Bokeh 探索抓取的各国人口数据时，悬停在点上时，工具提示展示了国家名称及相关数据。
- en: Congratulations for completing this introductory course on data science using
    Jupyter Notebooks! Regardless of your experience with Jupyter and Python coming
    into the book, you've learned some useful and applicable skills for practical
    data science!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了这个使用 Jupyter Notebooks 的数据科学入门课程！无论你在阅读本书之前对 Jupyter 和 Python 有多少经验，你都学到了一些有用且可以应用于实际数据科学的技能！
