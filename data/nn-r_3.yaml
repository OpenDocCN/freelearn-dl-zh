- en: Deep Learning Using Multilayer Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多层神经网络进行深度学习
- en: Deep learning is the recent hot trend in machine learning/AI. It is all about
    building advanced neural networks. By making multiple hidden layers work in a
    neural network model, we can work with complex nonlinear representations of data.
    We create deep learning using base neural networks. Deep learning has numerous
    use cases in real life, such as, driverless cars, medical diagnostics, computer
    vision, speech recognition, **Natural Language Processing** (**NLP**), handwriting
    recognition, language translation, and many other fields.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习/人工智能领域的最新热门趋势。它的核心在于构建先进的神经网络。通过使多个隐藏层在神经网络模型中工作，我们可以处理复杂的非线性数据表示。我们通过基础神经网络创建深度学习。深度学习在现实生活中有许多应用案例，例如无人驾驶汽车、医学诊断、计算机视觉、语音识别、**自然语言处理**（**NLP**）、手写识别、语言翻译等多个领域。
- en: 'In this chapter, we will deal with the deep learning process: how to train,
    test, and deploy a **Deep Neural Network** (**DNN**). We will look at the different
    packages available in R to handle DNNs. We will understand how to build and train
    a DNN with the `neuralnet` package. Finally, we will analyze an example of training
    and modeling a DNN using h2o, the scalable open-memory learning platform, to create
    models with large datasets and implement prediction with high-precision methods.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论深度学习过程：如何训练、测试和部署**深度神经网络**（**DNN**）。我们将探讨R中可用于处理DNN的不同包。我们将了解如何使用`neuralnet`包构建和训练DNN。最后，我们将分析一个使用h2o平台训练和建模DNN的例子，h2o是一个可扩展的开源内存学习平台，用于在大数据集上创建模型并实施高精度的预测方法。
- en: 'The following are the topics covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Types of DNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNN的类型
- en: R packages for deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的R包
- en: Training and modeling a DNN with `neuralnet`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`neuralnet`训练和建模DNN
- en: The `h2o` library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o`库'
- en: By the end of the chapter, we will understand the basic concepts of deep learning
    and how to implement it in the R environment. We will discover different types
    of DNNs. We will learn how to train, test, and deploy a model. We will know how
    to train and model a DNN using `h2o`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将理解深度学习的基本概念，以及如何在R环境中实现它。我们将了解不同类型的DNN。我们将学习如何训练、测试和部署模型。我们将知道如何使用`h2o`训练和建模DNN。
- en: Introduction of DNNs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）介绍
- en: With the advent of big data processing infrastructure, GPU, and GP-GPU, we are
    now able to overcome the challenges with shallow neural networks, namely overfitting
    and vanishing gradient, using various activation functions and L1/L2 regularization
    techniques. Deep learning can work on large amounts of labeled and unlabeled data
    easily and efficiently.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据处理基础设施、GPU和GP-GPU的出现，我们现在能够克服浅层神经网络的挑战，即过拟合和梯度消失问题，使用各种激活函数和L1/L2正则化技术。深度学习可以轻松高效地处理大量标注和未标注的数据。
- en: 'As mentioned, deep learning is a class of machine learning wherein learning
    happens on multiple levels of neuron networks. The standard diagram depicting
    a DNN is shown in the following figure:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习是机器学习中的一类，其中学习发生在多个神经网络层次上。标准的DNN图示如下所示：
- en: '![](img/00068.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpeg)'
- en: From the analysis of the previous figure, we can notice a remarkable analogy
    with the neural networks we have studied so far. We can then be quiet, unlike
    what it might look like, deep learning is simply an extension of the neural network.
    In this regard, most of what we have seen in the previous chapters is valid. In
    short, a DNN is a multilayer neural network that contains two or more hidden layers.
    Nothing very complicated here. By adding more layers and more neurons per layer,
    we increase the specialization of the model to train data but decrease the performance
    on the test data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图分析中，我们可以注意到与我们迄今为止研究的神经网络之间的显著相似性。然后，我们可以放心，尽管它看起来可能不一样，深度学习实际上只是神经网络的扩展。在这方面，我们在前几章中看到的大部分内容仍然适用。简而言之，DNN是一个包含两个或更多隐藏层的多层神经网络。这里没有什么特别复杂的内容。通过添加更多层和每层更多神经元，我们增加了模型对训练数据的专门化，但却降低了其在测试数据上的性能。
- en: 'As we anticipated, DNN are derivatives of ANN. By making the number of hidden
    layers more than one, we build DNNs. There are many variations of DNNs, as illustrated
    by the different terms shown next:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的，DNN是ANN的衍生物。通过增加隐藏层的数量，我们构建了DNN。DNN有很多变种，以下列举了几个不同的术语：
- en: '**Deep Belief Network** (**DBN**): It is typically a feed-forward network in
    which data flows from one layer to another without looping back. There is at least
    one hidden layer and there can be multiple hidden layers, increasing the complexity.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度信念网络**（**DBN**）：它通常是一个前馈网络，数据从一层流向另一层，而没有回流。它至少有一层隐藏层，并且可以有多个隐藏层，从而增加复杂性。'
- en: '**Restricted Boltzmann Machine** (**RBM**): It has a single hidden layer and
    there is no connection between nodes in a group. It is a simple MLP model of neural
    networks.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制玻尔兹曼机**（**RBM**）：它只有一个隐藏层，且同一组内的节点之间没有连接。它是一个简单的多层感知器（MLP）神经网络模型。'
- en: '**Recurrent Neural Networks** (**RNN**) and **Long Short Term Memory** (**LSTM**):
    These networks have data flowing in any direction within groups and across groups.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）和**长短期记忆网络**（**LSTM**）：这些网络在组内和组间的数据流动是双向的。'
- en: 'As with any machine learning algorithm, even DNNs require building, training,
    and evaluating processes. A basic workflow for deep learning in shown in the following
    figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何机器学习算法一样，DNN（深度神经网络）也需要构建、训练和评估的过程。以下图展示了深度学习的基本工作流程：
- en: '![](img/00069.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: The workflow we have seen in the previous figure remembers very closely that
    typical of a supervised learning algorithm. But what makes it different from other
    machine learning algorithms?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一图中看到的工作流程与典型的监督学习算法非常相似。但它与其他机器学习算法有何不同呢？
- en: Almost all machine learning algorithms demonstrate their limits in identifying
    the characteristics of raw input data, especially when they are complex and lacking
    an apparent order, such as in images. Usually, this limit is exceeded through
    the help of humans, who are concerned with identifying what the machine can not
    do. Deep learning removes this step, relying on the training process to find the
    most useful models through input examples. Also in that case human intervention
    is necessary in order to make choices before starting training, but automatic
    discovery of features makes life much easier. What makes the neural networks particularly
    advantageous, compared to the other solutions offered by machine learning, is
    the great generalization ability of the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的机器学习算法在识别原始输入数据的特征时都会显示出它们的局限性，尤其是在数据复杂且缺乏明显顺序的情况下，比如图像。通常，这种局限性是通过人类的帮助来突破的，人类负责识别机器无法做到的部分。深度学习则去除了这一环节，依赖于训练过程通过输入示例找到最有用的模型。在这种情况下，人类干预仍然是必要的，以便在开始训练之前做出选择，但自动发现特征大大简化了过程。与机器学习提供的其他解决方案相比，神经网络特别具有优势的地方在于其模型的强大泛化能力。
- en: These features have made deep learning very effective for almost all tasks that
    require automatic learning; although it is particularly effective in a case of
    complex hierarchical data. Its underlying ANN forms highly nonlinear representations;
    these are usually composed of multiple layers together with nonlinear transformations
    and custom architectures.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性使得深度学习在几乎所有需要自动学习的任务中都非常有效，尤其是在复杂的层级数据情境下表现尤为突出。它的基础人工神经网络（ANN）形式表现为高度非线性的表示；这些表示通常由多个层以及非线性变换和定制的架构组成。
- en: Essentially, deep learning works really well with messy data from the real world,
    making it a key instrument in several technological fields of the next few years.
    Until recently, it was a dark and daunting area to know, but its success has brought
    many great resources and projects that make it easier than ever to start.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，深度学习在处理来自现实世界的杂乱数据时表现得非常出色，使其成为未来几年多个技术领域的关键工具。直到最近，它仍然是一个让人感到陌生且令人生畏的领域，但它的成功带来了许多极好的资源和项目，使得开始学习它比以往任何时候都更加容易。
- en: Now that we know what the DNNs are, let's see what tools the R development environment
    offers us to deal with this particular topic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了什么是DNN，让我们看看R开发环境为我们处理这一特定主题提供了哪些工具。
- en: R for DNNs
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R for DNNs
- en: In the previous section, we clarified some key concepts that are at the deep
    learning base. We also understood the features that make the use of deep learning
    particularly convenient. Moreover, its rapid diffusion is also due to the great
    availability of a wide range of frameworks and libraries for various programming
    languages.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们澄清了一些深度学习基础的关键概念。我们还了解了深度学习在应用时特别方便的特点。此外，它的快速普及也得益于各种编程语言中框架和库的广泛可用性。
- en: The R programming language is widely used by scientists and programmers, thanks
    to its extreme ease of use. Additionally, there is an extensive collection of
    libraries that allow professional data visualization and analysis with the most
    popular algorithms. The rapid diffusion of deep learning algorithms has led to
    the creation of an ever-increasing number of packages available for deep learning,
    even in R.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: R 编程语言因其极易使用而被科学家和程序员广泛采用。此外，R 还有丰富的库集合，能够进行专业的数据可视化和分析，使用最流行的算法。深度学习算法的快速扩散促使了越来越多的深度学习包的创建，甚至在
    R 中也不例外。
- en: 'The following table shows the various packages/interfaces available for deep
    learning using R:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了使用 R 进行深度学习的各种包/接口：
- en: '| **CRAN package** | **Supported taxonomy of neural network** | **Underlying
    language/** **vendor** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **CRAN 包** | **支持的神经网络分类** | **基础语言/供应商** |'
- en: '| `MXNet` | Feed-forward, CNN | C/C++/CUDA |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `MXNet` | 前馈神经网络，卷积神经网络（CNN） | C/C++/CUDA |'
- en: '| `darch` | RBM, DBN | C/C++ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `darch` | RBM，DBN | C/C++ |'
- en: '| `deepnet` | Feed-forward, RBM, DBN, autoencoders | R |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `deepnet` | 前馈神经网络，RBM，DBN，自编码器 | R |'
- en: '| `h2o` | Feed-forward network, autoencoders | Java |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `h2o` | 前馈神经网络，自编码器 | Java |'
- en: '| `nnet` and `neuralnet` | Feed-forward | R |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `nnet` 和 `neuralnet` | 前馈神经网络 | R |'
- en: '| `Keras` | Variety of DNNs | Python/keras.io |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `Keras` | 多种深度神经网络（DNN） | Python/keras.io |'
- en: '| `TensorFlow` | Variety of DNNs | C++, Python/Google |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `TensorFlow` | 多种深度神经网络（DNN） | C++、Python/Google |'
- en: '`MXNet` is a modern, portable, deep learning library that can support multiple
    machines. The world''s largest companies and universities have adopted `MXNet`
    as a machine learning framework. These include Amazon, Intel, Data, Baidu, Microsoft,
    Wolfram Research, Carnegie Mellon, MIT, University of Washington, and Hong Kong
    University of Science and Technology.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`MXNet` 是一个现代的、可移植的深度学习库，支持多台机器。世界上最大的公司和大学都已将 `MXNet` 作为机器学习框架。这些公司包括亚马逊、英特尔、Data、百度、微软、Wolfram
    Research、卡内基梅隆大学、麻省理工学院、华盛顿大学和香港科技大学。'
- en: '`MXNet` is an open source framework that allows for fast modeling, and supports
    a flexible programming model in multiple programming languages (C ++, Python,
    Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`MXNet` 是一个开源框架，支持快速建模，并在多种编程语言中支持灵活的编程模型（C++、Python、Julia、MATLAB、JavaScript、Go、R、Scala、Perl
    和 Wolfram Language）。'
- en: The `MXNet` framework supports R programming language. The `MXNet` R package
    provides flexible and efficient GPU computing and a state-of-the-art deepening
    at R. It allows us to write a seamless tensorial/ matrix calculation with multiple
    GPUs in R. It also allows us to build and customize the state-of-the-art deep
    learning models in R and apply them to activities such as image classification
    and data science challenges.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`MXNet` 框架支持 R 编程语言。`MXNet` R 包提供灵活高效的 GPU 计算和最先进的 R 深度学习。它允许我们在 R 中使用多个 GPU
    进行无缝的张量/矩阵计算。它还允许我们在 R 中构建和定制最先进的深度学习模型，并将其应用于图像分类、数据科学挑战等活动。'
- en: The `darch` framework is based on the code written by G. E. Hinton and R. R.
    Salakhutdinov, and is available in the MATLAB environment for DBN. This package
    can generate neural networks with many levels (deep architectures) and form them
    with an innovative method developed by the authors. This method provides a pre-formation
    with the contrasting divergence method published by G. Hinton (2002) and fine-tuning
    with common training algorithms known as backpropagation or conjugated gradients.
    In addition, fine-tuning supervision can be improved with maxout and dropout,
    two recently developed techniques to improve fine-tuning for deep learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`darch` 框架基于 G. E. Hinton 和 R. R. Salakhutdinov 编写的代码，并在 MATLAB 环境中实现 DBN。该包能够生成多层神经网络（深度架构），并通过作者开发的创新方法对其进行构建。该方法提供了一种与
    G. Hinton（2002）发布的对比发散法相结合的预形成方法，并通过常见的训练算法如反向传播或共轭梯度法进行微调。此外，微调监督可以通过 maxout
    和 dropout 进行改善，这两种是近年来为提高深度学习微调效果而开发的技术。'
- en: 'The `deepnet` library is a relatively small, yet quite powerful package with
    variety of architectures to pick from. This library implements some deep learning
    architectures and neural network algorithms, including backpropagation, RBM, DBN,
    deep autoencoder, and so on. Unlike the other libraries we have analyzed, it was
    specifically written for R. It has several functions, including:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`deepnet` 库是一个相对较小但功能强大的包，提供多种架构供选择。该库实现了一些深度学习架构和神经网络算法，包括反向传播、RBM、DBN、深度自编码器等。与我们分析的其他库不同，它是专为
    R 编写的。它有几个功能，包括：'
- en: '`nn.train`: For training single or multiple hidden layers neural network by
    BP'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.train`：用于通过 BP 训练单个或多个隐藏层的神经网络'
- en: '`nn.predict`: For predicting new samples by trained neural network'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.predict`：用于通过训练好的神经网络预测新的样本'
- en: '`dbn.dnn.train`: For training a DNN with weights initialized by DBN'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dbn.dnn.train`：用于训练一个由 DBN 初始化权重的 DNN'
- en: '`rbm.train`: For training an RBM'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rbm.train`：用于训练一个 RBM'
- en: The `h2o` R package has functions for building general linear regression, K-means,
    Naive Bayes, **Principal Component Analysis** (**PCA**), forests, and deep learning
    (multilayer `neuralnet` models). `h2o` is an external package to CRAN and is built
    using Java, and is available for a variety of platforms. It is an open source
    math engine for big data that computes parallel distributed machine learning algorithms.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`h2o` R 包具有构建普通线性回归、K-means、朴素贝叶斯、**主成分分析** (**PCA**)、森林和深度学习（多层 `neuralnet`
    模型）等功能。`h2o` 是一个外部包，不属于 CRAN，使用 Java 构建，支持多种平台。它是一个开源的大数据数学引擎，能够并行分布式计算机器学习算法。'
- en: The packages `nnet` and `neuralnet` have been widely discussed in the previous
    chapters. These are two packages for the management of neural networks in R. They
    are also able to build and train multicore neural networks, so they rely on deep
    learning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中我们广泛讨论了 `nnet` 和 `neuralnet` 包。这是两个用于管理 R 中神经网络的包。它们还能够构建和训练多核神经网络，因此依赖于深度学习。
- en: '`Keras` is an open source neural network library written in Python. Designed
    to enable fast experimentation with DNNs, it focuses on being minimal, modular,
    and extensible. The library contains numerous implementations of commonly used
    neural network building blocks, such as layers, objectives, activation functions,
    optimizers, and a host of tools to make working with image and text data easier.
    The code is hosted on GitHub, and community support forums include the GitHub
    issues page, a Gitter channel, and a Slack channel.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`Keras` 是一个用 Python 编写的开源神经网络库。它旨在支持快速实验，专注于最小化、模块化和可扩展性。该库包含许多常用神经网络构建块的实现，如层、目标、激活函数、优化器，以及一系列使得处理图像和文本数据更简单的工具。代码托管在
    GitHub 上，社区支持论坛包括 GitHub 问题页面、Gitter 频道和 Slack 频道。'
- en: '`TensorFlow` is an open source software library for machine learning. It contains
    a system for building and training neural networks to detect and decipher patterns
    and correlations, with methods similar to those adopted by human learning. It
    is used both for search and for Google production.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorFlow` 是一个开源的机器学习软件库。它包含一个用于构建和训练神经网络的系统，用来检测和解码模式与关联，方法类似于人类学习采用的方式。它被用于搜索以及
    Google 的生产环境中。'
- en: Multilayer neural networks with neuralnet
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 neuralnet 训练多层神经网络
- en: After understanding the basics of deep learning, it's time to apply the skills
    acquired to a practical case. We've seen in the previous section that two libraries
    we know are listed in packages available in *R for DNNs* section. I refer to the
    `nnet` and `neuralnet` packages that we learned to use in the previous chapters
    through practical examples. Since we have some practice with the `neuralnet` library,
    I think we should start our practical exploration of the amazing world of deep
    learning from here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了深度学习的基本原理后，是时候将所学技能应用到实际案例中了。我们在上一节中看到，两个我们熟悉的库已经列出在 *R for DNNs* 部分的可用包中。我指的是我们在前几章中通过实际示例学习使用的
    `nnet` 和 `neuralnet` 包。由于我们已经对 `neuralnet` 库有了一些实践经验，我认为我们应该从这里开始，实际探索深度学习的精彩世界。
- en: To start, we introduce the dataset we will use to build and train the network.
    It is named the `College` dataset, and it contains statistics for a large number
    of US colleges, collected from the 1995 issue of *US News and World Report*. This
    dataset was taken from the `StatLib` library, which is maintained at Carnegie
    Mellon University, and was used in the *ASA Section on Statistical Graphics*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍将用于构建和训练网络的数据集。它名为 `College` 数据集，包含大量美国大学的统计数据，数据来自 1995 年 *US News and
    World Report* 的刊物。该数据集来自由卡内基梅隆大学维护的 `StatLib` 库，并曾在 *ASA Section on Statistical
    Graphics* 中使用。
- en: 'Things for us are further simplified because we do not have to retrieve the
    data and then import it into R, as these data are contained in a R package. I
    refer to the `ISLR` package. We just have to install the package and load the
    relative library. But we will see this later, when we explain the codices in detail.
    Now let''s just look at the content of the dataset `College`. It is a dataframe
    with `777` observations on the following `18` variables:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，情况更加简化，因为我们不需要获取数据然后导入到 R 中，因为这些数据已包含在 R 包中。我指的是 `ISLR` 包。我们只需要安装包并加载相应的库。但我们稍后会看到，在详细解释代码时会提到这一点。现在，让我们仅仅查看数据集
    `College` 的内容。它是一个包含 `777` 个观测值的 dataframe，包含以下 `18` 个变量：
- en: '`Private`: A factor with levels `No` and `Yes` indicating private or public
    university'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Private`：一个包含 `No` 和 `Yes` 两个级别的因子，表示私立或公立大学'
- en: '`Apps`: Number of applications received'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Apps`：收到的申请数量'
- en: '`Accept`: Number of applications accepted'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Accept`：被录取的申请数量'
- en: '`Enroll`: Number of new students enrolled'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Enroll`：新入学学生人数'
- en: '`Top10perc`: Percentage of new students from top 10 percent of H.S. class'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Top10perc`：来自高年级前 10%学生的新生比例'
- en: '`Top25perc`: Percentage of new students from top 25 percent of H.S. class'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Top25perc`：来自高年级前 25%学生的新生比例'
- en: '`F.Undergrad`: Number of full time undergraduates'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F.Undergrad`：全日制本科生人数'
- en: '`P.Undergrad`: Number of part time undergraduates'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P.Undergrad`：兼职本科生人数'
- en: '`Outstate`: Out-of-state tuition'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Outstate`：州外学费'
- en: '`Room.Board`: Room and board costs'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Room.Board`：住宿和餐饮费用'
- en: '`Books`: Estimated book costs'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Books`：预计书籍费用'
- en: '`Personal`: Estimated personal spending'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Personal`：预计个人开销'
- en: '`PhD`: Percentage of faculty with Ph.D.s'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PhD`：拥有博士学位的教师比例'
- en: '`Terminal`: Percentage of faculty with terminal degree'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Terminal`：拥有终极学位的教师比例'
- en: '`S.F.Ratio`: Student-faculty ratio'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S.F.Ratio`：师生比'
- en: '`perc.alumni`: Percentage of alumni who donate'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perc.alumni`：捐赠校友的比例'
- en: '`Expend`: Instructional expenditure per student'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Expend`：每个学生的教学支出'
- en: '`Grad.Rate`: Graduation rate'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Grad.Rate`：毕业率'
- en: 'Our aim will be to build a multilayer neural network capable of predicting
    whether the school is public or private, based on the values assumed by the other
    `17` variables:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是建立一个多层神经网络，能够根据其他 `17` 个变量的值预测学校是公立还是私立：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As usual, we will analyze the code line-by-line, by explaining in detail all
    the features applied to capture the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将逐行分析代码，详细解释所有用于捕获结果的功能。
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As usual, the first two lines of the initial code are used to load the libraries
    needed to run the analysis.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，初始代码的前两行用于加载运行分析所需的库。
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，要安装 R 初始分发版中没有的库，必须使用`install.package`函数。这是安装包的主要函数。它接收一个名称向量和一个目标库，从仓库下载包并进行安装。此函数应该仅使用一次，而不是每次运行代码时都使用。
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This command loads the `College` dataset, which as we anticipated is contained
    in the `ISLR` library, and saves it in a given dataframe. Use the `View` function
    to view a compact display of the structure of an arbitrary R object. The following
    figure shows some of the data contained in the `College` dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令加载了 `College` 数据集，正如我们所预期，它包含在 `ISLR` 库中，并将其保存到一个给定的数据框中。使用 `View` 函数查看任意
    R 对象的结构的简洁显示。以下图显示了 `College` 数据集中的部分数据：
- en: '![](img/00070.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpeg)'
- en: 'How it is possible to note for each college are listed a series of statistics;
    the rows represent the observations on the columns instead are present the detected
    features:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个大学都有一系列统计数据，如何表示；行表示观察，列则呈现所检测的特征：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this snippet of code we need to normalize the data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们需要对数据进行归一化。
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在训练神经网络之前进行数据归一化是良好的实践。通过归一化，数据单位被消除，允许你轻松比较来自不同地点的数据。
- en: For this example, we will use the **min-max method** (usually called feature
    **scaling**) to get all the scaled data in the range *[0,1]*. Before applying
    the method chosen for normalization, you must calculate the minimum and maximum
    values of each database column. This procedure has already been adopted in the
    example we analyzed in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用**最小-最大方法**（通常称为特征**缩放**）来将所有数据缩放到*【0,1】*的范围内。在应用所选择的归一化方法之前，必须计算每个数据库列的最小值和最大值。这个过程在我们分析的[第2章](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4)《神经网络中的学习过程》示例中已经采用。
- en: 'The last line scales the data by adopting the expected normalization rule.
    Note that we performed normalization only on the last *17* rows (from *2* to *18*),
    excluding the first column, `Private`, that contains a factor with levels `No`
    and `Yes`, indicating private or public university. This variable will be our
    target in the network we are about to build. To get a confirmation of what we
    say, check the typologies of the variables contained in the dataset. To do this,
    we will use the function `str` to view a compactly display the structure of an
    arbitrary R object:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行通过采用预期的归一化规则来缩放数据。请注意，我们仅对最后*17*行（从*2*到*18*）进行了归一化，排除了第一列`Private`，该列包含有`No`和`Yes`两个层次的因子，表示私立或公立大学。这个变量将是我们即将构建的网络中的目标。为了验证我们所说的内容，可以检查数据集中变量的类型。为此，我们将使用`str`函数，以紧凑的方式显示一个任意R对象的结构：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As anticipated, the first variable is of the `Factor` type, with two `levels`:
    `No` and `Yes`. For the remaining `17` variables, these are of the numeric type.
    As anticipated in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, only numeric data can be
    used in the model, as neural network is a mathematical model with approximation
    functions. So we have a problem with the first variable, `Private`. Do not worry,
    the problem can be easily resolved; just turn it into a numeric variable:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，第一个变量是`Factor`类型，具有两个`levels`：`No`和`Yes`。对于剩余的*17*个变量，它们是数值型的。正如在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)《神经网络与人工智能概念》中预期的那样，只有数值数据可以用于模型，因为神经网络是一个具有逼近函数的数学模型。所以我们遇到了第一个变量`Private`的问题。别担心，这个问题可以很容易地解决；只需将其转换为数值变量即可：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this regard, the first line transforms the `Private` variable into numeric,
    while the second line of code is used to reconstruct the dataset with that variable
    and the remaining *17* appropriately normalized variables. To do this, we use
    the `cbind` function, that takes a sequence of vector, matrix, or dataframe arguments
    and combines by columns or rows, respectively:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，第一行将`Private`变量转换为数值型，而第二行代码则用于用该变量和剩余的*17*个适当归一化的变量重新构建数据集。为此，我们使用`cbind`函数，它接受一系列向量、矩阵或数据框作为参数，并按列或行分别组合：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The time has come to split the data for training and testing of the network.
    In the first line of the code just suggested, the dataset is split into *70:30*,
    with the intention of using *70* percent of the data at our disposal to train
    the network and the remaining *30* percent to test the network. In the third line,
    the data of the dataframe named data is subdivided into two new dataframes, called
    `train_data` and `test_data`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将数据拆分用于网络的训练和测试了。在刚才建议的代码的第一行中，数据集被拆分为*70:30*，目的是将*70*%的数据用于训练网络，剩余的*30*%用于测试网络。在第三行中，名为data的数据框被细分为两个新的数据框，分别称为`train_data`和`test_data`：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this piece of code, we first recover all the variable names using the `names`
    function. This function gets or sets the name of an object. Next, we build the
    formula that we will use to build the network, so we use the `neuralnet` function
    to build and train the network. Everything so far has only been used to prepare
    the data. Now it is time to build the network:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先使用`names`函数恢复所有变量名。该函数用于获取或设置对象的名称。接下来，我们构建将用于构建网络的公式，因此我们使用`neuralnet`函数来构建和训练网络。到目前为止，一切只是用于准备数据。现在是时候构建网络了：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is the key line of the code. Here the network is built and trained; let's
    analyze it in detail. We had anticipated that we would use the `neuralnet` library
    to build our DNN. But what has changed with respect to the cases which we have
    built a single hidden layer network? Everything is played in the `hidden` argument
    setting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码中的关键行。这里，网络被构建并训练完成；让我们详细分析一下。我们曾预期使用`neuralnet`库来构建我们的DNN。但与我们之前构建单隐层网络的情况相比，发生了什么变化呢？一切都发生在`hidden`参数的设置上。
- en: Remember that the `hidden` argument must contain a vector of integers specifying
    the number of hidden neurons (vertices) in each layer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`hidden`参数必须包含一个整数向量，指定每一层中隐藏神经元（顶点）的数量。
- en: In our case, we set the hidden layer to contain the vector *(5,3)*, which corresponds
    to two hidden levels with respective five neurons in the first hidden layer and
    three neurons in the second.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将隐藏层设置为包含向量*(5,3)*，这对应于两个隐藏层，第一个隐藏层有五个神经元，第二个隐藏层有三个神经元。
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The previous line simply plots the network diagram, as shown in the following
    figure:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行简单地绘制了网络图，如下图所示：
- en: '![](img/00071.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.jpeg)'
- en: 'As we can see, the network is built and trained, and we only have to verify
    its ability to predict:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，网络已经构建并训练完成，我们只需要验证其预测能力：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To predict the data reserved for testing, we can use the `compute` method.
    This is a method for objects of class `nn`, typically produced by the `neuralnet`
    function. Computes the outputs of all the neurons for specific arbitrary covariate
    vectors given a trained neural network. It''s crucial to make sure that the order
    of the covariates is the same in the new matrix or dataframe as in the original
    neural network. Subsequently, to visualize, the first lines of the prediction
    result is used the `print` function, shown as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测保留用于测试的数据，我们可以使用`compute`方法。这是`nn`类对象的方法，通常由`neuralnet`函数生成。给定一个经过训练的神经网络，它计算所有神经元在特定自变量向量上的输出。确保新矩阵或数据框中的自变量顺序与原始神经网络中的一致是至关重要的。然后，为了可视化，使用`print`函数展示预测结果的前几行，如下所示：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As can be seen, the forecast results are provided in the form of decimal numbers,
    which approach the values of the two expected classes (one and zero), but do not
    exactly assume these values. We need to assume these values precisely, so we can
    make a comparison with the current values. To do this, we will use the `sapply()`
    function to round these off to either zero or one class, so we can evaluate them
    against the test labels:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，预测结果以小数形式提供，接近预期的两个类别的值（一和零），但并不完全等于这些值。我们需要精确假设这些值，以便与当前值进行比较。为此，我们将使用`sapply()`函数将这些结果四舍五入到零或一类别，从而与测试标签进行评估：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As anticipated, the `sapply()` function has rounded the prediction results
    in the two available classes. Now we have everything we need to make a comparison
    in order to assess the DNN as a prediction tool:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，`sapply()`函数将两个可用类别的预测结果进行了四舍五入。现在我们拥有了进行比较所需的一切，以评估DNN作为预测工具的效果：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To make a comparison, we rely on the confusion matrix. To build it, just use
    the `table` function. Indeed, the `table` function uses the cross-classifying
    factors to construct a contingency table of counts at each combination of factor
    levels.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，我们依赖于混淆矩阵。构建它时，只需使用`table`函数。实际上，`table`函数利用交叉分类因子来构建每个因子水平组合的计数列联表。
- en: The confusion matrix is a specific table layout that allows visualization of
    the performance of an algorithm. Each row represents the instances in an actual
    class, while each column represents the instances in a predicted class. The term
    confusion matrix results from the fact that it makes it easy to see if the system
    is confusing two classes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一种特定的表格布局，用于可视化算法的表现。每一行代表实际类别中的实例，而每一列代表预测类别中的实例。术语“混淆矩阵”源于它能清楚地显示系统是否将两个类别混淆。
- en: 'Let''s see then the results obtained:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看得到的结果：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let us understand the results. Let us first remember that in a confusion matrix,
    the terms on the main diagonal represent the number of correct predictions, that
    is, the number of instances of the predicted class that coincide with the instances
    of the actual class. It seems that in our simulation, things have gone well. In
    fact, we got `49` occurrences of class `0` (`No`) and `167` of class `1` (`Yes`).
    But let's now analyze the other two terms, these represent the mistakes made by
    the model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下这些结果。首先，让我们记住，在混淆矩阵中，主对角线上的术语表示正确预测的数量，即预测类别与实际类别的实例相符的数量。看来在我们的模拟中，一切顺利。事实上，我们得到了`49`个类别`0`（`No`）的结果，和`167`个类别`1`（`Yes`）的结果。但现在让我们分析另外两个术语，它们表示模型犯的错误。
- en: 'As defined in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*, `8` are FN and `9` are FP. In this regard,
    we recall that FN means the number of negative predictions that are positive in
    actual data, while FPs are the number of positive predictions that are negative
    in actual data. We can check this by again using the `table` function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4)中定义的，*神经网络中的学习过程*，`8`是FN，`9`是FP。在这方面，我们回顾一下，FN表示实际数据中为负的预测为正，而FP表示实际数据中为正的预测为负。我们可以通过再次使用`table`函数来检查这一点：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'These represent the actual results, in particular, `57` results belonging to
    class `0` and `176` to class `1`. By summing the data contained in the rows of
    the confusion matrix, we get exactly those values in fact results:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示实际结果，特别是，`57`个结果属于类别`0`，`176`个结果属于类别`1`。通过求和混淆矩阵中行的数据，我们实际上得到了这些值的实际结果：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we again use the `table` function to obtain the occurrences in the predicted
    data:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再次使用`table`函数来获取预测数据中的出现次数：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'These represent the results of the prediction, in particular, `58` results
    belonging to class `0` and `175` to class `1`. By summing the data contained in
    the columns of the confusion matrix, we get exactly those values in fact results:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示预测结果，特别是，`58`个结果属于类别`0`，`175`个结果属于类别`1`。通过求和混淆矩阵中列的数据，我们实际上得到了这些值的结果：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At this point, we calculate the accuracy of the simulation by using the data
    contained in the confusion matrix. Let''s remember that accuracy is defined by
    the following equation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们通过使用混淆矩阵中的数据来计算模拟的准确率。让我们记住，准确率由以下公式定义：
- en: '![](img/00072.gif)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.gif)'
- en: 'Here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '*TP = TRUE POSITIVE*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*TP = 真阳性*'
- en: '*TN = TRUE NEGATIVE*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*TN = 真负性*'
- en: '*FP = FALSE POSITIVE*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*FP = 假阳性*'
- en: '*FN = TRUE NEGATIVE*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*FN = 真负性*'
- en: 'Let us take a look at this in the following code sample:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下代码示例来看一下：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We've got an accuracy of around 93 percent, confirming that our model is able
    to predict data with a good result.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了大约93%的准确率，确认我们的模型能够很好地预测数据并取得良好结果。
- en: Training and modeling a DNN using H2O
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O训练和建模DNN
- en: In this section, we will cover an example of training and modeling a DNN using
    `h2o`. `h2o` is an open source, in-memory, scalable machine learning and AI platform
    used to build models with large datasets and implement predictions with high-accuracy
    methods. The `h2o` library is adapted at a large scale in numerous organizations
    to operationalize data science and provide a platform to build data products.
    `h2o` can run on individual laptops or large clusters of high-performance scalable
    servers. It works very fast, exploiting the machine architecture advancements
    and GPU processing. It has high-accuracy implementations of deep learning, neural
    networks, and other machine learning algorithms.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍使用`h2o`训练和建模DNN的示例。`h2o`是一个开源的内存中可扩展的机器学习和AI平台，用于构建大数据集的模型并实现高准确率的预测方法。`h2o`库在许多组织中被大规模应用，用于实现数据科学并提供构建数据产品的平台。`h2o`可以运行在单个笔记本电脑上，也可以在高性能可扩展服务器的集群上运行。它运行非常快，充分利用了机器架构的进展和GPU处理。它具有高准确度的深度学习、神经网络和其他机器学习算法的实现。
- en: As said earlier, the `h2o` R package has functions for building general linear
    regression, K-means, Naive Bayes, PCA, forests, and deep learning (multilayer
    `neuralnet` models). The `h2o` package is an external package to CRAN and is built
    using Java. It is available for a variety of platforms.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`h2o` R包包含用于构建一般线性回归、K均值、朴素贝叶斯、PCA、森林和深度学习（多层`neuralnet`模型）等功能。`h2o`包是一个外部包，不属于CRAN，使用Java构建。它适用于多种平台。
- en: 'We will install `h2o` in R using the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下代码在R中安装`h2o`：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We obtain the following results:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To test the package, let's go through the following example that uses the popular
    dataset named `Irisdataset`. I'm referring to the Iris flower dataset, a multivariate
    dataset introduced by the British statistician and biologist Ronald Fisher in
    his 1936 paper *The use of multiple measurements in taxonomic problems* as an
    example of linear discriminant analysis.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个包，让我们通过以下示例来使用流行的数据集 `Irisdataset`。我所指的是鸢尾花数据集，这是英国统计学家和生物学家 Ronald Fisher
    在其 1936 年的论文《*多重测量在分类问题中的应用*》中引入的一个多变量数据集，用作线性判别分析的例子。
- en: 'The dataset contains *50* samples from each of the three species of Iris (Iris
    `setosa`, Iris `virginica`, and Iris `versicolor`). Four features were measured
    from each sample: the length and the width of the sepals and petals, in centimeters.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含来自三种鸢尾花（鸢尾花 `setosa`，鸢尾花 `virginica` 和鸢尾花 `versicolor`）的 *50* 个样本。每个样本测量了四个特征：花萼和花瓣的长度和宽度，单位是厘米。
- en: 'The following variables are contained:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 包含以下变量：
- en: '`Sepal.Length` in centimeter'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sepal.Length` 单位：厘米'
- en: '`Sepal.Width` in centimeter'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sepal.Width` 单位：厘米'
- en: '`Petal.Length` in centimeter'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Petal.Length` 单位：厘米'
- en: '`Petal.Width` in centimeter'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Petal.Width` 单位：厘米'
- en: 'Class: `setosa`, `versicolour`, `virginica`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别：`setosa`，`versicolour`，`virginica`
- en: 'The following figure shows a compactly display the structure of the `iris`
    dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下图紧凑地显示了 `iris` 数据集的结构：
- en: '![](img/00073.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.jpeg)'
- en: 'We want to build a classifier that, depending on the size of the sepal and
    petal, is able to classify the flower species:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个分类器，能够根据花萼和花瓣的大小来对花的种类进行分类：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, let's go through the code to understand how to apply the `h2o` package
    to solve a pattern recognition problem.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过代码来了解如何应用 `h2o` 包解决模式识别问题。
- en: Before proceeding, it is necessary to specify that running `h2o` on R requires
    Java 8 runtime. Verify the Java version installed on your machine beforehand and
    eventually download Java version 8 from [https://www.java.com/en/download/win10.jsp](https://www.java.com/en/download/win10.jsp).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，必须说明的是，在 R 中运行 `h2o` 需要 Java 8 运行时。请提前验证您计算机上安装的 Java 版本，并最终从 [https://www.java.com/en/download/win10.jsp](https://www.java.com/en/download/win10.jsp)
    下载 Java 8 版本。
- en: 'The following figure shows the Java download page from Oracle''s site:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了来自 Oracle 网站的 Java 下载页面：
- en: '![](img/00074.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.jpeg)'
- en: 'Furthermore, the `h2o` package is built with some required packages; so in
    order to properly install the `h2o` package, remember to install the following
    dependencies, all of which are available in CRAN:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`h2o` 包是由一些必需的包构建的；因此，为了正确安装 `h2o` 包，请记得安装以下依赖项，所有这些都可以在 CRAN 中找到：
- en: '`RCurl`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RCurl`'
- en: '`bitops`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bitops`'
- en: '`rjson`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rjson`'
- en: '`jsonlite`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jsonlite`'
- en: '`statmod`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statmod`'
- en: '`tools`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tools`'
- en: 'After we have successfully installed the `h2o` package, we can proceed with
    loading the library:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功安装 `h2o` 包后，我们可以继续加载库：
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This command loads the library in R environment. The following messages are
    returned:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在 R 环境中加载该库。返回以下信息：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We follow the directions on the R prompt:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照 R 提示中的说明进行操作：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `h20.init` function initiates the `h2o` engine with a maximum memory size
    of 2 GB and two parallel cores. The console for `h2o` is initialized and we get
    the following messages once we run this script:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`h20.init` 函数初始化 `h2o` 引擎，最大内存大小为 2 GB，并使用两个并行核心。初始化 `h2o` 控制台后，我们运行此脚本时会收到以下信息：'
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once `h2o` is initiated, the console can be viewed from any browser by pointing
    to `localhost:54321`. The `h2o` library runs on a JVM and the console allows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `h2o` 被初始化，控制台可以通过任何浏览器访问，只需指向 `localhost:54321`。`h2o` 库运行在 JVM 上，控制台允许：
- en: '![](img/00075.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.jpeg)'
- en: The console is intuitive and provides an interface to interact with the h[2]o
    engine. We can train and test models and run predictions on top of them. The first
    textbox, labeled CS, allows us to enter routines for execution. The `assist` command
    gives the list of the routines available. Let us continue to analyze the following
    sample code.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台直观易用，并提供与 h[2]o 引擎交互的界面。我们可以训练和测试模型，并基于这些模型进行预测。第一个文本框标记为 CS，允许我们输入要执行的例程。`assist`
    命令会列出可用的例程。让我们继续分析以下示例代码。
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The first command loads the `iris` dataset, which is contained in the datasets
    library, and saves it in a given dataframe. Then we use the `summary` function
    to produce result summaries of the results of the dataset. The function invokes
    particular methods which depend on the class of the first argument. The results
    are shown as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令加载了 `iris` 数据集，该数据集包含在数据集库中，并将其保存在指定的数据框中。然后，我们使用 `summary` 函数来生成数据集结果的摘要。该函数调用依赖于第一个参数的类的特定方法。结果如下所示：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s analyze the next lines of code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析接下来的代码：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `h2o.deeplearning` function is an important function within `h2o` and can
    be used for variety of operations. This function builds a DNN model using CPUs
    builds a feed-forward multilayer ANN on an `H2OFrame`. The `hidden` argument is
    used to set the number of hidden layers and the number of neurons for each hidden
    layer. In our case, we have set up a DNN with two hidden layers and `5` neurons
    for each hidden layer. Finally, the parameter `export_weights_and_biases` tells
    us that the weights and biases can be stored in `H2OFrame` and can be accessed
    like other dataframes for further processing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`h2o.deeplearning` 函数是 `h2o` 中的一个重要函数，可用于多种操作。此函数使用 CPU 构建 DNN 模型，并在 `H2OFrame`
    上构建一个前馈多层人工神经网络（ANN）。`hidden` 参数用于设置隐藏层的数量和每个隐藏层的神经元数量。在我们的例子中，我们设置了一个具有两个隐藏层的
    DNN，每个隐藏层有 `5` 个神经元。最后，参数 `export_weights_and_biases` 告诉我们，权重和偏置可以存储在 `H2OFrame`
    中，并可以像其他数据框一样访问以进行进一步处理。'
- en: 'Before proceeding with the code analysis, a clarification should be made. The
    attentive reader can ask that on the basis of which evaluation we have chosen
    the number of hidden layers and the number of neurons for each hidden layer. Unfortunately,
    there is no precise rule or even a mathematical formula that allows us to determine
    which numbers are appropriate for that specific problem. This is because every
    problem is different from each other and each network approximates a system differently.
    So what makes the difference between one model and another? The answer is obvious
    and once again very clear: the researcher''s experience.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续代码分析之前，需要做一个澄清。细心的读者可能会问，基于什么评估我们选择了每个隐藏层的隐藏层数量和神经元数量。不幸的是，没有一个精确的规则，甚至没有一个数学公式可以帮助我们确定哪些数字适合特定问题。这是因为每个问题都不同，每个网络对系统的近似方式也不同。那么，是什么使得一个模型和另一个模型之间有所不同呢？答案显而易见，而且非常清楚：那就是研究者的经验。
- en: The advice I can give, which stems from the vast experience in data analysis,
    is to try, try, and try again. The secret to experimental activity is just that.
    In the case of neural networks, this results in trying to set up different networks
    and then verifying their performance. For example, in our case, we could have
    started from a network with two hidden layers and *100* neurons per hidden layer,
    then progressively reduced those values, and then arrive at those that I proposed
    in the example. This procedure can be automated with the use of the iterative
    structures that R owns.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我能给出的建议，来源于我在数据分析方面的丰富经验，就是尝试，尝试，再尝试。实验活动的秘密就在于此。在神经网络的情况下，这意味着要尝试搭建不同的网络，然后验证它们的性能。例如，在我们的例子中，我们可以从一个包含两个隐藏层且每个隐藏层有
    *100* 个神经元的网络开始，然后逐渐减少这些值，最终达到我在示例中提出的值。这个过程可以通过 R 所拥有的迭代结构来自动化。
- en: 'However, some things can be said, for example, for the optimum choice of the
    number of neurons we need to know that:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些事情可以说，例如，对于神经元数量的最佳选择，我们需要知道：
- en: Small number of neurons will lead to high error for your system, as the predictive
    factors might be too complex for a small number of neurons to capture
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元数量过少会导致系统误差较大，因为预测因子可能过于复杂，难以为少量神经元所捕捉。
- en: Large number of neurons will overfit to your training data and not generalize
    well
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元数量过多会导致训练数据过拟合，且泛化效果差。
- en: The number of neurons in each hidden layer should be somewhere between the size
    of the input and the output layer, potentially the mean
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的神经元数量应介于输入层和输出层的大小之间，可能是其均值。
- en: The number of neurons in each hidden layer shouldn't exceed twice the number
    of input neurons, as you are probably grossly overfit at this point
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的神经元数量不应超过输入神经元数量的两倍，否则你可能会严重过拟合。
- en: 'That said, we return to the code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们回到代码：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At the R prompt, this command prints a brief description of the features of
    the model we just created, as shown in the following figure:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在R提示符下，此命令打印出我们刚创建的模型的简要描述，如下图所示：
- en: '**![](img/00076.jpeg)**'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/00076.jpeg)**'
- en: 'By carefully analyzing the previous figure, we can clearly distinguish the
    details of the model along with the confusion matrix. Let''s now look at how the
    training process went on:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细分析前面的图，我们可以清晰地区分模型的细节以及混淆矩阵。现在让我们看看训练过程是如何进行的：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `plot` method dispatches on the type of h2o model to select the correct
    scoring history. The arguments are restricted to what is available in the scoring
    history for a particular type of model, as shown in the following figure:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot`方法会根据h2o模型的类型来选择正确的评分历史记录。参数仅限于特定模型类型的评分历史记录中可用的内容，如下图所示：'
- en: '![](img/00077.gif)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00077.gif)'
- en: 'In this plot are shown the training classification errors versus epochs, as
    we can see that the gradient descents and the errors decrease as we progress in
    the epochs. How many times the dataset should be iterated (streamed) can be fractional.
    It defaults to `10`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，展示了训练分类误差与迭代次数（epochs）的关系，我们可以看到梯度下降过程以及误差随着迭代次数的增加而减少。数据集的迭代次数（流式处理）可以是小数，默认值为`10`：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The last six lines of the code simply print a short summary of the weights
    and biases for the three species of iris flower, as is shown next:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后六行简单地打印出三种鸢尾花物种的权重和偏差的简要总结，如下所示：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We have restricted ourselves to seeing weights and biases only for the `setosa`
    species, for space reasons. In the following code we use the plot function again:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 出于空间原因，我们将自己限制只查看`setosa`物种的权重和偏差。在以下代码中，我们再次使用了绘图函数：
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This command plots the weights of the first hidden layer neurons versus sepal
    lengths, as is shown in the following figure:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令绘制了第一隐藏层神经元的权重与花萼长度的关系，如下图所示：
- en: '![](img/00078.gif)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.gif)'
- en: Now, let us dedicate some time to the analysis of the results; in particular,
    we recover the confusion matrix that we just glimpsed in the model summary screen,
    shown earlier. To invoke the confusion matrix, we can use the `h2o.confusionMatrix`
    function as shown in the following code sample, which retrieves either single
    or multiple confusion matrices from the `h2o` objects.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们花点时间分析结果；特别是，我们恢复了刚才在模型摘要屏幕上看到的混淆矩阵。要调用混淆矩阵，我们可以使用`h2o.confusionMatrix`函数，如下代码示例所示，它从`h2o`对象中获取单个或多个混淆矩阵。
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'From the analysis of the confusion matrix, it can be seen that the model manages
    to correctly classify the three floral species by committing only four errors.
    These errors are fairly divided among only two species: `versicolor`, and `virginica`.
    However, the `setosa` species is correctly classified in all `50` occurrences.
    But why is this happening? To understand, let''s take a look at the starting data.
    In the case of multidimensional data, the best way to do this is to plot a scatterplot
    matrix of selected variables in a dataset:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵的分析中可以看出，模型能够正确地分类三种花卉物种，只犯了四个错误。这些错误相当分布在两个物种中：`versicolor`和`virginica`。然而，`setosa`物种在所有`50`次出现中都被正确分类。那么，为什么会这样呢？为了理解这一点，我们来看一下原始数据。在多维数据的情况下，最好的方法是绘制数据集中选定变量的散点图矩阵：
- en: '[PRE36]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The results are shown in the following figure:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下图所示：
- en: '![](img/00079.gif)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00079.gif)'
- en: Let's analyze in detail the plot just proposed. The variables are written in
    a diagonal line from top left to bottom right. Then each variable is plotted against
    each other. For example, the second box in the first column is an individual scatterplot
    of `Sepal.Length` versus `Sepal.Width`, with `Sepal.Length` as the *X* axis and
    `Sepal.Width` as the *Y* axis. This same plot is replicated in the first plot
    of the second column. In essence, the boxes on the upper right hand side of the
    whole scatterplot are mirror images of the plots on the lower left hand.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分析一下刚才提出的图表。变量沿着从左上到右下的对角线排列。然后，每个变量都会与其他变量进行绘制。例如，第一列第二个框是`Sepal.Length`与`Sepal.Width`的单独散点图，`Sepal.Length`作为*X*轴，`Sepal.Width`作为*Y*轴。这个图在第二列的第一个图中得到了复制。从本质上讲，整个散点图右上角的框是左下角图的镜像。
- en: From the analysis of the figure just seen, it can be seen that the `versicolor`
    and `virginica` species show overlapping boundaries. This makes us understand
    that the model's attempt to classify it when it falls into that area can cause
    errors. We can see what happens for the `setosa` species, which instead has far
    distant borders from other floral species without any classification error.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从刚刚看到的图表分析中，可以看出`versicolor`和`virginica`物种的边界重叠。这使我们明白，当模型在这些区域进行分类时，可能会出现错误。我们可以看到`setosa`物种的情况，它的边界与其他花卉物种相距甚远，且没有任何分类错误。
- en: 'That said, we evaluate the accuracy of the model in classifying floral species
    on the basis of the size of petals and sepals:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们根据花瓣和萼片的大小来评估模型在分类花卉物种方面的准确性：
- en: '[PRE37]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The results show that the simulation, based on the first hypothesis, ranked
    the species with *97* percent accuracy. I would say that is a good result; the
    model fit the data very well. But how can we measure this feature? One method
    to find a better fit is to calculate the coefficient of determination (R-squared).
    To calculate R-squared in `h2o`, we can use the `h2o.r2` method:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，基于第一个假设的模拟将物种的分类准确率排到了*97*百分比。我认为这是一个不错的结果；模型拟合数据非常好。但我们如何衡量这个特性呢？找到更好拟合的方法之一是计算决定系数（R-squared）。在`h2o`中计算R-squared的方法是使用`h2o.r2`方法：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now let's understand what we've calculated and how to read the result. R-squared
    measures how well a model can predict the data, and falls between zero and one.
    The higher the value of coefficient of determination, the better the model is
    at predicting the data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们理解一下我们所计算的内容以及如何解读结果。决定系数（R-squared）衡量模型预测数据的能力，其值介于零和一之间。决定系数的值越高，模型预测数据的能力越强。
- en: We got a value of *0.96*, so according to what we have said, this is a great
    result. To get a confirmation of this, we have to compare it with the result of
    another simulation model. So, we build a linear regression model based on the
    same data, that is the `iris` dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了*0.96*的值，所以根据我们之前所说的，这是一个很棒的结果。为了确认这一点，我们需要将其与另一个模拟模型的结果进行比较。因此，我们基于相同的数据构建了一个线性回归模型，即`iris`数据集。
- en: 'To build a linear regression model, we can use the `glm` function. This function
    is used to fit generalized linear models, specified by giving a symbolic description
    of the linear predictor and a description of the error distribution:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建线性回归模型，我们可以使用`glm`函数。该函数用于拟合广义线性模型，通过给出线性预测符号描述和误差分布描述来指定：
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we calculate the model''s coefficient of determination:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算模型的决定系数：
- en: '[PRE40]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now we can make a comparison between the model based on DNNs and the linear
    regression model. DNN had provided a R-squared value of *0.96*, while the resulting
    regression model provided a R-squared value of *0.87*. It is clear that DNN provides
    much better performance.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以比较基于DNN的模型和线性回归模型。DNN提供的R-squared值为*0.96*，而回归模型的R-squared值为*0.87*。显然，DNN提供了更好的性能。
- en: 'Finally, it may be useful to analyze the parameters that are important for
    a neural network specialist, as shown in the following table:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分析神经网络专家认为重要的参数可能会很有用，如下表所示：
- en: '| **Argument** | **Description** |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **Argument** | **Description** |'
- en: '| `x` | A vector containing the names or indices of the predictor variables
    to use in building a model. If `x` is missing, then all columns except `y` are
    used. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `x` | 包含用于构建模型的预测变量名称或索引的向量。如果`x`缺失，则使用除`y`外的所有列。 |'
- en: '| `y` | The name of the response variable in a model. If the data does not
    contain a header, this is the first column index, increasing from left to right
    (the response must be either an integer or a categorical variable). |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `y` | 模型中响应变量的名称。如果数据没有包含表头，则这是第一列的索引，从左到右递增（响应变量必须是整数或类别变量）。 |'
- en: '| `model_id` | This is the destination `id` for a model; it is autogenerated
    if not specified. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `model_id` | 这是模型的目标`id`；如果没有指定，则会自动生成。 |'
- en: '| `standardize` | It is a logical function. If enabled, it automatically standardizes
    the data. If disabled, the user must provide properly scaled input data. It defaults
    to `TRUE`. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| `standardize` | 这是一个逻辑函数。如果启用，它会自动标准化数据。如果禁用，用户必须提供适当缩放的输入数据。默认值为`TRUE`。
    |'
- en: '| `activation` | It is an activation function. It must be one of `Tanh`, `TanhWithDropout`,
    `Rectifier`, `RectifierWithDropout`, `Maxout`, or `MaxoutWithDropout`. It defaults
    to `Rectifier`. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `activation` | 这是一个激活函数。它必须是`Tanh`、`TanhWithDropout`、`Rectifier`、`RectifierWithDropout`、`Maxout`或`MaxoutWithDropout`之一。默认值为`Rectifier`。
    |'
- en: '| `hidden` | This argument specifies hidden layer sizes (for example, `[100,
    100]`). It defaults to `[200, 200]`. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `hidden` | 该参数指定隐藏层的大小（例如，`[100, 100]`）。默认值为`[200, 200]`。 |'
- en: '| `epochs` | How many times the dataset should be iterated (streamed) can be
    fractional. It defaults to `10`. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `epochs` | 数据集应被迭代（流式处理）的次数，可以是小数。默认值为`10`。 |'
- en: '| `adaptive_rate` | It is a logical argument specifying the Adaptive learning
    rate. It defaults to `TRUE`. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| `adaptive_rate` | 这是一个逻辑参数，用于指定自适应学习率。默认值为`TRUE`。 |'
- en: '| `rho` | This describes the adaptive learning rate time decay factor (similar
    to prior updates). It defaults to `0.99`. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `rho` | 这是描述自适应学习率时间衰减因子的参数（类似于先前的更新）。默认值为`0.99`。 |'
- en: '| `rate_annealing` | Learning rate annealing is given by `rate/(1 + rate_annealing
    * samples`). It defaults to `1e- 06`. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `rate_annealing` | 学习率退火由`rate/(1 + rate_annealing * samples)`给出。默认值为`1e-06`。
    |'
- en: '| `rate_decay` | This is the learning rate decay factor between layers (*N^(th)*
    layer: `rate * rate_decay ^ (n - 1`). It defaults to `1`. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `rate_decay` | 这是层之间的学习率衰减因子（*N^(th)*层: `rate * rate_decay ^ (n - 1)`）。默认值为`1`。
    |'
- en: '| `input_dropout_ratio` | Input layer dropout ratio (can improve generalization,
    try `0.1` or `0.2`). It defaults to `0`. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `input_dropout_ratio` | 输入层的 dropout 比率（可以提高泛化能力，尝试`0.1`或`0.2`）。默认值为`0`。
    |'
- en: '| `hidden_dropout_ratios` | Hidden layer dropout ratios can improve generalization.
    Specify one value per hidden layer. It defaults to `0.5`. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `hidden_dropout_ratios` | 隐藏层的 dropout 比率可以提高泛化能力。每个隐藏层指定一个值。默认值为`0.5`。 |'
- en: '| `l1` | L1 regularization can add stability and improve generalization, and
    it causes many weights to become `0`. It defaults to `0`. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `l1` | L1 正则化可以增加稳定性并提高泛化能力，它使得许多权重变为`0`。默认值为`0`。 |'
- en: '| `l2` | L2 regularization can add stability and improve generalization, and
    it causes many weights to be small. It defaults to `0`. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `l2` | L2 正则化可以增加稳定性并提高泛化能力，它使得许多权重变得很小。默认值为`0`。 |'
- en: '| `initial_weights` | This is a list of the `H2OFrame` IDs to initialize the
    weight matrices of this model with. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `initial_weights` | 这是一个`H2OFrame` ID的列表，用于初始化模型的权重矩阵。 |'
- en: '| `initial_biases` | It is a list of the `H2OFrame` IDs to initialize the bias
    vectors of this model with. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `initial_biases` | 这是一个`H2OFrame` ID的列表，用于初始化模型的偏置向量。 |'
- en: '| `loss` | The loss function must be one of `Automatic`, `CrossEntropy`, `Quadratic`,
    `Huber`, `Absolute`, or `Quantile`. It defaults to `Automatic`. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `loss` | 损失函数必须是`Automatic`、`CrossEntropy`、`Quadratic`、`Huber`、`Absolute`或`Quantile`之一。默认值为`Automatic`。
    |'
- en: '| `distribution` | The distribution function must be one of `AUTO`, `bernoulli`,
    `multinomial`, `gaussian`, `poisson`, `gamma`, `tweedie`, `laplace`, `quantile`,
    or `huber`. It defaults to `AUTO`. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `distribution` | 分布函数必须是`AUTO`、`bernoulli`、`multinomial`、`gaussian`、`poisson`、`gamma`、`tweedie`、`laplace`、`quantile`或`huber`之一。默认值为`AUTO`。
    |'
- en: '| `score_training_samples` | It is the number of training set samples for scoring
    (0 for all). It defaults to `10000`. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `score_training_samples` | 它是用于评分的训练集样本数量（0表示所有样本）。默认值为`10000`。 |'
- en: '| `score_validation_samples` | It is the number of validation set samples for
    scoring (0 for all). It defaults to `0`. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `score_validation_samples` | 它是用于评分的验证集样本数量（0表示所有样本）。默认值为`0`。 |'
- en: '| `classification_stop` | The stopping criterion for the classification error
    fraction on training data (-1 to disable). It defaults to `0`. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `classification_stop` | 分类误差比例的停止准则，针对训练数据（`-1`表示禁用）。默认值为`0`。 |'
- en: '| `regression_stop` | It is the stopping criterion for the regression error
    (MSE) on training data (`-1` to disable). It defaults to `1e-06`. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `regression_stop` | 这是回归误差（均方误差）在训练数据上的停止准则（`-1`表示禁用）。默认值为`1e-06`。 |'
- en: '| `stopping_rounds` | Early stopping based on convergence of `stopping_metric`.
    Stop if simple moving average of length `k` of the `stopping_metric` does not
    improve for `k:=stopping_rounds` scoring events (0 to disable) It defaults to
    `5`. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `stopping_rounds` | 基于`stopping_metric`收敛情况的早期停止。如果`stopping_metric`的长度为`k`的简单移动平均值在`k:=stopping_rounds`次评分事件中没有改善，则停止训练（`0`表示禁用）。默认值为`5`。
    |'
- en: '| `max_runtime_secs` | It is maximum allowed runtime in seconds for model training.
    Use `0` to disable it. It defaults to `0`. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `max_runtime_secs` | 这是模型训练的最大允许运行时间，单位为秒。使用`0`可以禁用该设置。默认值为`0`。 |'
- en: '| `diagnostics` | It enables diagnostics for hidden layers. It defaults to
    `TRUE`. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `diagnostics` | 启用隐藏层的诊断。默认值为`TRUE`。 |'
- en: '| `fast_mode` | It enables fast mode (minor approximation in backpropagation).
    It defaults to `TRUE`. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `fast_mode` | 启用快速模式（反向传播时的微小近似）。默认值为`TRUE`。 |'
- en: '| `replicate_training_data` | It replicates the entire training dataset on
    every node for faster training on small datasets. It defaults to `TRUE`. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `replicate_training_data` | 在每个节点上复制整个训练数据集，以加快小数据集上的训练。默认值为`TRUE`。 |'
- en: '| `single_node_mode` | It runs on a single node for fine-tuning of model parameters.
    It defaults to `FALSE`. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `single_node_mode` | 在单个节点上运行，用于微调模型参数。默认值为`FALSE`。 |'
- en: '| `shuffle_training_data` | It enables shuffling of training data (recommended
    if training data is replicated and `train_samples_per_iteration` is close to `#nodes`
    `x` `#rows`, or if using `balance_classes`). It defaults to `FALSE`. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `shuffle_training_data` | 启用训练数据的随机打乱（如果训练数据被复制且`train_samples_per_iteration`接近`#nodes`
    `x` `#rows`，或者如果使用`balance_classes`，建议启用）。默认值为`FALSE`。 |'
- en: '| `missing_values_handling` | Handling of missing values must be one of `MeanImputation`
    or `Skip`. It defaults to `MeanImputation`. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `missing_values_handling` | 处理缺失值时，必须选择`MeanImputation`或`Skip`。默认值为`MeanImputation`。
    |'
- en: '| `quiet_mode` | It enables quiet mode for less output to standard output.
    It defaults to `FALSE`. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `quiet_mode` | 启用安静模式，减少标准输出的内容。默认值为`FALSE`。 |'
- en: '| `verbose` | It prints scoring history to the console (metrics per tree for
    GBM, DRF, and XGBoost; metrics per epoch for deep learning. It defaults to `False`
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `verbose` | 将评分历史打印到控制台（对于GBM、DRF和XGBoost，每棵树的指标；对于深度学习，每个epoch的指标）。默认值为`False`
    |'
- en: '| `autoencoder` | Logical autoencoder defaults to `FALSE` |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| `autoencoder` | 逻辑自编码器，默认值为`FALSE` |'
- en: '| `export_weights_and_biases` | Whether to export neural network weights and
    biases to `H2OFrame`. It defaults to `FALSE`. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `export_weights_and_biases` | 是否将神经网络的权重和偏差导出到`H2OFrame`。默认值为`FALSE`。 |'
- en: '| `mini_batch_size` | Mini-batch size (smaller leads to better fit, whereas
    larger can speed up and generalize better). It defaults to `1`. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `mini_batch_size` | 小批量大小（较小的值有助于更好地拟合，而较大的值可以加速并更好地泛化）。默认值为`1`。 |'
- en: 'There are other functions related to R with `h2o` for deep learning. Some useful
    ones are listed as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些与R和`h2o`相关的深度学习函数。以下列出了一些有用的函数：
- en: '| **Function** | **Description** |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **函数** | **描述** |'
- en: '| `predict.H2Omodel` | Returns an `H2OFrame` object with probabilities and
    default predictions. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `predict.H2Omodel` | 返回一个包含概率和默认预测的`H2OFrame`对象。 |'
- en: '| `h2o.deepwater` | Builds a deep learning model using multiple native GPU
    backends. Builds DNN on `H2OFrame` containing various data sources. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| `h2o.deepwater` | 使用多个本地GPU后端构建深度学习模型。在包含各种数据源的`H2OFrame`上构建DNN。 |'
- en: '| `as.data.frame.H2OFrame` | Converts `H2OFrame` to a dataframe. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| `as.data.frame.H2OFrame` | 将`H2OFrame`转换为数据框。 |'
- en: '| `h2o.confusionMatrix` | Displays the confusion matrix for a classification
    model. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| `h2o.confusionMatrix` | 显示分类模型的混淆矩阵。 |'
- en: '| `print.H2OFrame` | Prints `H2OFrame`. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| `print.H2OFrame` | 打印`H2OFrame`。 |'
- en: '| `h2o.saveModel` | Saves an `h2o` model object to disk. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| `h2o.saveModel` | 将一个`h2o`模型对象保存到磁盘。 |'
- en: '| `h2o.importFile` | Imports a file into h2o. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| `h2o.importFile` | 将文件导入到h2o中。 |'
- en: Deep autoencoders using H2O
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O的深度自编码器
- en: 'Autoencoders are unsupervised learning methods on neural networks. We''ll see
    more of this in [Chapter 7](part0123.html#3L9L60-263fb608a19f4bb5955f37a7741ba5c4),
    *Use Cases of Neural Networks – Advanced Topics*. `h2o` can be used to detect
    an anomaly by using deep autoencoders. To train such a model, the same function,
    `h2o.deeplearning()`, is used, with some changes in the parameters:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是神经网络中的无监督学习方法。在[第7章](part0123.html#3L9L60-263fb608a19f4bb5955f37a7741ba5c4)，*神经网络的应用案例
    – 高级主题*中，我们将进一步了解这些内容。`h2o`可以通过使用深度自编码器来检测异常。要训练这样的模型，使用相同的函数`h2o.deeplearning()`，只需要对参数进行一些更改：
- en: '[PRE41]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `autoencoder=TRUE` sets the `deeplearning` method to use the autoencoder
    technique unsupervised learning method. We are using only the training data, without
    the test set and the labels. The fact that we need a deep `autoencoder` instead
    of a feed-forward network is specified by the `autoencoder` parameter.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`autoencoder=TRUE`将`deeplearning`方法设置为使用自编码器技术的无监督学习方法。我们仅使用训练数据，而没有测试集和标签。需要使用深度`autoencoder`而不是前馈网络的事实是由`autoencoder`参数指定的。'
- en: We can choose the number of hidden units to be present in different layers.
    If we choose an integer value, what we get is called a **naive autoencoder**.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择不同层中要包含的隐藏单元的数量。如果选择一个整数值，得到的模型被称为**朴素自编码器**。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Deep learning is a subject of importance right from image detection to speech
    recognition and AI-related activity. There are numerous products and packages
    in the market for deep learning. Some of these are `Keras`, `TensorFlow`, `h2o`,
    and many others.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个非常重要的学科，涉及从图像识别到语音识别及 AI 相关活动。市场上有许多深度学习的产品和包，其中包括`Keras`、`TensorFlow`、`h2o`以及其他许多。
- en: In this chapter, we learned the basics of deep learning, many variations of
    DNNs, the most important deep learning algorithms, and the basic workflow for
    deep learning. We explored the different packages available in R to handle DNNs.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我们学习了深度学习的基础知识、DNN 的许多变种、最重要的深度学习算法以及深度学习的基本工作流程。我们探索了 R 中处理 DNN 的不同包。
- en: To understand how to build and train a DNN, we analyzed a practical example
    of DNN implementation with the `neuralnet` package. We learned how to normalize
    data across the various available techniques, to remove data units, allowing you
    to easily compare data from different locations. We saw how to split the data
    for the training and testing of the network. We learned to use the `neuralnet`
    function to build and train a multilayered neural network. So we understood how
    to use the trained network to make predictions and we learned to use the confusion
    matrix to evaluate model performance.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解如何构建和训练 DNN，我们分析了一个使用`neuralnet`包实现的 DNN 实践示例。我们学习了如何通过各种现有的技术对数据进行归一化，以去除数据单位，从而便于比较来自不同位置的数据。我们还了解了如何将数据拆分用于网络的训练和测试。我们学会了使用`neuralnet`函数来构建和训练多层神经网络。因此，我们理解了如何使用训练好的网络进行预测，并学会了使用混淆矩阵来评估模型性能。
- en: We saw some basics of the h2o package. Overall, The `h2o` package is a highly
    user-friendly package that can be used to train feed-forward networks or deep
    autoencoders. It supports distributed computations and provides a web interface.
    By including the `h2o` package like any other package in R, we can do all kinds
    of modeling and processing of DNNs. The power of h2o can be utilized by the various
    features available in the package.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了`h2o`包的一些基础知识。总体而言，`h2o`包是一个非常用户友好的包，可以用来训练前馈网络或深度自编码器。它支持分布式计算，并提供了一个
    web 界面。通过像使用其他包一样引入`h2o`包，我们可以进行各种类型的 DNN 模型构建和处理。`h2o`的强大功能可以通过包中提供的多种特性来充分利用。
- en: In the next chapter, we will understand what a perceptron is and what are the
    applications that can be built using the basic perceptron. We will learn a simple
    perceptron implementation function in R environment. We will also learn how to
    train and model a MLP . We will discover the linear separable classifier.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将了解什么是感知机，以及可以使用基本感知机构建的应用程序。我们将学习在 R 环境中实现简单感知机的函数。我们还将学习如何训练和建模一个 MLP。我们将探索线性可分的分类器。
