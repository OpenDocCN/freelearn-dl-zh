- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Analyzing Text Data with Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习分析文本数据
- en: Language is one of the most amazing abilities of human beings; it evolves during
    the individual’s lifetime and is capable of conveying a message with complex meaning.
    Language in its natural form is not understandable to machines, and it is extremely
    challenging to develop an algorithm that can pick up the different nuances. Therefore,
    in this chapter, we will discuss how to represent text in a form that is digestible
    by machines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是人类最惊人的能力之一；它在个体的生命周期中不断发展，能够传达具有复杂意义的消息。自然形式下的语言对机器来说是不可理解的，开发一个能够捕捉不同细微差异的算法极具挑战性。因此，在本章中，我们将讨论如何将文本表示成机器可以消化和理解的形式。
- en: In natural form, text cannot be directly fed to a **deep learning** model. In
    this chapter, we will discuss how text can be represented in a form that can be
    used by **machine learning** models. Starting with natural text, we will transform
    the text into numerical vectors that are increasingly sophisticated (one-hot encoding,
    **bag of words** (**BoW**), **term frequency-inverse document frequency** (**TF-IDF**))
    until we create vectors of real numbers that represent the meaning of a word (or
    document) and allow us to conduct operations (word2vec). In this chapter, we introduce
    deep learning models, such as **recurrent neural networks** (**RNNs**), l**ong
    short-term memory** (**LSTM**), **gated recurrent units** (**GRUs**), and **convolutional
    neural network** (**CNNs**), to analyze sequences and discuss their strengths
    as well as the problems associated with them. Finally, we will assemble these
    models all together to conduct text classification, showing the power of the learned
    approaches.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然形式下，文本不能直接输入到**深度学习**模型中。在本章中，我们将讨论如何将文本表示成可以被**机器学习**模型使用的形式。从自然文本开始，我们将文本转换成越来越复杂的数值向量（one-hot编码、**词袋模型**（BoW）、**词频-逆文档频率**（TF-IDF）），直到我们创建出代表一个词（或文档）意义的实数向量，并允许我们进行操作（word2vec）。在本章中，我们介绍了深度学习模型，如**循环神经网络**（RNNs）、**长短期记忆**（LSTM）、**门控循环单元**（GRUs）和**卷积神经网络**（CNNs），用于分析序列，并讨论它们的优点以及与之相关的问题。最后，我们将这些模型组合在一起进行文本分类，展示学习方法的强大之处。
- en: By the end of this chapter, we will be able to take a corpus of text and use
    deep learning to analyze it. These are the bases that will help us understand
    how a **large language model** (**LLM**) (such as ChatGPT) works internally.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将能够使用文本语料库并利用深度学习进行分析。这些基础将帮助我们理解**大型语言模型**（LLM）（如ChatGPT）的内部工作原理。
- en: 'In this chapter, we''ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Representing text for AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为AI表示文本
- en: Embedding, application, and representation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入、应用和表示
- en: RNNs, LSTMs, GRUs, and CNNs for text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于文本的RNNs、LSTMs、GRUs和CNNs
- en: Performing sentiment analysis with embedding and deep learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入和深度学习进行情感分析
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use standard libraries for Python. The necessary libraries
    can be found within each of the Jupyter notebooks that are in the GitHub repository
    for this chapter: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1).
    The code can be executed on a CPU, but a GPU is advised.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python的标准库。必要的库可以在本章节GitHub仓库中的每个Jupyter笔记本中找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1)。代码可以在CPU上执行，但建议使用GPU。
- en: Representing text for AI
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为AI表示文本
- en: Compared to other types of data (such as images or tables), it is much more
    challenging to represent text in a digestible representation for computers, especially
    because there is no unique relationship between the meaning of a word (signified)
    and the symbol that represents it (signifier). In fact, the meaning of a word
    changes from the context and the author’s intentions in using it in a sentence.
    In addition, native text has to be transformed into a numerical representation
    to be ingested by an algorithm, which is not a trivial task. Nevertheless, several
    approaches were initially developed to be able to find a vector representation
    of a text. These vector representations have the advantage that they can then
    be used as input to a computer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的数据（如图像或表格）相比，将文本以可消化的形式表示给计算机是一个更大的挑战，尤其是由于一个词的意义（所指）与其所代表的符号（能指）之间没有唯一的关系。实际上，一个词的意义会随着上下文和作者在句子中使用它的意图而变化。此外，原生文本必须转换成数值表示，以便算法可以处理，这并非易事。尽管如此，最初开发了几种方法来找到文本的向量表示。这些向量表示的优点在于它们可以被用作计算机的输入。
- en: First, a collection of texts (**corpus**) should be divided into fundamental
    units (words). This process requires making certain decisions and process operations
    that collectively are called **text normalization**. A sentence, therefore, is
    divided into words by exploiting the natural division of spaces (**text segmentation**);
    each punctuation mark is also considered a single word. In fact, punctuation marks
    are considered to be the boundaries of sentences and convey important information
    (change of topic, questions, exclamations).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，应该将文本集合（**语料库**）划分为基本单元（单词）。这个过程需要做出某些决定和操作，这些操作统称为**文本归一化**。因此，一个句子通过利用空格的自然分隔（**文本分割**）被划分为单词；每个标点符号也被视为一个单独的单词。实际上，标点符号被认为是句子的边界，并传达重要的信息（主题变化、疑问、感叹）。
- en: 'The second step is the definition of what a word is and whether some terms
    in the corpus should be directly joined under the same vocabulary instance. For
    example, “He” and “he” represent the same instance; the former is only capitalized.
    Since an algorithm does not include such nuances, one must normalize the text
    in lowercase. In some cases, we want to conduct more sophisticated normalizations
    such as **lemmatization** (joining words with the same root: “came” and “comes”
    are two forms of the verb) or **stemming** (stripping all suffixes of words).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是定义一个词是什么，以及语料库中的一些术语是否应该直接归入同一个词汇实例。例如，“He”和“he”代表同一个实例；前者仅大写。由于算法不包括这样的细微差别，因此必须将文本规范化为小写。在某些情况下，我们希望进行更复杂的规范化，例如**词形还原**（将具有相同词根的单词连接起来：“came”和“comes”是动词的两种形式）或**词干提取**（去除单词的所有后缀）。
- en: '**Tokenization** is the task of transforming a text into fundamental units.
    This is because, in addition to words, a text may also include percentages, numbers,
    websites, and other components. We will return to this later, but in the meantime,
    we will look at some simpler forms of tokenization.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**是将文本转换为基本单元的任务。这是因为，除了单词之外，文本还可能包括百分比、数字、网站和其他组件。我们将在稍后回到这一点，但在此同时，我们将看看一些更简单的分词形式。'
- en: One-hot encoding
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'In traditional **natural language processing** (**NLP**), text representation
    is conducted using discrete symbols. The simplest example is one-hot encoding.
    From a sequence of text in a corpus (consisting of *n* different words), we obtain
    an *n*-dimensional vector. In fact, the first step is to compute the set of different
    words present in the whole text corpus called vocabulary. For each word, we obtain
    a vector as long as the size of the vocabulary. Then for each word, we will have
    a long vector composed mainly of zeros and ones to represent the word (one-hot
    vectors). This system is mainly used when we want a matrix of features and then
    train a model. This process is also called **vectorization**; here''s a sparse
    vector for the following two words:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的**自然语言处理**（**NLP**）中，文本表示使用离散符号进行。最简单的例子是独热编码。从一个语料库中的文本序列（由*n*个不同的单词组成），我们得到一个*n*维向量。实际上，第一步是计算整个文本语料库中存在的不同单词集合，称为词汇表。对于每个单词，我们得到一个与词汇表大小相同的向量。然后对于每个单词，我们将有一个主要由零和一组成的长向量来表示该单词（独热向量）。这个系统主要用于当我们想要一个特征矩阵并训练一个模型时。这个过程也称为**向量化**；以下是一个稀疏向量的例子：
- en: <mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center
    center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto"
    rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>r</mi><mi>e</mi><mi}s</mi><mi>t</mi><mi>a</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center
    center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto"
    rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow>
- en: <mrow><mrow><mi>p</mi><mi>i</mi><mi>z</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>a</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center
    center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto"
    rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>p</mi><mi>i</mi><mi>z</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>a</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center
    center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto"
    rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow>
- en: There are different problems associated with this representation. First, it
    captures only the presence (or the absence) of a word in a document. Thus, we
    are losing all the semantic relationships between the words. Second, an average
    language has about 200,000 words, so for each word, we would have a vector of
    length 200,000\. This leads to very sparse and high-dimensional vectors. For large
    corpora, we need high memory to store the vectors and high computational capacity
    to handle them. In addition, there is no notion of similarity. The two words in
    the preceding example are two places that sell food, and we would like the vectors
    representing these words to encode this similarity. If the vectors had a notion
    of similarity, we could conduct clustering, and the synonyms would be in the same
    cluster.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种表示相关的问题有很多。首先，它只捕捉到文档中单词的存在（或不存在）。因此，我们失去了单词之间的所有语义关系。其次，平均语言大约有200,000个单词，所以对于每个单词，我们会有一个长度为200,000的向量。这导致向量非常稀疏且维度很高。对于大型语料库，我们需要大量的内存来存储这些向量，以及高计算能力来处理它们。此外，没有相似性的概念。前一个例子中的两个单词是两个卖食物的地方，我们希望代表这些单词的向量能够编码这种相似性。如果向量有相似性的概念，我们就可以进行聚类，同义词将位于同一个簇中。
- en: 'In order to obtain such a matrix, we must do the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这样的矩阵，我们必须执行以下操作：
- en: Standardize the text before tokenization. In this case, we simply transform
    everything into lowercase.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分词之前标准化文本。在这种情况下，我们只是将所有内容转换为小写。
- en: We construct a vocabulary constituted of unique words and save the vocabulary
    so that in a case from a vector, we can get the corresponding word.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们构建一个由唯一单词组成的词汇表，并将其保存下来，以便在向量中可以获取相应的单词。
- en: We create an array and then populate it with `1` at the index of the word in
    the vocabulary; `0`s elsewhere.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个数组，然后在其词汇表中单词的索引位置填充 `1`，其他位置填充 `0`。
- en: 'Let’s take a look at how this works in code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码是如何实现这一点的：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at a specific example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We get the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/B21257_01_Figure_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21257_01_Figure_01.jpg)'
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Observe how choosing another sentence will result in a different matrix and
    how, by increasing the length of the sentence, the matrix grows proportionally
    to the number of different words. Also, note that for repeated words, we get equal
    vectors. Check the preceding output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 观察选择另一个句子将如何导致不同的矩阵，以及通过增加句子的长度，矩阵如何成比例地增长到不同单词的数量。此外，请注意，对于重复的单词，我们得到相同的向量。检查前面的输出。
- en: Even if it is a simple method, we have obtained a first representation of text
    in a vectorial form.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这是一个简单的方法，我们也得到了文本向量化形式的第一种表示。
- en: Bag-of-words
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: In the previous section, we discussed one-hot encoding and some of the problems
    associated with this form of text representation. In the previous example, we
    worked with a single sentence, but a corpus is made up of thousands if not millions
    of documents; each of these documents contains several words with a different
    frequency. We want a system that preserves this frequency information, as it is
    important for the classification of text. In fact, documents that have similar
    content are similar, and their meaning will also be similar.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了一热编码以及与这种文本表示形式相关的一些问题。在之前的例子中，我们处理了一个句子，但语料库由成千上万甚至数百万个文档组成；每个文档都包含不同频率的几个单词。我们希望有一个系统可以保留这些频率信息，因为它对于文本的分类很重要。事实上，内容相似的文档是相似的，它们的含义也将是相似的。
- en: '**BoW** is an algorithm for extracting features from text that preserves this
    frequency property. BoW is a very simple algorithm that ignores the position of
    words in the text and only considers this frequency property. The name “bag” comes
    precisely from the fact that any information concerning sentence order and structure
    is not preserved by the algorithm. For BoW, we only need a vocabulary and a way
    to be able to count words. In this case, the idea is to create document vectors:
    a single vector represents a document and the frequency of words contained in
    the vocabulary. *Figure 1**.1* visualizes this concept with a few lines from *Hamlet*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**BoW** 是一种从文本中提取特征并保留此频率属性的算法。BoW 是一个非常简单的算法，它忽略了文本中单词的位置，只考虑这种频率属性。名称“bag”正是由于算法不保留关于句子顺序和结构的信息。对于
    BoW，我们只需要一个词汇表和一种能够计数单词的方法。在这种情况下，我们的想法是创建文档向量：一个向量代表一个文档，以及词汇表中包含的单词的频率。*图 1**.1*
    使用《哈姆雷特》的一些行来可视化这个概念：'
- en: '![Figure 1.1 – Representation of the BoW algorithm](img/B21257_01_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 1.1 – BoW 算法的表示](img/B21257_01_01.jpg)'
- en: Figure 1.1 – Representation of the BoW algorithm
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – BoW 算法的表示
- en: Even this representation is not without problems. Again, as the vocabulary grows,
    so will the size of the vectors (the size of each vector is equal to the length
    of the vocabulary). In addition, these vectors tend to be scattered, especially
    when the documents are very different from each other. High-dimensional or sparse
    vectors are not only problematic for memory and computational costs but for algorithms
    as well (the longer the vectors, the more weight you need in the algorithm, leading
    to a risk of overfitting). This is called the **curse of dimensionality**; the
    greater the number of features, the less meaningful the distances between examples.
    For large corpora, some solutions have been proposed, such as ignoring punctuation,
    correcting misspelled words, stemming algorithms, or ignoring words with high
    frequency that don’t add information (articles, prepositions, and so on).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这种表示法也存在问题。同样，随着词汇量的增加，向量的尺寸（每个向量的尺寸等于词汇表的长度）也会增加。此外，这些向量往往分布得很散，尤其是当文档彼此非常不同时。高维或稀疏向量不仅对内存和计算成本有影响，而且对算法也有影响（向量越长，算法中需要的权重就越多，从而导致过拟合的风险）。这被称为**维度诅咒**；特征的数量越多，示例之间的距离就越没有意义。对于大型语料库，已经提出了一些解决方案，例如忽略标点符号、纠正拼写错误、词干算法，或者忽略那些不增加信息的高频词（如冠词、介词等）。
- en: 'In order to get a BoW matrix for a list of documents, we need to do the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为文档列表获取一个词袋矩阵，我们需要做以下几步：
- en: Tokenize each document to get a list of words.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个文档分词，以获得一个单词列表。
- en: Create our vocabulary of unique words and map each word to the corresponding
    index in the vocabulary.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建我们独特的词汇表，并将每个词映射到词汇表中的相应索引。
- en: Create a matrix where each row represents a document and each column, instead,
    a word in the vocabulary (the documents are the examples, and the words are the
    associated features).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个矩阵，其中每一行代表一个文档，而每一列则代表词汇表中的一个词（文档是例子，而单词是相关特征）。
- en: 'Let’s look at the code again:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次查看代码：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s an example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This prints the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '![](img/B21257_01_Figure_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21257_01_Figure_02.jpg)'
- en: Important note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: Note how in the example, the word “`awesome`” is associated with a review with
    a positive, neutral, or negative meaning. Without context, the frequency of the
    word “awesome” alone does not tell us the sentiment of the review.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在例子中，单词“`awesome`”与具有积极、中性或负面意义的评论相关联。没有上下文，单词“awesome”的频率本身并不能告诉我们评论的情感。
- en: Here, we have learned how to transform text in a vectorial form while keeping
    the notion of frequency for each word.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经学会了如何将文本转换为向量形式，同时保持每个单词的频率概念。
- en: TF-IDF
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: 'In the previous section, we obtained a document-term matrix. However, the raw
    frequency is very skewed and does not always allow us to discriminate between
    two documents. The document-term matrix was born in information retrieval to find
    documents, though words such as “good” or “bad” are not very discriminative since
    they are often used in text with a generic meaning. In contrast, words with low
    frequency are much more informative, so we are interested more in relative than
    absolute frequency:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们获得了一个文档-词矩阵。然而，原始频率分布非常不均匀，并不总是能让我们区分两个文档。文档-词矩阵起源于信息检索，用于查找文档，尽管像“好”或“坏”这样的词并不具有很强的区分性，因为它们通常在具有通用意义的文本中使用。相比之下，低频词更有信息量，所以我们更感兴趣的是相对频率而不是绝对频率：
- en: '![Figure 1.2 – Intuition of the components of TF-IDF](img/B21257_01_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – TF-IDF 各组成部分的直观理解](img/B21257_01_02.jpg)'
- en: Figure 1.2 – Intuition of the components of TF-IDF
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – TF-IDF 各组成部分的直观理解
- en: Instead of using raw frequency, we can use the logarithm in base 10, because
    a word that occurs 100 times in a document is not 100 times more relevant to its
    meaning in the document. Of course, since vectors can be very sparse, we assign
    0 if the frequency is 0\. Second, we want to pay more attention to words that
    are present only in some documents. These words will be more relevant to the meaning
    of the document, and we want to preserve this information. To do this, we normalize
    by IDF. IDF is defined as the ratio of the total number of documents in the corpus
    to how many documents a term is present in. To summarize, to obtain the TF-IDF,
    we multiply TF by the logarithm of IDF.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以10为底的对数代替原始频率，因为一个在文档中出现100次的单词并不比它在文档中的意义相关性强100倍。当然，由于向量可以非常稀疏，如果频率为0，我们则分配0。其次，我们希望更多地关注仅存在于某些文档中的单词。这些单词将更相关于文档的意义，我们希望保留这些信息。为此，我们通过IDF进行规范化。IDF定义为语料库中总文档数与一个术语出现在多少文档中的比率。总结来说，为了获得TF-IDF，我们将TF乘以IDF的对数。
- en: 'This is demonstrated in the following code block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下代码块中得到了演示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This generates the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下输出：
- en: '![](img/B21257_01_Figure_03.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21257_01_Figure_03.jpg)'
- en: Important note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In this example, we used the same corpus as in the previous section. Note how
    word frequencies changed after this normalization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用了与上一节相同的语料库。注意在规范化之后单词频率是如何变化的。
- en: In this section, we learned how we can normalize text to decrease the impact
    of the most frequent words and give relevance to words that are specific to a
    subset of documents. Next, we’ll discuss embedding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何规范化文本以减少最频繁单词的影响，并赋予特定于文档子集的单词相关性。接下来，我们将讨论嵌入。
- en: Embedding, application, and representation
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入、应用和表示
- en: 'In the previous section, we discussed how to use vectors to represent text.
    These vectors are digestible for a computer, but they still suffer from some problems
    (sparsity, high dimensionality, etc.). According to the distributional hypothesis,
    words with a similar meaning frequently appear close together (or words that appear
    often in the same context have the same meaning). Similarly, a word can have a
    different meaning depending on its context: “I went to deposit money in the *bank*”
    or “We went to do a picnic on the river *bank*.” In the following diagram, we
    have a high-level representation of the embedding process. So, we want a process
    that allows us to start from text to obtain an array of vectors, where each vector
    corresponds to the representation of a word. In this case, we want a model that
    will then allow us to map each word to a vector representation. In the next section,
    we will describe the process in detail and discuss the theory behind it.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何使用向量来表示文本。这些向量对计算机来说是可消化的，但它们仍然存在一些问题（稀疏性、高维性等）。根据分布假设，具有相似意义的单词经常出现在一起（或者经常出现在相同上下文中的单词具有相同的意义）。同样，一个单词可以根据其上下文有不同的意义：“我去银行存钱”或“我们去河边野餐。”在下面的图中，我们有嵌入过程的高级表示。因此，我们希望有一个过程，使我们能够从文本开始，获得一个向量数组，其中每个向量对应于一个单词的表示。在这种情况下，我们希望有一个模型，然后允许我们将每个单词映射到向量表示。在下一节中，我们将详细描述这个过程，并讨论其背后的理论。
- en: '![Figure 1.3 – High-level representation of the embedding process](img/B21257_01_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 嵌入过程的概述](img/B21257_01_03.jpg)'
- en: Figure 1.3 – High-level representation of the embedding process
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 嵌入过程的高级表示
- en: We would, therefore, like to generate vectors that are small in size, composed
    of real (dense) numbers, and that preserve this contextual information. Thus,
    the purpose is to have vectors of limited size that can represent the meaning
    of a word. The scattered vectors we obtained earlier cannot be used efficiently
    for mathematical operations or downstream tasks. Also, the more words there are
    in the vocabulary, the larger the size of the vectors we get. Therefore, we want
    dense vectors (with real numbers) that are small in size and whose size does not
    increase as the number of words in the vocabulary increases.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望生成尺寸小、由实数（密集）数字组成，并且保留这种上下文信息的向量。因此，目的是拥有有限大小的向量，可以表示一个单词的意义。我们之前获得的散点向量不能有效地用于数学运算或下游任务。此外，词汇表中的单词越多，我们得到的向量尺寸就越大。因此，我们希望得到的是尺寸小、由实数组成的密集向量，并且其尺寸不会随着词汇表中单词数量的增加而增加。
- en: In addition, these vectors have a distributed representation of the meaning
    of the word (whereas in sparse vectors, it was local or where the `1` was located).
    As we will see a little later, these dense vectors can be used for different operations
    because they better represent the concept of similarity between words. These dense
    vectors are called **word embeddings**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些向量具有词语意义的分布式表示（而在稀疏向量中，它是局部的或`1`所在的位置）。正如我们稍后将看到的，这些密集向量可以用于不同的操作，因为它们更好地代表了词语之间相似性的概念。这些密集向量被称为**词嵌入**。
- en: This concept was introduced in 2013 by Mikolov with a framework called **word2vec**,
    which will be described in detail in the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念是在2013年由Mikolov提出的，名为**word2vec**的框架，将在下一节中详细介绍。
- en: Word2vec
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2vec
- en: 'The intuition behind word2vec is simple: predict a word *w* from its context.
    To do this, we need a **neural network** and a large corpus. The revolutionary
    idea is that by training this neural network to predict which words *c* are needed
    near the target word *w*, the weights of the neural network will be the embedding
    vectors. This model is self-supervised; the labels in this case are implicit,
    and we do not provide them.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec背后的直觉很简单：从其上下文中预测一个词 *w*。为此，我们需要一个**神经网络**和大量语料库。革命性的想法是，通过训练这个神经网络来预测哪些词
    *c* 需要靠近目标词 *w*，神经网络的权重将是嵌入向量。这个模型是自监督的；在这种情况下，标签是隐含的，我们不提供它们。
- en: 'Word2vec simplifies this idea by making the system extremely fast and effective
    in two ways: by turning the task into binary classification (Is the word *c* needed
    in the context of the word *w*? Yes or no?) and using a logistic regression classifier:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec通过两种方式简化了这个想法，使其系统在速度和效率上都非常快：通过将任务转化为二元分类（词 *c* 是否在词 *w* 的上下文中需要？是或否？）并使用逻辑回归分类器：
- en: '![Figure 1.4 – In word2vec, we slide a context window (here represented as
    a  three-word context window), and then we randomly sample some negative words](img/B21257_01_04.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 在word2vec中，我们滑动一个上下文窗口（这里表示为三个词的上下文窗口），然后我们随机采样一些负向词](img/B21257_01_04.jpg)'
- en: Figure 1.4 – In word2vec, we slide a context window (here represented as a three-word
    context window), and then we randomly sample some negative words
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 在word2vec中，我们滑动一个上下文窗口（这里表示为三个词的上下文窗口），然后我们随机采样一些负向词
- en: Given a text *t*, we scroll a window *c* (our context) for a word *w* in the
    center of our window; the words around it are examples of the positive class.
    After that, we select other random words as negative examples. Finally, we train
    a model to classify the positive and negative examples; the weights of the model
    are our embeddings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本 *t*，我们在窗口 *c*（我们的上下文）中滚动一个以词 *w* 为中心的窗口；它周围的词是正类示例。之后，我们选择其他随机词作为负例。最后，我们训练一个模型来分类正例和负例；模型的权重是我们的嵌入。
- en: 'Given a word *w* and a word *c*, we want the probability that the word *c*
    is in the context of *w* to be similar to its embedding similarity. In other words,
    if the vector representing *w* and *c* are similar, *c* must often be in the context
    of *w* (word2vec is based on the notion of context similarity). We define this
    embedding similarity by the dot product between the two embedding vectors for
    *w* and *c* (we use the sigmoid function to transform this dot product into a
    probability and thus allow comparison). So, the probability that *c* is in the
    context of *w* is equal to the probability that their embeddings are similar:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个词 *w* 和一个词 *c*，我们希望词 *c* 在 *w* 的上下文中的概率与它的嵌入相似度相似。换句话说，如果代表 *w* 和 *c* 的向量相似，*c*
    必须经常出现在 *w* 的上下文中（word2vec基于上下文相似性的概念）。我们通过两个嵌入向量之间的点积来定义这种嵌入相似度（我们使用sigmoid函数将这个点积转换为概率，从而允许比较）。因此，*c*
    在 *w* 的上下文中的概率等于它们的嵌入相似的概率：
- en: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mi>σ</mi><mfenced
    close=")" open="("><mrow><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mi>σ</mi><mfenced
    close=")" open="("><mrow><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow>
- en: 'This is done for all words in context *L*. To simplify, we assume that all
    words in the context window are independent, so we can multiply the probabilities
    of the various words *c*. Similarly, we want to ensure that this dot product is
    minimal for words that are not in the context of word *w*. So, on the one hand,
    we maximize the probability for words in the context, and on the other hand, we
    minimize the probability for words that are not in the context. In fact, words
    that are not in the context of *w* are randomly extracted during training, and
    the process is the same:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对上下文中的所有单词 *L* 进行的操作。为了简化，我们假设上下文窗口中的所有单词都是独立的，因此我们可以将各种单词 *c* 的概率相乘。同样，我们希望确保不在单词
    *w* 的上下文中的单词的点积是最小的。因此，一方面，我们最大化上下文中的单词的概率，另一方面，我们最小化不在上下文中的单词的概率。实际上，不在 *w* 的上下文中的单词在训练过程中是随机提取的，过程是相同的：
- en: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi
    mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi
    mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow>
- en: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow>
- en: 'For simplicity, we take the logarithm of probability:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们取概率的对数：
- en: <mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi
    mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi
    mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow>
- en: <mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow>
- en: 'The matrix of weights *w* is our embedding; it is what we will use from now
    on. Actually, the model learns two matrices of vectors (one for *w* and one for
    *c*), but the two matrices are very similar, so we take just one. We then use
    cross-entropy to train the models and learn the weights for each vector:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵 *w* 是我们的嵌入；这是我们接下来将要使用的。实际上，模型学习两个向量矩阵（一个用于 *w*，一个用于 *c*），但这两个矩阵非常相似，所以我们只取一个。然后我们使用交叉熵来训练模型并学习每个向量的权重：
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
- en: 'This is represented visually in the following diagram:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图表中得到了直观的表示：
- en: '![Figure 1.5 – Word and context embedding](img/B21257_01_05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 单词和上下文嵌入](img/B21257_01_05.jpg)'
- en: Figure 1.5 – Word and context embedding
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 单词和上下文嵌入
- en: 'The following choices affect the quality of embedding:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下选择会影响嵌入的质量：
- en: '*Data quality is critical*. For example, leveraging Wikipedia allows better
    embedding for semantic tasks, while using news improves performance for syntactic
    tasks (a mixture of the two is recommended). Using Twitter or other social networks
    can insert bias.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据质量至关重要*。例如，利用维基百科可以更好地嵌入语义任务，而使用新闻可以提高句法任务的性能（推荐使用两者的混合）。使用Twitter或其他社交网络可能会引入偏差。'
- en: At the same time, *a larger amount of text improves embedding performance*.
    A large amount of text can partially compensate for poor quality but at the cost
    of much longer training (for example, Common Crawl is a huge dataset downloaded
    from the internet that is pretty dirty, though).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，*更多的文本可以提高嵌入性能*。大量的文本可以部分弥补质量较差的问题，但代价是更长的训练时间（例如，Common Crawl是从互联网下载的巨大数据集，尽管相当脏乱）。
- en: '*The number of dimensions is another important factor*. The larger the size
    of the embedding, the better its performance. 300 is considered a sweet spot because,
    beyond this size, number performance does not increase significantly.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*维度数量是另一个重要因素*。嵌入的大小越大，性能越好。300被认为是一个理想的点，因为超过这个大小，性能的提升并不显著。'
- en: '*The size of the context window also has an impact*. Generally, a context window
    of 4 is used, but a context window of 2 allows for vectors that better identify
    parts of speech. In contrast, long context windows are more useful if we are interested
    in similarity more broadly.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文窗口的大小也有影响*。通常，上下文窗口为4，但2个上下文窗口的向量能更好地识别词性。相比之下，如果我们对更广泛的相似度感兴趣，长上下文窗口更有用。'
- en: 'In Python, we can easily get an embedding from lists of tokens using the following
    code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们可以使用以下代码轻松地从标记列表中获取嵌入：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Important note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: The complete code is present in the GitHub repository. We used an embedding
    of 100 dimensions and a window of 5 words.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码位于GitHub仓库中。我们使用了100维的嵌入和一个5词的窗口。
- en: 'Once we have our embedding, we can visualize it. For example, if we try **clustering**
    the vectors of some words, words that have similar meanings should be closer together:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了嵌入，我们就可以可视化它。例如，如果我们尝试**聚类**一些单词的向量，具有相似意义的单词应该更靠近：
- en: '![Figure 1.6 – Clustering of some of the vectors obtained from the embedding](img/B21257_01_06.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 从嵌入中获得的某些向量的聚类](img/B21257_01_06.jpg)'
- en: Figure 1.6 – Clustering of some of the vectors obtained from the embedding
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 从嵌入中获得的某些向量的聚类
- en: 'Another way to visualize vectors is to use dimensionality reduction techniques.
    Vectors are multidimensional (100-1,024), so it is more convenient to reduce them
    to two or three dimensions so that they can be visualized more easily. Some of
    the most commonly used techniques are **principal component analysis** (**PCA**)
    and **t-distributed stochastic neighbor embedding** (**t-SNE)**. **Uniform Manifold
    Approximation and Projection** (**UMAP**), on the other hand, is a technique that
    has become the first choice for visualizing multidimensional data in recent years:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化向量的方法是使用降维技术。向量是多维的（100-1,024），因此将它们降低到两或三个维度以便更容易可视化会更方便。一些最常用的技术是**主成分分析**（PCA）和**t分布随机邻域嵌入**（t-SNE）。另一方面，**统一流形近似和投影**（UMAP）是一种近年来成为可视化多维数据的首选技术：
- en: '![Figure 1.7 – 2D projection of word2vec embedding highlighting some examples](img/B21257_01_07.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图1.7 – word2vec嵌入的2D投影，突出显示了一些示例](img/B21257_01_07.jpg)'
- en: Figure 1.7 – 2D projection of word2vec embedding highlighting some examples
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 – word2vec嵌入的2D投影，突出显示了一些示例
- en: UMAP has emerged because it produces visualizations that better preserve semantic
    meaning and relationships between examples and also better represent local and
    global structures. This makes for better clusters, and UMAP can also be used in
    preprocessing steps before a classification task on vectors.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP之所以出现，是因为它产生的可视化更好地保留了示例之间的语义意义和关系，同时也更好地表示了局部和全局结构。这使得聚类更好，UMAP也可以在向量分类任务之前的预处理步骤中使用。
- en: A notion of similarity for text
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本相似度概念
- en: Once we have obtained vector representations, we need a method to calculate
    the similarity between them. This is crucial in many applications—for instance,
    to find words in an embedding space that are most similar to a given word, we
    compute the similarity between its vector and those of other words. Similarly,
    given a query sentence, we can retrieve the most relevant documents by comparing
    its vector with document embeddings and selecting those with the highest similarity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了向量表示，我们需要一种方法来计算它们之间的相似度。这在许多应用中至关重要——例如，为了找到嵌入空间中最接近给定单词的单词，我们计算其向量与其他单词向量的相似度。同样，给定一个查询句子，我们可以通过比较其向量与文档嵌入并选择那些相似度最高的来检索最相关的文档。
- en: 'Most similarity measures are based on the **dot product**. This is because
    the dot product is high when the two vectors have values in the same dimension.
    In contrast, vectors that have zero alternately will have a dot product of zero,
    thus orthogonal or dissimilar. This is why the dot product was used as a similarity
    measure for word co-occurrence matrices or with vectors derived from document
    TF matrices:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数相似度度量都是基于**点积**。这是因为当两个向量在同一维度上有值时，点积很高。相比之下，交替为零的向量将具有零点积，因此是正交或不相似的。这就是为什么点积被用作单词共现矩阵或从文档TF矩阵导出的向量之间的相似度度量：
- en: <mrow><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">o</mi><mi
    mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi
    mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">u</mi><mi
    mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">t</mi><mo>:</mo><mi
    mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><msub><mi>a</mi><mn>1</mn></msub><mo>×</mo><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><mo>×</mo><msub><mi>b</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msub><mi>a</mi><mi>n</mi></msub><mo>×</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">o</mi><mi
    mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi
    mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">u</mi><mi
    mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">t</mi><mo>:</mo><mi
    mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><msub><mi>a</mi><mn>1</mn></msub><mo>×</mo><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><mo>×</mo><msub><mi>b</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msub><mi>a</mi><mi>n</mi></msub><mo>×</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow>
- en: <mrow><mrow><mi>m</mi><mi>a</mi><mi>g</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>u</mi><mi>d</mi><mi>e</mi><mo>=</mo><mfenced
    close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mo>=</mo><mroot><mrow><munderover><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mrow>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>m</mi><mi>a</mi><mi>g</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>u</mi><mi>d</mi><mi>e</mi><mo>=</mo><mfenced
    close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mo>=</mo><mroot><mrow><munderover><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mrow>
- en: 'The dot product has several problems, though:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 点积有几个问题，尽管如此：
- en: It tends to favor vectors with long dimensions
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它倾向于偏好长维度的向量
- en: It favors vectors with high values (which, in general, are those of very frequent
    and, therefore, useless words)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它倾向于高值向量（通常是非常频繁的词，因此是无用的词）
- en: The value of the dot product has no limits
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积的值没有限制
- en: 'Therefore, alternatives have been sought, such as a normalized version of the
    dot product. The normalized dot product is equivalent to the cosine of the angle
    between the two vectors, hence **cosine similarity**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经寻求了替代方案，例如点积的归一化版本。归一化点积等同于两个向量之间角度的余弦值，因此称为**余弦相似度**：
- en: <mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">Θ</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi></mrow><mrow><mfenced
    close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mfenced close="|"
    open="|"><mi mathvariant="bold-italic">b</mi></mfenced></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub></mrow></mrow><mrow><mroot><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot><mroot><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mfrac></mrow></mrow>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">Θ</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi></mrow><mrow><mfenced
    close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mfenced close="|"
    open="|"><mi mathvariant="bold-italic">b</mi></mfenced></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub></mrow></mrow><mrow><mroot><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot><mroot><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mfrac></mrow></mrow>
- en: 'Cosine similarity has some interesting properties:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度有一些有趣的特性：
- en: It is between -1 and 1\. Opposite or totally dissimilar vectors will have a
    value of -1, 0 for orthogonal vectors (or totally dissimilar for scattered vectors),
    and 1 for perfectly similar vectors. Since it measures the angle between two vectors,
    the interpretation is easier and is within a specific range, so it allows one
    intuitively to understand the similarity or dissimilarity.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的值介于 -1 和 1 之间。相反或完全不同的向量将具有 -1 的值，正交向量（或完全不同的散点向量）为 0，完全相似的向量为 1。由于它测量两个向量之间的角度，因此解释更容易，并且在一个特定的范围内，因此它允许人们直观地理解相似性或差异性。
- en: It is fast and cheap to compute.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的计算速度快且成本低。
- en: It is less sensitive to word frequency and, thus, more robust to outliers.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对词频不太敏感，因此对异常值更稳健。
- en: It is scale-invariant, meaning that it is not influenced by the magnitude of
    the vectors.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是尺度不变的，这意味着它不受向量大小的影响。
- en: Being normalized, it can also be used with high-dimensional data.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过归一化处理后，它也可以用于处理高维数据。
- en: 'For 2D vectors, we can plot to observe these properties:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二维向量，我们可以绘制图表来观察这些属性：
- en: '![Figure 1.8 – Example of cosine similarity between two vectors](img/B21257_01_08.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 两个向量之间余弦相似性的示例](img/B21257_01_08.jpg)'
- en: Figure 1.8 – Example of cosine similarity between two vectors
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 两个向量之间余弦相似性的示例
- en: In the next section, we can define the properties of our trained embedding using
    this notion of similarity.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们可以使用这个相似性的概念来定义我们训练的嵌入的性质。
- en: Properties of embeddings
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入的性质
- en: Embeddings are a surprisingly flexible method and manage to encode different
    syntactic and semantic properties that can both be visualized and exploited for
    different operations. Once we have a notion of similarity, we can search for the
    words that are most similar to a word *w*. Note that similarity is defined as
    appearing in the same context window; the model cannot differentiate synonyms
    and antonyms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一个令人惊讶的灵活方法，并能够编码不同的句法和语义属性，这些属性既可以可视化，也可以用于不同的操作。一旦我们有了相似性的概念，我们就可以搜索与字词
    *w* 最相似的词。请注意，相似性定义为出现在相同的上下文窗口中；模型无法区分同义词和反义词。
- en: In addition, the model is also capable of representing grammatical relations
    such as superlatives or verb forms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型还能够表示诸如最高级或动词形式之类的语法关系。
- en: Another interesting relationship we can study is analogies. The parallelogram
    model is a system for representing analogies in a cognitive space. The classic
    example is *king:queen::man:?* (which in a formula would be *a:b::a*:?*). Given
    that we have vectors, we can turn this into an *a-a*+b* operation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以研究的另一个有趣的关系是类比。平行四边形模型是一个在认知空间中表示类比的系统。经典的例子是 *king:queen::man:?*（在公式中为
    *a:b::a*:?*）。鉴于我们有向量，我们可以将其转化为 *a-a*+b* 操作。
- en: 'We can test this in Python using the embedding model we have trained:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们训练的嵌入模型在 Python 中进行此测试：
- en: We can check the most similar words
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以检查最相似的字词。
- en: We can test the analogy
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以测试类比
- en: We can then test the capacity to identify synonyms and antonyms
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们可以测试识别同义词和反义词的能力。
- en: 'The code for this process is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的代码如下：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Important note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This is done with the embedding we have trained before. Notice how the model
    is not handling antonyms well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前训练的嵌入完成的。注意模型在处理反义词方面并不好。
- en: The method is not exactly perfect, so sometimes the right answer is not the
    first result, but it could be among the first three inputs. Also, this system
    works for entities that are frequent within the text (city names, common words)
    but much less with rarer entities.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法并不完全完美，因此有时正确答案不是第一个结果，但它可能是前三个输入之一。此外，该系统适用于文本中频繁出现的实体（如城市名称、常用词），但对于罕见实体则作用甚微。
- en: 'Embeddings can also be used as a tool to study how the meaning of a word changes
    over time, especially if you have text corpora that span several decades. This
    is demonstrated in the following diagram, which shows how the meanings of the
    words *gay*, *broadcast*, and *awful* have changed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入还可以作为工具来研究一个词的意义如何随时间变化，特别是如果你有跨越几个十年文本语料库的话。以下图表展示了如何随着时间的推移，字词 *gay*、*broadcast*
    和 *awful* 的意义发生了变化：
- en: '![Figure 1.9 – 2D visualization of how the semantic meaning of a word changes
    over the years; these projections are obtained using text from different decades
    and embeddings (https://arxiv.org/abs/1605.09096)](img/B21257_01_09.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 随着年份变化，字词语义意义的二维可视化；这些投影是通过使用来自不同年代的文本和嵌入获得的（https://arxiv.org/abs/1605.09096）](img/B21257_01_09.jpg)'
- en: Figure 1.9 – 2D visualization of how the semantic meaning of a word changes
    over the years; these projections are obtained using text from different decades
    and embeddings ([https://arxiv.org/abs/1605.09096](https://arxiv.org/abs/1605.09096))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 – 2D可视化展示了单词的语义意义如何随时间变化；这些投影是通过使用不同年代的文本和嵌入获得的([https://arxiv.org/abs/1605.09096](https://arxiv.org/abs/1605.09096))
- en: 'Finally, a word can still have multiple meanings! For example, common words
    such as “good” have more than one meaning depending on the context. One may wonder
    whether a vector for a word in an embedding represents only one meaning or whether
    it represents the set of meanings of a word. Fortunately, embedding vectors represent
    a weighted sum of the various meanings of a word (linear superposition). The weights
    of each meaning are proportional to the frequency of that meaning in the text.
    Although these meanings reside in the same vector, when we add or subtract during
    the calculation of analogies, we are working with these components. For example,
    “apple” is both a fruit and the name of a company; if we conduct the operation
    *apple:red::banana:?*, we are subtracting only a very specific semantic component
    from the apple vector (the component that is similar to red). This flexibility
    can be useful when we want to disambiguate meanings. Also, since the vector space
    is sparse, by exploiting sparse coding, we can separate the various meanings:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个单词仍然可以有多个意义！例如，像“good”这样的常用词根据上下文有不同的意义。有人可能会想知道一个单词在嵌入中的向量是否只代表一个意义，或者它是否代表了一个单词的所有意义集合。幸运的是，嵌入向量代表了一个单词各种意义的加权总和（线性叠加）。每个意义的权重与该意义在文本中的频率成正比。尽管这些意义位于同一个向量中，但在计算类比时，我们处理的是这些组成部分。例如，“apple”既是水果，也是一家公司的名字；如果我们进行操作
    *apple:red::banana:?*，我们只是在苹果向量中减去一个非常具体的语义成分（与红色相似的成分）。这种灵活性在我们想要消除歧义时非常有用。此外，由于向量空间是稀疏的，通过利用稀疏编码，我们可以分离出不同的意义：
- en: '![Figure 1.10 – Table showing how a vector in word2vec is encoding for different
    meanings at the same time (https://aclanthology.org/Q18-1034/)](img/B21257_01_10.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 表格展示了word2vec中的向量如何同时编码不同的意义 (https://aclanthology.org/Q18-1034/)](img/B21257_01_10.jpg)'
- en: Figure 1.10 – Table showing how a vector in word2vec is encoding for different
    meanings at the same time ([https://aclanthology.org/Q18-1034/](https://aclanthology.org/Q18-1034/))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 表格展示了word2vec中的向量如何同时编码不同的意义([https://aclanthology.org/Q18-1034/](https://aclanthology.org/Q18-1034/))
- en: These vectors are now providing contextual and semantic meaning for each word
    in the text. We can use this rich source of information for tasks such as text
    classification. What we need now are models that can handle the sequential nature
    of text, which we will learn about in the next section.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量现在为文本中的每个单词提供了上下文和语义意义。我们可以利用这个丰富的信息源来完成诸如文本分类等任务。我们现在需要的是能够处理文本序列性质的模型，我们将在下一节中学习到这方面的内容。
- en: RNNs, LSTMs, GRUs, and CNNs for text
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNNs，LSTMs，GRUs和CNNs用于文本
- en: So far, we have discussed how to represent text in a way that is digestible
    for the model; in this section, we will discuss how to analyze the text once a
    representation has been obtained. Traditionally, once we obtained a representation
    of the text, it was fed to models such as naïve Bayes or even algorithms such
    as logistic regression. The success of neural networks has made these machine
    learning algorithms outdated. In this section, we will discuss deep learning models
    that can be used for various tasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何以模型可消化的方式表示文本；在本节中，我们将讨论在获得表示后如何分析文本。传统上，一旦我们获得了文本的表示，它就会被输入到诸如朴素贝叶斯或甚至逻辑回归等模型中。神经网络的成功使得这些机器学习算法变得过时。在本节中，我们将讨论可用于各种任务的深度学习模型。
- en: RNNs
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNNs
- en: 'The problem with classical neural networks is that they have no memory. This
    is especially problematic for time series and text inputs. In a sequence of words
    *t*, the word *w* at time *t* depends on the *w* at time *t-1*. In fact, in a
    sentence, the last word is often dependent on several words in the sentence. Therefore,
    we want an NN model that maintains a memory of previous inputs. An **RNN** maintains
    an internal state that maintains this memory; that is, it stores information about
    previous inputs, and the outputs it produces are affected by previous inputs.
    These networks perform the same operation on all elements of the sequence (hence
    recurrent) and maintain the memory of this operation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 经典神经网络的缺点是它们没有记忆功能。这对于时间序列和文本输入尤其成问题。在一个单词序列 *t* 中，时间 *t* 的单词 *w* 依赖于时间 *t-1*
    的 *w*。实际上，在一句话中，最后一个单词通常依赖于句子中的几个单词。因此，我们希望有一个NN模型能够记住之前的输入。一个**RNN**（循环神经网络）维护一个内部状态来保持这种记忆；也就是说，它存储有关先前输入的信息，并且它产生的输出受先前输入的影响。这些网络对序列的所有元素执行相同的操作（因此是循环的）并保持这种操作的内存：
- en: '![Figure 1.11 – Simple example of an RNN (https://arxiv.org/pdf/1506.00019)](img/B21257_01_11.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11 – RNN的简单示例](https://arxiv.org/pdf/1506.00019)(img/B21257_01_11.jpg)'
- en: Figure 1.11 – Simple example of an RNN ([https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 – RNN的简单示例([https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019))
- en: 'A classical neural network (**feedforward neural network**) considers inputs
    to be independent, and one layer of a neural network performs the following operation
    for a vector representing the element at time *t*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的神经网络（**前馈神经网络**）认为输入是独立的，神经网络的一层对时间 *t* 的元素表示的向量执行以下操作：
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi
    mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:math>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi
    mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:math>
- en: 'However, in a simple RNN, the following operations are conducted:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在简单的RNN中，以下操作被执行：
- en: <mrow><mrow><msup><mi mathvariant="bold-italic">a</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi
    mathvariant="bold-italic">b</mi><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mi
    mathvariant="bold-italic">W</mi><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup></mrow></mrow>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msup><mi mathvariant="bold-italic">a</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi
    mathvariant="bold-italic">b</mi><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mi
    mathvariant="bold-italic">W</mi><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup></mrow></mrow>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold-italic">c</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold-italic">c</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
- en: These operations may seem complicated, but in fact, we are simply maintaining
    a hidden state that considers the previous iterations. The first equation is a
    normal feedforward layer modified, in which we multiply the previously hidden
    state *h* by a set of weights *U*. This matrix *U* allows us to control how the
    neural network uses the previous context to bind input and past inputs (how the
    past influences the output for input at time *t*). In the second equation, we
    create a new hidden state that will then be used for subsequent computations but
    also for the next input. In the third equation, we are creating the output; we
    use a bias vector and a matrix to compute the output. In the last equation, it
    is simply passed as a nonlinearity function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作看起来可能很复杂，但实际上，我们只是在维护一个考虑先前迭代的隐藏状态。第一个方程是一个普通的正向传播层，其中我们将先前隐藏状态 *h* 乘以一组权重
    *U*。这个矩阵 *U* 允许我们控制神经网络如何使用先前上下文来绑定输入和过去输入（过去如何影响时间 *t* 的输入输出）。在第二个方程中，我们创建一个新的隐藏状态，该状态将被用于后续计算，但也将用于下一个输入。在第三个方程中，我们创建输出；我们使用一个偏置向量和矩阵来计算输出。在最后一个方程中，它简单地作为一个非线性函数传递。
- en: 'These RNNs can be seen as unrolled entities throughout time, in which we can
    represent the network and its computations throughout the sequence:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 RNN 可以被视为随时间展开的实体，其中我们可以表示整个序列中的网络及其计算：
- en: '![Figure 1.12 – Simple example of an RNN unrolled through the sequence (https://arxiv.org/pdf/1506.00019)](img/B21257_01_12.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – RNN 通过序列展开的简单示例](https://arxiv.org/pdf/1506.00019)(img/B21257_01_12.jpg)'
- en: Figure 1.12 – Simple example of an RNN unrolled through the sequence ([https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019))
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – RNN 通过序列展开的简单示例([https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019))
- en: 'We can test in Python with a PyTorch RNN to see how it is transforming the
    data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 PyTorch RNN 在 Python 中测试，看看它是如何转换数据的：
- en: '[PRE7]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Important note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Notice how the model is transforming the data; we can also access the hidden
    state.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意模型如何转换数据；我们还可以访问隐藏状态。
- en: 'We can see several interesting things:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到几个有趣的事情：
- en: The RNN is not limited by the size of the input; it is a cyclic operation that
    is conducted over the entire sequence. RNNs basically process one word at a time.
    This cyclicality also means backpropagation is conducted for each time step. Although
    this model works well for series analysis, its sequential nature does not allow
    it to be parallelized.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 不受输入大小的限制；它是一个在整个序列上进行的循环操作。RNN基本上一次处理一个单词。这种循环性也意味着反向传播在每个时间步进行。尽管这个模型在序列分析方面表现良好，但其顺序性质不允许它并行化。
- en: Theoretically, this model could be trained with infinite sequences of words;
    theoretically, after a few time steps, it begins to forget the initial inputs.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论上，这个模型可以用无限序列的单词进行训练；理论上，经过几个时间步后，它开始忘记初始输入。
- en: Training can become inefficient due to the vanishing gradient problem, where
    gradients must propagate from the final cell back to the initial one. During this
    process, they can shrink exponentially and approach zero, making it difficult
    for the model to learn long-range dependencies. Conversely, the exploding gradient
    problem can also occur, where gradients grow uncontrollably large, leading to
    unstable training.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能因为梯度消失问题而变得低效，其中梯度必须从最终细胞传播回初始细胞。在这个过程中，它们可以指数级缩小并接近零，使得模型难以学习长距离依赖。相反，梯度爆炸问题也可能发生，其中梯度无控制地变得非常大，导致训练不稳定。
- en: RNNs are not the only form of deep learning models that are relevant to this
    topic.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 不是唯一与这个主题相关的深度学习模型形式。
- en: LSTMs
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM
- en: 'In theory, RNNs should be able to process long sequences and remember the initial
    input. However, in reality, the information inside the hidden state is local rather
    than global, and for a time *t*, it considers only the previous time steps and
    not the entire sequence. The main problem with such a simple model is that the
    hidden state must simultaneously fulfill two roles: provide information relevant
    to the output at time *t* and store memory for future decisions.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，RNN 应该能够处理长序列并记住初始输入。然而，在现实中，隐藏状态中的信息是局部的而不是全局的，对于时间 *t*，它只考虑前一个时间步而不是整个序列。这种简单模型的主要问题是隐藏状态必须同时履行两个角色：提供与时间
    *t* 的输出相关的信息，并为未来的决策存储记忆。
- en: An **LSTM** is an extension of RNNs, designed with the idea that the model can
    forget information that is not important and keep only the important context.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM** 是 RNN 的扩展，其设计理念是模型可以忘记不重要的信息，只保留重要的上下文。'
- en: '![Figure 1.13 – Internal structure of an LSTM cell (https://arxiv.org/pdf/2304.11461)](img/B21257_01_13.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.13 – LSTM 单元的内部结构](https://arxiv.org/pdf/2304.11461)(img/B21257_01_13.jpg)'
- en: Figure 1.13 – Internal structure of an LSTM cell ([https://arxiv.org/pdf/2304.11461](https://arxiv.org/pdf/2304.11461))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – LSTM 单元的内部结构([https://arxiv.org/pdf/2304.11461](https://arxiv.org/pdf/2304.11461))
- en: An LSTM has internal mechanisms to control the information (gates) within the
    layer; additionally, it has a dedicated context layer. So, we have two hidden
    states in which the first, *h*, serves for information at time *t* (short memory)
    and the other, *c*, for information at long term. The gates can be open (1) or
    closed (0); this is achieved by a feed-forward layer with sigmoid activation to
    squeeze values between zero and one. After that, we use the **Hadamard product**
    (or pointwise multiplication) for the gating mechanism of a layer. This multiplication
    acts as a binary gate, allowing information to pass if the value is close to 1
    or blocking it when the value is close to 0\. These gates allow a dynamic system
    in which during a time step, we decide how much information we preserve and how
    much we forget.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 有内部机制来控制层内的信息（门）；此外，它还有一个专门上下文层。因此，我们有两个隐藏状态，其中第一个 *h* 用于时间 *t* 的信息（短期记忆），另一个
    *c* 用于长期信息。门可以是开启的（1）或关闭的（0）；这是通过一个具有 sigmoid 激活的前馈层来实现的，以压缩值在零和一之间。之后，我们使用 **Hadamard
    积**（或逐点乘法）作为层的门控机制。这种乘法作为一个二进制门，当值接近 1 时允许信息通过，当值接近 0 时阻止信息通过。这些门允许一个动态系统，在时间步内，我们决定保留多少信息以及忘记多少信息。
- en: 'The first gate is called the **forget gate** because it is used to forget information
    that is no longer needed from the context and, therefore, will no longer be needed
    in the next time step. So, we will use the output of the forget gate to multiply
    the context. At this time, we extract both information from the input and the
    previous time step’s hidden state. Each gate has a set of gate-specific *U* weights:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个门被称为 **遗忘门**，因为它用于忘记上下文中不再需要的信息，因此在下一次时间步中也不再需要。所以，我们将使用遗忘门的输出去乘以上下文。此时，我们从输入和前一个时间步的隐藏状态中提取信息。每个门都有一组特定于门的
    *U* 权重：
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">f</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">f</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">f</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">f</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
- en: 'The next step is to extract information from the input and decide which of
    that information will be added to the context. This is controlled by an **input
    gate** *i* that controls how much information will then be added. The context
    is then obtained by the sum of what we add and what we forget:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从输入中提取信息并决定哪些信息将被添加到上下文中。这由一个**输入门** *i* 控制，它决定了将添加多少信息。上下文随后是通过我们添加的和忘记的内容的总和来获得的：
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">g</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><msub><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">g</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">g</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><msub><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">g</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">i</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">i</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">i</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">i</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mi>t</mml:mi><mml:mo>)</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
- en: 'The final step is the calculation of the output; this is achieved with a final
    gate. The output or final layer decision is also used to update the hidden state:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最终步骤是计算输出；这是通过一个最终门来实现的。输出或最终层的决策也用于更新隐藏状态：
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">o</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">o</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">o</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">o</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi
    mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mi
    mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
- en: 'These gates are independent of each other so that efficient implementations
    of the LSTM can parallelize them. We can test in Python with a PyTorch RNN to
    see how it is transforming the data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门控单元相互独立，因此LSTM的高效实现可以并行化。我们可以使用Python中的PyTorch RNN来测试它如何转换数据：
- en: '[PRE8]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Important note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: Notice how the model is transforming the data; we can access the hidden state
    as well as the cell state.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意模型如何转换数据；我们可以访问隐藏状态以及细胞状态。
- en: 'We can also note some other interesting properties:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以注意一些其他有趣的特性：
- en: Computation augmentation is internal to layers, which means we can easily substitute
    LSTMs for RNNs.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算增强是层内进行的，这意味着我们可以轻松地将LSTMs替换为RNN。
- en: The LSTM manages to preserve information for a long time because it retains
    only the relevant part of the information and forgets what is not needed.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM能够长时间保留信息，因为它只保留相关信息，并遗忘不需要的信息。
- en: Standard practice is to initialize an LSTM with vector 1 (preserves everything),
    after which it learns by itself what to forget and what to add.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准做法是用向量1初始化LSTM（保留一切），然后它自己学习要忘记什么和要添加什么。
- en: An LSTM, as opposed to an RNN, can remember up to 100 time steps (the RNN after
    7 time steps starts forgetting). The plus operation makes vanishing or exploding
    gradients less likely.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM与RNN不同，可以记住多达100个时间步（RNN在7个时间步之后开始遗忘）。加法运算使得梯度消失或爆炸的可能性降低。
- en: Let’s look at another model option that is computationally lighter but still
    has this notion of context vector.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一种计算上更轻量但仍然具有上下文向量概念的模型选项。
- en: GRUs
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GRUs
- en: '**GRUs** are another variant of RNNs to solve the vanishing gradient problem,
    thus making them more effective in remembering information. They are very similar
    to LSTMs since they have internal gates, but they are much simpler and lighter.
    Despite having fewer parameters, GRUs can converge faster than LSTMs and still
    achieve comparable performance. GRUs exploit some of the elements that have made
    LSTMs so effective: the plus operation, the Hadamard product, the presence of
    a context, and the control of information within the layer:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRUs**是RNN的另一种变体，用于解决梯度消失问题，因此它们在记住信息方面更有效。它们与LSTMs非常相似，因为它们具有内部门控单元，但它们更简单、更轻量。尽管参数更少，GRUs的收敛速度比LSTMs快，并且仍然可以达到可比的性能。GRUs利用了使LSTMs如此有效的某些元素：加法运算、Hadamard乘积、上下文的存在以及层内信息控制的机制：'
- en: '![Figure 1.14 – Internal structure of a GRU cell (https://arxiv.org/pdf/2304.11461)](img/B21257_01_14.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图1.14 – GRU单元的内部结构 (https://arxiv.org/pdf/2304.11461)](img/B21257_01_14.jpg)'
- en: Figure 1.14 – Internal structure of a GRU cell ([https://arxiv.org/pdf/2304.11461](https://arxiv.org/pdf/2304.11461))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14 – GRU单元的内部结构 ([https://arxiv.org/pdf/2304.11461](https://arxiv.org/pdf/2304.11461))
- en: 'In a GRU, the forget gate is called the **update gate**, but it has the same
    purpose: important information is retained (values near 1) and unimportant information
    is rewritten during the update (values near 0). In a GRU, the input gate is called
    the **reset gate** and is not independent as in an LSTM, but connected to the
    update gate.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在GRU中，遗忘门被称为**更新门**，但它的目的相同：在更新过程中保留重要信息（接近1的值）并重写不重要信息（接近0的值）。在GRU中，输入门被称为**重置门**，它不像LSTM那样独立，而是与更新门相连。
- en: 'The first step is the update gate *z*, which is practically the same as the
    forget gate in an LSTM. At the same time, we calculate the reset gate *r*:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是更新门*z*，它实际上与LSTM中的遗忘门相同。同时，我们计算重置门*r*：
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">z</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">z</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">r</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">r</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
- en: 'The next step is to update the hidden state; this depends on the reset gate.
    In this way, we decide what new information is put into the hidden state and what
    relevant information from the past is saved. This is called the **current** **memory
    gate**:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是更新隐藏状态；这取决于重置门。这样，我们决定将哪些新信息放入隐藏状态，以及保存哪些过去的相关信息。这被称为**当前****记忆门**：
- en: <mrow><mrow><mrow><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="normal">t</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">h</mi><mo>(</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">h</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>+</mo><msup><mi mathvariant="bold-italic">r</mi><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msub><mi mathvariant="bold-italic">U</mi><mi
    mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced
    close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="normal">t</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">h</mi><mo>(</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">h</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>+</mo><msup><mi mathvariant="bold-italic">r</mi><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msub><mi mathvariant="bold-italic">U</mi><mi
    mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced
    close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>)</mo></mrow></mrow></mrow>
- en: 'At this point, we have the final update of the hidden state in which we also
    use the update gate:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了隐藏状态的最终更新，其中我们也使用了更新门：
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msup><mi mathvariant="bold-italic">h</mi><mfenced
    close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi
    mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>)</mo><mo>⊙</mo><msup><mover><mi
    mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup></mrow></mrow></mrow>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msup><mi mathvariant="bold-italic">h</mi><mfenced
    close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi
    mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>)</mo><mo>⊙</mo><msup><mover><mi
    mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup></mrow></mrow></mrow>
- en: 'We can test in Python with a PyTorch RNN to see how it is transforming the
    data:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 PyTorch RNN 在 Python 中进行测试，以查看它是如何转换数据的：
- en: '[PRE9]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Notice how the model is transforming the data; we can also access the hidden
    state.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意模型是如何转换数据的；我们也可以访问隐藏状态。
- en: 'We can see some interesting elements here as well:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里也可以看到一些有趣的因素：
- en: GRU networks are similar to LSTM networks, but they have the advantage of fewer
    parameters and are computationally more efficient. This means, though, they are
    more prone to overfitting.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU 网络类似于 LSTM 网络，但它们具有参数更少和计算效率更高的优势。但这意味着它们更容易过拟合。
- en: They can handle long sequences of data without forgetting previous inputs. For
    many textual tasks (but also speech recognition and music generation) they perform
    quite well, though they are less efficient than LSTMs when it comes to modeling
    long-term dependencies or complex patterns.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理长序列的数据而不会忘记之前的输入。对于许多文本任务（以及语音识别和音乐生成），它们的性能相当不错，尽管在建模长期依赖或复杂模式时，它们不如LSTMs高效。
- en: Next, we’ll look at CNNs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨CNN。
- en: CNNs for text
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本CNN
- en: '**CNNs** are designed to find patterns in images (or other 2D matrixes) by
    running a filter (a matrix or kernel) along them. The convolution is conducted
    pixel by pixel, and the filter values are multiplied by the pixels in the image
    and then summed. During training, a weight is learned for each of the filter entries.
    For each filter, we get a different scan of the image that can be visualized;
    this is called a feature map.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**CNN**被设计用来通过在图像（或其他二维矩阵）上运行滤波器（一个矩阵或核）来寻找其中的模式。卷积是逐像素进行的，滤波器的值与图像中的像素相乘然后求和。在训练过程中，为每个滤波器条目学习一个权重。对于每个滤波器，我们得到一个不同的图像扫描，这可以可视化；这被称为特征图。'
- en: 'Convolutional networks have been successful in **computer vision** because
    of their ability to extract local information and recognize complex patterns.
    For this reason, convolutional networks have been proposed for sequences. In this
    case, 1-dimensional convolutional networks are exploited, but the idea is the
    same. In fact, on a sequence, 1D convolution is used to extract a feature map
    (instead of being a 2-dimensional filter or matrix, we have a uni-dimensional
    filter that can be seen as the context window of word2vec):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络在**计算机视觉**中取得了成功，这得益于它们提取局部信息并识别复杂模式的能力。因此，卷积网络被提出用于序列。在这种情况下，使用了一维卷积网络，但原理是相同的。实际上，在序列上，一维卷积用于提取特征图（而不是作为一个二维滤波器或矩阵，我们有一个一维滤波器，它可以被视为word2vec的上下文窗口）：
- en: '![Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence](img/B21257_01_15.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图1.15 – 在一维卷积中，我们正在序列上滑动一个一维滤波器](img/B21257_01_15.jpg)'
- en: Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15 – 在一维卷积中，我们正在序列上滑动一个一维滤波器
- en: In the preceding figure, we scroll a uni-dimensional filter over the sequence;
    the process is very fast, and the filter can have an arbitrary size (three to
    seven words or even more). The model tries to learn patterns among the various
    words found within this kernel. It can also be used on vectors previously obtained
    from an embedding, and we can also use multiple kernels (so as to learn different
    patterns for each sequence). As with image CNNs, we can add operations such as
    max pooling to extract the most important features.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们在序列上滚动一个一维滤波器；这个过程非常快，滤波器可以具有任意大小（三到七个单词甚至更多）。模型试图学习在这个核中找到的各种单词之间的模式。它也可以用于从嵌入中获得的向量，我们还可以使用多个核（以便为每个序列学习不同的模式）。与图像CNN一样，我们可以添加如最大池化等操作来提取最重要的特征。
- en: 'We can test in Python with a PyTorch RNN to see how it is transforming the
    data:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用Python中的PyTorch RNN来测试，看看它是如何转换数据的：
- en: '[PRE10]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Important note
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Notice how the model is transforming the data and how this is different from
    what we have seen before.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察模型是如何转换数据的，以及这与我们之前所看到的不同之处。
- en: Now that we have a method to transform text into numerical representation (while
    preserving the contextual information) and models that can handle this representation,
    we can combine them to obtain an end-to-end system.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了将文本转换为数值表示（同时保留上下文信息）的方法，并且有可以处理这种表示的模型，我们可以将它们结合起来以获得端到端系统。
- en: Performing sentiment analysis with embedding and deep learning
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用嵌入和深度学习进行情感分析
- en: 'In this section, we will train a model for conducting sentiment analysis on
    movie reviews. The model we will train will be able to classify reviews as positive
    or negative. To build and train the model, we will exploit the elements we have
    encountered so far. In brief, we’re doing the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练一个用于对电影评论进行情感分析模型的模型。我们将训练的模型能够将评论分类为正面或负面。为了构建和训练模型，我们将利用我们迄今为止遇到的所有元素。简而言之，我们正在做以下事情：
- en: We are preprocessing the dataset, transforming in numerical vectors, and harmonizing
    the vectors
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在预处理数据集，将其转换为数值向量，并使向量和谐
- en: We are defining a neural network with an embedding and training it
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在定义一个带有嵌入的神经网络并对其进行训练
- en: 'The dataset consists of 50,000 positive and negative reviews. We can see that
    it contains a heterogeneous length for reviews and that on average, there are
    230 words:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由50,000条正面和负面评论组成。我们可以看到它包含不同长度的评论，平均有230个单词：
- en: '![Figure 1.16 – Graphs showing the distribution of the length of the review
    in the text; the left plot is for positive reviews, while the right plot is for
    negative reviews](img/B21257_01_16.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图1.16 – 显示文本中评论长度分布的图表；左图是正面评论，右图是负面评论](img/B21257_01_16.jpg)'
- en: Figure 1.16 – Graphs showing the distribution of the length of the review in
    the text; the left plot is for positive reviews, while the right plot is for negative
    reviews
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16 – 显示文本中评论长度分布的图表；左图是正面评论，右图是负面评论
- en: 'In addition, the most prevalent words are, obviously, “*movie*” and “*film*”:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最常出现的词显然是“*电影*”和“*影片*”：
- en: '![Figure 1.17 – Word cloud for the most frequent words in positive (left plot)
    and negative (right plot) reviews](img/B21257_01_17.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图1.17 – 正面（左图）和负面（右图）评论中最常出现单词的词云](img/B21257_01_17.jpg)'
- en: Figure 1.17 – Word cloud for the most frequent words in positive (left plot)
    and negative (right plot) reviews
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17 – 正面（左图）和负面（右图）评论中最常出现单词的词云
- en: 'The text is messy and must be cleaned before the model can be trained. The
    first step is binary encoding of the label (“positive” equals 0, “negative” equals
    1). After that, we divide the features and the labels (for a dataset in `X` are
    the features and `y` are the labels). Next, we create three balanced datasets
    for training, validation, and testing:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 文本很杂乱，在模型训练之前必须进行清理。第一步是对标签进行二进制编码（“正面”等于0，“负面”等于1）。之后，我们将特征和标签（对于数据集中的`X`是特征，`y`是标签）分开。接下来，我们创建三个平衡的数据集用于训练、验证和测试：
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A few steps are necessary before proceeding with the training:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行训练之前需要几个步骤：
- en: A **preprocessing** step in which we remove excessive spaces, special characters,
    and punctuation.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**预处理**步骤，其中我们移除过多的空格、特殊字符和标点符号。
- en: A **tokenization** step in which we convert the various reviews into tokens.
    In this step, we also remove stopwords and single-character words. We extract
    for each review only the 1,000 most popular words (this step is only to reduce
    computation time during training).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**分词**步骤，其中我们将各种评论转换为标记。在这个步骤中，我们还移除了停用词和单字符词。对于每个评论，我们只提取最流行的1000个单词（这一步骤只是为了在训练期间减少计算时间）。
- en: Transformation of the the words into indices (**vectorization**) according to
    our vocabulary to make the model work with numerical values.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单词转换为索引（**向量化**）以根据我们的词汇表使模型使用数值值。
- en: Since the reviews have different lengths, we apply padding to harmonize the
    length of the review to a fixed number (we need this for the training).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于评论长度不同，我们应用填充以使评论长度与固定数字一致（我们这样做是为了训练）。
- en: These preprocessing steps depend on the dataset. The code is in the GitHub repository.
    Note, however, that the tokenization and preprocessing choices alter the properties
    of the reviews – in this case, the summary statistics.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预处理步骤取决于数据集。代码在GitHub仓库中。请注意，然而，分词和预处理选择会改变评论的性质——在这种情况下，是摘要统计。
- en: '![Figure 1.18 – Graph showing the distribution of review length after tokenization](img/B21257_01_18.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图1.18 – 显示分词后评论长度分布的图表](img/B21257_01_18.jpg)'
- en: Figure 1.18 – Graph showing the distribution of review length after tokenization
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18 – 显示分词后评论长度分布的图表
- en: 'We are defining the model with its hyperparameters. In this case, we are training
    a neural network to predict sentiment data composed of embeddings and GRUs. To
    make the training more stable, we add regularization (dropout). The linear layer
    is to map these features that we extracted to a single representation. We use
    this representation to calculate the probability that the review is positive or
    negative:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在定义模型及其超参数。在这种情况下，我们正在训练一个神经网络来预测由嵌入和GRUs组成的情感数据。为了使训练更稳定，我们添加了正则化（dropout）。线性层是将我们提取的特征映射到单个表示。我们使用这个表示来计算评论是正面还是负面的概率：
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that in this case, we use binary cross-entropy loss because we have only
    two categories (positive and negative). Also, we use `Adam` as an optimizer, but
    one can test others. In this case, we conduct batch training since we have thousands
    of reviews:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，我们使用二元交叉熵损失，因为我们只有两个类别（正面和负面）。此外，我们使用`Adam`作为优化器，但也可以测试其他优化器。在这种情况下，我们进行批量训练，因为我们有成千上万的评论：
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following graph displays the accuracy and loss for the training and validation
    sets:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了训练集和验证集的准确率和损失：
- en: '![Figure 1.19 – Training curves for training and validation set, for accuracy
    and loss](img/B21257_01_19.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图1.19 – 训练集和验证集的准确率和损失训练曲线](img/B21257_01_19.jpg)'
- en: Figure 1.19 – Training curves for training and validation set, for accuracy
    and loss
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.19 – 训练集和验证集的准确率和损失训练曲线
- en: 'The model achieves good accuracy, as we can see from the following confusion
    matrix:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面的混淆矩阵中我们可以看到，模型达到了良好的准确率：
- en: '![Figure 1.20 – Confusion matrix for the test set](img/B21257_01_20.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图1.20 – 测试集的混淆矩阵](img/B21257_01_20.jpg)'
- en: Figure 1.20 – Confusion matrix for the test set
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20 – 测试集的混淆矩阵
- en: 'In addition, if we look at the projection of reviews before and after the training,
    we can see that the model has learned how to separate positive and negative reviews:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们观察训练前后评论的投影，我们可以看到模型已经学会了如何区分正面和负面评论：
- en: '![Figure 1.21 – Embedding projection obtained from the model before (left plot)
    and after (right plot) training](img/B21257_01_21.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图1.21 – 模型训练前后（左图）和训练后（右图）获得的嵌入投影](img/B21257_01_21.jpg)'
- en: Figure 1.21 – Embedding projection obtained from the model before (left plot)
    and after (right plot) training
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.21 – 模型训练前后（左图）和训练后（右图）获得的嵌入投影
- en: We have now trained a model that can take a review in plain text and classify
    it as positive or negative. We did that by combining the elements we saw previously
    in the chapter. The same approach can be followed with any other dataset; that
    is the power of deep learning.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经训练了一个模型，它可以接受纯文本评论并将其分类为正面或负面。我们通过结合本章中看到的前述元素来实现这一点。同样的方法可以应用于任何其他数据集；这就是深度学习的力量。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how to transform text to an increasingly complex vector
    representation. This numerical representation of text allowed us to be able to
    use machine learning models. We saw how to preserve the contextual information
    (word embedding) of a text and how this can then be used for later analysis (for
    example, searching synonyms or clustering words). In addition, we saw how neural
    networks (RNNs, LSTM, GRUs) can be used to analyze text and perform tasks (for
    example, sentiment analysis).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何将文本转换为越来越复杂的向量表示。这种文本的数值表示使我们能够使用机器学习模型。我们看到了如何保留文本的上下文信息（词嵌入），以及如何将这些信息用于后续分析（例如，搜索同义词或聚类单词）。此外，我们还看到了如何使用神经网络（RNNs、LSTM、GRUs）来分析文本并执行任务（例如，情感分析）。
- en: In the next chapter, we will see how to solve some of the remaining unsolved
    challenges and see how this will lead to the natural evolution of the models seen
    here.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何解决一些尚未解决的挑战，并看到这将如何导致这里看到的模型的自然进化。
