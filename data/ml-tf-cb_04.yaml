- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Linear Regression
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear regression may be one of the most important algorithms in statistics,
    machine learning, and science in general. It's one of the most widely used algorithms,
    and it is very important to understand how to implement it and its various flavors.
    One of the advantages that linear regression has over many other algorithms is
    that it is very interpretable. We end up with a number (a coefficient) for each
    feature and such a number directly represents how that feature influences the
    target (the so-called dependent variable).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可能是统计学、机器学习和科学领域中最重要的算法之一。它是最广泛使用的算法之一，理解如何实现它及其不同变种非常重要。线性回归相较于许多其他算法的一个优势是它非常易于解释。我们为每个特征得到一个数字（系数），这个数字直接表示该特征如何影响目标（即所谓的因变量）。
- en: For instance, if you had to predict the selling value of a house and you obtained
    a dataset of historical sales comprising house characteristics (such as the lot
    size, indicators of the quality and condition of the house, and the distance from
    the city center), you could easily apply a linear regression. You could obtain
    a reliable estimator in a few steps and the resulting model would be easy to understand
    and explain to others, too. A linear regression, in fact, first estimates a baseline
    value, called the intercept, and then estimates a multiplicative coefficient for
    each feature. Each coefficient can transform each feature into a positive and
    negative part of the prediction. By summing the baseline and all the coefficient-transformed
    features, you get your final prediction. Therefore, in our house sale price prediction
    problem, you could get a positive coefficient for the lot size, implying that
    larger lots will sell for more, and a negative coefficient for the distance from
    the city center, an indicator that estates located in the outskirts have less
    market value.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你需要预测一栋房子的售价，并且你获得了一组包含房屋特征的历史销售数据（例如地块大小、房屋质量和状况的指标，以及离市中心的距离），你可以轻松地应用线性回归。你可以通过几步获得一个可靠的估算器，所得模型也容易理解并向他人解释。事实上，线性回归首先估算一个基准值，即截距，然后为每个特征估算一个乘法系数。每个系数可以将每个特征转化为预测的正向或负向部分。通过将基准值和所有系数转化后的特征相加，你可以得到最终预测。因此，在我们的房屋售价预测问题中，你可能会得到一个正系数表示地块大小，这意味着较大的地块会卖得更贵，而离市中心的负系数则表示位于郊区的房产市场价值较低。
- en: Computing such kinds of models with TensorFlow is fast, suitable for big data,
    and much easier to put into production because it will be accessible to general
    interpretation by inspection of a weights vector.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 计算此类模型快速、适合大数据，并且更容易投入生产，因为它可以通过检查权重向量进行普遍的解释。
- en: In this chapter, we will introduce you to recipes explaining how linear regression
    is implemented in TensorFlow, via Estimators or Keras, and then move on to providing
    solutions that are even more practical. In fact, we will explain how to tweak
    it using different loss functions, how to regularize coefficients in order to
    achieve feature selection in your models, and how to use regression for classification,
    for non-linear problems, and when you have categorical variables with high-cardinality
    (high-cardinality means variables with many unique values).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向你介绍如何通过 Estimators 或 Keras 在 TensorFlow 中实现线性回归的配方，并进一步提供更为实用的解决方案。事实上，我们将解释如何使用不同的损失函数进行调整，如何正则化系数以实现特征选择，以及如何将回归应用于分类、非线性问题和具有高基数的类别变量（高基数指的是具有许多独特值的变量）。
- en: Remember that all the code is available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，所有的代码都可以在 GitHub 上找到，链接：[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)。
- en: 'In this chapter, we will cover recipes involving linear regression. We start
    with the mathematical formulation for solving linear regression with matrices,
    before moving on to implementing standard linear regression and variants with
    the TensorFlow paradigm. We will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍涉及线性回归的配方。我们从使用矩阵求解线性回归的数学公式开始，然后转向使用 TensorFlow 范式实现标准线性回归及其变体。我们将涵盖以下主题：
- en: Learning the TensorFlow way of regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 TensorFlow 回归方法
- en: Turning a Keras model into an Estimator
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Keras 模型转化为 Estimator
- en: Understanding loss functions in linear regression
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线性回归中的损失函数
- en: Implementing Lasso and Ridge regression
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 Lasso 和 Ridge 回归
- en: Implementing logistic regression
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现逻辑回归
- en: Resorting to non-linear solutions
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 求助于非线性解决方案
- en: Using Wide & Deep models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Wide & Deep 模型
- en: By the end of the chapter, you will find that creating linear models (and some
    non-linear ones, too) using TensorFlow is easy using the recipes provided.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你会发现，使用 TensorFlow 创建线性模型（以及一些非线性模型）非常简单，利用提供的配方可以轻松完成。
- en: Learning the TensorFlow way of linear regression
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 TensorFlow 中的线性回归方法
- en: The statistical approach in linear regression, using matrices and decomposition
    methods on data, is very powerful. In any event TensorFlow has another means to
    solve for the coefficients of a slope and an intercept in a regression problem.
    TensorFlow can achieve a result in such problems iteratively, that is, gradually
    learning the best linear regression parameters that will minimize the loss, as
    we have seen in the recipes in previous chapters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归中的统计方法，使用矩阵和数据分解方法，非常强大。无论如何，TensorFlow 有另一种方式来解决回归问题中的斜率和截距系数。TensorFlow
    可以通过迭代方式解决这些问题，即逐步学习最佳的线性回归参数，以最小化损失，就像我们在前面的章节中所看到的配方一样。
- en: 'The interesting fact is that you actually don''t have to write all the code
    from scratch when dealing with a regression problem in TensorFlow: Estimators
    and Keras can assist you in doing that. Estimators are to be found in `tf.estimator`,
    a high-level API in TensorFlow.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，实际上在处理回归问题时，你不需要从零开始编写所有代码：Estimators 和 Keras 可以帮助你完成这项工作。Estimators 位于
    `tf.estimator` 中，这是 TensorFlow 的一个高级 API。
- en: Estimators were introduced in TensorFlow 1.3 (see [https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc2](https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc2))
    as ''**canned Estimators''**, pre-made specific procedures (such as regression
    models or basic neural networks) created to simplify training, evaluation, predicting,
    and the exporting of models for serving. Using pre-made procedures aids development
    in an easier and more intuitive way, leaving the low-level API for customized
    or research solutions (for instance, when you want to test the solutions you found
    in a paper or when your problem requires a completely customized approach). Moreover,
    Estimators are easily deployed on CPUs, GPUs, or TPUs, as well as on a local host
    or on a distributed multi-server environment, without any further code changes
    on your model, making them suitable for ready-to-production use cases. That is
    the reason why Estimators are absolutely not going away anytime soon from TensorFlow,
    even if Keras, as presented in the previous chapter, is the main high-level API
    for TensorFlow 2.x. On the contrary, more and more support and development will
    be made to integrate between Keras and Estimators and you will soon realize in
    our recipes how easily you can turn Keras models into your own custom Estimators.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Estimators 在 TensorFlow 1.3 中首次引入（见 [https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc2](https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc2)）作为
    "**现成 Estimators**"，这些是预先制作的特定程序（例如回归模型或基本神经网络），旨在简化训练、评估、预测以及模型导出以供服务使用。使用现成的程序有助于以更简单、更直观的方式进行开发，而低级
    API 则用于定制或研究解决方案（例如，当你想要测试你在论文中找到的解决方案时，或当你的问题需要完全定制的方法时）。此外，Estimators 可以轻松部署到
    CPU、GPU 或 TPU 上，也可以在本地主机或分布式多服务器环境中运行，而无需对模型进行任何额外的代码更改，这使得它们适用于生产环境的现成使用场景。这也是
    Estimators 不会在短期内从 TensorFlow 中消失的原因，即使 Keras，正如上一章所介绍的，是 TensorFlow 2.x 的主要高级
    API。相反，更多的支持和开发将致力于 Keras 和 Estimators 之间的集成，你将很快在我们的配方中看到，如何轻松地将 Keras 模型转化为你自己的自定义
    Estimators。
- en: 'Four steps are involved in developing an Estimator model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 开发 Estimator 模型涉及四个步骤：
- en: Acquire your data using `tf.data` functions
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tf.data` 函数获取数据
- en: Instantiate the feature column(s)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化特征列
- en: Instantiate and train the Estimator
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化并训练 Estimator
- en: Evaluate the model's performance
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: In our recipes, we will explore all four steps providing you with reusable solutions
    for each.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的配方中，我们将探索这四个步骤，并为每个步骤提供可重用的解决方案。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will loop through batches of data points and let TensorFlow
    update the slope and y intercept. Instead of generated data, we will use the Boston
    Housing dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将循环遍历数据点的批次，并让 TensorFlow 更新斜率和 y 截距。我们将使用波士顿房价数据集，而不是生成的数据。
- en: Originating in the paper by Harrison, D. and Rubinfeld, D.L. *Hedonic Housing
    Prices and the Demand for Clean Air* (J. Environ. Economics & Management, vol.5,
    81-102, 1978), the Boston Housing dataset can be found in many analysis packages
    (such as in scikit-learn) and is present at the UCI Machine Learning Repository,
    as well as at the original StatLib archive (http://lib.stat.cmu.edu/datasets/boston).
    It is a classical dataset for regression problems, but not a trivial one. For
    instance, the samples are ordered and if you do not shuffle the examples randomly,
    you may produce ineffective and biased models when you make a train/test split.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿住房数据集来源于 Harrison, D. 和 Rubinfeld, D.L. 的论文 *Hedonic Housing Prices and the
    Demand for Clean Air*（《环境经济学与管理学杂志》，第5卷，第81-102页，1978年），该数据集可以在许多分析包中找到（例如 scikit-learn
    中），并且存在于 UCI 机器学习库以及原始的 StatLib 存档（http://lib.stat.cmu.edu/datasets/boston）。这是一个经典的回归问题数据集，但并非简单数据集。例如，样本是有顺序的，如果你没有随机打乱样本，进行训练/测试集划分时可能会产生无效且有偏的模型。
- en: Going into the details, the dataset is made up of 506 census tracts of Boston
    from the 1970 census and it features 21 variables regarding various aspects that
    could affect real estate value. The target variable is the median monetary value
    of the houses, expressed in thousands of USD. Among the available features, there
    are a number of obvious ones, such as the number of rooms, the age of the buildings,
    and the crime levels in the neighborhood, and some others that are a bit less
    obvious, such as the pollution concentration, the availability of nearby schools,
    the access to highways, and the distance from employment centers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 详细来说，数据集由1970年波士顿人口普查中的506个普查区组成，包含21个变量，涉及可能影响房地产价值的各个方面。目标变量是房屋的中位数货币价值，单位为千美元。在这些可用的特征中，有一些非常明显的特征，如房间数量、建筑物年龄和邻里犯罪水平，还有一些不那么明显的特征，如污染浓度、附近学校的可用性、高速公路的接入情况和距离就业中心的远近。
- en: Getting back to our solution, specifically, we will find an optimal of the features
    that will assist us in estimating the house prices in Boston. Before talking more
    about the effects of different loss functions on this problem in the next section,
    we are also going to show you how to create a regression Estimator in TensorFlow
    starting from Keras functions, which opens up important customizations for solving
    different problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的解决方案，具体来说，我们将找到一个最优特征集，帮助我们估算波士顿的房价。在下一节讨论不同损失函数对这一问题的影响之前，我们还将展示如何从 Keras
    函数开始创建一个回归 Estimator，在 TensorFlow 中进行自定义，这为解决不同问题提供了重要的自定义选项。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We proceed with the recipe as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤继续进行：
- en: 'We start by loading the necessary libraries, and then load the data in-memory
    using pandas functions. We will also separate predictors from targets (the MEDV,
    median house values) and divide the data into training and test sets:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载必要的库，然后使用 pandas 函数将数据加载到内存中。接下来，我们将预测变量与目标变量（MEDV，中位房价）分开，并将数据划分为训练集和测试集：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then declare two key functions for our recipe:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为我们的方案声明两个关键函数：
- en: '`make_input_fn`, which is a function that creates a `tf.data` dataset from
    a pandas DataFrame turned into a Python dictionary of pandas Series (the features
    are the keys, the values are the feature vectors). It also provides batch size
    definition and random shuffling.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`make_input_fn` 是一个函数，用于从转换为 Python 字典的 pandas DataFrame（特征作为键，值为特征向量的 pandas
    Series）创建一个 `tf.data` 数据集。它还提供批量大小定义和随机打乱功能。'
- en: '`define_feature_columns`, which is a function that maps each column name to
    a specific `tf.feature_column` transformation. `tf.feature_column` is a TensorFlow
    module ([https://www.tensorflow.org/api_docs/python/tf/feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column))
    offering functions that can process any kind of data in a suitable way for being
    inputted into a neural network.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`define_feature_columns` 是一个函数，用于将每一列的名称映射到特定的 `tf.feature_column` 转换。`tf.feature_column`
    是一个 TensorFlow 模块 ([https://www.tensorflow.org/api_docs/python/tf/feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column))，提供能够以适当方式处理各种数据的函数，以便将其输入到神经网络中。'
- en: The `make_input_fn` function is used to instantiate two data functions, one
    for training (the data is shuffled, with a batch size of 256 and set to consume
    1,400 epochs), and one for test (set to a single epoch, no shuffling, so the ordering
    is the original one).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_input_fn` 函数用于实例化两个数据函数，一个用于训练（数据被打乱，批量大小为256，设置为消耗1400个周期），一个用于测试（设置为单个周期，无打乱，因此顺序保持原样）。'
- en: 'The `define_feature_columns` function is used to map the numeric variables
    using the `numeric_column` function ([https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column))
    and the categorical ones using `categorical_column_with_vocabulary_list` ([https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)).
    Both will signal to our Estimator how to handle such data in the optimal manner:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`define_feature_columns`函数用于通过`numeric_column`函数（[https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column)）映射数值变量，通过`categorical_column_with_vocabulary_list`（[https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)）映射类别变量。两者都会告诉我们的估算器如何以最佳方式处理这些数据：'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As a next step, we pass to instantiate the Estimator for a linear regression
    model. We will just recall the formula for the linear model, *y = aX +b*, which
    implies that there is a coefficient for the intercept value and then a coefficient
    for each feature or feature transformation (for instance, categorical data is
    one-hot encoded, so you have a single coefficient for each value of the variable):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是实例化线性回归模型的估算器。我们将回顾线性模型的公式，*y = aX + b*，这意味着存在一个截距值的系数，然后对于每个特征或特征转换（例如，类别数据是独热编码的，因此每个变量值都有一个单独的系数）也会有一个系数：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we just have to train the model and evaluate its performance. The metric
    used is the root mean squared error (the less the better):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要训练模型并评估其性能。使用的指标是均方根误差（越小越好）：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here are the reported results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是报告的结果：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here is a good place to note how to see whether the model is overfitting or
    underfitting the data. If our data is broken into test and training sets, and
    the performance is greater on the training set and lower on the test set, then
    we are overfitting the data. If the accuracy is still increasing on both the test
    and training sets, then the model is underfitting and we should continue training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里是个很好的地方，可以注意如何判断模型是否出现了过拟合或欠拟合。如果我们的数据被分为测试集和训练集，并且在训练集上的表现优于测试集，则说明我们正在过拟合数据。如果准确率在测试集和训练集上都在增加，则说明模型欠拟合，我们应该继续训练。
- en: In our case, the training ended with an average loss of 25.0\. Our test average
    is instead 32.7, implying we have probably overfitted and we should reduce the
    training iterations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，训练结束时的平均损失为25.0。我们的测试集平均损失为32.7，这意味着我们可能已经过拟合，应该减少训练的迭代次数。
- en: We can visualize the performances of the Estimator as it trains the data and
    as it is compared to the test set results. This requires the use of TensorBoard
    ([https://www.tensorflow.org/tensorboard/](https://www.tensorflow.org/tensorboard/)),
    TensorFlow's visualization kit, which will be explained in more detail later in
    the book.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化估算器在训练数据时的表现，以及它与测试集结果的比较。这需要使用TensorBoard（[https://www.tensorflow.org/tensorboard/](https://www.tensorflow.org/tensorboard/)），TensorFlow的可视化工具包，稍后在本书中将详细讲解。
- en: In any event, you can just replicate the visualizations by using the `4\. Linear
    Regression with TensorBoard.ipynb` notebook instead of the `4\. Linear Regression.ipynb`
    version. Both can be found in the book's GitHub repository at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，你只需使用`4\. Linear Regression with TensorBoard.ipynb`笔记本，而不是`4\. Linear Regression.ipynb`版本，就可以重现这些可视化内容。两者都可以在本书的GitHub仓库中找到，链接为[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)。
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\01 Linear regression.png](img/B16254_04_01.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\01 Linear regression.png](img/B16254_04_01.png)'
- en: 'Figure 4.1: TensorBoard visualization of the training loss of the regression
    Estimator'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：回归估算器训练损失的TensorBoard可视化
- en: The visualization shows that the Estimator fitted the problem quickly, reaching
    an optimal value after 1,000 observed batches. Afterward, it oscillated near the
    minimum loss value reached. The test performance, represented by a blue dot, is
    near the best reached value, thereby proving that the model is performing and
    stable even with unseen examples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化结果显示，估计器快速拟合了问题，并在 1,000 个观测批次后达到了最佳值。随后，它围绕已达到的最小损失值波动。由蓝点表示的测试性能接近最佳值，从而证明即使是在未见过的示例上，模型也表现稳定。
- en: How it works...
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The Estimator that calls the proper TensorFlow functionalities, sifts the data
    from the data functions, and converts the data into the proper form based on the
    matched feature name and `tf.feature_column` function does the entire job. All
    that remains is to check the fitting. Actually, the optimal line found by the
    Estimator is not guaranteed to be the line of best fit. Convergence to the line
    of best fit depends on the number of iterations, batch size, learning rate, and
    loss function. It is always good practice to observe the loss function over time
    as this can help you troubleshoot problems or hyperparameter changes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调用适当 TensorFlow 功能的估计器，从数据函数中筛选数据，并根据匹配的特征名称和`tf.feature_column`函数将数据转换为适当的形式，完成了整个工作。剩下的就是检查拟合情况。实际上，估计器找到的最佳拟合线并不保证就是最优拟合线。是否收敛到最优拟合线取决于迭代次数、批量大小、学习率和损失函数。始终建议在训练过程中观察损失函数，因为这有助于排查问题或调整超参数。
- en: There's more...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'If you want to increase the performance of your linear model, interactions
    could be the key. This means that you create a combination between two variables
    and that combination can explain the target better than the features taken singularly.
    In our Boston Housing dataset, combining the average room number in a house and
    the proportion of the lower income population in an area can reveal more about
    the type of neighborhood and help infer the housing value of the area. We combine
    the two just by pointing them out to the `tf.feature_column.crossed_column` function.The
    Estimator, also receiving this output among the features, will automatically create
    the interaction:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想提高线性模型的性能，交互特征可能是关键。这意味着你在两个变量之间创建组合，而这个组合比单独的特征更能解释目标。在我们的波士顿住房数据集中，结合房屋的平均房间数和某个地区低收入人群的比例，可以揭示更多关于邻里类型的信息，并帮助推断该地区的住房价值。我们通过将它们传递给`tf.feature_column.crossed_column`函数来组合这两个特征。估计器在接收这些输出作为特征的一部分时，会自动创建这个交互特征：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here is the plot of the training loss and the resulting test set result.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是训练损失和相应测试集结果的图表。
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\02 Linear regression.png](img/B16254_04_02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\02 Linear regression.png](img/B16254_04_02.png)'
- en: 'Figure 4.2: TensorBoard plot of the regression model with interactions'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：带有交互特征的回归模型的 TensorBoard 图
- en: Observe how the fitting is now faster and much more stable than before, indicating
    that we provided more informative features to the model (the interactions).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 观察现在的拟合速度比之前更快、更稳定，这表明我们为模型提供了更多有用的特征（交互特征）。
- en: 'Another useful recipe function is suitable for handling predictions: the Estimator
    returns them as a dictionary. A simple function will convert everything into a
    more useful array of predictions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的配方函数适用于处理预测：估计器将其作为字典返回。一个简单的函数将把所有内容转换为更有用的预测数组：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Having your predictions as an array will help you to reuse and export the results
    in a more convenient way than a dictionary could.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测结果作为数组有助于你以比字典更方便的方式重用和导出结果。
- en: Turning a Keras model into an Estimator
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Keras 模型转化为估计器
- en: Up to now, we have worked out our linear regression models using specific Estimators
    from the `tf.estimator` module. This has clear advantages because our model is
    mostly run automatically and we can easily deploy it in a scalable way on the
    cloud (such as Google Cloud Platform, offered by Google) and on different kinds
    of servers (CPU-, GPU-, and TPU-based). Anyway, by using Estimators, we may lack
    the flexibility in our model architecture as required by our data problem, which
    is instead offered by the Keras modular approach that we discussed in the previous
    chapter. In this recipe, we will remediate this by showing how we can transform
    Keras models into Estimators and thus take advantage of both the Estimators API
    and Keras versatility at the same time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了`tf.estimator`模块中特定的Estimators来解决我们的线性回归模型。这具有明显的优势，因为我们的模型大部分是自动运行的，并且我们可以轻松地在云端（如Google提供的Google
    Cloud Platform）和不同类型的服务器（基于CPU、GPU和TPU）上进行可伸缩的部署。然而，通过使用Estimators，我们可能会缺乏模型架构的灵活性，而这正是Keras模块化方法所要求的，我们在前一章中已经讨论过。在这个配方中，我们将通过展示如何将Keras模型转换为Estimators来解决这个问题，从而同时利用Estimators
    API和Keras的多样性。
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will use the same Boston Housing dataset as in the previous recipe, while
    also making use of the `make_input_fn` function. As before, we need our core packages
    to be imported:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前一配方中相同的波士顿住房数据集，同时还将利用`make_input_fn`函数。与之前一样，我们需要导入我们的核心包：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will also need to import the Keras module from TensorFlow.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从TensorFlow导入Keras模块。
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Importing `tf.keras` as `keras` will also allow you to easily reuse any previous
    script that you wrote using the standalone Keras package.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将`tf.keras`导入为`keras`还允许您轻松地重用之前使用独立Keras包编写的任何脚本。
- en: How to do it...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Our first step will be to redefine the function creating the feature columns.
    In fact, now we have to specify an input to our Keras model, something that was
    not necessary with native Estimators since they just need a `tf.feature` function
    mapping the feature:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是重新定义创建特征列的函数。事实上，现在我们必须为我们的Keras模型指定一个输入，这是在原生Estimators中不需要的，因为它们只需要一个`tf.feature`函数来映射特征：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The same goes for interactions. Here, too, we need to define the input that
    will be used by our Keras model (in this case, one-hot encoding):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 互动也是一样的。在这里，我们还需要定义将由我们的Keras模型使用的输入（在本例中是独热编码）：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After preparing the necessary inputs, we can proceed to the model itself. The
    inputs will be collected in a feature layer that will pass the data to a `batchNormalization`
    layer, which will automatically standardize it. After that the data will be directed
    to the output node, which will produce the numeric output.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好必要的输入后，我们可以开始模型本身。这些输入将被收集在一个特征层中，该层将数据传递给一个`batchNormalization`层，该层将自动标准化数据。然后数据将被导向输出节点，该节点将生成数值输出。
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point, having set all the necessary inputs, new functions are created
    and we can run them:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，已经设置了所有必要的输入，新函数被创建，我们可以运行它们：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have now obtained a working Keras model. We can convert it into an Estimator
    using the `model_to_estimator` function. This requires the establishment of a
    temporary directory for the Estimator''s outputs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了一个工作的Keras模型。我们可以使用`model_to_estimator`函数将其转换为Estimator。这需要为Estimator的输出建立一个临时目录：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Having `canned` the Keras model into an Estimator, we can proceed as before
    to train the model and evaluate the results.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将Keras模型转换为Estimator后，我们可以像以前一样训练模型并评估结果。
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we plot the fitting process using TensorBoard, we will observehow the
    training trajectory is quite similar to the one obtained by previous Estimators:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用TensorBoard绘制拟合过程时，我们将观察到训练轨迹与之前Estimators获得的轨迹非常相似：
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\03 canned linear regression.png](img/B16254_04_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\03 canned linear regression.png](img/B16254_04_03.png)'
- en: 'Figure 4.3: Canned Keras linear Estimator training'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：使用Keras线性Estimator进行训练
- en: Canned Keras Estimators are indeed a quick and robust way to bind together the
    flexibility of user-defined solutions by Keras and the high-performance training
    and deployment from Estimators.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Canned Keras Estimators确实是将Keras用户定义解决方案的灵活性与Estimators高性能训练和部署结合在一起的快速而健壮的方法。
- en: How it works...
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `model_to_estimator` function is not a wrapper of your Keras model. Instead,
    it parses your model and transforms it into a static TensorFlow graph, allowing
    distributed training and scaling for your model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_to_estimator`函数并不是Keras模型的包装器。相反，它解析你的模型并将其转换为静态TensorFlow图，从而实现分布式训练和模型扩展。'
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'One great advantage of using linear models is to be able to explore their weights
    and get an idea of what feature is producing the result we obtained. Each coefficient
    will tell us, given the fact that the inputs are standardized by the batch layer,
    how that feature is impacted with respect to the others (the coefficients are
    comparable in terms of absolute value) and whether it is adding or subtracting
    from the result (given a positive or negative sign):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性模型的一个重要优势是能够探索其权重，并了解哪个特征对我们获得的结果产生了影响。每个系数会告诉我们，鉴于输入在批处理层被标准化，特征相对于其他特征的影响（系数在绝对值上是可以比较的），以及它是增加还是减少结果（根据正负符号）：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Anyway, if we extract the weights from our model we will find out that we cannot
    easily interpret them because they have no labels and the dimensionality is different
    since the `tf.feature` functions have applied different transformations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，如果我们从模型中提取权重，我们会发现无法轻松解释它们，因为它们没有标签且维度不同，因为`tf.feature`函数应用了不同的转换。
- en: 'We need a function that can extract the correct labels from our feature columns
    as we mapped them prior to feeding them to our canned Estimator:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来提取从特征列中正确的标签，因为在将它们输入到预设估算器之前，我们已经将它们映射过：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function only works with TensorFlow version 2.2 or later because in earlier
    TensorFlow 2.x versions the `get_config` method was not present in `tf.feature`
    objects.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数仅适用于TensorFlow 2.2或更高版本，因为在早期的TensorFlow 2.x版本中，`get_config`方法在`tf.feature`对象中并不存在。
- en: 'Now we can extract all the labels and meaningfully match each weight in the
    output to its respective feature:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以提取所有标签，并有意义地将每个输出中的权重与其相应的特征匹配：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Once you have the weights, you can easily get the contribution of each feature
    to the result by observing the sign and the magnitude of each coefficient. The
    scale of the feature can, however, influence the magnitude unless you previously
    statistically standardized the features by subtracting the mean and dividing by
    the standard deviation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你得到了权重，就可以通过观察每个系数的符号和大小，轻松地得出每个特征对结果的贡献。然而，特征的尺度可能会影响大小，除非你事先通过减去均值并除以标准差对特征进行了统计标准化。
- en: Understanding loss functions in linear regression
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线性回归中的损失函数
- en: It is important to know the effect of loss functions in algorithm convergence.
    Here, we will illustrate how the L1 and L2 loss functions affect convergence and
    predictions in linear regression. This is the first customization that we are
    applying to our canned Keras Estimator. More recipes in this chapter will enhance
    that initial Estimator by adding more functionality.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 了解损失函数对算法收敛性的影响非常重要。在这里，我们将说明L1和L2损失函数如何影响线性回归的收敛性和预测。这是我们对预设Keras估算器进行的第一次自定义。本章的更多配方将在此基础上通过添加更多功能来增强该初始估算器。
- en: Getting ready
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will use the same Boston Housing dataset as in the previous recipe, as well
    as utilize the following functions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前面配方中相同的波士顿房价数据集，并使用以下函数：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: However, we will change our loss functions and learning rates to see how convergence
    changes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将更改损失函数和学习率，看看收敛性如何变化。
- en: How to do it...
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We proceed with the recipe as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按如下方式继续配方：
- en: 'The start of the program is the same as the last recipe. We therefore load
    the necessary packages and also we download the Boston Housing dataset, if it
    is not already available:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的开始与上一个配方相同。因此，我们加载必要的包，并且如果波士顿房价数据集尚不可用，我们将下载它：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After that, we need to redefine our `create_linreg` by adding a new parameter
    controlling the type of loss. The default is still the mean squared error (L2
    loss), but now it can be easily changed when instantiating the canned Estimator:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们需要重新定义`create_linreg`，通过添加一个新的参数来控制损失类型。默认值仍然是均方误差（L2损失），但现在在实例化预设估算器时可以轻松更改：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After doing so, we can train our model explicitly using the `Ftrl` optimizer
    with a different learning rate, more suitable for an L1 loss (we set the loss
    to mean absolute error):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做之后，我们可以通过使用不同学习率的`Ftrl`优化器显式地训练我们的模型，更适合L1损失（我们将损失设置为平均绝对误差）：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here are the results that we obtained by switching to an L1 loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们通过切换到 L1 损失函数得到的结果：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can now visualize the training performances along iterations using TensorBoard:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 TensorBoard 来可视化训练过程中的性能：
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\04 understanding loss.png](img/B16254_04_04.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\04 understanding loss.png](img/B16254_04_04.png)'
- en: 'Figure 4.4: Mean absolute error optimization'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：均方误差优化
- en: The resulting plot shows a nice descent of the mean absolute error, which simply
    slows down after 400 iterations and tends to stabilize in a plateau after 1,400
    iterations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示了均方误差的良好下降，直到 400 次迭代后减慢，并在 1,400 次迭代后趋于稳定，形成一个平台。
- en: How it works...
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运作……
- en: When choosing a loss function, we must also choose a corresponding learning
    rate that will work with our problem. Here, we test two situations, the first
    in which L2 is adopted and the second in which L1 is preferred.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择损失函数时，我们还必须选择一个相应的学习率，以确保其与我们的问题匹配。在这里，我们测试了两种情况，第一种是采用 L2，第二种是首选 L1。
- en: If our learning rate is small, our convergence will take more time. However,
    if our learning rate is too large, we will have issues with our algorithm never
    converging.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的学习率较小，收敛过程将需要更多时间。然而，如果学习率过大，我们的算法可能会无法收敛。
- en: There's more...
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'To understand what is happening, we should look at how a large learning rate
    and small learning rate act on **L1 norms** and **L2 norms**. If the rate is too
    large, L1 can get stuck at a suboptimal result, whereas L2 can achieve an even
    worse performance. To visualize this, we will look at a one-dimensional representation
    of learning steps on both norms, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解发生了什么，我们应当观察大学习率和小学习率对**L1 范数**和**L2 范数**的作用。如果学习率过大，L1 可能会停滞在次优结果，而 L2
    可能会得到更差的性能。为了可视化这一点，我们将查看关于两种范数的学习步长的一维表示，如下所示：
- en: '![](img/B16254_04_05.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_04_05.png)'
- en: 'Figure 4.5: What can happen with the L1 and L2 norm with larger and smaller
    learning rates'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：L1 和 L2 范数在较大和较小学习率下的表现
- en: Small learning rates, as depicted in the preceding diagram, are indeed a guarantee
    of a better optimization in any case. Larger rates do not really work with L2,
    but may prove just suboptimal with L1 by stopping further optimizations after
    a while, without causing any further damage.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示，小学习率确实能保证更好的优化结果。较大的学习率与 L2 不太适用，但可能对 L1 证明是次优的，因为它会在一段时间后停止进一步优化，而不会造成更大的损害。
- en: Implementing Lasso and Ridge regression
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Lasso 和 Ridge 回归
- en: There are ways to limit the influence of coefficients on the regression output.
    These methods are called regularization methods, and two of the most common regularization
    methods are Lasso and Ridge regression. We cover how to implement both of these
    in this recipe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以限制系数对回归输出的影响。这些方法被称为正则化方法，其中最常见的两种正则化方法是 Lasso 和 Ridge 回归。在本食谱中，我们将讲解如何实现这两种方法。
- en: Getting ready
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Lasso and Ridge regression are very similar to regular linear regression, except
    that we add regularization terms to limit the slopes (or partial slopes) in the
    formula. There may be multiple reasons for this, but a common one is that we wish
    to restrict the number of features that have an impact on the dependent variable.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 和 Ridge 回归与普通线性回归非常相似，不同之处在于我们在公式中加入了正则化项，以限制斜率（或偏斜率）。这背后可能有多个原因，但常见的一个原因是我们希望限制对因变量有影响的特征数量。
- en: How to do it...
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We proceed with the recipe as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤继续进行：
- en: 'We will use the Boston Housing dataset again and set up our functions in the
    same way as in the previous recipes. In particular we need `define_feature_columns_layers`,
    `make_input_fn`, and `create_interactions`. We again first load the libraries,
    and then we define a new `create_ridge_linreg` where we set a new Keras model
    using `keras.regularizers.l2` as the `regularizer` of our dense layer:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用波士顿房价数据集，并按照之前的食谱设置函数。特别是，我们需要`define_feature_columns_layers`、`make_input_fn`
    和 `create_interactions`。我们再次首先加载库，然后定义一个新的 `create_ridge_linreg`，在其中我们使用`keras.regularizers.l2`作为我们密集层的`regularizer`来设置一个新的
    Keras 模型：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once this is done, we can again run our previous linear model with L1 loss
    and see the results improve:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们可以再次运行之前的线性模型，并使用 L1 损失来观察结果的改进：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here are the Ridge regression results:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Ridge 回归的结果：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In addition, here is the plot of the training using TensorBoard:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里是使用 TensorBoard 进行训练的图表：
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\05 ridge.png](img/B16254_04_06.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\05 ridge.png](img/B16254_04_06.png)'
- en: 'Figure 4.6: Ridge regression training loss'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：Ridge 回归训练损失
- en: 'We can also replicate that for L1 regularization by creating a new function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过创建一个新函数来复制 L1 正则化：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here are the results obtained from the L1 Lasso regression:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从 L1 Lasso 回归得到的结果：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In addition, here is the plot of the training loss:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里是训练损失的图表：
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\06 Lasso.png](img/B16254_04_07.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\06 Lasso.png](img/B16254_04_07.png)'
- en: 'Figure 4.7: Lasso regression training loss'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：Lasso 回归训练损失
- en: Comparing the Ridge and Lasso approach, we notice that they are not too dissimilar
    in terms of training loss, but the test result favors Lasso. This could be explained
    by a noisy variable that had to be excluded in order for the model to improve,
    since Lasso routinely excludes non-useful variables from the prediction estimation
    (by assigning a zero coefficient to them), whereas Ridge just down-weights them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 Ridge 和 Lasso 方法时，我们注意到它们在训练损失方面没有太大差异，但测试结果偏向 Lasso。这可能是由于一个噪声变量必须被排除，才能让模型得到改进，因为
    Lasso 会定期从预测估计中排除无用的变量（通过赋予它们零系数），而 Ridge 只是对它们进行下调权重。
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We implement Lasso regression by adding a continuous Heaviside step function
    to the loss function of linear regression. Owing to the steepness of the step
    function, we have to be careful with step size. Too big a step size and it will
    not converge. For Ridge regression, see the change required in the next section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向线性回归的损失函数添加一个连续的 Heaviside 阶跃函数来实现 Lasso 回归。由于阶跃函数的陡峭性，我们必须小心步长。如果步长太大，模型将无法收敛。关于
    Ridge 回归，请参见下一节所需的更改。
- en: There's more...
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Elastic net regression is a type of regression that combines Lasso regression
    with Ridge regression by adding L1 and L2 regularization terms to the loss function.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网回归是一种回归方法，通过将 Lasso 回归与 Ridge 回归结合，在损失函数中添加 L1 和 L2 正则化项。
- en: Implementing elastic net regression is straightforward following the previous
    two recipes, because you just need to change the regularizer.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 实现弹性网回归很简单，跟随前两个方法，因为你只需更改正则化器。
- en: 'We just create a `create_elasticnet_linreg` function, which picks up as parameters
    the values of L1 and L2 strengths:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需创建一个 `create_elasticnet_linreg` 函数，它将 L1 和 L2 强度的值作为参数传入：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we re-run the complete steps for training from data and obtain an
    evaluation of the model''s performances:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们重新运行从数据开始的完整训练步骤，并评估模型的性能：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here are the results:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是得到的结果：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here is the training loss plot for the ElasticNet model:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ElasticNet 模型的训练损失图：
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\07 ElasticNet.png](img/B16254_04_08.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\07 ElasticNet.png](img/B16254_04_08.png)'
- en: 'Figure 4.8: ElasticNet training loss'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：ElasticNet 训练损失
- en: The test results obtained do not differ too much from Ridge and Lasso, landing
    somewhere between them. As stated previously, the problem involves removing variables
    from the dataset in order to improve the performances, and as we've now seen the
    Lasso model is the best for doing so.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的测试结果与 Ridge 和 Lasso 相差不大，位于它们之间。如前所述，问题在于从数据集中去除变量以提高性能，正如我们现在所看到的，Lasso
    模型是最适合执行这一任务的。
- en: Implementing logistic regression
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现逻辑回归
- en: For this recipe, we will implement logistic regression to predict the probability
    of breast cancer using the Breast Cancer Wisconsin dataset ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))).
    We will be predicting the diagnosis from features that are computed from a digitized
    image of a **fine needle aspiration** (**FNA**) of a breast mass. An FNA is a
    common breast cancer test, consisting of a small tissue biopsy that can be examined
    under a microscope.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将实现逻辑回归，利用乳腺癌威斯康星数据集（[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)))来预测乳腺癌的概率。我们将从基于乳腺肿块的**细针穿刺**（**FNA**）图像计算得到的特征中预测诊断结果。FNA是一个常见的乳腺癌检测方法，通过小量组织活检进行，活检样本可以在显微镜下进行检查。
- en: The dataset can immediately be used for a classification model, without further
    transformations, since the target variable consists of 357 benign cases and 212
    malignant ones. The two classes do not have the exact same consistency (an important
    requirement when doing binary classification with regression models), but they
    are not extremely different, allowing us to build a straightforward example and
    evaluate it using plain accuracy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以直接用于分类模型，无需进一步转换，因为目标变量由357个良性病例和212个恶性病例组成。这两个类别的样本数并不完全相同（在进行二分类回归模型时，这是一个重要要求），但它们的差异并不极端，使我们能够构建一个简单的例子并使用普通准确率来评估模型。
- en: 'Please remember to check whether your classes are balanced (in other words,
    having approximately the same number of cases), otherwise you will have to apply
    specific recipes to balance the cases, such as applying weights, or your model
    may provide inaccurate predictions (you can refer to the following Stack Overflow
    question if you just need further details: [https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras)).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请记得检查你的类别是否平衡（换句话说，是否具有大致相同数量的样本），否则你需要采取特定的方法来平衡样本，例如应用权重，或者你的模型可能会提供不准确的预测（如果你需要更多细节，可以参考以下Stack
    Overflow问题：[https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras)）。
- en: Getting ready
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Logistic regression is a way to turn linear regression into a binary classification.
    This is accomplished by transforming the linear output into a sigmoid function
    that scales the output between zero and one. The target is a zero or one, which
    indicates whether a data point is in one class or another. Since we are predicting
    a number between zero and one, the prediction is classified into class value 1
    if the prediction is above a specified cut-off value, and class 0 otherwise. For
    the purpose of this example, we will specify that cutoff to be 0.5, which will
    make the classification as simple as rounding the output.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种将线性回归转化为二分类的方法。这是通过将线性输出转换为一个 sigmoid 函数来实现的，该函数将输出值缩放到0和1之间。目标值是0或1，表示一个数据点是否属于某个类别。由于我们预测的是一个0到1之间的数值，当预测值超过指定的阈值时，预测结果被分类为类别1，否则为类别0。对于本例，我们将阈值设置为0.5，这样分类就变得和四舍五入输出一样简单。
- en: When classifying, anyway, sometimes you need to control the kinds of mistakes
    you make, and this is especially true for medical applications (such as the example
    we are proposing), but it may be a sensible problem for other ones, too (for example,
    in the case of fraud detection in the insurance or banking sectors). In fact,
    when you classify, you get correct guesses, but also **false positives** and **false
    negatives**. False positives are the errors the model makes when it predicts a
    positive (class 1), but the true label is negative. False negatives, on the other
    hand, are cases labeled by the model as negative when they are actually positive.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类时，无论如何，有时候你需要控制自己犯的错误，这对于医疗应用（比如我们提出的这个例子）尤其重要，但对于其他应用（例如保险或银行领域的欺诈检测），这也是一个值得关注的问题。事实上，在分类时，你会得到正确的预测结果，但也会有**假阳性**和**假阴性**。假阳性是模型在预测为阳性（类别1）时，真实标签却为阴性所产生的错误。假阴性则是模型将实际为阳性的样本预测为阴性。
- en: When using a 0.5 threshold for deciding the class (positive or negative class),
    you are actually equating the expectations for false positives and false negatives.
    In reality, according to your problem, false positive and false negative errors
    may have different consequences. In the case of detecting cancer, clearly you
    absolutely do not want false negatives because that would mean predicting a patient
    as healthy when they are instead facing a life-threatening situation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用0.5阈值来决定类别（正类或负类）时，实际上你是在平衡假阳性和假阴性的期望值。实际上，根据你的问题，假阳性和假阴性错误可能会有不同的后果。例如，在癌症检测的情况下，显然你绝对不希望发生假阴性错误，因为这意味着将一个实际患病的病人误判为健康，可能导致生命危险。
- en: By setting the classification threshold higher or lower, you can trade-off false
    positives for false negatives. Higher thresholds will have more false negatives
    than false positives. Lower ones will have fewer false negatives but more false
    positives. For our recipe, we will just use the 0.5 threshold, but please be aware
    that the threshold is also something you have to consider for your model's real-world
    applications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置更高或更低的分类阈值，你可以在假阳性和假阴性之间进行权衡。较高的阈值将导致更多的假阴性，而假阳性较少。较低的阈值将导致较少的假阴性，但假阳性更多。对于我们的示例，我们将使用0.5的阈值，但请注意，阈值也是你需要考虑的因素，尤其是在模型的实际应用中。
- en: How to do it...
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We proceed with the recipe as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤继续进行示例：
- en: 'We start by loading the libraries and recovering the data from the internet:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载库并从互联网恢复数据：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we specify the logistic regression function. The main modification with
    respect to our linear regression model is that we change the activation in the
    single output neuron from `linear` to `sigmoid`, which is enough to obtain a logistic
    regression because our output will be a probability expressed in the range 0.0
    to 1.0:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定逻辑回归函数。与我们之前的线性回归模型相比，主要的修改是将单个输出神经元的激活函数从`linear`改为`sigmoid`，这就足够让我们得到一个逻辑回归模型，因为我们的输出将是一个概率值，范围从0.0到1.0：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we run our procedure:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行我们的程序：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the reported accuracy of our logistic regression:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们逻辑回归的准确率报告：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In addition, here you can find the loss plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以在这里找到损失图：
- en: '![](img/B16254_04_09.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_04_09.png)'
- en: 'Figure 4.9: TensorBoard plot of training loss for a logistic regression model'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：逻辑回归模型的TensorBoard训练损失图
- en: Using a few commands, we achieved a good result in terms of accuracy and loss
    for this problem, in spite of a slightly unbalanced target class (more benign
    cases than malignant ones).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几个命令，我们在准确率和损失方面取得了不错的结果，尽管目标类别略有不平衡（良性病例比恶性病例更多）。
- en: How it works...
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Logistic regression predictions are based on the sigmoid curve and, to modify
    our previous linear model accordingly, we just need to switch to a sigmoid activation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的预测基于sigmoid曲线，若要相应地修改我们之前的线性模型，我们只需要切换到sigmoid激活函数。
- en: There's more...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: When you are predicting a multi-class or multi-label you don't need to extend
    the binary model using different kinds of **One Versus All** (**OVA**) strategies,
    but you just need to extend the number of output nodes to match the number of
    classes you need to predict. Using multiple neurons with sigmoid activation, you
    will obtain a multi-label approach, while using a softmax activation, you'll get
    a multi-class prediction. You will find more recipes in the later chapters of
    this book that indicate how to do this using simple Keras functions.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在进行多类或多标签预测时，你不需要通过不同类型的**一对多**（**OVA**）策略来扩展二分类模型，而只需要扩展输出节点的数量，以匹配你需要预测的类别数。使用带有sigmoid激活函数的多个神经元，你将得到一个多标签方法，而使用softmax激活函数，你将获得一个多类预测。你将在本书的后续章节中找到更多的示例，说明如何使用简单的Keras函数来实现这一点。
- en: Resorting to non-linear solutions
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻求非线性解决方案
- en: 'Linear models are approachable and interpretable, given the one-to-one relation
    between feature columns and regression coefficients. Sometimes, anyway, you may
    want to try non-linear solutions in order to check whether models that are more
    complex can model your data better and solve your prediction problem in a more
    expert manner. **Support Vector Machines** (**SVMs**) are an algorithm that rivaled
    neural networks for a long time and they are still a viable option thanks to recent
    developments in terms of random features for large-scale kernel machines (Rahimi,
    Ali; Recht, Benjamin. Random features for large-scale kernel machines. In: *Advances
    in neural information processing systems*. 2008\. pp. 1177-1184). In this recipe,
    we demonstrate how to leverage Keras and obtain a non-linear solution to a classification
    problem.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '线性模型具有较强的可接近性和可解释性，因为特征列与回归系数之间存在一对一的关系。然而，有时候你可能希望尝试非线性解法，以检查更复杂的模型是否能更好地拟合你的数据，并以更专业的方式解决你的预测问题。**支持向量机**（**SVMs**）是一种与神经网络竞争了很长时间的算法，且由于最近在大规模核机器的随机特征方面的进展，它们仍然是一个可行的选择（Rahimi,
    Ali; Recht, Benjamin. Random features for large-scale kernel machines. In: *Advances
    in neural information processing systems*. 2008\. 第1177-1184页）。在本示例中，我们展示了如何利用Keras获得非线性解法来解决分类问题。'
- en: Getting ready
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will still be using functions from the previous recipes, including `define_feature_columns_layers`
    and `make_input_fn`. As in the logistic regression recipe, we will continue using
    the breast cancer dataset. As before, we need to load the following packages:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然会使用之前示例中的函数，包括`define_feature_columns_layers`和`make_input_fn`。和逻辑回归示例一样，我们将继续使用乳腺癌数据集。和以前一样，我们需要加载以下包：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: At this point we are ready to proceed with the recipe.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好继续执行这个步骤。
- en: How to do it...
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: In addition to the previous packages, we also specifically import the `RandomFourierFeatures`
    function, which can apply a non-linear transformation to the input. Depending
    on the loss function, a `RandomFourierFeatures` layer can approximate kernel-based
    classifiers and regressors. After this, we just need to apply our usual single-output
    node and get our predictions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前的包外，我们还专门导入了`RandomFourierFeatures`函数，它可以对输入进行非线性变换。根据损失函数，`RandomFourierFeatures`层可以逼近基于核的分类器和回归器。之后，我们只需要应用我们通常的单输出节点并获取预测结果。
- en: 'Depending on the TensorFlow 2.x version you are using you may need to import
    it from different modules:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的TensorFlow 2.x版本，你可能需要从不同的模块导入它：
- en: '[PRE36]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we develop the `create_svc` function. It contains an L2 regularizer for
    the final dense node, a batch normalization layer for the input, and a `RandomFourierFeatures`
    layer inserted among them. In this intermediate layer non-linearities are generated
    and you can set the `output_dim` parameter in order to determine the number of
    non-linear interactions that will be produced by the layers. Naturally, you can
    contrast the overfitting caused after setting higher `output_dim` values by raising
    the L2 regularization value, thereby achieving more regularization:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开发`create_svc`函数。它包含了一个L2正则化项用于最终的全连接节点，一个用于输入的批归一化层，以及一个插入其中的`RandomFourierFeatures`层。在这个中间层中，产生了非线性特征，你可以通过设置`output_dim`参数来确定层生成的非线性交互的数量。当然，你可以通过增加L2正则化值来对比在设置较高`output_dim`值后出现的过拟合，从而实现更多的正则化：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As in the previous recipes, we define the different columns, we set the model
    and the optimizer, we prepare the input function, and finally we train and evaluate
    the results:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的示例一样，我们定义了不同的列，设置了模型和优化器，准备了输入函数，最后我们训练并评估结果：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is the reported accuracy. For an even better result, you have to try different
    combinations of the output dimension of the `RandomFourierFeatures` layer and
    the regularization term:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是报告的准确度。为了获得更好的结果，你需要尝试不同的`RandomFourierFeatures`层的输出维度和正则化项的组合：
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here is the loss plot from TensorBoard:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自TensorBoard的损失图：
- en: '![](img/B16254_04_10.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_04_10.png)'
- en: 'Figure 4.10: Loss plot for the RandomFourierFeatures-based model'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：基于RandomFourierFeatures的模型的损失图
- en: The plot is indeed quite nice, thanks to the fact that we used a larger batch
    than usual. Given the complexity of the task, due to the large number of neurons
    to be trained, a larger batch generally works better than a smaller one.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了比平常更大的批次，图形的效果确实相当好。由于任务的复杂性（需要训练大量的神经元），通常更大的批次会比较小的批次效果更好。
- en: How it works...
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Random Fourier features are a way to approximate the work done by SVM kernels,
    thereby achieving a lower computational complexity and making such an approach
    also feasible for a neural network implementation. If you require a more in-depth
    explanation, you can read the original paper, quoted at the beginning of the recipe,
    or you can take advantage of this very clear answer on Stack Exchange: [https://stats.stackexchange.com/questions/327646/how-does-a-random-kitchen-sink-work#327961](https://stats.stackexchange.com/questions/327646/how-does-a-random-kitchen-sink-work#327961).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 随机傅里叶特征是一种近似支持向量机（SVM）核函数的方法，从而实现更低的计算复杂度，并使得这种方法在神经网络实现中也变得可行。如果你需要更深入的解释，可以阅读本文开头引用的原始论文，或者你可以参考这个非常清晰的Stack
    Exchange回答：[https://stats.stackexchange.com/questions/327646/how-does-a-random-kitchen-sink-work#327961](https://stats.stackexchange.com/questions/327646/how-does-a-random-kitchen-sink-work#327961)。
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Depending on the loss function, you can obtain different non-linear models:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 根据损失函数的不同，你可以得到不同的非线性模型：
- en: '**Hinge loss** sets your model in an SVM'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**铰链损失**将你的模型设定为支持向量机（SVM）。'
- en: '**Logistic loss** turns your model into kernel logistic regression (classification
    performance is almost the same as SVM, but kernel logistic regression can provide
    class probabilities)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑损失**将你的模型转化为核逻辑回归模型（分类性能几乎与支持向量机（SVM）相同，但核逻辑回归可以提供类别概率）。'
- en: '**Mean squared error** transforms your model into a kernel regression'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**将你的模型转化为一个核回归模型。'
- en: It is up to you to decide what loss to try first, and decide how to set the
    dimension of the output from the random Fourier transformation. By way of a general
    suggestion you could start with a large number of output nodes and iteratively
    test whether shrinking their number improves the result.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由你决定首先尝试哪种损失函数，并决定如何设置来自随机傅里叶变换的输出维度。一般建议是，你可以从较多的输出节点开始，并逐步测试减少节点数量是否能改善结果。
- en: Using Wide & Deep models
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用宽深模型
- en: 'Linear models can boast a great advantage over complex models: they are efficient
    and easily interpretable, even when you work with many features and with features
    that interact with each other. Google researchers mentioned this aspect as the
    power of **memorization** because your linear model records the association between
    the features and the target into single coefficients. On the other hand, neural
    networks are blessed with the power of **generalization**, because in their complexity
    (they use multiple layers of weights and they interrelate each input), they can
    manage to approximate the general rules that govern the outcome of a process.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型相较于复杂模型有一个巨大优势：它们高效且易于解释，即使在你使用多个特征且特征之间存在相互作用时也如此。谷歌研究人员提到这一点作为**记忆化**的力量，因为你的线性模型将特征与目标之间的关联记录为单一系数。另一方面，神经网络具备**泛化**的力量，因为在其复杂性中（它们使用多个权重层并且相互关联每个输入），它们能够近似描述支配过程结果的一般规则。
- en: 'Wide & Deep models, as conceived by Google researchers ([https://arxiv.org/abs/1606.07792](https://arxiv.org/abs/1606.07792)),
    can blend memorization and generalization because they combine a linear model,
    applied to numeric features, together with generalization, applied to sparse features,
    such as categories encoded into a sparse matrix. Therefore, **wide** in their
    name implies the regression part, and **deep** the neural network aspect:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 宽深模型，如谷歌研究人员所构想的那样（[https://arxiv.org/abs/1606.07792](https://arxiv.org/abs/1606.07792)），能够融合记忆化和泛化，因为它们将线性模型（应用于数值特征）与泛化（应用于稀疏特征，例如编码为稀疏矩阵的类别特征）结合在一起。因此，名称中的**宽**指的是回归部分，**深**指的是神经网络部分：
- en: '![https://1.bp.blogspot.com/-Dw1mB9am1l8/V3MgtOzp3uI/AAAAAAAABGs/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB/s640/image04.png](img/B16254_04_11.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![https://1.bp.blogspot.com/-Dw1mB9am1l8/V3MgtOzp3uI/AAAAAAAABGs/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB/s640/image04.png](img/B16254_04_11.png)'
- en: 'Figure 4.11: How wide models (linear models) blend with neural networks in
    Wide & Deep models (from the paper by Cheng, Heng-Tze, et al. "Wide & deep learning
    for recommender systems." Proceedings of the 1st workshop on deep learning for
    recommender systems. 2016)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：宽模型（线性模型）如何与神经网络在宽深模型中融合（摘自 Cheng, Heng-Tze 等人的论文《Wide & deep learning
    for recommender systems》，《第1届深度学习推荐系统研讨会论文集》，2016年）。
- en: Such a blend can achieve the best results when working on recommender system
    problems (such as the one featured in Google Play). Wide & Deep models work the
    best in recommendation problems because each part handles the right kind of data.
    The **wide** part handles the features relative to the user's characteristics
    (dense numeric features, binary indicators, or their combination in interaction
    features) that are more stable over time, whereas the **deep** part processes
    feature strings representing previous software downloads (sparse inputs on very
    large matrices), which instead are more variable over time and so require a more
    sophisticated kind of representation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的结合可以在处理推荐系统问题时取得最佳结果（例如 Google Play 中展示的推荐系统）。Wide & Deep 模型在推荐问题中表现最好，因为每个部分处理的是正确类型的数据。**宽度**部分处理与用户特征相关的特征（密集的数值特征、二进制指示符或它们在交互特征中的组合），这些特征相对稳定；而**深度**部分处理表示先前软件下载的特征字符串（稀疏输入在非常大的矩阵中），这些特征随着时间变化而更为不稳定，因此需要一种更复杂的表示方式。
- en: Getting ready
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Actually, Wide & Deep models also work fine with many other data problems, recommender
    systems being their speciality, and such models are readily available among Estimators
    (see [https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedEstimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedEstimator)).
    In this recipe we will use a mixed data dataset, the **Adult dataset** ([https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)).
    Also widely known as the **Census dataset**, the purpose of this dataset is to
    predict whether your income exceeds $50K/annum based on census data. The available
    features are quite varied, from continuous values related to age to variables
    with a large number of classes, including occupation. We will then use each different
    type of feature to feed the correct part of the Wide & Deep model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Wide & Deep 模型同样适用于许多其他数据问题，推荐系统是它们的专长，且此类模型在 Estimators 中已经很容易获得（见[https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedEstimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedEstimator)）。在这个示例中，我们将使用一个混合数据集，**成人数据集**([https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult))。该数据集也被广泛称为**人口普查数据集**，其目的是预测基于人口普查数据，您的年收入是否超过
    5 万美元。可用的特征种类非常多样，从与年龄相关的连续值，到具有大量类别的变量，包括职业。然后，我们将使用每种不同类型的特征，输入到 Wide & Deep
    模型的正确部分。
- en: How to do it...
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We start by downloading the Adult dataset from the UCI archive:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从 UCI 存档中下载成人数据集：
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, we select a subset of features for our purposes and we extract the target
    variable and transform it from the string type to the int type:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们根据需要选择特征子集，并提取目标变量，将其从字符串类型转换为整数类型：
- en: '[PRE41]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This dataset requires additional manipulation since some fields present missing
    values. We treat them by replacing missing values with a mean value. As a general
    rule, we have to impute all missing data before feeding it into a TensorFlow model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集需要额外处理，因为某些字段存在缺失值。我们通过用均值替换缺失值来处理这些数据。作为一般规则，我们必须在将数据输入 TensorFlow 模型之前，填补所有缺失的数据：
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can proceed to define columns by means of the proper `tf.feature_column`
    function:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过正确的 `tf.feature_column` 函数来定义列：
- en: '**Numeric columns**: dealing with numeric values (such as the age)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值列**：处理数值型数据（如年龄）'
- en: '**Categorical columns**: dealing with categorical values when the unique categories
    are just a few in number (such as the gender)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类列**：处理分类值，当唯一类别数量较少时（如性别） '
- en: '**Embeddings**: dealing with categorical values when unique categories are
    many in number by mapping categorical values into a dense, low-dimensional, numeric
    space'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**：处理当唯一类别数量较多时的分类值，将分类值映射到一个密集的低维数值空间'
- en: 'We also define the function that faciliates the interaction of categorical
    and numeric columns:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个函数，用于简化分类列和数值列之间的交互：
- en: '[PRE43]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that all the functions have been defined, we map the different columns
    and add some meaningful interaction (such as crossing education with occupation).
    We map high-dimensional categorical features into a fixed lower-dimensional numeric
    space of 32 dimensions by setting the dimension parameter:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有函数已经定义完毕，我们将不同的列进行映射，并添加一些有意义的交互（例如将教育与职业进行交叉）。我们通过设置维度参数，将高维分类特征映射到一个固定的32维低维数值空间：
- en: '[PRE44]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Having mapped the features, we then input them into our estimator (see [https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)),
    specifying the feature columns to be handled by the wide part and those by the
    deep part. For each part we also specify an optimizer (usually Ftrl for the linear
    part and Adam for the deep part) and, for the deep part, we specify the architecture
    of hidden layers as a list of numbers of neurons:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 映射完特征后，我们将它们输入到我们的估计器中（参见[https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)），并指定由宽部分处理的特征列和由深部分处理的特征列。对于每个部分，我们还指定优化器（通常线性部分使用Ftrl，深部分使用Adam），并且对于深部分，我们指定隐藏层的架构，作为一个神经元数量的列表：
- en: '[PRE45]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We then proceed to define the input function (no different to what we have
    done in the other recipes presented in this chapter):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续定义输入函数（与本章其他食谱中所做的没有不同）：
- en: '[PRE46]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we train the Estimator for 1,500 steps and evaluate the results on
    the test data:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练Estimator 1,500步并在测试数据上评估结果：
- en: '[PRE47]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We obtain an accuracy of about 0.83 on our test set, as reported using the
    evaluate method on the Estimator:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上获得了约0.83的准确率，这是通过使用Estimator的evaluate方法报告的：
- en: '[PRE48]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Here is the plot of the training loss and the test estimate (the blue dot):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练损失和测试估计（蓝点）的图示：
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\10 wide and deep.png](img/B16254_04_12.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\10 wide and deep.png](img/B16254_04_12.png)'
- en: 'Figure 4.12: Training loss and test estimate for the Wide & Deep model'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：Wide & Deep模型的训练损失和测试估计
- en: 'For full prediction probabilities, we just extract them from the dictionary
    data type used by the Estimator. The `predict_proba` function will return a NumPy
    array with the probabilities for the positive (income in excess of USD 50K) and
    negative classes in distinct columns:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的预测概率，我们只需从Estimator使用的字典数据类型中提取它们。`predict_proba`函数将返回一个NumPy数组，包含正类（收入超过50K美元）和负类的概率，分别位于不同的列中：
- en: '[PRE49]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: How it works...
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Wide & Deep models represent a way to handle linear models together with a more
    complex approach involving neural networks. As for other Estimators, this Estimator
    is also quite straightforward and easy to use. The keys for the success of the
    recipe in terms of other applications definitely rest upon defining an input data
    function and mapping the features with the more suitable functions from `tf.features_columns`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Wide & Deep模型代表了一种将线性模型与更复杂的神经网络方法结合使用的方式。与其他Estimator一样，这个Estimator也非常简单易用。该方法在其他应用中的成功关键，绝对在于定义输入数据函数并将特征与`tf.features_columns`中更合适的函数进行映射。
