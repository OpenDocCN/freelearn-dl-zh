- en: Pose Estimation on 3D models Using ConvNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络（ConvNets）进行3D模型的姿势估计
- en: Welcome to our chapter on human pose estimation. In this chapter, we will be
    building a neural network that will predict 3D human poses using 2D images. We
    will do this with the help of transfer learning by using the VGG16 model architecture
    and modifying it accordingly for our current problem. By the end of this chapter,
    you will have a **deep learning** (**DL**) model that does a really good job of
    predicting human poses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们关于人体姿势估计的章节。在本章中，我们将构建一个神经网络，该网络将使用2D图像预测3D人体姿势。我们将借助迁移学习，使用VGG16模型架构，并根据当前问题对其进行修改。到本章结束时，你将拥有一个**深度学习**（**DL**）模型，它能够很好地预测人体姿势。
- en: '**Visual effects** (**VFX**) in movies are expensive. They involve using a
    lot of expensive sensors that will be placed on the body of the actor when shooting.
    The information from these sensors will then be used to build visual effects,
    all of which ends up being super expensive. We have been asked (in this hypothetical
    use case) by a major movie studio whether we can help their graphics department
    build cheaper and better visual effects by building a human pose estimator, which
    they will use to better estimate poses on the screen while editing.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 电影中的**视觉效果**（**VFX**）成本高昂。它们涉及使用大量昂贵的传感器，这些传感器将在拍摄时安装在演员的身体上。然后，来自这些传感器的信息将用于构建视觉效果，所有这些都最终变得非常昂贵。在这个假设的案例中，我们被一家大型电影制片厂问是否可以通过构建一个人体姿势估算器来帮助他们的图形部门构建更便宜、更好的视觉效果，他们将使用该估算器来在编辑时更好地估计屏幕上的姿势。
- en: For this task, we will be using images from **Frames Labeled In Cinema** (**FLIC**).
    These images are not ready to be used for modeling just yet. So, get ready to
    spend a bit more time on preparing the image data in this chapter. Also, we will
    only be estimating the pose of arms, shoulders, and the head.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本任务，我们将使用来自**电影标签框架**（**FLIC**）的图像。这些图像尚未准备好用于建模。所以，请准备好在本章花费更多时间来准备图像数据。此外，我们将只估算手臂、肩膀和头部的姿势。
- en: 'In this chapter, we''ll learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Processing/preparing images for pose estimation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为姿势估计处理/准备图像
- en: The VGG16 model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG16模型
- en: Transfer learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Building and understanding the training loop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建并理解训练循环
- en: Testing the model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: It would be better if you implement the code snippets as you go along in this
    chapter, either in a Jupyter notebook or any source code editor. This will make
    it easier for you to follow along, as well as understand what each part of the
    code does.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本章中逐步实现代码片段，最好使用Jupyter Notebook或任何源代码编辑器。这将使你更容易跟上进度，并理解代码的每一部分的作用。
- en: All of the Python files and the Jupyter Notebooks for this chapter can be found
    at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有Python文件和Jupyter笔记本可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12)找到。
- en: Code implementation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实现
- en: In this exercise, we will be using the Keras deep learning library, which is
    a high-level neural network API capable of running on top of TensorFlow, Theano,
    and CNTK.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用Keras深度学习库，它是一个高级神经网络API，能够在TensorFlow、Theano和CNTK之上运行。
- en: If you ever have a question related to Keras, refer to this easy-to-understand
    Keras documentation at [https://keras.io](https://keras.io).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何关于Keras的问题，请参考这个易于理解的Keras文档：[https://keras.io](https://keras.io)。
- en: Please download the `Chapter12` folder from GitHub before moving forward with
    this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本章内容之前，请从GitHub下载`Chapter12`文件夹。
- en: 'This project involves downloading files from various sources that will be called
    inside the scripts. To make sure that the Python scripts or the Jupyter Notebook
    have no issues locating the downloaded files, follow these steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目涉及从多个来源下载文件，这些文件将在脚本中调用。为了确保Python脚本或Jupyter Notebook能够正确定位下载的文件，请按照以下步骤操作：
- en: Open a Terminal and change your directory by using the `cd` command in the `Chapter12`
    folder.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并使用`cd`命令切换到`Chapter12`文件夹。
- en: 'Download the `FLIC-full` data file with the following command:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令下载`FLIC-full`数据文件：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unzip the ZIP file with the following command:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令解压ZIP文件：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Remove the ZIP file with the following command:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令删除ZIP文件：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Change directories in the `FLIC-full` folder by using the following command:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在`FLIC-full`文件夹中切换目录：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Download the file containing the training indices:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载包含训练索引的文件：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Change the directory back to the `Chapter12` folder.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录切换回`Chapter12`文件夹。
- en: Launch your Jupyter Notebook or run the Python scripts from the `Chapter12`
    directory.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动你的Jupyter Notebook或从`Chapter12`目录运行Python脚本。
- en: Further information on the `FLIC-full` data folder can be found at [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)找到关于`FLIC-full`数据文件夹的更多信息。
- en: Importing the dependencies
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入依赖包
- en: 'We will be using `numpy`, `matplotlib`, `keras`, `tensorflow`, and the `tqdm`
    package in this exercise. Here, TensorFlow is used as the backend for Keras. You
    can install these packages with `pip`. For the MNIST data, we will be using the dataset
    that''s available in the `keras` module with a simple `import`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习中我们将使用`numpy`、`matplotlib`、`keras`、`tensorflow`和`tqdm`包。在这里，TensorFlow作为Keras的后端。你可以通过`pip`安装这些包。对于MNIST数据，我们将使用`keras`模块中提供的数据集，使用简单的`import`即可：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It is important that you set `seed` for reproducibility:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可重复性，请务必设置`seed`：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exploring and pre-processing the data
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和预处理数据
- en: With the `FLIC-full` data folder downloaded and unpacked, inside the `FLIC-full` folder
    you should find the `tr_plus_indices.mat` and `examples.mat` MATLAB files, and
    also the folder named `images`, inside which are the images that will be used
    in this project.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并解压完`FLIC-full`数据文件夹后，你应该能在`FLIC-full`文件夹中找到`tr_plus_indices.mat`和`examples.mat`
    MATLAB文件，并且还有一个名为`images`的文件夹，里面是将用于本项目的图像。
- en: You will find that the images have been captured from movies such as *2 Fast
    2 Furious*, *Along Came Polly*, *American Wedding*, and a few others. Each of
    these images is 480*720 px in size. These images are nothing but screenshots of
    scenes involving actors from the selected movies, which we will use for pose estimation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，这些图像是从电影如*速度与激情 2*、*与波莉同行*、*美国婚礼*等中截取的。每张图像的大小为480*720像素。这些图像只是选定电影中演员场景的截图，我们将使用它们进行姿势估计。
- en: 'Let''s load the MATLAB file `examples.mat`. We will do this with the help of
    the `loadmat` module, which we have imported already, along with other imports.
    Also, let''s print out some of the information from this file:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载MATLAB文件`examples.mat`。我们将借助已经导入的`loadmat`模块来完成这项操作，并打印出文件中的一些信息：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following is the output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/218fec22-6838-416c-af68-de63f548ad27.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/218fec22-6838-416c-af68-de63f548ad27.png)'
- en: 'Figure 12.1: Example file information from printout 1'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：来自打印输出 1 的示例文件信息
- en: 'From the printout, we can see that the MATLAB file has been loaded as a dictionary
    with four keys, one of which is the one we need: the `examples` key. Let''s see
    what this key holds:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从打印输出中，我们可以看到该MATLAB文件已作为字典加载，其中包含四个键，其中一个是我们需要的：`examples`键。我们来看一下这个键的内容：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Following is the output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/d9b8d6cd-c3f0-4827-884a-2e649bbb0a4b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9b8d6cd-c3f0-4827-884a-2e649bbb0a4b.png)'
- en: Figure 12.2: Example file information from printout 1
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：来自打印输出 1 的示例文件信息
- en: The notable thing here is that the value of the `examples` key is a numpy array
    of shape (1, 20928). You will also see that the array has been reshaped to shape
    `(20928,)`. The `examples` key contains the IDs of the images (in the `images`
    folder) and the corresponding pose coordinates that can be used for modeling.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，`examples`键的值是一个形状为(1, 20928)的numpy数组。你还会看到，这个数组已经被重塑为形状`(20928,)`。`examples`键包含的是图像（在`images`文件夹中）的ID及其对应的姿势坐标，可用于建模。
- en: 'Let''s print out an image ID and its corresponding coordinates array with its
    shape. The image ID we need is stored at index `3`, and the corresponding coordinates
    are at index `2`. Let''s print these out:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出一个图像ID及其对应的坐标数组及其形状。我们需要的图像ID存储在索引`3`中，相关的坐标存储在索引`2`中。我们来打印这些出来：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Following is the output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/212857ff-d2e7-4450-bcdc-f2048900451f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/212857ff-d2e7-4450-bcdc-f2048900451f.png)'
- en: Figure 12.3: Example file information from printout 2
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：来自打印输出 2 的示例文件信息
- en: 'From the preceding screenshot, we can see that the coordinates array is of
    shape (2,29):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的截图中，我们可以看到坐标数组的形状是(2,29)：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Following is the output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/c0036acc-54ae-44b1-8f58-654bf1668521.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0036acc-54ae-44b1-8f58-654bf1668521.png)'
- en: 'Figure 12.4: List of joint labels'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：关节标签列表
- en: 'But, if you look back at the coordinates array that we printed in the preceding
    screenshot, out of the 29 coordinates, we only have information on 11 of the body
    joints/locations. These are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你回顾前面的截图中我们打印的坐标数组，在29个坐标中，我们只得到了11个身体关节/位置的信息。具体如下：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Following is the output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/24a820f3-2079-4162-9183-9a3a15431900.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24a820f3-2079-4162-9183-9a3a15431900.png)'
- en: 'Figure 12.5: List of joint labels with coordinates'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：带坐标的关节标签列表
- en: 'For the purpose of this project, we only need information on the following
    body joints/locations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本项目，我们只需要以下身体关节/位置的信息：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Following is the output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/33982f76-450e-4511-9f30-0b326d58db3f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33982f76-450e-4511-9f30-0b326d58db3f.png)'
- en: 'Figure 12.6: Required joints and their indices in the array'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：数组中所需的关节及其索引
- en: '`lsho`: Left shoulder'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`lsho`：左肩'
- en: '`lelb`: Left elbow'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`lelb`：左肘'
- en: '`lwri`: Left wrist'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`lwri`：左手腕'
- en: '`rsho`: Right shoulder'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`rsho`：右肩'
- en: '`relb`: Right elbow'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`relb`：右肘'
- en: '`rwri`: Right wrist'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`rwri`：右手腕'
- en: '`leye`: Left eye'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`leye`：左眼'
- en: '`reye`: Right eye'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`reye`：右眼'
- en: '`nose`: Nose'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`nose`：鼻子'
- en: 'Now, let''s define a function that takes in a dictionary of nine joint labels
    and coordinates and returns a list with seven coordinates (*7* (*x*,*y*) pairs).
    The reason for seven coordinates is that the `leye`, `reye`, and the `nose` coordinates
    are converted into one head coordinate when we take the mean across them:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个函数，该函数接受包含九个关节标签和坐标的字典，并返回一个包含七个坐标（*7*（*x*,*y*）对）的列表。七个坐标的原因是，当我们对`leye`、`reye`和`nose`坐标取平均时，它们会合并成一个头部坐标：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s load the `tr_plus_indices.mat` MATLAB file, just like we did previously:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载`tr_plus_indices.mat` MATLAB文件，就像我们之前做的那样：
- en: The reason why we need to use the `tr_plus_indices.mat` file is because it contains
    indices of images that should only be used for training, as well as some unlisted
    ones for testing. The reason for such a split is to make sure that the train set
    and the test set have frames from completely different movies so as to avoid overfitting.
    More on this can be found at [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用`tr_plus_indices.mat`文件的原因是，它包含了仅应用于训练的图像的索引，以及一些未列出的用于测试的图像。这样的划分目的是确保训练集和测试集来自完全不同的电影片段，从而避免过拟合。有关更多信息，请访问
    [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)。
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Following is the output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/facd390a-cf75-42f5-aa65-a71b2491ef90.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/facd390a-cf75-42f5-aa65-a71b2491ef90.png)'
- en: 'Figure 12.7: train_indices file information printout 1'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：train_indices文件信息输出1
- en: 'From the preceding screenshot, you can see that the MATLAB file has been loaded
    as a dictionary with four keys, one of which is `tr_plus_indices`, which is the
    one we need. Let''s look at the content of this key:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的截图可以看到，MATLAB文件已作为字典加载，包含四个键，其中之一是`tr_plus_indices`，这是我们需要的。让我们看一下这个键的内容：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Following is the output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/d444061b-4688-40f8-910a-e6371a2d6dee.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d444061b-4688-40f8-910a-e6371a2d6dee.png)'
- en: 'Figure 12.8: train_indices file information printout 2'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：train_indices文件信息输出2
- en: We can see that the `tr_plus_indices` key corresponds to a (17380*1) shaped
    array. We will reshape this to `(17380, )` for convenience.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`tr_plus_indices`键对应一个形状为(17380*1)的数组。为了方便起见，我们将其重塑为`(17380, )`。
- en: '`tr_plus_indices` contains the indices of the data in the `examples` key of
    the `examples.mat` file, which should only be used for training. Using this information,
    we will subset the data into a train set and a test set:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`tr_plus_indices`包含了`examples.mat`文件中`examples`键的数据索引，这些数据仅应用于训练。使用这些信息，我们将数据划分为训练集和测试集：'
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For the remaining part of this code snippet, please refer to the `deeppose.ipynb`
    file here : [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码片段的其余部分，请参阅`deeppose.ipynb`文件： [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: We can see that the train data has 17,380 data points, with each data point
    having an image ID and *7*(*x*,*y*) joint coordinates. Similarly, the test data
    has 3,548 data points.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，训练数据有17,380个数据点，每个数据点有一个图像ID和*7*（*x*,*y*）的关节坐标。同样，测试数据有3,548个数据点。
- en: 'In the preceding snippet, we first initialize four empty lists, two for saving
    train and test image IDs, and two for saving train and test joints. Then, for
    each data point in the `examples` key, we do the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们首先初始化了四个空列表，两个用于保存训练和测试图像 ID，两个用于保存训练和测试关节。然后，对于 `examples` 键中的每个数据点，我们执行以下操作：
- en: Extract the file name.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取文件名。
- en: Extract the joint coordinates.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取关节坐标。
- en: ZIP the target joints (target joint labels) and the corresponding joint coordinates
    and convert them into a dictionary.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标关节（目标关节标签）与相应的关节坐标配对，并将其转换为字典。
- en: Feed the dictionary to the `joint_coordinates` function to obtain the joints
    needed for this task.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将字典传递给 `joint_coordinates` 函数，以获取此任务所需的关节。
- en: Append the image IDs and the resulting joints from the previous step to a train
    or test list by using the `train_indices` list.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_indices` 列表，将之前步骤中得到的图像 ID 和相应的关节添加到训练或测试列表中。
- en: Finally, convert the lists into train and test data frames and save them as
    a CSV file. Make sure that you don't set the index and header parameters to `False`
    when saving the data frame as a CSV file.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将列表转换为训练和测试数据框，并将其保存为 CSV 文件。保存数据框时，请确保不将索引和标题参数设置为 `False`。
- en: 'Let''s load the `train_joints.csv` and `test_joints.csv` files we saved in
    the previous step and print out some details:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载在前一步保存的 `train_joints.csv` 和 `test_joints.csv` 文件，并打印出一些细节：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Following is the output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/2bc02243-2847-4e3f-a809-0e189df92240.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bc02243-2847-4e3f-a809-0e189df92240.png)'
- en: 'Figure 12.9: Printout of image IDs and the joint''s array shape'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9：图像 ID 和关节数组形状的打印输出
- en: 'Now, let''s load some images from the `images` folder and plot them to see
    what they look like:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从 `images` 文件夹加载一些图像，并绘制它们，看看它们的样子：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Following is the output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/7856b703-fe37-4781-8fdb-66b7bdacda1e.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7856b703-fe37-4781-8fdb-66b7bdacda1e.png)'
- en: 'Figure 12.10: Plot of eight images from the images folder in the FLIC_full
    folder'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10：来自 FLIC_full 文件夹中的图像文件夹的八张图像的绘图
- en: 'We can see that each image is of shape (480*720*3). Our next task will be to
    crop the original image and focus on the person of interest by using the joint
    coordinates that are available to us. We do this by resizing the images into a
    shape of 224*24*3 so that we can feed them into the VGG16 model. Finally, we will
    also build a `plotting` function to plot the joints on the image:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每张图像的形状为 (480*720*3)。接下来的任务是通过使用我们所拥有的关节坐标来裁剪原始图像并聚焦于感兴趣的对象。我们将图像调整为 224*224*3
    的大小，以便将其输入到 VGG16 模型中。最后，我们还将构建一个 `plotting` 函数，用于在图像上绘制关节：
- en: '![](img/9fe4c5d7-476a-4402-a583-bfdd7c2a9121.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fe4c5d7-476a-4402-a583-bfdd7c2a9121.png)'
- en: 'Figure 12.11: Plot showing the transformation each image has to go through'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11：显示每张图像必须经过的变换的绘图
- en: Preparing the data
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Now let's implement the functions that will perform the tasks that we discussed
    when we ended the previous section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现之前章节结尾时讨论的任务所需的函数。
- en: Cropping
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 裁剪
- en: 'We will first start off with the `image_cropping()` function. This function
    accepts an image ID and its corresponding joint coordinates. It loads the image
    into memory and then crops the image so that it only includes the section of the
    image that''s bound within the coordinates. The cropped image is then padded so
    that the joints and limbs are completely visible. For the added padding, the joint
    coordinates are also adjusted accordingly. When it has done this, the image is
    returned. This is the most important part of the transformation. Take your time
    and dissect the function to see exactly what is happening (the `crop_pad_inf` and `crop_pad_sup`
    parameters control the amount of padding):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先从 `image_cropping()` 函数开始。此函数接受图像 ID 及其相应的关节坐标。它将图像加载到内存中，然后裁剪图像，只保留图像中由坐标框定的部分。裁剪后的图像将被填充，以便关节和肢体完全可见。对于添加的填充，关节坐标也会相应调整。完成此操作后，图像将被返回。这是转换中最重要的部分。花点时间分析这个函数，看看到底发生了什么（`crop_pad_inf`
    和 `crop_pad_sup` 参数控制填充量）：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For the remaining part of this code snippet please refer to the file `deeppose.ipynb`
    here : [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
    [](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码片段的剩余部分，请参阅此处的文件`deeppose.ipynb`：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: 'Let''s pass an image ID and its joints to the `image_cropping()` function and
    plot the output image:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将图像 ID 及其关节传递给`image_cropping()`函数，并绘制输出图像：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Following is the output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/cf6ad6e1-8611-4155-8cab-5e90b983a008.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf6ad6e1-8611-4155-8cab-5e90b983a008.png)'
- en: 'Figure 12.12: Plot of the resulting cropped image compared to the original
    image'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12：裁剪后的图像与原始图像的绘图对比
- en: Resizing
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整大小
- en: 'In the *Cropping* section, we saw that the original image of shape (480*720*3)
    is cropped to shape (393*254*3). However, the VGG16 architecture accepts images
    of shape (224*224*3). Hence, we will define a function called `image_resize()`
    that does the resizing for us. It accepts the cropped image and the joint resulting
    from the `image_cropping()` function as input and returns the resized image and
    its joint coordinates:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Cropping*部分中，我们看到形状为(480*720*3)的原始图像被裁剪为形状为(393*254*3)。然而，VGG16架构接受形状为(224*224*3)的图像。因此，我们将定义一个名为`image_resize()`的函数来完成调整大小。它接受裁剪后的图像及其由`image_cropping()`函数产生的关节作为输入，并返回调整大小后的图像及其关节坐标：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Following is the output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/d66c1090-a53d-4026-985b-34ba59dc99ee.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d66c1090-a53d-4026-985b-34ba59dc99ee.png)'
- en: 'Figure 12.13: Plot of the resized image'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：调整大小后图像的绘图
- en: After passing the cropped image to the `image_resize()` function, we can see
    that the resulting image is of shape (224*224*3). Now this image and its joints
    can be passed into the model for training.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将裁剪后的图像传递给`image_resize()`函数后，我们可以看到生成的图像形状为(224*224*3)。现在可以将此图像及其关节传递到模型进行训练。
- en: Plotting the joints and limbs
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制关节和四肢
- en: 'Let''s also define the plotting functions that will plot the limbs on the resized
    image. The following defined `plot_joints()` function accepts the resized image
    and its joints and returns an image of the same shape with the limbs plotted on
    top:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也定义绘制功能，它将在调整大小后的图像上绘制四肢。以下定义的`plot_joints()`函数接受调整大小后的图像及其关节，并返回相同形状的带有绘制四肢的图像：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For the remaining part of this code snippet please refer to the `deeppose.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码片段的剩余部分，请参阅此处的`deeppose.ipynb`文件：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: 'Following is the output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/04effbe1-f2bc-41a1-8d0c-3d789946e1d5.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04effbe1-f2bc-41a1-8d0c-3d789946e1d5.png)'
- en: 'Figure 12.14: Plot showing the true joint coordinates on top of the image'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14：显示真实关节坐标在图像顶部的绘图
- en: Transforming the images
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换图像
- en: 'Now let''s transform the images and their corresponding joints to the desired
    form by using the functions we have defined previously. We will do this with the
    help of the `model_data()` function, which is defined as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用我们之前定义的函数将图像及其对应的关节转换为所需形式。我们将借助定义如下的`model_data()`函数来实现这一点：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For the remaining part of this code snippet, please refer to the `deeppose.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码片段的剩余部分，请参阅此处的`deeppose.ipynb`文件：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: The preceding defined `model_data()` function accepts three parameters: `image_ids`
    (array of image IDs), `joints` (array of joints), and a Boolean parameter called `train`.
    Set the `train` parameter to `True` when transforming the training images and
    joints and to `False` when transforming the test images and joints.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前述定义的 `model_data()` 函数接受三个参数：`image_ids`（图像 ID 数组）、`joints`（关节点数组），以及一个布尔类型的参数
    `train`。在转换训练图像和关节点时，将 `train` 参数设置为 `True`，在转换测试图像和关节点时，将其设置为 `False`。
- en: 'When the `train` parameter is set to `True`, perform the following steps:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `train` 参数设置为 `True` 时，执行以下步骤：
- en: Initialize an empty list to store the ID of the transformed image and its joint
    coordinates.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空列表，用于存储转换后的图像 ID 及其关节点坐标。
- en: A new directory called `train` will be created inside the `images` folder if
    the folder does not exist.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `images` 文件夹中不存在 `train` 文件夹，则将在该文件夹内创建一个新的 `train` 文件夹。
- en: An image and its joint coordinates are first passed to the `image_cropping`
    function we defined previously, which will return the cropped image and joint
    coordinates.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将图像及其关节点坐标传递给我们之前定义的 `image_cropping` 函数，该函数将返回裁剪后的图像和关节点坐标。
- en: The result of *Step 3* is passed to the `image_resize` function, which will
    then resize the image to the desired shape. In our case, this is 224*224*3.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 3* 的结果被传递给 `image_resize` 函数，然后该函数将图像调整为所需的形状。在我们的例子中，这是 224*224*3。'
- en: The resized image is then written into the `train` folder via the OpenCV `imwrite()`
    function with a new image ID (for example, `train0.jpg`).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，调整大小后的图像通过 OpenCV 的 `imwrite()` 函数写入 `train` 文件夹，并附上新的图像 ID（例如 `train0.jpg`）。
- en: The new image ID and its joints are appended to the list initialized in *Step
    1.*
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新的图像 ID 和关节点将被附加到 *步骤 1* 中初始化的列表中。
- en: '*Step 3* through *Step 6* are repeated until all of the training images are
    transformed.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 3* 到 *步骤 6* 会重复执行，直到所有训练图像都被转换。'
- en: The list defined in *Step 1* now contains the new image IDs and the joint coordinates,
    which are then converted to a data frame and saved as a CSV file in the `train`
    folder.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *步骤 1* 中定义的列表现在包含了新的图像 ID 和关节点坐标，这些数据将被转换为数据框并保存为 CSV 文件，存放在 `train` 文件夹中。
- en: For transforming the test data, the preceding procedure is repeated by setting
    the `train` parameter to `False` and feeding the test image IDs and the joints.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了转换测试数据，前述过程会重复执行，将 `train` 参数设置为 `False`，并输入测试图像 ID 和关节点。
- en: The train and test data frames that get generated inside the `model_data()`
    function are stored as a CSV file with no header and no index column. Take this
    into consideration when loading these files.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `model_data()` 函数中生成的训练和测试数据框将作为没有头部和索引列的 CSV 文件存储。加载这些文件时要注意这一点。
- en: Defining hyperparameters for training
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义训练的超参数
- en: 'The following are some of the hyperparameters that have been defined that we
    will be using throughout our code. These are totally configurable:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在代码中将使用的一些已定义的超参数，这些都可以进行配置：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Experiment with different learning rates, optimizers, batch size, as well as
    smoothing value to see how these factors affect the quality of your model. If
    you get better results, show these to the deep learning community.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率、优化器、批量大小，以及平滑值，看看这些因素如何影响模型的质量。如果获得了更好的结果，可以向深度学习社区展示。
- en: Building the VGG16 model
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 VGG16 模型
- en: The VGG16 model is a deep convolution neural network image classifier. The model
    uses a combination of Conv2D, MaxPooling2D, and Dense layers to form the final
    architecture, and the activation function that's used is ReLU. It accepts color
    images of shape 224*224*3, and is capable of predicting 1,000 classes. This means that
    the final Dense layer has 1,000 neurons, and it uses softmax activation to get
    scores for each class.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 模型是一种深度卷积神经网络图像分类器。该模型使用 Conv2D、MaxPooling2D 和 Dense 层的组合形成最终的架构，且使用 ReLU
    激活函数。它接受形状为 224*224*3 的彩色图像，并能够预测 1,000 个类别。这意味着最终的 Dense 层有 1,000 个神经元，并使用 softmax
    激活函数来获得每个类别的得分。
- en: Defining the VGG16 model
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 VGG16 模型
- en: In this project, we want to feed in images of shape 224*224*3 and be able to
    predict the joint coordinates for the body in the image. That is, we want to be
    able to predict 14 numerical values (*7* (*x*,*y*) pairs). Therefore, we modify
    the final Dense layer to have 14 neurons and use ReLU activation instead of sigmoid.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们希望输入形状为 224*224*3 的图像，并能够预测图像中身体的关节点坐标。也就是说，我们希望能够预测 14 个数值（*7*（*x*，*y*）对）。因此，我们将最后的
    Dense 层修改为 14 个神经元，并使用 ReLU 激活函数代替 sigmoid。
- en: Training a deep learning model such as VGG16 can take up to a week on a local
    machine. This is a lot of time. An alternative to this in our case is to use the
    weights of a trained VGG16 model through transfer learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地机器上训练一个深度学习模型，如VGG16，可能需要一周的时间。这是非常耗时的。在我们的案例中，替代方案是通过迁移学习使用已经训练好的VGG16模型的权重。
- en: We will do this with the help of the applications module in Keras that we imported
    in the beginning, along with the other imports.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将借助于在开始时导入的Keras应用模块以及其他导入来完成此操作。
- en: 'In the following code, we will load part of the VGG16 model up to, but not
    including, the Flatten layer and the corresponding weights. Setting the `include_top`
    parameter to `False` does this for us:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将加载VGG16模型的一部分，直到但不包括Flatten层以及相应的权重。将`include_top`参数设置为`False`可以实现这一点：
- en: The first line of the following snippet will also download the VGG16 weights
    from the Keras server, so you don't have to worry about downloading the weights
    file from anywhere else.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码的第一行也将从Keras服务器下载VGG16的权重，因此你无需担心从其他地方下载权重文件。
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Following is the output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/8ceeddcd-bb4b-4e32-a851-699bd00f3362.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ceeddcd-bb4b-4e32-a851-699bd00f3362.png)'
- en: 'Figure 12.15: Summary of the VGG16 model (up to Flatten)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.15：VGG16模型的总结（直到Flatten）
- en: From the summary, we can see that all of the layers of the VGG16 model up to, but
    not including, the Flatten layer have been loaded with their weights.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从总结中，我们可以看到VGG16模型的所有层（直到但不包括Flatten层）已经加载了它们的权重。
- en: To learn more about the additional functionality of the applications module
    of Keras, take a look at the official documentation at [https://keras.io/applications/](https://keras.io/applications/).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解Keras的应用模块的附加功能，请查看官方文档：[https://keras.io/applications/](https://keras.io/applications/)。
- en: 'We don''t want weights of any of these layers to be trained. So, in the following
    code, we need to set the `trainable` parameter of each layer to `False`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望这些层的任何权重被训练。因此，在以下代码中，我们需要将每个层的`trainable`参数设置为`False`：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As a next step, flatten the output of the preceding section of the model and
    then add three Dense layers, of which two layers have 1,024 neurons each with
    a dropout between them, and a final Dense layer with 14 neurons to obtain the
    14 joint coordinates. We will only be training the weights of the layers defined
    in the following code snippet:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，扁平化前面部分的模型输出，然后添加三个Dense层，其中两个层每个有1,024个神经元，并且它们之间有一个dropout，最后添加一个Dense层，包含14个神经元，用来获取14个关节坐标。我们将仅训练以下代码片段中定义的层的权重：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once all of the layers have been defined and configured, we will combine them
    by using the `Model` function in Keras, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有层都被定义和配置完毕，我们将使用Keras中的`Model`函数将它们组合在一起，如下所示：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Following is the output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/656b47f0-c68e-4ac3-9fa9-77310a06ebc9.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/656b47f0-c68e-4ac3-9fa9-77310a06ebc9.png)'
- en: 'Figure 12.16: Summary of the customized VGG16 model'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.16：定制化VGG16模型的总结
- en: From the summary, we can see that `26,755,086` parameters are trainable and
    that `14,714,688` parameters are untrainable, since we have set them as untrainable.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从总结中，我们可以看到`26,755,086`个参数是可训练的，而`14,714,688`个参数是不可训练的，因为我们已经将它们设置为不可训练。
- en: 'The model is then compiled with `mean_squared_error` as `loss`. The `optimizer`
    used here is Adam, which has a learning rate of 0.0001, as defined by the optimizer
    variable in the hyperparameter section:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型使用`mean_squared_error`作为`loss`进行编译。这里使用的`optimizer`是Adam，其学习率为0.0001，这是在超参数部分通过优化器变量定义的：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Training loop
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: Now that the VGG16 model is all set to be used for training, let's load the
    `train_joints.csv` file from the `train` folder containing the IDs of the cropped
    and resized images with their joint coordinates.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在VGG16模型已经准备好用于训练，接下来我们加载`train`文件夹中的`train_joints.csv`文件，该文件包含了裁剪和调整大小后的图像的ID以及它们的关节坐标。
- en: 'Then, split the data into an 80:20 train and validation set by using the `train_test_split`
    module from `sklearn`. We imported it with the other imports at the beginning
    of this chapter. Since the validation data is small, load all of the corresponding
    images into memory:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用来自`sklearn`的`train_test_split`模块将数据拆分为80:20的训练集和验证集。我们在本章开头与其他导入一起导入了它。由于验证数据较少，将所有相应的图像加载到内存中：
- en: Be mindful of how many validation images you load into memory, as this may become
    an issue with systems that have less RAM.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意加载多少验证图像到内存，因为这可能会在内存较小的系统中成为问题。
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Explore the data with the pandas `head`, `tail`, and `info` functions. Please
    note that when loading the `.csv` file using pandas, set the `header` parameter
    to `False` so that pandas knows that the file has no header.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 的`head`、`tail`和`info`函数探索数据。请注意，在使用 pandas 加载`.csv`文件时，要将`header`参数设置为`False`，这样
    pandas 就知道文件没有头部。
- en: 'We will now define the `training()` function, which will train the VGG16 model
    on the train images. This function accepts the VGG16 model, train image IDs, train
    joints, validation images, and validation joints as parameters. The following
    steps define what is happening in the `training()` function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将定义`training()`函数，该函数将在训练图像上训练 VGG16 模型。此函数接受 VGG16 模型、训练图像 ID、训练关节、验证图像和验证关节作为参数。以下步骤定义了`training()`函数的工作过程：
- en: The function defines empty lists by using `loss_lst` to store the train loss
    and `val_loss_lst` to store the validation loss. It also defines a counter count
    to keep track of the total number of batches.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数通过使用`loss_lst`定义空列表来存储训练损失，并使用`val_loss_lst`来存储验证损失。它还定义了一个计数器 count，用于跟踪批次的总数。
- en: It then creates a batch of train image IDs and their corresponding joints.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接着创建了一个包含训练图像 ID 和其对应关节的批次。
- en: Using the batch image IDs, it loads the corresponding images into memory by
    using the OpenCV `imread()` function.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用批次图像 ID，它通过使用 OpenCV 的`imread()`函数将相应的图像加载到内存中。
- en: It then converts the loaded train images into a `float`, which it feeds along
    with the joint IDs to the `train_on_batch()` function of the model for the fit.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接着将加载的训练图像转换为`float`，并将其与联合 ID 一起输入到模型的`train_on_batch()`函数进行拟合。
- en: At every 40^(th) batch, it evaluates the model on the validation data and stores
    the train and validation loss in the defined lists.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每经过第 40 次批次，它会在验证数据上评估模型，并将训练和验证损失存储在定义的列表中。
- en: It then repeats *Steps 2* through *5* for the desired number of epochs.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接着重复*步骤 2* 到 *步骤 5*，直到达到所需的训练轮次。
- en: 'Following is the code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码：
- en: '[PRE31]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For the remaining part of this code snippet, please refer to the `deeppose.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此代码段的其余部分，请参阅`deeppose.ipynb`文件：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: 'The output is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/2296d449-0116-40b7-a1ca-55132b90f859.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2296d449-0116-40b7-a1ca-55132b90f859.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行完毕后的输出：
- en: '![](img/048171cb-94b9-4623-a205-f0b0a8cc14c1.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/048171cb-94b9-4623-a205-f0b0a8cc14c1.png)'
- en: 'Figure 12.17: Loss output when training the model'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.17：训练模型时的损失输出
- en: If you are using a small GPU for training, reduce the batch size to avoid GPU
    memory issues. Also, remember that a smaller batch size may or may not result
    in the same fit that this chapter indicates.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是小型 GPU 进行训练，请减少批次大小以避免 GPU 内存问题。还要记住，较小的批次大小可能不会得到本章节中所示的相同拟合结果。
- en: Plot training and validation loss
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制训练和验证损失图
- en: 'With `loss_lst` and `val_loss_lst` containing the train and validation MSE
    loss at intervals of 40 batches, let''s plot this and see how the learning has
    progressed:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`loss_lst`和`val_loss_lst`在每 40 个批次的间隔中包含训练和验证的 MSE 损失，让我们绘制图表并查看学习进展：
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Following is the output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/445bc33e-63bd-4581-bbce-2f481f74228f.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/445bc33e-63bd-4581-bbce-2f481f74228f.png)'
- en: 'Figure 12.18: Plot of train and validation loss'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.18：训练和验证损失图
- en: A smoother train validation loss plot can be obtained by reducing the store
    hyperparameter. A small store value will result in a longer training time.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少存储超参数，可以获得平滑的训练验证损失图。较小的存储值将导致更长的训练时间。
- en: Predictions
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: This is what we have been waiting for...
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们一直在等待的……
- en: Making test predictions!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 进行测试预测！
- en: 'We will define a function that takes the model as input and tests the model
    on the test data we have preprocessed and saved in the `test` folder. Along with
    predictions, it will also save test images with the true and predicted joints
    plotted on it by using the `plot_limb()` and the `plot_joints()` functions we
    defined in the preceding section:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个函数，该函数以模型作为输入，测试我们已经预处理并保存在`test`文件夹中的测试数据。除了预测结果外，它还将通过使用我们在前一部分定义的`plot_limb()`和`plot_joints()`函数，将测试图像上的真实和预测关节绘制出来：
- en: '[PRE33]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Following is the output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/42596d64-7380-405a-8807-c875b26a8887.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42596d64-7380-405a-8807-c875b26a8887.png)'
- en: 'Figure 12.19: Test loss'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.19：测试损失
- en: On a test set with 200 images, the test MSE loss is 454.80, which is very close
    to the validation MSE loss of 503.85, indicating that the model is not overfitting
    on the train data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在200张测试图像的测试集中，测试MSE损失为454.80，与验证MSE损失503.85非常接近，这表明该模型没有在训练数据上过拟合。
- en: Train the model for a few more epochs if possible, and check if a better fit
    is possible. Be mindful of how many test images you want to load into memory for
    evaluation since it might become a problem on machines with RAM limitations.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，继续训练模型更多的周期，并检查是否能得到更好的拟合。注意你想加载多少测试图像到内存中进行评估，因为在内存有限的机器上，这可能会成为问题。
- en: 'Now let''s plot the images we saved during testing to get a measure of how
    the true joints compare to the predicted joints:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制在测试期间保存的图像，以衡量真实关节点与预测关节点的对比：
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Following is the output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/dc8f0519-8697-48cb-98e9-306819b4afd3.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc8f0519-8697-48cb-98e9-306819b4afd3.png)'
- en: 'Figure 12.20: Test images with true and predicted joints plotted on top'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.20：测试图像及其真实和预测关节点的重叠图
- en: From the preceding picture, we can see that the model is doing a really good
    job of predicting the seven joints on unseen images.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像中，我们可以看到模型在预测未见图像中的七个关节点时表现得非常好。
- en: Scripts in modular form
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化形式的脚本
- en: The entire script can be split into four modules named `train.py`, `test.py`,
    `plotting.py`, and `crop_resize_transform.py`. You should be able to find these
    scripts in the `Chapter12` folder. Follow the instructions under the *Code implementation* section
    of this chapter to run the scripts. Set `Chapter12` as the project folder in your
    favorite source code editor and just run the `train.py` file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 整个脚本可以分为四个模块，分别是`train.py`、`test.py`、`plotting.py`和`crop_resize_transform.py`。你应该能够在`Chapter12`文件夹中找到这些脚本。按照本章*代码实现*部分的说明来运行这些脚本。在你最喜欢的源代码编辑器中将`Chapter12`设置为项目文件夹，然后运行`train.py`文件。
- en: The `train.py` Python file will import functions from all of the other modules
    in places where they are needed for execution.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.py` Python 文件将在执行所需的位置导入其他模块中的函数。'
- en: Now let's walk through the contents of each file.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐步查看每个文件的内容。
- en: Module 1 – crop_resize_transform.py
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 1 – crop_resize_transform.py
- en: 'This Python file contains the `image_cropping()`, `image_resize()`, and `model_data()`
    functions, as shown:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该Python文件包含`image_cropping()`、`image_resize()`和`model_data()`函数，如下所示：
- en: '[PRE35]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Module 2 – plotting.py
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 2 – plotting.py
- en: 'This Python file contains two functions, namely `plot_limb()` and `plot_joints()`,
    as shown:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 该Python文件包含两个函数，分别是`plot_limb()`和`plot_joints()`，如下所示：
- en: '[PRE36]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Module 3 – test.py
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 3 – test.py
- en: 'This module contains the `test()` function that will be called in the `train_dqn.py`
    script so that it can test the performance of the trained model, as shown:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块包含了`test()`函数，该函数将在`train_dqn.py`脚本中调用，用于测试训练模型的性能，如下所示：
- en: '[PRE37]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Module 4 – train.py
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 4 – train.py
- en: 'In this module, we have the `joint_coordinates()` and `training()` functions,
    as well as the calls to train and test the VGG16 model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模块中，我们有`joint_coordinates()`和`training()`函数，以及用于训练和测试VGG16模型的调用：
- en: '[PRE38]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Conclusion
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This project was all about building a **convolutional neural network **(**CNN**)
    classifier to solve the problem of estimating 3D human poses using frames captured
    from movies. Our hypothetical use case was to enable visual effects specialists
    to easily estimate the pose of actors (from their shoulders, necks, and heads
    from the frames in a video. Our task was to build the intelligence for this application.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的目的是构建一个**卷积神经网络** (**CNN**)分类器，来解决使用从电影中捕捉的帧估计3D人体姿势的问题。我们的假设用例是让视觉特效专家能够轻松估计演员的姿势（通过视频帧中的肩膀、脖子和头部）。我们的任务是为这个应用程序构建智能。
- en: The modified VGG16 architecture we built using transfer learning has a test
    mean squared error loss of 454.81 squared units over 200 test images for each
    of the 14 coordinates (that is, the *7*(*x*, *y*) pairs). We can also say that
    the test root mean squared error over 200 test images for each of the 14 coordinates is
    21.326 units. What does this mean?
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过迁移学习构建的修改版VGG16架构，其在200个测试图像中对每个14个坐标（即*7*（*x*，*y*）对）的测试均方误差（MSE）为454.81平方单位。我们还可以说，对于每个14个坐标，200个测试图像的测试均方根误差（RMSE）为21.326单位。这意味着什么呢？
- en: The **root mean squared error** (**RMSE**), in this case, is a measure of how
    far off the predicted joint coordinates/joint pixel location are from the actual
    joint coordinate/joint pixel location.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差** (**RMSE**) 在此情况下是衡量预测的关节坐标/关节像素位置与实际关节坐标/关节像素位置之间差距的指标。'
- en: An RMSE loss of 21.32 units is equivalent to having each predicted coordinate
    off by 21.32 pixels within an image of shape 224*224*3\. The test results plotted
    in *Figure 13.20* represent this measure.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 损失值为 21.32 单位，相当于在形状为 224*224*3 的图像中，每个预测的坐标偏差为 21.32 像素。*图 13.20* 中的测试结果展示了这一度量。
- en: Each coordinate being off by 21.32 pixels is good at a general level, but we
    want to build a product that will be used in movies for which the margin for error
    is much less, and being off by 21 pixels is not acceptable.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 每个坐标偏差 21.32 像素在一般层面上是可以接受的，但我们希望构建的产品将用于电影制作，其中误差的容忍度要低得多，偏差 21 像素是不可接受的。
- en: 'To improve the model, you can do the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进模型，您可以采取以下措施：
- en: Try using a lower learning rate for a larger number of epochs
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用较低的学习率，增加训练的轮数
- en: Try using a different loss function (for example, **mean absolute error** (**MAE**))
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用不同的损失函数（例如，**均值绝对误差** (**MAE**)）
- en: Try using an even deeper model, such as RESNET50 or VGG19
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更深的模型，例如 RESNET50 或 VGG19
- en: Try centering and scaling the data
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试对数据进行中心化和标准化
- en: Get more data
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更多的数据
- en: These are some of the additional steps you should take if you are interested
    in becoming an expert in this specific area once you are done with this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在完成本章后有兴趣成为该领域的专家，以下是一些您应该采取的额外步骤。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we successfully built a deep convolution neural network/VGG16
    model in Keras on FLIC images. We got hands-on experience in preparing these images
    for modeling. We successfully implemented transfer learning, and understood that
    doing so will save us a lot of time. We defined some key hyperparameters as well
    in some places, and reasoned about why we used what we used. Finally, we tested
    the modified VGG16 model performance on unseen data and determined that we succeeded
    in achieving our goals.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们成功地在 Keras 中构建了一个深度卷积神经网络/VGG16 模型，应用于 FLIC 图像。我们亲手实践了如何准备这些图像以供建模使用。我们成功实现了迁移学习，并理解到这样做将节省大量时间。在某些地方，我们还定义了一些关键的超参数，并推理为什么使用这些参数。最后，我们测试了修改后的
    VGG16 模型在未见数据上的表现，并确认我们成功达成了目标。
