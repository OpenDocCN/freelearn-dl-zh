- en: Looking Ahead
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展望未来
- en: Over the past few hundred pages, we have faced numerous challenges, to which
    we applied reinforcement and deep learning algorithms. To conclude our **reinforcement
    learning** (**RL**) journey, this chapter will look at several aspects of the
    field that we have not covered yet. We will start by looking at several of the
    drawbacks of reinforcement learning, which any practitioner or researcher should
    be aware of. To end on a positive note, we will follow up by describing numerous
    exciting academic developments and achievements the field has seen in recent years.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几百页中，我们面临了许多挑战，并应用了强化学习和深度学习算法。为了总结我们的**强化学习**（**RL**）之旅，本章将探讨我们尚未涵盖的该领域的几个方面。我们将从讨论强化学习的几个缺点开始，任何从业者或研究人员都应该对此有所了解。为了以积极的语气结束，我们将描述该领域近年来所看到的许多令人兴奋的学术进展和成就。
- en: The shortcomings of reinforcement learning
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的缺点
- en: So far, we have only covered what reinforcement learning algorithms can do.
    To the reader, reinforcement learning may seem like the panacea for all kinds
    of problems. But why do we not see a ubiquitous application of reinforcement learning
    algorithms in real-life situations? The reality is that the field has a myriad
    of shortcomings that hinder commercial adoption.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了强化学习算法能做什么。对于读者而言，强化学习可能看起来是解决各种问题的灵丹妙药。但为什么我们在现实生活中并没有看到强化学习算法的广泛应用呢？现实情况是，该领域存在许多缺点，阻碍了其商业化应用。
- en: Why is it necessary to talk about the field's flaws? We think this will help
    you build a more holistic, less biased view of reinforcement learning. Moreover,
    understanding the weaknesses of reinforcement learning and machine learning is
    an important quality of a good machine learning researcher or practitioner. In
    the following subsections, we will discuss a few of the most important limitations
    that reinforcement learning is currently facing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么有必要谈论该领域的缺陷？我们认为这将帮助你建立一个更全面、更客观的强化学习观念。此外，理解强化学习和机器学习的弱点是一个优秀的机器学习研究员或从业者的重要素质。在接下来的小节中，我们将讨论强化学习目前面临的一些最重要的局限性。
- en: Resource efficiency
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源效率
- en: Current deep reinforcement learning algorithms require vast amounts of time,
    training data, and computational resources in order to reach a desirable level
    of proficiency. For algorithms such as AlphaGo Zero, where our reinforcement learning
    algorithm learns to play Go with zero prior knowledge and experience, resource
    efficiency becomes a major bottleneck for taking such algorithms to commercial
    scales. Recall that when DeepMind implemented AlphaGo Zero, they needed to train
    the agent on tens of millions of games using hundreds of GPUs and thousands of
    CPUs. For AlphaGo Zero to reach a reasonable proficiency, it needs to play a number
    of games, equivalent to what hundreds of thousands of humans would play in their
    lifetimes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的深度强化学习算法需要大量的时间、训练数据和计算资源，才能达到理想的熟练程度。对于像 AlphaGo Zero 这样的算法，它的强化学习算法在没有任何先验知识和经验的情况下学习围棋，资源效率成为了将此类算法推广到商业规模的主要瓶颈。回想一下，当
    DeepMind 实现 AlphaGo Zero 时，他们需要在数千万场游戏中使用数百个 GPU 和数千个 CPU 来训练代理。为了让 AlphaGo Zero
    达到合理的熟练度，它需要进行数百万场游戏，相当于数十万人的一生所进行的游戏数量。
- en: Unless, in the future, the average consumer can readily leverage vast amounts
    of computational power that only the likes of Google and Nvidia can offer today,
    the ability to develop superhuman reinforcement learning algorithms will continue
    to be way beyond the public's reach. This means that powerful, resource-hungry
    reinforcement learning algorithms will be monopolized by a small consortium of
    institutions, which is probably not a great thing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除非未来普通消费者能够轻松利用像谷歌和英伟达今天所提供的庞大计算能力，否则开发超人类的强化学习算法的能力仍将远远超出公众的掌控。这意味着，强大的、资源密集型的强化学习算法将被少数几家机构垄断，这可能并非一件好事。
- en: Thus, making reinforcement learning algorithms trainable under limited resources
    will continue to be an important issue that the community must address.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在有限资源下使强化学习算法可训练将继续是社区必须解决的重要问题。
- en: Reproducibility
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重复性
- en: In numerous fields of scientific research, a prevalent problem has been the
    inability to reproduce the experimental results claimed in academic papers and
    journals. In a 2016 survey conducted by Nature, the world's most renowned scientific
    journal, 70% of respondents claimed that they have failed to reproduce their own
    or another researcher's experimental results. Moreover, the attitude toward the
    inability to reproduce experimental results was a stark one, with 90% of researchers
    thinking that there is indeed a reproducibility crisis.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多科学研究领域，一个普遍存在的问题是无法重复学术论文和期刊中所声称的实验结果。在2016年《自然》杂志（世界上最著名的科学期刊）进行的一项调查中，70%的受访者表示他们未能重复自己或其他研究者的实验结果。此外，对于无法重复实验结果的态度十分严峻，90%的研究人员认为确实存在可重复性危机。
- en: The original work reported by nature can be found here: [https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 《自然》报道的原始工作可以在这里找到：[https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970)。
- en: While this survey targeted researchers across a number of disciplines, including
    biology and chemistry, reinforcement learning is also facing a similar problem.
    In the paper *Deep Reinforcement Learning Matters* (reference at the end of this
    chapter; you can view it at [https://arxiv.org/pdf/1709.06560.pdf](https://arxiv.org/pdf/1709.06560.pdf) for
    the online version), Peter Henderson et al. study the effects of different configurations
    of a deep reinforcement learning algorithm on experimental outcomes. These configurations
    include hyperparameters, seeds for the random number generator, and network architecture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项调查面向多个学科的研究人员，包括生物学和化学，但强化学习也面临类似的问题。在论文《深度强化学习的重要性》（参考文献见本章末尾；你可以在[https://arxiv.org/pdf/1709.06560.pdf](https://arxiv.org/pdf/1709.06560.pdf)查看在线版本）中，Peter
    Henderson等人研究了深度强化学习算法的不同配置对实验结果的影响。这些配置包括超参数、随机数生成器的种子以及网络架构。
- en: In extreme cases, they found that, when training the same model on two sets
    of five different random seed configurations, the resulting average return for
    the two sets of models diverged  significantly. Moreover, changing other settings,
    such as the architecture of the CNN, activation functions, and learning rates,
    have profound effects on the outcome.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端情况下，他们发现，在对同一个模型进行训练时，使用两组五个不同的随机种子配置，最终得到的两个模型的平均回报存在显著差异。此外，改变其他设置，如CNN架构、激活函数和学习率，也对结果产生深远影响。
- en: What are the implications of inconsistent, unreproducible results? As the adoption
    and popularity of reinforcement learning and machine learning continues to grow
    at near exponential rates, the number of implementations of reinforcement learning
    algorithms freely available on the internet also increases. If those implementations
    cannot reproduce the results they claim to be able to achieve, this would cause
    major issues and potential danger in real-life applications. Certainly, no one
    would want their self-driving car to be implemented so that it cannot produce
    consistent decisions!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致和无法重复的结果意味着什么呢？随着强化学习和机器学习的应用和普及以接近指数的速度增长，互联网上可自由获取的强化学习算法实现数量也在增加。如果这些实现无法重现它们声称能够达到的结果，这将会在现实应用中引发重大问题和潜在危险。毫无疑问，没有人希望他们的自动驾驶汽车被实现得无法做出一致的决策！
- en: Explainability/accountability
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性/可追溯性
- en: We have seen how an agent's policy can return either a single action or a probability
    distribution over a set of possible actions and how its value function can return
    how desirable a certain state is. But how can a model explain how it arrived at
    such predictions? As reinforcement learning becomes more popular and potentially
    more prevalent in real-life applications, there will be an ever-increasing need
    to be able to explain the output of reinforcement learning algorithms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，代理的策略可以返回单一的动作或一组可能动作的概率分布，而它的价值函数可以返回某一状态的期望程度。那么，模型如何解释它是如何得出这些预测的呢？随着强化学习变得更加流行并有可能在现实应用中得到广泛应用，将会有越来越大的需求去解释强化学习算法的输出。
- en: Today, most advanced reinforcement learning algorithms incorporate deep neural
    networks, which, as of now, can only be represented as a set of weights and a
    sequence of non-linear functions. Moreover, due to its high dimensional nature,
    neural networks are not able to provide any meaningful, intuitive relationships
    between input and their corresponding output that can be understood easily by
    humans. Hence, deep learning algorithms are often referred to as black boxes,
    for it is difficult for us to understand what is really going on inside a neural
    network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，大多数先进的强化学习算法都包含深度神经网络，而这些网络目前只能通过一组权重和一系列非线性函数来表示。此外，由于神经网络的高维特性，它们无法提供任何有意义的、直观的输入与相应输出之间的关系，普通人难以理解。因此，深度学习算法通常被称为“黑盒”，因为我们很难理解神经网络内部究竟发生了什么。
- en: Why is it important for a reinforcement learning algorithm to be explainable?
    Suppose an autonomous car is involved in a car accident (let's assume it was just
    an innocuous bump between two cars and the drivers are not hurt). Human drivers
    would be able to explain what led to the crash; they can give reasons for why
    they performed a particular maneuver and what exactly happened when the accident
    occurred. This would help law enforcement ascertain the cause of the accident
    and potentially determine who or what was accountable. However, even if we create
    an agent that can drive cars sufficiently well using algorithms available today,
    this is simply not possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么强化学习算法需要具有可解释性？假设一辆自动驾驶汽车发生了车祸（假设这只是两辆车之间无害的小碰撞，驾驶员没有受伤）。人类驾驶员可以解释导致事故发生的原因；他们能够说明为什么采取某个特定的操作，以及事故发生时究竟发生了什么。这将帮助执法部门确定事故原因，并可能追究责任。然而，即使我们使用现有的算法创造出一个能够驾驶汽车的智能体，这依然是做不到的。
- en: Without the ability to explain predictions, it will be difficult for users and
    the general public to trust software that uses any kind of machine learning, especially
    in use cases where the algorithms are accountable for making important decisions.
    This is a serious impediment to the adoption of reinforcement learning algorithms
    in practical applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不能解释预测结果，用户和大众将难以信任任何使用机器学习的软件，尤其是在算法需要为做出重要决策负责的应用场景中。这对强化学习算法在实际应用中的普及构成了严重障碍。
- en: Susceptibility to attacks
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 易受攻击的风险
- en: 'Deep learning algorithms have shown incredible results across numerous tasks,
    including computer vision, natural language processing, and speech recognition.
    In several tasks, deep learning has already surpassed human capabilities. However,
    recent work has shown that these algorithms are incredibly vulnerable to attacks.
    By attacks, we mean attempts to make imperceptible modifications to the input
    which causes the model to behave differently. Take the following example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法在多个任务中展现了惊人的成果，包括计算机视觉、自然语言处理和语音识别。在一些任务中，深度学习已经超越了人类的能力。然而，最近的研究表明，这些算法对攻击极为脆弱。所谓攻击，指的是对输入进行难以察觉的修改，从而导致模型表现出不同的行为。举个例子：
- en: '![](img/e6104b4d-4113-4c78-9833-54ef67989b8e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6104b4d-4113-4c78-9833-54ef67989b8e.png)'
- en: An illustration of adversarial attacks. By adding imperceptible perturbations
    to an image, an attacker can easily fool deep learning image classifiers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击的示意图。通过对图像添加难以察觉的扰动，攻击者可以轻易欺骗深度学习图像分类器。
- en: The rightmost image is the result of adding the left image, which is the original
    image, and the middle image, which represents the perturbations added to the original
    image. Even the most accurate, well-performing deep neural network image classifier
    fails to identify the right image as a goat and instead predicts it to be a toaster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最右侧的图片是通过将左侧的原始图像和中间的扰动图像相加得到的结果。即便是最准确、表现最好的深度神经网络图像分类器，也无法将右侧的图像识别为山羊，反而将其误判为烤面包机。
- en: These examples have shocked many in the research community, for people did not
    expect that deep learning algorithms can be incredibly brittle and susceptible
    to such attacks. This field is now called **adversarial machine learning** and
    has been rapidly increasing in prominence and importance as more researchers around
    the world are investigating the robustness and vulnerabilities of deep learning
    algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子让许多研究人员感到震惊，因为人们没想到深度学习算法如此脆弱，并容易受到此类攻击。这一领域现在被称为**对抗性机器学习**，随着越来越多的研究者关注深度学习算法的鲁棒性和漏洞，它的知名度和重要性也在迅速提升。
- en: Reinforcement learning algorithms are also no stranger to these results and
    attacks. According to the paper titled *Robust Deep Reinforcement Learning with
    Adversarial Attacks* ([https://arxiv.org/abs/1712.03632](https://arxiv.org/abs/1712.03632))
    by Anay Pattanaik et. al., adversarial attacks to reinforcement learning algorithms
    can be defined as any possible perturbation that leads the agent into an increased
    probability of taking the worst possible action in that state. For example, we
    can add noise to the screen of an Atari game with the intention of tricking the
    RL agent playing the game to make a poor decision, which leads to a lower score.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法同样无法避免这些结果和攻击。根据Anay Pattanaik等人撰写的题为*《带有对抗攻击的鲁棒深度强化学习》*（[https://arxiv.org/abs/1712.03632](https://arxiv.org/abs/1712.03632)）的论文，对抗性攻击强化学习算法可以定义为任何可能的扰动，导致智能体在该状态下采取最差行动的概率增加。例如，我们可以在Atari游戏的屏幕上添加噪声，目的是欺骗玩游戏的RL智能体做出错误的决策，从而导致更低的分数。
- en: More serious applications include adding noise to street signs to trick a self-driving
    car into thinking that a STOP sign is a speed sign, making an ATM recognize a
    $100 check as a $1,000,000 one, or even fooling a facial-recognition system to
    identify an attacker's face as that of another user.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更为严重的应用包括向街道标志添加噪声，以欺骗自动驾驶汽车将STOP标志误认为速度限制标志，或让ATM识别$100支票为$1,000,000支票，甚至欺骗面部识别系统将攻击者的面孔识别为其他用户的面孔。
- en: Needless to say, these vulnerabilities further add to the risks of adopting
    deep learning algorithms in practical, safety-critical use cases. While there
    are numerous ongoing efforts to countervail adversarial attacks, there is still
    a long way to go for deep learning algorithms to become robust enough for such
    use cases.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不用多说，这些漏洞进一步增加了在实际、关乎安全的使用场景中采用深度学习算法的风险。虽然目前已有大量努力在应对对抗性攻击，但深度学习算法要足够强大以适应这些使用场景，仍然有很长的路要走。
- en: Upcoming developments in reinforcement learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的未来发展
- en: The past few sections may have painted a stark outlook for deep learning and
    reinforcement learning. However, there is no need to feel entirely discouraged;
    this is, in fact, an exciting time for DL and RL, where many significant advances
    in research are continuing to shape the field and cause it to evolve at a rapid
    pace. With increasing availability of computational resources and data, the possibilities
    of expanding and improving deep learning and reinforcement learning algorithms
    continue to expand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节可能为深度学习（DL）和强化学习（RL）描绘了一个严峻的前景。然而，不必感到完全沮丧；事实上，现在正是深度学习和强化学习的激动人心时刻，许多重大的研究进展正在持续塑造该领域，并促使其以飞快的速度发展。随着计算资源和数据的不断增加，扩展和改进深度学习和强化学习算法的可能性也在不断扩展。
- en: Addressing the limitations
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决局限性
- en: For one, the issues raised in the preceding section are recognized and acknowledged
    by the research community. There are several efforts being made to address them.
    In the work by Pattanaik et. al., not only do the authors demonstrate that current
    deep reinforcement learning algorithms are susceptible to adversarial attacks,
    they also propose techniques that can make the same algorithms more robust toward
    such attacks. In particular, by training deep RL algorithms on examples that were
    adversarially perturbed, the model can improve its robustness against similar
    attacks. This technique is commonly referred to as adversarial training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，前述问题已被研究界认识和承认，正在有多个方向进行解决。在Pattanaik等人研究中，作者不仅展示了当前深度强化学习算法容易受到对抗性攻击的影响，还提出了可以使这些算法对这些攻击更具鲁棒性的方法。特别是，通过在经过对抗性扰动的示例上训练深度RL算法，模型能够提高其对类似攻击的鲁棒性。这一技术通常被称为对抗训练。
- en: Moreover, the research community is actively taking actions to solve the reproducibility
    problem. ICLR and ICML, two of the biggest conferences in machine learning, have
    hosted challenges where participants are invited to reimplement and re-run experiments
    conducted by submitted papers to reproduce the reported results. Participants
    are then required to critique the original work by writing a reproducibility report
    that describes the problem statement, experimental methodology, implementation
    details, analyses, and the reproducibility of the original paper. Organized by
    Joelle Pineau and McGill University, this challenge aims to promote transparency
    in experiments and academic work as well as to ensure the reproducibility and
    integrity of results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究界正在积极采取行动解决可复现性问题。ICLR和ICML是机器学习领域两个最大的会议，它们举办了挑战赛，邀请参与者重新实现并重新运行已提交论文中的实验，以复制报告的结果。参与者随后需要通过撰写可复现性报告来批评原始工作，报告应描述问题陈述、实验方法、实施细节、分析以及原始论文的可复现性。该挑战由Joelle
    Pineau和麦吉尔大学组织，旨在促进实验和学术工作的透明度，确保结果的可复现性和完整性。
- en: More information on the ICLR 2018 reproducibility challenge can be found here: [https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html](https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html).
    Similarly, the original ICML workshop on reproducibility can be found here: [https://sites.google.com/view/icml-reproducibility-workshop/home](https://sites.google.com/view/icml-reproducibility-workshop/home).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ICLR 2018可复现性挑战的更多信息可以在此找到：[https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html](https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html)。同样，关于ICML原始可复现性研讨会的信息可以在此找到：[https://sites.google.com/view/icml-reproducibility-workshop/home](https://sites.google.com/view/icml-reproducibility-workshop/home)。
- en: Transfer learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Another important topic that is increasing in importance and attention is transfer
    learning. Transfer learning is a paradigm in machine learning, where a model trained
    on one task is fine-tuned to accomplish another.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个越来越受到关注的重要话题是迁移学习。迁移学习是机器学习中的一种范式，其中在一个任务上训练的模型经过微调后，用于完成另一个任务。
- en: For example, we can train a model to recognize images of cars and use the weights
    of that model to initialize an identical model that learns to recognize trucks.
    The main intuition is that certain abstract concepts and features learned by training
    on one task are transferable to other similar tasks. This idea is applicable to
    many reinforcement learning problems as well. An agent that learns to play a particular
    Atari game should be able to play other Atari games proficiently without training
    entirely from scratch, much like how a human can.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以训练一个模型来识别汽车图像，并使用该模型的权重来初始化一个相同的模型，该模型学习识别卡车。主要的直觉是，通过在一个任务上进行训练学到的某些抽象概念和特征，可以迁移到其他类似的任务。这一思想同样适用于许多强化学习问题。一个学会玩特定Atari游戏的智能体应该能够熟练地玩其他Atari游戏，而不需要从头开始训练，就像人类一样。
- en: Demis Hassabis, the founder of DeepMind and a pioneer in deep reinforcement
    learning, said in a recent talk that transfer learning is the key to general intelligence. And
    I think the key to doing transfer learning will be the acquisition of conceptual
    knowledge that is abstracted away from perceptual details of where you learned
    it from.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Demis Hassabis，DeepMind的创始人和深度强化学习的先驱，在最近的一次演讲中提到，迁移学习是实现通用智能的关键。而我认为，成功实现迁移学习的关键在于获取概念性知识，这些知识是从你学习的地方的感知细节中抽象出来的。
- en: The Demis Hassabis quote and the talk in which this was mentioned can be found
    here: [https://www.youtube.com/watch?v=YofMOh6_WKo](https://www.youtube.com/watch?v=YofMOh6_WKo)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Demis Hassabis的引用和相关演讲可以在此找到：[https://www.youtube.com/watch?v=YofMOh6_WKo](https://www.youtube.com/watch?v=YofMOh6_WKo)
- en: There have already been several advances in computer vision and natural language
    processing, where models initialized with knowledge and priors from one domain
    are used to learn about data from another domain.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉和自然语言处理领域，已经有多个进展，其中利用从一个领域初始化的知识和先验知识来学习另一个领域的数据。
- en: 'This is especially useful when the second domain lacks data. Called **few-shot**
    or **one-shot** learning, these techniques allow models to learn to perform tasks
    well, even when the dataset is small, as illustrated in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这在第二领域缺乏数据时尤其有用。被称为**少样本**或**单样本**学习，这些技术允许模型即使在数据集较小的情况下，也能很好地学习执行任务，如下图所示：
- en: '![](img/13a45856-a815-4b92-9a54-68b401f7db6a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13a45856-a815-4b92-9a54-68b401f7db6a.png)'
- en: An illustration of a few-shot learning classifier learning good decision boundaries
    for classes with small volumes of data
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于少样本学习分类器学习如何为数据量较少的类别划分良好决策边界的示例
- en: Few-shot learning for reinforcement learning would involve having the agent
    learn to achieve high proficiency on a given task without a high dependence on
    time, data, and computational resources. Imagine a generalized game-playing agent
    that can easily be fine-tuned to perform well on any other video game using readily-available
    computational resources; this would make training RL algorithms a lot more efficient
    and thus more accessible to a wider audience.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的少样本学习涉及让智能体在给定任务上达到高水平的熟练度，而不依赖于大量的时间、数据和计算资源。设想一个可以轻松微调以在任何其他视频游戏中表现良好的通用游戏玩家智能体，且使用现成的计算资源；这将使强化学习算法的训练更加高效，从而更易于让更广泛的受众访问。
- en: Multi-agent reinforcement learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多智能体强化学习
- en: 'Another promising area making significant strides is multi-agent reinforcement
    learning. Contrary to the problems we''ve seen where only one agent makes decisions,
    this topic involves having multiple agents make decisions simultaneously and cooperatively
    in order to achieve a common objective. One of the most significant works related
    to this has been OpenAI''s Dota2-playing system, called **OpenAI Five**. Dota2
    is one of the world''s most popular **Massively Multiplayer Online Role Playing
    Game** (**MMORPGs**). Compared to traditional RL games such as Go and Atari, Dota2
    is more complex for the following reasons:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个取得显著进展的有前景领域是多智能体强化学习。与我们之前看到的只有一个智能体做出决策的问题不同，这一主题涉及多个智能体同时并协作地做出决策，以实现共同目标。与此相关的最重要的工作之一是OpenAI的Dota2对战系统，名为**OpenAI
    Five**。Dota2是世界上最受欢迎的**大型多人在线角色扮演游戏**（**MMORPGs**）之一。与围棋和Atari等传统的强化学习游戏相比，Dota2由于以下原因更为复杂：
- en: '**Multiple agents**: Dota2 games involve two teams of five players, each fighting
    to destroy the other team''s base. Hence there are multiple agents, not just one,
    making decisions simultaneously.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个智能体**：Dota2游戏包含两支五人队伍，每支队伍争夺摧毁对方的基地。因此，决策不仅仅由一个智能体做出，而是由多个智能体同时做出。'
- en: '**Observability**: The screen only shows the proximity of the agent''s character
    instead of the whole map. This means that the whole game state, including the
    locations of opponents and what they are doing, is not observable. In reinforcement
    learning, we call this a *partially-observable* state.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：屏幕仅显示智能体角色的周围环境，而不是整个地图。这意味着游戏的整体状态，包括对手的位置和他们的行动，是不可观察的。在强化学习中，我们称这种情况为*部分可观察*状态。'
- en: '**High dimensionality**: A Dota2 agent''s observations can include 20,000 points,
    each depicting what a human player may observe on the screen, including health,
    the location of the controlling character, the location of enemies, and any attacks.
    Go, on the other hand, requires fewer data points to construct an observation
    (19 x 19 board, past moves). Hence, observations have high dimensionality and
    complexity. This also goes for decisions, where a Dota2 AI''s action space consists
    of 170,000 possibilities, which includes decisions on movement, casting spells,
    and using items.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维度性**：Dota2智能体的观察可以包括20,000个数据点，每个点展示了人类玩家可能在屏幕上看到的内容，包括健康状态、控制角色的位置、敌人的位置以及任何攻击。而围棋则需要更少的数据点来构建一个观察（19
    x 19棋盘，历史走法）。因此，观察具有高维度性和复杂性。这同样适用于决策，Dota2 AI的动作空间包含17万个可能性，包括移动、施放技能和使用物品的决策。'
- en: For more information on OpenAI's Dota2 AI, check out their blogs on the project
    at [https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于OpenAI的Dota2 AI的信息，请查看他们的项目博客：[https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/)。
- en: 'Moreover, by using novel upgrades on traditional reinforcement learning algorithms,
    each agent in OpenAI Five was able to learn to cooperate with one another in order
    to reach the common objective of destroying the enemy''s base. They were even
    able to learn several team strategies that experienced human players employ. The
    following is a screenshot from a game being played between a team of Dota players
    and OpenAI Five:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过对传统强化学习算法进行创新升级，OpenAI Five中的每个智能体都能够学会与其他智能体协作，共同实现摧毁敌方基地的目标。它们甚至能够学习到一些经验丰富的玩家使用的团队策略。以下是Dota2玩家队伍与OpenAI
    Five之间比赛的一张截图：
- en: '![](img/c596eab2-e204-41ac-930a-2206475bec90.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c596eab2-e204-41ac-930a-2206475bec90.png)'
- en: OpenAI versus human players (source: [https://www.youtube.com/watch?v=eaBYhLttETw](https://www.youtube.com/watch?v=eaBYhLttETw))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 对抗人类玩家（来源：[https://www.youtube.com/watch?v=eaBYhLttETw](https://www.youtube.com/watch?v=eaBYhLttETw)）
- en: Despite the extreme levels of resource requirements (240 GPUs, 120,000 CPU cores,
    ~200 human years of gameplay in a single day), this project demonstrates that
    current AI algorithms are indeed able to cooperate with one another to reach a
    common objective in a vastly complex environment. This work symbolizes another
    significant advancement in AI and RL research and demonstrates what the current
    technology is capable of.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个项目需要极高的资源要求（240个GPU、120,000个CPU核心、约200年人类游戏时间），它展示了当前的AI算法确实能够在一个极为复杂的环境中互相合作，达成共同目标。这项工作象征着AI和强化学习研究的另一个重要进展，并展示了当前技术的潜力。
- en: Summary
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This concludes our introductory journey into reinforcement learning. Over the
    course of this book, we learned how to implement agents that can play Atari games,
    navigate Minecraft, predict stock market prices, play the complex board game of
    Go, and even generate other neural networks to train on `CIFAR-10` data. In doing
    so, you acquired and became accustomed to some of the fundamental and state-of-the-art
    deep learning and reinforcement learning algorithms. In short, you have achieved
    a lot!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们在强化学习的入门之旅的结束。在本书的过程中，我们学习了如何实现能够玩Atari游戏、在Minecraft中导航、预测股市价格、玩复杂的围棋棋盘游戏，甚至生成其他神经网络来训练`CIFAR-10`数据的智能体。在此过程中，您已经掌握并习惯了许多基础的和最先进的深度学习与强化学习算法。简而言之，您已经取得了很多成就！
- en: But the journey does not and should not end here. We hope that, with your newfound
    skills and knowledge, you will continue to utilize deep learning and reinforcement
    learning algorithms to tackle problems that you face outside of this book. More
    importantly, we hope that this guide motivates you to explore other fields of
    machine learning and further develop your knowledge and experience.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但这段旅程并不会也不应当就此结束。我们希望，凭借您新获得的技能和知识，您将继续利用深度学习和强化学习算法，解决本书之外的实际问题。更重要的是，我们希望本指南能激励您去探索机器学习的其他领域，进一步发展您的知识和经验。
- en: There are many obstacles for the reinforcement learning community to overcome.
    However, there is much to look forward to. With the increasing popularity and
    development of the field, we can't wait to see what new developments and milestones
    the field will achieve. We hope the reader, upon completing this guide, will feel
    more equipped and ready to build reinforcement learning algorithms and make significant
    contributions to the field.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习社区面临许多障碍需要克服。然而，未来值得期待。随着该领域的日益流行和发展，我们迫不及待想要看到该领域将取得的新进展和里程碑。我们希望读者在完成本指南后，能够感到更加充实并准备好构建强化学习算法，并为该领域做出重要贡献。
- en: References
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Open Science Collaboration. (2015). *Estimating the reproducibility of psychological
    science*. Science, 349(6251), aac4716.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Open Science Collaboration. (2015)。*估算心理学科学的可重复性*。Science, 349(6251), aac4716。
- en: Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
    (2017). *Deep reinforcement learning that matters*. arXiv preprint arXiv:1709.06560.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., 和 Meger, D. (2017)。*真正重要的深度强化学习*。arXiv预印本
    arXiv:1709.06560。
- en: Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., and Chowdhary, G. (2018, July).
    *Robust deep reinforcement learning with adversarial attacks*. In Proceedings
    of the 17th International Conference on Autonomous Agents and MultiAgent Systems (pp.
    2040-2042). International Foundation for Autonomous Agents and Multiagent Systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., 和 Chowdhary, G. (2018年7月)。*抗干扰的强大深度强化学习*。载于第17届国际自主代理与多智能体系统会议论文集（第2040-2042页）。国际自主代理与多智能体系统基金会。
