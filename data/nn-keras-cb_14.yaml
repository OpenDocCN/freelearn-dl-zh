- en: End-to-End Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端学习
- en: In the previous chapters, we have learnt about analyzing sequential data (text)
    using the **Recurrent Neural Network** (**RNN**), and also about analyzing image
    data using the **Convolutional Neural Network** (**CNN**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何使用**循环神经网络**（**RNN**）分析顺序数据（文本），以及如何使用**卷积神经网络**（**CNN**）分析图像数据。
- en: 'In this chapter, we will be learning about using the CNN + RNN combination
    to solve the following case studies:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何使用CNN + RNN组合来解决以下案例研究：
- en: Handwritten-text recognition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写文本识别
- en: Generating caption from image
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像生成标题
- en: Additionally, we will also be learning about a new loss function called **Connectionist
    Temporal Classification** (**CTC**) loss while solving the handwritten-text-recognition
    problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将学习一种新的损失函数——**连接主义时间分类**（**CTC**）损失，用于解决手写文本识别问题。
- en: Finally, we will be learning about beam search to come up with plausible alternatives
    to the generated text, while solving the caption generating from image problem.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将学习如何使用束搜索（beam search）来为生成的文本提出合理的替代方案，同时解决从图像生成标题的问题。
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Consider a scenario where we are transcribing the image of a handwritten text.
    In this case, we would be dealing with image data and also sequential data (as
    the content in the image needs to be transcribed sequentially).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在转录手写文本的图像。在这种情况下，我们既要处理图像数据，又要处理顺序数据（因为图像中的内容需要顺序转录）。
- en: 'In traditional analysis, we would have hand-crafted the solution—for example:
    we might have slid a window across the image (where the window is of the average
    size of a character) so that the window would detect each character, and then
    output characters that it detects, with high confidence.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统分析中，我们通常会手工设计解决方案——例如：我们可能会在图像上滑动一个窗口（该窗口的大小大致与一个字符相当），使得窗口能够检测每个字符，然后输出它所检测到的字符，并且具有较高的置信度。
- en: However, in this scenario, the size of the window or the number of windows we
    shall slide is hand crafted by us—which becomes a feature-engineering (feature
    generation) problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，窗口的大小或我们滑动的窗口数量是由我们手工设计的——这就成为了特征工程（特征生成）问题。
- en: A more end-to-end approach shall be extracting the features obtained by passing
    the image through a CNN and then passing these features as inputs to various time
    steps of an RNN, so that we extract the output at various time steps.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 更加端到端的方法是通过将图像传递给CNN来提取特征，然后将这些特征作为输入传递给RNN的各个时间步，以便在各个时间步提取输出。
- en: Thus, we will be using a combination of CNN and RNN, and by approaching the
    problem this way, we do not have to build a hand-crafted feature at all and let
    the model figure the optimal parameters of CNN and RNN.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将使用CNN和RNN的组合，通过这种方式处理问题，我们不需要构建任何手工设计的特征，而是让模型自行调整CNN和RNN的最优参数。
- en: Connectionist temporal classification (CTC)
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接主义时间分类（CTC）
- en: One of the limitations to perform supervised learning on top of handwritten
    text recognition or in speech transcription is that, using a traditional approach,
    we would have to provide the label of which part of the image contain a certain
    character (in the case of hand-writing recognition) or which subsegment of the
    audio contains a certain phoneme (multiple phonemes combine to form a word utterance).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在手写文本识别或语音转录的监督学习中，传统方法的一大限制是，我们需要提供图像中哪些部分包含某个字符的标签（在手写识别中），或哪个音频子段包含某个音素（多个音素组合形成一个单词发音）。
- en: However, providing the ground truth for each character in image, or each phoneme
    in speech transcription, is prohibitively costly when building the dataset, where
    there are thousands of words or hundreds of hours of speech to transcribe.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在构建数据集时，为每个字符在图像中的位置或每个音素在语音转录中的位置提供真实标签是非常昂贵的，因为要转录的数据集可能包含成千上万的单词或数百小时的语音。
- en: CTC comes in handy to address the issue of not knowing the mapping of different
    parts of images to different characters. In this section, we will learn about
    how CTC loss functions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CTC对于解决图像的不同部分与不同字符之间的映射问题非常有用。在这一节中，我们将学习CTC损失函数的原理。
- en: Decoding CTC
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码CTC
- en: 'Let''s say, we are transcribing an image that contains the text **ab**. The
    example can look like any of the following (with varying space between the characters
    **a** and **b**) and the output label (ground truth) is just the same **ab**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在转录一个包含**ab**文本的图像。这个例子可能看起来像以下任意一种（字符**a**和**b**之间的空格不同），而输出标签（地面真相）仍然是相同的**ab**：
- en: '![](img/cf409f12-0d59-4815-8c13-0abc072eb181.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf409f12-0d59-4815-8c13-0abc072eb181.png)'
- en: 'In the next step, we shall divide these examples into multiple time steps,
    as follows (where each box represents a time step):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步，我们将这些例子分成多个时间步，如下所示（每个框代表一个时间步）：
- en: '![](img/77d67303-323b-495a-a8c4-8170d6abca28.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77d67303-323b-495a-a8c4-8170d6abca28.png)'
- en: In the preceding example, we have a total of six time steps (each cell represents
    a time step).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们总共有六个时间步（每个单元格代表一个时间步）。
- en: We shall be predicting the output from each time step where the output of a
    time step is the softmax across the vocabulary.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从每个时间步预测输出，其中每个时间步的输出是整个词汇表的softmax。
- en: 'Given that we are performing softmax, let''s say the output of each time step
    for the first picture of **ab** is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在执行softmax，让我们看一下**ab**的第一个图片中每个时间步的输出如下：
- en: '![](img/1b3bac58-0c5a-48a2-a533-44f676518f0b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b3bac58-0c5a-48a2-a533-44f676518f0b.png)'
- en: Note that, the **-** in the preceding picture represents a blank space. Additionally,
    the output in the fourth and fifth time steps can be **b** if the features of
    the image are passed through a bidirectional LSTM (or GRU)—as the information
    in the next time step can also influence the output in a previous time step while
    performing a bidirectional analysis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前图中的**-**表示一个空格。此外，如果图像的特征通过双向LSTM（或GRU）传递，则第四和第五时间步的输出可以是**b**——因为下一时间步中的信息也可以影响执行双向分析时的前一个时间步的输出。
- en: In the final step, we shall be squashing all the softmax outputs that have the
    same value in consecutive time steps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们将压缩所有在连续时间步中具有相同值的softmax输出。
- en: The preceding results in our final output being: **-a-b-** for this example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果导致我们最终的输出为：**-a-b-**（以此示例为准）。
- en: If, in case the ground truth is **abb**, we shall expect a **-** in between
    the two **b**s so that the consecutive **b**s do not get squashed into one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果地面真相是**abb**，我们将期望在两个**b**之间有一个**-**，以避免连续的**b**被压缩成一个。
- en: Calculating the CTC loss value
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算CTC损失值
- en: 'For the problem we were solving in the previous section, let''s consider we
    have the following scenario, where the probability of having the character in
    a given time step is provided in the circle of the following diagram (note that,
    the probabilities add up to one in each time step from **t0** to **t5**):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在上一节中解决的问题，假设我们有以下情形，其中给定时间步中某个字符出现的概率如图中圆圈所示（请注意，每个时间步从**t0**到**t5**的概率和为1）：
- en: '![](img/e6ae3e2b-0198-4204-93e2-c3099b144404.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6ae3e2b-0198-4204-93e2-c3099b144404.png)'
- en: 'However, to keep the calculation simple for us to understand, let''s consider
    the scenario where the ground truth is **a** and not **ab** and also that the
    output has only three time steps and not six. The modified output across the three
    time steps looks as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了使计算更简洁，便于我们理解，假设地面真相是**a**而非**ab**，并且假设输出仅有三个时间步而不是六个。修改后的三个时间步的输出如下：
- en: '![](img/f49d376c-e7ca-493c-9c01-8642c6ba3ff2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f49d376c-e7ca-493c-9c01-8642c6ba3ff2.png)'
- en: 'We can obtain the ground truth of **a** if the softmax in each time step is
    any of the following scenarios:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个时间步的softmax值满足以下任一情形，我们可以得到**a**的地面真相：
- en: '| **Output in each time step** | **Prob of character in time step 1** | **Prob
    of character in time step 2** | **Prob of character in time step 3** | **Probability
    of combination** | **Final probability** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **每个时间步的输出** | **时间步1的字符概率** | **时间步2的字符概率** | **时间步3的字符概率** | **组合的概率**
    | **最终概率** |'
- en: '| - - a | 0.8 | 0.1 | 0.1 | 0.8 x 0.1 x 0.1 |  0.008 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| - - a | 0.8 | 0.1 | 0.1 | 0.8 x 0.1 x 0.1 |  0.008 |'
- en: '| - a a | 0.8 | 0.9 | 0.1 | 0.8 x 0.9 x 0.1 | 0.072 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| - a a | 0.8 | 0.9 | 0.1 | 0.8 x 0.9 x 0.1 | 0.072 |'
- en: '| a a a | 0.2 | 0.9 | 0.1 | 0.2 x 0.9 x 0.1 | 0.018 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| a a a | 0.2 | 0.9 | 0.1 | 0.2 x 0.9 x 0.1 | 0.018 |'
- en: '| - a - | 0.8 | 0.9 | 0.8 | 0.8 x 0.9 x 0.8 | 0.576 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| - a - | 0.8 | 0.9 | 0.8 | 0.8 x 0.9 x 0.8 | 0.576 |'
- en: '| - a a | 0.8 | 0.9 | 0.1 | 0.8 x 0.9 x 0.1 | 0.072 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| - a a | 0.8 | 0.9 | 0.1 | 0.8 x 0.9 x 0.1 | 0.072 |'
- en: '| a - -  | 0.2 | 0.1 | 0.8 | 0.2 x 0.1 x 0.8 | 0.016 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| a - -  | 0.2 | 0.1 | 0.8 | 0.2 x 0.1 x 0.8 | 0.016 |'
- en: '| a a - | 0.2 | 0.9 | 0.8 | 0.2 x 0.9 x 0.8 | 0.144 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| a a - | 0.2 | 0.9 | 0.8 | 0.2 x 0.9 x 0.8 | 0.144 |'
- en: '| **Overall probability** | **0.906** |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **总体概率** | **0.906** |'
- en: From the preceding results, we see that the overall probability of obtaining
    the ground truth **a** is 0.906.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的结果中我们可以看到，获得地面真值 **a** 的总体概率为 0.906
- en: CTC loss is the negative logarithm of the overall probability = *-log(0.906)
    = 0.04*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CTC 损失是总体概率的负对数 = *-log(0.906) = 0.04*。
- en: Note that, as the combination of characters with the highest probability in
    each time step indicate the ground truth of **a**, the CTC loss is close to zero.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于每个时间步中概率最高的字符组合表示地面真值**a**，因此 CTC 损失接近于零
- en: Handwritten-text recognition
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手写文本识别
- en: In this case study, we will be working toward transcribing the handwritten images
    so that we extract the text that is present in the pictures.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将致力于将手写图像转录成文本，从而提取出图像中的文字
- en: 'A sample of the handwriting looks as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个手写样本如下所示：
- en: '![](img/900d721e-6a43-4de5-9ced-850bc6332af3.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/900d721e-6a43-4de5-9ced-850bc6332af3.png)'
- en: Note that in the preceding diagram, the handwritten characters have varied length,
    the images are of different dimensions, the separation between the characters
    is varied, and the images are of different quality.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上图中，手写字符的长度各不相同，图像的尺寸不同，字符之间的间距各异，且图像的质量不同
- en: In this section, we will be learning about using CNN, RNN, and the CTC loss
    function together to transcribe the handwritten examples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何使用 CNN、RNN 和 CTC 损失函数结合起来进行手写示例的转录
- en: Getting ready
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The strategy we will adopt to transcribe the handwritten examples is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的手写示例转录策略如下：
- en: 'Download images that contain images of handwritten text:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载包含手写文本图像的图像：
- en: Multiple datasets containing handwritten text images are provided in the code
    file associated with this case study in GitHub
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在与该案例研究相关的 GitHub 代码文件中提供了多个包含手写文本图像的数据集
- en: Ensure that, along with the images, you have also taken the ground truth corresponding
    to the images
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保在获得图像的同时，也获取了与图像对应的地面真值
- en: Resize all images to be of the same size, let's say 32 x 128 in size
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有图像调整为相同的大小，比如 32 x 128 尺寸
- en: 'While resizing, we should also ensure that the aspect ratio of the picture
    is not distorted:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调整图像大小时，我们还应确保图像的纵横比没有被扭曲：
- en: This is to ensure that images cannot look very blurred because the original
    image was changed to 32 x 128 in size
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以确保图像不会因为原始图像被调整为 32 x 128 尺寸而显得模糊不清
- en: We'll resize the images without distorting the aspect ratio, and then superimpose
    each of them on a different blank 32 x 128 image
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将调整图像大小，确保不扭曲纵横比，然后将每个图像叠加到不同的空白 32 x 128 图像上
- en: Invert the colors of the images so that the background is in black and the handwritten
    content is in white
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反转图像的颜色，使背景为黑色，手写内容为白色
- en: Scale the images so that their value is between zero and one
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放图像，使其数值介于零和一之间
- en: 'Pre-process the output (ground truth):'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理输出（地面真值）：
- en: Extract all the unique characters in output
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取输出中的所有唯一字符
- en: Assign an index for each character
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个字符分配一个索引
- en: Find the maximum length of output, and then ensure that the number of time steps
    for which we are predicting the content of time step is more than the maximum
    length of output
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到输出的最大长度，然后确保我们预测的时间步数超过输出的最大长度
- en: Ensure the same length of output for all outputs by padding the ground truths
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对地面真值进行填充，确保所有输出的长度相同
- en: Pass the processed picture through a CNN so that we extract features that are
    of 32 x 256 in shape
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将处理过的图像通过 CNN 进行处理，以便我们提取出形状为 32 x 256 的特征
- en: Pass the extracted features from CNN through GRU unit that is bidirectional
    so that we encapsulate the information that is present in adjacent time steps
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将从 CNN 中提取的特征传递到双向的 GRU 单元中，以便我们能够封装相邻时间步中的信息
- en: Each of the 256 features in 32 time steps is an input for the respective time
    step
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时间步的 256 个特征作为该时间步的输入
- en: Pass the output through a dense layer that has as many output values as the
    total number of unique characters in ground truth (the padded value (**-** in
    the example given in the introduction to the CTC loss section) shall also be one
    of the unique characters—where the padded value **-** represents either the space
    between characters, or the padding in the blank portion of the picture
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出通过一个稠密层，该层的输出值与地面真值中的唯一字符总数相同（在CTC损失部分介绍中给出的示例中，填充值（**-**）也将是唯一字符之一——其中填充值
    **-** 表示字符之间的空格，或图片空白部分的填充
- en: Extract the softmax and its corresponding output character at each of the 32
    time steps
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个32个时间步中提取softmax及其对应的输出字符
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The preceding algorithm in code is performed as follows (the code file is available
    as `Handwritten_text_recognition.ipynb` in GitHub):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码中的前述算法如下执行（代码文件在GitHub上作为`Handwritten_text_recognition.ipynb`可用）：
- en: Download and import the dataset. This dataset will contain the images of handwritten
    text and their corresponding ground truth (transcription).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并导入数据集。该数据集包含手写文本的图像及其对应的地面真值（转录）。
- en: 'Build a function that resizes pictures without distorting the aspect ratio
    and pad the rest of pictures so that all of them have the same shape:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个函数，调整图片大小而不扭曲其宽高比，并填充其余图片，使它们都具有相同的形状：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we are creating a blank picture (named `target`). In
    the next step, we have reshaped the picture to maintain its aspect ratio.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们创建了一个空白图片（名为 `target`）。在下一步中，我们已经重塑了图片以保持其宽高比。
- en: Finally, we have overwritten the rescaled picture on top of the blank one we
    created, and have returned the picture where the background is in black (255-target).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们覆盖了我们创建的空白图片的重新缩放图片，并返回了背景为黑色的图片（255-target）。
- en: 'Read the pictures and store them in a list, as shown in the following code:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取图片并将其存储在列表中，如下所示的代码中所示：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we are extracting each picture and also are modifying
    it per the function that we defined. The input and modified examples for different
    scenario:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们提取了每个图片，并根据我们定义的函数进行了修改。输入和不同场景的修改示例：
- en: '![](img/cb7b3958-fbd3-4d1c-b6cb-40a4cc1ec9be.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb7b3958-fbd3-4d1c-b6cb-40a4cc1ec9be.png)'
- en: 'Extract the unique characters in output, shown as follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取输出中的唯一字符，如下所示：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create the output ground truth, as demonstrated in the following code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输出地面真值，如下所示的代码中所示：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, we are storing the index of each character in an output
    into a list. Additionally, if the output is less than 32 characters in size, we
    pad it with 79, which represents the blank value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们将每个字符的索引存储到一个列表中。此外，如果输出的大小少于32个字符，我们会用79进行填充，79表示空白值。
- en: Finally, we are also storing the label length (in the ground truth) and also
    the input length (which is always 32 in size).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还存储标签长度（在地面真值中）和输入长度（始终为32）。
- en: 'Convert the input and output into NumPy arrays, as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和输出转换为NumPy数组，如下所示：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the objective, as shown here:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标，如下所示：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are initializing 32 zeros, as the batch size will be 32\. For each value
    in batch size, we expect the loss value to be zero.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化32个零，因为批量大小将为32。对于批量大小中的每个值，我们期望损失值为零。
- en: 'Define the CTC loss function as follows:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义CTC损失函数如下：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding function takes the predicted values, ground truth (labels) and
    input, label lengths as input and calculates the CTC loss value.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前述功能将预测值、地面真值（标签）和输入、标签长度作为输入，并计算CTC损失值。
- en: 'Define the model, demonstrated as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型，如下所示：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding code, we are building the CNN that converts a picture with
    32 x 128 shape into a picture of 32 x 256 in shape:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们正在构建CNN，将具有32 x 128形状的图片转换为具有32 x 256形状的图片：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The architecture of model till the layers defined previously are as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止定义的模型的体系结构如下所示：
- en: '![](img/ceeb523d-37f3-4c1d-9828-32dc1dddfe58.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ceeb523d-37f3-4c1d-9828-32dc1dddfe58.png)'
- en: 'In the preceding code, we are passing the features obtained from CNN into a
    GRU. The architecture defined previously continues from the preceding graph shown
    is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们将从CNN获取的特征传递到GRU。如前面所示的定义的体系结构继续从所示的图形开始如下：
- en: '![](img/3703ea67-229b-4f73-bc63-a71be57a06d0.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3703ea67-229b-4f73-bc63-a71be57a06d0.png)'
- en: 'In the following code, we are concatenating the output of two GRUs so that
    we take both bidirectional GRU and normal GRU-generated features into account:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将两个 GRU 的输出进行拼接，从而同时考虑双向 GRU 和普通 GRU 生成的特征：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The architecture after adding the preceding layer is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 添加前面一层后的架构如下：
- en: '![](img/d3b78853-dfea-445c-8be8-538b17c2b466.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3b78853-dfea-445c-8be8-538b17c2b466.png)'
- en: 'In the following code, we are passing the features of GRU output through a
    dense layer and applying softmax to get one of the possible 80 values as output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们通过一个全连接层传递 GRU 输出的特征，并应用 softmax 得到 80 个可能值中的一个作为输出：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The architecture of the model continues as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的架构继续如下：
- en: '![](img/1f69ad73-9c27-40b1-8720-e37833d6fce1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f69ad73-9c27-40b1-8720-e37833d6fce1.png)'
- en: 'Initialize the variables that are required for the CTC loss:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 CTC 损失所需的变量：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, we are mentioning that `y_pred` (predicted character
    values), actual labels, input length, and the label length are the inputs to the
    CTC loss function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们提到 `y_pred`（预测的字符值）、实际标签、输入长度和标签长度是 CTC 损失函数的输入。
- en: 'Build and compile the model as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式构建并编译模型：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that there are multiple inputs that we are passing to our model. The CTC
    calculation is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们传递给模型的输入有多个。CTC 计算如下：
- en: '![](img/8ad595f7-6135-494d-8787-4979a3460b79.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ad595f7-6135-494d-8787-4979a3460b79.png)'
- en: 'Create the following vectors of inputs and outputs:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建以下输入和输出向量：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Fit the model on multiple batches of pictures, demonstrated in the following
    code:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多个批次的图片上拟合模型，代码如下所示：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we are sampling 32 pictures at a time, converting them
    into an array, and fitting the model to ensure that the CTC loss is zero.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们一次抽取 32 张图片，将它们转换为数组，并拟合模型以确保 CTC 损失为零。
- en: Note that, we are excluding the last 100 pictures (in `x2`) from passing as
    input to model, so that we can test our model's accuracy on that data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们排除了最后 100 张图片（在 `x2` 中），不将其作为输入传递给模型，以便测试模型在该数据上的准确性。
- en: Furthermore, we are looping through the total dataset multiple times, as fetching
    all pictures into RAM and converting them into an array is very likely to crash
    the system, due to the huge memory requirement.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们多次遍历整个数据集，因为将所有图片加载到 RAM 并转换为数组很可能会导致系统崩溃，因为需要大量内存。
- en: 'The training loss over increasing epochs is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练轮数的增加，训练损失如下所示：
- en: '![](img/65786b6a-f385-4f80-a721-9e7111c729f5.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65786b6a-f385-4f80-a721-9e7111c729f5.png)'
- en: 'Predict the output at each time for a test picture, using the following code:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码预测测试图片在每个时间步的输出：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, we are discarding the output if the predicted character
    at a time step is the character of 79.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，如果在某个时间步预测的字符是 79 号字符，我们将丢弃该输出：
- en: 'A test examples and its corresponding predictions (in title) are as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 测试示例及其对应的预测（标题中）如下：
- en: '![](img/3841110b-83d4-4e17-a9ed-1a43606a7f33.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3841110b-83d4-4e17-a9ed-1a43606a7f33.png)'
- en: Image caption generation
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像字幕生成
- en: In the previous case study, we learned about using CNN, RNN, and CTC loss together
    to transcribe the handwritten digits.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的案例研究中，我们学习了如何将 CNN、RNN 和 CTC 损失一起使用，以转录手写数字。
- en: In this case study, we will learn about integrating CNN and RNN architectures
    to generate captions for a given picture.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们将学习如何将 CNN 和 RNN 架构结合起来，以为给定图片生成字幕。
- en: 'Here is a sample of the picture:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是图片的一个样本：
- en: '![](img/d236b475-d8ec-405c-bc4a-712208d8b564.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d236b475-d8ec-405c-bc4a-712208d8b564.png)'
- en: A girl in red dress with Christmas tree in background
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位穿红色裙子的女孩，背景是圣诞树
- en: A girl is showing the Christmas tree
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位女孩正在展示圣诞树
- en: A girl is playing in the park
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位女孩正在公园里玩耍
- en: A girl is celebrating Christmas
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位女孩正在庆祝圣诞节
- en: Getting ready
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In this section, let''s list the strategy that we shall adopt to transcribe
    pictures:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们列出转录图片的策略：
- en: We'll be working toward generating captions from pictures by working on a dataset
    that has images as well as the captions associated with the images. Links of datasets
    that have images and their corresponding captions are provided in the corresponding
    notebook in GitHub.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将致力于通过处理一个包含图片和与之相关的描述的数据集，生成图片的字幕。包含图片及其对应字幕的数据集链接会在 GitHub 上的相关笔记本中提供。
- en: We'll extract the VGG16 features of each picture.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将提取每张图片的 VGG16 特征。
- en: 'We will also preprocess the captions text:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将对字幕文本进行预处理：
- en: Convert all words to lower case
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有单词转换为小写
- en: Remove punctuation
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除标点符号
- en: Add start and end tokens to each caption
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个字幕添加开始和结束标记
- en: Keep only the pictures that are of a dog or a girl (we are performing this analysis
    only so that we train our model faster, as it takes ~5 hours to run this model,
    even on a GPU).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只保留狗或女孩的图片（我们执行此分析仅仅是为了加速模型训练，因为即使使用 GPU，运行此模型也需要大约 5 小时）。
- en: Assign an index to each unique word in the vocabulary of captions.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为字幕词汇表中的每个唯一单词分配索引。
- en: Pad all captions (where each word is represented by an index value) so that
    all captions are now of the same size.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充所有字幕（每个单词由索引值表示），以确保所有字幕现在都是相同的大小。
- en: To predict the first word, the model shall take the combination of the VGG16
    features and the embedding of the start token.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了预测第一个词，模型应将 VGG16 特征与开始标记的嵌入组合起来作为输入。
- en: Similarly, to predict the second word, the model will take the combination of
    the VGG16 features and the embedding combination of start token and the first
    word.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，为了预测第二个词，模型将采用 VGG16 特征和开始标记及第一个词的嵌入组合。
- en: In a similar manner, we proceed to fetch all the predicted words.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以类似方式，我们继续获取所有预测的词。
- en: We continue with the preceding steps until we predict the end token.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们继续执行前面的步骤，直到预测到结束标记。
- en: How to do it...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We''ll code up the strategy that we have defined previously, as follows (the
    code file is available as `Image_captioning.ipynb` in GitHub):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写之前定义的策略，如下所示（代码文件可在 GitHub 上找到 `Image_captioning.ipynb`）：
- en: Download and import a dataset that contains images and their corresponding captions.
    The recommended datasets are provided in GitHub
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并导入一个包含图像及其相应字幕的数据集。推荐的数据集可在 GitHub 上找到。
- en: 'Import the relevant packages, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包，如下所示：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Load the caption dataset, shown in the following code:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载字幕数据集，如以下代码所示：
- en: '[PRE17]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Load the pictures and store the VGG16 features:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图片并存储 VGG16 特征：
- en: '[PRE18]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Convert the VGG16 features into NumPy arrays:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 VGG16 特征转换为 NumPy 数组：
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Build a function that removes the punctuation in captions, and also converts
    all words to lowercase:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，移除字幕中的标点符号，并将所有单词转换为小写：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the following code, we preprocess all the captions and also append the start
    and end tokens:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们预处理所有字幕并附加开始和结束标记：
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Append only the pictures that are of a child or a dog:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只附加属于儿童或狗的图片：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Extract all the unique words in captions, as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取字幕中的所有唯一单词，如下所示：
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Assign indexes to words in the vocabulary, demonstrated in the following code:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为词汇表中的单词分配索引，如以下代码所示：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Identify the maximum length of a caption so that we pad all captions to be
    of the same length:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定字幕的最大长度，以便我们将所有字幕填充到相同的长度：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Pad all the captions to be of the same length, as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有字幕填充到相同的长度，如下所示：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Build the model that takes pictures as input and creates features from it:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个模型，该模型以图片为输入并从中创建特征：
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Build a model that takes captions as input and creates features from it:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个模型，该模型以字幕为输入并从中创建特征：
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Concatenate the two models and come up with a softmax of probabilities across
    all the possible output words:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个模型连接起来，并对所有可能的输出词进行 softmax 概率计算：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Compile the model, as follows:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型，如下所示：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Fit the model, shown in the following code:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型，如以下代码所示：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code, we are looping through all the pictures, 32 at a time.
    Additionally, we are creating the input dataset in such a way that the first *n* number
    of output words in a caption are input along with the VGG16 features of a picture,
    and the corresponding output is the *n+1^(th)* word of the caption.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们以每次 32 张图片的速度遍历所有图片。此外，我们正在创建输入数据集，方式是将字幕中的前 *n* 个输出词与图片的 VGG16 特征一起作为输入，而相应的输出则是字幕中的第
    *n+1^(th)* 个词。
- en: Furthermore, we are dividing the VGG16 features (`x3`) by 12, as we need to
    scale the input values between zero and one.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将 VGG16 特征（`x3`）除以 12，因为我们需要将输入值缩放到 0 到 1 之间。
- en: 'The output caption of a sample picture can be obtained as follows:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过如下方式获取示例图片的输出字幕：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/4344dfc9-c830-4c95-abbe-a425403cb3d0.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4344dfc9-c830-4c95-abbe-a425403cb3d0.jpg)'
- en: 'The output is decoded as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出解码如下：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/6ef9c565-3bcb-4d8b-bc54-4776d13e1022.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ef9c565-3bcb-4d8b-bc54-4776d13e1022.png)'
- en: Note that the generated caption correctly detected that the dog is black and
    is also jumping.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成的字幕正确地检测到了狗是黑色的，并且正在跳跃。
- en: Generating captions, using beam search
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成字幕，使用束搜索
- en: In the previous section on caption generation, we have decoded based on the
    word that has the highest probability in a given time step. In this section, we'll
    improve upon the predicted captions by using beam search.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面关于字幕生成的部分，我们根据给定时间步中概率最高的单词进行了解码。在本节中，我们将通过使用束搜索来改进预测的字幕。
- en: Getting ready
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Beam search works as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索的工作原理如下：
- en: Extract the probability of various words in first time step (where VGG16 features
    of the picture and the start token are the input)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取第一时间步中各种单词的概率（其中VGG16特征和开始标记是输入）
- en: Instead of providing the most probable word as the output, we'll consider the
    top three probable words
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不会仅提供最可能的单词作为输出，而是会考虑前三个最可能的单词。
- en: We'll proceed to the next time step, where we extract the top three characters
    in this time step
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将进入下一个时间步，在该时间步提取前三个字符
- en: 'We''ll loop through the top three predictions in first time step, as an input
    to the prediction of second time step, and extract the top three predictions for
    each of the possible top three predictions in input:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将循环遍历第一时间步的前三个预测，将其作为第二时间步预测的输入，并为每个可能的前三个输入预测提取前三个预测：
- en: Let's say that *a*, *b*, and *c* are the top three predictions in time-step
    one
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设*a*、*b*和*c*是第一时间步的前三个预测
- en: We'll use *a* as input along with VGG16 features to predict the top three probable
    characters in time-step two, and similarly for *b* and *c*
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用*a*作为输入，并结合VGG16特征来预测第二时间步中最可能的三个字符，同样地，对于*b*和*c*也是如此。
- en: We have a total of nine combinations of outputs between the first time- step
    and the second time-step
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在第一时间步和第二时间步之间有九个输出组合。
- en: 'Along with the combination, we''ll also store the confidence of each prediction
    across all nine combinations:'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了组合外，我们还将存储每个预测在所有九个组合中的置信度：
- en: 'For example: if the probability of *a* in time-step one is 0.4 and the probability
    of *x* in time step two is 0.5, then the probability of combination is 0.4 x 0.5
    = 0.2'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如：如果*a*在第一时间步的概率是0.4，而*x*在第二时间步的概率是0.5，那么组合的概率是0.4 x 0.5 = 0.2
- en: We'll keep the top three combinations and discard the rest of the combinations
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将保留前三个组合，丢弃其他组合
- en: We'll repeat the preceding step of shortlisting the top three combinations until
    we reach the end of the sentence
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将重复前一步骤，筛选出前三个组合，直到达到句子的结尾。
- en: The value of three is the beam length across which we are searching for the
    combination.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 三的值是我们在搜索组合时所用的束长度。
- en: How to do it...
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'In this section, we''ll code up the beam-search strategy that we discussed
    previously (the code file is available as `Image_captioning.ipynb` in GitHub):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写之前讨论过的束搜索策略的代码（代码文件可在GitHub中的`Image_captioning.ipynb`找到）：
- en: 'Define a function that takes the VGG16 features of the picture as input, along
    with the sequence of words and their corresponding confidences from the previous
    time steps and return the top three predictions in the current time step:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，接受图片的VGG16特征作为输入，连同来自前一步时间步的单词序列及其相应的置信度，并返回当前时间步的前三个预测：
- en: '[PRE34]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding step, we are separating the word IDs and their corresponding
    confidences provided in the `string_with_conf` parameter. Furthermore, we are
    storing the sequence of tokens in an array and using that to make a prediction.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们将单词ID及其对应的置信度从`string_with_conf`参数中分离出来。此外，我们将令牌序列存储在数组中，并用它来进行预测。
- en: In the next step, we are extracting the top three predictions in the next time
    step and storing it in `best_pred`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们提取下一个时间步的前三个预测，并将其存储在`best_pred`中。
- en: Additionally, along with the best prediction of word IDs, we are also storing
    the confidence associated with each top three prediction in the current time step.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了最好的单词ID预测，我们还会存储当前时间步内每个前三名预测的置信度。
- en: Finally, we are returning the three predictions of the second time step.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回第二时间步的三个预测。
- en: 'Loop through the range of the maximum possible length of sentence and extract
    the top three possible combinations of words across all time steps:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在句子的最大可能长度范围内循环，并提取所有时间步中的前三个可能的单词组合：
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Loop through the preceding `best_strings` obtained to print the output:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历之前获得的`best_strings`并打印输出：
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output sentences for the same picture that we tested in the previous section
    are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节测试的相同图片的输出句子如下：
- en: '![](img/ec9b27db-9d6e-46a2-9716-f0b4e19d595b.png)  ![](img/1448ba62-e00b-4a79-a24b-23d879f650c1.png)  ![](img/764e7e00-72e6-45cc-ae51-1fe3818710f2.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec9b27db-9d6e-46a2-9716-f0b4e19d595b.png)  ![](img/1448ba62-e00b-4a79-a24b-23d879f650c1.png)  ![](img/764e7e00-72e6-45cc-ae51-1fe3818710f2.png)'
- en: Note that, in this specific case, the first and second sentences differed when
    it came to the words `jumping` and `playing`, and the third sentence happened
    to be the same as the first, as the probability of combination was much higher.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个特定的案例中，第一句和第二句在“`jumping`”和“`playing`”这两个词上有所不同，而第三句恰好和第一句相同，因为组合的概率要高得多。
