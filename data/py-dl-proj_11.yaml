- en: Automated Image Captioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动图像描述
- en: In the previous chapter, we learned about building an object detection and classification
    model, which was really exciting. But in this chapter, we are going to do something
    even more impressive by combining current state-of-the-art techniques in both computer
    visionandnatural language processing to form a complete image description approach
    ([https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf)). This
    will be responsible for constructing computer-generated natural descriptions of
    any provided images.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了如何构建物体检测和分类模型，这非常令人兴奋。但在这一章中，我们将做一些更令人印象深刻的事情，结合计算机视觉和自然语言处理的当前最先进技术，形成一个完整的图像描述方法（[https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf)）。这将负责构建任何提供图像的计算机生成的自然描述。
- en: Our team has been asked to build this model to generate natural language descriptions
    of images to be used as the core intelligence of a company that wants to help
    the visually impaired take advantage of the explosion of photo sharing that's
    done on the web. It's exciting to think that this deep learning technology could
    have the power to effectively bring image content alive to this community. People
    who are likely to enjoy the outcome of our work are those who are visually impaired
    from birth right up to our aging population. Each of these user types and many
    more could use an image captioning bot that could be based on the model in this
    project so that they can keep up with family by knowing the content of posted
    images, for example.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的团队被要求构建这个模型，以生成自然语言的图像描述，用作一家公司的核心智能，该公司希望帮助视障人士利用网络上照片分享的爆炸式增长。想到这项深度学习技术可能具备有效地为这一社区带来图像内容的能力，令人兴奋。可能会喜欢我们工作的成果的人群包括从出生时就视力受损的人到老年人群体。这些用户类型及更多人群可以使用基于本项目模型的图像描述机器人，从而了解发布的图像内容，举个例子，他们可以跟上家人的动态。
- en: With this in mind, let's look at the deep learning engineering that we need
    to do. The idea is to replace the encoder (RNN layer) in an encoder-decoder architecture
    with a deep **convolutional neural network** (**CNN**) trained to classify objects
    in images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们来看一下我们需要做的深度学习工程。这个想法是用一个经过训练的深度**卷积神经网络**（**CNN**）替换编码器-解码器架构中的编码器（RNN层），该CNN用于分类图像中的物体。
- en: Normally, the CNN's last layer is the softmax layer, which assigns the probability
    that each object might be in the image. But if we remove that softmax layer from
    CNN, we can feed the CNN's rich encoding of the image into the decoder (language
    generation RNN) designed to produce phrases. We can then train the whole system
    directly on images and their captions, so it maximizes the likelihood that the
    descriptions it produces best match the training descriptions for each image.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN的最后一层是softmax层，它为每个物体分配该物体在图像中出现的概率。但如果我们将CNN中的softmax层去除，我们可以将CNN对图像的丰富编码传递给解码器（语言生成RNN），该解码器设计用于生成短语。然后，我们可以直接在图像及其描述上训练整个系统，这样可以最大化生成的描述与每个图像的训练描述最佳匹配的可能性。
- en: 'Here is the small illustration of the **Auto Image Captioning Model**. In the
    top left corner is the **Encoder-Decoder** architecture for sequence-to-sequence
    model which is combined with the **Object Detection model** as shown in the following
    diagram:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**自动图像描述模型**的小示意图。在左上角是**编码器-解码器**架构，用于序列到序列的模型，并与**物体检测模型**结合，如下图所示：
- en: '![](img/ec12594b-416d-4a3e-a4c0-db121bec1eae.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec12594b-416d-4a3e-a4c0-db121bec1eae.png)'
- en: In this implementation, we will be using a pretrained Inception-v3 model as
    a feature extractor in an encoder trained on the ImageNet dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中，我们将使用预训练的Inception-v3模型作为特征提取器，在ImageNet数据集上训练一个编码器。
- en: Data preparation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Let's import all of the dependencies that we will need to build an auto-captioning
    model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入构建自动描述模型所需的所有依赖项。
- en: All of the Python files and the Jupyter Notebooks for this chapter can be found
    at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter11](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter11).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有Python文件和Jupyter Notebook可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter11](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter11)找到。
- en: Initialization
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化
- en: 'For this implementation, we need a TensorFlow version greater than or equal
    to 1.9 and we will also enable the eager execution ([https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager))
    mode, which will help us use the debug the code more effectively. Here is the
    code for this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此实现，我们需要 TensorFlow 版本大于或等于 1.9，并且我们还将启用即时执行模式（[https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager)），这将帮助我们更有效地调试代码。以下是代码：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Download and prepare the MS-COCO dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并准备 MS-COCO 数据集
- en: 'We are going to use the MS-COCO dataset ([http://cocodataset.org/#home](http://cocodataset.org/#home)) to
    train our model. This dataset contains more than 82,000 images, each of which
    has been annotated with at least five different captions. The following code will
    download and extract the dataset automatically:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 MS-COCO 数据集 ([http://cocodataset.org/#home](http://cocodataset.org/#home))
    来训练我们的模型。该数据集包含超过 82,000 张图片，每张图片都至少有五个不同的标题注释。以下代码将自动下载并提取数据集：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This involves a large download ahead. We'll use the training set; it's a 13
    GB file.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这将涉及一个大的下载过程。我们将使用训练集，它是一个 13 GB 的文件。
- en: 'The following will be the output:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For this example, we''ll select a subset of 40,000 captions and use these and
    the corresponding images to train our model. As always, captioning quality will
    improve if you choose to use more data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将选择 40,000 个标题的子集，并使用这些标题及其对应的图片来训练模型。和往常一样，如果选择使用更多数据，标题质量会得到提升：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the data preparation is completed, we will have all of the image path
    stored in the `img_name_vector` list variable, and the associated captions are
    stored in `train_caption,` as shown in the following screenshot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备完成后，我们将把所有图片路径存储在 `img_name_vector` 列表变量中，相关的标题存储在 `train_caption` 中，如下图所示：
- en: '![](img/8f149cda-f700-48c4-94ef-ebaaa0e549c7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f149cda-f700-48c4-94ef-ebaaa0e549c7.png)'
- en: Data preparation for a deep CNN encoder
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 CNN 编码器的数据准备
- en: 'Next, we will use Inception-v3 (pretrained on ImageNet) to classify each image.
    We will extract features from the last convolutional layer. We will create a helper
    function that will transform the input image to the format that is expected by
    Inception-v3:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用预训练的 Inception-v3（在 ImageNet 上训练）来对每张图片进行分类。我们将从最后一个卷积层提取特征。我们将创建一个辅助函数，将输入图像转换为
    Inception-v3 期望的格式：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now let's initialize the Inception-v3 model and load the pretrained ImageNet
    weights. To do so, we'll create a `tf.keras` model where the output layer is the
    last convolutional layer in the Inception-v3 architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们初始化 Inception-v3 模型，并加载预训练的 ImageNet 权重。为此，我们将创建一个 `tf.keras` 模型，其中输出层是
    Inception-v3 架构中的最后一个卷积层。
- en: 'While creating the `keras` model, you can see a parameter called `include_top=False`
    that indicates whether to include the fully connected layer at the top of the
    network or not:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 `keras` 模型时，你可以看到一个名为 `include_top=False` 的参数，它表示是否包括网络顶部的全连接层：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So, the `image_features_extract_model` is our deep CNN encoder, which is responsible
    for learning the features from the given image.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`image_features_extract_model` 是我们的深度 CNN 编码器，它负责从给定的图像中学习特征。
- en: Performing feature extraction
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行特征提取
- en: 'Now we will pre-process each image with the deep CNN encoder and dump the output
    to the disk:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用深度 CNN 编码器对每张图片进行预处理，并将输出保存到磁盘：
- en: We will load the images in batches using the `load_image()` helper function
    that we created before
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用之前创建的 `load_image()` 辅助函数按批加载图像
- en: We will feed the images into the encoder to extract the features
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把图像输入编码器以提取特征
- en: 'Dump the features as a `numpy` array:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征作为 `numpy` 数组输出：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Data prep for a language generation (RNN) decoder
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言生成（RNN）解码器的数据准备
- en: The first step is to pre-process the captions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是对标题进行预处理。
- en: 'We will perform a few basic pre-processing steps on the captions, such as the
    following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对标题执行一些基本的预处理步骤，例如以下操作：
- en: First, we'll tokenize the captions (for example, by splitting on spaces). This
    will help us to build a vocabulary of all the unique words in the data (for example,
    "playing", "football", and so on).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将对标题进行分词（例如，通过空格拆分）。这将帮助我们构建一个包含数据中所有唯一单词的词汇表（例如，“playing”，“football”等）。
- en: Next, we'll limit the vocabulary size to the top 5,000 words to save memory.
    We'll replace all other words with the token `unk` (for unknown). You can obviously
    optimize that according to the use case.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将把词汇表大小限制为前5,000个单词以节省内存。我们将用`unk`（表示未知）替换所有其他单词。你显然可以根据使用场景进行优化。
- en: Finally, we will create a word --> index mapping and vice versa.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个单词到索引的映射以及反向映射。
- en: We will then pad all sequences to be the same length as the longest one.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将对所有序列进行填充，使其长度与最长的序列相同。
- en: 'Here is the code for that:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是相关代码：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So, the end result will be an array of a sequence of integers, as shown in
    the following screenshot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最终的结果将是一个整数序列数组，如下图所示：
- en: '![](img/15faa166-9e04-4e32-b06f-28c4c2650c4d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15faa166-9e04-4e32-b06f-28c4c2650c4d.png)'
- en: 'Now, we will split the data into training and validation samples using an 80:20
    split ratio:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用80:20的比例将数据分为训练样本和验证样本：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Setting up the data pipeline
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置数据管道
- en: 'Our images and captions are ready! Next, let''s create a `tf.data` dataset
    ([https://www.tensorflow.org/api_docs/python/tf/data/Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset))
    to use for training our model. Now we will prepare the pipeline for an image and
    the text model by performing transformations and batching on them:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图像和标题已准备好！接下来，让我们创建一个`tf.data`数据集（[https://www.tensorflow.org/api_docs/python/tf/data/Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)）用于训练我们的模型。现在，我们将通过对它们进行转换和批处理来为图像和文本模型准备管道：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Defining the captioning model
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义字幕生成模型
- en: The model architecture we are using to build the auto captioning is inspired
    by the *Show, Attend and Tell* paper ([https://arxiv.org/pdf/1502.03044.pdf](https://arxiv.org/pdf/1502.03044.pdf)).
    The features that we extracted from the lower convolutional layer of Inception-v3
    gave us a vector of a shape of (8, 8, 2048). Then, we squash that to a shape of
    (64, 2048).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来构建自动字幕的模型架构灵感来自于*Show, Attend and Tell*论文（[https://arxiv.org/pdf/1502.03044.pdf](https://arxiv.org/pdf/1502.03044.pdf)）。我们从Inception-v3的低层卷积层提取的特征给我们一个形状为(8,
    8, 2048)的向量。然后，我们将其压缩成(64, 2048)的形状。
- en: 'This vector is then passed through the CNN encoder, which consists of a single
    fully connected layer. The RNN (GRU in our case) attends over the image to predict
    the next word:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个向量通过CNN编码器传递，该编码器由一个单一的全连接层组成。RNN（在我们的案例中是GRU）会关注图像来预测下一个词：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Attention
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意
- en: 'Now we will define the attention mechanism popularly known as Bahdanau attention
    ([https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)). We
    will need the features from the CNN encoder of a shape of (`batch_size`, `64`,
    `embedding_dim`). This attention mechanism will return the context vector and
    the attention weights over the time axis:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义广为人知的注意力机制——巴赫达诺注意力（Bahdanau Attention）（[https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)）。我们将需要来自CNN编码器的特征，形状为（`batch_size`，`64`，`embedding_dim`）。该注意力机制将返回上下文向量和时间轴上的注意力权重：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: CNN encoder
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN编码器
- en: 'Now let''s define the CNN encoder that will be the single, fully connected
    layer followed by the ReLU activation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义CNN编码器，它将是一个单一的全连接层，后面跟着ReLU激活函数：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: RNN decoder
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN解码器
- en: 'Here, we will define the RNN decoder which will take the encoded features from
    the encoder. The features are fed into the attention layer, which is concatenated
    with the input embedding vector. Then, the concatenated vector is passed into
    the GRU module, which is further passed through two fully connected layers:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将定义RNN解码器，它将接受来自编码器的编码特征。这些特征被输入到注意力层，与输入的嵌入向量连接。然后，连接后的向量被传递到GRU模块，进一步通过两个全连接层：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Loss function
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'We are using the `Adam` optimizer to train the model and masking the loss calculated
    for the `<PAD>` key:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用`Adam`优化器来训练模型，并对为`<PAD>`键计算的损失进行屏蔽：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the captioning model
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练字幕生成模型
- en: Now, let's train the model. The first thing we need to do is to extract the
    features stored in the respective `.npy` files and then pass those features through
    the CNN encoder.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型。我们需要做的第一件事是提取存储在相应`.npy`文件中的特征，然后将这些特征通过CNN编码器传递。
- en: The encoder output, hidden state (initialized to 0) and the decoder input (which
    is the start token) are passed to the decoder. The decoder returns the predictions
    and the decoder hidden state.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输出、隐藏状态（初始化为0）以及解码器输入（即起始标记）将传递给解码器。解码器返回预测结果和解码器的隐藏状态。
- en: The decoder hidden state is then passed back into the model and the predictions
    are used to calculate the loss. While training, we use the **teacher forcing technique**
    to decide the next input to the decoder.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，解码器的隐藏状态会被传回模型中，预测结果将用于计算损失。在训练过程中，我们使用**教师强迫技术**来决定解码器的下一个输入。
- en: Teacher forcing is the technique where the target word is passed as the next
    input to the decoder. This technique helps to learn the correct sequence or correct
    statistical properties for the sequence, quickly.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 教师强迫是一种技术，将目标词作为下一个输入传递给解码器。这种技术有助于快速学习正确的序列或序列的正确统计特性。
- en: 'The final step is to calculate the gradient and apply it to the optimizer and
    backpropagate:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是计算梯度并将其应用于优化器，然后进行反向传播：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/c5a47375-df57-44ad-99fd-06addbe5908f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5a47375-df57-44ad-99fd-06addbe5908f.png)'
- en: 'After performing the training process over few epochs lets plot the `Epoch`
    vs `Loss` graph:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行了几次训练迭代后，让我们绘制`Epoch`与`Loss`的图表：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/c859ad39-eb10-4791-9641-d43a4bc2a54a.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c859ad39-eb10-4791-9641-d43a4bc2a54a.png)'
- en: The loss vs Epoch plot during training process
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的损失与迭代次数图
- en: Evaluating the captioning model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估字幕生成模型
- en: The evaluation function is similar to the training loop, except we don't use
    teacher forcing here. The input to the decoder at each time step is its previous
    predictions, along with the hidden state and the encoder output.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 评估函数与训练循环类似，唯一的不同是我们这里不使用教师强迫。每次时间步解码器的输入是它之前的预测结果、隐藏状态以及编码器的输出。
- en: 'A few key points to remember while making predictions:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 做预测时需要记住的几个要点：
- en: Stop predicting when the model predicts the end token
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型预测结束标记时，停止预测
- en: Store the attention weights for every time step
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储每个时间步的注意力权重
- en: 'Let’s define the `evaluate()` function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义`evaluate()`函数：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Also, let''s create a `helper` function to visualize the attention points that
    predict the words:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们创建一个`helper`函数来可视化预测单词的注意力点：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/9edc8d8c-9af0-4bf5-a0df-1fc5f89145f1.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9edc8d8c-9af0-4bf5-a0df-1fc5f89145f1.png)'
- en: '![](img/ccb51a85-bbc0-4031-8da8-bfaa6b5d147e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb51a85-bbc0-4031-8da8-bfaa6b5d147e.png)'
- en: Deploying the captioning model
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署字幕生成模型
- en: Now let's deploy the complete module as a RESTful service. To do so, we will
    write an inference code that loads the latest checkpoint and makes the prediction
    on the given image.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将整个模块部署为 RESTful 服务。为此，我们将编写一个推理代码，加载最新的检查点，并对给定的图像进行预测。
- en: Look into the `inference.py` file in the repository. All the code is similar
    to the training loop except we don't use teacher forcing here. The input to the
    decoder at each time step is its previous predictions, along with the hidden state
    and the encoder output.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 查看仓库中的`inference.py`文件。所有代码与训练循环相似，唯一的不同是我们这里不使用教师强迫。每次时间步解码器的输入是它之前的预测结果、隐藏状态以及编码器的输出。
- en: 'One important part is to load the model in memory for which we are using the
    `tf.train.Checkpoint()` method, which loads all of the learned weights for `optimizer`,
    `encoder`, `decoder` into the memory. Here is the code for that:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要部分是将模型加载到内存中，我们使用`tf.train.Checkpoint()`方法来加载所有学习到的权重，包括`optimizer`、`encoder`、`decoder`，并将它们加载到内存中。以下是相应的代码：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So, we will create an `evaluate()` function, which defines the prediction loop. To
    make sure that the prediction ends after certain words, we will stop predicting
    when the model predicts the end token, `<end>`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将创建一个`evaluate()`函数，该函数定义了预测循环。为了确保预测在某些词语后停止，我们将在模型预测结束标记`<end>`时停止预测：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let''s use this `evaluate()` function in our web application code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在 Web 应用程序代码中使用这个`evaluate()`函数：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Execute the following command in the Terminal to run the web app:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端执行以下命令来运行 Web 应用程序：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should get the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we request the API, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们请求 API，如下所示：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We should get our caption predicted, as shown in the following screenshot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该能得到预测的字幕，如下图所示：
- en: '![](img/8becf540-8d9a-4cd4-89b5-dba2bd53f48e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8becf540-8d9a-4cd4-89b5-dba2bd53f48e.png)'
- en: Make sure to train the model on the large image to get better predictions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在大图像上训练模型，以获得更好的预测效果。
- en: Voila! We just deployed the state-of-the-art automatic captioning module.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们刚刚部署了最先进的自动字幕生成模块。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this implementation, we used a pretrained Inception-v3 model as a feature
    extractor in an encoder trained on the ImageNet dataset as part of a deep learning
    solution. This solution combines current state-of-the-art techniques in both *computer
    vision *and* natural language processing,* to form a complete image description approach ([https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf)) able
    to construct computer-generated natural descriptions of any provided images. We've
    effectively broken the barrier between images and language with this trained model
    and we've provided a technology that could be used as part of an application,
    helping the visually impaired enjoy the benefits of the megatrend of photo sharing!
    Great work!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们使用了一个预训练的Inception-v3模型作为特征提取器，并将其作为编码器的一部分，在ImageNet数据集上进行训练，作为深度学习解决方案的一部分。该解决方案结合了目前最先进的*计算机视觉*和*自然语言处理*技术，形成了一个完整的图像描述方法([https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf))，能够为任何提供的图像构建计算机生成的自然描述。通过这个训练好的模型，我们有效地打破了图像与语言之间的障碍，并提供了一项技术，可以作为应用程序的一部分，帮助视障人士享受照片分享这一大趋势带来的益处！伟大的工作！
