- en: Chapter 2.  Distributed Deep Learning for Large-Scale Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 分布式深度学习与大规模数据
- en: '|   | *"In God we trust, all others must bring data"* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“我们信仰上帝，其他的都必须提供数据”* |   |'
- en: '|   | --*W. Edwards Deming* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*W. Edwards Deming* |'
- en: In this exponentially growing digital world, big data and deep learning are
    the two hottest technical trends. Deep learning and big data are two interrelated
    topics in the world of data science, and in terms of technological growth, both
    are critically interconnected and equally significant.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个指数增长的数字世界中，大数据和深度学习是两大最热门的技术趋势。深度学习和大数据是数据科学领域中两个相互关联的主题，在技术发展方面，它们密切相连，且同样重要。
- en: Digital data and cloud storage follow a generic law, termed as Moore's law [50],
    which roughly states that the world's data are doubling every two years; however,
    the cost of storing that data decreases at approximately the same rate. This profusion
    of data generates more features and verities, hence, to extract all the valuable
    information out of it, better deep learning models should be built.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数字数据和云存储遵循一种通用法则，称为摩尔定律[50]，大致表示全球数据每两年翻一番；然而，存储这些数据的成本大致以相同的速度下降。这种数据的激增带来了更多的特征和多样性，因此，为了从中提取所有有价值的信息，应该构建更好的深度学习模型。
- en: This voluminous availability of data helps to bring huge opportunities for multiple
    sectors. Moreover, big data, with its analytic part, has produced lots of challenges
    in the field of data mining, harnessing the data, and retrieving the hidden information
    out of it. In the field of Artificial Intelligence, deep learning algorithms provide
    their best output with large-scale data during the learning process. Therefore,
    as data are growing faster than ever before, deep learning also plays a crucial
    part in delivering all the big data analytic solutions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这种海量数据的可用性为多个行业带来了巨大的机会。此外，带有分析部分的大数据，在数据挖掘、数据获取和从中提取隐藏信息的领域中也带来了许多挑战。在人工智能领域，深度学习算法在学习过程中能够在大规模数据下提供最佳输出。因此，随着数据增长速度空前加快，深度学习在提供大数据分析解决方案中也发挥着至关重要的作用。
- en: This chapter will give an insight into how deep learning models behave with
    big data, and reveal the associated challenges. The later part of the chapter
    will introduce Deeplearning4j, an open source distributed framework, with a provision
    for integration with Hadoop and Spark, used to deploy deep learning for large-scale
    data. The chapter will provide examples to show how basic deep neural networks
    can be implemented with Deeplearning4j, and its integration to Apache Spark and
    Hadoop YARN.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨深度学习模型在大数据中的表现，并揭示相关的挑战。章末将介绍Deeplearning4j，一个开源分布式框架，提供与Hadoop和Spark的集成，用于大规模数据的深度学习部署。本章还将提供示例，展示如何使用Deeplearning4j实现基本的深度神经网络，以及它与Apache
    Spark和Hadoop YARN的集成。
- en: 'The following are the important topics that will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下重要主题：
- en: Deep learning for massive amounts of data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面向海量数据的深度学习
- en: Challenges of deep learning for big data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习在大数据中的挑战
- en: Distributed deep learning and Hadoop
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式深度学习与Hadoop
- en: 'Deeplearning4j: An open source distributed framework for deep learning'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deeplearning4j：一个用于深度学习的开源分布式框架
- en: Setting up Deeplearning4j on Hadoop YARN
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Hadoop YARN上设置Deeplearning4j
- en: Deep learning for massive amounts of data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向海量数据的深度学习
- en: In this Exa-Byte scale era, the data are increasing at an exponential rate.
    This growth of data are analyzed by many organizations and researchers in various
    ways, and also for so many different purposes. According to the survey of **International
    Data Corporation** (**IDC**), the Internet is processing approximately 2 Petabytes
    of data every day [51]. In 2006, the size of digital data was around 0.18 ZB,
    whereas this volume has increased to 1.8 ZB in 2011\. Up to 2015, it was expected
    to reach up to 10 ZB in size, and by 2020, its volume in the world will reach
    up to approximately 30 ZB to 35 ZB. The timeline of this data mountain is shown
    in *Figure 2.1*. These immense amounts of data in the digital world are formally
    termed as big data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Exa-Byte级别的时代，数据以指数级的速度增长。许多组织和研究人员以各种方式分析这种数据增长，且目的各异。根据**国际数据公司**（**IDC**）的调查，互联网每天处理约2
    Petabytes的数据[51]。2006年，数字数据的大小约为0.18 ZB，而这一数据量在2011年增加到了1.8 ZB。到2015年，预计这一数据量将达到10
    ZB，而到2020年，全球数据量将达到大约30 ZB至35 ZB。这一数据山脉的时间线如*图2.1*所示。数字世界中这些庞大的数据量正式被称为大数据。
- en: '|   | *"The world of Big Data is on fire"* |   |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|   | *“大数据的世界正在燃烧”* |   |'
- en: '|   | --*The Economist, Sept 2011* |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|   | --*《经济学人》，2011年9月* |'
- en: '![Deep learning for massive amounts of data](img/image_02_001-1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![海量数据的深度学习](img/image_02_001-1.jpg)'
- en: 'Figure 2.1: Figure shows the increasing trend of data for a time span of around
    20 years'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：图表展示了约20年时间跨度内数据的增长趋势
- en: Facebook has almost 21 PB in 200M objects [52], whereas Jaguar ORNL has more
    than 5 PB data. These stored data are growing so rapidly that Exa-Byte scale storage
    systems are likely to be used by 2018 to 2020.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的存储量几乎为21PB，存储了2亿个对象[52]，而Jaguar ORNL的存储量超过5PB。这些存储的数据增长速度如此之快，以至于预计在2018到2020年间，Exa-Byte级别的存储系统将被使用。
- en: This explosion of data certainly poses an immediate threat to the traditional
    data-intensive computations, and points towards the need for some distributed
    and scalable storage architecture for querying and analysis of the large-scale
    data. A generic line of thought for big data is that raw data is extremely complex,
    sundry, and increasingly growing. An ideal Big dataset consists of a vast amount
    of unsupervised raw data, and with some negligible amount of structured/categorized
    data. Therefore, while processing these amounts of non-stationary structured data,
    the conventional data-intensive computations often fail. As a result, big data,
    having unrestricted diversity, requires sophisticated methods and tools, which
    could be implemented to extract patterns and analyze the large-scale data. The
    growth of big data has mostly been caused by an increasing computational processing
    power and the capability of the modern systems to store data at lower cost.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的爆炸性增长无疑对传统的数据密集型计算构成了直接威胁，并指向了需要某种分布式和可扩展的存储架构，用于查询和分析大规模数据。关于大数据的一个普遍观点是，原始数据非常复杂、种类繁多，并且不断增长。一个理想的大数据集包含大量的无监督原始数据，并且只有少量的结构化或分类数据。因此，在处理这些非静态结构化数据时，传统的数据密集型计算通常会失败。因此，具有无限多样性的大数据需要复杂的方法和工具，以便提取模式并分析大规模数据。大数据的增长主要是由计算处理能力的提升和现代系统以更低成本存储数据的能力推动的。
- en: 'Considering all these features of big data, it can be broken into four distinct
    dimensions, often referred to as the four Vs: **Volume**, **Variety**, **Velocity**,
    and **Veracity**. Following *figure 2.2* shows the different characteristics of
    big data by providing all the 4Vs of data:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到大数据的所有特征，可以将其分为四个不同的维度，通常称为四个V：**数据量（Volume）**、**数据种类（Variety）**、**数据速度（Velocity）**和**数据真实性（Veracity）**。下图*图2.2*展示了大数据的不同特征，并提供了所有4V的数据：
- en: '![Deep learning for massive amounts of data](img/B05883_02_02-1.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![海量数据的深度学习](img/B05883_02_02-1.jpg)'
- en: 'Figure 2.2: Figure depicts the visual representation of 4Vs of big data'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：图表展示了大数据4V的可视化表现
- en: In this current data-intensive technology era, the velocity of the data, the
    escalating rate at which the data are collected and obtained is as significant
    as the other parameters of the big data, that is, **Volume** and **Variety**.
    With the given pace, at which this data is getting generated, if it is not collected
    and analyzed sensibly, there is a huge risk of important data loss. Although,
    there is an option to retain this rapid-moving data into bulk storage for batch
    processing at a later period, the genuine importance in tackling this high velocity
    data lies in how quickly an organization can convert the raw data to a structured
    and usable format. Specifically, time-sensitive information such as flight fare,
    hotel fare, or some e-commerce product's price, and so on would become obsolete
    if the data is not immediately retained and processed in a systemic manner. The
    parameter veracity in big data is concerned with the accuracy of the results obtained
    after the data analysis. As data turns more complex each day, sustaining trust
    in the hidden information of big data throws a significant challenge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据密集型的技术时代，数据的速度、数据收集和获取的增长速度与大数据的其他参数一样重要，即**数据量（Volume）**和**数据种类（Variety）**。随着数据生成速度的加快，如果不加以合理收集和分析，就有可能丧失重要的数据。尽管可以将这些快速生成的数据存储到大容量存储中，供以后批量处理，但处理这些高速度数据的真正关键在于组织能够多快地将原始数据转化为结构化且可用的格式。具体而言，像航班票价、酒店费用或某些电子商务产品的价格等时间敏感信息，如果不立即以系统化的方式保存和处理，将会变得过时。大数据中的真实性（Veracity）参数涉及数据分析后得到的结果的准确性。随着数据变得越来越复杂，保持对大数据中隐藏信息的信任，带来了重大挑战。
- en: To extract and analyze such critically complex data, a better, well-planned
    model is desired. In ideal cases, a model should perform better dealing with big
    data compared to data with small sizes. However, this is not always the case.
    Here, we will show one example to discuss more on this point.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取和分析如此复杂的数据，需要一个更好、更周密的模型。在理想情况下，与小数据集相比，模型在处理大数据时应该表现更好。然而，这并不总是如此。接下来，我们将通过一个例子进一步讨论这个问题。
- en: As illustrated in *Figure 2.3*, with a small size dataset, the performance of
    the best algorithm is *n%* better than the worst one. However, as the size of
    the dataset increases (big data), the performance also enhances exponentially
    to some *k % >> n %*. Such kind of traces can well be found from [53], which clearly
    shows the effect of a large-scale training dataset in the performance of the model.
    However, it would be completely misleading that with any of the simplest models,
    one can achieve the best performance only using Big dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 2.3*所示，在小数据集的情况下，最好的算法比最差的算法要*多 n%*。然而，随着数据集大小的增加（大数据），性能也会呈指数级增强，达到*多 k
    % >> n %*。从[53]中可以清晰地看到，大规模训练数据集对模型性能的影响。然而，认为仅使用大数据就能通过任何最简单的模型达到最佳性能，这种说法是完全误导的。
- en: From [53] we can see that algorithm 1 is basically a Naive Bayes model, algorithm
    2 belongs to a memory-based model, and algorithm 3 corresponds to Winnow. The
    following graph shows, with a small dataset, that the performance of Winnow is
    less that the memory-based one. Whereas when dealing with Big dataset, both the
    Naive Bayes and Winnow show better performance than the memory-based model. So,
    looking at the *Figure 2.3*, it would be really difficult to infer on what basis
    any one of these simple models work better in an environment of large dataset.
    An intuitive explanation for the relatively poor performance of the memory-based
    method with large datasets is that the algorithm suffered due to the latency of
    loading a huge amount of data to its memory. Hence, it is purely a memory related
    issue, and only using big data would not resolve that. Therefore, a primary reason
    for the performance should be how sophisticated the models are. Hence, the importance
    of deep learning model comes into play.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从[53]中我们可以看到，算法1基本上是一个朴素贝叶斯模型，算法2属于基于记忆的模型，而算法3对应的是Winnow模型。下图显示，在小数据集的情况下，Winnow的表现不如基于记忆的模型。而在处理大数据集时，朴素贝叶斯和Winnow的表现都优于基于记忆的模型。因此，查看*图
    2.3*时，很难推测出这些简单模型在大数据环境中哪个表现更好。一个直观的解释是，基于记忆的方法在处理大数据时表现较差，原因在于加载大量数据到内存时产生的延迟。因此，这纯粹是一个与内存相关的问题，仅仅使用大数据并不能解决这个问题。因此，性能的主要原因应是模型的复杂性。这也说明了深度学习模型的重要性。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Big data. Small Minds. No Progress! Big data. Big Brains. Breakthrough! [54]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据。小智慧。没有进步！大数据。大智慧。突破！[54]
- en: Deep learning stands in contrast to big data. Deep learning has triumphantly
    been implemented in various industry products and widely practiced by various
    researchers by taking advantage of this large-scale digital data. Famous technological
    companies such as Facebook, Apple, and Google collect and analyze this voluminous
    amount of data on a daily basis, and have been bellicosely going forward with
    various deep learning related projects over the last few years.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与大数据有所不同。深度学习通过利用大规模的数字数据，已成功地应用于各种行业产品，并被各类研究人员广泛实践。像Facebook、Apple和Google等著名科技公司每天都会收集并分析大量数据，并且在过去几年里，他们在多个深度学习相关项目中不断取得进展。
- en: Google deploys deep learning algorithms on the massive unstructured data collected
    from various sources including Google's Street view, image search engine, Google's
    translator, and Android's voice recognition.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Google在从各种来源收集的大量非结构化数据上部署深度学习算法，包括Google的街景、图像搜索引擎、Google翻译以及Android的语音识别。
- en: '![Deep learning for massive amounts of data](img/image_02_003.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习用于海量数据](img/image_02_003.jpg)'
- en: 'Figure 2.3: Variation of percentage of accuracy of different types of algorithms
    with increasing size of datasets'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：不同类型算法在数据集大小增加时准确率的变化
- en: Apple's Siri, a virtual personal assistant for iPhones, provides a bulk of different
    services, such as sport news, weather reports, answers to users' questions, and
    so on. The entire application of Siri is based on deep learning, which collects
    data from different Apple services and obtains its intelligence. Other industries,
    mainly Microsoft and IBM, are also using deep learning as their major domain to
    deal with this massive amount of unstructured data. IBM's brain-like computer,
    Watson, and Microsoft's Bing search engine primarily use deep learning techniques
    to leverage the big data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果的Siri是iPhone的虚拟个人助手，提供各种不同的服务，如体育新闻、天气预报、用户问题的回答等。Siri的整个应用基于深度学习，收集来自不同Apple服务的数据并获取其智能。其他行业，主要是微软和IBM，也在使用深度学习作为其处理这海量非结构化数据的主要领域。IBM的类大脑计算机沃森和微软的必应搜索引擎主要利用深度学习技术来挖掘大数据。
- en: Current deep learning architectures comprise of millions or even billions of
    data points. Moreover, the scale at which the data is growing prevents the model
    from the risk of overfitting. The rapid increase in computation power too has
    made the training of advanced models much easier.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的深度学习架构包含数百万甚至数十亿个数据点。此外，数据增长的规模使得模型避免了过拟合的风险。计算能力的快速增长也使得训练先进模型变得更加容易。
- en: '*Table 2.1* shows how big data is practiced with popular deep learning models
    in recent research to get maximum information out of data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*表2.1* 显示了如何在最近的研究中使用流行的深度学习模型来最大化数据的提取信息：'
- en: '| **Models** | **Computing power** | **Datasets** | **Average running time**
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **计算能力** | **数据集** | **平均运行时间** |'
- en: '| Convolutional Neural Network[55] | Two NVIDIA GTX 580 3 GB GPUs. | Roughly
    90 cycles through the training set of 1.2 million high resolution images. | Five
    to six days. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 卷积神经网络 [55] | 两个NVIDIA GTX 580 3 GB GPU | 大约90轮训练，使用120万张高分辨率图片 | 五到六天 |'
- en: '| Deep Belief Network [41] | NVIDIA GTX 280 1 GB GPU. | 1 million images. |
    Approximately one day. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 深度置信网络 [41] | NVIDIA GTX 280 1 GB GPU | 100万张图片 | 大约一天 |'
- en: '| Sparse autoencoder[ 66] | 1000 CPU having 16000 cores each. | 10 million
    200*200 pixel images. | Approximately three days. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏自编码器 [66] | 1000个CPU，每个拥有16000个核心 | 1000万张200*200像素的图片 | 大约三天 |'
- en: 'Table 2.1: Recent research progress of large-scale deep learning models. Partial
    information taken from [55]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：大规模深度学习模型的最新研究进展。部分信息摘自[55]
- en: Deep learning algorithms, with the help of a hierarchical learning approach,
    are basically used to extract meaningful generic representations from the input
    raw data. Basically, at a higher level, more complex and abstract representations
    of the data are learnt from the previous layers and the less abstracted data of
    the multi-level learning model. Although deep learning can also learn from massive
    amounts of labelled (categorized) data, the models generally look attractive when
    they can learn from unlabeled/uncategorized data [56], and hence, help in generating
    some meaningful patterns and representation of the big unstructured data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层学习方法的帮助下，深度学习算法主要用于从输入的原始数据中提取有意义的通用表示。基本上，在更高的层次上，从前一层和多层学习模型的低抽象数据中学习到更复杂、抽象的数据表示。尽管深度学习也可以从大量标注（分类）数据中学习，但当模型能够从未标注/未分类数据中学习时，它们通常更具吸引力[56]，从而帮助生成大规模非结构化数据的有意义模式和表示。
- en: 'While dealing with large-scale unsupervised data, deep learning algorithms
    can extract the generic patterns and relationships among the data points in a
    much better way than the shallow learning architectures. The following are a few
    of the major characteristics of deep learning algorithms, when trained with large-scale
    unlabeled data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大规模无监督数据时，深度学习算法可以比浅层学习架构更好地提取数据点之间的通用模式和关系。以下是深度学习算法在用大规模未标注数据训练时的一些主要特征：
- en: From the higher level of abstractions and representation, semantics and relational
    knowledge of the big data can be obtained from the deep learning models
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从更高层次的抽象和表示中，可以从深度学习模型中获得大数据的语义和关系知识。
- en: Even a simple linear model can perform effectively with the knowledge obtained
    from excessively complex and more abstract representations of the huge dataset
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是一个简单的线性模型，也可以通过从过于复杂和更抽象的巨大数据集表示中获得的知识，进行有效的表现。
- en: This huge variety of data representation from the unsupervised data opens its
    door for learning other data types such as textual, audio, video, image, and the
    like
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自无监督数据的这种巨大数据表示种类为学习其他数据类型（如文本、音频、视频、图像等）打开了大门。
- en: Therefore, it can be surely concluded that deep learning will become an essential
    ingredient for providing big data sentiment analysis, predictive analysis, and
    so on, particularly with the enhanced processing power and advancement in the
    **graphics processing unit** (**GPU**) capacity. The aim of this chapter is not
    to extensively cover big data, but to represent the relationship between big data
    and deep learning. The subsequent sections will introduce the key concepts, applications,
    and challenges of deep learning while working with large-scale uncategorized data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以肯定地得出结论，深度学习将成为提供大数据情感分析、预测分析等的关键要素，特别是随着处理能力的增强和**图形处理单元**（**GPU**）容量的提升。本章的目的是不是广泛讨论大数据，而是展示大数据与深度学习之间的关系。接下来的章节将介绍深度学习在处理大规模未分类数据时的关键概念、应用和挑战。
- en: Challenges of deep learning for big data
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在大数据中的挑战
- en: The potential of big data is certainly noteworthy. However, to fully extract
    valuable information at this scale, we would require new innovations and promising
    algorithms to address many of these related technical problems. For example, to
    train the models, most of the traditional machine learning algorithms load the
    data in memory. But with a massive amount of data, this approach will surely not
    be feasible, as the system might run out of memory. To overcome all these gritty
    problems, and get the most out of the big data with the deep learning techniques,
    we will require brain storming.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的潜力无疑是值得注意的。然而，要在如此规模下充分提取有价值的信息，我们需要新的创新和有前景的算法来解决这些相关的技术问题。例如，为了训练模型，大多数传统的机器学习算法会将数据加载到内存中。但对于海量数据，这种方法显然不可行，因为系统可能会耗尽内存。为了克服这些棘手的问题，并通过深度学习技术最大化大数据的价值，我们将需要集思广益。
- en: Although, as discussed in the earlier section, large-scale deep learning has
    achieved many accomplishments in the past decade, this field is still in a growing
    phase. Big data is constantly raising limitations with its 4Vs. Therefore, to
    tackle all of those, many more advancements in the models need to take place.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正如前面一节讨论的那样，大规模深度学习在过去十年里取得了许多成就，但这个领域仍处于成长阶段。大数据不断地通过其4V特性提出新的限制。因此，为了应对这些挑战，模型中还需要进行更多的进步。
- en: Challenges of deep learning due to massive volumes of data (first V)
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由于海量数据，深度学习面临的挑战（第一个V）
- en: The volume of large-scale data imposes a great challenge to deep learning. With
    very high dimensionality (attributes), a large number of examples (input) and
    large varieties of classifications (outputs), big data often increases the complexity
    of the model, as well as the running-time complexity of the algorithm. The mountain
    of data makes the training of deep learning algorithms almost impossible using
    centralized storage and its limited processing ability. To provide a cushion to
    this challenge, pushed by the huge volume of data, distributed frameworks with
    parallelized servers should be used. The upgraded deep network models have started
    to use clusters of CPUs and GPUs to enhance the training speed, without compromising
    the algorithm's accuracy. Various new strategies have been evolved for model parallelism
    and data parallelism.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据的体量对深度学习构成了巨大的挑战。随着维度（属性）非常高，示例（输入）数量庞大以及分类（输出）种类繁多，大数据常常增加模型的复杂性，同时也增加了算法的运行时间复杂度。大量数据使得使用集中存储及其有限的处理能力几乎不可能进行深度学习算法的训练。为了应对由数据量巨大带来的挑战，应该使用分布式框架和并行化的服务器。这些升级后的深度网络模型已经开始使用CPU和GPU集群来加速训练速度，同时不妥协算法的准确性。模型并行性和数据并行性已经发展出了各种新的策略。
- en: In these types, the models or data are split into blocks, which can fit with
    the in-memory data, and then be distributed to various nodes with forward and
    backward propagations [57]. Deeplearning4j, a Java-based distributed tool for
    deep learning, uses data parallelism for this purpose, and will be explained in
    the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些类型中，模型或数据被拆分成可以适应内存数据的块，然后分发到各个节点进行前向和反向传播[57]。基于Java的深度学习分布式工具Deeplearning4j使用数据并行性来实现这一目标，下一节将详细介绍。
- en: High volumes of data are always associated with noisy labels and data incompleteness.
    This poses a major challenge during the training of large-scale deep learning.
    A huge proportion of the big data is contained by the unlabeled or unstructured
    data, where the noisy labels predominantly exist. To overcome this issue, some
    manual curation of the datasets is required to a significant extent. For example,
    all the search engines are used to collect the data over the last one year span.
    For this data, we need some sort of filtering, particularly to remove redundancy
    and the low-value data. Advanced deep learning methods are essential to handle
    such noisy, redundant data. Also, the associated algorithms should be able to
    tolerate these disarray datasets. One can also implement some more efficient cost
    function and updated training strategy to fully overcome the effect of noisy labels.
    Moreover, the use of semi-supervised learning [58] [59] could help to enhance
    the solution associated with this noisy data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大量数据总是伴随着标签噪声和数据不完整性，这在大规模深度学习训练中构成了一个重大挑战。大部分大数据由无标签或非结构化数据构成，其中噪声标签主要存在于此。为了克服这个问题，数据集需要进行一定程度的人工整理。例如，所有搜索引擎都用于收集过去一年间的数据。对于这些数据，我们需要进行某种形式的筛选，特别是去除冗余和低价值数据。先进的深度学习方法对于处理这类噪声和冗余数据至关重要。此外，相关算法应能够容忍这些杂乱的数据集。还可以实现更高效的代价函数和更新的训练策略，以完全克服噪声标签的影响。此外，使用半监督学习[58]
    [59]有助于增强与这些噪声数据相关的解决方案。
- en: Challenges of deep learning from a high variety of data (second V)
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习面临的来自高种类数据的挑战（第二个V）
- en: This is the second dimension of big data, which represents all types of formats,
    with different distributions and from numerous sources. The exponentially growing
    data come from heterogeneous sources, which include a mammoth collection of audio
    streams, images, videos, animations, graphics, and unstructured texts from various
    log files. These varieties of data possess different characteristics and behavior.
    Data integration could be the only way to deal with such situations. As stated
    in [Chapter 1](ch01.html "Chapter 1. Introduction to Deep Learning") , *Introduction
    to Deep Learning*, deep learning has the ability to represent learning from structured/unstructured
    data. Deep learning can carry out unsupervised learning in a hierarchical fashion,
    which is training performed one level at a time, and the higher level features
    are defined by the immediate lower levels. This property of deep learning can
    be used to address the data integration problem. The natural solution of this
    could be to learn the data representation from each individual data sources, and
    then integrate the learned features at the subsequent levels.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大数据的第二个维度，代表所有类型的格式，具有不同的分布并来自众多来源。指数级增长的数据来自异质来源，其中包括大量的音频流、图像、视频、动画、图形以及来自各种日志文件的非结构化文本。这些不同种类的数据具有不同的特性和行为。数据集成可能是解决此类情况的唯一途径。正如[第1章](ch01.html
    "第1章 深度学习简介")《深度学习简介》中所述，深度学习有能力从结构化/非结构化数据中进行学习。深度学习可以以分层的方式执行无监督学习，即逐层训练，更高层次的特征由紧接着的较低层次定义。深度学习的这一特性可以用来解决数据集成问题。一个自然的解决方案是从每个单独的数据源中学习数据表示，然后在随后的层次中集成学到的特征。
- en: There have already been a few experiments [60] [61], which have successfully
    demonstrated that deep learning can easily be used for the heterogeneous data
    sources for its significant gains in system performance. However, there are still
    many unanswered questions which deep learning has to address in the upcoming years.
    Currently, most of the deep learning models are mainly tested on bi-modalities
    (data from only two sources), but will the system performance be enhanced while
    dealing with multiple modalities? It might happen that multiple sources of data
    will offer conflicting information; in those cases, how will the model be able
    to nullify such conflicts and integrate the data in a constructive and fruitful
    way? Deep learning seems perfectly appropriate for the integration of various
    sources of data with multiple modalities, on account of its capability of learning
    intermediate representations and the underlying factors associated with a variety
    of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一些实验[60] [61]成功证明，深度学习可以轻松应用于异构数据源，显著提高系统性能。然而，深度学习在未来几年仍需解决许多未解之谜。目前，大多数深度学习模型主要在双模态（仅来自两个源的数据）上进行测试，但在处理多模态数据时，系统性能是否会得到提升呢？可能出现多种数据源提供相互冲突的信息；在这种情况下，模型如何能够消除这些冲突，并以一种建设性且富有成效的方式整合数据呢？由于深度学习能够学习中间表示和与各种数据相关的潜在因素，因此它似乎非常适合于多模态的各种数据源的集成。
- en: Challenges of deep learning from a high velocity of data (third V)
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习面临的高速度数据挑战（第三个V）
- en: The extreme velocity at which data is growing poses an enormous challenge to
    the deep learning technique. For data analytics, data created at this speed should
    also be processed in a timely manner. Online learning is one of the solutions
    to learning from this high velocity data [62-65]. However, online learning uses
    a sequential learning strategy, where the entire dataset should be kept in-memory,
    which becomes extremely difficult for traditional machines. Although the conventional
    neural network has been modified for online learning [67-71], there is still so
    much scope for progress in this field for deep learning. As an alternate approach
    to online learning, the stochastic gradient descent approach [72], [73] is also
    applied for deep learning. In this type, one training example with the known label
    is fed to the next label to update the model parameters. Further, to speed up
    learning, the updates can also be performed on a small batch basis [74]. This
    mini batch can provide a good balance between running time and the computer memory.
    In the next section, we will explain why mini batch data is most important for
    distributed deep learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增长的极端速度给深度学习技术带来了巨大挑战。对于数据分析，按此速度创建的数据也应该及时处理。在线学习是从这种高速数据中学习的解决方案之一[62-65]。然而，在线学习使用的是一种顺序学习策略，其中整个数据集需要保存在内存中，这对于传统机器来说极为困难。尽管传统神经网络已经针对在线学习进行了修改[67-71]，但在深度学习领域仍有很大的进展空间。作为在线学习的替代方法，随机梯度下降法[72]，[73]也被应用于深度学习。在这种方法中，一个带有已知标签的训练样本被馈送到下一个标签，以更新模型参数。此外，为了加速学习，更新也可以基于小批量进行[74]。这种小批量能够在运行时间和计算机内存之间提供良好的平衡。在下一部分，我们将解释为什么小批量数据对分布式深度学习至关重要。
- en: One more big challenge related to this high velocity of data is that this data
    is extremely changeable in nature. The distribution of data happens too frequently
    over time. Ideally, the data that changes over time is split into chunks taken
    from small time durations. The basic idea is that the data remains stationary
    for some time, and also possesses some major degree of correlation [75] [76].
    Therefore, the deep learning algorithms of big data should have the feature of
    learning the data as a stream. Algorithms which can learn from those non-stationary
    data are really crucial for deep learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种高速度数据相关的另一个重大挑战是，这些数据在性质上极为变化无常。数据的分布在时间上变化过于频繁。理想情况下，随着时间的推移变化的数据被划分为来自小时间段的块。基本的思想是数据在一段时间内保持静止，并且具有一定程度的相关性[75]
    [76]。因此，大数据的深度学习算法应具备将数据作为流来学习的特性。能够从这些非平稳数据中学习的算法对于深度学习至关重要。
- en: Challenges of deep learning to maintain the veracity of data (fourth V)
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习面临的数据真实性挑战（第四个V）
- en: Data veracity, imprecise, or uncertain data, is sometime overlooked, though
    it is equally consequential as the other 3Vs of big data. With the immense variety
    and velocity of big data, an organization can no longer rely on the traditional
    models to measure the accuracy of data. Unstructured data, by definition, contains
    a huge amount of imprecise and uncertain data. For example, social media data
    is excessively uncertain in nature. Although there are tools that can automate
    the normalization and cleansing of data, they are mostly in the pre-industrial
    stage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的真实性、不精确性或不确定性有时会被忽视，尽管它与大数据的其他3Vs一样具有重要的影响。随着大数据的巨大多样性和速度，组织再也无法依赖传统模型来衡量数据的准确性。根据定义，非结构化数据包含大量的不精确和不确定数据。例如，社交媒体数据本质上是不确定的。尽管有一些工具可以自动化数据的规范化和清理，但它们大多处于前工业化阶段。
- en: Distributed deep learning and Hadoop
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度学习与Hadoop
- en: 'From the earlier sections of this chapter, we already have enough insights
    on why and how the relationship of deep learning and big data can bring major
    changes to the research community. Also, a centralized system is not going to
    help this relationship substantially with the course of time. Hence, distribution
    of the deep learning network across multiple servers has become the primary goal
    of the current deep learning practitioners. However, dealing with big data in
    a distributed environment is always associated with several challenges. Most of
    those are explained in-depth in the previous section. These include dealing with
    higher dimensional data, data with too many features, amount of memory available
    to store, processing the massive Big datasets, and so on. Moreover, Big datasets
    have a high computational resource demand on CPU and memory time. So, the reduction
    of processing time has become an extremely significant criterion. The following
    are the central and primary challenges in distributed deep learning:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章的前面部分，我们已经获得了足够的洞察力，了解深度学习与大数据之间的关系如何以及为何能给研究领域带来重大变化。而且，随着时间的推移，集中式系统不会对这种关系产生实质性帮助。因此，将深度学习网络分布到多个服务器上已成为当前深度学习从业者的主要目标。然而，在分布式环境中处理大数据总是伴随着多个挑战。大多数这些挑战在前一节中已被深入解释。这些挑战包括处理高维数据、特征过多的数据、可用内存的存储量、大规模大数据集的处理等等。此外，大数据集对CPU和内存的计算资源需求很高。因此，减少处理时间已成为一个极其重要的标准。以下是分布式深度学习中的核心和主要挑战：
- en: How can we keep chunks of dataset in the primary memory of the nodes?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将数据集的块保存在节点的主内存中？
- en: How can we maintain coordination among the chunks of data, so that later they
    can be moved together to result in the final outcome?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何保持数据块之间的协调，以便它们能够一起被移动并最终得出结果？
- en: How can we make distributed and parallel processing extremely scheduled and
    coordinated?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使分布式和并行处理变得极其有序和协调？
- en: How can we achieve an orchestral search process across the dataset to achieve
    high performance?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在数据集上实现一种管弦乐式的搜索过程，从而获得高效能？
- en: There are multiple ways of using distributed deep learning with big datasets.
    However, when we talk about big data, the framework that is performing tremendously
    well in defending most of the challenges from the past half decade is the Hadoop
    framework [77-80]. Hadoop allows for parallel and distributed processing. It is
    undoubtedly the most popular and widely used framework, and it can store and process
    the data mountain more efficiently compared to the other traditional frameworks.
    Almost all the major technology companies, such as Google, Facebook, and so on
    use Hadoop to deploy and process their data in a sophisticated fashion. Most of
    the software designed at Google, which requires the use of an ocean of data, uses
    Hadoop. The primary advantage of Hadoop is the way it stores and processes enormous
    amount of data across thousands of commodity servers, bringing some well-organized
    results [81]. From our general understanding of deep learning, we can relate that
    deep learning surely needs that sort of distributed computing power to produce
    some wondrous outcomes from the input data. The Big dataset can be split into
    chunks and distributed across multiple commodity hardware for parallel training.
    Further more, the complete stage of a deep neural network can be split into subtasks,
    and then those subtasks can be processed in parallel.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布式深度学习处理大数据集有多种方式。然而，当我们谈论大数据时，过去五年中应对大多数挑战表现出色的框架是 Hadoop 框架 [77-80]。Hadoop
    支持并行和分布式处理。它无疑是最受欢迎且最广泛使用的框架，且相比其他传统框架，它能够更高效地存储和处理大数据量。几乎所有主要的科技公司，如 Google、Facebook
    等，都使用 Hadoop 来部署和处理其数据。Google 所设计的多数需要处理大量数据的软件，都使用 Hadoop。Hadoop 的主要优势在于它能跨越成千上万台普通服务器存储和处理大量数据，从而产生一些有序的结果
    [81]。从我们对深度学习的基本理解来看，我们可以联想到，深度学习确实需要那种分布式计算能力，才能从输入数据中产生一些奇妙的成果。大数据集可以被拆分成小块，分布到多个普通硬件上进行并行训练。此外，深度神经网络的完整阶段可以拆分为多个子任务，然后这些子任务可以并行处理。
- en: Note
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Hadoop has turned out to be the point of convergence for all the data lakes.
    The need to shif deep learning to the data, which is already residing in Hadoop,
    has become quintessential.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 已经成为所有数据湖的汇聚点。将深度学习转移到已经存储在 Hadoop 中的数据，已成为必然的需求。
- en: Hadoop operates on the concept that *moving computation is cheaper than moving
    data* [86] [87]. Hadoop allows for the distributed processing of large-scale datasets
    across clusters of commodity servers. It also provides efficient load balancing,
    has a very high degree of fault tolerance, and is highly horizontally scalable
    with minimal effort. It can detect and tolerate failures in the application layers,
    and hence, is suitable for running on commodity hardware. To achieve the high
    availability of data, Hadoop, by default, keeps a replication factor of three,
    with a copy of each block placed on two other separate machines. So, if a node
    fails, the recovery can be done instantly from the other two nodes. The replication
    factor of Hadoop can be easily increased based on how valuable the data is and
    other associated requirements on the data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 基于*移动计算比移动数据更便宜*的概念运作 [86] [87]。Hadoop 允许在集群中的普通服务器上分布式处理大规模数据集。它还提供高效的负载均衡，具有非常高的容错能力，并且在水平扩展方面几乎不需要额外的努力。它可以检测并容忍应用层的故障，因此非常适合在普通硬件上运行。为了实现数据的高可用性，Hadoop
    默认保持一个复制因子为三，每个数据块的副本分布在另外两台机器上。因此，如果某个节点发生故障，恢复可以立即从另外两台节点完成。Hadoop 的复制因子可以根据数据的重要性及其他相关需求轻松增加。
- en: Hadoop was initially built mainly for processing the batch tasks, so it is mostly
    suitable for deep learning networks, where the main task is to find the classification
    of large-scale data. The selection of features to learn how to classify the data
    is mainly done on a large batch of datasets.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 最初主要用于处理批量任务，因此它最适合用于深度学习网络，其中主要任务是对大规模数据进行分类。特征选择用于学习如何对数据进行分类，通常是在一大批数据集上进行的。
- en: Hadoop is extremely configurable, and can easily be optimized as per the user's
    requirements. For example, if a user wants to keep more replicas of the data for
    better reliability, he can increase the replication factor. However, an increase
    in the number of replicas will eventually increase the storage requirements. Here
    we will not be explaining more about the features and configuration of data, rather
    we will mostly discuss the part of Hadoop which will be used extensively in distributed
    deep neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 是极其可配置的，可以根据用户的需求轻松进行优化。例如，如果用户希望为数据保留更多副本以提高可靠性，他可以增加复制因子。然而，副本数量的增加最终会增加存储需求。在这里，我们不会详细解释数据的特性和配置，而是主要讨论
    Hadoop 在分布式深度神经网络中的广泛应用部分。
- en: In the new version of Hadoop, the parts which we will mainly use in this book
    are HDFS, Map-Reduce, and **Yet Another Resource Negotiator** (**YARN**). YARN
    has already dominated Hadoop's Map-Reduce (explained in the next part) in a large
    manner. YARN currently has the responsibility to assign the works to the Data
    nodes (data server) of Hadoop. **Hadoop Distributed File System** (**HDFS**),
    on the other hand, is a distributed file system, which is distributed across all
    the Data nodes under a centralized meta-data server called NameNode. To achieve
    high-availability, in the later version, a secondary NameNode was integrated to
    Hadoop framework, the purpose of which is to have a copy of the metadata from
    primary NameNode after certain checkpoints.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在新版本的 Hadoop 中，本书主要使用的部分是 HDFS、Map-Reduce 和 **Yet Another Resource Negotiator**
    (**YARN**)。YARN 已经在很大程度上主导了 Hadoop 的 Map-Reduce（在下一部分将详细解释）。YARN 目前负责将任务分配给 Hadoop
    的数据节点（数据服务器）。**Hadoop 分布式文件系统** (**HDFS**) 是一种分布式文件系统，分布在所有数据节点上，通过一个集中管理的元数据服务器称为
    NameNode 来管理。为了实现高可用性，在后续版本中，Hadoop 框架集成了一个二级 NameNode，目的是在特定的检查点后保留主 NameNode
    的元数据副本。
- en: Map-Reduce
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Map-Reduce
- en: The Map-Reduce paradigm [83] is a distributed programming model developed by
    Google in 2004, and is associated with processing huge datasets with a parallel
    and distributed algorithm on a cluster of machines. The entire Map-Reduce application
    is useful with large-scale datasets. Basically, it has two primary components,
    one is called Map and the other is called Reduce, along with a few intermediate
    stages like shuffling, sorting and partitioning. In the map phase, the large input
    job is broken down into smaller ones, and each of the jobs is distributed to different
    cores. The operation(s) are then carried out on every small job placed on those
    machines. The Reduce phase accommodates all the scattered and transformed output
    into one single dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Map-Reduce 模式 [83] 是 Google 在 2004 年开发的一种分布式编程模型，旨在通过并行和分布式算法在集群中的多个机器上处理海量数据集。整个
    Map-Reduce 应用程序在大规模数据集上非常有用。基本上，它有两个主要组件，一个称为 Map，另一个称为 Reduce，此外还有一些中间阶段，如洗牌、排序和分区。在
    Map 阶段，大的输入任务被拆分为多个小任务，每个任务会被分配到不同的核心上。然后，在这些机器上的每个小任务上执行相应的操作。Reduce 阶段将所有散布的和转化的输出汇集成一个单一的数据集。
- en: 'Explaining the concept of Map-Reduce in detail is beyond the scope of this
    chapter; interested readers can go through *"Map-Reduce: Simplified data processing
    on large clusters*" [83] to get an in-depth knowledge of this.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '详细解释 Map-Reduce 的概念超出了本章的范围；有兴趣的读者可以阅读 *"Map-Reduce: Simplified data processing
    on large clusters"* [83] 以深入了解该内容。'
- en: Iterative Map-Reduce
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代 Map-Reduce
- en: The deep learning algorithms are iterative in nature - the models learn from
    the optimization algorithms, which go through multiple steps so that it leads
    to a point of minimal error. For these kinds of models the Map-Reduce application
    does not seem to work as efficiently as it does for other use-cases.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法具有迭代特性——模型从优化算法中学习，这些算法会经过多个步骤，以实现最小误差的目标。对于这些模型，Map-Reduce 应用程序似乎不像在其他用例中那样高效。
- en: Iterative Map-Reduce, a next generation YARN framework (unlike the traditional
    Map-Reduce) does multiple iterations on the data, which passes through only once.
    Although the architecture of Iterative Map-Reduce and Map-Reduce is dissimilar
    in design, the high level of understanding of both the architectures is simple.
    Iterative Map-Reduce is nothing but a sequence of Map-Reduce operations, where
    the output of the first Map-Reduce operation becomes the input to the next operation
    and so on. In the case of deep learning models, the map phase places all the operations
    of a particular iteration on each node of the distributed systems. It then distributes
    that massive input dataset to all the machines in the cluster. The training of
    the models is performed on each node of the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**Iterative Map-Reduce**，一种下一代YARN框架（与传统的Map-Reduce不同），对数据进行多次迭代，而数据仅经过一次。尽管**Iterative
    Map-Reduce**和**Map-Reduce**的架构设计不同，但对这两种架构的高层次理解是简单的。**Iterative Map-Reduce**不过是一个Map-Reduce操作的序列，其中第一个Map-Reduce操作的输出成为下一个操作的输入，依此类推。在深度学习模型中，**map**阶段将某一特定迭代的所有操作放置在分布式系统的每个节点上。然后，它将巨大的输入数据集分发到集群中的所有机器上。模型的训练在集群的每个节点上进行。'
- en: Before sending the aggregated new model back to each of the machines, the reduce
    phase takes all the outputs collected from the map phase and calculates the average
    of the parameters. The same operations are iterated over and over again by the
    Iterative Reduce algorithm until the learning process completes and the errors
    minimize to almost zero.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在将聚合后的新模型发送回每台机器之前，**reduce**阶段会将从**map**阶段收集的所有输出进行处理，并计算参数的平均值。**Iterative
    Reduce**算法会反复执行相同的操作，直到学习过程完成，并且误差降至接近零。
- en: '*Figure 2.4* compares the high-level functionalities of the two methods. The
    left image shows the block diagram of Map-Reduce, while on the right, we have
    the close-up of Iterative Map-Reduce. Each ''Processor'' is a working deep network,
    which is learning on small chunks of the larger dataset. In the ''Superstep''
    phase, the averaging of the parameters is done before the entire model is redistributed
    to the whole cluster as shown in the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.4*对比了这两种方法的高层次功能。左侧图像显示的是Map-Reduce的框图，而右侧则是Iterative Map-Reduce的特写。每个“Processor”是一个工作中的深度网络，正在对较大数据集的小块进行学习。在“Superstep”阶段，参数的平均值在整个模型重新分发到整个集群之前进行计算，如下图所示：'
- en: '![Iterative Map-Reduce](img/B05883_02_04.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Iterative Map-Reduce](img/B05883_02_04.jpg)'
- en: 'Figure 2.4: Difference of functionalities in Map-Reduce and parallel iterative
    reduce'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：Map-Reduce与并行迭代式reduce的功能差异
- en: Yet Another Resource Negotiator (YARN)
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Yet Another Resource Negotiator (YARN)
- en: 'The primary idea of YARN is to dissociate job scheduling and resource management
    from the data processing. So the data can continue to process in the system in
    parallel with the Map-Reduce batch jobs. YARN possesses a central resource manager,
    which mostly manages the Hadoop system resources according to the need. The node
    manager (specific to nodes) is responsible for managing and monitoring the processing
    of individual nodes of the cluster. This processing is dedicatedly controlled
    by an ApplicationMaster, which monitors the resources from the central resource
    manager, and works with the node manager to monitor and execute the tasks. The
    following figure gives an overview of the architecture of YARN:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: YARN的主要思想是将作业调度和资源管理与数据处理分离开来。这样，数据可以在与Map-Reduce批处理作业并行的情况下继续在系统中处理。YARN拥有一个中央资源管理器，它主要根据需要管理Hadoop系统资源。节点管理器（特定于节点）负责管理和监控集群中各个节点的处理过程。此处理过程由一个**ApplicationMaster**专门控制，**ApplicationMaster**监控来自中央资源管理器的资源，并与节点管理器协作，监控和执行任务。下图展示了YARN架构的概览：
- en: '![Yet Another Resource Negotiator (YARN)](img/B05883_02_05-1.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![Yet Another Resource Negotiator (YARN)](img/B05883_02_05-1.jpg)'
- en: 'Figure 2.5: An overview of the high-level architecture of YARN'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：YARN高层架构概览
- en: All these components of Hadoop are primarily used in distributed deep learning
    to overcome all the challenges stated earlier. The following subsection shows
    the criteria that need to be satisfied for better performance of distributed deep
    learning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的所有这些组件主要用于分布式深度学习，以克服之前提到的所有挑战。以下小节展示了分布式深度学习要实现更好性能所需满足的标准。
- en: Important characteristics for distributed deep learning design
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式深度学习设计的重要特征
- en: 'the following are the important characteristics of distributed deep learning
    design:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是分布式深度学习设计的重要特征：
- en: '**Small batch processing**: In distributed deep learning, the network must
    intake and process data quickly in parallel. To process and provide results more
    accurately, every node of the cluster should receive small chunks of data of approximately
    10 elements at a time.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**小批量处理**：在分布式深度学习中，网络必须快速并行地接收和处理数据。为了更准确地处理并提供结果，集群中的每个节点应每次接收约10个元素的小数据块。'
- en: For example, say the master node of YARN is coordinating 20 worker nodes for
    a Big dataset of 200 GB. The master node will split the dataset into 10 GB of
    20 small batches of data, allocating one small batch to each worker. The workers
    will process the data in parallel, and send the results back to the master as
    soon as it finishes the computing. All these outcomes will be aggregated by the
    master node, and the average of the results will be finally redistributed to the
    individual workers.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，假设YARN的主节点正在协调20个工作节点处理一个200GB的大数据集。主节点将数据集拆分成10GB的20个小批次数据，每个工作节点分配一个小批次。工作节点将并行处理数据，并在完成计算后将结果发送回主节点。所有这些结果将由主节点汇总，并最终将平均结果重新分配给各个工作节点。
- en: Deep learning networks perform well with small batches of nearly 10, rather
    than working with 100 or 200 large batches of data. Small batches of data empower
    the networks to learn from different orientations of the data in-depth, which
    later on recompiles to give a broader knowledge to the model.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度学习网络在使用接近10的小批量数据时表现更好，而不是使用100或200个大批量数据。小批量数据使网络能够深入地从数据的不同方向学习，这些学习成果最终重新编译，为模型提供更广泛的知识。
- en: On the other hand, if the batch size is too large, the network tries to learn
    quickly, which maximizes the errors. Conversly, smaller batch size slows down
    the speed of learning, and results in the possibility of divergence as the network
    approaches towards the minimum error rate.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一方面，如果批量大小过大，网络会尝试快速学习，这会最大化错误。相反，较小的批量大小会减慢学习速度，并可能导致发散，随着网络接近最小误差率。
- en: '**Parameter Averaging**: Parameter averaging is a crucial operation for the
    training of distributed deep network. In a network, parameters are generally the
    weight and biases of the node layers. As mentioned in the small batch processing
    section, once training is completed for several workers, they will pass different
    sets of parameters back to the master. With every iteration, the parameters are
    averaged, updated, and sent back to the master for further operations.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**参数平均**：参数平均是分布式深度网络训练中的关键操作。在一个网络中，参数通常是节点层的权重和偏置。如在小批量处理部分所述，一旦多个工作节点完成训练，它们会将不同的参数集返回给主节点。每次迭代时，参数会被平均、更新，并返回主节点进行进一步的操作。'
- en: 'The sequential process of parameter averaging can be outlined as follows:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数平均的顺序过程可以概述如下：
- en: The master configures the initial network and sets the different hyperparameters
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点配置初始网络并设置不同的超参数。
- en: Based on the configuration of the training master, the Big dataset is split
    into chunks of several smaller datasets
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据训练主节点的配置，大数据集被拆分成多个较小的数据集块。
- en: 'For each split of the training dataset, until the error rate approaches towards
    zero, perform the following:'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个训练数据集的拆分，直到误差率趋近于零，执行以下操作：
- en: The master distributes the parameter from the master to each individual worker
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点将参数从主节点分配给每个独立的工作节点。
- en: Each worker starts the training of the model with its dedicated chunk of dataset
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作节点开始使用其专门的数据集块训练模型。
- en: The average of the parameters is calculated and returned back to the master.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的平均值被计算并返回给主节点。
- en: The training completes, and the master will have one copy of the training network
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完成后，主节点将拥有一份训练网络的副本。
- en: 'Parameter averaging offers the following two important advantages in case of
    distributed training:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分布式训练中，参数平均提供以下两个重要优势：
- en: It enables parallelism by generating simultaneous results.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过生成同时的结果来实现并行化。
- en: It helps to prevent over-fitting by distributing the given dataset into multiple
    datasets of smaller sizes. The network then learns the average result, rather
    than just aggregating the results from different smaller batches.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过将给定的数据集分配到多个较小的数据集来帮助防止过拟合。然后，网络学习平均结果，而不是仅仅汇总来自不同小批次的结果。
- en: '*Figure 2.6* shows a combined diagrammatic overview of the small batch processing
    and parameter averaging operation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.6*展示了小批量处理和参数平均操作的结合图示概览：'
- en: '![Important characteristics for distributed deep learning design](img/B05883_02_06-1.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![分布式深度学习设计的关键特性](img/B05883_02_06-1.jpg)'
- en: 'Figure 2.6: Figure shows the high level architecture of a distributed deep
    learning architecture'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：图中展示了分布式深度学习架构的高层次架构图
- en: Deeplearning4j - an open source distributed framework for deep learning
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Deeplearning4j - 一个开源分布式深度学习框架
- en: '**Deeplearning4j** (**DL4J**) [82] is an open source deep learning framework
    which is written for JVM, and mainly used for commercial grade. The framework
    is written entirely in Java, and thus, the name ''4j'' is included. Because of
    its use with Java, Deeplearning4j has started to earn popularity with a much wider
    audience and range of practitioners.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**Deeplearning4j**（**DL4J**）[82]是一个为JVM编写的开源深度学习框架，主要用于商业级应用。该框架完全使用Java编写，因此名称中包含“4j”一词。由于与Java的兼容性，Deeplearning4j开始在更广泛的受众和从业人员中获得越来越多的关注。'
- en: This framework is basically composed of a distributed deep learning library
    that is integrated with Hadoop and Spark. With the help of Hadoop and Spark, we
    can very easily distribute the model and Big datasets, and run multiple GPUs and
    CPUs to perform parallel operations. Deeplearning4j has primarily shown substantial
    success in performing pattern recognition in images, sound, text, time series
    data, and so on. Apart from that, it can also be applied for various customer
    use cases such as facial recognition, fraud detection, business analytics, recommendation
    engines, image and voice search, and predictive maintenance with the sensor data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架基本上由一个分布式深度学习库组成，集成了Hadoop和Spark。借助Hadoop和Spark，我们可以非常容易地分发模型和大数据集，并运行多个GPU和CPU进行并行计算。Deeplearning4j主要在图像、声音、文本、时间序列数据等领域的模式识别中取得了显著的成功。除此之外，它还可以应用于各种客户场景，如人脸识别、欺诈检测、商业分析、推荐引擎、图像和语音搜索、以及传感器数据的预测性维护等。
- en: 'The following *Figure 2.7* shows a generic high-level architectural block diagram
    of Deeplearning4j:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图 2.7*展示了Deeplearning4j的通用高层架构框图：
- en: '![Deeplearning4j - an open source distributed framework for deep learning](img/B05883_02_07-1.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Deeplearning4j - 一个开源分布式深度学习框架](img/B05883_02_07-1.jpg)'
- en: 'Figure 2.7: High level architectural block diagram of Deeplearning4j [82]'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：Deeplearning4j的高层架构框图 [82]
- en: Major features of Deeplearning4j
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Deeplearning4j的主要特点
- en: Deeplearning4j comes with various attractive features, which completely distinguishes
    it from other existing deep learning tools like Theano, Torch, and so on.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Deeplearning4j具有许多独特的功能，使其与其他现有深度学习工具（如Theano、Torch等）完全不同。
- en: '**Distributed architecture**: Training in Deeplearning4j can be performed in
    two ways - with distributed, multi-threaded deep-learning, or with traditional,
    normal single-threaded deep-learning techniques. The training is carried out in
    clusters of commodity nodes. Therefore, Deeplearning4j is able to process any
    amount of data quickly. The neural networks are trained in parallel using the
    iterative reduce method, which works on Hadoop YARN and Spark. It also integrates
    with Cuda kernels to conduct pure GPU operations, and works with distributed GPUs.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式架构**：Deeplearning4j的训练可以通过两种方式进行——使用分布式、多线程深度学习，或使用传统的单线程深度学习技术。训练在商品节点的集群中进行。因此，Deeplearning4j能够快速处理任何量级的数据。神经网络采用迭代减少方法进行并行训练，该方法可在Hadoop
    YARN和Spark上运行。它还与Cuda内核集成，进行纯GPU操作，并支持分布式GPU。'
- en: Deeplearning4j operations can be run on Hadoop YARN or Spark as a job. In Hadoop,
    Iterative Reduce workers work on every block of HDFS, and synchronously process
    the data in parallel. As the processing completes, they push the transformed parameters
    back to their master, where the average of the parameters are taken and the model
    of each worker's node is updated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Deeplearning4j的操作可以作为作业在Hadoop YARN或Spark上运行。在Hadoop中，迭代减少工人会在每个HDFS块上工作，并同步并行处理数据。处理完成后，它们将转化后的参数推送回主节点，主节点计算参数的平均值，并更新每个工人节点的模型。
- en: In Deeplearning4j, the distributed runtimes are interchangeable, where they
    act like a directory in a huge modular architecture, which can be swapped in or
    out.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在Deeplearning4j中，分布式运行时是可互换的，它们在一个巨大的模块化架构中充当目录的角色，可以根据需要进行替换。
- en: '**Data parallelism**: There are two ways in which the neural networks can be
    trained in a distributed manner: one is data parallelism, and the other is model
    parallelism. Deeplearning4j follows data parallelism for training. In data parallelism,
    we can split the large dataset into chunks of smaller datasets, and distribute
    those to parallel models running on different servers to train in parallel.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据并行性**：神经网络可以通过两种方式进行分布式训练：一种是数据并行性，另一种是模型并行性。Deeplearning4j采用数据并行性进行训练。在数据并行性中，我们可以将大型数据集拆分成小块数据集，并将这些数据集分发给在不同服务器上运行的并行模型进行并行训练。'
- en: '**Scientific computing capability for the JVM**: For scientific computing in
    Java and Scala, Deeplearning4j includes an N-dimensional array class using **N-Dimensional
    Arrays for Java** (**ND4J**). The functionality of ND4J is much faster than what
    Numpy provides to Python, and its mostly written in C++. It''s effectively based
    on a library for matrix manipulation and linear algebra in a production environment.
    Most of the routines of ND4J are designed to run fast with minimum RAM requirements.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JVM的科学计算能力**：在Java和Scala中的科学计算，Deeplearning4j包括一个N维数组类，使用**Java的N维数组**（**ND4J**）。ND4J的功能比Python的Numpy要快得多，且大部分代码用C++编写。它实际上是一个用于矩阵操作和线性代数的生产环境库。ND4J的大多数例程都设计为以最小的内存需求快速运行。'
- en: '**Vectorization tool for machine learning**: For vectorization of various file
    formats and data types, Canova has been merged with Deeplearning4j. Canova performs
    vectorization using an input/output system similar to how Hadoop uses Map-Reduce.
    Canova is primarily designed to vectorize text, CSVs, images, sounds, videos,
    and so on from the **command line interface** (**CLI**).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习的向量化工具**：为了对各种文件格式和数据类型进行向量化，Canova已与Deeplearning4j合并。Canova使用类似于Hadoop使用Map-Reduce的输入/输出系统来执行向量化。Canova主要用于通过**命令行界面**（**CLI**）对文本、CSV、图像、声音、视频等进行向量化。'
- en: Summary of functionalities of Deeplearning4j
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Deeplearning4j功能总结
- en: 'The following are summary of functionalities of Deeplearning4j:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Deeplearning4j功能的总结：
- en: Deeplearning4j can be claimed as the most complete, production-ready, open source
    deep learning library ever built
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deeplearning4j可以称为迄今为止最完整、最具生产力、开源的深度学习库
- en: Compared to Theano-based tools, it has many more features specially designed
    for deep networks
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与基于Theano的工具相比，它有更多专为深度网络设计的功能
- en: Deeplearning4j is very easy to use; even non-specialists can apply its conventions
    to solve computationally intensive problems
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deeplearning4j非常易于使用，即使是非专业人士也能运用其惯例解决计算密集型问题
- en: The tools provide a wide range of applicability, hence, the networks work equally
    well with image, sound, text, and time-series
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些工具具有广泛的适用性，因此，网络可以同样有效地处理图像、声音、文本和时间序列数据
- en: It is completely distributed and can run multiple GPUs in parallel, unlike Theano
    [84], which is not distributed, and Torch7 [85], which has not automated its distribution
    like DL4J
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是完全分布式的，可以并行运行多个GPU，这与未分布式的Theano [84]和未自动化分布式处理的Torch7 [85]不同，后者像DL4J一样没有自动化其分布式功能
- en: Setting up Deeplearning4j on Hadoop YARN
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Hadoop YARN上设置Deeplearning4j
- en: Deeplearning4j primarily works on networks having multiple layers. To get started
    working with Deeplearning4j, one needs to get accustomed with the prerequisites,
    and how to install all the dependent software. Most of the documentation can be
    easily found on the official website of Deeplearning4j at [https://deeplearning4j.org/](https://deeplearning4j.org/) [88].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Deeplearning4j主要用于多层网络的工作。要开始使用Deeplearning4j，需要熟悉相关的先决条件，以及如何安装所有依赖的软件。大多数文档可以很容易地在Deeplearning4j的官方网站上找到：[https://deeplearning4j.org/](https://deeplearning4j.org/)
    [88]。
- en: In this section of the chapter, we will help you to get familiar with the code
    of Deeplearning4j. Initially, we will show the implementation of a simple operation
    of a multilayer neural network with Deeplearning4j. The later part of the section
    will discuss distributed deep learning with Deeplearning4j library. Deeplearning4j
    trains distributed deep neural network on multiple distributed GPUs using Apache
    Spark. The later part of this section will also introduce the setup of Apache
    Spark for Deeplearning4j.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一部分，我们将帮助你熟悉Deeplearning4j的代码。最初，我们将展示如何使用Deeplearning4j实现一个简单的多层神经网络操作。本节的后半部分将讨论使用Deeplearning4j库进行分布式深度学习。Deeplearning4j通过使用Apache
    Spark在多个分布式GPU上训练分布式深度神经网络。本节的后半部分还将介绍Deeplearning4j的Apache Spark设置。
- en: Getting familiar with Deeplearning4j
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熟悉Deeplearning4j
- en: This part will mainly introduce the 'Hello World' programs of deep learning
    with deeplearning4j. We will explain the basic functions of the library with the
    help of two simple deep learning problems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将主要介绍使用 deeplearning4j 的“Hello World”深度学习程序。我们将通过两个简单的深度学习问题，借助库的基本功能进行讲解。
- en: In Deeplearning4j, `MultiLayerConfiguration`, a class of the library can be
    considered as the base of the building block, which is responsible for organizing
    the layers and the corresponding hyperparameters of a neural network. This class
    can be considered as the core building block of Deeplearning4j for neural networks.
    Throughout the book, we will use this class to configure different multilayer
    neural networks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Deeplearning4j 中，`MultiLayerConfiguration` 类可以视为构建块的基础，负责组织神经网络的各层及其相应的超参数。这个类可以看作是
    Deeplearning4j 中神经网络的核心构建块。在本书中，我们将使用这个类来配置不同的多层神经网络。
- en: Note
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Hyperparameters are the main backbone to determine the learning process of a
    neural network. They mostly include how to initialize the weights of the models,
    how many times they should be updated, the learning rate of the model, which optimization
    algorithms to use, and so on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是决定神经网络学习过程的主要支柱。它们通常包括如何初始化模型的权重、更新次数、模型的学习率、使用的优化算法等。
- en: In the first example, we will show how to classify data patterns for the multilayer
    perceptron classifier with the help of Deeplearning4j.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们将展示如何利用 Deeplearning4j 对多层感知器分类器进行数据模式分类。
- en: 'The following is the sample training dataset that will be used in this program:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本程序中将使用的示例训练数据集：
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Initially, we need to initialize the various hyperparameters of the networks.
    The following piece of code will set the ND4J environment for the program:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们需要初始化网络的各种超参数。以下代码片段将为程序设置 ND4J 环境：
- en: '[PRE1]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Number of epochs is set to `30`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 训练周期数设置为 `30`：
- en: '[PRE2]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following piece of code will load the training data to the network:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将把训练数据加载到网络中：
- en: '[PRE3]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As the training data is loaded next we load the test data into the model with
    the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据加载后，接下来我们将通过以下代码将测试数据加载到模型中：
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Organization of all the layers of the network model as well as setting up the
    hyperparameters can be done with the following piece of code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络层的组织以及超参数的设置可以通过以下代码片段完成：
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we have loaded the training and test dataset, the initialization of the
    model can be done by calling the `init()` method. This will also start the training
    of the model from the given inputs:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经加载了训练和测试数据集，通过调用 `init()` 方法可以完成模型的初始化。这也将开始从给定输入中训练模型：
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To check the output after a certain internal, let''s print the scores every
    `5` parameter updates:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查一定时间间隔后的输出，我们可以每进行 `5` 次参数更新时打印一次得分：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, the network is trained by calling the `.fit()` method:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过调用 `.fit()` 方法来训练网络：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So the training of the model is done. In the next part, the data points will
    be plotted and the corresponding accuracy of the data will be calculated as shown
    in the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以模型的训练已经完成。在下一部分，数据点将被绘制，并计算数据的对应准确性，如下代码所示：
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code will store all the training data in an array before plotting
    those in the graph:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将在图表中绘制之前，将所有训练数据存储在一个数组中：
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the test data through the network and generating the prediction can
    be done with the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网络运行测试数据并生成预测，可以使用以下代码：
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When the preceding code is executed, it will run for approximately 5-10 seconds,
    depending upon your system configuration. During that time, you can check the
    console, which will display the updated score of training for your model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行上述代码时，它将运行大约 5-10 秒，具体时间取决于你的系统配置。在此期间，你可以查看控制台，控制台将显示模型训练的更新得分。
- en: 'A piece of evaluation is displayed as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了一段评估结果：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, the program will output the different statistics of the training for
    the model using Deeplearning4j as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，程序将使用 Deeplearning4j 输出模型训练的不同统计信息，如下所示：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the background, we can visualize the plotting of the data, which will give
    an impression of what the planet Saturn looks like. In the next part, we will
    show how to integrate Hadoop YARN and Spark with Deeplearning4j. The following
    *Figure 2.8* shows the output of the program in graphical representation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，我们可以将数据的绘制过程可视化，这将给人一种土星行星的印象。在下一部分中，我们将展示如何将Hadoop YARN与Spark以及Deeplearning4j进行集成。以下的*图2.8*展示了程序的图形化输出：
- en: '![Getting familiar with Deeplearning4j](img/image_02_008.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![熟悉Deeplearning4j](img/image_02_008.jpg)'
- en: 'Figure 2.8: The scattered data points are plotted when the preceding program
    is executed. The data points give an impression of the planet Saturn'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：当前面的程序执行时，绘制的散点数据点给人一种土星行星的印象
- en: Integration of Hadoop YARN and Spark for distributed deep learning
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop YARN和Spark集成进行分布式深度学习
- en: 'To use Deeplearning4j on Hadoop, we need to include the `deeplearning-hadoop`
    dependency as shown in the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Hadoop上使用Deeplearning4j，我们需要包括`deeplearning-hadoop`依赖项，如下所示：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, for Spark, we have to include the `deeplearning-spark` dependency
    as shown in the following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于Spark，我们需要包括`deeplearning-spark`依赖项，如下所示：
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Explaining the detailed functionalities of Apache Spark is beyond the scope
    of this book. Interested readers can catch up on the same at [http://spark.apache.org/](http://spark.apache.org/)
    .
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 解释Apache Spark的详细功能超出了本书的范围。感兴趣的读者可以在[http://spark.apache.org/](http://spark.apache.org/)了解更多内容。
- en: Rules to configure memory allocation for Spark on Hadoop YARN
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Spark在Hadoop YARN上内存分配的规则
- en: 'As already stated in the previous section, Apache Hadoop YARN is a cluster
    resource manager. When Deeplearning4j submits a training job to a YARN cluster
    via Spark, it is the responsibility of YARN to manage the allocation of resources
    such as CPU cores, amount of memory consumed by each executor, and so on. However,
    to extract the best performance from Deeplearning4j on YARN, some careful memory
    configuration is desired. This is done as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Apache Hadoop YARN是一个集群资源管理器。当Deeplearning4j通过Spark提交训练任务到YARN集群时，YARN负责管理资源分配，如CPU核心、每个执行者消耗的内存量等。然而，为了从YARN上的Deeplearning4j获得最佳性能，需要进行一些精确的内存配置。操作如下：
- en: The executer JVM memory amount needs to be specified using `spark.executor.memory`.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行者JVM内存量需要通过`spark.executor.memory`来指定。
- en: The YARN container memory overhead needs to be specified using `spark.yarn.executor.memoryOverhead`.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN容器内存开销需要通过`spark.yarn.executor.memoryOverhead`来指定。
- en: The sum of `spark.executor.memory` and `spark.yarn.executor.memoryOverhead`
    must always be less than the amount of memory allocated to a container by the
    YARN.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.executor.memory`和`spark.yarn.executor.memoryOverhead`的总和必须始终小于YARN分配给容器的内存量。'
- en: ND4j and JavaCPP should know the allotment of the off-heap memory; this can
    be done using the `org.bytedeco.javacpp.maxbytes` system property.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ND4j和JavaCPP应当知道堆外内存的分配；这可以通过`org.bytedeco.javacpp.maxbytes`系统属性来完成。
- en: '`org.bytedeco.javacpp.maxbytes` must be less than `spark.yarn.executor.memoryOverhead`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.bytedeco.javacpp.maxbytes`必须小于`spark.yarn.executor.memoryOverhead`。'
- en: 'The current version of Deeplearning4j uses parameter averaging to perform distributed
    training of the neural network. The following operation is performed exactly the
    way it is described in the parameter averaging part of the earlier section:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当前版本的Deeplearning4j使用参数平均法执行神经网络的分布式训练。以下操作正是按照前面章节中参数平均部分的描述进行的：
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To list all the files from HDFS so as to run the code on different nodes, run
    the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出HDFS中的所有文件，以便在不同节点上运行代码，请运行以下代码：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A complete code for how to set up Spark with YARN and HDFS will be provided
    along with the code bundle. For simplicity, only part of the code is shown here
    for the purpose of understanding.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如何设置Spark与YARN和HDFS的完整代码将与代码包一起提供。为了简化，本文只展示部分代码，以便理解。
- en: Now, we will show an example to demonstrate how to use Spark and load the data
    into memory with Deeplearning4j. We will use a basic DataVec example to show some
    pre-processing operation on some CSV data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将展示一个示例，演示如何使用Spark并将数据加载到内存中与Deeplearning4j结合使用。我们将使用一个基本的DataVec示例来展示如何对一些CSV数据进行预处理操作。
- en: 'The sample dataset will look as like the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据集将如下所示：
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The problem statement of the program is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的问题描述如下：
- en: Remove some unnecessary columns
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除一些不必要的列
- en: Filter out data, and keep only examples with values `USA` and `MX` for the `MerchantCountryCode`
    column
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤数据，仅保留`MerchantCountryCode`列中值为`USA`和`MX`的示例
- en: Replace the invalid entries in the `TransactionAmountUSD` column
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换`TransactionAmountUSD`列中的无效条目
- en: Parse the data string, and collect the hour of day from it to create a new `HourOfDay`
    column
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析数据字符串，并从中提取出一天中的小时数，创建一个新的`HourOfDay`列
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following part will define the operations that we want to perform on the
    dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将定义我们要在数据集上执行的操作：
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In unstructured data, the datasets are generally noisy, and so we need to take
    care of some of the invalid data. In case of negative dollar value, the program
    will replace those to `0.0`. We will keep the positive dollar amounts intact.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在非结构化数据中，数据集通常比较嘈杂，因此我们需要处理一些无效数据。如果出现负数美元值，程序会将其替换为`0.0`。我们会保留正数美元金额不变。
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, to format the `DateTime` format as per the problem statement, the following
    piece of code is used:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了按照问题描述格式化`DateTime`格式，使用以下代码段：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A different schema is created after execution of the all these operations as
    follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所有这些操作后，会创建一个不同的架构，具体如下：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following piece of code will set Spark to perform all the operations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段将设置Spark执行所有操作：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To take the data directly from HDFS, one has to pass `hdfs://{the filepath
    name}`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要直接从HDFS获取数据，必须传递`hdfs://{文件路径名称}`：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The input data are parsed using `CSVRecordReader()` method as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据通过`CSVRecordReader()`方法进行解析，具体如下：
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The pre-defined transformation of Spark is performed as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 执行Spark的预定义转换，具体如下：
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As mentioned, to save the data back to HDFS, just putting the file path after
    `hdfs://` will do:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，要将数据保存回HDFS，只需将文件路径放在`hdfs://`后面即可：
- en: '[PRE28]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'When the program is executed with Spark using Deeplearning4j, we will get the
    following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Deeplearning4j执行Spark程序时，我们将得到以下输出：
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following is the output:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Similar to this example, lots of other datasets can be processed in a customized
    way in Spark. From the next chapter, we will show the Deeplearning4j codes for
    specific deep neural networks. The implementation of Apache Spark and Hadoop YARN
    is a generic procedure, and will not change according to neural network. Readers
    can use that code to deploy the deep network code in cluster or locally, based
    on their requirements.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与此示例类似，许多其他数据集可以在Spark中以自定义方式处理。从下一章开始，我们将展示具体深度神经网络的Deeplearning4j代码。Apache
    Spark和Hadoop YARN的实现是通用过程，不会根据神经网络的不同而改变。读者可以根据需要使用该代码在集群或本地部署深度网络代码。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In contrast to the traditional machine learning algorithms, deep learning models
    have the capability to address the challenges imposed by a massive amount of input
    data. Deep learning networks are designed to automatically extract complex representation
    of data from the unstructured data. This property makes deep learning a precious
    tool to learn the hidden information from the big data. However, due to the velocity
    at which the volume and varieties of data are increasing day by day, deep learning
    networks need to be stored and processed in a distributed manner. Hadoop, being
    the most widely used big data framework for such requirements, is extremely convenient
    in this situation. We explained the primary components of Hadoop that are essential
    for distributed deep learning architecture. The crucial characteristics of distributed
    deep learning networks were also explained in depth. Deeplearning4j, an open source
    distributed deep learning framework, integrates with Hadoop to achieve the mentioned
    indispensable requirement. Deeplearning4j is entirely written in Java, can process
    data faster in a distributed manner with iterative Map-Reduce, and can address
    many problems imposed by the large-scale data. We have provided two sample examples
    to let you know about basic Deeplearning4j codes and syntax. We have also provided
    some code snippets for Spark configuration with integration with Hadoop YARN and
    Hadoop Distributed File System.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习算法相比，深度学习模型具有解决大量输入数据所带来的挑战的能力。深度学习网络旨在自动从非结构化数据中提取复杂的表示。这一特性使得深度学习成为从大数据中学习隐藏信息的宝贵工具。然而，由于数据的数量和种类每天都在快速增加，深度学习网络需要以分布式方式进行存储和处理。Hadoop，作为应对这种需求最广泛使用的大数据框架，在这种情况下非常方便。我们解释了Hadoop中一些分布式深度学习架构所必需的主要组件。分布式深度学习网络的关键特性也进行了深入的讲解。Deeplearning4j，一个开源的分布式深度学习框架，能够与Hadoop集成，满足上述不可或缺的需求。Deeplearning4j完全用Java编写，能够以分布式方式通过迭代的Map-Reduce更快地处理数据，并能够应对大规模数据所带来的许多问题。我们提供了两个示例，帮助您了解Deeplearning4j的基本代码和语法。我们还提供了一些Spark配置的代码片段，并与Hadoop
    YARN和Hadoop分布式文件系统集成。
- en: The next chapter of this book will introduce convolutional neural network, a
    popular deep learning network. The chapter will discuss the method convolution
    and how it can be used to build an advanced neural network mainly for image processing
    and image recognition. The chapter will then provide information on how a convolutional
    neural network can be implemented using Deeplearning4j.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的下一章将介绍卷积神经网络（CNN），一种流行的深度学习网络。该章节将讨论卷积方法及其如何用于构建主要用于图像处理和图像识别的高级神经网络。接下来，本章将提供如何使用Deeplearning4j实现卷积神经网络的信息。
