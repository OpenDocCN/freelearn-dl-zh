- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Considering Hardware for Inference
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑用于推理的硬件
- en: In *Part 3, Serving Deep Learning Models* of this book, we will focus on how
    to develop, optimize, and operationalize inference workloads for **deep learning**
    (**DL**) models. Just like training, DL inference is computationally intensive
    and requires an understanding of specific types of hardware built for inference,
    model optimization techniques, and specialized software servers to manage model
    deployment and handle inference traffic. Amazon SageMaker provides a wide range
    of capabilities to address these aspects.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*第 3 部分，深度学习模型的服务*中，我们将重点讨论如何开发、优化和将推理工作负载实现生产化，适用于**深度学习**（**DL**）模型。与训练类似，DL
    推理是计算密集型的，且需要理解特定类型的硬件，这些硬件专为推理设计，此外还需要掌握模型优化技术，以及专门的软件服务器来管理模型部署并处理推理流量。Amazon
    SageMaker 提供了广泛的功能来解决这些方面的问题。
- en: In this chapter, we will discuss hardware options and model optimization for
    model serving. We will review the available hardware accelerators that are suitable
    for DL inference and discuss how to select one. Amazon SageMaker offers multiple
    NVIDIA GPU accelerators and a proprietary chip built for DL inference – **AWS
    Inferentia**. SageMaker also allows you to access accelerator capacity using its
    **Elastic Inference** capability. As each inference use case is unique and comes
    with its own set of business requirements, we will propose a set of selection
    criteria that can be used when evaluating the optimal hardware accelerator for
    inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论模型服务的硬件选项和模型优化。我们将回顾适用于 DL 推理的可用硬件加速器，并讨论如何选择其中之一。Amazon SageMaker
    提供了多种 NVIDIA GPU 加速器和专为 DL 推理设计的专有芯片——**AWS Inferentia**。SageMaker 还允许您使用其 **Elastic
    Inference** 功能访问加速器容量。由于每个推理用例都是独特的，并且有其特定的业务需求，我们将提出一套选择标准，供您在评估最适合推理的硬件加速器时参考。
- en: Another important aspect when building your DL inference workload is understanding
    how to optimize a specific model architecture for inference on the target hardware
    accelerator. This process is known as model compilation. We will review the popular
    optimizer and runtime environment known as NVIDIA **TensorRT**, which delivers
    optimal latency and throughput for models running on NVIDIA GPU accelerators.
    Then, we will discuss **Neuron SDK**, which optimizes models to run on AWS Inferentia
    chips. We will also discuss **SageMaker Neo** – a managed model compilation service
    that allows you to compile models for a wide range of data center and edge hardware
    accelerators. Note that you won’t cover any edge or embedded platforms in this
    book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 DL 推理工作负载时，另一个重要的方面是理解如何针对目标硬件加速器优化特定的模型架构。这一过程被称为模型编译。我们将回顾流行的优化器和运行时环境——NVIDIA
    **TensorRT**，它为在 NVIDIA GPU 加速器上运行的模型提供最佳的延迟和吞吐量。接着，我们将讨论 **Neuron SDK**，它优化模型以便在
    AWS Inferentia 芯片上运行。我们还将讨论 **SageMaker Neo**——一种托管的模型编译服务，允许您为各种数据中心和边缘硬件加速器编译模型。请注意，本书不涉及任何边缘或嵌入式平台。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Selecting hardware accelerators in AWS Cloud
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 云中选择硬件加速器
- en: Compiling models for inference
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为推理编译模型
- en: After reading this chapter, you will be able to select an efficient hardware
    configuration for your inference workloads with optimal price/performance characteristics
    and perform further optimizations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将能够为您的推理工作负载选择一种高效的硬件配置，具有最佳的价格/性能特性，并进行进一步的优化。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will provide code samples so that you can develop practical
    skills. The full code examples are available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供代码示例，帮助您培养实际技能。完整的代码示例可以在此处查看：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/)。
- en: 'To follow along with this code, you will need the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随此代码，您需要以下内容：
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 AWS 账户和具有管理 Amazon SageMaker 资源权限的 IAM 用户。
- en: Have a SageMaker notebook, SageMaker Studio notebook, or local SageMaker compatible
    environment established.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已建立 SageMaker 笔记本、SageMaker Studio 笔记本或本地兼容 SageMaker 的环境。
- en: Access to GPU training instances in your AWS account. Each example in this chapter
    will provide recommended instance types to use. You may need to increase your
    compute quota for **SageMaker Training Job** to have GPU instances enabled. In
    this case, please follow the instructions at [https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问AWS账户中的GPU训练实例。本章中的每个示例将提供推荐的实例类型。你可能需要增加**SageMaker训练作业**的计算配额，以启用GPU实例。在这种情况下，请按照[https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml)中的说明进行操作。
- en: You must install the required Python libraries by running `pip install -r requirements.txt`.
    The file that contains the required libraries can be found in the `chapter8` directory.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须通过运行`pip install -r requirements.txt`来安装所需的Python库。包含所需库的文件可以在`chapter8`目录中找到。
- en: In this chapter, we will provide examples of compiling models for inference,
    which requires access to specific accelerator types. If you intend to follow these
    code samples, please provision a SageMaker notebook instance or SageMaker Studio
    notebook with the target accelerator.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供编译模型以进行推理的示例，这需要访问特定的加速器类型。如果你打算跟随这些代码示例，请准备好具有目标加速器的SageMaker笔记本实例或SageMaker
    Studio笔记本。
- en: Selecting hardware accelerators in AWS Cloud
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS Cloud中选择硬件加速器
- en: AWS Cloud and Amazon SageMaker provide a range of hardware accelerators that
    are suitable for inference workloads. Choosing a hardware platform often requires
    multiple experiments to be performed with various accelerators and serving parameters.
    Let’s look at some key selection criteria that can be useful during the evaluation
    process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Cloud和Amazon SageMaker提供一系列适用于推理工作负载的硬件加速器。选择硬件平台通常需要通过多次实验，使用不同的加速器和服务参数进行测试。我们来看一些在评估过程中可能有用的关键选择标准。
- en: Latency-throughput trade-offs
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟-吞吐量权衡
- en: Inference latency defines how quickly your model can return inference outcomes
    to the end user, and we want to minimize latency to improve user experience. Inference
    throughput defines how many inference requests can be processed simultaneously,
    and we want to maximize it to guarantee that as many inference requests as possible
    are served. In software engineering, it’s common to discuss latency-throughput
    trade-offs as it’s usually impractical to minimize latency and maximize throughput
    at the same time, so you need to find a balance between these characteristics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 推理延迟定义了你的模型能多快地返回推理结果给最终用户，我们希望最小化延迟以改善用户体验。推理吞吐量定义了可以同时处理多少个推理请求，我们希望最大化吞吐量，以确保尽可能多的推理请求能够被处理。在软件工程中，通常会讨论延迟和吞吐量之间的权衡，因为通常很难同时最小化延迟并最大化吞吐量，因此你需要在这两个特性之间找到平衡。
- en: It’s common to set target latency and throughput SLAs as part of the business
    requirements of your specific use case. Finding an acceptable latency-throughput
    trade-off requires benchmarking with different accelerators and model/server parameters
    against target SLAs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体的使用案例中，通常会将目标延迟和吞吐量SLA作为业务需求的一部分。找到可接受的延迟和吞吐量的权衡需要通过与不同加速器以及模型/服务器参数进行基准测试，以符合目标SLA。
- en: For instance, when running a real-time inference endpoint, usually, you are
    concerned with the latency SLA as it will have a direct impact on your end users.
    You may start by finding the hardware and model configuration with the latency
    within the target SLA number, and then scale it to reach the desired throughput.
    In the case of batch inference, overall system throughput is usually more important
    than latency. We want to maximize throughput to guarantee that our hardware resources
    are utilized efficiently.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在运行实时推理端点时，通常关注的是延迟SLA，因为它会直接影响到最终用户。你可以先找到延迟在目标SLA范围内的硬件和模型配置，然后再扩展以达到期望的吞吐量。在批量推理的情况下，整体系统吞吐量通常比延迟更为重要。我们希望最大化吞吐量，以确保我们的硬件资源得到有效利用。
- en: Cost
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: The cost of running the inference workload is another important parameter that
    influences which hardware you use and your latency and throughput SLAs. While
    AWS and SageMaker offer one of the most powerful GPU accelerators on the market,
    their cost may be prohibitively high for your specific use case. Hence, it’s often
    the case that you may need to adjust your latency and throughput SLAs to make
    your DL inference application economically viable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 运行推理工作负载的成本是另一个重要参数，它会影响你使用的硬件以及延迟和吞吐量服务水平协议（SLA）。虽然 AWS 和 SageMaker 提供了市场上最强大的
    GPU 加速器之一，但它们的成本对于你的特定用例可能过于高昂。因此，通常情况下，你可能需要调整延迟和吞吐量 SLA，以使你的深度学习推理应用在经济上可行。
- en: Supported frameworks and operators
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的框架和操作符
- en: Running inference workloads is computationally intensive as it requires calculating
    a model forward pass on a single or batch of inputs. Each forward pass consists
    of a sequence of individual compute tasks. A type of computing task is known as
    an operator. Some examples of common DL operators include matrix multiplication,
    convolution, and average pooling.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 运行推理工作负载需要大量的计算资源，因为它需要对单个或一批输入执行模型的前向传递。每次前向传递由一系列独立的计算任务组成。计算任务的一种类型被称为操作符。常见的深度学习操作符的例子包括矩阵乘法、卷积和平均池化。
- en: Operators supported by DL frameworks can always run on CPU devices. However,
    CPU, as we discussed in [*Chapter 5*](B17519_05.xhtml#_idTextAnchor083), *Considering
    Hardware for Training*, is not the most efficient DL accelerator. Therefore, running
    inference using a CPU results in higher latency compared to specialized accelerators
    such as GPUs and ASIC chips.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架支持的操作符始终可以在 CPU 设备上运行。然而，正如我们在 [*第 5 章*](B17519_05.xhtml#_idTextAnchor083)
    中讨论的，*考虑用于训练的硬件*，CPU 并不是最有效的深度学习加速器。因此，使用 CPU 运行推理会导致比使用 GPU 和 ASIC 芯片等专用加速器更高的延迟。
- en: NVIDIA GPU accelerators support a wide range of operators via the CUDA toolkit.
    In certain cases, you may need to implement a new operator for your specific model
    architecture. The CUDA toolkit provides a programming API for such custom operator
    development.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA GPU 加速器通过 CUDA 工具包支持广泛的操作符。在某些情况下，你可能需要为你的特定模型架构实现新的操作符。CUDA 工具包为此类自定义操作符开发提供了编程
    API。
- en: ASIC accelerators such as AWS Inferentia provide support for a finite list of
    operators and frameworks. In cases where a specific operator is not supported,
    this operator will be executed on the CPU device. This allows you to run many
    model architectures on specialized accelerators, but on the other hand, it will
    likely result in increased inference latency due to overall slowness of CPU execution
    and the necessary handoff between the ASIC and CPU accelerators of the tensors
    during the model’s forward pass.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Inferentia 等 ASIC 加速器支持有限的操作符和框架列表。在某些操作符不受支持的情况下，该操作符将由 CPU 设备执行。这使得你可以在专用加速器上运行许多模型架构，但另一方面，它很可能会导致推理延迟增加，因为
    CPU 执行的整体缓慢性以及在模型前向传递过程中 ASIC 和 CPU 加速器之间的必要数据传递。
- en: Hence, when choosing your target hardware accelerator, you need to understand
    which DL frameworks and operators are supported.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在选择目标硬件加速器时，你需要了解支持哪些深度学习框架和操作符。
- en: In [*Chapter 5*](B17519_05.xhtml#_idTextAnchor083), *Considering Hardware for
    Deep Learning Training*, we provided an overview of the available hardware accelerators
    for DL on the Amazon SageMaker platform. In the following sections, we will highlight
    some accelerators and compute instances recommended for inference workloads.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 5 章*](B17519_05.xhtml#_idTextAnchor083) 中，*考虑用于深度学习训练的硬件*，我们概述了 Amazon
    SageMaker 平台上可用的深度学习硬件加速器。在接下来的部分，我们将重点介绍一些推荐用于推理工作负载的加速器和计算实例。
- en: G4 instance family – best price and performance ratio for inference
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: G4 实例系列 – 具有最佳价格和性能比的推理选项
- en: The G4 instances feature NVIDIA T4 Tensor Core GPUs with 16 GB of memory. This
    accelerator is designed by NVIDIA for inference in the cloud and data centers.
    It supports FP32, FP16, INT8, and INT4 precision types. G4 should be considered
    as the default option for running DL inference workloads since it combines performance
    characteristics relevant for inference workloads and lower cost compared to the
    more powerful P3 family.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: G4 实例配备了 NVIDIA T4 Tensor Core GPU，拥有 16 GB 的内存。这款加速器是 NVIDIA 为云计算和数据中心的推理任务设计的。它支持
    FP32、FP16、INT8 和 INT4 精度类型。由于 G4 综合了与推理工作负载相关的性能特征，并且与更强大的 P3 系列相比成本更低，因此应该被视为运行深度学习推理工作负载的默认选项。
- en: For further performance optimizations, you can compile your model using the
    NVIDIA TensorRT optimizer. We will discuss the TensorRT optimizer in detail in
    the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步优化性能，你可以使用 NVIDIA TensorRT 优化器编译你的模型。我们将在下一节详细讨论 TensorRT 优化器。
- en: P3 instance family – performant and expensive for inference
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P3 实例系列——适用于推理的高性能但昂贵
- en: A P3 instance with NVIDIA V100 accelerators is primarily designed for large-scale
    training. Compared to G4, the P3 family has up to 32 GB of GPU memory and larger
    network bandwidth (both inter-GPU and inter-node). P3 also supports the F64, FP32,
    FP16, and INT8 precision types.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 配备 NVIDIA V100 加速器的 P3 实例主要是为大规模训练设计的。与 G4 相比，P3 系列最多可提供 32 GB 的 GPU 内存和更大的网络带宽（包括
    GPU 间和节点间的带宽）。P3 还支持 F64、FP32、FP16 和 INT8 精度类型。
- en: Many of P3’s characteristics are very desirable for large-scale distributed
    training, but they are less relevant for inference. For instance, you rarely need
    to have a double precision type; rather, you want to reduce precision during inference
    to minimize latency. Higher network bandwidth (specifically inter-node) is also
    less relevant for inference workloads since it’s rare to distribute your model
    across nodes at serving time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: P3 的许多特性非常适合大规模分布式训练，但对于推理则不太相关。例如，你很少需要使用双精度类型；相反，你希望在推理过程中降低精度，以减少延迟。更高的网络带宽（特别是节点间带宽）对于推理工作负载也不太相关，因为在推理时通常不需要将模型分布到不同节点上。
- en: So, while the P3 family is more performant than G4, it costs more and has minimal
    benefits for inference workloads. One scenario where you may want to use P3 instead
    of G4 is when you’re running inference for large models. In this case, the `P3dn.24xlarge`
    instance can provide you with 8 V100 GPUs with 32 GB each.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然 P3 系列的性能优于 G4，但其成本更高，对于推理工作负载的好处很少。你可能想要选择 P3 而非 G4 的一种情况是，当你正在运行大模型的推理时。在这种情况下，`P3dn.24xlarge`
    实例可以为你提供 8 个每个有 32 GB 内存的 V100 GPU。
- en: Important note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Not that here, we are only considering accelerators that are available as part
    of SageMaker. Some instance families (such as the G5 and P4 families) are only
    available as part of the Amazon EC2 service. We expect these instances to be supported
    by Amazon SageMaker in the future.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们只考虑了作为 SageMaker 一部分提供的加速器。一些实例系列（如 G5 和 P4 系列）仅作为 Amazon EC2 服务的一部分提供。我们预计这些实例将在未来被
    Amazon SageMaker 支持。
- en: AWS Inferentia
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Inferentia
- en: AWS Inferentia is a purpose-built ASIC accelerator for DL inference workloads.
    According to AWS, it offers the lowest inference cost in the cloud. Each Inferentia
    chip consists of four NeuronCores, which are high-performance matrix-multiply
    engines. NeuronCores are optimized for operations on small batch sizes to guarantee
    the lowest possible inference latency. Inferentia supports the FP16, BF16, and
    INT8 precision types. The `g4dn.xlarge` alternative, while costing 70% less to
    run.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Inferentia 是一个专门为深度学习推理工作负载设计的 ASIC 加速器。根据 AWS 的说法，它提供了云中最低的推理成本。每个 Inferentia
    芯片由四个 NeuronCore 组成，它们是高性能的矩阵乘法引擎。NeuronCore 优化了小批量操作，以确保最低的推理延迟。Inferentia 支持
    FP16、BF16 和 INT8 精度类型。`g4dn.xlarge` 替代方案的运行成本则低 70%。
- en: To run inference on an Inferentia instance, you need to compile the model using
    AWS Neuron SDK ([https://github.com/aws/aws-neuron-sdk/](https://github.com/aws/aws-neuron-sdk/)).
    Neuron SDK supports TensorFlow, PyTorch, and MXNet DL frameworks. We will discuss
    model compilation and optimization with Neuron SDK in the next section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Inferentia 实例上运行推理，你需要使用 AWS Neuron SDK 编译模型（[https://github.com/aws/aws-neuron-sdk/](https://github.com/aws/aws-neuron-sdk/)）。Neuron
    SDK 支持 TensorFlow、PyTorch 和 MXNet 深度学习框架。我们将在下一节讨论使用 Neuron SDK 进行模型编译和优化。
- en: AWS Inferentia offers a performant and cost-efficient inference accelerator.
    Additionally, you can further optimize your models using Neuron SDK. Note that
    you need to consider whether the given model architecture and its operators are
    supported by Neuron SDK. Unsupported operators will be executed on the CPU device,
    which will result in additional latency. This may or may not be acceptable based
    on your target SLAs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Inferentia 提供了一种高性能且具有成本效益的推理加速器。此外，你还可以使用 Neuron SDK 进一步优化模型。请注意，你需要考虑给定模型架构及其操作符是否被
    Neuron SDK 支持。对于不受支持的操作符，它们将由 CPU 设备执行，这将导致额外的延迟。根据目标 SLA，可能可以接受，也可能无法接受。
- en: Amazon Elastic Inference
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊弹性推理
- en: '**Elastic Inference** (**EI**) is a capability that allows you to attach user-defined
    accelerator capacity to regular CPU instances. EI was designed specifically for
    inference use cases. Accelerator capacity is available via an attached network
    interface. EI supports TensorFlow, MXNet, and PyTorch frameworks and the ONNX
    model format. To be able to use EI, you need to load your models in a special
    EI-enabled version of DL frameworks. The modified versions of DL frameworks automatically
    detect the presence of EI accelerators and execute operators over the network
    interface. The following diagram illustrates this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性推理** (**EI**) 是一种功能，允许你将用户定义的加速器能力附加到常规的 CPU 实例上。EI 专为推理用例设计。加速器能力通过附加的网络接口提供。EI
    支持 TensorFlow、MXNet 和 PyTorch 框架以及 ONNX 模型格式。要使用 EI，你需要将模型加载到专门为 EI 启用的深度学习框架版本中。这些修改后的深度学习框架会自动检测到
    EI 加速器的存在，并通过网络接口执行操作。下图展示了这一点：'
- en: '![Figure 8.1 – Accessing the EI GPU capacity via a network interface ](img/B17519_08_001.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 通过网络接口访问 EI GPU 能力 ](img/B17519_08_001.jpg)'
- en: Figure 8.1 – Accessing the EI GPU capacity via a network interface
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 通过网络接口访问 EI GPU 能力
- en: 'EI has several accelerator types available. You can select one based on the
    amount of required accelerator memory or anticipated throughput (in TFLOPS). EI
    provides low cost and high flexibility when it comes to instance configuration.
    Unlike dedicated GPU instances with restricted configurations, you can mix and
    match CPU instances and EI to achieve acceptable SLAs for inference latency and
    throughput while keeping the overall cost low:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: EI 提供多种加速器类型。你可以根据所需的加速器内存或预期的吞吐量（以 TFLOPS 为单位）选择合适的类型。EI 在实例配置方面提供了低成本和高灵活性。与配置受限的专用
    GPU 实例不同，你可以将 CPU 实例和 EI 混合使用，以实现可接受的推理延迟和吞吐量，同时保持整体成本较低：
- en: '| Accelerator Type | FP32 Throughput (TFLOPS) | FP16 Throughput (TFLOPS) |
    Memory (GB) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 加速器类型 | FP32 吞吐量（TFLOPS） | FP16 吞吐量（TFLOPS） | 内存（GB） |'
- en: '| eia2.medium | 1 | 8 | 2 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| eia2.medium | 1 | 8 | 2 |'
- en: '| eia2.large | 2 | 16 | 4 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| eia2.large | 2 | 16 | 4 |'
- en: '| eia2.xlarge | 4 | 32 | 8 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| eia2.xlarge | 4 | 32 | 8 |'
- en: Figure 8.2 – EI performance characteristics
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – EI 性能特性
- en: 'When selecting EI, you need to keep several caveats in mind:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 EI 时，你需要牢记几个注意事项：
- en: By design, EI accelerators always introduce additional latency due to network
    transfer. EI accelerators may underperform on models with complex control flows.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于设计原因，EI 加速器通常会由于网络传输引入额外的延迟。对于具有复杂控制流的模型，EI 加速器可能表现不佳。
- en: The EI-enabled DL frameworks are lagging considerably behind the latest open
    source versions. Also, you may experience compatibility issues trying to run the
    latest model architectures on EI.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EI 启用的深度学习框架远远落后于最新的开源版本。此外，在 EI 上运行最新的模型架构时，你可能会遇到兼容性问题。
- en: EI provides relatively low GPU memory (compared to the latest generations of
    GPU instances), which may restrict the types of models you can run on it.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EI 提供的 GPU 内存相对较低（与最新一代 GPU 实例相比），这可能限制你在其上运行的模型类型。
- en: Like GPU instances and Inferentia, EI supports model compilation and optimization.
    You can use a SageMaker Neo optimizer job for TensorFlow models, which uses the
    **TF-TRT** library for TensorRT. Optimized models typically have better latency-throughput
    characteristics but may take up large GPU memory at inference time. This may lead
    to potential **out-of-memory** (**OOM**) issues.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GPU 实例和 Inferentia 一样，EI 支持模型编译和优化。你可以使用 SageMaker Neo 优化器任务来优化 TensorFlow
    模型，该任务使用 **TF-TRT** 库进行 TensorRT 优化。优化后的模型通常具有更好的延迟-吞吐量特性，但在推理时可能会占用大量 GPU 内存，这可能会导致
    **内存溢出** (**OOM**) 问题。
- en: EI can be a useful option in your toolbox when selecting a DL accelerator, especially
    when you are looking for a highly flexible and cost-efficient solution and are
    running more compact and less demanding model architectures. However, if you are
    looking for high-performant inference for demanding models, you should consider
    Inferentia and G4 instances first.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择深度学习加速器时，EI 是一个有用的选项，特别是当你在寻找一个高度灵活且具有成本效益的解决方案，并且运行较为紧凑且需求较低的模型架构时。然而，如果你正在寻找高性能的推理处理，尤其是对高要求模型的推理，应该优先考虑
    Inferentia 和 G4 实例。
- en: Compiling models for inference
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为推理编译模型
- en: To achieve optimal inference performance on the given accelerator hardware,
    you usually need to compile your model for this accelerator. The compilation process
    includes various computational optimizations, such as layer and tensor fusion,
    precision calibration, and discarding unused parameters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在给定的加速器硬件上实现最佳推理性能，你通常需要为该加速器编译模型。编译过程包括各种计算优化，例如层和张量融合、精度校准，以及丢弃未使用的参数。
- en: 'In this section, we will review the optimizers that perform compilation for
    previously discussed inference accelerators: NVIDIA TensorRT for NVIDIA GPU accelerators
    and Neuron SDK compiler for AWS Inferentia. After that, we will review a managed
    compilation service called SageMaker Neo, which supports multiple cloud and edge
    hardware accelerators.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾为之前讨论的推理加速器执行编译的优化器：NVIDIA TensorRT（用于NVIDIA GPU加速器）和Neuron SDK编译器（用于AWS
    Inferentia）。之后，我们将回顾一种名为SageMaker Neo的托管编译服务，它支持多种云和边缘硬件加速器。
- en: We will start by looking at the TensorRT compiler for NVIDIA GPU accelerators.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看用于 NVIDIA GPU 加速器的 TensorRT 编译器开始。
- en: Using TensorRT
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorRT
- en: 'NVIDIA TensorRT is a compiler and inference runtime built for the CUDA ecosystem.
    According to NVIDIA benchmarks, it can improve model performance up to six times
    compared to an uncompiled model version on the same hardware accelerator. TensorRT
    supports TensorFlow and PyTorch frameworks, as well as the cross-framework ONNX
    model format. TensorRT is integrated with the NVIDIA Triton model server to manage
    model deployment and serve inference requests. TensorRT provides both C++ and
    Python runtime environments. The C++ runtime can be especially useful for inference
    at the edge and embedded devices that may not have a Python runtime configured:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA TensorRT 是为 CUDA 生态系统构建的编译器和推理运行时。根据NVIDIA的基准测试，相比于未编译的模型版本，它能够在相同的硬件加速器上将模型性能提高最多六倍。TensorRT
    支持 TensorFlow 和 PyTorch 框架，以及跨框架的 ONNX 模型格式。TensorRT 集成了 NVIDIA Triton 模型服务器，用于管理模型部署并提供推理请求服务。TensorRT
    提供了 C++ 和 Python 运行时环境。C++ 运行时在边缘设备和嵌入式设备上尤其有用，这些设备可能没有配置 Python 运行时：
- en: '![Figure 8.3 – Accessing EI GPU capacity via a network interface ](img/B17519_08_003.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 通过网络接口访问 EI GPU 容量](img/B17519_08_003.jpg)'
- en: Figure 8.3 – Accessing EI GPU capacity via a network interface
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 通过网络接口访问 EI GPU 容量
- en: 'TensorRT provides several key optimization mechanisms when compiling models
    (refer to *Figure 8.3*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT 在编译模型时提供了几种关键的优化机制（参见*图 8.3*）：
- en: '**Precision Calibration** converts weights and activations into INT8 precision
    type without impacting accuracy to maximize model throughput'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度校准**将权重和激活值转换为INT8精度类型，而不会影响准确性，从而最大化模型吞吐量。'
- en: '**Layer and Tensor Fusion** combines multiple layers and tensor operators into
    a single computation to optimize memory utilization and latency'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层和张量融合**将多个层和张量运算合并为单一计算，以优化内存利用率和延迟。'
- en: '**Kernel Auto-Tuning** selects optimal data layers and algorithms for the given
    hardware accelerator'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核自动调优**为给定的硬件加速器选择最佳的数据层和算法。'
- en: '**Dynamic Tensor Memory** allows you to efficiently reuse the memory that’s
    been allocated for tensors'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态张量内存**允许你高效地重用为张量分配的内存。'
- en: '**Multi-Stream Execution** allows you to process multiple inputs in parallel'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多流执行**允许你并行处理多个输入。'
- en: 'Most of these optimizations happen automatically without user input. At compile
    time, you need to set the following parameters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化大多数都会在没有用户输入的情况下自动发生。在编译时，你需要设置以下参数：
- en: '**Precision mode** defines the precision type that the model parameters will
    be converted into. TensorRT allows you to reduce precision without or with minimal
    impact on accuracy. A lower precision allows you to reduce the memory footprint,
    which, in turn, speeds up memory-bound operations.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度模式**定义了模型参数将转换成的精度类型。TensorRT 允许你在几乎不影响准确性的情况下降低精度。较低的精度可以减少内存占用，从而加速内存绑定的操作。'
- en: '**Input batch size** sets how many sample inputs are expected in a single inference
    request. Increasing the batch size usually increases the overall system throughput.
    However, a larger batch size requires more available memory and may also increase
    inference request latency.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入批量大小**设置单次推理请求中预期的样本输入数量。增大批量大小通常会提高整体系统吞吐量。然而，较大的批量大小需要更多的可用内存，并且可能增加推理请求的延迟。'
- en: '**Max memory size** defines how much GPU memory is available for the model
    at inference time.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大内存大小**定义了在推理时可用于模型的GPU内存量。'
- en: It’s recommended to experiment with various combinations of these parameters
    to achieve optimal performance, given your available resources and latency-throughput
    SLAs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 建议根据可用资源和延迟吞吐量服务水平协议（SLA），尝试各种这些参数的组合，以获得最佳性能。
- en: Depending on the DL framework model, the compilation path to the TensorRT format
    is different. For TensorFlow, you can use the **TensorFlow-TensorRT** (**TRT**)
    integration library ([https://github.com/tensorflow/tensorrt](https://github.com/tensorflow/tensorrt)).
    For PyTorch, you need to convert the model into TorchScript format using the PyTorch
    JIT compiler. Then, you can use Torch-TensorRT integration ([https://github.com/pytorch/TensorRT](https://github.com/pytorch/TensorRT))
    to compile the model into TensorRT format. Then, the compiled model can be served
    using the model server of your choice. In [*Chapter 9*](B17519_09.xhtml#_idTextAnchor137),
    *Implementing Model Servers*, we will develop an inference application for the
    TensorRT compiled model using the NVIDIA Triton model server.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 DL 框架模型，编译到 TensorRT 格式的路径不同。对于 TensorFlow，你可以使用 **TensorFlow-TensorRT**（**TRT**）集成库（[https://github.com/tensorflow/tensorrt](https://github.com/tensorflow/tensorrt)）。对于
    PyTorch，你需要使用 PyTorch JIT 编译器将模型转换为 TorchScript 格式。然后，你可以使用 Torch-TensorRT 集成库（[https://github.com/pytorch/TensorRT](https://github.com/pytorch/TensorRT)）将模型编译为
    TensorRT 格式。然后，编译后的模型可以使用你选择的模型服务器进行服务。在 [*第9章*](B17519_09.xhtml#_idTextAnchor137)，*实现模型服务器*中，我们将使用
    NVIDIA Triton 模型服务器开发用于 TensorRT 编译模型的推理应用程序。
- en: Let’s review an example of how to compile the PyTorch ResNet50 model using TensorRT
    and then benchmark it against an uncompiled model. To compile the model using
    TensorRT, you need to have access to the environment that contains the target
    NVIDIA GPU. In the case of Amazon SageMaker, you can use a SageMaker notebook
    instance with the NVIDIA GPU accelerator. It’s recommended to use the official
    NVIDIA PyTorch container that comes with all the dependencies preconfigured.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何使用 TensorRT 编译 PyTorch ResNet50 模型的示例，并将其与未编译的模型进行基准测试。要使用 TensorRT
    编译模型，你需要访问包含目标 NVIDIA GPU 的环境。以 Amazon SageMaker 为例，你可以使用具有 NVIDIA GPU 加速器的 SageMaker
    笔记本实例。建议使用官方的 NVIDIA PyTorch 容器，其中预配置了所有依赖项。
- en: Important note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that Amazon SageMaker Studio notebooks don't allow you to run Docker containers.
    Hence, in this example, we will use a SageMaker notebook instance. Choose a notebook
    instance with the same GPU accelerator as the intended inference cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Amazon SageMaker Studio 笔记本不允许运行 Docker 容器。因此，在本示例中，我们将使用 SageMaker 笔记本实例。选择一个具有与目标推理集群相同
    GPU 加速器的笔记本实例。
- en: 'Follow the next steps to compile PyTorch model for TensorRT runtime:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤为 TensorRT 运行时编译 PyTorch 模型：
- en: Start a SageMaker notebook instance with the NVIDIA GPU accelerator. For example,
    you can use the `ml.p3.2xlarge` notebook instance.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个带有 NVIDIA GPU 加速器的 SageMaker 笔记本实例。例如，你可以使用 `ml.p3.2xlarge` 笔记本实例。
- en: Once your notebook has been fully provisioned, open the JupyterLab service via
    the respective link in the AWS Console.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你的笔记本完全配置好，请通过 AWS 控制台中的相应链接打开 JupyterLab 服务。
- en: 'In your JupyterLab environment, open a **Terminal** session and run the following
    commands to copy the source code for model compilation:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 JupyterLab 环境中，打开一个**终端**会话并运行以下命令以复制模型编译的源代码：
- en: '[PRE0]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the same Terminal session, run the following commands to download the NVIDIA
    PyTorch container with TensorRT configured:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一终端会话中，运行以下命令以下载配置了 TensorRT 的 NVIDIA PyTorch 容器：
- en: '[PRE1]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A new Terminal session will open in the PyTorch container. Run the following
    commands to download the test images and start benchmarking:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个新的终端会话将在 PyTorch 容器中打开。运行以下命令以下载测试图像并开始基准测试：
- en: '[PRE2]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The benchmarking script should take several minutes to complete. You will be
    able to get inference results for the uncompiled ResNet50 model and the compiled
    model with FP32 precision and with FP16 precision. As you can see from the following
    summary, there is a latency improvement of more than five times in the FP16 model
    compared to the uncompiled model with the same accuracy:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试脚本需要几分钟才能完成。你将能够获得未编译的 ResNet50 模型与编译后的模型（分别使用 FP32 精度和 FP16 精度）的推理结果。正如以下总结所示，与相同精度的未编译模型相比，FP16
    模型的延迟提高了五倍以上：
- en: '*Uncompiled ResNet50 model*: Average batch time: 102.17 ms'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*未编译的 ResNet50 模型*：平均批处理时间：102.17 毫秒'
- en: '*Compiled ResNet50 model with FP32 precision*: Average batch time: 70.79 ms'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编译后的 ResNet50 模型（使用 FP32 精度）*：平均批处理时间：70.79 毫秒'
- en: '*ResNet50 model with FP16 precision*: Average batch time: 17.26 ms'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ResNet50模型（FP16精度）*：平均批处理时间：17.26毫秒'
- en: 'Let’s review the compilation and inference part of the benchmarking script
    to familiarize ourselves with the PyTorch TensorRT API:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下基准测试脚本中的编译和推理部分，熟悉PyTorch TensorRT API：
- en: 'First, we will load the regular, uncompiled ResNet50 model from PyTorch Hub:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将从PyTorch Hub加载常规的、未编译的ResNet50模型：
- en: '[PRE3]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To compile the model, we can use the `torch_tensorrt` integration API. In the
    following example, we are compiling the model into a TorchScript module, optimized
    for the TensorRT engine:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要编译模型，我们可以使用`torch_tensorrt`集成API。在以下示例中，我们将模型编译成一个TorchScript模块，以便针对TensorRT引擎进行优化：
- en: '[PRE4]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, you can save and load the compiled model as a regular TorchScript program:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以像普通TorchScript程序一样保存并加载编译后的模型：
- en: '[PRE5]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this section, you learned how to manually compile a PyTorch model for NVIDIA
    GPU accelerators using TensorRT and reviewed the latency improvements of the compiled
    model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您了解了如何使用TensorRT手动编译PyTorch模型以适配NVIDIA GPU加速器，并且回顾了编译后模型的延迟改进。
- en: If you are interested in compiling TensorFlow models, you can use a similar
    approach. Note that you would need to use the official NVIDIA TensorFlow container
    instead. For a code example for this, you can refer to the official TensorFlow
    tutorial at [https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.xhtml](https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.xhtml).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣编译TensorFlow模型，您可以使用类似的方法。请注意，您需要使用官方的NVIDIA TensorFlow容器。关于此的代码示例，您可以参考官方的TensorFlow教程：[https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.xhtml](https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.xhtml)。
- en: As you can see, the overall compilation process is manual. Later in this chapter,
    we will review SageMaker Neo, which allows us to compile TensorFlow and PyTorch
    models for NVIDIA GPU accelerators with minimal manual effort.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，整个编译过程是手动的。本章后面我们将介绍SageMaker Neo，它可以让我们以最少的手动操作编译TensorFlow和PyTorch模型以适配NVIDIA
    GPU加速器。
- en: Using Neuron SDK
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Neuron SDK
- en: AWS Neuron SDK allows you to compile your DL models for AWS Inferentia instances.
    It provides several parameters to help you optimize your inference program based
    on the available Inferentia chips and your latency and throughput SLAs. Neuron
    SDK supports TensorFlow, PyTorch, and MXNet frameworks. Neuron SDK is an ahead-of-time
    compiler, so you must explicitly provide the batch size at compilation time. It
    also includes a runtime environment in which we load the model and get predictions
    at inference time. Note that the compiled model by Neuron SDK can only be used
    on AWS Inferentia chips.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Neuron SDK允许您将DL模型编译为AWS Inferentia实例。它提供了多个参数，帮助您根据可用的Inferentia芯片以及您的延迟和吞吐量SLA来优化推理程序。Neuron
    SDK支持TensorFlow、PyTorch和MXNet框架。Neuron SDK是一个提前编译的工具，因此您必须在编译时显式提供批量大小。它还包括一个运行时环境，我们在其中加载模型并在推理时获取预测。请注意，Neuron
    SDK编译的模型只能在AWS Inferentia芯片上使用。
- en: 'Neuron SDK supports a wide but finite set of operators. AWS tested Neuron SDK
    on the following popular model architectures:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron SDK支持一组广泛但有限的操作符。AWS在以下流行的模型架构上测试了Neuron SDK：
- en: '*NLP models from the HuggingFace Transformer library*: **BERT**, **distilBERT**,
    **XLM-BERT**, **Robert**, **BioBERT**, **MarianMT**, **Pegasus**, **and Bart**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*来自HuggingFace Transformer库的NLP模型*：**BERT**，**distilBERT**，**XLM-BERT**，**Robert**，**BioBERT**，**MarianMT**，**Pegasus**，**和Bart**'
- en: '*Computer vision models*: **Resnet**, **Renext**, **VGG**, **Yolo v3/v4/v5**,
    **SSD**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算机视觉模型*：**Resnet**，**Renext**，**VGG**，**Yolo v3/v4/v5**，**SSD**'
- en: Neuron SDK also supports generic model layers such as a fully connected layer
    or embeddings lookup. If your model architecture uses supported operators, you
    will be able to fully utilize Neuron SDK optimizations. You can refer to the list
    of supported operators for specific DL frameworks in the official Neuron SDK documentation
    at [https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.xhtml](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.xhtml).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron SDK还支持通用模型层，如全连接层或嵌入查找。如果您的模型架构使用了支持的操作符，您将能够充分利用Neuron SDK的优化。您可以参考Neuron
    SDK官方文档中的支持操作符列表，查看具体的DL框架：[https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.xhtml](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.xhtml)。
- en: 'When compiling your model using Neuron SDK, keep the following caveats in mind:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Neuron SDK编译模型时，请牢记以下几点注意事项：
- en: If a specific operator is not supported, its execution will be performed on
    the CPU accelerator. This will lead to slower performance.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某个特定操作符不被支持，则该操作的执行将转移到CPU加速器上，这会导致性能变慢。
- en: The control flows in your model may not be fully supported.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您模型中的控制流可能无法完全得到支持。
- en: If you expect variable batch size, you will need to implement **dynamic batching**.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您期望变量批量大小，您需要实现**动态批处理**。
- en: If you expect a variable input size (for example, the variable size of input
    images), you should consider implementing padding or bucketing.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您期望输入大小可变（例如，输入图像的大小不固定），您应该考虑实现填充或分桶。
- en: Now, let’s discuss the available Neuron SDK optimizations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论可用的Neuron SDK优化。
- en: FP32 Autocasting
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FP32 自动类型转换
- en: Whenever possible, Neuron SDK converts your model into the BF16 precision type
    to reduce the memory footprint and improve latency-throughput characteristics.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每当可能时，Neuron SDK将您的模型转换为BF16精度类型，以减少内存占用并改善延迟-吞吐特性。
- en: Batching inference inputs
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量推理输入
- en: 'Batching refers to combining multiple inference inputs into a single batch.
    In this regard, it’s the same as batching during model training. In the case of
    an inference workload, batching influences your throughput. Like TensorRT, Neuron
    SDK requires you to define the target batch size at compilation time. The Inferentia
    accelerator is specifically optimized for running inference on smaller batch sizes.
    This is achieved through combining latency-sensitive operations (such as reading
    weights from memory) for the whole inference batch, thus achieving better latency-throughput
    characteristics than performing the same operations on each inference input. The
    following diagram illustrates this concept:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理是指将多个推理输入合并为一个批次。在这方面，它与模型训练中的批处理相同。对于推理工作负载，批处理会影响您的吞吐量。像TensorRT一样，Neuron
    SDK要求您在编译时定义目标批量大小。Inferentia加速器特别优化了对较小批量大小的推理运行。这是通过将延迟敏感的操作（如从内存中读取权重）组合到整个推理批次中，从而比对每个推理输入执行相同操作时获得更好的延迟-吞吐特性。下图说明了这一概念：
- en: '![Figure 8.4 – Batched inference with single memory retrieval ](img/B17519_08_004.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 使用单一内存检索的批量推理 ](img/B17519_08_004.jpg)'
- en: Figure 8.4 – Batched inference with single memory retrieval
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 使用单一内存检索的批量推理
- en: Dynamic batching is a feature of Neuron SDK that allows you to slice the input
    tensors so that they match the batch size that’s used at compilation time. Please
    note that dynamic batching is available for several eligible model architectures.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 动态批处理是Neuron SDK的一项功能，它允许您切分输入张量，使其匹配编译时使用的批量大小。请注意，动态批处理适用于若干合适的模型架构。
- en: NeuronCore pipelining
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeuronCore流水线
- en: 'Each Inferentia accelerator consists of four **NeuronCores**. Pipelining allows
    you to shard a model across multiple NeuronCores, caching the model parameters
    in on-chip memory. This allows you to process network operators with locally cached
    data faster and avoid accessing external memory. According to AWS, internal benchmark
    pipelines usually allow us to achieve the highest hardware utilization without
    batching. The following diagram shows an example of pipelining:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Inferentia加速器由四个**NeuronCores**组成。流水线技术允许您将模型分片到多个NeuronCore上，将模型参数缓存到片上内存中。这使得您可以使用本地缓存的数据更快地处理网络运算符，并避免访问外部内存。根据AWS的说法，内部基准流水线通常使我们在没有批处理的情况下实现最高的硬件利用率。下图展示了流水线的示例：
- en: '![Figure 8.5 – Pipelining model across three NeuronCores ](img/B17519_08_005.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 将模型流水线化到三个NeuronCores中 ](img/B17519_08_005.jpg)'
- en: Figure 8.5 – Pipelining model across three NeuronCores
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 将模型流水线化到三个NeuronCores中
- en: 'In the following example, we will compile and benchmark the ResNet50 model
    on an AWS Inferentia instance. At the time of writing, Amazon SageMaker doesn’t
    support managed notebook instances. Hence, we used the Amazon EC2 `inf1.xlarge`
    instance with a `8888`. For this, you will need to set up your instance security
    group like so:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将在AWS Inferentia实例上编译并基准测试ResNet50模型。撰写本文时，Amazon SageMaker不支持托管的笔记本实例。因此，我们使用了Amazon
    EC2 `inf1.xlarge`实例，配置为`8888`。为此，您需要像这样设置实例的安全组：
- en: '![Figure 8.6 – Security group configuration to allow Jupyter traffic ](img/B17519_08_006.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 配置安全组以允许Jupyter流量 ](img/B17519_08_006.jpg)'
- en: Figure 8.6 – Security group configuration to allow Jupyter traffic
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 配置安全组以允许Jupyter流量
- en: 'Before we can start compiling Neuron SDK, we need to install Neuron SDK and
    its dependencies on the EC2 instance. Follow these steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编译 Neuron SDK 之前，我们需要在 EC2 实例上安装 Neuron SDK 及其依赖项。按照以下步骤进行操作：
- en: 'First, you will need to SSH to your instance using the following commands:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要使用以下命令 SSH 连接到你的实例：
- en: '[PRE6]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once you have logged into the EC2 instance, please follow the instructions at
    [https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-intro/pytorch-setup/pytorch-install.xhtml](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-intro/pytorch-setup/pytorch-install.xhtml)
    to install Neuron PyTorch on your Ubuntu OS. Note that installation may take around
    5 minutes to complete.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到 EC2 实例后，请按照 [https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-intro/pytorch-setup/pytorch-install.xhtml](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-intro/pytorch-setup/pytorch-install.xhtml)
    上的说明，在你的 Ubuntu 操作系统上安装 Neuron PyTorch。请注意，安装过程可能需要大约 5 分钟完成。
- en: 'Once the installation has finished, clone the sources and start the Jupyter
    server application:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，克隆源代码并启动 Jupyter 服务器应用程序：
- en: '[PRE7]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After that, you can open `<your_instance_public_DNS>:8888/tree` to access the
    Jupyter notebook for this example. Note that the first time you do this, you will
    need to copy the security token that was returned by `jupyter notebook...` previously.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，你可以打开 `<your_instance_public_DNS>:8888/tree` 以访问此示例的 Jupyter notebook。请注意，第一次进行此操作时，你需要复制之前由
    `jupyter notebook...` 返回的安全令牌。
- en: 'Once the setup is done, we can compile and benchmark the models on the AWS
    Inferentia accelerator. The full code is available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/2_Neuron_SDK_compilation.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/2_Neuron_SDK_compilation.ipynb).
    Follow these steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，我们可以在 AWS Inferentia 加速器上编译并基准测试模型。完整代码可在此查看：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/2_Neuron_SDK_compilation.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/2_Neuron_SDK_compilation.ipynb)。按照以下步骤操作：
- en: In the opened Jupyter notebook, change the kernel to **Python (Neuron PyTorch)**,
    which we configured previously.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打开的 Jupyter notebook 中，将内核更改为我们之前配置的**Python (Neuron PyTorch)**。
- en: 'Next, we must import the required libraries, including `torch_neuron`, and
    download the ResNet50 model:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须导入所需的库，包括 `torch_neuron`，并下载 ResNet50 模型：
- en: '[PRE8]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we must analyze the model operators to identify if any model operators
    are not supported by Inferentia/Neuron SDK. Since the ResNet50 model is supported,
    the output of this command should confirm that all the model operators are supported:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须分析模型操作符，确定是否有任何操作符不被 Inferentia/Neuron SDK 支持。由于 ResNet50 模型已被支持，此命令的输出应确认所有模型操作符均受支持：
- en: '[PRE9]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we are ready to compile by running the following command. You will see
    the compilation statistics and status in the output:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已准备好通过运行以下命令来进行编译。你将看到编译统计信息和状态输出：
- en: '[PRE10]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since Neuron SDK compiles into a TorchScript program, saving and loading the
    model is similar to what you would do in regular PyTorch:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 Neuron SDK 编译成了一个 TorchScript 程序，保存和加载模型的方式类似于你在常规 PyTorch 中的操作：
- en: '[PRE11]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let’s benchmark the compiled model with batching or pipelining. For this,
    we will prepare preprocessing and benchmark methods to form an inference batch
    and **measure latency** (**ms**) and **throughput** (**samples/s**):'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过批处理或流水线对已编译的模型进行基准测试。为此，我们将准备预处理和基准测试方法，形成推理批处理，并**测量延迟**（**ms**）和**吞吐量**（**samples/s**）：
- en: '[PRE12]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The benchmark results should be similar to the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试结果应类似于以下内容：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will recompile the model with batching enabled (by setting `batch_size`
    to `5` samples per NeuronCore):'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将重新编译模型，并启用批处理功能（通过将 `batch_size` 设置为每个 NeuronCore `5` 个样本）：
- en: '[PRE14]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After rerunning the benchmark, please note that while latency is decreased,
    the overall throughput has increased, as shown here:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在重新运行基准测试后，请注意，尽管延迟有所减少，但整体吞吐量已增加，如下所示：
- en: '[PRE15]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The benchmark’s output should look as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试的输出应如下所示：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Lastly, let’s compile and benchmark the model with pipelining enabled. We will
    start by tracing the original model with the `neuroncore-pipeline-cores` parameter:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们在启用流水线的情况下编译并基准测试模型。我们将首先通过 `neuroncore-pipeline-cores` 参数追踪原始模型：
- en: '[PRE17]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we will rerun the benchmark on this new model:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将在这个新模型上重新运行基准测试：
- en: '[PRE18]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of this benchmarking will be as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此基准测试的输出将如下所示：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the resulting latency and throughput of the pipeline model are lower
    than the model without batching and with batching. One reason for this is that
    in our benchmark test, we run inference requests sequentially. To leverage the
    pipeline model better, we would need to create several parallel inference requests.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，管道模型的最终延迟和吞吐量低于无批处理和有批处理的模型。出现这种情况的一个原因是，在我们的基准测试中，推理请求是按顺序运行的。为了更好地利用管道模型，我们需要创建多个并行推理请求。
- en: Using SageMaker Neo
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Neo
- en: SageMaker Neo allows you to compile and optimize DL models for a wide range
    of target hardware platforms. It supports PyTorch, TensorFlow, MXNet, and ONNX
    models for hardware platforms such as Ambarella, ARM, Intel, NVIDIA. NXP, Qualcomm,
    Texas Instruments, and Xilinx. SageMaker Neo also supports deployment for cloud
    instances, as well as edge devices.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Neo 允许你编译和优化深度学习模型，以适配各种硬件平台。它支持 PyTorch、TensorFlow、MXNet 和 ONNX 模型，适用于
    Ambarella、ARM、Intel、NVIDIA、NXP、Qualcomm、Texas Instruments 和 Xilinx 等硬件平台。SageMaker
    Neo 还支持云实例部署以及边缘设备部署。
- en: 'Under the hood, SageMaker Neo converts your trained model from a framework-specific
    representation into an intermediate framework-agnostic representation. Then, it
    applies automatic optimizations and generates binary code for the optimized operations.
    Once the model has been compiled, you can deploy it to the target instance type
    it using the SageMaker Inference service. Neo also provides a runtime for each
    target platform that loads and executes the compiled model. An overview of SageMaker
    Neo is shown in the following diagram:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，SageMaker Neo 将训练好的模型从框架特定的表示转换为中间的框架无关表示。接着，它会应用自动优化，并生成优化操作的二进制代码。模型编译完成后，你可以通过
    SageMaker 推理服务将其部署到目标实例类型。Neo 还为每个目标平台提供运行时，加载并执行编译后的模型。下图展示了 SageMaker Neo 的概览：
- en: '![Figure 8.7 – SageMaker Neo overview ](img/B17519_08_007.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – SageMaker Neo 概览](img/B17519_08_007.jpg)'
- en: Figure 8.7 – SageMaker Neo overview
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – SageMaker Neo 概览
- en: 'SageMaker Neo enables users to significantly reduce any additional development
    or setup work. However, it also comes with certain limitations that may or may
    not be suitable for your specific use case:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Neo 能够显著减少额外的开发或设置工作。然而，它也有一些限制，这些限制可能适合或不适合你的具体使用案例。
- en: SageMaker Neo primarily supports computer vision models such as **Image Classification**,
    **Object Detection**, and **Semantic Segmentation**. It doesn’t support, for example,
    NLP model architectures.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Neo 主要支持计算机视觉模型，如**图像分类**、**目标检测**和**语义分割**。它不支持 NLP 模型架构等其他类型的模型。
- en: SageMaker Neo supports various DL frameworks but they are several major versions
    behind. So, if you are looking to use the latest model architecture and/or the
    latest framework version features, you will have to consider other compilation
    options (for instance, manually compiling using TensorRT). For the latest details
    on SageMaker Neo support, refer to [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.xhtml).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Neo 支持多个深度学习框架，但它们通常落后几个主要版本。因此，如果你希望使用最新的模型架构和/或框架版本特性，你需要考虑其他编译选项（例如，使用
    TensorRT 手动编译）。有关 SageMaker Neo 支持的最新详情，请参考 [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.xhtml)。
- en: SageMaker Neo supports a set of cloud instances. At the time of writing , it
    supports compilation for `ml.c5`, `ml.c4`, `ml.m5`, `ml.m4`, `ml.p3`, `ml.p2`,
    and `ml.inf1` instances.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Neo 支持一系列云实例。截至目前，它支持 `ml.c5`、`ml.c4`、`ml.m5`、`ml.m4`、`ml.p3`、`ml.p2`
    和 `ml.inf1` 实例的编译。
- en: SageMaker Neo sets specific requirements on the inference request format, (specifically,
    around the shape of the input).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Neo 对推理请求格式有特定要求（特别是在输入形状方面）。
- en: These are key limitations you need to keep in mind when considering using SageMaker
    Neo. In many cases, SageMaker Neo can be a convenient and efficient way to compile
    your DL models if your model architecture, framework version, and target hardware
    accelerators are supported.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑使用 SageMaker Neo 时，需牢记以下关键限制。在很多情况下，如果你的模型架构、框架版本和目标硬件加速器受到支持，SageMaker Neo
    可以是一个方便高效的编译方式。
- en: 'Let’s review how to compile a TensorFlow model using SageMaker Neo. In this
    example, we will train the ResNet50 model, compile it for several hardware platforms,
    and deploy inference endpoints of optimized models. We will highlight the key
    aspects. The full source code is available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/3_SageMaker_Neo_TF.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/3_SageMaker_Neo_TF.ipynb).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何使用SageMaker Neo编译TensorFlow模型。在这个例子中，我们将训练ResNet50模型，针对多个硬件平台进行编译，并部署优化后的模型推理端点。我们将重点介绍关键部分。完整的源代码请参见这里：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/3_SageMaker_Neo_TF.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter8/3_SageMaker_Neo_TF.ipynb)。
- en: Developing a training and inference script
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发训练和推理脚本
- en: 'In the previous chapters, we mentioned that SageMaker must implement specific
    methods to be able to train models and run inference on SageMaker. Depending on
    the DL framework and target hardware platform, the required API will be slightly
    different. Refer to the official documentation for details: [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-prerequisites.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-prerequisites.xhtml).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们提到过，SageMaker必须实现特定的方法才能在SageMaker上训练模型和运行推理。根据深度学习框架和目标硬件平台的不同，所需的API会略有不同。详细信息请参见官方文档：[https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-prerequisites.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-prerequisites.xhtml)。
- en: 'To serve TensorFlow models, we implemented the simple `serving_input_fn()`
    method, which passes inputs to the model and returns predictions:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了服务TensorFlow模型，我们实现了简单的`serving_input_fn()`方法，该方法将输入传递给模型并返回预测结果：
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we schedule our compilation job.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们安排编译作业。
- en: Running compilation jobs and deployment
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行编译作业和部署
- en: 'To start compilation jobs, we must train our model using the SageMaker Python
    SDK from `sagemaker.tensorflow` and then import TensorFlow:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始编译作业，我们必须使用SageMaker Python SDK中的`sagemaker.tensorflow`来训练模型，然后导入TensorFlow：
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once the model has been trained, we can test how compilation works for two
    different hardware accelerators: a `p2` instance with NVIDIA GPU devices and a
    `c5` instance without any specialized hardware. Follow these steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以测试在两种不同硬件加速器上编译的效果：一个是配备NVIDIA GPU设备的`p2`实例，另一个是没有任何专用硬件的`c5`实例。请按照以下步骤操作：
- en: 'For this, first, we must compile the model for the NVIDIA GPU and deploy the
    endpoint using the same hardware type. Note the `input_shape` parameter, which
    tells SageMaker what input share to use during the compilation process. You will
    need to convert your inference sample into the same input share at inference time:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，首先，我们必须为NVIDIA GPU编译模型，并使用相同的硬件类型部署端点。请注意`input_shape`参数，它告诉SageMaker在编译过程中使用何种输入形状。你需要在推理时将推理样本转换为相同的输入形状：
- en: '[PRE22]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To access the logs of your compilation job, you can navigate to **SageMaker**
    | **Inference** | **Compilation jobs** in the AWS Console. In these logs, you
    can find, for instance, what compilation framework SageMaker Neo is using under
    the hood (Apache TVM) and see the compilation status of your model operators.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 若要访问编译作业的日志，你可以在AWS控制台中导航到**SageMaker** | **推理** | **编译作业**。在这些日志中，你可以找到例如SageMaker
    Neo使用的编译框架（Apache TVM），并查看模型操作符的编译状态。
- en: 'Running compilation jobs for the c5 instance is very similar. Note that we
    are using the same `estimator` object that we did to compile the p2 instance.
    As mentioned previously, you only need to train the model once; then, you can
    compile it for as many target platforms as you need:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于c5实例，运行编译作业非常相似。请注意，我们使用的是与编译p2实例时相同的`estimator`对象。如前所述，你只需要训练一次模型；然后，你可以为多个目标平台进行编译：
- en: '[PRE23]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As a result of a successful compilation, the resulting model artifact will be
    persisted in the S3 location available upon calling the `c5_estimator.model_data`
    attribute.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功编译后，生成的模型工件将保存在调用`c5_estimator.model_data`属性时可访问的S3位置。
- en: 'Calling the endpoint with compiled models is the same as calling an uncompiled
    model. Here is an example of the `p2` inference endpoint:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用已编译模型的端点与调用未编译模型的端点相同。以下是`p2`推理端点的示例：
- en: '[PRE24]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that we reshaped our input data so that it matches the input data that
    was used during the compilation process.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们重新调整了输入数据，以便与编译过程中使用的输入数据匹配。
- en: This brief example demonstrates how a single model can be compiled using SageMaker
    Neo. It’s recommended to benchmark SageMaker compiled models before deploying
    them to production against uncompiled models to confirm latency-throughput improvements.
    Please note that uncompiled and compiled models may have different memory requirements.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简短的例子演示了如何使用SageMaker Neo编译单个模型。建议在将模型部署到生产环境之前，对SageMaker编译的模型进行基准测试，以确认延迟吞吐量的改进。请注意，未编译和已编译的模型可能具有不同的内存需求。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we reviewed the available hardware accelerators that are suitable
    for running DL inference programs. We also discussed how your models can be optimized
    for target hardware accelerators using the TensorRT compiler for NVIDIA GPU accelerators
    and Neuron SDK for AWS Inferentia accelerators. Then, we reviewed the SageMaker
    Neo service, which allows you to compile supported models for a wide range of
    hardware platforms with minimal development efforts and highlighted several limitations
    of this service. After reading this chapter, you should be able to make decisions
    about which hardware accelerators to use and how to optimize them based on your
    specific use case requirements around latency, throughput, and cost.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了适用于运行DL推断程序的可用硬件加速器。我们还讨论了如何使用TensorRT编译器优化您的模型，以适配NVIDIA GPU加速器和Neuron
    SDK适配AWS Inferentia加速器。然后，我们回顾了SageMaker Neo服务，该服务允许您为广泛的硬件平台编译支持的模型，减少开发工作，并突出了该服务的几个限制。阅读完本章后，您应该能够根据您特定的使用案例需求（如延迟、吞吐量和成本）来做出关于使用哪种硬件加速器以及如何优化它们的决策。
- en: Once you have selected your hardware accelerator and model optimization strategy,
    you will need to decide which model server to use and how to further tune your
    inference workload at serving time. In the next chapter, we will discuss popular
    model server solutions and gain practical experience in developing and deploying
    them on the **SageMaker Inference** service.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了您的硬件加速器和模型优化策略，您将需要决定使用哪种模型服务器以及如何在服务时间进一步调整您的推理工作负载。在下一章中，我们将讨论流行的模型服务器解决方案，并获得在SageMaker推理服务上开发和部署它们的实际经验。
