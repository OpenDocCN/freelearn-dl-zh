- en: Understanding Recurrent Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解循环网络
- en: In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts
    of Neural Networks*, and [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, we took an in-depth look at the properties
    of general feedforward networks and their specialized incarnation, **Convolutional
    Neural Networks** (**CNNs**). In this chapter, we'll close this story arc with **Recurrent
    Neural Networks** (**RNNs**). The NN architectures we discussed in the previous
    chapters take in a fixed-sized input and provide a fixed-sized output. RNNs lift
    this constraint with their ability to process input sequences of a variable length
    by defining a recurrent relationship over these sequences (hence the name). If
    you are familiar with some of the topics that will be discussed in this chapter,
    you can skip them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)《神经网络的基础》和[第二章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)《理解卷积网络》中，我们深入探讨了普通前馈网络的特性以及其专业化形式——**卷积神经网络**（**CNNs**）。在本章中，我们将通过**循环神经网络**（**RNNs**）结束这一故事线。我们在前几章讨论的神经网络架构采用固定大小的输入并提供固定大小的输出，而RNN通过定义这些序列上的递归关系，突破了这一限制，从而能够处理可变长度的输入序列（因此得名）。如果你已经熟悉本章将讨论的某些主题，可以跳过它们。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Introduction to RNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN简介
- en: Introducing long short-term memory
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入长短期记忆（LSTM）
- en: Introducing gated recurrent units
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入门控循环单元
- en: Implementing text classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现文本分类
- en: Introduction to RNNs
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN简介
- en: RNNs are neural networks that can process sequential data with a variable length.
    Examples of such data include the words of a sentence or the price of stock at
    various moments in time. By using the word sequential, we imply that the elements
    of the sequence are related to each other and that their order matters. For example,
    if we take a book and randomly shuffle all of the words in it, the text will lose
    its meaning, even though we'll still know the individual words. Naturally, we
    can use RNNs to solve tasks that relate to sequential data. Examples of such tasks
    are language translation, speech recognition, predicting the next element of a
    time series, and so on.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是可以处理具有可变长度序列数据的神经网络。这类数据的例子包括一个句子的单词或股市在不同时间点的价格。通过使用“序列”一词，我们意味着序列中的元素是彼此相关的，并且它们的顺序是重要的。例如，如果我们把一本书的所有单词随机打乱，文本就会失去其含义，尽管我们仍然能知道各个单词的意思。自然地，我们可以使用RNN来解决与序列数据相关的任务。这类任务的例子包括语言翻译、语音识别、预测时间序列的下一个元素等。
- en: 'RNNs get their name because they apply the same function over a sequence recurrently.
    We can define an RNN as a recurrence relation:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RNN得名于其对序列应用相同的函数进行递归运算。我们可以将RNN定义为一个递归关系：
- en: '![](img/aaf28f6c-9839-430b-b02a-7b0580618ccf.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaf28f6c-9839-430b-b02a-7b0580618ccf.png)'
- en: 'Here, *f* is a differentiable function, **s***[t]* is a vector of values called
    the internal network state (at step *t*), and **x***[t]* is the network input
    at step *t*. Unlike regular networks, where the state only depends on the current
    input (and network weights), here, **s***[t]* is a function of both the current
    input as well as the previous state, **s***[t-1]*. You can think of **s***[t-1]* as
    the network''s summary of all of the previous inputs. This is unlike the regular
    feedforward networks (including CNNs), which take only the current input sample
    as input. The recurrence relationship defines how the state evolves step by step
    over the sequence via a feedback loop over previous states, as illustrated in
    the following diagram:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， *f* 是一个可微分函数，**s**[t] 是一个值的向量，称为内部网络状态（在第*t*步），**x**[t] 是第*t*步的网络输入。与普通网络不同，普通网络中的状态仅依赖于当前输入（和网络权重），而在这里，**s**[t] 是当前输入和前一个状态**s**[t-1]的函数。你可以将**s**[t-1]看作是网络对所有先前输入的总结。这与常规的前馈网络（包括CNN）不同，后者只将当前输入样本作为输入。递归关系定义了状态如何通过对先前状态的反馈循环，逐步在序列中演化，如下图所示：
- en: '![](img/d4773e34-6b4f-4007-8316-601d70c94e8a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4773e34-6b4f-4007-8316-601d70c94e8a.png)'
- en: 'Left: Visual illustration of the RNN recurrence relation: s*[t] =*Ws*[t-1] + *Ux*[t;]* The
    final output will be y*[t] =*Vs*[t] . *Right: The RNN states are recurrently unfolded
    over the sequence *t-1, t, t+1*. Note that the parameters U, V, and W are shared
    between all steps'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 左：RNN递归关系的可视化说明：*s*[t] =*Ws*[t-1] + *Ux*[t;]* 最终输出为 y*[t] =*Vs*[t] 。右：RNN状态在序列 *t-1,
    t, t+1* 上递归展开。注意，参数 U、V和W 在所有步骤之间是共享的
- en: 'The RNN has three sets of parameters (or weights):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RNN有三组参数（或权重）：
- en: '**U** transforms the input, **x***[t]*, into the state, **s***[t].*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U** 将输入 **x***[t]* 转换为状态 **s***[t]*。'
- en: '**W** transforms the previous state, **s***[t-1]*, into the current state, **s***[t].*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W** 将前一个状态 **s***[t-1]* 转换为当前状态 **s***[t]*。'
- en: '**V** maps the newly computed internal state, **s***[t]*, to the output, **y***[t].*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V** 将新计算的内部状态 **s***[t]* 转换为输出 **y***[t]。'
- en: '**U**, **V**, and **W** apply linear transformation over their respective inputs.
    The most basic case of such a transformation is the familiar weighted sum we know
    and love. We can now define the internal state and the network output as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**U**、**V** 和 **W** 对其各自的输入进行线性变换。最基本的这种变换案例是我们熟悉的加权和。我们现在可以定义内部状态和网络输出如下：'
- en: '![](img/584cff1d-869f-4eae-a3e4-6d928b765678.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/584cff1d-869f-4eae-a3e4-6d928b765678.png)'
- en: '![](img/0d0c0977-4890-4872-8e65-ad1e5cef01af.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d0c0977-4890-4872-8e65-ad1e5cef01af.png)'
- en: Here, *f* is the non-linear activation function (such as tanh, sigmoid, or ReLU).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f* 是非线性激活函数（如tanh、sigmoid或ReLU）。
- en: For example, in a word-level language model, the input, *x*, will be a sequence
    of words encoded in input vectors *(***x***[1] ...* **x***[t] ...)*. The state, *s*, will
    be a sequence of state vectors *(***s***[1] ...* **s***[t] ... )*. Finally, the
    output, *y*, will be a sequence of probability vectors *(***y***[1] ...* **y***[t] ...
    )* of the next words in the sequence.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个单词级别的语言模型中，输入，*x*，将是一个由输入向量（*(**x**[1] ... **x**[t] ...)*）编码的单词序列。状态，*s*，将是一个状态向量序列（*(**s**[1]
    ... **s**[t] ...)*）。最后，输出，*y*，将是一个表示下一个单词的概率向量序列（*(**y**[1] ... **y**[t] ...)*）。
- en: Note that, in an RNN, each state is dependent on all of the previous computations
    through this recurrence relation. An important implication of this is that RNNs
    have memory over time because the states, *s*, contain information based on the
    previous steps. In theory, RNNs can remember information for an arbitrarily long
    period of time, but in practice, they are limited to looking back only a few steps.
    We will address this issue in more detail in the *Vanishing and exploding gradients* section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在RNN中，每个状态都依赖于通过递归关系得到的所有先前计算结果。这一点的一个重要含义是，RNN在时间上具有记忆，因为状态 *s* 包含基于前一步骤的信息。从理论上讲，RNN可以记住信息任意长的时间，但在实际应用中，它们只能回顾到前几步。我们将在“*消失与爆炸梯度*”一节中详细讨论这个问题。
- en: 'The RNN we described here is somewhat equivalent to a single layer regular
    neural network (with an additional recurrence relationship). As we now know from
    [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of
    Neural Networks*, a network with a single layer has some serious limitations.
    Fear not! As with regular networks, we can stack multiple RNNs to form a **stacked
    RNN**. The cell state, **s***^l[t]*, of an RNN cell at level *l* at time *t* will
    take the output, **y***[t]^(l-1)*, of the RNN cell from level *l-1* and the previous cell
    state, **s***^l[t-1]*, of the cell at the same level, *l*, as the input:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里描述的RNN在某种程度上等同于一个单层的常规神经网络（并且具有额外的递归关系）。正如我们现在从[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)《神经网络的基本原理》中所知，单层网络存在一些严重的限制。别担心！像常规网络一样，我们可以堆叠多个RNN，形成**堆叠RNN**。在时间步`t`，位于第`l`层的RNN单元的细胞状态，**s***^l[t]*，将接收来自第`l-1`层RNN单元的输出，**y***[t]^(l-1)*，以及该层第`l`单元在时间步`t-1`的前一时刻的细胞状态，**s***^l[t-1]*，作为输入：
- en: '![](img/10bc347d-66bb-471a-a2a7-476e5854a08c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10bc347d-66bb-471a-a2a7-476e5854a08c.png)'
- en: 'In the following diagram, we can see an unfolded, stacked RNN:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到一个展开的堆叠RNN：
- en: '![](img/bede0919-4493-44ef-987c-42c64ddaa706.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bede0919-4493-44ef-987c-42c64ddaa706.png)'
- en: Stacked RNN
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠RNN
- en: The RNN we've discussed so far takes the preceding elements of the sequence
    to produce an output. This makes sense for tasks such as time series prediction,
    where we want to predict the next element of the series based on the previous
    elements. But it also imposes unnecessary limitations on other tasks, such as
    the ones from the NLP domain. As we saw in [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml),
    *Language Modeling*, we can obtain a lot of information about a word by its context
    and it makes sense to extract that context from both the preceding and succeeding
    words.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们讨论的RNN利用序列的前一个元素来产生输出。这对于时间序列预测等任务是有意义的，我们希望根据前面的元素预测序列中的下一个元素。但它也在其他任务上施加了不必要的限制，例如自然语言处理领域的任务。正如我们在[第6章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)《语言建模》中所看到的，我们可以通过上下文获得关于一个词的很多信息，因此从前后词语中提取上下文是有意义的。
- en: 'We can extend the regular RNN to the so-called **bidirectional RNN** to cover
    this scenario, as shown in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以扩展常规的RNN，使用所谓的**双向RNN**来覆盖这种情况，如下图所示：
- en: '![](img/bb274fc3-b90e-4e5d-bb80-4e9bd1fc1e1f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb274fc3-b90e-4e5d-bb80-4e9bd1fc1e1f.png)'
- en: Bidirectional RNN
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 双向RNN
- en: 'This network has two propagation loops working in both directions, that is,
    left to right from steps *t* to *t+1* and right to left from steps *t+1* to *t*.
    We''ll denote the right to left propagation related notations with the prim symbol
    (not to be confused with derivatives). At each time step, *t*, the network maintains
    two internal state vectors: **s***[t]* for the left to right propagation and **s***''[t]*
    for the right to left propagation. The right to left phase has its own set of
    input weights, *U''* and *W''* , mirroring the weights, **U** and **W**, for the
    left to right phase. The formula for the right to left hidden state vector is
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络有两个传播循环，分别朝两个方向工作，也就是说，从步长 *t* 到 *t+1* 的从左到右传播和从步长 *t+1* 到 *t* 的从右到左传播。我们用符号‘来表示从右到左的传播（不要与导数混淆）。在每一个时间步长
    *t*，网络保持两个内部状态向量：**s***[t]*用于从左到右的传播，**s***'[t]*用于从右到左的传播。右到左阶段有自己的输入权重集，*U'*
    和 *W'*，它们对应左到右阶段的权重**U** 和 **W**。右到左隐藏状态向量的公式如下：
- en: '![](img/c13ef94c-4676-406e-8c86-ff41b28e8cb2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c13ef94c-4676-406e-8c86-ff41b28e8cb2.png)'
- en: 'The output of the network, **y***[t]*, is a combination of the internal states, **s***[t]*
    and **s***[t+1]*. One way to combine them is with concatenation. In this case,
    we''ll denote the weight matrix of the concatenated states to the output with
    **V**. Here, the formula for the output is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出，**y***[t]*，是内部状态**s***[t]*和**s***[t+1]*的组合。一种组合它们的方式是通过连接。在这种情况下，我们将连接后的状态的权重矩阵表示为**V**。这里，输出的公式如下：
- en: '![](img/18d9dcef-80b9-42f3-a675-907cc3c52d53.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18d9dcef-80b9-42f3-a675-907cc3c52d53.png)'
- en: 'Alternatively, we can simply sum the two state vectors:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以简单地将两个状态向量相加：
- en: '![](img/598c746e-ca6e-4734-85bb-84b909c5ca84.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/598c746e-ca6e-4734-85bb-84b909c5ca84.png)'
- en: Because RNNs are not limited to processing fixed-size inputs, they really expand
    the possibilities of what we can compute with neural networks, such as sequences
    of different lengths or images of varied sizes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因为RNN不局限于处理固定大小的输入，它们真正扩展了我们使用神经网络计算的可能性，比如不同长度的序列或大小不同的图像。
- en: 'Let''s go over some different combinations:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一些不同的组合方式：
- en: '**One**-**to**-**one**: This is non-sequential processing, such as feedforward neural
    networks and CNNs. Note that, there isn''t much difference between a feedforward
    network and applying an RNN to a single time step. An example of one-to-one processing
    is image classification, which we looked at in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, and [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一**：这是非顺序处理，例如前馈神经网络和卷积神经网络（CNN）。需要注意的是，前馈网络和将RNN应用于单一时间步长之间并没有太大区别。一对一处理的一个例子是图像分类，我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)《理解卷积网络》和[第3章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)《高级卷积网络》中有讨论。'
- en: '**One**-**to**-**many**: This processing generates a sequence based on a single
    input, for example, caption generation from an image (*Show and Tell: A Neural
    Image Caption Generator*, [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555)).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：这种处理方式基于单一输入生成一个序列，例如，从图像生成描述（*Show and Tell: A Neural Image Caption
    Generator*，[https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555)）。'
- en: '**Many**-**to**-**one**: This processing outputs a single result based on a
    sequence, for example, sentiment classification of text.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：这种处理方式基于一个序列输出单一结果，例如文本情感分类。'
- en: '**Many**-**to**-**many indirect**: A sequence is encoded into a state vector,
    after which this state vector is decoded into a new sequence, for example, language
    translation (*Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation, *[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078) and *Sequence
    to Sequence Learning with Neural Networks*, [http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多间接**：一个序列被编码成一个状态向量，之后这个状态向量被解码成一个新的序列，例如，语言翻译（*使用RNN编码器-解码器进行短语表示学习用于统计机器翻译，*[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)和*使用神经网络的序列到序列学习*，[http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)）。'
- en: '**Many**-**to**-**many direct:** This outputs a result for each input step,
    for example, frame phoneme labeling in speech recognition*.*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多直接**：对于每个输入步骤，输出一个结果，例如，语音识别中的帧音素标注*。'
- en: The many-to-many models are often referred to as **sequence-to-sequence** (**seq2seq**)
    models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 多对多模型通常被称为**序列到序列**（**seq2seq**）模型。
- en: 'The following is a graphical representation of the preceding input-output combinations:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述输入输出组合的图形表示：
- en: '![](img/b66a358c-a343-4da9-b294-a1430120170c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b66a358c-a343-4da9-b294-a1430120170c.png)'
- en: 'RNN input-output combinations: Inspired by http://karpathy.github.io/2015/05/21/rnn-effectiveness/.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的输入输出组合：灵感来自[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。
- en: Now that we've introduced RNNs, in the next section, we'll implement a simple
    RNN example from scratch to improve our knowledge.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了RNN，在接下来的部分中，我们将从头开始实现一个简单的RNN示例，以加深我们的理解。
- en: RNN implementation and training
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的实现和训练
- en: 'In the preceding section, we briefly discussed what RNNs are and what problems
    they can solve. Let''s dive into the details of an RNN and how to train it with
    a very simple toy example: counting ones in a sequence.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的一节中，我们简要讨论了什么是RNN以及它们能解决哪些问题。接下来我们将深入探讨RNN的细节以及如何通过一个非常简单的玩具示例来训练它：计算序列中的1的数量。
- en: In this problem, we will teach a basic RNN how to count the number of ones in
    the input and then output the result at the end of the sequence. This is an example
    of a many-to-one relationship, which we defined in the previous section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们将教一个基本的RNN如何计算输入中1的数量，然后在序列的最后输出结果。这是一个多对一关系的示例，我们在前一部分中已经定义过。
- en: 'We''ll implement this example with Python (no DL libraries) and NumPy. An example
    of the input and output is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用Python（不使用深度学习库）和NumPy来实现这个示例。输入和输出的示例如下：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The RNN we''ll use is illustrated in the following diagram:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的RNN在下图中有所示例：
- en: '![](img/c817848d-b9e9-4fb0-928d-02becdbb5127.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c817848d-b9e9-4fb0-928d-02becdbb5127.png)'
- en: Basic RNN for counting ones in the input
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算输入中1的基本RNN
- en: 'The network will have only two parameters: an input weight, **U**, and a recurrence
    weight, **W**. The output weight, **V**, is set to 1 so that we just read out
    the last state as the output, **y**.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将只有两个参数：一个输入权重，**U**，和一个递归权重，**W**。输出权重，**V**，被设置为1，以便我们只读取最后一个状态作为输出，**y**。
- en: Since *s[t]*, *x[t]*, *U*, and *W* are scalar values, we won't use the matrix
    notation (bold capital letters) in the *RNN implementation and training* section
    and its subsections. However, note that the generic versions of these formulas
    use matrix and vector parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*s[t]*、*x[t]*、*U*和*W*是标量值，我们在*RNN实现和训练*部分及其子部分中将不使用矩阵符号（粗体大写字母）。然而，请注意，这些公式的通用版本使用矩阵和向量参数。
- en: 'Before we continue, let''s add some code so that our example can be executed.
    We''ll import `numpy` and define our training and data, `x`, and labels, `y`. `x` is
    two-dimensional since the first dimension represents the sample in the mini-batch.
    For the sake of simplicity, we''ll use a mini-batch with a single sample:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，先添加一些代码以使我们的示例能够执行。我们将导入`numpy`并定义我们的训练数据`x`和标签`y`。`x`是二维的，因为第一维表示小批量中的样本。为了简化起见，我们将使用一个只包含一个样本的小批量：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The recurrence relation defined by this network is [![](img/6a316df2-66ff-4de3-ac56-4e852c351973.png)].
    Note that this is a linear model since we don''t apply a non-linear function in
    this formula. We can implement a recurrence relationship as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由此网络定义的递归关系是[![](img/6a316df2-66ff-4de3-ac56-4e852c351973.png)]。请注意，这是一个线性模型，因为我们在这个公式中没有应用非线性函数。我们可以按如下方式实现递归关系：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The states, *s[t]*, and the weights, *W* and *U*, are single scalar values. A
    good solution to this is to just get the sum of the inputs across the sequence.
    If we set *U=1*, then whenever input is received, we will get its full value.
    If we set *W=1*, then the value we would accumulate would never decay. So, for
    this example, we would get the desired output: 3.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *s[t]* 和权重 *W* 以及 *U* 是单一的标量值。一个好的解决方案是直接对序列中的输入进行求和。如果我们设置 *U=1*，那么每当输入被接收时，我们将获得其完整的值。如果我们设置
    *W=1*，那么我们累积的值将永远不会衰减。因此，对于这个示例，我们将得到期望的输出：3。
- en: Nevertheless, let's use this simple example to network the training and implementation
    of this neural network. This will be interesting, as we will see in the rest of
    this section. First, let's look at how we can get this result through backpropagation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们用这个简单的示例来训练和实现这个神经网络。这将非常有趣，因为我们将在本节的其余部分看到。首先，让我们看看如何通过反向传播得到这个结果。
- en: Backpropagation through time
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: 'Backpropagation through time is the typical algorithm we use to train recurrent
    networks (*Backpropagation Through Time: What It Does and How to Do It*, [http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)).
    As the name suggests, it''s based on the backpropagation algorithm we discussed
    in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts
    of Neural Networks*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '时间反向传播是我们用来训练递归网络的典型算法（*Backpropagation Through Time: What It Does and How
    to Do It*，[http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)）。顾名思义，它基于我们在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中讨论的反向传播算法，*神经网络的基础与核心*。'
- en: 'The main difference between regular backpropagation and backpropagation through
    time is that the recurrent network is unfolded through time for a certain number
    of time steps (as illustrated in the preceding diagram). Once the unfolding is
    complete, we end up with a model that is quite similar to a regular multi-layer
    feedforward network, that is, one hidden layer of that network represents one
    step through time. The only differences are that each layer has multiple inputs:
    the previous state, *s[t-1]*, and the current input, *x[t]*. The parameters *U* and *W* are
    shared between all of the hidden layers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正常的反向传播和时间反向传播的主要区别在于，递归网络是通过时间展开的，展开的时间步数是有限的（如前面的图示所示）。一旦展开完成，我们将得到一个与常规多层前馈网络非常相似的模型，即该网络的一个隐藏层代表了一个时间步。唯一的区别是，每一层都有多个输入：前一个状态
    *s[t-1]* 和当前输入 *x[t]*。参数 *U* 和 *W* 在所有隐藏层之间共享。
- en: 'The forward pass unwraps the RNN along the sequence and builds a stack of states
    for each step. In the following code block, we can see an implementation of the
    forward pass, which returns the activation, *s*, for each recurrent step and each
    sample in the batch:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播解开了RNN在序列中的展开，并为每个步骤构建了状态栈。在以下的代码块中，我们可以看到一个前向传播的实现，它返回每个递归步骤和批次中每个样本的激活值
    *s*：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have our forward step and loss function, we can define how the gradient
    is propagated backward. Since the unfolded RNN is equivalent to a regular feedforward
    network, we can use the backpropagation chain rule we introduced in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了前向步骤和损失函数，我们可以定义如何反向传播梯度。由于展开后的RNN等同于常规的前馈网络，我们可以使用在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中介绍的反向传播链式法则，*神经网络的基础与核心*。
- en: Because the weights, *W* and *U*, are shared across the layers, we'll accumulate
    the error derivatives for each recurrent step, and in the end, we'll update the
    weights with the accumulated value.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为权重 *W* 和 *U* 在各层之间共享，我们将为每个递归步骤累积误差导数，最后，我们将使用累积的值来更新权重。
- en: 'First, we need to get the gradient of the output, **s***[t]*, with respect
    to the loss function (*∂J/∂s*). Once we have it, we''ll propagate it backward
    through the stack of activities we built during the forward step. This backward
    pass pops activities off of the stack to accumulate their error derivatives at
    each time step. The recurrence relation to propagate this gradient through the
    network can be written as follows (chain rule):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要获取输出 **s***[t]* 关于损失函数的梯度（*∂J/∂s*）。一旦我们得到它，我们将通过在前向步骤中构建的活动栈向后传播它。这个反向传播过程将从栈中弹出活动，以在每个时间步骤累积它们的误差导数。通过网络传播这个梯度的递归关系可以写成如下（链式法则）：
- en: '![](img/3b2df19a-bc53-40d8-97f4-44a99dce9073.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b2df19a-bc53-40d8-97f4-44a99dce9073.png)'
- en: Here, *J* is the loss function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*J* 是损失函数。
- en: 'The gradients of the weights, *U* and *W*, are accumulated as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的梯度，*U* 和 *W*，是通过以下方式累积的：
- en: '![](img/e86ed439-4010-4d3c-bb46-ae9c1d94a108.png)![](img/21a0bab0-78dc-4092-9285-083b28271940.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e86ed439-4010-4d3c-bb46-ae9c1d94a108.png)![](img/21a0bab0-78dc-4092-9285-083b28271940.png)'
- en: 'The following is an implementation of the backward pass:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是反向传播的实现：
- en: 'The gradients for `U` and `W` are accumulated in `gU` and `gW`, respectively:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`U` 和 `W` 的梯度分别在 `gU` 和 `gW` 中累积：'
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now try to use gradient descent to optimize our network. We compute `gradients` (using
    mean square error) with the help of the `backward` function and we use them to
    update the `weights` value:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以尝试使用梯度下降来优化我们的网络。我们通过`backward`函数计算`gradients`（使用均方误差），并利用它们来更新`weights`值：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we''ll implement the related `plot_training` function, which displays
    the `loss` function and the gradients for each weight over the epochs:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现相关的`plot_training`函数，该函数在每个周期显示`loss`函数和每个权重的梯度：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we can run this code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以运行以下代码：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code produces the following diagram:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下图示：
- en: '![](img/c8b9ef4b-3dd9-449e-a387-15c4cd59b827.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8b9ef4b-3dd9-449e-a387-15c4cd59b827.png)'
- en: 'The RNN loss: the uninterrupted line represents the loss, where the dashed
    lines represent the weight gradients during training'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的损失：不中断的线表示损失，其中虚线表示训练过程中的权重梯度。
- en: Now that we've learned about backpropagation through time, let's discuss how
    the familiar vanishing and exploding gradient problems affect it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了时间反向传播，接下来我们讨论一下熟悉的梯度消失和梯度爆炸问题是如何影响它的。
- en: Vanishing and exploding gradients
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失与梯度爆炸
- en: 'The preceding example has an issue, though. Let''s run the training process
    with a longer sequence:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的示例存在一个问题。让我们使用更长的序列运行训练过程：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The reason for these warnings is that the final parameters, *U* and *W*, end
    up as **Not a Number** (**NaN**). To display the gradients properly, we'll need
    to change the scale of the gradient axis in the `plot_training` function from
    `ax1.set_ylim(-3, 20)` to `ax1.set_ylim(-3, 600)`, as well as the scale of the
    loss axis from `ax2.set_ylim(-3, 10)` to `ax2.set_ylim(-3, 200)`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些警告的原因是，最终的参数，*U* 和 *W*，变成了**不是一个数字**（**NaN**）。为了正确显示梯度，我们需要在`plot_training`函数中将梯度轴的尺度从`ax1.set_ylim(-3,
    20)`改为`ax1.set_ylim(-3, 600)`，并将损失轴的尺度从`ax2.set_ylim(-3, 10)`改为`ax2.set_ylim(-3,
    200)`。
- en: 'Now, the program will produce the following diagram of the new loss and gradients:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，程序将生成以下新损失和梯度的图示：
- en: '![](img/d20b08b7-7629-4068-9b31-bc8201873414.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d20b08b7-7629-4068-9b31-bc8201873414.png)'
- en: Parameters and loss function during exploding gradients scenario
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸场景中的参数和损失函数
- en: 'In the initial epochs, the gradients slowly increase, similar to the way they
    increased for the shorter sequence. However, when they get to epoch 23 (the exact
    epoch is unimportant, though), the gradient becomes so large that it goes out
    of the range of the `float` variable and becomes NaN (as illustrated by the jump
    in the plot). This problem is known as exploding gradients. We can stumble upon
    exploding gradients in a regular feedforward NN, but it is especially pronounced
    in RNNs. To understand why, let''s recall the recurrent gradient propagation chain
    rule for the two consecutive sequence steps we defined in the *Backpropagation
    through time* section:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始的训练周期中，梯度逐渐增大，类似于它们在较短序列中增加的方式。然而，当达到第23个周期时（确切的周期并不重要），梯度变得非常大，以至于超出了`float`变量的范围并变成了NaN（如图中的跳跃所示）。这个问题称为梯度爆炸。我们可能会在常规的前馈神经网络（NN）中遇到梯度爆炸问题，但在循环神经网络（RNN）中尤为明显。为了理解为什么会这样，我们回顾一下在*时间反向传播*部分中定义的两个连续序列步骤的循环梯度传播链规则：
- en: '![](img/fba12913-286f-4108-8ddf-650abdee55f7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fba12913-286f-4108-8ddf-650abdee55f7.png)'
- en: 'Depending on the sequence''s length, an unfolded RNN can be much deeper compared
    to a regular network. At the same time, the weights, *W*, of an RNN are shared
    across all of the steps. Therefore, we can generalize this formula to compute
    the gradient between two non-consecutive steps of the sequence. Because *W* is
    shared, the equation forms a geometric progression:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据序列的长度，展开的RNN比常规网络可能更深。同时，RNN的权重*W*在所有步骤中是共享的。因此，我们可以将这个公式推广到计算序列中两个非连续步骤之间的梯度。由于*W*是共享的，方程形成了几何级数：
- en: '![](img/4d8f4eb3-d708-4568-938b-1815bf786810.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d8f4eb3-d708-4568-938b-1815bf786810.png)'
- en: In our simple linear RNN, the gradient grows exponentially if *|W|* > *1* (exploding
    gradient), where *W* is a single scalar weight, for example, 50 time steps over
    W=1.5 is *W^(50) ≈ 637621500*. The gradient shrinks exponentially if *|W| <1* (vanishing
    gradient), for example, 10 time steps over *W=0.6* is *W^(20) = 0.00097*. If the
    weight parameter, **W**, is a matrix instead of a scalar, this exploding or vanishing
    gradient is related to the largest eigenvalue (*ρ*) of **W** (also known as a
    spectral radius). It is sufficient for *ρ* < *1* for the gradients to vanish,
    and it is necessary for *ρ* > *1* for them to explode.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的线性RNN中，如果*|W|* > *1*（梯度爆炸），梯度会呈指数增长，其中*W*是一个单一的标量权重，例如，50个时间步长时，W=1.5时，*W^(50)
    ≈ 637621500*。如果*|W| <1*（梯度消失），梯度会呈指数衰减，例如，10个时间步长时，W=0.6时，*W^(20) = 0.00097*。如果权重参数**W**是一个矩阵而非标量，那么这种梯度爆炸或梯度消失与**W**的最大特征值（*ρ*）（也称为谱半径）相关。如果*ρ*
    < *1*，则梯度会消失，而如果*ρ* > *1*，则梯度会爆炸。
- en: The vanishing gradients problem, which we first mentioned in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of Neural Networks*, has another more subtle effect in RNNs. The
    gradient decays exponentially over the number of steps to a point where it becomes
    extremely small in the earlier states. In effect, they are overshadowed by the
    larger gradients from more recent time steps, and the network's ability to retain
    the history of these earlier states vanishes. This problem is harder to detect
    because the training will still work and the network will produce valid outputs
    (unlike with exploding gradients). It just won't be able to learn long-term dependencies.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题，我们在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)《神经网络的基本原理》中首次提到，在RNN中有另一种更加微妙的影响。梯度会随着步数的增加呈指数衰减，直到变得极其微小，特别是在较早的状态中。实际上，它们会被来自更近期时间步的较大梯度所掩盖，导致网络无法保持这些较早状态的历史信息。这个问题更难以检测，因为训练仍然可以进行，网络也会产生有效的输出（不同于梯度爆炸）。它只是无法学习长期的依赖关系。
- en: Now, we are familiar with some of the problems of RNNs. This knowledge will
    serve us well because, in the next section, we'll discuss how to solve these problems
    with the help of a special type of RNN.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经熟悉了RNN的一些问题。这些知识对我们非常有用，因为在下一节中，我们将讨论如何借助一种特殊类型的RNN来解决这些问题。
- en: Introducing long short-term memory
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入长短期记忆
- en: Hochreiter and Schmidhuber studied the problems of vanishing and exploding gradients
    extensively and came up with a solution called **Long Short-Term Memory** (**LSTM**, [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)).
    LSTMs can handle long-term dependencies due to a specially crafted memory cell.
    In fact, they work so well that most of the current accomplishments in training
    RNNs on a variety of problems are due to the use of LSTMs. In this section, we'll
    explore how this memory cell works and how it solves the vanishing gradients issue.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter和Schmidhuber广泛研究了梯度消失和梯度爆炸问题，并提出了一个叫做**长短期记忆**（**LSTM**， [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)）的解决方案。LSTM可以通过特别设计的记忆单元处理长期依赖关系。事实上，它们工作得非常好，以至于目前大多数RNN在各种问题上的成功都归功于LSTM的使用。在本节中，我们将探索这个记忆单元是如何工作的，以及它是如何解决梯度消失问题的。
- en: 'The key idea of LSTM is the cell state, **c***[t]* (in addition to the hidden
    RNN state, **h***[t]*), where the information can only be explicitly written in
    or removed so that the state stays constant if there is no outside interference. The
    cell state can only be modified by specific gates, which are a way to let information
    pass through. These gates are composed of a sigmoid function and element-wise
    multiplication. Because the sigmoid only outputs values between 0 and 1, the multiplication
    can only reduce the value running through the gate. A typical LSTM is composed
    of three gates: a forget gate, an input gate, and an output gate. The cell state,
    input, and output are all vectors so that the LSTM can hold a combination of different
    information blocks at each time step.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的关键思想是单元状态 **c***[t]*（除了隐藏的 RNN 状态 **h***[t]*），在没有外部干扰时，只能明确写入或移除信息，使状态保持恒定。
    单元状态只能通过特定的门进行修改，这些门是信息传递的一种方式。 这些门由 Sigmoid 函数和逐元素乘法组成。 由于 Sigmoid 仅输出介于 0 和
    1 之间的值，乘法只能减少通过门传递的值。 典型的 LSTM 由三个门组成：遗忘门、输入门和输出门。 单元状态、输入和输出都是向量，因此 LSTM 可以在每个时间步骤中保存不同信息块的组合。
- en: 'The following is a diagram of an LSTM cell:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 LSTM 单元的图示：
- en: '![](img/1d5075a0-c9b0-4eb5-9145-f44374225915.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d5075a0-c9b0-4eb5-9145-f44374225915.png)'
- en: 'Top: LSTM cell; bottom: Unfolded LSTM cell: Inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部：LSTM 单元；底部：展开的 LSTM 单元：灵感源于 [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: Before we continue, let's introduce some notations. **x***[t]*, **c***[t]*,
    and **h***[t]* are the LSTM's input, cell memory state, and output (or hidden
    state) vectors in moment *t*. **c***'[t]* is the candidate cell state vector (more
    on that later). The input, **x***[t]*, and the previous cell output, **h***[t-]*, are
    connected to each gate and the candidate cell vector with sets of fully connected
    weights, **W** and **U**, respectively*. ***f***[t]*, **i***[t]*, and **o***[t]* are
    the forget, input, and output gates of the LSTM cell. These gates are fully connected
    layers with sigmoid activations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们介绍一些符号。 **x***[t]*，**c***[t]* 和 **h***[t]* 是 LSTM 在时刻 *t* 的输入、单元记忆状态和输出（或隐藏状态）向量。
    **c***'[t]* 是候选单元状态向量（稍后详细介绍）。输入 **x***[t]* 和上一个单元输出 **h***[t-]* 通过完全连接的权重集 **W**
    和 **U** 分别连接到每个门和候选单元向量。 **f***[t]*，**i***[t]* 和 **o***[t]* 是 LSTM 单元的遗忘、输入和输出门。
    这些门是具有 Sigmoid 激活的全连接层。
- en: 'Let''s start with the forget gate, **f***[t]*. As the name suggests, it decides
    whether we want to erase parts of the existing cell state or not. It bases its
    decision on the weighted vector sum of the output of the previous cell, **h***[t-1]*, and
    the current input, **x***[t]*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从遗忘门 **f***[t]* 开始。 正如其名称所示，它决定我们是否要擦除现有单元状态的部分内容。 它基于上一个单元的加权向量之和输出，**h***[t-1]*，以及当前输入，**x***[t]*：
- en: '![](img/2cd9c1ec-6f5b-4f43-a1e4-ca82b60dbb4a.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cd9c1ec-6f5b-4f43-a1e4-ca82b60dbb4a.png)'
- en: From the preceding diagram, we can see that the forget gate applies element-wise
    sigmoid activations on each element of the previous state vector, **c***[t-1]*: **f**[*t* ]***
    c**[*t*-1]. Again, note that because the operation is element-wise, the values
    of this vector are squashed in the [0, 1] range. An output of 0 erases a specific **c***[t-1]* cell
    block completely and an output of 1 allows the information in that cell block
    to pass through. This means that the LSTM can get rid of irrelevant information
    in its cell state vector.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以看到遗忘门在上一个状态向量 **c***[t-1]* 的每个元素上应用元素级 Sigmoid 激活：**f***[t]* **c***[t-1]*。
    再次注意，因为操作是元素级的，因此该向量的值被压缩到 [0, 1] 范围内。 输出为 0 将完全擦除特定 **c***[t-1]* 单元块，输出为 1 则允许该单元块中的信息通过。
    这意味着 LSTM 可以在其单元状态向量中去除不相关的信息。
- en: 'The forget gate was not in the original LSTM that was proposed by Hochreiter.
    Instead, it was proposed in *Learning to Forget: Continual Prediction with* *LSTM*
    ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '遗忘门不是由Hochreiter最初提出的LSTM中的一部分。 相反，它是在*Learning to Forget: Continual Prediction
    with LSTM* ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf))
    中提出的。'
- en: 'The input gate, **i***[t]*, decides what new information is going to be added
    to the memory cell in a multi-step process. The first step determines whether
    any information is going to be added. As in the forget gate, it bases its decision
    on **h***[t-1]* and **x***[t]*: it outputs 0 or 1 through the sigmoid function
    for each cell of the candidate state vector. An output of 0 means that no information
    is added to that cell block''s memory. As a result, the LSTM can store specific
    pieces of information in its cell state vector:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门**i***[t]*决定要将什么新信息添加到记忆单元中，这是一个多步过程。第一步决定是否添加任何信息。像遗忘门一样，它基于**h***[t-1]*和**x***[t]*来做决策：它通过sigmoid函数输出0或1，表示候选状态向量的每个单元。输出为0意味着不会向该单元块的记忆中添加任何信息。因此，LSTM可以在其单元状态向量中存储特定的信息：
- en: '![](img/6cb87dbb-950c-4bee-9bc1-b88634ab8942.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cb87dbb-950c-4bee-9bc1-b88634ab8942.png)'
- en: 'In the next step, we compute the new candidate cell state, **c***''[t]*. It
    is based on the previous output, **h***[t-1]*, and the current input, **x***[t]*,
    and is transformed via a tanh function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们计算新的候选单元状态，**c***'[t]*。它基于前一个输出**h***[t-1]*和当前输入**x***[t]*，并通过`tanh`函数进行转换：
- en: '![](img/1d895b63-4b75-4f0a-b827-bb48c7556b0f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d895b63-4b75-4f0a-b827-bb48c7556b0f.png)'
- en: Next, *c'[t]* is combined with the sigmoid outputs of the input gate via element-wise
    multiplication, [![](img/9f951b19-2361-4769-a807-039a1bab3515.png)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**c'[t]**与输入门的sigmoid输出通过逐元素相乘进行组合，[![](img/9f951b19-2361-4769-a807-039a1bab3515.png)]。
- en: 'To recap, the forget and input gates decide what information to forget and
    include from the previous and candidate cell states, respectively. The final version
    of the new cell state, *c[t]*, is just an element-wise sum between these two components:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，遗忘门和输入门分别决定了从先前和候选单元状态中忘记和包含哪些信息。新的单元状态**c[t]**的最终版本只是这两个组件的逐元素相加：
- en: '![](img/3774867d-3a9b-4dc8-90a8-4fd0ed6a5c91.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3774867d-3a9b-4dc8-90a8-4fd0ed6a5c91.png)'
- en: 'Next, let''s focus on the output gate, which decides what the total cell output
    is going to be. It takes **h***[t-1]* and **x***[t]* as inputs and outputs, that
    is, 0 or 1 (via the sigmoid function), for each block of the cell''s memory. Like
    before, 0 means that the block doesn''t output any information and 1 means that
    the block can pass through as a cell''s output. Therefore, the LSTM can output
    specific blocks of information from its cell state vector:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们关注输出门，它决定总的单元输出是什么。它以**h***[t-1]*和**x***[t]*作为输入，并输出0或1（通过sigmoid函数），用于每个单元记忆块。如之前所述，0意味着该块不会输出任何信息，1意味着该块可以作为单元的输出。因此，LSTM可以从其单元状态向量中输出特定的块信息：
- en: '![](img/9e95c81e-cf77-4e91-b9c8-0a636838726d.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e95c81e-cf77-4e91-b9c8-0a636838726d.png)'
- en: 'Finally, the LSTM cell output is transferred by a tanh function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LSTM单元的输出通过`tanh`函数进行转换：
- en: '![](img/de7b058d-73fd-40e3-82be-1dc81faf8045.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de7b058d-73fd-40e3-82be-1dc81faf8045.png)'
- en: Because all of these formulas are derivable, we can chain LSTM cells together,
    just like when we chain simple RNN states together and train the network via backpropagation
    through time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因为所有这些公式是可导的，我们可以将LSTM单元链在一起，就像我们将简单的RNN状态链在一起并通过时间反向传播训练网络一样。
- en: 'But how does the LSTM protect us from vanishing gradients? Let''s start with
    the forward phase. Notice that the cell state is copied identically from step
    to step if the forget gate is 1 and the input gate is 0: [![](img/dbc2d8d2-a890-4200-a527-fe635ba668e8.png)].
    Only the forget gate can completely erase the cell''s memory. As a result, the
    memory can remain unchanged over a long period of time. Also, note that the input
    is a tanh activation that''s been added to the current cell''s memory. This means
    that the cell''s memory doesn''t blow up and is quite stable.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，LSTM是如何保护我们免受梯度消失的呢？我们从前向传播阶段开始。请注意，如果遗忘门为1且输入门为0，则单元状态会在每一步中完全复制：[![](img/dbc2d8d2-a890-4200-a527-fe635ba668e8.png)]。只有遗忘门才能完全擦除单元的记忆。因此，记忆可以在长时间内保持不变。另外，注意输入是一个`tanh`激活函数，它被添加到当前单元的记忆中。这意味着单元的记忆不会爆炸，并且相当稳定。
- en: 'Let''s use an example to demonstrate how a LSTM cell is unfolded. For the sake
    of simplicity, we''ll assume that it has one-dimensional (single scalar value)
    input, state, and output vectors. Because the values are scalar, we won''t use
    vector notation for the rest of this example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来演示LSTM单元是如何展开的。为了简化问题，我们假设它具有一维（单标量值）输入、状态和输出向量。由于值是标量，我们在这个例子中不会使用向量符号：
- en: '![](img/020ba50e-a95e-4095-9cff-1cae9504ff52.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/020ba50e-a95e-4095-9cff-1cae9504ff52.png)'
- en: Unrolling an LSTM through time: Inspired by http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 随时间展开 LSTM：灵感来自于 [http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)。
- en: 'The process is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 过程如下：
- en: First, we have a value of 3 as a candidate state. The input gate is set to *f[i]
    = 1* and the forget gate is set to *f[t] = 0*. This means that the previous state, *c[t-1 ]=
    N*, is erased and is replaced with the new state, [![](img/c33ecfc0-f1db-45ae-b1bc-e6c0bb7238bf.png)].
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们有一个 3 的值作为候选状态。输入门设置为 *f[i] = 1*，而忘记门设置为 *f[t] = 0*。这意味着先前的状态 *c[t-1] =
    N* 被擦除，并被新的状态 [![](img/c33ecfc0-f1db-45ae-b1bc-e6c0bb7238bf.png)] 替代。
- en: 'For the next two time steps, the forget gate is set to 1, while the input gate
    is set to 0\. By doing this, all of the information is kept throughout these steps
    and no new information is added because the input gate is set to 0: [![](img/b9053fd0-79ab-4ead-afba-eff0e9fbfba0.png)].'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于接下来的两个时间步骤，忘记门设置为 1，而输入门设置为 0。这样，所有信息在这两个步骤中都保持不变，因为输入门被设置为 0：[![](img/b9053fd0-79ab-4ead-afba-eff0e9fbfba0.png)]。
- en: Finally, the output gate is set to *o[t] = 1* and 3 is output and remains unchanged.
    We have successfully demonstrated how the internal state is stored across multiple
    steps.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，输出门设置为 *o[t] = 1*，输出 3 并保持不变。我们已经成功演示了如何在多个步骤中存储内部状态。
- en: 'Next, let''s focus on the backward phase. The cell state, *c[t]*, can mitigate
    the vanishing/ exploding gradients as well with the help of the forget gate, *f[t]*.
    Like the regular RNN, we can use the chain rule to compute the partial derivative, [![](img/d4892443-090e-4c1c-ab29-1bdac99af414.png)],
    for two consecutive steps. Following the formula [![](img/5571ece6-2c96-43b6-8702-a073dcdfb7c2.png)] and
    without going into details, its partial derivative is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注反向传播阶段。通过忘记门（*f[t]*）的帮助，单元状态 *c[t]* 可以减轻梯度消失/爆炸的问题。与常规的 RNN 一样，我们可以使用链式法则计算两个连续步骤的偏导数，[![](img/d4892443-090e-4c1c-ab29-1bdac99af414.png)]。根据公式
    [![](img/5571ece6-2c96-43b6-8702-a073dcdfb7c2.png)]，不展开细节，其偏导数如下：
- en: '![](img/02048859-47b3-49b0-8762-a8f98ea96f59.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02048859-47b3-49b0-8762-a8f98ea96f59.png)'
- en: 'We can generalize this to non-consecutive steps as well:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将其推广到非连续步骤：
- en: '![](img/7e03905d-1642-4d01-ac62-bf7c04e91213.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e03905d-1642-4d01-ac62-bf7c04e91213.png)'
- en: If the forget gate values are close to 1, gradient information can pass back
    through the network states almost unchanged. This is because *f[t]* uses sigmoid
    activation and information flow is still subject to the vanishing gradient that's
    specific to sigmoid activations ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of Neural Networks*). But unlike the gradients in the regular RNN, *f[t]* has
    a different value at each time step. Therefore, this is not a geometric progression
    and the vanishing gradient effect is less pronounced.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果忘记门的值接近 1，梯度信息几乎可以不变地通过网络状态反向传播。这是因为 *f[t]* 使用了 sigmoid 激活，信息流仍然受到特定于 sigmoid
    激活的梯度消失问题的影响（[第 1 章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本构件*）。但与常规
    RNN 中的梯度不同，*f[t]* 在每个时间步都有不同的值。因此，这不是几何级数，梯度消失效应不那么明显。
- en: 'We can stack LSTM cells in the same way as we stack regular RNNs, with the
    exception that a cell state of step *t* at one level serves as an input to the
    cell state of the same level at step *t+1*. The following diagram shows an unfolded
    stacked LSTM:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像堆叠常规 RNN 一样堆叠 LSTM 单元，唯一的不同是步长 *t* 的单元状态在一个层级上作为该层级在步长 *t+1* 的单元状态的输入。以下图示展示了展开的堆叠
    LSTM：
- en: '![](img/84fa3898-e1b6-4bf3-b4b6-cb75e50b1b7a.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84fa3898-e1b6-4bf3-b4b6-cb75e50b1b7a.png)'
- en: Stacked LSTM
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠 LSTM
- en: Now that we've introduced LSTM, let's solidify our knowledge by implementing
    it in the next section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 LSTM，接下来让我们通过实现它来巩固我们的知识。
- en: Implementing LSTM
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 LSTM
- en: 'In this section, we''ll implement an LSTM cell with PyTorch 1.3.1\. First,
    let''s note that PyTorch already has an LSTM implementation, which is available
    at `torch.nn.LSTM`. However, our goal is to understand how the LSTM cell works,
    so we''ll implement our own version from scratch instead. The cell will be a subclass
    of `torch.nn.Module` and we''ll use it as a building block for larger models.
    The source code for this example is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_cell.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_cell.py).
    Let''s get started:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用PyTorch 1.3.1实现一个LSTM单元。首先，我们要注意，PyTorch已经有一个LSTM实现，位于`torch.nn.LSTM`。但是，我们的目标是理解LSTM单元的工作原理，所以我们将从头开始实现自己的版本。该单元将是`torch.nn.Module`的子类，我们将把它作为更大模型的构建模块。本示例的源代码可以在[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_cell.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_cell.py)找到。让我们开始吧：
- en: 'First, we''ll do the imports:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们进行导入：
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we''ll implement the class and the `__init__` method:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现类和`__init__`方法：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To understand the role of the fully connected layers, `self.x_fc` and `self.h_fc`,
    let's recall that the candidate cell state and the input, forget, and output gates
    all depend on the weighted vector sum of the input, **x**[*t*], and the previous
    cell output, **h***[t-1]*. Therefore, instead of having eight separate [![](img/4ef433cf-f877-43bd-beec-91ec7b59eb90.png)] and
    [![](img/c78961c9-01c2-45ed-89a8-39a10cdc5a2d.png)] operations for each cell,
    we can combine these and make two large fully connected layers, `self.x_fc` and `self.h_fc`,
    each with an output size of `4 * hidden_size`. Once we need the output for a specific
    gate, we can extract the necessary slice from either of the two tensor outputs
    of the fully connected layers (we'll see how to do that in the implementation
    of the `forward` method).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解全连接层`self.x_fc`和`self.h_fc`的作用，让我们回顾一下，候选细胞状态以及输入、遗忘和输出门都依赖于输入**x**[*t*]和前一个细胞输出**h**[*t-1*]的加权向量和。因此，我们不需要为每个细胞执行八个独立的[![](img/4ef433cf-f877-43bd-beec-91ec7b59eb90.png)]和[![](img/c78961c9-01c2-45ed-89a8-39a10cdc5a2d.png)]操作，而是可以将它们合并，做成两个大的全连接层`self.x_fc`和`self.h_fc`，每个层的输出大小为`4
    * hidden_size`。当我们需要某个特定门的输出时，我们可以从这两个全连接层的任意一个张量输出中提取必要的切片（我们将在`forward`方法的实现中看到如何做到这一点）。
- en: 'Let''s continue with the `reset_parameters` method, which initializes all of
    the weights of the network with the LSTM-specific Xavier initializer (if you copy
    and paste this code directly, you may have to check the indentation):'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续讨论`reset_parameters`方法，该方法使用LSTM特定的Xavier初始化器初始化网络的所有权重（如果你直接复制并粘贴此代码，可能需要检查缩进）：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we''ll start implementing the `forward` method, which contains all of
    the LSTM execution logic we described in the *Introducing long short-term memory*
    section. It takes the current mini-batch at step *t*, as well as a tuple that
    contains the cell output and cell state at step *t-1*, as input:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将开始实现`forward`方法，该方法包含我们在*介绍长短期记忆*部分中描述的所有LSTM执行逻辑。它接收当前时间步*t*的mini-batch，以及一个包含时间步*t-1*时刻的细胞输出和细胞状态的元组作为输入：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We''ll continue by computing activations for all three gates and the candidate
    state simultaneously. It''s as simple as doing the following:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将继续同时计算三个门和候选状态的激活。做法很简单，像这样：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we''ll split the output for each gate:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将为每个门分离输出：
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we''ll apply the `activation` functions over them:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将对它们应用`activation`函数：
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we''ll compute the new cell state, **c***[t]*:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将计算新的细胞状态，**c**[*t*]：
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we''ll compute the cell output, `ht`, and we''ll return it along with
    the new cell state, *c[t]*:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将计算细胞输出`ht`，并将其与新的细胞状态*c[t]*一起返回：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once we have the LSTM cell, we can apply it to the same task of counting the
    ones in a sequence, like we did with the regular RNN. We''ll only include the
    most relevant parts of the source code, but the full example is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py).
    This time, we''ll use a full training set of 10,000 binary sequences that have
    a length of 20 (these are arbitrary numbers). The premise of the implementation
    is similar to the RNN example: we feed the binary sequence to the LSTM in a recurrent
    manner and the cell outputs the predicted count of the ones as a single scalar
    value (regression task). However, our `LSTMCell` implementation has two limitations:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了LSTM单元，就可以将其应用于与常规RNN相同的任务——计算序列中1的数量。我们只会包含源代码中最相关的部分，但完整的示例可以在[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py)中找到。这次，我们将使用一个包含10,000个二进制序列的完整训练集，每个序列的长度为20（这些是随意设定的数字）。实现的前提与RNN示例类似：我们以递归的方式将二进制序列输入到LSTM中，单元输出作为单一标量值的预测1的数量（回归任务）。然而，我们的`LSTMCell`实现有两个限制：
- en: It only covers a single step of the sequence.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它仅涵盖序列的单个步骤。
- en: It outputs the cell state and the network output vector. This is a regression
    task and we have a single output value, but the cell state and network output
    have more dimensions.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它输出单元状态和网络输出向量。这是一个回归任务，我们有一个单一的输出值，但单元状态和网络输出有更多维度。
- en: To solve these problems, we'll implement a custom `LSTMModel` class, which extends
    `LSTMCell`. It feeds the `LSTMCell` instance with all of the elements of the sequence
    and handles the transition of the cell state and network output from one element
    of the sequence to the next.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们将实现一个自定义的`LSTMModel`类，它扩展了`LSTMCell`。它将整个序列的所有元素输入给`LSTMCell`实例，并处理单元状态和网络输出从序列的一个元素过渡到下一个元素的过程。
- en: 'Once the final output has been produced, it is fed to a fully connected layer,
    which transforms it into a single scalar value that represents the network''s
    prediction of the number of ones. The following is the implementation of this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦最终输出产生，它将被传递给一个全连接层，转换为一个单一的标量值，表示网络预测的1的数量。以下是其实现：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we''ll jump straight to the train/test setup stage (recall that this is
    just a snippet of the full source code):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将直接跳到训练/测试设置阶段（请记住，这只是完整源代码的一部分）：
- en: 'First, we''ll generate the training and testing datasets. The `generate_dataset`
    function returns an instance of `torch.utils.data.TensorDataset`. It contains
    `TRAINING_SAMPLES = 10000` two-dimensional tensors of binary sequences with a
    length of `SEQUENCE_LENGTH = 20` and scalar value labels for the number of ones
    in each sequence:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将生成训练和测试数据集。`generate_dataset`函数返回一个`torch.utils.data.TensorDataset`实例。它包含`TRAINING_SAMPLES
    = 10000`个长度为`SEQUENCE_LENGTH = 20`的二进制序列的二维张量，以及每个序列中1的数量的标量值标签：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We''ll instantiate the model with `HIDDEN_UNITS = 20`. The model takes a single
    input (each sequence element) and outputs a single value (number of ones):'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`HIDDEN_UNITS = 20`来实例化模型。模型接受一个输入（每个序列元素），并输出一个值（1的数量）：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we''ll instantiate the `MSELoss` function (because of the regression)
    and the Adam optimizer:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化`MSELoss`函数（因为这是回归问题）和Adam优化器：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we can run the training/testing cycle for `EPOCHS = 10`. The `train_model`
    and `test_model` functions are the same as the ones we implemented in the *Implementing
    transfer learning with PyTorch* section of [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding
    Convolutional Networks*:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以运行`EPOCHS = 10`的训练/测试周期。`train_model`和`test_model`函数与我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)《理解卷积网络》节中实现的函数相同：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If we run this example, the network will achieve 100% test accuracy in 5-6 epochs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这个示例，网络将在5到6个epoch内达到100%的测试准确率。
- en: Now that we've learned about LSTMs, let's shift our attention to gated recurrent
    units. This is another type of recurrent block that tries to replicate the properties
    of LSTM, but with a simplified structure.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经学习了LSTM，现在让我们将注意力转向门控循环单元（GRU）。这是一种尝试复制LSTM特性但结构更简化的另一种循环块。
- en: Introducing gated recurrent units
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍门控循环单元
- en: 'A **Gated Recurrent Unit **(**GRU**) is a type of recurrent block that was
    introduced in 2014 (*Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078) and *Empirical
    Evaluation of Gated Recurrent Neural Networks on Sequence Modeling*, [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555))
    as an improvement over LSTM. A GRU unit usually has similar or better performance
    than an LSTM, but it does so with fewer parameters and operations:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控循环单元（GRU）**是一种递归单元块，首次提出于 2014 年（*使用 RNN 编码器-解码器进行统计机器翻译的学习短语表示*，[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    和 *门控递归神经网络在序列建模中的经验评估*，[https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)），它是对
    LSTM 的改进。GRU 单元通常具有与 LSTM 相似或更好的性能，但它以更少的参数和操作实现：'
- en: '![](img/1a0ea6ee-84f5-4e7a-8d8f-d9a8deaf2e10.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a0ea6ee-84f5-4e7a-8d8f-d9a8deaf2e10.png)'
- en: A GRU cell
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 GRU 单元
- en: 'Similar to the *classic* RNN, a GRU cell has a single hidden state, **h***[t]*.
    You can think of it as a combination of the hidden and cell states of an LSTM.
    The GRU cell has two gates:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 *经典* RNN，GRU 单元有一个隐藏状态，**h**[t]。你可以将其视为 LSTM 的隐藏状态和单元状态的组合。GRU 单元有两个门：
- en: 'An update gate, **z***[t]*, which combines the input and forget LSTM gates. It
    decides what information to discard and what new information to include in its
    place, based on the network input, **x***[t]*, and the previous cell hidden state, **h***[t-1]*.
    By combining the two gates, we can ensure that the cell will forget information,
    but only when we are going to include new information in its place:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个更新门，**z**[t]，它结合了输入门和忘记门。它根据网络输入 **x**[t] 和前一个单元隐藏状态 **h**[t-1] 来决定丢弃哪些信息，以及替换哪些新信息。通过结合这两个门，我们可以确保单元忘记信息，但只有在我们打算用新信息替代时：
- en: '![](img/c9f1074d-3c22-42ef-bf8b-885f7c8246f9.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9f1074d-3c22-42ef-bf8b-885f7c8246f9.png)'
- en: 'A reset gate, **r***[t]*, which uses the previous cell state, **h***[t-1]*,
    and the network input, **x***[t]*, to decide how much of the previous state to
    pass through:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个重置门，**r**[t]，它利用前一个单元状态 **h**[t-1] 和网络输入 **x**[t] 来决定通过多少前一状态：
- en: '![](img/63df3872-1fcc-41dd-aeff-9dd0960de4af.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63df3872-1fcc-41dd-aeff-9dd0960de4af.png)'
- en: 'Next, we have the candidate state, **h***''[t]*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有候选状态，**h**'[t]：
- en: '![](img/20e8ab88-8f11-4a81-86b9-a9a7a57952ce.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20e8ab88-8f11-4a81-86b9-a9a7a57952ce.png)'
- en: 'Finally, the GRU output, **h***[t]*, at time *t* is an element-wise sum between the
    previous output, **h***[t−1]*, and the candidate output, **h***''[t]*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，GRU 在时间 *t* 的输出，**h**[t]，是前一输出 **h**[t−1] 和候选输出 **h**'[t] 之间的逐元素和：
- en: '![](img/b4f6da4f-4787-48f8-891f-0194aad6be2b.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4f6da4f-4787-48f8-891f-0194aad6be2b.png)'
- en: Since the update gate allows us to both forget and store data, it is directly
    applied over the previous output, **h[t]***[−1]*, and applied over the candidate
    output, **h***'[t]*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更新门允许我们同时忘记和存储数据，因此它直接作用于先前的输出，**h[t]**[−1]，并作用于候选输出，**h**'[t]。
- en: Implementing GRUs
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 GRU
- en: 'In this section, we''ll implement a GRU cell with PyTorch 1.3.1 by following
    the blueprint from the *Implementing LSTM* section. Let''s get started:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过遵循 *实现 LSTM* 部分的蓝图，使用 PyTorch 1.3.1 实现 GRU 单元。让我们开始吧：
- en: 'First, we''ll do the imports:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将进行导入：
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we''ll write the class definition and the `init` method. In LSTM, we
    were able to create a shared fully connected layer for all gates, because each
    gate required the same input combination of **x**[*t*] and **h**[*t-1*]. The GRU
    gates use different inputs, so we''ll create separate fully connected operations
    for each GRU gate:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将编写类定义和 `init` 方法。在 LSTM 中，我们能够为所有门创建共享的全连接层，因为每个门都需要相同的输入组合：**x**[*t*]
    和 **h**[*t-1*]。GRU 的门使用不同的输入，因此我们将为每个 GRU 门创建单独的全连接操作：
- en: '[PRE25]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We'll omit the definition of `reset_parameters` because it's the same as it
    is in `LSTMCell`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将省略 `reset_parameters` 的定义，因为它与 `LSTMCell` 中相同。
- en: 'Then, we''ll implement the `forward` method with the cell by following the
    steps we described in the *Gated recurrent units* section. The method takes the
    current input vector, **x***[t]*, and the previous cell state/output, **h***[t-1]*,
    as input. First, we''ll compute the forget and update gates, similar to how we
    computed the gates in the LSTM cell:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将按照 *门控循环单元* 部分中描述的步骤，使用单元实现 `forward` 方法。该方法将当前输入向量，**x**[t]，和先前的单元状态/输出，**h**[t-1]，作为输入。首先，我们将计算忘记门和更新门，类似于我们在
    LSTM 单元中计算门的方式：
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we''ll compute the new candidate state/output, which uses the reset gate:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将计算新的候选状态/输出，它使用重置门：
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we''ll compute the new output based on the candidate state and the
    update gate:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将根据候选状态和更新门计算新的输出：
- en: '[PRE28]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can implement the counting of ones task with a GRU cell in the same way that
    we did with LSTM. To avoid repetition, we won't include the implementation here,
    but it is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像实现LSTM一样，使用GRU单元来实现计数任务。为了避免重复，我们在此不包括实现，但可以在[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py)找到相应的代码。
- en: This concludes our discussion about various types of RNNs. Next, we'll channel
    this knowledge by implementing a text sentiment analysis example.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这也结束了我们关于各种RNN类型的讨论。接下来，我们将通过实现一个文本情感分析的例子来运用这些知识。
- en: Implementing text classification
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现文本分类
- en: Let's recap on this chapter so far. We started by implementing an RNN using
    only `numpy`. Then, we continued with an LSTM implementation using primitive PyTorch
    operations. We'll conclude this arc by training the default PyTorch 1.3.1 LSTM
    implementation for a text classification problem. This example also requires the
    `torchtext` 0.4.0 package. Text classification (or categorization) refers to the
    task of assigning categories (or labels) depending on its contents. Text classification
    tasks include spam detection, topic labeling, and sentiment analysis. This type
    of problem is an example of a *many-to-one* relationship, which we defined in
    the *Introduction to RNNs* section.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下目前为止的内容。我们首先使用`numpy`实现了一个RNN。接着，我们继续使用原始的PyTorch操作实现了一个LSTM。我们将通过训练默认的PyTorch
    1.3.1 LSTM实现来解决文本分类问题，从而结束这一部分的内容。这个例子还需要`torchtext` 0.4.0包。文本分类（或标记）是指根据内容为文本分配类别（或标签）的任务。文本分类任务包括垃圾邮件检测、主题标签和情感分析。这类问题是一个*多对一*关系的例子，我们在*RNN简介*部分有过定义。
- en: In this section, we'll implement a sentiment analysis example over the Large
    Movie Review Dataset ([http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)),
    which consists of 25,000 training and 25,000 testing reviews of popular movies.
    Each review has a binary label that indicates whether it is positive or negative.
    Besides PyTorch, we'll use the `torchtext` package ([https://torchtext.readthedocs.io/](https://torchtext.readthedocs.io/)).
    It consists of data processing utilities and popular datasets for natural language.
    You'll also need to install the `spacy` open source software library ([https://spacy.io](https://spacy.io))
    for advanced NLP, which we'll use to tokenize the dataset.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在大型电影评论数据集上实现情感分析示例（[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)），该数据集包含25,000个训练评论和25,000个测试评论，均来自流行电影。每个评论都有一个二进制标签，指示其是正面还是负面。除了PyTorch，我们还将使用`torchtext`包（[https://torchtext.readthedocs.io/](https://torchtext.readthedocs.io/)）。它包含数据处理工具和自然语言处理的流行数据集。你还需要安装`spacy`开源软件库（[https://spacy.io](https://spacy.io)）以进行高级NLP处理，我们将使用它来对数据集进行分词。
- en: 'The sentiment analysis algorithm is displayed in the following diagram:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析算法如下图所示：
- en: '![](img/3a5f7310-9b5a-4cce-b31d-90cdd382136c.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a5f7310-9b5a-4cce-b31d-90cdd382136c.png)'
- en: Sentiment analysis with word embeddings and LSTM
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词嵌入和LSTM进行情感分析
- en: 'Let''s describe the algorithm steps (these are valid for any text classification
    algorithm):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述算法的步骤（这些步骤适用于任何文本分类算法）：
- en: Each word of the sequence is replaced with its embedding vector ([Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml),
    *Language Modeling*). These embeddings can be produced with word2vec, fastText,
    GloVe, and so on.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列中的每个单词都用其嵌入向量进行替换（[第6章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)，*语言建模*）。这些嵌入可以通过word2vec、fastText、GloVe等方法生成。
- en: The word embedding is fed as input to the LSTM cell.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词嵌入作为输入提供给LSTM单元。
- en: The cell output, **h***[t]*, serves as input to a fully connected layer with
    a single output unit. The unit uses sigmoid activation, which represents the probability
    of the review to be positive (1) or negative (0). If the problem is multinomial
    (and not binary), we can replace the sigmoid with softmax.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单元格输出，**h***[t]*，作为输入传递给一个具有单一输出单元的全连接层。该单元使用 sigmoid 激活函数，表示评论为正面（1）或负面（0）的概率。如果问题是多项式的（而非二分类问题），我们可以用
    softmax 替换 sigmoid。
- en: The network output for the final element of the sequence is taken as a result
    for the whole sequence.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列的最后一个元素的网络输出被视为整个序列的结果。
- en: Now that we have provided an overview of the algorithm, let's implement it.
    We'll only include the interesting portions of the code, but the full implementation
    is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py)[. ](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了算法，让我们开始实现它。我们只会包括代码中的有趣部分，完整的实现可以在 [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py)[. ](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py)
- en: This example is partially based on [https://github.com/bentrevett/pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例部分基于 [https://github.com/bentrevett/pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)。
- en: 'Let''s get started:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'First, we''ll add the imports:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将添加导入：
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we''ll instantiate a `torchtext.data.Field` object:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化一个 `torchtext.data.Field` 对象：
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This object declares a text processing pipeline, which starts with the raw text
    and outputs a tensor representation of the text. More specifically, it uses the
    `spacy` tokenizer, converts all of the letters into lowercase, and includes the
    length (in words) of each movie review.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该对象声明了一个文本处理流水线，从原始文本开始，输出文本的张量表示。更具体地说，它使用 `spacy` 分词器，将所有字母转换为小写，并包括每个电影评论的单词数（长度）。
- en: 'Then, we''ll do the same for the labels (positive or negative):'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将对标签（正面或负面）做同样的处理：
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we''ll instantiate the training and testing dataset splits:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化训练和测试数据集的拆分：
- en: '[PRE32]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The movie review dataset is included in `torchtext` and we don't need to do
    any additional work. The `splits` method takes the `TEXT` and `LABEL` fields as
    parameters. By doing this, the specified pipelines are applied over the selected
    dataset.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 电影评论数据集已包含在 `torchtext` 中，我们无需做任何额外的工作。`splits` 方法将 `TEXT` 和 `LABEL` 字段作为参数。这样，指定的流水线将应用于选定的数据集。
- en: 'Then, we''ll instantiate the vocabulary:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们将实例化词汇表：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The vocabulary presents a mechanism for the numerical representation of the
    words. In this case, the numerical representation of the `TEXT` field is a pretrained
    100d GloVe vector. On the other hand, the labels in the dataset have a string
    value of either `pos` or `neg`. The role of the vocabulary here is to assign numbers
    (0 and 1) to these two labels.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表提供了一个将单词转换为数字表示的机制。在此情况下，`TEXT` 字段的数字表示为预训练的 100d GloVe 向量。另一方面，数据集中的标签具有
    `pos` 或 `neg` 的字符串值。词汇表在这里的作用是为这两个标签分配数字（0 和 1）。
- en: 'Next, we''ll define iterators for the training and testing datasets, where
    `device` represents either GPU or CPU. The iterators will return one mini-batch
    at each call:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将为训练和测试数据集定义迭代器，其中 `device` 代表 GPU 或 CPU。每次调用时，迭代器将返回一个小批次：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We''ll proceed by implementing and instantiating the `LSTMModel` class. This
    is at the core of the program, which implements the algorithm steps we defined
    in the diagram at the beginning of this section:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现并实例化 `LSTMModel` 类。这是程序的核心，执行我们在本节开头的图表中定义的算法步骤：
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`LSTMModel` processes a mini-batch of sequences (in this case, movie reviews)
    with varying lengths. However, the mini-batch is a tensor, which assigns slices
    with equal length for each sequence. Because of this, all of the sequences are
    padded in advance with a special symbol to reach the length of the longest sequence
    in the batch. The `padding_idx` parameter in the constructor of `torch.nn.Embedding`
    represents the index of the padding symbol in the vocabulary. But using sequences
    with padding will lead to unnecessary calculations for the padded portions. Because
    of this, the forward propagation of the model takes both the `text` mini-batch
    and `text_lengths` of each sequence as parameters. They are fed to the `pack_padded_sequence`
    function, which transforms them into a `packed_sequence` object. We do all of
    this because the `self.rnn` object (the instance of `torch.nn.LSTM`) has a special
    routine for processing packed sequences, which optimizes the computation with
    respect to the padding.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTMModel` 处理一个具有不同长度的序列小批量（在这个例子中是电影评论）。然而，mini-batch 是一个张量，它为每个序列分配了等长的切片。因此，所有序列都会预先使用特殊符号进行填充，以达到批次中最长序列的长度。`torch.nn.Embedding`
    构造函数中的 `padding_idx` 参数表示词汇表中填充符号的索引。但是，使用填充序列会导致对填充部分进行不必要的计算。为了避免这一点，模型的前向传播接受
    `text` mini-batch 和每个序列的 `text_lengths` 作为参数。它们会被传递给 `pack_padded_sequence` 函数，该函数将它们转换为
    `packed_sequence` 对象。我们之所以这样做，是因为 `self.rnn` 对象（即 `torch.nn.LSTM` 的实例）有一个专门处理打包序列的例程，从而优化了填充部分的计算。'
- en: 'Next, we''ll copy the GloVe word embedding vectors to the embedding layer of the
    model:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将 GloVe 词嵌入向量复制到模型的嵌入层：
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we''ll set the embedding entries for the padding and unknown tokens to
    zeros so that they don''t influence the propagation:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将把填充和未知标记的嵌入项设置为零，以避免它们对传播产生影响：
- en: '[PRE37]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we can run the whole thing with the following code (the `train_model`
    and `test_model` functions are the same as they were previously):'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以运行以下代码来执行整个过程（`train_model` 和 `test_model` 函数与之前相同）：
- en: '[PRE38]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If everything works as intended, the model will achieve a test accuracy of around
    88%.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按预期工作，模型将达到大约 88% 的测试准确率。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed RNNs. First, we started with the RNN and backpropagation
    through time theory. Then, we implemented an RNN from scratch to solidify our
    knowledge on the subject. Next, we moved on to more complex LSTM and GRU cells
    using the same pattern: a theoretical explanation, followed by a practical PyTorch
    implementation. Finally, we combined our knowledge from [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml),
    *Language Modeling*, with the new material from this chapter for a full-featured
    sentiment analysis task implementation.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 RNN。首先，我们从 RNN 和时间反向传播理论开始。然后，我们从头实现了一个 RNN，以巩固我们对该主题的知识。接下来，我们使用相同的模式，逐步深入到更复杂的
    LSTM 和 GRU 单元：先进行理论解释，再进行 PyTorch 实现。最后，我们将[第六章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)《语言建模》中的知识与本章的新内容结合，完成了一个功能齐全的情感分析任务实现。
- en: In the next chapter, we'll discuss seq2seq models and their variations—an exciting
    new development in sequence processing.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论 seq2seq 模型及其变种——序列处理中的一个令人兴奋的新发展。
