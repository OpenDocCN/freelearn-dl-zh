- en: TensorFlow on Mobile with Speech-to-Text with the WaveNet Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用WaveNet模型进行语音转文本的TensorFlow移动端实现
- en: In this chapter, we are going to learn how to convert audio to text using the
    WaveNet model. We will then build a model that will take audio and convert it
    into text using an Android application.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用WaveNet模型将音频转换为文本。然后，我们将构建一个模型，使用Android应用程序将音频转换为文本。
- en: 'This chapter is based on the *WaveNet: A Generative Model for Raw Audio* paper,
    by Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
    Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. You can find
    this paper at [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '本章基于Aaron van den Oord、Sander Dieleman、Heiga Zen、Karen Simonyan、Oriol Vinyals、Alex
    Graves、Nal Kalchbrenner、Andrew Senior和Koray Kavukcuoglu的论文《*WaveNet: A Generative
    Model for Raw Audio*》。您可以在[https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)找到这篇论文。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: WaveNet and how it works
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WaveNet及其工作原理
- en: The WaveNet architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WaveNet架构
- en: Building a model using WaveNet
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用WaveNet构建模型
- en: Preprocessing datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集预处理
- en: Training the WaveNet network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练WaveNet网络
- en: Transforming a speech WAV file into English text
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将语音WAV文件转换为英文文本
- en: Building an Android application
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Android应用程序
- en: Let's dig deeper into what Wavenet actually is.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨WaveNet究竟是什么。
- en: WaveNet
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WaveNet
- en: WaveNet is a deep generative network that is used to generate raw audio waveforms.
    Sounds waves are generated by WaveNet to mimic the human voice. This generated
    sound is more natural than any of the currently existing text-to-speech systems,
    reducing the gap between system and human performance by 50%.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet是一个深度生成网络，用于生成原始音频波形。WaveNet通过模拟人声来生成声音波形。与当前任何现有的语音合成系统相比，这种生成的声音更为自然，缩小了系统与人类表现之间的差距达50%。
- en: With a single WaveNet, we can differentiate between multiple speakers with equal
    fidelity. We can also switch between individual speakers based on their identity.
    This model is autoregressive and probabilistic, and it can be trained efficiently
    on thousands of audio samples per second. A single WaveNet can capture the characteristics
    of many different speakers with equal fidelity, and can switch between them by
    conditioning the speaker identity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一WaveNet，我们可以以相同的保真度区分多个说话者。我们还可以根据说话者的身份在不同的说话者之间切换。该模型是自回归的和概率性的，可以在每秒处理成千上万的音频样本时高效训练。单一WaveNet能够以相同的保真度捕捉多个不同说话者的特点，并且可以通过条件化说话者身份来在它们之间切换。
- en: As shown in the movie *Her*, the long-standing dream of human-computer interaction
    is to allow people to talk to machines. The computer's ability to understand voices
    has increased tremendously over the past few years as a result of deep neural
    networks (for example, Google Assistant, Siri, Alexa, and Cortana). On the other
    hand, to generate speech with computers, a process referred to as speech synthesis
    or text to speech is followed. In the text-to-speech method, a large database
    of short sound fragments are recorded by a single speaker and then combined to
    form the required utterances. This process is very difficult because we can't
    change the speaker.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如电影《她》中所示，人机交互的长期梦想就是让人类与机器对话。近年来，随着深度神经网络（例如，Google助手、Siri、Alexa和Cortana）的发展，计算机理解语音的能力大幅提高。另一方面，计算机生成语音的过程被称为语音合成或文本转语音。在文本转语音方法中，一个说话者录制大量短音频片段，然后将这些片段拼接成所需的语句。这一过程非常困难，因为我们不能更换说话者。
- en: This difficulty has led to a great need for other methods of generating speech,
    where all the information that is needed for generating the data is stored in
    the parameters of the model. Additionally, using the inputs that are given to
    the model, we can control the contents and various attributes of speech. When
    speech is generated by adding sound fragments together, attribution graphs are
    generated.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一难题促使了对其他生成语音方法的巨大需求，其中生成数据所需的所有信息都存储在模型的参数中。此外，使用输入数据，我们可以控制语音的内容和各种属性。当通过拼接音频片段生成语音时，会生成归属图。
- en: 'The following is the attribution graph of speech that is generated in **1 second**:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成的**1秒**语音归属图：
- en: '![](img/6b3c68bc-06ff-4998-9a13-f5d1541ce051.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b3c68bc-06ff-4998-9a13-f5d1541ce051.png)'
- en: 'The following is the attribution graph of speech that is generated in **100
    milliseconds**:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成的**100毫秒**语音归属图：
- en: '![](img/aa119a92-08a6-4354-820c-5b7a2b351b3e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa119a92-08a6-4354-820c-5b7a2b351b3e.png)'
- en: 'The following is the attribution graph of speech that is generated in **10
    milliseconds**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成**10毫秒**语音的归因图：
- en: '![](img/c40a3332-995c-4e3b-904f-0e5108ee9f50.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c40a3332-995c-4e3b-904f-0e5108ee9f50.png)'
- en: 'The following is the attribution graph of speech that is generated in **1 millisecond**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成**1毫秒**语音的归因图：
- en: '![](img/81b3eadd-6104-4a28-8e83-631eed845c03.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81b3eadd-6104-4a28-8e83-631eed845c03.png)'
- en: 'The **Pixel Recurrent Neural Network** (**PixelRNN**) and **Pixel C****onvolutional
    Neural Network** (**PixelCNN**) models from Google ensure that it''s possible
    to generate images that include complex formations – not by generating one pixel
    at a time, but by an entire color channel altogether. At any one time, a color
    channel will need at least a thousand predictions per image. This way, we can
    alter a two-dimensional PixelNet into a one-dimensional WaveNet; this idea is
    shown in the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的**像素递归神经网络**（**PixelRNN**）和**像素卷积神经网络**（**PixelCNN**）模型确保能够生成包含复杂形态的图像——不是一次生成一个像素，而是同时生成整个颜色通道。在任何时刻，一个颜色通道至少需要对每个图像进行一千次预测。通过这种方式，我们可以将二维的PixelNet转换为一维的WaveNet；这一思路如图所示：
- en: '![](img/d8f342fa-40b2-47a9-bb24-c3764be2bacd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8f342fa-40b2-47a9-bb24-c3764be2bacd.png)'
- en: The preceding diagram displays the structure of a WaveNet model. WaveNet is
    a full CNN, in which the convolutional layers include a variety of dilation factors.
    These factors help the receptive field of WaveNet to grow exponentially with depth,
    and it also helps to cover thousands of time steps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了WaveNet模型的结构。WaveNet是一个完整的卷积神经网络（CNN），其中卷积层包含多种扩张因子。这些因子帮助WaveNet的感受野随着深度的增加呈指数增长，并且有助于覆盖成千上万个时间步长。
- en: During training, the human speaker records the input sequences to create waveforms.
    Once the training is complete, we generate synthetic utterances by sampling the
    network. A value is taken from the probability distribution which is computed
    by the network at each step of sampling. The value that's received is fed as the
    input for the next step, and then a new prediction is made. Building these samples
    at each step is expensive; however, it's necessary to generate complex and realistic-sounding
    audio.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，人工语音记录输入序列以创建波形。训练完成后，我们通过从网络中采样生成合成语音。每次采样时，从网络计算出的概率分布中选取一个值。接收到的值作为下一步的输入，然后进行新的预测。在每一步生成这些样本是非常昂贵的；然而，为了生成复杂且逼真的音频，这是必需的。
- en: More information about PixelRNN can be found at [https://arxiv.org/pdf/1601.06759.pdf](https://arxiv.org/pdf/1601.06759.pdf),
    while information about *Conditional Image Generation with PixelCNN Decoders*
    can be found at [https://arxiv.org/pdf/1606.05328.pdf](https://arxiv.org/pdf/1606.05328.pdf).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有关PixelRNN的更多信息，请访问[https://arxiv.org/pdf/1601.06759.pdf](https://arxiv.org/pdf/1601.06759.pdf)，而关于*条件图像生成与PixelCNN解码器*的信息请访问[https://arxiv.org/pdf/1606.05328.pdf](https://arxiv.org/pdf/1606.05328.pdf)。
- en: Architecture
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: The architecture of WaveNet neural networks shows amazing outputs by generating
    audio and text-to-speech translations, since it directly produces a raw audio
    waveform.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet神经网络的架构通过生成音频和语音合成翻译展示了惊人的效果，因为它直接生成原始音频波形。
- en: When the previous samples and additional parameters are given as the input,
    the network produces the next sample in the form of an audio waveform using conditional
    probability.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入包括之前的样本和额外的参数时，网络通过条件概率生成下一个音频样本，输出为音频波形。
- en: The waveform that is given as the input is quantized to a fixed range of integers.
    This happens after the audio is preprocessed. The tensors are produced by one-hot
    encoding these integer amplitudes. Hence, the dimensions of the channel are reduced
    by the convolutional layer that only accesses the current and previous inputs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入的波形被量化为固定范围的整数。这发生在音频预处理之后。通过对这些整数幅度进行独热编码，生成张量。因此，卷积层仅访问当前和先前的输入，从而减少了通道的维度。
- en: 'The following diagram displays the WaveNet architecture:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是WaveNet架构的示意图：
- en: '![](img/bf310496-903e-4995-8f65-fb92e47535cc.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf310496-903e-4995-8f65-fb92e47535cc.png)'
- en: A stack of causal dilated layers is used to build the network core. Each layer
    is a dilated convolution with holes, and it accesses only the past and current
    audio samples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一堆因果扩张卷积层来构建网络核心。每一层都是一个带孔的扩张卷积层，仅访问过去和当前的音频样本。
- en: Then, the outputs that are received from all the layers are combined and, using
    an array of dense postprocessing layers, they are fed to the original channels.
    Later, the softmax function converts the output into a categorical distribution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，所有层接收到的输出被合并，并通过一系列密集的后处理层，输入到原始通道中。之后，softmax函数将输出转换为分类分布。
- en: The loss function is calculated as the cross entropy between the output for
    each time step and the input at the next time step.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是通过计算每个时间步的输出与下一个时间步输入之间的交叉熵来得到的。
- en: Network layers in WaveNet
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WaveNet中的网络层
- en: Here, we will focus on generating dilated causal convolution network layers
    with the filter size of two. Note that these ideas are relevant to larger filter
    sizes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将重点介绍生成过滤器大小为2的扩张因果卷积网络层。注意，这些概念也适用于更大的过滤器大小。
- en: 'During this generation, the computational graph that''s used to compute a single
    output value can be seen as a binary tree:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在此生成过程中，用于计算单个输出值的计算图可以看作是一个二叉树：
- en: '![](img/0bf29096-a2fc-4312-94bf-6ff9d81ebe45.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bf29096-a2fc-4312-94bf-6ff9d81ebe45.png)'
- en: The **Input** nodes on the bottom layer of the diagram are the leaves of the
    tree, while the **Output** layer is the root. The intermediate computations are
    represented by the nodes above the **Input** layer. The edges of the graph correspond
    to multiple matrices. Since the computation is a binary tree, the overall computation
    time for the graph is *O(2^L)*. When *L* is large, the computation exponentially
    shoots up.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图中底层的**输入**节点是树的叶子节点，而**输出**层是根节点。中间的计算由**输入**层上方的节点表示。图中的边对应于多个矩阵。由于计算是二叉树结构，因此图的总体计算时间为*O(2^L)*。当*L*较大时，计算时间会呈指数级增长。
- en: However, since this model is being applied repeatedly over time, there is a
    lot of redundant computation, which we can cache to increase the speed of generating
    a single sample.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于该模型是随着时间反复应用的，因此存在大量的冗余计算，我们可以缓存这些计算以提高生成单个样本的速度。
- en: 'The key insight is this – given certain nodes in the graph, we have all the
    information that we need to compute the current output. We call these nodes **recurrent
    states** by using the analogy of RNNs. These nodes have already been computed,
    so all we need to do is cache them on the different layers, as shown in the following
    diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的洞察是这样的——给定图中的某些节点，我们已经拥有计算当前输出所需的所有信息。我们通过类比RNNs，称这些节点为**递归状态**。这些节点已经被计算过，因此我们所需要做的就是将它们缓存到不同的层中，如下图所示：
- en: '![](img/b45b4536-8ba4-4a1d-a815-f6dfcca66d84.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b45b4536-8ba4-4a1d-a815-f6dfcca66d84.png)'
- en: 'Note that at the next time point, we will need a different subset of recurrent
    states. As a result, we will need to cache several recurrent states per layer.
    The number we need to keep is equal to the dilation of that layer, as shown in
    the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在下一个时间点，我们将需要不同子集的递归状态。因此，我们需要在每一层缓存多个递归状态。我们需要保留的数量等于该层的扩张值，如下图所示：
- en: '![](img/86c05505-5b7f-42e1-9818-403ede26646a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86c05505-5b7f-42e1-9818-403ede26646a.png)'
- en: As shown in the preceding diagram with arrow marks, the number of recurrent
    states is the same as the dilation value in the layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，带有箭头标记的地方，递归状态的数量与层中的扩张值相同。
- en: The algorithm's components
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法的组件
- en: 'The algorithm behind building a speech detector has two components:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 构建语音检测器的算法包含两个组件：
- en: '**The generation model**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**'
- en: '**The convolution queues**'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积队列**'
- en: 'These two components are shown in the following diagram:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个组件在下图中展示：
- en: '![](img/3654f9ba-3c2d-4ff7-8c5a-652e55f1b401.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3654f9ba-3c2d-4ff7-8c5a-652e55f1b401.png)'
- en: The generation model can be viewed as one step of an RNN. It takes the current
    observation and several recurrent states as input, and then computes the output
    prediction and new recurrent states.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以视为RNN的一个步骤。它以当前观测值和若干递归状态作为输入，然后计算输出预测和新的递归状态。
- en: The convolution queues store the new recurrent states that have been computed
    by the layer underneath it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积队列存储了由下面的层计算得到的新递归状态。
- en: Let's jump into building the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建模型。
- en: Building the model
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'We will implement sentence-level English speech recognition using DeepMind''s
    WaveNet. However, we need to consider a number of data points before building
    the model:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用DeepMind的WaveNet实现句子级别的英语语音识别。但是，在构建模型之前，我们需要考虑一些数据点：
- en: First, while the paper on WaveNet (provided at the beginning of this chapter)
    used the TIMIT dataset for the speech recognition experiment, we will use the
    free VCTK dataset instead.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，虽然WaveNet论文（在本章开头提供）使用了TIMIT数据集进行语音识别实验，但我们将使用免费的VCTK数据集代替。
- en: Second, the paper added a mean pooling layer after the dilated convolution layer
    for downsampling. We have extracted **mel-frequency cepstral coefficients** (**MFCC**)
    from the `.wav` files and removed the final mean pooling layer because the original
    setting is impossible to run on our TitanX **Graphics Processing Unit** **GPU**).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，论文在扩张卷积层后添加了一个均值池化层进行降采样。我们已经从`.wav`文件中提取了**梅尔频率倒谱系数**（**MFCC**），并删除了最后的均值池化层，因为原始设置在我们的TitanX
    **图形处理单元**（**GPU**）上无法运行。
- en: 'Third, since the TIMIT dataset has phoneme labels, the paper trained the model
    with two loss terms: **phoneme classification** and **next phoneme prediction**.
    Instead, we will use a single **connectionist temporal classification** (**CTC**)
    loss because VCTK provides sentence-level labels. As a result, we only use dilated
    Conv1D layers without any dilated Conv1D layers.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，由于TIMIT数据集具有音素标签，论文使用了两个损失项来训练模型：**音素分类**和**下一个音素预测**。相反，我们将使用单一的**连接时序分类**（**CTC**）损失，因为VCTK提供了句子级标签。因此，我们只使用扩张的Conv1D层，而没有任何扩张的Conv1D层。
- en: Finally, we won't do quantitative analyses, such as the **bilingual evaluation
    understudy score** (**BLEU**) score and postprocessing by combining a language
    model, due to time constraints.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，由于时间限制，我们不会进行定量分析，如**双语评估下游评分**（**BLEU**）得分以及通过结合语言模型的后处理。
- en: Dependencies
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖项
- en: 'Here is a list of all the dependency libraries that will need to be installed
    first:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是所有需要首先安装的依赖库列表：
- en: '`tensorflow`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow`'
- en: '`sugartensor`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sugartensor`'
- en: '`pandas`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`librosa`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`librosa`'
- en: '`scikits.audiolab`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikits.audiolab`'
- en: If you have problems with the `librosa` library, you can try installing `ffmpeg`
    using `pip`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到`librosa`库的问题，可以尝试使用`pip`安装`ffmpeg`。
- en: Datasets
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: We used the VCTK, LibriSpeech, and TED-LIUM release 2 datasets. The total number
    of sentences in the training set are composed of the previous three datasets,
    which equals 240,612 sentences. The validation and test sets are built using only
    LibriSpeech and the TED-LIUM corpus, because the VCTK corpus does not have validation
    and test sets. After downloading each corpus, extract them in the `asset/data/VCTK-Corpus`,
    `asset/data/LibriSpeech`, and `asset/data/TEDLIUM_release2` directories.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了VCTK、LibriSpeech和TED-LIUM发布2的数据集。训练集中的句子总数由这三个数据集组成，总计240,612个句子。验证集和测试集仅使用LibriSpeech和TED-LIUM语料库构建，因为VCTK语料库没有验证集和测试集。下载每个语料库后，将它们提取到`asset/data/VCTK-Corpus`、`asset/data/LibriSpeech`和`asset/data/TEDLIUM_release2`目录中。
- en: 'You can find the links to these datasets here: *CSTR VCTK Corpus:* [http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html)
    *LibriSpeech ASR corpus:* [http://www.openslr.org/12](http://www.openslr.org/12/)
    *TED-LIUM:* [http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus](https://lium.univ-lemans.fr/en/ted-lium2/)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到这些数据集的链接：*CSTR VCTK语料库:* [http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html)
    *LibriSpeech ASR语料库:* [http://www.openslr.org/12](http://www.openslr.org/12/)
    *TED-LIUM:* [http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus](https://lium.univ-lemans.fr/en/ted-lium2/)
- en: Preprocessing the dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集预处理
- en: 'The TED-LIUM release 2 dataset provides audio data in the SPH format, so we
    should convert it into a format that the `librosa` library can handle. To do this,
    run the following command in the `asset/data` directory to convert the SPH format
    into the WAV format:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: TED-LIUM发布2数据集提供的是SPH格式的音频数据，因此我们应该将其转换为`librosa`库能够处理的格式。为此，请在`asset/data`目录下运行以下命令，将SPH格式转换为WAV格式：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you don't have `sox` installed,please install it first.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有安装`sox`，请先安装它。
- en: We found that the main bottleneck is the disk read time when training because
    of the size of the audio files. It is better to have smaller audio files before
    processing for faster execution. So, we have decided to preprocess the whole audio
    data into the MFCC feature files, which are much smaller. Additionally, we highly
    recommend using a **solid-state drive** (**SSD**) instead of a hard drive.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现训练中的主要瓶颈是磁盘读取时间，因为音频文件的大小。在处理之前，最好将音频文件缩小，以便更快地执行。因此，我们决定将所有音频数据预处理为MFCC特征文件，这些文件要小得多。此外，我们强烈建议使用**固态硬盘**（**SSD**），而不是传统硬盘。
- en: 'Run the following command in the console to preprocess the whole dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中运行以下命令以预处理整个数据集：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With the processed audio files, we can now train the network.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用处理过的音频文件，我们现在可以开始训练网络。
- en: Training the network
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'We will start training the network by executing the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过执行以下命令开始训练网络：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you are using a machine with CUDA enabled, use the following command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用启用了CUDA的机器，请使用以下命令：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can see the resulting `.ckpt` files and log files in the `asset/train` directory.
    Launch `tensorboard--logdir asset/train/log` to monitor the training process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`asset/train`目录中看到生成的`.ckpt`文件和日志文件。启动`tensorboard--logdir asset/train/log`来监控训练过程。
- en: We've trained this model on a 3 Nvidia 1080 Pascal GPU for 40 hours until 50
    epochs were reached, and then we picked the epoch when the validation loss is
    at a minimum. In our case, it is epoch 40\. If you can see the out-of-memory error,
    reduce `batch_size` in the `train.py` file from 16 to 4.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在3台Nvidia 1080 Pascal GPU上训练了这个模型40小时，直到达到50个纪元，然后选择了验证损失最小的纪元。在我们的例子中，这是第40个纪元。如果你看到内存溢出的错误，请在`train.py`文件中将`batch_size`从16减少到4。
- en: 'The CTC losses at each epoch are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每个纪元的CTC损失如下：
- en: '| **Epoch** | **Train set** | **Valid set** | **Test set** |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **纪元** | **训练集** | **验证集** | **测试集** |'
- en: '| 20 | 79.541500 | 73.645237 | 83.607269 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 79.541500 | 73.645237 | 83.607269 |'
- en: '| 30 | 72.884180 | 69.738348 | 80.145867 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 72.884180 | 69.738348 | 80.145867 |'
- en: '| 40 | 69.948266 | 66.834316 | 77.316114 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 69.948266 | 66.834316 | 77.316114 |'
- en: '| 50 | 69.127240 | 67.639895 | 77.866674 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 69.127240 | 67.639895 | 77.866674 |'
- en: Here, you can see the difference between the values from the training dataset
    and the testing dataset. The difference is largely due to the bigger volume of
    data in the training dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到训练数据集和测试数据集的值之间的差异。这个差异主要是由于训练数据集的体积更大。
- en: Testing the network
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试网络
- en: 'After training the network, you can check the validation or test set CTC loss
    by using the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络之后，你可以使用以下命令检查验证集或测试集的CTC损失：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `frac` option will be useful if you want to test only a fraction of the
    dataset for fast evaluation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想测试数据集的一部分以进行快速评估，`frac`选项将非常有用。
- en: Transforming a speech WAV file into English text
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一个语音WAV文件转换成英文文本
- en: 'Next, you can convert the speech WAV file into English text by executing the
    following command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以通过执行以下命令将语音WAV文件转换为英文文本：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will transform a speech WAV file into an English sentence.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把语音WAV文件转换为英文句子。
- en: 'The result will be printed on the console; try the following command as an
    example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将会打印在控制台上；尝试以下命令作为示例：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result will be as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '*he hoped there would be stoo for dinner turnips and charrats and bruzed patatos
    and fat mutton pieces to be ladled out in th thick peppered flower fatan sauce
    stuffid into you his belly counsiled him after early night fall the yetl lampse
    woich light hop here and there on the squalled quarter of the browfles o berty
    and he god in your mind numbrt tan fresh nalli is waiting on nou cold nit husband*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*他希望晚餐有炖菜，胡萝卜和萝卜，还有破损的土豆和肥羊肉块，放入浓厚的胡椒面调料中，一勺勺地倒进你肚子里。他劝告自己，在早早的夜幕降临后，黄色的灯光会在这里和那里亮起，肮脏的妓院区，嘿，伯蒂，脑袋里有好主意吗？10号新鲜的内莉在等你，晚安，丈夫。*'
- en: 'The ground truth is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 真实情况如下：
- en: '*HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES
    AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE
    STUFF IT INTO YOU HIS BELLY COUNSELLED HIM AFTER EARLY NIGHTFALL THE YELLOW LAMPS
    WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS HELLO BERTIE
    ANY GOOD IN YOUR MIND NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*他希望晚餐有炖菜，胡萝卜和萝卜，还有破损的土豆和肥羊肉块，放入浓厚的胡椒面调料中，一勺勺地倒进你肚子里。他劝告自己，在早早的夜幕降临后，黄色的灯光会在这里和那里亮起，肮脏的妓院区，嘿，伯蒂，脑袋里有好主意吗？10号新鲜的内莉在等你，晚安，丈夫。*'
- en: As we mentioned earlier, there is no language model, so there are some cases
    where capital letters and punctuation are misused, or words are misspelled.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，由于没有语言模型，因此在某些情况下会出现大写字母和标点符号误用，或者单词拼写错误。
- en: Getting the model
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取模型
- en: 'Unlike image problems, it''s not easy to find a pretrained deep learning model
    for speech-to-text that gives out checkpoints. Luckily, I found the following
    WaveNet speech-to-text implementation. To export the model for compression, I
    ran the Docker image, loaded the checkpoint, and wrote it into a protocol buffers
    file. To run this, use the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像问题不同，找到一个提供检查点的语音转文本预训练深度学习模型并不容易。幸运的是，我找到了以下的 WaveNet 语音转文本实现。为了将模型导出以进行压缩，我运行了
    Docker 镜像，加载了检查点，并将其写入了协议缓冲文件。运行此操作时，请使用以下命令：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will build the graph for inference, load the checkpoint, and write it into
    a protocol buffer file, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为推理构建图，加载检查点，并将其写入协议缓冲文件，如下所示：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we need to build a TensorFlow model and quantize the model so that it
    can be consumed in the mobile application.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要构建一个 TensorFlow 模型并对其进行量化，以便在移动应用中使用。
- en: Bazel build TensorFlow and quantizing the model
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Bazel 构建 TensorFlow 并量化模型
- en: 'To quantize the model with TensorFlow, you need to have Bazel installed and
    the cloned Tensorflow repository. I recommend creating a new virtual environment
    to install and build TensorFlow there. Once you''re done, you can run the following
    command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 TensorFlow 对模型进行量化，你需要安装 Bazel 并克隆 TensorFlow 仓库。我建议创建一个新的虚拟环境，在其中安装和构建
    TensorFlow。完成后，你可以运行以下命令：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can check out the official quantization tutorial on the TensorFlow website
    for other options in transforms. After quantization, the model was downsized by
    75%, from 15.5 MB to 4 MB due to the 8-bit conversion. Due to the time limit,
    I haven't calculated the letter error rate with a test set to quantify the accuracy
    drop before and after quantization.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看 TensorFlow 网站上的官方量化教程，了解变换中的其他选项。量化后，模型大小减小了 75%，从 15.5 MB 降到 4 MB，原因是进行了
    8 位转换。由于时间限制，我还没有通过测试集计算字母错误率，以量化量化前后准确率的下降。
- en: For a detailed discussion on neural network quantization, there is a great post
    by Pete Warden, called *Neural network quantization with TensorFlow* ([https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络量化的详细讨论，Pete Warden 写了一篇很好的文章，名为 *使用 TensorFlow 进行神经网络量化* ([https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/))。
- en: Note that you can also do a full 8-bit calculation graph transformation by following
    the instructions in this section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你也可以按照本节中的说明进行完整的 8 位计算图转换。
- en: The model's size is down to 5.9 MB after this conversion, and the inference
    time is doubled. This could be due to the fact that the 8-bit calculation is not
    optimized for the Intel i5 processor on the macOS platform, which was used to
    write the application.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 经过此转换后，模型的大小减少到 5.9 MB，推理时间加倍。这可能是由于 8 位计算没有针对 macOS 平台上的 Intel i5 处理器进行优化，而该平台用于编写应用程序。
- en: So, now that we have a compressed pretrained model, let's see what else we need
    to deploy the model on Android.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个压缩的预训练模型，让我们看看在 Android 上部署该模型还需要做些什么。
- en: TensorFlow ops registration
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 操作注册
- en: Here, we will build the TensorFlow model using Bazel to create a `.so` file
    that can be called by the **Java Native Interface** (**JNI**), and includes all
    the operation libraries that we need for the pretrained WaveNet model inference.
    We will use built model in the Android application.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 Bazel 构建 TensorFlow 模型，创建一个 `.so` 文件，该文件可以通过 **Java 原生接口**（**JNI**）调用，并包括我们在预训练
    WaveNet 模型推理中需要的所有操作库。我们将在 Android 应用程序中使用构建好的模型。
- en: To find out more about Bazel, you can refer the following link: [https://docs.bazel.build/versions/master/bazel-overview.html](https://docs.bazel.build/versions/master/bazel-overview.html).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于 Bazel 的信息，可以参考以下链接：[https://docs.bazel.build/versions/master/bazel-overview.html](https://docs.bazel.build/versions/master/bazel-overview.html)。
- en: Let's begin by editing the WORKSPACE file in the cloned TensorFlow repository
    by uncommenting and updating the paths to **Software** **Development** **Kit**
    (**SDK**) and **Native Development Kit** (**NDK**).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过取消注释并更新**软件** **开发** **工具包**（**SDK**）和**原生开发工具包**（**NDK**）的路径，编辑克隆的 TensorFlow
    仓库中的 WORKSPACE 文件。
- en: Next, we need to find out what ops were used in the pretrained model and generate
    a `.so` file with that piece of information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要找出在预训练模型中使用了哪些操作，并生成一个包含这些信息的 `.so` 文件。
- en: 'First, run the following command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，运行以下命令：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: All the ops in the `.pb` file will be listed in `ops_to_register.h`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`.pb`文件中的操作将列在`ops_to_register.h`中。
- en: 'Next, move `op_to_register.h` to `/tensorflow/tensorflow/core/framework/` and
    run the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，移动`op_to_register.h`到`/tensorflow/tensorflow/core/framework/`，并运行以下命令：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Unfortunately, while I didn''t get any error message, the `.so` file still
    didn''t include all the ops listed in the header file:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，虽然我没有收到任何错误消息，但是`.so`文件仍然没有包含在头文件中列出的所有操作：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you haven''t tried the first option and have got the list of ops in the
    model, you can get the ops by using the `tf.train.write_graph` command and typing
    the following into your Terminal:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未尝试第一个选项并且已经获取了模型中操作的列表，则可以通过使用`tf.train.write_graph`命令并在终端中键入以下内容来获取操作：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, edit the BUILD file by adding the missing ops into `android_extended_ops_group1` or
    `android_extended_ops_group2` in the Android libraries section. You can also make
    the `.so` file smaller by removing any unnecessary ops. Now, run the following
    command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在Android库部分的`android_extended_ops_group1`或`android_extended_ops_group2`中添加缺失的操作，并使`.so`文件更小化，删除任何不必要的操作。现在，运行以下命令：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You''ll find the `libtensorflow_inference.so` file, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在以下位置找到`libtensorflow_inference.so`文件：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that while running this command on Android, we ran into an error with the
    `sparse_to_dense` op. If you'd like to repeat this work, add `REGISTER_KERNELS_ALL(int64);` to
    `sparse_to_dense_op.cc` on line 153, and compile again.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Android上运行此命令时，我们遇到了与`sparse_to_dense`操作相关的错误。如果您希望重复此工作，请在第153行将`REGISTER_KERNELS_ALL(int64);`添加到`sparse_to_dense_op.cc`中，并重新编译。
- en: 'In addition to the `.so` file, we also need a JAR file. You can simply add
    this in the `build.gradle` file, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`.so`文件外，我们还需要一个JAR文件。您可以简单地将其添加到`build.gradle`文件中，如下所示：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Or, you can run the following command:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以运行以下命令：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You''ll find the file, as shown in the following code block:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您会在以下代码块中找到该文件：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, move both files into your Android project.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将这两个文件移动到您的Android项目中。
- en: Building an Android application
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个Android应用程序
- en: In this section, we are going to build an Android application that will convert
    the user's voice input into text. Essentially, we are going to build a speech-to-text
    converter. We have modified the TensorFlow speech example in the TensorFlow Android
    demo repository for this exercise.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个Android应用程序，将用户的语音输入转换为文本。基本上，我们将构建一个语音到文本的转换器。我们已经修改了TensorFlow语音示例，放置在TensorFlow
    Android演示库中以供本练习使用。
- en: You can find the TensorFlow Android demo application at [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)找到TensorFlow
    Android演示应用程序。
- en: The `build.gradle` file in the demo actually helps you build the `.so` and JAR
    files. So, if you'd like to start the demo examples with your own model, you can
    simply get the list of your ops, modify the BUILD file, and let the `build.gradle`
    file take care of the rest. We will get into the details of setting up the Android
    application in the following sections.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示的`build.gradle`文件实际上帮助您构建`.so`和JAR文件。因此，如果您想要使用自己的模型开始演示示例，您只需获取您的操作列表，修改BUILD文件，并让`build.gradle`文件处理其余部分。我们将在以下部分详细介绍设置Android应用程序的细节。
- en: Requirements
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求
- en: 'The requirements you will need to build the Android application are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 构建Android应用程序所需的要求如下：
- en: TensorFlow 1.13
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 1.13
- en: Python 3.7
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7
- en: NumPy 1.15
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 1.15
- en: python-speech-features
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: python-speech-features
- en: TensorFlow link: [https://github.com/tensorflow/tensorflow/releases](https://github.com/tensorflow/tensorflow/releases)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow链接：[https://github.com/tensorflow/tensorflow/releases](https://github.com/tensorflow/tensorflow/releases)
- en: Python link: [https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Python链接：[https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/)
- en: Numpy link: [https://docs.scipy.org/doc/numpy-1.13.0/user/install.html](https://docs.scipy.org/doc/numpy-1.13.0/user/install.html)
    Python-speech-features link: [https://github.com/jameslyons/python_speech_features](https://github.com/jameslyons/python_speech_features)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy链接：[https://docs.scipy.org/doc/numpy-1.13.0/user/install.html](https://docs.scipy.org/doc/numpy-1.13.0/user/install.html)
    Python-speech-features链接：[https://github.com/jameslyons/python_speech_features](https://github.com/jameslyons/python_speech_features)
- en: Now, let's start building the Android application from scratch. In this application,
    we will record audio and then convert it into text.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从头开始构建Android应用程序。在此应用程序中，我们将录制音频然后将其转换为文本。
- en: Set up Android Studio based on your operating system by going to the following
    link: [https://developer.android.com/studio/install](https://developer.android.com/studio/install).The
    code repository that's used in this project has been modified from the TensorFlow
    example provided here: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android.](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的操作系统设置Android Studio，方法是访问以下链接：[https://developer.android.com/studio/install](https://developer.android.com/studio/install)。本项目中使用的代码库已从此处提供的TensorFlow示例进行修改：[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android.](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)
- en: We will use the TensorFlow sample application and edit it according to our needs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow示例应用程序并根据我们的需求进行编辑。
- en: 'Add Application name and the Company domain name, as shown in the following
    screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 添加应用程序名称和公司域名，如下图所示：
- en: '![](img/5683c7fa-8f1d-4492-87c0-0cd7db2e0d83.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5683c7fa-8f1d-4492-87c0-0cd7db2e0d83.png)'
- en: 'In the next step, select the Target Android Devices version. We will select
    the minimum version as API 15:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，选择目标Android设备的版本。我们将选择最低版本为API 15：
- en: '![](img/c5033299-5f41-41d1-8d47-856fa7f342a4.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5033299-5f41-41d1-8d47-856fa7f342a4.png)'
- en: 'After this, we will add either Empty Activity or No Activity:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加Empty Activity或No Activity：
- en: '![](img/43d69a73-8f73-4ac9-9c39-dbc1de2ae0d9.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43d69a73-8f73-4ac9-9c39-dbc1de2ae0d9.png)'
- en: 'Now, let''s start adding the activity and use the generated TensorFlow model
    to get the result. We need to enable two permissions so that we can use them in
    our application, as shown in the following code block:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始添加活动并使用生成的TensorFlow模型来获取结果。我们需要启用两个权限，以便在应用程序中使用它们，如下代码块所示：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will have a minimal UI for the application, with a couple of `TextView`
    components and a `Button`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为应用程序提供一个最小的UI，包含几个`TextView`组件和一个`Button`：
- en: '![](img/922ef678-9ff8-4124-ae40-3d2095533b4f.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/922ef678-9ff8-4124-ae40-3d2095533b4f.png)'
- en: 'The following XML layout mimics the UI in the preceding screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下XML布局模仿了上面截图中的UI：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s add the steps for the speech recognizer activity, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加语音识别活动的步骤，如下：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that we are not going to discuss the basics of Android here.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们这里不会讨论Android的基础知识。
- en: 'Next, we will launch the recorder, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将启动录音机，具体步骤如下：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following code shows the implementation of the `record()` method:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了`record()`方法的实现：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following code shows the implementation of the audio recognizing method:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了音频识别方法的实现：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The model is run through the `TensorFlowInferenceInterface`class, as shown in
    the preceding code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过`TensorFlowInferenceInterface`类运行，如上面的代码所示。
- en: Once we have the completed code running, run the application.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成代码并成功运行，启动应用程序。
- en: 'On the first run, you will need to allow the application to use the phone''s
    internal microphone, as demonstrated in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次运行时，您需要允许应用程序使用手机的内部麦克风，如下图所示：
- en: '![](img/8c35468a-ce89-472c-98d3-94da70ed4595.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c35468a-ce89-472c-98d3-94da70ed4595.jpg)'
- en: Once we give permission to use the microphone, click on RECORD VOICE and give
    your voice input within 5 seconds. There are two attempts shown in the following
    screenshots for the `how are you` input keyword with an Indian accent. It works
    better with US and UK accents.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们授权使用麦克风，点击“录音”按钮并在5秒内输入语音。在以下截图中显示了使用印度口音输入`how are you`关键字的两次尝试。对于美国和英国口音效果更好。
- en: 'The first attempt is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试如下：
- en: '![](img/6434a237-92fa-4d77-9063-99252d3510ab.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6434a237-92fa-4d77-9063-99252d3510ab.png)'
- en: 'The second attempt is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次尝试如下：
- en: '![](img/84aa3d2f-5f7f-4a8b-b273-5d3b7ad89d80.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84aa3d2f-5f7f-4a8b-b273-5d3b7ad89d80.jpg)'
- en: You should try this with your own accent to get the correct output. This is
    a very simple way to start building your own speech detector that you can improve
    on even further.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该尝试使用您自己的口音来获取正确的输出。这是开始构建您自己语音检测器的一个非常简单的方法，您可以进一步改进。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to build a complete speech detector on your
    own. We discussed how the WaveNet model works in detail. With this application,
    we can make a simple speech-to-text converter work; however, a lot of improvements
    and updates need to be done to get perfect results. You can build the same application
    on the iOS platform as well by converting the model into CoreML.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何独立构建一个完整的语音检测器。我们详细讨论了WaveNet模型的工作原理。有了这个应用，我们可以使一个简单的语音转文本转换器运行；然而，要获得完美的结果，还需要做很多改进和更新。你也可以通过将模型转换为CoreML，在iOS平台上构建相同的应用。
- en: In the next chapter, we will move on and build a handwritten digit classifier
    using the **Modified National Institute of Standards and Technology** (**MNIST**)
    model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续构建一个手写数字分类器，使用**修改版国家标准与技术研究所**（**MNIST**）模型。
