- en: Multilayer Perceptron for Signal Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于信号检测的多层感知机
- en: This chapter will show you how to build a multilayer perceptron neural network
    for signal detection. We will first discuss the architecture of multilayer perceptron
    neural networks. Then we will cover how to prepare the data, how to decide on
    hidden layers and neurons, and how to train and evaluate the model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示如何为信号检测构建多层感知机神经网络。我们将首先讨论多层感知机神经网络的架构。然后，我们将介绍如何准备数据、如何决定隐藏层和神经元数量，以及如何训练和评估模型。
- en: The section on preparing the data will be important going forward as these deep
    learning models require data to be in particular formats in order to pass the
    data to the models. The hidden layer is the part of the neural network that separates
    it from other machine learning algorithms, and in this chapter, we will show you
    how to search for the optimal number of nodes in a hidden layer. In addition,
    over the course of this chapter, you will become much more familiar with the MXNet
    syntax, including the model training and evaluation steps.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据准备的部分将是未来学习中的关键，因为这些深度学习模型需要数据以特定的格式进行传递才能送入模型。隐藏层是神经网络与其他机器学习算法的区别所在，在本章中，我们将展示如何寻找隐藏层中的最佳节点数。此外，在本章的过程中，你将更加熟悉MXNet的语法，包括模型的训练和评估步骤。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding multilayer perceptrons
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解多层感知机
- en: Preparing and processing the data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备和处理数据
- en: Deciding on the hidden layers and neurons
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定隐藏层和神经元数量
- en: Training and evaluating the model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files of this chapter at the corresponding GitHub link
    at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在相应的GitHub链接[https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)找到本章的代码文件。
- en: Understanding multilayer perceptrons
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解多层感知机
- en: A multilayer perceptron is an instance of a feedforward neural network that
    only uses fully connected layers consisting of perceptrons. A perceptron is a
    node that takes input values and multiplies them by weights, and then passes this
    aggregate value to an activation function that returns a value that indicates
    how much this set of inputs and weights matches the pattern we are trying to find.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机是一个前馈神经网络的实例，只有使用由感知机组成的全连接层。感知机是一个节点，它接受输入值并将其与权重相乘，然后将这个聚合值传递给激活函数，该函数返回一个值，表示这一组输入和权重与我们尝试寻找的模式的匹配程度。
- en: The multilayer perceptron can be thought of as the most basic neural network
    implementation. As we mentioned, all layers are fully connected, which means that
    there are no convolution or pooling layers. It is also a feedforward model, which
    means that information from backpropagation is not looped back at every step,
    as it is in a recurrent neural network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机可以被视为最基本的神经网络实现。如我们所述，所有层都是全连接的，这意味着没有卷积层或池化层。它也是一个前馈模型，这意味着来自反向传播的信息不会在每一步都回传，如同在递归神经网络中那样。
- en: Simplicity can be an asset in terms of the ease of the interpretability of the network
    and its initial setup; however, the main drawback is that with so many fully connected
    layers, the count of weights will grow to such a level that the model will take
    a long time to train for large datasets. It also has a vanishing gradient problem,
    which means that the model will reach a point where the value that is passed back
    to correct the model is so small that it no longer significantly impacts the results.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性在网络的可解释性和初始设置方面可能是一个优点；然而，主要的缺点是，由于有这么多全连接层，权重的数量会增加到一个程度，以至于在大数据集上训练模型的时间会非常长。它还存在梯度消失问题，这意味着模型会达到一个点，在这个点上，回传给模型以进行修正的值非常小，以至于它不再显著地影响结果。
- en: Preparing and preprocessing data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备和预处理数据
- en: 'For this example, we will use the Adult dataset. We will walk through the steps
    to get this dataset in the proper form so that we can train a multilayer perceptron
    on it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用Adult数据集。我们将逐步展示如何将此数据集转换为合适的格式，以便我们可以在其上训练多层感知机：
- en: 'We will first load the libraries that we need. We will use the `mxnet` package
    to train the MLP model, the `tidyverse` family of packages for our data cleaning
    and manipulation, and `caret` to evaluate our model. We load the libraries using
    the following code:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载需要的库。我们将使用`mxnet`包来训练MLP模型，使用`tidyverse`系列包进行数据清理和操作，使用`caret`来评估我们的模型。我们使用以下代码加载这些库：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code will not produce any output to the console; however, you will see
    a checkmark next to these libraries in the Packages pane, indicating that the
    packages are now ready to use. Your Packages pane should look like the following
    screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码不会在控制台中输出任何内容；但是，你会在Packages面板中看到这些库旁边有一个勾号，表示这些库已经准备好使用。你的Packages面板应显示如下截图：
- en: '![](img/989aa574-bd75-4019-a461-f2c865aeba62.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/989aa574-bd75-4019-a461-f2c865aeba62.png)'
- en: 'Next, we will load our training and test data. In addition, we will add a column
    called `dataset` that we will populate with the `train` and `test` values to label
    our data. We will do this so that we can combine our data and perform some manipulation
    on the full data to save ourselves from repeating steps, and then be able to split
    the data again afterward. We then load the data and add the label with the following
    code:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载我们的训练数据和测试数据。此外，我们会添加一个名为`dataset`的列，并用`train`和`test`的值填充该列来标记数据。我们这样做是为了能够将数据合并，并对整个数据集进行一些操作，避免重复步骤，然后再将数据拆分。我们使用以下代码加载数据并添加标签：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code will place two data objects in your Environment pane. This
    will be the train and test data that we will use for this modeling exercise. Your
    Environment pane will look like the following screenshot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会在你的环境面板中放置两个数据对象。这将是我们用来进行建模练习的训练数据和测试数据。你的环境面板应显示如下截图：
- en: '![](img/5528f5aa-b276-40ec-b87f-0c5476e831c1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5528f5aa-b276-40ec-b87f-0c5476e831c1.png)'
- en: 'In this step, we will combine our data with a row bind and then remove any
    rows with NA values. Removing rows with missing values is not always the most
    appropriate course of action; at times, other tactics should be used to handle
    missing values. These tactics include **imputation** and **replacement**. In this
    case, we would just like to remove these rows for ease of use, since we are just
    using this data for example purposes. We combine the data and remove rows with
    missing values using the following code:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用行绑定（row bind）合并数据，并删除任何包含NA值的行。删除包含缺失值的行并不总是最合适的做法；有时，应该使用其他策略来处理缺失值。这些策略包括**插补**和**替代**。在这种情况下，由于我们仅仅是为了示范，我们希望移除这些行以便简化操作。我们使用以下代码合并数据并移除含缺失值的行：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code will add one more data object to our Environment pane. Your
    Environment pane will now look like the following screenshot:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将向我们的环境面板中添加另一个数据对象。现在你的环境面板应显示如下截图：
- en: '![](img/427aa6d5-af4b-4b3e-a1be-f66395687c9e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/427aa6d5-af4b-4b3e-a1be-f66395687c9e.png)'
- en: You can see that the data object `all` contains rows from `test` and `train`.
    We can now modify both `train` and `test` at the same time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，数据对象`all`包含了来自`test`和`train`的数据。现在我们可以同时修改`train`和`test`。
- en: 'In the next step, we take any factor value and trim the whitespace. We do this
    because there are multiple values that should mean the same thing but are showing
    up as distinct values because of whitespace, such as `Male` and ` Male`. We will
    demonstrate that whitespace is causing an issue when accurately defining categories
    and then correct the issue with the following code:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们会处理任何因子值并去除空格。这样做是因为有些值本应表示相同的内容，但由于空格问题，显示为不同的值，例如`Male`和` Male`。我们将演示空格是如何在准确定义类别时造成问题，并使用以下代码修正该问题：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When we run the first line of the preceding code, we will print the distinct
    factor levels to the console. Your console will look similar to the following
    screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码的第一行时，我们会将不同的因子水平打印到控制台。你的控制台应类似于以下截图：
- en: '![](img/5e7c5839-7c1a-4d51-bcf8-a17cbb8456cb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e7c5839-7c1a-4d51-bcf8-a17cbb8456cb.png)'
- en: 'However, after running the second line and removing the whitespace, we will
    see that this has been corrected, and the output will look different. If you run
    the `unique()` function again, the output in your console will look like the following
    screenshot:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在运行第二行并移除空格后，我们会发现问题已经被修正，输出也将有所不同。如果你再次运行`unique()`函数，控制台中的输出会显示如下截图：
- en: '![](img/0ea849c2-c230-40ee-a6f6-edd8bec4b1ce.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ea849c2-c230-40ee-a6f6-edd8bec4b1ce.png)'
- en: Correcting this issue will help the algorithm to use the proper number of categories
    when creating the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 纠正这个问题将有助于算法在创建模型时使用正确数量的类别。
- en: 'Next, we filter just our training data. We will extract the target variable
    to a vector and convert the values to numeric. Afterward, we can remove the target
    variable from the dataset, as well as the dataset variable. We extract the train
    data, create the target variable vector, and remove the unneeded columns using
    the following code:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们仅筛选出训练数据。我们将目标变量提取为一个向量，并将值转换为数值型。之后，我们可以从数据集中删除目标变量和数据集变量。我们通过以下代码提取训练数据，创建目标变量向量，并删除不需要的列：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After we run this code, we will see that the `train` data has been updated
    in our `Environment` pane and the `train_target` vector has been added. Your `Environment`
    pane will look like the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们会看到`train`数据已在`Environment`面板中更新，并且已添加了`train_target`向量。您的`Environment`面板将显示以下截图：
- en: '![](img/cceeb682-871e-4583-b29e-2363fc2e411f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cceeb682-871e-4583-b29e-2363fc2e411f.png)'
- en: We can see that the train data has two fewer variables now that we have removed
    the target and the dataset. The `target` column is now extracted to its own vector
    and we have the dependent variable and independent variables in separate data
    objects so that they are in the proper format and ready to be passed to the modeling
    function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，训练数据减少了两个变量，因为我们已删除了目标和数据集。`target`列现在被提取到自己的向量中，我们将因变量和自变量分开，存放在不同的数据对象中，以便它们处于正确格式，准备传递给建模函数。
- en: 'The next step is to separate the data column-wise so that one subset contains
    only columns containing numeric values while the other contains only columns containing
    string values. As stated earlier, all values will need to be numeric, so we will
    be using one-hot encoding on the string values, which is also known as creating
    dummy variables. This will create a column for every possible field name–value
    pair and populate this column with either a `1` or `0`, representing whether the
    value is present for the given field name per row. We split our data column-wise
    in the way described here using the following code:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是按列分离数据，以便一个子集仅包含包含数值的列，而另一个子集仅包含包含字符串的列。如前所述，所有值都需要是数值型，因此我们将对字符串值进行独热编码，也称为创建虚拟变量。这将为每个可能的字段名称–值对创建一列，并用`1`或`0`填充该列，表示每行是否存在该字段名称的值。我们使用以下代码按描述的方式列分割数据：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After we run the preceding code, we will see two new data objects—one with
    the 6 numeric columns among the 14 total and the other with the remaining 8 columns,
    which contain string values. Your Environment pane will now look as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后，我们将看到两个新的数据对象——一个包含14个总列中的6个数值列，另一个包含剩余的8列，这些列包含字符串值。您的环境面板现在将如下所示：
- en: '![](img/b77bc154-3b9b-4e8e-80ea-bbd5dfa878f2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b77bc154-3b9b-4e8e-80ea-bbd5dfa878f2.png)'
- en: Now that our data is in this format, we can one-hot encode the columns that
    contain only strings.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经是这种格式，我们可以对仅包含字符串的列进行独热编码。
- en: 'In this step, we will actually create our dummy variables. We use the `dummyVars()`
    function from the caret package. This takes two steps. In the first, we define
    the columns that we would like converted to dummy variables. Since we would like
    all columns to be converted, we just include a dot after the tilde. Next, we use
    the `predict()` function to actually create the new variables using the formula.
    We create our new dummy variable columns using the following code:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们实际上会创建虚拟变量。我们使用来自caret包的`dummyVars()`函数。这包含两个步骤。第一步，我们定义希望转换为虚拟变量的列。由于我们希望所有列都进行转换，因此我们只需在波浪号后加一个点。接下来，我们使用`predict()`函数，根据公式实际创建新的变量。我们通过以下代码创建新的虚拟变量列：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running the preceding code, we will have two new data objects in our
    Environment pane. One is the `ohe` object, which is a list with all the details
    that we need to convert our string columns to dummy variables, and the other is
    the `train_ohe` object, which contains the dummy variables. Your Environment pane
    will now look as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后，您将在环境面板中看到两个新的数据对象。一个是`ohe`对象，它是一个包含所有细节的列表，用于将字符串列转换为虚拟变量；另一个是`train_ohe`对象，包含虚拟变量。您的环境面板现在将如下所示：
- en: '![](img/24d01370-cffd-45fa-8dd4-8d9aefdf3c0d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24d01370-cffd-45fa-8dd4-8d9aefdf3c0d.png)'
- en: We can see that creating dummy variables results in a dataset with many more
    columns than our original data. As stated, we take every column name and value
    pair and create a new column, which results in the growth of columns.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，创建虚拟变量会导致数据集的列数比原始数据多得多。如前所述，我们会为每个列名和对应值对创建一个新的列，这导致了列数的增加。
- en: 'After the columns containing string values have been converted to columns with
    numeric values, then we can column-bind both subsets back together again. We combine
    the data that was already numeric and the data that was converted to a numeric
    format using the following line of code:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在包含字符串值的列被转换为数值列后，我们可以将两个子集重新合并。我们使用以下代码将原本是数值型的数据和转换后的数值型数据合并：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that the `train` object in our Environment pane has changed. Your
    Environment pane will now look as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，环境面板中的`train`对象发生了变化。你的环境面板现在会显示如下：
- en: '![](img/02f29f52-6162-4468-bb11-5b3183ef0f51.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02f29f52-6162-4468-bb11-5b3183ef0f51.png)'
- en: The train data now contains all numeric columns and is in the proper format
    to be used with a neural network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练数据包含了所有的数值型列，且已处于正确的格式，适用于神经网络。
- en: 'Lastly, for the columns that originally held numeric values, we will rescale
    the values so that all values are in a range between `0` and `1`, and as such
    are on the same scale as our one-hot encoded columns. We get all of our data on
    the same scale using the following code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，对于原本包含数值的列，我们将对其进行缩放，使得所有值都在`0`到`1`之间，从而与我们的独热编码列处于同一尺度。我们通过以下代码将所有数据调整到相同的尺度：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Before we run the preceding code, let''s first see what the train data columns
    look like in our Environment pane. Yours will look like the following screenshot
    before running the code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码之前，让我们先看看环境面板中`train`数据列的样子。你的界面在运行代码前将如下所示：
- en: '![](img/cd3cc60c-0bfc-4778-aaa0-a9c37cbd6a09.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd3cc60c-0bfc-4778-aaa0-a9c37cbd6a09.png)'
- en: 'Your Environment pane will look like the following screenshot after running
    the code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，您的环境面板将显示如下截图：
- en: '![](img/c752c279-20c6-473c-8f59-667ce1ce0966.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c752c279-20c6-473c-8f59-667ce1ce0966.png)'
- en: We can see that all the values that were on different scales in the first image
    are now all rescaled so that all values are between `0` and `1`. Having all values
    on the same scale will help to make training the model more efficient.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，原先在第一张图中处于不同尺度的所有值，现在已经重新缩放，使得所有值都在`0`到`1`之间。将所有值统一尺度有助于提高模型训练的效率。
- en: 'We can now repeat the same steps for the test dataset. We prepare our test
    data for modeling using the same steps as the training data by running the following
    lines of code:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以对测试数据集重复相同的步骤。通过运行以下代码行，我们使用与训练数据相同的步骤来准备测试数据进行建模：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a result of running this code, you will see test data objects that have
    been modified in the same way as the training data objects. Your Environment pane
    will now look like the following screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完此代码后，你会看到测试数据对象已按与训练数据对象相同的方式被修改。你的环境面板现在会显示如下截图：
- en: '![](img/db7590e8-e1ac-4168-b993-6534fce3a247.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db7590e8-e1ac-4168-b993-6534fce3a247.png)'
- en: All of our data is now in the proper format and ready to be used for training
    our neural network model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有数据现在都已经是正确的格式，并且可以用于训练我们的神经网络模型。
- en: 'There is one last cleanup step. If we look at our column count, we can see
    that the `train` dataframe has one more column than the test dataframe. We can
    use the `setdiff` function to see which column exists in the `train` and not in
    the `test` set. Once that has been identified, we can just remove that column
    from the `train` set. We need our data to have the same number of columns for
    modeling. We find the column that doesn''t exist in both datasets and remove it
    using the following two lines of code:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有最后一步清理。如果我们查看列的数量，可以发现`train`数据框比测试数据框多了一个列。我们可以使用`setdiff`函数查看哪些列在`train`中存在但在`test`集中不存在。一旦找到了该列，我们就可以从`train`数据集中删除它。我们需要确保数据具有相同的列数以进行建模。我们通过以下两行代码找出并删除在两个数据集中不存在的列：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When we run the first line of code, we will print the value of the output to
    our console. Your console output will look like the following screenshot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行第一行代码时，输出的值会打印到我们的控制台。你的控制台输出将如下所示：
- en: '![](img/9b94150a-edb4-4a3c-bc8e-a2de1c69d51d.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b94150a-edb4-4a3c-bc8e-a2de1c69d51d.png)'
- en: 'We now know that the `train` has the column `native.countryHoland.Netherlands` while
    `test` does not. We use the second line of code to remove this column from the
    `train`. After we run the second line of code, we will notice a difference in
    our Environment pane. Your Environment pane will now look like the following screenshot:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，`train` 数据集有列 `native.countryHoland.Netherlands`，而 `test` 数据集没有。我们使用第二行代码将该列从
    `train` 数据集中移除。运行第二行代码后，我们会注意到环境面板发生了变化。你的环境面板现在将像下面的截图一样：
- en: '![](img/f8d30171-6ba2-47ab-9ec8-0872b958a569.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8d30171-6ba2-47ab-9ec8-0872b958a569.png)'
- en: When we look at `train` and `test` now, we can see that both data objects have
    the same number of columns, which is required for using the two data objects to
    train and test our model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们现在查看 `train` 和 `test` 数据集时，我们可以看到这两个数据对象的列数相同，这是使用这两个数据对象来训练和测试模型所必需的。
- en: 'The last data preparation step is to take our target variable vectors and subtract `1`
    from all values. When we first cast these to numeric format, we took their factor
    level values so that we got vectors coded with values of either `1` or `2`; however,
    we want these to be coded as either `0` or `1` so that they are at the same scale
    as our independent variables. We get our target variables on the same scale as
    our independent variables by running the following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的数据准备步骤是对我们的目标变量向量中的所有值减去 `1`。当我们第一次将这些变量转换为数值格式时，我们取了它们的因子水平值，从而得到了编码为 `1`
    或 `2` 的向量；然而，我们希望这些向量被编码为 `0` 或 `1`，这样它们与我们的自变量在同一尺度上。我们通过运行以下代码将目标变量调整到与自变量相同的尺度：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After running the following code, you will notice one last change to your Environment
    pane. Your Environment pane will now look like the following screenshot:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码后，你会注意到环境面板发生最后一次变化。你的环境面板现在将像下面的截图一样：
- en: '![](img/59e1b6ed-2c81-4007-b77f-a5df543d743c.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59e1b6ed-2c81-4007-b77f-a5df543d743c.png)'
- en: All of our data is now numeric and on the same scale, so the data is completely
    prepared for modeling at this point.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的所有数据都是数值型并且处于相同的尺度上，因此数据已经完全准备好用于建模。
- en: We started with data in the state that it was originally stored in and took
    some steps to get the data into the proper format so that we could use it to train
    a neural network. Neural networks, especially deep-learning implementations, offer
    the convenience of not needing to perform feature engineering like you might have
    to do with other machine-learning techniques. That being the case, some data preparation
    is still often required as neural networks require all data to be stored as numeric
    values, and your model will perform better if all numeric value data is on the
    same scale. The data manipulation and transformation steps that we just performed
    are representative of the type of data preparation work that will need to be completed
    for other data that you will use for training neural networks. With our data in
    the proper format, we can now devise a system to search for the optimal number
    of nodes for the hidden layer in our model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数据最初存储的状态开始，并采取了一些步骤将数据转换为正确的格式，以便用来训练神经网络。神经网络，特别是深度学习的实现，提供了不需要像其他机器学习技术那样进行特征工程的便利。尽管如此，仍然常常需要一些数据准备，因为神经网络要求所有数据必须存储为数值型数据，而且如果所有数值型数据处于相同的尺度下，模型的表现会更好。我们刚才进行的数据处理和转换步骤，代表了为其他数据进行神经网络训练时需要完成的数据准备工作。数据已经转换为正确的格式，现在我们可以设计一个系统来搜索我们模型中隐藏层的最优节点数。
- en: Deciding on the hidden layers and neurons
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决定隐藏层和神经元数量
- en: 'Multilayer perceptrons provide only a few choices during the model design process:
    the activation function used in the hidden layers, the number of hidden layers,
    and the number of nodes or artificial neurons in each layer. The topic of selecting
    the optimal number of layers and nodes will be covered in this section. We can
    begin with a single layer and use a set of heuristics to guide our starting point
    for selecting the number of nodes to include in this hidden layer.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器在模型设计过程中只有少数选择：隐藏层使用的激活函数、隐藏层的数量，以及每一层中节点或人工神经元的数量。本节将讨论选择最优层数和节点数的问题。我们可以从单层开始，使用一套启发式规则来指导我们选择该隐藏层中节点的数量。
- en: When beginning this process, a good starting point is 66% of the length of the
    input or the number of independent variable columns. This value, in general, will
    fall within a range between the size of the output to two times the size of the
    input; however, 66% of the length of the input is a good starting point within
    this range.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这一过程时，一个好的起始点是输入长度或独立变量列数的66%。一般来说，这个值会位于输出大小与输入大小的两倍之间；然而，输入长度的66%是这个范围内的一个好起始点。
- en: This does not mean that this starting point will always be the optimal number
    of nodes to use. To discover the optimal number, we can write a function that
    will train our model using a different number of nodes right around our starting
    point in order to see trends and attempt to find the optimal value. In this case,
    we will train with a larger learning rate using only a few rounds for fast run-time.
    If you are working with a large dataset, then it may be necessary to use a subset
    of the data when using this strategy so that long run times per iteration do not
    create an issue.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着这个起始值将始终是使用的最优节点数量。为了发现最优数量，我们可以编写一个函数，使用与起始点附近的不同节点数量来训练我们的模型，以观察趋势并尝试找到最优值。在这种情况下，我们将使用较大的学习率，进行少量的训练轮次以加快运行时间。如果你正在处理大型数据集，则可能需要在使用该策略时仅使用数据的子集，以避免每次迭代过长的运行时间成为问题。
- en: 'We will now walk through the creation of a function to test the performance
    of the model given a number of different nodes in the hidden layer:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将逐步讲解如何创建一个函数来测试模型在不同节点数情况下的性能：
- en: 'To start, let''s look at the number of independent variable columns and then
    get 66% of this value to arrive at our starting point. We decide on the starting
    point for the number of nodes to include in our hidden layer by running the following
    code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们来看一下独立变量列的数量，然后获取该值的66%，得到我们的起始点。我们通过运行以下代码来决定隐层中节点数量的起始值：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code will print the following output to your console:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将在控制台中打印以下输出：
- en: '![](img/7531e213-13f8-491c-9efc-96204c5e25a7.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7531e213-13f8-491c-9efc-96204c5e25a7.png)'
- en: The precise value is `67.98`, but we will round this up to `70` as our starting
    value. Keep in mind that you can use any value you like, as this is just a heuristic—working
    with round numbers is convenient, and you can always drill down to the exact optimal
    number of neurons at a later time—however, the performance difference when making
    small changes will be minimal and may not be present when generalizing with this
    model later.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的数值是`67.98`，但我们将把它四舍五入到`70`作为起始值。请记住，你可以使用任何你喜欢的值，因为这只是一个启发式方法——使用整数是方便的，你也可以在之后深入挖掘，找到最优的神经元数量——然而，在做小幅度变化时，性能差异通常是最小的，甚至在后续泛化模型时可能不会出现。
- en: 'Next, let''s choose two values that are larger than this starting point as
    well as two that are smaller and store these in a vector. These will be the options
    that we will pass through as arguments to our function. In this case, we started
    with `70`, so we will also include `50`, `60`, `80`, and `90`. We create the vector
    of possible nodes for our hidden layer by running the following code:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们选择两个大于该起始点的值，以及两个小于该值的，存储在一个向量中。这些将作为参数传递给我们的函数。在此案例中，我们从`70`开始，因此我们还将包含`50`、`60`、`80`和`90`。我们通过运行以下代码来创建隐层可能节点的向量：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After running the preceding code, we will see that this data object is now
    in our Environment pane. Your Environment pane will now look like the following
    screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们将看到这个数据对象现在出现在我们的环境面板中。你的环境面板现在应该类似于以下截图：
- en: '![](img/3babbfa2-371d-43bf-9fa7-e06d5470bcef.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3babbfa2-371d-43bf-9fa7-e06d5470bcef.png)'
- en: We will use the values from this vector later to loop through these choices
    and see which performs best.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将使用这个向量中的值来循环遍历这些选项，看看哪个性能最好。
- en: 'We will set our seed at this point to ensure reproducibility. This is always
    important, and should always be done when working with any type of model that
    introduces quasirandom numbers. In our case, it is important for this demonstration
    to show that the function we create produces the same result as running the code
    alone. We set the seed specifically for use with our MXNet model by running the
    following line of code:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将设置种子，以确保结果的可复现性。这一点始终很重要，在使用任何引入准随机数的模型时都应这样做。在我们的示范中，重要的是展示我们创建的函数能产生与直接运行代码相同的结果。我们通过运行以下代码行，专门为我们的MXNet模型设置种子：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After running this code, no output is printed to the console and there are no
    noticeable changes in RStudio; however, using this method, we ensure consistent
    model results.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，控制台没有任何输出，RStudio中也没有明显的变化；然而，使用此方法，我们确保了模型结果的一致性。
- en: 'Before writing our function, we will first define and run our model and look
    at the syntax and options for training a multilayer perceptron using the `mxnet`
    package. We define and run our multilayer perceptron model using the following
    code:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编写我们的函数之前，我们将首先定义并运行我们的模型，并查看使用`mxnet`包训练多层感知机的语法和选项。我们使用以下代码定义并运行我们的多层感知机模型：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After running the preceding code, we will see model details printed to our
    console for every run. The output to your console will look like the following
    screenshot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们会看到每次运行时模型的详细信息被打印到控制台。控制台输出将如下图所示：
- en: '![](img/fc49ba07-9139-4554-9c98-dbb9f22d7795.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc49ba07-9139-4554-9c98-dbb9f22d7795.png)'
- en: The output to the console lists all the accuracy values using a holdout set
    from the `train` data. We will cover all the options for modeling with MXNet after
    this code to cover each argument in more detail. Simply put, we are using values
    to make the model run quickly at this point, while we are preparing to test for
    the optimal number of nodes to include.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出列出了所有使用`train`数据中的保留集计算的准确度值。我们将在运行完这段代码后介绍更多MXNet建模的选项，以便更详细地说明每个参数。简而言之，此时我们使用一些值来使模型运行更快，同时我们正在准备测试最优节点数。
- en: 'In addition to training this model, we would also like a data object to hold
    performance results so that we can compare the performance after trying the different
    hidden layer sizes. Here, we can make a prediction using our model and then select
    the class with the highest likelihood. Lastly, we calculate the accuracy by summing
    up the cases where the prediction is correct over the length of the test target
    variables. We can also see how we can now store these two values in a table. We
    do this here to demonstrate the entire inside of our function, which will hold
    all the different node size choices along with the accuracy of using that given
    number of nodes. We make predictions and calculate the accuracy using the following
    code:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了训练这个模型之外，我们还希望有一个数据对象来保存性能结果，以便在尝试不同的隐藏层大小后进行比较。在这里，我们可以使用模型进行预测，并选择具有最高可能性的类别。最后，我们通过将预测正确的案例总数与测试目标变量的长度进行比较，来计算准确度。我们还可以看到现在如何将这两个值存储在表格中。我们这样做是为了演示函数内部的整个过程，该函数将包含所有不同的节点大小选择及其对应的准确度。我们使用以下代码进行预测并计算准确度：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After running the preceding code, we will have four new data objects in our
    Environment pane. Your Environment pane will look like the following screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们将在“环境”面板中看到四个新的数据对象。您的“环境”面板将如下图所示：
- en: '![](img/ee70f05b-0c90-4498-85cf-f6b13c457734.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee70f05b-0c90-4498-85cf-f6b13c457734.png)'
- en: The `preds` object holds the results from making predictions with our model.
    MXNet stores these prediction results in a matrix with the probabilities for each
    class. If we transpose this matrix or rotate it 90 degrees and select the maximum
    value for every column, then we will get the highest row number that corresponds
    with the most likely class; however, this will be using values `1` and `2` for
    the rows, so we subtract 1 from all values to get our prediction values on the
    same scale as our true test classes, which are `0` or `1`. For the accuracy value,
    we sum all the cases where the predicted values and true values are the same over
    the total number of true cases. Finally, we can put the node count and accuracy
    score in a table. This will be useful for comparing results when we try different
    node counts.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`preds`对象保存了我们通过模型进行预测的结果。MXNet将这些预测结果存储在一个矩阵中，矩阵中包含每个类别的概率。如果我们转置这个矩阵或将其旋转90度，并选择每列的最大值，那么我们将得到与最可能类别对应的最高行号；然而，这样做时，矩阵中的行值为`1`和`2`，因此我们需要从所有值中减去1，以便将预测值与我们真实的测试类别（即`0`或`1`）保持一致。对于准确度值，我们通过将预测值与真实值相同的情况总数与真实案例的总数相比，计算得到准确度。最后，我们可以将节点数和准确度分数放入一个表格中，这对比较不同节点数的结果非常有用。'
- en: 'Now that we have everything coded for an individual case, we can create our
    function by replacing the value assigned to the argument that we would like to
    test with a variable. We then move that variable to be the argument for our new
    function. We can see that everything in the code is the exact same as it was previously,
    except that the `70` that we had as a value for the `hidden_node` argument, and
    that we had later as a value to add under the `nodes` column in the new table
    we will create, are now replaced with `x`. The `x` is then moved outside and added
    as an argument for our new function. In this way, we can now pass any value to
    our new `mlp_loop()` function and have it replace the two instances of `x` in
    our code. We write our custom function to try different values for the `hidden_node`
    argument using the following code:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经为单个案例编写了所有代码，我们可以通过将我们想要测试的参数的赋值替换为变量来创建我们的函数。然后，我们将这个变量移到我们的新函数中作为参数。我们可以看到，代码中的一切与之前完全相同，只是我们曾经将`hidden_node`参数的值设为`70`，并且后来在我们要创建的新表格中的`nodes`列下添加这个值的地方，现在它已被替换为`x`。然后，`x`被移到外部并作为我们新函数的参数传入。通过这种方式，我们现在可以将任何值传递给我们的新`mlp_loop()`函数，并让它替换代码中两个`x`的实例。我们编写自定义函数来尝试不同的`hidden_node`参数值，使用以下代码：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After we run the preceding code, we will see the change in our Environment
    pane. Your Environment pane will now look like the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码后，我们将看到我们的环境面板发生了变化。你的环境面板现在将像下面的截图一样：
- en: '![](img/4b33b27f-1380-4193-a2eb-ce826e38f194.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b33b27f-1380-4193-a2eb-ce826e38f194.png)'
- en: We can see that we now have a custom function defined and stored in our environment.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在我们的环境中已经定义并存储了一个自定义函数。
- en: 'Now we can first test our function with the value from the previous run. We
    test our function by supplying the value `70` to the function that we just made
    using the following lines of code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以首先使用之前运行时的值来测试我们的函数。我们通过向刚才创建的函数提供`70`这个值来测试我们的函数，代码如下：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After we run the following code, we will get a printout on our console with
    the accuracy value and the number of nodes that we included in the hidden layer,
    as well as the results of testing for equality. Your console will look like the
    following screenshot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行以下代码后，我们将会在控制台上得到一份打印输出，显示准确度值、我们在隐藏层中包含的节点数以及测试相等性结果。你的控制台将会像下面的截图一样：
- en: '![](img/7700478a-3334-4134-922a-cfe0332d5964.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7700478a-3334-4134-922a-cfe0332d5964.png)'
- en: Based on the results of the code that we just ran, we can see that our function
    produces the same results that it would produce by just passing the values to
    the modeling code directly. This makes sense, as we are just swapping in `70`
    for all places where `x` is in the model function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们刚刚运行的代码结果，我们可以看到我们的函数生成的结果与直接将这些值传递给建模代码时生成的结果相同。这是合理的，因为我们只是将所有`x`的地方替换成了`70`。
- en: 'Now that it is confirmed that our new function is working (as we get the same
    result when passing the value `70` through the function that we get by just having
    it in the code), we can now pass our entire vector of values through the function.
    In order to do so, we will use the `map()` function from the `purrr` package,
    which makes iterating very simple and straightforward. In this case, we will use
    the `map_df()` function in order to get a dataframe, after looping all values
    through the function call. We loop through our function, passing in all values
    from the vector we created earlier by using the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在可以确认我们的新函数正常工作（因为当我们通过函数传递`70`时，得到的结果与直接在代码中使用`70`时相同），我们可以将整个数值向量传递给这个函数。为此，我们将使用`purrr`包中的`map()`函数，它使得迭代变得非常简单和直接。在这种情况下，我们将使用`map_df()`函数，以便在遍历所有值后获得一个数据框。我们通过以下代码循环遍历我们的函数，将之前创建的向量中的所有值传递进去：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When we run the preceding code, we will see a printout to the console just
    like in *step 4* for all five model runs. After this, we will get the `results`
    dataframe, which now has all the accuracy scores for all the node count attempts.
    In your console, you may notice some rounding, which prevents an immediate determination
    of which model performed best. Let''s click on the results from our Environment
    pane instead and view the data that way. After clicking on `results`, you should
    see a table similar to the following screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们将在控制台看到一个类似于*步骤4*的输出，显示所有五次模型运行的结果。之后，我们将得到一个`results`数据框，它现在包含了所有节点数尝试的准确度分数。在控制台中，你可能会注意到一些四舍五入的情况，这可能会妨碍我们立刻判断哪个模型表现最佳。让我们改为点击环境面板中的`results`，以表格形式查看数据。点击`results`后，你应该会看到类似以下截图的表格：
- en: '![](img/d23427e0-cf3d-4a59-ad58-20f362e8ac02.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d23427e0-cf3d-4a59-ad58-20f362e8ac02.png)'
- en: 'While we were able to create a loop to pass different node count values to
    the `mlp()` function to determine the optimal count, we will see in the next step
    that using a similar loop technique for finding the optimal layers is not as straightforward.
    To add layers, we must abandon the convenience of the `mlp()` function and create
    our multilayer perceptron one layer at a time. We can create an MLP one layer
    at a time using the following code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们已经能够创建一个循环，将不同的节点数值传递给`mlp()`函数以确定最佳节点数，但在下一步中我们会看到，使用类似的循环技巧来寻找最佳层数并不像预期的那么简单。要添加层，我们必须放弃`mlp()`函数的便捷性，一次创建一个层来构建我们的多层感知器（MLP）。我们可以使用以下代码一次性创建一个MLP层：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After running the preceding code, we will have a number of new data objects
    in our Environment pane and an accuracy score printed to our console. Your console
    will look like the following screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们将在环境面板中看到一些新的数据对象，并且控制台会打印出一个准确度分数。你的控制台将类似于以下截图：
- en: '![](img/a223a863-7dc1-4156-94e1-19c920739a33.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a223a863-7dc1-4156-94e1-19c920739a33.png)'
- en: From this output, we can see that using the two highest-scoring values from
    our test of node counts in two separate layers did not improve our score. Adding
    more layers will not always lead to a better performing model, though you can
    continue to experiment with different layers using the preceding code as a guide
    to try to see if you can improve the score. Let's review what the preceding code
    is doing and how it differs from the model we created using the `mlp()` function.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果中，我们可以看到，在两层不同节点数的测试中，选取两个最高得分值并没有改善我们的得分。虽然你可以继续使用前面的代码尝试不同的层数，但增加更多的层并不总是能带来更好的模型表现。让我们回顾一下前面的代码在做什么，以及它与我们使用`mlp()`函数创建的模型有何不同。
- en: In this case, we initiate our model by creating a symbolic variable. We then
    create two fully connected layers with 90 and 50 nodes respectively. We then define
    an output layer using the softmax activation function. We then use the `FeedForward()`
    function to define the other options that we used previously. In doing this, we
    can see that most of the arguments can be passed to `FeedForward` while the `hidden_node`
    argument moves to the `FullyConnected()` function for as many layers as you want
    and the `out_node` and `out_activation` arguments move to an output function,
    which in this case is `SoftmaxOutput`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们通过创建一个符号变量来初始化模型。然后，我们创建了两个完全连接的层，分别包含90和50个节点。接着，我们定义了一个输出层，使用softmax激活函数。接下来，我们使用`FeedForward()`函数定义了之前使用的其他选项。通过这样做，我们可以看到大多数参数可以传递给`FeedForward`，而`hidden_node`参数则转移到`FullyConnected()`函数中，适用于任意数量的层，`out_node`和`out_activation`参数则传递给输出函数，在本例中为`SoftmaxOutput`。
- en: Using our prepared data, we looked at how to test for the optimal number of
    nodes for a hidden layer. We also looked at how we need to change our code to
    add additional layers. With MLPs, there are fewer options than other neural network
    implementations, so we have focused on making changes to the hidden layers to
    try to optimize our model using the main strength of neural network models. In
    the next step, we will take everything we have learned while tuning parameters
    to run a model that will maximize performance while giving a more in-depth explanation
    of our model options using MXNet.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们准备的数据，我们查看了如何测试隐藏层的最佳节点数。我们还讨论了如何更改代码以添加更多的层。与其他神经网络实现相比，MLP的选项较少，因此我们主要集中在调整隐藏层，以利用神经网络模型的主要优势来优化我们的模型。在下一步中，我们将利用我们在调整参数时学到的一切，运行一个能够最大化性能的模型，并使用MXNet提供更深入的模型选项解释。
- en: Training and evaluating the model
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: 'After parameter tuning, we can now run the model for maximum performance. In
    order to do so, we will make a few important changes to the model options. Ahead
    of making the changes, let''s have a more in-depth review of the model options:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 调整参数后，我们现在可以运行模型以获得最佳性能。为了实现这一点，我们将对模型选项进行一些重要更改。在进行更改之前，让我们更深入地审查一下模型选项：
- en: '`hidden_node`: These are the number of nodes in the hidden layer. We used a
    looping function to find the optimal number of nodes.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_node`：这是隐藏层中的节点数。我们使用循环函数来找到最佳的节点数。'
- en: '`out_node`: These are the number of nodes in the output layer and must be set
    equal to the number of target classes. In this case, that number is `2`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_node`：这是输出层中的节点数，必须设置为目标类别的数量。在这种情况下，该数字是`2`。'
- en: '`out_activation`: This is the activation function to use for the output layer.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_activation`：这是用于输出层的激活函数。'
- en: '`num.round`: This is the number of iterations we take to train our model. In
    the parameter tuning stage, we set this number low so that we could quickly loop
    through a number of options; to get maximum accuracy, we would allow the model
    to run for more rounds while at the same time dropping the learning rate, which
    we will cover soon.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num.round`：这是我们训练模型的迭代次数。在参数调整阶段，我们将此数字设置得较低，以便能够快速循环通过多个选项；为了获得最佳准确性，我们将允许模型运行更多轮次，同时降低学习率，稍后我们将讨论这一点。'
- en: '`array.batch.size`: This sets the batch size, which is the number of rows that
    are trained at the same time during each round. The higher this is set, the more
    memory will be required.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`array.batch.size`：这是设置批次大小的参数，即每一轮训练时同时训练的行数。该值设置得越高，所需的内存也越多。'
- en: '`learning.rate`: This is the constant value applied to the gradient from the
    loss function that is used to adjust weights. For parameter tuning, we set this
    to a large number to move quickly along the cost surface in a small number of
    rounds. To achieve the best performance, we will set this to a lower number to
    make more subtle adjustments while learning new weights so we don''t constantly
    overadjust the values.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning.rate`：这是应用于损失函数梯度的常数值，用来调整权重。为了参数调整，我们将其设置为较大的数值，以便在较少的轮次内快速沿成本面移动。为了获得最佳性能，我们将其设置为较小的数值，以便在学习新权重时做出更细微的调整，从而避免不断过度调整数值。'
- en: '`momentum`: This uses the decaying values from previous gradients to avoid
    sudden shifts in movement along the cost surface. As a heuristic, a good starting
    value for `momentum` is between `0.5` and `0.8`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`momentum`：这个参数利用先前梯度的衰减值，避免在成本面上出现突然的移动变化。作为启发式方法，`momentum`的一个良好起始值在`0.5`和`0.8`之间。'
- en: '`eval.metric`: This is the metric that you will use to evaluate performance.
    In our case, we are using `accuracy`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval.metric`：这是用来评估性能的度量标准。在我们的案例中，我们使用的是`accuracy`（准确率）。'
- en: 'Now, that we have covered the options included in our model using the `mlp()`
    function, we will make adjustments to improve accuracy. In order to improve accuracy,
    we will increase the number of rounds while simultaneously dropping the learning
    rate. We will keep the other values constant and use the node count that led to
    the best performance from our loop earlier. You can set the model for better performance
    using what we learned when parameter tuning using the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经涵盖了使用`mlp()`函数包含的模型选项，我们将进行调整以提高准确性。为了提高准确性，我们将增加训练轮次，同时降低学习率。我们将保持其他值不变，并使用之前循环中获得最佳性能的节点数。你可以使用以下代码，根据我们在参数调整时学到的内容来设置模型以提高性能：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After running this code, you will see a printout in your console with the accuracy
    score after running the model with the adjustments to the parameters. Your console
    output will look like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，你将在控制台中看到模型在调整参数后运行的准确率得分。你的控制台输出将如下所示：
- en: '![](img/2d45fe33-d287-4bb3-a3ae-0a911f7975de.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d45fe33-d287-4bb3-a3ae-0a911f7975de.png)'
- en: We can see that our accuracy has improved from our adjustments. Before, when
    we were testing parameters, the best accuracy we could achieve was 84.28%, and
    we can see that we now have an accuracy score of 85.01%.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过我们的调整，准确率得到了提升。之前，当我们测试参数时，最佳准确率为84.28%，现在我们可以看到准确率已经达到了85.01%。
- en: After preparing our data so that it is in the proper format to model with MXNet,
    and then parameter tuning to find the best values for our model, we then made
    adjustments to further improve performance using what we learned earlier. All
    of these steps together describe a complete cycle of manipulating and transforming
    data, optimizing parameters, and then running our final model. We saw how to use
    MXNet, which offers a convenience function for simple MLPs and also offers the
    functionality to build MLPs with additional hidden layers using the activation,
    output, and feedforward functions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据准备好以符合 MXNet 模型的格式，并进行参数调优以找到最佳模型值后，我们进一步调整了模型，利用之前学到的内容来提升性能。所有这些步骤一起描述了一个完整的数据操作和转换周期，优化参数，最后运行我们的最终模型。我们展示了如何使用
    MXNet，它提供了一个简单的 MLP 便利函数，还可以通过激活、输出和前馈函数来构建带有额外隐藏层的 MLP。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Multilayer perceptrons are the simplest form of neural networks. They are feedforward
    without the feedback loops of recurrent neural networks, and all hidden layers
    are dense, fully connected layers, unlike convolutional neural networks, which
    feature convolutional layers and pooling layers. Given their simplicity, there
    are fewer options to adjust; however, in this chapter, we focused on adjusting
    the nodes in the hidden layer and looked at adding additional layers, as this
    aspect is the main element that separates neural network models, and as such,
    all deep learning methods from other machine learning algorithms. Using all the
    code in this chapter, you have learned how to process data so that it was ready
    to model, how to select the optimal number of nodes and layers, and how to train
    and evaluate a model using the `mxnet` library for R.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机是最简单的神经网络形式。它们是前馈式的，没有递归神经网络的反馈循环，所有隐藏层都是密集的完全连接层，不像卷积神经网络，它们包含卷积层和池化层。由于其简单性，调整的选项较少；然而，在本章中，我们专注于调整隐藏层中的节点数，并探索添加额外的层，因为这一点是区分神经网络模型和其他机器学习算法，进而区别深度学习方法的主要因素。通过本章中的所有代码，你已经学会了如何处理数据以使其准备好建模，如何选择最优的节点和层数，以及如何使用
    `mxnet` 库在 R 中训练和评估模型。
- en: In the next chapter, you will learn how to code deep autoencoders. This model
    is a form of unsupervised learning that is used to automatically categorize our
    input data. We will use this clustering process to code a recommender system using
    collaborative filtering.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习如何编写深度自编码器代码。这是一种无监督学习模型，用于自动分类输入数据。我们将使用这种聚类过程来编写一个基于协同过滤的推荐系统。
