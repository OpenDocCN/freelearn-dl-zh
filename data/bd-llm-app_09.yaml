- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Working with Code
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与代码一起工作
- en: In this chapter, we are going to cover another great capability of Large Language
    Models, that is, working with programming languages. In the previous chapter,
    we’ve already seen a glimpse of this capability, namely, SQL query generation
    in a SQL database. In this chapter, we are going to examine the other ways in
    which LLMs can be used with code, from “simple” code generation to interaction
    with code repositories and, finally, to the possibility of letting an application
    behave as if it were an algorithm. By the end of this chapter, you will be able
    to leverage LLMs to code-related projects, as well as build LLM-powered applications
    with natural language interfaces to work with code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍大型语言模型（LLM）的另一个伟大能力，即与编程语言一起工作。在前一章中，我们已经看到了这种能力的一瞥，即SQL数据库中的SQL查询生成。在本章中，我们将探讨LLM可以与代码一起使用的其他方式，从“简单”的代码生成到与代码仓库的交互，最后到让应用程序表现得像算法的可能性。到本章结束时，你将能够利用LLM进行与代码相关的项目，以及构建具有自然语言界面的LLM驱动应用程序来处理代码。
- en: 'Throughout this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Analysis of the main LLMs with top-performing code capabilities
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析具有最佳代码能力的顶级LLM
- en: Using LLMs for code understanding and generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM进行代码理解和生成
- en: Building LLM-powered agents to “act as” algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建充当算法的LLM代理
- en: Leveraging Code Interpreter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用代码解释器
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the tasks in this chapter, you will need the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的任务，你需要以下内容：
- en: A Hugging Face account and user access token.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face账户和用户访问令牌。
- en: An OpenAI account and user access token.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI账户和用户访问令牌。
- en: Python 3.7.1 or a later version.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7.1或更高版本。
- en: 'Python packages. Make sure you have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub`, `streamlit`, `codeinterpreterapi`,
    and `jupyter_kernel_gateway`. Those can be easily installed via `pip` `install`
    in your terminal.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保已安装以下Python包：`langchain`、`python-dotenv`、`huggingface_hub`、`streamlit`、`codeinterpreterapi`和`jupyter_kernel_gateway`。这些可以通过在终端中使用`pip
    install`轻松安装。
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_09.xhtml).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到所有代码和示例：[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_09.xhtml)。
- en: Choosing the right LLM for code
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的代码LLM
- en: In *Chapter 3*, we described a decision framework to use in order to decide
    the proper LLM for a given application. Generally speaking, all LLMs are endowed
    with knowledge of code understanding and generation; however, some of them are
    particularly specialized in doing so. More specifically, there are some evaluation
    benchmarks – such as the HumanEval – that are specifically tailored to assessing
    LLMs’ capabilities of working with code. The leaderboard of HumanEval One is a
    good source for determining the top-performing models, available at [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval).
    HumanEval is a benchmark introduced by OpenAI to assess the code generation capabilities
    of LLMs, where the model completes Python functions based on their signature and
    docstring. It has been used to evaluate models like Codex, demonstrating its effectiveness
    in measuring functional correctness.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*中，我们描述了一个决策框架，用于决定给定应用中适当的LLM。一般来说，所有LLM都具备代码理解和生成的知识；然而，其中一些特别擅长于此。更具体地说，有一些评估基准——例如HumanEval——专门用于评估LLM与代码工作的能力。HumanEval
    One的排行榜是一个很好的资源，可以确定表现最好的模型，可在[https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval)找到。HumanEval是由OpenAI引入的一个基准，用于评估LLM的代码生成能力，其中模型根据其签名和文档字符串完成Python函数。它已被用于评估像Codex这样的模型，证明了其在衡量功能正确性方面的有效性。
- en: 'In the following screenshot, you can see the situation of the leaderboard as
    of January 2024:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下屏幕截图，你可以看到截至2024年1月的排行榜情况：
- en: '![](img/B21714_09_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_09_01.png)'
- en: 'Figure 9.1: HumanEval benchmark in January 2024'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：2024年1月的HumanEval基准
- en: As you can see, the majority of the models are fine-tuned versions of the GPT-4
    (as well as the GPT-4 itself), as it is the state-of-the-art LLM in basically
    all the domains. Nevertheless, there are many open-source models that reached
    stunning results in the field of code understanding and generation, some of which
    will be covered in the next sections. Another benchmark is **Mostly Basic Programming
    Problems** (**MBPP**), a dataset of 974 programming tasks in Python, designed
    to be solvable by entry-level programmers. Henceforth, when choosing your model
    for a code-specific task, it might be useful to have a look at these benchmarks
    as well as other similar code metrics (we will see throughout the chapter some
    further benchmarks for code-specific LLMs).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，大多数模型都是GPT-4的微调版本（以及GPT-4本身），因为它是基本上所有领域中最先进的LLM。尽管如此，还有许多开源模型在代码理解和生成领域取得了惊人的成果，其中一些将在下一节中介绍。另一个基准是**大多数基本编程问题**（**MBPP**），一个包含974个Python编程任务的Python数据集，旨在由初级程序员解决。因此，在选择用于特定代码任务的模型时，查看这些基准以及其他类似的代码指标可能很有用（我们将在本章中看到一些针对特定代码LLM的进一步基准）。
- en: 'Staying within the scope of coding, below you can find three additional benchmarks
    often used in the market:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的范围内，以下您可以找到三个在市场上常用的额外基准：
- en: '**MultiPL-E**: An extension of HumanEval to many other languages, such as Java,
    C#, Ruby, and SQL.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MultiPL-E**：HumanEval的扩展，适用于许多其他语言，如Java、C#、Ruby和SQL。'
- en: '**DS-1000**: A data science benchmark that tests if the model can write code
    for common data analysis tasks in Python.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DS-1000**：一个数据科学基准，测试模型是否能够用Python编写常见的数据分析任务的代码。'
- en: '**Tech Assistant Prompt**: A prompt that tests if the model can act as a technical
    assistant and answer programming-related requests.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术助手提示**：一个测试模型是否能作为技术助手并回答编程相关请求的提示。'
- en: 'In this chapter, we are going to test different LLMs: two code-specific (CodeLlama
    and StarCoder) and one general-purpose, yet also with emerging capabilities in
    the field of code generation (Falcon LLM).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将测试不同的LLM：两个针对代码的（CodeLlama和StarCoder）和一个通用型，但在代码生成领域也具有新兴能力的（Falcon
    LLM）。
- en: Code understanding and generation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码理解和生成
- en: The first experiment we are going to run will be code understanding and generation
    leveraging LLMs. This simple use case is at the base of the many AI code assistants
    that were developed since the launch of ChatGPT, first among all the GitHub Copilot.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要进行的第一个实验将是利用LLM进行代码理解和生成。这个简单的用例是ChatGPT推出后开发的许多AI代码助手的基础，GitHub Copilot就是其中之一。
- en: '**Definition**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: GitHub Copilot is an AI-powered tool that assists developers in writing code
    more efficiently. It analyzes code and comments to provide suggestions for individual
    lines and entire functions. The tool is developed by GitHub, OpenAI, and Microsoft
    and supports multiple programming languages. It can perform various tasks such
    as code completion, modification, explanation, and technical assistance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot是一个由AI驱动的工具，它帮助开发者更高效地编写代码。它分析代码和注释，为单个行和整个函数提供建议。该工具由GitHub、OpenAI和Microsoft开发，支持多种编程语言。它可以执行各种任务，如代码补全、修改、解释和技术支持。
- en: 'In this experiment, we are going to try three different models: Falcon LLM,
    which we already explored in *Chapter 3*; CodeLlama, a fine-tuned version of Meta
    AI’s Llama; and StarCoder, a code-specific model that we are going to investigate
    in the upcoming sections.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将尝试三种不同的模型：Falcon LLM，这是我们已经在*第三章*中探讨过的；CodeLlama，这是Meta AI的Llama的微调版本；以及StarCoder，这是一个针对代码的特定模型，我们将在接下来的章节中对其进行研究。
- en: Since those models are pretty heavy to run on a local machine, for this purpose
    I’m going to use a Hugging Face Hub Inference Endpoint, with a GPU-powered virtual
    machine. You can link one model per Inference Endpoint and then embed it in your
    code, or use the convenient library `HuggingFaceEndpoint`, available in LangChain.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些模型在本地机器上运行相当沉重，为此我将使用一个Hugging Face Hub推理端点，它是一个由GPU驱动的虚拟机。您可以为每个推理端点链接一个模型，然后将其嵌入到您的代码中，或者使用LangChain中可用的方便的库`HuggingFaceEndpoint`。
- en: 'To start using your Inference Endpoint, you can use the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用您的推理端点，您可以使用以下代码：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, you can copy and paste the Python code provided on your endpoint’s
    webpage at [https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name](https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以将您端点网页上提供的Python代码复制并粘贴到您的端点：
- en: '![](img/B21714_09_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_09_02.png)'
- en: 'Figure 9.2: User interface of the Hugging Face Inference Endpoint'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：Hugging Face推理端点的用户界面
- en: To create your Hugging Face Inference Endpoint, you can follow the instructions
    at [https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建您的Hugging Face推理端点，您可以按照[https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index)中的说明进行操作。
- en: You can always leverage the free Hugging Face API as described in *Chapter 4*,
    but you have to expect some latency when running the models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以始终利用第4章中描述的免费Hugging Face API，但当你运行模型时，你必须预期会有一些延迟。
- en: Falcon LLM
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falcon LLM
- en: Falcon LLM is an open-source model developed by Abu Dhabi’s **Technology Innovation
    Institute** (**TII**) and launched on the market in May 2023\. It is an autoregressive,
    decoder-only transformer, trained on 1 trillion tokens, and has 40 billion parameters
    (although it has also been released as a lighter version with 7 billion parameters).
    As discussed in *Chapter 3*, “small” language models are a representation of a
    new trend of LLMs, consisting of building lighter models (with fewer parameters)
    that focus instead on the quality of the training dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon LLM是由阿布扎比的**技术创新研究所**（**TII**）开发的开源模型，并于2023年5月上市。它是一个自回归、仅解码器的transformer，在1万亿个标记上训练，拥有400亿个参数（尽管它也被发布为一个较轻的版本，参数为70亿）。在第3章中讨论过，“小型”语言模型是LLM的新趋势的体现，它侧重于构建更轻的模型（参数更少），而不是训练数据集的质量。
- en: 'To start using Falcon LLM, we can follow these steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Falcon LLM，我们可以按照以下步骤进行：
- en: 'We can leverage the HuggingFaceHub wrapper available in LangChain (remember
    to set the Hugging Face API in the `.env` file, passing your secrets as `os.environ["HUGGINGFACEHUB_API_TOKEN"]
    = HUGGINGFACEHUB_API_TOKEN`):'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以利用LangChain中可用的HuggingFaceHub包装器（记得在`.env`文件中设置Hugging Face API，将您的机密信息作为`os.environ["HUGGINGFACEHUB_API_TOKEN"]
    = HUGGINGFACEHUB_API_TOKEN`传递）：
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we’ve initialized the model, let’s ask it to generate the code for
    a simple webpage:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经初始化了模型，让我们要求它生成一个简单网页的代码：
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the corresponding output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的输出：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you save it as an HTML file and execute it, the result will look like the
    following:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您将其保存为HTML文件并执行它，结果将如下所示：
- en: '![A close up of a text  Description automatically generated](img/B21714_09_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![文本的特写  自动生成的描述](img/B21714_09_03.png)'
- en: 'Figure 9.3: Sample webpage generated by FalconLLM'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：FalconLLM生成的示例网页
- en: 'We can also try to generate a Python function to generate random passwords:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以尝试生成一个Python函数来生成随机密码：
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is our output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的输出：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We now have a function named `generate_password()`, which uses random functions
    to generate a password as per our prompt.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个名为`generate_password()`的函数，它使用随机函数根据我们的提示生成密码。
- en: 'Finally, let’s do the opposite, asking the model to explain to us the above
    code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们做相反的事情，要求模型向我们解释上述代码：
- en: '[PRE6]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the obtained output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是获得的结果：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Overall, even if not code-specific, the model was able to correctly perform
    all the tasks. Note also that this is the “light” version of the model (7 billion
    parameters), yet its performance is great.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，即使不是针对代码的，模型也能正确执行所有任务。请注意，这仍然是模型的“轻”版本（70亿参数），但性能仍然很出色。
- en: Let’s now investigate the capabilities of CodeLlama.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来调查CodeLlama的能力。
- en: CodeLlama
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CodeLlama
- en: CodeLlama is a family of LLMs for code based on Llama 2, which is a general-purpose
    language model developed by Meta AI (as discussed in *Chapter 3*). CodeLlama can
    generate and discuss code in various programming languages, such as Python, C++,
    Java, PHP, and more. CodeLlama can also perform infilling, which is the ability
    to fill in missing parts of code based on the surrounding context, as well as
    follow instructions given in natural language and produce code that matches the
    desired functionality.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CodeLlama是基于Llama 2的代码LLM系列，Llama 2是由Meta AI（在第3章中讨论过）开发的一种通用语言模型。CodeLlama可以在各种编程语言中生成和讨论代码，例如Python、C++、Java、PHP等。CodeLlama还可以执行补全，即根据周围上下文填充代码缺失部分的能力，以及遵循自然语言中的指令并生成符合所需功能代码。
- en: The model comes in three sizes (7B, 13B, and 34B parameters) and three flavors
    (base model, Python fine-tuned, and instruction-tuned) to cover a wide range of
    applications. CodeLlama is trained on sequences of 16k tokens and can handle inputs
    with up to 100k tokens.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有三种大小（7B、13B和34B参数）和三种风味（基础模型、Python微调模型和指令微调模型），以覆盖广泛的应用范围。CodeLlama在16k个标记的序列上训练，可以处理多达100k个标记的输入。
- en: 'In the model paper “Code Llama: Open Foundation Models for Code” by Rozière
    Baptiste et al, released in August 2023, the authors describe how the various
    models were tested against some of the most popular evaluation benchmarks in the
    domain of code understanding and generation, including HumanEval and MBPP, according
    to which CodeLlama models achieved a score up to 53% and 55%, respectively. On
    top of those remarkable results, it is stunning that the Python fine-tuned CodeLlama’s
    smallest size (7 billion parameters) outperformed the largest version of Llama
    2 (70 billion parameters) on HumanEval and MBPP.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '在Rozière Baptiste等人于2023年8月发布的模型论文“Code Llama: Open Foundation Models for Code”中，作者们描述了如何将各种模型与代码理解和生成领域中最受欢迎的评估基准进行了测试，包括HumanEval和MBPP。根据这些基准，CodeLlama模型在HumanEval和MBPP上的得分分别达到了53%和55%。在这些令人瞩目的结果之上，令人震惊的是，经过Python微调的CodeLlama最小版本（7亿参数）在HumanEval和MBPP上超过了Llama
    2最大版本（70亿参数）。'
- en: 'Now, let’s run some tests with this model. As per the previous section, we
    can initialize the model leveraging either the Hugging Face Inference API (pay
    per use) or the free Hugging Face API (with the constraint of higher latency).
    You can consume it as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用这个模型进行一些测试。根据前面的章节，我们可以利用Hugging Face推理API（按使用付费）或免费的Hugging Face API（具有更高的延迟限制）来初始化模型。您可以按以下方式使用它：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s now test it with some code tasks. The first task will be that of optimizing
    Python code so that it runs more efficiently. Let’s see how our model performs
    in this task. In the following code snippet, we simply prompt the model to regenerate
    the provided code in a more efficient way:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用一些代码任务来测试它。第一个任务将是优化Python代码，使其运行更高效。让我们看看我们的模型在这个任务上的表现。在下面的代码片段中，我们只是提示模型以更高效的方式重新生成提供的代码：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'def factorial(n):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'def factorial(n):'
- en: result = 1
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: result = 1
- en: 'for i in range(1, n + 1):'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'for i in range(1, n + 1):'
- en: result *= i
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: result *= i
- en: return result
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return result
- en: 'Example usage:'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例用法：
- en: n = 5
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: n = 5
- en: print("Factorial of", n, "is", factorial(n))
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: print("Factorial of", n, "is", factorial(n))
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is our output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的输出：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'def factorial(n):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'def factorial(n):'
- en: 'if n == 0:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'if n == 0:'
- en: return 1
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: return 1
- en: 'else:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'else:'
- en: return n * factorial(n - 1)
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: return n * factorial(n - 1)
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, the model was able to use a recursive approach, which is more
    efficient and “Pythonic.” It also provides a reference for the user to dive deeper
    into the mathematical theory behind the function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型能够使用递归方法，这更高效且“Pythonic”。它还提供了用户深入了解函数背后的数学理论的参考。
- en: Next, let’s leverage the model’s completion capabilities by initializing a function
    to remove non-ASCII characters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们利用模型的补全能力，初始化一个用于删除非ASCII字符的函数。
- en: '**Definition**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: '**American Standard Code for Information Interchange** (**ASCII**) is a character
    encoding standard that uses 7 bits to represent 128 characters, such as letters,
    digits, punctuation marks, and control codes.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**美国信息交换标准代码**（**ASCII**）是一个使用7位来表示128个字符的字符编码标准，如字母、数字、标点符号和控制代码。'
- en: Non-ASCII characters are those that are not part of the ASCII standard and use
    more than 7 bits to encode. They include special characters such as letters with
    accents, glyphs, ideograms, and mathematical symbols. Non-ASCII characters can
    be encoded using different standards, such as Unicode, ISO 8859-1, Windows-1252,
    etc.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 非ASCII字符是指不属于ASCII标准且使用超过7位进行编码的字符。它们包括带有重音的字母、符号、象形文字和数学符号等特殊字符。非ASCII字符可以使用不同的标准进行编码，例如Unicode、ISO
    8859-1、Windows-1252等。
- en: For example, the letter é is a non-ASCII character that can be encoded using
    Unicode as U+00E9 or using Windows-1252 as 0xE9.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，字母é是一个非ASCII字符，可以使用Unicode编码为U+00E9，或者使用Windows-1252编码为0xE9。
- en: 'Accordingly, here is the code to generate the function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下是生成函数的代码：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is the function that we receive as the output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的函数是我们收到的输出：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s now leverage the model as a bug fixer, prompting it with the wrong function
    and also asking it to provide an explanation of why it is wrong and how it can
    be fixed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们利用模型作为错误修复者，提示它使用错误的功能，并要求它提供错误的原因以及如何修复的解释：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#wrong function'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#错误的函数'
- en: import random
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: a = random.randint(1, 12)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: a = random.randint(1, 12)
- en: b = random.randint(1, 12)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: b = random.randint(1, 12)
- en: 'for i in range(10):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: question = "What is " + a + " x " + b + "? "
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: question = "What is " + a + " x " + b + "? "
- en: answer = input(question)
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: answer = input(question)
- en: 'if answer = a * b:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'if answer = a * b:'
- en: print (Well done!)
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: print (Well done!)
- en: 'else:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'else:'
- en: print("No.")
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: print("No.")
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is our output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的输出：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, let’s ask the model in natural language to generate specific Python
    code for a given task that, in our example, will be that of writing a function
    that finds the longest substring of a given string containing only unique characters:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们用自然语言向模型请求为给定任务生成特定的Python代码，在我们的例子中，这将是要编写一个函数，该函数可以找到给定字符串中只包含唯一字符的最长子串：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then get the following function as our output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到以下函数作为输出：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As per the Falcon LLM, in this case we used the light version of the model
    (7 billion parameters), still obtaining great results. This is a perfect example
    of how the task you want to address with your application must be a factor in
    deciding what LLM to use: if you are only interested in code generation, completion,
    infilling, debugging, or any other code-related tasks, a light and open-source
    model could be more than enough, rather than 70 billion parameters of a state-of-the-art
    GPT-4.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Falcon LLM，在这种情况下，我们使用了模型的轻量版本（70亿参数），仍然获得了很好的结果。这是一个完美的例子，说明了您想要通过应用程序解决的问题必须是在决定使用哪个LLM时的一个因素：如果您只对代码生成、补全、填充、调试或其他任何与代码相关的任务感兴趣，一个轻量级和开源的模型可能就足够了，而不是使用具有70亿参数的顶尖GPT-4。
- en: In the next section, we are going to cover the third and last LLM in the context
    of code generation and understanding.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍代码生成和理解背景下的第三个也是最后一个LLM。
- en: StarCoder
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StarCoder
- en: The StarCoder model is an LLM for code that can perform various tasks, such
    as code completion, code modification, code explanation, and technical assistance.
    It was trained on permissively licensed data from GitHub, including from 80+ programming
    languages, Git commits, GitHub issues, and Jupyter notebooks. It has a context
    length of over 8,000 tokens, which enables it to process more input than any other
    open-source language model. It also has an improved license that simplifies the
    process for companies to integrate the model into their products.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: StarCoder模型是一个代码LLM，可以执行各种任务，例如代码补全、代码修改、代码解释和技术支持。它是在GitHub的许可数据上训练的，包括来自80多种编程语言的Git提交、GitHub问题和Jupyter笔记本。它具有超过8,000个token的上下文长度，这使得它能够处理比任何其他开源语言模型更多的输入。它还拥有改进的许可，简化了公司将其模型集成到产品中的过程。
- en: The StarCoder model was evaluated on several benchmarks that test its ability
    to write and understand code in different languages and domains, including the
    aforementioned HumanEval and MBPP, where the model scored, respectively, 33.6%
    and 52.7%. Additionally, it was tested against MultiPL-E (where the model matched
    or outperformed the code-cushman-001 model from OpenAI on many languages), the
    DS-1000 (where the model clearly beat the code-cushman-001 model as well as all
    other open-access models), and the Tech Assistant Prompt (where the model was
    able to respond to various queries with relevant and accurate information).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: StarCoder模型在多个基准测试中被评估，以测试其在不同语言和领域编写和理解代码的能力，包括上述的HumanEval和MBPP，模型在这两个测试中分别获得了33.6%和52.7%的分数。此外，它还与MultiPL-E（在许多语言上，模型与OpenAI的code-cushman-001模型相匹配或表现更好）进行了测试，DS-1000（模型明显击败了code-cushman-001模型以及其他所有开放访问模型），以及Tech
    Assistant Prompt（模型能够以相关和准确的信息回答各种查询）。
- en: 'According to a survey published on May 4 2023 by Hugging Face, StarCoder demonstrated
    great capabilities compared to other models, using HumanEval and MBPP as benchmarks.
    You can see an illustration of this study below:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Hugging Face于2023年5月4日发布的一项调查，StarCoder与其他模型相比，在HumanEval和MBPP作为基准的情况下，展现了出色的能力。您可以在下面的插图看到这项研究：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_09_04.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/B21714_09_04.png)'
- en: 'Figure 9.4: Results of evaluation benchmarks for various LLMs. Source: [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：各种LLM的评估基准结果。来源：[https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)
- en: 'To start using StarCoder, we can follow these steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用StarCoder，我们可以遵循以下步骤：
- en: 'We can leverage the HuggingFaceHub wrapper available in LangChain (remember
    to set the Hugging Face API in the `.env` file):'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以利用LangChain中可用的HuggingFaceHub包装器（请记住在`.env`文件中设置Hugging Face API）：
- en: '[PRE20]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s set the `repo_id` for the StarCoder model and initialize it:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置 StarCoder 模型的 `repo_id` 并初始化它：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Note**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: StarCoder is a gated model on the Hugging Face Hub, meaning that you will need
    to request access directly from the bigcode/starcoderplus repo before being able
    to connect to it.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: StarCoder 是 Hugging Face Hub 上的一个门控模型，这意味着在能够连接到它之前，您需要直接从 bigcode/starcoderplus
    仓库请求访问权限。
- en: 'Now that we’re set up, let’s start asking our model to compile some code. To
    start with, we will ask it to generate a Python function to generate the nth Fibonacci
    number:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了，让我们开始要求我们的模型编译一些代码。首先，我们将要求它生成一个 Python 函数来生成第 n 个斐波那契数：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Definition**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: The Fibonacci sequence is a mathematical series that begins with 0 and 1, and
    each subsequent number is the sum of the two preceding numbers. For instance,
    the first 10 numbers of the Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21,
    and 34.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 斐波那契数列是一个以 0 和 1 开头的数学序列，每个后续数字是前两个数字的和。例如，斐波那契数列的前 10 个数字是 0, 1, 1, 2, 3, 5,
    8, 13, 21 和 34。
- en: 'There are different ways to compute the nth Fibonacci number, which is denoted
    by F(n). One way is to use a recursive formula:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 计算第 n 个斐波那契数（用 F(n) 表示）有不同的方法。一种方法是使用递归公式：
- en: '![](img/B21714_09_001.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![img/B21714_09_001.png](img/B21714_09_001.png)'
- en: This means that to find F(n), we need to find F(n-1) and F(n-2) first, and then
    add them together. This works for any n greater than or equal to 2\. For n equal
    to 0 or 1, we simply return n as the answer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着要找到 F(n)，我们首先需要找到 F(n-1) 和 F(n-2)，然后将它们相加。这对于任何大于或等于 2 的 n 都有效。对于 n 等于 0
    或 1，我们只需返回 n 作为答案。
- en: 'We then see the following output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后看到以下输出：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_09_05.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer Description automatically generated](img/B21714_09_05.png)'
- en: 'Figure 9.5: Example of Fibonacci functions generated by StarCode'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：StarCode 生成的斐波那契函数示例
- en: As you can see, it also proposed different approaches to solve the problem,
    alongside the explanation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它还提出了不同的方法来解决问题，并附有解释。
- en: 'Let’s now ask the model to generate a webpage to play tic tac toe against the
    computer:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们要求模型生成一个网页，与电脑玩井字棋：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the corresponding output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是相应的输出：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: git clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: git clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git
- en: cd tic-tac-toe
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: cd tic-tac-toe
- en: python3 -m http.server
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: python3 -m http.server
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Interestingly enough, the model in this case didn’t generate the whole code;
    rather, it gave the instructions to clone and run a git repository that can achieve
    this result.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这个模型并没有生成整个代码；相反，它给出了克隆和运行一个可以完成此结果的 git 仓库的指令。
- en: 'Finally, StarCoder is also available as an extension in VS Code to act as your
    code copilot. You can find it as **HF Code Autocomplete**, as shown in the following
    screenshot:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，StarCoder 也可以作为 VS Code 的扩展使用，作为您的代码伴侣。您可以在以下截图中的 **HF Code Autocomplete**
    中找到它：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_09_06.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer Description automatically generated](img/B21714_09_06.png)'
- en: 'Figure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：由 StarCoder 驱动的 Hugging Face 代码自动补全扩展
- en: 'Once enabled, you can see that, while compiling your code, StarCoder will provide
    suggestions to complete the code. For example:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用，您可以看到，在编译代码时，StarCoder 将提供代码补全建议。例如：
- en: '![](img/B21714_09_07.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![img/B21714_09_07.png](img/B21714_09_07.png)'
- en: 'Figure 9.7: Screenshot of a suggested completion, given a function description'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：给定函数描述的完成建议截图
- en: As you can see, I commented my code, describing a function to generate the nth
    Fibonacci number, and then started defining the function. Automatically, I’ve
    been provided with the StarCoder auto-completion suggestion.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我注释了我的代码，描述了一个生成第 n 个斐波那契数的函数，然后开始定义这个函数。自动地，我得到了 StarCoder 的自动补全建议。
- en: Code understanding and generation are great capabilities of LLMs. On top of
    those capabilities, there are further applications that we can think about, going
    beyond code generation. In fact, the code can be seen also as a backend reasoning
    tool to propose solutions to complex problems, such as an energy optimization
    problem rather than an algorithm task. To do this, we can leverage LangChain to
    create powerful agents that can *act as if they were algorithms*. In the upcoming
    section, we will see how to do so.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 代码理解和生成是LLM的强大功能。在这些能力之上，我们还可以考虑进一步的应用，这些应用超越了代码生成。事实上，代码也可以被视为一个后端推理工具，用于提出解决复杂问题的解决方案，如能源优化问题，而不是算法任务。为此，我们可以利用LangChain创建强大的代理，它们可以*表现得像算法一样*。在下一节中，我们将看到如何做到这一点。
- en: Act as an algorithm
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为算法行动
- en: Some problems are complex by definition and difficult to solve leveraging “only”
    LLMs’ analytical reasoning skills. However, LLMs are still intelligent enough
    to understand the problems overall and leverage their coding capabilities to solve
    them.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一些问题由于其本质而复杂，仅利用“仅”LLM的分析推理技能很难解决。然而，LLM仍然足够智能，能够理解问题整体，并利用它们的编码能力来解决它们。
- en: In this context, LangChain provides a tool that empowers the LLM to reason “in
    Python,” meaning that the LLM-powered agent will leverage Python to solve complex
    problems. This tool is the Python REPL, which is a simple Python shell that can
    execute Python commands. The Python REPL is important because it allows users
    to perform complex calculations, generate code, and interact with language models
    using Python syntax. In this section, we will cover some examples of the tool’s
    capabilities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，LangChain提供了一个工具，使LLM能够以“Python”的方式进行推理，这意味着由LLM驱动的代理将利用Python来解决复杂问题。这个工具是Python
    REPL，它是一个简单的Python外壳，可以执行Python命令。Python REPL之所以重要，是因为它允许用户使用Python语法进行复杂计算、生成代码，并与语言模型进行交互。在本节中，我们将介绍该工具的一些功能示例。
- en: 'Let’s first initialize our agent using the `create_python_agent` class in LangChain.
    To do so, we will need to provide this class with an LLM and a tool, which, in
    our example, will be the Python REPL:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用LangChain中的`create_python_agent`类初始化我们的代理。为此，我们需要向这个类提供一个LLM和一个工具，在我们的例子中，这个工具将是Python
    REPL：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As always, before starting to work with the agent, let’s first inspect the
    default prompt:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，在开始与代理一起工作之前，让我们首先检查默认提示：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is our output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们的输出：
- en: '![A screenshot of a computer program  Description automatically generated](img/B21714_09_08.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B21714_09_08.png)'
- en: 'Figure 9.8: Default prompt of the Python agent'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：Python代理的默认提示
- en: 'Now, let’s start with an easy query, asking the model to generate a scatter
    plot based on sample attributes of basketball players:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从一个简单的查询开始，要求模型根据篮球运动员的样本属性生成散点图：
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then get the following output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到以下输出：
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This output is accompanied by the following graph based on the players’ statistics:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出附带了以下基于球员统计数据的图表：
- en: '![A screenshot of a computer program  Description automatically generated](img/B21714_09_09.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B21714_09_09.png)'
- en: 'Figure 9.9: Sample plot generated by the Python agent'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：Python代理生成的示例绘图
- en: 'Let’s look at another example. Say we want to predict the price of a house
    based on some features, such as the number of bedrooms or the size of the house.
    To do so, we can ask our agent to design and train a model to give us the result
    of a given house. For example, let’s consider the following prompt:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个例子。假设我们想要根据一些特征（如卧室数量或房屋大小）预测房屋价格。为此，我们可以要求我们的代理设计和训练一个模型，给出给定房屋的结果。例如，让我们考虑以下提示：
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, we ask the agent to train a regression model on synthetic data (representative
    of houses with various configurations of rooms, bathrooms, and area, each with
    an associated price as a dependent variable) to give us the estimated price of
    a house with the above features. Let’s see the output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们要求代理在合成数据（代表具有各种房间、浴室和面积配置的房屋，每个配置都有一个相关的价格作为因变量）上训练回归模型，以给出具有上述特征的房屋的估计价格。让我们看看输出：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, the agent was able to generate synthetic training data, train
    a proper regression model using the `sklearn` libraries, and predict with the
    model the price of the house we provided.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，代理能够生成合成训练数据，使用`sklearn`库训练适当的回归模型，并使用模型预测我们提供的房屋价格。
- en: 'With this approach, we can program an agent to act as an algorithm in real-time
    scenarios. Imagine, for example, that we want to design an agent that is capable
    of solving optimization problems in a smart building environment. The goal is
    to optimize the **Heating, Ventilation and Air Conditioning** (**HVAC**) setpoints
    in the building to minimize energy costs while ensuring occupant comfort. Let’s
    define the variables and constraints of the problem: the objective is to adjust
    the temperature setpoints within the specified comfort ranges for each of the
    three zones while considering the varying energy costs per degree, per hour.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们可以编程一个智能体在实时场景中充当算法。想象一下，例如，我们想要设计一个智能体，使其能够在智能建筑环境中解决优化问题。目标是优化建筑中的**供暖、通风和空调**（**HVAC**）设定点，以最小化能源成本同时确保居住者的舒适度。让我们定义问题的变量和约束条件：目标是调整三个区域中每个区域的温度设定点，在指定的舒适范围内，同时考虑每度、每小时的能源成本变化。
- en: 'The goal is to strike a balance between energy efficiency and occupant comfort.
    Below, you can find a description of the problem and also the initialization of
    our variables and constraints (energy cost per zone, initial temperature per zone,
    and comfort range per zone):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在能源效率和居住者舒适度之间取得平衡。以下你可以找到问题的描述，以及我们变量和约束条件的初始化（每个区域的能源成本、每个区域的初始温度和舒适度范围）：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then get the following output (you can find the whole reasoning chain in
    the book’s GitHub repository):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后得到以下输出（你可以在书籍的GitHub仓库中找到整个推理链）：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The agent was able to solve the smart building optimization problem, finding
    the minimum total energy cost, given some constraints. Staying in the scope of
    optimization problems, there are further use cases that these models could address
    with a similar approach, including:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体能够解决智能建筑优化问题，在给定一些约束条件下找到最小的总能源成本。在优化问题的范围内，还有其他一些使用案例，这些模型可以用类似的方法解决，包括：
- en: '**Supply chain optimization**: Optimize the logistics and distribution of goods
    to minimize transportation costs, reduce inventory, and ensure timely deliveries.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**供应链优化**：优化商品物流和分销，以最小化运输成本、减少库存并确保及时交付。'
- en: '**Portfolio optimization**: In finance, use algorithms to construct investment
    portfolios that maximize returns while managing risk.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资组合优化**：在金融领域，使用算法构建投资组合，以最大化回报同时管理风险。'
- en: '**Route planning**: Plan optimal routes for delivery trucks, emergency services,
    or ride-sharing platforms to minimize travel time and fuel consumption.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路线规划**：为送货卡车、紧急服务或共享出行平台规划最优路线，以最小化旅行时间和燃料消耗。'
- en: '**Manufacturing process optimization**: Optimize manufacturing processes to
    minimize waste, energy consumption, and production costs while maintaining product
    quality.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制造流程优化**：优化制造流程，以最小化浪费、能源消耗和生产成本，同时保持产品质量。'
- en: '**Healthcare resource allocation**: Allocate healthcare resources like hospital
    beds, medical staff, and equipment efficiently during a pandemic or other healthcare
    crisis.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗资源分配**：在大流行或其他医疗危机期间，高效分配医疗资源，如医院床位、医疗人员和设备。'
- en: '**Network routing**: Optimize data routing in computer networks to reduce latency,
    congestion, and energy consumption.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络路由**：优化计算机网络中的数据路由，以减少延迟、拥塞和能源消耗。'
- en: '**Fleet management**: Optimize the use of a fleet of vehicles, such as taxis
    or delivery vans, to reduce operating costs and improve service quality.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**车队管理**：优化车队（如出租车或送货货车）的使用，以降低运营成本并提高服务质量。'
- en: '**Inventory management**: Determine optimal inventory levels and reorder points
    to minimize storage costs while preventing stockouts.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库存管理**：确定最优库存水平和再订购点，以最小化存储成本同时防止缺货。'
- en: '**Agricultural planning**: Optimize crop planting and harvesting schedules
    based on weather patterns and market demand to maximize yield and profits.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**农业规划**：根据天气模式和市场需求优化作物种植和收获计划，以最大化产量和利润。'
- en: '**Telecommunications network design**: Design the layout of telecommunications
    networks to provide coverage while minimizing infrastructure costs.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电信网络设计**：设计电信网络的布局，以提供覆盖范围同时最小化基础设施成本。'
- en: '**Waste management**: Optimize routes for garbage collection trucks to reduce
    fuel consumption and emissions.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**废物管理**：优化垃圾收集车的路线以减少燃料消耗和排放。'
- en: '**Airline crew scheduling**: Create efficient flight crew schedules that adhere
    to labor regulations and minimize costs for airlines.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**航空公司机组人员排班**：创建符合劳动法规且能最小化航空公司成本的航班机组人员排班。'
- en: 'The Python REPL agent is amazing; however, it comes with some caveats:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Python REPL代理非常出色；然而，它也有一些限制：
- en: It does not allow for FileIO, meaning that it cannot read and write with your
    local file system.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不允许进行文件输入输出（FileIO），这意味着它不能与您的本地文件系统进行读写操作。
- en: It forgets the variables after every run, meaning that you cannot keep trace
    of your initialized variables after the model’s response.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次运行后都会忘记变量，这意味着您不能在模型响应后追踪初始化的变量。
- en: 'To bypass these caveats, in the next section, we are going to cover an open-source
    project built on top of the LangChain agent: the Code Interpreter API.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这些限制，在下一节中，我们将介绍一个基于LangChain代理构建的开源项目：代码解释器API。
- en: Leveraging Code Interpreter
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用代码解释器
- en: The name “Code Interpreter” was coined by OpenAI, referring to the recently
    developed plugin for ChatGPT. The Code Interpreter plugin allows ChatGPT to write
    and execute computer code in various programming languages. This enables ChatGPT
    to perform tasks such as calculations, data analysis, and generating visualizations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: “代码解释器”这个名字是由OpenAI提出的，指的是最近为ChatGPT开发的插件。代码解释器插件允许ChatGPT在多种编程语言中编写和执行计算机代码。这使得ChatGPT能够执行诸如计算、数据分析以及生成可视化等任务。
- en: The Code Interpreter plugin is one of the tools designed specifically for language
    models with safety as a core principle. It helps ChatGPT access up-to-date information,
    run computations, or use third-party services. The plugin is currently in private
    beta and is available for selected developers and ChatGPT Plus users.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 代码解释器插件是专为具有安全核心原则的语言模型设计的工具之一。它帮助ChatGPT访问最新信息，运行计算或使用第三方服务。该插件目前处于私有测试阶段，仅对选定开发者和ChatGPT
    Plus用户开放。
- en: While OpenAI’s Code Interpreter still doesn’t offer an API, there are some open-source
    projects that adapted the concept of this plugin in an open-source Python library.
    In this section, we are going to leverage the work of Shroominic, available at
    [https://github.com/shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api).
    You can install it via `pip install codeinterpreterapi`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然OpenAI的代码解释器（Code Interpreter）仍然不提供API，但有一些开源项目已经将此插件的概念应用于开源Python库中。在本节中，我们将利用Shroominic的工作，该工作可在[https://github.com/shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api)找到。您可以通过`pip
    install codeinterpreterapi`来安装它。
- en: According to the blog post published by Shroominic, the author of the Code Interpreter
    API (which you can read at [https://blog.langchain.dev/code-interpreter-api/](https://blog.langchain.dev/code-interpreter-api/)),
    it is based on the LangChain agent `OpenAIFunctionsAgent`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Shroominic发布的博客文章，他是代码解释器API的作者（您可以在[https://blog.langchain.dev/code-interpreter-api/](https://blog.langchain.dev/code-interpreter-api/)阅读），该API基于LangChain代理`OpenAIFunctionsAgent`。
- en: '**Definition**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: OpenAIFunctionsAgent is a type of agent that can use the OpenAI functions’ ability
    to respond to the user’s prompts using an LLM. The agent is driven by a model
    that supports using OpenAI functions, and it has access to a set of tools that
    it can use to interact with the user.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIFunctionsAgent是一种可以使用OpenAI函数能力通过LLM响应用户提示的代理。该代理由支持使用OpenAI函数的模型驱动，并且可以访问一组它可以用来与用户交互的工具。
- en: The OpenAIFunctionsAgent can also integrate custom functions. For example, you
    can define custom functions to get the current stock price or stock performance
    using Yahoo Finance. The OpenAIFunctionsAgent can use the ReAct framework to decide
    which tool to use, and it can use memory to remember the previous conversation
    interactions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIFunctionsAgent也可以集成自定义函数。例如，您可以使用Yahoo Finance定义自定义函数来获取当前的股票价格或股票表现。OpenAIFunctionsAgent可以使用ReAct框架来决定使用哪个工具，并且可以使用内存来记住之前的对话交互。
- en: The API comes already with some tools, such as the possibility to navigate the
    web to get up-to-date information.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 该API已经内置了一些工具，例如导航网络以获取最新信息的能力。
- en: Yet the greatest difference from the Python REPL tool that we covered in the
    previous section is that the Code Interpreter API can actually execute the code
    it generates. In fact, when a Code Interpreter session starts, a miniature of
    a Jupyter Kernel is launched on your device, thanks to the underlying Python execution
    environment called CodeBox.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与我们在上一节中介绍的 Python REPL 工具相比，最大的不同之处在于代码解释器API实际上可以执行它生成的代码。实际上，当代码解释器会话开始时，由于底层名为
    CodeBox 的 Python 执行环境，您的设备上会启动一个 Jupyter 内核的迷你版。
- en: 'To start using the code interpreter in your notebook, you can install all the
    dependencies as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始在您的笔记本中使用代码解释器，您可以按照以下方式安装所有依赖项：
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In this case, I will ask it to generate a plot of COVID-19 cases in a specific
    time range:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我会要求它生成特定时间范围内COVID-19病例的图表：
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is the generated output, including a graph that shows the number of global
    confirmed cases in the specified time period:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成的输出，包括一个图表，显示了指定时间段的全球确诊病例数：
- en: '[PRE36]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![A graph with a line going up  Description automatically generated](img/B21714_09_10.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![一个向上的线条图表，描述自动生成](img/B21714_09_10.png)'
- en: 'Figure 9.10: Line chart generated by the Code Intepreter API'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：由代码解释器API生成的折线图
- en: As you can see, the Code Interpreter answered the question with an explanation
    as well as a plot.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，代码解释器不仅用解释回答了问题，还用图表进行了展示。
- en: 'Let’s try another one, this time also leveraging its real-time capabilities
    of searching for up-to-date information. In the following snippet, we ask the
    model to plot the price of the S&P 500 index over the last 5 days:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一个，这次也利用其搜索最新信息的实时能力。在下面的代码片段中，我们要求模型绘制过去5天内 S&P 500指数的价格：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We then get the following output, together with a line graph showing the price
    of the S&P 500 index over the last 5 days:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到以下输出，以及一个显示过去5天内S&P 500指数价格的折线图：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![A graph with numbers and a line  Description automatically generated](img/B21714_09_11.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![一个带有数字和线的图表，描述自动生成](img/B21714_09_11.png)'
- en: 'Figure 9.11: S&P 500 index price plotted by the Code Interpreter API'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：由代码解释器API绘制的S&P 500指数价格
- en: Finally, we can provide local files to the Code Interpreter so that it can perform
    some analyses on that specific data. For example, I’ve downloaded the Titanic
    dataset from Kaggle at [https://www.kaggle.com/datasets/brendan45774/test-file](https://www.kaggle.com/datasets/brendan45774/test-file).
    The Titanic dataset is a popular dataset for machine learning that describes the
    survival status of individual passengers on the Titanic. It contains information
    such as age, sex, class, fare, and whether they survived or not.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以向代码解释器提供本地文件，以便它可以对该特定数据进行一些分析。例如，我从 Kaggle 下载了泰坦尼克号数据集，网址为 [https://www.kaggle.com/datasets/brendan45774/test-file](https://www.kaggle.com/datasets/brendan45774/test-file)。泰坦尼克号数据集是机器学习的一个流行数据集，描述了泰坦尼克号上个别乘客的生存状态。它包含诸如年龄、性别、舱位、船票和是否幸存等信息。
- en: 'Once the dataset had downloaded, I passed it as a parameter to the model as
    follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集下载完成，我就将其作为参数传递给模型，如下所示：
- en: '[PRE39]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We then get the following output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到以下输出：
- en: '[PRE40]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![A screenshot of a graph  Description automatically generated](img/B21714_09_12.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的截图，描述自动生成](img/B21714_09_12.png)'
- en: 'Figure 9.12: Sample plots generated by the Code Interpreter API'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：代码解释器API生成的示例图表
- en: As you can see, the model was able to generate to bar charts showing the survival
    status grouped by sex (in the first plot) and then by class (in the second plot).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型能够生成条形图，展示了按性别（在第一个图表中）和类别（在第二个图表中）分组的生存状态。
- en: 'The Code Interpreter plugin, together with code-specific LLMs and the Python
    agent, are great examples of how LLMs are having a huge impact on the world of
    software development. This can be summarized in two main capabilities:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 代码解释器插件，连同特定于代码的 LLM 和 Python 代理，是 LLM 对软件开发世界产生巨大影响的绝佳例子。这可以总结为两个主要能力：
- en: LLMs can understand and generate code, since they have been trained on a huge
    amount of programming languages, GitHub repos, StackOverflow conversations, and
    so on. Henceforth, along with natural language, programming languages are part
    of their parametric knowledge.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 可以理解和生成代码，因为它们已经在大量的编程语言、GitHub 仓库、StackOverflow 对话等上进行了训练。因此，除了自然语言之外，编程语言也成为了它们参数化知识的一部分。
- en: LLMs can understand a user’s intent and act as a reasoning engine to activate
    tools like Python REPL or Code Interpreter, which are then able to provide a response
    by working with code.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可以理解用户的意图，并作为推理引擎激活Python REPL或Code Interpreter等工具，然后通过与代码一起工作来提供响应。
- en: 'Overall, LLMs are going well beyond the elimination of the gap between natural
    language and machine language: rather, they are integrating the two so that they
    can leverage each other to respond to a user’s query.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LLMs正在超越消除自然语言和机器语言之间差距的目标：相反，它们正在将两者结合起来，以便它们可以相互利用来响应用户的查询。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored multiple ways in which LLMs can be leveraged to
    work with code. Armed with a refresher of how to evaluate LLMs and the specific
    evaluation benchmarks to take into account when choosing an LLM for code-related
    tasks, we delved into practical experimentations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了多种利用LLMs与代码一起工作的方法。在回顾了如何评估LLMs以及在选择LLM进行代码相关任务时需要考虑的具体评估基准后，我们深入进行了实际实验。
- en: We started from the “plain vanilla” application that we have all tried at least
    once using ChatGPT, which is code understanding and generation. For this purpose,
    we leveraged three different models – Falcon LLM, CodeLlama, and StarCoder – each
    resulting in very good results.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从我们都至少尝试过一次的“普通”应用开始，即使用ChatGPT进行代码理解和生成。为此，我们利用了三个不同的模型——Falcon LLM、CodeLlama和StarCoder，每个模型都取得了非常好的结果。
- en: We then moved forward with the additional applications that LLMs’ coding capabilities
    can have in the real world. In fact, we saw how code-specific knowledge can be
    used as a booster to solve complex problems, such as algorithmic or optimization
    tasks. Furthermore, we covered how code knowledge can not only be used in the
    backend reasoning of an LLM but also actually executed in a working notebook,
    leveraging the open-source version of the Code Interpreter API.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后继续探讨LLMs的编码能力在现实世界中的额外应用。事实上，我们看到了如何将特定于代码的知识用作解决复杂问题的助推器，例如算法或优化任务。此外，我们还介绍了代码知识不仅可以在LLM的后端推理中使用，还可以在实际的工作笔记本中执行，利用Code
    Interpreter API的开源版本。
- en: With this chapter, we are getting closer to the end of Part 2\. So far, we have
    covered the multiple capabilities of LLMs, while always handling language data
    (natural or code). In the next chapter, we will see how to go a step further toward
    multi-modality and build powerful multi-modal agents that can handle data in multiple
    formats.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们越来越接近第二部分的结尾。到目前为止，我们已经涵盖了LLMs的多种能力，同时始终处理语言数据（自然或代码）。在下一章中，我们将看到如何进一步迈向多模态，构建能够处理多种格式数据的强大多模态代理。
- en: References
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The open-source version of the Code Interpreter API: [https://github.com/shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Code Interpreter API的开源版本：[https://github.com/shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api)
- en: 'StarCoder: [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'StarCoder: [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)'
- en: 'The LangChain agent for the Python REPL: [https://python.langchain.com/docs/integrations/toolkits/python](https://python.langchain.com/docs/integrations/toolkits/python)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python REPL的LangChain代理：[https://python.langchain.com/docs/integrations/toolkits/python](https://python.langchain.com/docs/integrations/toolkits/python)
- en: 'A LangChain blog about the Code Interpreter API: [https://blog.langchain.dev/code-interpreter-api/](https://blog.langchain.dev/code-interpreter-api/)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于Code Interpreter API的LangChain博客：[https://blog.langchain.dev/code-interpreter-api/](https://blog.langchain.dev/code-interpreter-api/)
- en: 'The Titanic dataset: [https://www.kaggle.com/datasets/brendan45774/test-file](https://www.kaggle.com/datasets/brendan45774/test-file)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泰坦尼克号数据集：[https://www.kaggle.com/datasets/brendan45774/test-file](https://www.kaggle.com/datasets/brendan45774/test-file)
- en: 'The HF Inference Endpoint: [https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index
    )'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HF推理端点：[https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index)
- en: 'The CodeLlama model card: [https://huggingface.co/codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CodeLlama模型卡片：[https://huggingface.co/codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)
- en: 'Code Llama: Open Foundation Models for Code, *Rozière. B., et al* (2023): [https://arxiv.org/abs/2308.12950](https://arxiv.org/abs/2308.12950)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Code Llama：开源代码基础模型，*Rozière. B.，等人*（2023年）：[https://arxiv.org/abs/2308.12950](https://arxiv.org/abs/2308.12950)
- en: 'The Falcon LLM model card: [https://huggingface.co/tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鹰隼 LLM 模型卡片：[https://huggingface.co/tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
- en: 'The StarCoder model card: [https://huggingface.co/bigcode/starcoder](https://huggingface.co/bigcode/starcoder)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StarCoder 模型卡片：[https://huggingface.co/bigcode/starcoder](https://huggingface.co/bigcode/starcoder)
- en: Join our community on Discord
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm]( https://packt.link/llm )'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code214329708533108046.png)'
