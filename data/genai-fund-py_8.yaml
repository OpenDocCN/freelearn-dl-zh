- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Addressing Ethical Considerations and Charting a Path Toward Trustworthy Generative
    AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决伦理考量并绘制通往可信赖生成式AI的路径
- en: As generative AI advances, it will extend beyond basic language tasks, integrating
    into daily life and impacting almost every sector. The inevitability of its widespread
    adoption highlights the need to address its ethical implications. The promise
    of this technology to revolutionize industries, enhance creativity, and solve
    complex problems must be coupled with the responsibility to navigate its ethical
    landscape diligently. This chapter will explore these ethical considerations,
    dissect the intricacies of biases entangled in these models, and look at strategies
    for cultivating trust in general-purpose AI systems. Through thorough examination
    and reflection, we can begin to outline a path toward responsible use, helping
    to ensure that advancements in generative AI are leveraged for the greater good
    while minimizing harm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成式AI的进步，它将超越基本语言任务，融入日常生活，并影响几乎每个行业。其广泛采用的必然性突出了解决其伦理影响的必要性。这项技术有望变革行业、增强创造力和解决复杂问题，必须与认真导航其伦理景观的责任相结合。本章将探讨这些伦理考量，剖析这些模型中纠缠在一起的偏见复杂性，并探讨培养通用人工智能系统信任的策略。通过彻底的审查和反思，我们可以开始勾勒出一条负责任使用的路径，帮助确保生成式AI的进步被用于更大的善，同时最大限度地减少伤害。
- en: To ground our discussion, we will first identify some ethical norms and universal
    values relevant to generative AI. While this chapter cannot be exhaustive, it
    aims to introduce key ethical considerations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的讨论有据可依，我们首先将确定一些与生成式AI相关的伦理规范和普遍价值观。虽然本章不能详尽无遗，但它旨在介绍关键的伦理考量。
- en: Ethical norms and values in the context of generative AI
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI背景下的伦理规范和价值
- en: 'The ethical norms and values guiding the development and deployment of generative
    AI are rooted in transparency, equity, accountability, privacy, consent, security,
    and inclusivity. These principles can serve as a foundation for developing and
    adopting systems aligned with societal values and supporting the greater good.
    Let’s explore these in detail:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 指导生成式AI开发和部署的伦理规范和价值根植于透明度、公平性、问责制、隐私、同意、安全和包容性。这些原则可以作为开发并采用符合社会价值观并支持更大善的系统的基础。让我们详细探讨这些原则：
- en: '**Transparency** involves clearly explaining the methodologies, data sources,
    and processes behind large language model (LLM) construction. This practice builds
    trust by enabling stakeholders to understand the technology’s reliability and
    limits. For example, a company could publish a detailed report on the types of
    data trained on their LLM and the steps taken to ensure data privacy and bias
    mitigation.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度**涉及清楚地解释大型语言模型（LLM）构建背后的方法、数据来源和过程。这种做法通过使利益相关者能够理解技术的可靠性和局限性来建立信任。例如，一家公司可以发布一份详细的报告，说明其LLM训练的数据类型以及为确保数据隐私和偏见缓解所采取的步骤。'
- en: '**Equity** in the context of LLMs ensures fair treatment and outcomes for all
    users by actively preventing biases in models. This requires thorough analysis
    and correction of training data and continuous monitoring of exchanges to reduce
    discrimination. One measure a firm might apply is a routine review of LLM performance
    across various demographic groups to identify and address unintended biases.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM（大型语言模型）的背景下，**公平性**通过积极预防模型中的偏见，确保所有用户都得到公平对待和结果。这需要彻底分析并纠正训练数据，并持续监控交易以减少歧视。一家公司可能采取的措施是对LLM在各种人口群体中的表现进行常规审查，以识别和解决未预见的偏见。
- en: '**Accountability** establishes that developers and users of LLMs are responsible
    for model outputs and impacts. It includes transparent and accessible mechanisms
    for reporting and addressing negative consequences or ethical violations. In practice,
    this could manifest as the establishment of an independent review board that oversees
    AI projects and intervenes in cases of ethical misconduct.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**确立LLM的开发者和用户对模型输出和影响负责。它包括透明和易于访问的报告和解决负面后果或伦理违规的机制。在实践中，这可能表现为建立一个独立的审查委员会，监督AI项目并在伦理不当行为的情况下进行干预。'
- en: '**Privacy and consent**, in principle, involves ensuring that individual privacy
    and consent are respected and preserved during the use of personal data as input
    to LLMs. In practice, developers should avoid using personal data for training
    without explicit permission and implement strong data protection measures. For
    example, a developer might use data anonymization or privacy-preserving techniques
    to train models, ensuring that personal identifiers and sensitive information
    are removed before data processing.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私和同意**原则上涉及确保在将个人数据作为LLM输入使用时尊重和保护个人隐私和同意。在实践中，开发者应避免在没有明确许可的情况下使用个人数据进行训练，并实施强大的数据保护措施。例如，开发者可能使用数据匿名化或隐私保护技术来训练模型，确保在数据处理之前移除个人标识符和敏感信息。'
- en: '**Security** involves protecting LLM-integrated systems and their data from
    unauthorized access and cyber threats. In practice, setting up LLM-specific red
    teams (or teams that test defenses by simulating attacks) can help safeguard AI
    systems against potential breaches.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全**涉及保护集成LLM的系统及其数据免受未经授权的访问和网络威胁。在实践中，建立LLM特定的红队（或通过模拟攻击测试防御的团队）可以帮助保护AI系统免受潜在的安全漏洞。'
- en: '**Inclusivity** involves the deliberate effort to include diverse voices and
    perspectives in the development process of LLMs, ensuring the technology is accessible
    and beneficial to a broad spectrum of users. In practice, it is vital to collaborate
    with socio-technical subject-matter experts who can guide appropriate actions
    to promote and preserve inclusion.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包容性**涉及在LLM的开发过程中有意识地包括不同的声音和观点，确保技术对广泛的用户可访问和有益。在实践中，与能够指导适当行动以促进和保护包容性的社会-技术领域的专家合作至关重要。'
- en: This set of principles is not comprehensive but may help to form a conceptual
    foundation for ethical LLM development and adoption with the universal goal of
    advancing the technology in ways that avoid harm.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这套原则并非全面，但可能有助于形成道德LLM开发和采用的 conceptual foundation，其普遍目标是避免伤害地推进技术。
- en: Additionally, various leading authorities have published guidance regarding
    responsible AI, inclusive of ethical implications. These include the US Department
    of Commerce’s **National Institute of Standards and Technology** (**NIST**), Stanford
    University’s **Institute for Human-Centered Artificial Intelligence** (**HAI**),
    and the **Distributed AI Research Institute** (**DAIR**), to name a few.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，各种领先的权威机构已经发布了关于负责任AI的指南，包括伦理影响。这些包括美国商务部**国家标准与技术研究院**（**NIST**）、斯坦福大学**以人为本的人工智能研究所**（**HAI**）和**分布式人工智能研究院**（**DAIR**），等等。
- en: Investigating and minimizing bias in generative LLMs and generative image models
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查和最小化生成式LLM和生成式图像模型中的偏差
- en: Bias in generative AI models, including both LLMs and generative image models,
    is a complex issue that requires careful investigation and mitigation strategies.
    Bias can manifest as unintended stereotypes, inaccuracies, and exclusions in the
    generated outputs, often stemming from biased datasets and model architectures.
    Recognizing and addressing these biases is crucial to creating equitable and trustworthy
    AI systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型中的偏差，包括LLM和生成式图像模型，是一个复杂的问题，需要仔细的调查和缓解策略。偏差可能表现为在生成输出中的无意刻板印象、不准确性和排除，通常源于有偏差的数据集和模型架构。识别和解决这些偏差对于创建公平和值得信赖的AI系统至关重要。
- en: At its core, algorithmic or model bias refers to systematic errors that lead
    to preferential treatment or unfair outcomes for certain groups. In generative
    AI, this can appear as gender, racial, or socioeconomic biases in outputs, often
    mirroring societal stereotypes. For example, an LLM may produce content that reinforces
    these biases, reflecting the historical and societal biases present in its training
    data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，算法或模型偏差指的是导致某些群体受到优先对待或不公平结果的系统性错误。在生成式AI中，这可能会表现为输出中的性别、种族或社会经济偏差，通常反映了社会刻板印象。例如，一个大型语言模型（LLM）可能会生成强化这些偏差的内容，反映出其训练数据中存在的历
    史和社会偏差。
- en: Let us again revisit our hypothetical fashion retailer, StyleSprint. Consider
    a situation where StyleSprint experimented with using a multimodal generative
    LLM model to generate promotional images and captions for its latest sneaker line.
    It finds that the model predominantly generates sneakers in urban, graffiti-laden
    backgrounds, unintentionally drawing an association that relies on stereotypes.
    Moreover, the team begins noticing that the captions are also laden with language
    that perpetuates stereotypes. This realization prompts a reevaluation of the imagery
    and text, first with an investigation of how the problem surfaced.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下我们的假设时尚零售商，StyleSprint。考虑一种情况，StyleSprint尝试使用多模态生成式LLM模型为其最新的运动鞋系列生成促销图像和标题。它发现该模型主要生成背景为城市、涂鸦遍布的鞋子，无意中引发了一种基于刻板印象的联想。此外，团队开始注意到标题中也充满了持续刻板印象的语言。这一认识促使他们对图像和文本进行重新评估，首先是对问题如何出现进行调查研究。
- en: Investigating bias involves various techniques, from analyzing the diversity
    and representativeness of training datasets to implementing testing protocols
    that specifically look for biased outputs across different demographics and scenarios.
    Statistical analysis can reveal disparities in model outcomes, while comparative
    studies and user feedback can help identify biases in the generated content.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 调查偏见涉及各种技术，从分析训练数据集的多样性和代表性到实施针对不同群体和场景的特定测试协议以寻找偏见输出。统计分析可以揭示模型结果的差异，而比较研究和用户反馈可以帮助识别生成内容中的偏见。
- en: 'In this case, let us assume that StyleSprint was using an LLM-provider without
    the ability to influence its training data or development process. To mitigate
    the risk of bias, the team might employ the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，假设StyleSprint使用的是一个无法影响其训练数据或开发过程的LLM提供商。为了减轻偏见风险，团队可能采取以下措施：
- en: Post-processing adjustments to diversify the imagery, ensuring a broader representation
    of backgrounds that resonate with its customer base
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理调整以多样化图像，确保更广泛地反映与其客户群产生共鸣的背景
- en: The institution of a manual review process, enlisting team members to scrutinize
    and curate AI-generated images and captions before publishing (i.e., “human-in-the-loop”),
    ensuring that every piece of content aligns with the brand’s commitment to diversity
    and inclusion
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个手动审查流程，让团队成员仔细审查和精选AI生成的图像和标题，在发布之前确保每件内容都符合品牌对多元化和包容性的承诺（即“人机协同”）
- en: As is true for other kinds of evaluation of generative AI, evaluating bias demands
    both quantitative and qualitative methods. Statistical analysis can uncover performance
    disparities across groups, and comparative studies can detect biases in outputs.
    Gathering feedback from diverse users aids the understanding of real-world bias
    impacts, while independent audits and research are essential for identifying issues
    that internal evaluations may miss.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成式AI的其他类型评估来说，评估偏见需要定量和定性方法。统计分析可以揭示不同群体之间的性能差异，比较研究可以检测输出中的偏见。收集来自不同用户的反馈有助于理解现实世界中的偏见影响，而独立的审计和研究对于识别内部评估可能遗漏的问题至关重要。
- en: With a better understanding of how we might investigate and evaluate model outcomes
    for societal bias, we can explore technical methods for guiding model outcomes
    toward reliability, equity, and general trustworthiness to curb biased or inequitable
    outcomes during inference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在更好地理解我们如何调查和评估模型的社会偏见结果之后，我们可以探索技术方法来引导模型结果向可靠性、公平性和普遍可信度发展，以减少推理过程中的偏见或不公平结果。
- en: Constrained generation and eliciting trustworthy outcomes
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制生成和诱导可信结果
- en: In practice, it is possible to constrain model generation and guide outcomes
    toward factuality and equitable outcomes. As discussed, guiding models toward
    trustworthy outcomes can be done through continued training and fine-tuning, or
    during inference. For example, methodologies such as **reinforcement learning
    from human feedback** (**RLHF**) and **direct preference optimization** (**DPO**)
    increasingly refine model outputs to align model outcomes with human judgment.
    Additionally, as discussed in [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225),
    various grounding techniques help to ensure that model outputs reflect verified
    data, continuously guiding the model toward responsible and accurate content generation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，可以通过限制模型生成并引导结果向事实性和公平性方向发展。正如所讨论的，通过持续训练和微调，或在进行推理时，可以引导模型向可信赖的结果发展。例如，**从人类反馈中进行强化学习（RLHF**）和**直接偏好优化（DPO**）等方法越来越多地细化模型输出，以使模型结果与人类判断一致。此外，正如在第7章[*](B21773_07.xhtml#_idTextAnchor225)中讨论的，各种定位技术有助于确保模型输出反映经过验证的数据，持续引导模型向负责任和准确的内容生成方向发展。
- en: Constrained generation with fine-tuning
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过微调进行受限生成
- en: Refinement strategies such as RLHF integrate human judgments into the model
    training process, steering the AI toward behavior that aligns with ethical and
    truthful standards. By incorporating human feedback loops, RLHF ensures that the
    AI’s outputs meet technical accuracy and societal norms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在第7章[*](B21773_07.xhtml#_idTextAnchor225)中讨论的，将人类判断整合到模型训练过程中的细化策略，如RLHF，将AI引导至符合伦理和真实标准的行动。通过引入人类反馈循环，RLHF确保AI的输出达到技术准确性和社会规范。
- en: Similarly, DPO refines model outputs based on explicit human preferences, providing
    precise control to ensure outcomes adhere to ethical standards and human values.
    This technique exemplifies the shift toward more ethically aligned content generation
    by directly incorporating human values into the optimization process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，数据保护官（DPO）根据明确的人类偏好来细化模型输出，提供精确的控制以确保结果符合道德标准和人类价值观。这种技术体现了内容生成向更符合伦理标准转变的趋势，因为它直接将人类价值观纳入优化过程。
- en: Constrained generation through prompt engineering
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过提示工程进行受限生成
- en: As we discovered in [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225), we can
    guide model responses by grounding the LLM with factual information. This can
    be achieved directly using the context window or retrieval approach (e.g., Retrieval
    Augmented Generation (RAG)). Just as we can apply these methods to induce factual
    responses, we can apply the same technique to guide the model toward equitable
    and inclusive outcomes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第7章*](B21773_07.xhtml#_idTextAnchor225)中发现的那样，我们可以通过使用事实信息来定位LLM来引导模型响应。这可以通过直接使用上下文窗口或检索方法（例如，检索增强生成（RAG））来实现。正如我们可以应用这些方法来诱导事实性响应一样，我们也可以应用相同的技巧来引导模型向公平和包容的结果发展。
- en: For example, consider an online news outlet looking to use an LLM to review
    article content for grammar and readability. The model does an excellent job of
    reviewing and revising its drafts. However, during peer review, it realizes some
    of the language is culturally insensitive or lacks inclusivity. As discussed,
    qualitative evaluation and human oversight are critical to ensuring that model
    output aligns with human judgment. Notwithstanding, the writing team can guide
    the model toward alignment with company values using a set of general guidelines
    for inclusive and debiased language. For example, it could ground the model with
    excerpts from its internal policy documents or content from its unconscious bias
    training guides.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一家在线新闻机构希望使用大型语言模型（LLM）来审查文章内容的语法和可读性。该模型在审查和修订草稿方面表现出色。然而，在同行评审过程中，它意识到其中一些语言可能存在文化不敏感或缺乏包容性的问题。正如所讨论的，定性评估和人工监督对于确保模型输出与人类判断一致至关重要。尽管如此，写作团队可以使用一套关于包容性和无偏见语言的通用指南来引导模型与公司价值观保持一致。例如，可以通过其内部政策文件的摘录或其无意识偏见培训指南的内容来对模型进行定位。
- en: Employing methodologies such as RLHF and DPO, alongside grounding techniques,
    ensures that LLMs generate content that is not only factual but also ethically
    aligned, demonstrating the potential of generative AI to adhere to high standards
    of truthfulness and inclusivity. Although we cannot underestimate or deemphasize
    the importance of human judgment in shaping model outputs, we can apply practical
    supplemental methods such as grounding to reduce the likelihood of harmful or
    biased model outputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 采用RLHF和DPO等方法论，以及基于事实的技术，确保LLM生成的内容不仅符合事实，而且符合伦理标准，展示了生成式AI遵守高标准的真实性和包容性的潜力。尽管我们不能低估或淡化人类判断在塑造模型输出中的重要性，但我们可以通过应用如基于事实的补充方法等实用方法来降低有害或偏见模型输出的可能性。
- en: In the next section, we’ll explore the risks and ethical dilemmas posed by attempts
    to circumvent the constraints we have just discussed, highlighting the ongoing
    challenge of balancing the rapid adoption of generative LLMs with appropriate
    safeguards against misuse.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨试图规避我们刚才讨论的约束所提出的风险和伦理困境，强调在快速采用生成式LLM的同时，平衡适当的滥用防范措施的持续挑战。
- en: Understanding jailbreaking and harmful behaviors
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解越狱和有害行为
- en: In the context of generative LLMs, the term **jailbreaking** describes techniques
    and strategies that intend to manipulate models to override any ethical safeguards
    or content restrictions, thereby enabling the generation of restricted or harmful
    content. Jailbreaking exploits models through sophisticated adversarial prompting
    that can induce unexpected or harmful responses. For example, an attacker might
    try to instruct an LLM to explain how to generate explicit content or express
    discriminatory views. Understanding this susceptibility is crucial for developers
    and stakeholders to safeguard applied generative AI against misuse and minimize
    potential harm.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式LLM的背景下，**越狱**一词描述了旨在操纵模型以覆盖任何伦理保障或内容限制的技术和策略，从而允许生成受限制或有害的内容。越狱通过复杂的对抗性提示来利用模型，可以诱导出意外或有害的响应。例如，攻击者可能会试图指示LLM解释如何生成显式内容或表达歧视性观点。理解这种易受攻击性对于开发者和利益相关者来说至关重要，以确保应用生成式AI免受滥用并最大限度地减少潜在的危害。
- en: These jailbreaking attacks exploit the fact that LLMs are trained to interpret
    and respond to instructions. Despite sophisticated efforts to defend against misuse,
    attackers can take advantage of the complex and expansive knowledge embedded in
    LLMs to find gaps in their safety precautions. In particular, models that have
    been trained on uncurated datasets are the most susceptible, as the universe of
    possible outputs that the models sample from can include harmful and toxic content.
    Moreover, LLMs are multilingual and can accept various encodings as input. For
    example, an encoding such as **base64**, which can be used to translate plain
    text into binary format, could be applied to obfuscate a harmful instruction.
    In this case, safety filters may perform inconsistently, failing to detect some
    languages or alternative inputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些越狱攻击利用了LLM被训练来解释和响应指令的事实。尽管已经做出了复杂的努力来防御滥用，攻击者仍然可以利用LLM中嵌入的复杂和广泛的知识来发现其安全预防措施中的漏洞。特别是，在未经审查的数据集上训练的模型最易受影响，因为模型从中采样的可能输出范围可能包括有害和有毒内容。此外，LLM是多语言的，可以接受各种编码作为输入。例如，**base64**这样的编码可以将纯文本转换为二进制格式，可以用来混淆有害指令。在这种情况下，安全过滤器可能表现不一致，无法检测某些语言或替代输入。
- en: 'Despite this inherent weakness in LLMs, developers and practitioners can take
    several practical steps to mitigate jailbreaking risks. Remember, these cannot
    be exhaustive as new adversarial techniques are often uncovered:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM存在这种固有的弱点，但开发者和从业者可以采取一些实际步骤来减轻越狱风险。记住，这些不能详尽无遗，因为新的对抗性技术通常会被发现：
- en: '**Preprocessing and safety filtering**: Implement robust content filtering
    to detect and block unsafe semantic patterns across languages and input types.
    For example, a firm might apply machine learning techniques to analyze prompts
    for adversarial patterns and block suspicious inputs before passing them to the
    LLM.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理和安全过滤**：实施强大的内容过滤，以检测和阻止跨语言和输入类型的危险语义模式。例如，一家公司可能会应用机器学习技术来分析提示中的对抗性模式，并在将它们传递给LLM之前阻止可疑的输入。'
- en: '**Postprocessing and output screening**: Apply a specialized classifier or
    other sophisticated technique to screen LLM outputs for inappropriate content
    before returning them.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理和输出筛选**：在返回之前，应用一个专门的分类器或其他复杂技术来筛选LLM输出中的不适当内容。'
- en: '**Safety-focused fine-tuning**: Provide additional safety-focused fine-tuning
    to the LLM to reinforce and expand its safety knowledge. Focus on known jailbreaking
    tactics.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以安全为重点的微调**：为LLM提供额外的以安全为重点的微调，以加强和扩展其安全知识。重点关注已知的越狱策略。'
- en: '**Monitoring and iterating**: Actively monitor for jailbreaking or policy violation
    attempts in production, analyze them to identify gaps, and continually update
    defense measures to stay ahead of creative attackers.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和迭代**：在生产环境中积极监控越狱或政策违规尝试，分析它们以识别差距，并不断更新防御措施以保持领先于有创造性的攻击者。'
- en: While eliminating all possible jailbreaking attempts is infeasible, a multi-layered
    defense and operational best practices can significantly mitigate the risk.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然消除所有可能的越狱尝试是不切实际的，但多层防御和操作最佳实践可以显著降低风险。
- en: In the next section, we will apply a real-time defense mechanism for jailbreaking,
    all while reducing the likelihood of biased and harmful output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将应用实时越狱防御机制，同时降低产生偏见和有害输出的可能性。
- en: 'Practice project: Minimizing harmful behaviors with filtering'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践项目：使用过滤来最小化有害行为
- en: For this project, we will use response filtering to try to minimize misuse and
    curb unwanted LLM output. Again, we’ll consider our hypothetical business, StyleSprint.
    After successfully using an LLM to generate product descriptions and fine-tuning
    it to answer FAQs, StyleSprint now wants to attempt to use a general-purpose LLM
    (without fine-tuning) to refine its website search. However, giving its customers
    direct access to the LLM poses the risk of misuse. Bad actors may attempt to use
    the LLM search to produce harmful content with the intention of harming StyleSprint’s
    reputation. To prevent this behavior, we can revisit our RAG implementation from
    [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225), applying a filter that evaluates
    whether queries deviate from the appropriate use.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将使用响应过滤来尝试最小化误用并遏制不想要的LLM输出。再次强调，我们将考虑我们的假设业务，StyleSprint。在成功使用LLM生成产品描述并微调它以回答常见问题之后，StyleSprint现在想要尝试使用一个通用的LLM（不进行微调）来优化其网站搜索。然而，直接让客户访问LLM存在误用的风险。恶意行为者可能会尝试使用LLM搜索来产生有害内容，意图损害StyleSprint的声誉。为了防止这种行为，我们可以回顾我们在[*第7章*](B21773_07.xhtml#_idTextAnchor225)中提到的RAG实现，应用一个过滤器来评估查询是否偏离了适当的使用。
- en: 'Reusing our previous implementation from the last chapter (found in the GitHub
    repository: [https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)),
    which applied RAG to answer specific product-related questions, we can evaluate
    how the model would respond to questions outside the desired scope. Recall that
    RAG is simply a vector search engine combined with an LLM to produce coherent
    and more precise responses, contextualized by a specific data source. We will
    directly reuse that implementation and the same product data for simplicity, but
    this time, we’ll input a completely unrelated query instead of asking about products:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重新使用上一章中（可在GitHub仓库[https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)找到）的先前实现，该实现将RAG应用于回答特定产品相关的问题，我们可以评估模型对超出预期范围的问题的响应。回想一下，RAG只是一个向量搜索引擎与LLM结合，以产生由特定数据源上下文化的连贯且更精确的响应。我们将直接重用该实现和相同的产品数据以简化流程，但这次，我们将输入一个完全不相关的查询而不是询问产品：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we can see, the model did not attempt to constrain its answer to the contents
    of the search index. It returned an answer based on its vast training. This is
    precisely the behavior we want to avoid. Imagine that a bad actor induced the
    model to produce explicit content or some other unwanted output. Moreover, consider
    a sophisticated attacker that could induce the model to leak training data or
    expose sensitive information accidentally memorized during training procedures
    (Carlini et al., 2018; Hu et al., 2022). In either case, StyleSprint could face
    material risk and exposure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，该模型没有尝试将其答案限制在搜索索引的内容之内。它基于其庞大的训练数据返回了一个答案。这正是我们想要避免的行为。想象一下，一个恶意行为者诱导模型生成显式内容或某些其他不希望产生的输出。此外，考虑一个复杂的攻击者，可能诱导模型泄露训练数据或在训练过程中意外记住的敏感信息（Carlini等，2018；胡等，2022）。在任何情况下，StyleSprint都可能面临重大的风险和暴露。
- en: 'To prevent this, we can leverage a filter to constrain the output to provide
    answers relevant to a given question explicitly. The implementation is already
    built into the LlamaIndex RAG interface. It is a feature they call Structured
    Answer Filtering:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况，我们可以利用过滤器来限制输出，明确提供与给定问题相关的答案。该实现已经内置到LlamaIndex RAG接口中。他们称之为结构化答案过滤：
- en: With structured_answer_filtering set to True, our refine module is able to filter
    out any input nodes that are not relevant to the question being asked. This is
    particularly useful for RAG-based Q&A systems that involve retrieving chunks of
    text from external vector store for a given user query. (LlamaIndex)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当structured_answer_filtering设置为True时，我们的refine模块能够过滤掉任何与所提问题不相关的输入节点。这对于涉及从外部向量存储检索给定用户查询的文本块的基础RAG问答系统特别有用。（LlamaIndex）
- en: In short, this functionality gives us fine-grained control to restrict the context
    we provide to the LLM for synthesis, ensuring that only the most relevant results
    are included. Filtering out irrelevant content before synthesizing responses ensures
    that only information related to the user’s question is used. This approach helps
    avoid answers that are off-topic or outside the intended subject matter. We can
    quickly reimplement our RAG approach, applying minor changes that enable the feature.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这项功能让我们能够精细控制提供给LLM用于综合的上下文，确保只包含最相关的结果。在综合响应之前过滤掉不相关的内容，可以确保只使用与用户问题相关的信息。这种方法有助于避免离题或超出预期主题范围的答案。我们可以快速重新实现我们的RAG方法，通过进行一些小的改动来实现这一功能。
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This functionality is most reliable when using an LLM that can support function
    calling.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用能够支持函数调用的LLM时，这项功能最为可靠。
- en: Let’s see how this functionality can be implemented.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个功能是如何实现的。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Using this approach, the model returns a response to the standard question
    but no response to the irrelevant question. In fact, we can take this further
    and compound this filtering with additional instructions in the prompt template.
    For example, if we revise `response_synthesizer`, we can promote a stricter response
    from the LLM:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，模型对标准问题返回了响应，但对不相关的问题没有返回响应。实际上，我们可以进一步将这种过滤与提示模板中的额外指令相结合。例如，如果我们修订`response_synthesizer`，我们可以从LLM促进更严格的响应：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This time, the model responded explicitly, `I cannot answer`. Using a prompt
    template, StyleSprint could return a message it deems appropriate in response
    to inputs unrelated to the search index and, as a side effect, ignore queries
    that do not adhere to its policies. Although not entirely a perfect solution,
    combining RAG with more strict answer filtering can help deter or defend against
    harmful instructions or adversarial prompting. Additionally, as explored in [*Chapter
    7*](B21773_07.xhtml#_idTextAnchor225), we can apply RAG-specific evaluation techniques
    such as RAGAS to measure factual consistency and answer relevancy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，模型明确地响应了，“我无法回答”。使用提示模板，StyleSprint可以返回它认为适当的消息，以响应与搜索索引无关的输入，并且作为副作用，忽略不符合其政策的查询。虽然这并不是一个完美的解决方案，但将RAG与更严格的答案过滤相结合可以帮助阻止或防御有害指令或对抗性提示。此外，如第7章[*](B21773_07.xhtml#_idTextAnchor225)所探讨的，我们可以应用RAG特定的评估技术，如RAGAS，以衡量事实一致性和答案的相关性。
- en: Summary
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this section, we recognized the increasing prominence of generative AI and
    explored the ethical considerations that should steer its progress. We outlined
    key concepts such as transparency, fairness, accountability, respect for privacy,
    informed consent, security, and inclusivity, which are essential to the responsible
    development and use of these technologies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们认识到生成式人工智能日益突出，并探讨了应引导其进步的伦理考量。我们概述了关键概念，如透明度、公平性、问责制、尊重隐私、知情同意、安全和包容性，这些对于这些技术的负责任开发和利用至关重要。
- en: We reviewed strategies to attempt to counter these biases, including human-aligned
    training techniques and practical application-level measures against susceptibilities
    such as jailbreaking. In sum, we explored a multidimensional and human-centered
    approach to generative AI adoption.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了尝试对抗这些偏差的策略，包括与人类对齐的训练技术和针对如越狱等易受攻击性的实际应用级措施。总之，我们探索了生成式人工智能采用的多元化和以人为本的方法。
- en: Having completed our foundational exploration of generative AI, we can now reflect
    on our journey. We began by laying the groundwork, examining foundational generative
    architectures such as generative adversarial networks (GANs), diffusion models,
    and transformers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成我们对生成式人工智能的基础探索之后，我们现在可以反思我们的旅程。我们开始时是奠定基础，检查基础生成架构，例如生成对抗网络（GANs）、扩散模型和转换器。
- en: '*Chapters 2* and *3* guided us through the evolution of language models, with
    a particular focus on autoregressive transformers. We explored how these models
    have significantly advanced the capabilities of generative AI, pushing the boundaries
    of machine understanding and the generation of human-like language.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*第2章*和*第3章*引导我们了解语言模型的演变，特别关注自回归转换器。我们探讨了这些模型如何显著提升生成式人工智能的能力，推动机器理解和生成类似人类语言边界的扩展。'
- en: '[*Chapter 4*](B21773_04.xhtml#_idTextAnchor123) provided us with practical
    experience in production-ready environments. In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180),
    we explored the fine-tuning of LLMs for specific tasks, a technique that enhances
    their performance and adaptability to specific applications. [*Chapter 6*](B21773_06.xhtml#_idTextAnchor211)
    focused on the concept of domain adaptation, demonstrating how tailoring AI models
    to understand domain-specific nuances can greatly improve their utility in specialized
    fields such as finance, law, and healthcare.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B21773_04.xhtml#_idTextAnchor123)为我们提供了在生产就绪环境中的实践经验。在[*第5章*](B21773_05.xhtml#_idTextAnchor180)中，我们探讨了针对特定任务的LLM微调，这是一种增强其性能和适应特定应用的技术。[*第6章*](B21773_06.xhtml#_idTextAnchor211)专注于领域自适应的概念，展示了如何定制AI模型以理解特定领域的细微差别，可以极大地提高其在金融、法律和医疗保健等专门领域的实用性。'
- en: '*Chapters 7* and *8* centered on prompt engineering and constrained generation,
    addressing techniques to ensure that AI-generated content remains trustworthy
    and aligned with ethical guidelines.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*第7章*和*第8章*集中在提示工程和约束生成上，讨论了确保AI生成内容保持可信并与伦理指南保持一致的技术。'
- en: This book has aimed to provide a solid foundation in generative AI, preparing
    professionals across disciplines and sectors with the necessary theoretical knowledge
    and practical skills to effectively engage with this transformative technology.
    The potential of generative AI is significant, and with our deeper understanding
    of its technologies, coupled with a thoughtful approach to ethical and societal
    considerations, we are ready to responsibly leverage its advantages.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在为生成式人工智能提供一个坚实的基础，为来自各个学科和行业的专业人士提供必要的理论知识与实践技能，以便有效地参与这一变革性技术。生成式人工智能的潜力是巨大的，随着我们对其技术的更深入理解，以及我们对伦理和社会考量的深思熟虑的方法，我们准备好负责任地利用其优势。
- en: References
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本参考部分作为本书中引用的来源库；您可以探索这些资源，以进一步加深对主题内容的理解和知识：
- en: 'Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W.,
    Zhang, Y., Li, X., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong,
    C., Xiao, C., Li, C., Xing, E., . . . Zhao, Y. (2024). *TrustLLM: Trustworthiness
    in Large Language Models*. *ArXiv*. /abs/2401.05561'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W.,
    Zhang, Y., Li, X., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong,
    C., Xiao, C., Li, C., Xing, E., . . . Zhao, Y. (2024). *TrustLLM: 大型语言模型的可信度*.
    *ArXiv*. /abs/2401.05561'
- en: 'Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., & Song, D. (2018). *The secret
    sharer: Evaluating and testing unintended memorization in neural networks*. In
    arXiv [cs.LG]. [http://arxiv.org/abs/1802.08232](http://arxiv.org/abs/1802.08232)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., & Song, D. (2018). *秘密分享者：评估和测试神经网络中的无意记忆*.
    在 arXiv [cs.LG]. [http://arxiv.org/abs/1802.08232](http://arxiv.org/abs/1802.08232)
- en: 'Hu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., & Zhang, X. (2022). *Membership
    inference attacks on machine learning: A survey. ACM Computing Surveys*, 54(11s),
    1–37\. [https://doi.org/10.1145/3523273](https://doi.org/10.1145/3523273)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., & Zhang, X. (2022). *机器学习中的成员推理攻击：综述.
    ACM 计算评论*, 54(11s), 1–37\. [https://doi.org/10.1145/3523273](https://doi.org/10.1145/3523273)
- en: LlamaIndex. (n.d.). *Response synthesizers. In LlamaIndex Documentation (stable
    version)*. Retrieved March 12, 2024\. [https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LlamaIndex. (n.d.). *响应合成器. 在 LlamaIndex 文档（稳定版本）中*. 2024年3月12日检索。 [https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html)
