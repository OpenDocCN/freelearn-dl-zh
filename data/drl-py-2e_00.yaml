- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: With significant enhancement in the quality and quantity of algorithms in recent
    years, this second edition of *Hands-On Reinforcement Learning with Python* has
    been revamped into an example-rich guide to learning state-of-the-art **reinforcement
    learning** (**RL**) and deep RL algorithms with TensorFlow 2 and the OpenAI Gym
    toolkit.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，强化学习算法的质量和数量显著提升，本书第二版《Python强化学习实战》已经被重塑为一本充满实例的指南，帮助读者学习最先进的**强化学习**（**RL**）和深度强化学习算法，使用TensorFlow
    2和OpenAI Gym工具包。
- en: In addition to exploring RL basics and foundational concepts such as the Bellman
    equation, Markov decision processes, and dynamic programming, this second edition
    dives deep into the full spectrum of value-based, policy-based, and actor-critic
    RL methods. It explores state-of-the-art algorithms such as DQN, TRPO, PPO and
    ACKTR, DDPG, TD3, and SAC in depth, demystifying the underlying math and demonstrating
    implementations through simple code examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 除了探讨强化学习（RL）的基础知识和基础概念，如贝尔曼方程、马尔可夫决策过程和动态规划外，本第二版深入探讨了基于价值、基于策略和演员-评论员（actor-critic）强化学习方法的全貌。书中详细介绍了如DQN、TRPO、PPO、ACKTR、DDPG、TD3和SAC等最先进的算法，揭示了背后的数学原理，并通过简单的代码示例演示了实现过程。
- en: The book has several new chapters dedicated to new RL techniques including distributional
    RL, imitation learning, inverse RL, and meta RL. You will learn to leverage Stable
    Baselines, an improvement of OpenAI's baseline library, to implement popular RL
    algorithms effortlessly. The book concludes with an overview of promising approaches
    such as meta-learning and imagination augmented agents in research.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书新增了几章，专门介绍了新的强化学习技术，包括分布式强化学习、模仿学习、逆向强化学习和元强化学习。你将学习如何利用Stable Baselines（一种OpenAI基准库的改进版）来轻松实现流行的强化学习算法。本书最后将概述一些有前景的研究方法，如元学习和想象增强代理（imagination
    augmented agents）。
- en: Who this book is for
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合谁阅读
- en: If you're a machine learning developer with little or no experience with neural
    networks interested in artificial intelligence and want to learn about reinforcement
    learning from scratch, this book is for you. Basic familiarity with linear algebra,
    calculus, and Python is required. Some experience with TensorFlow would be a plus.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个对神经网络经验较少或没有经验的机器学习开发者，并且有兴趣了解人工智能、从零开始学习强化学习，那么本书适合你。需要具备一些线性代数、微积分和Python的基本知识，若有TensorFlow的经验会更有帮助。
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容
- en: '*Chapter 1*, *Fundamentals of Reinforcement Learning*, helps you build a strong
    foundation on RL concepts. We will learn about the key elements of RL, the Markov
    decision process, and several important fundamental concepts such as action spaces,
    policies, episodes, the value function, and the Q function. At the end of the
    chapter, we will learn about some of the interesting applications of RL and we
    will also look into the key terms and terminologies frequently used in RL.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一章*，*强化学习基础*，帮助你建立强化学习概念的坚实基础。我们将学习强化学习的关键要素、马尔可夫决策过程，以及几个重要的基础概念，如动作空间、策略、回合、价值函数和Q函数。章节末我们还将了解一些强化学习的有趣应用，并探讨在强化学习中常用的关键术语和术语。'
- en: '*Chapter 2*, *A Guide to the Gym Toolkit*, provides a complete guide to OpenAI''s
    Gym toolkit. We will understand several interesting environments provided by Gym
    in detail by implementing them. We will begin our hands-on RL journey from this
    chapter by implementing several fundamental RL concepts using Gym.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二章*，*Gym工具包指南*，提供了OpenAI Gym工具包的完整指南。通过实际实现，我们将详细了解Gym提供的几种有趣的环境。我们将从这一章开始动手实践强化学习，通过Gym实现一些基本的强化学习概念。'
- en: '*Chapter 3*, *The Bellman Equation and Dynamic Programming*, will help us understand
    the Bellman equation in detail with extensive math. Next, we will learn two interesting
    classic RL algorithms called the value and policy iteration methods, which we
    can use to find the optimal policy. We will also see how to implement value and
    policy iteration methods for solving the Frozen Lake problem.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*第三章*，*贝尔曼方程与动态规划*，将帮助我们通过大量数学推导详细理解贝尔曼方程。接下来，我们将学习两种有趣的经典强化学习算法——价值迭代和策略迭代方法，利用这些方法可以找到最优策略。我们还将看到如何通过实现价值迭代和策略迭代方法来解决Frozen
    Lake问题。'
- en: '*Chapter 4*, *Monte Carlo Methods*, explains the model-free method, Monte Carlo.
    We will learn what prediction and control tasks are, and then we will look into
    Monte Carlo prediction and Monte Carlo control methods in detail. Next, we will
    implement the Monte Carlo method to solve the blackjack game using the Gym toolkit.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*第4章*，*蒙特卡罗方法*，解释了无模型方法——蒙特卡罗方法。我们将了解什么是预测和控制任务，然后详细研究蒙特卡罗预测方法和蒙特卡罗控制方法。接下来，我们将使用
    Gym 工具包实现蒙特卡罗方法来解决二十一点游戏。'
- en: '*Chapter 5*, *Understanding Temporal Difference Learning*, deals with one of
    the most popular and widely used model-free methods called **Temporal Difference**
    (**TD**) learning. First, we will learn how the TD prediction method works in
    detail, and then we will explore the on-policy TD control method called SARSA
    and the off-policy TD control method called Q learning in detail. We will also
    implement TD control methods to solve the Frozen Lake problem using Gym.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*第5章*，*理解时间差分学习*，讲解了一种非常流行且广泛使用的无模型方法——**时间差分**（**TD**）学习。首先，我们将详细了解 TD 预测方法如何工作，然后我们将深入研究基于策略的
    TD 控制方法——SARSA，以及基于行为的 TD 控制方法——Q 学习。我们还将实现 TD 控制方法，使用 Gym 来解决冰湖问题。'
- en: '*Chapter 6*, *Case Study – The MAB Problem*, explains one of the classic problems
    in RL called the **multi-armed bandit** (**MAB**) problem. We will start the chapter
    by understanding what the MAB problem is and then we will learn about several
    exploration strategies such as epsilon-greedy, softmax exploration, upper confidence
    bound, and Thompson sampling methods for solving the MAB problem in detail.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6章*，*案例研究——多臂老虎机问题*，讲解了强化学习中的经典问题之一——**多臂老虎机**（**MAB**）问题。我们将从理解 MAB 问题开始，然后学习几种探索策略，如
    ε-贪婪、软最大探索、上置信界和汤普森采样方法，以详细解决 MAB 问题。'
- en: '*Chapter 7*, *Deep Learning Foundations*, helps us to build a strong foundation
    on deep learning. We will start the chapter by understanding how artificial neural
    networks work. Then we will learn several interesting deep learning algorithms,
    such as recurrent neural networks, LSTM networks, convolutional neural networks,
    and generative adversarial networks.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*第7章*，*深度学习基础*，帮助我们建立深度学习的坚实基础。我们将从理解人工神经网络的工作原理开始。接着，我们将学习几种有趣的深度学习算法，如递归神经网络、LSTM
    网络、卷积神经网络和生成对抗网络。'
- en: '*Chapter 8*, *A Primer on TensorFlow*, deals with one of the most popular deep
    learning libraries called TensorFlow. We will understand how to use TensorFlow
    by implementing a neural network to recognize handwritten digits. Next, we will
    learn to perform several math operations using TensorFlow. Later, we will learn
    about TensorFlow 2.0 and see how it differs from the previous TensorFlow versions.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*第8章*，*TensorFlow 入门*，讲解了一个非常流行的深度学习库——TensorFlow。我们将通过实现一个神经网络来识别手写数字，了解如何使用
    TensorFlow。接下来，我们将学习如何使用 TensorFlow 执行几种数学操作。之后，我们将学习 TensorFlow 2.0，并了解它与以前版本的区别。'
- en: '*Chapter 9*, *Deep Q Network and Its Variants*, enables us to kick-start our
    deep RL journey. We will learn about one of the most popular deep RL algorithms
    called the **Deep Q Network** (**DQN**). We will understand how DQN works step
    by step along with the extensive math. We will also implement a DQN to play Atari
    games. Next, we will explore several interesting variants of DQN, called Double
    DQN, Dueling DQN, DQN with prioritized experience replay, and DRQN.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*第9章*，*深度 Q 网络及其变种*，帮助我们启动深度强化学习之旅。我们将了解一种非常流行的深度强化学习算法——**深度 Q 网络**（**DQN**）。我们将逐步理解
    DQN 的工作原理，并进行详细的数学推导。我们还将实现一个 DQN 来玩 Atari 游戏。接下来，我们将探讨 DQN 的几个有趣变种，包括双重 DQN、对抗
    DQN、带优先经验回放的 DQN 和 DRQN。'
- en: '*Chapter 10*, *Policy Gradient Method*, covers policy gradient methods. We
    will understand how the policy gradient method works along with the detailed derivation.
    Next, we will learn several variance reduction methods such as policy gradient
    with reward-to-go and policy gradient with baseline. We will also understand how
    to train an agent for the Cart Pole balancing task using policy gradient.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*第10章*，*策略梯度方法*，介绍了策略梯度方法。我们将理解策略梯度方法如何工作，并进行详细推导。接下来，我们将学习几种方差减少方法，如带有回报目标的策略梯度和带有基准的策略梯度。我们还将理解如何使用策略梯度训练代理来完成平衡杆任务。'
- en: '*Chapter 11*, *Actor-Critic Methods – A2C and A3C*, deals with several interesting
    actor-critic methods such as advantage actor-critic and asynchronous advantage
    actor-critic. We will learn how these actor-critic methods work in detail, and
    then we will implement them for a mountain car climbing task using OpenAI Gym.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*第11章*，*演员-评论家方法——A2C和A3C*，介绍了几种有趣的演员-评论家方法，如优势演员-评论家和异步优势演员-评论家。我们将详细学习这些演员-评论家方法的工作原理，然后我们将实现它们来进行山地车爬坡任务，使用OpenAI
    Gym。'
- en: '*Chapter 12*, *Learning DDPG, TD3, and SAC*, covers state-of-the-art deep RL
    algorithms such as deep deterministic policy gradient, twin delayed DDPG, and
    soft actor, along with step by step derivation. We will also learn how to implement
    the DDPG algorithm for performing the inverted pendulum swing-up task using Gym.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*第12章*，*学习DDPG、TD3和SAC*，涵盖了最先进的深度强化学习算法，如深度确定性策略梯度、双延迟DDPG和软演员方法，并进行了逐步推导。我们还将学习如何实现DDPG算法来执行倒立摆摆动任务，使用Gym。'
- en: '*Chapter 13*, *TRPO, PPO, and ACKTR Methods*, deals with several popular policy
    gradient methods such as TRPO and PPO. We will dive into the math behind TRPO
    and PPO step by step and understand how TRPO and PPO helps an agent find the optimal
    policy. Next, we will learn to implement PPO for performing the inverted pendulum
    swing-up task. At the end, we will learn about the actor-critic method called
    actor-critic using Kronecker-Factored trust region in detail.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*第13章*，*TRPO、PPO和ACKTR方法*，介绍了几种流行的策略梯度方法，如TRPO和PPO。我们将一步步深入探讨TRPO和PPO背后的数学原理，并理解TRPO和PPO如何帮助代理找到最优策略。接下来，我们将学习如何实现PPO来执行倒立摆摆动任务。最后，我们将详细了解一种名为使用Kronecker分解信任区域的演员-评论家方法。'
- en: '*Chapter 14*, *Distributional Reinforcement Learning*, covers distributional
    RL algorithms. We will begin the chapter by understanding what distributional
    RL is. Then we will explore several interesting distributional RL algorithms such
    as categorical DQN, quantile regression DQN, and distributed distributional DDPG.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*第14章*，*分布式强化学习*，介绍了分布式强化学习算法。我们将从理解什么是分布式强化学习开始。然后，我们将探索一些有趣的分布式强化学习算法，如分类DQN、分位回归DQN和分布式分布式DDPG。'
- en: '*Chapter 15*, *Imitation Learning and Inverse RL*, explains imitation and inverse
    RL algorithms. First, we will understand how supervised imitation learning, DAgger,
    and deep Q learning from demonstrations work in detail. Next, we will learn about
    maximum entropy inverse RL. At the end of the chapter, we will learn about generative
    adversarial imitation learning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*第15章*，*模仿学习与反向强化学习*，解释了模仿学习和反向强化学习算法。首先，我们将详细了解如何进行监督模仿学习、DAgger和从示范中学习深度Q学习。接下来，我们将学习最大熵反向强化学习。最后，在本章的结尾，我们将学习生成对抗模仿学习。'
- en: '*Chapter 16*, *Deep Reinforcement Learning with Stable Baselines*, helps us
    to understand how to implement deep RL algorithms using a library called Stable
    Baselines. We will learn what Stable Baselines is and how to use it in detail
    by implementing several interesting Deep RL algorithms such as DQN, A2C, DDPG
    TRPO, and PPO.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*第16章*，*使用Stable Baselines的深度强化学习*，帮助我们了解如何使用一个名为Stable Baselines的库来实现深度强化学习算法。我们将通过实现几种有趣的深度强化学习算法，如DQN、A2C、DDPG、TRPO和PPO，详细学习什么是Stable
    Baselines以及如何使用它。'
- en: '*Chapter 17*, *Reinforcement Learning Frontiers*, covers several interesting
    avenues in RL, such as meta RL, hierarchical RL, and imagination augmented agents
    in detail.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*第17章*，*强化学习前沿*，详细介绍了强化学习中的几个有趣领域，如元强化学习、层次强化学习和想象增强代理。'
- en: To get the most out of this book
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要充分利用本书
- en: 'You need the following software for this book:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要以下软件来学习本书：
- en: Anaconda
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Python
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: Any web browser
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何网络浏览器
- en: Download the example code files
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from your account at [http://www.packtpub.com](http://www.packtpub.com).
    If you purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您的账户下载本书的示例代码文件，访问[http://www.packtpub.com](http://www.packtpub.com)。如果您是在其他地方购买本书，可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)，注册后将文件直接通过电子邮件发送给您。
- en: 'You can download the code files by following these steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下步骤下载代码文件：
- en: Log in or register at [http://www.packtpub.com](http://www.packtpub.com).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录或注册[http://www.packtpub.com](http://www.packtpub.com)。
- en: Select the **SUPPORT** tab.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**支持**标签。
- en: Click on **Code Downloads & Errata**.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**代码下载与勘误**。
- en: Enter the name of the book in the **Search** box and follow the on-screen instructions.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**搜索**框中输入书名并按照屏幕上的指示操作。
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文件下载完成后，请确保使用以下最新版本解压或提取文件夹：
- en: WinRAR / 7-Zip for Windows
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WinRAR / 7-Zip for Windows
- en: Zipeg / iZip / UnRarX for Mac
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipeg / iZip / UnRarX for Mac
- en: 7-Zip / PeaZip for Linux
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7-Zip / PeaZip for Linux
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包也托管在GitHub上，地址为：[https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python)。我们还提供了来自我们丰富书籍和视频目录中的其他代码包，地址为：[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)。快去看看吧！
- en: Download the color images
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一份包含本书中使用的截图/图表的彩色图像的PDF文件。你可以在此下载：[https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf)。
- en: Conventions used
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了多种文本约定。
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    For example: "`epsilon_greedy` computes the optimal policy."'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`CodeInText`：表示文本中的代码单词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟网址、用户输入和Twitter用户名。例如：“`epsilon_greedy`计算最佳策略。”'
- en: 'A block of code is set as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一段代码如下设置：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are highlighted:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起你对代码块中特定部分的注意时，相关的行或项目会被高亮显示：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出如下所示：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: "The **Markov Reward Process** (**MRP**) is an extension of
    the Markov chain with the reward function."'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要单词，或者你在屏幕上看到的词汇，例如在菜单或对话框中，也会像这样出现在文本中。例如：“**马尔可夫奖励过程**（**MRP**）是带有奖励函数的马尔可夫链的扩展。”'
- en: Warnings or important notes appear like this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要提示如下所示。
- en: Tips and tricks appear like this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧如下所示。
- en: Get in touch
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '**General feedback**: Email `feedback@packtpub.com`, and mention the book''s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：请发送电子邮件至`feedback@packtpub.com`，并在邮件主题中注明书名。如果你对本书的任何方面有疑问，请通过`questions@packtpub.com`与我们联系。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book we would
    be grateful if you would report this to us. Please visit, [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：尽管我们已尽力确保内容的准确性，但错误仍然可能发生。如果你在本书中发现错误，我们将非常感激你能报告给我们。请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择你的书籍，点击勘误提交表单链接，并输入相关信息。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果你在互联网上遇到我们作品的任何非法复制形式，我们将非常感激你能提供位置地址或网站名称。请通过`copyright@packtpub.com`与我们联系，并附上材料链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有兴趣成为作者**：如果你在某个领域有专长并且有意撰写或贡献书籍内容，请访问[http://authors.packtpub.com](http://authors.packtpub.com)。'
- en: Reviews
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评论
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请留下评论。当您阅读并使用本书后，为什么不在您购买书籍的网站上留下评论呢？潜在的读者可以查看并参考您的公正意见来做出购买决定，我们 Packt 也可以了解您对我们产品的看法，而我们的作者可以看到您对他们书籍的反馈。谢谢！
- en: For more information about Packt, please visit [packtpub.com](http://packtpub.com).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于 Packt 的信息，请访问 [packtpub.com](http://packtpub.com)。
