- en: Overview of Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络概述
- en: 'Greetings to you, fellow sentient being; welcome to our exciting journey. The
    journey itself is to understand the concepts and inner workings behind an elusively
    powerful computing paradigm: the **artificial neural network** (**ANN**). While
    this notion has been around for almost half a century, the ideas accredited to
    its birth (such as *what an agent is*, or *how an agent may learn from its surroundings*),
    date back to Aristotelian times, and perhaps even to the dawn of civilization
    itself. Unfortunately, people in the time of Aristotle were not blessed with the
    ubiquity of big data, or the speeds of **Graphical Processing Unit** (**GPU**)-accelerated
    and massively parallelized computing, which today open up some very promising
    avenues for us. We now live in an era where the majority of our species has access
    to the building blocks and tools required to assemble artificially-intelligent
    systems. While covering the entire developmental timeline that brings us here
    today is slightly beyond the scope of this book, we will attempt to briefly summarize
    some pivotal concepts and ideas that will help us think intuitively about our
    problem here.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 向你问好，同胞；欢迎加入我们这段激动人心的旅程。这段旅程的核心是理解一个极具潜力的计算范式背后的概念和内在运作：**人工神经网络**（**ANN**）。虽然这个概念已经存在近半个世纪，但其诞生时的思想（例如*什么是智能体*，或*智能体如何从环境中学习*）可以追溯到亚里士多德时代，甚至可能追溯到文明的黎明。遗憾的是，亚里士多德时代的人们并未拥有今天我们所拥有的大数据普及，或者**图形处理单元**（**GPU**）加速和大规模并行计算的速度，这为我们打开了非常有前景的道路。如今我们生活在一个时代，在这个时代，大多数人类种群都能接触到构建人工智能系统所需的基础工具和资源。虽然涵盖从过去到今天的整个发展历程稍微超出了本书的范围，但我们将尝试简要总结一些关键概念和思想，帮助我们直观地思考我们在此面临的问题。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Defining our goal
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义我们的目标
- en: Knowing our tools
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解我们的工具
- en: Understanding neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经网络
- en: Observing the brain
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察大脑
- en: Information modeling and functional representations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息建模与功能表示
- en: Some fundamental refreshers in data science
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学中的一些基础复习
- en: Defining our goal
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义我们的目标
- en: Essentially, our task here is to conceive a mechanism that is capable of dealing
    with any data that it is introduced to. In doing so, we want this mechanism to
    detect any underlying patterns present in our data, in order to leverage it for
    our own benefit. Succeeding at this task means that we will be able to translate
    any form of raw data into knowledge, in the form of actionable business insights,
    burden-alleviating services, or life-saving medicines. Hence, what we actually
    want is to construct a mechanism that is capable of universally approximating
    any possible function that could represent our data; the elixir of knowledge,
    if you will. Do step back and imagine such a world for a moment; a world where
    the deadliest diseases may be cured in minutes. A world where all are fed, and
    all may choose to pursue the pinnacle of human achievement in any discipline without
    fear of persecution, harassment, or poverty. Too much of a promise? Perhaps. Achieving
    this utopia will take a bit more than designing efficient computer systems. It
    will require us to evolve our moral perspective in parallel, reconsider our place
    on this planet as individuals, as a species, and as a whole. But you will be surprised
    by how much computers can help us get there.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们的任务是构思一个能够处理任何输入数据的机制。在这个过程中，我们希望这个机制能够检测数据中潜在的模式，并将其利用为我们带来利益。成功完成这个任务意味着我们将能够将任何形式的原始数据转化为知识，进而形成可操作的商业洞察、减轻负担的服务，或是救命的药物。因此，我们真正想要的是构建一个能够普遍近似任何可能代表我们数据的函数的机制；如果你愿意，可以称之为知识的灵丹妙药。请暂时退后一步，想象一下这样的世界；一个能够在几分钟内治愈最致命疾病的世界。一个所有人都能获得食物、每个人都可以选择无惧迫害、骚扰或贫困地追求任何学科人类成就巅峰的世界。这个承诺是否过于遥不可及？或许吧。实现这个理想社会不仅仅需要设计高效的计算机系统。它还需要我们在道德观念上同步进化，重新思考作为个体、物种和整体我们在这个星球上的位置。但你会惊讶于计算机能在多大程度上帮助我们实现这一目标。
- en: It's important here to understand that it is not just any kind of computer system
    that we are talking about. This is something very different from what our computing
    forefathers, such as Babbage and Turing, dealt with. This is not a simple Turing
    machine or difference engine (although many, if not all, of the concepts we will
    review in our journey relate directly back to those enlightened minds and their
    inventions). Hence, our goal will be to cover the pivotal academic contributions,
    practical experimentation, and implementation insights that followed from centuries,
    if not decades, of scientific research behind the fundamental concept of generating
    intelligence; a concept that is arguably most innate to us humans, yet so scarcely
    understood.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要理解的是，我们谈论的并不仅仅是任何一种计算机系统。这与我们的计算先驱们（如巴贝奇和图灵）所处理的内容截然不同。这并不是一个简单的图灵机或差分机（尽管我们将要回顾的许多概念直接与这些伟大的思想家和他们的发明相关）。因此，我们的目标是涵盖从几百年、如果不是几十年，科学研究中关于生成智能这一基本概念的学术贡献、实际实验和实现见解；这个概念可以说是最本能地属于我们人类，但却少有人真正理解。
- en: Knowing our tools
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解我们的工具
- en: 'We will mainly be working with the two most popular deep learning frameworks
    that exist, and are freely available to the public at large. This does not mean
    that we will completely limit our implementations and exercises to these two platforms.
    It may well occur that we experiment with other prominent deep learning frameworks
    and backends. We will, however, try to use either TensorFlow or Keras, due to
    their widespread popularity, large support community, and flexibility in interfacing
    with other prominent backend and frontend frameworks (such as Theano, Caffe, or
    Node.js, respectively). We will now provide a little background information on
    Keras and TensorFlow:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要使用目前最受欢迎的两个深度学习框架，这些框架对公众免费开放。这并不意味着我们将完全局限于这两个平台进行实现和练习。也可能会遇到我们尝试其他著名的深度学习框架和后端。不过，由于
    TensorFlow 和 Keras 的广泛流行、大量的支持社区以及它们在与其他著名后端和前端框架（如 Theano、Caffe 或 Node.js）接口的灵活性，我们将尽量使用它们。接下来我们将提供一些关于
    Keras 和 TensorFlow 的背景信息：
- en: '![](img/815fa0e1-067d-4b25-a6c6-e698f9b3f879.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/815fa0e1-067d-4b25-a6c6-e698f9b3f879.png)'
- en: Keras
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: Many have named Keras the *lingua franca* of deep learning, due to its user
    friendliness, modularity, and extendibility. Keras is a high-level application
    programming interface for neural networks, and focuses on enabling fast experimentation.
    It is written in Python and is capable of running on top of backends such as TensorFlow
    or Keras. Keras was initially developed as part of the research effort of the
    ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System) project.
    Its name is a reference to the Greek word, ![](img/5cd7fa77-066f-42ed-aa01-4a3a6e07fdd5.png),
    which literally translates to *horn*. The word eludes to a play on words dating
    back to ancient Greek literature, referring to the horn of Amalthea (also known
    as **Cornucopia**), an eternal symbol of abundance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人称 Keras 为深度学习的*通用语言*，因为它的用户友好性、模块化和可扩展性。Keras 是一个高层次的神经网络应用程序编程接口，专注于快速实验的实现。它是用
    Python 编写的，可以在 TensorFlow 或 Keras 等后端上运行。Keras 最初是作为 ONEIROS（开放式神经电子智能机器人操作系统）项目的研究工作的一部分开发的。它的名字来源于希腊语单词，
    ![](img/5cd7fa77-066f-42ed-aa01-4a3a6e07fdd5.png)，字面意思是*角*。这个词暗指一段古希腊文学中的文字游戏，指的是阿马尔忒亚的角（也称为**丰饶之角**），这是丰盈和繁荣的永恒象征。
- en: 'Some functionalities of Keras include the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的一些功能包括以下内容：
- en: Easy and fast prototyping
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单快速的原型开发
- en: Supports implementation of several of the latest neural network architectures,
    as well as pretrained models and exercise datasets
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多种最新神经网络架构的实现，以及预训练模型和练习数据集
- en: Executes impeccably on CPUs and GPUs
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CPU 和 GPU 上完美执行
- en: TensorFlow
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow is an open source software library for high-performance numerical
    computation using a data representation known as **tensors**. It allows people
    like me and you to implement something called **dataflow graphs**. A dataflow
    graph is essentially a structure that describes how data moves through a network,
    or a series of processing neurons. Every neuron in the network represents a mathematical
    operation, and each connection (or *edge*) between neurons is a multidimensional
    data array, or *tensor*. In this manner, TensorFlow provides a flexible API that
    allows easy deployment of computation across a variety of platforms (such as CPUs,
    GPUs, and their very own **Tensor Processing Units** (**TPUs**)), and from desktops,
    to clusters of servers, to mobile and edge devices. Originally developed by researchers
    and engineers from the Google Brain team, it provides an excellent programmatic
    interface that supports neural network design and deep learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个开源软件库，用于高性能数值计算，使用一种叫做**张量**的数据表示方法。它让像我和你这样的人能够实现所谓的**数据流图**。数据流图本质上是一种结构，描述了数据如何在网络中移动，或者在一系列处理神经元中流动。网络中的每个神经元代表一个数学运算，每个神经元之间的连接（或*边*）是一个多维数据数组，或称为*张量*。通过这种方式，TensorFlow
    提供了一个灵活的 API，允许在各种平台（如 CPU、GPU 及其自有的**张量处理单元**（**TPUs**））上轻松部署计算，涵盖从桌面到服务器集群，再到移动设备和边缘设备。最初由
    Google Brain 团队的研究人员和工程师开发，它提供了一个出色的编程接口，支持神经网络设计和深度学习。
- en: The fundamentals of neural learning
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经学习的基础知识
- en: We begin our journey with an attempt to gain a fundamental understanding of
    the concept of learning. Moreover, what we are really interested in is how such
    a rich and complex phenomenon as learning has been implemented on what many call
    the most advanced computer known to humankind. As we will observe, scientists
    seem to continuously find inspiration from the inner workings of our own biological
    neural networks. If nature has indeed figured out a way to leverage loosely connected
    signals from the outside world and patch them together as a continuous flow of
    responsive and adaptive awareness (something most humans will concur with), we
    would indeed like to know exactly what tricks and treats it may have used to do
    so. Yet, before we can move on to such topics, we must establish a baseline to
    understand why the notion of neural networks are far different from most modern
    **machine learning** (**ML**) techniques.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程从试图获得学习概念的基本理解开始。此外，我们真正感兴趣的是，像学习这样一个丰富而复杂的现象是如何在被许多人称为人类已知最先进的计算机上实现的。正如我们将观察到的那样，科学家们似乎不断从我们自身生物神经网络的内部运作中获得灵感。如果大自然确实已经找到了利用外部世界的松散连接信号，并将其拼接成一个连续的响应和适应性意识流的方法（这是大多数人类都会认同的），我们确实希望了解它是如何做到这一点的。然而，在我们进入这些话题之前，我们必须建立一个基准，以理解为什么神经网络的概念与大多数现代**机器学习**（**ML**）技术有如此大的不同。
- en: What is a neural network?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: It is extremely hard to draw a parallel between neural networks and any other
    existing algorithmic mannerism for problem-solving that we have thus far. Linear
    regression, for example, simply deals with calculating a line of best fit with
    respect to the mean of squared errors from plotted observation points. Similarly,
    centroid clustering just recursively separates data by calculating ideal distances
    between similar points iteratively until it reaches an asymptotic configuration.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络与我们目前为止所知道的任何其他算法问题解决方式进行比较是非常困难的。例如，线性回归仅仅处理计算一条最佳拟合线，该线是相对于从绘制的观察点中计算的平方误差的均值。而类似地，质心聚类则是通过计算相似点之间的理想距离，递归地分离数据，直到达到渐近配置。
- en: 'Neural networks, on the other hand, are not that easily explicable, and there
    are many reasons for this. One way of looking at this is that a neural network
    is an algorithm that itself is composed of different algorithms, performing smaller
    local calculations as data propagates through it. This definition of neural networks
    presented here is, of course, not complete. We will iteratively improve it throughout
    this book, as we go over more complex notions and neural network architectures.
    Yet, for now, we may well begin with a layman''s definition: a neural network
    is a mechanism that automatically learns associations between the inputs you feed
    it (such as images) and the outputs you are interested in (that is, whether an
    image has a dog, a cat, or an attack helicopter).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 而神经网络则没有那么容易解释，原因有很多。一个看待这个问题的方式是，神经网络本身是一个由不同算法组成的算法，在数据传播的过程中执行更小的局部计算。这里所描述的神经网络定义，当然不是完整的。我们将在本书中通过讨论更复杂的概念和神经网络架构，逐步完善这一定义。不过，现在我们可以从一个外行的定义开始：神经网络是一种机制，能够自动学习你提供的输入（如图像）与你关心的输出之间的关联（也就是判断图像中是狗、猫还是攻击直升机）。
- en: So, now we have a rudimentary idea of what a neural network is—a mechanism that
    takes inputs and learns associations to predict some outputs. This versatile mechanism
    is, of course, not limited to being fed images only. Indeed, such networks are
    equally capable of taking inputs such as some text or recorded audio, and guessing
    whether it is looking at Shakespeare's *Hamlet*, or listening to *Billie Jean*,
    respectively. But how could such a mechanism compensate for the variety of data,
    in both form and size, while still producing relevant results? To understand this,
    many academics find it useful to examine how nature can solve this problem. In
    fact, the millions of years of evolution that occurred on our planet, through
    genetic mutations and environmental conditions, has produced something quite similar.
    Better yet, nature has even equipped each of us with a version of this universal
    function approximator, right between our two ears! We speak, of course, of the
    human brain.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对神经网络有了初步的理解——它是一种接受输入并学习关联以预测输出的机制。这个多功能的机制当然不限于仅仅接收图像作为输入。事实上，这样的网络同样能够接收一些文本或录制的音频作为输入，并猜测它是在看《哈姆雷特》还是在听《比莉·简》。但是，如何使这样的机制能够应对数据在形式和大小上的多样性，同时仍然产生相关的结果呢？为了理解这一点，许多学者发现，研究自然界是解决这一问题的一个有效途径。实际上，地球上经过数百万年的进化，经历了基因突变和环境变化，已经产生了一种非常相似的机制。更妙的是，大自然甚至为我们每个人配备了一个这种通用功能逼近器的版本，就在我们的双耳之间！我们当然在说的是人类大脑。
- en: Observing the brain
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察大脑
- en: Before we briefly delve into this notorious comparison, it is important for
    us to clarify here that it is indeed just a comparison, and not a parallel. We
    do not propose that neural networks work exactly in the manner that our brains
    do, as this would not only anger quite a few neuroscientists, but also does no
    justice to the engineering marvel represented by the anatomy of the mammalian
    brain. This comparison, however, helps us to better understand the workflow by
    which we may design systems that are capable of picking up relevant patterns from
    data. The versatility of the human brain, be it in making musical orchestras,
    art masterpieces, or pioneering scientific machinery such as the Large Hydron
    Collider, shows how the same architecture is capable of learning and applying
    highly complex and specialized knowledge to great feats. It turns out that nature
    is a pretty smart cookie, and hence we can learn a lot of valuable lessons by
    just observing how it has gone about implementing something so novel as a learning
    agent.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简要探讨这一著名类比之前，有必要在此澄清，这确实只是一个类比，而不是平行比较。我们并不提议神经网络的工作方式完全与我们的大脑相同，因为这样不仅会激怒不少神经科学家，还无法公正地评价哺乳动物大脑解剖学所代表的工程奇迹。然而，这一类比有助于我们更好地理解工作流程，以便设计能够从数据中提取相关模式的系统。人类大脑的多功能性，无论是在创作音乐乐团、艺术杰作，还是在开创科学设备如大型强子对撞机方面，展示了同一结构如何学习并应用高度复杂和专业的知识，完成伟大的壮举。事实证明，大自然真的是一个相当聪明的存在，因此，我们可以通过观察它如何实现像学习代理这样新颖的事物，获得许多宝贵的经验。
- en: Building a biological brain
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建生物大脑
- en: Quarks build up atoms, atoms build up molecules, and molecules grouped together
    may, once in a while, build up chemically excitable biomechanical units. We call
    these units **cells**; the fundamental building blocks of all biological life
    forms. Now, cells themselves come in exuberant variety, but one specific type
    of them is of interest to us here. It is a specific class of cells, known as **nerve
    cells**, or **neurons**. Why? Well, it turns out that if you take about 10^(11)
    neurons and set them up in a specific, complementary configuration, you get an
    organ that is capable of discovering fire, agriculture, and space travel. To realize
    how these bundles of neurons learn, however, we must first comprehend how one
    single neuron works. As you will see, it is the repetitive architecture in our
    brain, composed of these very same neurons, that gives rise to the grander phenomenon
    that we (pompously) call intelligence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 夸克构成原子，原子构成分子，分子聚集在一起，偶尔可能构成可电刺激的生物机械单元。我们将这些单元称为**细胞**；它们是所有生物生命形式的基本构建模块。现在，细胞本身种类繁多，但其中有一种特定类型对我们来说很有意义。那就是一类特定的细胞，叫做**神经细胞**或**神经元**。为什么呢？事实证明，如果你将约10^(11)个神经元按照特定且互补的配置排列，它们就能组成一个能够发现火、农业和太空旅行的器官。然而，要了解这些神经元如何学习，我们首先需要理解单个神经元是如何工作的。正如你将看到的，正是我们大脑中这些神经元所组成的重复架构，催生了我们（自负地）称之为智能的更宏大的现象。
- en: The physiology of a neuron
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经元的生理学
- en: 'A neuron is simply an electrically excitable cell that receives, processes,
    and transmits information through electrical and chemical signals. Dendrites extend
    from the neuron cell body and receive messages from other neurons. When we say
    that neurons *receive* or *send* messages, what we actually mean is that they
    transmit electrical impulses along their axons. Lastly, neurons are *excitable*.
    In other words, the right impulse supplied to a neuron will produce electrical
    events, known as **action potentials**. When a neuron reaches its action potential
    (or *spikes*), it releases a neurotransmitter, which is a chemical that travels
    a tiny distance across a synapse before reaching other neurons. Any time a neuron
    spikes, neurotransmitters are released from hundreds of its synapses, reaching
    the dendrites of other neurons that themselves may or may not spike, depending
    on the nature of the impulse. This is the very mannerism that allows these vast
    networks of neurons to communicate, compute, and work together to solve complex
    tasks that we humans face daily:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元只是一个能够接受、处理和通过电信号和化学信号传递信息的电刺激细胞。树突从神经元细胞体延伸出来，接收来自其他神经元的信息。当我们说神经元*接收*或*发送*信息时，实际上是指它们沿着轴突传递电信号。最后，神经元是*可兴奋的*。换句话说，向神经元提供合适的电刺激将引发电事件，这些电事件被称为**动作电位**。当神经元达到其动作电位（或*尖峰*）时，它会释放神经递质，这是一种通过突触传播到其他神经元的化学物质。每当神经元发生尖峰时，神经递质会从它的数百个突触中释放出来，进入其他神经元的树突，这些神经元可能会或可能不会发生尖峰，这取决于刺激的性质。这正是让这些庞大的神经网络相互通信、计算并协同工作来解决我们每天面对的复杂任务的方式：
- en: '![](img/85e7d29f-354c-41af-8351-083135d85a75.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85e7d29f-354c-41af-8351-083135d85a75.png)'
- en: So, all a neuron really does is take in some electric input, undergo some sort
    of processing, and then *fire* if the outcome is positive, or remain inactive
    if the outcome of that processing is negative. What do we mean here by whether
    an outcome is *positive*? To understand this, it is useful to have a little parenthesis
    on how information and knowledge is represented in our own brains.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，神经元真正做的事情就是接收一些电输入，进行某种处理，然后如果结果是积极的，就*发射*信号，或者如果处理结果是消极的，则保持不活跃。我们这里所说的结果是*积极的*是什么意思呢？要理解这一点，有必要稍微了解一下我们大脑中信息和知识是如何表示的。
- en: Representing information
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息表示
- en: Consider a task where you have to correctly classify images of dogs, cats, and
    attack helicopters. One way of thinking about a neuronal learning system is that
    we dedicate several neurons to represent the various features that exist in the
    three respective classes. In other words, let's say that we have employed three
    expert neurons for our classification task here. Each one of these neurons is
    an expert in the domain of what a dog, cat, and an attack helicopter looks like.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个任务，你需要正确地对狗、猫和攻击直升机的图像进行分类。可以这样理解神经学习系统：我们为这三类图像的不同特征分配了多个神经元。换句话说，假设我们为分类任务分配了三个专家神经元。每一个神经元都是狗、猫和攻击直升机的外观方面的专家。
- en: How are they experts? Well, for now, we can think that each of our domain expert
    neurons are supported by their own cabinet of employees and support staff, all
    diligently working for these experts, collecting and representing different breeds
    of dogs, cats, and attack helicopters, respectively. But we don't deal with their
    support staff for the time being. At the moment, we simply present any image to
    each of our three domain experts. If the picture is of a dog, our *dog expert*
    neuron immediately recognizes the creature and fires, almost as if it were saying,
    *Hello, I believe this is a dog. Trust me, I'm an expert*. Similarly, when we
    present our three experts a picture of a cat, our cat neuron will signal to us
    that they have detected a cat in our image by firing. While this is not exactly
    how each neuron represents real-world objects, such as cats and dogs, it helps
    us gain a functional understanding of neuron-based learning systems. Hopefully,
    you have enough information now to be introduced to the biological neuron's less
    sophisticated brother, the artificial neuron, next.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 他们为什么是专家呢？嗯，目前我们可以认为，我们每个领域专家的神经元都有自己的员工和支持人员来提供支持，所有人都在为这些专家勤奋工作，收集和表示不同种类的狗、猫和攻击直升机。但我们暂时不涉及他们的支持人员。目前，我们仅仅将任何图像呈现给我们的三位领域专家。如果图像是一只狗，我们的*狗专家*神经元立刻识别出这种生物并激活，几乎就像它在说，*你好，我相信这是一只狗。相信我，我是专家*。类似地，当我们将一张猫的图片呈现给我们的三位专家时，我们的猫神经元会通过激活告诉我们它们已经检测到图像中的猫。虽然这并不完全代表每个神经元如何表示现实世界中的物体，如猫和狗，但它帮助我们获得对基于神经元的学习系统的功能性理解。希望你现在有足够的信息，可以被介绍给生物神经元的“更不复杂”的兄弟——人工神经元。
- en: The mysteries of neural encoding
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经编码的奥秘
- en: In reality, many neuroscientists argue that this idea of a unified representative
    neuron, such as our *cat expert* neuron, doesn't really exist in our brain. They
    note how such a mechanism will require our brain to have thousands of neurons
    dedicated only to specific faces we have known, such as our grandmother, the baker
    around the corner, or Donald Trump. Instead, they postulate a more distributed
    representation architecture. This distributed theory states that a specific stimulus,
    such as the picture of a cat, is represented by (and will trigger) a unique pattern
    of firing neurons, widely distributed in the brain. In other words, a cat will
    be represented by perhaps (a wild guess) 100 different neurons, each dedicated
    to identifying specific cat-like features from the image (such as its ears, tail,
    eyes, and general body shape). The intuition here is that some of these cat neurons
    may be recombined with other neurons to represent other images that have elements
    of *cat* within. The picture of a jaguar, or the cartoon cat *Garfield*, for example,
    could be reconstructed using a subset of the very same cat neurons, in conjunction
    with some other neurons that have learned attributes that are more specific to
    the size of jaguars, or Garfield's famous orange and black stripes, perhaps.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多神经科学家认为，像我们的*猫专家*神经元这样统一代表性神经元的想法并不真实存在于我们的大脑中。他们指出，这样的机制要求我们的脑袋拥有成千上万只神经元，只专门用于识别我们已知的特定面孔，比如我们的祖母、街角的面包师或唐纳德·特朗普。相反，他们假设了一个更分布式的表示架构。这种分布式理论认为，特定的刺激，比如一张猫的图片，是通过一组独特的神经元激活模式来表示的，这些神经元广泛分布在大脑中。换句话说，一只猫可能由大约（这只是一个猜测）100个不同的神经元表示，每个神经元都专门负责识别图片中的特定猫类特征（比如耳朵、尾巴、眼睛和总体体型）。这里的直觉是，这些猫神经元中的一些可能与其他神经元重新组合，用于表示包含*猫*元素的其他图像。例如，
    jaguar的图片，或卡通猫*加菲猫*，可能通过使用相同猫神经元的一个子集来重建，同时结合一些已经学习了关于美洲豹体型或加菲猫著名的橙色和黑色条纹的神经元。
- en: Distributed representation and learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式表示与学习
- en: In some curious medical cases, patients with physical trauma to the head have
    not only failed to associate with their loved ones when confronted with them,
    but even claimed that these very loved ones were impostors just disguised as their
    loved ones! While a bizarre occurrence, such situations may shed more light onto
    the exact mechanisms of neural learning. Clearly, the patient recognizes this
    person, as some neurons encoding the visual patterns corresponding to the features
    of their loved ones (such as face and clothes) are fired. However, since they
    interestingly report this disassociation with these same loved ones despite being
    able to recognize them, it must mean that all the neurons that would normally
    fire upon coming across this loved one (including the neurons encoding the emotional
    representations our patient may have for this person) did not fire at the moment
    when our patient met their significant acquaintance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些令人好奇的医学案例中，头部受到身体创伤的患者不仅未能与他们的亲人产生联系，甚至声称这些亲人只是伪装成他们的亲人！虽然这是一个离奇的事件，但这种情况可能为我们揭示神经学习的具体机制。显然，患者能够识别这个人，因为一些编码视觉模式的神经元（如面部和衣物特征）被激活了。然而，由于他们尽管能够识别这些亲人，却报告自己与这些亲人失去了联系，这意味着当患者遇到他们的亲人时，所有正常情况下会被激活的神经元（包括编码情感反应的神经元）都没有被激活。
- en: These sorts of distributed representations may well allow our brain the versatility
    in extrapolating patterns from very little data, as we observe ourselves capable
    of doing. Modern neural networks, for example, still require you to provide it
    with hundreds (if not thousands) of images before it can reliably predict whether
    it is looking at a bus or a toaster. My three year-old niece, on the other hand,
    is able to parallel this accuracy with about three to five pictures of buses and
    toasters each. Even more fascinating is the fact that the neural networks running
    on your computer can, at times, take gigawatts of energy to perform computations.
    My niece only needs 12 watts. She will get what she needs from a few biscuits,
    or perhaps a small piece of a cake that she carefully sneaks away from the kitchen.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分布式表示方式可能让我们的脑袋在从极少的数据中推断模式时具有灵活性，正如我们观察到自己能够做到的那样。例如，现代神经网络仍然需要你提供数百张（如果不是数千张）图像，才能可靠地预测它是否在看一辆公交车或一台烤面包机。而我的三岁侄女，另一方面，能够凭借大约三到五张公交车和烤面包机的图片，就能达到相同的准确性。更令人着迷的是，运行在你电脑上的神经网络，有时需要耗费数千瓦的能源来进行计算。而我的侄女只需要12瓦特。她会从几块饼干中获得所需的能量，或者也许从她小心翼翼地从厨房偷走的一小块蛋糕中得到。
- en: The fundamentals of data science
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学基础
- en: Let's get acquainted with some basic terminologies and concepts of data science.
    We will get into some theory and then move on to understand some complex terms
    such as entropy and dimensionality.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来了解一下数据科学的一些基本术语和概念。我们将探讨一些理论内容，然后继续理解一些复杂的术语，如熵和维度。
- en: Information theory
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息理论
- en: Before a deeper dive into various network architectures and some hands-on examples,
    it would be a pity if we did not elaborate a little on the pivotal notion of gaining
    information through processing real-world signals. We speak of the science of
    quantifying the amount of information present in a signal, also referred to as
    information theory. While we don't wish to provide a deep mathematical overview
    on this notion, it is useful to know some background on learning from a probabilistic
    perspective.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨各种网络架构和一些实践案例之前，如果我们不对通过处理现实世界信号来获取信息这一关键概念进行一些阐述，那就太遗憾了。我们讨论的是量化信号中信息量的科学，也就是信息理论。虽然我们不打算提供关于这一概念的深度数学概述，但了解一些基于概率的学习背景是很有用的。
- en: 'Intuitively, learning that an unlikely event has occurred is more informative
    than learning that an expected event has occurred. If I were to tell you that
    you can *buy food at all supermarkets today*, I won''t be met with gasps of surprise.
    Why? Well, I haven''t really told you something beyond your expectations. Conversely,
    if I told you that you *cannot* buy food at all supermarkets today, perhaps due
    to some general strike, well, then you would be surprised. You would be surprised
    because an unlikely piece of information has been presented (in our case, this
    is the word *not*, appearing in the configuration previously presented). Such
    intuitive knowledge is what we attempt to codify, in the field of information
    theory. Other similar notions include the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，得知一个不太可能发生的事件已经发生，比得知一个预期事件已经发生更具信息量。如果我告诉你今天你可以*在所有超市购买食物*，你不会感到惊讶。为什么？嗯，我实际上并没有告诉你一些超出预期的信息。相反，如果我告诉你今天*不能*在所有超市购买食物，可能是因为某种大规模罢工，那么你可能会感到惊讶。你会感到惊讶是因为呈现了一个不太可能的信息（在我们的例子中，就是“*不*”这个词出现在之前的句子配置中）。这种直观的知识是我们试图在信息理论领域中加以规范的。其他类似的概念包括以下几点：
- en: An event with a lower likelihood of occurrence should have lower information
    content
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个发生可能性较低的事件应该具有较低的信息内容。
- en: An event with a higher likelihood of occurrence should have higher information
    content
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个发生可能性更高的事件应该具有更高的信息内容。
- en: An event with a guaranteed occurrence should have no information content
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个必定发生的事件应该没有信息内容。
- en: An event with an independent likelihood of occurrence should have additive information
    content
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有独立发生可能性的事件应该具有加法性的信息内容。
- en: 'Mathematically, we can actually satisfy all of these conditions by using the
    simple equation modeling the self-information of an event (*x*), as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，我们可以通过使用简单的方程来满足所有这些条件，该方程用于建模事件（*x*）的自信息，公式如下：
- en: '![](img/2e12687f-e26e-4842-8aab-7f75ea9f170b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e12687f-e26e-4842-8aab-7f75ea9f170b.png)'
- en: '*l(x)* is denoted in the *nat* unit, quantifying the amount of information
    gained by observing an event of probability, *1/e*. Although the preceding equation
    is nice and neat, it only allows us to deal with a single outcome; this is not
    too helpful in modeling the dependent complexities of the real world. What if
    we wanted to quantify the amount of uncertainty in an entire probability distribution
    of events? Then, we employ another measure, known as **Shannon entropy**, as shown
    in the following equation*:*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*l(x)*以*nat*为单位，表示通过观察概率为*1/e*的事件所获得的信息量。尽管前面的方程式简洁明了，但它仅允许我们处理单一结果；这在建模现实世界的复杂依赖性时并不太有帮助。如果我们想量化整个事件概率分布中的不确定性怎么办？那么，我们就使用另一种度量方法，称为**香农熵**，如下方程所示：'
- en: '![](img/55ae7519-72f3-46cb-b253-dfcb9de84553.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55ae7519-72f3-46cb-b253-dfcb9de84553.png)'
- en: Entropy
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵
- en: 'Let''s say you''re a soldier stuck behind enemy lines. Your goal is to let
    your allies know what kind of enemies are coming their way. Sometimes, the enemy
    may send tanks, but more often, they send patrols of people. Now, the only way
    you can signal your friends is by using a radio with simple binary signals. You
    need to figure out the best way to communicate with your allies, so as to not
    waste your precious time and get discovered by the enemy. How do you do this?
    Well, first you map out many sequences of binary bits, each specific sequence
    corresponding to a specific type of enemy (such as patrols or tanks). With a little
    knowledge of the environment, you already know that patrols are much more frequent
    than tanks. It stands to reason then, that you probably will be using the binary
    signal for *patrol* much more often than the one for *tank*. Hence, you will allocate
    fewer binary bits to communicate the presence of an incoming patrol, as you know
    you will be sending that signal more often than others. What you''re doing is
    exploiting your knowledge about the distribution over types of enemies to reduce
    the number of bits that you need to send on average. In fact, if you have access
    to the overall underlining distribution of incoming patrols and tanks, then you
    could theoretically use the smallest number of bits to communicate most efficiently
    with the friendlies on the other side. We do this by using the optimal number
    of bits at each transmission. The number of bits to represent a signal is known
    as the entropy of this data, and can be formulated with the following equation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一名被困在敌军后方的士兵。你的目标是让盟友知道敌人正在朝他们进发的方向。敌人有时会派出坦克，但更常见的是派出巡逻队。现在，你唯一能通知朋友们的方法是使用简单的二进制信号发射器。你需要弄清楚最好的沟通方式，以免浪费宝贵的时间并被敌人发现。你该如何做呢？首先，你需要规划出许多二进制信号序列，每个特定的序列对应一种特定类型的敌人（如巡逻队或坦克）。凭借对环境的一些了解，你已经知道巡逻队比坦克更常见。因此，合理的推断是，你很可能会比使用
    *坦克* 信号更频繁地使用 *巡逻队* 信号。因此，你会分配较少的二进制比特来传达巡逻队的存在，因为你知道你会比其他信号更频繁地发送这个信号。你正在利用你对敌人类型分布的了解，减少平均需要发送的比特数。事实上，如果你能够访问来袭巡逻队和坦克的整体基础分布，那么理论上你可以使用最少的比特数来最有效地与对面友军沟通。我们通过在每次传输时使用最佳比特数来实现这一点。表示一个信号所需的比特数被称为数据的熵，可以用以下方程来表示：
- en: '![](img/6e57d53f-400b-4a7f-a19f-98a98b9d752a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e57d53f-400b-4a7f-a19f-98a98b9d752a.png)'
- en: 'Here, *H(y)* denotes a function that refers to the optimal number of bits to
    represent an event with the probability distribution, *y*. *y[i]* simply refers
    to the probability of another event, *i.* So, supposing that seeing an enemy patrol
    is 256 times more likely to happen than seeing an enemy tank, we would model the
    number of bits to use to encode the presence of an enemy patrol, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*H(y)* 表示一个函数，表示用概率分布 *y* 来表示一个事件的最佳比特数。*y[i]* 只是指另一个事件 *i* 的概率。因此，假设看到敌方巡逻队的概率是看到敌方坦克的
    256 倍，我们将按如下方式建模用于编码敌方巡逻队存在的比特数：
- en: '*Patrol bits* = *log*(*1*/*256pTank*)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*巡逻队比特* = *log*(*1*/*256pTank*)'
- en: = *log*(*1*/*pTank*) + *log*(*1*/(*2*^*8*))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: = *log*(*1*/*pTank*) + *log*(*1*/(*2*^*8*))
- en: = *Tank bits* - *8*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: = *坦克比特* - *8*
- en: Cross entropy
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵
- en: 'Cross entropy is yet another mathematical notion, allowing us to compare two
    distinct probability distributions, denoted by *p* and *q*. In fact, as you will
    see later, we often employ entropy-based loss function in neural networks when
    dealing with categorical features. Essentially, the cross entropy between two
    probability distributions ([https://en.wikipedia.org/wiki/Probability_distribution](https://en.wikipedia.org/wiki/Probability_distribution)),
    *(p, q)*, over the same underlying set of events, measures the average number
    of pieces of information needed to identify an event picked at random from a set,
    under a condition; the condition being that the coding scheme used is optimized
    for a predicted probability distribution, rather than the *true* distribution.
    We will revisit this notion in later chapters to clarify and implement our understandings:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是另一个数学概念，它允许我们比较两个不同的概率分布，分别用*p*和*q*表示。事实上，正如你稍后会看到的，当处理分类特征时，我们经常在神经网络中使用基于熵的损失函数。从本质上讲，两个概率分布（[https://en.wikipedia.org/wiki/Probability_distribution](https://en.wikipedia.org/wiki/Probability_distribution)）之间的交叉熵，*(p,
    q)*，是在相同的事件集合上，衡量为了识别从该集合中随机选出的一个事件所需的平均信息量，前提是所使用的编码方案是针对预测的概率分布进行优化，而不是*真实*分布。我们将在后续章节中重新探讨这一概念，以澄清并实现我们的理解：
- en: '![](img/eb12840f-51f3-43cb-90f4-d093bb7a94f0.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb12840f-51f3-43cb-90f4-d093bb7a94f0.png)'
- en: The nature of data processing
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理的本质
- en: Earlier, we discussed how neurons may electrically propagate information and
    communicate with other neurons using chemical reactions. These same neurons help
    us determine what a *cat* or *dog* look like. But these neurons never actually
    see the full image of a cat. All they deal with is chemical and electric impulses.
    These networks of neurons can carry out their task only because of other sensory
    preprocessing organs, such as our eyes and optic nerve, that have prepared the
    data in an appropriate format for our neurons to be able to interpret. Our eyes
    take in the electromagnetic radiation (or light) that represents the image of
    a cat, and convert it into efficient representations thereof, communicated through
    electrical impulses. Hence, a prime difference between artificial and biological
    neurons relates to the medium of their intercommunication. As we saw, biological
    neurons use chemicals and electrical impulses as a means of communication. Similarly,
    artificial neurons rely on the universal language of mathematics to represent
    patterns from data. In fact, there exists a whole discipline surrounding the concept
    of representing real-world phenomena mathematically for the purpose of knowledge
    extraction. This discipline, as many of you are familiar with, is known as **data
    science**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了神经元如何通过电信号传播信息，并利用化学反应与其他神经元进行交流。这些神经元帮助我们判断*猫*或*狗*长什么样。但这些神经元从来没有真正看到过完整的猫的图像。它们处理的只是化学和电信号。这些神经元网络能够完成任务，仅仅因为有其他感官预处理器官（如我们的眼睛和视神经）在为神经元提供适当格式的数据，使其能够进行解读。我们的眼睛接收表示猫图像的电磁辐射（或光），并将其转换为高效的表示形式，通过电信号传递出去。因此，人工神经元与生物神经元的一个主要区别就在于它们之间交流的媒介。如我们所见，生物神经元通过化学物质和电信号进行通信。类似地，人工神经元依赖于数学这一通用语言来表示数据中的模式。实际上，围绕着将现实世界现象以数学形式表示的概念，存在一个完整的学科，旨在从中提取知识。正如许多人所熟悉的，这个学科被称为**数据科学**。
- en: From data science to ML
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据科学到机器学习
- en: 'Pick up any book on data science; there is a fair chance that you will come
    across an elaborate explanation, involving the intersection of fields such as
    statistics and computer science, as well as some domain knowledge. As you flip
    through the pages rapidly, you will notice some nice visualizations, graphs, bar
    charts—the works. You will be introduced to statistical models, significance tests,
    data structures, and algorithms, each providing impressive results for some demonstrative
    use case. This is not data science. These are indeed the very tools you will be
    using as a successful data scientist. However, the essence of what data science
    is can be summarized in a much simpler manner: *data science is the scientific
    domain that deals with generating actionable knowledge from raw data. This is
    done by iteratively observing a real-world problem, quantifying the overall phenomena
    in different dimensions, or features, and predicting future outcomes that permit
    desired ends to be achieved. ML is just the discipline of teaching machines data
    science*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 拿起任何一本关于数据科学的书；你很可能会遇到一些复杂的解释，涉及到统计学、计算机科学等领域的交叉，以及一些领域知识。当你快速翻阅书页时，你会注意到一些漂亮的可视化图表、条形图——这些都是数据科学的经典呈现形式。你会接触到统计模型、显著性检验、数据结构和算法，每一种都能为某些演示案例提供令人印象深刻的结果。这些并不是数据科学。这些确实是你作为一名成功数据科学家将使用的工具。然而，数据科学的本质可以用更简单的方式来概括：*数据科学是一个科学领域，专注于从原始数据中生成可操作的知识。这是通过反复观察现实世界的问题，量化不同维度或特征中的整体现象，并预测未来的结果，以实现期望的目标。机器学习（ML）仅仅是教机器数据科学的学科*。
- en: While some computer scientists may appreciate this recursive definition, some
    of you may ponder what is meant by *quantifying a phenomenon.* Well, you see,
    most observations in the real world, be it the amount of food you eat, the kind
    of programs you watch, or the colors you like on your clothes, can be all defined
    as (approximate) functions of some other quasi-dependent features. For example,
    the amount of food you will eat in any given day can be defined as a function
    of other things, such as how much you ate in your previous meal, your general
    inclination to certain types of food, or even the amount of physical exertion
    you get.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些计算机科学家可能会欣赏这种递归定义，但你们中的一些人可能会思考什么是*量化现象*。嗯，你看，现实世界中的大多数观察，无论是你吃了多少食物、你看什么类型的节目，还是你喜欢穿什么颜色的衣服，都可以定义为（近似的）其他某些准依赖特征的函数。例如，你在某一天会吃多少食物，可以定义为其他因素的函数，比如你在上一餐吃了多少食物、你对某些类型食物的偏好，甚至是你进行的体力活动量。
- en: Similarly, the kind of programs you like to watch may be approximated by features
    such as your personality traits, interests, and the amount of free time in your
    schedule. Reductively speaking, we work with quantifying and representing differences
    between observations (for example, the viewing habits between people), to deduce
    a functional predictive rule that machines may work with.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你喜欢观看的节目可以通过一些特征来近似，例如你的个性特征、兴趣和日程中可用的空闲时间。简言之，我们通过量化和表示观察之间的差异（例如，不同人群的观看习惯），来推导出机器可以使用的功能性预测规则。
- en: 'We induce these rules by defining the possible outcomes that we are trying
    to predict (that is, whether a given person likes comedies or thrillers) as a
    function of input features (that is, how this person ranks on the Big Five personality
    test) that we collect when observing a phenomenon at large (such as personalities
    and the viewing habits of a population):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义我们试图预测的可能结果（即某人是否喜欢喜剧或惊悚片）来诱导这些规则，作为输入特征的函数（即我们在观察现象时收集的关于此人的大五人格测试排名），这在广义上涉及对现象的观察（例如，人口的个性特征和观看习惯）：
- en: '![](img/ca603c1f-d4a1-4da7-84fd-4be4ef2dfce5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca603c1f-d4a1-4da7-84fd-4be4ef2dfce5.png)'
- en: If you have selected the right set of features, you will be able to derive a
    function that is able to reliably predict the output classes that you are interested
    in (in our case, this is viewer preferences). What do I mean by the right features?
    Well, it stands to reason that viewing habits have more to do with a person's
    personality traits than their travel habits. Predicting whether someone is inclined
    towards horror movies as a function of, say, their eye color and real-time GPS
    coordinates, will be quite useless, as they are not informative to what we are
    trying to predict. Hence, we always choose relevant features (through domain knowledge
    or significance tests) to reductively represent a real-world phenomenon. Then,
    we simply use this representation to predict the future outcomes that we are interested
    in. This representation itself is what we call a predictive model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择了正确的特征集，你将能够推导出一个可靠的函数，该函数能够准确预测你感兴趣的输出类别（在我们的例子中，这是观众的偏好）。我所说的正确特征是什么意思？很显然，观看习惯更多地与一个人的个性特征相关，而不是与他们的旅行习惯相关。例如，通过眼睛颜色和实时
    GPS 坐标来预测某人是否倾向于观看恐怖片几乎毫无意义，因为这些信息与我们试图预测的内容没有关联。因此，我们总是选择相关的特征（通过领域知识或显著性检验）来简洁地表示一个现实世界的现象。然后，我们只是用这个表示来预测我们感兴趣的未来结果。这个表示本身就是我们所称的预测模型。
- en: Modeling data in high-dimensional spaces
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在高维空间中建模数据
- en: As you saw, we can represent real-world observations by redefining them as a
    function of different features. The speed of an object, for example, is a function
    of the distance it traveled over a given time. Similarly, the color of a pixel
    on your TV screen is actually a function of the red, green, and blue intensity
    values that make up that pixel. These elements are what data scientists call features
    or dimensions of your data. When we have dimensions that are labeled, we deal
    with a supervised learning task, as we can check the learning of our model with
    respect to what is truly the case. When we have unlabeled dimensions, we calculate
    the distances between our observation points to find similar groups in our data.
    This is known as **unsupervised ML**. Hence, in this manner, we can start building
    a model of a real-world phenomenon, by simply representing it using informative
    features.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们可以通过将实际世界的观察结果重新定义为不同特征的函数来表示它们。例如，一个物体的速度是它在给定时间内所行驶的距离的函数。类似地，电视屏幕上像素的颜色实际上是由构成该像素的红、绿、蓝三种颜色强度值所决定的。这些元素就是数据科学家所称的数据特征或维度。当我们拥有已标记的维度时，我们处理的是监督学习任务，因为我们可以根据实际情况检查我们模型的学习效果。当我们拥有未标记的维度时，我们通过计算观察点之间的距离来找出数据中相似的群体。这就是所谓的**无监督机器学习**。因此，通过这种方式，我们可以通过简单地使用信息特征来表示它，从而开始构建一个现实世界现象的模型。
- en: The curse of dimensionality
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度的诅咒
- en: 'The natural question that follows is: how exactly do we build a model? Long
    story short, the features that we choose to collect while observing an outcome
    can all be plotted on a high-dimensional space. While this may sound complicated,
    it is just an extension of the Cartesian coordinate system that you may be familiar
    with from high school mathematics. Let''s recall how to represent a single point
    on a graph, using the Cartesian coordinate system. For this task, we require two
    values, *x* and *y*. This is an example of a two-dimensional feature space, with
    the *x* and *y* axis each being a dimension in the representational space. Add
    a *z* axis, and we get a three-dimensional feature space. Essentially, we define
    ML problems in an *n*-dimensional feature space, where *n* refers to the number
    of features that we have on the phenomenon we are trying to predict. In our previous
    case of predicting viewer preference, if we solely use the Big Five personality
    test scores as input features, we will essentially have a five-dimensional feature
    space, where each dimension corresponds to a person''s score on one of the five
    personality dimensions. In fact, modern ML problems can range from 100 to 100,000
    dimensions (and sometimes even more). Since the number of possible configurations
    of features increases exponentially with respect to increases in the number of
    different features, it becomes quite hard, even for computers, to conceive and
    compute in such proportions. This problem in ML is generally referred to as the
    *curse of dimensionality*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的自然问题是：我们到底是如何构建一个模型的？简而言之，我们在观察一个结果时选择收集的特征都可以绘制在高维空间中。虽然这听起来很复杂，但它仅仅是你在高中数学中可能熟悉的笛卡尔坐标系统的扩展。让我们回忆一下如何使用笛卡尔坐标系统在图表上表示一个点。对于这个任务，我们需要两个值，*x*
    和 *y*。这是一个二维特征空间的示例，其中 *x* 和 *y* 轴分别是表示空间中的一个维度。如果再加上一个 *z* 轴，我们就得到了一个三维特征空间。本质上，我们在一个
    *n* 维特征空间中定义机器学习问题，其中 *n* 表示我们试图预测的现象中的特征数量。在我们之前预测观众偏好的例子中，如果我们仅使用大五人格测试的分数作为输入特征，那么我们实际上拥有一个五维特征空间，其中每个维度对应一个人五个性维度之一的得分。事实上，现代机器学习问题的维度可以从
    100 到 100,000（有时甚至更多）。由于特征数量的不同配置的可能性随着特征数量的增加而指数级增长，因此即使是计算机，也很难在如此大的比例下进行构思和计算。这个在机器学习中出现的问题通常被称为
    *维度诅咒*。
- en: Algorithmic computation and predictive models
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法计算与预测模型
- en: Once we have a high-dimensional representation of relevant data, we can commence
    the task of deriving a predictive function. We do this by using algorithms, which
    are essentially a set of preprogrammed recursive instructions that categorize
    and divide our high-dimensional data representation in a certain manner. These
    algorithms (these are most commonly clustering, classification, and regression)
    recursively separate our data points (that is, personality rankings per person)
    on the feature space into smaller groups where the data points are comparatively
    more alike. In this manner, we use algorithms to iteratively segment our high-dimensional
    feature space into smaller regions, which will eventually correspond to our output
    classes (ideally). Hence, we can reliably predict the output class of any future
    data points simply by placing them on our high-dimensional feature space and comparing
    them to the regions corresponding to our model's predicted output classes. Congratulations,
    we have a predictive model!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了相关数据的高维表示，我们就可以开始推导预测函数的任务。我们通过使用算法来实现这一点，算法本质上是一组预编程的递归指令，用于以某种方式对我们的高维数据表示进行分类和划分。这些算法（最常见的有聚类、分类和回归）递归地将我们的数据点（即每个人的个性排名）在特征空间中划分成更小的组，在这些组内数据点相对更相似。通过这种方式，我们使用算法迭代地将我们的高维特征空间划分成更小的区域，这些区域最终将对应我们的输出类别（理想情况下）。因此，我们可以通过简单地将任何未来的数据点放置到我们的高维特征空间中，并将其与模型预测输出类别对应的区域进行比较，从而可靠地预测其输出类别。恭喜，我们已经有了一个预测模型！
- en: Matching a model to use cases
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配模型与使用案例
- en: Every time we choose to define an observation as a function of some features,
    we open up a Pandora's box of semi-causally linked features, where each feature
    itself could be redefined (or quantified) as a function of other features. In
    doing so, we might want to take a step back, and consider what exactly we are
    trying to represent. Is our model capturing relevant patterns? Can we rely on
    our data? Will our resources, be it algorithms or computational firepower, suffice
    for learning from the data we have?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们选择将一个观察定义为某些特征的函数时，我们就打开了一个潘多拉魔盒，里面是半因果相关的特征，其中每个特征本身也可能被重新定义（或量化）为其他特征的函数。这样做时，我们可能需要退一步，思考我们到底在尝试表示什么。我们的模型是否捕捉到了相关的模式？我们可以依赖我们的数据吗？我们的资源——无论是算法还是计算能力——是否足够从我们拥有的数据中学习？
- en: Recall our earlier scenario of predicting the quantity of food an individual
    is likely to consume in each meal. The features that we discussed, such as their
    physical exertion, could be redefined as a function of their metabolic and hormonal
    activity. Similarly, dietary preferences could be redefined as a function of their
    gut bacteria and stool composition. Each of these redefinitions adds new features
    to our model, bringing with them additional complexity.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆我们之前讨论的预测个人每餐可能消耗的食物数量的场景。我们讨论过的特征，例如他们的体力活动，可以重新定义为其代谢和荷尔蒙活动的函数。类似地，饮食偏好可以重新定义为肠道细菌和大便组成的函数。每一次这样的重新定义都会为我们的模型增加新的特征，并带来额外的复杂性。
- en: 'Perhaps we can even achieve a greater accuracy in predicting exactly how much
    takeout you should order. Would this be worth the effort of getting a stomach
    biopsy every day? Or installing a state-of-the-art electron microscope in your
    toilet? Most of you will agree: no, it would not be. How did we come to this consensus?
    Simply by assessing our use case of dietary prediction and selecting features
    that are relevant *enough* to predict what we want to predict, in a fashion that
    is reliable and proportional to our situation. A complex model supplemented by
    high-quality hardware (such as toilet sensors) is unnecessary and unrealistic
    for the use case of dietary prediction. You could as easily achieve a functional
    predictive model based on easily obtainable features, such as purchase history
    and prior preferences.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们甚至可以更准确地预测你应该点多少外卖。是否值得为了每天做一次胃部活检？或者在你的厕所里安装一台最先进的电子显微镜？你们大多数人会同意：不，完全不值得。我们是如何达成这个共识的？仅仅通过评估我们的饮食预测用例，并选择足够*相关*的特征来预测我们想要预测的内容，以一种可靠且与我们的情况相称的方式。一个复杂的模型，配合高质量的硬件（比如厕所传感器），对于饮食预测这个用例来说既不必要也不现实。你完全可以基于一些易于获取的特征，例如购买历史和以往偏好，来实现一个功能性预测模型。
- en: The essence of this story is that you may define any observable phenomenon as
    a function of other phenomenon in a recursive manner, but a clever data scientist
    will know when to stop by picking appropriate features that reasonably fit your
    use case; are readily observable and verifiable; and robustly deal with all relevant
    situations. All we need is to approximate a function that reliably predicts the
    output classes for our data points. Inducting a too complex or simplistic representation
    of our phenomenon will naturally lead to the demise of our ML project.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事的本质是，你可以将任何可观察现象定义为其他现象的函数，以递归的方式进行，但聪明的数据科学家会知道何时停止，通过选择适合你的用例的合适特征来停止；这些特征是易于观察和验证的；并且能稳健地处理所有相关情况。我们所需要的仅仅是逼近一个函数，能够可靠地预测数据点的输出类别。过于复杂或过于简单的现象表示自然会导致我们的机器学习项目失败。
- en: Functional representations
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数表示
- en: Before we march forth in our journey to understand, build, and master neural
    networks, we must at least refresh our perception of some fundamental ML concepts.
    For example, it is important to understand that you are never modeling a phenomenon
    completely. You are only *functionally* representing a part of it. This helps
    you think about data intuitively, forming but a small piece in the large puzzle,
    represented by a general phenomenon that you are trying to understand. This also
    helps you realize that times change. The importance of features, as well as surrounding
    environments, are both subject to such change, eroding the predictive power of
    your model. Such intuition is naturally built with practice and domain knowledge.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续进行理解、构建和掌握神经网络的旅程之前，至少要刷新一下我们对一些基础机器学习概念的认识。例如，理解一个关键点：你从来不会完全地建模一个现象，你只是在*功能上*表示它的一部分。这有助于你直观地思考数据，将其视为理解过程中大拼图中的一小部分。它还帮助你意识到，时间在变化，特征的重要性以及周围环境的变化都可能影响模型的预测能力。这种直觉通过实践和领域知识自然形成。
- en: In the following section, we will briefly refresh our memory with some classic
    pitfalls of ML use cases, with a few simple scenario-driven examples. This is
    important to do as we will notice these same problems reappear when we undertake
    our main journey of understanding and applying neural networks to various use
    cases.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过一些简单的场景驱动示例，简要回顾机器学习应用中的一些经典陷阱。这样做很重要，因为在我们将神经网络应用到各种用例时，我们会发现这些相同的问题再次出现。
- en: The pitfalls of ML
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的陷阱
- en: Consider the problem of predicting the weather forecast. We will begin constructing
    our predictive model by doing some feature selection. With some domain knowledge,
    we initially identify the feature *air pressure* as a relevant predictor. We will
    record different *Pa* values (Pascals, a measure of air pressure) over different
    days on the island of Hawaii, where we live. Some of these days turn out to be
    sunny, and others rainy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 设想一个天气预报预测问题。我们将通过进行特征选择来构建我们的预测模型。凭借一些领域知识，我们首先识别出特征*气压*是一个相关的预测因子。我们将在夏威夷岛记录不同天数的*Pa*值（帕斯卡，气压的测量单位），其中一些日子是晴天，另一些是雨天。
- en: Unbalanced class priors
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡的类别先验
- en: After several sunny days, your predictive model tells you that there is a very
    high chance of the following day also being sunny, yet it rains. Why? This is
    simply because your model has not seen enough instances of both prediction classes
    (sunny and rainy days) to accurately assess the chance of there being rain. In
    this case, it is said to have unbalanced class priors, which misrepresent the
    overall weather pattern. According to your model, there are only sunny days because
    it has only seen sunny days as of yet.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了几天的晴天后，你的预测模型告诉你第二天也有很高的可能性是晴天，但实际却下雨了。为什么？这只是因为你的模型没有看到足够的两种预测类别（晴天和雨天）的实例，无法准确评估下雨的概率。在这种情况下，模型存在不平衡的类别先验，这会误导整体天气模式的判断。根据你的模型，只有晴天，因为它到目前为止只看到了晴天。
- en: Underfitting
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合
- en: You have collected about two months worth of air pressure data, and balanced
    the number of observations in each of your output classes. Your prediction accuracy
    has steadily increased, but starts tapering off at a suboptimal level (let's say
    61%). Suddenly, your model's accuracy starts dropping again, as it gets colder
    and colder outside. Here, we face the problem of underfitting, as our simplistic
    model is unable to capture the underlying pattern of our data, caused by the seasonal
    coming of winter. There are a few simple remedies to this situation. Most prominently,
    we may simply improve our model by adding more predictive features, such as the
    outside temperature. Doing so, we observe after a few days of data collection
    that once again, our accuracy climbs up, as the additional feature adds more information
    to our model, increasing its predictive power. In other cases of underfitting,
    we may well have chosen to select a more computationally-intensive predictive
    model, add more data and engineer better features, or reduce any mathematical
    constraints (such as the lambda hyperparameter for regularization) on the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你收集了大约两个月的气压数据，并平衡了每个输出类别中的观测值数量。你的预测准确率稳步提升，但在达到一个次优水平后（假设是 61%）开始趋于平稳。突然之间，随着外面越来越冷，你的模型准确率开始再次下降。这里我们面临的是欠拟合问题，因为我们简单的模型无法捕捉到数据的潜在模式，这种模式是由冬季的季节性变化引起的。对此情况有一些简单的解决办法。最明显的做法是通过增加更多的预测特征来改进模型，比如增加外部温度这一变量。这样做后，我们观察到在几天的数据收集后，准确率再次上升，因为额外的特征为模型提供了更多信息，提升了其预测能力。在其他欠拟合的情况下，我们可能会选择更计算密集的预测模型，增加更多数据，工程化地优化特征，或者减少模型中的数学约束（例如正则化的
    lambda 超参数）。
- en: Overfitting
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: After collecting about a few years of data, you confidently boast that you have
    developed a robust predictive model with 96% accuracy to your farmer friend. Your
    friend says, *Well, great news, can I have it?* You, being an altruist and philanthropist,
    immediately agree and send him the code. A day later the same friend calls back
    from his home in Guangdong province in China, angry that your model did not work
    and has ruined his crop harvest. What happened here? This was simply a case of
    overfitting our model to the tropical climate of Hawaii, which does not generalize
    well outside of this sample. Our model did not see enough variations that actually
    exist in the possible values of pressure and temperatures, with the corresponding
    labels of *sunny* and *rainy*, to sufficiently be able to predict the weather
    on another continent. In fact, since our model only saw Hawaiian temperatures
    and air pressures, it memorized trivial patterns in the data (for example, there
    are never two rainy days in a row) and uses these patterns as rules for making
    a prediction, instead of picking up on more informative trends. One simple remedy
    here is, of course, to gather more weather data in China, and fine-tune your prediction
    model to the local weather dynamics. In other similar situations involving overfitting,
    you may attempt to select a simpler model, denoise the data by removing outliers
    and errors, and center it with respect to mean values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了大约几年的数据后，你自信地对你的农民朋友吹嘘，称你已经开发了一个预测准确率为 96% 的强大模型。你的朋友说，*太好了，我能用这个吗？* 作为一个利他主义者和慈善家，你立刻同意并把代码发给他。一天后，同一个朋友从他位于中国广东省的家里打电话回来，生气地说你的模型没能工作，并且毁掉了他的作物收成。发生了什么事？这其实只是一个将我们的模型过拟合到夏威夷热带气候的例子，导致模型无法很好地推广到其他样本之外。我们的模型没有看到气压和温度的足够变化，缺少了与*晴天*和*雨天*相对应的标签，无法充分预测另一个大陆的天气。实际上，由于我们的模型只看到了夏威夷的温度和气压，它记住了数据中的一些微不足道的模式（例如，两天连着的雨天是从未出现过的），并且将这些模式作为预测规则，而不是抓住更有信息量的趋势。当然，这里的一个简单解决办法是收集更多中国的天气数据，并根据当地的天气动态来微调你的预测模型。在其他类似的过拟合情况下，你可以尝试选择一个更简单的模型，通过去除离群值和错误来去噪数据，并使数据围绕均值进行中心化。
- en: Bad data
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误数据
- en: After explaining to your dear friend from China (henceforth referred to as Chan)
    the miscalculation that just occurred, you instruct him to set up sensors and
    start collecting local air pressure and temperature to construct a labeled dataset
    of sunny and rainy days, just like you did in Hawaii. Chan diligently places sensors
    on his roof and in his fields. Unfortunately, Chan's roof is made of a reinforced
    metal alloy with a high thermal conductivity, which erratically inflates the reading
    from both the pressure and temperature sensors from the roof in an inconsistent
    and unreliable manner. This corrupted data, when fed to our predictive model,
    will naturally produce suboptimal results, as the learned line is perturbed by
    noisy and misrepresentative data. A clear remedy would be to replace the sensors,
    or simply discard the faulty sensor readings.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在向你亲爱的中国朋友（以下简称“Chan”）解释刚刚发生的误算后，你指示他设置传感器并开始收集本地的气压和温度数据，以构建一个标注的晴天和雨天数据集，就像你在夏威夷做的那样。Chan勤奋地将传感器安装在他的屋顶和田地中。不幸的是，Chan的屋顶由高热导性的强化金属合金制成，这种材料会不规则地使屋顶的气压和温度传感器读数波动，导致读数不一致且不可靠。将这些损坏的数据输入到我们的预测模型中，自然会产生次优的结果，因为学到的线条被噪声和不具有代表性的数据干扰了。一个明显的解决方法是更换传感器，或者干脆丢弃有问题的传感器读数。
- en: Irrelevant features and labels
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无关特征和标签
- en: Eventually, using enough data from Hawaii, China, and some other places in the
    world, we notice a clear and globally generalizable pattern, which we can use
    to predict the weather. So, everybody is happy, until one day, your prediction
    model tells you it is going to be a bright sunny day, and a tornado comes knocking
    on your door. What happened? Where did we go wrong? Well, it turns out that when
    it comes to tornadoes, our two-featured binary classification model does not incorporate
    enough information about our problem (this being the dynamics of tornadoes) to
    allow us to approximate a function that reliably predicts this specifically devastating
    outcome. So far, our model did not even try to predict tornadoes, and we only
    collected data for sunny and rainy days.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，通过使用来自夏威夷、中国和世界其他地方的足够数据，我们注意到一个明显的、全球普适的模式，这个模式可以用来预测天气。所以，每个人都很开心，直到有一天，你的预测模型告诉你今天将是一个明媚的晴天，结果却有龙卷风来敲门。发生了什么？我们哪里出错了？事实证明，当涉及到龙卷风时，我们的这个具有两个特征的二分类模型并没有包含足够的关于问题（即龙卷风动态）的信息，无法逼近一个可靠预测这一特定灾难结果的函数。到目前为止，我们的模型甚至没有尝试预测龙卷风，我们只收集了晴天和雨天的数据。
- en: A climatologist here might say, *Well, then start collecting data on altitude,
    humidity, wind speed, and direction, and add some labeled instanced of tornadoes
    to your data*, and, indeed, this would help us fend off future tornadoes. That
    is, until an earthquake hits the continental shelf and causes a tsunami. This
    illustrative example shows how whatever model you choose to use, you need to keep
    tracking relevant features, and have enough data per each prediction class (such
    as whether it is sunny, rainy, tornado-ey, and so on) to achieve good predictive
    accuracy. Having a good prediction model simply means that you have discovered
    a mechanism that is capable of using the data you have collected so far, to induct
    a set of predictive rules that are seemingly being obeyed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一位气候学家可能会说，*那就开始收集关于海拔、湿度、风速和风向的数据，并在你的数据中添加一些标注的龙卷风实例*，的确，这会帮助我们抵御未来的龙卷风。但这也仅限于此，直到某天地震袭击了大陆架并引发了海啸。这个例子说明了无论你选择什么样的模型，都需要持续跟踪相关特征，并且每个预测类别（例如是否是晴天、雨天、龙卷风天气等）都要有足够的数据，才能实现良好的预测精度。拥有一个好的预测模型意味着你已经发现了一种能够使用到目前为止收集的数据来推导出一组似乎被遵守的预测规则的机制。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we gained a functional overview of biological neural networks,
    with a small and brief preview covering concepts such as neural learning and distributed
    representations. We also refreshed our memory on some classic data science dilemmas
    that are equally relevant for neural networks as they are for other ML techniques.
    In the following chapter, we will finally dive into the much-anticipated learning
    mechanism loosely inspired by our biological neural networks, as we explore the
    basic architecture of an ANN. We amicably describe ANNs in such a manner because,
    despite aiming to work as effectively as their biological counterparts, they are
    not quite there yet. In the next chapter, you will go over the main implementation
    considerations involved in designing ANNs and progressively discover the complexity
    that such an endeavour entails.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对生物神经网络进行了功能性概述，简要介绍了神经学习和分布式表征等概念。我们还回顾了一些经典的数据科学难题，这些难题对于神经网络与其他机器学习技术同样适用。在接下来的章节中，我们将深入探讨受到生物神经网络启发的学习机制，并探索人工神经网络（ANN）的基本架构。我们以友好的方式描述ANN，因为尽管它们旨在像生物神经网络一样高效工作，但目前还未完全达到这一目标。在下一章中，您将了解设计ANN时的主要实现考虑因素，并逐步发现这一过程所涉及的复杂性。
- en: Further reading
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '**Symbolic versus connectionist learning**: [http://www.cogsci.rpi.edu/~rsun/sun.encyc01.pdf](http://www.cogsci.rpi.edu/~rsun/sun.encyc01.pdf)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符号主义学习与联结主义学习**：[http://www.cogsci.rpi.edu/~rsun/sun.encyc01.pdf](http://www.cogsci.rpi.edu/~rsun/sun.encyc01.pdf)'
- en: '**History of artificial intelligence**: [http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/](http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能的历史**：[http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/](http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/)'
- en: '**History of the human brain**: [http://www.mybrain.co.uk/public/learn_history4.php](http://www.mybrain.co.uk/public/learn_history4.php)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类大脑的历史**：[http://www.mybrain.co.uk/public/learn_history4.php](http://www.mybrain.co.uk/public/learn_history4.php)'
