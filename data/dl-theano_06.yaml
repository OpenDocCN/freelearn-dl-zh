- en: Chapter 6. Locating with Spatial Transformer Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. 使用空间变换器网络进行定位
- en: In this chapter, the NLP field is left to come back to images, and get an example
    of application of recurrent neural networks to images. In [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* we addressed the case of image
    classification, consisting of predicting the class of an image. Here, we'll address
    object localization, a common task in computer vision as well, consisting of predicting
    the bounding box of an object in the image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将NLP领域留到后面再回到图像，并展示递归神经网络在图像中的应用实例。在[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "第2章. 使用前馈网络分类手写数字")，*使用前馈网络分类手写数字*中，我们处理了图像分类的问题，即预测图像的类别。在这里，我们将讨论对象定位，这是计算机视觉中的一个常见任务，旨在预测图像中对象的边界框。
- en: While [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* solved the classification task
    with neural nets built with linear layers, convolutions, and non-linarites, the
    spatial transformer is a new module built on very specific equations dedicated
    to the localization task.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 而[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b "第2章.
    使用前馈网络分类手写数字")，*使用前馈网络分类手写数字*通过使用线性层、卷积层和非线性激活函数构建的神经网络解决了分类任务，而空间变换器是一个新的模块，基于非常特定的方程，专门用于定位任务。
- en: In order to locate multiple objects in the image, spatial transformers are composed
    with recurrent networks. This chapter takes the opportunity to show how to use
    prebuilt recurrent networks in **Lasagne**, a library on top of Theano that brings
    extra modules, and helps you develop your neural networks very fast with pre-built
    components, while not changing the way you build and handle nets with Theano.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在图像中定位多个对象，空间变换器是通过递归网络构成的。本章借此机会展示如何在**Lasagne**中使用预构建的递归网络，Lasagne是一个基于Theano的库，提供额外的模块，并通过预构建的组件帮助你快速开发神经网络，同时不改变你使用Theano构建和处理网络的方式。
- en: 'To sum up, the list of topics is composed of:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，主题列表由以下内容组成：
- en: An introduction to the Lasagne library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lasagne库简介
- en: Spatial transformer networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空间变换器网络
- en: Classification network with spatial transformers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有空间变换器的分类网络
- en: Recurrent modules with Lasagne
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Lasagne的递归模块
- en: Recurrent read of digits
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字的递归读取
- en: Unsupervised training with hinge loss functions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用铰链损失函数的无监督训练
- en: Region-based object localization neural nets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于区域的对象定位神经网络
- en: MNIST CNN model with Lasagne
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Lasagne的MNIST CNN模型
- en: 'The Lasagne library has packaged layers and tools to handle neural nets easily.
    Let''s first install the latest version of Lasagne:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Lasagne库打包了层和工具，能够轻松处理神经网络。首先，让我们安装最新版本的Lasagne：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let us reprogram the MNIST model from [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* with Lasagne:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Lasagne重新编写[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "第2章. 使用前馈网络分类手写数字")，*使用前馈网络分类手写数字*中的MNIST模型：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The layers are `layer0_input`, `conv1_out`, `pooled_out`, `conv2_out`, `pooled2_out`,
    `hidden_output`. They are built with pre-built modules, such as, `InputLayer`,
    `Conv2DLayer`, `MaxPool2DLayer`, `DenseLayer`, dropout non-linearities such as
    rectify or softmax, and initialization such as `GlorotUniform`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 层包括`layer0_input`、`conv1_out`、`pooled_out`、`conv2_out`、`pooled2_out`、`hidden_output`。它们是通过预构建的模块构建的，例如，`InputLayer`、`Conv2DLayer`、`MaxPool2DLayer`、`DenseLayer`，以及诸如修正线性单元（rectify）或softmax的丢弃层非线性和`GlorotUniform`的初始化方式。
- en: 'To connect the network graph composed of modules with the input symbolic `var`
    and get the output `var`, use the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接由模块组成的网络图，将输入符号`var`与输出`var`连接，使用以下代码：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or use this code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用这段代码：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A very convenient feature is that you can print the output shape of any module:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常方便的功能是，你可以打印任何模块的输出形状：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Lasagne''s `get_all_params` method lists the parameters of the model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Lasagne的`get_all_params`方法列出了模型的参数：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lastly, Lasagne comes with different learning rules, such as `RMSprop`, `Nesterov`
    `Momentum`, `Adam`, and `Adagrad`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Lasagne提供了不同的学习规则，如`RMSprop`、`Nesterov` `Momentum`、`Adam`和`Adagrad`：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All other things remain unchanged.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其他所有内容保持不变。
- en: 'To test our MNIST model, download the MNIST dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的 MNIST 模型，下载 MNIST 数据集：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Train an MNIST classifier for digit classification:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个 MNIST 分类器来进行数字分类：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model parameters are saved in `model.npz`. The accuracy is again above 99%.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数保存在 `model.npz` 中。准确率再次超过 99%。
- en: A localization network
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个定位网络
- en: 'In **Spatial Transformer Networks** (**STN**), instead of applying the network
    directly to the input image signal, the idea is to add a module to preprocess
    the image and crop it, rotate it, and scale it to fit the object, to assist in
    classification:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **空间变换网络** (**STN**) 中，想法不是直接将网络应用于输入图像信号，而是添加一个模块来预处理图像，对其进行裁剪、旋转和缩放以适应物体，从而辅助分类：
- en: '![A localization network](img/00089.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![一个定位网络](img/00089.jpeg)'
- en: Spatial Transformer Networks
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 空间变换网络
- en: 'For that purpose, STNs use a localization network to predict the affine transformation
    parameters and process the input:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，STNs 使用一个定位网络来预测仿射变换参数并处理输入：
- en: '![A localization network](img/00090.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![一个定位网络](img/00090.jpeg)'
- en: Spatial transformer networks
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 空间变换网络
- en: In Theano, differentiation through the affine transformation is automatic, we
    simply have to connect the localization net with the input of the classification
    net through the affine transformation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Theano 中，仿射变换的微分是自动完成的，我们只需通过仿射变换将定位网络与分类网络的输入连接起来。
- en: 'First, we create a localization network not very far from the MNIST CNN model,
    to predict six parameters of the affine transformation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个与 MNIST CNN 模型相差不远的定位网络，用于预测仿射变换的六个参数：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we simply add to the input array, with `DimshuffleLayer`, a channel dimension
    that will have only value 1\. Such a dimension add is named a broadcast.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只需通过 `DimshuffleLayer` 向输入数组添加一个通道维度，该维度的值仅为 1。这样的维度添加被称为广播。
- en: The pooling layer resizes the input image to *50x50*, which is enough to determine
    the position of the digit.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层将输入图像大小调整为 *50x50*，这足以确定数字的位置。
- en: The localization layer weight is initiated with zeros, except for the bias,
    which is initiated to the Identity affine parameters; the STN modules will not
    have any impact at the beginning and the full input image will be transmitted.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 定位层的权重初始化为零，偏置则初始化为单位仿射参数；STN 模块在开始时不会产生任何影响，整个输入图像将被传输。
- en: 'To crop given the affine parameters:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据仿射参数进行裁剪：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `down_sampling_factor` enables us to define the size of the output image
    with respect to the input. In this case, it is three, meaning the image will be
    *33x33*—not very far from our MNIST digit size of *28x28*. Lastly, we simply add
    our MNIST CNN model to classify the output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`down_sampling_factor` 使我们能够根据输入定义输出图像的大小。在这种情况下，它的值是三，意味着图像将是 *33x33*——与我们的
    MNIST 数字大小 *28x28* 相差不远。最后，我们简单地将 MNIST CNN 模型添加到分类输出中：'
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To test the classifier, let us create images of *100x100* pixels, with some
    distortions and one digit:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试分类器，让我们创建一些 *100x100* 像素的图像，带有一些变形和一个数字：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Plot the first three images (corresponding to 1, 0, 5):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制前三个图像（对应 1、0、5）：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![A localization network](img/00091.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![一个定位网络](img/00091.jpeg)'
- en: 'Let''s run the command to train the model:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 运行命令以训练模型：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here again, the accuracy gets above 99% when the digit is alone without distortions,
    which is typically not possible with the simple MNIST CNN model alone, and above
    96.9% with distortions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，当数字没有变形时，准确率超过 99%，这通常是仅用简单的 MNIST CNN 模型无法实现的，并且在有变形的情况下，准确率超过 96.9%。
- en: 'The command to plot the crops is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制裁剪图像的命令是：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It gives us the following result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它给我们带来了以下结果：
- en: '![A localization network](img/00092.jpeg)![A localization network](img/00093.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![一个定位网络](img/00092.jpeg)![一个定位网络](img/00093.jpeg)'
- en: 'And with distortions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 带有变形的情况：
- en: '![A localization network](img/00094.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![一个定位网络](img/00094.jpeg)'
- en: STN can be thought of as a module to include in any network, at any place between
    two layers. To improve the classification results further, adding multiple STNs
    between different layers of a classification network helps get better results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: STN 可以被看作是一个模块，可以包含在任何网络中，位于两个层之间的任何地方。为了进一步提高分类结果，在分类网络的不同层之间添加多个 STN 有助于获得更好的结果。
- en: 'Here is an example of a network with two branches inside the network, each
    with its SPN that will, when unsupervised, try to catch different parts of the
    image to classify it:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含两个分支的网络示例，每个分支都有自己的 SPN，它们在无监督的情况下将尝试捕捉图像的不同部分进行分类：
- en: '![A localization network](img/00095.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![一个定位网络](img/00095.jpeg)'
- en: (Spatial transformer networks paper, Jaderberg et al., 2015)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: （空间变换网络论文，Jaderberg等，2015年）
- en: Recurrent neural net applied to images
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用于图像的递归神经网络
- en: 'The idea is to use recurrency to read multiple digits, instead of just one:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是使用递归来读取多个数字，而不仅仅是一个：
- en: '![Recurrent neural net applied to images](img/00096.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![应用于图像的递归神经网络](img/00096.jpeg)'
- en: 'In order to read multiple digits, we simply replace the localization feedforward
    network with a recurrent network that will output multiple affine transformations
    corresponding to each digit:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取多个数字，我们只需将定位前馈网络替换为递归网络，它将输出多个仿射变换，分别对应于每个数字：
- en: '![Recurrent neural net applied to images](img/00097.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![应用于图像的递归神经网络](img/00097.jpeg)'
- en: 'From the previous example, we replace the fully connected layer with the GRU
    layer:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中，我们将全连接层替换为GRU层：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This outputs a tensor of dimension (None, 3, 256), where the first dimension
    is the batch size, 3 is the number of steps in the GRU, and 256 is the hidden
    layer size. On top of this layer, we simply add the same fully connected layer
    as before to output three identity images at the beginning:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出一个维度为(None, 3, 256)的张量，其中第一维是批量大小，3是GRU中的步骤数，256是隐藏层的大小。在这个层的上面，我们仅仅添加一个和之前一样的全连接层，输出三个初始的身份图像：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To test the classifier, let us create images of *100x100* pixels with some
    distortions, and three digits this time:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试分类器，我们创建一些具有*100x100*像素的图像，并加入一些扭曲，这次包含三个数字：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Plot the first three images (corresponding to sequences **296**, **490**, **125**):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制前三个图像（对应序列**296**、**490**、**125**）：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Recurrent neural net applied to images](img/00098.jpeg)![Recurrent neural
    net applied to images](img/00099.jpeg)![Recurrent neural net applied to images](img/00100.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![应用于图像的递归神经网络](img/00098.jpeg)![应用于图像的递归神经网络](img/00099.jpeg)![应用于图像的递归神经网络](img/00100.jpeg)'
- en: 'Let''s run the command to train our recurrent model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行命令来训练我们的递归模型：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The classification accuracy is 99.3%.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 分类准确率为99.3%。
- en: 'Plot the crops:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制裁剪图：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Recurrent neural net applied to images](img/00101.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![应用于图像的递归神经网络](img/00101.jpeg)'
- en: Unsupervised learning with co-localization
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有共同定位的无监督学习
- en: 'The first layers of the digit classifier trained in [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* as an encoding function to represent
    the image in an embedding space, as for words:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b "第2章.
    使用前馈网络分类手写数字")中训练的数字分类器的前几层，*使用前馈网络分类手写数字*作为编码函数，将图像表示为嵌入空间中的向量，就像对待单词一样：
- en: '![Unsupervised learning with co-localization](img/00102.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![带有共同定位的无监督学习](img/00102.jpeg)'
- en: 'It is possible to train unsurprisingly the localization network of the spatial
    transformer network by minimizing the hinge loss objective function on random
    sets of two images supposed to contain the same digit:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化随机集的合页损失目标函数，有可能训练空间变换网络的定位网络，这些图像被认为包含相同的数字：
- en: '![Unsupervised learning with co-localization](img/00103.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![带有共同定位的无监督学习](img/00103.jpeg)'
- en: Minimizing this sum leads to modifying the weights in the localization network,
    so that two localized digits become closer than two random crops.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化这个和意味着修改定位网络中的权重，使得两个定位的数字比两个随机裁剪的数字更接近。
- en: 'Here are the results:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Unsupervised learning with co-localization](img/00104.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![带有共同定位的无监督学习](img/00104.jpeg)'
- en: (Spatial transformer networks paper, Jaderberg et al., 2015)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: （空间变换网络论文，Jaderberg等，2015年）
- en: Region-based localization networks
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于区域的定位网络
- en: Historically, the basic approach in object localization was to use a classification
    network in a sliding window; it consists of sliding a window one pixel by one
    pixel in each direction and applying a classifier at each position and each scale
    in the image. The classifier learns to say if the object is present and centered.
    It requires a large amount of computations since the model has to be evaluated
    at every position and scale.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，目标定位的基本方法是使用分类网络在滑动窗口中；它的过程是将一个窗口在每个方向上逐像素滑动，并在每个位置和每个尺度上应用分类器。分类器学习判断目标是否存在且居中。这需要大量的计算，因为模型必须在每个位置和尺度上进行评估。
- en: 'To accelerate such a process, the **Region Proposal Network** (**RPN**) in
    the Fast-R-CNN paper from the researcher Ross Girshick consists of transforming
    the fully connected layers of a neural net classifier such as MNIST CNN into convolutional
    layers as well; in fact, network dense on 28x28 image, there is no difference
    between a convolution and a linear layer when the convolution kernel has the same
    dimensions as the input. So, any fully connected layers can be rewritten as convolutional
    layers, with the same weights and the appropriate kernel dimensions, which enables
    the network to work on a wider image than 28x28, at any size, outputting a feature
    map with a classification score at each position. The only difference may come
    from the stride of the whole network, which can be set different to 1 and can
    be large (a few 10 pixels) with convolution kernels set to stride different to
    1 in order to reduce the number of evaluation positions and thus the computations.
    Such a transformation is worth because the convolutions are very efficient:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速这一过程，Fast-R-CNN论文中的**区域提议网络**（**RPN**）由研究员Ross Girshick提出，目的是将神经网络分类器的全连接层（如MNIST
    CNN）转换为卷积层；事实上，在28x28的图像上，卷积层和线性层之间没有区别，只要卷积核的尺寸与输入相同。因此，任何全连接层都可以重写为卷积层，使用相同的权重和适当的卷积核尺寸，这使得网络能够在比28x28更大的图像上工作，输出在每个位置的特征图和分类得分。唯一的区别可能来自于整个网络的步幅，步幅可以设置为不同于1，并且可以很大（例如几个10像素），通过将卷积核的步幅设置为不同于1，以减少评估位置的数量，从而减少计算量。这样的转换是值得的，因为卷积非常高效：
- en: '![Region-based localization networks](img/00105.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![基于区域的定位网络](img/00105.jpeg)'
- en: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN：使用区域提议网络实现实时物体检测
- en: 'An end-to-end network has been designed, taking ideas from deconvolution principles
    where an output feature map gives all bounding boxes at once: **You Only Look
    Once** (**YOLO**) architecture predicts B possible bounding boxes for each position
    in the feature map. Each bounding box is defined by its coordinates (x, y, w,
    h) in proportion to the whole image as a regression problem, and a confidence
    (probability) that corresponds to the **Intersection over Union** (**IOU**) between
    the box and the true box. Comparable approaches are proposed with SSD models.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 已经设计了一种端到端网络，借鉴了解卷积原理，其中输出特征图一次性给出所有的边界框：**你只看一次**（**YOLO**）架构预测每个位置可能的B个边界框。每个边界框由其坐标（x,
    y, w, h）按比例表示为回归问题，并具有一个与**交并比**（**IOU**）相对应的置信度（概率），该交并比表示该框与真实框之间的重叠程度。类似的方式也提出了SSD模型。
- en: Lastly, segmentation networks introduced in [Chapter 8,](part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 8. Translating and Explaining with Encoding – decoding Networks") *Translating
    and Explaining with Encoding – decoding Networks* can also be considered as neural
    net implementations towards localizing objects.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[第8章](part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b "第8章.
    使用编码-解码网络进行翻译和解释")中介绍的分割网络，*使用编码-解码网络进行翻译和解释*，也可以看作是神经网络实现的目标定位方法。
- en: Further reading
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can further refer to these sources for more information:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进一步参考以下来源以获取更多信息：
- en: Spatial Transformer Networks, Max Jaderberg, Karen Simonyan, Andrew Zisserman,
    Koray Kavukcuoglu, Jun 2015
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空间变换网络，Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, 2015年6月
- en: Recurrent Spatial Transformer Networks, Søren Kaae Sønderby, Casper Kaae Sønderby,
    Lars Maaløe, Ole Winther, Sept 2015
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环空间变换网络，Søren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe, Ole Winther,
    2015年9月
- en: 'Original code: [https://github.com/skaae/recurrent-spatial-transformer-code](https://github.com/skaae/recurrent-spatial-transformer-code)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始代码：[https://github.com/skaae/recurrent-spatial-transformer-code](https://github.com/skaae/recurrent-spatial-transformer-code)
- en: Google Street View Character Recognition, Jiyue Wang, Peng Hui How
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌街景字符识别，Jiyue Wang, Peng Hui How
- en: Reading Text in the Wild with Convolutional Neural Networks, Max Jaderberg,
    Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, 2014
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络在野外读取文本，Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman,
    2014年
- en: Multi-digit Number Recognition from Street View Imagery using Deep Convolutional
    Neural Networks, Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud,
    Vinay Shet, 2013
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度卷积神经网络从街景图像中进行多位数字识别，Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz,
    Sacha Arnoud, Vinay Shet, 2013
- en: Recognizing Characters From Google Street View Images, Guan Wang, Jingrui Zhang
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从谷歌街景图像中识别字符，Guan Wang, Jingrui Zhang
- en: Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition,
    Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, 2014
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《用于自然场景文本识别的合成数据与人工神经网络》，Max Jaderberg，Karen Simonyan，Andrea Vedaldi，Andrew
    Zisserman，2014年
- en: R-CNN minus R, Karel Lenc, Andrea Vedaldi, 2015
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去掉R的R-CNN，Karel Lenc，Andrea Vedaldi，2015年
- en: Fast R-CNN, Ross Girshick, 2015
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fast R-CNN，Ross Girshick，2015年
- en: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,
    Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, 2015'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faster R-CNN：基于区域提议网络的实时物体检测，Shaoqing Ren，Kaiming He，Ross Girshick，Jian Sun，2015年
- en: 'You Only Look Once: Unified, Real-Time Object Detection, Joseph Redmon, Santosh
    Divvala, Ross Girshick, Ali Farhadi, Jun 2015'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你只需看一次：统一的实时物体检测，Joseph Redmon，Santosh Divvala，Ross Girshick，Ali Farhadi，2015年6月
- en: YOLO demo in real time [http://pjreddie.com/darknet/yolo/](http://pjreddie.com/darknet/yolo/)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO实时演示 [http://pjreddie.com/darknet/yolo/](http://pjreddie.com/darknet/yolo/)
- en: 'YOLO9000: Better, Faster, Stronger, Joseph Redmon, Ali Farhadi, Dec 2016'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO9000：更好、更快、更强，Joseph Redmon，Ali Farhadi，2016年12月
- en: 'SSD: Single Shot MultiBox Detector, Wei Liu, Dragomir Anguelov, Dumitru Erhan,
    Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, Dec 2015'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSD：单次多框检测器，Wei Liu，Dragomir Anguelov，Dumitru Erhan，Christian Szegedy，Scott
    Reed，Cheng-Yang Fu，Alexander C. Berg，2015年12月
- en: Rich feature hierarchies for accurate object detection and semantic segmentation,
    Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, 2013
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确的物体检测和语义分割的丰富特征层次，Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik，2013年
- en: 'Text Flow: A Unified Text Detection System in Natural Scene Images Shangxuan
    Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan, 2016'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本流：一种统一的自然场景图像文本检测系统，Shangxuan Tian，Yifeng Pan，Chang Huang，Shijian Lu，Kai Yu，Chew
    Lim Tan，2016年
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The spatial transformer layer is an original module to localize an area of the
    image, crop it and resize it to help the classifier focus on the relevant part
    in the image, and increase its accuracy. The layer is composed of differentiable
    affine transformation, for which the parameters are computed through another model,
    the localization network, and can be learned via backpropagation as usual.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 空间变换器层是一个原创模块，用于定位图像区域、裁剪并调整大小，帮助分类器集中注意图像中的相关部分，从而提高准确性。该层由可微分的仿射变换组成，参数通过另一个模型——定位网络进行计算，并且可以像往常一样通过反向传播进行学习。
- en: An example of the application to reading multiple digits in an image can be
    inferred with the use of recurrent neural units. To simplify our work, the Lasagne
    library was introduced.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用循环神经单元可以推断出图像中读取多个数字的应用示例。为了简化工作，引入了Lasagne库。
- en: Spatial transformers are one solution among many others for localizations; region-based
    localizations, such as YOLO, SSD, or Faster RCNN, provide state-of-the-art results
    for bounding box prediction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 空间变换器是众多定位方法中的一种；基于区域的定位方法，如YOLO、SSD或Faster RCNN，提供了最先进的边界框预测结果。
- en: In the next chapter, we'll continue with image recognition to discover how to
    classify full size images that contain a lot more information than digits, such
    as natural images of indoor scenes and outdoor landscapes. In the meantime, we'll
    continue with Lasagne's prebuilt layer and optimization modules.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续进行图像识别，探索如何对包含比数字更多信息的完整图像进行分类，例如室内场景和户外风景的自然图像。与此同时，我们将继续使用Lasagne的预构建层和优化模块。
