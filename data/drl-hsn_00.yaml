- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: This book is on reinforcement learning (RL), which is a subfield of machine
    learning (ML); it focuses on the general and challenging problem of learning optimal
    behavior in complex environments. The learning process is driven only by the reward
    value and observations obtained from the environment. This model is very general
    and can be applied to many practical situations, from playing games to optimizing
    complex manufacturing processes. We largely focus on deep RL in this book, which
    is RL that leverages deep learning (DL) methods.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讲述的是强化学习（RL），它是机器学习（ML）的一个子领域；本书专注于学习在复杂环境中学习最优行为这一普遍而具有挑战性的问题。学习过程仅由从环境中获得的奖励值和观察结果驱动。这一模型非常通用，可以应用于许多实际情况，从玩游戏到优化复杂的制造过程。在本书中，我们主要集中在深度强化学习上，深度强化学习是利用深度学习（DL）方法的强化学习。
- en: Due to its flexibility and generality, the field of RL is developing very quickly
    and attracting lots of attention, both from researchers who are trying to improve
    existing methods or create new methods and from practitioners interested in solving
    their problems in the most efficient way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其灵活性和通用性，强化学习领域发展迅速，吸引了大量关注，既来自那些试图改进现有方法或创造新方法的研究人员，也来自那些希望以最有效的方式解决实际问题的实践者。
- en: Why I wrote this book
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我写这本书
- en: There is a lot of ongoing research activity in the RL field all around the world.
    New research papers are being published almost every day, and a large number of
    DL conferences, such as Neural Information Processing Systems (NeurIPS) or the
    International Conference on Learning Representations (ICLR), are dedicated to
    RL methods. There are also several large research groups focusing on the application
    of RL methods to robotics, medicine, multi-agent systems, and others.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）领域在全球范围内有着大量的持续研究活动。几乎每天都有新的研究论文发表，许多深度学习（DL）会议，如神经信息处理系统会议（NeurIPS）或国际学习表示会议（ICLR），都专注于强化学习方法。此外，还有一些大型研究团队专注于将强化学习方法应用于机器人技术、医学、多智能体系统等领域。
- en: However, although information about the recent research is widely available,
    it is too specialized and abstract to be easily understandable. Even worse is
    the situation surrounding the practical aspect of RL, as it is not always obvious
    how to make the step from an abstract method described in its mathematics-heavy
    form in a research paper to a working implementation solving an actual problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管关于最新研究的信息已广泛可得，但它们过于专业化和抽象，难以轻松理解。更糟糕的是，强化学习的实际应用方面，往往并不明显如何将一篇研究论文中以数学为主的抽象方法转化为能够解决实际问题的有效实现。
- en: This makes it hard for somebody interested in the field to get a clear understanding
    of the methods and ideas behind papers and conference talks. There are some very
    good blog posts about various aspects of RL that are illustrated with working
    examples, but the limited format of a blog post allows authors to describe only
    one or two methods, without building a complete structured picture and showing
    how different methods are related to each other in a systematic way. This book
    was written as an attempt to fill this obvious gap in practical and structured
    information about RL methods and approaches.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得有兴趣的人很难清楚地理解论文和会议报告中方法和思想的背后。虽然有一些关于强化学习各个方面的很好的博客文章，并附带了实际的示例，但博客文章的有限格式使得作者只能描述一两种方法，无法建立一个完整的结构化图像，也不能系统地展示不同方法之间的关系。本书写作的目的就是填补这一强化学习方法和途径的实践性和结构性信息的明显空白。
- en: The approach
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: A key aspect of the book is its orientation to practice. Every method is implemented
    for various environments, from the very trivial to the quite complex. I’ve tried
    to make the examples clean and easy to understand, which was made possible by
    the expressiveness and power of PyTorch. On the other hand, the complexity and
    requirements of the examples are oriented to RL hobbyists without access to very
    large computational resources, such as clusters of graphics processing units (GPUs)
    or very powerful workstations. This, I believe, will make the fun-filled and exciting
    RL domain accessible to a much wider audience than just research groups or large
    artificial intelligence companies. On the other hand, this is still deep RL, so
    access to a GPU is highly recommended, as computation speed up will make experimentations
    much more convenient (waiting for several weeks for a single optimization to complete
    is not very fun). Approximately half of the examples in the book will benefit
    from being run on a GPU.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的一个关键方面是其实践导向。每种方法都适用于各种环境，从非常简单到相当复杂。我尝试使示例简洁易懂，这得益于 PyTorch 的表达力和强大功能。另一方面，示例的复杂性和要求是面向没有访问非常大计算资源（如图形处理单元（GPU）集群或非常强大的工作站）的强化学习（RL）爱好者的。我相信，这将使充满乐趣和激动人心的
    RL 领域可以为比研究小组或大型人工智能公司更广泛的受众所接触。另一方面，这仍然是深度强化学习，因此强烈建议使用 GPU，因为加速计算将使实验变得更加方便（等待数周才能完成一次优化并不有趣）。本书中大约一半的示例将在
    GPU 上运行时受益。
- en: In addition to traditional medium-sized examples of environments used in RL,
    such as Atari games or continuous control problems, this book contains several
    chapters (10, 13, 14, 19, 20, and 21) that contain larger projects, illustrating
    how RL methods can be applied to more complicated environments and tasks. These
    examples are still not full-sized, real-life projects (they would occupy a separate
    book on their own), but just larger problems illustrating how the RL paradigm
    can be applied to domains beyond the well-established benchmarks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的中型 RL 环境示例，如 Atari 游戏或连续控制问题外，本书还包含了若干章节（第 10、13、14、19、20 和 21 章），这些章节包含了更大的项目，展示了如何将
    RL 方法应用于更复杂的环境和任务。这些示例仍然不是完整的、现实生活中的项目（这些将占据一本独立的书），但只是一些更大的问题，说明了 RL 范式如何应用于超越公认基准的领域。
- en: Another thing to note about the examples in Parts 1, 2, and 3 of the book is
    that I’ve tried to make them self-contained, with the source code shown in full.
    Sometimes this has led to the repetition of code pieces (for example, the training
    loop is very similar in most of the methods), but I believe that giving you the
    freedom to jump directly into the method you want to learn is more important than
    avoiding a few repetitions. All examples in the book are available on GitHub at
    [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/),
    and you’re welcome to fork them, experiment, and contribute.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的事情是，本书第一、二、三部分的示例我尽力使其自包含，源代码全部展示。有时这导致代码片段的重复（例如，大多数方法中的训练循环非常相似），但我认为给予你直接跳入你想学习的方法的自由，比避免一些重复更加重要。本书中的所有示例都可以在
    GitHub 上找到，网址是 [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/)，欢迎你进行分支、实验和贡献。
- en: 'Besides the source code, several chapters (15, 16, 19, and 22) are accompanied
    by video recordings of the trained model. All these recordings are available in
    the following YouTube playlist: [https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7](https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了源代码外，几个章节（第 15、16、19 和 22 章）还附带了训练模型的视频录制。这些录制可以在以下 YouTube 播放列表中找到：[https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7](https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7)。
- en: Who this book is for
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书的目标读者
- en: This book is ideal for machine learning engineers, software engineers, and data
    scientists looking to learn and apply deep RL in practice. It assumes familiarity
    with Python, calculus, and ML concepts. With practical examples and high-level
    overviews, it’s also suitable for experienced professionals looking to deepen
    their understanding of advanced deep RL methods and apply them across industries,
    such as gaming and finance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书非常适合机器学习工程师、软件工程师和数据科学家，他们希望学习并实际应用深度强化学习。书中假设读者已经熟悉Python、微积分和机器学习概念。通过实际示例和高级概述，本书也适合有经验的专业人士，帮助他们加深对高级深度强化学习方法的理解，并在各行业中应用，如游戏和金融。
- en: What this book covers
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概览
- en: Chapter 1, What Is Reinforcement Learning?, contains an introduction to RL ideas
    and the main formal models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第1章，《什么是强化学习？》介绍了强化学习（RL）的基本概念和主要的正式模型。
- en: Chapter 2, OpenAI Gym API and Gymansium, introduces the practical aspects of
    RL, using the open source library Gym and its descendant, Gymnasium.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章，《OpenAI Gym API与Gymnasium》介绍了RL的实践方面，使用开源库Gym及其后代Gymnasium。
- en: Chapter 3, Deep Learning with PyTorch, gives you a quick overview of the PyTorch
    library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章，《使用PyTorch进行深度学习》为你提供了PyTorch库的快速概述。
- en: Chapter 4, The Cross-Entropy Method, introduces one of the simplest methods
    in RL to give you an impression of RL methods and problems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章，《交叉熵方法》介绍了RL中最简单的方法之一，让你对RL方法和问题有个基本了解。
- en: Chapter 5, Tabular Learning and the Bellman Equation, this chapter opens Part
    2 of the book, devoted to value-based family of methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章，《表格学习与贝尔曼方程》本章开启了本书的第二部分，专注于基于价值的方法。
- en: Chapter 6, Deep Q-Networks, describes deep Q-networks (DQNs), an extension of
    the basic value-based methods, allowing us to solve complicated environments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章，《深度Q网络》描述了深度Q网络（DQN），这是一种扩展基本价值方法的技术，使我们能够解决复杂的环境问题。
- en: Chapter 7, Higher-Level RL Libraries, describes the library PTAN, which we will
    use in the book to simplify the implementations of RL methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章，《更高级的RL库》描述了PTAN库，我们将在本书中使用该库来简化RL方法的实现。
- en: Chapter 8, DQN Extensions, gives a detailed overview of a modern extension to
    the DQN method, to improve its stability and convergence in complex environments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章，《DQN扩展》详细概述了DQN方法的现代扩展，以改善其在复杂环境中的稳定性和收敛性。
- en: Chapter 9, Ways to Speed up RL Methods, provides an overview of ways to make
    the execution of RL code faster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章，《加速RL方法的方式》概述了加速RL代码执行的几种方法。
- en: Chapter 10, Stocks Trading Using RL, is the first practical project and focuses
    on applying the DQN method to stock trading.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章，《使用RL进行股票交易》是第一个实际项目，重点应用DQN方法进行股票交易。
- en: Chapter 11, Policy Gradients, opens Part 3 of the book and introduces another
    family of RL methods that is based on direct policy optimisation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章，《策略梯度》开启了本书的第三部分，并介绍了另一类基于直接优化策略的RL方法。
- en: 'Chapter 12, The Actor-Critic Method: A2C and A3C, describes one of the most
    widely used policy-based method in RL.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第12章，《演员-评论员方法：A2C和A3C》描述了强化学习中最广泛使用的基于策略的方法之一。
- en: Chapter 13, The TextWorld Environment, covers the application of RL methods
    to interactive fiction games.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章，《TextWorld环境》介绍了将RL方法应用于互动小说游戏。
- en: Chapter 14, Web Navigation, is another long project that applies RL to web page
    navigation using the MiniWoB++ environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第14章，《网页导航》是另一个长篇项目，应用强化学习（RL）于网页导航，使用MiniWoB++环境。
- en: Chapter 15, Continuous Action Space, opens the advanced RL part of the book
    and describes the specifics of environments using continuous action spaces and
    various methods (widely used in robotics).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第15章，《连续动作空间》开启了本书的高级RL部分，描述了使用连续动作空间的环境的特点和各种方法（广泛应用于机器人技术）。
- en: 'Chapter 16, Trust Regions, is yet another chapter about continuous action spaces
    describing the trust region set of methods: PPO, TRPO, ACKTR and SAC.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第16章，《信任域》是另一章关于连续动作空间的内容，描述了信任域集方法：PPO、TRPO、ACKTR和SAC。
- en: Chapter 17, Black-Box Optimization in RL, shows another set of methods that
    don’t use gradients in their explicit form.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第17章，《RL中的黑箱优化》展示了另一类不显式使用梯度的优化方法。
- en: Chapter 18, Advanced Exploration, covers different approaches that can be used
    for better exploration of the environment — a very important aspect of RL.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第18章，《高级探索》介绍了更好探索环境的不同方法——这是RL中的一个非常重要的方面。
- en: Chapter 19, Reinforcement Learning with Human Feedback, introduces and implements
    recent approach to guide the process of learning by giving human feedback. This
    methed is widely used in training large language models (LLMs). In this chapter,
    we’ll implement RLHF pipeline from scratch and check its efficiency.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第19章，《带有人工反馈的强化学习（RLHF）》，介绍并实现了通过给予人类反馈来指导学习过程的最新方法。这种方法在训练大型语言模型（LLMs）中被广泛应用。在这一章中，我们将从零开始实现RLHF流程，并检查其效率。
- en: Chapter 20, AlphaGo Zero and MuZero, describes the AlphaGo Zero method and its
    evolution into MuZero, and applies both these methods to the game Connect 4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第20章，《AlphaGo Zero与MuZero》，描述了AlphaGo Zero方法及其演变为MuZero，并将这两种方法应用于游戏《四子连珠》。
- en: Chapter 21, RL in Discrete Optimization, describes the application of RL methods
    to the domain of discrete optimization, using the Rubik’s cube as an environment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第21章，《离散优化中的强化学习（RL）》，描述了将强化学习方法应用于离散优化领域，使用魔方作为环境。
- en: Chapter 22, Multi-Agent RL, introduces a relatively new direction of RL methods
    for situations with multiple agents.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第22章，《多智能体强化学习（Multi-Agent RL）》，介绍了一种相对较新的强化学习方法方向，适用于多个智能体的情境。
- en: To get the most out of this book
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了最大化本书的价值。
- en: This book is suitable for you if you’re using a machine with at least 32 GB
    of RAM. A GPU is not strictly required, but an Nvidia GPU is highly recommended.
    The code has been tested on Linux and macOS. For more details on the hardware
    and software requirements, refer to Chapter 2.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书适合你，如果你使用的是至少32 GB RAM的机器。虽然并不严格要求GPU，但强烈推荐使用Nvidia GPU。代码已经在Linux和macOS上进行了测试。有关硬件和软件要求的更多详细信息，请参阅第2章。
- en: 'All the chapters in this book that describe RL methods have the same structure:
    in the beginning, we discuss the motivation of the method, its theoretical foundation,
    and the idea behind it. Then, we follow several examples of the method applied
    to different environments with the full source code.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所有描述强化学习方法的章节都有相同的结构：一开始，我们讨论该方法的动机、理论基础以及背后的思想。然后，我们通过多个示例，展示该方法应用于不同环境的过程，并附上完整的源代码。
- en: 'You can use the book in different ways:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以以不同的方式使用本书：
- en: To quickly become familiar with a particular method, you can read only the introductory
    part of the relevant chapter
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了快速熟悉某一特定方法，你可以仅阅读相关章节的引言部分。
- en: To get a deeper understanding of the way the method is implemented, you can
    read the code and the explanations accompanying it
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更深入地理解方法的实现方式，你可以阅读代码及其附带的解释。
- en: To gain a deeper familiarity with the method (which I beleive is the best way
    to learn) you can try to reimplement the method and make it work, using the provided
    source code as a reference point
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更深入地熟悉该方法（我认为这是最好的学习方式），你可以尝试重新实现该方法并使其正常工作，使用提供的源代码作为参考。
- en: Whichever approach you choose, I hope the book will be useful for you!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种方式，我希望本书对你有帮助！
- en: Changes in the third edition
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三版的变化
- en: 'In comparison to the second edition of this book (published in 2020), there
    are several major changes made to the book’s contents in this new edition:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于本书的第二版（2020年出版），在新版本中对内容做了几个重大更改：
- en: All the dependencies of code examples have been updated to the recent versions
    or replaced with better alternatives. For example, OpenAI Gym is not supported
    anymore, but we have the Farama Foundation Gymnasium fork. Another example is
    the MiniWoB++ library, which has replaced the MiniWoB and Universe environment.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有代码示例的依赖项已更新为最新版本或替换为更好的替代品。例如，OpenAI Gym不再被支持，但我们有Farama Foundation的Gymnasium分支。另一个例子是MiniWoB++库，它替代了MiniWoB和Universe环境。
- en: A new chapter on RLHF has been included, and the MuZero method has been added
    to the chapter on AlphaGo Zero.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新增了一章关于RLHF（人类反馈强化学习），并且将MuZero方法加入了AlphaGo Zero章节。
- en: There are lots of small fixes and improvements — most of the figures have been
    redrawn to make them clearer and more easily understandable.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多小的修复和改进——大多数图示已经重新绘制，以使其更清晰、更易理解。
- en: To better meet book volume limitations, several chapters were rearranged, which
    I hope made the book more consistent and easier to read.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地满足书籍篇幅的限制，几个章节进行了重新安排，我希望这样能使本书更加一致且易于阅读。
- en: Download the example code files
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: The code bundle for the book is hosted on GitHub at [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包托管在GitHub上，网址为[https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)。我们还提供来自丰富书籍和视频目录的其他代码包，可在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)查看！
- en: Download the color images
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://packt.link/gbp/9781835882702](https://packt.link/gbp/9781835882702).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个PDF文件，其中包含本书中使用的屏幕截图/图表的彩色图像。您可以在此处下载：[https://packt.link/gbp/9781835882702](https://packt.link/gbp/9781835882702)。
- en: Conventions used
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: 'There are a number of text conventions used throughout this book. CodeInText:
    Indicates code words in text, database table names, folder names, filenames, file
    extensions, pathnames, dummy URLs, user input, and Twitter handles. For example:
    ”For the reward table, it is represented as a tuple with [State, Action, State]
    and for the transition table, it is written as [State, Action].”'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本约定。CodeInText：指示文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter用户名。例如：“对于奖励表，它表示为一个元组，其中[State，Action，State]，而对于转换表，则写为[State，Action]。”
- en: 'A block of code is set as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出将写成以下形式：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Bold: Indicates a new term, an important word, or words that you see on the
    screen. For instance, words in menus or dialog boxes appear in the text like this.
    For example: ”The second term is called cross-entropy, which is a very common
    optimization objective in deep learning.” Citations are represented using a condensed
    author–year format within square brackets, similar to [Sut88] or [Kro+11]. You
    can find the details of the corresponding paper in the Bibliography section at
    the end of the book.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体：**表示新术语、重要词汇或屏幕上显示的词语。例如，菜单或对话框中的词汇会以此方式出现在文本中。例如：“第二个术语称为交叉熵，这是深度学习中非常常见的优化目标。”引用使用紧凑的作者-年份格式放在方括号内，类似于[Sut88]或[Kro+11]。您可以在书末的参考书目部分找到相应论文的详细信息。'
- en: Warnings or important notes appear like this.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要提示将以此方式显示。
- en: Tips and tricks appear like this.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧将以此方式显示。
- en: Get in touch
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常欢迎读者的反馈。
- en: 'General feedback: Email feedback@packtpub.com and mention the book’s title
    in the subject of your message. If you have questions about any aspect of this
    book, please email us at questions@packtpub.com.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一般反馈：电子邮件feedback@packtpub.com，并在邮件主题中提及书名。如果您对本书的任何方面有疑问，请通过questions@packtpub.com与我们联系。
- en: 'Errata: Although we have taken every care to ensure the accuracy of our content,
    mistakes do happen. If you have found a mistake in this book, we would be grateful
    if you reported this to us. Please visit [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    click Submit Errata, and fill in the form.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 勘误：尽管我们已经尽一切努力确保内容的准确性，但错误难免发生。如果您在本书中发现错误，请向我们报告。请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，点击“提交勘误”，填写表格。
- en: 'Piracy: If you come across any illegal copies of our works in any form on the
    internet, we would be grateful if you would provide us with the location address
    or website name. Please contact us at copyright@packtpub.com with a link to the
    material.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 盗版：如果您在互联网上发现我们作品的任何非法副本，请您提供位置地址或网站名称。请通过链接将信息发送至copyright@packtpub.com。
- en: 'If you are interested in becoming an author: If there is a topic that you have
    expertise in and you are interested in either writing or contributing to a book,
    please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣成为作者：如果您在某个专题上有专业知识，并且有意撰写或为一本书作贡献，请访问[http://authors.packtpub.com](http://authors.packtpub.com)。
- en: Leave a Review!
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an [Amazon review](https://packt.link/r/1835882714);
    it will only take a minute, but it makes a big difference for readers like you.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你购买Packt出版的这本书——希望你喜欢！你的反馈非常宝贵，能够帮助我们改进和成长。阅读完后，请花点时间留下一个[亚马逊评论](https://packt.link/r/1835882714)；这只需一分钟，但对像你这样的读者来说，意义重大。
- en: Scan the QR code below to receive a free ebook of your choice.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描下方二维码，选择一本免费的电子书。
- en: '![PIC](img/file3.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file3.png)'
- en: '*https://packt.link/NzOWQ*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*https://packt.link/NzOWQ*'
- en: Download a free PDF copy of this book
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载此书的免费PDF副本
- en: Thanks for purchasing this book!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你购买本书！
- en: Do you like to read on the go but are unable to carry your print books everywhere?
    Is your eBook purchase not compatible with the device of your choice?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢随时阅读，但又不能把纸质书籍带到处走吗？你购买的电子书是否与你选择的设备不兼容？
- en: Don’t worry; with every Packt book, you now get a DRM-free PDF version of that
    book at no cost.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心；每本Packt书籍，你现在都能免费获得该书的无DRM PDF版本。
- en: Read anywhere, on any device. Search, copy, and paste code from your favorite
    technical books directly into your application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何设备上随时阅读。直接从你最喜欢的技术书籍中搜索、复制并粘贴代码到你的应用程序中。
- en: The perks don’t stop there! You can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特权不止于此！你还可以获得独家折扣、新闻通讯以及每天发送到你邮箱的精彩免费内容。
- en: 'Follow these simple steps to get the benefits:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下简单步骤获得福利：
- en: 'Scan the QR code or visit the link below:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描二维码或访问以下链接：
- en: '![PIC](img/file4.png)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/file4.png)'
- en: '*https://packt.link/free-ebook/9781835882702*'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*https://packt.link/free-ebook/9781835882702*'
- en: Submit your proof of purchase.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交你的购买凭证。
- en: That’s it! We’ll send your free PDF and other benefits to your email address
    directly.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就是这样！我们会直接将你的免费PDF和其他福利发送到你的电子邮箱。
- en: Part 1
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分
- en: Introduction to RL
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
