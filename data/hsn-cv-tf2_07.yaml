- en: Object Detection Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测模型
- en: From self-driving cars to content moderation, detecting objects and their position
    in an image is a canonical task in computer vision. In this chapter, we will introduce
    techniques used for **object detection**. We will detail the architecture of two
    of the most prevalent models among the current state of the art—**You Only Look
    Once** (**YOLO**) and **Regions with Convolutional Neural Networks** (**R-CNN**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从自动驾驶汽车到内容审查，检测图像中的物体及其位置是计算机视觉中的经典任务。本章将介绍用于**目标检测**的技术。我们将详细说明当前最主流的两种模型架构——**You
    Only Look Once**（**YOLO**）和**Regions with Convolutional Neural Networks**（**R-CNN**）。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The history of object detection techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测技术的发展历史
- en: The main object detection approaches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要的目标检测方法
- en: Implementing fast object detection using YOLO architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YOLO架构实现快速目标检测
- en: Improving object detection using Faster R-CNN architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Faster R-CNN架构提高目标检测效果
- en: Using Faster R-CNN with the TensorFlow Object Detection API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Faster R-CNN与TensorFlow目标检测API
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is available in the form of notebooks at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码以笔记本的形式可以在[https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05)获取。
- en: Introducing object detection
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍目标检测
- en: Object detection was briefly introduced in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*. In this section, we will cover its history,
    as well as the core technical concepts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测在[第一章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)《计算机视觉与神经网络》中做了简要介绍。在本节中，我们将介绍其历史以及核心技术概念。
- en: Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Object detection, also called **object localization**, is the process of detecting
    objects and their **bounding boxes** in an image. A bounding box is the smallest
    rectangle of an image that fully contains an object.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测，也称为**目标定位**，是检测图像中的物体及其**边界框**的过程。边界框是能够完全包含物体的图像中最小的矩形。
- en: A common input for an object detection algorithm is an image. A common output
    is a list of bounding boxes and object classes. For each bounding box, the model
    outputs the corresponding predicted class and its confidence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测算法的常见输入是图像。常见的输出是边界框和物体类别的列表。对于每个边界框，模型会输出对应的预测类别及其置信度。
- en: Applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'The applications of object detection are numerous and cover many industries.
    For instance, object detection can be used for the following purposes:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测的应用广泛，涵盖了许多行业。例如，目标检测可用于以下目的：
- en: In self-driving cars, to locate other vehicles and pedestrians
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自动驾驶汽车中，用于定位其他车辆和行人
- en: For content moderation, to locate forbidden objects and their respective size
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于内容审查，定位禁止物体及其大小
- en: In health, to locate tumors or dangerous tissue using radiographs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在医疗领域，通过放射线影像定位肿瘤或危险组织
- en: In manufacturing, for assembly robots to put together or repair products
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在制造业中，用于装配机器人组装或修理产品
- en: In the security industry, to detect threats or count people
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在安全行业，用于检测威胁或统计人数
- en: In wildlife conservation, to monitor an animal population
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在野生动物保护中，用于监控动物种群
- en: These are just a few examples—more and more applications are being discovered
    every day as object localization becomes more powerful.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是其中的一些例子——随着目标定位技术越来越强大，越来越多的应用每天都在被发现。
- en: Brief history
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要历史
- en: Historically, object detection relied on a classical computer vision technique: **image
    descriptors**. To detect an object, for instance, a bike, you would start with
    several pictures of this object. Descriptors corresponding to the bike would be
    extracted from the image. Those descriptors would represent specific parts of
    the bike. When looking for this object, the algorithm would attempt to find the
    descriptors again in the target images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，目标检测依赖于经典的计算机视觉技术：**图像描述符**。为了检测一个物体，例如一辆自行车，你需要从多张该物体的照片中开始。然后从图像中提取与自行车相关的描述符。这些描述符表示自行车的特定部分。在寻找该物体时，算法会尝试在目标图像中再次找到这些描述符。
- en: To locate the bike in the image, the most commonly used technique was the **floating
    window**. Small rectangular areas of the images are examined, one after the other.
    The part with the most matching descriptors would be considered to be the one
    containing the object. Over time, many variations were used.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中定位自行车时，最常用的技术是**浮动窗口**。图像的多个小矩形区域依次进行检查，匹配描述符最多的部分将被认为包含物体。随着时间的推移，使用了许多变种。
- en: 'This technique presented a few advantages: it was robust to rotation and color
    changes, it did not require a lot of training data, and it worked with most objects.
    However, the level of accuracy was not satisfactory.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术具有一些优点：它对旋转和颜色变化具有鲁棒性，不需要大量的训练数据，并且适用于大多数物体。然而，准确度水平并不令人满意。
- en: While neural networks were already in use in the early 1990s (for detecting
    faces, hands, or text in images), they started outperforming the descriptor technique on
    the ImageNet challenge by a very large margin in the early 2010s.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然神经网络在1990年代初期就已经被用来检测图像中的面部、手部或文本，但它们在2010年代初期开始在ImageNet挑战赛中大幅超越描述符技术。
- en: 'Since then, performance has been improving steadily. Performance refers to
    how good the algorithm is at the following things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，性能稳步提高。性能指的是算法在以下方面的表现：
- en: '**Bounding box precision**: Providing the correct bounding box (not too large
    or too narrow)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界框精度**：提供正确的边界框（既不太大也不太小）'
- en: '**Recall**: Finding all the objects (not missing any objects)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：找到所有物体（没有遗漏任何物体）'
- en: '**Class precision**: Outputting the correct class for each object (not mistaking
    a cat for a dog)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别精度**：为每个物体输出正确的类别（避免将猫误认为狗）'
- en: Performance improvement also means that the models are getting faster and faster
    at computing results (for a specific input image size and at a specific computing
    power). While early models took considerable time (more than a few seconds) to
    detect objects, they can now be used in real time. In the context of computer
    vision, real time usually means more than five detections per second.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 性能改进还意味着模型在计算结果时变得越来越快（针对特定的输入图像尺寸和计算能力）。虽然早期的模型需要相当长的时间（超过几秒钟）才能检测到物体，但现在它们可以实时使用。在计算机视觉的背景下，实时通常意味着每秒检测超过五次。
- en: Evaluating the performance of a model
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: To compare different object detection models, we need common evaluation metrics.
    For a given test set, we run each model and gather its predictions. We use the
    predictions and the ground truth to compute an evaluation metric. In this section,
    we will have a look at the metrics used to evaluate object detection models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较不同的物体检测模型，我们需要统一的评估指标。对于给定的测试集，我们运行每个模型并收集其预测结果。我们使用预测结果和真实值计算评估指标。在本节中，我们将看看用于评估物体检测模型的指标。
- en: Precision and recall
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: While they are usually not used to evaluate object detection models, **precision**
    and **recall** serve as a basis to compute other metrics. A good understanding
    of precision and recall is, therefore, essential.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们通常不用于评估物体检测模型，**精确度**和**召回率**是计算其他指标的基础。因此，理解精确度和召回率非常重要。
- en: 'To measure precision and recall, we first need to compute the following for
    each image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要衡量精确度和召回率，我们首先需要为每张图像计算以下内容：
- en: '**The number of** **true positives**: **True positives** (**TP**) determine
    how many predictions match with a ground truth box of the same class.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**的数量：**真阳性**（**TP**）决定了有多少个预测与同类的真实框匹配。'
- en: '**The number of false positives**: **False positives** (**FP**) determine how
    many predictions do not match with a ground truth box of the same class.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**的数量：**假阳性**（**FP**）决定了有多少个预测与同类的真实框不匹配。'
- en: '**The number of** **false negatives**: **False negatives** (**FN**) determine
    how many ground truths do not have a matching prediction.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**的数量：**假阳性**（**FN**）决定了有多少个真实值没有匹配的预测。'
- en: 'Then, precision and recall are defined as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，精确度和召回率的定义如下：
- en: '![](img/5464f034-f644-497d-a77f-ab28ca5437ae.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5464f034-f644-497d-a77f-ab28ca5437ae.png)'
- en: Notice that if the predictions exactly match all the ground truths, there will
    not be any false positives or false negatives. Therefore, precision and recall
    will be equal to 1, a perfect score. If a model too often predicts the presence
    of an object based on non-robust features, precision will deteriorate because
    there will be many false positives. On the contrary, if a model is too strict
    and considers an object detected only when precise conditions are met, recall
    will suffer because there will be many false negatives.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果预测完全匹配所有的实际标签，则不会出现假阳性或假阴性。因此，准确率和召回率都将等于1，这是一个完美的分数。如果模型基于不稳健的特征频繁预测物体的存在，准确率将下降，因为会有很多假阳性。相反，如果模型过于严格，只有在满足精确条件时才会认为物体被检测到，召回率将下降，因为会有很多假阴性。
- en: Precision-recall curve
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确召回曲线
- en: '**Precision-recall curve** is used in many machine learning problems. The general
    idea is to visualize the precision and the recall of the model at each **threshold
    of confidence**. With every bounding box, our model will output a confidence—a
    number between 0 and 1 characterizing how confident the model is that a prediction
    is correct.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确召回曲线**在许多机器学习问题中都有应用。其基本思想是可视化模型在每个**置信度阈值**下的准确率和召回率。对于每个边界框，我们的模型会输出一个置信度——这是一个介于0和1之间的数字，表示模型对预测正确性的信心。'
- en: Because we do not want to keep the less confident predictions, we usually remove
    those below a certain threshold, 𝑇. For instance, if 𝑇 = 0.4, we will not consider
    any prediction with a confidence below this number.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们不希望保留那些信心较低的预测，通常会移除低于某个阈值的预测，𝑇。例如，如果𝑇 = 0.4，我们将不考虑任何置信度低于此数值的预测。
- en: 'Moving the threshold has an impact on precision and on recall:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 移动阈值会对准确率和召回率产生影响：
- en: '**If T is close to 1**: Precision will be high, but the recall will be low.
    As we filter out many objects, we miss a lot of them—recall shrinks. As we only
    keep confident predictions, we do not have many false positives—precision rises.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果 T 接近 1**：准确率会很高，但召回率会很低。由于我们筛除了很多对象，错过了很多对象——召回率下降。由于我们只保留有信心的预测，因此没有太多假阳性——准确率上升。'
- en: '**If T is close to 0**: Precision will be low, but the recall will be high.
    As we keep most predictions, we will not have any false negatives—recall rises.
    As our model is less confident in its predictions, we will have many false positives—precision
    shrinks.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果 T 接近 0**：准确率会很低，但召回率会很高。由于我们保留了大部分预测，因此不会有假阴性——召回率上升。由于模型对其预测的信心较低，我们会有很多假阳性——准确率下降。'
- en: 'By computing the precision and the recall at each threshold value between 0
    and 1, we can obtain a precision-recall curve, as shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算在0到1之间每个阈值下的准确率和召回率，我们可以获得精确召回曲线，如下所示：
- en: '![](img/2a65d1ce-2626-4dfb-b798-106009cd4eaf.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a65d1ce-2626-4dfb-b798-106009cd4eaf.png)'
- en: Figure 5.1: Precision-Recall curve
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：精确召回曲线
- en: Choosing a threshold is a trade-off between accuracy and recall. If a model
    is detecting pedestrians, we will pick a high recall in order not to miss any
    passers-by, even if it means stopping the car for no valid reason from time to
    time. If a model is detecting investment opportunities, we will pick a high precision
    to avoid choosing the wrong opportunities, even if it means missing some.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个阈值是在准确率和召回率之间的权衡。如果模型检测行人，我们会选择一个较高的召回率，以确保不会错过任何路人，即使这意味着有时车辆会因没有有效理由而停下。如果模型检测投资机会，我们会选择较高的准确率，以避免选择错误的机会，即使这意味着错过一些机会。
- en: Average precision and mean average precision
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均准确率和均值平均准确率
- en: While the precision-recall curve can tell us a lot about the model, it is often
    more convenient to have a single number. **Average precision** (**AP**) corresponds
    to the area under the curve. Since it is always contained in a one-by-one rectangle,
    AP is always between 0 and 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然精确召回曲线能告诉我们很多关于模型的信息，但通常更方便的是拥有一个单一的数字。**平均准确率**（**AP**）对应于曲线下的面积。由于它始终包含在一个1×1的矩形内，因此AP的值始终介于0和1之间。
- en: Average precision gives information about the performance of a model for a single
    class. To get a global score, we use **mean Average Precision** (**mAP**). This
    corresponds to the mean of the average precision for each class. If the dataset
    has 10 classes, we will compute the average precision for each class and take
    the average of those numbers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 平均准确率提供了模型在单个类别上的性能信息。为了获得全局评分，我们使用**均值平均准确率**（**mAP**）。这对应于每个类别的平均准确率的平均值。如果数据集有10个类别，我们将计算每个类别的平均准确率，并取这些数值的平均值。
- en: Mean average precision is used in at least two object detection challenges—**PASCAL
    Visual Object Classes** (usually referred to as **Pascal VOC**), and **Common
    Objects in Context** (usually referred to as **COCO**). The latter is larger and
    contains more classes; therefore, the scores obtained are usually lower than for
    the former.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度（mAP）至少在两个目标检测挑战中使用——**PASCAL Visual Object Classes**（通常称为**Pascal VOC**）和**Common
    Objects in Context**（通常称为**COCO**）。后者规模更大，包含的类别更多；因此，通常得到的分数比前者低。
- en: Average precision threshold
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均精度阈值
- en: 'We mentioned earlier that true and false positives were defined by the number
    of predictions matching or not matching the ground truth boxes. However, how do
    you decide when a prediction and the ground truth are matching? A common metric
    is the **Jaccard index**, which measures how well two sets overlap (in our case,
    the sets of pixels represented by the boxes). Also known as **Intersection over
    Union** (**IoU**), it is defined as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，真阳性和假阳性是通过预测与真实框是否匹配来定义的。然而，如何决定预测和真实框是否匹配呢？一个常见的指标是**Jaccard指数**，它衡量两个集合重叠的程度（在我们的例子中，就是由框表示的像素集合）。它也被称为**交集与并集的比率**（**IoU**），定义如下：
- en: '![](img/7c274090-cac1-480c-b08a-93c2adb684e7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c274090-cac1-480c-b08a-93c2adb684e7.png)'
- en: '|𝐴| and |𝐵| are the **cardinality** of each set; that is, the number of elements
    they each contain. 𝐴 ⋂ 𝐵 is the intersection of the two sets, and therefore the
    numerator |𝐴 ⋂ 𝐵| represents the number of elements they have in common. Similarly,
    𝐴 ⋃ 𝐵 is the union of the sets (as seen in the following diagram), and therefore
    the denominator |𝐴 ⋃ 𝐵| represents the total number of elements the two sets cover
    together:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '|𝐴| 和 |𝐵| 是每个集合的**基数**；即它们各自包含的元素数量。𝐴 ⋂ 𝐵 是两个集合的交集，因此分子 |𝐴 ⋂ 𝐵| 代表它们共有的元素数量。类似地，𝐴
    ⋃ 𝐵 是集合的并集（如下面的图示所示），因此分母 |𝐴 ⋃ 𝐵| 代表两个集合总共覆盖的元素数量：'
- en: '![](img/d55b879f-d640-4d10-a5eb-5af732559f3a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d55b879f-d640-4d10-a5eb-5af732559f3a.png)'
- en: Figure 5.2: Intersection and union of boxes illustrated
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：框的交集与并集示意图
- en: Why compute such a fraction and not just use the intersection? While the intersection
    would provide a good indicator of how much two sets/boxes overlap, this value
    is absolute and not relative. Therefore, two big boxes would probably overlap
    by many more pixels than two small boxes. This is why this ratio is used—it will
    always be between 0 (if the two boxes do not overlap) and 1 (if two boxes overlap
    completely).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要计算这样的比率，而不是直接使用交集呢？虽然交集可以很好地指示两个集合/框的重叠程度，但这个值是绝对的，而非相对的。因此，两个大框可能会比两个小框重叠更多的像素。这就是为什么要使用这个比率——它的值总是介于0（如果两个框没有重叠）和1（如果两个框完全重叠）之间。
- en: When computing the average precision, we say that two boxes overlap if their
    IoU is above a certain threshold. The threshold usually chosen is *0.5*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平均精度时，我们说两个框重叠，当它们的IoU超过某个阈值时。通常选择的阈值是*0.5*。
- en: For the Pascal VOC challenge, 0.5 is also used—we say that we use mAP@0.5 (pronounced
    *mAP* *at 0.5*). For the COCO challenge, a slightly different metric is used—mAP@[0.5:0.95].
    This means that we compute mAP@0.5, mAP@0.55, ..., *mAP*@0.95, and take the average.
    Averaging over IoUs rewards models with better localization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Pascal VOC挑战，0.5也被使用——我们说使用的是mAP@0.5（读作*mAP* *at 0.5*）。对于COCO挑战，使用略有不同的指标——mAP@[0.5:0.95]。这意味着我们计算mAP@0.5，mAP@0.55，...，*mAP*@0.95，并取其平均值。对IoU进行平均会奖励定位更精确的模型。
- en: A fast object detection algorithm – YOLO
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个快速的目标检测算法——YOLO
- en: While the acronym may make you smile, YOLO is one of the fastest object detection
    algorithms available. The latest version, YOLOv3, can run at more than 170 **frames
    per second** (**FPS**) on a modern GPU for an image size of *256 *× *256*. In
    this section, we will introduce the theoretical concept behind its architecture.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个缩写可能会让你会心一笑，但YOLO是目前最快的目标检测算法之一。最新版本YOLOv3在现代GPU上，对于*256×256*的图像大小，可以以每秒超过170帧的速度运行（**FPS**）。在这一节中，我们将介绍其架构背后的理论概念。
- en: Introducing YOLO
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍YOLO
- en: 'First released in 2015, YOLO outperformed almost all other object detection
    architectures, both in terms of speed and accuracy. Since then, the architecture
    has been improved several times. In this chapter, we will draw our knowledge from
    the following three papers:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO首次发布于2015年，在速度和精度上超越了几乎所有其他目标检测架构。此后，该架构已被多次改进。在本章中，我们将借鉴以下三篇论文的内容：
- en: '*You Only Look Once: Unified, real-time object detection (2015)*, Joseph Redmon,
    Santosh Divvala, Ross Girshick, and Ali Farhadi'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*You Only Look Once: 统一的实时目标检测（2015）*，Joseph Redmon、Santosh Divvala、Ross Girshick
    和 Ali Farhadi'
- en: '*YOLO9000: Better, Faster, Stronger (2016)*, Joseph Redmon and Ali Farhadi'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLO9000：更好、更快、更强（2016）*，Joseph Redmon 和 Ali Farhadi'
- en: '*YOLOv3: An Incremental Improvement (2018)*, Joseph Redmon and Ali Farhadi'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv3：一个渐进的改进（2018）*，Joseph Redmon 和 Ali Farhadi'
- en: For the sake of clarity and simplicity, we will not describe all the small details
    that allow YOLO to reach its maximum performance. Instead, we will focus on the
    general architecture of the network. We'll provide an implementation of YOLO so
    that you can compare our architecture with code. It is available in the chapter's
    repository.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰简洁，我们不会描述所有细节，来阐述 YOLO 如何达到其最大性能。相反，我们将专注于网络的总体架构。我们将提供 YOLO 的实现，以便你可以将我们的架构与代码进行比较。它可以在本章的代码库中找到。
- en: This implementation has been designed to be easy to read and understand. We
    invite those readers who wish to acquire a deep understanding of the architecture
    to first read this chapter and then refer to the original papers and the implementation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现已被设计为易于阅读和理解。我们邀请那些希望深入理解架构的读者，先阅读本章内容，然后参考原始论文和实现。
- en: The main author of the YOLO paper maintains a deep learning framework called
    **Darknet** ([https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)).
    This hosts the official implementation of YOLO and can be used to reproduce the
    paper's results. It is coded in C++ and is not based on TensorFlow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 论文的主要作者维护了一个深度学习框架，叫做 **Darknet**（[https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)）。这个框架包含了
    YOLO 的官方实现，并且可以用于复现论文中的结果。它是用 C++ 编写的，并且没有基于 TensorFlow。
- en: Strengths and limitations of YOLO
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO 的优缺点
- en: YOLO is known for its speed. However, it has been recently outperformed in terms
    of accuracy by **Faster R-CNN** (covered later in this chapter). Moreover, due
    to the way it detects objects, YOLO struggles with smaller objects. For instance,
    it would have trouble detecting single birds from a flock. As with most deep learning
    models, it also struggles to properly detect objects that deviate too much from
    the training set (unusual aspect ratios or appearance). Nevertheless, the architecture
    is constantly evolving, and those issues are being worked on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 以其速度而闻名。然而，最近在准确性方面被 **Faster R-CNN**（将在本章后面介绍）所超越。此外，由于 YOLO 检测物体的方式，它在处理小物体时表现不佳。例如，它可能很难从一群鸟中检测出单只鸟。与大多数深度学习模型一样，它也很难正确检测出与训练集偏差较大的物体（例如不寻常的长宽比或外观）。尽管如此，架构在不断演进，相关问题正在得到解决。
- en: YOLO's main concepts
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO的主要概念
- en: 'The core idea of YOLO is this: **reframing object detection as a single regression
    problem**. What does this mean? Instead of using a sliding window or another complex
    technique, we will divide the input into a *w × h* grid, as represented in this
    diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 的核心思想是：**将目标检测重新框定为一个单一的回归问题**。这是什么意思？与其使用滑动窗口或其他复杂技术，我们将把输入划分为一个 *w ×
    h* 的网格，如下图所示：
- en: '![](img/266bcb07-9913-4888-a3d2-8a2bfcab7b6c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/266bcb07-9913-4888-a3d2-8a2bfcab7b6c.png)'
- en: 'Figure 5.3: An example involving a plane taking off. Here, w = 5, h = 5, and
    B = 2, meaning, in total, 5 × 5 × 2 = 50 potential boxes, but only 2 are shown
    in the image'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：一个涉及飞机起飞的示例。这里，w = 5，h = 5，B = 2，这意味着总共有 5 × 5 × 2 = 50 个潜在的框，但图像中仅显示了
    2 个框
- en: 'For each part of the grid, we will define *B* bounding boxes. Then, our only
    task will be to predict the following for each bounding box:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网格的每一部分，我们将定义*B*个边界框。然后，我们的唯一任务就是预测每个边界框的以下内容：
- en: The center of the box
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盒子的中心
- en: The width and height of the box
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盒子的宽度和高度
- en: The probability that this box contains an object
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个框包含了一个物体的概率
- en: The class of said object
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该物体的类别
- en: Since all those predictions are numbers, we have therefore transformed the object
    detection problem into a regression problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些预测都是数值，我们因此将目标检测问题转化为回归问题。
- en: It is important to make a distinction between the grid cells that divide the
    pictures into equal parts (*w* × *h* parts to be precise) and the bounding boxes
    that will locate the objects. Each grid cell contains *B* bounding boxes. Therefore,
    there will be *w* × *h* × *B* possible bounding boxes in the end.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 很重要的一点是要区分网格单元（将图像分成等份的部分，准确来说是 *w* × *h* 部分）与定位物体的边界框。每个网格单元包含 *B* 个边界框。因此，最终会有
    *w* × *h* × *B* 个可能的边界框。
- en: In practice, the concepts used by YOLO are a bit more complex than this. What
    if there are several objects in one part of the grid? What if an object overlaps
    several parts of the grid? More importantly, how do we choose a loss to train
    our model? We will now have a deeper look at YOLO architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，YOLO使用的概念比这个稍微复杂。假设网格的一部分有多个物体怎么办？如果一个物体跨越多个网格部分怎么办？更重要的是，如何选择一个损失函数来训练我们的模型？接下来，我们将深入了解YOLO架构。
- en: Inferring with YOLO
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YOLO进行推理
- en: Because the architecture of the model can be quite hard to understand in one
    go, we will split the model into two parts—inference and training. **Inference**
    is the process of taking an image input and computing results. **Training** is
    the process of learning the weights of the model. When implementing a model from
    scratch, inference cannot be used before the model is trained. But, for the sake
    of simplicity, we are going to start with inference.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的架构可能很难一次性理解，我们将把模型分为两个部分——推理和训练。**推理**是将图像输入并计算结果的过程。**训练**是学习模型权重的过程。在从头实现模型时，推理在模型训练之前无法使用。但为了简化，我们将从推理开始。
- en: The YOLO backbone
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO主干模型
- en: 'Like most image detection models, YOLO is based on a **backbone model**. The
    role of this model is to extract meaningful features from the image that will
    be used by the final layers. This is why the backbone is also called the **feature
    extractor**, a concept introduced in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*. The general YOLO architecture is depicted
    here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数图像检测模型一样，YOLO基于**主干模型**。该模型的作用是从图像中提取有意义的特征，供最终的层使用。这也是为什么主干模型被称为**特征提取器**，这一概念在[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)《有影响力的分类工具》中介绍。YOLO的总体架构如下图所示：
- en: '![](img/18e34cb2-6234-4212-8ca4-d06843d049df.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18e34cb2-6234-4212-8ca4-d06843d049df.png)'
- en: Figure 5.4: YOLO architecture summarized. Note that the backbone is exchangeable
    and that its architecture may vary
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：YOLO架构总结。请注意，主干模型是可交换的，其架构可能有所不同。
- en: While any architecture can be chosen as a feature extractor, the YOLO paper
    employs a custom architecture. The performance of the final model depends heavily
    on the choice of the feature extractor's architecture.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以选择任何架构作为特征提取器，但YOLO论文使用了一个自定义架构。最终模型的性能在很大程度上取决于特征提取器架构的选择。
- en: The final layer of the backbone outputs a feature volume of size *w* × *h* ×
    *D*, where *w* × *h* is the size of the grid and *D* is the depth of the feature
    volume. For instance, for VGG-16, *D = 512*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 主干模型的最终层输出一个大小为*w* × *h* × *D*的特征体积，其中*w* × *h*是网格的大小，*D*是特征体积的深度。例如，对于VGG-16，*D
    = 512*。
- en: 'The size of the grid, *w* × *h*, depends on two factors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 网格的大小，*w* × *h*，取决于两个因素：
- en: '**The stride of the complete feature extractor**: For VGG-16, the stride is
    16, meaning that the feature volume output will be 16 times smaller than the input
    image.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整特征提取器的步幅**：对于VGG-16，步幅为16，意味着特征体积输出将是输入图像的1/16大小。'
- en: '**The size of the input image:** Since the feature volume''s size is proportional
    to the size of the image, the smaller the input, the smaller the grid.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入图像的大小**：由于特征体积的大小与图像大小成正比，因此输入图像越小，网格也越小。'
- en: YOLO's final layer accepts the feature volume as an input. It is composed of
    convolutional filters of size *1* × *1*. As seen in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*, a convolutional layer of size *1* × *1* can
    be used to change the depth of the feature volume without affecting its spatial
    structure.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的最终层接受特征体积作为输入。它由大小为*1* × *1*的卷积滤波器组成。如在[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)《有影响力的分类工具》中所示，*1*
    × *1*的卷积层可以用于改变特征体积的深度，而不影响其空间结构。
- en: YOLO's layers output
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO的层输出
- en: 'YOLO''s final output is a *w* × *h* × *M* matrix, where *w* × *h* is the size
    of the grid, and *M* corresponds to the formula *B* × *(C + 5)*, where the following
    applies:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的最终输出是一个*w* × *h* × *M*矩阵，其中*w* × *h*是网格的大小，*M*对应于公式*B* × *(C + 5)*，其中适用以下内容：
- en: '*B* is the number of bounding boxes per grid cell.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*是每个网格单元的边界框数量。'
- en: '*C* is the number of classes (in our example, we will use 20 classes).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*是类别的数量（在我们的例子中，我们将使用20个类别）。'
- en: 'Notice that we add *5* to the number of classes. This is because, for each
    bounding box, we need to predict *(C + 5)* numbers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在类别数上加了*5*。这是因为对于每个边界框，我们需要预测*(C + 5)*个数字：
- en: '*t[x]* and *t[y]* will be used to compute the coordinates of the center of
    the bounding box.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[x]*和*t[y]*将用来计算边界框中心的坐标。'
- en: '*t[w]* and *t[h]* will be used to compute the width and height of the bounding
    box.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[w]*和*t[h]*将用来计算边界框的宽度和高度。'
- en: '*c* is the confidence that an object is in the bounding box.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c*是物体位于边界框中的置信度。'
- en: '*p1*, *p2*, ..., and *pC* are the probability that the bounding box contains
    an object of class *1*, *2*, ..., *C* (where *C = 20* in our example).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p1*、*p2*、...和*pC*是边界框包含物体属于类别*1*、*2*、...、*C*的概率（在我们的示例中，*C = 20*）。'
- en: 'This diagram summarizes how the output matrix appears:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该图总结了输出矩阵的显示方式：
- en: '![](img/6eb909f6-a8e5-40ef-b202-9763d8637aec.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6eb909f6-a8e5-40ef-b202-9763d8637aec.png)'
- en: 'Figure 5.5: Final matrix output of YOLO. In this example, *B = 5*, *C = 20*,
    *w = 13,* and *h = 13*. The size is 13 × 13 × 125'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：YOLO的最终矩阵输出。在这个示例中，*B = 5*，*C = 20*，*w = 13*，*h = 13*。大小为13 × 13 × 125
- en: Before we explain how to use this matrix to compute the final bounding boxes,
    we need to introduce an important concept—**anchor boxes**.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解释如何使用该矩阵计算最终的边界框之前，我们需要介绍一个重要概念——**锚框**。
- en: Introducing anchor boxes
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入锚框
- en: We mentioned that *t[x]*, *t[y]*, *t[w]*, and *t[h]* are used to compute the
    bounding box coordinates. Why not ask the network to output the coordinates directly
    (*x*, *y*, *w*, and *h*)? In fact, that is how it was done in YOLO v1\. Unfortunately,
    this resulted in a lot of errors because objects vary in size.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，*t[x]*、*t[y]*、*t[w]*和*t[h]*用来计算边界框的坐标。为什么不直接让网络输出坐标（*x*、*y*、*w*、*h*）呢？实际上，这正是YOLO
    v1中的做法。不幸的是，这会导致很多误差，因为物体的大小各异。
- en: Indeed, if most of the objects in the train dataset are big, the network will
    tend to predict *w* and *h* as being very large. And when using the trained model
    on small objects, it will often fail. To fix this problem, YOLO v2 introduced
    **anchor boxes**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，如果训练数据集中的大多数物体较大，网络将倾向于预测*宽度*（*w*）和*高度*（*h*）非常大。当使用训练好的模型来检测小物体时，它通常会失败。为了解决这个问题，YOLO
    v2引入了**锚框**。
- en: 'Anchor boxes (also called **priors**) are a set of bounding box sizes that
    are decided upon before training the network. For instance, when training a neural
    network to detect pedestrians, tall and narrow anchor boxes would be picked. An
    example is shown here:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框（也叫**先验框**）是一组在训练网络之前就决定好的边界框大小。例如，当训练神经网络来检测行人时，会选择高而窄的锚框。如下所示：
- en: '![](img/776b173e-60b2-45e3-8f90-1b96f67f2ae5.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/776b173e-60b2-45e3-8f90-1b96f67f2ae5.png)'
- en: Figure 5.6: On the left are the three bounding box sizes picked to detect pedestrians.
    On the right is how we adapt one of the bounding boxes to match a pedestrian
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：左侧是用于检测行人的三种边界框大小。右侧是我们如何调整其中一个边界框以匹配行人。
- en: A set of anchor boxes is usually small—from 3 to 25 different sizes in practice.
    As those boxes cannot exactly match all the objects, the network is used to refine
    the closest anchor box. In our example, we fit the pedestrian in the image with
    the closest anchor box and use the neural network to correct the height of the
    anchor box. This is what *t[x]*, *t[y]*, *t[w]*, and *t[h]* correspond to—**corrections
    to the anchor box**.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一组锚框通常较小——在实践中通常包含3到25种不同的尺寸。由于这些框不能完全匹配所有物体，网络会用于细化最接近的锚框。在我们的示例中，我们将行人图像中的物体与最接近的锚框匹配，并使用神经网络来修正锚框的高度。这就是*t[x]*、*t[y]*、*t[w]*和*t[h]*对应的内容——**锚框的修正**。
- en: 'When they were first introduced in the literature, anchor boxes were picked
    manually. Usually, nine box sizes were used:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当锚框首次在文献中提出时，它们是手动选择的。通常使用九种框大小：
- en: Three squares (small, medium, and large)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个正方形（小号、中号、大号）
- en: Three horizontal rectangles (small, medium, and large)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个水平矩形（小号、中号、大号）
- en: Three vertical rectangles (small, medium, and large)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个垂直矩形（小号、中号、大号）
- en: However, in the YOLOv2 paper, the authors recognized that the sizes of anchor
    boxes are different for each dataset. Therefore, before training the model, they
    recommend analyzing the data to pick the size of the anchor boxes. To detect pedestrians,
    as previously, vertical rectangles would be used. To detect apples, square anchor
    boxes would be used.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在YOLOv2的论文中，作者意识到锚框的大小因数据集而异。因此，在训练模型之前，他们建议对数据进行分析，以选择合适的锚框大小。比如，在检测行人时，如前所述，使用垂直矩形框；在检测苹果时，则使用正方形的锚框。
- en: How YOLO refines anchor boxes
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO如何细化锚框
- en: 'In practice, YOLOv2 computes each final bounding box''s coordinates using the
    following formulas:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，YOLOv2 使用以下公式计算每个最终边界框的坐标：
- en: '![](img/3043125d-8af4-4a95-b9df-5ffa624f1859.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3043125d-8af4-4a95-b9df-5ffa624f1859.png)'
- en: 'The terms of the preceding equation can be explained as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前面公式中的各项可以按如下方式解释：
- en: '*t[x] , t[y] , t[w] ,* and *t*[*h* ] are the outputs from the last layer.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[x] , t[y] , t[w] ,* 和 *t*[*h*] 是最后一层的输出。'
- en: '*b[x] , b[y] , b[w] ,* and*  b[h]* are the position and size of the predicted
    bounding box, respectively.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b[x] , b[y] , b[w] ,* 和 *b[h]* 分别表示预测边界框的位置和大小。'
- en: '*p[w]* and *p[h]* represent the original size of the anchor box.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[w]* 和 *p[h]* 代表锚框的原始尺寸。'
- en: '*c[x]* and *c*[*y*] are the coordinates of the current grid cell (they will
    be (0,0) for the top-left box, (w - 1,0) for the top-right box, and (0, h - 1)
    for the bottom-left box).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c[x]* 和 *c*[*y*] 是当前网格单元的坐标（对于左上框，它们将是 (0,0)，对于右上框，它们将是 (w - 1,0)，对于左下框，它们将是
    (0, h - 1)）。'
- en: '*exp* is the exponential function.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*exp* 是指数函数。'
- en: '*sigmoid* is the sigmoid function, described in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*sigmoid* 是 sigmoid 函数，描述见[第 1 章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)，*计算机视觉与神经网络*。'
- en: 'While this formula may seem complex, this diagram may help to clarify matters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个公式看起来复杂，但这个示意图可能有助于澄清问题：
- en: '![](img/37cbf53f-ae4f-4945-9b9f-d2ed447467de.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37cbf53f-ae4f-4945-9b9f-d2ed447467de.png)'
- en: Figure 5.7: How YOLO refines and positions anchor boxes
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：YOLO 如何精炼并定位锚框
- en: In the preceding diagram, we see that on the left, the solid line is the anchor
    box, and the dotted line is the refined bounding box. On the right, the dot is
    the center of the bounding box.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，我们看到左侧实线为锚框，虚线为精炼后的边界框。右侧的点是边界框的中心。
- en: 'The output of the neural network, a matrix with raw numbers, needs to be transformed
    into a list of bounding boxes. A simplified version of the code would look like
    this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出是一个包含原始数值的矩阵，需要转换为边界框列表。简化版的代码如下所示：
- en: '[PRE0]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code needs to be run for every inference in order to compute bounding boxes
    for an image. Before we can display the boxes, we need one more post-processing
    operation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要在每次推理时运行，以便为图像计算边界框。在我们显示框之前，还需要进行一次后处理操作。
- en: Post-processing the boxes
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后处理框
- en: 'We end up with the coordinates and the size of the predicted bounding boxes,
    as well as the confidence and the class probabilities. All we have to do now is
    to multiply the confidence by the class probabilities and threshold them in order
    to only keep high probabilities:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了预测边界框的坐标和大小，以及置信度和类概率。现在我们只需将置信度乘以类概率，并进行阈值处理，只保留高概率：
- en: '[PRE1]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is an example of this operation with a simple sample, with a threshold
    of `0.3` and a box confidence (for this specific box) of `0.5`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用简单示例的该操作示例，阈值为 `0.3`，该框的置信度（对于此特定框）为 `0.5`：
- en: '| **CLASS_LABELS** | *dog* | *airplane* | *bird* | *elephant* |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **CLASS_LABELS** | *dog* | *airplane* | *bird* | *elephant* |'
- en: '| **classes_scores** | 0.7 | 0.8 | 0.001 | 0.1 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **classes_scores** | 0.7 | 0.8 | 0.001 | 0.1 |'
- en: '| **final_scores** | 0.35 | 0.4 | 0.0005 | 0.05 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **final_scores** | 0.35 | 0.4 | 0.0005 | 0.05 |'
- en: '| **filtered_scores** | 0.35 | 0.4 | 0 | 0 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **filtered_scores** | 0.35 | 0.4 | 0 | 0 |'
- en: 'Then, if `filtered_scores` contains non-null values, this means we have at
    least one class above the threshold. We keep the class with the highest score:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果`filtered_scores`包含非空值，这意味着我们至少有一个类的分数超过了阈值。我们保留得分最高的类：
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In our example, `class_label` would be *airplane*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`class_label` 将是 *airplane*。
- en: 'Once we have applied this filtering to all of the bounding boxes in the grid,
    we end up with all the information we need to draw the predictions. The following
    photograph shows what we would obtain by doing so:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对网格中的所有边界框应用了这个过滤操作，我们就会得到绘制预测所需的所有信息。以下照片显示了这样做后的结果：
- en: '![](img/fe6e18ee-9cb9-4659-89cf-852764887aa2.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe6e18ee-9cb9-4659-89cf-852764887aa2.png)'
- en: Figure 5.8: Example of the raw bounding box output being drawn over the image
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：原始边界框输出在图像上的绘制示例
- en: Numerous bounding boxes are overlapping. As the plane is covering several grid
    cells, it has been detected more than once. To correct this, we need one last
    step in our post-processing pipeline—**non-maximum suppression** (**NMS**).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 许多边界框重叠。由于平面覆盖了多个网格单元，因此它被多次检测到。为了解决这个问题，我们需要在后处理管道中进行最后一步——**非极大值抑制** (**NMS**)。
- en: NMS
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NMS
- en: The idea of NMS is to remove boxes that overlap the box with the highest probability.
    We therefore remove boxes that are **non-maximum**. To do so, we sort all the
    boxes by probability, taking the ones with the highest probability first. Then,
    for each box, we compute the IoU with all the other boxes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: NMS的思路是去除与概率最高框重叠的框。因此，我们移除**非最大**框。为此，我们根据概率对所有框进行排序，先选取概率最高的框。然后，对于每个框，我们计算它与其他所有框的IoU。
- en: After computing the IoU between a box and the other boxes, we remove the ones
    with an IoU above a certain threshold (the threshold is usually around 0.5-0.9).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一个框与其他框的IoU后，我们去除那些IoU超过某个阈值的框（该阈值通常在0.5到0.9之间）。
- en: 'With pseudo-code, this is what NMS would look like:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用伪代码，NMS的实现如下所示：
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In practice, TensorFlow provides its own implementation of NMS, `tf.image.non_max_suppression(boxes,
    ...)` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression](https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression)),
    which we recommend using (it is well optimized and offers useful options). Also
    note that NMS is used in most object detection model post-processing pipelines.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，TensorFlow提供了自己的NMS实现，`tf.image.non_max_suppression(boxes, ...)`（请参阅文档：[https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression](https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression)），我们建议使用它（它已经过优化并提供了有用的选项）。还要注意，NMS被大多数物体检测模型的后处理管道所使用。
- en: 'After performing NMS, we obtain a much better result with a single bounding
    box, as illustrated in the following photograph:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行NMS后，我们得到了一个更好的结果，只有一个边界框，如下图所示：
- en: '![](img/6d43a0d2-d97b-4cad-9353-09a301e13916.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d43a0d2-d97b-4cad-9353-09a301e13916.png)'
- en: Figure 5.9: Example of the bounding boxes drawn over the image after NMS
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：NMS后在图像上绘制的边界框示例
- en: YOLO inference summarized
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO推理总结
- en: 'Putting it all together, the YOLO inference comprises several smaller steps.
    YOLO''s architecture is illustrated in the following diagram:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有步骤整合起来，YOLO推理包括若干个小步骤。YOLO的架构示意图如下所示：
- en: '![](img/dec87acc-3dd0-47e1-abf8-8c9f58fef318.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dec87acc-3dd0-47e1-abf8-8c9f58fef318.png)'
- en: Figure 5.10: YOLO's architecture. In this example, we use two bounding boxes
    per grid cell
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：YOLO架构示意图。在这个示例中，我们对每个网格单元使用两个边界框。
- en: 'The YOLO inference process can be summarized as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO推理过程可以总结如下：
- en: Accept an input image and compute a feature volume using a CNN backbone.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受输入图像，并使用CNN骨干网计算特征体积。
- en: Use a convolutional layer to compute anchor box corrections, objectness scores,
    and class probabilities.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卷积层计算锚框修正、物体存在分数和类别概率。
- en: Using this output, compute the coordinates of the bounding boxes.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该输出计算边界框的坐标。
- en: Filter out the boxes with a low threshold, and post-process the remaining ones
    using NMS.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 筛选掉低阈值的框，并使用NMS对剩余框进行后处理。
- en: At the conclusion of this process, we end up with the final predictions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程的最后，我们得到了最终的预测结果。
- en: Since the whole process is composed of convolutions and filtering operations,
    the network can accept images of any size and any ratio. Hence, it is very flexible.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整个过程由卷积和过滤操作组成，网络可以接受任何大小和任何比例的图像。因此，它具有很高的灵活性。
- en: Training YOLO
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练YOLO
- en: We have outlined the process of inference for YOLO. Using pretrained weights
    provided online, it is possible to instantiate a model directly and generate predictions.
    However, you might want to train a model on a specific dataset. In this section,
    we will go through the training procedure of YOLO.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经概述了YOLO的推理过程。利用在线提供的预训练权重，可以直接实例化模型并生成预测结果。然而，您可能希望在特定数据集上训练一个模型。在本节中，我们将讲解YOLO的训练过程。
- en: How the YOLO backbone is trained
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO骨干网的训练方式
- en: As we mentioned earlier, the YOLO model is composed of two main parts—the backbone
    and the YOLO head. Many architectures can be used for the backbone. Before training
    the full model, the backbone is trained on a traditional classification task with
    the aid of ImageNet using the transfer learning technique detailed in [Chapter
    4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml), *Influential Classification Tools*.
    While we could train YOLO from scratch, it would take much more time to do so.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，YOLO模型由两个主要部分组成——骨干网和YOLO头。骨干网可以使用许多架构。在训练完整模型之前，骨干网会在传统的分类任务中，通过使用ImageNet并采用[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)中详细描述的迁移学习技术进行训练。虽然我们可以从头开始训练YOLO，但这样做需要花费更多的时间。
- en: 'Keras makes it very easy to use a pretrained backbone for our network:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Keras使得在我们的网络中使用预训练的主干网络变得非常简单：
- en: '[PRE4]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our implementation, we will employ the architecture presented in the YOLO
    paper because it yields the best results. However, if you were to run your model
    on a mobile, you might want to use a smaller model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们将采用YOLO论文中提出的架构，因为它能提供最佳的结果。然而，如果你要在手机上运行你的模型，你可能会想要使用一个更小的模型。
- en: YOLO loss
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO损失
- en: 'As the output of the last layer is quite unusual, the corresponding loss will
    also be. Actually, the YOLO loss is notoriously complex. To explain it, we will
    break the loss into several parts, each corresponding to one kind of output returned
    by the last layer. The network predicts multiple kinds of information:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最后一层的输出相当不寻常，因此相应的损失也是如此。实际上，YOLO损失因其复杂性而著称。为了说明它，我们将把损失分解为多个部分，每个部分对应于最后一层返回的某种输出。网络预测多种信息：
- en: The bounding box coordinates and size
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界框的坐标和大小
- en: The confidence that an object is in the bounding box
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体出现在边界框中的置信度
- en: The scores for the classes
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别的得分
- en: The general idea of the loss is that we want it to be high when the error is
    high. The loss will penalize the incorrect values. However, we only want to do
    so when it makes sense—if a bounding box contains no objects, we do not want to
    penalize its coordinates as they will not be used anyway.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的一般思路是，当误差较大时，我们希望损失也较大。损失会惩罚不正确的值。然而，我们只希望在有意义的情况下这样做——如果一个边界框不包含任何物体，我们就不希望惩罚它的坐标，因为它们反正不会被使用。
- en: The implementation details of neural networks are usually not available in the
    source paper. Therefore, they will vary from one implementation to another. What
    we are outlining here is an implementation suggestion, not an absolute reference.
    We suggest reading the code from existing implementations to understand how the
    loss is calculated.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的实现细节通常在原始论文中没有提供。因此，它们在不同的实现之间会有所不同。我们在这里概述的是一种实现建议，而不是绝对参考。我们建议阅读现有实现中的代码，以了解损失是如何计算的。
- en: Bounding box loss
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 边界框损失
- en: 'The first part of the loss helps the network learn the weights to predict the
    bounding box coordinates and size:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的第一部分帮助网络学习预测边界框坐标和大小的权重：
- en: '![](img/db3d3535-c2a2-4c13-8121-b753b3c96264.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db3d3535-c2a2-4c13-8121-b753b3c96264.png)'
- en: 'While this equation may seem scary at first, this part is actually relatively
    simple. Let''s break it down:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个方程式一开始看起来可能很吓人，但这一部分其实相对简单。让我们分解它：
- en: λ (lambda) is the weighting of the loss—it reflects how much importance we want
    to give to bounding box coordinates during training.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: λ（lambda）是损失的权重——它反映了在训练过程中，我们希望给予边界框坐标多大重要性。
- en: ∑ (capital sigma) means that we sum what is right after them. In this case,
    we sum for each part of the grid (from i = 0 to *i = S²*) and for each box in
    this part of the grid (from 0 to B).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∑（大写希腊字母sigma）表示我们要对它后面的内容求和。在这个例子中，我们对网格的每个部分（从i = 0到*i = S²*）以及该部分网格中的每个框（从0到B）求和。
- en: '*1^(obj)* (*indicator function* for objects) is a function equal to 1 when
    the i^(th) part of the grid and the j^(th) bounding box are **responsible** for
    an object. We will explain what responsible means in the next paragraph.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1^(obj)*（*物体指示函数*）是一个函数，当网格的第i部分和第j个边界框**负责**某个物体时，其值为1。我们将在下一段解释“负责”是什么意思。'
- en: '*x[i]*, *y[i]*, *w[i]*, and *h[i]*  correspond to the bounding box size and
    coordinates. We take the difference between the predicted value (the output of
    the network) and the target value (also called the **ground truth**). Here, the
    predicted value has a hat (`ˆ`).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[i]*、*y[i]*、*w[i]* 和 *h[i]* 对应于边界框的大小和坐标。我们取预测值（网络输出）与目标值（也称为**真实值**）之间的差异。这里，预测值带有上标(`ˆ`)。'
- en: We square the difference to make sure it is positive.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将差异平方化，以确保其为正数。
- en: Notice that we take the square root of w[i] and h[i]. We do so to make sure
    errors for small bounding boxes are penalized more heavily than errors for big
    bounding boxes.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，我们取了w[i]和h[i]的平方根。这样做是为了确保小的边界框错误受到比大的边界框更重的惩罚。
- en: The key part of this loss is the **indicator function**. The coordinates will
    be correct if, and only if, the box is responsible for detecting an object. For
    each object in the image, the difficult part is determining which bounding box
    is responsible for it. For YOLOv2, the anchor box with the highest IoU with the
    detected object is deemed responsible. The rationale here is to make each anchor
    box specialize in one type of object.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失的关键部分是**指示函数**。只有当边界框负责检测物体时，坐标才会是正确的。对于图像中的每个物体，难点是确定哪个边界框负责它。对于YOLOv2，具有与检测到物体最高IoU的锚框被认为是负责的。这里的基本思想是让每个锚框专注于一种物体类型。
- en: Object confidence loss
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体置信度损失
- en: 'The second part of the loss teaches the network to learn the weights to predict
    whether a bounding box contains an object:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的第二部分教会网络学习预测边界框是否包含物体的权重：
- en: '![](img/28a418fd-60cd-4562-a00a-79ebdddfff3f.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28a418fd-60cd-4562-a00a-79ebdddfff3f.png)'
- en: 'We have already covered most of the symbols in this function. The remaining
    ones are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了该函数中的大部分符号。剩下的符号如下：
- en: '**C[ij]**: The confidence that the box, *j*, in the part, *i*, of the grid
    contains an object (of any kind)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C[ij]**: 第*i*网格部分的第*j*边界框包含物体（任意类型）的置信度'
- en: '**1^(noobj)  (indicator function for no object)**: A function equal to 1 when the
    *i*^(th) part of the grid and the *j*^(th) bounding box are *not responsible*
    for an object'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1^(noobj)（无物体的指示函数）**：当第*i*网格部分和第*j*边界框*不负责*某物体时，函数值为1'
- en: 'A naive approach to compute *1^(noobj)*  is  *(1 - 1^(obj))*. However, if we
    do so, it can cause some problems during training. Indeed, we have many bounding
    boxes on our grid. When determining that one of them is responsible for a specific
    object, there may have been other suitable candidates for this object. We do not
    want to penalize the objectness score of those other good candidates that also
    fit the object. Therefore, *1^(noobj)*  is defined as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*1^(noobj)*的一个简单方法是*(1 - 1^(obj))*。然而，如果我们这样做，它可能会在训练过程中引发一些问题。实际上，我们的网格上有很多边界框。在确定某个边界框负责特定物体时，可能会有其他适合该物体的候选框。我们不希望惩罚那些适合物体的其他优秀候选框的物体性得分。因此，*1^(noobj)*的定义如下：
- en: '![](img/3c2f6c72-c27f-4116-b1a0-dcf00b38d25c.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c2f6c72-c27f-4116-b1a0-dcf00b38d25c.png)'
- en: In practice, for each bounding box at position (*i*, *j*), the IoU with regard
    to each of the ground truth boxes is computed. If the IoU is over a certain threshold
    (usually 0.6), 1^(noobj) is set to 0\. The rationale behind this idea is to avoid
    punishing boxes that contain objects but are not responsible for said object.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，对于位置(*i*, *j*)上的每个边界框，都会计算它与每个地面真实框的IoU。如果IoU超过某个阈值（通常为0.6），则将1^(noobj)设为0。这个想法的依据是避免惩罚那些包含物体但并不负责该物体的边界框。
- en: Classification loss
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类损失
- en: 'The final part of the loss, the classification loss, ensures that the network
    learns to predict the proper class for each bounding box:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的最后一部分——分类损失，确保网络学习为每个边界框预测正确的类别：
- en: '![](img/4e8f99f7-0dac-45f7-9782-af77f2c0a604.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e8f99f7-0dac-45f7-9782-af77f2c0a604.png)'
- en: This loss is very similar to the one presented in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer
    Vision and Neural Networks*. Note that while the loss presented in the YOLO paper
    is the L2 loss, many implementations use cross-entropy. This part of the loss
    ensures that correct object classes are predicted.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失与[第1章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)中介绍的*计算机视觉与神经网络*的损失非常相似。需要注意的是，虽然YOLO论文中展示的损失是L2损失，但许多实现使用的是交叉熵损失。这个损失部分确保正确的物体类别被预测出来。
- en: Full YOLO loss
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整YOLO损失
- en: '**Full YOLO loss** is the sum of the three losses previously detailed. By combining
    the three terms, the loss penalizes the error for bounding box coordinate refinement,
    objectness scores, and class prediction. By backpropagating the error, we are
    able to train the YOLO network to predict correct bounding boxes.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整YOLO损失**是前面提到的三个损失的总和。通过组合这三个项，损失惩罚边界框坐标的精细调整、物体性得分和类别预测的错误。通过反向传播错误，我们能够训练YOLO网络预测正确的边界框。'
- en: In the book's GitHub repository, readers will find a simplified implementation
    of the YOLO network. In particular, the implementation contains a heavily commented
    loss function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的GitHub仓库中，读者将找到YOLO网络的简化实现。特别地，代码中包含了大量注释的损失函数。
- en: Training techniques
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练技巧
- en: 'Once the loss has been properly defined, YOLO can be trained using backpropagation.
    However, to make sure the loss does not diverge and to obtain good performance,
    we will detail a few training techniques:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦损失函数被正确定义，YOLO就可以通过反向传播进行训练。然而，为了确保损失不会发散，并且获得良好的性能，我们将详细介绍一些训练技巧：
- en: Augmentation (explained in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*) and dropout (explained in [Chapter
    3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern Neural Networks*) are
    used. Without these two techniques, the network would overfit on the training
    data and would not be able to generalize much.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强（在[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)中解释，*在复杂和稀缺数据集上的训练*）和丢弃法（在[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)中解释，*现代神经网络*）被用来防止网络过拟合训练数据，从而使其能够更好地泛化。
- en: Another technique is **multi-scale training**. Every *n* batches, the network's
    input is changed to a different size. This forces the network to learn to predict
    with accuracy across a variety of input dimensions.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种技术是**多尺度训练**。每经过*n*个批次，网络的输入会被更改为不同的尺寸。这迫使网络学习在各种输入维度下进行精确预测。
- en: Like most detection networks, YOLO is pretrained on an image classification
    task.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与大多数检测网络一样，YOLO在图像分类任务上进行预训练。
- en: While not mentioned in the paper, the official YOLO implementation uses **burn-in**—the
    learning rate is reduced at the beginning of training to avoid a loss explosion.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管论文中没有提到，官方YOLO实现使用了**预热**——在训练开始时降低学习率，以避免损失爆炸。
- en: Faster R-CNN – a powerful object detection model
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN – 一个强大的目标检测模型
- en: The main benefit of YOLO is its speed. While it can achieve very good results,
    it is now outperformed by more complex networks. **Faster Region with Convolutional
    Neural Networks** (**Faster R-CNN**) is considered state of the art at the time
    of writing. It is also quite fast, reaching 4-5 FPS on a modern GPU. In this section,
    we will explore its architecture.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的主要优点是其速度。尽管它可以取得非常好的结果，但现在已被更复杂的网络超越。**更快的区域卷积神经网络**（**Faster R-CNN**）被认为是在写作时的最先进技术。它也相当快速，在现代GPU上达到4-5帧每秒。在这一部分，我们将探讨其架构。
- en: 'The Faster R-CNN architecture was engineered over several years of research.
    More precisely, it was built incrementally from two architectures—R-CNN and Fast
    R-CNN. In this section, we will focus on the latest architecture, Faster R-CNN:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN架构经过多年的研究逐步设计完成。更准确地说，它是从两个架构——R-CNN和Fast R-CNN——逐步构建而成的。在这一部分，我们将重点介绍最新的架构，Faster
    R-CNN：
- en: '*Faster R-CNN: towards real-time object detection with region proposal networks
    (2015)*, Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Faster R-CNN：通过区域提议网络实现实时目标检测（2015）*，Shaoqing Ren, Kaiming He, Ross Girshick
    和 Jian Sun'
- en: 'This paper draws a lot of knowledge from the two previous designs. Therefore,
    some of the architecture details can be found in the following papers:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本文借鉴了之前两个设计中的大量知识。因此，部分架构细节可以在以下论文中找到：
- en: '*Rich feature hierarchies for accurate object detection and semantic segmentation
    (2013)*, Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Mali'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精确目标检测和语义分割的丰富特征层次（2013）*，Ross Girshick, Jeff Donahue, Trevor Darrell 和 Jitendra
    Mali'
- en: '*Fast R-CNN (2015)*, Ross Girshick'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fast R-CNN（2015）*，Ross Girshick'
- en: Just as with YOLO architecture, we recommend reading this chapter first and
    then having a look at the papers to get a deeper understanding. In this chapter,
    we will use the same notations as in the papers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 就像YOLO架构一样，我们建议先阅读本章，然后查看这些论文以获得更深的理解。在本章中，我们将使用与论文中相同的符号。
- en: Faster R-CNN's general architecture
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN的通用架构
- en: 'YOLO is considered a single-shot detector—as its name implies, each pixel of
    the image is analyzed once. This is the reason for its very high speed. To obtain
    more accurate results, Faster R-CNN works in two stages:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO被认为是一个单次检测器——正如其名称所示，每个图像像素只分析一次。这就是它速度非常快的原因。为了获得更精确的结果，Faster R-CNN分为两个阶段工作：
- en: The first stage is to extract a **region of interest** (**RoI**, or RoIs in
    the plural form). An RoI is an area of the input image that may contain an object.
    For each image, the first step generates about 2,000 RoIs**.**
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一阶段是提取**兴趣区域**（**RoI**，复数形式为RoIs）。RoI是输入图像中可能包含物体的区域。对于每张图像，第一步生成大约2,000个RoI**。**
- en: The second stage is the **classification step** (sometimes referred to as the **detection
    step**). We resize each of the 2,000 RoIs to a square to fit the input of a convolutional
    network. We then use the CNN to classify the RoI.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二阶段是 **分类步骤**（有时也称为 **检测步骤**）。我们将每个 2,000 个 RoI 调整为正方形，以适应卷积网络的输入。然后我们使用 CNN
    对 RoI 进行分类。
- en: In R-CNN and Fast R-CNN, regions of interest are generated using a technique
    called **selective search**. This will not be covered here because it was removed
    from the Faster R-CNN paper on account of its slowness. Moreover, selective search
    does not involve any deep learning techniques.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R-CNN 和 Fast R-CNN 中，兴趣区域是使用一种叫做 **选择性搜索** 的技术生成的。这里不再详细介绍，因为它在 Faster R-CNN
    论文中被删除了，原因是其速度较慢。此外，选择性搜索并不涉及任何深度学习技术。
- en: As the two parts of Faster R-CNN are independent, we will cover each one separately.
    We will then cover the training details of the full model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Faster R-CNN 的两个部分是独立的，我们将分别介绍每一部分。然后我们将介绍完整模型的训练细节。
- en: Stage 1 – Region proposals
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一阶段 – 区域提议
- en: Regions of interest are generated using the **region proposal network** (**RPN**).
    To generate RoIs, the RPN uses convolutional layers. Therefore, it can be implemented
    on the GPU and is very fast.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **区域提议网络**（**RPN**）生成兴趣区域。为了生成 RoI，RPN 使用卷积层。因此，它可以在 GPU 上实现，并且速度非常快。
- en: 'The RPN architecture shares quite a lot of features with YOLO''s architecture:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: RPN 架构与 YOLO 的架构共享许多特征：
- en: It also uses anchor boxes—in the Faster R-CNN paper, nine anchor sizes are used
    (three vertical rectangles, three horizontal rectangles, and three squares).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还使用锚框——在 Faster R-CNN 论文中，使用了九种锚框尺寸（三种竖直矩形、三种水平矩形和三种正方形）。
- en: It can use any backbone to generate the feature volume.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以使用任何骨干网络来生成特征体积。
- en: It uses a grid, and the size of the grid depends on the size of the feature
    volume.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用一个网格，网格的大小取决于特征体积的大小。
- en: Its last layer outputs numbers that allow the anchor box to be refined into
    a proper bounding box fitting the object.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的最后一层输出数字，允许锚框被精炼成一个合适的边界框以适应物体。
- en: 'However, the architecture is not completely identical to YOLO''s. The RPN accepts
    an image as input and outputs regions of interest. Each region of interest consists
    of a bounding box and an objectness probability. To generate those numbers, a
    CNN is used to extract a feature volume. The feature volume is then used to generate
    the regions, coordinates, and probabilities. The RPN architecture is illustrated
    in the following diagram:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该架构与 YOLO 的架构并不完全相同。RPN 接受图像作为输入并输出兴趣区域。每个兴趣区域由一个边界框和一个物体存在概率组成。为了生成这些数字，使用
    CNN 提取特征体积。然后使用该特征体积生成区域、坐标和概率。RPN 的架构在下图中进行了说明：
- en: '![](img/2b7a4bb9-19a3-4de6-b361-a9e97eadd515.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b7a4bb9-19a3-4de6-b361-a9e97eadd515.png)'
- en: Figure 5.11: RPN architecture summary
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：RPN 架构概述
- en: 'The step-by-step process represented in *Figure 5.11* is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 中表示的逐步过程如下：
- en: The network accepts an image as input and applies several convolutional layers.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络接受图像作为输入，并应用多个卷积层。
- en: It outputs a feature volume. A convolutional filter is applied over the feature
    volume. Its size is *3* × *3* × *D*, where *D* is the depth of the feature volume
    (*D = 512* in our example).
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它输出一个特征体积。一个卷积滤波器应用于特征体积。其大小为 *3* × *3* × *D*，其中 *D* 是特征体积的深度（在我们的示例中，*D = 512*）。
- en: At each position in the feature volume, the filter generates an intermediate
    *1* × *D* vector.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特征体积的每个位置，滤波器生成一个中间的 *1* × *D* 向量。
- en: Two sibling *1* × *1* convolutional layers compute the objectness scores and
    the bounding box coordinates. There are two objectness scores for each of the
    *k* bounding boxes. There are also four floats that will be used to refine the
    coordinates of the anchor boxes.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个兄弟 *1* × *1* 的卷积层计算物体存在性分数和边界框坐标。每个 *k* 个边界框有两个物体存在性分数。此外，还有四个浮动值，用于优化锚框的坐标。
- en: After post-processing, the final output is a list of RoIs. At this step, no
    information about the class of the object is generated, only about its location.
    During the next step, classification, we will classify the objects and refine
    the bounding boxes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在后处理之后，最终输出是 RoI 的列表。在这一步骤中，不会生成有关物体类别的信息，只会生成它的位置。在下一步分类中，我们将对物体进行分类，并优化边界框。
- en: Stage 2 – Classification
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二阶段 – 分类
- en: The second part of Faster R-CNN is **classification**. It outputs the final
    bounding boxes and accepts two inputs—the list of RoIs from the previous step
    (RPN), and a feature volume computed from the input image.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN的第二部分是**分类**。它输出最终的边界框，并接受两个输入——来自前一步（RPN）的RoI列表，以及从输入图像计算出的特征体积。
- en: Since most of the classification stage architecture comes from the previous
    paper, Fast R-CNN, it is sometimes referred to with the same name. Therefore,
    Faster R-CNN can be regarded as a combination of RPN and Fast R-CNN.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大部分分类阶段的架构来自之前的论文《Fast R-CNN》，有时也称其为相同的名字。因此，Faster R-CNN可以视为RPN和Fast R-CNN的结合。
- en: 'The classification part can work with any feature volume corresponding to the
    input image. However, as feature maps have already been computed in the previous
    region-proposal step, they are simply reused here. This technique has two benefits:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 分类部分可以与任何对应于输入图像的特征体积一起工作。然而，由于特征图已在前一个区域提议步骤中计算，因此在这里它们只是被重复使用。这种技术有两个好处：
- en: '**Sharing the weights**: If we were to use a different CNN, we would have to
    store the weights for two backbones—one for the RPN, and one for the classification.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享权重**：如果我们使用不同的CNN，我们将不得不为两个骨干网络存储权重——一个用于RPN，另一个用于分类。'
- en: '**Sharing the computation**: For one input image, we only compute one feature
    volume instead of two. As this operation is the most expensive of the whole network,
    not having to run it twice allows for a consequent gain in computational performance.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享计算**：对于一个输入图像，我们只计算一个特征体积，而不是两个。由于这一操作是整个网络中最昂贵的部分，避免重复计算可以显著提高计算性能。'
- en: Faster R-CNN architecture
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN架构
- en: 'The second stage of Faster R-CNN accepts the feature maps from the first stage,
    as well as the list of RoIs. For each RoI, convolutional layers are applied to
    obtain class predictions and **bounding box refinement** information. The operations
    are represented here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN的第二阶段接受第一阶段的特征图，以及RoI列表。对于每个RoI，应用卷积层以获得类别预测和**边界框精细调整**信息。操作流程如下：
- en: '![](img/aa0b02c5-788f-4935-9266-12fbee0b6000.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa0b02c5-788f-4935-9266-12fbee0b6000.png)'
- en: Figure 5.12: Architecture summary of Faster R-CNN
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：Faster R-CNN的架构总结
- en: 'Step by step, the process is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步过程如下：
- en: Accept the feature maps and the RoIs from the RPN step. The RoIs generated in
    the original image coordinate system are converted into the feature map coordinate
    system. In our example, the stride of the CNN is 16\. Therefore, their coordinates
    are divided by 16.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受来自RPN步骤的特征图和RoI。原始图像坐标系统中生成的RoI被转换为特征图坐标系统。在我们的示例中，CNN的步幅为16。因此，它们的坐标会被除以16。
- en: Resize each RoI to make it fit the input of the fully connected layers.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整每个RoI的大小，使其适应全连接层的输入。
- en: Apply the fully connected layer. It is very similar to the final layers of any
    convolutional network. We obtain a feature vector.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用全连接层。它与任何卷积网络的最终层非常相似。我们得到一个特征向量。
- en: Apply two different convolutional layers. One handles the classification (called
    **cls**) and the other handles the refinement of the RoI (called **rgs**).
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用两层不同的卷积层。一层处理分类（称为**cls**），另一层处理RoI的精细调整（称为**rgs**）。
- en: The final results are the class scores and bounding box refinement floats that
    we will be able to post-process to generate the final output of the model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是类别分数和边界框精细调整浮动，我们可以对其进行后处理以生成模型的最终输出。
- en: The size of the feature volume depends on the size of the input and the architecture
    of the CNN. For instance, for VGG-16, the size of the feature volume is *w* ×
    *h* × *512*, where *w = input_width/16* and *h = input_height/16*. We say that
    VGG-16 has a stride of 16 because one pixel in the feature map equals 16 pixels
    in the input image.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 特征体积的大小取决于输入的大小和CNN的架构。例如，对于VGG-16，特征体积的大小是*w* × *h* × *512*，其中*w = input_width/16*，*h
    = input_height/16*。我们说VGG-16的步幅为16，因为特征图中的一个像素等于输入图像中的16个像素。
- en: While convolutional networks can accept inputs of any size (as they use a sliding
    window over the image), the final fully connected layer (between steps 2 and 3)
    accepts a feature volume of a fixed size as an input. And since region proposals
    are of different sizes (a vertical rectangle for a person, a square for an apple...),
    this makes the final layer impossible to use as is.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然卷积网络可以接受任何大小的输入（因为它们在图像上使用滑动窗口），但最终的全连接层（在步骤2和步骤3之间）只能接受固定大小的特征体积作为输入。由于区域提议的大小不同（例如，人的垂直矩形、苹果的正方形……），这使得最终层无法直接使用。
- en: To circumvent that, a technique was introduced in Fast R-CNN—**region of interest
    pooling** (**RoI** **pooling**). This converts a variable-size area of the feature
    map into a fixed-size area. The resized feature area can then be passed to the
    final classification layers.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这个问题，在Fast R-CNN中引入了一种技术——**兴趣区域池化**（**RoI** **池化**）。它将特征图的可变大小区域转换为固定大小的区域。调整大小后的特征区域可以传递给最终的分类层。
- en: RoI pooling
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoI池化
- en: The goal of the RoI pooling layer is simple—to take a part of the activation
    map of variable size and convert it into a fixed size. The input activation map
    sub-window is of size *h × w*. The target activation map is of size *H* × *W*.
    RoI pooling works by dividing its input into a grid where each cell is of size
    *h/H* × *w/W*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: RoI池化层的目标很简单——将一个大小可变的激活图部分转换为固定大小。输入激活图的子窗口大小为*h × w*，目标激活图的大小为*H × W*。RoI池化通过将输入划分为一个网格来工作，每个单元格的大小为*h/H*
    × *w/W*。
- en: 'Let''s use an example. If the input is of size *h* × *w = 5 *× *4*, and the
    target activation map is of size *H* × *W = 2* × *2*, then each cell should be
    of size *2.5* × *2*. Because we can only use integers, we will make some cells
    of size *3* × *2* and others of size *2* × *2*. Then, we will take the maximum
    of each cell:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明。如果输入的大小是*h* × *w = 5 × 4*，目标激活图的大小是*H* × *W = 2 × 2*，那么每个单元格的大小应该是*2.5*
    × *2*。由于我们只能使用整数，因此我们将使某些单元格的大小为*3* × *2*，其他单元格的大小为*2* × *2*。然后，我们将取每个单元格的最大值：
- en: '![](img/c10a0d84-1b1d-4139-9ef1-f9ffc8c49095.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c10a0d84-1b1d-4139-9ef1-f9ffc8c49095.png)'
- en: Figure 5.13: Example of RoI pooling with an RoI of size 5 × 4 (from B3 to E7)
    and an output of size 2 × 2 (from J4 to K5)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：RoI池化示例，其中RoI的大小为5 × 4（从B3到E7），输出大小为2 × 2（从J4到K5）
- en: An RoI pooling layer is very similar to a max-pooling layer. The difference
    is that RoI pooling works with inputs of variable size, while max-pooling works
    with a fixed size only. RoI pooling is sometimes referred to as **RoI max-pooling**.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: RoI池化层非常类似于最大池化层。不同之处在于，RoI池化能够处理大小可变的输入，而最大池化只能处理固定大小的输入。RoI池化有时被称为**RoI最大池化**。
- en: In the original R-CNN paper, RoI pooling had not yet been introduced. Therefore,
    each RoI was extracted from the original image, resized, and directly passed to
    the convolutional network. Since there were around 2,000 RoIs, it was extremely
    slow. The *Fast* in Fast R-CNN comes from the huge speedup introduced by the RoI
    pooling layer.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的R-CNN论文中，RoI池化尚未引入。因此，每个RoI是从原始图像中提取的，经过调整大小后直接传递给卷积网络。由于大约有2,000个RoI，这样做非常慢。Fast
    R-CNN中的*Fast*来源于RoI池化层带来的巨大加速。
- en: Training Faster R-CNN
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练Faster R-CNN
- en: 'Before we explain how to train the network, let''s have a look at the full
    architecture of Faster R-CNN:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解释如何训练网络之前，先来看看Faster R-CNN的完整架构：
- en: '![](img/c4d852b4-bffc-49e3-9c77-0d5c54b0c056.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4d852b4-bffc-49e3-9c77-0d5c54b0c056.png)'
- en: Figure 5.14: Full architecture of Faster R-CNN. Note that it can work with any
    input size
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14：Faster R-CNN的完整架构。注意，它可以处理任何输入大小
- en: Because of its unique architecture, Faster R-CNN cannot be trained like a regular
    CNN. If each of the two parts of the network were trained separately, the feature
    extractors of each part would not share the same weights. In the next section,
    we will explain the training of each section and how to make the two sections
    share the convolutional weights.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其独特的架构，Faster R-CNN不能像普通的CNN一样进行训练。如果将网络的两个部分分别训练，两个部分的特征提取器将无法共享相同的权重。在接下来的部分中，我们将解释如何训练每个部分，以及如何使这两个部分共享卷积权重。
- en: Training the RPN
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RPN
- en: The input of the RPN is an image, and the output is a list of RoIs. As we saw
    previously, there are *H* × *W* × *k* proposals for each image (where *H* and
    *W* represent the size of a feature map and *k* is the number of anchors). At
    this step, the class of the object is not yet considered.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: RPN的输入是图像，输出是一个RoI列表。正如我们之前看到的，对于每张图像，有*H* × *W* × *k*个提议（其中*H*和*W*代表特征图的大小，*k*是锚点的数量）。在这一步，物体的类别尚未被考虑。
- en: It would be difficult to train all the proposals at once—since images are mostly
    made of background, most of the proposals would be trained to predict *background*.
    As a consequence, the network would learn to always predict background. Instead,
    a sampling technique is favored.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性训练所有候选框会很困难——由于图像大多由背景组成，大多数候选框将被训练为预测*背景*。因此，网络将学习始终预测背景。相反，更倾向于使用采样技术。
- en: Mini-batches of 256 ground truth anchors are built; 128 of them are positive
    (they contain an object), and the other 128 are negative (they only contain background).
    If there are fewer than 128 positive samples in the image, all the positive samples
    available are used and the batch is filled with negative samples.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 构建了256个实际锚点的小批量；其中128个是正样本（包含物体），另外128个是负样本（仅包含背景）。如果图像中正样本少于128个，则使用所有可用的正样本，并用负样本填充批次。
- en: The RPN loss
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RPN损失
- en: 'The RPN loss is simpler than YOLO''s. It is composed of two terms:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: RPN损失比YOLO的损失更简单。它由两部分组成：
- en: '![](img/aed32ea7-fda1-442a-84b6-6b7f5b3f8862.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aed32ea7-fda1-442a-84b6-6b7f5b3f8862.png)'
- en: 'The terms in the preceding equation can be explained as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 前面公式中的各项可以解释如下：
- en: '*i* is the index of an anchor in a training batch.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i*是训练批次中一个锚点的索引。'
- en: '*p[i]* is the probability of the anchor being an object. *p[i]** is the ground
    truth—it''s 1 if the anchor is "positive"; otherwise, it''s 0.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[i]*是锚点为物体的概率。*p[i]**是实际值——如果锚点是“正样本”，则为1；否则为0。'
- en: '*t[i]* is the vector representing coordinate refinement; *t[i]** is the ground
    truth.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[i]*是表示坐标修正的向量；*t[i]**是实际值。'
- en: '*N[cls]* is the number of ground truth anchors in the training mini-batch.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[cls]*是训练小批量中实际锚点的数量。'
- en: '*N[reg]* is the number of possible anchor locations.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[reg]*是可能的锚点位置数量。'
- en: '*L[cls]* is the log loss over two classes (object and background).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[cls]*是两个类别（物体和背景）的对数损失。'
- en: '*λ* is a balancing parameter to balance the two parts of the loss.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*λ*是一个平衡参数，用于平衡损失的两个部分。'
- en: 'Finally, the loss is composed of *L[reg](t[i], t[i]*) = R(t[i] - t[i]*)*, where
    R is the *smooth* L1 loss function, defined as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，损失由 *L[reg](t[i], t[i]*) = R(t[i] - t[i]*)* 组成，其中R是*平滑*L1损失函数，定义如下：
- en: '![](img/e47ce6d0-1db7-40fa-b1d2-f385079918e9.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e47ce6d0-1db7-40fa-b1d2-f385079918e9.png)'
- en: The *smooth[L1]* function was introduced as a replacement for the L2 loss used
    previously. When the error was too important, the L2 loss would become too large,
    causing training instability.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*平滑[L1]*函数被引入，作为替代先前使用的L2损失。当误差过大时，L2损失会变得过大，导致训练不稳定。'
- en: Just as with YOLO, the regression loss is used only for anchor boxes that contain
    an object thanks to the *p[i]** term. The two parts are divided by *N[cls]* and
    *N[reg]*. Those two values are called **normalization terms**—if we were to change
    the size of mini-batches, the loss would not lose its equilibrium.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 正如YOLO一样，回归损失仅用于包含物体的锚点框，这要归功于*p[i]**项。两部分通过*N[cls]*和*N[reg]*进行划分。这两个值被称为**归一化项**——如果我们改变小批量的大小，损失不会失去平衡。
- en: Finally, lambda is a balancing parameter. In the paper configuration, *N[cls]
    ~= 256* and *N[reg] ~= 2,400*. The authors set *λ* to 10 so that the two terms
    have the same total weight.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，lambda是一个平衡参数。在论文配置中，*N[cls] ~= 256* 和 *N[reg] ~= 2,400*。作者将*λ*设置为10，以便两个项具有相同的总权重。
- en: 'In summary, similar to YOLO, the loss penalizes the following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，类似于YOLO，损失惩罚以下内容：
- en: The error in objectness classification with the first term
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个项中物体分类的误差
- en: The error in bounding box refinement with the second term
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二项中边界框修正的误差
- en: However, contrary to YOLO's loss, it does not deal with object classes bceause
    the RPN only predicts RoIs. Apart from the loss and the way mini-batches are constructed,
    the RPN is trained like any other network using backpropagation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与YOLO的损失相反，它并不处理物体类别，因为RPN只预测RoI。除了损失和小批量构建方式外，RPN的训练方式与其他网络相同，使用反向传播。
- en: Fast R-CNN loss
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fast R-CNN损失
- en: 'As stated earlier, the second stage of Faster R-CNN is also referred to as
    Fast R-CNN. Therefore, its loss is often referenced as the Fast R-CNN loss. While
    the formulation of the Fast R-CNN loss is different to the RPN loss, it is very
    similar in essence:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Faster R-CNN的第二阶段也被称为Fast R-CNN。因此，其损失通常被称为Fast R-CNN损失。尽管Fast R-CNN损失的公式与RPN损失不同，但在本质上非常相似：
- en: '![](img/4f9b7e1d-4c9c-4cdb-9b7a-985a6e11a49d.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f9b7e1d-4c9c-4cdb-9b7a-985a6e11a49d.png)'
- en: 'The terms in the preceding equation can be explained as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程中的术语可以解释如下：
- en: '*L[cls](p,u)* is the log loss between the ground truth class, *u*, and the
    class probabilities, *p*.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[cls](p,u)* 是实际类别 *u* 与类别概率 *p* 之间的对数损失。'
- en: '*L[loc](t^u, v)* is the same loss as L[reg] in the RPN loss.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[loc](t^u, v)* 与 RPN 损失中的 L[reg] 相同。'
- en: '*λ[u ≥ 1]* is equal to 1 when u ≥ 1 and 0 otherwise.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*λ[u ≥ 1]* 在 u ≥ 1 时等于 1，否则为 0。'
- en: During Fast R-CNN training, we always use a background class with *id = 0*.
    Indeed, the RoIs may contain background regions, and it is important to classify
    them as such. The term *λ[u ≥ 1]* avoids penalizing the bounding box error for
    background boxes. For all the other classes, since *u* will be above *0*, we will
    penalize the error.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fast R-CNN 训练过程中，我们始终使用背景类别，*id = 0*。事实上，RoI 可能包含背景区域，且将其分类为背景是非常重要的。术语 *λ[u
    ≥ 1]* 用于避免惩罚背景框的边界框误差。对于所有其他类别，由于 *u* 将大于 *0*，我们将惩罚误差。
- en: Training regimen
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练方案
- en: 'As described earlier, sharing the weights between the two parts of the network
    allows the model to be faster (as the CNN is only applied once) and lighter. In
    the Faster R-CNN paper, the recommended training procedure is called **4-step
    alternating training**. A simplified version of this procedure goes like this:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在网络的两部分之间共享权重可以使模型更快（因为 CNN 只应用一次）且更轻。在 Faster R-CNN 论文中，推荐的训练过程被称为 **四步交替训练**。该过程的简化版本如下：
- en: Train the RPN so that it predicts acceptable RoIs.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 RPN，使其预测可接受的 RoI。
- en: Train the classification part using the output of the trained RPN. At the end
    of the training, the RPN and the classification part have different convolutional
    weights since they have been trained separately.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的 RPN 输出训练分类部分。在训练结束时，由于 RPN 和分类部分分别训练，它们的卷积权重不同。
- en: Replace the RPN's CNN with the classification's CNN so that they now share convolutional
    weights. Freeze the shared CNN weights. Train the RPN's last layers again.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用分类部分的 CNN 替换 RPN 的 CNN，使它们共享卷积权重。冻结共享的 CNN 权重。重新训练 RPN 的最后几层。
- en: Train the classification's last layer using the output of the RPN again.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用 RPN 的输出训练分类部分的最后一层。
- en: At the end of this process, we obtain a trained network with the two parts sharing
    the convolutional weights.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程结束后，我们得到一个训练好的网络，其中两部分共享卷积权重。
- en: TensorFlow Object Detection API
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 物体检测 API
- en: As Faster R-CNN is always improving, we do not provide a reference implementation
    with this book. Instead, we recommend using the TensorFlow Object Detection API.
    It offers an implementation of Faster R-CNN that's maintained by contributors
    and by the TensorFlow team. It offers pretrained models and code to train your
    own model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Faster R-CNN 始终在改进，我们没有提供本书的参考实现。相反，我们建议使用 TensorFlow 物体检测 API。它提供了一个由贡献者和
    TensorFlow 团队维护的 Faster R-CNN 实现，提供了预训练模型和训练自己模型的代码。
- en: The Object Detection API is not part of the core TensorFlow library, but is
    available in a separate repository, which was introduced in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*: [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测 API 不属于核心 TensorFlow 库的一部分，而是作为一个单独的仓库提供，详细介绍见 [第 4 章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)，*影响力分类工具*：[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)。
- en: Using a pretrained model
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练模型
- en: The object detection API comes with several pretrained models trained on the
    COCO dataset. The models vary in architecture—while they are all based on Faster
    R-CNN, they use different parameters and backbones. This has an impact on inference
    speed and performance. A rule of thumb is that the inference time grows with the
    mean average precision.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测 API 提供了多个在 COCO 数据集上训练的预训练模型。这些模型在架构上有所不同——虽然它们都基于 Faster R-CNN，但使用了不同的参数和骨干网络。这对推理速度和性能产生了影响。一个经验法则是，推理时间随着平均精度的提高而增加。
- en: Training on a custom dataset
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在自定义数据集上训练
- en: It is also possible to train a model to detect objects that are not in the COCO
    dataset. To do so, a large amount of data is needed. In general, it is recommended
    to have at least 1,000 samples per object class. To generate a training set, training
    images need to be manually annotated by drawing the bounding boxes around them.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以训练一个模型来检测不在 COCO 数据集中的物体。为此，需要大量的数据。一般来说，建议每个物体类别至少有 1,000 个样本。为了生成训练集，需要手动标注训练图像，方法是画出物体的边界框。
- en: Using the Object Detection API does not involve writing Python code. Instead,
    the architecture is defined using configuration files. We recommend starting from
    an existing configuration and working from there to obtain good performance. A
    walk-through is available in this chapter's repository.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 使用物体检测 API 不涉及编写 Python 代码。相反，架构是通过配置文件来定义的。我们建议从现有的配置文件开始，并在此基础上进行调整以获得良好的性能。本章的代码库中提供了一个完整的示例。
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We covered the architecture of two object detection models. The first one, YOLO,
    is known for its inference speed. We went through the general architecture and
    how inference works, as well as the training procedure. We also detailed the loss
    used to train the model. The second one, Faster R-CNN, is known for its state-of-the-art
    performance. We analyzed the two stages of the network and how to train them.
    We also described how to use Faster R-CNN through the TensorFlow Object Detection
    API.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了两种物体检测模型的架构。第一个是 YOLO，以推理速度快著称。我们讲解了其整体架构以及推理的工作原理，此外还讲解了训练过程。我们还详细说明了用于训练模型的损失函数。第二个是
    Faster R-CNN，以其最先进的性能著称。我们分析了网络的两个阶段以及如何训练它们。我们还描述了如何通过 TensorFlow 物体检测 API 使用
    Faster R-CNN。
- en: In the next chapter, we will extend object detection further by learning how
    to segment images into meaningful parts, as well as how to transform and enhance
    them.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将通过学习如何将图像分割成有意义的部分，以及如何转换和增强它们，进一步扩展物体检测。
- en: Questions
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between a bounding box, an anchor box, and a ground truth
    box?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边界框、锚框和真实框有什么区别？
- en: What is the role of the feature extractor?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器的作用是什么？
- en: What model should be favored, YOLO or Faster R-CNN?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该偏向使用哪种模型，YOLO 还是 Faster R-CNN？
- en: What does the use of anchor boxes entail?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用锚框意味着什么？
- en: Further reading
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '*Mastering OpenCV 4* ([https://www.packtpub.com/application-development/mastering-opencv-4-third-edition](https://www.packtpub.com/application-development/mastering-opencv-4-third-edition)),byRoy
    Shilkrot and David Millán Escrivá, contains practical computer vision projects,
    including advanced object detection techniques.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精通 OpenCV 4* ([https://www.packtpub.com/application-development/mastering-opencv-4-third-edition](https://www.packtpub.com/application-development/mastering-opencv-4-third-edition))，由
    Roy Shilkrot 和 David Millán Escrivá 编写，包含了实际的计算机视觉项目，包括高级物体检测技术。'
- en: '*OpenCV 4 Computer Vision Application Programming Cookbook* ([https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition](https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition)),
    by David Millán Escrivá and Robert Laganiere, covers classical object descriptors
    as well as object detection concepts.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenCV 4 计算机视觉应用编程实战* ([https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition](https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition))，由David
    Millán Escrivá 和 Robert Laganiere 编写，涵盖了经典的物体描述符以及物体检测概念。'
