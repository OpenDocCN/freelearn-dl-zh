- en: Object Detection Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹æ¨¡å‹
- en: From self-driving cars to content moderation, detecting objects and their position
    in an image is a canonical task in computer vision. In this chapter, we will introduce
    techniques used for **object detection**. We will detail the architecture of two
    of the most prevalent models among the current state of the artâ€”**You Only Look
    Once** (**YOLO**) and **Regions with Convolutional Neural Networks** (**R-CNN**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°å†…å®¹å®¡æŸ¥ï¼Œæ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“åŠå…¶ä½ç½®æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ç»å…¸ä»»åŠ¡ã€‚æœ¬ç« å°†ä»‹ç»ç”¨äº**ç›®æ ‡æ£€æµ‹**çš„æŠ€æœ¯ã€‚æˆ‘ä»¬å°†è¯¦ç»†è¯´æ˜å½“å‰æœ€ä¸»æµçš„ä¸¤ç§æ¨¡å‹æ¶æ„â€”â€”**You
    Only Look Once**ï¼ˆ**YOLO**ï¼‰å’Œ**Regions with Convolutional Neural Networks**ï¼ˆ**R-CNN**ï¼‰ã€‚
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: The history of object detection techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹æŠ€æœ¯çš„å‘å±•å†å²
- en: The main object detection approaches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸»è¦çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•
- en: Implementing fast object detection using YOLO architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨YOLOæ¶æ„å®ç°å¿«é€Ÿç›®æ ‡æ£€æµ‹
- en: Improving object detection using Faster R-CNN architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Faster R-CNNæ¶æ„æé«˜ç›®æ ‡æ£€æµ‹æ•ˆæœ
- en: Using Faster R-CNN with the TensorFlow Object Detection API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Faster R-CNNä¸TensorFlowç›®æ ‡æ£€æµ‹API
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: The code for this chapter is available in the form of notebooks at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„ä»£ç ä»¥ç¬”è®°æœ¬çš„å½¢å¼å¯ä»¥åœ¨[https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter05)è·å–ã€‚
- en: Introducing object detection
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»ç›®æ ‡æ£€æµ‹
- en: Object detection wasÂ briefly introduced in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*. In this section, we will cover its history,
    as well as the core technical concepts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹åœ¨[ç¬¬ä¸€ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ã€Šè®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œã€‹ä¸­åšäº†ç®€è¦ä»‹ç»ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å…¶å†å²ä»¥åŠæ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µã€‚
- en: Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: Object detection, also called **object localization**, is the process of detecting
    objects and their **bounding boxes**Â in an image. A bounding box is the smallest
    rectangle of an image that fully contains an object.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹ï¼Œä¹Ÿç§°ä¸º**ç›®æ ‡å®šä½**ï¼Œæ˜¯æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“åŠå…¶**è¾¹ç•Œæ¡†**çš„è¿‡ç¨‹ã€‚è¾¹ç•Œæ¡†æ˜¯èƒ½å¤Ÿå®Œå…¨åŒ…å«ç‰©ä½“çš„å›¾åƒä¸­æœ€å°çš„çŸ©å½¢ã€‚
- en: A common input for an object detection algorithm is an image. A common output
    is a list of bounding boxes and object classes. For each bounding box, the model
    outputs the corresponding predicted class and its confidence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹ç®—æ³•çš„å¸¸è§è¾“å…¥æ˜¯å›¾åƒã€‚å¸¸è§çš„è¾“å‡ºæ˜¯è¾¹ç•Œæ¡†å’Œç‰©ä½“ç±»åˆ«çš„åˆ—è¡¨ã€‚å¯¹äºæ¯ä¸ªè¾¹ç•Œæ¡†ï¼Œæ¨¡å‹ä¼šè¾“å‡ºå¯¹åº”çš„é¢„æµ‹ç±»åˆ«åŠå…¶ç½®ä¿¡åº¦ã€‚
- en: Applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åº”ç”¨
- en: 'The applications of object detection are numerous and cover many industries.
    For instance, object detection can be used for the following purposes:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹çš„åº”ç”¨å¹¿æ³›ï¼Œæ¶µç›–äº†è®¸å¤šè¡Œä¸šã€‚ä¾‹å¦‚ï¼Œç›®æ ‡æ£€æµ‹å¯ç”¨äºä»¥ä¸‹ç›®çš„ï¼š
- en: In self-driving cars, to locate other vehicles and pedestrians
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼Œç”¨äºå®šä½å…¶ä»–è½¦è¾†å’Œè¡Œäºº
- en: For content moderation, to locate forbidden objects and their respective size
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºå†…å®¹å®¡æŸ¥ï¼Œå®šä½ç¦æ­¢ç‰©ä½“åŠå…¶å¤§å°
- en: In health, to locate tumors or dangerous tissue using radiographs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åŒ»ç–—é¢†åŸŸï¼Œé€šè¿‡æ”¾å°„çº¿å½±åƒå®šä½è‚¿ç˜¤æˆ–å±é™©ç»„ç»‡
- en: In manufacturing, for assembly robots to put together or repair products
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åˆ¶é€ ä¸šä¸­ï¼Œç”¨äºè£…é…æœºå™¨äººç»„è£…æˆ–ä¿®ç†äº§å“
- en: In the security industry, to detect threats or count people
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å®‰å…¨è¡Œä¸šï¼Œç”¨äºæ£€æµ‹å¨èƒæˆ–ç»Ÿè®¡äººæ•°
- en: In wildlife conservation, to monitor an animal population
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¸­ï¼Œç”¨äºç›‘æ§åŠ¨ç‰©ç§ç¾¤
- en: These are just a few examplesâ€”more and more applications are being discovered
    every day as object localization becomes more powerful.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åªæ˜¯å…¶ä¸­çš„ä¸€äº›ä¾‹å­â€”â€”éšç€ç›®æ ‡å®šä½æŠ€æœ¯è¶Šæ¥è¶Šå¼ºå¤§ï¼Œè¶Šæ¥è¶Šå¤šçš„åº”ç”¨æ¯å¤©éƒ½åœ¨è¢«å‘ç°ã€‚
- en: Brief history
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®€è¦å†å²
- en: Historically, object detection relied on a classical computer vision technique:Â **image
    descriptors**. To detect an object, for instance, a bike, you would start with
    several pictures of this object. Descriptors corresponding to the bike would be
    extracted from the image. Those descriptors would represent specific parts of
    the bike. When looking for this object, the algorithm would attempt to find the
    descriptors again in the target images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å†å²ä¸Šï¼Œç›®æ ‡æ£€æµ‹ä¾èµ–äºç»å…¸çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼š**å›¾åƒæè¿°ç¬¦**ã€‚ä¸ºäº†æ£€æµ‹ä¸€ä¸ªç‰©ä½“ï¼Œä¾‹å¦‚ä¸€è¾†è‡ªè¡Œè½¦ï¼Œä½ éœ€è¦ä»å¤šå¼ è¯¥ç‰©ä½“çš„ç…§ç‰‡ä¸­å¼€å§‹ã€‚ç„¶åä»å›¾åƒä¸­æå–ä¸è‡ªè¡Œè½¦ç›¸å…³çš„æè¿°ç¬¦ã€‚è¿™äº›æè¿°ç¬¦è¡¨ç¤ºè‡ªè¡Œè½¦çš„ç‰¹å®šéƒ¨åˆ†ã€‚åœ¨å¯»æ‰¾è¯¥ç‰©ä½“æ—¶ï¼Œç®—æ³•ä¼šå°è¯•åœ¨ç›®æ ‡å›¾åƒä¸­å†æ¬¡æ‰¾åˆ°è¿™äº›æè¿°ç¬¦ã€‚
- en: To locate the bike in the image, the most commonly used technique was the **floating
    window**. Small rectangular areas of the images are examined, one after the other.
    The part with the most matching descriptors would be considered to be the one
    containing the object. Over time, many variations were used.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾åƒä¸­å®šä½è‡ªè¡Œè½¦æ—¶ï¼Œæœ€å¸¸ç”¨çš„æŠ€æœ¯æ˜¯**æµ®åŠ¨çª—å£**ã€‚å›¾åƒçš„å¤šä¸ªå°çŸ©å½¢åŒºåŸŸä¾æ¬¡è¿›è¡Œæ£€æŸ¥ï¼ŒåŒ¹é…æè¿°ç¬¦æœ€å¤šçš„éƒ¨åˆ†å°†è¢«è®¤ä¸ºåŒ…å«ç‰©ä½“ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä½¿ç”¨äº†è®¸å¤šå˜ç§ã€‚
- en: 'This technique presented a few advantages: it was robust to rotation and color
    changes, it did not require a lot of training data, and it worked with most objects.
    However, the level of accuracy was not satisfactory.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æŠ€æœ¯å…·æœ‰ä¸€äº›ä¼˜ç‚¹ï¼šå®ƒå¯¹æ—‹è½¬å’Œé¢œè‰²å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œä¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”é€‚ç”¨äºå¤§å¤šæ•°ç‰©ä½“ã€‚ç„¶è€Œï¼Œå‡†ç¡®åº¦æ°´å¹³å¹¶ä¸ä»¤äººæ»¡æ„ã€‚
- en: While neural networks were already in use in the early 1990s (for detecting
    faces, hands, or text in images), they started outperforming the descriptor techniqueÂ on
    the ImageNet challengeÂ by a very large margin in the early 2010s.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç¥ç»ç½‘ç»œåœ¨1990å¹´ä»£åˆæœŸå°±å·²ç»è¢«ç”¨æ¥æ£€æµ‹å›¾åƒä¸­çš„é¢éƒ¨ã€æ‰‹éƒ¨æˆ–æ–‡æœ¬ï¼Œä½†å®ƒä»¬åœ¨2010å¹´ä»£åˆæœŸå¼€å§‹åœ¨ImageNetæŒ‘æˆ˜èµ›ä¸­å¤§å¹…è¶…è¶Šæè¿°ç¬¦æŠ€æœ¯ã€‚
- en: 'Since then, performance has been improving steadily. Performance refers to
    how good the algorithm is at the following things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£æ—¶èµ·ï¼Œæ€§èƒ½ç¨³æ­¥æé«˜ã€‚æ€§èƒ½æŒ‡çš„æ˜¯ç®—æ³•åœ¨ä»¥ä¸‹æ–¹é¢çš„è¡¨ç°ï¼š
- en: '**Bounding box precision**: Providing the correct bounding box (not too large
    or too narrow)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾¹ç•Œæ¡†ç²¾åº¦**ï¼šæä¾›æ­£ç¡®çš„è¾¹ç•Œæ¡†ï¼ˆæ—¢ä¸å¤ªå¤§ä¹Ÿä¸å¤ªå°ï¼‰'
- en: '**Recall**: Finding all the objects (not missing any objects)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¬å›ç‡**ï¼šæ‰¾åˆ°æ‰€æœ‰ç‰©ä½“ï¼ˆæ²¡æœ‰é—æ¼ä»»ä½•ç‰©ä½“ï¼‰'
- en: '**Class precision**: Outputting the correct class for each object (not mistaking
    a cat for a dog)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ç²¾åº¦**ï¼šä¸ºæ¯ä¸ªç‰©ä½“è¾“å‡ºæ­£ç¡®çš„ç±»åˆ«ï¼ˆé¿å…å°†çŒ«è¯¯è®¤ä¸ºç‹—ï¼‰'
- en: Performance improvement also means that the models are getting faster and faster
    at computing results (for a specific input image size and at a specific computing
    power). While early models took considerable time (more than a few seconds) to
    detect objects, they can now be used in real time. In the context of computer
    vision, real time usually means more than five detections per second.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ€§èƒ½æ”¹è¿›è¿˜æ„å‘³ç€æ¨¡å‹åœ¨è®¡ç®—ç»“æœæ—¶å˜å¾—è¶Šæ¥è¶Šå¿«ï¼ˆé’ˆå¯¹ç‰¹å®šçš„è¾“å…¥å›¾åƒå°ºå¯¸å’Œè®¡ç®—èƒ½åŠ›ï¼‰ã€‚è™½ç„¶æ—©æœŸçš„æ¨¡å‹éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼ˆè¶…è¿‡å‡ ç§’é’Ÿï¼‰æ‰èƒ½æ£€æµ‹åˆ°ç‰©ä½“ï¼Œä½†ç°åœ¨å®ƒä»¬å¯ä»¥å®æ—¶ä½¿ç”¨ã€‚åœ¨è®¡ç®—æœºè§†è§‰çš„èƒŒæ™¯ä¸‹ï¼Œå®æ—¶é€šå¸¸æ„å‘³ç€æ¯ç§’æ£€æµ‹è¶…è¿‡äº”æ¬¡ã€‚
- en: Evaluating the performance of a model
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¨¡å‹æ€§èƒ½
- en: To compare different object detection models, we need common evaluation metrics.
    For a given test set, we run each model and gather its predictions. We use the
    predictions and the ground truth to compute an evaluation metric. In this section,
    we will have a look at the metrics used to evaluate object detection models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ¯”è¾ƒä¸åŒçš„ç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡ã€‚å¯¹äºç»™å®šçš„æµ‹è¯•é›†ï¼Œæˆ‘ä»¬è¿è¡Œæ¯ä¸ªæ¨¡å‹å¹¶æ”¶é›†å…¶é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬ä½¿ç”¨é¢„æµ‹ç»“æœå’ŒçœŸå®å€¼è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ç”¨äºè¯„ä¼°ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æŒ‡æ ‡ã€‚
- en: Precision and recall
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç²¾ç¡®åº¦å’Œå¬å›ç‡
- en: While they are usually not used to evaluate object detection models, **precision**
    and **recall** serve as a basis to compute other metrics. A good understanding
    of precision and recall is, therefore, essential.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å®ƒä»¬é€šå¸¸ä¸ç”¨äºè¯„ä¼°ç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼Œ**ç²¾ç¡®åº¦**å’Œ**å¬å›ç‡**æ˜¯è®¡ç®—å…¶ä»–æŒ‡æ ‡çš„åŸºç¡€ã€‚å› æ­¤ï¼Œç†è§£ç²¾ç¡®åº¦å’Œå¬å›ç‡éå¸¸é‡è¦ã€‚
- en: 'To measure precision and recall, we first need to compute the following for
    each image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¡¡é‡ç²¾ç¡®åº¦å’Œå¬å›ç‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸ºæ¯å¼ å›¾åƒè®¡ç®—ä»¥ä¸‹å†…å®¹ï¼š
- en: '**The number of** **true positives**: **True positives** (**TP**) determine
    how many predictions match with a ground truth box of the same class.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çœŸé˜³æ€§**çš„æ•°é‡ï¼š**çœŸé˜³æ€§**ï¼ˆ**TP**ï¼‰å†³å®šäº†æœ‰å¤šå°‘ä¸ªé¢„æµ‹ä¸åŒç±»çš„çœŸå®æ¡†åŒ¹é…ã€‚'
- en: '**The number of false positives**: **False positives** (**FP**) determine how
    many predictions do notÂ match with a ground truth box of the same class.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‡é˜³æ€§**çš„æ•°é‡ï¼š**å‡é˜³æ€§**ï¼ˆ**FP**ï¼‰å†³å®šäº†æœ‰å¤šå°‘ä¸ªé¢„æµ‹ä¸åŒç±»çš„çœŸå®æ¡†ä¸åŒ¹é…ã€‚'
- en: '**The number of** **false negatives**: **False negatives** (**FN**) determine
    how many ground truths do not have a matching prediction.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‡é˜³æ€§**çš„æ•°é‡ï¼š**å‡é˜³æ€§**ï¼ˆ**FN**ï¼‰å†³å®šäº†æœ‰å¤šå°‘ä¸ªçœŸå®å€¼æ²¡æœ‰åŒ¹é…çš„é¢„æµ‹ã€‚'
- en: 'Then, precision and recall are defined as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œç²¾ç¡®åº¦å’Œå¬å›ç‡çš„å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](img/5464f034-f644-497d-a77f-ab28ca5437ae.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5464f034-f644-497d-a77f-ab28ca5437ae.png)'
- en: Notice that if the predictions exactly match all the ground truths, there will
    not be any false positives or false negatives. Therefore, precision and recall
    will be equal to 1, a perfect score. If a model too often predicts the presence
    of an object based on non-robust features, precision will deteriorate because
    there will be many false positives. On the contrary, if a model is too strict
    and considers an object detected only when precise conditions are met, recall
    will suffer because there will be many false negatives.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœé¢„æµ‹å®Œå…¨åŒ¹é…æ‰€æœ‰çš„å®é™…æ ‡ç­¾ï¼Œåˆ™ä¸ä¼šå‡ºç°å‡é˜³æ€§æˆ–å‡é˜´æ€§ã€‚å› æ­¤ï¼Œå‡†ç¡®ç‡å’Œå¬å›ç‡éƒ½å°†ç­‰äº1ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œç¾çš„åˆ†æ•°ã€‚å¦‚æœæ¨¡å‹åŸºäºä¸ç¨³å¥çš„ç‰¹å¾é¢‘ç¹é¢„æµ‹ç‰©ä½“çš„å­˜åœ¨ï¼Œå‡†ç¡®ç‡å°†ä¸‹é™ï¼Œå› ä¸ºä¼šæœ‰å¾ˆå¤šå‡é˜³æ€§ã€‚ç›¸åï¼Œå¦‚æœæ¨¡å‹è¿‡äºä¸¥æ ¼ï¼Œåªæœ‰åœ¨æ»¡è¶³ç²¾ç¡®æ¡ä»¶æ—¶æ‰ä¼šè®¤ä¸ºç‰©ä½“è¢«æ£€æµ‹åˆ°ï¼Œå¬å›ç‡å°†ä¸‹é™ï¼Œå› ä¸ºä¼šæœ‰å¾ˆå¤šå‡é˜´æ€§ã€‚
- en: Precision-recall curve
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç²¾ç¡®å¬å›æ›²çº¿
- en: '**Precision-recall curve** is used in many machine learning problems. The general
    idea is to visualize the precision and the recall of the model at each **threshold
    of confidence**. With every bounding box, our model will output a confidenceâ€”a
    number between 0 and 1 characterizing how confident the model is that a prediction
    is correct.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç²¾ç¡®å¬å›æ›²çº¿**åœ¨è®¸å¤šæœºå™¨å­¦ä¹ é—®é¢˜ä¸­éƒ½æœ‰åº”ç”¨ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯å¯è§†åŒ–æ¨¡å‹åœ¨æ¯ä¸ª**ç½®ä¿¡åº¦é˜ˆå€¼**ä¸‹çš„å‡†ç¡®ç‡å’Œå¬å›ç‡ã€‚å¯¹äºæ¯ä¸ªè¾¹ç•Œæ¡†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼šè¾“å‡ºä¸€ä¸ªç½®ä¿¡åº¦â€”â€”è¿™æ˜¯ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹é¢„æµ‹æ­£ç¡®æ€§çš„ä¿¡å¿ƒã€‚'
- en: Because we do not want to keep the less confident predictions, we usually remove
    those below a certain threshold, ğ‘‡. For instance, if ğ‘‡ = 0.4, we will not consider
    any prediction with a confidence below this number.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›ä¿ç•™é‚£äº›ä¿¡å¿ƒè¾ƒä½çš„é¢„æµ‹ï¼Œé€šå¸¸ä¼šç§»é™¤ä½äºæŸä¸ªé˜ˆå€¼çš„é¢„æµ‹ï¼Œğ‘‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœğ‘‡ = 0.4ï¼Œæˆ‘ä»¬å°†ä¸è€ƒè™‘ä»»ä½•ç½®ä¿¡åº¦ä½äºæ­¤æ•°å€¼çš„é¢„æµ‹ã€‚
- en: 'Moving the threshold has an impact on precision and on recall:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç§»åŠ¨é˜ˆå€¼ä¼šå¯¹å‡†ç¡®ç‡å’Œå¬å›ç‡äº§ç”Ÿå½±å“ï¼š
- en: '**If TÂ is close to 1**: Precision will be high, but the recall will be low.
    As we filter out many objects, we miss a lot of themâ€”recall shrinks. As we only
    keep confident predictions, we do not have many false positivesâ€”precision rises.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¦‚æœ T æ¥è¿‘ 1**ï¼šå‡†ç¡®ç‡ä¼šå¾ˆé«˜ï¼Œä½†å¬å›ç‡ä¼šå¾ˆä½ã€‚ç”±äºæˆ‘ä»¬ç­›é™¤äº†å¾ˆå¤šå¯¹è±¡ï¼Œé”™è¿‡äº†å¾ˆå¤šå¯¹è±¡â€”â€”å¬å›ç‡ä¸‹é™ã€‚ç”±äºæˆ‘ä»¬åªä¿ç•™æœ‰ä¿¡å¿ƒçš„é¢„æµ‹ï¼Œå› æ­¤æ²¡æœ‰å¤ªå¤šå‡é˜³æ€§â€”â€”å‡†ç¡®ç‡ä¸Šå‡ã€‚'
- en: '**If TÂ is close to 0**: Precision will be low, but the recall will be high.
    As we keep most predictions, we will not have any false negativesâ€”recall rises.
    As our model is less confident in its predictions, we will have many false positivesâ€”precision
    shrinks.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¦‚æœ T æ¥è¿‘ 0**ï¼šå‡†ç¡®ç‡ä¼šå¾ˆä½ï¼Œä½†å¬å›ç‡ä¼šå¾ˆé«˜ã€‚ç”±äºæˆ‘ä»¬ä¿ç•™äº†å¤§éƒ¨åˆ†é¢„æµ‹ï¼Œå› æ­¤ä¸ä¼šæœ‰å‡é˜´æ€§â€”â€”å¬å›ç‡ä¸Šå‡ã€‚ç”±äºæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ä¿¡å¿ƒè¾ƒä½ï¼Œæˆ‘ä»¬ä¼šæœ‰å¾ˆå¤šå‡é˜³æ€§â€”â€”å‡†ç¡®ç‡ä¸‹é™ã€‚'
- en: 'By computing the precision and the recall at each threshold value between 0
    and 1, we can obtain a precision-recall curve, as shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¡ç®—åœ¨0åˆ°1ä¹‹é—´æ¯ä¸ªé˜ˆå€¼ä¸‹çš„å‡†ç¡®ç‡å’Œå¬å›ç‡ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ç²¾ç¡®å¬å›æ›²çº¿ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/2a65d1ce-2626-4dfb-b798-106009cd4eaf.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a65d1ce-2626-4dfb-b798-106009cd4eaf.png)'
- en: Figure 5.1:Â Precision-Recall curve
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.1ï¼šç²¾ç¡®å¬å›æ›²çº¿
- en: Choosing a threshold is a trade-off between accuracy and recall. If a model
    is detecting pedestrians, we will pick a high recall in order not to miss any
    passers-by, even if it means stopping the car for no valid reason from time to
    time. If a model is detecting investment opportunities, we will pick a high precision
    to avoid choosing the wrong opportunities, even if it means missing some.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªé˜ˆå€¼æ˜¯åœ¨å‡†ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´çš„æƒè¡¡ã€‚å¦‚æœæ¨¡å‹æ£€æµ‹è¡Œäººï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©ä¸€ä¸ªè¾ƒé«˜çš„å¬å›ç‡ï¼Œä»¥ç¡®ä¿ä¸ä¼šé”™è¿‡ä»»ä½•è·¯äººï¼Œå³ä½¿è¿™æ„å‘³ç€æœ‰æ—¶è½¦è¾†ä¼šå› æ²¡æœ‰æœ‰æ•ˆç†ç”±è€Œåœä¸‹ã€‚å¦‚æœæ¨¡å‹æ£€æµ‹æŠ•èµ„æœºä¼šï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œä»¥é¿å…é€‰æ‹©é”™è¯¯çš„æœºä¼šï¼Œå³ä½¿è¿™æ„å‘³ç€é”™è¿‡ä¸€äº›æœºä¼šã€‚
- en: Average precision and mean average precision
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹³å‡å‡†ç¡®ç‡å’Œå‡å€¼å¹³å‡å‡†ç¡®ç‡
- en: While the precision-recall curve can tell us a lot about the model, it is often
    more convenient to have a single number. **Average precision** (**AP**) corresponds
    to the area under the curve. Since it is always containedÂ in a one-by-one rectangle,
    AP is always between 0 and 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç²¾ç¡®å¬å›æ›²çº¿èƒ½å‘Šè¯‰æˆ‘ä»¬å¾ˆå¤šå…³äºæ¨¡å‹çš„ä¿¡æ¯ï¼Œä½†é€šå¸¸æ›´æ–¹ä¾¿çš„æ˜¯æ‹¥æœ‰ä¸€ä¸ªå•ä¸€çš„æ•°å­—ã€‚**å¹³å‡å‡†ç¡®ç‡**ï¼ˆ**AP**ï¼‰å¯¹åº”äºæ›²çº¿ä¸‹çš„é¢ç§¯ã€‚ç”±äºå®ƒå§‹ç»ˆåŒ…å«åœ¨ä¸€ä¸ª1Ã—1çš„çŸ©å½¢å†…ï¼Œå› æ­¤APçš„å€¼å§‹ç»ˆä»‹äº0å’Œ1ä¹‹é—´ã€‚
- en: Average precision gives information about the performance of a model for a single
    class. To get a global score, we useÂ **mean Average Precision** (**mAP**). This
    corresponds to the mean of the average precision for each class. If the dataset
    has 10 classes, we will compute the average precision for each class and take
    the average of those numbers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³å‡å‡†ç¡®ç‡æä¾›äº†æ¨¡å‹åœ¨å•ä¸ªç±»åˆ«ä¸Šçš„æ€§èƒ½ä¿¡æ¯ã€‚ä¸ºäº†è·å¾—å…¨å±€è¯„åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨**å‡å€¼å¹³å‡å‡†ç¡®ç‡**ï¼ˆ**mAP**ï¼‰ã€‚è¿™å¯¹åº”äºæ¯ä¸ªç±»åˆ«çš„å¹³å‡å‡†ç¡®ç‡çš„å¹³å‡å€¼ã€‚å¦‚æœæ•°æ®é›†æœ‰10ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬å°†è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶å–è¿™äº›æ•°å€¼çš„å¹³å‡å€¼ã€‚
- en: Mean average precision is used in at least two object detection challengesâ€”**PASCAL
    Visual Object Classes** (usually referred to as **Pascal VOC**), and **Common
    Objects in Context** (usually referred to as **COCO**). The latter is larger and
    contains more classes; therefore, the scores obtained are usually lower than for
    the former.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰è‡³å°‘åœ¨ä¸¤ä¸ªç›®æ ‡æ£€æµ‹æŒ‘æˆ˜ä¸­ä½¿ç”¨â€”â€”**PASCAL Visual Object Classes**ï¼ˆé€šå¸¸ç§°ä¸º**Pascal VOC**ï¼‰å’Œ**Common
    Objects in Context**ï¼ˆé€šå¸¸ç§°ä¸º**COCO**ï¼‰ã€‚åè€…è§„æ¨¡æ›´å¤§ï¼ŒåŒ…å«çš„ç±»åˆ«æ›´å¤šï¼›å› æ­¤ï¼Œé€šå¸¸å¾—åˆ°çš„åˆ†æ•°æ¯”å‰è€…ä½ã€‚
- en: Average precision threshold
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹³å‡ç²¾åº¦é˜ˆå€¼
- en: 'We mentioned earlier that true and false positives were defined by the number
    of predictions matching or not matching the ground truth boxes. However, how do
    you decide when a prediction and the ground truth are matching? A common metric
    is the **Jaccard index**, which measures how well two sets overlap (in our case,
    the sets of pixels represented by the boxes). Also known as **Intersection over
    Union** (**IoU**), it is defined as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡ï¼ŒçœŸé˜³æ€§å’Œå‡é˜³æ€§æ˜¯é€šè¿‡é¢„æµ‹ä¸çœŸå®æ¡†æ˜¯å¦åŒ¹é…æ¥å®šä¹‰çš„ã€‚ç„¶è€Œï¼Œå¦‚ä½•å†³å®šé¢„æµ‹å’ŒçœŸå®æ¡†æ˜¯å¦åŒ¹é…å‘¢ï¼Ÿä¸€ä¸ªå¸¸è§çš„æŒ‡æ ‡æ˜¯**JaccardæŒ‡æ•°**ï¼Œå®ƒè¡¡é‡ä¸¤ä¸ªé›†åˆé‡å çš„ç¨‹åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå°±æ˜¯ç”±æ¡†è¡¨ç¤ºçš„åƒç´ é›†åˆï¼‰ã€‚å®ƒä¹Ÿè¢«ç§°ä¸º**äº¤é›†ä¸å¹¶é›†çš„æ¯”ç‡**ï¼ˆ**IoU**ï¼‰ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](img/7c274090-cac1-480c-b08a-93c2adb684e7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c274090-cac1-480c-b08a-93c2adb684e7.png)'
- en: '|ğ´| and |ğµ| are the **cardinality** of each set; that is, the number of elements
    they each contain. ğ´Â â‹‚ ğµ is the intersection of the two sets, and therefore the
    numerator |ğ´Â â‹‚ ğµ| represents the number of elements they have in common. Similarly,
    ğ´ â‹ƒ ğµ is the union of the sets (as seen in the following diagram), and therefore
    the denominator |ğ´ â‹ƒ ğµ| represents the total number of elements the two sets cover
    together:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '|ğ´| å’Œ |ğµ| æ˜¯æ¯ä¸ªé›†åˆçš„**åŸºæ•°**ï¼›å³å®ƒä»¬å„è‡ªåŒ…å«çš„å…ƒç´ æ•°é‡ã€‚ğ´Â â‹‚ ğµ æ˜¯ä¸¤ä¸ªé›†åˆçš„äº¤é›†ï¼Œå› æ­¤åˆ†å­ |ğ´Â â‹‚ ğµ| ä»£è¡¨å®ƒä»¬å…±æœ‰çš„å…ƒç´ æ•°é‡ã€‚ç±»ä¼¼åœ°ï¼Œğ´
    â‹ƒ ğµ æ˜¯é›†åˆçš„å¹¶é›†ï¼ˆå¦‚ä¸‹é¢çš„å›¾ç¤ºæ‰€ç¤ºï¼‰ï¼Œå› æ­¤åˆ†æ¯ |ğ´ â‹ƒ ğµ| ä»£è¡¨ä¸¤ä¸ªé›†åˆæ€»å…±è¦†ç›–çš„å…ƒç´ æ•°é‡ï¼š'
- en: '![](img/d55b879f-d640-4d10-a5eb-5af732559f3a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d55b879f-d640-4d10-a5eb-5af732559f3a.png)'
- en: Figure 5.2:Â Intersection and union of boxes illustrated
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.2ï¼šæ¡†çš„äº¤é›†ä¸å¹¶é›†ç¤ºæ„å›¾
- en: Why compute such a fraction and not just use the intersection? While the intersection
    would provide a good indicator of how much two sets/boxes overlap, this value
    is absolute and not relative. Therefore, two big boxes would probably overlap
    by many more pixels than two small boxes. This is why this ratio is usedâ€”it will
    always be between 0 (if the two boxes do not overlap) and 1 (if two boxes overlap
    completely).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦è®¡ç®—è¿™æ ·çš„æ¯”ç‡ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨äº¤é›†å‘¢ï¼Ÿè™½ç„¶äº¤é›†å¯ä»¥å¾ˆå¥½åœ°æŒ‡ç¤ºä¸¤ä¸ªé›†åˆ/æ¡†çš„é‡å ç¨‹åº¦ï¼Œä½†è¿™ä¸ªå€¼æ˜¯ç»å¯¹çš„ï¼Œè€Œéç›¸å¯¹çš„ã€‚å› æ­¤ï¼Œä¸¤ä¸ªå¤§æ¡†å¯èƒ½ä¼šæ¯”ä¸¤ä¸ªå°æ¡†é‡å æ›´å¤šçš„åƒç´ ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è¿™ä¸ªæ¯”ç‡â€”â€”å®ƒçš„å€¼æ€»æ˜¯ä»‹äº0ï¼ˆå¦‚æœä¸¤ä¸ªæ¡†æ²¡æœ‰é‡å ï¼‰å’Œ1ï¼ˆå¦‚æœä¸¤ä¸ªæ¡†å®Œå…¨é‡å ï¼‰ä¹‹é—´ã€‚
- en: When computing the average precision, we say that two boxes overlap if their
    IoU is above a certain threshold. The threshold usually chosen is *0.5*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å¹³å‡ç²¾åº¦æ—¶ï¼Œæˆ‘ä»¬è¯´ä¸¤ä¸ªæ¡†é‡å ï¼Œå½“å®ƒä»¬çš„IoUè¶…è¿‡æŸä¸ªé˜ˆå€¼æ—¶ã€‚é€šå¸¸é€‰æ‹©çš„é˜ˆå€¼æ˜¯*0.5*ã€‚
- en: For the Pascal VOC challenge, 0.5 is also usedâ€”we say that we use mAP@0.5 (pronounced
    *mAP* *at 0.5*). For the COCO challenge, a slightly different metric is usedâ€”mAP@[0.5:0.95].
    This means that we compute mAP@0.5, mAP@0.55, ..., *mAP*@0.95,Â and take the average.
    Averaging over IoUs rewards models with better localization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºPascal VOCæŒ‘æˆ˜ï¼Œ0.5ä¹Ÿè¢«ä½¿ç”¨â€”â€”æˆ‘ä»¬è¯´ä½¿ç”¨çš„æ˜¯mAP@0.5ï¼ˆè¯»ä½œ*mAP* *at 0.5*ï¼‰ã€‚å¯¹äºCOCOæŒ‘æˆ˜ï¼Œä½¿ç”¨ç•¥æœ‰ä¸åŒçš„æŒ‡æ ‡â€”â€”mAP@[0.5:0.95]ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬è®¡ç®—mAP@0.5ï¼ŒmAP@0.55ï¼Œ...ï¼Œ*mAP*@0.95ï¼Œå¹¶å–å…¶å¹³å‡å€¼ã€‚å¯¹IoUè¿›è¡Œå¹³å‡ä¼šå¥–åŠ±å®šä½æ›´ç²¾ç¡®çš„æ¨¡å‹ã€‚
- en: A fast object detection algorithm â€“ YOLO
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¿«é€Ÿçš„ç›®æ ‡æ£€æµ‹ç®—æ³•â€”â€”YOLO
- en: While the acronym may make you smile, YOLO is one of the fastest object detection
    algorithms available. The latest version, YOLOv3, can run at more than 170 **frames
    per second** (**FPS**) on a modern GPU for an image size of *256Â *Ã— *256*. In
    this section, we will introduce the theoretical concept behind its architecture.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ä¸ªç¼©å†™å¯èƒ½ä¼šè®©ä½ ä¼šå¿ƒä¸€ç¬‘ï¼Œä½†YOLOæ˜¯ç›®å‰æœ€å¿«çš„ç›®æ ‡æ£€æµ‹ç®—æ³•ä¹‹ä¸€ã€‚æœ€æ–°ç‰ˆæœ¬YOLOv3åœ¨ç°ä»£GPUä¸Šï¼Œå¯¹äº*256Ã—256*çš„å›¾åƒå¤§å°ï¼Œå¯ä»¥ä»¥æ¯ç§’è¶…è¿‡170å¸§çš„é€Ÿåº¦è¿è¡Œï¼ˆ**FPS**ï¼‰ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å…¶æ¶æ„èƒŒåçš„ç†è®ºæ¦‚å¿µã€‚
- en: Introducing YOLO
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»YOLO
- en: 'First released in 2015, YOLO outperformed almost all other object detection
    architectures, both in terms of speed and accuracy. Since then, the architecture
    has been improved several times. In this chapter, we will draw our knowledge from
    the following three papers:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOé¦–æ¬¡å‘å¸ƒäº2015å¹´ï¼Œåœ¨é€Ÿåº¦å’Œç²¾åº¦ä¸Šè¶…è¶Šäº†å‡ ä¹æ‰€æœ‰å…¶ä»–ç›®æ ‡æ£€æµ‹æ¶æ„ã€‚æ­¤åï¼Œè¯¥æ¶æ„å·²è¢«å¤šæ¬¡æ”¹è¿›ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å€Ÿé‰´ä»¥ä¸‹ä¸‰ç¯‡è®ºæ–‡çš„å†…å®¹ï¼š
- en: '*You Only Look Once: Unified, real-time object detection (2015)*, Joseph Redmon,
    Santosh Divvala, Ross Girshick, and Ali Farhadi'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*You Only Look Once: ç»Ÿä¸€çš„å®æ—¶ç›®æ ‡æ£€æµ‹ï¼ˆ2015ï¼‰*ï¼ŒJoseph Redmonã€Santosh Divvalaã€Ross Girshick
    å’Œ Ali Farhadi'
- en: '*YOLO9000: Better, Faster, Stronger (2016)*, Joseph Redmon and Ali Farhadi'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLO9000ï¼šæ›´å¥½ã€æ›´å¿«ã€æ›´å¼ºï¼ˆ2016ï¼‰*ï¼ŒJoseph Redmon å’Œ Ali Farhadi'
- en: '*YOLOv3: An Incremental Improvement (2018)*, Joseph Redmon and Ali Farhadi'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv3ï¼šä¸€ä¸ªæ¸è¿›çš„æ”¹è¿›ï¼ˆ2018ï¼‰*ï¼ŒJoseph Redmon å’Œ Ali Farhadi'
- en: For the sake of clarity and simplicity, we will not describe all the small details
    that allow YOLO to reach its maximum performance. Instead, we will focus on the
    general architecture of the network. We'll provide an implementation of YOLO so
    that you can compare our architecture with code. It is available in the chapter's
    repository.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ™°ç®€æ´ï¼Œæˆ‘ä»¬ä¸ä¼šæè¿°æ‰€æœ‰ç»†èŠ‚ï¼Œæ¥é˜è¿° YOLO å¦‚ä½•è¾¾åˆ°å…¶æœ€å¤§æ€§èƒ½ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºç½‘ç»œçš„æ€»ä½“æ¶æ„ã€‚æˆ‘ä»¬å°†æä¾› YOLO çš„å®ç°ï¼Œä»¥ä¾¿ä½ å¯ä»¥å°†æˆ‘ä»¬çš„æ¶æ„ä¸ä»£ç è¿›è¡Œæ¯”è¾ƒã€‚å®ƒå¯ä»¥åœ¨æœ¬ç« çš„ä»£ç åº“ä¸­æ‰¾åˆ°ã€‚
- en: This implementation has been designed to be easy to read and understand. We
    invite those readers who wish to acquire a deep understanding of the architecture
    to first read this chapter and then refer to the original papers and the implementation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å®ç°å·²è¢«è®¾è®¡ä¸ºæ˜“äºé˜…è¯»å’Œç†è§£ã€‚æˆ‘ä»¬é‚€è¯·é‚£äº›å¸Œæœ›æ·±å…¥ç†è§£æ¶æ„çš„è¯»è€…ï¼Œå…ˆé˜…è¯»æœ¬ç« å†…å®¹ï¼Œç„¶åå‚è€ƒåŸå§‹è®ºæ–‡å’Œå®ç°ã€‚
- en: The main author of the YOLO paper maintains a deep learning framework called
    **Darknet** ([https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)).
    This hosts the official implementation of YOLO and can be used to reproduce the
    paper's results. It is coded in C++ and is not based on TensorFlow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO è®ºæ–‡çš„ä¸»è¦ä½œè€…ç»´æŠ¤äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå«åš **Darknet**ï¼ˆ[https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)ï¼‰ã€‚è¿™ä¸ªæ¡†æ¶åŒ…å«äº†
    YOLO çš„å®˜æ–¹å®ç°ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºå¤ç°è®ºæ–‡ä¸­çš„ç»“æœã€‚å®ƒæ˜¯ç”¨ C++ ç¼–å†™çš„ï¼Œå¹¶ä¸”æ²¡æœ‰åŸºäº TensorFlowã€‚
- en: Strengths and limitations of YOLO
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO çš„ä¼˜ç¼ºç‚¹
- en: YOLO is known for its speed. However, it has been recently outperformed in terms
    of accuracy by **Faster R-CNN** (covered later in this chapter). Moreover, due
    to the way it detects objects, YOLO struggles with smaller objects. For instance,
    it would have trouble detecting single birds from a flock. As with most deep learning
    models, it also struggles to properly detect objects that deviate too much from
    the training set (unusual aspect ratios or appearance). Nevertheless, the architecture
    is constantly evolving, and those issues are being worked on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO ä»¥å…¶é€Ÿåº¦è€Œé—»åã€‚ç„¶è€Œï¼Œæœ€è¿‘åœ¨å‡†ç¡®æ€§æ–¹é¢è¢« **Faster R-CNN**ï¼ˆå°†åœ¨æœ¬ç« åé¢ä»‹ç»ï¼‰æ‰€è¶…è¶Šã€‚æ­¤å¤–ï¼Œç”±äº YOLO æ£€æµ‹ç‰©ä½“çš„æ–¹å¼ï¼Œå®ƒåœ¨å¤„ç†å°ç‰©ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯èƒ½å¾ˆéš¾ä»ä¸€ç¾¤é¸Ÿä¸­æ£€æµ‹å‡ºå•åªé¸Ÿã€‚ä¸å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œå®ƒä¹Ÿå¾ˆéš¾æ­£ç¡®æ£€æµ‹å‡ºä¸è®­ç»ƒé›†åå·®è¾ƒå¤§çš„ç‰©ä½“ï¼ˆä¾‹å¦‚ä¸å¯»å¸¸çš„é•¿å®½æ¯”æˆ–å¤–è§‚ï¼‰ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ¶æ„åœ¨ä¸æ–­æ¼”è¿›ï¼Œç›¸å…³é—®é¢˜æ­£åœ¨å¾—åˆ°è§£å†³ã€‚
- en: YOLO's main concepts
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOçš„ä¸»è¦æ¦‚å¿µ
- en: 'The core idea of YOLO is this:Â **reframing object detection as a single regression
    problem**. What does this mean? Instead of using a sliding window or another complex
    technique, we will divide the input into a *wÂ Ã— h* grid, as represented in this
    diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å°†ç›®æ ‡æ£€æµ‹é‡æ–°æ¡†å®šä¸ºä¸€ä¸ªå•ä¸€çš„å›å½’é—®é¢˜**ã€‚è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿä¸å…¶ä½¿ç”¨æ»‘åŠ¨çª—å£æˆ–å…¶ä»–å¤æ‚æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†æŠŠè¾“å…¥åˆ’åˆ†ä¸ºä¸€ä¸ª *w Ã—
    h* çš„ç½‘æ ¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](img/266bcb07-9913-4888-a3d2-8a2bfcab7b6c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/266bcb07-9913-4888-a3d2-8a2bfcab7b6c.png)'
- en: 'Figure 5.3: An example involving a plane taking off. Here, w = 5, h = 5, and
    B = 2, meaning, in total, 5Â Ã— 5Â Ã— 2 = 50 potential boxes, but only 2 are shown
    in the image'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.3ï¼šä¸€ä¸ªæ¶‰åŠé£æœºèµ·é£çš„ç¤ºä¾‹ã€‚è¿™é‡Œï¼Œw = 5ï¼Œh = 5ï¼ŒB = 2ï¼Œè¿™æ„å‘³ç€æ€»å…±æœ‰ 5 Ã— 5 Ã— 2 = 50 ä¸ªæ½œåœ¨çš„æ¡†ï¼Œä½†å›¾åƒä¸­ä»…æ˜¾ç¤ºäº†
    2 ä¸ªæ¡†
- en: 'For each part of the grid, we will define *B* bounding boxes. Then, our only
    task will be to predict the following for each bounding box:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç½‘æ ¼çš„æ¯ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®šä¹‰*B*ä¸ªè¾¹ç•Œæ¡†ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„å”¯ä¸€ä»»åŠ¡å°±æ˜¯é¢„æµ‹æ¯ä¸ªè¾¹ç•Œæ¡†çš„ä»¥ä¸‹å†…å®¹ï¼š
- en: The center of the box
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›’å­çš„ä¸­å¿ƒ
- en: The width and height of the box
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›’å­çš„å®½åº¦å’Œé«˜åº¦
- en: The probability that this box contains an object
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¡†åŒ…å«äº†ä¸€ä¸ªç‰©ä½“çš„æ¦‚ç‡
- en: The class of said object
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥ç‰©ä½“çš„ç±»åˆ«
- en: Since all those predictions are numbers, we have therefore transformed the object
    detection problem into a regression problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ‰€æœ‰è¿™äº›é¢„æµ‹éƒ½æ˜¯æ•°å€¼ï¼Œæˆ‘ä»¬å› æ­¤å°†ç›®æ ‡æ£€æµ‹é—®é¢˜è½¬åŒ–ä¸ºå›å½’é—®é¢˜ã€‚
- en: It is important to make a distinction between the grid cells that divide the
    pictures into equal parts (*w* Ã— *h* parts to be precise) and the bounding boxes
    that will locate the objects. Each grid cell contains *B* bounding boxes. Therefore,
    there will be *w* Ã— *h* Ã— *B* possible bounding boxes in the end.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯è¦åŒºåˆ†ç½‘æ ¼å•å…ƒï¼ˆå°†å›¾åƒåˆ†æˆç­‰ä»½çš„éƒ¨åˆ†ï¼Œå‡†ç¡®æ¥è¯´æ˜¯ *w* Ã— *h* éƒ¨åˆ†ï¼‰ä¸å®šä½ç‰©ä½“çš„è¾¹ç•Œæ¡†ã€‚æ¯ä¸ªç½‘æ ¼å•å…ƒåŒ…å« *B* ä¸ªè¾¹ç•Œæ¡†ã€‚å› æ­¤ï¼Œæœ€ç»ˆä¼šæœ‰
    *w* Ã— *h* Ã— *B* ä¸ªå¯èƒ½çš„è¾¹ç•Œæ¡†ã€‚
- en: In practice, the concepts used by YOLO are a bit more complex than this. What
    if there are several objects in one part of the grid? What if an object overlaps
    several parts of the grid? More importantly, how do we choose a loss to train
    our model? We will now have a deeper look at YOLO architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒYOLOä½¿ç”¨çš„æ¦‚å¿µæ¯”è¿™ä¸ªç¨å¾®å¤æ‚ã€‚å‡è®¾ç½‘æ ¼çš„ä¸€éƒ¨åˆ†æœ‰å¤šä¸ªç‰©ä½“æ€ä¹ˆåŠï¼Ÿå¦‚æœä¸€ä¸ªç‰©ä½“è·¨è¶Šå¤šä¸ªç½‘æ ¼éƒ¨åˆ†æ€ä¹ˆåŠï¼Ÿæ›´é‡è¦çš„æ˜¯ï¼Œå¦‚ä½•é€‰æ‹©ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Ÿæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·±å…¥äº†è§£YOLOæ¶æ„ã€‚
- en: Inferring with YOLO
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨YOLOè¿›è¡Œæ¨ç†
- en: Because the architecture of the model can be quiteÂ hard to understand in one
    go, we will split the model into two partsâ€”inference and training. **Inference**
    is the process of taking an image input and computing results. **Training** is
    the process of learning the weights of the model. When implementing a model from
    scratch, inference cannot be used before the model is trained. But, for the sake
    of simplicity, we are going to start with inference.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¨¡å‹çš„æ¶æ„å¯èƒ½å¾ˆéš¾ä¸€æ¬¡æ€§ç†è§£ï¼Œæˆ‘ä»¬å°†æŠŠæ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†â€”â€”æ¨ç†å’Œè®­ç»ƒã€‚**æ¨ç†**æ˜¯å°†å›¾åƒè¾“å…¥å¹¶è®¡ç®—ç»“æœçš„è¿‡ç¨‹ã€‚**è®­ç»ƒ**æ˜¯å­¦ä¹ æ¨¡å‹æƒé‡çš„è¿‡ç¨‹ã€‚åœ¨ä»å¤´å®ç°æ¨¡å‹æ—¶ï¼Œæ¨ç†åœ¨æ¨¡å‹è®­ç»ƒä¹‹å‰æ— æ³•ä½¿ç”¨ã€‚ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†ä»æ¨ç†å¼€å§‹ã€‚
- en: The YOLO backbone
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOä¸»å¹²æ¨¡å‹
- en: 'Like most image detection models, YOLO is based on a **backbone model**. The
    role of this model is to extract meaningful features from the image that will
    be used by the final layers. This is why the backbone is also called theÂ **feature
    extractor**, a concept introduced inÂ [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*. The general YOLO architecture is depicted
    here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¤§å¤šæ•°å›¾åƒæ£€æµ‹æ¨¡å‹ä¸€æ ·ï¼ŒYOLOåŸºäº**ä¸»å¹²æ¨¡å‹**ã€‚è¯¥æ¨¡å‹çš„ä½œç”¨æ˜¯ä»å›¾åƒä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œä¾›æœ€ç»ˆçš„å±‚ä½¿ç”¨ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä¸»å¹²æ¨¡å‹è¢«ç§°ä¸º**ç‰¹å¾æå–å™¨**ï¼Œè¿™ä¸€æ¦‚å¿µåœ¨[ç¬¬4ç« ](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)ã€Šæœ‰å½±å“åŠ›çš„åˆ†ç±»å·¥å…·ã€‹ä¸­ä»‹ç»ã€‚YOLOçš„æ€»ä½“æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](img/18e34cb2-6234-4212-8ca4-d06843d049df.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18e34cb2-6234-4212-8ca4-d06843d049df.png)'
- en: Figure 5.4:Â YOLO architecture summarized. Note that the backbone is exchangeable
    and that its architecture may vary
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.4ï¼šYOLOæ¶æ„æ€»ç»“ã€‚è¯·æ³¨æ„ï¼Œä¸»å¹²æ¨¡å‹æ˜¯å¯äº¤æ¢çš„ï¼Œå…¶æ¶æ„å¯èƒ½æœ‰æ‰€ä¸åŒã€‚
- en: While any architecture can be chosen as a feature extractor, the YOLO paper
    employs a custom architecture. The performance of the final model depends heavily
    on the choice of the feature extractor's architecture.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å¯ä»¥é€‰æ‹©ä»»ä½•æ¶æ„ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œä½†YOLOè®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰æ¶æ„ã€‚æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç‰¹å¾æå–å™¨æ¶æ„çš„é€‰æ‹©ã€‚
- en: The final layer of the backbone outputs a feature volume of size *w* Ã— *h* Ã—
    *D*, where *w* Ã— *h* is the size of the grid and *D* is the depth of the feature
    volume. For instance, for VGG-16, *D = 512*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»å¹²æ¨¡å‹çš„æœ€ç»ˆå±‚è¾“å‡ºä¸€ä¸ªå¤§å°ä¸º*w* Ã— *h* Ã— *D*çš„ç‰¹å¾ä½“ç§¯ï¼Œå…¶ä¸­*w* Ã— *h*æ˜¯ç½‘æ ¼çš„å¤§å°ï¼Œ*D*æ˜¯ç‰¹å¾ä½“ç§¯çš„æ·±åº¦ã€‚ä¾‹å¦‚ï¼Œå¯¹äºVGG-16ï¼Œ*D
    = 512*ã€‚
- en: 'The size of the grid, *w* Ã— *h*, depends on two factors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘æ ¼çš„å¤§å°ï¼Œ*w* Ã— *h*ï¼Œå–å†³äºä¸¤ä¸ªå› ç´ ï¼š
- en: '**The stride of the complete feature extractor**: For VGG-16, the stride is
    16, meaning that the feature volume output will be 16 times smaller than the input
    image.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®Œæ•´ç‰¹å¾æå–å™¨çš„æ­¥å¹…**ï¼šå¯¹äºVGG-16ï¼Œæ­¥å¹…ä¸º16ï¼Œæ„å‘³ç€ç‰¹å¾ä½“ç§¯è¾“å‡ºå°†æ˜¯è¾“å…¥å›¾åƒçš„1/16å¤§å°ã€‚'
- en: '**The size of the input image:** Since the feature volume''s size is proportional
    to the size of the image, the smaller the input, the smaller the grid.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥å›¾åƒçš„å¤§å°**ï¼šç”±äºç‰¹å¾ä½“ç§¯çš„å¤§å°ä¸å›¾åƒå¤§å°æˆæ­£æ¯”ï¼Œå› æ­¤è¾“å…¥å›¾åƒè¶Šå°ï¼Œç½‘æ ¼ä¹Ÿè¶Šå°ã€‚'
- en: YOLO's final layer accepts the feature volume as an input. It is composed of
    convolutional filters of size *1* Ã— *1*. As seen in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*, a convolutional layer of size *1* Ã— *1* can
    be used to change the depth of the feature volume without affecting its spatial
    structure.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOçš„æœ€ç»ˆå±‚æ¥å—ç‰¹å¾ä½“ç§¯ä½œä¸ºè¾“å…¥ã€‚å®ƒç”±å¤§å°ä¸º*1* Ã— *1*çš„å·ç§¯æ»¤æ³¢å™¨ç»„æˆã€‚å¦‚åœ¨[ç¬¬4ç« ](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)ã€Šæœ‰å½±å“åŠ›çš„åˆ†ç±»å·¥å…·ã€‹ä¸­æ‰€ç¤ºï¼Œ*1*
    Ã— *1*çš„å·ç§¯å±‚å¯ä»¥ç”¨äºæ”¹å˜ç‰¹å¾ä½“ç§¯çš„æ·±åº¦ï¼Œè€Œä¸å½±å“å…¶ç©ºé—´ç»“æ„ã€‚
- en: YOLO's layers output
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOçš„å±‚è¾“å‡º
- en: 'YOLO''s final output is a *w* Ã— *h* Ã— *M* matrix, where *w* Ã— *h* is the size
    of the grid, and *M* corresponds to the formula *B* Ã— *(C + 5)*, where the following
    applies:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOçš„æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ª*w* Ã— *h* Ã— *M*çŸ©é˜µï¼Œå…¶ä¸­*w* Ã— *h*æ˜¯ç½‘æ ¼çš„å¤§å°ï¼Œ*M*å¯¹åº”äºå…¬å¼*B* Ã— *(C + 5)*ï¼Œå…¶ä¸­é€‚ç”¨ä»¥ä¸‹å†…å®¹ï¼š
- en: '*B* is the number of bounding boxes per grid cell.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*æ˜¯æ¯ä¸ªç½‘æ ¼å•å…ƒçš„è¾¹ç•Œæ¡†æ•°é‡ã€‚'
- en: '*C* is the number of classes (in our example, we will use 20 classes).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*æ˜¯ç±»åˆ«çš„æ•°é‡ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨20ä¸ªç±»åˆ«ï¼‰ã€‚'
- en: 'Notice that we add *5* to the number of classes. This is because, for each
    bounding box, we need to predict *(C + 5)* numbers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨ç±»åˆ«æ•°ä¸ŠåŠ äº†*5*ã€‚è¿™æ˜¯å› ä¸ºå¯¹äºæ¯ä¸ªè¾¹ç•Œæ¡†ï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹*(C + 5)*ä¸ªæ•°å­—ï¼š
- en: '*t[x]* and *t[y]* will be used to compute the coordinates of the center of
    the bounding box.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[x]*å’Œ*t[y]*å°†ç”¨æ¥è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒçš„åæ ‡ã€‚'
- en: '*t[w]* and *t[h]* will be used to compute the width and height of the bounding
    box.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[w]*å’Œ*t[h]*å°†ç”¨æ¥è®¡ç®—è¾¹ç•Œæ¡†çš„å®½åº¦å’Œé«˜åº¦ã€‚'
- en: '*c*Â is the confidence that an object is in the bounding box.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c*æ˜¯ç‰©ä½“ä½äºè¾¹ç•Œæ¡†ä¸­çš„ç½®ä¿¡åº¦ã€‚'
- en: '*p1*, *p2*, ..., andÂ *pC* are the probability that the bounding box contains
    an object of class *1*, *2*, ..., *C* (where *C = 20* in our example).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p1*ã€*p2*ã€...å’Œ*pC*æ˜¯è¾¹ç•Œæ¡†åŒ…å«ç‰©ä½“å±äºç±»åˆ«*1*ã€*2*ã€...ã€*C*çš„æ¦‚ç‡ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œ*C = 20*ï¼‰ã€‚'
- en: 'This diagram summarizes how the output matrix appears:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å›¾æ€»ç»“äº†è¾“å‡ºçŸ©é˜µçš„æ˜¾ç¤ºæ–¹å¼ï¼š
- en: '![](img/6eb909f6-a8e5-40ef-b202-9763d8637aec.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6eb909f6-a8e5-40ef-b202-9763d8637aec.png)'
- en: 'Figure 5.5: Final matrix output of YOLO. In this example,Â *B = 5*, *C = 20*,
    *w = 13,* and *h = 13*. The size is 13 Ã— 13 Ã— 125'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.5ï¼šYOLOçš„æœ€ç»ˆçŸ©é˜µè¾“å‡ºã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œ*B = 5*ï¼Œ*C = 20*ï¼Œ*w = 13*ï¼Œ*h = 13*ã€‚å¤§å°ä¸º13 Ã— 13 Ã— 125
- en: Before we explain how to use this matrix to compute the final bounding boxes,
    we need to introduce an important conceptâ€”**anchor boxes**.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬è§£é‡Šå¦‚ä½•ä½¿ç”¨è¯¥çŸ©é˜µè®¡ç®—æœ€ç»ˆçš„è¾¹ç•Œæ¡†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä»‹ç»ä¸€ä¸ªé‡è¦æ¦‚å¿µâ€”â€”**é”šæ¡†**ã€‚
- en: Introducing anchor boxes
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•å…¥é”šæ¡†
- en: We mentioned that *t[x]*, *t[y]*, *t[w]*, and *t[h]* are used to compute the
    bounding box coordinates. Why not ask the network to output the coordinates directly
    (*x*, *y*, *w*, and *h*)? In fact, that is how it was done in YOLO v1\. Unfortunately,
    this resulted in a lot of errors because objects vary in size.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æåˆ°è¿‡ï¼Œ*t[x]*ã€*t[y]*ã€*t[w]*å’Œ*t[h]*ç”¨æ¥è®¡ç®—è¾¹ç•Œæ¡†çš„åæ ‡ã€‚ä¸ºä»€ä¹ˆä¸ç›´æ¥è®©ç½‘ç»œè¾“å‡ºåæ ‡ï¼ˆ*x*ã€*y*ã€*w*ã€*h*ï¼‰å‘¢ï¼Ÿå®é™…ä¸Šï¼Œè¿™æ­£æ˜¯YOLO
    v1ä¸­çš„åšæ³•ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ä¼šå¯¼è‡´å¾ˆå¤šè¯¯å·®ï¼Œå› ä¸ºç‰©ä½“çš„å¤§å°å„å¼‚ã€‚
- en: Indeed, if most of the objects in the train dataset are big, the network will
    tend to predict *w* and *h* as being very large. And when using the trained model
    on small objects, it will often fail. To fix this problem, YOLO v2 introduced
    **anchor boxes**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œå¦‚æœè®­ç»ƒæ•°æ®é›†ä¸­çš„å¤§å¤šæ•°ç‰©ä½“è¾ƒå¤§ï¼Œç½‘ç»œå°†å€¾å‘äºé¢„æµ‹*å®½åº¦*ï¼ˆ*w*ï¼‰å’Œ*é«˜åº¦*ï¼ˆ*h*ï¼‰éå¸¸å¤§ã€‚å½“ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¥æ£€æµ‹å°ç‰©ä½“æ—¶ï¼Œå®ƒé€šå¸¸ä¼šå¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒYOLO
    v2å¼•å…¥äº†**é”šæ¡†**ã€‚
- en: 'Anchor boxes (also called **priors**) are a set of bounding box sizes that
    are decided upon before training the network. For instance, when training a neural
    network to detect pedestrians, tall and narrow anchor boxes would be picked. An
    example is shown here:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é”šæ¡†ï¼ˆä¹Ÿå«**å…ˆéªŒæ¡†**ï¼‰æ˜¯ä¸€ç»„åœ¨è®­ç»ƒç½‘ç»œä¹‹å‰å°±å†³å®šå¥½çš„è¾¹ç•Œæ¡†å¤§å°ã€‚ä¾‹å¦‚ï¼Œå½“è®­ç»ƒç¥ç»ç½‘ç»œæ¥æ£€æµ‹è¡Œäººæ—¶ï¼Œä¼šé€‰æ‹©é«˜è€Œçª„çš„é”šæ¡†ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/776b173e-60b2-45e3-8f90-1b96f67f2ae5.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/776b173e-60b2-45e3-8f90-1b96f67f2ae5.png)'
- en: Figure 5.6:Â On the left are the three bounding box sizes picked to detect pedestrians.
    On the right is how we adapt one of the bounding boxes to match a pedestrian
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.6ï¼šå·¦ä¾§æ˜¯ç”¨äºæ£€æµ‹è¡Œäººçš„ä¸‰ç§è¾¹ç•Œæ¡†å¤§å°ã€‚å³ä¾§æ˜¯æˆ‘ä»¬å¦‚ä½•è°ƒæ•´å…¶ä¸­ä¸€ä¸ªè¾¹ç•Œæ¡†ä»¥åŒ¹é…è¡Œäººã€‚
- en: A set of anchor boxes is usually smallâ€”from 3 to 25 different sizes in practice.
    As those boxes cannot exactlyÂ match all the objects, the network is used to refine
    the closest anchor box. In our example, we fit the pedestrian in the image with
    the closest anchor box and use the neural network to correct the height of the
    anchor box. This is what *t[x]*, *t[y]*, *t[w]*, and *t[h]* correspond toâ€”**corrections
    to the anchor box**.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç»„é”šæ¡†é€šå¸¸è¾ƒå°â€”â€”åœ¨å®è·µä¸­é€šå¸¸åŒ…å«3åˆ°25ç§ä¸åŒçš„å°ºå¯¸ã€‚ç”±äºè¿™äº›æ¡†ä¸èƒ½å®Œå…¨åŒ¹é…æ‰€æœ‰ç‰©ä½“ï¼Œç½‘ç»œä¼šç”¨äºç»†åŒ–æœ€æ¥è¿‘çš„é”šæ¡†ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è¡Œäººå›¾åƒä¸­çš„ç‰©ä½“ä¸æœ€æ¥è¿‘çš„é”šæ¡†åŒ¹é…ï¼Œå¹¶ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ä¿®æ­£é”šæ¡†çš„é«˜åº¦ã€‚è¿™å°±æ˜¯*t[x]*ã€*t[y]*ã€*t[w]*å’Œ*t[h]*å¯¹åº”çš„å†…å®¹â€”â€”**é”šæ¡†çš„ä¿®æ­£**ã€‚
- en: 'When they were first introduced in the literature, anchor boxes were picked
    manually. Usually, nine box sizes were used:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é”šæ¡†é¦–æ¬¡åœ¨æ–‡çŒ®ä¸­æå‡ºæ—¶ï¼Œå®ƒä»¬æ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ã€‚é€šå¸¸ä½¿ç”¨ä¹ç§æ¡†å¤§å°ï¼š
- en: Three squares (small, medium, and large)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªæ­£æ–¹å½¢ï¼ˆå°å·ã€ä¸­å·ã€å¤§å·ï¼‰
- en: Three horizontal rectangles (small, medium, and large)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªæ°´å¹³çŸ©å½¢ï¼ˆå°å·ã€ä¸­å·ã€å¤§å·ï¼‰
- en: Three vertical rectangles (small, medium, and large)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªå‚ç›´çŸ©å½¢ï¼ˆå°å·ã€ä¸­å·ã€å¤§å·ï¼‰
- en: However, in the YOLOv2 paper, the authors recognized that the sizes of anchor
    boxes are different for each dataset. Therefore, before training the model, they
    recommend analyzing the data to pick the size of the anchor boxes. To detect pedestrians,
    as previously, vertical rectangles would be used. To detect apples, square anchor
    boxes would be used.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨YOLOv2çš„è®ºæ–‡ä¸­ï¼Œä½œè€…æ„è¯†åˆ°é”šæ¡†çš„å¤§å°å› æ•°æ®é›†è€Œå¼‚ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œä»–ä»¬å»ºè®®å¯¹æ•°æ®è¿›è¡Œåˆ†æï¼Œä»¥é€‰æ‹©åˆé€‚çš„é”šæ¡†å¤§å°ã€‚æ¯”å¦‚ï¼Œåœ¨æ£€æµ‹è¡Œäººæ—¶ï¼Œå¦‚å‰æ‰€è¿°ï¼Œä½¿ç”¨å‚ç›´çŸ©å½¢æ¡†ï¼›åœ¨æ£€æµ‹è‹¹æœæ—¶ï¼Œåˆ™ä½¿ç”¨æ­£æ–¹å½¢çš„é”šæ¡†ã€‚
- en: How YOLO refines anchor boxes
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOå¦‚ä½•ç»†åŒ–é”šæ¡†
- en: 'In practice, YOLOv2 computes each final bounding box''s coordinates using the
    following formulas:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼ŒYOLOv2 ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—æ¯ä¸ªæœ€ç»ˆè¾¹ç•Œæ¡†çš„åæ ‡ï¼š
- en: '![](img/3043125d-8af4-4a95-b9df-5ffa624f1859.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3043125d-8af4-4a95-b9df-5ffa624f1859.png)'
- en: 'The terms of the preceding equation can be explained as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢å…¬å¼ä¸­çš„å„é¡¹å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è§£é‡Šï¼š
- en: '*t[x] , t[y] , t[w] ,* and *t*[*h*Â ] are the outputs from the last layer.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[x] , t[y] , t[w] ,* å’Œ *t*[*h*] æ˜¯æœ€åä¸€å±‚çš„è¾“å‡ºã€‚'
- en: '*b[x] , b[y] , b[w] ,* and*Â  b[h]* are the position and size of the predicted
    bounding box, respectively.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b[x] , b[y] , b[w] ,* å’Œ *b[h]* åˆ†åˆ«è¡¨ç¤ºé¢„æµ‹è¾¹ç•Œæ¡†çš„ä½ç½®å’Œå¤§å°ã€‚'
- en: '*p[w]* and *p[h]* represent the original size of the anchor box.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[w]* å’Œ *p[h]* ä»£è¡¨é”šæ¡†çš„åŸå§‹å°ºå¯¸ã€‚'
- en: '*c[x]*Â and *c*[*y*]Â are the coordinates of the current grid cell (they will
    be (0,0) for the top-left box, (w - 1,0) for the top-right box, and (0, h - 1)
    for the bottom-left box).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c[x]* å’Œ *c*[*y*] æ˜¯å½“å‰ç½‘æ ¼å•å…ƒçš„åæ ‡ï¼ˆå¯¹äºå·¦ä¸Šæ¡†ï¼Œå®ƒä»¬å°†æ˜¯ (0,0)ï¼Œå¯¹äºå³ä¸Šæ¡†ï¼Œå®ƒä»¬å°†æ˜¯ (w - 1,0)ï¼Œå¯¹äºå·¦ä¸‹æ¡†ï¼Œå®ƒä»¬å°†æ˜¯
    (0, h - 1)ï¼‰ã€‚'
- en: '*exp* is the exponential function.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*exp* æ˜¯æŒ‡æ•°å‡½æ•°ã€‚'
- en: '*sigmoid* is the sigmoid function, described in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*sigmoid* æ˜¯ sigmoid å‡½æ•°ï¼Œæè¿°è§[ç¬¬ 1 ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ï¼Œ*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ*ã€‚'
- en: 'While this formula may seem complex, this diagram may help to clarify matters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªå…¬å¼çœ‹èµ·æ¥å¤æ‚ï¼Œä½†è¿™ä¸ªç¤ºæ„å›¾å¯èƒ½æœ‰åŠ©äºæ¾„æ¸…é—®é¢˜ï¼š
- en: '![](img/37cbf53f-ae4f-4945-9b9f-d2ed447467de.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37cbf53f-ae4f-4945-9b9f-d2ed447467de.png)'
- en: Figure 5.7:Â How YOLO refines and positions anchor boxes
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.7ï¼šYOLO å¦‚ä½•ç²¾ç‚¼å¹¶å®šä½é”šæ¡†
- en: In the preceding diagram, we see that on the left, the solid line is the anchor
    box, and the dotted line is the refined bounding box. On the right, the dot is
    the center of the bounding box.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„ç¤ºæ„å›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å·¦ä¾§å®çº¿ä¸ºé”šæ¡†ï¼Œè™šçº¿ä¸ºç²¾ç‚¼åçš„è¾¹ç•Œæ¡†ã€‚å³ä¾§çš„ç‚¹æ˜¯è¾¹ç•Œæ¡†çš„ä¸­å¿ƒã€‚
- en: 'The output of the neural network, a matrix with raw numbers, needs to be transformed
    into a list of bounding boxes. A simplified version of the code would look like
    this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«åŸå§‹æ•°å€¼çš„çŸ©é˜µï¼Œéœ€è¦è½¬æ¢ä¸ºè¾¹ç•Œæ¡†åˆ—è¡¨ã€‚ç®€åŒ–ç‰ˆçš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code needs to be run for every inference in order to compute bounding boxes
    for an image. Before we can display the boxes, we need one more post-processing
    operation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç éœ€è¦åœ¨æ¯æ¬¡æ¨ç†æ—¶è¿è¡Œï¼Œä»¥ä¾¿ä¸ºå›¾åƒè®¡ç®—è¾¹ç•Œæ¡†ã€‚åœ¨æˆ‘ä»¬æ˜¾ç¤ºæ¡†ä¹‹å‰ï¼Œè¿˜éœ€è¦è¿›è¡Œä¸€æ¬¡åå¤„ç†æ“ä½œã€‚
- en: Post-processing the boxes
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åå¤„ç†æ¡†
- en: 'We end up with the coordinates and the size of the predicted bounding boxes,
    as well as the confidence and the class probabilities. All we have to do now is
    to multiply the confidence by the class probabilities and threshold them in order
    to onlyÂ keep high probabilities:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†é¢„æµ‹è¾¹ç•Œæ¡†çš„åæ ‡å’Œå¤§å°ï¼Œä»¥åŠç½®ä¿¡åº¦å’Œç±»æ¦‚ç‡ã€‚ç°åœ¨æˆ‘ä»¬åªéœ€å°†ç½®ä¿¡åº¦ä¹˜ä»¥ç±»æ¦‚ç‡ï¼Œå¹¶è¿›è¡Œé˜ˆå€¼å¤„ç†ï¼Œåªä¿ç•™é«˜æ¦‚ç‡ï¼š
- en: '[PRE1]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is an example of this operation with a simple sample, with a threshold
    of `0.3` and a box confidence (for this specific box) of `0.5`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä½¿ç”¨ç®€å•ç¤ºä¾‹çš„è¯¥æ“ä½œç¤ºä¾‹ï¼Œé˜ˆå€¼ä¸º `0.3`ï¼Œè¯¥æ¡†çš„ç½®ä¿¡åº¦ï¼ˆå¯¹äºæ­¤ç‰¹å®šæ¡†ï¼‰ä¸º `0.5`ï¼š
- en: '| **CLASS_LABELS** | *dog* | *airplane* | *bird* | *elephant* |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **CLASS_LABELS** | *dog* | *airplane* | *bird* | *elephant* |'
- en: '| **classes_scores** | 0.7 | 0.8 | 0.001 | 0.1 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **classes_scores** | 0.7 | 0.8 | 0.001 | 0.1 |'
- en: '| **final_scores** | 0.35 | 0.4 | 0.0005 | 0.05 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **final_scores** | 0.35 | 0.4 | 0.0005 | 0.05 |'
- en: '| **filtered_scores** | 0.35 | 0.4 | 0 | 0 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **filtered_scores** | 0.35 | 0.4 | 0 | 0 |'
- en: 'Then, ifÂ `filtered_scores` contains non-null values, this means we have at
    least one class above the threshold. We keep the class with the highest score:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¦‚æœ`filtered_scores`åŒ…å«éç©ºå€¼ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬è‡³å°‘æœ‰ä¸€ä¸ªç±»çš„åˆ†æ•°è¶…è¿‡äº†é˜ˆå€¼ã€‚æˆ‘ä»¬ä¿ç•™å¾—åˆ†æœ€é«˜çš„ç±»ï¼š
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In our example, `class_label` would be *airplane*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œ`class_label` å°†æ˜¯ *airplane*ã€‚
- en: 'Once we have applied this filtering to all of the bounding boxes in the grid,
    we end up with all the information we need to draw the predictions. The following
    photograph shows what we would obtain by doing so:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å¯¹ç½‘æ ¼ä¸­çš„æ‰€æœ‰è¾¹ç•Œæ¡†åº”ç”¨äº†è¿™ä¸ªè¿‡æ»¤æ“ä½œï¼Œæˆ‘ä»¬å°±ä¼šå¾—åˆ°ç»˜åˆ¶é¢„æµ‹æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚ä»¥ä¸‹ç…§ç‰‡æ˜¾ç¤ºäº†è¿™æ ·åšåçš„ç»“æœï¼š
- en: '![](img/fe6e18ee-9cb9-4659-89cf-852764887aa2.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe6e18ee-9cb9-4659-89cf-852764887aa2.png)'
- en: Figure 5.8:Â Example of the raw bounding box output being drawn over the image
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.8ï¼šåŸå§‹è¾¹ç•Œæ¡†è¾“å‡ºåœ¨å›¾åƒä¸Šçš„ç»˜åˆ¶ç¤ºä¾‹
- en: Numerous bounding boxes are overlapping. As the plane is covering several grid
    cells, it has been detected more than once. To correct this, we need one last
    step in our post-processing pipelineâ€”**non-maximum suppression** (**NMS**).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šè¾¹ç•Œæ¡†é‡å ã€‚ç”±äºå¹³é¢è¦†ç›–äº†å¤šä¸ªç½‘æ ¼å•å…ƒï¼Œå› æ­¤å®ƒè¢«å¤šæ¬¡æ£€æµ‹åˆ°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åå¤„ç†ç®¡é“ä¸­è¿›è¡Œæœ€åä¸€æ­¥â€”â€”**éæå¤§å€¼æŠ‘åˆ¶** (**NMS**)ã€‚
- en: NMS
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NMS
- en: The idea of NMS is to remove boxes that overlap the box with the highest probability.
    We therefore remove boxes that are **non-maximum**. To do so, we sort all the
    boxes by probability, taking the ones with the highest probability first. Then,
    for each box, we compute the IoU with all the other boxes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: NMSçš„æ€è·¯æ˜¯å»é™¤ä¸æ¦‚ç‡æœ€é«˜æ¡†é‡å çš„æ¡†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç§»é™¤**éæœ€å¤§**æ¡†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ ¹æ®æ¦‚ç‡å¯¹æ‰€æœ‰æ¡†è¿›è¡Œæ’åºï¼Œå…ˆé€‰å–æ¦‚ç‡æœ€é«˜çš„æ¡†ã€‚ç„¶åï¼Œå¯¹äºæ¯ä¸ªæ¡†ï¼Œæˆ‘ä»¬è®¡ç®—å®ƒä¸å…¶ä»–æ‰€æœ‰æ¡†çš„IoUã€‚
- en: After computing the IoU between a box and the other boxes,Â we remove the ones
    with an IoU above a certain threshold (the threshold is usually around 0.5-0.9).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ä¸€ä¸ªæ¡†ä¸å…¶ä»–æ¡†çš„IoUåï¼Œæˆ‘ä»¬å»é™¤é‚£äº›IoUè¶…è¿‡æŸä¸ªé˜ˆå€¼çš„æ¡†ï¼ˆè¯¥é˜ˆå€¼é€šå¸¸åœ¨0.5åˆ°0.9ä¹‹é—´ï¼‰ã€‚
- en: 'With pseudo-code, this is what NMS would look like:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¼ªä»£ç ï¼ŒNMSçš„å®ç°å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In practice, TensorFlow provides its own implementation of NMS, `tf.image.non_max_suppression(boxes,
    ...)` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression](https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression)),
    which we recommend using (it is well optimized and offers useful options). Also
    note that NMS is used in most object detection model post-processing pipelines.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼ŒTensorFlowæä¾›äº†è‡ªå·±çš„NMSå®ç°ï¼Œ`tf.image.non_max_suppression(boxes, ...)`ï¼ˆè¯·å‚é˜…æ–‡æ¡£ï¼š[https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression](https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression)ï¼‰ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨å®ƒï¼ˆå®ƒå·²ç»è¿‡ä¼˜åŒ–å¹¶æä¾›äº†æœ‰ç”¨çš„é€‰é¡¹ï¼‰ã€‚è¿˜è¦æ³¨æ„ï¼ŒNMSè¢«å¤§å¤šæ•°ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„åå¤„ç†ç®¡é“æ‰€ä½¿ç”¨ã€‚
- en: 'After performing NMS, we obtain a much better result with a single bounding
    box, as illustrated in the following photograph:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡ŒNMSåï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ›´å¥½çš„ç»“æœï¼Œåªæœ‰ä¸€ä¸ªè¾¹ç•Œæ¡†ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](img/6d43a0d2-d97b-4cad-9353-09a301e13916.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d43a0d2-d97b-4cad-9353-09a301e13916.png)'
- en: Figure 5.9:Â Example of the bounding boxes drawn over the image after NMS
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.9ï¼šNMSååœ¨å›¾åƒä¸Šç»˜åˆ¶çš„è¾¹ç•Œæ¡†ç¤ºä¾‹
- en: YOLO inference summarized
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOæ¨ç†æ€»ç»“
- en: 'Putting it all together, the YOLO inference comprises several smaller steps.
    YOLO''s architecture is illustrated in the following diagram:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰æ­¥éª¤æ•´åˆèµ·æ¥ï¼ŒYOLOæ¨ç†åŒ…æ‹¬è‹¥å¹²ä¸ªå°æ­¥éª¤ã€‚YOLOçš„æ¶æ„ç¤ºæ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/dec87acc-3dd0-47e1-abf8-8c9f58fef318.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dec87acc-3dd0-47e1-abf8-8c9f58fef318.png)'
- en: Figure 5.10:Â YOLO's architecture. In this example, we use two bounding boxes
    per grid cell
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.10ï¼šYOLOæ¶æ„ç¤ºæ„å›¾ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªç½‘æ ¼å•å…ƒä½¿ç”¨ä¸¤ä¸ªè¾¹ç•Œæ¡†ã€‚
- en: 'The YOLO inference process can be summarized as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOæ¨ç†è¿‡ç¨‹å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š
- en: Accept an input image and compute a feature volume using a CNN backbone.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥å—è¾“å…¥å›¾åƒï¼Œå¹¶ä½¿ç”¨CNNéª¨å¹²ç½‘è®¡ç®—ç‰¹å¾ä½“ç§¯ã€‚
- en: Use a convolutional layer to compute anchor box corrections, objectness scores,
    and class probabilities.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å·ç§¯å±‚è®¡ç®—é”šæ¡†ä¿®æ­£ã€ç‰©ä½“å­˜åœ¨åˆ†æ•°å’Œç±»åˆ«æ¦‚ç‡ã€‚
- en: Using this output, compute the coordinates of the bounding boxes.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯¥è¾“å‡ºè®¡ç®—è¾¹ç•Œæ¡†çš„åæ ‡ã€‚
- en: Filter out the boxes with a low threshold, and post-process the remaining ones
    using NMS.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç­›é€‰æ‰ä½é˜ˆå€¼çš„æ¡†ï¼Œå¹¶ä½¿ç”¨NMSå¯¹å‰©ä½™æ¡†è¿›è¡Œåå¤„ç†ã€‚
- en: At the conclusion of this process, we end up with the final predictions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡ç¨‹çš„æœ€åï¼Œæˆ‘ä»¬å¾—åˆ°äº†æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚
- en: Since the whole process is composed of convolutions and filtering operations,
    the network can accept images of any size and any ratio. Hence, it is very flexible.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ•´ä¸ªè¿‡ç¨‹ç”±å·ç§¯å’Œè¿‡æ»¤æ“ä½œç»„æˆï¼Œç½‘ç»œå¯ä»¥æ¥å—ä»»ä½•å¤§å°å’Œä»»ä½•æ¯”ä¾‹çš„å›¾åƒã€‚å› æ­¤ï¼Œå®ƒå…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§ã€‚
- en: Training YOLO
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒYOLO
- en: We have outlined the process of inference for YOLO. Using pretrained weights
    provided online, it is possible to instantiate a model directly and generate predictions.
    However, you might want to train a model on a specific dataset. In this section,
    we will go through the training procedure of YOLO.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ¦‚è¿°äº†YOLOçš„æ¨ç†è¿‡ç¨‹ã€‚åˆ©ç”¨åœ¨çº¿æä¾›çš„é¢„è®­ç»ƒæƒé‡ï¼Œå¯ä»¥ç›´æ¥å®ä¾‹åŒ–æ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹ç»“æœã€‚ç„¶è€Œï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®²è§£YOLOçš„è®­ç»ƒè¿‡ç¨‹ã€‚
- en: How the YOLO backbone is trained
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOéª¨å¹²ç½‘çš„è®­ç»ƒæ–¹å¼
- en: As we mentioned earlier, the YOLO model is composed of two main partsâ€”the backbone
    and the YOLO head. Many architectures can be used for the backbone. Before training
    the full model, the backbone is trained on a traditional classification task with
    the aid of ImageNet using the transfer learning technique detailed inÂ [Chapter
    4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml), *Influential Classification Tools*.
    While we could train YOLO from scratch, it would take much more time to do so.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼ŒYOLOæ¨¡å‹ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆâ€”â€”éª¨å¹²ç½‘å’ŒYOLOå¤´ã€‚éª¨å¹²ç½‘å¯ä»¥ä½¿ç”¨è®¸å¤šæ¶æ„ã€‚åœ¨è®­ç»ƒå®Œæ•´æ¨¡å‹ä¹‹å‰ï¼Œéª¨å¹²ç½‘ä¼šåœ¨ä¼ ç»Ÿçš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ä½¿ç”¨ImageNetå¹¶é‡‡ç”¨[ç¬¬4ç« ](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)ä¸­è¯¦ç»†æè¿°çš„è¿ç§»å­¦ä¹ æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚è™½ç„¶æˆ‘ä»¬å¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒYOLOï¼Œä½†è¿™æ ·åšéœ€è¦èŠ±è´¹æ›´å¤šçš„æ—¶é—´ã€‚
- en: 'Keras makes it very easy to use a pretrained backbone for our network:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kerasä½¿å¾—åœ¨æˆ‘ä»¬çš„ç½‘ç»œä¸­ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸»å¹²ç½‘ç»œå˜å¾—éå¸¸ç®€å•ï¼š
- en: '[PRE4]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our implementation, we will employ the architecture presented in the YOLO
    paper because it yields the best results. However, if you were to run your model
    on a mobile, you might want to use a smaller model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨YOLOè®ºæ–‡ä¸­æå‡ºçš„æ¶æ„ï¼Œå› ä¸ºå®ƒèƒ½æä¾›æœ€ä½³çš„ç»“æœã€‚ç„¶è€Œï¼Œå¦‚æœä½ è¦åœ¨æ‰‹æœºä¸Šè¿è¡Œä½ çš„æ¨¡å‹ï¼Œä½ å¯èƒ½ä¼šæƒ³è¦ä½¿ç”¨ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ã€‚
- en: YOLO loss
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLOæŸå¤±
- en: 'As the output of the last layer is quite unusual, the corresponding loss will
    also be. Actually, the YOLO loss is notoriously complex. To explain it, we will
    break the loss into several parts, each corresponding to one kind of output returned
    by the last layer. The network predicts multiple kinds of information:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæœ€åä¸€å±‚çš„è¾“å‡ºç›¸å½“ä¸å¯»å¸¸ï¼Œå› æ­¤ç›¸åº”çš„æŸå¤±ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®é™…ä¸Šï¼ŒYOLOæŸå¤±å› å…¶å¤æ‚æ€§è€Œè‘—ç§°ã€‚ä¸ºäº†è¯´æ˜å®ƒï¼Œæˆ‘ä»¬å°†æŠŠæŸå¤±åˆ†è§£ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†å¯¹åº”äºæœ€åä¸€å±‚è¿”å›çš„æŸç§è¾“å‡ºã€‚ç½‘ç»œé¢„æµ‹å¤šç§ä¿¡æ¯ï¼š
- en: The bounding box coordinates and size
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾¹ç•Œæ¡†çš„åæ ‡å’Œå¤§å°
- en: The confidence that an object is in the bounding box
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰©ä½“å‡ºç°åœ¨è¾¹ç•Œæ¡†ä¸­çš„ç½®ä¿¡åº¦
- en: The scores for the classes
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ«çš„å¾—åˆ†
- en: The general idea of the loss is that we want it to be high when the error is
    high. The loss will penalize the incorrect values. However, we only want to do
    so when it makes senseâ€”if a bounding box contains no objects, we do not want to
    penalize its coordinates as they will not be used anyway.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±çš„ä¸€èˆ¬æ€è·¯æ˜¯ï¼Œå½“è¯¯å·®è¾ƒå¤§æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æŸå¤±ä¹Ÿè¾ƒå¤§ã€‚æŸå¤±ä¼šæƒ©ç½šä¸æ­£ç¡®çš„å€¼ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åªå¸Œæœ›åœ¨æœ‰æ„ä¹‰çš„æƒ…å†µä¸‹è¿™æ ·åšâ€”â€”å¦‚æœä¸€ä¸ªè¾¹ç•Œæ¡†ä¸åŒ…å«ä»»ä½•ç‰©ä½“ï¼Œæˆ‘ä»¬å°±ä¸å¸Œæœ›æƒ©ç½šå®ƒçš„åæ ‡ï¼Œå› ä¸ºå®ƒä»¬åæ­£ä¸ä¼šè¢«ä½¿ç”¨ã€‚
- en: The implementation details of neural networks are usually not available in the
    source paper. Therefore, they will vary from one implementation to another. What
    we are outlining here is an implementation suggestion, not an absolute reference.
    We suggest reading the code from existing implementations to understand how the
    loss is calculated.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„å®ç°ç»†èŠ‚é€šå¸¸åœ¨åŸå§‹è®ºæ–‡ä¸­æ²¡æœ‰æä¾›ã€‚å› æ­¤ï¼Œå®ƒä»¬åœ¨ä¸åŒçš„å®ç°ä¹‹é—´ä¼šæœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ¦‚è¿°çš„æ˜¯ä¸€ç§å®ç°å»ºè®®ï¼Œè€Œä¸æ˜¯ç»å¯¹å‚è€ƒã€‚æˆ‘ä»¬å»ºè®®é˜…è¯»ç°æœ‰å®ç°ä¸­çš„ä»£ç ï¼Œä»¥äº†è§£æŸå¤±æ˜¯å¦‚ä½•è®¡ç®—çš„ã€‚
- en: Bounding box loss
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¾¹ç•Œæ¡†æŸå¤±
- en: 'The first part of the loss helps the networkÂ learn the weights to predict the
    bounding box coordinates and size:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°çš„ç¬¬ä¸€éƒ¨åˆ†å¸®åŠ©ç½‘ç»œå­¦ä¹ é¢„æµ‹è¾¹ç•Œæ¡†åæ ‡å’Œå¤§å°çš„æƒé‡ï¼š
- en: '![](img/db3d3535-c2a2-4c13-8121-b753b3c96264.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db3d3535-c2a2-4c13-8121-b753b3c96264.png)'
- en: 'While this equation may seem scary at first, this part is actually relatively
    simple. Let''s break it down:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªæ–¹ç¨‹å¼ä¸€å¼€å§‹çœ‹èµ·æ¥å¯èƒ½å¾ˆå“äººï¼Œä½†è¿™ä¸€éƒ¨åˆ†å…¶å®ç›¸å¯¹ç®€å•ã€‚è®©æˆ‘ä»¬åˆ†è§£å®ƒï¼š
- en: Î» (lambda) is the weighting of the lossâ€”it reflects how much importance we want
    to give to bounding box coordinates during training.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Î»ï¼ˆlambdaï¼‰æ˜¯æŸå¤±çš„æƒé‡â€”â€”å®ƒåæ˜ äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ç»™äºˆè¾¹ç•Œæ¡†åæ ‡å¤šå¤§é‡è¦æ€§ã€‚
- en: âˆ‘ (capital sigma) means that we sum what is right after them. In this case,
    we sum for each part of the grid (from i = 0 to *i = SÂ²*) and for each box in
    this part of the grid (from 0 to B).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: âˆ‘ï¼ˆå¤§å†™å¸Œè…Šå­—æ¯sigmaï¼‰è¡¨ç¤ºæˆ‘ä»¬è¦å¯¹å®ƒåé¢çš„å†…å®¹æ±‚å’Œã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¹ç½‘æ ¼çš„æ¯ä¸ªéƒ¨åˆ†ï¼ˆä»i = 0åˆ°*i = SÂ²*ï¼‰ä»¥åŠè¯¥éƒ¨åˆ†ç½‘æ ¼ä¸­çš„æ¯ä¸ªæ¡†ï¼ˆä»0åˆ°Bï¼‰æ±‚å’Œã€‚
- en: '*1^(obj)* (*indicator function* for objects) is a function equal to 1 when
    the i^(th) part of the grid and the j^(th) bounding box areÂ **responsible** for
    an object. We will explain what responsible means in the next paragraph.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1^(obj)*ï¼ˆ*ç‰©ä½“æŒ‡ç¤ºå‡½æ•°*ï¼‰æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå½“ç½‘æ ¼çš„ç¬¬iéƒ¨åˆ†å’Œç¬¬jä¸ªè¾¹ç•Œæ¡†**è´Ÿè´£**æŸä¸ªç‰©ä½“æ—¶ï¼Œå…¶å€¼ä¸º1ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€æ®µè§£é‡Šâ€œè´Ÿè´£â€æ˜¯ä»€ä¹ˆæ„æ€ã€‚'
- en: '*x[i]*, *y[i]*, *w[i]*, and *h[i]*Â  correspond to the bounding box size and
    coordinates. We take the difference between the predicted value (the output of
    the network) and the target value (also called theÂ **ground truth**). Here, the
    predicted value has a hat (`Ë†`).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[i]*ã€*y[i]*ã€*w[i]* å’Œ *h[i]* å¯¹åº”äºè¾¹ç•Œæ¡†çš„å¤§å°å’Œåæ ‡ã€‚æˆ‘ä»¬å–é¢„æµ‹å€¼ï¼ˆç½‘ç»œè¾“å‡ºï¼‰ä¸ç›®æ ‡å€¼ï¼ˆä¹Ÿç§°ä¸º**çœŸå®å€¼**ï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚è¿™é‡Œï¼Œé¢„æµ‹å€¼å¸¦æœ‰ä¸Šæ ‡(`Ë†`)ã€‚'
- en: We square the difference to make sure it is positive.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å·®å¼‚å¹³æ–¹åŒ–ï¼Œä»¥ç¡®ä¿å…¶ä¸ºæ­£æ•°ã€‚
- en: Notice that we take the square root of w[i] and h[i]. We do so to make sure
    errors for small bounding boxes are penalized more heavily than errors for big
    bounding boxes.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬å–äº†w[i]å’Œh[i]çš„å¹³æ–¹æ ¹ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†ç¡®ä¿å°çš„è¾¹ç•Œæ¡†é”™è¯¯å—åˆ°æ¯”å¤§çš„è¾¹ç•Œæ¡†æ›´é‡çš„æƒ©ç½šã€‚
- en: The key part of this loss is the **indicator function**. The coordinates will
    be correct if, and only if, the box is responsibleÂ for detecting an object. For
    each object in the image, the difficult part is determining which bounding box
    is responsible for it. For YOLOv2, the anchor box with the highest IoU with the
    detected object is deemed responsible. The rationale here is to make each anchor
    box specialize in one type of object.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸå¤±çš„å…³é”®éƒ¨åˆ†æ˜¯**æŒ‡ç¤ºå‡½æ•°**ã€‚åªæœ‰å½“è¾¹ç•Œæ¡†è´Ÿè´£æ£€æµ‹ç‰©ä½“æ—¶ï¼Œåæ ‡æ‰ä¼šæ˜¯æ­£ç¡®çš„ã€‚å¯¹äºå›¾åƒä¸­çš„æ¯ä¸ªç‰©ä½“ï¼Œéš¾ç‚¹æ˜¯ç¡®å®šå“ªä¸ªè¾¹ç•Œæ¡†è´Ÿè´£å®ƒã€‚å¯¹äºYOLOv2ï¼Œå…·æœ‰ä¸æ£€æµ‹åˆ°ç‰©ä½“æœ€é«˜IoUçš„é”šæ¡†è¢«è®¤ä¸ºæ˜¯è´Ÿè´£çš„ã€‚è¿™é‡Œçš„åŸºæœ¬æ€æƒ³æ˜¯è®©æ¯ä¸ªé”šæ¡†ä¸“æ³¨äºä¸€ç§ç‰©ä½“ç±»å‹ã€‚
- en: Object confidence loss
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰©ä½“ç½®ä¿¡åº¦æŸå¤±
- en: 'The second part of the loss teaches the network to learn the weights to predict
    whether a bounding box contains an object:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±çš„ç¬¬äºŒéƒ¨åˆ†æ•™ä¼šç½‘ç»œå­¦ä¹ é¢„æµ‹è¾¹ç•Œæ¡†æ˜¯å¦åŒ…å«ç‰©ä½“çš„æƒé‡ï¼š
- en: '![](img/28a418fd-60cd-4562-a00a-79ebdddfff3f.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28a418fd-60cd-4562-a00a-79ebdddfff3f.png)'
- en: 'We have already covered most of the symbols in this function. The remaining
    ones are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ¶µç›–äº†è¯¥å‡½æ•°ä¸­çš„å¤§éƒ¨åˆ†ç¬¦å·ã€‚å‰©ä¸‹çš„ç¬¦å·å¦‚ä¸‹ï¼š
- en: '**C[ij]**: The confidence that the box, *j*, in the part, *i*, of the grid
    contains an object (of any kind)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C[ij]**: ç¬¬*i*ç½‘æ ¼éƒ¨åˆ†çš„ç¬¬*j*è¾¹ç•Œæ¡†åŒ…å«ç‰©ä½“ï¼ˆä»»æ„ç±»å‹ï¼‰çš„ç½®ä¿¡åº¦'
- en: '**1^(noobj)Â  (indicator function for no object)**: A function equal to 1 whenÂ the
    *i*^(th) part of the grid and the *j*^(th) bounding box areÂ *not responsible*
    for an object'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1^(noobj)ï¼ˆæ— ç‰©ä½“çš„æŒ‡ç¤ºå‡½æ•°ï¼‰**ï¼šå½“ç¬¬*i*ç½‘æ ¼éƒ¨åˆ†å’Œç¬¬*j*è¾¹ç•Œæ¡†*ä¸è´Ÿè´£*æŸç‰©ä½“æ—¶ï¼Œå‡½æ•°å€¼ä¸º1'
- en: 'A naive approach to compute *1^(noobj)*Â  isÂ  *(1 - 1^(obj))*. However, if we
    do so, it can cause some problems during training. Indeed, we have many bounding
    boxes on our grid. When determining that one of them is responsible for a specific
    object, there may have been other suitable candidates for this object. We do not
    want to penalize the objectness score of those other good candidates that also
    fit the object. Therefore, *1^(noobj)*Â  is defined as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—*1^(noobj)*çš„ä¸€ä¸ªç®€å•æ–¹æ³•æ˜¯*(1 - 1^(obj))*ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è¿™æ ·åšï¼Œå®ƒå¯èƒ½ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å‘ä¸€äº›é—®é¢˜ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬çš„ç½‘æ ¼ä¸Šæœ‰å¾ˆå¤šè¾¹ç•Œæ¡†ã€‚åœ¨ç¡®å®šæŸä¸ªè¾¹ç•Œæ¡†è´Ÿè´£ç‰¹å®šç‰©ä½“æ—¶ï¼Œå¯èƒ½ä¼šæœ‰å…¶ä»–é€‚åˆè¯¥ç‰©ä½“çš„å€™é€‰æ¡†ã€‚æˆ‘ä»¬ä¸å¸Œæœ›æƒ©ç½šé‚£äº›é€‚åˆç‰©ä½“çš„å…¶ä»–ä¼˜ç§€å€™é€‰æ¡†çš„ç‰©ä½“æ€§å¾—åˆ†ã€‚å› æ­¤ï¼Œ*1^(noobj)*çš„å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](img/3c2f6c72-c27f-4116-b1a0-dcf00b38d25c.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c2f6c72-c27f-4116-b1a0-dcf00b38d25c.png)'
- en: In practice, for each bounding box at position (*i*, *j*), the IoU with regard
    to each of the ground truth boxes is computed. If the IoU is over a certain threshold
    (usually 0.6), 1^(noobj) is set to 0\. The rationale behind this idea is to avoid
    punishing boxes that contain objects but are not responsible for said object.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œå¯¹äºä½ç½®(*i*, *j*)ä¸Šçš„æ¯ä¸ªè¾¹ç•Œæ¡†ï¼Œéƒ½ä¼šè®¡ç®—å®ƒä¸æ¯ä¸ªåœ°é¢çœŸå®æ¡†çš„IoUã€‚å¦‚æœIoUè¶…è¿‡æŸä¸ªé˜ˆå€¼ï¼ˆé€šå¸¸ä¸º0.6ï¼‰ï¼Œåˆ™å°†1^(noobj)è®¾ä¸º0ã€‚è¿™ä¸ªæƒ³æ³•çš„ä¾æ®æ˜¯é¿å…æƒ©ç½šé‚£äº›åŒ…å«ç‰©ä½“ä½†å¹¶ä¸è´Ÿè´£è¯¥ç‰©ä½“çš„è¾¹ç•Œæ¡†ã€‚
- en: Classification loss
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†ç±»æŸå¤±
- en: 'The final part of the loss, the classification loss, ensures that the network
    learns to predict the proper class for each bounding box:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±çš„æœ€åä¸€éƒ¨åˆ†â€”â€”åˆ†ç±»æŸå¤±ï¼Œç¡®ä¿ç½‘ç»œå­¦ä¹ ä¸ºæ¯ä¸ªè¾¹ç•Œæ¡†é¢„æµ‹æ­£ç¡®çš„ç±»åˆ«ï¼š
- en: '![](img/4e8f99f7-0dac-45f7-9782-af77f2c0a604.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e8f99f7-0dac-45f7-9782-af77f2c0a604.png)'
- en: This loss is very similar to the one presented in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),Â *Computer
    Vision and Neural Networks*. Note that while the loss presented in the YOLO paper
    is the L2 loss, many implementations use cross-entropy. This part of the loss
    ensures that correct object classes are predicted.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸå¤±ä¸[ç¬¬1ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ä¸­ä»‹ç»çš„*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ*çš„æŸå¤±éå¸¸ç›¸ä¼¼ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶YOLOè®ºæ–‡ä¸­å±•ç¤ºçš„æŸå¤±æ˜¯L2æŸå¤±ï¼Œä½†è®¸å¤šå®ç°ä½¿ç”¨çš„æ˜¯äº¤å‰ç†µæŸå¤±ã€‚è¿™ä¸ªæŸå¤±éƒ¨åˆ†ç¡®ä¿æ­£ç¡®çš„ç‰©ä½“ç±»åˆ«è¢«é¢„æµ‹å‡ºæ¥ã€‚
- en: Full YOLO loss
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œæ•´YOLOæŸå¤±
- en: '**Full YOLO loss** is the sum of the three losses previously detailed. By combining
    the three terms, the loss penalizes the error for bounding box coordinate refinement,
    objectness scores, and class prediction. By backpropagating the error, we are
    able to train the YOLO network to predict correct bounding boxes.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®Œæ•´YOLOæŸå¤±**æ˜¯å‰é¢æåˆ°çš„ä¸‰ä¸ªæŸå¤±çš„æ€»å’Œã€‚é€šè¿‡ç»„åˆè¿™ä¸‰ä¸ªé¡¹ï¼ŒæŸå¤±æƒ©ç½šè¾¹ç•Œæ¡†åæ ‡çš„ç²¾ç»†è°ƒæ•´ã€ç‰©ä½“æ€§å¾—åˆ†å’Œç±»åˆ«é¢„æµ‹çš„é”™è¯¯ã€‚é€šè¿‡åå‘ä¼ æ’­é”™è¯¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒYOLOç½‘ç»œé¢„æµ‹æ­£ç¡®çš„è¾¹ç•Œæ¡†ã€‚'
- en: In the book's GitHub repository, readers will find a simplified implementation
    of the YOLO network. In particular, the implementation contains a heavily commented
    loss function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­ï¼Œè¯»è€…å°†æ‰¾åˆ°YOLOç½‘ç»œçš„ç®€åŒ–å®ç°ã€‚ç‰¹åˆ«åœ°ï¼Œä»£ç ä¸­åŒ…å«äº†å¤§é‡æ³¨é‡Šçš„æŸå¤±å‡½æ•°ã€‚
- en: Training techniques
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæŠ€å·§
- en: 'Once the loss has been properly defined, YOLO can be trained using backpropagation.
    However, to make sure the loss does not diverge and to obtain good performance,
    we will detail a few training techniques:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æŸå¤±å‡½æ•°è¢«æ­£ç¡®å®šä¹‰ï¼ŒYOLOå°±å¯ä»¥é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œä¸ºäº†ç¡®ä¿æŸå¤±ä¸ä¼šå‘æ•£ï¼Œå¹¶ä¸”è·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»ä¸€äº›è®­ç»ƒæŠ€å·§ï¼š
- en: Augmentation (explained in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*) and dropout (explained in [Chapter
    3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern Neural Networks*) are
    used. Without these two techniques, the network would overfit on the training
    data and would not be able to generalize much.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®å¢å¼ºï¼ˆåœ¨[ç¬¬7ç« ](337ec077-c215-4782-b56c-beae4d94d718.xhtml)ä¸­è§£é‡Šï¼Œ*åœ¨å¤æ‚å’Œç¨€ç¼ºæ•°æ®é›†ä¸Šçš„è®­ç»ƒ*ï¼‰å’Œä¸¢å¼ƒæ³•ï¼ˆåœ¨[ç¬¬3ç« ](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)ä¸­è§£é‡Šï¼Œ*ç°ä»£ç¥ç»ç½‘ç»œ*ï¼‰è¢«ç”¨æ¥é˜²æ­¢ç½‘ç»œè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–ã€‚
- en: Another technique is **multi-scale training**. Every *n* batches, the network's
    input is changed to a different size. This forces the network to learn to predict
    with accuracy across a variety of input dimensions.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æŠ€æœ¯æ˜¯**å¤šå°ºåº¦è®­ç»ƒ**ã€‚æ¯ç»è¿‡*n*ä¸ªæ‰¹æ¬¡ï¼Œç½‘ç»œçš„è¾“å…¥ä¼šè¢«æ›´æ”¹ä¸ºä¸åŒçš„å°ºå¯¸ã€‚è¿™è¿«ä½¿ç½‘ç»œå­¦ä¹ åœ¨å„ç§è¾“å…¥ç»´åº¦ä¸‹è¿›è¡Œç²¾ç¡®é¢„æµ‹ã€‚
- en: Like most detection networks, YOLO is pretrained on an image classification
    task.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸å¤§å¤šæ•°æ£€æµ‹ç½‘ç»œä¸€æ ·ï¼ŒYOLOåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚
- en: While not mentioned in the paper, the official YOLO implementation uses **burn-in**â€”the
    learning rate is reduced at the beginning of training to avoid a loss explosion.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°½ç®¡è®ºæ–‡ä¸­æ²¡æœ‰æåˆ°ï¼Œå®˜æ–¹YOLOå®ç°ä½¿ç”¨äº†**é¢„çƒ­**â€”â€”åœ¨è®­ç»ƒå¼€å§‹æ—¶é™ä½å­¦ä¹ ç‡ï¼Œä»¥é¿å…æŸå¤±çˆ†ç‚¸ã€‚
- en: Faster R-CNN â€“ a powerful object detection model
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN â€“ ä¸€ä¸ªå¼ºå¤§çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹
- en: The main benefit of YOLO is its speed. While it can achieve very good results,
    it is now outperformed by more complex networks. **Faster Region with Convolutional
    Neural Networks** (**Faster R-CNN**) is considered state of the art at the time
    of writing. It is also quite fast, reaching 4-5 FPS on a modern GPU. In this section,
    we will explore its architecture.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å…¶é€Ÿåº¦ã€‚å°½ç®¡å®ƒå¯ä»¥å–å¾—éå¸¸å¥½çš„ç»“æœï¼Œä½†ç°åœ¨å·²è¢«æ›´å¤æ‚çš„ç½‘ç»œè¶…è¶Šã€‚**æ›´å¿«çš„åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œ**ï¼ˆ**Faster R-CNN**ï¼‰è¢«è®¤ä¸ºæ˜¯åœ¨å†™ä½œæ—¶çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚å®ƒä¹Ÿç›¸å½“å¿«é€Ÿï¼Œåœ¨ç°ä»£GPUä¸Šè¾¾åˆ°4-5å¸§æ¯ç§’ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ¢è®¨å…¶æ¶æ„ã€‚
- en: 'The Faster R-CNN architecture was engineered over several years of research.
    More precisely, it was built incrementally from two architecturesâ€”R-CNN and Fast
    R-CNN. In this section, we will focus on the latest architecture, Faster R-CNN:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNNæ¶æ„ç»è¿‡å¤šå¹´çš„ç ”ç©¶é€æ­¥è®¾è®¡å®Œæˆã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯ä»ä¸¤ä¸ªæ¶æ„â€”â€”R-CNNå’ŒFast R-CNNâ€”â€”é€æ­¥æ„å»ºè€Œæˆçš„ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»æœ€æ–°çš„æ¶æ„ï¼ŒFaster
    R-CNNï¼š
- en: '*Faster R-CNN: towards real-time object detection with region proposal networks
    (2015)*, Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Faster R-CNNï¼šé€šè¿‡åŒºåŸŸæè®®ç½‘ç»œå®ç°å®æ—¶ç›®æ ‡æ£€æµ‹ï¼ˆ2015ï¼‰*ï¼ŒShaoqing Ren, Kaiming He, Ross Girshick
    å’Œ Jian Sun'
- en: 'This paper draws a lot of knowledge from the two previous designs. Therefore,
    some of the architecture details can be found in the following papers:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å€Ÿé‰´äº†ä¹‹å‰ä¸¤ä¸ªè®¾è®¡ä¸­çš„å¤§é‡çŸ¥è¯†ã€‚å› æ­¤ï¼Œéƒ¨åˆ†æ¶æ„ç»†èŠ‚å¯ä»¥åœ¨ä»¥ä¸‹è®ºæ–‡ä¸­æ‰¾åˆ°ï¼š
- en: '*Rich feature hierarchies for accurate object detection and semantic segmentation
    (2013)*, Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Mali'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç²¾ç¡®ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²çš„ä¸°å¯Œç‰¹å¾å±‚æ¬¡ï¼ˆ2013ï¼‰*ï¼ŒRoss Girshick, Jeff Donahue, Trevor Darrell å’Œ Jitendra
    Mali'
- en: '*Fast R-CNN (2015)*, Ross Girshick'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fast R-CNNï¼ˆ2015ï¼‰*ï¼ŒRoss Girshick'
- en: Just as with YOLO architecture, we recommend reading this chapter first and
    then having a look at the papers to get a deeper understanding. In this chapter,
    we will use the same notations as in the papers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒYOLOæ¶æ„ä¸€æ ·ï¼Œæˆ‘ä»¬å»ºè®®å…ˆé˜…è¯»æœ¬ç« ï¼Œç„¶åæŸ¥çœ‹è¿™äº›è®ºæ–‡ä»¥è·å¾—æ›´æ·±çš„ç†è§£ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸è®ºæ–‡ä¸­ç›¸åŒçš„ç¬¦å·ã€‚
- en: Faster R-CNN's general architecture
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNNçš„é€šç”¨æ¶æ„
- en: 'YOLO is consideredÂ a single-shot detectorâ€”as its name implies, each pixel of
    the image is analyzed once. This is the reason for its very high speed. To obtain
    more accurate results, Faster R-CNN works in two stages:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOè¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå•æ¬¡æ£€æµ‹å™¨â€”â€”æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼Œæ¯ä¸ªå›¾åƒåƒç´ åªåˆ†æä¸€æ¬¡ã€‚è¿™å°±æ˜¯å®ƒé€Ÿåº¦éå¸¸å¿«çš„åŸå› ã€‚ä¸ºäº†è·å¾—æ›´ç²¾ç¡®çš„ç»“æœï¼ŒFaster R-CNNåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µå·¥ä½œï¼š
- en: The first stage is to extract aÂ **region of interest** (**RoI**,Â or RoIs in
    the plural form). An RoI is an area of the input image that may contain an object.
    For each image, the first step generates about 2,000 RoIs**.**
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é˜¶æ®µæ˜¯æå–**å…´è¶£åŒºåŸŸ**ï¼ˆ**RoI**ï¼Œå¤æ•°å½¢å¼ä¸ºRoIsï¼‰ã€‚RoIæ˜¯è¾“å…¥å›¾åƒä¸­å¯èƒ½åŒ…å«ç‰©ä½“çš„åŒºåŸŸã€‚å¯¹äºæ¯å¼ å›¾åƒï¼Œç¬¬ä¸€æ­¥ç”Ÿæˆå¤§çº¦2,000ä¸ªRoI**ã€‚**
- en: The second stage is the **classification step** (sometimes referred to as theÂ **detection
    step**).Â We resize each of the 2,000 RoIs to a square to fit the input of a convolutional
    network. We then use the CNN to classify the RoI.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒé˜¶æ®µæ˜¯ **åˆ†ç±»æ­¥éª¤**ï¼ˆæœ‰æ—¶ä¹Ÿç§°ä¸º **æ£€æµ‹æ­¥éª¤**ï¼‰ã€‚æˆ‘ä»¬å°†æ¯ä¸ª 2,000 ä¸ª RoI è°ƒæ•´ä¸ºæ­£æ–¹å½¢ï¼Œä»¥é€‚åº”å·ç§¯ç½‘ç»œçš„è¾“å…¥ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ CNN
    å¯¹ RoI è¿›è¡Œåˆ†ç±»ã€‚
- en: In R-CNN and Fast R-CNN, regions of interest are generated using a technique
    called **selective search**. This will not be covered here because it was removed
    from the Faster R-CNN paper on account of its slowness. Moreover, selective search
    does not involve any deep learning techniques.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ R-CNN å’Œ Fast R-CNN ä¸­ï¼Œå…´è¶£åŒºåŸŸæ˜¯ä½¿ç”¨ä¸€ç§å«åš **é€‰æ‹©æ€§æœç´¢** çš„æŠ€æœ¯ç”Ÿæˆçš„ã€‚è¿™é‡Œä¸å†è¯¦ç»†ä»‹ç»ï¼Œå› ä¸ºå®ƒåœ¨ Faster R-CNN
    è®ºæ–‡ä¸­è¢«åˆ é™¤äº†ï¼ŒåŸå› æ˜¯å…¶é€Ÿåº¦è¾ƒæ…¢ã€‚æ­¤å¤–ï¼Œé€‰æ‹©æ€§æœç´¢å¹¶ä¸æ¶‰åŠä»»ä½•æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚
- en: As the two parts of Faster R-CNN are independent, we will cover each one separately.
    We will then cover the training details of the full model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº Faster R-CNN çš„ä¸¤ä¸ªéƒ¨åˆ†æ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬å°†åˆ†åˆ«ä»‹ç»æ¯ä¸€éƒ¨åˆ†ã€‚ç„¶åæˆ‘ä»¬å°†ä»‹ç»å®Œæ•´æ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ã€‚
- en: Stage 1 â€“ Region proposals
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é˜¶æ®µ â€“ åŒºåŸŸæè®®
- en: Regions of interest are generated using the **region proposal network** (**RPN**).
    To generate RoIs, the RPNÂ uses convolutional layers. Therefore, it can be implemented
    on the GPU and is very fast.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ **åŒºåŸŸæè®®ç½‘ç»œ**ï¼ˆ**RPN**ï¼‰ç”Ÿæˆå…´è¶£åŒºåŸŸã€‚ä¸ºäº†ç”Ÿæˆ RoIï¼ŒRPN ä½¿ç”¨å·ç§¯å±‚ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥åœ¨ GPU ä¸Šå®ç°ï¼Œå¹¶ä¸”é€Ÿåº¦éå¸¸å¿«ã€‚
- en: 'The RPN architecture shares quite a lot of features with YOLO''s architecture:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: RPN æ¶æ„ä¸ YOLO çš„æ¶æ„å…±äº«è®¸å¤šç‰¹å¾ï¼š
- en: It also uses anchor boxesâ€”in the Faster R-CNN paper, nine anchor sizes are used
    (three vertical rectangles, three horizontal rectangles, and three squares).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒè¿˜ä½¿ç”¨é”šæ¡†â€”â€”åœ¨ Faster R-CNN è®ºæ–‡ä¸­ï¼Œä½¿ç”¨äº†ä¹ç§é”šæ¡†å°ºå¯¸ï¼ˆä¸‰ç§ç«–ç›´çŸ©å½¢ã€ä¸‰ç§æ°´å¹³çŸ©å½¢å’Œä¸‰ç§æ­£æ–¹å½¢ï¼‰ã€‚
- en: It can use any backbone to generate the feature volume.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥ä½¿ç”¨ä»»ä½•éª¨å¹²ç½‘ç»œæ¥ç”Ÿæˆç‰¹å¾ä½“ç§¯ã€‚
- en: It uses a grid, and the size of the grid depends on the size of the feature
    volume.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä½¿ç”¨ä¸€ä¸ªç½‘æ ¼ï¼Œç½‘æ ¼çš„å¤§å°å–å†³äºç‰¹å¾ä½“ç§¯çš„å¤§å°ã€‚
- en: Its last layer outputs numbers that allow the anchor box to be refined into
    a proper bounding box fitting the object.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒçš„æœ€åä¸€å±‚è¾“å‡ºæ•°å­—ï¼Œå…è®¸é”šæ¡†è¢«ç²¾ç‚¼æˆä¸€ä¸ªåˆé€‚çš„è¾¹ç•Œæ¡†ä»¥é€‚åº”ç‰©ä½“ã€‚
- en: 'However, the architecture is not completely identical to YOLO''s. The RPN accepts
    an image as input and outputs regions of interest. Each region of interest consists
    of a bounding box and an objectness probability. To generate those numbers, a
    CNN is used to extract a feature volume. The feature volume is then used to generate
    the regions, coordinates, and probabilities. The RPN architecture is illustrated
    in theÂ following diagram:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¯¥æ¶æ„ä¸ YOLO çš„æ¶æ„å¹¶ä¸å®Œå…¨ç›¸åŒã€‚RPN æ¥å—å›¾åƒä½œä¸ºè¾“å…¥å¹¶è¾“å‡ºå…´è¶£åŒºåŸŸã€‚æ¯ä¸ªå…´è¶£åŒºåŸŸç”±ä¸€ä¸ªè¾¹ç•Œæ¡†å’Œä¸€ä¸ªç‰©ä½“å­˜åœ¨æ¦‚ç‡ç»„æˆã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›æ•°å­—ï¼Œä½¿ç”¨
    CNN æå–ç‰¹å¾ä½“ç§¯ã€‚ç„¶åä½¿ç”¨è¯¥ç‰¹å¾ä½“ç§¯ç”ŸæˆåŒºåŸŸã€åæ ‡å’Œæ¦‚ç‡ã€‚RPN çš„æ¶æ„åœ¨ä¸‹å›¾ä¸­è¿›è¡Œäº†è¯´æ˜ï¼š
- en: '![](img/2b7a4bb9-19a3-4de6-b361-a9e97eadd515.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b7a4bb9-19a3-4de6-b361-a9e97eadd515.png)'
- en: Figure 5.11:Â RPN architecture summary
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.11ï¼šRPN æ¶æ„æ¦‚è¿°
- en: 'The step-by-step process represented inÂ *Figure 5.11* is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.11 ä¸­è¡¨ç¤ºçš„é€æ­¥è¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: The network accepts an image as input and applies several convolutional layers.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ¥å—å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶åº”ç”¨å¤šä¸ªå·ç§¯å±‚ã€‚
- en: It outputs a feature volume. A convolutional filter is applied over the feature
    volume. Its size is *3* Ã— *3* Ã— *D*,Â where *D* is the depth of the feature volume
    (*D = 512* in our example).
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒè¾“å‡ºä¸€ä¸ªç‰¹å¾ä½“ç§¯ã€‚ä¸€ä¸ªå·ç§¯æ»¤æ³¢å™¨åº”ç”¨äºç‰¹å¾ä½“ç§¯ã€‚å…¶å¤§å°ä¸º *3* Ã— *3* Ã— *D*ï¼Œå…¶ä¸­ *D* æ˜¯ç‰¹å¾ä½“ç§¯çš„æ·±åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œ*D = 512*ï¼‰ã€‚
- en: At each position in the feature volume, the filter generates an intermediate
    *1* Ã— *D* vector.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å¾ä½“ç§¯çš„æ¯ä¸ªä½ç½®ï¼Œæ»¤æ³¢å™¨ç”Ÿæˆä¸€ä¸ªä¸­é—´çš„ *1* Ã— *D* å‘é‡ã€‚
- en: Two sibling *1* Ã— *1* convolutional layers compute the objectness scores and
    the bounding box coordinates. There are two objectness scores for each of the
    *k* bounding boxes. There are also four floats that will be used to refine the
    coordinates of the anchor boxes.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå…„å¼Ÿ *1* Ã— *1* çš„å·ç§¯å±‚è®¡ç®—ç‰©ä½“å­˜åœ¨æ€§åˆ†æ•°å’Œè¾¹ç•Œæ¡†åæ ‡ã€‚æ¯ä¸ª *k* ä¸ªè¾¹ç•Œæ¡†æœ‰ä¸¤ä¸ªç‰©ä½“å­˜åœ¨æ€§åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰å››ä¸ªæµ®åŠ¨å€¼ï¼Œç”¨äºä¼˜åŒ–é”šæ¡†çš„åæ ‡ã€‚
- en: After post-processing, the final output is a list of RoIs. At this step, no
    information about the class of the object is generated, only about its location.
    During the next step, classification, we will classify the objects and refine
    the bounding boxes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åå¤„ç†ä¹‹åï¼Œæœ€ç»ˆè¾“å‡ºæ˜¯ RoI çš„åˆ—è¡¨ã€‚åœ¨è¿™ä¸€æ­¥éª¤ä¸­ï¼Œä¸ä¼šç”Ÿæˆæœ‰å…³ç‰©ä½“ç±»åˆ«çš„ä¿¡æ¯ï¼Œåªä¼šç”Ÿæˆå®ƒçš„ä½ç½®ã€‚åœ¨ä¸‹ä¸€æ­¥åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ç‰©ä½“è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¼˜åŒ–è¾¹ç•Œæ¡†ã€‚
- en: Stage 2 â€“ Classification
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äºŒé˜¶æ®µ â€“ åˆ†ç±»
- en: The second part of Faster R-CNN is **classification**. It outputs the final
    bounding boxes and accepts two inputsâ€”the list of RoIs from the previous step
    (RPN), and a feature volume computed from the input image.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNNçš„ç¬¬äºŒéƒ¨åˆ†æ˜¯**åˆ†ç±»**ã€‚å®ƒè¾“å‡ºæœ€ç»ˆçš„è¾¹ç•Œæ¡†ï¼Œå¹¶æ¥å—ä¸¤ä¸ªè¾“å…¥â€”â€”æ¥è‡ªå‰ä¸€æ­¥ï¼ˆRPNï¼‰çš„RoIåˆ—è¡¨ï¼Œä»¥åŠä»è¾“å…¥å›¾åƒè®¡ç®—å‡ºçš„ç‰¹å¾ä½“ç§¯ã€‚
- en: Since most of the classification stage architecture comes from the previous
    paper, Fast R-CNN, it is sometimes referred to with the same name. Therefore,
    Faster R-CNN can be regarded as a combination of RPN and Fast R-CNN.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¤§éƒ¨åˆ†åˆ†ç±»é˜¶æ®µçš„æ¶æ„æ¥è‡ªä¹‹å‰çš„è®ºæ–‡ã€ŠFast R-CNNã€‹ï¼Œæœ‰æ—¶ä¹Ÿç§°å…¶ä¸ºç›¸åŒçš„åå­—ã€‚å› æ­¤ï¼ŒFaster R-CNNå¯ä»¥è§†ä¸ºRPNå’ŒFast R-CNNçš„ç»“åˆã€‚
- en: 'The classification part can work with any feature volume corresponding to the
    input image. However, as feature maps have already been computed in the previous
    region-proposal step, they are simply reused here. This technique has two benefits:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»éƒ¨åˆ†å¯ä»¥ä¸ä»»ä½•å¯¹åº”äºè¾“å…¥å›¾åƒçš„ç‰¹å¾ä½“ç§¯ä¸€èµ·å·¥ä½œã€‚ç„¶è€Œï¼Œç”±äºç‰¹å¾å›¾å·²åœ¨å‰ä¸€ä¸ªåŒºåŸŸæè®®æ­¥éª¤ä¸­è®¡ç®—ï¼Œå› æ­¤åœ¨è¿™é‡Œå®ƒä»¬åªæ˜¯è¢«é‡å¤ä½¿ç”¨ã€‚è¿™ç§æŠ€æœ¯æœ‰ä¸¤ä¸ªå¥½å¤„ï¼š
- en: '**Sharing the weights**: If we were to use a different CNN, we would have to
    store the weights for two backbonesâ€”one for the RPN, and one for the classification.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…±äº«æƒé‡**ï¼šå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„CNNï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä¸ºä¸¤ä¸ªéª¨å¹²ç½‘ç»œå­˜å‚¨æƒé‡â€”â€”ä¸€ä¸ªç”¨äºRPNï¼Œå¦ä¸€ä¸ªç”¨äºåˆ†ç±»ã€‚'
- en: '**Sharing the computation**: For one input image, we only compute one feature
    volume instead of two. As this operation is the most expensive of the whole network,
    not having to run it twice allows for a consequent gain in computational performance.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…±äº«è®¡ç®—**ï¼šå¯¹äºä¸€ä¸ªè¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬åªè®¡ç®—ä¸€ä¸ªç‰¹å¾ä½“ç§¯ï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªã€‚ç”±äºè¿™ä¸€æ“ä½œæ˜¯æ•´ä¸ªç½‘ç»œä¸­æœ€æ˜‚è´µçš„éƒ¨åˆ†ï¼Œé¿å…é‡å¤è®¡ç®—å¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—æ€§èƒ½ã€‚'
- en: Faster R-CNN architecture
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNNæ¶æ„
- en: 'The second stage of Faster R-CNN accepts the feature maps from the first stage,
    as well as the list of RoIs. For each RoI, convolutional layers are applied to
    obtain class predictions and **bounding box refinement** information. The operations
    are represented here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNNçš„ç¬¬äºŒé˜¶æ®µæ¥å—ç¬¬ä¸€é˜¶æ®µçš„ç‰¹å¾å›¾ï¼Œä»¥åŠRoIåˆ—è¡¨ã€‚å¯¹äºæ¯ä¸ªRoIï¼Œåº”ç”¨å·ç§¯å±‚ä»¥è·å¾—ç±»åˆ«é¢„æµ‹å’Œ**è¾¹ç•Œæ¡†ç²¾ç»†è°ƒæ•´**ä¿¡æ¯ã€‚æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š
- en: '![](img/aa0b02c5-788f-4935-9266-12fbee0b6000.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa0b02c5-788f-4935-9266-12fbee0b6000.png)'
- en: Figure 5.12:Â Architecture summary of Faster R-CNN
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.12ï¼šFaster R-CNNçš„æ¶æ„æ€»ç»“
- en: 'Step by step, the process is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: é€æ­¥è¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: Accept the feature maps and the RoIs from the RPN step. The RoIs generated in
    the original image coordinate system are converted into the feature map coordinate
    system. In our example, the stride of the CNN is 16\. Therefore, their coordinates
    are divided by 16.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥å—æ¥è‡ªRPNæ­¥éª¤çš„ç‰¹å¾å›¾å’ŒRoIã€‚åŸå§‹å›¾åƒåæ ‡ç³»ç»Ÿä¸­ç”Ÿæˆçš„RoIè¢«è½¬æ¢ä¸ºç‰¹å¾å›¾åæ ‡ç³»ç»Ÿã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼ŒCNNçš„æ­¥å¹…ä¸º16ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„åæ ‡ä¼šè¢«é™¤ä»¥16ã€‚
- en: Resize each RoI to make it fit the input of the fully connected layers.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¯ä¸ªRoIçš„å¤§å°ï¼Œä½¿å…¶é€‚åº”å…¨è¿æ¥å±‚çš„è¾“å…¥ã€‚
- en: Apply the fully connected layer. It is very similar to the final layers of any
    convolutional network. We obtain a feature vector.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åº”ç”¨å…¨è¿æ¥å±‚ã€‚å®ƒä¸ä»»ä½•å·ç§¯ç½‘ç»œçš„æœ€ç»ˆå±‚éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚
- en: Apply two different convolutional layers. One handles the classification (called
    **cls**) and the other handles the refinement of the RoI (called **rgs**).
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åº”ç”¨ä¸¤å±‚ä¸åŒçš„å·ç§¯å±‚ã€‚ä¸€å±‚å¤„ç†åˆ†ç±»ï¼ˆç§°ä¸º**cls**ï¼‰ï¼Œå¦ä¸€å±‚å¤„ç†RoIçš„ç²¾ç»†è°ƒæ•´ï¼ˆç§°ä¸º**rgs**ï¼‰ã€‚
- en: The final results are the class scores and bounding box refinement floats that
    we will be able to post-process to generate the final output of the model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆç»“æœæ˜¯ç±»åˆ«åˆ†æ•°å’Œè¾¹ç•Œæ¡†ç²¾ç»†è°ƒæ•´æµ®åŠ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶è¿›è¡Œåå¤„ç†ä»¥ç”Ÿæˆæ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºã€‚
- en: The size of the feature volume depends on the size of the input and the architecture
    of the CNN. For instance, for VGG-16, the size of theÂ feature volume is *w* Ã—
    *h* Ã— *512*, where *w = input_width/16* and *h = input_height/16*. We say that
    VGG-16 has a stride of 16 because one pixel in the feature map equals 16 pixels
    in the input image.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾ä½“ç§¯çš„å¤§å°å–å†³äºè¾“å…¥çš„å¤§å°å’ŒCNNçš„æ¶æ„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºVGG-16ï¼Œç‰¹å¾ä½“ç§¯çš„å¤§å°æ˜¯*w* Ã— *h* Ã— *512*ï¼Œå…¶ä¸­*w = input_width/16*ï¼Œ*h
    = input_height/16*ã€‚æˆ‘ä»¬è¯´VGG-16çš„æ­¥å¹…ä¸º16ï¼Œå› ä¸ºç‰¹å¾å›¾ä¸­çš„ä¸€ä¸ªåƒç´ ç­‰äºè¾“å…¥å›¾åƒä¸­çš„16ä¸ªåƒç´ ã€‚
- en: While convolutional networks can accept inputs of any size (as they use a sliding
    window over the image), the final fully connected layer (between steps 2 and 3)
    accepts a feature volume of a fixed size as an input. And since region proposals
    are of different sizes (a vertical rectangle for a person, a square for an apple...),
    this makes the final layer impossible to use as is.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å·ç§¯ç½‘ç»œå¯ä»¥æ¥å—ä»»ä½•å¤§å°çš„è¾“å…¥ï¼ˆå› ä¸ºå®ƒä»¬åœ¨å›¾åƒä¸Šä½¿ç”¨æ»‘åŠ¨çª—å£ï¼‰ï¼Œä½†æœ€ç»ˆçš„å…¨è¿æ¥å±‚ï¼ˆåœ¨æ­¥éª¤2å’Œæ­¥éª¤3ä¹‹é—´ï¼‰åªèƒ½æ¥å—å›ºå®šå¤§å°çš„ç‰¹å¾ä½“ç§¯ä½œä¸ºè¾“å…¥ã€‚ç”±äºåŒºåŸŸæè®®çš„å¤§å°ä¸åŒï¼ˆä¾‹å¦‚ï¼Œäººçš„å‚ç›´çŸ©å½¢ã€è‹¹æœçš„æ­£æ–¹å½¢â€¦â€¦ï¼‰ï¼Œè¿™ä½¿å¾—æœ€ç»ˆå±‚æ— æ³•ç›´æ¥ä½¿ç”¨ã€‚
- en: To circumvent that, a technique was introduced in Fast R-CNNâ€”**region of interest
    pooling** (**RoI** **pooling**). This converts a variable-size area of the feature
    map into a fixed-size area. The resized feature area can then be passed to the
    final classification layers.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»•è¿‡è¿™ä¸ªé—®é¢˜ï¼Œåœ¨Fast R-CNNä¸­å¼•å…¥äº†ä¸€ç§æŠ€æœ¯â€”â€”**å…´è¶£åŒºåŸŸæ± åŒ–**ï¼ˆ**RoI** **æ± åŒ–**ï¼‰ã€‚å®ƒå°†ç‰¹å¾å›¾çš„å¯å˜å¤§å°åŒºåŸŸè½¬æ¢ä¸ºå›ºå®šå¤§å°çš„åŒºåŸŸã€‚è°ƒæ•´å¤§å°åçš„ç‰¹å¾åŒºåŸŸå¯ä»¥ä¼ é€’ç»™æœ€ç»ˆçš„åˆ†ç±»å±‚ã€‚
- en: RoI pooling
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoIæ± åŒ–
- en: The goal of the RoI pooling layer is simpleâ€”to take a part of the activation
    map of variable size and convert it into a fixed size. The input activation map
    sub-window is of size *hÂ Ã— w*. The target activation map is of size *H* Ã— *W*.
    RoI pooling works by dividing its input into a grid where each cell is of size
    *h/H* Ã— *w/W*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: RoIæ± åŒ–å±‚çš„ç›®æ ‡å¾ˆç®€å•â€”â€”å°†ä¸€ä¸ªå¤§å°å¯å˜çš„æ¿€æ´»å›¾éƒ¨åˆ†è½¬æ¢ä¸ºå›ºå®šå¤§å°ã€‚è¾“å…¥æ¿€æ´»å›¾çš„å­çª—å£å¤§å°ä¸º*h Ã— w*ï¼Œç›®æ ‡æ¿€æ´»å›¾çš„å¤§å°ä¸º*H Ã— W*ã€‚RoIæ± åŒ–é€šè¿‡å°†è¾“å…¥åˆ’åˆ†ä¸ºä¸€ä¸ªç½‘æ ¼æ¥å·¥ä½œï¼Œæ¯ä¸ªå•å…ƒæ ¼çš„å¤§å°ä¸º*h/H*
    Ã— *w/W*ã€‚
- en: 'Let''s use an example. If the input is of size *h* Ã— *w = 5Â *Ã— *4*, and the
    target activation map is of size *H* Ã— *W = 2* Ã— *2*, then each cell should be
    of size *2.5* Ã— *2*. Because we can only use integers, we will make some cells
    of size *3* Ã— *2* and others of size *2* Ã— *2*. Then, we will take the maximum
    of each cell:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜ã€‚å¦‚æœè¾“å…¥çš„å¤§å°æ˜¯*h* Ã— *w = 5 Ã— 4*ï¼Œç›®æ ‡æ¿€æ´»å›¾çš„å¤§å°æ˜¯*H* Ã— *W = 2 Ã— 2*ï¼Œé‚£ä¹ˆæ¯ä¸ªå•å…ƒæ ¼çš„å¤§å°åº”è¯¥æ˜¯*2.5*
    Ã— *2*ã€‚ç”±äºæˆ‘ä»¬åªèƒ½ä½¿ç”¨æ•´æ•°ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿æŸäº›å•å…ƒæ ¼çš„å¤§å°ä¸º*3* Ã— *2*ï¼Œå…¶ä»–å•å…ƒæ ¼çš„å¤§å°ä¸º*2* Ã— *2*ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å–æ¯ä¸ªå•å…ƒæ ¼çš„æœ€å¤§å€¼ï¼š
- en: '![](img/c10a0d84-1b1d-4139-9ef1-f9ffc8c49095.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c10a0d84-1b1d-4139-9ef1-f9ffc8c49095.png)'
- en: Figure 5.13:Â Example of RoI pooling with an RoI of size 5 Ã— 4 (from B3 to E7)
    and an output of size 2Â Ã— 2 (from J4 to K5)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.13ï¼šRoIæ± åŒ–ç¤ºä¾‹ï¼Œå…¶ä¸­RoIçš„å¤§å°ä¸º5 Ã— 4ï¼ˆä»B3åˆ°E7ï¼‰ï¼Œè¾“å‡ºå¤§å°ä¸º2 Ã— 2ï¼ˆä»J4åˆ°K5ï¼‰
- en: An RoI pooling layer is very similar to a max-pooling layer. The difference
    is that RoI pooling works with inputs of variable size, while max-pooling works
    with a fixed size only. RoI pooling is sometimes referred to as **RoI max-pooling**.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: RoIæ± åŒ–å±‚éå¸¸ç±»ä¼¼äºæœ€å¤§æ± åŒ–å±‚ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼ŒRoIæ± åŒ–èƒ½å¤Ÿå¤„ç†å¤§å°å¯å˜çš„è¾“å…¥ï¼Œè€Œæœ€å¤§æ± åŒ–åªèƒ½å¤„ç†å›ºå®šå¤§å°çš„è¾“å…¥ã€‚RoIæ± åŒ–æœ‰æ—¶è¢«ç§°ä¸º**RoIæœ€å¤§æ± åŒ–**ã€‚
- en: In the original R-CNN paper, RoI pooling had not yet been introduced. Therefore,
    each RoI was extracted from the original image, resized, and directly passed to
    the convolutional network. Since there were around 2,000 RoIs, it was extremely
    slow. The *Fast*Â in Fast R-CNN comes from the huge speedup introduced by the RoI
    pooling layer.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹çš„R-CNNè®ºæ–‡ä¸­ï¼ŒRoIæ± åŒ–å°šæœªå¼•å…¥ã€‚å› æ­¤ï¼Œæ¯ä¸ªRoIæ˜¯ä»åŸå§‹å›¾åƒä¸­æå–çš„ï¼Œç»è¿‡è°ƒæ•´å¤§å°åç›´æ¥ä¼ é€’ç»™å·ç§¯ç½‘ç»œã€‚ç”±äºå¤§çº¦æœ‰2,000ä¸ªRoIï¼Œè¿™æ ·åšéå¸¸æ…¢ã€‚Fast
    R-CNNä¸­çš„*Fast*æ¥æºäºRoIæ± åŒ–å±‚å¸¦æ¥çš„å·¨å¤§åŠ é€Ÿã€‚
- en: Training Faster R-CNN
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒFaster R-CNN
- en: 'Before we explain how to train the network, let''s have a look at the full
    architecture of Faster R-CNN:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬è§£é‡Šå¦‚ä½•è®­ç»ƒç½‘ç»œä¹‹å‰ï¼Œå…ˆæ¥çœ‹çœ‹Faster R-CNNçš„å®Œæ•´æ¶æ„ï¼š
- en: '![](img/c4d852b4-bffc-49e3-9c77-0d5c54b0c056.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4d852b4-bffc-49e3-9c77-0d5c54b0c056.png)'
- en: Figure 5.14:Â Full architecture of Faster R-CNN. Note that it can work with any
    input size
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.14ï¼šFaster R-CNNçš„å®Œæ•´æ¶æ„ã€‚æ³¨æ„ï¼Œå®ƒå¯ä»¥å¤„ç†ä»»ä½•è¾“å…¥å¤§å°
- en: Because of its unique architecture, Faster R-CNN cannot be trained like a regular
    CNN. If each of the two parts of the network were trained separately, the feature
    extractors of each part would not share the same weights. In the next section,
    we will explain the training of each section and how to make the two sections
    share the convolutional weights.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¶ç‹¬ç‰¹çš„æ¶æ„ï¼ŒFaster R-CNNä¸èƒ½åƒæ™®é€šçš„CNNä¸€æ ·è¿›è¡Œè®­ç»ƒã€‚å¦‚æœå°†ç½‘ç»œçš„ä¸¤ä¸ªéƒ¨åˆ†åˆ†åˆ«è®­ç»ƒï¼Œä¸¤ä¸ªéƒ¨åˆ†çš„ç‰¹å¾æå–å™¨å°†æ— æ³•å…±äº«ç›¸åŒçš„æƒé‡ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è§£é‡Šå¦‚ä½•è®­ç»ƒæ¯ä¸ªéƒ¨åˆ†ï¼Œä»¥åŠå¦‚ä½•ä½¿è¿™ä¸¤ä¸ªéƒ¨åˆ†å…±äº«å·ç§¯æƒé‡ã€‚
- en: Training the RPN
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒRPN
- en: The input of the RPN is an image, and the output is a list of RoIs. As we saw
    previously, there are *H* Ã— *W* Ã— *k* proposals for each image (where *H* and
    *W* represent the size of a feature map and *k* is the number of anchors). At
    this step, the class of the object is not yet considered.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: RPNçš„è¾“å…¥æ˜¯å›¾åƒï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªRoIåˆ—è¡¨ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œå¯¹äºæ¯å¼ å›¾åƒï¼Œæœ‰*H* Ã— *W* Ã— *k*ä¸ªæè®®ï¼ˆå…¶ä¸­*H*å’Œ*W*ä»£è¡¨ç‰¹å¾å›¾çš„å¤§å°ï¼Œ*k*æ˜¯é”šç‚¹çš„æ•°é‡ï¼‰ã€‚åœ¨è¿™ä¸€æ­¥ï¼Œç‰©ä½“çš„ç±»åˆ«å°šæœªè¢«è€ƒè™‘ã€‚
- en: It would be difficult to train all the proposals at onceâ€”since images are mostly
    made of background, most of the proposals would be trained to predict *background*.
    As a consequence, the network would learn to always predict background. Instead,
    a sampling technique is favored.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡æ€§è®­ç»ƒæ‰€æœ‰å€™é€‰æ¡†ä¼šå¾ˆå›°éš¾â€”â€”ç”±äºå›¾åƒå¤§å¤šç”±èƒŒæ™¯ç»„æˆï¼Œå¤§å¤šæ•°å€™é€‰æ¡†å°†è¢«è®­ç»ƒä¸ºé¢„æµ‹*èƒŒæ™¯*ã€‚å› æ­¤ï¼Œç½‘ç»œå°†å­¦ä¹ å§‹ç»ˆé¢„æµ‹èƒŒæ™¯ã€‚ç›¸åï¼Œæ›´å€¾å‘äºä½¿ç”¨é‡‡æ ·æŠ€æœ¯ã€‚
- en: Mini-batches of 256 ground truth anchors are built; 128 of them are positive
    (they contain an object), and the other 128 are negative (they only contain background).
    If there are fewer than 128 positive samples in the image, all the positive samples
    available are used and the batch is filled with negative samples.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºäº†256ä¸ªå®é™…é”šç‚¹çš„å°æ‰¹é‡ï¼›å…¶ä¸­128ä¸ªæ˜¯æ­£æ ·æœ¬ï¼ˆåŒ…å«ç‰©ä½“ï¼‰ï¼Œå¦å¤–128ä¸ªæ˜¯è´Ÿæ ·æœ¬ï¼ˆä»…åŒ…å«èƒŒæ™¯ï¼‰ã€‚å¦‚æœå›¾åƒä¸­æ­£æ ·æœ¬å°‘äº128ä¸ªï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ­£æ ·æœ¬ï¼Œå¹¶ç”¨è´Ÿæ ·æœ¬å¡«å……æ‰¹æ¬¡ã€‚
- en: The RPN loss
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RPNæŸå¤±
- en: 'The RPN loss is simpler than YOLO''s. It is composed of two terms:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: RPNæŸå¤±æ¯”YOLOçš„æŸå¤±æ›´ç®€å•ã€‚å®ƒç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
- en: '![](img/aed32ea7-fda1-442a-84b6-6b7f5b3f8862.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aed32ea7-fda1-442a-84b6-6b7f5b3f8862.png)'
- en: 'The termsÂ in theÂ preceding equation can be explained as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢å…¬å¼ä¸­çš„å„é¡¹å¯ä»¥è§£é‡Šå¦‚ä¸‹ï¼š
- en: '*i*Â is the index of an anchor in a training batch.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i*æ˜¯è®­ç»ƒæ‰¹æ¬¡ä¸­ä¸€ä¸ªé”šç‚¹çš„ç´¢å¼•ã€‚'
- en: '*p[i]* is the probability of the anchor being an object. *p[i]** is the ground
    truthâ€”it''s 1 if the anchor is "positive"; otherwise, it''s 0.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[i]*æ˜¯é”šç‚¹ä¸ºç‰©ä½“çš„æ¦‚ç‡ã€‚*p[i]**æ˜¯å®é™…å€¼â€”â€”å¦‚æœé”šç‚¹æ˜¯â€œæ­£æ ·æœ¬â€ï¼Œåˆ™ä¸º1ï¼›å¦åˆ™ä¸º0ã€‚'
- en: '*t[i]* is the vector representing coordinate refinement;Â *t[i]** is the ground
    truth.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[i]*æ˜¯è¡¨ç¤ºåæ ‡ä¿®æ­£çš„å‘é‡ï¼›*t[i]**æ˜¯å®é™…å€¼ã€‚'
- en: '*N[cls]* is the number of ground truth anchors in the training mini-batch.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[cls]*æ˜¯è®­ç»ƒå°æ‰¹é‡ä¸­å®é™…é”šç‚¹çš„æ•°é‡ã€‚'
- en: '*N[reg]* is the number of possible anchor locations.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[reg]*æ˜¯å¯èƒ½çš„é”šç‚¹ä½ç½®æ•°é‡ã€‚'
- en: '*L[cls]*Â is the log loss over two classes (object and background).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[cls]*æ˜¯ä¸¤ä¸ªç±»åˆ«ï¼ˆç‰©ä½“å’ŒèƒŒæ™¯ï¼‰çš„å¯¹æ•°æŸå¤±ã€‚'
- en: '*Î»* is a balancing parameter to balance the two parts of the loss.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î»*æ˜¯ä¸€ä¸ªå¹³è¡¡å‚æ•°ï¼Œç”¨äºå¹³è¡¡æŸå¤±çš„ä¸¤ä¸ªéƒ¨åˆ†ã€‚'
- en: 'Finally, the loss is composed of *L[reg](t[i], t[i]*) = R(t[i] - t[i]*)*, where
    R is the *smooth* L1 loss function, defined as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒæŸå¤±ç”± *L[reg](t[i], t[i]*) = R(t[i] - t[i]*)* ç»„æˆï¼Œå…¶ä¸­Ræ˜¯*å¹³æ»‘*L1æŸå¤±å‡½æ•°ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](img/e47ce6d0-1db7-40fa-b1d2-f385079918e9.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e47ce6d0-1db7-40fa-b1d2-f385079918e9.png)'
- en: The *smooth[L1]* function was introduced as a replacement for the L2 loss used
    previously. When the error was too important, the L2 loss would become too large,
    causing training instability.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¹³æ»‘[L1]*å‡½æ•°è¢«å¼•å…¥ï¼Œä½œä¸ºæ›¿ä»£å…ˆå‰ä½¿ç”¨çš„L2æŸå¤±ã€‚å½“è¯¯å·®è¿‡å¤§æ—¶ï¼ŒL2æŸå¤±ä¼šå˜å¾—è¿‡å¤§ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚'
- en: Just as with YOLO, the regression loss is used only for anchor boxes that contain
    an object thanks to the *p[i]** term. The two parts are divided by *N[cls]* and
    *N[reg]*. Those two values are called **normalization terms**â€”if we were to change
    the size of mini-batches, the loss would not lose its equilibrium.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚YOLOä¸€æ ·ï¼Œå›å½’æŸå¤±ä»…ç”¨äºåŒ…å«ç‰©ä½“çš„é”šç‚¹æ¡†ï¼Œè¿™è¦å½’åŠŸäº*p[i]**é¡¹ã€‚ä¸¤éƒ¨åˆ†é€šè¿‡*N[cls]*å’Œ*N[reg]*è¿›è¡Œåˆ’åˆ†ã€‚è¿™ä¸¤ä¸ªå€¼è¢«ç§°ä¸º**å½’ä¸€åŒ–é¡¹**â€”â€”å¦‚æœæˆ‘ä»¬æ”¹å˜å°æ‰¹é‡çš„å¤§å°ï¼ŒæŸå¤±ä¸ä¼šå¤±å»å¹³è¡¡ã€‚
- en: Finally, lambda is a balancing parameter. In the paper configuration,Â *N[cls]
    ~= 256* and *N[reg] ~= 2,400*. The authors set *Î»* to 10 so that the two terms
    have the same total weight.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œlambdaæ˜¯ä¸€ä¸ªå¹³è¡¡å‚æ•°ã€‚åœ¨è®ºæ–‡é…ç½®ä¸­ï¼Œ*N[cls] ~= 256* å’Œ *N[reg] ~= 2,400*ã€‚ä½œè€…å°†*Î»*è®¾ç½®ä¸º10ï¼Œä»¥ä¾¿ä¸¤ä¸ªé¡¹å…·æœ‰ç›¸åŒçš„æ€»æƒé‡ã€‚
- en: 'In summary, similar to YOLO, the loss penalizes the following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œç±»ä¼¼äºYOLOï¼ŒæŸå¤±æƒ©ç½šä»¥ä¸‹å†…å®¹ï¼š
- en: The error in objectness classification with the first term
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªé¡¹ä¸­ç‰©ä½“åˆ†ç±»çš„è¯¯å·®
- en: The error in bounding box refinement with the second term
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒé¡¹ä¸­è¾¹ç•Œæ¡†ä¿®æ­£çš„è¯¯å·®
- en: However, contrary to YOLO's loss, it does not deal with object classes bceause
    the RPN only predicts RoIs. Apart from the loss and the way mini-batches are constructed,
    the RPN is trained like any other network using backpropagation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸YOLOçš„æŸå¤±ç›¸åï¼Œå®ƒå¹¶ä¸å¤„ç†ç‰©ä½“ç±»åˆ«ï¼Œå› ä¸ºRPNåªé¢„æµ‹RoIã€‚é™¤äº†æŸå¤±å’Œå°æ‰¹é‡æ„å»ºæ–¹å¼å¤–ï¼ŒRPNçš„è®­ç»ƒæ–¹å¼ä¸å…¶ä»–ç½‘ç»œç›¸åŒï¼Œä½¿ç”¨åå‘ä¼ æ’­ã€‚
- en: Fast R-CNN loss
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fast R-CNNæŸå¤±
- en: 'As stated earlier, the second stage of Faster R-CNN is also referred to as
    Fast R-CNN. Therefore, its loss is often referenced as the Fast R-CNN loss. While
    the formulation of the Fast R-CNN loss is different to the RPN loss, it is very
    similar in essence:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒFaster R-CNNçš„ç¬¬äºŒé˜¶æ®µä¹Ÿè¢«ç§°ä¸ºFast R-CNNã€‚å› æ­¤ï¼Œå…¶æŸå¤±é€šå¸¸è¢«ç§°ä¸ºFast R-CNNæŸå¤±ã€‚å°½ç®¡Fast R-CNNæŸå¤±çš„å…¬å¼ä¸RPNæŸå¤±ä¸åŒï¼Œä½†åœ¨æœ¬è´¨ä¸Šéå¸¸ç›¸ä¼¼ï¼š
- en: '![](img/4f9b7e1d-4c9c-4cdb-9b7a-985a6e11a49d.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f9b7e1d-4c9c-4cdb-9b7a-985a6e11a49d.png)'
- en: 'The terms in the preceding equation can be explained as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: å‰è¿°æ–¹ç¨‹ä¸­çš„æœ¯è¯­å¯ä»¥è§£é‡Šå¦‚ä¸‹ï¼š
- en: '*L[cls](p,u)* is the log loss between the ground truth class, *u*, and the
    class probabilities, *p*.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[cls](p,u)* æ˜¯å®é™…ç±»åˆ« *u* ä¸ç±»åˆ«æ¦‚ç‡ *p* ä¹‹é—´çš„å¯¹æ•°æŸå¤±ã€‚'
- en: '*L[loc](t^u, v)* is the same loss as L[reg] in the RPN loss.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[loc](t^u, v)* ä¸ RPN æŸå¤±ä¸­çš„ L[reg] ç›¸åŒã€‚'
- en: '*Î»[u â‰¥ 1]* is equal to 1 when u â‰¥ 1 and 0 otherwise.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î»[u â‰¥ 1]* åœ¨ u â‰¥ 1 æ—¶ç­‰äº 1ï¼Œå¦åˆ™ä¸º 0ã€‚'
- en: During Fast R-CNN training, we always use a background class with *id = 0*.
    Indeed, the RoIs may contain background regions, and it is important to classify
    them as such. The term *Î»[u â‰¥ 1]* avoids penalizing the bounding box error for
    background boxes. For all the other classes, since *u* will be above *0*, we will
    penalize the error.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Fast R-CNN è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å§‹ç»ˆä½¿ç”¨èƒŒæ™¯ç±»åˆ«ï¼Œ*id = 0*ã€‚äº‹å®ä¸Šï¼ŒRoI å¯èƒ½åŒ…å«èƒŒæ™¯åŒºåŸŸï¼Œä¸”å°†å…¶åˆ†ç±»ä¸ºèƒŒæ™¯æ˜¯éå¸¸é‡è¦çš„ã€‚æœ¯è¯­ *Î»[u
    â‰¥ 1]* ç”¨äºé¿å…æƒ©ç½šèƒŒæ™¯æ¡†çš„è¾¹ç•Œæ¡†è¯¯å·®ã€‚å¯¹äºæ‰€æœ‰å…¶ä»–ç±»åˆ«ï¼Œç”±äº *u* å°†å¤§äº *0*ï¼Œæˆ‘ä»¬å°†æƒ©ç½šè¯¯å·®ã€‚
- en: Training regimen
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ–¹æ¡ˆ
- en: 'As described earlier, sharing the weights between the two parts of the network
    allows the model to be faster (as the CNN is only applied once) and lighter. In
    the Faster R-CNN paper, the recommended training procedure is called **4-step
    alternating training**. A simplified version of this procedure goes like this:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œåœ¨ç½‘ç»œçš„ä¸¤éƒ¨åˆ†ä¹‹é—´å…±äº«æƒé‡å¯ä»¥ä½¿æ¨¡å‹æ›´å¿«ï¼ˆå› ä¸º CNN åªåº”ç”¨ä¸€æ¬¡ï¼‰ä¸”æ›´è½»ã€‚åœ¨ Faster R-CNN è®ºæ–‡ä¸­ï¼Œæ¨èçš„è®­ç»ƒè¿‡ç¨‹è¢«ç§°ä¸º **å››æ­¥äº¤æ›¿è®­ç»ƒ**ã€‚è¯¥è¿‡ç¨‹çš„ç®€åŒ–ç‰ˆæœ¬å¦‚ä¸‹ï¼š
- en: Train the RPN so that it predicts acceptable RoIs.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒ RPNï¼Œä½¿å…¶é¢„æµ‹å¯æ¥å—çš„ RoIã€‚
- en: Train the classification part using the output of the trained RPN. At the end
    of the training, the RPN and the classification part have different convolutional
    weights since they have been trained separately.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è®­ç»ƒå¥½çš„ RPN è¾“å‡ºè®­ç»ƒåˆ†ç±»éƒ¨åˆ†ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œç”±äº RPN å’Œåˆ†ç±»éƒ¨åˆ†åˆ†åˆ«è®­ç»ƒï¼Œå®ƒä»¬çš„å·ç§¯æƒé‡ä¸åŒã€‚
- en: Replace the RPN's CNN with the classification's CNN so that they now share convolutional
    weights. Freeze the shared CNN weights. Train the RPN's last layers again.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”¨åˆ†ç±»éƒ¨åˆ†çš„ CNN æ›¿æ¢ RPN çš„ CNNï¼Œä½¿å®ƒä»¬å…±äº«å·ç§¯æƒé‡ã€‚å†»ç»“å…±äº«çš„ CNN æƒé‡ã€‚é‡æ–°è®­ç»ƒ RPN çš„æœ€åå‡ å±‚ã€‚
- en: Train the classification's last layer using the output of the RPN again.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†æ¬¡ä½¿ç”¨ RPN çš„è¾“å‡ºè®­ç»ƒåˆ†ç±»éƒ¨åˆ†çš„æœ€åä¸€å±‚ã€‚
- en: At the end of this process, we obtain a trained network with the two parts sharing
    the convolutional weights.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹ç»“æŸåï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªè®­ç»ƒå¥½çš„ç½‘ç»œï¼Œå…¶ä¸­ä¸¤éƒ¨åˆ†å…±äº«å·ç§¯æƒé‡ã€‚
- en: TensorFlow Object Detection API
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow ç‰©ä½“æ£€æµ‹ API
- en: As Faster R-CNN is always improving, we do not provide a reference implementation
    with this book. Instead, we recommend using the TensorFlow Object Detection API.
    It offers an implementation of Faster R-CNN that's maintained by contributors
    and by the TensorFlow team. It offers pretrained models and code to train your
    own model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº Faster R-CNN å§‹ç»ˆåœ¨æ”¹è¿›ï¼Œæˆ‘ä»¬æ²¡æœ‰æä¾›æœ¬ä¹¦çš„å‚è€ƒå®ç°ã€‚ç›¸åï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ TensorFlow ç‰©ä½“æ£€æµ‹ APIã€‚å®ƒæä¾›äº†ä¸€ä¸ªç”±è´¡çŒ®è€…å’Œ
    TensorFlow å›¢é˜Ÿç»´æŠ¤çš„ Faster R-CNN å®ç°ï¼Œæä¾›äº†é¢„è®­ç»ƒæ¨¡å‹å’Œè®­ç»ƒè‡ªå·±æ¨¡å‹çš„ä»£ç ã€‚
- en: The Object Detection API is not part of the core TensorFlow library, but is
    available in a separate repository, which was introduced in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*:Â [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹ API ä¸å±äºæ ¸å¿ƒ TensorFlow åº“çš„ä¸€éƒ¨åˆ†ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„ä»“åº“æä¾›ï¼Œè¯¦ç»†ä»‹ç»è§ [ç¬¬ 4 ç« ](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)ï¼Œ*å½±å“åŠ›åˆ†ç±»å·¥å…·*ï¼š[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)ã€‚
- en: Using a pretrained model
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
- en: The object detection API comes with several pretrained models trained on the
    COCO dataset. The models vary in architectureâ€”while they are all based on Faster
    R-CNN, they use different parameters and backbones. This has an impact on inference
    speed and performance. A rule of thumb is that the inference time grows with the
    mean average precision.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹ API æä¾›äº†å¤šä¸ªåœ¨ COCO æ•°æ®é›†ä¸Šè®­ç»ƒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åœ¨æ¶æ„ä¸Šæœ‰æ‰€ä¸åŒâ€”â€”è™½ç„¶å®ƒä»¬éƒ½åŸºäº Faster R-CNNï¼Œä½†ä½¿ç”¨äº†ä¸åŒçš„å‚æ•°å’Œéª¨å¹²ç½‘ç»œã€‚è¿™å¯¹æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½äº§ç”Ÿäº†å½±å“ã€‚ä¸€ä¸ªç»éªŒæ³•åˆ™æ˜¯ï¼Œæ¨ç†æ—¶é—´éšç€å¹³å‡ç²¾åº¦çš„æé«˜è€Œå¢åŠ ã€‚
- en: Training on a custom dataset
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒ
- en: It is also possible to train a model to detect objects that are not in the COCO
    dataset. To do so, a large amount of data is needed. In general, it is recommended
    to have at least 1,000 samples per object class. To generate a training set, training
    images need to be manually annotated by drawing the bounding boxes around them.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥æ£€æµ‹ä¸åœ¨ COCO æ•°æ®é›†ä¸­çš„ç‰©ä½“ã€‚ä¸ºæ­¤ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå»ºè®®æ¯ä¸ªç‰©ä½“ç±»åˆ«è‡³å°‘æœ‰ 1,000 ä¸ªæ ·æœ¬ã€‚ä¸ºäº†ç”Ÿæˆè®­ç»ƒé›†ï¼Œéœ€è¦æ‰‹åŠ¨æ ‡æ³¨è®­ç»ƒå›¾åƒï¼Œæ–¹æ³•æ˜¯ç”»å‡ºç‰©ä½“çš„è¾¹ç•Œæ¡†ã€‚
- en: Using the Object Detection API does not involve writing Python code. Instead,
    the architecture is defined using configuration files. We recommend starting from
    an existing configuration and working from there to obtain good performance. A
    walk-through is available in this chapter's repository.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç‰©ä½“æ£€æµ‹ API ä¸æ¶‰åŠç¼–å†™ Python ä»£ç ã€‚ç›¸åï¼Œæ¶æ„æ˜¯é€šè¿‡é…ç½®æ–‡ä»¶æ¥å®šä¹‰çš„ã€‚æˆ‘ä»¬å»ºè®®ä»ç°æœ‰çš„é…ç½®æ–‡ä»¶å¼€å§‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œè°ƒæ•´ä»¥è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚æœ¬ç« çš„ä»£ç åº“ä¸­æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ã€‚
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: We covered the architecture of two object detection models. The first one, YOLO,
    is known for its inference speed. We went through the general architecture and
    how inference works, as well as the training procedure. We also detailed the loss
    used to train the model. The second one, Faster R-CNN, is known for its state-of-the-art
    performance. We analyzed the two stages of the network and how to train them.
    We also described how to use Faster R-CNN through the TensorFlow Object Detection
    API.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æ¶æ„ã€‚ç¬¬ä¸€ä¸ªæ˜¯ YOLOï¼Œä»¥æ¨ç†é€Ÿåº¦å¿«è‘—ç§°ã€‚æˆ‘ä»¬è®²è§£äº†å…¶æ•´ä½“æ¶æ„ä»¥åŠæ¨ç†çš„å·¥ä½œåŸç†ï¼Œæ­¤å¤–è¿˜è®²è§£äº†è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜è¯¦ç»†è¯´æ˜äº†ç”¨äºè®­ç»ƒæ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚ç¬¬äºŒä¸ªæ˜¯
    Faster R-CNNï¼Œä»¥å…¶æœ€å…ˆè¿›çš„æ€§èƒ½è‘—ç§°ã€‚æˆ‘ä»¬åˆ†æäº†ç½‘ç»œçš„ä¸¤ä¸ªé˜¶æ®µä»¥åŠå¦‚ä½•è®­ç»ƒå®ƒä»¬ã€‚æˆ‘ä»¬è¿˜æè¿°äº†å¦‚ä½•é€šè¿‡ TensorFlow ç‰©ä½“æ£€æµ‹ API ä½¿ç”¨
    Faster R-CNNã€‚
- en: In the next chapter, we will extend object detection further by learning how
    to segment images into meaningful parts, as well as how to transform and enhance
    them.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†é€šè¿‡å­¦ä¹ å¦‚ä½•å°†å›¾åƒåˆ†å‰²æˆæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œä»¥åŠå¦‚ä½•è½¬æ¢å’Œå¢å¼ºå®ƒä»¬ï¼Œè¿›ä¸€æ­¥æ‰©å±•ç‰©ä½“æ£€æµ‹ã€‚
- en: Questions
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜
- en: What is the difference between a bounding box, an anchor box, and a ground truth
    box?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾¹ç•Œæ¡†ã€é”šæ¡†å’ŒçœŸå®æ¡†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- en: What is the role of the feature extractor?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‰¹å¾æå–å™¨çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
- en: What model should be favored, YOLO or Faster R-CNN?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åº”è¯¥åå‘ä½¿ç”¨å“ªç§æ¨¡å‹ï¼ŒYOLO è¿˜æ˜¯ Faster R-CNNï¼Ÿ
- en: What does the use of anchor boxes entail?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é”šæ¡†æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
- en: Further reading
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥é˜…è¯»
- en: '*Mastering OpenCV 4* ([https://www.packtpub.com/application-development/mastering-opencv-4-third-edition](https://www.packtpub.com/application-development/mastering-opencv-4-third-edition)),byRoy
    Shilkrot and David MillÃ¡n EscrivÃ¡, contains practical computer vision projects,
    including advanced object detection techniques.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç²¾é€š OpenCV 4* ([https://www.packtpub.com/application-development/mastering-opencv-4-third-edition](https://www.packtpub.com/application-development/mastering-opencv-4-third-edition))ï¼Œç”±
    Roy Shilkrot å’Œ David MillÃ¡n EscrivÃ¡ ç¼–å†™ï¼ŒåŒ…å«äº†å®é™…çš„è®¡ç®—æœºè§†è§‰é¡¹ç›®ï¼ŒåŒ…æ‹¬é«˜çº§ç‰©ä½“æ£€æµ‹æŠ€æœ¯ã€‚'
- en: '*OpenCV 4 Computer Vision Application Programming Cookbook* ([https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition](https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition)),
    by David MillÃ¡n EscrivÃ¡ and Robert Laganiere, covers classical object descriptors
    as well as object detection concepts.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenCV 4 è®¡ç®—æœºè§†è§‰åº”ç”¨ç¼–ç¨‹å®æˆ˜* ([https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition](https://www.packtpub.com/application-development/opencv-4-computer-vision-application-programming-cookbook-fourth-edition))ï¼Œç”±David
    MillÃ¡n EscrivÃ¡ å’Œ Robert Laganiere ç¼–å†™ï¼Œæ¶µç›–äº†ç»å…¸çš„ç‰©ä½“æè¿°ç¬¦ä»¥åŠç‰©ä½“æ£€æµ‹æ¦‚å¿µã€‚'
