- en: Neural Sentiment Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经情感分析
- en: In this chapter, we are going to address one of the hot and trendy applications
    in natural language processing, which is called **sentiment analysis**. Most people
    nowadays express their opinions about something through social media platforms,
    and making use of this vast amount of text to keep track of customer satisfaction
    about something is very crucial for companies or even governments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论自然语言处理领域中的一个热门应用，即**情感分析**。如今，大多数人通过社交媒体平台表达他们的意见，利用这一海量文本来跟踪客户对某事的满意度，对于公司甚至政府来说都是非常重要的。
- en: 'In this chapter, we are going to use recurrent-type neural networks to build
    a sentiment analysis solution. The following topics will be addressed in this
    chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用递归类型的神经网络来构建情感分析解决方案。本章将涉及以下主题：
- en: General sentiment analysis architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般的情感分析架构
- en: Sentiment analysis—model implementation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析——模型实现
- en: General sentiment analysis architecture
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一般的情感分析架构
- en: In this section, we are going to focus on the general deep learning architectures
    that can be used for sentiment analysis. The following figure shows the processing
    steps that are required for building the sentiment analysis model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论可以用于情感分析的一般深度学习架构。下图展示了构建情感分析模型所需的处理步骤。
- en: 'So, first off, we are going to deal with natural human language:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，我们将处理自然语言：
- en: '![](img/93bdac3a-0ed3-447d-84f2-8fae6e0ce5b0.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93bdac3a-0ed3-447d-84f2-8fae6e0ce5b0.png)'
- en: 'Figure 1: A general pipeline for sentiment analysis solutions or even sequence-based
    natural language solutions'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：情感分析解决方案或基于序列的自然语言解决方案的一般管道
- en: We are going to use movie reviews to build this sentiment analysis application.
    The goal of this application is to produce positive and negative reviews based
    on the input raw text. For example, if the raw text is something like, **This
    movie is good**, then we need the model to produce a positive sentiment for it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用电影评论来构建这个情感分析应用程序。这个应用程序的目标是根据输入的原始文本生成正面或负面评论。例如，如果原始文本是类似于**这部电影很好**的内容，那么我们需要模型为其生成一个正面情感。
- en: A sentiment analysis application will take us through a lot of processing steps
    that are needed to work with natural human languages inside a neural network such
    as embeddings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个情感分析应用程序将带我们经历许多必要的处理步骤，这些步骤是神经网络中处理自然语言所必需的，例如词嵌入。
- en: So in this case, we have a raw text, for example, **This is not a good movie!** What
    we want to end up with is whether this is a negative or positive sentiment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，我们有一个原始文本，例如**这不是一部好电影！** 我们希望最终得到的是它是负面还是正面的情感。
- en: 'There are several difficulties in this type of application:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的应用程序中，有几个难点：
- en: One of them is that the sequences may have **different lengths**. This is a
    very short one, but we will see examples of text that have more than 500 words.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中之一是序列可能具有**不同的长度**。这是一个非常短的序列，但我们会看到一些文本，它们的长度超过了500个单词。
- en: Another problem is that if we just look at individual words (for example, good),
    that indicates a positive sentiment. However, it is preceded by the word **not**,
    so now it's a negative sentiment. This can get a lot more complicated, and we
    will see an example of it later.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是，如果我们仅仅看单独的单词（例如，good），它表示的是正面情感。然而，它前面有一个**not**，所以现在它变成了负面情感。这可能变得更加复杂，我们稍后会看到一个例子。
- en: As we learned in the previous chapter, a neural network cannot work on raw text,
    so we need to first convert it into what are called **tokens**. These are basically
    just integer values, so we go through our entire dataset and we count the number
    of times each word is being used. Then, we make a vocabulary and each word gets
    an index in this vocabulary. So the word **this** has an integer ID or token **11**,
    the word **is** has a token **6**, **not** has a token **21**, and so forth. So
    now, we have converted the raw text into a list of integers called tokens.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章所学的，神经网络无法直接处理原始文本，因此我们需要先将其转换成所谓的**词元**。这些基本上就是整数值，所以我们遍历整个数据集，统计每个词出现的次数。然后，我们创建一个词汇表，每个词在该词汇表中都会有一个索引。因此，单词**this**有一个整数ID或词元**11**，单词**is**的词元是**6**，**not**的词元是**21**，依此类推。现在，我们已经将原始文本转换成了一个由整数构成的列表，称为词元。
- en: A neural network still cannot operate on this data, because if we have a vocabulary
    of 10,000 words, the tokens can take values between 0 and 9,999, and they may
    not be related at all. So, word number 998 may have a completely different semantic
    meaning than word number 999.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络仍然无法处理这些数据，因为如果我们有一个包含10,000个单词的词汇表，那么这些标记的值可以在0到9,999之间变化，而它们之间可能没有任何关联。因此，单词编号998和单词编号999的语义可能完全不同。
- en: Therefore, we will use the idea of representation learning or embeddings that
    we learned about in the previous chapter. This embedding layer converts integer
    tokens into real-valued vectors, so token **11** becomes the vector [0.67,0.36,...,0.39],
    as shown in *Figure 1*. The same applies to the next token number 6.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将使用上一章学习的表示学习或嵌入的概念。这个嵌入层将整数标记转换为实值向量，例如，标记**11**变为向量[0.67,0.36,...,0.39]，如*图1*所示。对于下一个标记6也是如此。
- en: 'A quick recap of what we studied in the previous chapter: this embedding layer
    in the preceding figure learns the mapping between tokens and their corresponding
    real-valued vector. Also, the embedding layer learns the semantic meanings of
    the words so that words that have similar meanings are somehow close to each other
    in this embedding space.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章学习内容的简要回顾：前面图中的嵌入层学习的是标记（tokens）与其对应的实值向量之间的映射关系。同时，嵌入层还学习单词的语义含义，使得具有相似含义的单词在这个嵌入空间中会彼此接近。
- en: Out of the input raw text, we get a two-dimensional matrix, or tensor, which
    can now be inputted to the **recurrent neural network** (**RNN**). This can process
    sequences of arbitrary length and the output of this network is then fed into
    a fully connected or dense layer with a sigmoid activation function. So, the output
    is between 0 and 1, where a value of 0 is taken to mean a negative sentiment.
    But what if the value of the sigmoid function is neither 0 nor 1? Then we need
    to introduce a cut-off or a threshold value in the middle so that if the value
    is below 0.5, then the corresponding input is taken to be a negative sentiment,
    and a value above this threshold is taken to be a positive sentiment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始输入文本中，我们得到一个二维矩阵或张量，它现在可以作为输入传递给**递归神经网络**（**RNN**）。该网络可以处理任意长度的序列，其输出随后会传递到一个全连接层或密集层，并使用sigmoid激活函数。因此，输出值介于0和1之间，其中0表示负面情感。那么，如果sigmoid函数的值既不是0也不是1该怎么办？此时我们需要引入一个中间的切割值或阈值，当该值低于0.5时，认为对应的输入是负面情感，而当该值高于此阈值时，则认为是正面情感。
- en: RNNs – sentiment analysis context
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN——情感分析的背景
- en: 'Now, let''s recap the basic concepts of RNNs and also talk about them in the
    context of the sentiment analysis application. As we mentioned in the RNN chapter,
    the basic building block of a RNN is a recurrent unit, as shown in this figure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下RNN的基本概念，并在情感分析应用的背景下讨论它们。正如我们在RNN章节中提到的，RNN的基本构建块是递归单元，如图所示：
- en: '![](img/9b9d9701-39be-4d45-938e-abb4c10c7bc3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b9d9701-39be-4d45-938e-abb4c10c7bc3.png)'
- en: 'Figure 2: An abstract idea of an RNN unit'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：RNN单元的抽象概念
- en: This figure is an abstraction of what goes on inside the recurrent unit. What
    we have here is the input, so this would be a word, for example, **good**. Of
    course, it has to be converted to embedding vectors. However, we will ignore that
    for now. Also, this unit has a kind of memory state, and depending on the contents
    of this **State** and the **Input**, we will update this state and write new data
    into the state. For example, imagine that we have previously seen the word **not** in
    the input; we write that to the state so that when we see the word **good** on
    one of the following inputs, we know from the state that we have just seen the
    word **not**. Now, we see the word **good**. Thus, we have to write into the state
    that we have seen the words **not good** together so that this might indicate
    that the whole input text probably has a negative sentiment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图是对递归单元内部运作的抽象。在这里，我们有输入数据，例如，单词**good**。当然，它需要被转换为嵌入向量。然而，我们暂时忽略这一部分。此外，这个单元还有一种内存状态，根据**State**和**Input**的内容，我们会更新这个状态并将新数据写入状态。例如，假设我们之前在输入中看到过单词**not**，我们将其写入状态，这样当我们在后续输入中看到单词**good**时，我们可以从状态中得知之前见过单词**not**。现在，我们看到单词**good**，因此，我们必须在状态中写入已见过**not
    good**这两个单词，这可能表明整个输入文本的情感是负面的。
- en: The mapping from the old state and the input to the new contents of the state
    is done through a so-called **Gate**, and the way these are implemented differs
    across different versions of recurrent units. It is basically a matrix operation
    with an activation function, but as we will see in a moment, there is a problem
    with backpropagating gradients. So, the RNN has to be designed in a special way
    so that the gradients are not distorted too much.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从旧状态和输入到新状态内容的映射是通过所谓的**门控**来完成的，这些门控在不同版本的递归单元中有不同的实现方式。它基本上是一个矩阵运算加上激活函数，但正如我们稍后会看到的，反向传播梯度时会遇到问题。因此，RNN必须以一种特殊方式设计，以避免梯度被过度扭曲。
- en: 'In a recurrent unit, we have a similar gate for producing the output, and once
    again the output of the recurrent unit depends on the current contents of the
    state and the input that we are seeing. So what we can try and do is unroll the
    processing that takes place with a recurrent unit:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归单元中，我们有一个类似的门控来生成输出，再次强调，递归单元的输出依赖于当前状态的内容和我们正在看到的输入。所以我们可以尝试展开递归单元中的处理过程：
- en: '![](img/5cb1e335-63bd-4471-b9c1-cff84f65718f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cb1e335-63bd-4471-b9c1-cff84f65718f.png)'
- en: 'Figure 3: Unrolled version of the recurrent neural net'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：递归神经网络的展开版本
- en: 'Now, what we have here is just one recurrent unit, but the flow chart shows
    what happens at different time steps. So:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们看到的是一个递归单元，但流程图展示了不同时间步发生的情况。所以：
- en: In time step 1, we input the word **this** to the recurrent unit and it has
    its internal memory state first initialized to zero. This is done by TensorFlow
    whenever we start processing a new sequence of data. So, we see the word **this** and
    the recurrent unit state is 0\. Hence, we use the internal gate to update the
    memory state and **this** is then used in time step number two where we input
    the word **is**; now, the memory state has some contents. There's not a whole
    lot of meaning in the word **this**, so the state might still be around 0.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步 1，我们将单词**this**输入到递归单元中，它的内部记忆状态首先初始化为零。当我们开始处理新的数据序列时，TensorFlow 会执行此操作。所以我们看到单词**this**，而递归单元的状态是
    0。因此，我们使用内部门控来更新记忆状态，**this**随后在时间步 2 被使用，在此时间步我们输入单词**is**，此时记忆状态已有内容。**this**这个词的意义并不大，因此状态可能仍然接近
    0。
- en: And there's also not a lot of meaning in **is**, so perhaps the state is still
    somewhat 0.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而且**is**也没有太多的意义，所以状态可能仍然接近 0。
- en: In the next time step, we see the word **not**, and this has meaning we ultimately
    want to predict, which is the sentiment of the whole input text. This one is what
    we need to store in the memory so that the gate inside the recurrent unit sees
    that the state already probably contains near-zero values. But now it wants to
    store what we have just seen the word **not**, so it saves some nonzero value
    in this state.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个时间步，我们看到单词**not**，这具有我们最终想要预测的意义，即整个输入文本的情感。这个信息需要存储在记忆中，以便递归单元中的门控能够看到该状态已经可能包含接近零的值。但现在它需要存储我们刚刚看到的单词**not**，因此它在该状态中保存了一些非零值。
- en: Then, we move on to the next time step, where we have the word **a**; this also
    doesn't have much information, so it's probably just ignored. It just copies over
    the state.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们进入下一个时间步，看到单词**a**，这个也没有太多信息，所以可能会被忽略。它只是将状态复制过去。
- en: Now, we have the word **very**, and this indicates that whatever sentiment exists
    might be a strong sentiment, so the recurrent unit now knows that we have seen
    **not **and **very**. It stores this somehow in its memory state.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们看到单词**very**，这表示任何情感可能是强烈的情感，因此递归单元现在知道我们已经看到了**not**和**very**。它以某种方式将其存储在内存状态中。
- en: In the next time step, we see the word **good**, so now the network knows **not
    very good** and it thinks, *Oh, this is probably a negative sentiment!* Hence,
    it stores that value in the internal state.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个时间步，我们看到单词**good**，所以现在网络知道**not very good**，并且它想，*哦，这可能是负面情感！*因此，它将这个值存储在内部状态中。
- en: Then, in the final time step, we see **movie**, and this is not really relevant,
    so it's probably just ignored.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，在最后一个时间步，我们看到**movie**，这实际上与情感无关，因此可能会被忽略。
- en: Next, we use the other gate inside the recurrent unit to output the contents
    of the memory state, and then it is processed with the sigmoid function (which
    we don't show here). We get an output value between 0 and 1.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们使用递归单元中的另一个门控输出记忆状态的内容，然后通过 sigmoid 函数处理（这里没有展示）。我们得到一个介于 0 和 1 之间的输出值。
- en: 'The idea then is that we want to train this network on many many thousands
    of examples of movie reviews from the Internet Movie database, where, for each
    input text, we give it the true sentiment value of either positive or negative.
    Then, we want TensorFlow to find out what the gates inside the recurrent unit
    should be so that they accurately map this input text to the correct sentiment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，我们希望在互联网上的电影评论数据集（Internet Movie Database）上训练这个网络，其中，对于每个输入文本，我们都会提供正确的情感值——正面或负面。接着，我们希望
    TensorFlow 能找出循环单元内部的门控应该是什么，以便它能够准确地将这个输入文本映射到正确的情感：
- en: '![](img/adf86d1b-4eba-43b5-b736-15177296aad2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adf86d1b-4eba-43b5-b736-15177296aad2.png)'
- en: 'Figure 4: Used architecture for this chapter''s implementation'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：本章实现所用的架构
- en: The architecture for the RNN we will be using in this implementation is an RNN-type
    architecture with three layers. In the first layer, what we've just explained happens, except
    that now we need to output the value from the recurrent unit at each time step.
    Then, we gather a new sequence of data, which is the output of the first recurrent
    layer. Next, we can input it to the second recurrent layer because recurrent units
    need sequences of input data (and the output that we got from the first layer
    and the one that we want to feed into the second recurrent layer are some floating-point
    values whose meanings we don't really understand). This has a meaning inside the
    RNN, but it's not something we as humans will understand. Then, we do similar
    processing in the second recurrent layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个实现中将使用的 RNN 架构是一个具有三层的 RNN 类型架构。在第一层，我们刚才解释的过程会发生，只是现在我们需要在每个时间步输出来自循环单元的值。然后，我们收集新的数据序列，即第一循环层的输出。接下来，我们可以将它输入到第二个循环层，因为循环单元需要输入数据的序列（而我们从第一层得到的输出和我们想要输入到第二个循环层的是一些浮点值，其含义我们并不完全理解）。这在
    RNN 内部有其意义，但我们作为人类并不能理解它。然后，我们在第二个循环层中进行类似的处理。
- en: So, first, we initialize the internal memory state of this recurrent unit to
    0; then, we take the first output from the first recurrent layer and input it.
    We process it with the gates inside this recurrent unit, update the state, take
    the output of the first layer's recurrent unit for the second word **is**, and
    use that as input as well as the internal memory state. We continue doing this
    until we have processed the whole sequence, and then we gather up all the outputs
    of the second recurrent layer. We use them as inputs in the third recurrent layer,
    where we do a similar processing. But here, we only want the output for the last
    time step, which is a kind of summary for everything that has been fed so far.
    We then output that to a fully connected layer that we don't show here. Finally,
    we have the sigmoid activation function, so we get a value between zero and one,
    which represents negative and positive sentiments, respectively.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，我们将这个循环单元的内部记忆状态初始化为 0；然后，我们取第一个循环层的第一个输出并输入。我们用循环单元内部的门控处理它，更新状态，取第一个层循环单元的输出作为第二个词**is**的输入，并使用该输入和内部记忆状态。我们继续这样做，直到处理完整个序列，然后我们将第二个循环层的所有输出收集起来。我们将它们作为输入传递给第三个循环层，在那里我们进行类似的处理。但在这里，我们只需要最后一个时间步的输出，它是迄今为止输入的所有内容的摘要。然后，我们将它输出到一个全连接层，这里没有展示。最后，我们使用
    sigmoid 激活函数，因此我们得到一个介于 0 和 1 之间的值，分别表示负面和正面情感。
- en: Exploding and vanishing gradients - recap
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度爆炸和梯度消失——回顾
- en: As we mentioned in the previous chapter, there's a phenomenon called **exploding**
    and **vanishing** of gradients values, which is very important in RNNs. Let's
    go back and look at *Figure 1*; that flowchart explains what this phenomenon is.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章提到的，存在一种现象叫做**梯度爆炸**和**梯度消失**，它在 RNN 中非常重要。让我们回过头来看一下*图 1*；该流程图解释了这个现象是什么。
- en: Imagine we have a text with 500 words in this dataset that we will be using
    to implement our sentiment analysis classifier. At every time step, we apply the
    internal gates in the recurrent unit in a recursive manner; so if there are 500
    words, we will apply these gates 500 times to update the internal memory state
    of the recurrent unit.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含 500 个词的文本数据集，这将用于实现我们的情感分析分类器。在每个时间步，我们以递归方式应用循环单元内的门控；因此，如果有 500
    个词，我们将在 500 次时间步中应用这些门控，以更新循环单元的内部记忆状态。
- en: As we know, the way neural networks are trained is by using so-called backpropagation
    of gradients, so we have some loss function that gets the output of the neural
    network and then the true output that we desire for the given input text. Then,
    we want to minimize this loss value so that the actual output of the neural network
    corresponds to the desired output for this particular input text. So, we need
    to take the gradient of this loss function with respect to the weights inside
    these recurrent units, and these weights are for the gates that are updating the
    internal state and outputting the value in the end.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，神经网络的训练方式是通过所谓的梯度反向传播，所以我们有一个损失函数，它获取神经网络的输出，然后是我们希望得到的该输入文本的真实输出。接下来，我们希望最小化这个损失值，以使神经网络的实际输出与此特定输入文本的期望输出相符。因此，我们需要计算这个损失函数关于这些递归单元内部权重的梯度，而这些权重用于更新内部状态并最终输出结果的门控。
- en: Now, the gate is applied maybe 500 times, and if this has a multiplication in
    it, what we essentially get is an exponential function. So, if you multiply a
    value with itself 500 times and if this value is slightly less than 1, then it
    will very quickly vanish or get lost. Similarly, if a value slightly more than
    1 is multiplied with itself 500 times, it'll explode.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个门可能会应用大约 500 次，如果其中有乘法运算，我们实际上得到的是一个指数函数。所以，如果你将一个值与其本身相乘 500 次，并且这个值略小于
    1，那么它将很快消失或丢失。同样地，如果一个值略大于 1，并与其本身相乘 500 次，它将爆炸。
- en: The only values that can survive 500 multiplications are 0 and 1\. They will
    remain the same, so the recurrent unit is actually much more complicated than
    what you see here. This is the abstract idea—that we want to somehow map the internal
    memory state and the input to update the internal memory state and to output some
    value—but in reality, we need to be very careful about propagating the gradients
    backwards through these gates so that we don't have this exponential multiplication
    over many many time steps. We also encourage you to see some tutorials on the
    mathematical definition of recurrent units.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一能在 500 次乘法中生存的值是 0 和 1。它们将保持不变，所以递归单元实际上比你看到的要复杂得多。这是一个抽象的概念——我们希望以某种方式将内部记忆状态和输入映射，用于更新内部记忆状态并输出某个值——但实际上，我们需要非常小心地将梯度反向传播通过这些门，以防止在多次时间步中发生这种指数级的乘法。我们也鼓励你查看一些关于递归单元数学定义的教程。
- en: Sentiment analysis – model implementation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析 – 模型实现
- en: We have seen all the bits and pieces of how to implement a stacked version of
    the LSTM variation of RNNs. To make things a bit exciting, we are going to use
    a higher level API called `Keras`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何实现堆叠版本的 LSTM 变种 RNN。为了让事情更有趣，我们将使用一个更高级的 API，叫做 `Keras`。
- en: Keras
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: '"Keras is a high-level neural networks API, written in Python and capable of
    running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on
    enabling fast experimentation. Being able to go from idea to result with the least
    possible delay is key to doing good research." – Keras website'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '"Keras 是一个高级神经网络 API，使用 Python 编写，可以在 TensorFlow、CNTK 或 Theano 上运行。它的开发重点是快速实验的实现。从想法到结果的转换延迟最小化是做出良好研究的关键。"
    – Keras 网站'
- en: So, Keras is just a wrapper around TensorFlow and other deep learning frameworks.
    It's really good for prototyping and getting things built very quickly, but on
    the other hand, it gives you less control over your code. We'll take a chance
    to implement this sentiment analysis model in Keras so that you get a hands-on
    implementation in both TensorFlow and Keras. You can use Keras for fast prototyping
    and TensorFlow for a production-ready system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Keras 只是 TensorFlow 和其他深度学习框架的一个封装。它非常适合原型设计和快速构建，但另一方面，它让你对代码的控制较少。我们将尝试在
    Keras 中实现这个情感分析模型，这样你可以在 TensorFlow 和 Keras 中获得一个动手实现。你可以使用 Keras 进行快速原型设计，而将
    TensorFlow 用于生产环境的系统。
- en: 'More interesting news for you is that you don''t have to switch to a totally
    different environment. You can now access Keras as a module in TensorFlow and
    import packages just like the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的消息是，你不需要切换到一个完全不同的环境。你现在可以在 TensorFlow 中将 Keras 作为模块访问，并像以下代码一样导入包：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So, let's go ahead and use what we can now call a more abstracted module inside
    TensorFlow that will help us to prototype deep learning solutions very fast. This
    is because we will get to write full deep learning solutions in just a few lines
    of code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续使用现在可以称之为更抽象的TensorFlow模块，它将帮助我们非常快速地原型化深度学习解决方案。因为我们只需几行代码就能写出完整的深度学习解决方案。
- en: Data analysis and preprocessing
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析和预处理
- en: Now, let's move on to the actual implementation where we need to load the data.
    Keras actually has a functionality that can be used to load this sentiment dataset
    from IMDb, but the problem is that it has already mapped all the words to integer
    tokens. This is such an essential part of working with natural human language
    insight neural networks that I really want to show you how to do it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入实际的实现，我们需要加载数据。Keras实际上有一个功能，可以用来从IMDb加载这个情感数据集，但问题是它已经将所有单词映射到整数令牌了。这是处理自然语言与神经网络之间非常关键的一部分，我真的想向你展示如何做到这一点。
- en: Also, if you want to use this code for sentiment analysis of whatever data you
    might have in some other language, you will need to do this yourself, so we have
    just quickly implemented some functions for downloading this dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想将这段代码用于其他语言的情感分析，你需要自己做这个转换，所以我们快速实现了一些下载这个数据集的函数。
- en: 'Let''s start off by importing a bunch of required packages:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入一些必需的包开始：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And then we load the dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载数据集：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, it has 25,000 texts in the training set and in the testing set.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，训练集中有25,000个文本，测试集中也有。
- en: 'Let''s just see one example from the training set and how it looks:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看训练集中的一个例子，它是如何呈现的：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is a fairly short one and the sentiment value is `1.0`, which means it
    is a positive sentiment, so this is a positive review of whatever movie this was
    about.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简短的文本，情感值为`1.0`，这意味着它是一个积极的情感，因此这是一篇关于某部电影的正面评价。
- en: Now, we get to the tokenizer, and this is the first step of processing this
    raw data because the neural network cannot work on text data. Keras has implemented
    what is called a **tokenizer** for building a vocabulary and mapping from words
    to an integer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入了分词器，这也是处理这些原始数据的第一步，因为神经网络不能直接处理文本数据。Keras实现了一个叫做**分词器**的工具，用来构建词汇表并将单词映射到整数。
- en: 'Also, we can say that we want a maximum of 10,000 words, so it will use only the
    10,000 most popular words from the dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以说我们希望最大使用10,000个单词，因此它将只使用数据集中最流行的10,000个单词：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we take all the text from the dataset and we call this function `fit`
    on texts:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从数据集中获取所有文本，并在文本上调用这个函数`fit`：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The tokenizer takes about 10 seconds, and then it will have built the vocabulary.
    It looks like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器大约需要10秒钟，然后它将构建出词汇表。它看起来是这样的：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So, each word is now associated with an integer; therefore, the word `the` has
    number `1`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在每个单词都与一个整数相关联；因此，单词`the`的编号是`1`：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, `and` has number `2`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`and`的编号是`2`：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The word `a` has `3`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 单词`a`的编号是`3`：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And so on. We see that `movie` has number `17`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。我们看到`movie`的编号是`17`：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And `film` has number `19`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 并且`film`的编号是`19`：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: What all this means is that `the` was the most used word in the dataset and
    `and` was the second most used in the dataset. So, whenever we want to map words
    to integer tokens, we will get these numbers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些的意思是，`the`是数据集中使用最多的单词，`and`是第二多的单词。因此，每当我们想要将单词映射到整数令牌时，我们将得到这些编号。
- en: 'Let''s try and take the word number `743` for example, and this was the word
    `romantic`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以单词编号`743`为例，这就是单词`romantic`：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So, whenever we see the word `romantic` in the input text, we map it to the
    token integer `743`. We use the tokenizer again to convert all the words in the
    text in the training set into integer tokens:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，每当我们在输入文本中看到单词`romantic`时，我们将其映射到令牌整数`743`。我们再次使用分词器将训练集中的所有单词转换为整数令牌：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When we convert that text to integer tokens, it becomes an array of integers:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些文本转换为整数令牌时，它就变成了一个整数数组：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: So, the word `this` becomes the number 11, the word `is` becomes the number
    59, and so forth.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，单词`this`变成了编号11，单词`is`变成了编号59，以此类推。
- en: 'We also need to convert the rest of the text:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要转换剩余的文本：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, there's another problem because the sequences of tokens have different
    lengths depending on the length of the original text, even though the recurrent
    units can work with sequences of arbitrary length. But the way that TensorFlow
    works is that all of the data in a batch needs to have the same length.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，还有另一个问题，因为标记的序列长度根据原始文本的长度而有所不同，尽管循环神经网络（RNN）单元可以处理任意长度的序列。但是TensorFlow的工作方式是，批量中的所有数据必须具有相同的长度。
- en: 'So, we can either ensure that all sequences in the entire dataset have the
    same length, or write a custom data generator that ensures that the sequences
    in a single batch have the same length. Now, it is a lot simpler to ensure that
    all the sequences in the dataset have the same length, but the problem is that
    there are some outliers. We have some sentences that, I think, are more than 2,200
    words long. It will hurt our memory very much if we have all the *short* sentences
    with more than 2,200 words. So what we will do instead is make a compromise; first,
    we need to count all the words, or the number of tokens in each of these input
    sequences. What we see is that the average number of words in a sequence is about
    221:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以确保数据集中的所有序列都具有相同的长度，或者编写一个自定义数据生成器，确保单个批次中的序列具有相同的长度。现在，确保数据集中的所有序列具有相同的长度要简单得多，但问题是有一些极端值。我们有一些句子，我认为，它们超过了2,200个单词。如果所有的*短*句子都超过2,200个单词，将极大地影响我们的内存。所以我们做的折衷是：首先，我们需要统计每个输入序列中的单词数，或者标记数。我们看到，序列中单词的平均数大约是221：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And we see that the maximum number of words is more than 2,200:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，最大单词数超过了2200个：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, there's a huge difference between the average and the max, and again we
    would be wasting a lot of memory if we just padded all the sentences in the dataset
    so that they would all have `2208` tokens. This would especially be a problem
    if you have a dataset with millions of sequences of text.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，平均值和最大值之间有很大的差异，如果我们仅仅将数据集中的所有句子都填充到`2208`个标记，这将浪费大量的内存。尤其是如果你有一个包含百万级文本序列的数据集，这个问题就更加严重。
- en: 'So what we will do is make a compromise where we will pad all sequences and
    truncate the ones that are too long so that they have `544` words. The way we
    calculated this was like this—we took the average number of words in all the sequences
    in the dataset and we added two standard deviations:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要做的折衷是，填充所有序列并截断那些太长的序列，使它们有`544`个单词。我们计算这一点的方式是——我们取了数据集中所有序列的平均单词数，并加上了两个标准差：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'What do we get out of this is? We cover about 95% of the text in the dataset,
    so only about 5% are longer than `544` words:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的结果是什么？我们覆盖了数据集中文本的约95%，所以只有大约5%的文本超过了`544`个单词：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we call these functions in Keras. They will either pad the sequences that
    are too short (so they will just add zeros) or truncate the sequences that are
    too long (basically just cut off some of the words if the text is too long).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们调用Keras中的这些函数。它们会填充那些太短的序列（即只会添加零），或者截断那些太长的序列（如果文本过长，基本上会删除一些单词）。
- en: 'Now, there''s an important thing here: we can do this padding and truncating
    in pre or post mode. So imagine we have a sequence of integer tokens and we want
    to pad this because it''s too short. We can:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有一个重要的点：我们可以选择在预处理模式（pre mode）或后处理模式（post mode）下进行填充和截断。假设我们有一个整数标记的序列，并且我们希望填充它，因为它太短了。我们可以：
- en: Either pad all of these zeros at the beginning so that we have the actual integer
    tokens down at the end.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要么在开头填充所有这些零，这样我们就可以把实际的整数标记放在最后。
- en: Or do it in the opposite way so that we have all this data at the beginning
    and then all the zeros at the end. But if we just go back and look at the preceding
    RNN flowchart, remember that it is processing the sequence one step at a time
    so if we start processing zeros, it will probably not mean anything and the internal
    state would have probably just remain zero. So, whenever it finally sees an integer
    token for a specific word, it will know okay now we start processing the data.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者以相反的方式进行处理，将所有数据放在开头，然后将所有的零放在末尾。但是，如果我们回头看看前面的RNN流程图，记住它是一步一步地处理序列的，所以如果我们开始处理零，它可能没有任何意义，内部状态可能会保持为零。因此，每当它最终看到特定单词的整数标记时，它就会知道，好，现在开始处理数据了。
- en: However, if all the zeros were at the end, we would have started processing
    all the data; then we'd have some internal state inside the recurrent unit. Right
    now, we see a whole lot of zeros, so that might actually destroy the internal
    state that we have just calculated. This is why it might be a good idea to have
    the zeros padded at the beginning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果所有的零都在末尾，那么我们将开始处理所有数据；接着，我们会在递归单元内部有一些内部状态。现在，我们看到的是一堆零，这实际上可能会破坏我们刚刚计算出来的内部状态。这就是为什么将零填充到开始处可能是个好主意。
- en: But the other problem is when we truncate a text, so if the text is very long,
    we will truncate it to get it to fit to `544` words, or whatever the number was.
    Now, imagine we've caught this sentence here in the middle somewhere and it says
    **this very good movie** or **this is not**. You know, of course, that we do this only for
    very long sequences, but it is possible that we lose essential information for
    properly classifying this text. So it is a compromise that we're making when we
    are truncating input text. A better way would be to create a batch and just pad
    text inside that batch. So, when we see a very very long sequence, we pad the
    other sequences to have the same length. But we don't need to store all of this
    data in memory because most of it is wasted.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但另一个问题是当我们截断文本时，如果文本非常长，我们会将其截断到`544`个单词，或者其他任何数字。现在，假设我们抓住了这句话，它在中间某个地方，并且它说的是**这部非常好的电影**或**这不是**。你当然知道，我们只有在处理非常长的序列时才会这样做，但很可能我们会丢失一些关键信息，无法正确分类这段文本。所以，当我们截断输入文本时，这是我们所做的妥协。更好的方法是创建一个批次，并在该批次中填充文本。所以，当我们看到非常非常长的序列时，我们将填充其他序列，使它们具有相同的长度。但我们不需要将所有这些数据都存储在内存中，因为其中大部分是浪费的。
- en: 'Let''s go back and convert the entire dataset so that it is truncated and padded;
    thus, it''s one big matrix of data:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到并转换整个数据集，使其被截断并填充；这样，它就变成了一个庞大的数据矩阵：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We check the shape of this matrix:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查这个矩阵的形状：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'So, let''s have a look at specific sample tokens before and after padding:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来看一下在填充前后的特定样本标记：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And after padding, this sample will look like the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 填充后，这个样本将如下所示：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Also, we need a functionality to map backwards so that it maps from integer
    tokens back to text words; we just need that here. It''s a very simple helper
    function, so let''s go ahead and implement it:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要一个功能来进行反向映射，使其能够将整数标记映射回文本单词；我们在这里只需要这个。它是一个非常简单的辅助函数，所以让我们继续实现它：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, for example, the original text in the dataset is like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，举个例子，数据集中的原始文本是这样的：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If we use a helper function to convert the tokens back to text words, we get
    this text:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用一个辅助函数将标记转换回文本单词，我们会得到以下文本：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It's basically the same except for punctuation and other symbols.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上是一样的，只是标点符号和其他符号不同。
- en: Building the model
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: Now, we need to create the RNN, and we will do this in Keras because it's very
    simple. We do that with the so-called `sequential` model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建RNN，我们将在Keras中实现，因为它非常简单。我们使用所谓的`sequential`模型来实现这一点。
- en: The first layer of this architecture will be what is called an **embedding**.
    If we look back at the flowchart in *Figure 1*, what we just did was we converted
    the raw input text to integer tokens. But we still cannot input this to a RNN,
    so we have to convert that into embedding vectors, which are values that are somewhere
    between -1 and 1\. They can exceed to some extent but are generally somewhere
    between -1 and 1, and this is data that we can then work on in the neural network.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的第一层将是所谓的**嵌入层**。如果我们回顾一下*图1*中的流程图，我们刚才做的是将原始输入文本转换为整数标记。但我们仍然无法将其输入到RNN中，所以我们必须将其转换为嵌入向量，这些值介于-1和1之间。它们可能在某种程度上超出这一范围，但通常情况下它们会在-1和1之间，这些是我们可以在神经网络中进行处理的数据。
- en: It's somewhat magical because this embedding layer trains simultaneously with
    the RNN and it doesn't see the raw words. It sees integer tokens but learns to
    recognize that there is some pattern in how words are being used together. So
    it can, sort of, deduce that some words or some integer tokens have similar meaning,
    and then it encodes this in embedding vectors that look somewhat the same.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点像魔法，因为这个嵌入层与RNN同时训练，它看不到原始单词。它看到的是整数标记，但学会了识别单词如何一起使用的模式。所以它可以在某种程度上推断出一些单词或一些整数标记具有相似的意义，然后它将这些信息编码到看起来相似的嵌入向量中。
- en: Therefore, what we need to decide is the length of each vector so that, for
    example, the token "11" gets converted into a real-valued vector. In this example,
    we will use a length of 8, which is actually extremely short (normally, it is
    somewhere between 100 and 300). Try and change this number of elements in the
    embedding vectors and rerun this code to see what you get as a result.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要决定每个向量的长度，例如，“11”这个标记将被转换成一个实值向量。在这个例子中，我们使用长度为8的向量，实际上它非常短（通常是100到300之间）。尝试改变这个嵌入向量中的元素数量，并重新运行这段代码，看看结果会是什么。
- en: 'So, we set the embedding size to 8 and then use Keras to add this embedding
    layer to the RNN. This has to be the first layer in the network:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将嵌入大小设置为8，然后使用Keras将这个嵌入层添加到RNN中。它必须是网络中的第一个层：
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Then, we can add the first recurrent layer, and we will use what is called a
    **Gated Recurrent Unit** (**GRU**). Often, you will see that people use what is
    called **LSTM**, but others seem to suggest that the GRU is better because there
    are gates inside LSTM that are redundant. And indeed the simpler code works just
    as well with fewer gates. You could add a thousand more gates to LSTM and that
    still doesn't mean it gets better.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以添加第一个循环层，我们将使用一个叫做**门控循环单元**（**GRU**）。通常，你会看到人们使用叫做**LSTM**的结构，但有些人似乎认为GRU更好，因为LSTM中有些门是冗余的。实际上，简单的代码在减少门的数量后也能很好地工作。你可以给LSTM加上更多的门，但那并不意味着它会变得更好。
- en: 'So, let''s define our GRU architectures; we say that we want an output dimensionality
    of 16 and we need to return sequences:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们定义我们的GRU架构；我们设定输出维度为16，并且需要返回序列：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we look at the flowchart in *Figure 4*, we want to add a second recurrent
    layer:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下*图4*中的流程图，我们想要添加第二个循环层：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we have the third and final recurrent layer, which will not output a
    sequence because it will be followed by a dense layer; it should only give the
    final output of the GRU and not a whole sequence of outputs:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有第三个也是最后一个循环层，它不会输出一个序列，因为它后面会跟随一个全连接层；它应该只给出GRU的最终输出，而不是一整个输出序列：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, the output here will be fed into a fully connected or dense layer, which
    is just supposed to output one value for each input sequence. This is processed
    with the sigmoid activation function so it outputs a value between 0 and 1:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这里输出的结果将被输入到一个全连接或密集层，这个层应该只输出每个输入序列的一个值。它通过sigmoid激活函数处理，因此输出一个介于0和1之间的值：
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, we say we want to use the Adam optimizer with this learning rate here, and
    the loss function should be the binary cross-entropy between the output from the
    RNN and the actual class value from the training set, which will be a value of
    either 0 or 1:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们说我们想使用Adam优化器，并设定学习率，同时损失函数应该是RNN输出与训练集中的实际类别值之间的二元交叉熵，这个值应该是0或1：
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And now, we can just print a summary of what the model looks like:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以打印出模型的摘要：
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So, as you can see, we have the embedding layer, the first recurrent unit, the
    second, third, and dense layer. Note that this doesn't have a lot of parameters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有嵌入层，第一个循环单元，第二、第三个循环单元和密集层。请注意，这个模型的参数并不多。
- en: Model training and results analysis
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练与结果分析
- en: 'Now, it''s time to kick off the training process, which is very easy here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候开始训练过程了，这里非常简单：
- en: '[PRE37]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s test the trained model against the test set:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试集上测试训练好的模型：
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, let's see an example of some misclassified texts.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些被错误分类的文本示例。
- en: 'So first, we calculate the predicted classes for the first 1,000 sequences
    in the test set and then we take the actual class values. We compare them and
    get a list of indices where this mismatch exists:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先，我们计算测试集中前1,000个序列的预测类别，然后取实际类别值。我们将它们进行比较，并得到一个索引列表，其中包含不匹配的地方：
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Use the cut-off threshold to indicate that all values above `0.5` will be considered
    positive and the others will be considered negative:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用阈值来表示所有大于`0.5`的值将被认为是正类，其他的将被认为是负类：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s get the actual class for these 1,000 sequences:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来获取这1,000个序列的实际类别：
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s get the incorrect samples from the output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从输出中获取错误的样本：
- en: '[PRE42]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'So, we see that there are 122 of these texts that were incorrectly classified;
    that''s 12.1% of the 1,000 texts we calculated here. Let''s look at the first
    misclassified text:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有122个文本被错误分类，占我们计算的1,000个文本的12.1%。让我们来看一下第一个被错误分类的文本：
- en: '[PRE43]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s have a look at the model output for this sample as well as the actual
    class:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个样本的模型输出以及实际类别：
- en: '[PRE46]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, let''s test our trained model against a set of new data samples and see
    its results:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试一下我们训练好的模型，看看它在一组新数据样本上的表现：
- en: '[PRE48]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, let''s convert them to integer tokens:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将它们转换为整数标记：
- en: '[PRE49]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And then pad them:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后进行填充：
- en: '[PRE50]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, let''s run the model against them:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将模型应用于这些数据：
- en: '[PRE51]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: So, a value close to zero means a negative sentiment and a value that's close
    to 1 means a positive sentiment; finally, these numbers will vary every time you
    train the model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，接近零的值意味着负面情感，而接近 1 的值意味着正面情感；最后，这些数字会在每次训练模型时有所变化。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered an interesting application, which is sentiment analysis.
    Sentiment analysis is used by different companies to track customer's satisfaction
    with their products. Even governments use sentiment analysis solutions to track
    citizen satisfaction about something that they want to do in the future.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了一个有趣的应用——情感分析。情感分析被不同的公司用来追踪客户对其产品的满意度。甚至政府也使用情感分析解决方案来跟踪公民对他们未来想做的事情的满意度。
- en: Next up, we are going to focus on some advanced deep learning architectures
    that can be used for semi-supervised and unsupervised applications.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点关注一些可以用于半监督和无监督应用的先进深度学习架构。
