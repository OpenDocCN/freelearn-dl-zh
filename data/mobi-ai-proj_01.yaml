- en: Artificial Intelligence Concepts and Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的概念和基础
- en: This chapter acts as a prelude to the entire book and the concepts within it.
    We will understand these concepts at a level high enough for us to appreciate
    what we will be building throughout the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章作为整本书的序章以及书中相关概念的引导。我们将在足够高的层次上理解这些概念，以便我们能够理解本书中的构建内容。
- en: 'We will start by getting our head around the general structure of **Artificial
    Intelligence** (**AI**) and its building blocks by comparing AI, machine learning,
    and deep learning, as these terms can be used interchangeably. Then, we will skim
    through the history, evolution, and principles behind **Artificial Neural Networks **(**ANNs**).
    Later, we will dive into the fundamental concepts and terms of ANNs and deep learning
    that will be used throughout the book. After that, we take a brief look at the
    TensorFlow Playground to reinforce our understanding of ANNs. Finally, we will
    finish off the chapter with thoughts on where to get a deeper theoretical reference
    for the high-level concepts of the AI and ANN principles covered in this chapter,
    which will be as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过比较人工智能、机器学习和深度学习来理解**人工智能**（**AI**）的整体结构及其构建块，因为这些术语可以互换使用。接着，我们将浏览人工神经网络（ANNs）的历史、演变和原理。然后，我们将深入了解ANNs和深度学习的基本概念和术语，这些将在全书中使用。之后，我们将简要了解TensorFlow
    Playground，以巩固我们对ANNs的理解。最后，我们将以关于在哪里获取人工智能和ANN原理的更深理论参考的思考来结束本章，具体如下：
- en: AI versus machine learning versus deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能与机器学习与深度学习
- en: Evolution of AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能的演变
- en: The mechanics behind ANNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）的机制
- en: Biological neurons
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生物神经元
- en: Working of artificial neurons
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经元的工作原理
- en: Activation and cost functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数与成本函数
- en: Gradient descent, backpropagation, and softmax
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降、反向传播和softmax
- en: TensorFlow Playground
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Playground
- en: AI versus machine learning versus deep learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能与机器学习与深度学习
- en: '**AI** is no new term given the plethora of articles we read online and the
    many movies based on it. So, before we proceed any further, let''s take a step
    back and understand AI and the terms that regularly accompany it from a practitioner''s
    point of view. We will get a clear distinction of what machine learning, deep
    learning, and AI are, as these terms are often used interchangeably:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（AI）并不是一个新术语，因为我们在线阅读了大量相关文章，并且有许多基于该主题的电影。所以，在我们进一步讨论之前，先退一步，从实践者的角度理解人工智能及其常见的术语。我们将清晰地区分机器学习、深度学习和人工智能，因为这些术语经常被交替使用：'
- en: '![](img/d60ec346-e3a6-4851-bab9-25e39508cbcd.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d60ec346-e3a6-4851-bab9-25e39508cbcd.png)'
- en: AI is the capability that can be embedded into machines that allows machines
    to perform tasks that are characteristic of human intelligence. These tasks include
    seeing and recognizing objects, listening and distinguishing sounds, understanding
    and comprehending language, and other similar tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是可以嵌入到机器中的能力，使得机器能够执行具有人的智能特征的任务。这些任务包括看见和识别物体、听见并区分声音、理解和领会语言以及其他类似的任务。
- en: '**Machine learning** (**ML**) is a subset of AI that encompasses techniques
    used to make these human-like tasks possible. So, in a way, ML is what is used
    to achieve AI.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是人工智能（AI）的一个子集，涵盖了使这些类人任务成为可能的技术。因此，从某种意义上讲，机器学习是实现人工智能的手段。'
- en: In essence, if we did not use ML to achieve these tasks, then we would actually
    be trying to write millions of lines of code with complex loops, rules, and decision
    trees.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，如果我们不使用机器学习来实现这些任务，那么我们实际上是在试图编写数百万行复杂的代码，包括循环、规则和决策树。
- en: ML gives machines the ability to learn without being explicitly programmed.
    So, instead of hardcoding rules for every possible scenario to a task, we simply
    provide examples of how the task is done versus how it should not be done. ML
    then trains the system on this provided data so it can learn for itself.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习赋予了机器在没有明确编程的情况下进行学习的能力。因此，机器学习不再是为每个可能的场景硬编码规则，而是通过提供任务执行方式和不应执行方式的示例来进行学习。然后，机器学习系统根据这些数据进行训练，以便它能自我学习。
- en: ML is an approach to AI where we can achieve tasks such as grouping or clustering,
    classifying, recommending, predicting, and forecasting data. Some common examples
    of this are classifying spam mail, stock market predictions, weather forecasting,
    and more.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ML是一种人工智能方法，通过它我们可以完成诸如分组或聚类、分类、推荐、预测和数据预测等任务。常见的例子包括垃圾邮件分类、股市预测、天气预报等。
- en: '**Deep learning** is a special technique in ML that emulates the human brain''s
    biological structure and works to accomplish human-like tasks. This is done by
    building a network of neurons just like in the brain through an algorithmic approach
    using ANNs, which are stack of algorithms that can solve problems at human-like
    efficiency or better.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习** 是机器学习中的一种特殊技术，它模仿了人类大脑的生物结构，并通过神经网络来完成类似人类的任务。这是通过使用人工神经网络（ANNs）——一种通过算法堆叠解决问题的技术——来构建像大脑一样的神经网络，从而以类人甚至更高效的能力解决问题。'
- en: These layers are commonly referenced as **d****eepnets **(deep architectures)
    and each has a specific problem that it can be trained to solve. The deep learning
    space is currently at the cutting edge of what we see today, with applications
    such as autonomous driving, Alexa and Siri, machine vision, and more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层通常被称为**深度网络**（深层架构），每一层都有一个可以训练解决的特定问题。目前，深度学习领域正处于前沿技术，应用包括自动驾驶、Alexa和Siri、机器视觉等。
- en: Throughout this book, we will be executing tasks and building apps that are
    built using these deepnets, and we will also solve use cases by building our very
    own deepnet architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将执行使用这些深度网络（deepnets）构建的任务和应用，并通过构建我们自己的深度网络架构来解决使用案例。
- en: Evolution of AI
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的发展
- en: To appreciate what we can currently do with AI, we need to get a basic understanding
    of how the idea of emulating the human brain was born, and how this idea evolved
    to a point where we can easily solve tasks in vision and language with human-like
    capability through machines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解我们当前在人工智能领域所能做到的事情，我们需要对模仿人类大脑的概念的起源有一个基本的了解，并且了解这个概念是如何发展到如今，通过机器我们能够轻松解决视觉和语言任务，且具有类似人类的能力。
- en: It all started in 1959 when a couple of Harvard scientists, Hubel and Wiesel,
    were experimenting with a cat's visual system by monitoring the primary visual
    cortex in the cat's brain.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一切始于1959年，当时哈佛的几位科学家Hubel和Wiesel通过监测猫的大脑初级视觉皮层，进行猫视觉系统的实验。
- en: The **primary visual cortex** is a collection of neurons in the brain placed
    at the back of the skull and is responsible for processing vision. It is the first
    part of the brain that receives input signals from the eye, very much like how
    a human brain would process vision.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**初级视觉皮层** 是大脑中位于后脑勺的一群神经元，负责处理视觉信息。它是大脑接收来自眼睛输入信号的第一部分，类似于人类大脑如何处理视觉信息。'
- en: 'The scientists started by showing complex pictures such as those of fish, dogs,
    and humans to the cat and observed its primary visual cortex. To their disappointment,
    they got no reading from the primary visual cortex initially. Consequently, to
    their surprise on one of the trials, as they were removing the slides, dark edges
    formed, causing some neurons to fire in the primary visual cortex:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 科学家们从向猫展示复杂的图像（例如鱼、狗和人类）开始，并观察猫的初级视觉皮层。令他们失望的是，初期没有从初级视觉皮层得到任何反应。因此，令他们惊讶的是，在某次实验中，当他们移除幻灯片时，黑暗的边缘形成，导致一些神经元在初级视觉皮层中激活。
- en: '![](img/8abe3823-b8a6-47b1-a11a-ac88a98fd276.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8abe3823-b8a6-47b1-a11a-ac88a98fd276.png)'
- en: Their serendipitous discovery was that these individual neurons or brain cells
    in the primary visual cortex were responding to bars or dark edges at various
    specific orientations. This led to the understanding that the mammalian brain
    processes a very small amount of information at every neuron, and as the information
    is passed from neuron to neuron, more complex shapes, edges, curves, and shades
    are comprehended. So, all these independent neurons holding very basic information
    need to fire together to comprehend a complete complex image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 他们偶然发现，初级视觉皮层中的这些个体神经元或脑细胞会对不同特定方向的条形或黑暗边缘做出反应。这导致了一个理解：哺乳动物的大脑每个神经元处理的信息量非常小，随着信息从一个神经元传递到另一个神经元，越复杂的形状、边缘、曲线和阴影逐渐被理解。因此，这些持有非常基础信息的独立神经元需要一同激活，才能理解一个完整而复杂的图像。
- en: After that, there was a lull in the progress of how to emulate the mammalian
    brain until 1980, when Fukushima proposed neocognitron. **Neocognitron** is inspired
    by the idea that we should be able to create an increasingly complex representation
    using a lot of very simplistic representations – just like the mammalian brain!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，关于如何模仿哺乳动物大脑的进展曾一度停滞，直到1980年，福岛提出了神经认知网络（neocognitron）。**神经认知网络** 的灵感来自于一个想法：我们应该能够通过许多非常简单的表现形式，创造出越来越复杂的表现形式——就像哺乳动物的大脑一样！
- en: 'The following is a representation of how neocognitron works, by Fukushima:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是福岛（Fukushima）提出的神经认知网络（Neocognitron）工作原理的示意图：
- en: '![](img/739593e1-54e0-414c-944d-30ddee2d1473.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/739593e1-54e0-414c-944d-30ddee2d1473.png)'
- en: He proposed that to identify your grandmother, there are a lot of neurons that
    are triggered in the primary visual cortex, and each cell or neuron understands
    an abstract part of the final image of your grandmother. All of these neurons
    work in sequence, parallel, and tandem, and then finally hits a grandmother cell
    or neuron which fires only when it sees your grandmother.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 他提出，为了识别你的祖母，初级视觉皮层中有许多神经元被激活，每个细胞或神经元理解你祖母最终图像的一个抽象部分。这些神经元以顺序、并行和协同的方式工作，最后会激活一个“祖母细胞”或神经元，只有在看到你的祖母时它才会被激活。
- en: Fast forward to today (2010-2018), with contributions from Yoshua Bengio, Yann
    LeCun, and Geoffrey Hinton, who are commonly known as the *fathers of deep learning*.
    They contribute massively to the AI space we work in today. They have given rise
    to a whole new approach to machine learning where feature engineering is automated.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到今天（2010-2018），在Yoshua Bengio、Yann LeCun和Geoffrey Hinton的贡献下，他们被广泛称为*深度学习之父*。他们为我们今天所工作的AI领域做出了巨大的贡献。他们催生了机器学习的一种全新方法，其中特征工程被自动化。
- en: The idea of not explicitly telling the algorithm what it should be looking for
    and letting it figure this out by itself by feeding it a lot of examples is the
    latest development. The analogy to this principle would be that of teaching a
    child to distinguish between an apple and an orange. We would show the child pictures
    of apples and oranges rather than only describing the two fruits' features, such
    as shape, color, size, and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不直接告诉算法它应该寻找什么，而是通过给它提供大量的例子让它自己搞清楚这一点，是最新的发展。这个原理的类比就像是教一个孩子区分苹果和橙子。我们会向孩子展示苹果和橙子的图片，而不仅仅是描述这两种水果的特征，比如形状、颜色、大小等。
- en: 'The following diagram shows the difference between ML and deep learning:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了机器学习和深度学习的区别：
- en: '![](img/bdfe861e-68f8-4f48-bdf4-a2be37b1ae07.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdfe861e-68f8-4f48-bdf4-a2be37b1ae07.png)'
- en: This is the primary difference between traditional ML and ML using neural networks
    (deep learning). In traditional ML, we provide features along with labels, but
    using ANNs, we let the algorithm decipher the features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是传统机器学习与使用神经网络（深度学习）进行机器学习的主要区别。在传统机器学习中，我们提供特征和标签，而在使用人工神经网络时，我们让算法自己解码特征。
- en: We live in an exciting time, an era we share with the fathers of deep learning,
    so much so that there are exchanges online in places such as Stack Exchange, where
    we can see contributions even from Yann LeCun and Geoffrey Hinton. This is analogous
    to living in the time of, and writing to, Nicholas Otto, the father of the internal
    combustion engine, who started the automobile revolution that we see evolving
    even to this day. The automobile revolution will be dwarfed by what could be possible
    with AI in the future. Exciting times, indeed!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在一个激动人心的时代，这个时代我们与深度学习之父共同度过，甚至在Stack Exchange等地方的在线交流中，我们还能看到Yann LeCun和Geoffrey
    Hinton的贡献。这就像是生活在尼古拉斯·奥托（内燃机的发明者）时代，并且向他写信，正是他启动了我们至今仍在演变的汽车革命。而未来AI的潜力将使汽车革命相形见绌。确实是一个令人兴奋的时代！
- en: The mechanics behind ANNs
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络的机制
- en: In this section, we will understand the nuts and bolts that are required to
    start building our own AI projects. We will get to grips with the common terms
    that are used in deep learning techniques.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解构建我们自己AI项目所需的基本要素。我们将掌握深度学习技术中常用的术语。
- en: This section aims to provide the essential theory at a high level, giving you
    enough insight so that you're able to build your own deep neural networks, tune
    them, and understand what it takes to make state-of-the-art neural networks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在提供高层次的基本理论，帮助你获得足够的洞察力，从而能够构建自己的深度神经网络，调整它们，并理解构建最先进的神经网络所需的条件。
- en: Biological neurons
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物神经元
- en: We previously discussed how the biological brain has been an inspiration behind
    ANNs. The brain is made up of hundreds of billions of independent units or cells
    called **neurons**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过生物大脑如何成为人工神经网络（ANN）的灵感来源。大脑由数百亿个独立的单元或细胞组成，这些单元被称为**神经元**。
- en: 'The following diagram depicts a **neuron**, and it has multiple inputs going
    into it, called **d****endrites**. There is also an output going out of the cell
    body, called the **a****xon**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个**神经元**，它有多个输入信号进入，称为**树突**。还有一个从细胞体输出的信号，称为**轴突**：
- en: '![](img/78c9025b-fe58-45e9-8559-f948bf0fa9f3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c9025b-fe58-45e9-8559-f948bf0fa9f3.png)'
- en: The dendrites carry information into the neuron and the axon allows the processed
    information to flow out of the neuron. But in reality, there are thousands of
    dendrites feeding input into the neuron body as small electrical charges. If these
    small electrical charges that are carried by the dendrites have an effect on the
    overall charge of the body or cross over some threshold, then the axon will fire.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 树突将信息带入神经元，而轴突则允许处理后的信息从神经元流出。但实际上，有成千上万的树突将输入以微小电荷的形式传递给神经元。如果这些由树突携带的微小电荷对神经元主体的总电荷产生影响，或超过某个阈值，那么轴突将会触发。
- en: Now that we know how a biological neuron functions, we will understand how an
    artificial neuron works.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了生物神经元的工作原理，接下来我们将理解人工神经元是如何工作的。
- en: Working of artificial neurons
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元的工作原理
- en: 'Just like the biological brain, ANNs are made up of independent units called
    neurons. Like the biological neuron, the artificial neuron has a body that does
    some computation and has many inputs that are feeding into the cell body or neuron:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生物大脑一样，人工神经网络（ANN）由独立的单元组成，这些单元被称为神经元。像生物神经元一样，人工神经元也有一个进行计算的主体，并且有许多输入馈送到细胞体或神经元：
- en: '![](img/823f3760-3563-4a25-bea7-485f97fd83ea.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/823f3760-3563-4a25-bea7-485f97fd83ea.png)'
- en: 'For example, let''s assume we have three inputs to the neuron. Each input carries
    a binary value of 0 or 1\. We have an output flowing out of the body, which also
    carries a binary value of 0 or 1\. For this example, the neuron decides whether
    I should eat a cake today or not. That is, the neuron should fire an output of
    1 if I should eat a cake or fire 0 if I shouldn''t:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有三个输入到神经元。每个输入携带一个 0 或 1 的二进制值。我们有一个从主体流出的输出，它也携带一个 0 或 1 的二进制值。对于这个示例，神经元决定我今天是否应该吃蛋糕。也就是说，如果我应该吃蛋糕，神经元应输出
    1；如果不该吃蛋糕，则输出 0：
- en: '![](img/3646d7fd-ccd3-47cc-a909-2ade266b8aa1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3646d7fd-ccd3-47cc-a909-2ade266b8aa1.png)'
- en: In our example, the three inputs represent the three factors that determine
    whether I should eat the cake or not. Each factor is given a weight of importance;
    for instance, the first factor is **I did cardio yesterday** and it has a weight
    of 2\. The second factor is **I went to the gym yesterday** and weighs 3\. The
    third factor is **It is an occasion for cake** and weighs 6.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，三个输入代表了决定我是否应该吃蛋糕的三个因素。每个因素都有一个重要性权重；例如，第一个因素是 **昨天我做了有氧运动**，权重为 2。第二个因素是
    **昨天我去了健身房**，权重为 3。第三个因素是 **这是一个吃蛋糕的场合**，权重为 6。
- en: 'The body of the neuron does some calculation to inputs, such as taking the
    sum of all of these inputs and checking whether it is over some threshold:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的主体对输入进行一些计算，比如将所有这些输入相加并检查它们是否超过某个阈值：
- en: '![](img/ace25c8d-9dd4-434e-b9e1-c8914bf0d60d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ace25c8d-9dd4-434e-b9e1-c8914bf0d60d.png)'
- en: So, for this example, let's set our threshold as 4\. If the sum of the input
    weights is above the threshold, then the neuron fires an output of 1, indicating
    that I can eat the cake.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对于这个示例，我们设定阈值为 4。如果输入权重的总和超过阈值，则神经元输出 1，表示我可以吃蛋糕。
- en: 'This can be expressed as an equation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以表示为一个方程式：
- en: '** ![](img/6d3e02c1-3c31-484c-af2a-516841aeed6f.png)**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '** ![](img/6d3e02c1-3c31-484c-af2a-516841aeed6f.png)**'
- en: 'Here, the following applies:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里适用以下内容：
- en: '*Xi* is the first input factor, *I did cardio yesterday.*'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xi* 是第一个输入因素，*昨天我做了有氧运动*。'
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi = 2*.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wi* 是第一个输入因素 *Xi* 的权重。在我们的示例中，*Wi = 2*。'
- en: '*Xii* is the second input factor, *I went to the gym yesterday*.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xii* 是第二个输入因素，*昨天我去了健身房*。'
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii =
    3*.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wii* 是第二个输入因素 *Xii* 的权重。在我们的示例中，*Wii = 3*。'
- en: '*Xiii* is the third input factor, *It is an occasion for cake*.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xiii* 是第三个输入因素，*这是一个吃蛋糕的场合*。'
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii=
    6*.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wiii* 是第三个输入因素 *Xiii* 的权重。在我们的示例中，*Wiii= 6*。'
- en: '*threshold* is 4.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*阈值* 为 4。'
- en: Now, let's use this neuron to decide whether I can eat a cake for three different
    scenarios.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用这个神经元来决定在三种不同的情况下我是否可以吃蛋糕。
- en: Scenario 1
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情境 1
- en: 'I want to eat a cake and I went to the gym yesterday, but I did not do cardio,
    nor is it an occasion for cake:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我想吃蛋糕，昨天我去了健身房，但我没有做有氧运动，也不是吃蛋糕的场合：
- en: '![](img/b48ff741-766f-4233-8e04-ddcadf19ae0e.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b48ff741-766f-4233-8e04-ddcadf19ae0e.jpg)'
- en: 'Here, the following applies:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里适用以下内容：
- en: '*Xi* is the first input factor, *I did cardio yesterday*. Now, *Xi= 0,* as
    this is false.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xi* 是第一个输入因素，*昨天我做了有氧运动*。现在，*Xi = 0*，因为这是假的。'
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi= 2*.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wi* 是第一个输入因素 *Xi* 的权重。在我们的例子中，*Wi = 2*。'
- en: '*Xii* is the second input factor, *I went to the gym yesterday*. Now, *Xii =
    1,* as this is true.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xii* 是第二个输入因素 *我昨天去了健身房*。现在，*Xii = 1*，因为这是正确的。'
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii* =
    3.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wii* 是第二个输入因素 *Xii* 的权重。在我们的例子中，*Wii* = 3。'
- en: '*Xiii* is the third input factor, *It is an occasion for cake*. Now, *Xiii=
    0,* as this is false.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xiii* 是第三个输入因素，*这时是吃蛋糕的时机*。现在，*Xiii = 0*，因为这个条件是错误的。'
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii* =
    6.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wiii* 是第三个输入因素 *Xiii* 的权重。在我们的例子中，*Wiii* = 6。'
- en: '*threshold* is 4.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*threshold* 是 4。'
- en: 'We know that the neuron computes the following equation:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道神经元计算以下方程：
- en: '![](img/f1cfb10c-000e-46eb-b95e-eca2430d94a0.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1cfb10c-000e-46eb-b95e-eca2430d94a0.png)'
- en: 'For scenario 1, the equation will translate to this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于场景 1，方程将转化为以下形式：
- en: '![](img/eea9930f-44c4-49bc-ab8a-1175b1b98d9b.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eea9930f-44c4-49bc-ab8a-1175b1b98d9b.png)'
- en: 'This is equal to this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于这个：
- en: '![](img/c72373a8-0c4a-4348-a339-a1bb34dee2ac.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c72373a8-0c4a-4348-a339-a1bb34dee2ac.png)'
- en: '*3 ≥ 4* is false, so it fires 0, which means I should not eat the cake.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*3 ≥ 4* 是假的，所以它输出 0，这意味着我不应该吃蛋糕。'
- en: Scenario 2
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景 2
- en: 'I want to eat a cake and it''s my birthday, but I did not do cardio, nor did
    I go to the gym yesterday:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我想吃蛋糕，而且今天是我的生日，但我没有做有氧运动，也没有去健身房：
- en: '![](img/c3b90c6b-70fb-4205-ae37-f004f3859436.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3b90c6b-70fb-4205-ae37-f004f3859436.png)'
- en: 'Here, the following applies:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下条件成立：
- en: '*Xi* is the first input factor, *I did cardio yesterday*. Now, *Xi= 0,* as
    this factor is false.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xi* 是第一个输入因素，*我昨天做了有氧运动*。现在，*Xi = 0*，因为这个因素是错误的。'
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi= 2*.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wi* 是第一个输入因素 *Xi* 的权重。在我们的例子中，*Wi = 2*。'
- en: '*Xii* is the second input factor, *I went to the gym yesterday*. Now, *Xii =
    0,* as this factor is false.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xii* 是第二个输入因素 *我昨天去了健身房*。现在，*Xii = 0*，因为这个因素是错误的。'
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii* =
    3.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wii* 是第二个输入因素 *Xii* 的权重。在我们的例子中，*Wii* = 3。'
- en: '*Xiii* is the third input factor, *It is an occasion for cake*. Now, *Xiii=
    1,* this factor is true.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xiii* 是第三个输入因素，*这时是吃蛋糕的时机*。现在，*Xiii = 1*，这个因素为真。'
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii* =
    6.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wiii* 是第三个输入因素 *Xiii* 的权重。在我们的例子中，*Wiii* = 6。'
- en: '*threshold* is 4.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*threshold* 是 4。'
- en: 'We know that the neuron computes the following equation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道神经元计算以下方程：
- en: '![](img/2f874d55-53b7-4829-a9be-6b010448f7ba.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f874d55-53b7-4829-a9be-6b010448f7ba.png)'
- en: 'For scenario 2, the equation will translate to this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于场景 2，方程将转化为以下形式：
- en: '![](img/5a277032-ef63-4868-b8d3-0e9101a89af3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a277032-ef63-4868-b8d3-0e9101a89af3.png)'
- en: 'It gives us the following output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '![](img/b326d771-dc1f-4461-acc1-976ccc80bcaa.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b326d771-dc1f-4461-acc1-976ccc80bcaa.png)'
- en: '*6 ≥ 4* is true, so this fires 1, which means I can eat the cake.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*6 ≥ 4* 为真，所以它输出 1，这意味着我可以吃蛋糕。'
- en: Scenario 3
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景 3
- en: 'I want to eat a cake and I did cardio and went to the gym yesterday, but it
    is also not an occasion for cake:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我想吃蛋糕，我昨天做了有氧运动并去了健身房，但这也不是吃蛋糕的时机：
- en: '![](img/9719159c-ae4d-4d84-bd40-40e2b9419f06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9719159c-ae4d-4d84-bd40-40e2b9419f06.png)'
- en: 'Here, the following applies:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下条件成立：
- en: '*Xi* is the first input factor, *I did cardio yesterday*. Now, *Xi= 1,* as
    this factor is true.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xi* 是第一个输入因素 *我昨天做了有氧运动*。现在，*Xi = 1*，因为这个因素为真。'
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi= 2*.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wi* 是第一个输入因素 *Xi* 的权重。在我们的例子中，*Wi = 2*。'
- en: '*Xii* is the second input factor, *I went to the gym yesterday*. Now, *Xii =
    1,* as this factor is true.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xii* 是第二个输入因素，*我昨天去了健身房*。现在，*Xii = 1*，因为这个因素为真。'
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii* =
    3.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wii* 是第二个输入因素 *Xii* 的权重。在我们的例子中，*Wii* = 3。'
- en: '*Xiii* is the third input factor, *It is an occasion for cake*. Now, *Xiii=
    0,* as this factor is false.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Xiii* 是第三个输入因素，*这时是吃蛋糕的时机*。现在，*Xiii = 0*，因为这个因素是错误的。'
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii* =
    6.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wiii* 是第三个输入因素 *Xiii* 的权重。在我们的例子中，*Wiii* = 6。'
- en: '*threshold* is 4.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*threshold* 是 4。'
- en: 'We know that the neuron computes the following equation:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道神经元计算以下方程：
- en: '![](img/d1697414-06e7-4cf0-8b74-ac1759983538.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1697414-06e7-4cf0-8b74-ac1759983538.png)'
- en: 'For scenario 3, the equation will translate to this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于场景 3，方程将转化为以下形式：
- en: '![](img/0e09f54b-fd7a-4f76-9c68-da19d034fefc.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e09f54b-fd7a-4f76-9c68-da19d034fefc.png)'
- en: 'This gives us the following equation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下方程：
- en: '![](img/920fd23a-247e-47fe-a27f-827f2a80209e.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/920fd23a-247e-47fe-a27f-827f2a80209e.png)'
- en: '*5 ≥ 4* is true, so this fires 1, which means I can eat the cake.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*5 ≥ 4* 为真，所以它触发 1，这意味着我可以吃蛋糕。'
- en: From the preceding three scenarios, we saw how a single artificial neuron works.
    This single unit is also called a **perceptron**. A perceptron essentially handles
    binary inputs, computes the sum, and then compares with a threshold to ultimately
    give a binary output.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的三个场景中，我们看到一个单一的人工神经元是如何工作的。这个单元也叫做**感知机**。感知机本质上处理二进制输入，计算总和，然后与阈值进行比较，最终给出二进制输出。
- en: To better appreciate how a perceptron works, we can translate our preceding
    equation into a more generalized form for the sake of explanation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解感知机（perceptron）是如何工作的，我们可以将前面的方程式转化为一个更通用的形式，以便解释。
- en: 'Let''s assume there is just one input factor, for simplicity:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设为了简化起见，只有一个输入因素：
- en: '**![](img/bec404cd-2afa-4e53-9147-ccf45328046c.png)**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/bec404cd-2afa-4e53-9147-ccf45328046c.png)**'
- en: 'Let''s also assume that *threshold = b*. Our equation was as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*阈值 = b*。我们的方程式如下：
- en: '**![](img/9b74e4cc-32e8-4628-bef0-98d523e06c5d.png)**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/9b74e4cc-32e8-4628-bef0-98d523e06c5d.png)**'
- en: 'It now becomes this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 它现在变成了这样：
- en: '![](img/41971681-10e2-4ff7-a7b5-a7906f788a70.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41971681-10e2-4ff7-a7b5-a7906f788a70.png)'
- en: It can also be written as ![](img/d3f7ba54-6bfa-4cc9-a9b2-6ab7de85e569.png),
    then output *1*else *0**.*
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 它也可以写作！[](img/d3f7ba54-6bfa-4cc9-a9b2-6ab7de85e569.png)，然后输出*1*否则*0**。
- en: 'Here, the following applies:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里适用以下规则：
- en: '*w* is the weight of the input'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*是输入的**权重**。'
- en: '*b* is the threshold and is referred to as the bias'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*是阈值，称为**偏置**。'
- en: This rule summarizes how a perceptron neuron works.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则总结了感知机神经元的工作原理。
- en: Just like the mammalian brain, an ANN is made up of many such perceptions that
    are stacked and layered together. In the next section, we will get an understanding
    of how these neurons work together within an ANN.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就像哺乳动物的大脑一样，人工神经网络由许多这样的感知机组成，它们被堆叠并分层在一起。在下一部分，我们将了解这些神经元如何在人工神经网络中协同工作。
- en: ANNs
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）
- en: Like biological neurons, artificial neurons also do not exist on their own.
    They exist in a network with other neurons. Basically, the neurons exist by feeding
    information to each other; the outputs of some neurons are inputs to some other
    neurons.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生物神经元一样，人工神经元也并不是孤立存在的。它们存在于一个由其他神经元组成的网络中。基本上，神经元通过互相传递信息来存在；一些神经元的输出是其他神经元的输入。
- en: 'In any ANN, the first layer is called the **Input Layer**. These inputs are
    real values, such as the factors with weights (*w.x*) in our previous example.
    The sum values from the input layer are propagated to each neuron in the next
    layer. The neurons of that layer do the computation and pass their output to the
    next layer, and so on:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何人工神经网络（ANN）中，第一个层被称为**输入层**。这些输入是真实值，比如我们之前示例中的带权重的因素（*w.x*）。从输入层传递的总和值会传播到下一层的每个神经元。该层的神经元进行计算并将它们的输出传递到下一层，以此类推：
- en: '![](img/d40f5cee-3ac8-47b8-8d00-5c206d33504a.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d40f5cee-3ac8-47b8-8d00-5c206d33504a.png)'
- en: The layer that receives input from all previous neurons and passes its output
    to all of the neurons of the next layer is called a **Dense **layer. As this layer
    is connected to all of the neurons of the previous and next layer, it is also
    commonly referred to as a **Fully Connected Layer**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接收来自所有前一层神经元的输入并将其输出传递给下一层所有神经元的层叫做**密集层**。由于这一层连接着前一层和下一层的所有神经元，它也常被称为**全连接层**。
- en: The input and computation flow from layer to layer and finally end at the **Output
    Layer**, which gives the end estimate of the whole ANN.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和计算从一层流向下一层，最终结束于**输出层**，它给出了整个人工神经网络的最终估计值。
- en: The layers in-between the input and the output layers are called the **Hidden
    Layers**, as the values of the neurons within these hidden layers are unknown
    and a complete black box to the practitioner.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层之间的层被称为**隐藏层**，因为这些隐藏层内神经元的值对从业者来说是未知的，完全是一个黑箱。
- en: As you increase the number of layers, you increase the abstraction of the network,
    which in turn increases the ability of the network to solve more complex problems.
    When there are over three hidden layers, then it is referred to as a deepnet.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 随着层数的增加，你会增加网络的抽象程度，进而提高网络解决更复杂问题的能力。当隐藏层超过三层时，就被称为深度网络（deepnet）。
- en: 'So, if this was a machine vision task, then the first hidden layer would be
    looking for edges, the next would look for corners, the next for curves and simple
    shapes, and so on:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果这是一个机器视觉任务，那么第一层隐藏层将寻找边缘，下一层会寻找角落，再下一层寻找曲线和简单的形状，以此类推：
- en: '![](img/a0dcb8f4-d1c7-41a7-b083-14c5f9e374e1.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0dcb8f4-d1c7-41a7-b083-14c5f9e374e1.png)'
- en: Therefore, the complexity of the problem can determine the number of layers
    that are required; more layers lead to more abstractions. These layers can be
    very deep, with 1,000 or more layers, to very shallow, with just about half a
    dozen layers. Increasing the number of hidden layers does not necessarily give
    better results as the abstractions may be redundant.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题的复杂性可以决定所需的层数；更多的层会导致更多的抽象。这些层可以非常深，达到1,000层或更多，也可以非常浅，只有大约六层。增加隐藏层的数量不一定能得到更好的结果，因为抽象可能是多余的。
- en: So far, we have seen how artificial neurons can be stacked together to form
    a neural network. But we have seen that the perceptron neuron takes only binary
    input and gives only binary output. But in practice, there is a problem in doing
    things based on the perceptron's idea. This problem is addressed by activation
    functions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何将人工神经元堆叠在一起形成神经网络。但我们已经看到，感知机神经元只能接受二进制输入，并且只给出二进制输出。但在实践中，基于感知机思想做事情会出现问题。这个问题就是通过激活函数来解决的。
- en: Activation functions
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: We now know that an ANN is created by stacking individual computing units called
    perceptrons. We have also seen how a perceptron works and have summarized it as *Output 1, IF***![](img/87250af0-6998-4cbb-b723-de4fc5b87ba3.png)**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，人工神经网络（ANN）是通过堆叠单个计算单元（称为感知机）来创建的。我们也已经看到感知机是如何工作的，并将其总结为*输出 1，如果*！[](img/87250af0-6998-4cbb-b723-de4fc5b87ba3.png)**。
- en: That is, it either outputs a *1* or a *0* depending on the values of the weight, *w*,
    and bias, *b*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，它根据权重*w*和偏置*b*的值，输出*1*或*0*。
- en: 'Let''s look at the following diagram to understand why there is a problem with
    just outputting either a *1* or a *0*. The following is a diagram of a simple
    perceptron with just a single input, *x*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的图表，以了解为什么仅仅输出*1*或*0*会出现问题。以下是一个简单感知机的图表，只有一个输入，*x*：
- en: '![](img/b59c88c9-dbea-4dc0-8ef5-99ade5565357.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b59c88c9-dbea-4dc0-8ef5-99ade5565357.png)'
- en: 'For simplicity, let''s call  ![](img/9105c1c0-e7e4-4706-8721-579dc2c88c01.png),
    where the following applies:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，假设！[](img/9105c1c0-e7e4-4706-8721-579dc2c88c01.png)，其中适用以下情况：
- en: '*w* is the weight of the input, *x,* and *b* is the bias'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w* 是输入的权重，*x*，*b* 是偏置。'
- en: '*a* is the output, which is either *1* or *0*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是输出，可以是*1*或*0*。'
- en: 'Here, as the value of *z* changes, at some point, the output, *a*, changes
    from *0* to *1*. As you can see, the change in output *a* is sudden and drastic:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，当*z*的值发生变化时，输出*a*会在某个点从*0*变为*1*。正如你所看到的，输出*a*的变化是突然和剧烈的：
- en: '![](img/0609f818-9b0f-4e5e-a00a-f2a2399e8303.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0609f818-9b0f-4e5e-a00a-f2a2399e8303.jpg)'
- en: What this means is that for some small change, ![](img/3f95d5b7-7f37-417f-8d51-d3e966dd1a9f.png) ,
    we get a dramatic change in the output, *a*. This is not particularly helpful
    if the perceptron is part of a network, because if each perceptron has such drastic
    change, it makes the network unstable and hence the network fails to learn.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，对于某些小的变化，![](img/3f95d5b7-7f37-417f-8d51-d3e966dd1a9f.png)，我们会得到输出*a*的剧烈变化。如果每个感知机都有如此剧烈的变化，那么它会导致网络不稳定，因此网络无法学习。
- en: 'Therefore, to make the network more efficient and stable, we need to slow down
    the way each perceptron learns. In other words, we need to eliminate this sudden
    change in output from *0* to *1* to a more gradual change:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了使网络更高效、更稳定，我们需要减缓每个感知机学习的方式。换句话说，我们需要消除从*0*到*1*的突变，转而进行更渐进的变化：
- en: '![](img/558d13a2-28ee-4578-a130-9118cf059004.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/558d13a2-28ee-4578-a130-9118cf059004.jpg)'
- en: This is made possible by activation functions. Activation functions are functions
    that are applied to a perceptron so that instead of outputting a *0* or a *1*,
    it outputs any value between *0* and *1*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过激活函数来实现的。激活函数是应用于感知机的函数，使其不再输出*0*或*1*，而是输出介于*0*和*1*之间的任何值。
- en: This means that each neuron can learn slower and at a greater level of detail
    by using smaller changes, ![](img/660fdf4e-3332-4f80-a224-575de3824318.png).  Activation
    functions can be looked at as transformation functions that are used to transform
    binary values in to a sequence of smaller values between a given minimum and maximum.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每个神经元可以通过使用更小的变化来更慢地学习，并且能够以更大的细节进行学习，![](img/660fdf4e-3332-4f80-a224-575de3824318.png)。激活函数可以被看作是转换函数，用于将二进制值转换为介于给定最小值和最大值之间的更小的值序列。
- en: There are a number of ways to transform the binary outcomes to a sequence of
    values, namely the sigmoid function, the tanh function, and the ReLU function.
    We will have a quick look at each of these activation functions now.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以将二进制输出转化为一系列值，分别是 Sigmoid 函数、**tanh** 函数和 ReLU 函数。我们现在将快速浏览这些激活函数。
- en: Sigmoid function
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: 'The **sigmoid function** is a function in mathematics that outputs a value
    between 0 and 1 for any input:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid 函数**是一个数学函数，对于任何输入，它都会输出一个介于 0 和 1 之间的值：'
- en: '![](img/c0adb54a-93c5-414f-988e-c067850e6eb4.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0adb54a-93c5-414f-988e-c067850e6eb4.png)'
- en: Here, ![](img/60bb9873-524f-45a9-84ba-8ac74caac355.png) and ![](img/f5076aa3-efc1-47e4-a41f-6ee338e6bfd8.png).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/60bb9873-524f-45a9-84ba-8ac74caac355.png) 和 ![](img/f5076aa3-efc1-47e4-a41f-6ee338e6bfd8.png)。
- en: 'Let''s understand sigmoid functions better with the help of some simple code.
    If you do not have Python installed, no problem: we will use an online alternative
    for now at [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online). We
    will go through a complete setup from scratch in [Chapter 2](eb50909d-69fb-4228-967b-af2f58c543c3.xhtml),
    *Creating a Real-Estate Price Prediction Mobile App*. Right now, let''s quickly
    continue with the online alternative.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些简单的代码更好地理解 Sigmoid 函数。如果你没有安装 Python，没关系：我们现在将使用一个在线替代方案，网址是 [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online)。我们将在
    [第 2 章](eb50909d-69fb-4228-967b-af2f58c543c3.xhtml) *创建一个房地产价格预测移动应用* 中从零开始进行完整的设置。目前，让我们继续使用这个在线替代方案。
- en: 'Once we have the page at [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online) loaded,
    we can go through the code step by step and understand sigmoid functions:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们加载了页面 [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online)，我们可以逐步浏览代码并理解
    Sigmoid 函数：
- en: 'First, let''s import the `math` library so that we can use the exponential
    function:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 `math` 库，这样我们就可以使用指数函数：
- en: '[PRE0]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let''s define a function called `sigmoid`, based on the earlier formula:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个名为 `sigmoid` 的函数，基于之前的公式：
- en: '[PRE1]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s take a scenario where our *z* is very small, `-10`. Therefore, the function
    outputs a number that is very small and close to 0:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们的 *z* 非常小，比如 `-10`，因此该函数将输出一个非常小且接近 0 的数字：
- en: '[PRE2]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If *z* is very large, such as `10000`, then the function will output the maximum possible value,
    1:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *z* 非常大，比如 `10000`，那么该函数将输出最大可能值 1：
- en: '[PRE3]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Therefore, the sigmoid function transforms any value, *z*, to a value between
    0 and 1. When the sigmoid activation function is used on a neuron instead of the
    traditional perceptron algorithm, we get what is called a **sigmoid neuron**:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Sigmoid 函数将任何值 *z* 转换为介于 0 和 1 之间的值。当 Sigmoid 激活函数在神经元上使用时，而不是传统的感知机算法，我们得到的就是所谓的
    **Sigmoid 神经元**：
- en: Tanh function
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tanh 函数
- en: Similar to the sigmoid neuron, we can apply an activation function called tanh(*z*),
    which transforms any value to a value between -1 and 1.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Sigmoid 神经元类似，我们可以应用一个名为 tanh(*z*) 的激活函数，它将任何值转换为介于 -1 和 1 之间的值。
- en: 'The neuron that uses this activation function is called a **t****anh neuron**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个激活函数的神经元被称为**tanh 神经元**：
- en: '![](img/2161bec9-1aae-4b92-91f7-21d0ee1ca56d.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2161bec9-1aae-4b92-91f7-21d0ee1ca56d.png)'
- en: ReLU function
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU 函数
- en: 'Then there is an activation function called the **Rectified Linear Unit**,
    **ReLU(z)**, that transforms any value, *z*, to 0 or a value above 0\. In other
    words, it outputs any value below 0 as 0 and any value above 0 as the value itself:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一个激活函数，称为 **修正线性单元**，**ReLU(z)**，它将任何值 *z* 转换为 0 或大于 0 的值。换句话说，它将小于 0 的任何值输出为
    0，而将大于 0 的任何值输出为其本身：
- en: '![](img/af0454a7-3151-4abf-a1af-da3be88ab020.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af0454a7-3151-4abf-a1af-da3be88ab020.png)'
- en: Just to summarize our understanding so far, the perceptron is the traditional
    and outdated neuron that is rarely used in real implementations. They are great
    to get a simplistic understanding of the underlying principle; however, they had
    the problem of fast learning due to the drastic changes in output values.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 简要总结我们目前的理解，感知机是传统的、过时的神经元，在实际应用中很少使用。它们非常适合帮助我们简单理解底层原理；然而，由于输出值的剧烈变化，它们存在快速学习的问题。
- en: 'We use activation functions to reduce the learning speed and determine finer
    changes in *z* or  ![](img/2469dc05-58c0-4c3f-848c-ab28a70b8385.png). Let''s sum
    up these activation functions:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用激活函数来减少学习速度，并确定 *z* 或 ![](img/2469dc05-58c0-4c3f-848c-ab28a70b8385.png)
    的细微变化。让我们总结一下这些激活函数：
- en: The **sigmoid neuron** is the neuron that uses the sigmoid activation function
    to transform the output to a value between 0 and 1.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sigmoid 神经元** 是使用 sigmoid 激活函数的神经元，它将输出转化为介于 0 和 1 之间的值。'
- en: The **t****anh neuron** is the neuron that uses the tanh activation function
    to transform the output to a value between -1 and 1.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tanh 神经元** 是使用 tanh 激活函数的神经元，它将输出转化为介于 -1 和 1 之间的值。'
- en: The **ReLU neuron** is the neuron that uses the ReLU activation function to
    transform the output to a value of either 0 or any value above 0.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU 神经元** 是使用 ReLU 激活函数的神经元，它将输出转化为 0 或任何大于 0 的值。'
- en: The sigmoid function is used in practice but is slow compared to the tanh and
    ReLU functions. The tanh and ReLU functions are commonly used activation functions.
    The ReLU function is also considered state of the art and is usually the first
    choice of activation function that's used to build ANNs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数在实际中被使用，但与 tanh 和 ReLU 函数相比较慢。tanh 和 ReLU 函数是常用的激活函数。ReLU 函数也被认为是最先进的，通常是构建人工神经网络（ANN）时首选的激活函数。
- en: 'Here is a list of commonly used activation functions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是常用的激活函数列表：
- en: '![](img/67a40589-04e2-4744-b539-5d814a516ef2.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67a40589-04e2-4744-b539-5d814a516ef2.jpg)'
- en: In the projects within this book, we will be primarily using either the sigmoid, tanh, or
    the ReLU neurons to build our ANN.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中的项目中，我们将主要使用 sigmoid、tanh 或 ReLU 神经元来构建人工神经网络（ANN）。
- en: Cost functions
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代价函数
- en: To quickly recap, we know how a basic perceptron works and its pitfalls. We
    then saw how activation functions overcame the perceptron's pitfalls, giving rise
    to other neuron types that are in use today.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾一下，我们已经了解了基本的感知机工作原理及其局限性。然后，我们看到激活函数如何克服了感知机的缺陷，从而诞生了今天使用的其他神经元类型。
- en: Now, we are going to look at how we can tell when the neurons are wrong. For
    any type of neuron to learn, it needs to know when it outputs the wrong value
    and by what margin. The most common way to measure how wrong the neural network
    is, is to use a cost function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨如何判断神经元的输出是否错误。任何类型的神经元要学习，都需要知道何时输出了错误的值以及错误的幅度。衡量神经网络错误的最常见方法是使用代价函数。
- en: 'A **cost function** quantifies the difference between the output we get from
    a neuron to an output that we need from that neuron. There are two common types
    of cost functions that are used: mean squared error and cross entropy.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**代价函数** 定量化了神经元输出与我们需要的输出之间的差异。常用的两种代价函数是均方误差和交叉熵。'
- en: Mean squared error
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: 'The **mean squared error** (**MSE**) is also called a quadratic cost function
    as it uses the squared difference to measure the magnitude of the error:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）也被称为二次代价函数，因为它使用平方差来衡量误差的大小：'
- en: '![](img/9f61fab8-dc30-44bd-9b79-a6b70a6ca878.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f61fab8-dc30-44bd-9b79-a6b70a6ca878.png)'
- en: 'Here, the following applies:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，适用以下公式：
- en: '*a* is the output from the ANN'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是来自人工神经网络（ANN）的输出'
- en: '*y* is the expected output'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是期望的输出'
- en: '*n* is the number of samples used'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是使用的样本数'
- en: 'The cost function is pretty straightforward. For example, consider a single
    neuron with just one sample, (*n=1*). If the expected output is 2 (*y=2*) and
    the neuron outputs 3 (*a=3*), then the MSE is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 代价函数相当直接。例如，考虑一个只有一个样本的单神经元（*n=1*）。如果期望的输出是 2 (*y=2*)，而神经元的输出是 3 (*a=3*)，则 MSE
    如下所示：
- en: '![](img/c18a1923-6add-41b7-a4ae-43a0943bdb46.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c18a1923-6add-41b7-a4ae-43a0943bdb46.png)'
- en: '![](img/c7b65bad-5cd3-4ce3-b65a-70b26c4d8bbb.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7b65bad-5cd3-4ce3-b65a-70b26c4d8bbb.png)'
- en: '![](img/d14ec931-b364-4688-97ed-3661cdae3063.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d14ec931-b364-4688-97ed-3661cdae3063.png)'
- en: 'Similarly, if the expected output is 3 (*y=3*) and the neuron outputs 2 (*a=2*),
    then the MSE is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果期望的输出是 3 (*y=3*)，而神经元输出是 2 (*a=2*)，那么 MSE 如下所示：
- en: '![](img/1cbaaf60-0a6c-4b58-9fb8-cbbbc36a0acc.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cbaaf60-0a6c-4b58-9fb8-cbbbc36a0acc.png)'
- en: '![](img/4d35819d-c83d-4b5a-8f39-c195161ac485.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d35819d-c83d-4b5a-8f39-c195161ac485.png)'
- en: '![](img/39b26616-25dc-4fdf-b431-4b1762ed882a.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39b26616-25dc-4fdf-b431-4b1762ed882a.png)'
- en: Therefore, the MSE quantifies the magnitude of the error made by the neuron.
    One of the issues with MSE is that when the values in the network get large, the
    learning becomes slow. In other words, when the weights (*w*) and bias (*b*) or
    *z* get large, the learning becomes very slow. Keep in mind that we are talking
    about thousands of neurons in an ANN, which is why the learning slows down and
    eventually stagnates with no further learning.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，均方误差（MSE）量化了神经元所犯错误的大小。MSE 的问题之一是，当网络中的数值变得很大时，学习会变得很慢。换句话说，当权重（*w*）和偏差（*b*）或
    *z* 很大时，学习会变得非常慢。请记住，我们讨论的是人工神经网络（ANN）中的成千上万个神经元，这就是为什么当学习变得缓慢并最终停滞时，不再有任何进一步的学习。
- en: Cross entropy
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵
- en: '**Cross entropy** is a derivative-based function as it uses the derivative
    of a specially designed equation, which is given as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉熵** 是一种基于导数的函数，因为它使用了一个特别设计的方程的导数，该方程如下所示：'
- en: '![](img/d7e1d48b-f68b-4431-974a-dba3b24e7c85.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7e1d48b-f68b-4431-974a-dba3b24e7c85.png)'
- en: Cross entropy allows the network to learn faster when the difference between
    the expected and actual output is greater. In other words, the bigger the error,
    the faster it helps the network learn. We will get our heads around this using
    some simple code.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵使得当预期输出和实际输出之间的差距较大时，网络可以更快地学习。换句话说，错误越大，它越能帮助网络加速学习。我们将通过一些简单的代码来理解这一点。
- en: 'Like before, for now, you can use an online alternative if you do not have
    Python already installed, at [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online).
    We will cover the installation and setup in [Chapter 2](eb50909d-69fb-4228-967b-af2f58c543c3.xhtml),
    *Creating a Real-Estate Price Prediction* *Mobile App*. Follow these steps to
    see how a network learns using cross entropy:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，如果你没有安装 Python，可以使用在线替代工具，网址为 [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online)。我们将在[第二章](eb50909d-69fb-4228-967b-af2f58c543c3.xhtml)《创建一个房地产价格预测
    *移动应用*》中讲解安装和设置。请按照以下步骤，了解网络如何通过交叉熵学习：
- en: 'First, let''s import the `math` library so that we can use the `log` function:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 `math` 库，以便使用 `log` 函数：
- en: '[PRE4]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, let''s define a function called `cross_enrtopy`, based on the preceding
    formula:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个名为 `cross_entropy` 的函数，基于前面的公式：
- en: '[PRE5]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For example, consider a single neuron with just one sample, (*n=1*). Say the
    expected output is `0` (*y=0*) and the neuron outputs `0.01` (*a=0.01*):'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，考虑一个只有一个样本的单个神经元（*n=1*）。假设预期输出是 `0` (*y=0*)，而神经元的输出是 `0.01` (*a=0.01*)：
- en: '[PRE6]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE7]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Since the expected and actual output values are very small, the resultant cost
    is very small.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预期输出和实际输出值非常小，结果的成本也非常小。
- en: 'Similarly, if the expected and actual output values are very large, then the
    resultant cost is still small:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果预期输出和实际输出值非常大，那么结果的成本仍然很小：
- en: '[PRE8]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE9]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, if the expected and actual output values are far apart, then the
    resultant cost is large:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果预期输出与实际输出之间的差距很大，那么结果的成本也会很大：
- en: '[PRE10]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE11]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Therefore, the larger the difference in expected versus actual output, the faster
    the learning becomes. Using cross entropy, we can get the error of the network,
    and at the same time, the magnitude of the weights and bias is irrelevant, helping
    the network learn faster.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，预期输出与实际输出之间的差异越大，学习速度就越快。通过使用交叉熵，我们可以获得网络的误差，同时，权重和偏差的大小并不重要，有助于网络更快地学习。
- en: Gradient descent
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Up until now, we have covered the different kind of neurons based on the activation
    functions that are used. We have covered the ways to quantify inaccuracy in the
    output of a neuron using cost functions. Now, we need a mechanism to take that
    inaccuracy and remedy it.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了基于激活函数使用的不同类型的神经元。我们已经讨论了如何使用成本函数来量化神经元输出的误差。现在，我们需要一个机制来解决这个误差。
- en: The mechanism through which the network can learn to output values closer to
    the expected or desired output is called **gradient descent**. Gradient descent is
    a common approach in machine learning for finding the lowest cost possible.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 网络能够学习输出接近预期或期望输出值的机制被称为 **梯度下降**。梯度下降是机器学习中常用的一种方法，用于寻找最低成本。
- en: 'To understand gradient descent, let''s use the single neuron equation we have
    been using so far:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解梯度下降，让我们使用我们迄今为止一直在使用的单个神经元方程：
- en: '![](img/c8dd1e82-570f-42cd-920e-4395ad993c09.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8dd1e82-570f-42cd-920e-4395ad993c09.png)'
- en: 'Here, the following applies:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下公式适用：
- en: '*x* is the input'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*是输入'
- en: '*w* is the weight of the input'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*是输入的权重'
- en: '*b* is the bias of the input'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*是输入的偏置'
- en: 'Gradient descent can be represented as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降可以表示如下：
- en: '![](img/a9278133-1d21-4301-b560-08a75e14b516.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9278133-1d21-4301-b560-08a75e14b516.png)'
- en: Initially, the neuron starts by assigning random values for *w* and *b*. From
    that point onward, the neuron needs to adjust the values of *w* and *b* sothat
    it lowers or decreases the error or cost (cross entropy).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，神经元通过为*w*和*b*分配随机值来开始。从那时起，神经元需要调整*w*和*b*的值，以便降低或减少误差或成本（交叉熵）。
- en: Taking the derivative of the cross entropy (cost function) results in a step-by-step
    change in *w* and *b* in the direction of the lowest cost possible. In other words,
    **gradient descent** tries to find the finest line between the network output
    and expected output.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对交叉熵（成本函数）求导，会导致*w*和*b*朝着最低成本的方向逐步变化。换句话说，**梯度下降**尝试找到网络输出与预期输出之间的最佳线。
- en: The weights are adjusted based on a parameter called the **l****earning rate.** The
    learning rate is the value that is adjusted to the weight of the neuron to get
    an output closer to the expected output.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 权重会根据一个叫做**学习率**的参数进行调整。学习率是调整神经元权重的值，以使输出更接近预期输出。
- en: Keep in mind that here, we have used only a single parameter; this is only to
    make things easier to comprehend. In reality, there are thousands upon millions
    of parameters that are taken into consideration to lower the cost.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这里我们只用了一个单一的参数；这是为了让事情更容易理解。实际上，考虑到降低成本，会有成千上万的参数需要考虑。
- en: Backpropagation – a method for neural networks to learn
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播——一种让神经网络学习的方法
- en: Great! We have come a long way, from looking at the biological neuron, to the
    types of neuron, to determining accuracy, and correcting the learning of the neuron.
    Only one question remains: *how can the whole network of neurons learn together?*
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经走了很长一段路，从观察生物学神经元，到研究神经元类型，再到确定准确性并纠正神经元的学习。只剩下一个问题：*整个神经网络如何一起学习？*
- en: '**Backpropagation** is an incredibly smart approach to making gradient descent
    happen throughout the network across all layers. Backpropagation leverages the
    chain rule from calculus to make it possible to transfer information back and
    forth through the network:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**是一种非常聪明的方式，使梯度下降能够遍及整个网络的所有层。反向传播利用微积分中的链式法则，使得信息可以在网络中前后传递：'
- en: '![](img/2347ee42-e9b4-4236-8b0c-8dafa257cf50.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2347ee42-e9b4-4236-8b0c-8dafa257cf50.jpg)'
- en: In principle, the information from the input parameters and weights is propagated
    through the network to make a guess at the expected output and then the overall
    inaccuracy is backpropagated through the layers of the network so that the weights
    can be adjusted and the output can be guessed again.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，输入参数和权重的信息会通过网络传播，猜测预期输出，然后整体的不准确性通过网络的各层进行反向传播，以便调整权重并重新猜测输出。
- en: This single cycle of learning is called a **t****raining step** or **i****teration**.
    Each iteration is performed on a batch of the input training samples. The number
    of samples in a batch is called **b****atch size**. When all of the input samples
    have been through an iteration or training step, then it is called an **epoch**.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习的单一循环叫做**训练步骤**或**迭代**。每次迭代都是在输入训练样本的一批数据上进行的。每批样本中的样本数叫做**批量大小**。当所有的输入样本都经历了一次迭代或训练步骤，那么这就叫做一个**周期**。
- en: For example, let's say there are 100 training samples and in every iteration
    or training step, there are 10 samples being used by the network to learn. Then,
    we can say that the batch size is 10 and it will take 10 iterations to complete
    a single epoch. Provided each batch has unique samples, that is, if every sample
    is used by the network at least once, then it is a single epoch.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设有100个训练样本，每次迭代或训练步骤中，网络使用10个样本来学习。那么，我们可以说批量大小是10，并且它需要10次迭代才能完成一个周期。如果每个批次的样本是独特的，也就是说每个样本至少被网络使用一次，那么这就是一个周期。
- en: This back-and-forth propagation of the predicted output and the cost through
    the network is how the network learns.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预测输出和成本在网络中前后传播的方式就是网络如何学习的过程。
- en: We will revisit training step, epoch, learning rate, cross entropy, batch size,
    and more during our hands-on sections.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实践环节中，我们将重新讨论训练步骤、周期、学习率、交叉熵、批量大小等内容。
- en: Softmax
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax
- en: We have reached our final conceptual topic for this chapter. We've covered types
    of neurons, cost functions, gradient descent, and finally a mechanism to apply
    gradient descent across the network, making it possible to learn over repeated
    iterations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达本章的最后一个概念性主题。我们已经讨论了神经元类型、成本函数、梯度下降，最后是应用梯度下降跨越网络的机制，使得在多次迭代中学习成为可能。
- en: 'Previously, we saw the input layer and dense or hidden layers of an ANN:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到了ANN的输入层和密集层或隐藏层：
- en: '![](img/a8dbb075-44b2-41c1-9380-a3ea3f8dd76d.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8dbb075-44b2-41c1-9380-a3ea3f8dd76d.jpg)'
- en: '**Softmax** is a special kind of neuron that''s used in the output layer to
    describe the probability of the respective output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**Softmax**是一种特殊的神经元，用于输出层中描述各输出的概率：'
- en: '![](img/4f904f31-079f-4eee-b8da-a970c52f121c.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f904f31-079f-4eee-b8da-a970c52f121c.png)'
- en: To understand the softmax equation and its concepts, we will be using some code.
    Like before, for now, you can use any online Python editor to follow the code.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解softmax方程及其概念，我们将使用一些代码。像以前一样，现在你可以使用任何在线Python编辑器来跟随代码。
- en: 'First, import the exponential methods from the `math` library:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从`math`库中导入指数方法：
- en: '[PRE12]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For the sake of this example, let''s say that this network is designed to classify
    three possible labels: `A`, `B`, and `C`. Let''s say that there are three signals
    going into the softmax from the previous layers (-1, 1, 5):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个例子，假设这个网络被设计为分类三个可能的标签：`A`、`B`和`C`。假设从前面的层有三个信号输入到softmax（-1，1，5）：
- en: '[PRE13]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The explanation is as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 解释如下：
- en: The first signal indicates that the output should be `A`, but is weak and is
    represented with a value of -1
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个信号表明输出应该是`A`，但信号较弱，其值为-1。
- en: The second signal indicates that the output should be `B` and is slightly stronger
    and represented with a value of 1
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个信号表明输出应该是`B`，并且略强，值为1。
- en: The third signal is the strongest, indicating that the output should be `C`
    and is represented with a value of 5
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个信号最强，表明输出应该是`C`，其值为5。
- en: These represented values are confidence measures of what the expected output
    should be.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示的值是对预期输出的置信度度量。
- en: 'Now, let''s take the numerator of the softmax for the first signal, guessing
    that the output is `A`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来查看softmax中第一个信号的分子，猜测输出是`A`：
- en: '![](img/a354238f-684e-4657-b6b2-326b60172ab6.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a354238f-684e-4657-b6b2-326b60172ab6.png)'
- en: 'Here, *M* is the output signal strength indicating that the output should be
    `A`:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*M*是表示输出应该是`A`的输出信号强度：
- en: '[PRE14]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, there''s the numerator of the softmax for the second signal, guessing
    that the output is `B`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这是softmax中第二个信号的分子，猜测输出是`B`：
- en: '![](img/a354238f-684e-4657-b6b2-326b60172ab6.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a354238f-684e-4657-b6b2-326b60172ab6.png)'
- en: 'Here, `M` is the output signal strength indicating that the output should be
    `B`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`M`是表示输出应该是`B`的输出信号强度：
- en: '[PRE15]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, there''s the numerator of the softmax for the second signal, guessing
    that the output is `C`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是softmax中第二个信号的分子，猜测输出是`C`：
- en: '![](img/dc4b8b76-9465-424c-95f0-a7814745c5c7.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc4b8b76-9465-424c-95f0-a7814745c5c7.png)'
- en: 'Here, `M` is the output signal strength indicating that the output should be `C`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`M`是表示输出应该是`C`的输出信号强度：
- en: '[PRE16]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can observe that the represented confidence values are always placed above
    0 and that the resultant is made exponentially larger.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，表示的置信度值总是大于0，并且结果会被指数级放大。
- en: 'Now, let''s interpret the denominator of the softmax function, which is a sum
    of the exponential of each signal value:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解释softmax函数的分母，它是每个信号值的指数和：
- en: '![](img/75d8dee4-c88e-40b4-9a49-dc4e732088e8.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75d8dee4-c88e-40b4-9a49-dc4e732088e8.png)'
- en: 'Let''s write some code for softmax function:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为softmax函数写一些代码：
- en: '[PRE17]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Therefore, the probability that the first signal is correct is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一个信号正确的概率如下：
- en: '[PRE18]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is less than a 1% chance that it is `A`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着`A`的概率不到1%。
- en: 'Similarly, the probability that the third signal is correct is as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，第三个信号正确的概率如下：
- en: '[PRE19]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This means there is over a 97% chance that the expected output is indeed `C`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着预期输出是`C`的概率超过97%。
- en: Essentially, the softmax accepts a weighted signal that indicates the confidence
    of some class prediction and outputs a probability score between 0 to 1 for all
    of those classes.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，softmax接受一个加权信号，表示某个类别预测的置信度，并输出一个介于0到1之间的概率分数，针对所有这些类别。
- en: Great! We have made it through the essential high-level theory that's required
    to get us hands on with our projects. Next up, we will summarize our understanding
    of these concepts by exploring the TensorFlow Playground.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们已经掌握了实现项目所需的基本高层理论。接下来，我们将通过探索 TensorFlow Playground 来总结我们对这些概念的理解。
- en: TensorFlow Playground
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Playground
- en: Before we get started with the TensorFlow Playground, let's recap the essential
    concepts quickly. It will help us appreciate the TensorFlow Playground better.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用 TensorFlow Playground 之前，让我们快速回顾一下基本概念。这将帮助我们更好地理解 TensorFlow Playground。
- en: The inspiration for neural networks is the biological brain, and the smallest
    unit in the brain is a **neuron**.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的灵感来源于生物大脑，而大脑中的最小单元是 **神经元**。
- en: A **P****erceptron** is a neuron based on the idea of the biological neuron.
    The perceptron basically deals with binary inputs and outputs, making it impractical
    for actual pragmatic purposes. Also, because of its binary nature, it learns too
    fast due to the drastic change in output for a small change in input, and so does
    not provide fine details.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知器（Perceptron）** 是一种基于生物神经元概念的神经元。感知器基本上处理二进制输入和输出，这使得它在实际应用中不太可行。此外，由于其二进制性质，它对输入的微小变化会产生剧烈的输出变化，因此学习速度过快，无法提供细节。'
- en: '**Activation ****functions** were used to negate the issue with perceptrons.
    This gave rise to other types of neurons that deal with values between ranges
    of 0 to 1, -1 to 1, and so on, instead of just a 0 or a 1.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数** 用于解决感知器的问题。这催生了其他类型的神经元，这些神经元处理介于 0 到 1、-1 到 1 等区间之间的值，而不仅仅是 0 或 1。'
- en: '**ANNs** are made up of these neurons stacked in layers. There is an input
    layer, a dense or fully connected layer, and an output layer.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络（ANNs）** 由这些堆叠在各层中的神经元组成。它包括输入层、密集层或全连接层，以及输出层。'
- en: '**Cost functions**, such as MSE and cross entropy, are ways to measure the
    magnitude of error in the output of a neuron.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**代价函数**，如均方误差（MSE）和交叉熵（cross entropy），是衡量神经元输出误差大小的方式。'
- en: '**Gradient descent** is a mechanism through which a neuron can learn to output
    values closer to the expected or desired output.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降** 是一种机制，通过它，神经元可以学习输出更接近期望或目标输出的值。'
- en: '**Backpropagation **is an incredibly smart approach to making gradient descent
    happen throughout the network across all layers.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播** 是一种非常聪明的方法，它使梯度下降在网络中的所有层中得以实现。'
- en: Each back and forth propagation or iteration of the predicted output and the
    cost through the network is called a **training step**.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 每次反向传播或预测输出与成本在网络中的迭代过程称为 **训练步骤**。
- en: The **learning rate** is the value that is adjusted to the weight of the neuron
    at each training step to get an output that's closer to the expected output.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率** 是在每次训练步骤中调整神经元权重的值，以使输出更接近期望的输出。'
- en: '**Softmax** is a special kind of neuron that accepts a weighted signal indicating
    the confidence of some class prediction and outputting a probability score between
    0 to 1 for all of those classes.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**Softmax** 是一种特殊的神经元，它接受一个加权信号，表示某个类别预测的置信度，并为所有这些类别输出一个介于 0 到 1 之间的概率值。'
- en: Now, we can proceed to TensorFlow Playground at [https://Playground.tensorflow.org](https://playground.tensorflow.org).
    TensorFlow Playground is an online tool to visualize an ANN or deepnet in action,
    and is an excellent place to reiterate what we have learned conceptually in a
    visual and intuitive way.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以访问 TensorFlow Playground：[https://Playground.tensorflow.org](https://playground.tensorflow.org)。TensorFlow
    Playground 是一个在线工具，用于可视化人工神经网络（ANN）或深度网络的运行，是一个很好的地方，可以直观地重复我们在概念上学到的内容。
- en: 'Now, without further ado, let''s get on with TensorFlow Playground. Once the
    page is loaded, you will see a dashboard to create your own neural network for
    predefined classification problems. Here is a screenshot of the default page and
    its sections:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，废话不多说，让我们开始使用 TensorFlow Playground。页面加载完成后，您将看到一个仪表板，可以创建自己的神经网络来解决预定义的分类问题。以下是默认页面及其各个部分的截图：
- en: '![](img/b9d0dbd1-00ed-4a54-9bfb-2740326d4a34.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9d0dbd1-00ed-4a54-9bfb-2740326d4a34.png)'
- en: 'Let''s look at each of the sections from this screenshot:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个截图中的各个部分：
- en: Section 1: The data section shows choices of the pre-built problems to build
    and visualize the network. The first problem is chosen, which is basically to
    distinguish between the blue an orange dots. Below that, there are controls to
    divide the data into training and testing subsets. There is also a parameter to
    set the batch size. The Batch size is the number of samples that are taken into
    the network for learning during each training step.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1部分：数据部分显示了选择预构建问题的选项，用于构建和可视化网络。第一个问题是选择的，基本上是区分蓝点和橙点。在下面，还有将数据分为训练集和测试集的控制项。还有一个参数用于设置批次大小。批次大小是每次训练步骤中输入到网络进行学习的样本数量。
- en: Section 2: The features section indicates the number of input parameters. In
    this case, there are two features chosen as the input features.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2部分：特征部分表示输入参数的数量。在这种情况下，选择了两个特征作为输入特征。
- en: Section 3: The hidden layer section is where we can create hidden layers to
    increase complexity. There are also controls to increase and decrease the number
    of neurons within each hidden or dense layer. In this example, there are two hidden
    layers with four and two neurons, respectively.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3部分：隐藏层部分是我们可以创建隐藏层以增加复杂度的地方。这里还有控制项可以增加或减少每个隐藏层或全连接层中的神经元数量。在这个示例中，有两个隐藏层，分别包含4个和2个神经元。
- en: Section 4: The output section is where we can see the loss or the cost graph,
    along with a visualization of how well the network has learned to separate the
    red and blue dots.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4部分：输出部分是我们可以看到损失或代价图表的地方，同时还可视化网络分离红点和蓝点的学习效果。
- en: Section 5: This section is the control panel for adjusting the tuning parameters
    of the network. It has a widget to start, pause, and refresh the training of the
    network. Next to it, there is a counter indicating the number of epochs elapsed.
    Then there is Learning rate, the constant by which the weights are adjusted. That
    is followed by the choice of activation function to use within the neurons. Finally,
    there is an option to indicate the kind of problem to visualize, that is classification,
    or regression. In this example, we are visualizing a classification task.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5部分：这是调整网络调优参数的控制面板。它有一个小部件可以启动、暂停和刷新网络的训练。在它旁边是一个计数器，显示经过的训练轮数。接下来是学习率，表示权重调整的常数。然后是选择在神经元中使用的激活函数。最后，有一个选项可以指示可视化的问题类型，即分类或回归。在这个示例中，我们正在可视化一个分类任务。
- en: We will ignore the Regularization and Regularization rate for now, as we have
    not covered these terms in a conceptual manner as of yet. We will visit these
    terms in later in the book when it is ideal for appreciating its purpose.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前我们将忽略正则化和正则化率，因为我们尚未以概念性的方式讲解这些术语。我们将在书中的后续部分介绍这些术语，当时会更容易理解它们的目的。
- en: 'We are now ready to start fiddling around with TensorFlow Playground. We will
    start with the first dataset, with the following settings on the tuning parameters:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始使用 TensorFlow Playground。我们将从第一个数据集开始，调整以下调优参数：
- en: Learning rate = 0.01
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 = 0.01
- en: Activation = Tanh
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数 = Tanh
- en: Regularization = None
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化 = 无
- en: Regularization rate = 0
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化率 = 0
- en: Problem type = Classification
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题类型 = 分类
- en: DATA = Circle
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据 = Circle
- en: Ratio of training to test data = 50%
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据与测试数据的比例 = 50%
- en: Batch size = 10
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次大小 = 10
- en: FEATURES = X[1] and X[2]
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征 = X[1] 和 X[2]
- en: Two hidden/dense layers; the first layer with 4 neurons, and the second layer
    with 2 neurons
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个隐藏/全连接层；第一个层有4个神经元，第二个层有2个神经元
- en: 'Now start training by clicking the play button on the top-left corner of the
    dashboard. Moving right from the play/pause button, we can see the number of epochs
    that have elapsed. At about 200 epochs, pause the training and observe the output
    section:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过点击仪表盘左上角的播放按钮开始训练。从播放/暂停按钮向右移动，我们可以看到已经经过的训练轮数。在大约200轮时，暂停训练并观察输出部分：
- en: '![](img/ee92aba1-9adb-4172-a92b-0fa2bbd9c4b0.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee92aba1-9adb-4172-a92b-0fa2bbd9c4b0.jpg)'
- en: 'The key observations from the dashboard are as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表盘的关键观察点如下：
- en: We can see the performance graph of the network on the right section of the
    dashboard. The test and training loss is the cost of the network during testing
    and training, respectively. As discussed previously, the idea is to minimize cost.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在仪表盘的右侧看到网络的性能图。测试和训练损失分别是网络在测试和训练过程中的代价。如前所述，目标是最小化代价。
- en: Below that, you will observe that there is a visualization of how the network
    has separated or classified the blue dots from the orange ones.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下方，您将看到一个可视化图，展示了网络如何将蓝色点从橙色点中分离或分类。
- en: If we hover the mouse pointer over any of the neurons, we can see what the neuron
    has learned to separate the blue and orange dots. Having said this, let's take
    a closer look at both of the neurons from the second layer to see what they have
    learned about the task.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将鼠标指针悬停在任何神经元上，我们可以看到该神经元已学会如何将蓝色点和橙色点分开。话虽如此，我们来仔细看看第二层的两个神经元，看看它们在这个任务中学到了什么。
- en: When we hover over the first neuron in the second layer, we can see that this
    neuron has done a good job of learning the task at hand. In comparison, the second
    neuron in the second layer has learned less about the task.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将鼠标悬停在第二层的第一个神经元上时，我们可以看到这个神经元已经很好地学会了当前的任务。相比之下，第二层的第二个神经元学到的任务知识较少。
- en: 'That brings us to the dotted lines coming out of the neurons: they are the
    corresponding weights of the neuron. The blue dotted lines indicate positive weights
    while the orange dotted ones indicate negative weights. They are commonly called **tensors.**'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这引出了从神经元中出来的虚线：它们是神经元的对应权重。蓝色虚线表示正权重，而橙色虚线表示负权重。它们通常被称为**张量**。
- en: Another key observation is that the first neuron in the second layer has a stronger
    tensor signal coming out of it compared to the second one. This is indicative
    of the influence this neuron has in the overall task of separating the blue and
    orange dots, and it is quite apparent when we see what it has learned compared
    to the overall end results visual.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键观察是，第二层中的第一个神经元输出的张量信号比第二个神经元强。这表明该神经元在将蓝色和橙色点分离的整体任务中的影响力，而且当我们看到它与最终结果的可视化对比时，这一点非常明显。
- en: Now, keeping in mind all the terms we have learned in this chapter, we can play
    around by changing the parameters and seeing how this affects the overall network.
    It is even possible to add new layers and neurons.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，牢记我们在本章中学到的所有术语，我们可以通过改变参数来进行试验，看看这如何影响整个网络。甚至可以添加新的层和神经元。
- en: TensorFlow Playground is an excellent place to reiterate the fundamentals and
    essential concepts of ANNs.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Playground是一个很好的地方，可以重温ANNs的基本原理和核心概念。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: So far, we have covered the essential concepts at a high level, enough for us
    to appreciate the things we are going to be doing practically in this book. Having
    a conceptual understanding is good enough to get us rolling with building AI models,
    but it is also handy to have a deeper understanding.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在高层次上涵盖了基本概念，这足以让我们欣赏本书中将要做的实际操作。拥有概念性的理解已经足够让我们开始构建AI模型，但更深入的理解也是非常有用的。
- en: In the next chapter, we will set up our environment for building AI applications
    and create a small Android and iOS mobile app that can use a model built on Keras
    and TensorFlow to predict house prices.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将设置构建AI应用程序的环境，并创建一个小型的Android和iOS移动应用，能够使用基于Keras和TensorFlow构建的模型来预测房价。
- en: Further reading
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here is a list of resources that can be referenced to appreciate and dive deeper
    into the concepts of AI and deep learning:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一份资源列表，可以作为参考，以帮助我们更好地理解并深入探索AI和深度学习的概念：
- en: '*Neural Networks and deep learning*, [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Neural Networks and deep learning*，[http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
- en: 'Michael Taylor''s *Make Your Own Neural Network: An In-depth Visual Introduction
    For Beginners*, [https://www.amazon.in/Machine-Learning-Neural-Networks-depth-ebook/dp/B075882XCP](https://www.amazon.in/Machine-Learning-Neural-Networks-depth-ebook/dp/B075882XCP)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Michael Taylor的*Make Your Own Neural Network: An In-depth Visual Introduction
    For Beginners*，[https://www.amazon.in/Machine-Learning-Neural-Networks-depth-ebook/dp/B075882XCP](https://www.amazon.in/Machine-Learning-Neural-Networks-depth-ebook/dp/B075882XCP)'
- en: Tariq Rashid's *Make Your Own Neural Network*, [https://www.amazon.in/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G](https://www.amazon.in/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tariq Rashid的*Make Your Own Neural Network*，[https://www.amazon.in/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G](https://www.amazon.in/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G)
- en: Nick Bostrom's *Superintelligence*, [https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nick Bostrom的*Superintelligence*，[https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
- en: Pedro Domingos's *The Master Algorithm*, [https://en.wikipedia.org/wiki/The_Master_Algorithm](https://en.wikipedia.org/wiki/The_Master_Algorithm)
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedro Domingos的*《大师算法》*，[https://en.wikipedia.org/wiki/The_Master_Algorithm](https://en.wikipedia.org/wiki/The_Master_Algorithm)
- en: '*Deep Learning Book*, [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《深度学习》*，[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
