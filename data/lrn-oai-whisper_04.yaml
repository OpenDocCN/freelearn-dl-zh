- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Fine-Tuning Whisper for Domain and Language Specificity
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对领域和语言特定性微调 Whisper
- en: OpenAI’s Whisper represents a groundbreaking innovation in ASR through its ability
    to transcribe speech into text with unprecedented accuracy. However, as with any
    machine learning model, Whisper’s out-of-the-box performance still exhibits limitations
    in niche contexts. For example, during the onset of COVID-19, Whisper could not
    recognize the term for several months. Similarly, the model needed to accurately
    transcribe the names of key figures and places associated with the Russia–Ukraine
    conflict, which required prior training data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 Whisper 是语音识别（ASR）领域的一项突破性创新，能够以前所未有的准确性将语音转录为文本。然而，像任何机器学习模型一样，Whisper
    在特定小众语境下的初始表现仍然存在局限。例如，在 COVID-19 大流行初期，Whisper 数月未能识别这一术语。类似地，该模型还未能准确转录与俄乌冲突相关的关键人物和地点的名称，这需要事先的训练数据。
- en: Thus, to fully tap into this model’s potential, we must customize it for specific
    situations. This chapter will uncover techniques for adapting Whisper’s skills
    to handle unique business problems. Our adventure will stretch several milestones,
    from setting up systems to evaluating improvements.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了充分挖掘该模型的潜力，我们必须针对特定情况进行定制化。本章将揭示将 Whisper 的能力适应特定业务问题的技巧。我们的冒险将跨越多个里程碑，从系统设置到评估改进。
- en: First, we’ll establish and configure Python resources to power our coming work,
    incorporating datasets/modeling/experimentation libraries that form a solid base
    on which to build. Next, we’ll smartly pick multilingual speech data sources such
    as **Common Voice** to diversify Whisper’s knowledge further for specific niches.
    More focused data improves the quality of training.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将建立并配置 Python 资源，为即将进行的工作提供支持，整合数据集/建模/实验库，形成一个坚实的基础。接下来，我们将聪明地选择多语言语音数据源，例如**Common
    Voice**，进一步丰富 Whisper 在特定领域的知识。更加专注的数据可以提升训练质量。
- en: With the stage now set through tools and augmented data, we can tailor Whisper’s
    predictions to make them ideal for target applications. For example, we’ll explore
    how adjusting confidence levels, output classes, and time limits can match the
    expected results in our specific use cases. We’ll also unlock tools for radically
    fine-tuning Whisper using standard equipment.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过工具和增强数据的准备工作，我们可以调整 Whisper 的预测，使其更适合目标应用。例如，我们将探索如何调整置信度、输出类别和时间限制，以便在特定用例中实现预期的结果。我们还将解锁一些工具，使用标准设备对
    Whisper 进行深度微调。
- en: Tracking progress relies on straightforward testing. We’ll set up fixed benchmarks
    to objectively gauge gains in our fine-tuning. Setting high evaluation integrity
    builds trust in results. We’ll ultimately cycle between improving Whisper and
    double-checking how sound adaptations transfer into the real world by building
    and testing a lightweight demo.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪进展依赖于简单明了的测试。我们将设定固定的基准，以客观评估我们微调的进展。设立高标准的评估完整性有助于建立对结果的信任。最终，我们将在改进 Whisper
    和通过构建并测试轻量级演示来双重检查声音适配如何转化为现实世界应用之间循环。
- en: We’ll commit to bringing everyone together by fine-tuning low-resource languages
    rather than inadvertently forgetting groups with fewer advantages.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将致力于通过微调低资源语言来让每个人都能参与进来，而不是无意中忽视那些资源较少的群体。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Preparing the environment and data for fine-tuning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备环境和数据进行微调
- en: Preparing the feature extractor, tokenizer, and data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备特征提取器、分词器和数据
- en: Training and evaluating metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估指标
- en: Evaluating performance across datasets
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨数据集评估性能
- en: Through advanced fine-tuning methodologies covered in this chapter and the companion
    GitHub repository, we will learn the foundational process of fine-tuning Whisper’s
    performance on industry-specific vocabulary, regional accents, and the integration
    of real-time learning for unfamiliar emerging terminology. Let’s get started on
    this hands-on adventure!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章中介绍的先进微调方法以及配套的 GitHub 仓库，我们将学习微调 Whisper 在行业特定词汇、地区口音以及实时学习新兴术语方面的基础过程。让我们开始这段实践冒险吧！
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, we will leverage Google Colaboratory. We’ll try to secure
    the best GPU we can afford, with a minimum of 12 GB of GPU memory.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用 Google Colaboratory。我们将尽量确保能获得最好的 GPU，至少需要 12 GB 的 GPU 内存。
- en: To get a GPU, within Google Colab’s main menu, click **Runtime** | **Change
    runtime type**, then change the **Hardware accelerator** from **None** to **GPU**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 GPU，在 Google Colab 的主菜单中，点击 **Runtime** | **Change runtime type**，然后将 **Hardware
    accelerator** 从 **None** 更改为 **GPU**。
- en: Keep in mind that fine-tuning Whisper will take several hours. Thus, you must
    monitor your running notebook in Colab regularly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，微调 Whisper 需要数小时。因此，您必须定期监控 Colab 中运行的笔记本。
- en: This chapter teaches you how to fine-tune the Whisper model so that it can recognize
    speech in multiple languages using tools such as Hugging Face Datasets, Transformers,
    and the Hugging Face Hub. Check out the Google Colab Python notebook in this book’s
    GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04))
    and try fine-tuning yourself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将教您如何微调 Whisper 模型，使其能够使用如 Hugging Face 数据集、Transformers 和 Hugging Face Hub
    等工具识别多种语言的语音。请查看本书 GitHub 仓库中的 Google Colab Python 笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04)），并尝试自己进行微调。
- en: 'The general recommendation is to follow the Colab notebook and upload model
    checkpoints directly to the Hugging Face Hub while training. The Hub provides
    the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一般推荐按照 Colab 笔记本的步骤进行操作，并在训练过程中将模型检查点直接上传至 Hugging Face Hub。Hub 提供以下功能：
- en: '**Integrated version control**: You can be sure that no model checkpoint is
    lost during training'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成版本控制**：您可以确保在训练过程中不会丢失任何模型检查点。'
- en: '**TensorBoard logs**: Track important metrics throughout training'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorBoard 日志**：跟踪训练过程中的重要指标。'
- en: '**Model cards**: Document what a model does and its intended use cases'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型卡片**：记录模型的功能和预期的使用案例。'
- en: '**Community**: An easy way to share and collaborate with the community!'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社区**：与社区共享和协作的简便方式！'
- en: Linking the notebook to the Hub is straightforward – you must enter your Hub
    authentication token when prompted. The Colab notebook has specific instructions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将笔记本链接到 Hub 非常简单——在提示时输入您的 Hub 认证令牌即可。Colab 笔记本提供了具体的操作说明。
- en: Introducing the fine-tuning process for Whisper
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 Whisper 微调过程。
- en: 'Realizing Whisper’s full potential requires moving beyond out-of-the-box offerings
    through purposeful fine-tuning – configuring and enhancing the model to capture
    precise niche needs. This specialized optimization journey traverses nine key
    milestones:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 Whisper 的最大潜力需要超越开箱即用的功能，通过有目的的微调，配置和增强模型以捕捉精确的细分需求。这一专门优化过程跨越了九个关键里程碑：
- en: Preparing robust Python environments with essential libraries such as Transformers
    and datasets that empower rigorous experimentation.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备强大的 Python 环境，包含必要的库，如 Transformers 和数据集，以支持严谨的实验。
- en: Incorporating diverse, multilingual datasets, including Common Voice, for expanding
    linguistic breadth.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 纳入多样化的多语种数据集，包括 Common Voice，以扩大语言广度。
- en: Setting up Whisper pipeline components such as tokenizers for easier pre/post-processing.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 Whisper 管道组件，如分词器，以便更轻松地进行预处理和后处理。
- en: Transforming raw speech data into model-digestible log-Mel spectrogram features.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始语音数据转换为模型可处理的对数 Mel 频谱图特征。
- en: Defining training parameters and hardware configurations aligned to target model
    size.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义与目标模型大小相匹配的训练参数和硬件配置。
- en: Establishing standardized test sets and metrics for reliable performance benchmarking.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立标准化的测试集和指标，以便进行可靠的性能基准测试。
- en: Executing training loops that meld configured hyperparameters, data, and hardware.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行训练循环，将配置的超参数、数据和硬件结合起来。
- en: Evaluating fine-tuned models against test corpus and benchmark leaderboards.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据测试语料和基准排行榜评估微调后的模型。
- en: Building applications demonstrating customized speech recognition efficacy.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建展示定制化语音识别效果的应用程序。
- en: 'The end objective remains as we traverse techniques for enhancing Whisper across
    these milestones: matching model capabilities to unique production needs through
    specialized optimization.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在穿越这些里程碑的过程中，最终目标始终如一：通过专门的优化，将模型能力与独特的生产需求相匹配。
- en: With this overview of the fine-tuning process, the next section will cover leveraging
    Whisper checkpoints. To be clear, Whisper checkpoints are pre-trained models tailored
    to various computational and linguistic requirements. For our demonstration, we
    opted for the **small** checkpoint, owing to its balance between size and performance
    – offering a practical option for us to efficiently fine-tune Whisper on specialized
    training data, even with constraints on computational capacity, ensuring that
    we can still achieve remarkable results in speech recognition for languages not
    widely spoken.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述了微调过程之后，接下来的部分将介绍如何利用Whisper检查点。需要明确的是，Whisper检查点是预训练模型，专门针对不同的计算和语言要求进行调整。在我们的演示中，我们选择了**小型**检查点，因为它在大小和性能之间提供了平衡——即使在计算能力有限的情况下，它也能为我们提供一个高效的微调选项，确保我们能在不广泛使用的语言的语音识别中取得显著成果。
- en: Leveraging the Whisper checkpoints
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用Whisper检查点
- en: 'Whisper checkpoints come in five configurations of varying model sizes (Tiny,
    Base, Small, Medium, and Large). The checkpoints with the smallest four sizes
    are trained on either English-only or multilingual data. The largest checkpoints
    are multilingual only. All 11 pre-trained checkpoints are available on the Hugging
    Face Hub ([https://huggingface.co/models?search=openai/whisper](https://huggingface.co/models?search=openai/whisper)).
    The checkpoints are summarized in the following table with links to the models
    on the Hub:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper检查点有五种不同配置，分别对应不同的模型大小（微型、基础、小型、中型和大型）。前四种最小配置的检查点是用仅包含英语或多语言数据训练的。最大配置的检查点仅用于多语言。所有11个预训练的检查点都可以在Hugging
    Face Hub上找到（[https://huggingface.co/models?search=openai/whisper](https://huggingface.co/models?search=openai/whisper)）。以下表格总结了这些检查点，并提供了Hub上模型的链接：
- en: '| **Size** | **Layers** | **Width** | **Heads** | **Parameters** | **English-Only**
    | **Multilingual** |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **大小** | **层数** | **宽度** | **头数** | **参数** | **仅英语** | **多语言** |'
- en: '| Tiny | 4 | 384 | 6 | 39M | ✓ | ✓ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 微型 | 4 | 384 | 6 | 39M | ✓ | ✓ |'
- en: '| Base | 6 | 512 | 8 | 74M | ✓ | ✓ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 6 | 512 | 8 | 74M | ✓ | ✓ |'
- en: '| Small | 12 | 768 | 12 | 244M | ✓ | ✓ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 小型 | 12 | 768 | 12 | 244M | ✓ | ✓ |'
- en: '| Medium | 24 | 1,024 | 16 | 769M | ✓ | ✓ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 中型 | 24 | 1,024 | 16 | 769M | ✓ | ✓ |'
- en: '| Large-v1 | 32 | 1,280 | 20 | 1550M | x | ✓ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 大型-v1 | 32 | 1,280 | 20 | 1550M | x | ✓ |'
- en: '| Large-v2 | 32 | 1,280 | 20 | 1550M | x | ✓ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 大型-v2 | 32 | 1,280 | 20 | 1550M | x | ✓ |'
- en: '| Large-v3 | 32 | 1,280 | 20 | 1550M | x | ✓ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 大型-v3 | 32 | 1,280 | 20 | 1550M | x | ✓ |'
- en: Table 4.1 – Whisper checkpoints
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 – Whisper检查点
- en: We’ll fine-tune the multilingual version of the small checkpoint with 244M params
    (~= 1 GB) for demonstration purposes. We’ll use a language that’s not widely spoken,
    taken from the Common Voice dataset, to train and test our system. We’ll demonstrate
    that we can get good results in this language even with ~=8 hours of specialized
    training data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将微调具有244M参数（约1 GB）的多语言版本小型检查点，用于演示目的。我们将使用一种不广泛使用的语言，这种语言来自Common Voice数据集，用于训练和测试我们的系统。我们将演示即使只有约8小时的专业训练数据，我们仍能在这种语言中取得良好结果。
- en: Now that we’ve covered the strategic use of Whisper’s checkpoints, we’ll prepare
    the environment and data for fine-tuning. This crucial next step invites us to
    meticulously set up our working environment and curate our data, ensuring our
    foundation is robust for the fine-tuning process ahead. This transition is guided
    by the principle of moving from understanding to action, setting the stage for
    practical application and innovation with Whisper.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了如何战略性地使用Whisper的检查点，接下来我们将准备微调的环境和数据。这个至关重要的步骤要求我们仔细设置工作环境并整理数据，确保为即将到来的微调过程打下坚实基础。这个过渡过程遵循从理解到行动的原则，为在Whisper中进行实际应用和创新做好准备。
- en: Milestone 1 – Preparing the environment and data for fine-tuning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑1 – 准备微调的环境和数据
- en: Training a cutting-edge speech recognition model such as Whisper poses intense
    computational demands - specialized hardware configurations are vital for viable
    fine-tuning. This section demands reasonable programming familiarity – we’ll get
    our hands dirty with low-level APIs. But fret not if tweaking parameters is not
    your forte! We will structure explanations and unpack concepts without plunging
    straight into the depths. You need not actively code along – instead, the insights
    revealed here seek to empower you to apply these processes for your unique Whisper
    fine-tuning needs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个前沿的语音识别模型，比如 Whisper，要求极高的计算能力——专门的硬件配置对可行的微调至关重要。本节内容需要具备一定的编程基础——我们将深入使用底层
    API。即使调整参数并非你的强项，也不必担心！我们将以结构化的方式进行解释，逐步解开概念的谜团，而不是直接进入技术细节。你不必立即动手编写代码——而是通过这里揭示的见解，帮助你将这些过程应用到自己独特的
    Whisper 微调需求中。
- en: If you do crave getting hands-on, this book’s GitHub repository at [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04)
    contains a complementary notebook with annotated code blocks aligned to chapter
    content. Open the notebook and traverse alongside chapters to experiment with
    parameter tweaking concepts directly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的渴望动手操作，本书的 GitHub 仓库 [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04)
    中包含了一个补充的笔记本，其中的代码块与章节内容相对应。打开笔记本，随着章节的进行，直接进行参数调整概念的实验。
- en: Leveraging GPU acceleration
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 GPU 加速
- en: While Whisper can be trained on CPUs, convergence is prohibitive at around 100
    hours, even for tiny checkpoints. Thus, **GPU acceleration** is critical for feasible
    iteration cycles.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Whisper 可以在 CPU 上训练，但即便是小的检查点，收敛时间也大约需要 100 小时，因此 **GPU 加速** 对于可行的迭代周期至关重要。
- en: GPUs provide massively parallel computation, delivering 100x faster training
    through thousands of processing cores on specialized tensors. Models with over
    a billion parameters, such as Whisper, particularly benefit from additional throughput.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 提供大规模并行计算，通过成千上万的处理核心在专用张量上提供 100 倍速度的训练。像 Whisper 这样拥有十亿级参数的模型，特别受益于额外的吞吐量。
- en: 'As we proceed with fine-tuning Whisper, I will use excerpts from the Python
    notebook available in this book’s GitHub repository. The code listed here is for
    illustration and explanation purposes. If you want to see the entire code sequence,
    please refer to the Python notebook for this chapter. The following code excerpt
    shows how we can track and confirm GPU availability:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续进行 Whisper 微调的过程中，我将使用本书 GitHub 仓库中的 Python 笔记本摘录。这些代码是为了说明和解释而提供的。如果你想查看完整的代码序列，请参考本章的
    Python 笔记本。以下代码摘录展示了如何跟踪并确认 GPU 的可用性：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Most cloud computing instance types feature attached GPUs – selecting appropriately
    sized resources is pivotal.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云计算实例类型都配备了 GPU——选择适当大小的资源至关重要。
- en: Installing the appropriate Python libraries
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装适当的 Python 库
- en: 'We will use a few well-known Python packages to adjust the Whisper model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用几个知名的 Python 包来调整 Whisper 模型：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s take a closer look:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看：
- en: '`datasets` and `transformers` provide structured access to speech data and
    state-of-the-art models'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets` 和 `transformers` 提供对语音数据和最先进模型的结构化访问'
- en: '`accelerate` and `tensorboard` enable optimized model training using available
    **GPU/TPU** hardware and tracking experiment results'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accelerate` 和 `tensorboard` 利用可用的 **GPU/TPU** 硬件优化模型训练并跟踪实验结果'
- en: '`librosa` and `soundfile` preprocess audio files, which is a crucial step before
    feeding the data into Whisper'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`librosa` 和 `soundfile` 用于预处理音频文件，这是将数据输入 Whisper 前的关键步骤'
- en: '`jiwer` and `evaluate` support quantifying speech recognition efficacy'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jiwer` 和 `evaluate` 支持量化语音识别的有效性'
- en: '`gradio` will help us create an impressive demo of our refined model'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradio` 将帮助我们创建一个引人注目的演示，展示我们精炼后的模型'
- en: 'We’ll also link this environment to the Hugging Face Hub so that we can easily
    share fine-tuned models with the community:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将这个环境链接到 Hugging Face Hub，以便我们可以轻松地与社区共享微调后的模型：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Hugging Face provides version control, model documentation, and public access,
    thus ensuring full reproducibility while allowing us to build on each other’s
    work.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 提供版本控制、模型文档和公共访问，从而确保完全可复现，同时允许我们在彼此的工作基础上进行构建。
- en: Hugging Face and Whisper
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 和 Whisper
- en: Hugging Face is a data science company that provides a platform for sharing
    and collaborating on machine learning models, particularly NLP. It is widely recognized
    for its Transformers library, which offers a collection of pre-trained models
    and tools for various NLP tasks, including text classification, translation, summarization,
    and, pertinent to our discussion, ASR.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是一家数据科学公司，提供一个分享和协作机器学习模型的平台，特别是在自然语言处理（NLP）领域。它因其Transformers库而广受认可，该库提供了一系列预训练模型和工具，适用于各种NLP任务，包括文本分类、翻译、总结，以及与我们的讨论相关的自动语音识别（ASR）。
- en: Hugging Face provides a streamlined process for fine-tuning Whisper. It allows
    you to load and prepare your training data, execute the data preparation and fine-tuning
    steps, and evaluate your model’s performance. It also offers integrated version
    control, TensorBoard logs, model cards, and a community for sharing and collaboration.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face提供了一个简化的Whisper微调流程。它允许你加载和准备训练数据，执行数据准备和微调步骤，并评估模型的表现。它还提供了集成的版本控制、TensorBoard日志、模型卡以及一个用于分享和协作的社区。
- en: While Whisper already knows a lot about many languages, there’s room to grow
    – especially when handling specific situations such as niche vocabulary. We’ll
    walk through methods for bringing in complementary speech data to fill those gaps.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Whisper已经掌握了很多语言的知识，但仍然有成长的空间——尤其是在处理特定情况时，例如行业术语。我们将介绍一些方法，将补充的语音数据引入，以填补这些空白。
- en: The Common Voice project led by Mozilla is an ideal fit here, with its 100+
    languages sourced straight from global volunteers. We’ll cover easy ways to tap
    into these crowd-sourced datasets to balance Whisper’s accuracy and inclusiveness
    for niche international uses.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Mozilla主导的Common Voice项目在这里非常适用，它提供了100多种语言的数据，直接来自全球志愿者。我们将探讨如何轻松利用这些众包数据集，平衡Whisper的准确性和在特定国际应用中的包容性。
- en: Beyond Common Voice, we can create custom mixes from multiple datasets worldwide
    to test Whisper’s boundaries. Clever blending stresses flexibility, which is vital
    for commercial success. But we can’t just pursue giant datasets – diversity brings
    resilience. We’ll equip ourselves with best practices for construction representatives
    and varied combinations tailored to deployment needs across languages.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Common Voice，我们还可以从全球多个数据集中创建自定义混合数据，以测试Whisper的边界。巧妙的混合强调灵活性，这对商业成功至关重要。但我们不能只追求庞大的数据集——多样性带来韧性。我们将为自己配备建设性代表的最佳实践和适应各种语言部署需求的多样化组合。
- en: Let’s get started by plugging some Common Voice data into Whisper.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始将一些Common Voice数据导入Whisper。
- en: Milestone 2 – Incorporating the Common Voice 11 dataset
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑2 – 融入Common Voice 11数据集
- en: The Common Voice dataset, spearheaded by Mozilla, represents a pioneering effort
    in democratizing speech technology through open and diverse speech corpora. A
    **dataset** is a structured collection of data where the rows typically represent
    individual observations or instances, and the columns represent the features or
    variables of those instances. In the case of Common Voice, each row represents
    an audio record, and each column represents features or characteristics applicable
    to the audio record. As an ever-expanding, community-driven initiative across
    100+ languages, Common Voice optimally augments multilingual speech recognition
    systems like Whisper.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由Mozilla主导的Common Voice数据集代表了一项通过开放和多样化的语音语料库使语音技术大众化的开创性努力。**数据集**是一个结构化的数据集合，其中的行通常代表单个观测或实例，而列代表这些实例的特征或变量。对于Common
    Voice来说，每一行代表一个音频记录，每一列代表适用于该音频记录的特征或属性。作为一个跨越100多种语言、不断扩展的社区驱动项目，Common Voice理想地增强了像Whisper这样的多语种语音识别系统。
- en: 'Integrating Common Voice data is straightforward with the Hugging Face `Datasets`
    library. We load the desired language split in streaming mode to bypass extensive
    storage requirements and expedite fine-tuning workflows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hugging Face的`Datasets`库集成Common Voice数据非常简单。我们以流式模式加载所需的语言分割，避免了大量存储需求，并加速了微调工作流程：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When we initially loaded the Common Voice dataset, it came with much extra information,
    such as the speaker’s accent, gender, age, and more. It also included the path
    to the disk audio file, IDs, and votes for data quality assurance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们最初加载Common Voice数据集时，它包含了许多额外的信息，比如说话者的口音、性别、年龄等。它还包括了磁盘音频文件的路径、ID和数据质量保证的投票信息。
- en: But we don’t care about those extra metadata details for speech recognition
    using Whisper. The only data Whisper needs to predict is the audio itself and
    the matching text transcript. Everything else is unnecessary for our purposes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们并不关心 Whisper 在语音识别中使用的那些额外元数据细节。Whisper 需要预测的唯一数据就是音频本身和匹配的文本转录。其他所有的内容对于我们的目的来说都是不必要的。
- en: 'So, this line of code creates a trimmed-down version of the Common Voice dataset
    by removing those extra columns or features irrelevant to our speech recognition
    task. We pare it down to just the essential *audio* and *sentence* text that Whisper
    requires. This simplifies the data pipeline:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这行代码通过去除那些与我们的语音识别任务无关的额外列或特征，创建了一个精简版的 Common Voice 数据集。我们将其精简为只有 Whisper
    所需的核心*音频*和*句子*文本。这简化了数据管道：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: By stripping away unrelated metadata, we ensure that only meaningful features
    get fed into Whisper. This helps the model focus on learning speech-to-text mappings
    rather than irrelevant patterns from speaker details. The result is a cleaner
    dataset that is more tightly aligned with our end goals.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过剥离无关的元数据，我们确保只有有意义的特征被输入到 Whisper 中。这帮助模型专注于学习语音到文本的映射，而不是来自说话人细节的无关模式。最终结果是一个更加精简的数据集，更加紧密地与我们的最终目标对齐。
- en: Common Voice encapsulates notable domain diversity, recording conditions, and
    speaker demographics. These datasets exhibit substantial audio quality and accent
    variability as crowd-sourced collections from global contributors. The presence
    of real-world recording imperfections makes Common Voice a challenging benchmark
    for assessing model robustness.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Common Voice 包含了显著的领域多样性、录音条件和说话人群体统计数据。这些数据集展示了相当大的音频质量和口音变化性，因为它们是来自全球贡献者的众包集合。现实世界录音中的不完美使得
    Common Voice 成为评估模型鲁棒性的一个具有挑战性的基准。
- en: While expansive diversity poses difficulties, it also enables more resilient
    speech recognition. Systems trained exclusively on pristine corpora such as LibriSpeech
    falter when applied to noisy environments. Heterogeneous data that integrates
    noise is thus imperative for production-ready performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管广泛的多样性带来了挑战，但也使得语音识别系统更具韧性。仅仅在像 LibriSpeech 这样的纯净语料库上训练的系统，在应用于嘈杂环境时往往会出现问题。因此，集成噪音的异质数据对于生产级性能至关重要。
- en: By covering data diversity, Common Voice complements Whisper’s foundations.
    The model’s extensive multilingual pre-training provides comprehensive linguistic
    coverage; adapting this knowledge to Common Voice’s variability and low-resource
    languages is an optimal direction for bespoke enterprise applications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过涵盖数据多样性，Common Voice 补充了 Whisper 的基础。该模型的广泛多语言预训练提供了全面的语言覆盖；将这一知识适应到 Common
    Voice 的变异性和低资源语言，是为定制企业应用量身定制的最佳方向。
- en: For instance, call centers handling customer inquiries require ASR resilient
    to accents, recording artifacts, and domain lexicon. Contact center analytics
    currently needs help with niche terminology. Contact center agents discuss specialized
    concepts, from telecom acronyms such as CDMA/GSM to named entities such as iPhone
    14 Pro Max. Enhancing Whisper’s contextual mastery necessitates domain-specific
    data. Contact centers have a particular lexicon – the model must understand that
    specific lexicon. The model will learn the specifics of that industry by having
    in-domain data. So, fine-tuning Whisper on Common Voice call center recordings
    would boost its contact center efficacy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，处理客户咨询的呼叫中心需要能够应对口音、录音伪影和领域词汇的 ASR。当前，联系中心分析在处理专业术语时存在困难。联系中心的客服人员讨论的是一些专业的概念，从
    CDMA/GSM 这样的通信缩写到 iPhone 14 Pro Max 这样的专有名词。提升 Whisper 的语境掌握能力需要领域特定的数据。联系中心有其特定的词汇——模型必须理解这些特定的词汇。通过拥有领域内的数据，模型将学习该行业的细节。因此，在
    Common Voice 呼叫中心录音上进行微调将提高其在联系中心的效能。
- en: Besides domain optimization, multilingual support remains imperative for global
    businesses. While Whisper demonstrates impressive zero-shot cross-lingual ability,
    adapting acoustic and linguistic knowledge to under-represented languages is vital
    for equitable AI.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了领域优化，多语言支持仍然是全球业务的必要条件。尽管 Whisper 展示了令人印象深刻的零样本跨语言能力，但将声学和语言知识适应到不足代表的语言对于公平的人工智能至关重要。
- en: Expanding language coverage
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展语言覆盖范围
- en: While Whisper’s multilingual design provides comprehensive linguistic coverage,
    enhancing performance in low-resource languages remains an ethical imperative
    for inclusive speech technology. Strategically fine-tuning targeted language data
    is critical for equitable global deployment.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Whisper的多语言设计提供了全面的语言覆盖，但在低资源语言中提升表现仍然是包容性语音技术的伦理命题。战略性地微调目标语言数据对于全球公平部署至关重要。
- en: The Common Voice project shares these motivations for multilingual representation.
    The initiative provides datasets for over 100 languages, including many under-resourced
    tongues. This presents a unique opportunity to augment Whisper’s knowledge in
    languages needing more training data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Common Voice项目分享了多语言表征的这些动机。该计划提供了100多种语言的数据集，包括许多资源匮乏的语言。这为增强Whisper在需要更多训练数据的语言中的知识提供了独特的机会。
- en: For instance, the Lithuanian subset contains approximately 50 hours of labeled
    speech. Building an automated Lithuanian transcriber from scratch is infeasible
    for agile Baltic startups. However, by leveraging Whisper’s transfer learning
    capabilities, you can rapidly construct a performant Lithuania-optimized system
    through fine-tuning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，立陶宛语子集包含大约50小时的标注语音。从零开始构建一个自动化的立陶宛语转录器对于灵活的波罗的海初创公司来说是不可行的。然而，通过利用Whisper的迁移学习能力，可以通过微调快速构建一个高效的立陶宛语优化系统。
- en: The implications are profound for enterprises in lower-income regions often
    underserved by AI. Rather than building costly customized models, adapting Whisper
    alleviates economic barriers to speech technology access.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些通常被AI服务不足的低收入地区的企业而言，这些影响是深远的。与其构建昂贵的定制化模型，不如通过适应Whisper来消除进入语音技术的经济障碍。
- en: Constructively integrating these datasets presents a means of propagating social
    good through language technology. Strategic incorporation must balance accuracy,
    speed, and inclusion. While augmenting with all 100+ Common Voice languages maximizes
    coverage, convergence would be prohibitive for most applications. We must be selective.
    For global enterprises, carefully selecting ~10 diverse languages for enhancement
    ensures sustainable commercial viability without excluding underserved communities.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些数据集建设性地整合起来，提供了一种通过语言技术传播社会福祉的途径。战略性地整合必须平衡准确性、速度和包容性。虽然通过增加所有100多种Common
    Voice语言可以最大化覆盖范围，但对大多数应用来说，趋同将是不可行的。我们必须有所选择。对于全球企业，仔细选择大约10种不同语言进行增强，确保可持续的商业可行性，同时不排除服务不足的群体。
- en: This strategic balancing act permeates all forms of algorithmic bias mitigation.
    Prejudicial solutions, such as intentionally hampering performance in specific
    languages, should be avoided. Instead, we can proactively improve technologies
    for excluded groups through targeted data augmentation. Common Voice provides
    the data resources to achieve this sustainably.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种战略平衡贯穿于所有形式的算法偏见缓解中。应避免出现带有偏见的解决方案，比如故意削弱特定语言的表现。相反，我们可以通过有针对性的数据增强，主动改善排除群体的技术。Common
    Voice提供了可持续实现这一目标的数据资源。
- en: Improving translation capabilities
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升翻译能力
- en: Speech translation entails significant complexity – systems must map acoustic
    signals to not just text but also text in another language. This task requires
    multifaceted model capabilities, from source language comprehension to target
    language fluency.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 语音翻译涉及重大的复杂性——系统不仅需要将音频信号映射为文本，还需要将文本映射为另一种语言的文本。这个任务需要多方面的模型能力，从源语言理解到目标语言流利度。
- en: Whisper’s architecture provides strong foundations, integrating an encoder-decoder
    structure with deep attentional fusion between audio semantics and language generation.
    However, no organization alone can keep up with the continuous evolution of diverse
    acoustic environments and low-resource languages.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的架构提供了强大的基础，整合了编码器-解码器结构，并在音频语义与语言生成之间实现了深度注意力融合。然而，没有任何一个组织能够独立跟上多样化的声学环境和低资源语言的持续演变。
- en: Mozilla’s Common Voice project members are constructing accessible multilingual
    corpora. The project’s upcoming 12th edition will include speech translation data
    pairs in 50 languages to further democratization efforts. Integrating these datasets
    can optimize Whisper for production translation use cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Mozilla的Common Voice项目成员正在建设可访问的多语言语料库。该项目即将发布的第12版将包含50种语言的语音翻译数据对，进一步推动普及化工作。整合这些数据集可以优化Whisper在生产翻译应用中的表现。
- en: For instance, call centers again represent a compelling but challenging application
    area. Agents must handle customer inquiries globally across different languages
    – training models exclusively on individual high-resource language risks, excluding
    underrepresented tongues and accents.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，呼叫中心再次呈现了一个既有吸引力又具挑战性的应用领域。客服人员必须处理来自不同语言的全球客户咨询——仅在单一高资源语言上训练模型存在风险，会排除那些资源较少的语言和口音。
- en: So, constructively balancing languages is crucial for ethical deployment. Achieving
    parity requires the strategic incorporation of diverse linguistic data. Sources
    such as Common Voice, through crowdsourced global recordings, provide microcosms
    of real-world language variability. Models trained on these datasets learn to
    parse multifaceted accents and speech cadences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，语言的构建性平衡对于伦理性的部署至关重要。实现平等需要战略性地融入多样的语言数据。像Common Voice这样的来源，通过全球众包录音，提供了现实世界语言多样性的缩影。基于这些数据集训练的模型学会解析多样的口音和语调节奏。
- en: Progress in automatic speech translation has accelerated recently through self-supervised
    techniques. Models such as XLSR-Wav2vec2, pre-trained on 56k hours of Common Voice
    data across 50 languages, have created breakthroughs in direct speech-to-speech
    translation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音翻译的进展最近通过自监督技术加速。像XLSR-Wav2vec2这样的模型，在跨50种语言的56k小时Common Voice数据上进行了预训练，并在直接语音到语音的翻译中取得了突破。
- en: With our newfound strategies for enhancing Whisper’s translation capabilities,
    we’ll embark on setting up Whisper pipeline components. This shift in focus lays
    the groundwork for a more granular examination of the tools and processes integral
    to Whisper’s ASR workflow. By delving into the setup of Whisper’s pipeline components,
    we’re preparing to fine-tune our approach, ensuring our project’s success with
    a solid, practical foundation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们增强 Whisper 翻译能力的新策略，我们将开始设置 Whisper 流水线组件。这个聚焦的转变为深入审视 Whisper ASR 工作流中至关重要的工具和过程奠定了基础。通过深入探讨
    Whisper 流水线组件的设置，我们正在为微调方法做准备，确保项目成功，并以扎实、实际的基础进行。
- en: Milestone 3 – Setting up Whisper pipeline components
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 3 – 设置 Whisper 流水线组件
- en: 'The process of ASR can be broken down into three main parts:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ASR过程可以分为三个主要部分：
- en: '**Feature extractor**: This is the initial step of processing the raw audio
    inputs. Think of it as preparing the audio files, so the model can easily understand
    and use them. The feature extractor turns the audio into a format that highlights
    essential aspects of the sound, such as pitch or volume, which are crucial for
    the model to recognize different words and sounds.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取器**：这是处理原始音频输入的初步步骤。可以把它看作是为音频文件做准备，以便模型可以轻松理解和使用它们。特征提取器将音频转换为一种格式，突出显示音频的关键特征，例如音调或音量，这对于模型识别不同的单词和声音至关重要。'
- en: '**The model**: This is the core part of the ASR process. It performs what we
    call sequence-to-sequence mapping. In simpler terms, it takes the processed audio
    from the feature extractor and works to convert it into a sequence of text. It’s
    like translating the language of sounds into the language of text. This part involves
    complex calculations and patterns to accurately determine what the audio says.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：这是自动语音识别（ASR）过程的核心部分。它执行我们所说的序列到序列的映射。简单来说，它接收来自特征提取器的处理过的音频，并将其转换为一系列文本。就像是将声音的语言翻译成文本的语言。这一部分涉及复杂的计算和模式，以准确确定音频内容。'
- en: '**Tokenizer**: After the model has done its job of mapping the sounds to text,
    the tokenizer steps in. It post-processes the model’s outputs and formats them
    into readable text. It’s like giving the final touch to the translation, ensuring
    that it makes sense in text form and follows the rules of the language, such as
    proper spacing and punctuation.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词器**：在模型完成将声音映射为文本的工作后，分词器介入。它对模型的输出进行后处理，将其格式化为可读的文本。就像是对翻译做最后的修饰，确保其在文本形式中有意义，并遵循语言规则，如正确的空格和标点。'
- en: In Hugging Face Transformers, a popular toolkit for handling NLP tasks, such
    as text classification, language translation, and speech recognition, the Whisper
    model has a feature extractor and a tokenizer, aptly named *WhisperFeatureExtractor*
    and *WhisperTokenizer*, respectively.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hugging Face Transformers 中，这是一个处理 NLP 任务（如文本分类、语言翻译和语音识别）流行的工具包，Whisper 模型有一个特征提取器和一个分词器，分别命名为
    *WhisperFeatureExtractor* 和 *WhisperTokenizer*。
- en: We will look deeper into the feature extractor and tokenizer specifics separately.
    Understanding these components is critical as each plays a vital role in converting
    spoken words into written text. We’ll explore how the feature extractor fine-tunes
    the raw audio for the model and how the tokenizer ensures the output text is accurate
    and coherent. This detailed look will give you a clearer picture of how the Whisper
    model processes speech, turning the complex task of speech recognition into a
    streamlined, efficient process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别深入探讨特征提取器和分词器的具体细节。理解这些组件至关重要，因为它们每个都在将语音转换为书面文本的过程中扮演着关键角色。我们将探讨特征提取器如何将原始音频进行微调，以便为模型提供数据，以及分词器如何确保输出文本的准确性和连贯性。这一详细的探讨将让你对Whisper模型如何处理语音、将复杂的语音识别任务转化为流畅、高效的过程有更清晰的认识。
- en: We will return to the *WhisperFeatureExtractor*. For now, let’s first understand
    the *WhisperTokenizer* component.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到*WhisperFeatureExtractor*。现在，先让我们了解一下*WhisperTokenizer*组件。
- en: Loading WhisperTokenizer
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载WhisperTokenizer
- en: The Whisper tokenizer helps translate text token sequences (numbers) into actual
    readable text. For example, it can turn a sequence such as [1169, 3797, 3332]
    into the sentence “the cat sat.”
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper分词器帮助将文本标记序列（数字）转换为实际可读的文本。例如，它可以将像[1169, 3797, 3332]这样的序列转换为句子“the cat
    sat”。
- en: In traditional speech recognition models, we use a method called **connectionist
    temporal classification** (**CTC**) to decode speech, and a specific CTC tokenizer
    is needed for each dataset. However, the Whisper model, which uses a different
    architecture (encoder-decoder), lets us use its pre-trained tokenizer directly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的语音识别模型中，我们使用一种叫做**连接主义时间分类（CTC）**的方式来解码语音，并且每个数据集都需要一个特定的CTC分词器。然而，Whisper模型采用了不同的架构（编码器-解码器），允许我们直接使用其预训练的分词器。
- en: This Whisper tokenizer has been trained in many languages, making it suitable
    for almost any multilingual speech recognition task. For instance, if you’re working
    with Hindi, you can load the Whisper tokenizer without any changes. You need to
    specify the language you’re working with (for example, Hindi) and the task (for
    example, transcription). This tells the tokenizer to add particular language and
    task tokens at the beginning of the sequences it processes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Whisper分词器已经在多种语言中进行了训练，使其适用于几乎所有的多语言语音识别任务。例如，如果你在处理印地语，你可以直接加载Whisper分词器而无需任何修改。你需要指定你正在使用的语言（例如，印地语）和任务（例如，转录）。这会告诉分词器在处理的序列开头添加特定的语言和任务标记。
- en: 'Here’s an example of how to load the Whisper tokenizer for Hindi:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何加载印地语Whisper分词器的示例：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can also adapt this for speech translation by changing the task to `translate`
    and setting the language to your target language. This will ensure that the tokenizer
    adds the proper tokens for translating speech.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过将任务更改为`translate`并将语言设置为目标语言，来将其适应于语音翻译。这将确保分词器为翻译语音添加适当的标记。
- en: 'To check that the tokenizer works correctly with Hindi, test it on a sample
    from the Common Voice dataset. Of course, this does not necessarily mean the tokenizer
    can recognize the meaning of the text. Instead, it translates sequences of text
    tokens (numbers) into actual readable text indicating the language and other features.
    When encoding speech, the tokenizer adds *special tokens* at the beginning and
    end, such as tokens for the start/end of the transcript, language, and task. You
    can ignore these unique tokens when decoding to regain a clean, original text
    string. This ensures that the tokenizer can accurately handle the Hindi language
    in speech recognition tasks. The following Python snippet demonstrates a basic
    workflow for processing speech data for speech recognition tasks using a tokenizer
    – in this case, within the context of the Common Voice 11 dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查分词器是否正确处理印地语，可以在Common Voice数据集中的样本上进行测试。当然，这并不一定意味着分词器能够识别文本的含义。相反，它将文本标记序列（数字）转换为实际可读的文本，指示语言和其他特征。在对语音进行编码时，分词器会在序列的开始和结束添加*特殊标记*，例如转录的开始/结束标记、语言和任务标记。在解码时，你可以忽略这些独特的标记，以恢复干净、原始的文本字符串。这确保了分词器能够准确处理印地语在语音识别任务中的应用。以下Python代码片段展示了一个处理语音数据的基本工作流，适用于使用分词器的语音识别任务——在这种情况下，是在Common
    Voice 11数据集的上下文中：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here’s a high-level explanation of each step:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个步骤的高级解释：
- en: '**Extract the** **input sentence**:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取输入句子**：'
- en: '[PRE7]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This line retrieves the first sentence from the training set of the Common Voice
    11 dataset. `common_voice["train"][0]["sentence"]` is a dictionary access pattern
    where `"train"` indicates the subset of the dataset (training data in this case),
    `[0]` selects the first record, and `["sentence"]` extracts the sentence text.
    We want to process this sentence for speech recognition.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一行代码从 Common Voice 11 数据集的训练集（`common_voice["train"][0]["sentence"]`）中提取第一句话。这是一个字典访问模式，其中`"train"`表示数据集的子集（此例中为训练数据），`[0]`选择第一条记录，`["sentence"]`提取句子文本。我们希望处理这个句子进行语音识别。
- en: '**Tokenize the** **input sentence**:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对** **输入句子**进行分词：'
- en: '[PRE8]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The tokenizer converts the input string into a sequence of tokens. These tokens
    are numerical representations of the words or subwords in the sentence. `input_ids`
    are the indices assigned to each token by the tokenizer, effectively transforming
    the sentence into a format that a model can understand. This step is crucial for
    preparing text data for processing with neural networks as they require numerical
    input.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分词器将输入字符串转换为一系列的标记。这些标记是句子中单词或子词的数值表示。`input_ids`是分词器分配给每个标记的索引，实质上将句子转换为模型可以理解的格式。这个步骤对于准备神经网络处理的文本数据至关重要，因为神经网络需要数值输入。
- en: '**Decode the tokens (with and without** **special tokens)**:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码标记（包括和不包括** **特殊标记）**：'
- en: '[PRE9]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, the decoded string excludes special tokens. This version is closer to
    the original human-readable sentence, as it removes tokens not directly related
    to the original text content.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，解码后的字符串不包含特殊标记。这个版本更接近原始的人类可读句子，因为它去除了与原始文本内容无关的标记。
- en: '`print` statements display the original input sentence, the decoded sentences
    (with and without special tokens), and a Boolean value indicating whether the
    original and the decoded sentence (without special tokens) are identical. This
    comparison helps us check the fidelity of the tokenization and detokenization
    processes. It’s a simple way to verify that the tokenizer can accurately reproduce
    the original sentence after converting it into tokens and back, minus any special
    tokens used for processing.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print`语句显示原始输入句子、解码后的句子（包括和不包括特殊标记）以及一个布尔值，指示原始句子和解码后的句子（不包括特殊标记）是否相同。这个比较帮助我们检查分词和解词过程的准确性。这是验证分词器在将句子转换为标记后，能够准确地重现原始句子的简单方法，除去处理时使用的任何特殊标记。'
- en: This snippet illustrates how text data is prepared and handled in the context
    of speech recognition and processing with the Common Voice 11 dataset. Such a
    process is part of a larger pipeline that might include converting audio into
    text, processing the text for training or inference with machine learning models,
    and evaluating the models’ performance in tasks such as ASR. Understanding the
    role of tokenizers is essential as they bridge the gap between raw text data and
    the numerical formats required for effective model training and operation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段展示了在语音识别和处理过程中，如何准备和处理文本数据，使用的是 Common Voice 11 数据集。这个过程是更大工作流的一部分，可能包括将音频转换为文本，处理文本以用于机器学习模型的训练或推理，并评估模型在自动语音识别（ASR）等任务中的表现。理解分词器的作用至关重要，因为它们在原始文本数据和有效模型训练和操作所需的数值格式之间架起了桥梁。
- en: 'Here is the print output you will see after the previous snippet is ran:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你在运行上述代码片段后看到的打印输出：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Equipped with a better understanding of the purpose and capabilities of the
    *WhisperTokenizer*, let’s explore the *WhisperFeatureExtractor* in the next milestone.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更好地理解*WhisperTokenizer*的目的和功能，让我们在下一个里程碑中探索*WhisperFeatureExtractor*。
- en: Milestone 4 – Transforming raw speech data into Mel spectrogram features
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 4 - 将原始语音数据转化为 Mel 频谱特征
- en: Speech can be considered a one-dimensional array that changes over time, with
    each point in the array representing the loudness or amplitude of the sound. To
    understand speech, we need to capture its frequency and acoustic features, which
    can be done by analyzing the amplitude.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 语音可以被看作是一个随时间变化的单维数组，数组中的每个点表示声音的响度或振幅。为了理解语音，我们需要捕捉它的频率和声学特征，可以通过分析振幅来实现。
- en: However, speech is a continuous sound stream, and computers can’t handle infinite
    data. So, we must convert this continuous stream into a series of discrete values
    by sampling the speech at regular intervals. This sampling is measured in samples
    per second or Hertz (Hz). The higher the sampling rate, the more accurately it
    captures the speech, but it also means more data to store every second.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，语音是一个连续的声音流，而计算机无法处理无限的数据。因此，我们必须通过在规律的时间间隔对语音进行采样，将这个连续的流转换为一系列离散值。这种采样是以每秒样本数或赫兹（Hz）来衡量的。采样率越高，语音捕捉得越精确，但这也意味着每秒要存储更多的数据。
- en: It’s important to ensure that the sampling rate of the audio matches what the
    speech recognition model expects. If the rates don’t match, it can lead to errors.
    For example, playing a sound sampled at 16 kHz at 8 kHz will make it sound slower.
    The Whisper model, for instance, expects a sampling rate of 16 kHz, so we need
    to ensure our audio matches this rate. Otherwise, we might train the model on
    distorted audio, such as slow-motion speech.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 确保音频的采样率与语音识别模型的要求相匹配是非常重要的。如果采样率不匹配，可能会导致错误。例如，以8 kHz播放一个16 kHz采样的声音，会使其听起来变慢。以Whisper模型为例，它期望的采样率是16
    kHz，因此我们需要确保我们的音频符合这个采样率。否则，我们可能会在失真的音频上训练模型，例如慢动作的语音。
- en: The Whisper feature extractor, a tool used in speech recognition, does two things
    with audio samples. First, it makes sure all audio samples are precisely 30 seconds
    long. If a sample is shorter, it adds silence to the end to reach 30 seconds.
    If it’s longer, it cuts it down to 30 seconds. This means we don’t need an attention
    mask for the Whisper model, which is unique. Usually, in audio models, you need
    an attention mask to show where you’ve added silence, but Whisper can figure it
    out itself.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper特征提取器是一个用于语音识别的工具，它对音频样本做了两件事。首先，它确保所有音频样本的时长恰好为30秒。如果样本较短，它会在末尾添加静音以达到30秒。如果样本过长，它会将其裁剪为30秒。这意味着我们不需要为Whisper模型提供注意力掩码，这一点非常独特。通常在音频模型中，你需要提供注意力掩码来标识添加了静音的位置，但Whisper能够自行识别。
- en: The second thing the Whisper feature extractor does is turn these adjusted audio
    samples into **log-Mel** spectrograms. These are visual charts showing the frequencies
    in the sound over time, where different colors represent different intensities
    of frequencies. The Whisper model uses these charts to understand and process
    speech. They’re designed to mimic how humans hear, focusing on specific frequencies
    that are more important for understanding speech.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper特征提取器做的第二件事是将这些调整后的音频样本转换为**log-Mel**声谱图。这些是显示声音频率随时间变化的视觉图表，其中不同的颜色代表不同强度的频率。Whisper模型使用这些图表来理解和处理语音。它们的设计模仿人类的听觉，重点关注对理解语音更为重要的特定频率。
- en: In summary, ensuring your audio samples are at the proper sampling rate (16
    kHz for Whisper) is crucial when working with speech recognition and the Whisper
    model. The feature extractor then standardizes these samples to 30 seconds each
    by adding silence or cutting excess. Finally, it converts these samples into log-Mel
    spectrograms, visual representations of sound frequencies, which the Whisper model
    uses to recognize and process speech. These steps are essential for accurate speech
    recognition.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在使用语音识别和Whisper模型时，确保你的音频样本具有正确的采样率（Whisper的采样率是16 kHz）是至关重要的。特征提取器接着会通过添加静音或剪切多余部分，将这些样本标准化为每个30秒。最后，它将这些样本转换为log-Mel声谱图，声频的可视化表示，Whisper模型利用这些图表来识别和处理语音。这些步骤对于准确的语音识别至关重要。
- en: 'Luckily, the Hugging Face Transformers Whisper feature extractor performs the
    padding and spectrogram conversion in just one line of code! Let’s go ahead and
    load the feature extractor from the pre-trained checkpoint to have it ready for
    our audio data:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Hugging Face Transformers中的Whisper特征提取器只需一行代码就能完成填充和声谱图转换！让我们从预训练检查点加载特征提取器，以便为我们的音频数据做好准备：
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Combining to create a WhisperProcessor class
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并创建一个WhisperProcessor类
- en: 'To make it easier to work with the feature extractor and tokenizer, we can
    combine them into a single class called `WhisperProcessor`. This processor acts
    like both `WhisperFeatureExtractor` and `WhisperTokenizer`. It can be used on
    audio inputs and model predictions as needed. This way, during training, we only
    need to focus on two main components: the *processor* and the *model*. The following
    Python snippet illustrates how to initialize `WhisperProcessor` for the `openai/whisper-small`
    model, explicitly configured for transcribing Hindi language audio:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便与特征提取器和分词器一起工作，我们可以将它们组合成一个名为`WhisperProcessor`的类。这个处理器同时充当`WhisperFeatureExtractor`和`WhisperTokenizer`。它可以根据需要在音频输入和模型预测上使用。这样，在训练过程中，我们只需要关注两个主要组件：*处理器*和*模型*。以下Python代码片段展示了如何为`openai/whisper-small`模型初始化`WhisperProcessor`，并特别配置为转录印地语音频：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s check out the first record from the Common Voice dataset to understand
    the data format:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看Common Voice数据集中的第一条记录，以了解数据格式：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we see a one-dimensional audio array and a matching written transcript.
    Remember, the sampling rate of our audio must match the Whisper model’s rate (16
    kHz). Our example audio is recorded at 48 kHz, so we must adjust it to 16 kHz
    before using the Whisper feature extractor.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了一维音频数组和对应的书面转录。请记住，我们的音频采样率必须与Whisper模型的采样率（16 kHz）匹配。我们的示例音频以48 kHz录制，因此在使用Whisper特征提取器之前，必须将其调整为16
    kHz。
- en: 'We’ll change the audio to the proper sampling rate using the dataset’s `cast_column`
    method. It applies transformations to the data in a given column, such as resampling
    the audio data to a different sampling rate. It is beneficial when working with
    audio datasets in machine learning tasks. The `cast_column` method doesn’t modify
    the original audio file; instead, it tells the dataset to change the sample rate
    whenever the audio is first loaded:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用数据集的`cast_column`方法将音频转换为合适的采样率。该方法对给定列中的数据应用转换，例如将音频数据重新采样到不同的采样率。当在机器学习任务中处理音频数据集时，这是非常有益的。`cast_column`方法不会修改原始音频文件；它只是告诉数据集在首次加载音频时改变采样率：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here’s the print output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是打印输出：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we reload the first audio sample, it will be at the 16 kHz sampling rate
    we need.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们重新加载第一个音频样本时，它将以我们需要的16 kHz采样率呈现。
- en: Now, the sampling rate is down to 16 kHz. The values in the array have also
    changed – we now have about one value for every three we had before.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，采样率已降至16 kHz。数组中的值也发生了变化——我们现在大约每三个原来的值对应一个新值。
- en: 'Next, let’s write a function to get our data ready for the model:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写一个函数，准备我们的数据供模型使用：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding snippet, we do the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们执行了以下操作：
- en: Load and resample the audio by calling `batch["audio"]`. As mentioned previously,
    Hugging Face Datasets will automatically resample the audio.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用`batch["audio"]`加载并重新采样音频。如前所述，Hugging Face Datasets将自动重新采样音频。
- en: Use the feature extractor to turn the one-dimensional audio array into log-Mel
    spectrogram input features.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征提取器将一维音频数组转换为log-Mel谱图输入特征。
- en: Convert the transcripts into label IDs using the tokenizer.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分词器将转录文本转换为标签ID。
- en: 'Now that we have the `prepare_dataset()` function defined, we can apply this
    data preparation function to all our training examples using the dataset’s `.``map`
    method:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了`prepare_dataset()`函数，可以使用数据集的`.map`方法将该数据准备函数应用于所有训练样本：
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There we go! Our data is now fully prepped for training. Let’s move on to how
    to use this data to fine-tune Whisper.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！我们的数据现在已经完全准备好用于训练。接下来我们将讲解如何使用这些数据来微调Whisper。
- en: Note
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The datasets currently use both `torchaudio` and `librosa` for audio handling.
    If you want to do your own audio loading or sampling, you can use the `path` column
    to find the audio file location and ignore the `audio` column.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，数据集使用`torchaudio`和`librosa`来处理音频。如果你想自行加载音频或进行采样，可以使用`path`列找到音频文件的位置，并忽略`audio`列。
- en: As we culminate our exploration of synthesizing `WhisperProcessor`, merging
    the feature extractor and tokenizer into a unified workflow, we transition toward
    defining training parameters and hardware configurations. This crucial juncture
    signifies our preparation for the intricate task of fine-tuning, emphasizing the
    strategic selection of training parameters and hardware configurations that align
    with our learning project’s scale and complexity.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成合成 `WhisperProcessor` 的探索，融合特征提取器和标记器为统一的工作流后，我们将过渡到定义训练参数和硬件配置。这一关键的转折点标志着我们为微调的复杂任务做好了准备，强调了选择与我们的学习项目规模和复杂性相匹配的训练参数和硬件配置。
- en: Milestone 5 – Defining training parameters and hardware configurations
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 5 – 定义训练参数和硬件配置
- en: 'Now that our data is ready, we can start training our model. We’ll use the
    Hugging Face Trainer to help with most of the work. The Hugging Face `Trainer`
    class provides a feature-complete training and evaluation loop for PyTorch models
    optimized for Transformers. It supports distributed training on multiple GPUs/TPUs
    and mixed precision and offers a lot of customizability for users. The `Trainer`
    class abstracts away the complexities of the training loop, allowing users to
    focus on providing the essential components required for training, such as a model
    and a dataset. Here’s what we need to do:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经准备好，我们可以开始训练模型了。我们将使用 Hugging Face Trainer 来帮助完成大部分工作。Hugging Face
    的 `Trainer` 类提供了一个功能完整的训练和评估循环，专为优化 Transformer 的 PyTorch 模型而设计。它支持在多个 GPU/TPU
    上进行分布式训练和混合精度，并且提供了大量的可定制选项。`Trainer` 类抽象了训练循环的复杂性，使用户能够专注于提供训练所需的基本组件，比如模型和数据集。我们需要做的是：
- en: '**Set up a data collator**: This tool takes our prepared data into PyTorch
    tensors that the model can use.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置数据整理器**：这个工具将我们准备好的数据转化为模型可以使用的 PyTorch tensors。'
- en: '**Choose evaluation metrics**: We want to see how well the model performs using
    the **word error rate** (**WER**) metric. To perform this calculation, we’ll create
    a function called compute_metrics.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择评估指标**：我们希望通过 **字错误率** (**WER**) 指标来查看模型的表现。为了进行这个计算，我们将创建一个名为 `compute_metrics`
    的函数。'
- en: '**Load a pre-trained model**: We’ll start with an already-trained model and
    set it up for further training. Training Whisper from scratch is not an option
    due to the intense data and computing resources required for such a task.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载预训练模型**：我们将从一个已经训练好的模型开始，并将其设置为继续训练。由于从头开始训练 Whisper 需要大量的计算资源和数据，这个选项不可行。'
- en: '**Define training arguments**: These arguments guide the Hugging Face Trainer
    on how to train the model.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义训练参数**：这些参数将指导 Hugging Face Trainer 如何训练模型。'
- en: After fine-tuning the model, we’ll test it on new data to ensure it can accurately
    transcribe speech in Hindi.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调模型后，我们将对新的数据进行测试，以确保它能够准确地转录印地语的语音。
- en: Setting up the data collator
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据整理器
- en: 'The data collator for speech models like ours is a bit special. It handles
    *input features* and *labels* separately: the feature extractor manages the *input
    features*, whereas the tokenizer manages the *labels*.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们这样的语音模型，数据整理器有点特殊。它分别处理 *输入特征* 和 *标签*：特征提取器处理 *输入特征*，而标记器处理 *标签*。
- en: The input features are set to 30 seconds and have been turned into a fixed-size
    log-Mel spectrogram. We just need to convert them into grouped *PyTorch tensors*.
    We can do this using the feature extractor’s `self.processor.tokenizer.pad` method
    with `return_tensors="pt"`. Since the input features are already fixed in size,
    we’re just changing them into *PyTorch tensors* without adding extra padding.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输入特征设置为 30 秒，并已转换为固定大小的对数 Mel 频谱图。我们只需要将其转换为分组的 *PyTorch tensors*。我们可以使用特征提取器的
    `self.processor.tokenizer.pad` 方法，参数设置为 `return_tensors="pt"` 来完成此操作。由于输入特征已经是固定大小，我们只是将其转换为
    *PyTorch tensors*，而不添加额外的填充。
- en: The labels, however, still need to be padded. First, we must pad them to the
    longest length in our batch using the `self.processor.tokenizer.pad` method. We
    are replacing the padding tokens with `-100` so they don’t affect the loss calculation.
    We must also remove the start of the transcript token from the beginning of the
    label sequence, as we’ll add it back during training.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，标签仍然需要进行填充。首先，我们必须将标签填充到我们批次中的最长长度，使用 `self.processor.tokenizer.pad` 方法。我们将填充标记替换为
    `-100`，以避免它们影响损失计算。我们还需要从标签序列的开头删除转录起始标记，因为我们将在训练期间将其重新添加回来。
- en: 'We can use the `WhisperProcessor` class we made earlier to handle the feature
    extractor and tokenizer tasks:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前定义的`WhisperProcessor`类来处理特征提取和分词任务：
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s instantiate the data collator we’ve just defined:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实例化刚才定义的数据整理器：
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Milestone 6 – Establishing standardized test sets and metrics for performance
    benchmarking
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 6 - 为性能基准测试建立标准化的测试集和度量标准
- en: 'Now, let’s learn how to check our model’s performance. We’ll use the WER metric,
    a common way to evaluate speech recognition systems. We’ll load the WER metric
    from Hugging Face `evaluate`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何检查模型的性能。我们将使用WER（词错误率）指标，这是评估语音识别系统的常用方法。我们将从Hugging Face `evaluate`加载WER指标：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we’ll create a function called `compute_metrics` to calculate the WER:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个名为`compute_metrics`的函数来计算WER：
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This function fixes our `label_ids` (where we had replaced padding tokens with
    `-100`). Then, it turns both the predicted and label IDs into text strings. Lastly,
    it calculates the WER between these two.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数修复了我们的`label_ids`（我们将填充token替换为`-100`）。然后，它将预测的ID和标签ID转化为文本字符串。最后，它计算这两者之间的WER。
- en: Loading a pre-trained model checkpoint
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载预训练模型检查点
- en: 'We’ll start with a pre-trained Whisper model. This is easy with Hugging Face
    Transformers:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个预训练的Whisper模型开始。这在Hugging Face Transformers中很容易实现：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This model has settings that we need to adjust for training. We’ll set specific
    tokens to `None` and make sure no tokens are suppressed:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有一些设置需要我们调整以进行训练。我们将特定的tokens设置为`None`，并确保没有tokens被抑制：
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Defining training arguments
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义训练参数
- en: 'We must define the training details, such as where to save the model and how
    often to check its performance and other settings. There is a particular class
    called `Seq2SeqTrainingArguments` for explicitly declaring training arguments.
    A subset of the argument parameters are explained here:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须定义训练细节，例如模型保存的位置、检查性能的频率以及其他设置。这里有一个特别的类叫做`Seq2SeqTrainingArguments`，用于明确声明训练参数。这里解释了一部分参数：
- en: '`output_dir`: The local directory in which to save the model weights. This
    will also be the repository name on the Hugging Face Hub ([https://huggingface.co/](https://huggingface.co/)).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir`：保存模型权重的本地目录。这也将是Hugging Face Hub上的仓库名称（[https://huggingface.co/](https://huggingface.co/)）。'
- en: '`generation_max_length`: The maximum number of tokens to autoregressively generate
    during evaluation.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_max_length`：在评估期间，自回归生成的最大token数量。'
- en: '`save_steps`: The intermediate checkpoints will be saved and uploaded asynchronously
    to the Hub every `save_steps` training step during training.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps`：在训练过程中，每经过`save_steps`步训练，中间检查点将被保存并异步上传到Hub。'
- en: '`eval_steps`: During training, intermediate checkpoints will be performed every
    `eval_steps` training step.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_steps`：在训练过程中，每经过`eval_steps`步训练，就会执行一次中间检查点。'
- en: '`report_to`: Where to save training logs. Supported platforms are `azure_ml`,
    `comet_ml`, `mlflow`, `neptune`, `tensorboard`, and `wand`. Pick your favorite
    or leave it as `tensorboard` to log into the Hub.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`report_to`：用于保存训练日志的地方。支持的平台有`azure_ml`、`comet_ml`、`mlflow`、`neptune`、`tensorboard`和`wand`。选择你喜欢的平台，或者将其保留为`tensorboard`以便将日志记录到Hub中。'
- en: For more details on the other training arguments, refer to the `Seq2SeqTrainingArguments`
    documents ([https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#trainer](https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#trainer)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多其他训练参数的详细信息，请参阅`Seq2SeqTrainingArguments`文档（[https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#trainer](https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#trainer)）。
- en: 'The following code snippet illustrates the declaration of `Seq2SeqTrainingArguments`
    with some of the parameters. You will find a complete working example in the companion
    Python notebook in this book’s GitHub repository:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何声明`Seq2SeqTrainingArguments`及其部分参数。你可以在本书GitHub仓库中的Python笔记本中找到完整的工作示例：
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you don’t want to upload the model to the Hub, set `push_to_hub=False`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想将模型上传到Hub，可以将`push_to_hub=False`。
- en: 'We’ll give these training details to the Hugging Face Trainer, along with our
    `model`, `dataset`, `data collator`, and `compute_metrics` functions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些训练细节提供给Hugging Face Trainer，并与我们的`model`、`dataset`、`data collator`以及`compute_metrics`函数一起使用：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With robust metrics for evaluating model performance and a transparent process
    defined for executing the training, we’ll now focus on a practical implementation
    – executing optimized training loops while leveraging our configured hyperparameters,
    datasets, and hardware.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过强大的模型性能评估指标和定义明确的透明训练过程，我们现在将重点关注实际实现——在利用已配置的超参数、数据集和硬件的基础上，执行优化的训练循环。
- en: Milestone 7 – Executing the training loops
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 7 – 执行训练循环
- en: 'To begin training, just run the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，只需运行以下命令：
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Figure 4**.1* shows an example of the output you can expect to see from the
    `trainer.train()` command’s execution:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.1* 显示了你可以期望从 `trainer.train()` 命令执行中看到的输出示例：'
- en: '![Figure 4.1 – Sample output from trainer.train() in Google Colab](img/B21020_04_1.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – Google Colab 中 `trainer.train()` 的示例输出](img/B21020_04_1.jpg)'
- en: Figure 4.1 – Sample output from trainer.train() in Google Colab
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – Google Colab 中 `trainer.train()` 的示例输出
- en: Each training batch will have an evaluation step that calculates and displays
    training/validation losses and WER metrics. Depending on your GPU, training could
    take 5–10 hours. If you run into memory issues, try reducing the batch size and
    adjusting `gradient_accumulation_steps` in the declaration of `Seq2SeqTrainingArguments`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练批次都将包含一个评估步骤，计算并显示训练/验证损失和 WER 指标。根据你的 GPU，训练可能需要 5 到 10 小时。如果遇到内存问题，尝试减小批量大小，并在
    `Seq2SeqTrainingArguments` 的声明中调整 `gradient_accumulation_steps`。
- en: 'Because of the parameters we established when declaring `Seq2SeqTrainingArguments`,
    our model metrics and performance will be pushed to the Hugging Face Hub with
    each training iteration. The key parameters driving that push to the Hub are shown
    here:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在声明 `Seq2SeqTrainingArguments` 时设定的参数，我们的模型指标和性能将在每次训练迭代后推送到 Hugging Face
    Hub。驱动推送到 Hub 的关键参数如下所示：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following screenshots show how to navigate to the Hugging Face TensorBoard
    and examples of the board with metrics from one of my fine-tuned models:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了如何导航到 Hugging Face TensorBoard，并展示了我的某个微调模型的指标面板示例：
- en: '![Figure 4.2 – Within the Hugging Face repository, select “Training metrics”
    to display the TensorBoard](img/B21020_04_2.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 在 Hugging Face 仓库中，选择“训练指标”以显示 TensorBoard](img/B21020_04_2.jpg)'
- en: Figure 4.2 – Within the Hugging Face repository, select “Training metrics” to
    display the TensorBoard
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 在 Hugging Face 仓库中，选择“训练指标”以显示 TensorBoard
- en: '![Figure 4.3 – Example of some of the metrics in the Hugging Face TensorBoard](img/B21020_04_3.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – Hugging Face TensorBoard 中一些指标的示例](img/B21020_04_3.jpg)'
- en: Figure 4.3 – Example of some of the metrics in the Hugging Face TensorBoard
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – Hugging Face TensorBoard 中一些指标的示例
- en: 'After successful training, anyone can access and use your model via the Hugging
    Face Hub. They can load it using a link from the Hub or use the `your-hugging-face-id/the-name-you-picked`
    identifier. Here’s an example of how to load the model:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 成功训练后，任何人都可以通过 Hugging Face Hub 访问并使用你的模型。他们可以通过 Hub 的链接加载模型，或使用 `your-hugging-face-id/the-name-you-picked`
    标识符加载模型。以下是加载模型的示例：
- en: '[PRE28]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: While the model we’ve fine-tuned works well with the Common Voice Hindi test
    data, it’s not perfect. This guide is meant to show you how to fine-tune pre-trained
    Whisper models on any speech recognition dataset in multiple languages. You might
    get even better results by tweaking the training settings, such as learning rate
    and dropout, or using a bigger pre-trained model (such as the medium or large
    versions).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们微调的模型在 Common Voice Hindi 测试数据上表现良好，但它并不完美。本指南旨在展示如何在多语言的任何语音识别数据集上微调预训练的
    Whisper 模型。通过调整训练设置，如学习率和 dropout，或使用更大的预训练模型（例如中型或大型版本），你可能会获得更好的结果。
- en: With the optimized training process complete and our fine-tuned model uploaded,
    we’ll now transition to assessing the real-world efficacy of our enhanced speech
    recognition capabilities. We will validate how our tailored Whisper model generalizes
    across languages, domains, and acoustic environments by benchmarking performance
    across diverse datasets.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化的训练过程完成并且我们的微调模型上传后，我们将过渡到评估我们增强的语音识别能力在现实世界中的有效性。我们将通过在不同数据集上进行性能基准测试，验证我们定制的
    Whisper 模型在语言、领域和声学环境中的泛化能力。
- en: Milestone 8 – Evaluating performance across datasets
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 8 – 在数据集之间评估性能
- en: As we conclude our Whisper fine-tuning journey, validating model performance
    across diverse real-world conditions represents a pivotal final milestone. Before
    deploying our optimized speech recognizer into production scenarios, comprehensively
    assessing its effectiveness across datasets, languages, accents, and acoustic
    environments is essential for instilling confidence. This testing phase unveils
    actual capabilities, revealing where additional tuning may be required while spotlighting
    areas suitable for immediate application. The rigorous evaluation processes outlined
    in this section aim to verify customized performance gains while guiding ethical
    and inclusive rollout by covering key facets such as bias mitigation, domain optimization,
    translation abilities, and expectation management.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束Whisper微调之旅时，验证模型在各种真实世界条件下的表现是一个关键的最终里程碑。在将我们的优化语音识别器投入生产环境之前，全面评估其在不同数据集、语言、口音和声学环境中的有效性对于增强信心至关重要。这一测试阶段揭示了模型的实际能力，揭示了可能需要额外调整的地方，同时突出了适合立即应用的领域。本节中概述的严格评估过程旨在验证定制的性能提升，同时通过涵盖偏见缓解、领域优化、翻译能力和期望管理等关键方面，指导伦理和包容性部署。
- en: Mitigating demographic biases
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少人口统计偏见
- en: Machine learning models, including those for speech recognition, can sometimes
    detect biases against certain genders, ethnicities, or age groups. This happens
    because the audio data they learn from can vary greatly between different groups
    of people. To prevent this, we must train the model with a wide range of data
    and use unique methods to check for biases.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型，包括语音识别模型，有时会检测到对某些性别、种族或年龄群体的偏见。这是因为它们学习的音频数据在不同群体之间可能差异很大。为防止这种情况，我们必须用广泛的数据来训练模型，并使用独特的方法来检查偏见。
- en: We should carefully examine where the model might work better for certain groups
    of people. This will help us understand which groups might need more support from
    the model. We can also change the data the model learns from to see if it treats
    different groups of people differently. This will help us find the real reasons
    for any unfairness.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该仔细检查模型在哪些群体中可能表现更好。这将帮助我们了解哪些群体可能需要模型提供更多支持。我们还可以改变模型学习的数据，看看它是否对不同的群体表现不同。这将帮助我们找出不公平现象的真正原因。
- en: Finding problems is not enough. We also need to add a variety of data to the
    model. This means getting data from many different sources, especially those that
    haven’t been included much before. We can use methods such as web scraping to
    find new kinds of speech data. We can also create artificial voices, but we must
    be careful and transparent about how we do this.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 发现问题还不够，我们还需要向模型中添加多样化的数据。这意味着需要从多个不同来源获取数据，尤其是那些之前未被充分包含的数据。我们可以使用网页抓取等方法来寻找新的语音数据。我们还可以创建人工语音，但必须小心并且在这方面保持透明。
- en: We need to be careful to avoid overcorrecting and creating new problems. Our
    goal is to improve the model for everyone. We can do this by testing it equally
    with different groups of people to ensure it works well for everyone.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要小心避免过度修正并创造出新的问题。我们的目标是改善每个人的模型。我们可以通过对不同群体进行平等测试，确保它适用于所有人，从而实现这一目标。
- en: We should aim to use language technology to unite people, not separate them.
    We should focus on making speech technology that is fair and helpful for everyone.
    This means constantly checking and improving our models to ensure they are fair
    and helpful for all different groups of people.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该致力于使用语言技术来团结人们，而不是将人们分开。我们应该专注于开发公平且对每个人都有帮助的语音技术。这意味着我们需要不断检查和改进我们的模型，以确保它们对所有不同群体都公平且有帮助。
- en: Optimizing for content domains
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化内容领域
- en: While Whisper’s extensive pre-training provides broad linguistic capabilities,
    tailoring its knowledge toward specialized domains is pivotal for competitive
    enterprise use cases. Contact centers, legal firms, finance brokers, telemedicine
    providers—speech recognition permeates diverse industries, each carrying distinct
    challenges. Beyond vocabulary, accurately modeling nonverbal cues, discourse patterns,
    and subtle connotations underpins contextual understanding in niche domains.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Whisper的广泛预训练提供了丰富的语言能力，但将其知识针对特定领域进行定制对竞争性的企业应用至关重要。呼叫中心、律师事务所、金融经纪人、远程医疗服务商——语音识别已经渗透到各个行业，每个行业都有其独特的挑战。除了词汇量，准确建模非语言提示、话语模式和微妙的暗示也在特定领域的上下文理解中起着基础性作用。
- en: Yet out-of-the-box ASR systems often stumble on niche terminology and struggle
    to convey implicitly layered meaning. For example, a precise understanding of
    clauses has substantive significance in legal contexts. Models trained exclusively
    on generic datasets fail to distill these specialized connotations. Exposing systems
    to targeted in-domain data is thus vital for infusing contextual mastery.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现成的自动语音识别（ASR）系统常常在面对专业术语时碰壁，并且难以传达隐含的层次化意义。例如，在法律语境中，精确理解条款具有重要意义。仅用通用数据集训练的模型无法提炼出这些专业含义。因此，将系统暴露于特定领域的数据对于注入领域上下文掌握至关重要。
- en: The nucleus of domain optimization lies in terminology mastery. Legal, medical,
    and financial contexts involve extensive exotic lexicons that shape substantive
    task competencies. Yet glossaries alone fail to encapsulate the layered semantics
    encoded in specialist dictionaries.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 领域优化的核心在于术语掌握。法律、医学和金融等领域涉及大量的特殊词汇，这些词汇塑造了具体任务的能力。然而，仅仅依靠术语表无法概括专家词典中编码的层次化语义。
- en: One option is to employ **explicit semantic analysis** (**ESA**), a computational
    method for mathematically representing human notions of language meaning. ESA
    is a high-dimensional space of concepts derived from a large text corpus, and
    it is used in NLP and information retrieval.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是采用**显式语义分析**（**ESA**），这是一种通过数学表示人类语言意义的计算方法。ESA是从大量文本语料库中衍生出的高维概念空间，广泛应用于自然语言处理和信息检索。
- en: In simple terms, ESA is a way for computers to understand the meaning of a piece
    of text by comparing it to a large amount of text data it has already analyzed.
    It does this by mapping the text to a set of concepts or topics derived from a
    large corpus of text data. This mapping is done in a high-dimensional space, where
    each dimension represents a different concept or topic.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，ESA是一种通过将文本与其已经分析过的大量文本数据进行对比，帮助计算机理解文本含义的方法。它通过将文本映射到一个由大量文本数据衍生的概念或主题集来实现这一点。这种映射发生在一个高维空间中，其中每个维度代表不同的概念或主题。
- en: For example, if the text is about “dogs,” ESA might map it to concepts such
    as “animals,” “pets,” “canines,” and so on. By doing this, ESA can understand
    the semantic meaning of the text, which can be used for tasks such as information
    retrieval, text classification, and more. ESA is beneficial because it can capture
    the meaning of text even when the words used are not the same. For instance, it
    can be understood that “dogs” and “canines” refer to the same concept, even though
    the words are different. This makes it a powerful tool for understanding and processing
    natural language.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果文本关于“狗”，ESA可能会将其映射到“动物”、“宠物”、“犬类”等概念上。通过这样做，ESA可以理解文本的语义，这些语义可以用于信息检索、文本分类等任务。ESA的好处在于它能够捕捉文本的意义，即使使用的词汇不同。例如，可以理解“狗”和“犬类”指的是相同的概念，尽管这两个词不同。这使得它成为理解和处理自然语言的强大工具。
- en: Managing user expectations
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理用户期望
- en: Responsible use of AI speech recognition technology involves ensuring users
    understand what the technology can and cannot do. It’s essential to be open about
    the technology’s capabilities and limits so that people can make informed choices
    about using it. This is especially crucial for those who might not have much digital
    experience.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任地使用AI语音识别技术需要确保用户理解该技术的能与不能。对技术的能力和局限性保持透明至关重要，这样人们才能做出关于是否使用它的明智选择。对于那些可能缺乏数字经验的人来说，这一点尤为重要。
- en: Effective communication about technology’s abilities helps build trust. This
    can be done through easy-to-understand summaries and explanations that address
    specific user needs without overwhelming them with too much detail. Tools such
    as model confidence scores and visualizations can help users gauge the reliability
    of the technology’s predictions, making it more transparent when and how it’s
    best used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地传达技术能力有助于建立信任。这可以通过易于理解的总结和解释来实现，针对特定用户需求进行阐述，而不会让他们感到过于复杂。诸如模型置信度评分和可视化工具可以帮助用户评估技术预测的可靠性，使其在何时何种情况下最佳使用更加透明。
- en: Being upfront about what technology can’t do is just as important. Recognizing
    limitations is not a sign of failure; it’s an opportunity for growth and improvement.
    For example, areas where Whisper might struggle, such as real-time recognition
    in noisy environments, should be seen as challenges to be solved through collaborative
    effort rather than permanent flaws.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 公开技术的局限性同样重要。认识到局限性并非失败的标志，而是成长和改进的机会。例如，Whisper 在嘈杂环境中的实时识别可能存在困难，这应被视为通过协作努力解决的挑战，而不是永久的缺陷。
- en: Listening to users and incorporating their feedback is critical to improving
    speech recognition technology for everyone. Regularly checking how the technology
    performs in real-world situations helps prevent it from drifting away from users’
    needs. By involving users in the process via humans-in-the-loop, we can focus
    on addressing the most pressing issues and make improvements more efficiently.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 听取用户反馈并将其纳入技术改进至关重要，以便为每个人提升语音识别技术。定期检查技术在实际应用中的表现，有助于防止它偏离用户需求。通过让用户参与这个过程，我们能够专注于解决最紧迫的问题，并更高效地进行改进。
- en: Milestone 9 – Building applications that demonstrate customized speech recognition
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 里程碑 9 – 构建展示定制语音识别的应用程序
- en: Now that our model has been fine-tuned let’s demonstrate how good it is at speech
    recognition (ASR)! We’ll use the Hugging Face Transformers pipeline to handle
    everything, from preparing the audio to decoding what the model thinks the audio
    says. For our demo, we’ll use **Gradio**, a tool that makes it super easy to build
    machine learning demos. You can create a demo with Gradio in just a few minutes!
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经完成微调，让我们展示它在语音识别（ASR）方面的表现！我们将使用 Hugging Face Transformers 流水线来处理所有的工作，从准备音频到解码模型所认为音频的内容。为了展示，我们将使用**Gradio**，一个让构建机器学习演示变得超级简单的工具。你只需要几分钟就能用
    Gradio 创建一个演示！
- en: 'Here is an example of a Gradio demo. In this demo, you can record speech using
    your computer’s microphone, after which the fine-tuned Whisper model will transcribe
    it into text:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 Gradio 演示的一个示例。在这个演示中，你可以使用电脑麦克风录音，然后经过微调的 Whisper 模型会将其转录成文本：
- en: '[PRE29]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here’s the output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.4 – Example of Gradio’s user interface for the fine-tuned Whisper
    model in Hugging Face](img/B21020_04_4.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – Hugging Face 中微调 Whisper 模型的 Gradio 用户界面示例](img/B21020_04_4.jpg)'
- en: Figure 4.4 – Example of Gradio’s user interface for the fine-tuned Whisper model
    in Hugging Face
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – Hugging Face 中微调 Whisper 模型的 Gradio 用户界面示例
- en: Record the audio with the microphone to test the model directly from Google
    Colab;, then click `Namaste`, which was then transcribed perfectly to the Hindi
    word `नामास्ते`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用麦克风录制音频，直接从 Google Colab 测试模型；然后点击`Namaste`，它会被完美地转录为印地语单词`नामास्ते`。
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As we conclude our journey into the intricacies of OpenAI’s Whisper, it’s clear
    that we’ve traversed a path rich with technical insights and practical wisdom.
    Our exploration has been more than just a theoretical examination; it’s been a
    hands-on experience, equipping you with the skills to fine-tune Whisper for specific
    domain and language needs and to overcome the challenges inherent in speech recognition
    technology.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入 OpenAI Whisper 技术的细节，我们的探索已经超越了理论的讨论，变成了一个实际的体验过程，赋予了你微调 Whisper 以满足特定领域和语言需求的能力，并帮助你克服语音识别技术中固有的挑战。
- en: We commenced with the foundational work of setting up a robust Python environment,
    augmenting Whisper’s knowledge by integrating diverse, multilingual datasets such
    as Common Voice. This step was crucial as it expanded Whisper’s linguistic breadth
    and set the stage for the subsequent fine-tuning process.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从建立一个强大的 Python 环境的基础工作开始，通过整合诸如 Common Voice 等多语言数据集，增强了 Whisper 的知识。这一步至关重要，因为它扩展了
    Whisper 的语言广度，并为后续的微调过程奠定了基础。
- en: The heart of this chapter revolved around tailoring Whisper’s predictions to
    align perfectly with your target applications. You’ve learned to tweak confidence
    levels, output classes, and time limits to match the expected results in specific
    use cases. The knowledge you’ve gained here is invaluable, especially when dealing
    with niche terminologies and diverse language datasets.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的核心围绕着将 Whisper 的预测结果调整得与目标应用完美对接。你已经学会了如何调整置信度、输出类别和时间限制，以便在特定用例中匹配预期结果。你在这里获得的知识非常宝贵，尤其是在处理专业术语和多样化语言数据集时。
- en: Much of our effort was devoted to tracking progress through straightforward
    testing. We established fixed benchmarks to gauge gains across languages and uses
    objectively, ensuring that our fine-tuning efforts were grounded and free from
    data bias.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大部分努力都集中在通过简明的测试跟踪进展。我们设立了固定的基准，客观地衡量不同语言和使用场景中的进展，确保我们的微调工作是有根据的，并且没有数据偏差。
- en: One of the most critical aspects we covered was the ethical use of technology.
    We emphasized the need to ensure equitable performance across demographics, ensuring
    that advancements in speech technology don’t inadvertently exclude groups with
    fewer advantages.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的一个关键方面是技术的伦理使用。我们强调了确保跨人口群体实现公平表现的必要性，确保语音技术的进步不会无意中排除那些处于不利地位的群体。
- en: As you’ve seen, fine-tuning Whisper involved a deep dive into its architecture
    and training methodologies. You’ve learned about handling different languages,
    optimizing Whisper for various content domains, and balancing accuracy with efficiency.
    We’ve also tackled challenges such as demographic biases, technical and linguistic
    hurdles, and the need for rapid adaptation to new vocabulary.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，微调 Whisper 涉及对其架构和训练方法的深入探讨。你已经了解了如何处理不同语言，如何为各种内容领域优化 Whisper，以及如何平衡准确性和效率。我们还解决了诸如人口偏差、技术和语言障碍以及快速适应新词汇的挑战。
- en: Moreover, we’ve discussed managing user expectations, an essential aspect of
    deploying AI technology. It’s crucial to be transparent about what technology
    can do and its limitations, ensuring users make informed decisions and trust it.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还讨论了管理用户期望，这是部署 AI 技术的一个重要方面。必须透明地说明技术能做什么以及其局限性，确保用户能够做出明智的决策并对其产生信任。
- en: As we look forward to this book’s next section, *Part 3 – Real-World Applications
    and Use Cases*, we’re poised to embark on a new adventure. Here, we’ll explore
    how to effectively apply Whisper in various industries, integrating it into real-world
    scenarios. You’ll discover how to harness Whisper in sectors such as healthcare
    and voice-assisted technologies, leveraging the skills and knowledge you’ve gained
    from this chapter to make a tangible impact in ASR.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 展望本书的下一部分，*第三部分 - 现实世界的应用和使用案例*，我们即将踏上新的冒险旅程。在这里，我们将探索如何在各个行业中有效地应用 Whisper，并将其整合到实际场景中。你将发现如何在医疗保健和语音辅助技术等领域中利用
    Whisper，并运用你在本章中获得的技能和知识，在自动语音识别（ASR）中产生实际影响。
- en: So, let’s carry forward the knowledge and experience from this chapter and see
    how we can apply Whisper in diverse and impactful ways. The journey continues,
    and the possibilities are as exciting as they are endless.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们继续带着本章的知识和经验，看看如何以多种有影响力的方式应用 Whisper。旅程仍在继续，可能性既令人兴奋又无限。
- en: 'Part 3: Real-world Applications and Use Cases'
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：现实世界的应用和使用案例
- en: In this part, you will explore the diverse real-world applications and use cases
    of OpenAI’s Whisper, learning how to integrate this powerful tool into various
    contexts effectively. From transcription services and voice assistants to accessibility
    features and customer service, you will gain insights into leveraging Whisper’s
    capabilities to enhance multiple industries. You will also delve into advanced
    techniques such as quantization, real-time speech recognition, and speaker diarization
    using **WhisperX** and NVIDIA’s **NeMo** framework. Furthermore, you will discover
    how to harness Whisper for personalized voice synthesis, creating unique voice
    models that capture the distinct characteristics of a target voice. Finally, this
    part will provide a forward-looking perspective on the evolving landscape of ASR
    and voice technologies, discussing anticipated trends, ethical considerations,
    and strategies for preparing for the future.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，你将探索 OpenAI 的 Whisper 在现实世界中的多种应用场景，学习如何将这一强大工具有效地整合到不同的环境中。从转录服务、语音助手到无障碍功能和客户服务，你将深入了解如何利用
    Whisper 的功能来提升多个行业。你还将深入学习一些高级技术，如量化、实时语音识别和说话人分离，使用**WhisperX**和 NVIDIA 的 **NeMo**
    框架。此外，你将发现如何利用 Whisper 进行个性化语音合成，创建捕捉目标声音特征的独特语音模型。最后，本部分将提供一个前瞻性的视角，探讨自动语音识别（ASR）和语音技术的不断发展，讨论预期的趋势、伦理考量以及如何为未来做好准备。
- en: 'This part includes the following chapters:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 5*](B21020_05.xhtml#_idTextAnchor142)*, Applying Whisper in Various
    Contexts*'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第五章*](B21020_05.xhtml#_idTextAnchor142)*，在不同情境中应用 Whisper*'
- en: '[*Chapter 6*](B21020_06.xhtml#_idTextAnchor160)*, Expanding Applications with
    Whisper*'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B21020_06.xhtml#_idTextAnchor160)*, 用Whisper扩展应用*'
- en: '[*Chapter 7*](B21020_07.xhtml#_idTextAnchor177)*, Exploring Advanced Voice
    Capabilities*'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B21020_07.xhtml#_idTextAnchor177)*, 探索高级语音功能*'
- en: '*Chapter 8, Diarizing Speech with WhisperX and NVIDIA’s NeMo*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第8章, 使用WhisperX和NVIDIA的NeMo进行语音标注*'
- en: '[*Chapter 9*](B21020_09.xhtml#_idTextAnchor207)*, Harnessing Whisper for Personalized
    Voice Synthesis*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B21020_09.xhtml#_idTextAnchor207)*, 利用Whisper进行个性化语音合成*'
- en: '*Chapter 10, Shaping the Future with Whisper*'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第10章, 用Whisper塑造未来*'
