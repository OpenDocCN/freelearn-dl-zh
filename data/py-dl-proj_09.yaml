- en: Object Detection Using OpenCV and TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenCV 和 TensorFlow 进行物体检测
- en: Welcome to the second chapter focusing on computer vision in *Python Deep Learning
    Projects* (a data science pun to kick us off!). Let's think about what we accomplished
    in [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml), *Handwritten Digits
    Classification Using ConvNets*, where we were able to train an image classifier
    with a **convolutional neural network** (**CNN**) to accurately classify handwritten
    digits in an image. What was a key characteristic of the raw data, and what was
    our business objective? The data was less complicated than it could have been
    because each image only had one handwritten digit in it and our goal was to accurately
    assign a digital label to the image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第二章，专注于计算机视觉的内容，出自 *Python 深度学习项目*（让我们用一个数据科学的双关语开始吧！）。让我们回顾一下在[第八章](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml)中我们所取得的成就，*使用卷积神经网络（ConvNets）进行手写数字分类*，在这一章中，我们能够使用**卷积神经网络**（**CNN**）训练一个图像分类器，准确地对图像中的手写数字进行分类。原始数据的一个关键特征是什么？我们的业务目标又是什么？数据比可能的情况要简单一些，因为每张图片中只有一个手写数字，我们的目标是准确地为图像分配数字标签。
- en: What would have happened if each image had multiple handwritten digits in it?
    What would have happened if we had a video of the digits? What if we want to identify
    where the digits are in the image? These questions represent challenges that real-world
    data embodies, and they drive our data science innovation to new models and capabilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每张图片中包含多个手写数字会发生什么？如果我们有一段包含数字的视频呢？如果我们想要识别图片中数字的位置呢？这些问题代表了现实世界数据所体现的挑战，并推动我们的数据科学创新，朝着新的模型和能力发展。
- en: Let's expand our line of questions and imagination to the next (hypothetical)
    business use case for our Python deep learning project, where we're looking to
    build, train, and test an object detection and classification model to be used
    by an automobile manufacturer in their new line of self-driving cars. Autonomous
    vehicles need to have fundamental computer vision capabilities that you and I
    have organically by way of our physiology and experiential learning. We as humans
    can examine our field of vision and report whether or not a specific item is present
    and where in relation to other objects that item (if present) is located. So,
    if I were to ask you if you see a chicken, you'd likely say no, unless you live
    on a farm and are looking out your window. But if I ask you if you see a keyboard,
    you'd likely say yes, and could even say that the keyboard is different from other
    objects and is in front of the wall before you.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将问题和想象力扩展到下一个（假设的）业务用例，针对我们的 Python 深度学习项目，我们将构建、训练和测试一个物体检测和分类模型，供一家汽车制造商用于其新一代自动驾驶汽车。自动驾驶汽车需要具备基本的计算机视觉能力，而这种能力正是我们通过生理和经验性学习所自然具备的。我们人类可以检查我们的视野并报告是否存在特定物体，以及该物体与其他物体的位置关系（如果存在的话）。所以，如果我问你是否看到了一个鸡，你可能会说没有，除非你住在农场并正在望着窗外。但如果我问你是否看到了一个键盘，你可能会说是的，并且甚至能够说出键盘与其他物体的不同，且位于你面前的墙之前。
- en: This is no trivial task for a computer. As Deep Learning Engineers, you are
    going to learn the intuition and model architecture that empowers you to build
    a powerful object detection and classification engine that we can envision being
    tested for use in autonomous vehicles. The data inputs that we're going to be
    working with in this chapter will be much more informationally complex than what
    we've had in previous projects, and the outcomes when we get them right will be
    that much more impressive.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算机来说，这不是一项简单的任务。作为深度学习工程师，你将学习到直觉和模型架构，这将使你能够构建一个强大的物体检测与分类引擎，我们可以设想它将被用于自动驾驶汽车的测试。在本章中，我们将处理的数据输入比以往的项目更为复杂，且当我们正确处理这些数据时，结果将更加令人印象深刻。
- en: So, let's get started!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Object detection intuition
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测直觉
- en: When you need your application to find and name things in an image, you need
    to build a deep neural network for object detection. The visual field is very
    complex, and a camera for still images and video captures frames with many, many
    objects in them. Object detection is used in manufacturing for process automation
    in production lines; autonomous vehicles sensing pedestrians, other cars, the
    road, and signs, for example; and, of course, facial recognition. Computer vision
    solutions based on machine learning and deep learning require you, the Data Scientist,
    to build, train, and evaluate models that can differentiate one object from another
    and then accurately classify those detected objects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要让应用程序在图像中找到并命名物体时，你需要构建一个用于目标检测的深度神经网络。视觉领域非常复杂，静态图像和视频的相机捕捉的帧中包含了许多物体。目标检测被用于制造业的生产线过程自动化；自动驾驶车辆感知行人、其他车辆、道路和标志等；当然，还有面部识别。基于机器学习和深度学习的计算机视觉解决方案需要你——数据科学家——构建、训练和评估能够区分不同物体并准确分类检测到的物体的模型。
- en: As you've seen in other projects we've worked on, CNNs are very powerful models
    for image data. We need to look at expansions on the basic architecture that has
    performed so well on a single (still) image with simple information to see what
    works best for complex images and video.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在我们处理的其他项目中看到的，CNN是处理图像数据的非常强大的模型。我们需要查看在单张（静态）图像上表现非常好的基础架构的扩展，看看哪些方法在复杂图像和视频中最有效。
- en: 'Progress recently has been made with these networks: Faster R-CNN, **region-based
    fully convolutional network** (**R-FCN**), MultiBox, **solid-state drive** (**SSD**),
    and **you only look once** (**YOLO**). We''ve seen the value of these models in
    common consumer applications such as Google Photos and Pinterest Visual Search.
    We are even seeing some of these that are lightweight and fast enough to perform
    well on mobile devices.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，以下网络取得了进展：Faster R-CNN、**基于区域的全卷积网络** (**R-FCN**)、MultiBox、**固态硬盘** (**SSD**)
    和 **你只看一次** (**YOLO**)。我们已经看到了这些模型在常见消费者应用中的价值，例如Google Photos和Pinterest视觉搜索。我们甚至看到其中一些模型足够轻量且快速，能够在移动设备上表现良好。
- en: 'Recent progress in the field can be researched with the following list of references:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下参考文献列表进行近期该领域的研究：
- en: '*PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection*,
    arXiv:1608.08021'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PVANET: 用于实时目标检测的深度轻量级神经网络*, arXiv:1608.08021'
- en: '*R-CNN: Rich feature hierarchies for accurate object detection and semantic
    segmentation*, CVPR, 2014.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R-CNN: 用于准确目标检测和语义分割的丰富特征层次结构*, CVPR, 2014.'
- en: '*SPP: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition*,
    ECCV, 2014.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SPP: 用于视觉识别的深度卷积网络中的空间金字塔池化*, ECCV, 2014.'
- en: '*Fast R-CNN*, arXiv:1504.08083.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fast R-CNN*, arXiv:1504.08083.'
- en: '*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*,
    arXiv:1506.01497.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Faster R-CNN: 使用区域提议网络实现实时目标检测*, arXiv:1506.01497.'
- en: '*R-CNN minus R*, arXiv:1506.06981.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R-CNN减去R*, arXiv:1506.06981.'
- en: '*End-to-end people detection in crowded scenes*, arXiv:1506.04878.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*拥挤场景中的端到端人物检测*, arXiv:1506.04878.'
- en: '*YOLO – You Only Look Once: Unified, Real-Time Object Detection*, arXiv:1506.02640'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLO – 你只看一次：统一的实时目标检测*, arXiv:1506.02640'
- en: '*Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent
    Neural Networks*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Inside-Outside Net: 使用跳跃池化和递归神经网络在上下文中检测物体*'
- en: '*Deep Residual Network: Deep Residual Learning for Image Recognition*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度残差网络：用于图像识别的深度残差学习*'
- en: '*R-FCN: Object Detection via Region-based Fully Convolutional Networks*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R-FCN: 基于区域的全卷积网络进行目标检测*'
- en: '*SSD: Single Shot MultiBox Detector*, arXiv:1512.02325'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SSD: 单次多框检测器*, arXiv:1512.02325'
- en: 'Also, following is the timeline of how the evolution of object detection has
    developed from 1999–2017:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，以下是从1999年到2017年目标检测发展的时间线：
- en: '![](img/53eba6c6-940b-430f-a304-a6e636fbc0fd.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53eba6c6-940b-430f-a304-a6e636fbc0fd.png)'
- en: 'Figure 9.1: The timeline of the evolution of object detection from 1999 to
    2017'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：1999到2017年目标检测发展时间线
- en: The files for this chapter can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09)找到。
- en: Improvements in object detection models
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测模型的改进
- en: Object detection and classification has been the subject of study for quite
    some time. The models that have been used build on the great success of previous
    researchers. A brief summary of progress history starts by highlighting the computer
    vision model called **Histogram of Oriented Gradients** (**HOG**) features that
    was developed by Navneet Dalal and Bill Triggs in 2005.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测和分类一直是研究的主题。使用的模型建立在前人研究的巨大成功基础上。简要回顾进展历史，从2005年Navneet Dalal和Bill Triggs开发的计算机视觉模型**方向梯度直方图**（**HOG**）特征开始。
- en: HOG features were fast and performed well. Interest in deep learning and the
    great success of CNNs that were more accurate classifiers due to their deep networks.
    But the problem was that the CNNs of the time were too slow in comparison.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: HOG特征速度快，表现良好。深度学习和CNN的巨大成功使其成为更精确的分类器，因为其深层网络。然而，当时CNN的速度相较之下过于缓慢。
- en: The solution was to take advantage of the CNNs, improved classification capabilities
    and improve their speed with a technique and employ a selective search paradigm
    in what became known as R-CNN. Reducing the number of bounding boxes did show
    improvements in speed, but not sufficiently for the expectations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是利用CNN的改进分类能力，并通过一种技术提高其速度，采用选择性搜索范式，形成了R-CNN。减少边界框的数量确实在速度上有所提升，但不足以满足预期。
- en: SPP-net was a proposed solution, wherein a CNN representation for the whole
    image was calculated and drove CNN-calculated representations for each sub-section
    generated by selective search. Selective search uses image features to generate
    all the possible locations for an object by looking at pixel intensity, color,
    image texture, and a measure of insideness. These identified objects are then
    fed into the CNN model for classification.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: SPP-net是一种提出的解决方案，其中计算整个图像的CNN表示，并驱动通过选择性搜索生成的每个子部分的CNN计算表示。选择性搜索通过观察像素强度、颜色、图像纹理和内部度量来生成所有可能的物体位置。然后，这些识别出的物体会被输入到CNN模型中进行分类。
- en: This, in turn, saw improvements in a model named Fast R-CNN that trained end-to-end,
    and thereby fixed the primary problems with SPP-net and R-CNN. Advancing this
    technology further with a model named Faster R-CNN, the technique of using small
    regional proposal CNNs in place of the selective search performed very well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这一改进催生了名为Fast R-CNN的模型，采用端到端训练，从而解决了SPP-net和R-CNN的主要问题。通过名为Faster R-CNN的模型进一步推进了这项技术，使用小型区域提议CNN代替选择性搜索表现得非常好。
- en: 'Here is a quick overview of the Faster R-CNN object detection pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Faster R-CNN物体检测管道的快速概述：
- en: '![](img/368acba3-00c2-434a-8654-fbc475c089d2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/368acba3-00c2-434a-8654-fbc475c089d2.png)'
- en: 'A quick benchmark comparison of the versions of R-CNN discussed previously
    shows the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对之前讨论的R-CNN版本进行的快速基准对比显示如下：
- en: '|  | R-CNN | Fast R-CNN | Faster R-CNN |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | R-CNN | Fast R-CNN | Faster R-CNN |'
- en: '| Average response time |  ~50 sec | ~2 sec | ~0.2 sec |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 平均响应时间 |  ~50秒 | ~2秒 | ~0.2秒 |'
- en: '| Speed boost | 1x | 25x | 250x |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 速度提升 | 1倍 | 25倍 | 250倍 |'
- en: The performance improvement is impressive, with Faster R-CNN being one of the
    most accurate and fastest object detection algorithms deployed in real-time use
    cases. Other recent powerful alternatives include YOLO models, which we will look
    into in detail later in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提升令人印象深刻，Faster R-CNN是目前在实时应用中最准确、最快的物体检测算法之一。其他近期强大的替代方法包括YOLO模型，我们将在本章后面详细探讨。
- en: Object detection using OpenCV
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV进行物体检测
- en: Let's start our project with a basic or traditional implementation of **Open
    Source Computer Vision** (**OpenCV**). This library is primarily targeted at real-time
    applications that need computer vision capabilities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**开源计算机视觉**（**OpenCV**）的基本或传统实现开始我们的项目。该库主要面向需要计算机视觉能力的实时应用。
- en: OpenCV has its API wrappers in various languages such as C, C++, Python, and
    so on, and the best way forward is to build a quick prototype using Python wrappers
    or any other language you are comfortable with, and once you are ready with your
    code, rewrite it in C/C++ for production.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV在C、C++、Python等多种语言中都有API封装，最佳的前进方式是使用Python封装器或任何你熟悉的语言来快速构建原型，一旦代码完成，可以在C/C++中重写以用于生产。
- en: In this chapter, we will be using the Python wrappers to create our initial
    object detection module.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python封装器创建我们的初始物体检测模块。
- en: So, let's do it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们开始吧。
- en: A handcrafted red object detector
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种手工制作的红色物体检测器
- en: In this section, we will learn how to create a feature extractor that will be
    able to detect any red object from the provided image using various image processing
    techniques such as erosion, dilation, blurring, and so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何创建一个特征提取器，能够使用各种图像处理技术（如腐蚀、膨胀、模糊等）从提供的图像中检测任何红色物体。
- en: Installing dependencies
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装依赖
- en: 'First, we need to install OpenCV, which we do with this simple `pip` command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装 OpenCV，我们通过这个简单的 `pip` 命令来完成：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we will import it along with other modules for visualizations and matrix
    operations:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们将导入它以及其他用于可视化和矩阵运算的模块：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Also, let''s define some helper functions that will help us to plot the images
    and the contours:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们定义一些帮助函数，帮助我们绘制图像和轮廓：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Exploring image data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索图像数据
- en: 'The first thing in any data science problem is to explore and understand the
    data. This helps us to make our objective clear. So, let''s first load the image
    and examine the properties of that image, such as the color spectrum and the dimensions:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据科学问题中，首先要做的就是探索和理解数据。这有助于我们明确目标。所以，让我们首先加载图像并检查图像的属性，比如色谱和尺寸：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Following is the output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/098e0712-7ea7-4ac1-90da-ed992e3e6110.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/098e0712-7ea7-4ac1-90da-ed992e3e6110.png)'
- en: 'Since the order of the image stored in the memory is **Blue Green Red** (**BGR**),
    we need to convert it into **Red Green Blue** (**RGB**):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像在内存中存储的顺序是**蓝绿红**（**BGR**），我们需要将其转换为**红绿蓝**（**RGB**）：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Following is the output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/ac13e819-06e9-48c4-bc4b-484e1a59adc1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac13e819-06e9-48c4-bc4b-484e1a59adc1.png)'
- en: 'Figure 9.2: The raw input image in RGB color format.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：RGB 色彩格式中的原始输入图像。
- en: Normalizing the image
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对图像进行归一化处理
- en: 'We will be scaling down the image dimensions, for which we will be using the
    `cv2.resize()` function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将缩小图像尺寸，为此我们将使用 `cv2.resize()` 函数：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we will perform a blur operation to make the pixels more normalized, for
    which we will be using the Gaussian kernel. Gaussian filters are very popular
    in the research field and are used for various operations, one of which is the
    blurring effect that reduces the noise and balances the image. The following code
    performs a blur operation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将执行模糊操作，使像素更加规范化，为此我们将使用高斯核。高斯滤波器在研究领域非常流行，常用于各种操作，其中之一是模糊效果，能够减少噪声并平衡图像。以下代码执行了模糊操作：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then we will convert the RGB-based image into an HSV color spectrum, which will
    help us to extract other characteristics of the image using color intensity, brightness,
    and shades:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将把基于 RGB 的图像转换为 HSV 色谱，这有助于我们使用颜色强度、亮度和阴影来提取图像的其他特征：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following is the output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/d00d3547-1d33-4c34-8ee6-1abfc475d0f4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d00d3547-1d33-4c34-8ee6-1abfc475d0f4.png)'
- en: 'Figure: 9.3: The raw input image in HSV color format.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图：9.3：HSV 色彩格式中的原始输入图像。
- en: Preparing a mask
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备掩膜
- en: 'We need to create a mask that can detect the specific color spectrum; let''s
    say red in our case. Now we will create two masks that will be performing feature
    extraction using the color values and the brightness factors:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个掩膜，可以检测特定的颜色谱；假设我们要检测红色。现在我们将创建两个掩膜，它们将使用颜色值和亮度因子进行特征提取：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Following is how our mask looks:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的掩膜效果：
- en: '![](img/3e454d53-c8fa-420c-bbd0-267d80f9645d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e454d53-c8fa-420c-bbd0-267d80f9645d.png)'
- en: Post-processing of a mask
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掩膜的后处理
- en: Once we are able to create our mask successfully, we need to perform some morphological
    operations, which are basic image processing operations used for the analysis
    and processing of geometrical structures.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们成功创建了掩膜，我们需要执行一些形态学操作，这是用于几何结构分析和处理的基本图像处理操作。
- en: 'First, we will create a kernel that will perform various morphological operations
    over the input image:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个内核，执行各种形态学操作，对输入图像进行处理：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Closing**: *Dilation followed by erosion* is helpful to close small pieces
    inside the foreground objects or small black points on the object.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**闭操作**：*膨胀后腐蚀* 对于关闭前景物体内部的小碎片或物体上的小黑点非常有帮助。'
- en: 'Now let''s perform the close operation over the mask:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对掩膜执行闭操作：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The opening operation *erosion followed by dilation* is used to remove noise.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 开操作 *腐蚀后膨胀* 用于去除噪声。
- en: 'Then we perform the opening operation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行开操作：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Following is the output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/42972971-f2af-4212-a1e9-c8e181bf2560.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42972971-f2af-4212-a1e9-c8e181bf2560.png)'
- en: 'Figure 9.4: This figure illustrated the output of morphological close and open
    operation (left side) and we combine the both to get the final processed mask(right
    side).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：此图展示了形态学闭运算和开运算的输出（左侧），我们将二者结合起来得到最终处理后的掩膜（右侧）。
- en: In the preceding screenshot you can see (in the left part of the screenshot)
    how the morphological operation changes the structure of the mask and when combining
    both the operations (in the right side of the screenshot) you get a denoised cleaner
    structure.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，你可以看到（截图的左侧）形态学操作如何改变掩膜的结构，并且当将两种操作结合时（截图的右侧），你会得到去噪后的更干净的结构。
- en: Applying a mask
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用掩膜
- en: 'It''s time to use the mask that we created to extract the object from the image.
    First, we will find the biggest contour using the helper function, which is the
    largest region of our object that we need to extract. Then apply the mask to the
    image and draw a circle bounding box on the extracted object:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用我们创建的掩膜从图像中提取物体了。首先，我们将使用辅助函数找到最大的轮廓，这是我们需要提取的物体的最大区域。然后将掩膜应用于图像，并在提取的物体上绘制一个圆形边界框：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Following is the output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/4e4e5617-5b0c-483e-97a5-244910c92c22.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e4e5617-5b0c-483e-97a5-244910c92c22.png)'
- en: 'Figure 9.5: This figure shows that we have detected the red region (car body)
    from the image and plotted an ellipes around it.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：此图展示了我们从图像中检测到红色区域（汽车车身），并在其周围绘制了一个椭圆。
- en: Voila! So, we successfully extracted the image and also drew the bounding box
    around the object using simple image processing techniques.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 啪！我们成功提取了图像，并使用简单的图像处理技术在物体周围绘制了边界框。
- en: Object detection using deep learning
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习进行物体检测
- en: In this section, we will learn how to build a world-class object detection module
    without much use of traditional handcrafting techniques. Here, will be using the
    deep learning approach, which is powerful enough to extract features automatically
    from the raw image and then use those features for classification and detection
    purposes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何构建一个世界级的物体检测模块，而不需要太多使用传统的手工技术。我们将使用深度学习方法，这种方法足够强大，可以自动从原始图像中提取特征，然后利用这些特征进行分类和检测。
- en: First, we will build an object detector using a pre-baked Python library that
    can use most of the state-of-the-art pre-trained models, and later on, we will
    learn how to implement a really fast and accurate object detector using YOLO architecture.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用一个预制的Python库构建一个物体检测器，该库可以使用大多数最先进的预训练模型，之后我们将学习如何使用YOLO架构实现一个既快速又准确的物体检测器。
- en: Quick implementation of object detection
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速实现物体检测
- en: Object detection saw an increase in adoption as a result of the industry trend
    towards deep learning after 2012\. Accurate and increasingly fast models such as
    R-CNN, Fast-RCNN, Faster-RCNN, and RetinaNet, and fast yet highly accurate ones
    like SSD and YOLO are in production today. In this section, we will use fully-functional
    pre-baked feature extractors in a Python library that can be used in just a few lines
    of code. Also, we will touch base regarding the production-grade setup for the
    same.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测在2012年后由于行业趋势向深度学习的转变而得到了广泛应用。准确且越来越快的模型，如R-CNN、Fast-RCNN、Faster-RCNN和RetinaNet，以及快速且高度准确的模型如SSD和YOLO，现在都在生产中使用。在本节中，我们将使用Python库中功能齐全的预制特征提取器，只需几行代码即可使用。此外，我们还将讨论生产级设置。
- en: So, let's do it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，开始吧。
- en: Installing all the dependencies
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装所有依赖项
- en: 'This is the same drill that we performed in the previous chapters. First let''s
    install all the dependencies. Here, we are using a Python module called ImageAI
    ([https://github.com/OlafenwaMoses/ImageAI](https://github.com/OlafenwaMoses/ImageAI)),
    which is an effective way to start building your own object detection application
    from scratch in no time:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在前几章中执行的步骤相同。首先，让我们安装所有依赖项。在这里，我们使用一个名为ImageAI的Python模块（[https://github.com/OlafenwaMoses/ImageAI](https://github.com/OlafenwaMoses/ImageAI)），它是一个有效的方法，可以帮助你快速构建自己的物体检测应用程序：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We will be using the Python 3.x environment to run this module.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python 3.x环境来运行这个模块。
- en: 'For this implementation, we are going to use a pre-trained ResNet model that
    is trained on the COCO dataset ([http://cocodataset.org/#home](http://cocodataset.org/#home)) (a
    large-scale object detection, segmentation, and captioning dataset). You can also
    use other pre-trained models such as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此实现，我们将使用一个在COCO数据集上训练的预训练ResNet模型（[http://cocodataset.org/#home](http://cocodataset.org/#home)）（一个大规模的目标检测、分割和描述数据集）。你也可以使用其他预训练模型，如下所示：
- en: '`DenseNet-BC-121-32.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5)) (31.7
    MB)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DenseNet-BC-121-32.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5))
    (31.7 MB)'
- en: '`inception_v3_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5)) (91.7
    MB)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inception_v3_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5))
    (91.7 MB)'
- en: '`resnet50_coco_best_v2.0.1.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5)) (146
    MB)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resnet50_coco_best_v2.0.1.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5))
    (146 MB)'
- en: '`resnet50_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5))
    (98.1 MB)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resnet50_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5))
    (98.1 MB)'
- en: '`squeezenet_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5))
    (4.83 MB)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`squeezenet_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5))
    (4.83 MB)'
- en: '`yolo-tiny.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5))
    (33.9 MB)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yolo-tiny.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5))
    (33.9 MB)'
- en: '`yolo.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5)):
    237 MB'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yolo.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5)):
    237 MB'
- en: 'To get the dataset, use the following command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据集，请使用以下命令：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Implementation
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'Now that we have all the dependencies and pre-trained models ready, we will
    implement a state-of-the-art object detection model. We will import the ImageAI''s`ObjectDetection`
    class using the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好所有的依赖项和预训练模型，我们将实现一个最先进的目标检测模型。我们将使用以下代码导入ImageAI的`ObjectDetection`类：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then we create the instance for the `ObjectDetection` object and set the model
    type as `RetinaNet()`. Next, we set the part of the ResNet model that we downloaded
    and call the `loadModel()` function:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建`ObjectDetection`对象的实例，并将模型类型设置为`RetinaNet()`。接下来，我们设置下载的ResNet模型部分，并调用`loadModel()`函数：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once the model is loaded into the memory, we can feed a new image to the model,
    which can be of any popular image format, such as JPEG, PNG, and so on. Also,
    the function has no constraint on the size of the image, so, you can use any dimensional
    data and the model will handle it internally. We are using `detectObjectsFromImage()`
    to feed the input image. This method returns the image with some more information
    such as the bounding box coordinates of the detected object, the label of the
    detected object, and the confidence score.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被加载到内存中，我们就可以将新图像输入模型，图像可以是任何常见的图像格式，如JPEG、PNG等。此外，函数对图像的大小没有限制，因此你可以使用任何维度的数据，模型会在内部处理它。我们使用`detectObjectsFromImage()`来输入图像。此方法返回带有更多信息的图像，例如检测到的对象的边界框坐标、检测到的对象的标签以及置信度分数。
- en: 'Following are some images that are used as input into the model and to perform
    the object detection:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些用作输入模型并执行目标检测的图像：
- en: '![](img/f81fd00b-6c4a-4762-bb4d-db472d850275.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f81fd00b-6c4a-4762-bb4d-db472d850275.png)'
- en: 'Figure 9.6: Since I was traveling to Asia (Malaysia/Langkawi) while writing
    this chapter, I decided to give it a shot and use some real images that I captured
    on the go.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：由于在写这章时我正在去亚洲（马来西亚/兰卡威）旅行，我决定尝试使用一些我在旅行中拍摄的真实图像。
- en: 'The following code is used for inputting images into the model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于将图像输入到模型中：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Further, we iterate over the `object_detection` object to read all the objects
    that the model predicted with the respective confidence score:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们迭代`object_detection`对象，以读取模型预测的所有物体及其相应的置信度分数：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Following are how the results look:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果的展示方式：
- en: '![](img/ed6f0e48-301f-4510-87c2-034ed0882f5e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed6f0e48-301f-4510-87c2-034ed0882f5e.png)'
- en: '![](img/f0a2c2e0-4b28-4521-8feb-b59dc0bae9bd.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0a2c2e0-4b28-4521-8feb-b59dc0bae9bd.png)'
- en: '![](img/875aae8e-22f7-4858-9fdd-2728bee7c7a5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/875aae8e-22f7-4858-9fdd-2728bee7c7a5.png)'
- en: 'Figure 9.7: The results extracted from the object detection model with the
    bounding box around the detected object. Results contain the name of the object
    and the confidence score.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：从目标检测模型中提取的结果，图中包含了检测到的物体周围的边界框。结果包含物体的名称和置信度分数。
- en: So, we can see that the pre-trained models performed well enough with very few
    lines of code.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以看到，预训练模型表现得非常好，只用了很少的代码行。
- en: Deployment
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Now that we have all base code ready, let's deploy the `ObjectDetection` modules
    into production. In this section, we will write a RESTful service that will accept
    the image as an input and returns the detected object as a response.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好所有基本代码，让我们将`ObjectDetection`模块部署到生产环境中。在本节中，我们将编写一个RESTful服务，它将接受图像作为输入，并返回检测到的物体作为响应。
- en: 'We will define a `POST` function that accepts the image files with the PNG, JPG, JPEG, and
    GIF extensions. The uploaded image path is sent to the `ObjectDetection` module,
    which performs the detection and returns the following JSON results:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个`POST`函数，它接受带有PNG、JPG、JPEG和GIF扩展名的图像文件。上传的图像路径将传递给`ObjectDetection`模块，后者执行检测并返回以下JSON结果：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Save the file as `object_detection_ImageAI.py` and execute the following command
    to run the web services:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件保存为`object_detection_ImageAI.py`并执行以下命令来运行Web服务：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Following is the output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/71725c7a-3fab-4516-8cd7-1dd49fe574d0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71725c7a-3fab-4516-8cd7-1dd49fe574d0.png)'
- en: 'Figure 9.8: Output on the Terminal screen after successful execution of the
    web service.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：成功执行Web服务后的终端屏幕输出。
- en: 'In a separate Terminal, you can now try to call the API, as shown in the following
    command:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个终端中，你现在可以尝试调用API，如下所示的命令：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Following will be the response output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是响应输出：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, this was awesome; with just a few hours' work, you are ready with a production-grade
    object detection module that is something close to state-of-the-art.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这真是太棒了；仅用了几个小时的工作，你就准备好了一个接近最先进技术的生产级目标检测模块。
- en: Object Detection In Real-Time Using YOLOv2
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YOLOv2进行实时目标检测
- en: A great advancement in object detection and classification was made possible
    with a process where You Only Look Once (YOLO) at an input image. In this single
    pass, the goal is to set the coordinates for the corners of the bounding box to
    be drawn around the detected object and to then classify the object with a regression
    model. This process is capable of avoiding false positives because it takes into
    account contextual information from the whole image, and not just a smaller section
    as in a regional proposal of earlier described methods. The **convolutional neural
    network** (**CNN**) as follows can pass over the image once, and therefore be
    fast enough to function in applications where real-time processing is a requirement.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测和分类的重大进展得益于一个过程，即你只需对输入图像进行一次查看（You Only Look Once，简称YOLO）。在这一单次处理过程中，目标是设置边界框的角坐标，以便绘制在检测到的物体周围，并使用回归模型对物体进行分类。这个过程能够避免误报，因为它考虑了整个图像的上下文信息，而不仅仅是像早期描述的区域提议方法那样的较小区域。如下所示的**卷积神经网络**（**CNN**）可以一次性扫描图像，因此足够快，能够在实时处理要求的应用中运行。
- en: YOLOv2 predicts an N number of bounding boxes and associates a confidence level
    for the classification of the object for each individual grid in an S-by-S grid
    that is established in the immediately preceding step.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv2 在每个单独的网格中预测 N 个边界框，并为每个网格中的对象分类关联一个置信度级别，该网格是在前一步骤中建立的 S×S 网格。
- en: '![](img/294ebbd4-3a6f-4fbf-ad74-66368c53fa24.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/294ebbd4-3a6f-4fbf-ad74-66368c53fa24.png)'
- en: 'Figure 9.9: The overview of how YOLO works. The input image is divided into
    grids and then been sent into the detection process which results in lots of bounding
    boxes which is further been filtered by applying some thresholds.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：YOLO 工作原理概述。输入图像被划分为网格，然后被送入检测过程，结果是大量的边界框，这些框通过应用一些阈值进一步过滤。
- en: The outcome of this process is to produce a total of S-by-S by N complement
    of boxes. For a great percentage of these boxes you’ll get confidence scores that
    are quite low, and by applying a lower threshold (30% in this case), you can eliminate
    a majority of inaccurately classified objects as shown in the figure.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的结果是生成 S×S×N 个补充框。对于这些框的很大一部分，你会得到相当低的置信度分数，通过应用一个较低的阈值（在本例中为 30%），你可以消除大多数被错误分类的对象，如图所示。
- en: We will be using a pre-trained YOLOv2 model in this section for object detection
    and classification.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个预训练的 YOLOv2 模型进行目标检测和分类。
- en: Preparing the dataset
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: In this part, we will explore the data preparation using the existing the COCO
    dataset and a custom dataset. If you want to train the YOLO model with lots of
    classes, then you can follow the instructions provided in the pre-existing part,
    or else if you want to build your custom object detector, then follow the instructions
    provided in the custom build section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探索如何使用现有的 COCO 数据集和自定义数据集进行数据准备。如果你想用很多类别来训练 YOLO 模型，你可以按照已有部分提供的指示操作，或者如果你想建立自己的自定义目标检测器，跟随自定义构建部分的说明。
- en: Using the pre-existing COCO dataset
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预先存在的 COCO 数据集
- en: 'For this implementation, we will be using the COCO dataset. This is a great
    resource dataset for training YOLOv2 to detect, segment, and caption images on
    a large scale. Download the dataset from [http://cocodataset.org](http://cocodataset.org)
    and run the following command in the terminal:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中，我们将使用 COCO 数据集。这是一个用于训练 YOLOv2 以进行大规模图像检测、分割和标注的优质数据集资源。请从 [http://cocodataset.org](http://cocodataset.org)
    下载数据集，并在终端中运行以下命令：
- en: 'Get the training dataset:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练数据集：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Get the validation dataset:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取验证数据集：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Get the train and validation annotations:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练和验证的标注：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let''s convert the annotations in the COCO format to VOC format:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 COCO 格式的标注转换为 VOC 格式：
- en: 'Install Baker:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Baker：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create the folders to store the images and annotations:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件夹以存储图像和标注：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Unzip `train2014.zip` and `val2014.zip` under the `images` folder:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `images` 文件夹下解压 `train2014.zip` 和 `val2014.zip`：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Unzip `annotations_trainval2014.zip` into `annotations` folder:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `annotations_trainval2014.zip` 解压到 `annotations` 文件夹：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create a folder to store the converted data:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文件夹来存储转换后的数据：
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This is how the folder structure will look after the final transformation:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最终转换后的文件夹结构如下所示：
- en: '![](img/7be97663-700b-4520-8000-42130ae0f9b6.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7be97663-700b-4520-8000-42130ae0f9b6.png)'
- en: 'Figure 9.10: The illustration of the COCO data extraction and formatting process'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：COCO 数据提取和格式化过程示意图
- en: This establishes a perfect correspondence between the image and the annotation.
    When the validation set is empty, we will use a ratio of eight to automatically
    split the training and validation sets.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这建立了图像和标注之间的完美对应关系。当验证集为空时，我们将使用 8:2 的比例自动拆分训练集和验证集。
- en: The result is that we will have two folders, `./images` and `./annotation`,
    for the training purpose.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是我们将有两个文件夹，`./images` 和 `./annotation`，用于训练目的。
- en: Using the custom dataset
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义数据集
- en: Now, if you want to build an object detector for your specific use case, then
    you will need to scrape around 100–200 images from the web and annotate them.
    There are lots of annotation tools available online, such as LabelImg ([https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg))
    or **Fast Image Data Annotation Tool** (**FIAT**) ([https://github.com/christopher5106/FastAnnotationTool](https://github.com/christopher5106/FastAnnotationTool)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想为你的特定应用场景构建一个目标检测器，那么你需要从网上抓取大约100到200张图像并进行标注。网上有很多标注工具可供使用，比如LabelImg
    ([https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg))
    或 **Fast Image Data Annotation Tool** (**FIAT**) ([https://github.com/christopher5106/FastAnnotationTool](https://github.com/christopher5106/FastAnnotationTool))。
- en: For you to play around with the custom object detector, we have provided some
    sample images with respective annotations. Look into the repository folder called `Chapter0
    9/yolo/new_class/`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你更好地使用自定义目标检测器，我们提供了一些带有相应标注的示例图像。请查看名为 `Chapter09/yolo/new_class/` 的代码库文件夹。
- en: 'Each image has its respective annotations, as shown in the following picture:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都有相应的标注，如下图所示：
- en: '![](img/930a677f-c2de-4451-9dff-262b62d6ffbc.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/930a677f-c2de-4451-9dff-262b62d6ffbc.png)'
- en: 'Figure 9.11: The relation between the image and the annotation which is shown
    here'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：这里显示的是图像与标注之间的关系
- en: 'Also, let''s download the pre-trained weights from [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/),
    which we will use to initialize our model, and which will train the custom object
    detector on top of these pretrained weights:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要从 [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)
    下载预训练权重文件， 我们将用它来初始化模型，并在这些预训练权重的基础上训练自定义目标检测器：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Installing all the dependencies
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装所有依赖：
- en: 'We will be using the Keras APIs with a TensorFlow approach to create the YOLOv2
    architecture. Let''s import all the dependencies:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras API结合TensorFlow方法来创建YOLOv2架构。让我们导入所有依赖：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Following is the code for this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相关的代码：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It is always recommended to use GPUs to train any YOLO models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 总是建议使用GPU来训练任何YOLO模型。
- en: Configuring the YOLO model
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置YOLO模型
- en: 'YOLO models are designed with the set of hyperparameter and some other configuration.
    This configuration defines the type of model to construct, as well as other parameters
    of the model such as the input image size and the list of anchors. You have two
    options at the moment: tiny YOLO and full YOLO. The following code defines the
    type of model to construct:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO模型是通过一组超参数和其他配置来设计的。这个配置定义了构建模型的类型，以及模型的其他参数，如输入图像的大小和锚点列表。目前你有两个选择：tiny
    YOLO 和 full YOLO。以下代码定义了要构建的模型类型：
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Configure the path of the pre-trained model and the images, as in the following
    code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 配置预训练模型和图像的路径，如以下代码所示：
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Defining the YOLO v2 model
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义YOLO v2模型
- en: 'Now let''s have a look at the model architecture of the YOLOv2 model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看YOLOv2模型的架构：
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The network architecture that we just created can be found here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的网络架构可以在这里找到：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png)
- en: 'Following is the output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Training the model
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Following are the steps to train the model:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练模型的步骤：
- en: 'Load the weights that we downloaded and use them to initialize the model:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们下载的权重并用它们初始化模型：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Randomize the weights of the last layer:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化最后一层的权重：
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Generate the configurations as in the following code:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成如下代码中的配置：
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a training and validation batch:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和验证批次：
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Set early stop and checkpoint callbacks:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置早停和检查点回调：
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Use the following code to train the model:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码来训练模型：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Following is the output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Following is the TensorBoard plots output for just two epochs:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是仅两个epoch的TensorBoard输出：
- en: '![](img/3263e010-3d23-4ae6-8524-a956ff552158.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3263e010-3d23-4ae6-8524-a956ff552158.png)'
- en: 'Figure 9.12: The figure represents the loss plots for 2 epochs'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：该图表示2个epoch的损失曲线
- en: Evaluating the model
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Once the training is complete, let''s perform the prediction by feeding an
    input image into the model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，让我们通过将输入图像馈送到模型中来执行预测：
- en: 'First we will load the model into the memory:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将模型加载到内存中：
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now set the test image path and read it:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在设置测试图像路径并读取它：
- en: '[PRE46]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Normalize the image:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行归一化：
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Make a prediction:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 做出预测：
- en: '[PRE48]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'So, here are some of the results:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些结果：
- en: '![](img/b84c805d-2900-4d56-94f2-46ec024be573.png)![](img/1798f426-6f94-43c4-a20f-ec39fcf72364.png)![](img/9a416a09-dfae-4b5a-a7f6-8b5b2b55c087.png)![](img/cb951cf2-9574-4e15-b96c-2f5612422785.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b84c805d-2900-4d56-94f2-46ec024be573.png)![](img/1798f426-6f94-43c4-a20f-ec39fcf72364.png)![](img/9a416a09-dfae-4b5a-a7f6-8b5b2b55c087.png)![](img/cb951cf2-9574-4e15-b96c-2f5612422785.png)'
- en: Congratulations—you have developed a state-of-the-art object detector that is
    very fast and reliable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你已经开发出了一个非常快速且可靠的最先进物体检测器。
- en: We learned about building a world class object detection model using YOLO architecture
    and the results seems to be very promising. Now you can also deploy the same on
    other mobile devices or Raspberry Pi.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何使用YOLO架构构建一个世界级的物体检测模型，结果看起来非常有前景。现在，你也可以将相同的模型部署到其他移动设备或树莓派上。
- en: Image segmentation
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割
- en: Image segmentation is the process of categorizing what is in a picture at a
    pixel level. For example, if you were given a picture with a person in it, separating
    the person from the image is known as segmentation and is done using pixel-level
    information.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是将图像中的内容按像素级别进行分类的过程。例如，如果你给定一张包含人的图片，将人从图像中分离出来就是图像分割，并且是通过像素级别的信息来完成的。
- en: We will be using the COCO dataset for image segmentation.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用COCO数据集进行图像分割。
- en: 'Following is what you should do before executing any of the SegNet scripts:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行任何SegNet脚本之前，你需要做以下工作：
- en: '[PRE49]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: When executing SegNet scripts, make sure that your present working directory
    is `SegNet`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行SegNet脚本时，确保你的当前工作目录是`SegNet`。
- en: Importing all the dependencies
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所有依赖项
- en: Make sure to restart the session before proceeding forward.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，确保重新启动会话。
- en: 'We will be using `numpy`, `pandas`, `keras`, `pylab`, `skimage`, `matplotlib`,
    and `pycocotools`, as in the following code:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`numpy`、`pandas`、`keras`、`pylab`、`skimage`、`matplotlib`和`pycocotools`，如以下代码所示：
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Exploring the data
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'We will start off by defining the location of the annotation file we will be
    using for image segmentation, and then we will initialize the COCO API:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义用于图像分割的注释文件的位置，然后初始化COCO API：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Following should be the output:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是应该得到的输出：
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Images
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像
- en: 'Since we are building a binary segmentation model, let us consider the images
    from the `images/train2014` folder that are only tagged with the person label
    so that we can segment the person out of the image. The COCO API provides us with
    easy-to-use methods, two of which are the `getCatIds` and `getImgIds`. The following
    snippet will help us extract the image IDs of all the images with the label `person` tagged
    to it:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在构建一个二进制分割模型，让我们考虑从`images/train2014`文件夹中只标记为“person”标签的图像，以便将图像中的人分割出来。COCO
    API为我们提供了易于使用的方法，其中两个常用的方法是`getCatIds`和`getImgIds`。以下代码片段将帮助我们提取所有带有`person`标签的图像ID：
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This should be the output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是输出结果：
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now let us use the following snippet to plot an image:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用以下代码片段来绘制图像：
- en: '[PRE55]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Following should be the output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是应该得到的输出：
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We get the following picture as an output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出图像：
- en: '![](img/121f058f-bee6-4231-9169-85637d6d25df.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/121f058f-bee6-4231-9169-85637d6d25df.png)'
- en: 'Figure 9.13: The plot representation a sample image from the dataset.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：数据集中样本图像的绘制表示。
- en: In the previous code snippet, we feed in an image ID to the `loadImgs` method
    of COCO to extract the details of the image it corresponds to. If you look at
    the output of the `img` variable, one of the keys listed is the `file_name` key.
    This key holds the name of the image located in the `images/train2014/` folder.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们将一个图像ID传入COCO的`loadImgs`方法，以提取与该图像对应的详细信息。如果你查看`img`变量的输出，列出的一项键是`file_name`键。这个键包含了位于`images/train2014/`文件夹中的图像名称。
- en: Then we read the image using the `imread` method of the `io` module we have
    already imported and plot it using `matplotlib.pyplot`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用已导入的`io`模块的`imread`方法读取图像，并使用`matplotlib.pyplot`进行绘制。
- en: Annotations
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注释
- en: 'Now let us load the annotation corresponding to the previous picture and plot
    the annotation on top of the picture. The `coco.getAnnIds()` function helps load
    the annotation info of an image using its ID. Then, with the help of the `coco.loadAnns()`
    function, we load the annotations and plot it using the `coco.showAnns()` function.
    It is important that you first plot the image and then perform the annotation
    operations as shown in the following code snippet:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载与之前图片对应的标注，并在图片上绘制该标注。`coco.getAnnIds()`函数帮助我们通过图像ID加载标注信息。然后，借助`coco.loadAnns()`函数，我们加载标注并通过`coco.showAnns()`函数绘制出来。重要的是，你要先绘制图像，再进行标注操作，代码片段如下所示：
- en: '[PRE57]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Following should be the output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应为输出：
- en: '![](img/2953e1ec-c34f-4069-a3f0-92297a412b5f.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2953e1ec-c34f-4069-a3f0-92297a412b5f.png)'
- en: 'Figure 9.14: Visualizing annotation on an image'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：在图像上可视化标注
- en: 'To be able to obtain the annotation label array, use the `coco.annToMask()`
    function as shown in the following code snippet. This array will help us form
    the segmentation target:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够获取标注标签数组，使用`coco.annToMask()`函数，如以下代码片段所示。该数组将帮助我们形成分割目标：
- en: '[PRE58]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Following should be the output:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应为输出：
- en: '![](img/16120870-43e2-45da-8eea-aa4a4388ac25.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16120870-43e2-45da-8eea-aa4a4388ac25.png)'
- en: 'Figure 9.15: Visualizing just the annotation'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：仅可视化标注
- en: Preparing the data
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Let us now define a `data_list()` function that will automate the process of
    loading an image and its segmentation array into memory and resize them to the
    shape of 360*480 using OpenCV. This function returns two lists containing images
    and segmentation array:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一个`data_list()`函数，它将自动化加载图像及其分割数组到内存，并使用OpenCV将它们调整为360*480的形状。此函数返回两个列表，其中包含图像和分割数组：
- en: '[PRE59]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Following should be the output:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应为输出：
- en: '[PRE60]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Normalizing the image
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像归一化
- en: 'First, let''s define the `make_normalize()` function, which accepts an image
    and performs the histogram normalization operation on it. The return object is
    a normalized array:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义`make_normalize()`函数，它接受一张图像并对其进行直方图归一化操作。返回的对象是一个归一化后的数组：
- en: '[PRE61]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Following should be the output:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应为输出：
- en: '![](img/fbc0136c-88e6-40b6-9fae-5f3968f90ac7.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbc0136c-88e6-40b6-9fae-5f3968f90ac7.png)'
- en: 'Figure 9.16: Before and After histogram normalization on an image'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：图像直方图归一化前后对比
- en: In the preceding screenshot, we see the original picture on the left, which
    is very visible, and on the right we see the normalized picture, which is not
    at all visible.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们看到左边是原始图片，非常清晰，而右边是归一化后的图片，几乎看不见。
- en: Encoding
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码
- en: 'With the `make_normalize()` function defined, let''s now define a `make_target`
    function. This function accepts the segmentation array of shape (360,480) and
    then returns a segmentation target of shape (`360`,`480`,`2`). In the target,
    channel `0` represents the background and will have `1` in locations that represent
    the background in the image and zero elsewhere. Channel `1` represents the person
    and will have `1` in locations that represent the person in the image and `0` elsewhere. The
    following code implements the function:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`make_normalize()`函数后，我们现在定义一个`make_target`函数。该函数接受形状为(360, 480)的分割数组，然后返回形状为(`360`,`480`,`2`)的分割目标。在目标中，通道`0`表示背景，并且在图像中代表背景的位置为`1`，其他位置为零。通道`1`表示人物，并且在图像中代表人物的位置为`1`，其他位置为`0`。以下代码实现了该函数：
- en: '[PRE62]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Following should be the output:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应为输出：
- en: '![](img/0cc9dfd9-3ec8-42d1-a7b1-ac13c6fc0845.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cc9dfd9-3ec8-42d1-a7b1-ac13c6fc0845.png)'
- en: 'Figure 9.17: Visualizing the encoded target arrays'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17：可视化编码后的目标数组
- en: Model data
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型数据
- en: We will now define a function called `model_data()` that accepts a list of images
    and a list of labels. This function will apply the `make_normalize()` function on
    each image for the purpose of normalizing, and it will apply the `make_encode()` function
    on each label/segmentation array to obtain the encoded array.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义一个名为`model_data()`的函数，它接受图像列表和标签列表。该函数将对每个图像应用`make_normalize()`函数以进行归一化，并对每个标签/分割数组应用`make_encode()`函数以获得编码后的数组。
- en: 'The return of this function is two lists, one containing the normalized images
    and the other containing the corresponding target arrays:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回两个列表，一个包含归一化后的图像，另一个包含对应的目标数组：
- en: '[PRE63]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: In the preceding snippet, we have also split the data into train, test, and
    validation sets, with the train set containing `1500` data points, the validation
    set containing `400` data points, and the test set containing `97` data points.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们还将数据划分为训练集、测试集和验证集，其中训练集包含`1500`个数据点，验证集包含`400`个数据点，测试集包含`97`个数据点。
- en: Defining hyperparameters
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'The following are some of the defined hyperparameters that we will be using
    throughout the code, and they are totally configurable:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些我们将在整个代码中使用的定义超参数，它们是完全可配置的：
- en: '[PRE64]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: To learn more about `optimizers` and their APIs in Keras, visit [https://keras.io/optimizers/](https://keras.io/optimizers/).
    Reduce `batch_size` if you get a resource exhaustion error with respect to the
    GPU.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`optimizers`及其在Keras中的API，请访问[https://keras.io/optimizers/](https://keras.io/optimizers/)。如果遇到关于GPU的资源耗尽错误，请减少`batch_size`。
- en: Experiment with different learning rates, `optimizers`, and `batch_size` to
    see how these factors affect the quality of your model, and if you get better
    results, show them to the deep learning community.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率、`optimizers`和`batch_size`，看看这些因素如何影响模型的质量，如果得到更好的结果，可以与深度学习社区分享。
- en: Define SegNet
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义SegNet
- en: 'For the purpose of image segmentation, we will build a SegNet model, which
    is very similar to the autoencoder we built in [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml): *Handwritten
    Digits Classification Using ConvNets*, as shown:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行图像分割，我们将构建一个SegNet模型，它与我们在[第8章](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml)中构建的自编码器非常相似：*使用卷积神经网络进行手写数字分类*，如图所示：
- en: '![](img/695f6e37-8388-4a91-9d86-fffda1adfe5d.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/695f6e37-8388-4a91-9d86-fffda1adfe5d.png)'
- en: 'Figure 9.18: SegNet architecture used in this chapter'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18：本章使用的SegNet架构
- en: 'The SegNet model we''ll define will accept (*3,360, 480*) images as input with
    (*172800, 2*) segmentation arrays as the targets, and it will have the following
    characteristics in the encoder:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义的SegNet模型将接受(*3,360, 480*)的图像作为输入，目标是(*172800, 2*)的分割数组，并且在编码器中将具有以下特点：
- en: The first layer is a Convolution 2D layer with 64 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with MaxPooling2D of size 2*2.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个具有64个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化，然后是使用大小为2*2的MaxPooling2D进行下采样。
- en: The second layer is a Convolution 2D layer with 128 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with MaxPooling2D of size 2*2.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个具有128个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化，然后是使用大小为2*2的MaxPooling2D进行下采样。
- en: The third layer is a Convolution 2D layer with 256 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with MaxPooling2D of size 2*2.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层是一个具有256个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化，然后是使用大小为2*2的MaxPooling2D进行下采样。
- en: The fourth layer is again a Convolution 2D layer with 512 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四层再次是一个具有512个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化。
- en: 'And the model will have the following characteristics in the decoder:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在解码器中将具有以下特点：
- en: The first layer is a Convolution 2D layer with 512 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with UpSampling2D of size 2*2.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个具有512个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化，然后是使用大小为2*2的UpSampling2D进行下采样。
- en: The second layer is a Convolution 2D layer with 256 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with UpSampling2D of size 2*2.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个具有256个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化，然后是使用大小为2*2的UpSampling2D进行下采样。
- en: The third layer is a Convolution 2D layer with 128 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with UpSampling2D of size 2*2.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层是一个具有128个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化，然后是使用大小为2*2的UpSampling2D进行下采样。
- en: The fourth layer is a Convolution 2D layer with 64 filters of size 3*3 with
    `activation` as `relu`, followed by batch normalization.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四层是一个具有64个3*3大小滤波器的二维卷积层，`activation`为`relu`，接着是批量归一化。
- en: The fifth layer is a Convolution 2D layer with 2 filters of size 1*1, followed
    by Reshape, Permute and a `softmax` as `activation` layer for predicting scores.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五层是一个大小为1*1的2个卷积2D层，接着是Reshape、Permute和一个`softmax`激活层，用于预测得分。
- en: 'The model is described with the following code:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码描述模型：
- en: '[PRE65]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Compiling the model
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'With the model defined, compile the model with ''`categorical_crossentropy`''
    as `loss` and `optimizer` as `Adam`, as defined by the `optimizer` variable in
    the hyperparameters section. We will also define `ReduceLROnPlateau` to reduce
    the learning rate as needed when training, as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型定义完成后，使用`categorical_crossentropy`作为`loss`，`Adam`作为`optimizer`来编译模型，这由超参数部分中的`optimizer`变量定义。我们还将定义`ReduceLROnPlateau`，以便在训练过程中根据需要减少学习率，如下所示：
- en: '[PRE66]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Fitting the model
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'With the model compiled, we will now fit the model on the data using the `fit`
    method of the model. Here, since we are training on a small set of data, it is
    important to set the parameter shuffle to `True` so that the images are shuffled
    after each epoch:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 模型编译完成后，我们将使用模型的`fit`方法在数据上拟合模型。在这里，由于我们在一个小的数据集上进行训练，重要的是将参数shuffle设置为`True`，以便在每个epoch后对图像进行打乱：
- en: '[PRE67]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This should be the output:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这应为输出结果：
- en: '![](img/28aa5d4e-2914-4b58-9c56-19f9ff8f4da2.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28aa5d4e-2914-4b58-9c56-19f9ff8f4da2.png)'
- en: 'Figure 9.19: Training output'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19：训练输出
- en: 'The following shows the accuracy and loss plots:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了准确率和损失曲线：
- en: '![](img/2ed63cdf-9267-4845-9e39-e472d5301333.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ed63cdf-9267-4845-9e39-e472d5301333.png)'
- en: 'Figure 9.20: Plot showing training progression'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20：展示训练进展的曲线
- en: Testing the model
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'With the model trained, evaluate the model on test data, as in the following:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好模型后，在测试数据上评估模型，如下所示：
- en: '[PRE68]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This should be the output:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这应为输出结果：
- en: '[PRE69]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We see that the SegNet model we built has a loss of 0.539 and accuracy of 76.33
    on test images.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，我们构建的SegNet模型在测试图像上损失为0.539，准确率为76.33。
- en: 'Let''s plot the test images and their corresponding generated segmentations
    to understand model learning:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制测试图像及其相应生成的分割结果，以便理解模型的学习情况：
- en: '[PRE70]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Following should be the output:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应为输出结果：
- en: '![](img/24987ab2-aacf-4f3d-8d4f-d94bc4787c61.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24987ab2-aacf-4f3d-8d4f-d94bc4787c61.png)'
- en: 'Figure 9.21: Segmentation generated on test images'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21：在测试图像上生成的分割结果
- en: From the preceding figure, we see that the model was able to segment the person
    from the image.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到模型能够从图像中分割出人物。
- en: Conclusion
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The first part of the project was to build an object detection classifier using
    YOLO architecture in Keras.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的第一部分是使用Keras中的YOLO架构构建一个目标检测分类器。
- en: The second part of the project was to build a binary image segmentation model
    on COCO images that contain just a person, aside from the background. The goal
    was to build a good enough model to segment out the person from the background
    in the image.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的第二部分是建立一个二进制图像分割模型，针对的是包含人物和背景的COCO图像。目标是建立一个足够好的模型，将图像中的人物从背景中分割出来。
- en: The model we build by training on 1500 images, each of shape 360*480*3, has
    an accuracy of 79% on train data, and 78% on validation and test data. The model
    is successfully able to segment the person in the image, but the borders of the
    segmentations are slightly off from where they should be. This is due to using
    a small training set. Considering the number of images used for training, the
    model did a good job of segmenting.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在1500张形状为360*480*3的图像上进行训练构建的模型，在训练数据上的准确率为79%，在验证和测试数据上的准确率为78%。该模型能够成功地分割出图像中的人物，但分割的边缘略微偏离应有的位置。这是由于使用了一个较小的训练集。考虑到训练所用的图像数量，模型在分割上做得还是很不错的。
- en: There are more images available in this dataset that can be used for training,
    and it might take over a day to train on all of them using a Nvidia Tesla K80
    GPU, but doing so will give you really good segmentation.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中还有更多可用于训练的图像，虽然使用Nvidia Tesla K80 GPU训练所有图像可能需要一天以上的时间，但这样做将能够获得非常好的分割效果。
- en: Summary
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In the first part of this chapter, we learnt how to build a RESTful service
    for object detection using an existing classifier, and we also learned to build
    an accurate object detector using the YOLO architecture object detection classifier using Keras,
    while also implementing transfer learning. In the second part of the chapter,
    we understood what image segmentation is and built an image segmentation model
    on images from the COCO dataset. We also tested the performance of the object
    detector and the image segmenter on test data, and determined that we succeeded
    in achieving the goal.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们学习了如何使用现有的分类器构建一个RESTful服务来进行目标检测，并且我们还学习了如何使用YOLO架构的目标检测分类器和Keras构建一个准确的目标检测器，同时实现了迁移学习。在本章的第二部分，我们了解了图像分割是什么，并在COCO数据集的图像上构建了一个图像分割模型。我们还在测试数据上测试了目标检测器和图像分割器的性能，并确定我们成功达成了目标。
