- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Keras
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: In this chapter, we will focus on the high-level TensorFlow API named Keras.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍名为Keras的高层TensorFlow API。
- en: 'By the end of this chapter, you should have a better understanding of:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您应该对以下内容有更好的理解：
- en: The Keras Sequential API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras顺序API
- en: The Keras Functional API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras功能API
- en: The Keras Subclassing API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras子类化API
- en: The Keras Preprocessing API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras预处理API
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we covered TensorFlow's fundamentals, and we are now
    able to set up a computational graph. This chapter will introduce Keras, a high-level
    neural network API written in Python with multiple backends. TensorFlow is one
    of them. François Chollet, a French software engineer and AI researcher currently
    working at Google, created Keras for his own personal use before it was open-sourced
    in 2015\. Keras's primary goal is to provide an easy-to-use and accessible library
    to enable fast experiments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讲解了TensorFlow的基础知识，现在我们已经能够设置计算图。本章将介绍Keras，一个用Python编写的高层神经网络API，支持多个后端，TensorFlow是其中之一。Keras的创建者是法国软件工程师兼AI研究员François
    Chollet，他最初为个人使用而开发Keras，直到2015年开源。Keras的主要目标是提供一个易于使用且易于访问的库，以便快速实验。
- en: 'TensorFlow v1 suffers from usability issues; in particular, a sprawling and
    sometimes confusing API. For example, TensorFlow v1 offers two high-level APIs:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow v1存在可用性问题，特别是API庞大且有时令人困惑。例如，TensorFlow v1提供了两个高层API：
- en: The Estimator API (added in release 1.1) is used for training models on localhost
    or distributed environments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Estimator API（在1.1版本中添加）用于在本地主机或分布式环境中训练模型。
- en: The Keras API was then added later (release 1.4.0) and intended to be used for
    fast prototyping
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras API在之后的版本（1.4.0发布）中被添加，并旨在用于快速原型设计。
- en: 'With TensorFlow v2, Keras became the official high-level API. Keras can scale
    and suit various user profiles, from research to application development and from
    model training to deployment. Keras provides four key advantages: it''s user-friendly
    (without sacrificing flexibility and performance), modular, composable, and scalable.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着TensorFlow v2的发布，Keras成为官方高层API。Keras可以扩展并适应不同的用户需求，从研究到应用开发，从模型训练到部署。Keras提供四大优势：易于使用（不牺牲灵活性和性能）、模块化、可组合、可扩展。
- en: The TensorFlow Keras APIs are the same as the Keras API. However, the implementation
    of Keras in its TensorFlow version of the backend has been optimized for TensorFlow.
    It integrates TensorFlow-specific functionality, such as eager execution, data
    pipelines, and Estimators.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Keras API与Keras API相同。然而，Keras在TensorFlow后端中的实现已针对TensorFlow进行了优化。它集成了TensorFlow特有的功能，如急切执行、数据管道和Estimator。
- en: The difference between Keras, the independent library, and Keras' implementation
    as integrated with TensorFlow is only the way to import it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Keras作为独立库与Keras作为TensorFlow集成的实现之间的区别仅在于导入方式。
- en: 'Here is the command to import the Keras API specification:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是导入Keras API规范的命令：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is TensorFlow''s implementation of the Keras API specification:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是TensorFlow对Keras API规范的实现：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let's start by discovering the basic building blocks of Keras.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从发现Keras的基本构建块开始。
- en: Understanding Keras layers
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Keras层
- en: Keras layers are the fundamental building blocks of Keras models. Each layer
    receives data as input, does a specific task, and returns an output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Keras层是Keras模型的基本构建块。每一层接收数据作为输入，执行特定任务，并返回输出。
- en: 'Keras includes a wide range of built-in layers:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包括广泛的内置层：
- en: '**Core layers:** Dense, Activation, Flatten, Input, Reshape, Permute, RepeatVector,
    SpatialDropOut, and many more.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心层：**Dense、Activation、Flatten、Input、Reshape、Permute、RepeatVector、SpatialDropOut等。'
- en: '**Convolutional layers for Convolutional Neural Networks:** Conv1D, Conv2D,
    SeparableConv1D, Conv3D, Cropping2D, and many more.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层用于卷积神经网络：**Conv1D、Conv2D、SeparableConv1D、Conv3D、Cropping2D等。'
- en: '**Pooling** layers that perform a downsampling operation to reduce feature
    maps: MaxPooling1D, AveragePooling2D, and GlobalAveragePooling3D.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化**层执行下采样操作以减少特征图：MaxPooling1D、AveragePooling2D 和 GlobalAveragePooling3D。'
- en: '**Recurrent layers for recurrent neural networks to process recurrent or sequence
    data:** RNN, SimpleRNN, GRU, LSTM, ConvLSTM2D, etc.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环层用于循环神经网络处理循环或序列数据：**RNN、SimpleRNN、GRU、LSTM、ConvLSTM2D等。'
- en: '**The embedding layer**, only used as the first layer in a model and turns
    positive integers into dense vectors of fixed size.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入层**，仅用作模型中的第一层，将正整数转换为固定大小的稠密向量。'
- en: '**Merge layers:** Add, Subtract, Multiply, Average, Maximum, Minimum, and many
    more.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合并层：** 加法、减法、乘法、平均值、最大值、最小值等多种操作。'
- en: '**Advanced activation layers:** LeakyReLU, PReLU, Softmax, ReLU, etc.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级激活层：** LeakyReLU、PReLU、Softmax、ReLU 等。'
- en: '**The batch normalization layer**, which normalizes the activation of the previous
    layer at each batch.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化层**，它会在每个批次中归一化前一层的激活值。'
- en: '**Noise layers:** GausianNoise, GausianDropout, and AlphaDropout.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声层：** GaussianNoise、GaussianDropout 和 AlphaDropout。'
- en: '**Layer wrappers:** TimeDistributed applies a layer to every temporal slice
    of an input and bidirectional wrapper for RNNs.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层封装器：** TimeDistributed 将层应用于输入的每个时间切片，Bidirectional 是 RNN 的双向封装器。'
- en: '**Locally-connected layers:** LocallyConnected1D and LocallyConnected2D. They
    work like Conv1D or Conv2D without sharing their weights.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部连接层：** LocallyConnected1D 和 LocallyConnected2D。它们的工作方式类似于 Conv1D 或 Conv2D，但不共享权重。'
- en: We can also write our Keras layers as explained in the Keras Subclassing API
    section of this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以按照本章中 Keras 子类化 API 部分的说明编写自己的 Keras 层。
- en: Getting ready
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: To start, we'll review some methods that are common in all Keras layers. These
    methods are very useful to know the configuration and the state of a layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将回顾一些在所有 Keras 层中常见的方法。这些方法对于了解层的配置和状态非常有用。
- en: How to do it...
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s start with the layer''s weights. The weights are possibly the most essential
    concept in a layer; it decides how much influence the input will have on the output.
    It represents the state of a layer. The `get_weights()` function returns the weights
    of the layer as a list of NumPy arrays:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从层的权重开始。权重可能是层中最重要的概念；它决定了输入对输出的影响程度，表示了层的状态。`get_weights()` 函数返回层的权重，以 NumPy
    数组的列表形式：
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `set_weights()` method fixes the weights of the layer from a list of Numpy
    arrays:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`set_weights()` 方法可以通过一组 Numpy 数组来设置层的权重：'
- en: '[PRE3]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we''ll explain in the Keras Functional API recipe, sometimes neural network
    topology isn''t linear. In this case, a layer can be used several times in the
    network (shared layer). We can easily get the inputs and outputs of a layer by
    using this command if the layer is a single node (no shared layer):'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们将在 Keras 函数式 API 配方中解释的那样，有时神经网络的拓扑结构不是线性的。在这种情况下，层可以在网络中多次使用（共享层）。如果层是单一节点（无共享层），我们可以通过这个命令轻松获得层的输入和输出：
- en: '[PRE4]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or this one, if the layer has multiple nodes:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者如果层包含多个节点，可以使用这个：
- en: '[PRE5]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also easily get the layer''s input and output shapes by using this command
    if a layer is a single node (no shared layer):'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果层是单一节点（无共享层），我们还可以通过这个命令轻松获得层的输入和输出形状：
- en: '[PRE6]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or this one, if the layer has multiple nodes:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者如果层包含多个节点，可以使用这个：
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we''ll be discussing the layer''s configuration. As the same layer could
    be instantiating several times, the configuration doesn''t include the weights
    or connectivity information. The `get_config()` function returns a dictionary
    containing the configuration of the layer:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将讨论层的配置。由于相同的层可能会实例化多次，配置中不包括权重或连接信息。`get_config()` 函数返回一个字典，包含层的配置：
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `from_config()` method instantiates a layer''s configuration:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`from_config()` 方法用于实例化层的配置：'
- en: '[PRE9]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the layer configuration is stored in an associative array (Python
    dictionary), a data structure that maps keys to values.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，层配置存储在关联数组（Python 字典）中，这是一种将键映射到值的数据结构。
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The layers are the building blocks of the models. Keras offers a wide range
    of building layers and useful methods to know more about what's happening and
    get inside the models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 层是模型的构建模块。Keras 提供了广泛的构建层和有用的方法，让你更深入地了解模型的工作原理。
- en: 'With Keras, we can build models in three ways: with the Sequential, the Functional,
    or the Subclassing API. We''ll later see that only the last two APIs allow access
    to the layers.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras，我们可以通过三种方式构建模型：使用 Sequential、Functional 或 Subclassing API。稍后我们会看到，只有最后两种
    API 允许访问层。
- en: See also
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on the Keras Layers API, see the following documentation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Keras 层 API 的一些参考文档，请参见以下文档：
- en: 'Keras layers API documentation: [https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 层 API 文档：[https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)
- en: 'TensorFlow Keras layers API documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Keras 层 API 文档： [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)
- en: Using the Keras Sequential API
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras Sequential API
- en: The main goal of Keras is to make it easy to create deep learning models. The
    Sequential API allows us to create Sequential models, which are a linear stack
    of layers. Models that are connected layer by layer can solve many problems. To
    create a Sequential model, we have to create an instance of a Sequential class,
    create some model layers, and add them to it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的主要目标是简化深度学习模型的创建。Sequential API允许我们创建顺序模型，这是一种线性堆叠的层。通过逐层连接的模型可以解决许多问题。要创建一个顺序模型，我们需要实例化一个Sequential类，创建一些模型层并将它们添加进去。
- en: We will go from the creation of our Sequential model to its prediction via the
    compilation, training, and evaluation steps. By the end of this recipe, you will
    have a Keras model ready to be deployed in production.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建顺序模型开始，到编译、训练和评估步骤，最后实现模型的预测。在这个配方结束时，您将拥有一个可以在生产中部署的Keras模型。
- en: Getting ready
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will cover the main ways of creating a Sequential model and assembling
    layers to build a model with the Keras Sequential API.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将介绍创建顺序模型的主要方法，并使用Keras Sequential API组装层来构建模型。
- en: 'To start, we load TensorFlow and NumPy, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载TensorFlow和NumPy，如下所示：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好继续解释如何实现它。
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: First, we will create a Sequential model. Keras offers two equivalent ways of
    creating a Sequential model. Let's start by passing a list of layer instances
    as an array to the constructor. We'll build a multi-class classifier (10 categories)
    fully connected model, aka a multi-layer perceptron, by entering the following
    code.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个顺序模型。Keras提供了两种等效的方式来创建顺序模型。我们首先通过将层实例的列表作为数组传递给构造函数来开始。我们将通过输入以下代码构建一个多类分类器（10个类别）完全连接的模型，也称为多层感知机。
- en: '[PRE11]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Another way to create a Sequential model is to instantiate a Sequential class
    and then add layers via the `.add()` method.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建顺序模型的另一种方式是实例化一个Sequential类，然后通过`.add()`方法添加层。
- en: '[PRE12]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s take a closer look at the layer configuration. The `tf.keras.layers`
    API offers a lot of built-in layers and also provides an API to create our layers.
    In most of them, we can set these parameters to the layer''s constructor:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们仔细看看层的配置。`tf.keras.layers` API提供了许多内置层，并且还提供了创建我们自己的层的API。在大多数情况下，我们可以将这些参数设置为层的构造函数：
- en: We can add an activation function by specifying the name of a built-in function
    or as a callable object. This function decides whether a neuron should be activated
    or not. By default, a layer has no activation function. Below are the two ways
    to create a layer with an activation function. Note that you don't have to run
    the following code; these layers are not assigned to variables.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过指定内置函数的名称或可调用对象来添加激活函数。该函数决定一个神经元是否应该被激活。默认情况下，层没有激活函数。以下是创建带有激活函数的层的两种方式。请注意，您不需要运行以下代码，这些层未分配给变量。
- en: '[PRE13]'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can also specify an initialization strategy for the initial weights (kernel
    and bias) by passing the string identifier of built-in initializers or a callable
    object. The kernel is by default set to the "Glorot uniform" initializer, and
    the bias is set to zeros.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以通过传递内置初始化器的字符串标识符或可调用对象来指定初始权重（内核和偏置）的初始化策略。内核默认设置为“Glorot uniform”初始化器，偏置设置为零。
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can also specify regularizers for kernel and bias, such as L1 (also called
    Lasso) or L2 (also called Ridge) regularization. By default, no regularization
    is applied. A regularizer aims to prevent overfitting by penalizing a model for
    having large weights. These penalties are incorporated in the loss function that
    the network optimizes.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以为内核和偏置指定正则化器，如L1（也称为Lasso）或L2（也称为Ridge）正则化。默认情况下，不应用正则化。正则化器旨在通过惩罚具有大权重的模型来防止过拟合。这些惩罚被纳入网络优化的损失函数中。
- en: '[PRE15]'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In Keras, it's strongly recommended to set the input shape for the first layer.
    Yet, contrary to appearances, the input layer isn't a layer but a tensor. Its
    shape must be the same as our training data. The following layers perform automatic
    shape inference; their shapes are calculated based on the unit of the previous
    layer.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中，强烈建议为第一层设置输入形状。然而，与表面上看起来的不同，输入层不是一层，而是一个张量。它的形状必须与我们的训练数据相同。以下层将执行自动形状推断；它们的形状是根据前一层的单元计算出来的。
- en: 'Each type of layer requires input with a certain number of dimensions, so there
    are different ways to specify the input shape depending on the kind of layer.
    Here, we''ll focus on the Dense layer, so we''ll use the `input_dim` parameter.
    Since the shape of the weights depends on the input size, if the input shape isn''t
    specified in advance, the model has no weights: the model is not built. In this
    case, you can''t call any methods of the `Layer` class such as `summary`, `layers`,
    `weights`, and so on.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每种类型的层需要输入特定维度的张量，因此有不同的方法来指定输入形状，具体取决于层的种类。在这里，我们将重点介绍Dense层，因此我们将使用`input_dim`参数。由于权重的形状取决于输入的大小，如果没有预先指定输入形状，模型将没有权重：模型没有被构建。在这种情况下，你无法调用`Layer`类的任何方法，例如`summary`、`layers`、`weights`等。
- en: In this recipe, we'll create datasets with 64 features, and we'll process batches
    of 10 samples. The shape of our input data is (10,64), aka (`batch_size`, `number_of_features`).
    By default, a Keras model is defined to support any batch size, so the batch size
    isn't mandatory. We just have to specify the number of features through the `input_dim`
    parameter to our first layer.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将创建包含64个特征的数据集，并处理每批10个样本。我们的输入数据的形状是（10,64），即（`batch_size`，`number_of_features`）。默认情况下，Keras模型定义为支持任何批次大小，因此批次大小不是强制性的。我们只需要通过`input_dim`参数为第一层指定特征数量。
- en: '[PRE16]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: However, we can force the batch size for efficiency reasons with the `batch_size`
    argument.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，为了效率的考虑，我们可以通过`batch_size`参数强制设置批次大小。
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Before the learning phase, our model needs to be configured. This is done by
    the `compile` method. We have to specify:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在学习阶段之前，我们的模型需要进行配置。通过`compile`方法来完成这一配置。我们需要指定：
- en: An optimization algorithm for the training of our neural network. We can pass
    an optimizer instance from the `tf.keras.optimizers` module. For example, we can
    use an instance of `tf.keras.optimizers.RMSprop` or `'RMSprop'`, which is an optimizer
    that implements the `RMSprop` algorithm.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练我们神经网络的优化算法。我们可以从`tf.keras.optimizers`模块传递一个优化器实例。例如，我们可以使用`tf.keras.optimizers.RMSprop`的实例，或者使用字符串`'RMSprop'`，它是一个实现了`RMSprop`算法的优化器。
- en: A loss function called an objective function or optimization score function
    aims at minimizing the model. It can be the name of an existing loss function
    (such as `categorical_crossentropy` or `mse`), a symbolic TensorFlow loss function
    (`tf.keras.losses.MAPE`), or a custom loss function, which takes as input two
    tensors (true tensors and predicted tensors) and returns a scalar for each data
    point.
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个损失函数，也叫做目标函数或优化得分函数，目的是最小化模型。它可以是现有损失函数的名称（例如`categorical_crossentropy`或`mse`），一个符号化的TensorFlow损失函数（如`tf.keras.losses.MAPE`），或一个自定义的损失函数，它接收两个张量（真实张量和预测张量）并为每个数据点返回一个标量。
- en: A list of metrics used to judge our model's performance that aren't used in
    the model training process. We can either pass the string names or callables from
    the `tf.keras.metrics` module.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估我们模型性能的度量列表，这些度量不会在模型训练过程中使用。我们可以传递字符串名称或来自`tf.keras.metrics`模块的可调用函数。
- en: If you want to be sure that the model trains and evaluates eagerly, we can set
    the argument `run_eagerly` to true.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想确保模型以急切执行的方式进行训练和评估，可以将`run_eagerly`参数设置为true。
- en: Note that the graph is finalized with the `compile` method.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，图形通过`compile`方法最终确定。
- en: Now, we'll compile the model using the Adam optimizer for categorical cross-entropy
    loss and display the accuracy metric.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将使用Adam优化器来编译模型，采用类别交叉熵损失并显示准确率度量。
- en: '[PRE18]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we'll generate three toy datasets of 64 features with random values. One
    will be used to train the model (2,000 samples), another one to validate (500
    samples), and the last one to test (500 samples).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将生成三个包含64个特征的玩具数据集，数据值为随机生成。其中一个用于训练模型（2,000个样本），另一个用于验证（500个样本），最后一个用于测试（500个样本）。
- en: '[PRE19]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After the model has been configured, the learning phase begins by calling the
    `fit` method. The training configuration is done by these three arguments:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型配置完成后，通过调用`fit`方法开始学习阶段。训练配置由以下三个参数完成：
- en: We have to set the number of epochs, aka the number of iterations over the entire
    input data.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须设置训练轮次，即遍历整个输入数据的迭代次数。
- en: We have to specify the number of samples per gradient, called the `batch_size`
    argument. Note that the last batch may be smaller if the total number of samples
    is not divisible by the batch size.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要指定每个梯度的样本数，称为`batch_size`参数。请注意，如果总样本数不能被批次大小整除，则最后一批次的样本可能较小。
- en: We can specify a validation dataset by setting the `validation_data` argument
    (a tuple of inputs and labels). This dataset makes it easy to monitor the performance
    of the model. The loss and metrics are computed in inference mode at the end of
    each epoch.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过设置`validation_data`参数（一个包含输入和标签的元组）来指定验证数据集。这个数据集可以方便地监控模型的性能。在每个训练周期结束时，损失和度量会在推理模式下计算。
- en: 'Now, we''ll train the model on our toy datasets by calling the `fit` method:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将通过调用`fit`方法在我们的玩具数据集上训练模型：
- en: '[PRE20]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we''ll evaluate our model on the test dataset. We''ll call the `model.evaluate`
    function, which predicts the loss value and the metric values of the model in
    test mode. Computation is done in batches. It has three important arguments: the
    input data, the target data, and the batch size. This function predicts the output
    for a given input. Then, it computes the `metrics` function (specified in the
    `model.compile` based on the target data) and the model''s prediction and returns
    the computed metric value as the output.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在测试数据集上评估我们的模型。我们将调用`model.evaluate`函数，它预测模型在测试模式下的损失值和度量值。计算是按批次进行的。它有三个重要参数：输入数据、目标数据和批次大小。此函数为给定输入预测输出，然后计算`metrics`函数（在`model.compile`中根据目标数据指定），并返回计算后的度量值作为输出。
- en: '[PRE21]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also just use the model to make a prediction. The `tf.keras.Model.predict`
    method takes as input only data and returns a prediction. And here''s how to predict
    the output of the last layer of inference for the data provided, as a NumPy array:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以仅使用模型进行预测。`tf.keras.Model.predict`方法仅接受数据作为输入并返回预测结果。以下是如何预测提供数据的最后一层推理输出，结果以
    NumPy 数组形式呈现：
- en: '[PRE22]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Analyzing this model's performance is of no interest in this recipe because
    we randomly generated a dataset.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分析这个模型的性能在这个示例中并不重要，因为我们使用的是随机生成的数据集。
- en: Now, let's move on to an analysis of this recipe.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们继续分析这个示例。
- en: How it works...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Keras provides the Sequential API to create models composed of a linear stack
    of layers. We can either pass a list of layer instances as an array to the constructor
    or use the `add` method.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了 Sequential API 来创建由一系列线性堆叠的层组成的模型。我们可以将层实例的列表作为数组传递给构造函数，或者使用`add`方法。
- en: Keras provides different kinds of layers. Most of them share some common constructor
    arguments such as `activation`, `kernel_initializer` and `bias_initializer`, and
    `kernel_regularizer` and `bias_regularizer`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了不同种类的层。它们大多数共享一些常见的构造参数，如`activation`、`kernel_initializer`和`bias_initializer`，以及`kernel_regularizer`和`bias_regularizer`。
- en: 'Take care with the delayed-build pattern: if no input shape is specified on
    the first layer, the model gets built the first time the model is called on some
    input data or when methods such as `fit`, `eval`, `predict`, and `summary` are
    called. The graph is finalized with the `compile` method, which configures the
    model before the learning phase. Then, we can evaluate the model or make predictions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意延迟构建模式：如果没有在第一层指定输入形状，当第一次将模型应用于输入数据时，或者调用`fit`、`eval`、`predict`和`summary`等方法时，模型才会被构建。图形在调用`compile`方法时最终确定，该方法在学习阶段之前配置模型。然后，我们可以评估模型或进行预测。
- en: See also
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on the Keras Sequential API, visit the following websites:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keras Sequential API 的一些参考资料，请访问以下网站：
- en: 'tf.keras.Sequential model API documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential
    )'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'tf.keras.Sequential 模型 API 文档: [https://www.tensorflow.org/api_docs/python/tf/keras/Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)'
- en: 'Keras Sequential model API documentation: [https://keras.io/models/sequential/](https://keras.io/models/sequential/
    )'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Keras Sequential 模型 API 文档: [https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
- en: Using the Keras Functional API
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 函数式 API
- en: The Keras Sequential API is great for developing deep learning models in most
    situations. However, this API has some limitations, such as a linear topology,
    that could be overcome with the Functional API. Note that many high-performing
    networks are based on a non-linear topology such as Inception, ResNet, etc.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Keras Sequential API 在大多数情况下非常适合开发深度学习模型。然而，这个 API 有一些限制，例如线性拓扑结构，可以通过函数式 API
    来克服。需要注意的是，许多高效的网络都基于非线性拓扑结构，如 Inception、ResNet 等。
- en: The Functional API allows defining complex models with a non-linear topology,
    multiple inputs, multiple outputs, residual connections with non-sequential flows,
    and shared and reusable layers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式 API 允许定义具有非线性拓扑结构、多个输入、多个输出、残差连接的复杂模型，以及具有非顺序流动的共享和可重用层。
- en: The deep learning model is usually a directed acyclic graph (DAG). The Functional
    API is a way to build a graph of layers and create more flexible models than the
    `tf.keras.Sequential` API.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常是一个有向无环图（DAG）。功能API是一种构建层图的方式，它比`tf.keras.Sequential` API创建更灵活的模型。
- en: Getting ready
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will cover the main ways of creating a Functional model, using callable
    models, manipulating complex graph topologies, sharing layers, and finally introducing
    the concept of the layer "node" with the Keras Sequential API.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本实例将涵盖创建功能模型的主要方式，使用可调用模型、操作复杂的图拓扑、共享层，并最终引入使用Keras顺序API的“层节点”概念。
- en: 'As usual, we just need to import TensorFlow as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们只需要按如下方式导入TensorFlow：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好继续解释如何实现这一点。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Let's go and make a Functional model for recognizing the MNIST dataset of handwritten
    digits. We will predict the handwritten digits from grayscale images.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建一个功能模型，用于识别MNIST手写数字数据集。我们将从灰度图像中预测手写数字。
- en: Creating a Functional model
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建功能模型
- en: First, we will load the MNIST dataset.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载MNIST数据集。
- en: '[PRE24]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, we will create an input node with a 28x28 dimensional shape. Remember
    that in Keras, the input layer is not a layer but a tensor, and we have to specify
    the input shape for the first layer. This tensor must have the same shape as our
    training data. By default, a Keras model is defined to support any batch size,
    so the batch size isn't mandatory. `Input()` is used to instantiate a Keras tensor.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个28x28维度的输入节点。记住，在Keras中，输入层并不是一个层，而是一个张量，我们必须为第一个层指定输入形状。这个张量的形状必须与我们的训练数据形状一致。默认情况下，Keras模型被定义为支持任何批次大小，因此批次大小不是必需的。`Input()`用于实例化Keras张量。
- en: '[PRE25]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Then, we will flatten the images of size (28,28) using the following command.
    This will produce an array of 784 pixels.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用以下命令对大小为(28,28)的图像进行展平操作。这将生成一个包含784个像素的数组。
- en: '[PRE26]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We''ll add a new node in the graph of layers by calling `the flatten_layer`
    on the `inputs` object:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过在`inputs`对象上调用`flatten_layer`来在层图中添加一个新的节点：
- en: '[PRE27]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The "layer call" action is like drawing an arrow from `inputs` to the `flatten_layer`.
    We're "passing" the inputs to the flatten layer, and as a result, it produces
    outputs. A layer instance is callable (on a tensor) and returns a tensor.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “层调用”操作就像是从`inputs`到`flatten_layer`绘制一条箭头。我们正在“传递”输入到展平层，结果是它产生了输出。层实例是可调用的（在张量上）并返回一个张量。
- en: 'Then, we''ll create a new layer instance:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个新的层实例：
- en: '[PRE28]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ll add a new node:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加一个新节点：
- en: '[PRE29]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To build a model, multiple layers are stacked. In this example, we will add
    another `dense` layer to do a classification task between 10 classes:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建一个模型，多个层将被堆叠。在这个示例中，我们将添加另一个`dense`层来进行10个类别之间的分类任务：
- en: '[PRE30]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Input tensor(s) and output tensor(s) are used to define a model. The model is
    a function of one or more input layers and one or more output layers. The model
    instance formalizes the computational graph on how the data flows from input(s)
    to output(s).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入张量和输出张量用于定义模型。模型是一个由一个或多个输入层和一个或多个输出层构成的函数。模型实例形式化了计算图，描述数据如何从输入流向输出。
- en: '[PRE31]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, we'll print the summary.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将打印模型的摘要。
- en: '[PRE32]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This results in the following output:![](img/B16254_03_01.png)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生如下输出：![](img/B16254_03_01.png)
- en: 'Figure 3.1: Summary of the model'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.1：模型摘要
- en: Such a model can be trained and evaluated by the same `compile, fit`, `evaluate`,
    and `predict` methods used in the Keras Sequential model.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样的模型可以通过与Keras顺序模型中相同的`compile, fit`、`evaluate`和`predict`方法进行训练和评估。
- en: '[PRE33]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In this recipe, we have built a model using the Functional API.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们使用功能API构建了一个模型。
- en: Using callable models like layers
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用像层这样的可调用模型
- en: Let's go into the details of the Functional API with callable models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解功能API与可调用模型的细节。
- en: 'With the Functional API, it is easy to reuse trained models: any model can
    be treated as a layer, by calling it on a tensor. We will reuse the model defined
    in the previous section as a layer to see this in action. It''s a classifier for
    10 categories. This model returns 10 probabilities: 1 for each category. It''s
    called a 10-way softmax. So, by calling the model defined above, the model will
    predict for each input one of the 10 classes.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用功能API，重复使用训练好的模型变得非常容易：任何模型都可以通过在张量上调用它作为一层来处理。我们将重用前面定义的模型作为一层，以便查看其实现效果。它是一个用于10个类别的分类器。该模型返回10个概率值：每个类别一个概率值。这被称为10分类softmax。因此，通过调用上述模型，模型将为每个输入预测10个类别中的一个。
- en: '[PRE34]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that by calling a model, we are not just reusing the model architecture,
    we are also reusing its weights.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，通过调用一个模型，我们不仅仅是重用模型架构，我们还在重用它的权重。
- en: If we're facing a sequence problem, creating a model will become very easy with
    the Functional API. For example, instead of processing one image, we want to process
    a video composed of many images. We could turn an image classification model into
    a video classification model in just one line using the `TimeDistributed` layer
    wrapper. This wrapper applies our previous model to every temporal slice of the
    input sequence, or in other words, to each image of our video.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们面临一个序列问题，使用功能性 API 创建模型将变得非常简单。例如，假设我们不是处理一张图片，而是处理由多张图片组成的视频。我们可以通过使用 `TimeDistributed`
    层包装器，将图像分类模型转变为视频分类模型，仅需一行代码。这个包装器将我们的前一个模型应用于输入序列的每一个时间切片，换句话说，就是应用于视频的每一帧图像。
- en: '[PRE35]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have seen that models are callable like layers. Now, we'll learn how to create
    complex models with a non-linear topology.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，模型像层一样是可以调用的。现在，我们将学习如何创建具有非线性拓扑的复杂模型。
- en: Creating a model with multiple inputs and outputs
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个具有多个输入和输出的模型
- en: The Functional API makes it easy to manipulate a large number of intertwined
    datastreams with multiple inputs and outputs and non-linear connectivity topologies.
    These cannot be handled with the Sequential API, which isn't able to create a
    model with layers that aren't connected sequentially or with multiple inputs or
    outputs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性 API 使得操作大量交织的数据流变得简单，具有多个输入输出和非线性连接拓扑。这些是顺序 API 无法处理的，顺序 API 无法创建具有非顺序连接或多个输入输出的模型。
- en: Let's go with an example. We're going to build a system for predicting the price
    of a specific house and the elapsed time before its sale.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们将构建一个系统，用于预测特定房子的价格和销售前的经过时间。
- en: 'The model will have two inputs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将有两个输入：
- en: Data about the house such as the number of bedrooms, house size, air conditioning,
    fitted kitchen, etc.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于房子的资料，例如卧室数量、房屋大小、空调、内置厨房等。
- en: A recent picture of the house
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房子的最新照片
- en: 'This model will have two outputs:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将有两个输出：
- en: The elapsed time before the sale (two categories – slow or fast)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售前的经过时间（两个类别——慢或快）
- en: The predicted price
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测价格
- en: To build this system, we'll start by building the first block to process tabular
    data about the house.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建这个系统，我们将从构建第一个模块开始，用于处理关于房子的表格数据。
- en: '[PRE36]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then, we'll build the second block to process the house image data.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将构建第二个模块来处理房子的图像数据。
- en: '[PRE37]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now, we'll merge all available features into a single large vector via concatenation.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过拼接将所有可用特征合并为一个大的向量。
- en: '[PRE38]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Then, we'll stick a logistic regression for price prediction on top of the features.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将在特征上加一个用于价格预测的逻辑回归。
- en: '[PRE39]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: And, we'll stick a time classifier on top of the features.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们将在特征上加一个时间分类器。
- en: '[PRE40]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, we'll build the model.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将构建模型。
- en: '[PRE41]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now, we'll plot the model.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将绘制模型图。
- en: '[PRE42]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This results in the following output:![](img/B16254_03_02.png)
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：![](img/B16254_03_02.png)
- en: 'Figure 3.2: Plot of a model with multiple inputs and outputs'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.2：具有多个输入和输出的模型图
- en: In this recipe, we have created a complex model using the Functional API with
    multiple inputs and outputs that predicts the price of a specific house and the
    elapsed time before its sale. Now, we'll introduce the concept of shared layers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用功能性 API 创建了一个复杂模型，具有多个输入输出，用于预测特定房子的价格和销售前的经过时间。现在，我们将引入共享层的概念。
- en: Shared layers
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享层
- en: Some models reuse the same layer multiple times inside their architecture. These
    layer instances learn features that correspond to multiple paths in the graph
    of layers. Shared layers are often used to encode inputs from similar spaces.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型在其架构内多次重用相同的层。这些层实例学习与层图中多个路径对应的特征。共享层通常用于对来自相似空间的输入进行编码。
- en: To share a layer (weights and all) across different inputs, we only need to
    instantiate the layer once and call it on as many inputs as we want.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不同的输入之间共享一个层（包括权重），我们只需实例化该层一次，并将其应用于我们需要的多个输入。
- en: Let's consider two different sequences of text. We will apply the same embedding
    layer to these two sequences, which feature similar vocabulary.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两种不同的文本序列。我们将对这两个具有相似词汇的序列应用相同的嵌入层。
- en: '[PRE43]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In this recipe, we have learned how to reuse a layer multiple times in the same
    model. Now, we'll introduce the concept of extracting and reusing a layer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已经学习了如何在同一个模型中多次重用一个层。现在，我们将介绍提取和重用层的概念。
- en: Extracting and reusing nodes in the graph of layers
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在层的图中提取和重用节点
- en: In the first recipe of this chapter, we saw that a layer is an instance that
    takes a tensor as an argument and returns another tensor. A model is composed
    of several layer instances. These layer instances are objects that are chained
    one to another by their layer input and output tensors. Each time we instantiate
    a layer, the output of the layer is a new tensor. By adding a "node" to the layer,
    we link the input to the output tensor.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个例子中，我们看到一个层是一个实例，它以一个张量作为参数并返回另一个张量。一个模型是由多个层实例组成的。这些层实例是通过它们的输入和输出张量相互连接的对象。每次我们实例化一个层时，该层的输出是一个新的张量。通过向层添加一个“节点”，我们将输入和输出张量连接起来。
- en: The graph of layers is a static data structure. With the Keras Functional API,
    we can easily access and inspect the model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 层的图是一个静态数据结构。通过 Keras 函数式 API，我们可以轻松访问和检查模型。
- en: The `tf.keras.application` module contains canned architectures with pre-trained
    weights.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.application` 模块包含具有预训练权重的现成架构。'
- en: Let's go to download the ResNet 50 pre-trained model.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们去下载 ResNet 50 预训练模型。
- en: '[PRE44]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we''ll display the intermediate layers of the model by querying the graph
    data structure:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将通过查询图数据结构来显示模型的中间层：
- en: '[PRE45]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we''ll display the top 10 intermediate layers of the model by querying
    the graph data structure:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将通过查询图数据结构来显示模型的前 10 个中间层：
- en: '[PRE46]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This results in the following output:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE47]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now, we'll select all the feature layers. We'll go into the details in the convolution
    neural network chapter.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将选择所有特征层。我们将在卷积神经网络章节中详细讲解。
- en: '[PRE48]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Then, we'll reuse the nodes in order to create our feature-extraction model.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将重用这些节点来创建我们的特征提取模型。
- en: '[PRE49]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'One of the interesting benefits of a deep learning model is that it can be
    reused partly or wholly on similar predictive modeling problems. This technique
    is called "transfer learning": it significantly improves the training phase by
    decreasing the training time and the model''s performance on a related problem.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的一个有趣的好处是，它可以在类似的预测建模问题上部分或完全重复使用。这种技术称为“迁移学习”：通过减少训练时间，它显著提高了训练阶段的效果，并增强了模型在相关问题上的表现。
- en: The new model architecture is based on one or more layers from a pre-trained
    model. The weights of the pre-trained model may be used as the starting point
    for the training process. They can be either fixed or fine-tuned, or totally adapted
    during the learning phase. The two main approaches to implement transfer learning
    are weight initialization and feature extraction. Don't worry, we'll go into the
    details later in this book.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 新的模型架构基于预训练模型中的一个或多个层。预训练模型的权重可以作为训练过程的起点。它们可以是固定的、微调的，或者在学习阶段完全适应。实现迁移学习的两种主要方法是权重初始化和特征提取。别担心，我们稍后会在本书中详细讲解。
- en: In this recipe, we have loaded a pretrained model based on the VGG19 architecture.
    We have extracted nodes from this model and reused them in a new model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们加载了基于 VGG19 架构的预训练模型。我们从这个模型中提取了节点，并在新模型中重用了它们。
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The Keras Sequential API is appropriate in the vast majority of cases but is
    limited to creating layer-by-layer models. The Functional API is more flexible
    and allows extracting and reusing nodes, sharing layers, and creating non-linear
    models with multiple inputs and multiple outputs. Note that many high-performing
    networks are based on a non-linear topology.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 顺序 API 在绝大多数情况下是合适的，但仅限于创建逐层模型。函数式 API 更加灵活，允许提取和重用节点、共享层，并创建具有多个输入和多个输出的非线性模型。需要注意的是，许多高性能的网络基于非线性拓扑结构。
- en: In this recipe, we have learned how to build models using the Keras Functional
    API. These models are trained and evaluated by the same `compile`, `fit`, `evaluate`,
    and `predict` methods used by the Keras Sequential model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已经学习了如何使用 Keras 函数式 API 构建模型。这些模型的训练和评估使用与 Keras 顺序模型相同的 `compile`、`fit`、`evaluate`
    和 `predict` 方法。
- en: We have also viewed how to reuse trained models as a layer, how to share layers,
    and also how to extract and reuse nodes. This last approach is used in transfer
    learning techniques that speed up training and improve performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看了如何将训练好的模型作为层进行重用，如何共享层，以及如何提取并重用节点。最后一种方法在迁移学习技术中被使用，能够加速训练并提高性能。
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: As we can access every layer, models built with the Keras Functional API have
    specific features such as model plotting, whole-model saving, etc.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以访问每一层，使用 Keras 函数式 API 构建的模型具有一些特定功能，如模型绘图、整模型保存等。
- en: 'Models built with the Functional API could be complex, so here are some tips
    to consider to avoid pulling your hair out during the process:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用函数式 API 构建的模型可能会很复杂，因此这里有一些提示，帮助你避免在过程中抓狂：
- en: 'Name the layers: It will be quite useful when we display summaries and plots
    of the model graph.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名层：在展示模型图的摘要和图表时，这将非常有用。
- en: 'Separate submodels: Consider each submodel as being like a Lego brick that
    we will combine together with the others at the end.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分离子模型：将每个子模型视为一个乐高积木，最后我们将它们与其他子模型一起组合。
- en: 'Review the layer summary: Use the `summary` method to check the outputs of
    each layer.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看层摘要：使用 `summary` 方法查看每一层的输出。
- en: 'Review graph plots: Use the `plot` method to display and check the connection
    between the layers.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看图形绘图：使用 `plot` 方法显示并检查层之间的连接。
- en: 'Consistent variable names: Use the same variable name for the input and output
    layers. It avoids copy-paste mistakes.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致的变量命名：对输入和输出层使用相同的变量名，避免复制粘贴错误。
- en: See also
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on the Keras Functional API, visit the following websites:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keras 函数式 API 的一些参考资料，访问以下网站：
- en: 'Keras Functional API documentation: [https://keras.io/getting-started/functional-api-guide/](https://keras.io/getting-started/functional-api-guide/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 函数式 API 文档： [https://keras.io/getting-started/functional-api-guide/](https://keras.io/getting-started/functional-api-guide/)
- en: '`tf.keras.Model` API: [https://www.tensorflow.org/api_docs/python/tf/keras/Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model
    )'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.keras.Model` API： [https://www.tensorflow.org/api_docs/python/tf/keras/Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model
    )'
- en: 'Machine Learning Mastery: [https://machinelearningmastery.com/keras-functional-api-deep-learning/](https://machinelearningmastery.com/keras-functional-api-deep-learning/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习大师： [https://machinelearningmastery.com/keras-functional-api-deep-learning/](https://machinelearningmastery.com/keras-functional-api-deep-learning/)
- en: 'Inside TensorFlow: tf.Keras by François Chollet (part1) [https://www.youtube.com/watch?v=UYRBHFAvLSs](https://www.youtube.com/watch?v=UYRBHFAvLSs)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 内部：tf.Keras 由 François Chollet 提供（第一部分）[https://www.youtube.com/watch?v=UYRBHFAvLSs](https://www.youtube.com/watch?v=UYRBHFAvLSs)
- en: 'Inside TensorFlow: tf.Keras (part2) [https://www.youtube.com/watch?v=uhzGTijaw8A](https://www.youtube.com/watch?v=uhzGTijaw8A)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 内部：tf.Keras（第二部分）[https://www.youtube.com/watch?v=uhzGTijaw8A](https://www.youtube.com/watch?v=uhzGTijaw8A)
- en: Using the Keras Subclassing API
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 子类化 API
- en: Keras is based on object-oriented design principles. So, we can subclass the
    `Model` class and create our model architecture definition.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 基于面向对象设计原则。因此，我们可以继承 `Model` 类并创建我们的模型架构定义。
- en: The Keras Subclassing API is the third way proposed by Keras to build deep neural
    network models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 子类化 API 是 Keras 提出的第三种构建深度神经网络模型的方法。
- en: This API is fully customizable, but this flexibility also brings complexity!
    So, hold on to your hats, it's harder to use than the Sequential or Functional
    API.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 API 完全可定制，但这种灵活性也带来了复杂性！所以，请系好安全带，它比顺序 API 或函数式 API 更难使用。
- en: But you're probably wondering why we need this API if it's so hard to use. Some
    model architectures and some custom layers can be extremely challenging. Some
    researchers and some developers hope to have full control of their models and
    the way to train them. The Subclassing API provides these features. Let's go into
    the details.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能会想，如果这个 API 使用起来这么难，我们为什么还需要它。某些模型架构和一些自定义层可能会非常具有挑战性。一些研究人员和开发人员希望完全控制他们的模型以及训练模型的方式。子类化
    API 提供了这些功能。让我们深入了解细节。
- en: Getting ready
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Here, we will cover the main ways of creating a custom layer and a custom model
    using the Keras Subclassing API.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍使用 Keras 子类化 API 创建自定义层和自定义模型的主要方法。
- en: 'To start, we load TensorFlow, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载 TensorFlow，如下所示：
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已准备好继续解释如何操作。
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: Let's start by creating our layer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从创建我们的层开始。
- en: Creating a custom layer
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义层
- en: As explained in the *Understanding Keras layers* section, Keras provides various
    built-in layers such as dense, convolutional, recurrent, and normalization layers
    through its layered API.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 *理解 Keras 层* 部分所解释的那样，Keras 通过其层化 API 提供了各种内置的层，如全连接层、卷积层、循环层和归一化层。
- en: 'All layers are subclasses of the `Layer` class and implement these methods:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所有层都是 `Layer` 类的子类，并实现了这些方法：
- en: The `build` method, which defines the weights of the layer.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build` 方法，用于定义层的权重。'
- en: The `call` method, which specifies the transformation from inputs to outputs
    done by the layer.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call` 方法，指定层执行的从输入到输出的转换。'
- en: The `compute_output_shape` method, if the layer modifies the shape of its input.
    This allows Keras to perform automatic shape inference.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_output_shape` 方法，如果该层修改了输入的形状。这样 Keras 就可以执行自动的形状推断。'
- en: The `get_config` and `from_config` methods, if the layer is serialized and deserialized.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_config` 和 `from_config` 方法，如果该层被序列化和反序列化。'
- en: 'Let''s put the theory into action. First, we''ll make a subclass layer for
    a custom dense layer:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将理论付诸实践。首先，我们将为自定义全连接层创建一个子类层：
- en: '[PRE51]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we''ll create a model using the `MyCustomDense` layer created in the
    previous step:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用前一步创建的 `MyCustomDense` 层来创建模型：
- en: '[PRE52]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we will reload the model from the config:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将从配置文件重新加载模型：
- en: '[PRE53]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In this recipe, we have created our `Layer` class. Now, we'll create our model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们已经创建了我们的 `Layer` 类。现在，我们将创建我们的模型。
- en: Creating a custom model
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义模型
- en: By subclassing the `tf.keras.Model` class, we can build a fully customizable
    model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过子类化 `tf.keras.Model` 类，我们可以构建一个完全可定制的模型。
- en: We define our layers in the `__init__` method, and we can have full, complete
    control over the forward pass of the model by implementing the `call` method.
    The `training` Boolean argument can be used to specify different behavior during
    the training or inference phase.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `__init__` 方法中定义我们的层，并通过实现 `call` 方法完全控制模型的前向传播。`training` 布尔参数可以用来指定训练阶段和推理阶段期间的不同行为。
- en: 'First, we will load the MNIST dataset and normalize the grayscale:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载 MNIST 数据集并对灰度进行归一化：
- en: '[PRE54]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s go and make a subclass `Model` for recognizing MNIST data:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个 `Model` 的子类，用于识别 MNIST 数据：
- en: '[PRE55]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we are going to instantiate the model and process the training:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实例化模型并处理训练：
- en: '[PRE56]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The Subclassing API is a way for deep learning practitioners to build their
    layers or models using object-oriented Keras design principles. We recommend using
    this API only if your model cannot be achieved using the Sequential or the Functional
    API. Although this way can be complicated to implement, it remains useful in a
    few cases, and it is interesting for all developers and researchers to know how
    layers and models are implemented in Keras.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 子类化 API 是深度学习从业者使用面向对象的 Keras 设计原则构建层或模型的一种方式。如果你的模型无法使用 Sequential 或 Functional
    API 实现，我们建议使用此 API。尽管这种方式的实现可能会很复杂，但在某些情况下仍然非常有用，了解如何在 Keras 中实现层和模型对于所有开发者和研究人员来说都是非常有趣的。
- en: See also
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on the Keras Subclassing API, see the following tutorials,
    papers, and articles:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keras 子类化 API 的一些参考，见以下教程、论文和文章：
- en: 'Writing custom layers and models with Keras: [https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 编写自定义层和模型：[https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models)
- en: 'Writing your own Keras layers: [https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写你自己的 Keras 层：[https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/)
- en: Using the Keras Preprocessing API
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 预处理 API
- en: The Keras Preprocessing API gathers modules for data processing and data augmentation.
    This API provides utilities for working with sequence, text, and image data. Data
    preprocessing is an essential step in machine learning and deep learning. It converts,
    transforms, or encodes raw data into an understandable, useful, and efficient
    format for learning algorithms.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 预处理 API 汇集了数据处理和数据增强的模块。该 API 提供了处理序列、文本和图像数据的工具。数据预处理是机器学习和深度学习中的一个重要步骤，它将原始数据转换、转换或编码为适合学习算法理解、有效且有用的格式。
- en: Getting ready
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will cover some preprocessing methods provided by Keras for sequence,
    text, and image data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将介绍 Keras 提供的一些预处理方法，用于处理序列、文本和图像数据。
- en: 'As usual, we just need to import TensorFlow as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们只需导入 TensorFlow 如下：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好继续解释如何做了。
- en: How to do it...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Let's start with the sequence data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从序列数据开始。
- en: Sequence preprocessing
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列预处理
- en: Sequence data is data where the order matters, such as text or a time series.
    So, a time series is defined by a series of data points ordered by time.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 序列数据是有序的数据，如文本或时间序列。因此，时间序列由按时间顺序排列的一系列数据点定义。
- en: Time series generator
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间序列生成器
- en: Keras provides utilities for preprocessing sequence data such as time series
    data. It takes in consecutive data points and applies transformations using time
    series parameters such as stride, length of history, etc., to return a TensorFlow
    dataset instance.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了处理序列数据（如时间序列数据）的实用工具。它接收连续的数据点，并使用时间序列参数（如步幅、历史长度等）进行转换，返回一个 TensorFlow
    数据集实例。
- en: 'Let''s go with a toy time series dataset of 10 integer values:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用一个玩具时间序列数据集，包含 10 个整数值：
- en: '[PRE58]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This results in the following output:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE59]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We want to predict the next value from the last five lag observations. So,
    we''ll define a generator with the `length` argument set to 5\. This argument
    specifies the length of the output sequences in a number of timesteps:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望根据最后五个滞后观测值预测下一个值。因此，我们将定义一个生成器，并将 `length` 参数设置为 5。此参数指定输出序列的长度（以时间步为单位）：
- en: '[PRE60]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We want to generate samples composed of 5 lag observations for one prediction
    and the toy time series dataset contains 10 values. So, the number of samples
    generated is 5:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望生成由 5 个滞后观测组成的样本，用于预测，而玩具时间序列数据集包含 10 个值。因此，生成的样本数为 5：
- en: '[PRE61]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then, we''ll display the inputs and output of each sample and check that the
    data is well prepared:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将显示每个样本的输入和输出，并检查数据是否准备就绪：
- en: '[PRE62]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This results in the following output:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE63]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, we''ll create and compile a model:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建并编译一个模型：
- en: '[PRE64]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'And we''ll train the model by giving the generator as input data:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将通过将生成器作为输入数据来训练模型：
- en: '[PRE65]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Preparing time series data for modeling with deep learning methods can be very
    challenging. But fortunately, Keras provides a generator that will help us transform
    a univariate or multivariate time series dataset into a data structure ready to
    train models. This generator offers many options to prepare the data, such as
    the shuffle, the sampling rate, the start and end offsets, etc. We recommend consulting
    the official Keras API to get more details.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习方法准备时间序列数据可能非常具有挑战性。但幸运的是，Keras 提供了一个生成器，可以帮助我们将单变量或多变量时间序列数据集转换为准备用于训练模型的数据结构。该生成器提供了许多选项来准备数据，如洗牌、采样率、起始和结束偏移量等。建议查阅官方
    Keras API 获取更多详细信息。
- en: Now, we'll focus on how to prepare data for variable-length input sequences.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将专注于如何准备可变长度输入序列的数据。
- en: Padding sequences
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 填充序列
- en: When processing sequence data, each sample often has different lengths. In order
    for all the sequences to fit the desired length, the solution is to pad them.
    Sequences shorter than the defined sequence length are padded with values at the
    end (by default) or the beginning of each sequence. Otherwise, if the sequence
    is greater than the desired length, the sequence is truncated.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 处理序列数据时，每个样本的长度通常不同。为了使所有序列适合所需的长度，解决方案是对它们进行填充。比定义的序列长度短的序列在每个序列的末尾（默认）或开头填充值。否则，如果序列大于所需长度，则截断序列。
- en: 'Let''s start with four sentences:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从四个句子开始：
- en: '[PRE66]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: First, we'll build the vocabulary lookup table. We'll create two dictionaries
    to go from the words to integer identifiers and vice versa.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将构建词汇查找表。我们将创建两个字典，一个用于从单词到整数标识符的转换，另一个反之。
- en: '[PRE67]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Then after building the vocabulary lookup table, we'll encode the sentences
    as integer arrays.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在构建词汇查找表之后，我们将句子编码为整数数组。
- en: '[PRE68]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This results in the following output:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE69]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now, we'll use the `pad_sequences` function to truncate and pad sequences to
    a common length easily. The pre-sequence padding is activated by default.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `pad_sequences` 函数轻松地截断和填充序列到一个公共长度。默认情况下，启用了序列前填充。
- en: '[PRE70]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This results in the following output:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE71]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Then, we'll activate the post-sequence padding and set the `maxlen` argument
    to the desired length – here, 7.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将激活序列后填充，并将 `maxlen` 参数设置为所需的长度 – 这里是 7。
- en: '[PRE72]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This results in the following output:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE73]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The length of the sequence can also be trimmed to the desired length – here,
    3\. By default, this function removes timesteps from the beginning of each sequence.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列的长度也可以裁剪为所需的长度——此处为3。默认情况下，该函数会从每个序列的开头移除时间步长。
- en: '[PRE74]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This results in the following output:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE75]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Set the truncating argument to `post` to remove timesteps from the end of each
    sequence.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将截断参数设置为`post`，以便从每个序列的末尾移除时间步长。
- en: '[PRE76]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This results in the following output:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE77]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Padding is very useful when we want all sequences in a list to have the same
    length.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望列表中的所有序列具有相同长度时，填充非常有用。
- en: In the next section, we will cover a very popular technique for preprocessing
    text.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一种非常流行的文本预处理技术。
- en: Skip-grams
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Skip-grams
- en: Skip-grams is one of the unsupervised learning techniques in natural language
    processing. It finds the most related words for a given word and predicts the
    context word for this given word.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-grams是自然语言处理中的一种无监督学习技术。它为给定的单词找到最相关的单词，并预测该单词的上下文单词。
- en: Keras provides the `skipgrams` pre-processing function, which takes in an integer-encoded
    sequence of words and returns the relevance for each pair of words in the defined
    window. If the pair of words is relevant, the sample is positive, and the associated
    label is set to 1\. Otherwise, the sample is considered negative, and the label
    is set to 0.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了`skipgrams`预处理函数，它接收一个整数编码的单词序列，并返回定义窗口中每对单词的相关性。如果一对单词相关，样本为正样本，相关标签设置为1。否则，样本被认为是负样本，标签设置为0。
- en: An example is better than thousands of words. So, let's take this sentence,
    `"I like coconut and apple,"` select the first word as our "context word," and
    use a window size of two. We make pairs of the context word "I" with the word
    covered in the specified window. So, we have two pairs of words `(I, like)` and
    `(I, coconut)`, both of which equal `1`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 一例胜千言。让我们以这个句子为例：“I like coconut and apple，”选择第一个单词作为我们的“上下文单词”，并使用窗口大小为二。我们将上下文单词“I”与指定窗口中的单词配对。所以，我们有两个单词对`(I,
    like)`和`(I, coconut)`，这两者的值都为1。
- en: 'Let''s put the theory into action:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将理论付诸实践：
- en: 'First, we''ll encode a sentence as a list of word indices:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将把一个句子编码为单词索引的列表：
- en: '[PRE78]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Then, we''ll call the `skipgrams` function with a window size of 1:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将调用`skipgrams`函数，窗口大小为1：
- en: '[PRE79]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, we''ll print the results:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将打印结果：
- en: '[PRE80]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'This results in the following output:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE81]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Note that the non-word is defined by index 0 in the vocabulary and will be skipped.
    We recommend that readers consult the Keras API to find more details about padding.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，非单词在词汇表中由索引0表示，并将被跳过。我们建议读者参考Keras API，了解更多关于填充的细节。
- en: Now, let's introduce some tips to preprocess text data.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们介绍一些文本数据预处理的技巧。
- en: Text preprocessing
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本预处理
- en: In deep learning, we cannot feed raw text directly into our network. We have
    to encode our text as numbers and provide integers as input. Our model will generate
    integers as output. This module provides utilities for preprocessing text input.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们不能直接将原始文本输入到网络中。我们必须将文本编码为数字，并提供整数作为输入。我们的模型将生成整数作为输出。这个模块提供了文本输入预处理的工具。
- en: Split text to word sequence
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将文本拆分为单词序列
- en: Keras provides the `text_to_word_sequence` method, which transforms a sequence
    into a list of words or tokens.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了`text_to_word_sequence`方法，将序列转换为单词或标记的列表。
- en: 'Let''s go with this sentence:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用这个句子：
- en: '[PRE82]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Then, we'll call the method that converts a sentence into a list of words. By
    default, this method splits the text on whitespace.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将调用将句子转换为单词列表的方法。默认情况下，该方法会根据空格拆分文本。
- en: '[PRE83]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'This results in the following output:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE84]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, we''ll set the `lower` argument to `True`, and the text will be converted
    to lower case:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将`lower`参数设置为`True`，文本将被转换为小写：
- en: '[PRE85]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'This results in the following output:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE86]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Note that by default, the `filter` argument filters out a list of characters
    such as punctuation. In our last code execution, we removed all the predefined
    filters.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，默认情况下，`filter`参数会过滤掉一系列字符，如标点符号。在我们上一次的代码执行中，我们移除了所有预定义的过滤器。
- en: Let's continue with a method to encode words or categorical features.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用一种方法来编码单词或类别特征。
- en: Tokenizer
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Tokenizer
- en: The `Tokenizer` class is the utility class for text tokenization. It's the preferred
    approach for preparing text in deep learning.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tokenizer`类是文本分词的实用工具类。它是深度学习中准备文本的首选方法。'
- en: 'This class takes as inputs:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的输入参数为：
- en: The maximum number of words to keep. Only the most common words will be kept
    based on word frequency.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要保留的最大词数。只有最常见的词会根据词频被保留。
- en: A list of characters to filter out.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于过滤的字符列表。
- en: A boolean to convert the text into lower case, or not.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个布尔值，用于决定是否将文本转换为小写字母。
- en: The separator for word splitting.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分词的分隔符。
- en: 'Let''s go with this sentence:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从这句话开始：
- en: '[PRE87]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, we will create a `Tokenizer` instance and fit it on the previous sentences:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个`Tokenizer`实例，并对前述句子进行拟合：
- en: '[PRE88]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The tokenizer creates several pieces of information about the document. We can
    get a dictionary containing the count for each word.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词器创建了文档的几部分信息。我们可以获得一个字典，包含每个词的计数。
- en: '[PRE89]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'This results in the following outputs:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE90]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We can also get a dictionary containing, for each word, the number of documents
    in which it appears:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以获得一个字典，包含每个词出现在多少个文档中：
- en: '[PRE91]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This results in the following outputs:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE92]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'A dictionary contains, for each word, its unique integer identifier:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个字典包含每个词的唯一整数标识符：
- en: '[PRE93]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This results in the following outputs:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE94]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The number of unique documents that were used to fit the `Tokenizer`.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于拟合`Tokenizer`的独特文档数量。
- en: '[PRE95]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'This results in the following outputs:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE96]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Now, we are ready to encode our documents, thanks to the `texts_to_matrix` function.
    This function provides four different document encoding schemes to compute the
    coefficient for each token.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好编码文档，感谢`texts_to_matrix`函数。这个函数提供了四种不同的文档编码方案，用来计算每个标记的系数。
- en: Let's start with the binary mode, which returns whether or not each token is
    present in the document.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们从二进制模式开始，它返回文档中每个标记是否存在。
- en: '[PRE97]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'This results in the following outputs:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE98]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The `Tokenizer` API offers another mode based on word count – it returns the
    count of each word in the document:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Tokenizer` API 提供了另一种基于词频的模式——它返回文档中每个词的计数：'
- en: '[PRE99]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'This results in the following outputs:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE100]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Note that we can also use the `tfidf` mode or the frequency mode. The first
    returns the term frequency-inverse document frequency score for each word, and
    the second returns the frequency of each word in the document related to the total
    number of words in the document.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以使用`tfidf`模式或频率模式。前者返回每个词的词频-逆文档频率分数，后者返回文档中每个词的频率，且与文档中的总词数相关。
- en: The `Tokenizer` API can fit the training dataset and encode text data in the
    training, validation, and test datasets.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tokenizer` API 可以拟合训练数据集并对训练、验证和测试数据集中的文本数据进行编码。'
- en: In this section, we have covered a few techniques to prepare text data before
    training and prediction.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些在训练和预测之前准备文本数据的技术。
- en: Now, let's go on to prepare and augment images.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续准备并增强图像。
- en: Image preprocessing
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像预处理
- en: The data preprocessing module provides a set of tools for real-time data augmentation
    on image data.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理模块提供了一套实时数据增强工具，用于图像数据。
- en: In deep learning, the performance of a neural network is often improved by the
    number of examples available in the training dataset.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，神经网络的性能通常通过训练数据集中的示例数量来提高。
- en: The `ImageDataGenerator` class in the Keras preprocessing API allows the creation
    of new data from the training dataset. It isn't applied to the validation or test
    dataset because it aims to expand the number of examples in the training datasets
    with plausible new images. This technique is called data augmentation. Beware
    not to confuse data preparation with data normalization or image resizing, which
    is applied to all data in interaction with the model. Data augmentation includes
    many transformations from the field of image manipulation, such as rotation, horizontal
    and vertical shift, horizontal and vertical flip, brightness, and much more.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Keras预处理API中的`ImageDataGenerator`类允许从训练数据集中创建新数据。它不应用于验证或测试数据集，因为其目的是通过合理的新图像扩展训练数据集中的示例数量。这种技术称为数据增强。请注意不要将数据准备与数据归一化或图像调整大小混淆，后者适用于所有与模型交互的数据。数据增强包括图像处理领域的许多变换，例如旋转、水平和垂直偏移、水平和垂直翻转、亮度调整等。
- en: The strategy may differ depending on the task to realize. For example, in the
    MNIST dataset, which contains images of handwritten digits, applying a horizontal
    flip doesn't make sense. Except for the figure 8, this transformation isn't appropriate.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 策略可能会根据任务的不同而有所不同。例如，在MNIST数据集中，它包含了手写数字的图像，应用水平翻转没有意义。除了数字8之外，这种变换是不合适的。
- en: While in the case of a baby picture, applying this kind of transformation makes
    sense because the image could have been taken from the left or right.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 而在婴儿照片的情况下，应用这种变换是有意义的，因为图像可能是从左边或右边拍摄的。
- en: Let's put the theory into action and perform a data augmentation on the `CIFAR10`
    dataset. We will start by downloading the `CIFAR` dataset.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将理论付诸实践，对`CIFAR10`数据集进行数据增强。我们将首先下载`CIFAR`数据集。
- en: '[PRE101]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Now, we'll create an image data generator that applies a horizontal flip, a
    random rotation between 0 and 15, and a shift of 3 pixels on the width and on
    the height.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个图像数据生成器，应用水平翻转、0到15度之间的随机旋转，以及在宽度和高度方向上平移3个像素。
- en: '[PRE102]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Create an iterator on the train dataset.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练数据集的迭代器。
- en: '[PRE103]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Create a model and compile it.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个模型并编译它。
- en: '[PRE104]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: And process the training by calling the `fit` method. Take care to set the `step_per_epoch`
    argument, which specifies the number of sample batches comprising an epoch.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fit`方法来处理训练。请注意设置`step_per_epoch`参数，该参数指定一个epoch包含的样本批次数。
- en: '[PRE105]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: With the image data generator, we have extended the size of our original dataset
    by creating new images. With more images, the training of a deep learning model
    can be improved.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图像数据生成器，我们通过创建新图像扩展了原始数据集的大小。通过更多的图像，深度学习模型的训练效果可以得到提升。
- en: How it works...
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The Keras Preprocessing API allows transforming, encoding, and augmenting data
    for neural networks. It makes it easier to work with sequence, text, and image
    data.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: Keras预处理API允许转换、编码和增强神经网络的数据。它使得处理序列、文本和图像数据变得更加容易。
- en: First, we introduced the Keras Sequence Preprocessing API. We used the time
    series generator to transform a univariate or multivariate time series dataset
    into a data structure ready to train models. Then, we focused on the data preparation
    for variable-length input sequences, aka padding. And we finished this first part
    with the skip-gram technique, which finds the most related words for a given word
    and predicts the context word for that given word.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍了 Keras Sequence 预处理 API。我们使用时间序列生成器将单变量或多变量时间序列数据集转换为适合训练模型的数据结构。然后，我们重点介绍了可变长度输入序列的数据准备，即填充（padding）。最后，我们以skip-gram技术结束这一部分，skip-gram可以为给定单词找到最相关的词，并预测该单词的上下文词。
- en: Then, we covered the Keras Text Preprocessing API, which offers a complete turnkey
    solution to process natural language. We learned how to split text into words
    and tokenize the words using binary, word count, `tfidf`, or frequency mode.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们介绍了 Keras 文本预处理 API，它提供了一个完整的解决方案来处理自然语言。我们学习了如何将文本拆分为单词，并使用二进制、词频计数、`tfidf`
    或频率模式对单词进行标记化。
- en: Finally, we focused on the Image Preprocessing API using the `ImageDataGenerator`,
    which is a real advantage to increase the size of your training dataset and to
    work with images.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们重点介绍了使用`ImageDataGenerator`的图像预处理API，这是增加训练数据集大小并处理图像的一个实际优势。
- en: See also
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on the Keras Preprocessing API, visit the following websites:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keras 预处理 API 的一些参考资料，请访问以下网站：
- en: 'Sequence Preprocessing Keras API: [http://keras.io/preprocessing/sequence/](http://keras.io/preprocessing/sequence/)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列预处理 Keras API：[http://keras.io/preprocessing/sequence/](http://keras.io/preprocessing/sequence/)
- en: 'Text Processing Keras API: [https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本处理 Keras API：[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)
- en: 'More about syntactic and semantic word similarities: Tomas Mikolov and Kai
    Chen and Greg Corrado and Jeffrey Dean. (2013). Efficient Estimation of Word Representations
    in Vector Space [https://arxiv.org/pdf/1301.3781v3.pdf](https://arxiv.org/pdf/1301.3781v3.pdf)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于句法和语义词汇相似性的更多信息：Tomas Mikolov、Kai Chen、Greg Corrado 和 Jeffrey Dean。（2013）。高效估算词向量表示
    [https://arxiv.org/pdf/1301.3781v3.pdf](https://arxiv.org/pdf/1301.3781v3.pdf)
- en: 'Image Preprocessing Keras API: [http://keras.io/preprocessing/image/](http://keras.io/preprocessing/image/)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像预处理 Keras API：[http://keras.io/preprocessing/image/](http://keras.io/preprocessing/image/)
- en: 'More examples of data image augmentation: [https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多数据图像增强的示例：[https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)
