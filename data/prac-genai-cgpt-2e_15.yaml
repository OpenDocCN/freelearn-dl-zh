- en: Appendix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: In the main chapters of this book, we explored the power of OpenAI’s models
    through the lens of ChatGPT, diving into its conversational interface and understanding
    how it can revolutionize the way we interact with AI. However, the world of OpenAI
    extends beyond ChatGPT’s familiar chat-based experience. To fully harness the
    potential of these models, it’s crucial to understand the broader tools and interfaces
    OpenAI provides.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的主要章节中，我们通过ChatGPT的视角探索了OpenAI模型的力量，深入其对话界面，并理解了它如何改变我们与AI互动的方式。然而，OpenAI的世界远不止ChatGPT熟悉的基于聊天的体验。为了充分利用这些模型的能力，了解OpenAI提供的更广泛工具和界面至关重要。
- en: 'This appendix is dedicated to exploring one such tool: the **OpenAI Playground**.
    The Playground offers a versatile environment to experiment with OpenAI’s models,
    granting more control over parameters, outputs, and behaviors. Whether you want
    to fine-tune responses, test different use cases, or simply gain a deeper understanding
    of the models’ capabilities, the Playground is an invaluable resource.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录致力于探索这样一个工具：**OpenAI游乐场**。游乐场提供了一个灵活的环境，可以用来实验OpenAI的模型，允许用户对参数、输出和行为有更多的控制。无论您是想微调响应、测试不同的用例，还是仅仅想更深入地了解模型的能力，游乐场都是一个无价的资源。
- en: 'In this appendix, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将：
- en: Walk through the Playground interface and its key features.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示游乐场界面及其关键特性。
- en: Illustrate how to interact with OpenAI models directly from the Playground.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何从游乐场直接与OpenAI模型交互。
- en: Offer tips and best practices to maximize your outcomes when using the Playground.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供使用游乐场时最大化结果的技巧和最佳实践。
- en: By the end of this appendix, you’ll have the knowledge and confidence to use
    OpenAI’s Playground and its models, going beyond ChatGPT.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本附录结束时，您将具备使用OpenAI的游乐场及其模型的知识和信心，超越ChatGPT。
- en: Trying OpenAI models in the Playground
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在游乐场中尝试OpenAI模型
- en: 'To access an OpenAI Playground, you need to create an OpenAI account and navigate
    through [to https://platform.openai.com/playgro](https://platform.openai.com/playground)und.
    This is how the landing page looks:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问OpenAI游乐场，您需要创建一个OpenAI账户并导航到[https://platform.openai.com/playground](https://platform.openai.com/playground)。这是着陆页的样貌：
- en: '![A screenshot of a chat  Description automatically generated](img/Appendix_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图，自动生成描述](img/Appendix_01.png)'
- en: 'Figure 1: OpenAI Playground at https://platform.openai.com/playground'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：OpenAI游乐场，位于https://platform.openai.com/playground
- en: As you can see from *Figure 1*, the Playground offers a UI where the user can
    start interacting with the model, which you can select at the top of your chat
    interface. Note that, whenever consuming models via the OpenAI Playground, you
    will be charged a fee depending on the amount of interactions. You can find the
    pricing page at https://openai.com/api/pricing/.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从*图1*中看到的，游乐场提供了一个用户界面，用户可以从中开始与模型交互，您可以在聊天界面的顶部选择模型。请注意，无论何时通过OpenAI游乐场使用模型，您都将根据交互量支付费用。您可以在https://openai.com/api/pricing/找到定价页面。
- en: 'Before diving deeper into the main sections of the Playground, let’s first
    define some jargon you will see in this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨游乐场的主要部分之前，让我们首先定义一些您在本章中会看到的术语：
- en: '**Tokens**: Tokens can be considered as word fragments or segments that are
    used by the API to process input prompts. Unlike complete words, tokens may contain
    trailing spaces or even word segments. As a general rule of thumb, one token in
    English is approximately equivalent to four characters, or three-quarters of a
    word (you can refer to the following link to convert words to tokens in the context
    of OpenAI models: https://platform.openai.com/tokenizer).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记**：标记可以被认为是API用于处理输入提示的词片段或段。与完整单词不同，标记可能包含尾随空格甚至单词片段。一般来说，一个英文标记大约相当于四个字符，或者是一个单词的三分之四（您可以在以下链接中参考如何将单词转换为OpenAI模型中的标记：https://platform.openai.com/tokenizer）。'
- en: '**Prompt**: In the context of **natural language processing** (**NLP**) and
    Generative AI, a prompt refers to a piece of text that is given as input to an
    AI language model to generate a response or output. The prompt can be a question,
    a statement, or a sentence, and it is used to provide context and direction to
    the language model.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示词**：在自然语言处理（NLP）和生成式AI的背景下，提示词指的是作为输入提供给AI语言模型以生成响应或输出的文本片段。提示词可以是一个问题、一个陈述或一个句子，它被用来为语言模型提供上下文和方向。'
- en: '**Context**: In the field of GPT, context refers to the words and sentences
    that come before the user’s prompt. This context is used by the language model
    to generate the most probable next word or phrase, based on the patterns and relationships
    found in the training data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文**：在GPT领域，上下文指的是用户提示之前的单词和句子。这个上下文被语言模型用来根据训练数据中发现的模式和关系生成最可能的下一个单词或短语。'
- en: '**Model confidence**: Model confidence refers to the level of certainty or
    probability that an AI model assigns to a particular prediction or output. In
    the context of NLP, model confidence is often used to indicate how confident the
    AI model is in the correctness or relevance of its generated response to a given
    input prompt.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型置信度**：模型置信度是指AI模型对特定预测或输出的确定性或概率水平。在NLP的背景下，模型置信度通常用于表示AI模型对其生成的针对给定输入提示的响应的正确性或相关性的信心程度。'
- en: '**Tools**: With tools, we provide the model with an extra skill that it can
    invoke to accomplish the user’s task. A function will always have a description
    in natural language so that the model knows when to invoke it.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：通过工具，我们为模型提供额外的技能，使其能够调用以完成用户的任务。一个函数总会有自然语言描述，以便模型知道何时调用它。'
- en: In the Playground, there are four main sections to interact with the models.
    Let’s explore them in the next sections.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Playground中，有四个主要部分可以与模型交互。让我们在下一节中探讨它们。
- en: Chat
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天
- en: Here, you can test all the chat models available today, including both text-only
    models (like GPT-3.5) and multimodal models (like GPT-4o). You can provide a system
    message – the set of instructions that you provide your model with – all in natural
    language.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以测试今天所有可用的聊天模型，包括仅文本模型（如GPT-3.5）和多模态模型（如GPT-4o）。你可以提供系统消息——你提供给模型的指令集——全部使用自然语言。
- en: '**Definition**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In the context of LLMs, the system message is an instruction provided at the
    beginning of a conversation to establish the model’s role, behavior, and response
    guidelines. This message sets the overarching context, guiding the model’s interactions
    to align with specific objectives or constraints. For example, a system message
    might specify that the model should act as a friendly travel advisor or maintain
    a formal tone. This configuration can be set at the backend level by the AI developer,
    so that the end user will not have access to it and, henceforth, will not be able
    to “force” the model to behave differently.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的背景下，系统消息是在对话开始时提供的指令，用于确立模型的角色、行为和响应指南。这条消息设定了总体上下文，指导模型的交互以符合特定的目标或约束。例如，系统消息可能指定模型应充当友好的旅行顾问或保持正式的语气。这种配置可以在AI开发者后端级别设置，这样最终用户将无法访问它，因此无法“强迫”模型以不同的方式行事。
- en: 'You can also compare the output of two different models, given the same question.
    The following is an example of how to do that:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以比较对同一问题的两个不同模型的输出。以下是如何做到这一点的示例：
- en: '![A screenshot of a chat  Description automatically generated](img/Appendix_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图  自动生成的描述](img/Appendix_02.png)'
- en: 'Figure 2: An example of comparison between two models'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：两个模型之间比较的示例
- en: 'For each model, you can also play with some parameters that you can configure.
    Here is a list:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，你还可以调整一些你可以配置的参数。以下是一个列表：
- en: '**Temperature** (ranging from 0 to 2): This controls the randomness of the
    model’s response. A low-level temperature makes your model more deterministic,
    meaning that it will tend to give the same output to the same question. For example,
    if I ask my model multiple times *What is OpenAI?* with the temperature set as
    0, it will give, most of the time, the same answer. On the other hand, if I do
    the same with a temperature greater than 0, it will try to modify its answers
    each time, in terms of wording and style.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度**（范围从0到2）：这控制着模型响应的随机性。低温度使模型更加确定，意味着它倾向于对相同的问题给出相同的输出。例如，如果我将温度设置为0多次询问我的模型“什么是OpenAI？”的话，它大多数时候会给出相同的答案。另一方面，如果我用大于0的温度做同样的事情，它将尝试每次修改其答案，包括措辞和风格。'
- en: '**Max tokens**: This controls the length (in terms of tokens) of the model’s
    response to the user’s prompt.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大令牌数**：这控制着模型对用户提示的响应长度（以令牌为单位）。'
- en: '**Stop sequences** (user input): This makes responses end at the desired point,
    such as the end of a sentence or list.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止序列**（用户输入）：这使响应在期望的点结束，例如句子的结尾或列表的结尾。'
- en: '**Top probabilities** (ranging from 0 to 1): This controls which tokens the
    model will consider when generating a response. This means that the model will
    select from the smallest set of tokens whose cumulative probability adds up to
    90% of the distribution.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最高概率**（范围从0到1）：这控制了模型在生成响应时将考虑哪些标记。这意味着模型将从累积概率总和达到分布90%的最小标记集中进行选择。'
- en: '**Frequency penalty** (ranging from 0 to 1): This controls the repetition of
    the same tokens in the generated response. The higher the penalty, the lower the
    probability of seeing the same tokens more than once in the same response. The
    penalty reduces the chance proportionally, based on how often a token has appeared
    in the text so far (this is the key difference from the following parameter).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率惩罚**（范围从0到1）：这控制了在生成的响应中相同标记的重复。惩罚越高，同一响应中看到相同标记超过一次的概率越低。惩罚会根据标记到目前为止在文本中出现的频率成比例地减少（这是与以下参数的关键区别）。'
- en: '**Presence penalty** (ranging from 0 to 2): This is similar to the previous
    parameter but stricter. It reduces the chance of repeating any token that has
    appeared in the text at all so far. As it is stricter than the frequency penalty,
    the presence penalty also increases the likelihood of introducing new topics in
    a response.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存在惩罚**（范围从0到2）：这与之前的参数类似，但更严格。它减少了重复任何到目前为止已出现在文本中的标记的机会。由于比频率惩罚更严格，存在惩罚还增加了在响应中引入新主题的可能性。'
- en: Assistants
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 助手
- en: '**OpenAI Assistants** can be seen as a way to develop AI agents faster and
    more easily. In fact, Assistants can be defined as entities powered by an LLM,
    with a set of instructions to follow and a set of tools or plugins to use.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI助手**可以被视为一种更快、更简单地开发AI代理的方法。实际上，助手可以被定义为由LLM驱动的实体，具有要遵循的指令和要使用的工具或插件集。'
- en: 'In the case of OpenAI Assistants, they come with three pre-built tools:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI助手的案例中，它们附带三个预构建的工具：
- en: '**File Search**: This allows the user to upload custom documents so that the
    Assistant can navigate through them to accomplish the user’s query. It operates
    with a RAG-based framework.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件搜索**：这允许用户上传自定义文档，以便助手可以导航这些文档以完成用户的查询。它使用基于RAG的框架。'
- en: '**Function Calling**: This allows the user to define a set of custom functions
    that can be invoked by the Assistant to accomplish a given task.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数调用**：这允许用户定义一组自定义函数，这些函数可以被助手调用以完成特定任务。'
- en: '**Code Interpreter**: This refers to the capability of the Assistant to run
    code either against provided documents (for example, in the case of spreadsheets
    or analytical papers that require mathematical computations) or simply to solve
    complex tasks provided by the user (for example, complex mathematical problems).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码解释器**：这指的是助手运行代码的能力，无论是针对提供的文档（例如，在需要数学计算的电子表格或分析论文的情况下）还是简单地解决用户提供的复杂任务（例如，复杂的数学问题）。'
- en: 'In the following screenshot, you can see an example of an Assistant called
    **Chat with PDF**, which specializes in responding to provided documents (in my
    case, I uploaded the paper *LLaMA: Open and Efficient Foundation Language Models*
    by Hugo Touvron et al.).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下屏幕截图中，您可以看到一个名为**与PDF聊天**的助手示例，该助手专门用于响应提供的文档（在我的情况下，我上传了Hugo Touvron等人撰写的论文《LLaMA：开放和高效的基座语言模型》）。
- en: '![A screenshot of a computer  Description automatically generated](img/Appendix_03.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/Appendix_03.png)'
- en: 'Figure 3: Example of an OpenAI Assistant'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：OpenAI助手的示例
- en: As you can see from the preceding screenshot, the Assistant was able to answer
    my question, retrieving knowledge from the provided document. In fact, my question
    was pretty vague, since the term *toxicity* can refer to multiple domains; nevertheless,
    the Assistant knows to watch over the provided documents as the primary source
    of information.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的屏幕截图中所见，助手能够回答我的问题，从提供的文档中检索知识。实际上，我的问题相当模糊，因为“毒性”一词可以指多个领域；尽管如此，助手知道要监视提供的文档作为主要信息来源。
- en: Completions
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成内容
- en: This section refers to a class of models called **base models**, like GPT-3\.
    They are the basis on which the so-called “assistant models” (or chat models,
    as we saw previously) are built. For example, the chat model GPT-3.5 Turbo (the
    model behind ChatGPT) is a fine-tuned version of the base model GPT-3.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涉及一类称为**基础模型**的模型，如GPT-3。它们是所谓“助手模型”（或聊天模型，如我们之前所看到的）的基础。例如，聊天模型GPT-3.5 Turbo（ChatGPT背后的模型）是基础模型GPT-3的微调版本。
- en: '**Definition**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Completions (base) models are designed for generating single responses to prompts,
    making them suitable for tasks like text generation and summarization without
    maintaining context over multiple interactions. Chat (assistant) models, on the
    other hand, are optimized for interactive conversations, capable of maintaining
    context across multiple turns, and are ideal for applications like chatbots and
    virtual assistants.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完成模型（基础模型）旨在生成对提示的单个响应，这使得它们适合像文本生成和摘要这样的任务，而不需要在多次交互中保持上下文。另一方面，聊天（助手）模型针对交互式对话进行了优化，能够在多个回合中保持上下文，非常适合聊天机器人和虚拟助手等应用。
- en: 'Below you can see an example of a typical completion task in the Playground:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下面您可以看到游乐场中一个典型的完成任务的示例：
- en: '![A screenshot of a computer  Description automatically generated](img/Appendix_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的截图 自动生成的描述](img/Appendix_04.png)'
- en: 'Figure 4: Example of completion task in OpenAI Playground'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：OpenAI游乐场中完成任务的示例
- en: As you can see, using my words “Today I went to a grocery store and” the model
    completed the sentence with the most likely words.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用我的话“今天我去了一家杂货店和”，模型用最可能出现的词语完成了这个句子。
- en: Today, completion models are rarely used as they are outperformed by chat models,
    yet they can be further fine-tuned to tailored use cases (we will cover fine-tuning
    later on in this section).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，完成模型很少使用，因为它们在聊天模型面前表现不佳，但它们可以被进一步微调以适应特定的用例（我们将在本节稍后讨论微调）。
- en: Text to speech
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本转语音
- en: In addition to *Whisper*, the aforementioned speech-to-text model, OpenAI also
    released a **text-to-speech** (**TTS**) model that can be tested directly in the
    Playground.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述的语音识别模型 *Whisper* 之外，OpenAI 还发布了一个 **文本转语音**（**TTS**）模型，该模型可以直接在游乐场中测试。
- en: 'Let’s see an example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子：
- en: '![A screenshot of a video chat  Description automatically generated](img/Appendix_05.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![视频聊天的截图 自动生成的描述](img/Appendix_05.png)'
- en: 'Figure 5: Example of using OpenAI’s TTS models in the Playground'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在游乐场中使用OpenAI的TTS模型的示例
- en: As you can see from the above screenshot, you can select the voice, model, speed,
    and format of the generated audio.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从上面的截图中所见，您可以选择声音、模型、速度和生成的音频格式。
- en: All the previous models come pre-built, in the sense that they have already
    been pre trained on a huge knowledge base.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的前述模型都是预先构建的，也就是说，它们已经在庞大的知识库上进行了预训练。
- en: However, there are some ways you can make your model more customized and tailored
    for your use case.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些方法可以使你的模型更加定制化，更适合你的用例。
- en: Customizing your model
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定制你的模型
- en: The first method of tailoring your model for your use case is embedded in the
    way the model is designed, and it involves providing your model with the context
    in the few-shot learning approach.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定制你的模型的第一个方法嵌入在模型的设计方式中，它涉及到在少量样本学习方法中向模型提供上下文。
- en: For example, you could ask the model to generate an article whose template and
    lexicon recall another one you have already written. For this, you can provide
    the model with your query of generating an article and also with the former article
    as a reference or context, so that the model is better prepared for your request.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以要求模型生成一篇文章，其模板和词汇库回忆起你之前已经写过的另一篇文章。为此，你可以向模型提供生成文章的查询，以及前述文章作为参考或上下文，这样模型就能更好地准备你的请求。
- en: 'Here is an example of it:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '![A screenshot of a chat  Description automatically generated](img/Appendix_06.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![聊天的截图 自动生成的描述](img/Appendix_06.png)'
- en: 'Figure 6: An example of a conversation within the OpenAI Playground with the
    few-shot learning approach'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用OpenAI游乐场中的少量样本学习方法的对话示例
- en: In the previous example, I instructed the model to output only the label of
    the tweet’s sentiment, providing it with three examples of how to do that.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我指示模型只输出推文情感的标签，并提供了三个如何做到这一点的示例。
- en: The second method of customizing your model is more sophisticated and is called
    **fine-tuning**. Fine-tuning is the process of adapting a pre trained model to
    a new task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 定制你的模型的第二种方法更为复杂，被称为 **微调**。微调是将预训练模型适应新任务的过程。
- en: 'In fine-tuning, the parameters of the pre trained model are altered, either
    by adjusting the existing parameters or by adding new parameters, to better fit
    the data for the new task. This is done by training the model on a smaller labeled
    dataset that is specific to the new task. The key idea behind fine-tuning is to
    leverage the knowledge learned from the pre trained model and fine-tune it to
    the new task, rather than training a model from scratch. Have a look at the following
    figure:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调中，预训练模型的参数被调整，无论是通过调整现有参数还是添加新参数，以更好地适应新任务的数据。这是通过在针对新任务的小型标记数据集上训练模型来完成的。微调背后的关键思想是利用从预训练模型中学到的知识，并将其微调到新任务，而不是从头开始训练模型。看看下面的图：
- en: '![](img/Appendix_07.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Appendix_07.png)'
- en: 'Figure 7: Model fine-tuning'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：模型微调
- en: 'In the preceding figure, you can see a schema on how fine-tuning works on OpenAI
    pre-built models. The idea is that you have available a pre trained model with
    general-purpose weights or parameters. Then, you feed your model with custom data,
    typically in the form of *key-value* prompts and completions, as shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，你可以看到一个关于如何在OpenAI预构建模型上执行微调的架构。想法是，你有一个带有通用权重或参数的预训练模型可用。然后，你用自定义数据（通常是*键值*提示和完成）来喂养你的模型，如下所示：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the training is done, you will have a customized model that performs particularly
    well for a given task, for example, the classification of your company’s documentation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，你将拥有一个针对特定任务表现特别出色的定制模型，例如，对你公司文档的分类。
- en: The nice thing about fine-tuning is that you can make pre-built models tailored
    to your use cases, without the need to re-train them from scratch, yet leveraging
    smaller training datasets and hence needing less training time and computing.
    At the same time, the model keeps its generative power and accuracy learned via
    the original training, the one that was carried out on the massive dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的好处在于，你可以根据你的用例定制预构建模型，而无需从头开始重新训练它们，同时利用较小的训练数据集，因此需要更少的训练时间和计算。同时，模型保留了通过原始训练学习到的生成能力和准确性，这是在大量数据集上进行的。
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The OpenAI Playground presents a powerful tool for experimenting with advanced
    AI models through zero- or few-shot learning and fine-tuning techniques. The Playground
    allows users to interact directly with pre trained models, making it easier to
    customize and enhance them for specific tasks, such as sentiment analysis or document
    classification.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI游乐场提供了一个强大的工具，通过零样本或少量样本学习和微调技术进行高级AI模型的实验。游乐场允许用户直接与预训练模型互动，使其更容易为特定任务（如情感分析或文档分类）进行定制和增强。
- en: For developers looking to build AI applications that leverage OpenAI’s API,
    mastering these techniques is crucial to ascertain whether a specific model’s
    configuration will meet a specific application’s requirements.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望利用OpenAI API构建AI应用的开发者来说，掌握这些技术对于确定特定模型的配置是否满足特定应用的需求至关重要。
- en: Despite the focus of this book being mainly on ChatGPT, enterprise-scale scenarios
    (which we covered in *Chapter 10*) require more customized approaches when it
    comes to AI use cases; that’s why familiarizing yourself with the concept of the
    Playground and OpenAI models’ APIs is of a great value to embrace the mindset
    of this new wave of AI-powered application development.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的重点主要在ChatGPT上，但在企业级场景（我们在*第10章*中讨论过）中，当涉及到AI用例时需要更多定制的方法；这就是为什么熟悉游乐场和OpenAI模型API的概念对于拥抱这一波AI驱动应用开发的新思维非常有价值。
- en: Join our communities on Discord and Reddit
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord和Reddit社区
- en: Have questions about the book or want to contribute to discussions on Generative
    AI and LLMs? Join our Discord server at [https://packt.link/I1tSU](Appendix.xhtml)
    and our Reddit channel at [https://packt.link/jwAmA](Appendix.xhtml) to connect,
    share, and collaborate with like-minded enthusiasts.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对本书有任何疑问或想参与关于生成式AI和LLMs的讨论？加入我们的Discord服务器[https://packt.link/I1tSU](Appendix.xhtml)和Reddit频道[https://packt.link/jwAmA](Appendix.xhtml)，以连接、分享和与志同道合的爱好者合作。
- en: '![](img/Discord.png) ![](img/QR_Code757615820155951000.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Discord.png) ![](img/QR_Code757615820155951000.png)'
- en: '![](img/New_Packt_Logo1.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/New_Packt_Logo1.png)'
- en: '[packt.com](http://packt.com)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[packt.com](http://packt.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，全面访问超过 7,000 本书和视频，以及领先的行业工具，帮助您规划个人发展并推进职业生涯。欲了解更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自 4,000 多位行业专业人士的实用电子书和视频，减少学习时间，增加编码时间
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为您量身定制的技能计划提高学习效果
- en: Get a free eBook or video every month
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月免费获得一本电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，便于轻松访问关键信息
- en: Copy and paste, print, and bookmark content
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制粘贴，打印和收藏内容
- en: At [www.packt.com](http://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [www.packt.com](http://www.packt.com)，您还可以阅读一系列免费的技术文章，订阅各种免费通讯，并享受 Packt
    书籍和电子书的独家折扣和优惠。
- en: Other Books You May Enjoy
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您可能还会喜欢的其他书籍
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您喜欢这本书，您可能会对 Packt 的以下其他书籍感兴趣：
- en: '[![](img/9781835087718.png)](https://www.packtpub.com/en-in/product/generating-creative-images-with-dall-e-3-9781835089903)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/9781835087718.png)](https://www.packtpub.com/en-in/product/generating-creative-images-with-dall-e-3-9781835089903)'
- en: '**Generating Creative Images With DALL-E 3**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 DALL-E 3 生成创意图像**'
- en: Holly Picano
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Holly Picano
- en: 'ISBN: 9781835087718'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 9781835087718'
- en: Master DALL-E 3’s architecture and training methods
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握 DALL-E 3 的架构和训练方法
- en: Create fine prints and other AI-generated art with precision
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以精确的方式创建精细印刷和其他 AI 生成的艺术作品
- en: Seamlessly blend AI with traditional artistry
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无缝融合人工智能与传统艺术
- en: Address ethical dilemmas in AI art
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决人工智能艺术中的伦理困境
- en: Explore the future of digital creativity
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数字创造力的未来
- en: Implement practical optimization techniques for your artistic endeavors
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的艺术追求实施实用的优化技术
- en: '[![](img/9781835884003.png)](https://www.packtpub.com/en-in/product/building-ai-applications-with-openai-apis-9781835884010)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/9781835884003.png)](https://www.packtpub.com/en-in/product/building-ai-applications-with-openai-apis-9781835884010)'
- en: '**Building AI Applications with OpenAI APIs**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 OpenAI API 构建人工智能应用**'
- en: Martin Yanev
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Martin Yanev
- en: 'ISBN: 9781835884003'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 9781835884003'
- en: Develop a solid foundation in using the OpenAI API for NLP tasks
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用 OpenAI API 进行自然语言处理任务方面打下坚实的基础
- en: Build, deploy, and integrate payments into various desktop and SaaS AI applications
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建、部署并将支付集成到各种桌面和 SaaS 人工智能应用中
- en: Integrate ChatGPT with frameworks such as Flask, Django, and Microsoft Office
    APIs
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 ChatGPT 与 Flask、Django 和 Microsoft Office API 等框架集成
- en: Unleash your creativity by integrating DALL-E APIs to generate stunning AI art
    within your desktop apps
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过集成 DALL-E API 到您的桌面应用程序中，释放您的创造力，生成令人惊叹的 AI 艺术
- en: Experience the power of Whisper API’s speech recognition and text-to-speech
    features
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体验 Whisper API 的语音识别和文本到语音功能
- en: Find out how to fine-tune ChatGPT models for your specific use case
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何针对您的特定用例微调 ChatGPT 模型
- en: Master AI embeddings to measure the relatedness of text strings
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握 AI 嵌入技术以衡量文本字符串的相关性
- en: Packt is searching for authors like you
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Packt 正在寻找像您这样的作者
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](http://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣成为 Packt 的作者，请访问 [authors.packtpub.com](http://authors.packtpub.com) 并今天申请。我们已与成千上万的开发者和技术专业人士合作，就像您一样，帮助他们将见解与全球技术社区分享。您可以提交一般申请，申请我们正在招募作者的特定热门话题，或提交您自己的想法。
- en: Share your thoughts
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Now you’ve finished *Practical Generative AI with ChatGPT, Second Edition*,
    we’d love to hear your thoughts! If you purchased the book from Amazon, please
    [click here to go straight to the Amazon review page](https://packt.link/r/1836647859)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经完成了《Practical Generative AI with ChatGPT, Second Edition》，我们很乐意听听您的想法！如果您在亚马逊购买了这本书，请[点击此处直接进入亚马逊评论页面](https://packt.link/r/1836647859)并分享您的反馈或在该购买网站上留下评论。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您的评论对我们和科技社区都非常重要，并将帮助我们确保我们提供高质量的内容。
