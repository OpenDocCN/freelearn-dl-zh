- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Computer Vision with Convolutional Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积网络进行计算机视觉
- en: In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047) and [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    we set high expectations for **deep learning** (**DL**) and computer vision. First,
    we mentioned the ImageNet competition, and then we talked about some of its exciting
    real-world applications, such as semi-autonomous cars. In this chapter, and the
    next two chapters, we’ll deliver on those expectations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B19627_02.xhtml#_idTextAnchor047)和[*第3章*](B19627_03.xhtml#_idTextAnchor079)中，我们对**深度学习**（**DL**）和计算机视觉设定了很高的期望。首先，我们提到了ImageNet竞赛，然后讨论了它的一些令人兴奋的现实世界应用，例如半自动驾驶汽车。在本章及接下来的两章中，我们将实现这些期望。
- en: Vision is arguably the most important human sense. We rely on it for almost
    any action we take. But image recognition has (and in some ways still is), for
    the longest time, been one of the most difficult problems in computer science.
    Historically, it’s been very difficult to explain to a machine what features make
    up a specified object, and how to detect them. But, as we’ve seen, in DL, a **neural
    network** (**NN**) can learn those features by itself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉可以说是人类最重要的感官。我们几乎在进行的每一个动作中都依赖于它。但图像识别（并且在某些方面仍然是）长期以来一直是计算机科学中最困难的问题之一。历史上，很难向机器解释构成特定对象的特征，以及如何检测它们。但正如我们所见，在深度学习中，**神经网络**（**NN**）可以自己学习这些特征。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Intuition and justification for **convolutional neural** **networks** (**CNNs**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）的直觉和理论依据'
- en: Convolutional layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: The structure of a convolutional network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络的结构
- en: Classifying images with PyTorch and Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch和Keras对图像进行分类
- en: Advanced types of convolutions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积的高级类型
- en: Advanced CNN models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级CNN模型
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and Keras.
    If you don’t have an environment set up with these tools, fret not – the example
    is available as a Jupyter Notebook on Google Colab. You can find the code examples
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch和Keras来实现这个示例。如果你还没有设置这些工具的环境，不必担心——这个示例已经作为Jupyter Notebook在Google
    Colab上提供。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04)。
- en: Intuition and justification for CNNs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的直觉和理论依据
- en: The information we extract from sensory inputs is often determined by their
    context. With images, we can assume that nearby pixels are closely related, and
    their collective information is more relevant when taken as a unit. Conversely,
    we can assume that individual pixels don’t convey information related to each
    other. For example, to recognize letters or digits, we need to analyze the dependency
    of pixels close by because they determine the shape of the element. In this way,
    we could figure out the difference between, say, a 0 or a 1\. The pixels in an
    image are organized in a two-dimensional grid, and if the image isn’t grayscale,
    we’ll have a third dimension for the color channels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从感官输入中提取的信息通常取决于它们的上下文。对于图像，我们可以假设相邻的像素是密切相关的，当将它们作为一个整体来看时，它们的集合信息更为重要。相反，我们可以假设单独的像素并不传递相互之间相关的信息。例如，在识别字母或数字时，我们需要分析相邻像素之间的依赖关系，因为它们决定了元素的形状。通过这种方式，我们能够区分，例如，0和1之间的区别。图像中的像素被组织成二维网格，如果图像不是灰度图，我们还会有一个用于颜色通道的第三维度。
- en: 'Alternatively, a **magnetic resonance image** (**MRI**) also uses three-dimensional
    space. You might recall that, until now, if we wanted to feed an image to an NN,
    we had to reshape it from a two-dimensional array into a one-dimensional array.
    CNNs are built to address this issue: how to make information about units that
    are closer more relevant than information coming from units that are further apart.
    In visual problems, this translates into making units process information coming
    from pixels that are near to one another. With CNNs, we’ll be able to feed one-,
    two-, or three-dimensional inputs and the network will produce an output of the
    same dimensionality. As we’ll see later, this will give us several advantages.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，**磁共振成像**（**MRI**）也使用三维空间。你可能还记得，直到现在，如果我们想要将图像输入到神经网络中，我们必须将它从二维数组转换为一维数组。卷积神经网络就是为了解决这个问题而构建的：如何使得来自更近单元的信息比来自更远单元的信息更相关。在视觉问题中，这意味着让单元处理来自彼此接近的像素的信息。通过
    CNNs，我们将能够输入一维、二维或三维数据，网络也将输出相同维度的数据。正如我们稍后会看到的，这将为我们带来几个优势。
- en: You may recall that at the end of the previous chapter, we successfully classified
    the MNIST images (with around 98% accuracy) using an NN of `airplane`, `automobile`,
    `bird`, `cat`, `deer`, `dog`, `frog`, `horse`, `ship`, and `truck`. Had we tried
    to classify CIFAR-10 with an FC NN with one or more hidden layers, its validation
    accuracy would have been just around 50% (trust me, we did just that in the previous
    edition of this book). Compared to the MNIST result of nearly 98% accuracy, this
    is a dramatic difference, even though CIFAR-10 is also a toy problem. Therefore,
    FC NNs are of little practical use for computer vision problems. To understand
    why, let’s analyze the first hidden layer of our hypothetical CIFAR-10 network,
    which has 1,000 units. The input size of the image is
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在上一章的结尾，我们成功地对 MNIST 图像进行了分类（准确率约为 98%），使用的神经网络包括了`飞机`、`汽车`、`鸟`、`猫`、`鹿`、`狗`、`青蛙`、`马`、`船`和`卡车`。如果我们尝试使用一个具有一个或多个隐藏层的全连接神经网络（FC
    NN）来对 CIFAR-10 进行分类，其验证准确率大概会只有 50%左右（相信我，我们在本书的上一版中确实这么做过）。与接近 98% 准确率的 MNIST
    结果相比，这是一个显著的差异，即使 CIFAR-10 也是一个简单的玩具问题。因此，全连接神经网络对于计算机视觉问题的实际应用价值较小。为了理解原因，我们来分析一下我们假设中的
    CIFAR-10 网络的第一个隐藏层，该层有 1,000 个单元。图像的输入大小是
- en: 32 * 32 * 3 = 3,072\. Therefore, the first hidden layer had a total of 2,072
    * 1,000 = 2,072,000 weights. That’s no small number! Not only is it easy to overfit
    such a large network, but it’s also memory inefficient.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 32 * 32 * 3 = 3,072。因此，第一个隐藏层总共有 2,072 * 1,000 = 2,072,000 个权重。这可不是一个小数字！不仅如此，这么大的网络容易过拟合，而且在内存上也效率低下。
- en: 'Even more important, each input unit (or pixel) is connected to every unit
    in the hidden layer. Because of this, the network cannot take advantage of the
    spatial proximity of the pixels since it doesn’t have a way of knowing which pixels
    are close to each other. In contrast, CNNs have properties that provide an effective
    solution to these problems:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更为重要的是，每个输入单元（或像素）都与隐藏层中的每个单元相连。正因如此，网络无法利用像素的空间接近性，因为它无法知道哪些像素是彼此接近的。相比之下，卷积神经网络（CNNs）具有一些特性，能够有效地解决这些问题：
- en: They connect units that only correspond to neighboring pixels of the image.
    In this way, the units are “forced” to only take input from other units that are
    spatially close. This also reduces the number of weights since not all units are
    interconnected.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们仅连接与图像相邻像素对应的单元。通过这种方式，这些单元被“迫使”只从空间上接近的其他单元那里获取输入。这样也减少了权重的数量，因为并非所有单元都是互相连接的。
- en: CNNs use parameter sharing. In other words, a limited number of weights are
    shared among all units in a layer. This further reduces the number of weights
    and helps fight overfitting. It might sound confusing, but it will become clear
    in the next section.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）使用参数共享。换句话说，层中的所有单元共享有限数量的权重。这进一步减少了权重的数量，并有助于防止过拟合。虽然这可能听起来有些混乱，但在下一节中会变得更加清晰。
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, we’ll discuss CNNs in the context of computer vision, because
    computer vision is their most common application. However, CNNs are successfully
    applied in areas such as speech recognition and **natural language processing**
    (**NLP**). Many of the explanations we’ll describe here are also valid for those
    areas – that is, the principles of CNNs are the same regardless of the field of
    use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在计算机视觉的背景下讨论CNN，因为计算机视觉是其最常见的应用。然而，CNN也成功应用于语音识别和**自然语言处理**（**NLP**）等领域。我们在此描述的许多解释同样适用于这些领域——即，无论应用领域如何，CNN的原理都是相同的。
- en: To understand CNNs, we’ll first discuss their basic building blocks. Once we’ve
    done this, we’ll show you how to assemble them in a full-fledged NN. Then, we’ll
    demonstrate that such a network is good enough to classify the CIFAR-10 with high
    accuracy. Finally, we’ll discuss advanced CNN models, which can be applied to
    real-world computer vision tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解CNN，我们首先讨论它们的基本构建块。一旦完成这部分，我们将展示如何将它们组装成一个完整的神经网络。接着，我们将展示该网络足够好，能够以高精度分类CIFAR-10。最后，我们将讨论高级CNN模型，这些模型可以应用于实际的计算机视觉任务。
- en: Convolutional layers
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: The convolutional layer is the most important building block of a CNN. It consists
    of a set of **filters** (also known as **kernels** or **feature detectors**),
    where each filter is applied across all areas of the input data. A filter is defined
    by a **set of** **learnable weights**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是卷积神经网络（CNN）最重要的组成部分。它由一组**滤波器**（也称为**内核**或**特征检测器**）组成，每个滤波器都应用于输入数据的所有区域。滤波器由一组**可学习的权重**定义。
- en: 'To add some meaning to this laconic definition, we’ll start with the following
    figure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给这个简洁的定义增加一些意义，我们将从以下图开始：
- en: '![Figure 4.1 – Convolution operation start](img/B19627_04_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 卷积操作开始](img/B19627_04_01.jpg)'
- en: Figure 4.1 – Convolution operation start
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 卷积操作开始
- en: The preceding figure shows a two-dimensional input layer of a CNN. For the sake
    of simplicity, we’ll assume that this is the input layer, but it can be any layer
    of the network. We’ll also assume that the input is a grayscale image, and each
    input unit represents the color intensity of a pixel. This image is represented
    by a two-dimensional tensor.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了CNN的二维输入层。为了简化说明，我们假设这是输入层，但它也可以是网络的任何一层。我们还假设输入是一个灰度图像，每个输入单元代表一个像素的颜色强度。这个图像由一个二维张量表示。
- en: 'We’ll start the convolution by applying a 3×3 filter of weights (again, a two-dimensional
    tensor) in the top-left corner of the image. Each input unit is associated with
    a single weight of the filter. It has nine weights, because of the nine input
    units, but, in general, the size is arbitrary (2×2, 4×4, 5×5, and so on). The
    convolution operation is defined as the following weighted sum:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在图像的左上角应用一个3×3权重滤波器（同样是一个二维张量）来开始卷积操作。每个输入单元与滤波器的一个权重相关联。因为有九个输入单元，所以权重有九个，但一般来说，大小是任意的（例如2×2、4×4、5×5，等等）。卷积操作被定义为以下加权和：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:math>](img/289.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:math>](img/289.png)'
- en: Here, *row* and *col* represent the input layer position, where we apply the
    filter (*row=1* and *col=1* in the preceding figure); ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/290.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/291.png)
    are the height and width of the filter size (3×3); *i* and *j* are the filter
    indices of each filter weight, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/292.png);
    *b* is the bias weight. The group of units, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/293.png),
    which participates in the input, is called the **receptive field**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*row* 和 *col* 表示输入层的位置，在此处应用滤波器（*row=1* 和 *col=1* 在前述图中）； ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/290.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/291.png)
    是滤波器大小（3×3）的高度和宽度； *i* 和 *j* 是每个滤波器权重的索引，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml><mml:mi>j</mml></mml:mrow></mml:msub></mml:math>](img/292.png)；
    *b* 是偏置权重。参与输入的单元组，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/293.png)，参与输入的单元组称为**感受野**。
- en: We can see that in a convolutional layer, the unit activation value is defined
    in the same way as the activation value of the unit we defined in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)
    – that is, a weighted sum of its inputs. But here, the unit takes input only from
    a limited number of input units in its immediate surroundings (the receptive field).
    This is opposed to an FC layer, where the input comes from all input units. The
    difference matters because the purpose of the filter is to highlight a specific
    feature in the input, for example, an edge or a line in an image. In the context
    of the NN, the filter output represents the activation value of a unit in the
    next layer. The unit will be active if the feature is present at this spatial
    location. In hierarchically structured data, such as images, neighboring pixels
    form meaningful shapes and objects such as an edge or a line. However, a pixel
    at one end of the image is unlikely to have a relationship with a pixel at another
    end. Because of this, using an FC layer to connect all of the input pixels with
    each output unit is like asking the network to find a needle in a haystack. It
    has no way of knowing whether an input pixel is relevant (in the immediate surroundings)
    to the output unit or not (the other end of the image). Therefore, the limited
    receptive field of the convolutional layer is better suited to highlight meaningful
    features in the input data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在卷积层中，单元的激活值与我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中定义的单元激活值的计算方式相同——即输入的加权和。但在这里，单元的输入仅来自其周围有限数量的输入单元（感受野）。这与全连接（FC）层不同，在全连接层中，输入来自所有输入单元。这个区别很重要，因为滤波器的目的是突出输入中的某个特定特征，比如图像中的边缘或线条。在神经网络的上下文中，滤波器的输出代表下一层单元的激活值。如果该特征在此空间位置存在，单元将处于激活状态。在层次结构的数据中，如图像，邻近像素会形成有意义的形状和物体，如边缘或线条。然而，图像一端的像素与另一端的像素不太可能存在关系。因此，使用全连接层将所有输入像素与每个输出单元连接，就像让网络在大海捞针。它无法知道某个输入像素是否与输出单元相关（是否位于周围区域），还是与图像另一端的像素无关。因此，卷积层有限的感受野更适合突出输入数据中的有意义特征。
- en: 'We’ve calculated the activation of a single unit, but what about the others?
    It’s simple! For each new unit, we’ll slide the filter across the input image,
    and we’ll compute its output (the weighted sum) with each new set of input units.
    The following diagram shows how to compute the activations of the next two positions
    (one pixel to the right):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经计算了一个单元的激活值，但其他单元呢？很简单！对于每个新单元，我们会将滤波器滑动到输入图像上，并计算其输出（加权和），每次使用一组新的输入单元。下图展示了如何计算接下来两个位置的激活值（右移一个像素）：
- en: '![Figure 4.2 – The first three steps of a convolution operation](img/B19627_04_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 卷积操作的前三个步骤](img/B19627_04_02.jpg)'
- en: Figure 4.2 – The first three steps of a convolution operation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 卷积操作的前三个步骤
- en: 'By “slide,” we mean that the weights of the filter don’t change across the
    image. In effect, we’ll use the same nine filter weights and the single bias weight
    to compute the activations of all output units, each time with a different set
    of inputs. We call this **parameter sharing**, and we do it for two reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓“滑动”，是指滤波器的权重在整个图像上保持不变。实际上，我们会使用相同的九个滤波器权重和一个偏置权重来计算所有输出单元的激活值，每次使用不同的输入单元集。我们称之为**参数共享**，并且这样做有两个原因：
- en: By reducing the number of weights, we reduce the memory footprint and prevent
    overfitting.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少权重的数量，我们减少了内存占用并防止了过拟合。
- en: The filter highlights a specific visual feature in the image. We can assume
    that this feature is useful, regardless of its position on the image. Since we
    apply the same filter throughout the image, the convolution is translation invariant;
    that is, it can detect the same feature, regardless of its location on the image.
    However, the convolution is neither rotation-invariant (it is not guaranteed to
    detect a feature if it’s rotated) nor scale-invariant (it is not guaranteed to
    detect the same artifact in different scales).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器突出了图像中的特定视觉特征。我们可以假设该特征是有用的，无论它在图像中的位置如何。由于我们在整个图像中应用相同的滤波器，因此卷积具有平移不变性；也就是说，它可以检测到相同的特征，无论该特征在图像中的位置如何。然而，卷积既不是旋转不变的（如果特征被旋转，它不一定能检测到该特征），也不是尺度不变的（它不能保证在不同的尺度下检测到相同的特征）。
- en: To compute all output activations, we’ll repeat the sliding process until we’ve
    covered the whole input. The spatially arranged input and output units are called
    **depth slices** (**feature maps** or **channels**), implying that there is more
    than one slice. The slices, like the image, are represented by tensors. A slice
    tensor can serve as an input to other layers in the network. Finally, just as
    with regular layers, we can use an activation function, such as the **rectified
    linear unit** (**ReLU**), after each unit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算所有输出激活值，我们将重复滑动过程，直到覆盖整个输入。空间排列的输入和输出单元被称为**深度切片**（**特征图**或**通道**），意味着不仅仅有一个切片。切片和图像一样，是由张量表示的。切片张量可以作为网络中其他层的输入。最后，就像常规层一样，我们可以在每个单元后使用激活函数，如**修正线性单元**（**ReLU**）。
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It’s interesting to note that each input unit is part of the input of multiple
    output units. For example, as we slide the filter, the green unit in the preceding
    diagram will form the input of nine output units.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，每个输入单元都是多个输出单元的输入。例如，当我们滑动滤波器时，上图中的绿色单元将作为九个输出单元的输入。
- en: 'We can illustrate what we’ve learned so far with a simple example. The following
    diagram illustrates a 2D convolution with a 2×2 filter applied over a single 3×3
    slice:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的例子来说明迄今为止所学的内容。以下图示说明了如何对单个3×3切片应用2×2滤波器进行2D卷积：
- en: '![Figure 4.3 – A 2D convolution with a 2×2 ﬁlter applied over a single 3×3
    slice for a 2×2 output slice](img/B19627_04_03.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 使用2×2滤波器对单个3×3切片进行2D卷积，以获得2×2输出切片](img/B19627_04_03.jpg)'
- en: Figure 4.3 – A 2D convolution with a 2×2 ﬁlter applied over a single 3×3 slice
    for a 2×2 output slice
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 使用2×2滤波器对单个3×3切片进行2D卷积，以获得2×2输出切片
- en: This example also shows us that the input and output feature maps have different
    dimensions. Let’s say we have an input layer with a size of `(width_i, height_i)`
    and a filter with dimensions, `(filter_w, filter_h)`. After applying the convolution,
    the dimensions of the output layer are `width_o = width_i - filter_w + 1` and
    `height_o = height_i - filter_h + 1`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子还向我们展示了输入和输出特征图的尺寸是不同的。假设我们有一个大小为`(width_i, height_i)`的输入层和一个尺寸为`(filter_w,
    filter_h)`的滤波器。应用卷积后，输出层的尺寸为`width_o = width_i - filter_w + 1`和`height_o = height_i
    - filter_h + 1`。
- en: In this example, we have `width_o = height_o = 3 – 2 + 1 =` `2`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有`width_o = height_o = 3 – 2 + 1 =` `2`。
- en: In the next section, we’ll illustrate convolutions with a simple coding example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过一个简单的编码示例来说明卷积操作。
- en: A coding example of the convolution operation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积操作的代码示例
- en: 'We’ve now described how convolutional layers work, but we’ll gain better intuition
    with a visual example. Let’s implement a convolution operation by applying a couple
    of filters across an image. For the sake of clarity, we’ll implement the sliding
    of the filters across the image manually and we won’t use any DL libraries. We’ll
    only include the relevant parts and not the full program, but you can find the
    full example in this book’s GitHub repository. Let’s start:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经描述了卷积层是如何工作的，但通过一个可视化的例子，我们会更好地理解。让我们通过对图像应用几个滤波器来实现卷积操作。为了清晰起见，我们将手动实现滤波器在图像上的滑动，且不使用任何深度学习库。我们只会包括相关部分，而不是完整程序，但你可以在本书的GitHub仓库中找到完整示例。让我们开始：
- en: 'Import `numpy`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`：
- en: '[PRE0]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the `conv` function, which applies the convolution across the image.
    `conv` takes two parameters, both two-dimensional numpy arrays: `image`, for the
    pixel intensities of the grayscale image itself, and the hardcoded `im_filter`,
    for the filter:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`conv`函数，它对图像应用卷积。`conv`接受两个参数，都是二维numpy数组：`image`表示灰度图像本身的像素强度，`im_filter`表示硬编码的滤波器：
- en: First, we’ll compute the output image size, which depends on the input `image`
    and `im_filter` sizes. We’ll use it to instantiate the output image, `im_c`.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将计算输出图像的大小，它取决于输入`image`和`im_filter`的大小。我们将利用它来实例化输出图像`im_c`。
- en: 'Then, we’ll iterate over all pixels of `image`, applying `im_filter` at each
    location. This operation requires four nested loops: the first two for the `image`
    dimensions and the second two for iterating over the two-dimensional filter.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将对`image`的所有像素进行迭代，在每个位置应用`im_filter`。此操作需要四个嵌套循环：前两个循环处理`image`的维度，后两个循环用于迭代二维滤波器。
- en: We’ll check if any value is out of the [0, 255] interval and fix it, if necessary.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将检查是否有任何值超出[0, 255]的区间，并在必要时进行修正。
- en: 'This is shown in the following example:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如下例所示：
- en: '[PRE1]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Apply different filters across the image. To better illustrate our point, we’ll
    use a 10×10 blur filter, as well as Sobel edge detectors, as shown in the following
    example (`image_grayscale` is the two-dimensional `numpy` array, which represents
    the pixel intensities of a grayscale image):'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像上应用不同的滤波器。为了更好地说明我们的观点，我们将使用一个10×10的模糊滤波器，以及Sobel边缘检测器，如下例所示（`image_grayscale`是一个二维的`numpy`数组，表示灰度图像的像素强度）：
- en: '[PRE2]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The full program will produce the following output:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整程序将产生以下输出：
- en: '![Figure 4.4 – The ﬁrst image is the grayscale input. The second image is the
    result of a 10×10 blur ﬁlter. The third and fourth images use detectors and vertical
    Sobel edge detectors](img/B19627_04_04.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 第一张图是灰度输入图像。第二张图是10×10模糊滤波器的结果。第三和第四张图使用了检测器和垂直Sobel边缘检测器](img/B19627_04_04.jpg)'
- en: Figure 4.4 – The ﬁrst image is the grayscale input. The second image is the
    result of a 10×10 blur ﬁlter. The third and fourth images use detectors and vertical
    Sobel edge detectors
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 第一张图是灰度输入图像。第二张图是10×10模糊滤波器的结果。第三和第四张图使用了检测器和垂直Sobel边缘检测器
- en: In this example, we used filters with hardcoded weights to visualize how the
    convolution operation works in NNs. In reality, the weights of the filter will
    be set during the network’s training. All we’ll need to do is define the network
    architecture, such as the number of convolutional layers, the depth of the output
    volume, and the size of the filters. The network will figure out the features
    highlighted by each filter during training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了带有硬编码权重的滤波器来可视化卷积操作是如何在神经网络中工作的。实际上，滤波器的权重将在网络训练过程中设置。我们只需要定义网络架构，比如卷积层的数量、输出体积的深度以及滤波器的大小。网络将在训练过程中自动确定每个滤波器突出显示的特征。
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As we saw in this example, we had to implement four nested loops to implement
    the convolution. However, with some clever transformations, the convolution operation
    can be implemented with matrix-matrix multiplication. In this way, it can take
    full advantage of GPU parallelization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这个例子中看到的，我们需要实现四个嵌套循环来实现卷积。然而，通过一些巧妙的转换，卷积操作可以通过矩阵乘法实现。这样，它可以充分利用GPU并行计算。
- en: In the next few sections, we’ll discuss some of the finer details of the convolutional
    layers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论卷积层的一些细节。
- en: Cross-channel and depthwise convolutions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨通道和深度卷积
- en: 'So far, we have described the one-to-one slice relation, where we apply a single
    filter over a single input slice to produce a single output slice. But this arrangement
    is limiting for the following reasons:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了一个一对一的切片关系，其中我们在单个输入切片上应用单个滤波器，产生单个输出切片。但这种安排有以下局限性：
- en: A single input slice works well for a grayscale image, but it doesn’t work for
    color images with multiple channels or any other multi-dimensional input
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个输入切片适用于灰度图像，但对于具有多个通道的彩色图像或任何其他多维输入则不起作用
- en: A single filter can detect a single feature in the slice, but we are interested
    in detecting many different features
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个滤波器可以检测切片中的单个特征，但我们希望检测多个不同的特征
- en: 'How do we solve these limitations? It’s simple:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解决这些局限性呢？很简单：
- en: 'For the input, we’ll split the image into color channels. In the case of an
    RGB image, that would be three. We can think of each color channel as a depth
    slice, where the values are the pixel intensities for the given color (R, G, or
    B), as shown in the following example:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于输入，我们将图像分割成颜色通道。对于RGB图像来说，这将是三个通道。我们可以将每个颜色通道看作一个深度切片，其中的值是给定颜色（R、G或B）的像素强度，如下例所示：
- en: '![Figure 4.5 – An example of an input slice with a depth of 3](img/B19627_04_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 一个深度为3的输入切片示例](img/B19627_04_05.jpg)'
- en: Figure 4.5 – An example of an input slice with a depth of 3
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 一个深度为3的输入切片示例
- en: The combination of input slices is called **input volume** with a **depth**
    of 3\. An RGB image is represented by a 3D tensor of three 2D slices (one slice
    per color channel).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输入切片的组合被称为**输入体积**，深度为3。RGB图像由三层2D切片（每个颜色通道一个）组成的3D张量表示。
- en: The CNN convolution can have multiple filters, highlighting different features,
    which results in multiple output feature maps (one for each filter), combined
    in an **output volume**.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN卷积可以拥有多个滤波器，突出显示不同的特征，从而产生多个输出特征图（每个滤波器一个），这些特征图被合并成一个**输出体积**。
- en: 'Let’s say we have ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/294.png)
    input (uppercase *C*) and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/295.png)
    output slices. Depending on the relationship of the input and output slice, we
    get cross-channel and depthwise convolutions, as illustrated in the following
    diagram:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/294.png)输入（大写的*C*）和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/295.png)输出切片。根据输入和输出切片的关系，我们可以得到跨通道卷积和深度卷积，如下图所示：
- en: '![Figure 4.6 – Cross-channel convolution (left); depthwise convolution (right)](img/B19627_04_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 跨通道卷积（左）；深度卷积（右）](img/B19627_04_06.jpg)'
- en: Figure 4.6 – Cross-channel convolution (left); depthwise convolution (right)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 跨通道卷积（左）；深度卷积（右）
- en: 'Let’s discuss their properties:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论它们的性质：
- en: '**Cross-channel convolutions**: One output slice receives input from all input
    slices (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:math>](img/296.png)
    relationship). With multiple output slices, the relationship becomes ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/297.png).
    In other words, each input slice contributes to the output of each output slice.
    Each pair of input/output slices uses a separate filter slice that’s unique to
    that pair. Let’s denote the index of the input slice with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/298.png)
    (lowercase *c*); the index of the output slice with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/299.png);
    the dimensions of the filter with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/300.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/301.png).
    Then, the cross-channel 2D convolution of a single output cell in one of the output
    slices is defined as the following weighted sum:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨通道卷积**：一个输出切片接收所有输入切片的输入 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:math>](img/296.png)
    关系)。有了多个输出切片，关系变为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/297.png)。换句话说，每个输入切片都为每个输出切片的输出做出贡献。每一对输入/输出切片使用一个独立的过滤器切片，这个过滤器切片对该对切片是独有的。我们用
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/298.png)（小写
    *c*）表示输入切片的索引；用 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/299.png)
    表示输出切片的索引；用 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/300.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/301.png)
    表示过滤器的维度。然后，单个输出切片中一个输出单元的跨通道二维卷积定义为以下加权和：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/302.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/302.png)'
- en: Note that we have a unique bias, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math>](img/303.png)
    for each output slice.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有一个独特的偏置，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math>](img/303.png)
    每个输出切片都有一个。
- en: 'We can also compute the total number of weights, *W*, in a cross-channel 2D
    convolution with the following equation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下公式计算交叉通道 2D 卷积中的权重总数 *W*：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/304.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/304.png)'
- en: Here, *+1* represents the bias weight for each filter. Let’s say we have three
    input slices and want to apply four 5×5 filters to them. If we did this, the convolution
    filter would have a total of (3 * 5 * 5 + 1) * 4 = 304 weights, four output slices
    (an output volume with a depth of 4), and one bias per slice. The filter for each
    output slice will have three 5×5 filter patches for each of the three input slices
    and one bias for a total of 3 * 5 * 5 + 1 = 76 weights.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*+1* 表示每个滤波器的偏置权重。假设我们有三个输入切片，并且想要应用四个 5×5 的滤波器。如果这样做，卷积滤波器将有总共 (3 * 5 *
    5 + 1) * 4 = 304 个权重，四个输出切片（深度为 4 的输出体积），每个切片有一个偏置。每个输出切片的滤波器将有三个 5×5 的滤波器块，分别对应三个输入切片，并且有一个偏置，总共有
    3 * 5 * 5 + 1 = 76 个权重。
- en: '**Depthwise convolutions**: One output slice receives input from a single input
    slice. It’s a kind of reversal of the previous case. In its simplest form, we
    apply a filter over a single input slice to produce a single output slice. In
    this case, the input and output volumes have the same depth – that is, *C*. We
    can also specify a channel multiplier (an integer, *M*), where we apply *M* filters
    over a single output slice to produce *M* output slices per input slice. In this
    case, the total number of output slices is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:math>](img/305.png).
    The depthwise 2D convolution is defined as the following weighted sum:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积**：一个输出切片仅接收来自单个输入切片的信息。这是对前述情况的某种反转。在最简单的形式下，我们对单个输入切片应用滤波器，生成一个输出切片。在这种情况下，输入和输出的体积具有相同的深度——即
    *C*。我们还可以指定一个通道倍增器（一个整数 *M*），即对单个输出切片应用 *M* 个滤波器，生成每个输入切片的 *M* 个输出切片。在这种情况下，输出切片的总数为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:math>](img/305.png)。深度卷积
    2D 被定义为以下加权和：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/306.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/306.png)'
- en: 'We can compute the number of weights, *W*, in a 2D depthwise convolution with
    the following formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下公式计算二维深度卷积中的权重*W*：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/307.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/307.png)'
- en: Here, *+M* represents the biases of each output slice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*+M* 代表每个输出片的偏置。
- en: Next, we’ll discuss some more properties of the convolution operation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论卷积操作的一些更多属性。
- en: Stride and padding in convolutional layers
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层中的步幅和填充
- en: 'So far, we’ve assumed that sliding of the filter happens one pixel at a time,
    but that’s not always the case. We can slide the filter over multiple positions.
    This parameter of the convolutional layers is called **stride**. Usually, the
    stride is the same across all dimensions of the input. In the following diagram,
    we can see a convolutional layer with *stride = 2* (also called **stride convolution**):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设滤波器每次滑动一个像素，但并非总是如此。我们可以让滤波器滑动多个位置。卷积层的这个参数叫做**步幅**。通常，步幅在输入的所有维度上是相同的。在下图中，我们可以看到一个*步幅
    = 2*的卷积层（也叫做**步幅卷积**）：
- en: '![Figure 4.7 – With stride = 2, the ﬁlter is translated by two pixels at a
    time](img/B19627_04_07.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 步幅为 2 时，滤波器每次移动两个像素](img/B19627_04_07.jpg)'
- en: Figure 4.7 – With *stride = 2*, the ﬁlter is translated by two pixels at a time
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – *步幅 = 2* 时，滤波器每次移动两个像素
- en: 'The main effect of the larger stride is an increase in the receptive field
    of the output units at the expense of the size of the output slice itself. To
    understand this, let’s recall that in the previous section, we introduced a simple
    formula for the output size, which included the sizes of the input and the kernel.
    Now, we’ll extend it to also include the stride: `width_o = (width_i - filter_w)
    / stride_w + 1` and `height_o = 1 + (height_i - filter_h) / stride_h`. For example,
    the output size of a square slice generated by a 28×28 input image, convolved
    with a 3×3 filter with *stride = 1*, would be 1 + 28 - 3 = 26\. But with *stride
    = 2*, we get 1 + (28 - 3) / 2 = 13\. Therefore, if we use *stride = 2*, the size
    of the output slice will be roughly four times smaller than the input. In other
    words, one output unit will “cover” an area, which is four times larger compared
    to the input units. The units in the following layers will gradually capture input
    from larger regions from the input image. This is important because it would allow
    them to detect larger and more complex features of the input.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更大步幅的主要效果是增加输出单元的感受野，代价是输出切片的大小本身会变小。为了理解这一点，我们回顾一下在上一节中，我们介绍了一个输出大小的简单公式，其中包括输入和卷积核的大小。现在，我们将它扩展到包括步幅：`width_o
    = (width_i - filter_w) / stride_w + 1` 和 `height_o = 1 + (height_i - filter_h)
    / stride_h`。例如，一个由28×28的输入图像生成的方形切片，与一个3×3的滤波器进行卷积，且*步幅 = 1*，输出大小为 1 + 28 - 3
    = 26。 但是如果*步幅 = 2*，则为 1 + (28 - 3) / 2 = 13。因此，如果我们使用*步幅 = 2*，输出切片的大小大约是输入的四分之一。换句话说，一个输出单元将“覆盖”一个面积，比输入单元大四倍。接下来层的单元将逐渐捕捉来自输入图像更大区域的输入信息。这一点非常重要，因为它将允许它们检测输入图像中更大、更复杂的特征。
- en: 'The convolution operations we have discussed so far have produced smaller output
    than the input (even with *stride = 1*). But, in practice, it’s often desirable
    to control the size of the output. We can solve this by **padding** the edges
    of the input slice with rows and columns of zeros before the convolution operation.
    The most common way to use padding is to produce output with the same dimensions
    as the input. In the following diagram, we can see a convolutional layer with
    *padding = 1* and *stride =* *1*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的卷积操作产生的输出小于输入（即使*步幅 = 1*）。但是，在实践中，通常希望控制输出的大小。我们可以通过**填充**输入切片的边缘，用零的行和列进行填充，来解决这个问题。在以下图示中，我们可以看到一个填充大小为1且*步幅
    = 1*的卷积层：
- en: '![Figure 4.8 – A convolutional layer with padding = 1](img/B19627_04_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 填充大小为 1 的卷积层](img/B19627_04_08.png)'
- en: Figure 4.8 – A convolutional layer with padding = 1
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 填充大小为 1 的卷积层
- en: The white units represent the padding. The input and the output slices have
    the same dimensions (dark units). The newly padded zeros will participate in the
    convolution operation with the slice, but they won’t affect the result. The reason
    is that, even though the padded areas are connected with weights to the following
    layer, we’ll always multiply those weights by the padded value, which is 0\. At
    the same time, sliding the filter across the padded input slice will produce an
    output slice with the same dimensions as the unpadded input.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 白色单元表示填充。输入和输出切片的维度相同（深色单元）。新填充的零将在卷积操作中与切片一起参与，但它们不会影响结果。原因是，即使填充区域与权重连接到下一层，我们始终会将这些权重乘以填充值，而填充值为0。同时，滑动滤波器经过填充的输入切片时，会产生与未填充输入相同维度的输出切片。
- en: 'Now that we know about stride and padding, we can introduce the full formula
    for the size of the output slice:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了步幅和填充的概念，我们可以引入输出切片大小的完整公式：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now have a basic knowledge of convolutions, and we can continue to the next
    building block of CNNs – the pooling layer. Once we know all about pooling layers,
    we’ll introduce our first full CNN, and we’ll implement a simple task to solidify
    our knowledge. Then, we’ll focus on more advanced CNN topics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经对卷积有了基本的了解，可以继续学习卷积神经网络（CNN）的下一个基本组成部分——池化层。一旦我们了解了池化层的原理，我们将介绍第一个完整的CNN，并通过实现一个简单的任务来巩固我们的知识。接着，我们将聚焦于更高级的CNN话题。
- en: Pooling layers
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: 'In the previous section, we explained how to increase the receptive field of
    the units by using *stride > 1*. But we can also do this with the help of pooling
    layers. A pooling layer splits the input slice into a grid, where each grid cell
    represents a receptive field of several units (just as a convolutional layer does).
    Then, a pooling operation is applied over each cell of the grid. Pooling layers
    don’t change the volume depth because the pooling operation is performed independently
    on each slice. They are defined by two parameters: stride and receptive field
    size, just like convolutional layers (pooling layers usually don’t use padding).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们解释了如何通过使用*步长 > 1*来增大单元的感受野。但我们也可以借助池化层来实现这一点。池化层将输入切片分割成一个网格，每个网格单元代表多个单元的感受野（就像卷积层一样）。然后，在网格的每个单元上应用池化操作。池化层不会改变卷积深度，因为池化操作是独立地在每个切片上进行的。池化层由两个参数定义：步长和感受野大小，就像卷积层一样（池化层通常不使用填充）。
- en: 'In this section, we’ll discuss three types of pooling layers – max pooling,
    average pooling, and **global average pooling** (**GAP**). These three types of
    pooling are displayed in the following diagram:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论三种类型的池化层——最大池化、平均池化和**全局平均池化**（**GAP**）。这三种池化类型在下图中有所展示：
- en: '![Figure 4.9 – Max, average, and global average pooling](img/B19627_04_09.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 最大池化、平均池化和全局平均池化](img/B19627_04_09.jpg)'
- en: Figure 4.9 – Max, average, and global average pooling
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 最大池化、平均池化和全局平均池化
- en: '**Max pooling** is the most common way of pooling. The max pooling operation
    takes the unit with the highest activation value in each local receptive field
    (grid cell) and propagates only that value forward. In the preceding figure (left),
    we can see an example of max pooling with a receptive field of 2×2 and *stride
    = 2*. This operation discards 3/4 of the input units. Pooling layers don’t have
    any weights. In the backward pass of max pooling, the gradient is routed only
    to the unit with the highest activation during the forward pass. The other units
    in the receptive field backpropagate zeros.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大池化**是最常见的池化方法。最大池化操作会选择每个局部感受野（网格单元）中激活值最高的单元，并仅将该值传递给下一层。在上面的图示（左图）中，我们可以看到一个2×2感受野和*步长
    = 2*的最大池化示例。该操作丢弃了输入单元的3/4。池化层没有权重。在最大池化的反向传播过程中，梯度只会传递给前向传播时激活值最高的单元。其他单元的梯度为零。'
- en: '**Average pooling** is another type of pooling, where the output of each receptive
    field is the mean value of all activations within the field. In the preceding
    figure (middle), we can see an example of average pooling with a receptive field
    of 2×2 and *stride =* *2*.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均池化**是另一种池化方式，其中每个感受野的输出是该区域内所有激活值的均值。在上面的图示（中图）中，我们可以看到一个2×2感受野和*步长 = 2*的平均池化示例。'
- en: GAP is similar to average pooling, but a single pooling region covers the whole
    input slice. We can think of GAP as an extreme type of dimensionality reduction
    because it outputs a single value that represents the average of the whole slice.
    This type of pooling is usually applied at the end of the convolutional portion
    of a CNN. In the preceding figure (right), we can see an example of a GAP operation.
    Stride and receptive field size don’t apply to the GAP operation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GAP与平均池化相似，但一个池化区域覆盖整个输入切片。我们可以将GAP视为一种极端的降维方法，因为它输出一个代表整个切片均值的单一值。这种池化方式通常应用于CNN的卷积部分结束时。在上面的图示（右图）中，我们可以看到GAP操作的示例。步长和感受野大小不适用于GAP操作。
- en: In practice, only two combinations of stride and receptive field size are used.
    The first is a 2×2 receptive field with *stride = 2*, and the second is a 3×3
    receptive field with *stride = 2* (overlapping). If we use a larger value for
    either parameter, the network loses too much information. Alternatively, if the
    stride is 1, the size of the layer wouldn’t be smaller, and nor will the receptive
    field increase.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，通常只有两种步长和感受野大小的组合被使用。第一种是 2×2 的感受野，*步长 = 2*，第二种是 3×3 的感受野，*步长 = 2*（重叠）。如果我们为任一参数使用较大的值，网络将丧失过多的信息。或者，如果步长为
    1，层的大小不会变小，感受野也不会增大。
- en: 'Based on these parameters, we can compute the output size of a pooling layer:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些参数，我们可以计算池化层的输出大小：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Pooling layers are still very much used, but often, we can achieve similar
    or better results by simply using convolutional layers with larger strides. (See,
    for example, *J. Springerberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, Striving
    for Simplicity: The All Convolutional Net, (**2015)*, [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '池化层仍然被广泛使用，但通常，我们可以通过简单地使用步长较大的卷积层来实现相似甚至更好的结果。（例如，参见 *J. Springerberg, A.
    Dosovitskiy, T. Brox, 和 M. Riedmiller, Striving for Simplicity: The All Convolutional
    Net, (**2015)*, [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).）'
- en: We now have sufficient knowledge to introduce our first full CNN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有足够的知识来介绍我们的第一个完整的 CNN。
- en: The structure of a convolutional network
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积网络的结构
- en: 'The following figure shows the structure of a basic classification CNN:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个基本分类 CNN 的结构：
- en: '![Figure 4.10 – A basic convolutional network with convolutional, FC, and pooling
    layers](img/B19627_04_10.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 一个基本的卷积网络，包含卷积层、全连接层和池化层](img/B19627_04_10.jpg)'
- en: Figure 4.10 – A basic convolutional network with convolutional, FC, and pooling
    layers
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 一个基本的卷积网络，包含卷积层、全连接层和池化层
- en: 'Most CNNs share basic properties. Here are some of them:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 CNN 共享一些基本特性。以下是其中的一些：
- en: 'We would typically alternate one or more convolutional layers with one pooling
    layer (or a stride convolution). In this way, the convolutional layers can detect
    features at every level of the receptive field size. The aggregated receptive
    field size of deeper layers is larger than the ones at the beginning of the network.
    This allows them to capture more complex features from larger input regions. Let’s
    illustrate this with an example. Imagine that the network uses 3×3 convolutions
    with *stride = 1* and 2×2 pooling with *stride =* *2*:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通常会将一个或多个卷积层与一个池化层（或步长卷积）交替使用。通过这种方式，卷积层可以在每个感受野的层级上检测特征。更深层的感受野的聚合大小大于网络开始时的感受野，这使得它们能够从更大的输入区域中捕捉到更复杂的特征。我们通过一个例子来说明这一点。假设网络使用
    3×3 的卷积，*步长 = 1*，以及 2×2 的池化，*步长 = 2*：
- en: The units of the first convolutional layer will receive input from 3×3 pixels
    of the image.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一卷积层的单元将接收来自图像的 3×3 像素输入。
- en: A group of 2×2 output units of the first layer will have a combined receptive
    field size of 4×4 (because of the stride).
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层的 2×2 输出单元组的合并感受野大小为 4×4（由于步长的原因）。
- en: After the first pooling operation, this group will be combined in a single unit
    of the pooling layer.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一次池化操作后，这些组将合并成池化层的一个单元。
- en: The second convolution operation takes input from 3×3 pooling units. Therefore,
    it will receive input from a square with side 3×4 = 12 (or a total of 12×12 =
    144) pixels from the input image.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二次卷积操作将从 3×3 的池化单元接收输入。因此，它将接收来自一个边长为 3×4 = 12（或总共 12×12 = 144）像素的方形区域的输入图像。
- en: We use the convolutional layers to extract features from the input. The features
    detected by the deepest layers are highly abstract, but they are also not readable
    by humans. To solve this problem, we usually add one or more FC layers after the
    last convolutional/pooling layer. In this example, the last FC layer (output)
    will use softmax to estimate the class probabilities of the input. You can think
    of the FC layers as translators between the network’s language (which we don’t
    understand) and ours.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用卷积层从输入中提取特征。最深层检测到的特征非常抽象，但它们也不适合人类阅读。为了解决这个问题，我们通常会在最后一个卷积/池化层后添加一个或多个全连接层。在这个例子中，最后一个全连接层（输出层）将使用
    softmax 来估算输入的类别概率。你可以把全连接层看作是网络语言（我们无法理解）和我们语言之间的翻译器。
- en: The deeper convolutional layers usually have more filters (hence higher volume
    depth), compared to the initial ones. A feature detector at the beginning of the
    network works on a small receptive field. It can only detect a limited number
    of features, such as edges or lines, shared among all classes. On the other hand,
    a deeper layer would detect more complex and numerous features. For example, if
    we have multiple classes such as cars, trees, or people, each will have its own
    set of features, such as tires, doors, leaves and faces, and so on. This would
    require more feature detectors.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与初始卷积层相比，较深的卷积层通常具有更多的滤波器（因此卷积深度更大）。网络开始时的特征检测器在较小的感受野上工作。它只能检测到有限数量的特征，例如在所有类别中共享的边缘或线条。另一方面，较深的层则能够检测到更复杂和更多的特征。例如，如果我们有多个类别，如汽车、树木或人物，每个类别都会有一组独特的特征，如轮胎、车门、树叶和面孔等。这就需要更多的特征检测器。
- en: Now that we know the structure of a CNN, let’s implement one with PyTorch and
    Keras.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 CNN 的结构，接下来让我们用 PyTorch 和 Keras 实现一个 CNN。
- en: Classifying images with PyTorch and Keras
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 和 Keras 分类图像
- en: 'In this section, we’ll try to classify the images of the CIFAR-10 dataset with
    both PyTorch and Keras. It consists of 60,000 32x32 RGB images, divided into 10
    classes of objects. To understand these examples, we’ll first focus on two prerequisites
    that we haven’t covered until now: how images are represented in DL libraries
    and data augmentation training techniques.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试用 PyTorch 和 Keras 分类 CIFAR-10 数据集的图像。它由 60,000 张 32x32 的 RGB 图像组成，分为
    10 个类别的物体。为了理解这些示例，我们将首先关注到目前为止我们还没有覆盖的两个前提条件：图像在深度学习库中的表示方式和数据增强训练技术。
- en: Convolutional layers in deep learning libraries
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习库中的卷积层
- en: 'PyTorch, Keras, and **TensorFlow** (**TF**) have out-of-the-gate support for
    1D, 2D, and 3D convolutions. The inputs and outputs of the convolution operation
    are tensors. A 1D convolution with multiple input/output slices would have 3D
    input and output tensors. Their axes can be in either *SCW* or *SWC* order, where
    we have the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch、Keras 和 **TensorFlow**（**TF**）支持 1D、2D 和 3D 卷积。卷积操作的输入和输出是张量。一个具有多个输入/输出切片的
    1D 卷积将有 3D 的输入和输出张量。它们的轴可以是 *SCW* 或 *SWC* 顺序，我们有如下定义：
- en: '*S*: The index of the sample in the mini-batch'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S*：小批量中的样本索引'
- en: '*C*: The index of the depth slice in the volume'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*：深度切片在体积中的索引'
- en: '*W*: The content of the slice'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*：切片的内容'
- en: In the same way, a 2D convolution will be represented by *SCHW* or *SHWC* ordered
    tensors, where *H* and *W* are the height and width of the slices. A 3D convolution
    will have *SCDHW* or *SDHWC* order, where *D* stands for the depth of the slice.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，一个 2D 卷积将由 *SCHW* 或 *SHWC* 顺序的张量表示，其中 *H* 和 *W* 分别是切片的高度和宽度。一个 3D 卷积将有 *SCDHW*
    或 *SDHWC* 顺序，其中 *D* 代表切片的深度。
- en: Data augmentation
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'One of the most efficient regularization techniques is data augmentation. If
    the training data is too small, the network might start overfitting. Data augmentation
    helps counter this by artificially increasing the size of the training set. In
    the CIFAR-10 examples, we’ll train a CNN over multiple epochs. The network will
    “see” every sample of the dataset once per epoch. To prevent this, we can apply
    random augmentations to the images, before feeding them to train the CNN. The
    labels will stay the same. Some of the most popular image augmentations are as
    follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的正则化技术之一就是数据增强。如果训练数据太小，网络可能会开始过拟合。数据增强通过人工增加训练集的大小，帮助防止这种情况。在 CIFAR-10 示例中，我们将训练一个
    CNN，并进行多轮训练。网络每轮都会“看到”数据集中的每个样本。为了防止这种情况，我们可以在将图像输入到 CNN 训练之前，先对它们进行随机增强，标签保持不变。以下是一些最常用的图像增强方法：
- en: Rotation
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转
- en: Horizontal and vertical flip
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平和垂直翻转
- en: Zoom in/out
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 放大/缩小
- en: Crop
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裁剪
- en: Skew
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏斜
- en: Contrast and brightness adjustment
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比度和亮度调整
- en: 'The emboldened augmentations are shown in the following example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 加粗的增强方法将在以下示例中展示：
- en: '![Figure 4.11 – Examples of diﬀerent image augmentations](img/B19627_04_11.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 不同图像增强的示例](img/B19627_04_11.jpg)'
- en: Figure 4.11 – Examples of diﬀerent image augmentations
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 不同图像增强的示例
- en: With that, we’re ready to proceed with the examples.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以继续进行示例了。
- en: Classifying images with PyTorch
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch 分类图像
- en: 'We’ll start with PyTorch first:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从 PyTorch 开始：
- en: 'Select the device, preferably a GPU. This NN is larger than the MNIST ones
    and the CPU training would be very slow:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择设备，最好是 GPU。这个神经网络比 MNIST 的网络要大，使用 CPU 训练会非常慢：
- en: '[PRE5]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Load the training dataset (followed by the validation):'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练数据集（然后是验证数据集）：
- en: '[PRE6]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`train_transform` is of particular interest. It performs random horizontal
    and vertical flips, and it normalizes the dataset with `transforms.Normalize`
    using z-score normalization. The hardcoded numerical values represent the manually
    computed channel-wise mean and `std` values for the CIFAR-10 dataset. `train_loader`
    takes care of providing training minibatches.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`train_transform` 是特别重要的。它执行随机水平和垂直翻转，并使用 `transforms.Normalize` 通过 z-score
    标准化来规范化数据集。这些硬编码的数值表示 CIFAR-10 数据集手动计算的逐通道均值和 `std` 值。`train_loader` 负责提供训练的小批量数据。'
- en: 'Load the validation dataset. Note that we normalize the validation set with
    the mean and `std` values of the training dataset:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载验证数据集。请注意，我们使用训练数据集的均值和 `std` 值对验证集进行标准化：
- en: '[PRE7]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define our CNN using the `Sequential` class. It has the following properties:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Sequential` 类定义我们的 CNN。它具有以下特性：
- en: Three blocks of two convolutional layers (3×3 filters) and one max pooling layer.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个块，每个块由两个卷积层（3×3 滤波器）和一个最大池化层组成。
- en: Batch normalization after each convolutional layer.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层后进行批量归一化。
- en: The first two blocks apply `padding=1` to the convolutions, so they don’t decrease
    the size of the feature maps.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个块对卷积操作应用 `padding=1`，因此不会减少特征图的尺寸。
- en: '`Linear` (FC) layer with 10 outputs (one of each class). The final activation
    is softmax.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Linear`（全连接）层有 10 个输出（每个类别一个）。最终激活函数是 softmax。'
- en: 'Let’s see the definition:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看定义：
- en: '[PRE8]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Run the training and validation. We’ll use the same `train_model` and `test_model`
    functions that we implemented in the MNIST PyTorch example in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
    Because of this, we won’t implement them here, but the full source code is available
    in this chapter’s GitHub repository (including a Jupyter Notebook). We can expect
    the following results: 51% accuracy in 1 epoch, 70% accuracy in 5 epochs, and
    around 82% accuracy in 75 epochs.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行训练和验证。我们将使用与 [*第 3 章*](B19627_03.xhtml#_idTextAnchor079) 中的 MNIST PyTorch
    示例中实现的相同的 `train_model` 和 `test_model` 函数。因此，我们在此不会实现它们，但完整的源代码可以在本章的 GitHub 仓库中找到（包括
    Jupyter Notebook）。我们可以期待以下结果：在 1 轮时准确率为 51%，在 5 轮时准确率为 70%，在 75 轮时准确率约为 82%。
- en: This concludes our PyTorch example.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的 PyTorch 示例的结尾。
- en: Classifying images with Keras
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Keras 进行图像分类
- en: 'Our second example is the same task, but this time implemented with Keras:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个示例是相同的任务，但这次使用 Keras 实现：
- en: 'Start by downloading the dataset. We’ll also convert the numerical labels into
    one-hot-encoded tensors:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先下载数据集。我们还将把数字标签转换为独热编码的张量：
- en: '[PRE9]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create an instance of `ImageDataGenerator`, which applies z-normalization over
    each channel of the training set images. It also provides data augmentation (random
    horizontal and vertical flips) during training. Also, note that we apply the mean
    and standard variation of the training over the test set for the best performance:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `ImageDataGenerator` 实例，它对训练集图像的每个通道应用 z 标准化。在训练过程中，它还提供数据增强（随机水平和垂直翻转）。另外，请注意，我们将训练集的均值和标准差应用于测试集，以获得最佳性能：
- en: '[PRE10]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we can define our CNN using the `Sequential` class. We’ll use the same
    architecture we defined in the *Classifying images with PyTorch* section. The
    following is the Keras definition of that model:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `Sequential` 类定义我们的 CNN。我们将使用在 *使用 PyTorch 进行图像分类* 部分中定义的相同架构。以下是该模型的
    Keras 定义：
- en: '[PRE11]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the training parameters (we’ll also print the model summary for clarity):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练参数（我们还将打印模型总结以便于理解）：
- en: '[PRE12]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the training for 50 epochs:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 50 轮的训练：
- en: '[PRE13]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Depending on the number of epochs, this model will produce the following results:
    50% accuracy in 1 epoch, 72% accuracy in 5 epochs, and around 85% accuracy in
    45 epochs. Our Keras example has slightly higher accuracy compared to the one
    in PyTorch, although they should be identical. Maybe we’ve got a bug somewhere.
    We might never know, but we can learn a lesson, nevertheless: ML models aren’t
    easy to debug because they can fail with slightly degraded performance, instead
    of outright error. Finding the exact reason for this performance penalty can be
    hard.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 根据轮数的不同，这个模型将产生以下结果：在 1 轮时准确率为 50%，在 5 轮时准确率为 72%，在 45 轮时准确率约为 85%。我们的 Keras
    示例与 PyTorch 示例相比，准确率略高，尽管它们应该是相同的。也许我们在某处有一个 bug。我们可能永远不会知道，但我们仍然能从中学到一课：机器学习模型并不容易调试，因为它们可能只是性能稍微下降，而不是完全报错。找到这种性能下降的确切原因可能很难。
- en: Now that we’ve implemented our first full CNN twice, we’ll focus on some more
    advanced types of convolutions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经实现了第一次完整的 CNN 两次，我们将重点讨论一些更高级的卷积类型。
- en: Advanced types of convolutions
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级卷积类型
- en: So far, we’ve discussed the “classic” convolutional operation. In this section,
    we’ll introduce several new variations and their properties.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了“经典”卷积操作。在本节中，我们将介绍几种新的变体及其属性。
- en: 1D, 2D, and 3D convolutions
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1D、2D 和 3D 卷积
- en: 'In this chapter, we’ve used **2D convolutions** because computer vision with
    two-dimensional images is the most common CNN application. But we can also have
    1D and 3D convolutions, where the units are arranged in one-dimensional or three-dimensional
    space, respectively. In all cases, the filter has the same number of dimensions
    as the input, and the weights are shared across the input. For example, we would
    use 1D convolution with time series data because the values are arranged across
    a single time axis. In the following diagram, on the left, we can see an example
    of 1D convolution:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了**2D 卷积**，因为二维图像的计算机视觉是最常见的 CNN 应用。但我们也可以有 1D 和 3D 卷积，其中单位分别排列在一维或三维空间中。在所有情况下，滤波器的维度与输入的维度相同，并且权重在输入上是共享的。例如，我们会对时间序列数据使用
    1D 卷积，因为这些值是沿单一时间轴排列的。在下图的左侧，我们可以看到一个 1D 卷积的示例：
- en: '![Figure 4.12 –1D convolution (left); 3D convolution (right)](img/B19627_04_12.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 1D 卷积（左）；3D 卷积（右）](img/B19627_04_12.jpg)'
- en: Figure 4.12 –1D convolution (left); 3D convolution (right)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 1D 卷积（左）；3D 卷积（右）
- en: The weights with the same dashed lines share the same value. The output of the
    1D convolution is also 1D. If the input is 3D, such as a 3D MRI, we could use
    3D convolution, which will also produce 3D output. In this way, we’ll maintain
    the spatial arrangement of the input data. We can see an example of 3D convolution
    in the preceding diagram, on the right. The input has dimensions of H/W/L, and
    the filter has a single size, *F*, for all dimensions. The output is also 3D.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 相同虚线的权重共享相同的值。1D 卷积的输出也是 1D。如果输入是 3D，例如 3D MRI，我们可以使用 3D 卷积，它也会产生 3D 输出。这样，我们可以保持输入数据的空间排列。在上面的图示中，我们可以看到右侧的
    3D 卷积示例。输入的维度是 H/W/L，滤波器在所有维度上有一个单一的大小，*F*。输出也是 3D。
- en: 1×1 convolutions
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1×1 卷积
- en: A 1×1 (pointwise) convolution is a special case of convolution where each dimension
    of the convolution filter is of size 1 (1×1 in 2D convolutions and 1×1×1 in 3D).
    At first, this doesn’t make sense – a 1×1 filter doesn’t increase the receptive
    field size of the output units. The result of such a convolution would be pointwise
    scaling. But it can be useful in another way – we can use them to change the depth
    between the input and output volumes. To understand this, let’s recall that, in
    general, we have an input volume with a depth of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/308.png)
    slices and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/309.png)
    filters for ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/309.png)
    output slices. Each output slice is generated by applying a unique filter over
    all the input slices. If we use a 1×1 filter and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/311.png),
    we’ll have output slices of the same size, but with different volume depths. At
    the same time, we won’t change the receptive field size between the input and
    output. The most common use case is to reduce the output volume, or ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>></mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/312.png)
    (dimension reduction), nicknamed the **“****bottleneck” layer**.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 1×1（点卷积）卷积是卷积的一种特殊情况，其中卷积滤波器的每个维度大小为 1（在 2D 卷积中是 1×1，在 3D 卷积中是 1×1×1）。起初，这看起来没有意义——一个
    1×1 的滤波器并不会增加输出单元的感受野大小。这样的卷积结果只是逐点缩放。但它在另一个方面可能会有用——我们可以用它们来改变输入和输出体积之间的深度。为了理解这一点，让我们回顾一下，通常我们有一个深度为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/308.png)
    的输入体积和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/309.png)
    的滤波器，生成 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/309.png)
    输出体积的切片。每个输出切片都是通过对所有输入切片应用一个独特的滤波器来生成的。如果我们使用 1×1 滤波器且 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/311.png)，我们会得到相同大小的输出切片，但深度不同。同时，输入和输出之间的感受野大小不会发生改变。最常见的用例是减少输出体积，或者
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>></mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/312.png)（降维），也被称为**“瓶颈层”**。
- en: Depthwise separable convolutions
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: An output slice in a cross-channel convolution receives input from all of the
    input slices using a single filter. The filter tries to learn features in a 3D
    space, where two of the dimensions are spatial (the height and width of the slice)
    and the third is the channel. Therefore, the filter maps both spatial and cross-channel
    correlations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨通道卷积中，一个输出切片从所有输入切片中接收输入，并使用一个滤波器。滤波器试图学习 3D 空间中的特征，其中两个维度是空间的（切片的高度和宽度），第三个维度是通道。因此，滤波器同时映射空间和跨通道的相关性。
- en: '**Depthwise separable convolutions** (**DSCs**, *Xception: Deep Learning with
    Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    can completely decouple'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度可分离卷积**（**DSC**，*Xception: 深度学习与深度可分离卷积*， [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)）可以完全解耦'
- en: 'cross-channel and spatial correlations. A DSC combines two operations: a depthwise
    convolution and a 1×1 convolution. In a depthwise convolution, a single input
    slice produces a single output slice, so it only maps spatial (and not cross-channel)
    correlations. With 1×1 convolutions, we have the opposite. The following diagram
    represents the DSC:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 跨通道和空间相关性。深度可分离卷积（DSC）结合了两种操作：深度卷积和 1×1 卷积。在深度卷积中，单个输入切片生成单个输出切片，因此它只映射空间相关性（而非跨通道相关性）。而在
    1×1 卷积中，则恰好相反。以下图示表示 DSC：
- en: '![Figure 4.13 – A depth-wise separable convolution](img/B19627_04_13.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – 一种深度可分离卷积](img/B19627_04_13.jpg)'
- en: Figure 4.13 – A depth-wise separable convolution
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 一种深度可分离卷积
- en: The DSC is usually implemented without non-linearity after the first (depthwise)
    operation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积（DSC）通常在第一次（深度方向）操作后不加非线性处理。
- en: Note
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Let’s compare the standard and depthwise separable convolutions. Imagine that
    we have 32 input and output channels and a filter with a size of 3×3\. In a standard
    convolution, one output slice is the result of applying one filter for each of
    the 32 input slices for a total of 32 * 3 * 3 = 288
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较标准卷积和深度可分离卷积。假设我们有 32 个输入和输出通道，滤波器大小为 3×3。在标准卷积中，一个输出切片是将每个 32 个输入切片应用一个滤波器的结果，总共是
    32 * 3 * 3 = 288
- en: weights (excluding bias). In a comparable depthwise convolution, the filter
    has only 3 * 3 = 9 weights and the filter for the 1×1 convolution has 32 * 1 *
    1 = 32 weights. The total number of weights is 32 + 9 = 41\. Therefore, the depthwise
    separable convolution is faster and more memory-efficient compared to the standard
    one.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 权重（不包括偏差）。在一个类似的深度方向卷积中，滤波器只有 3 * 3 = 9 个权重，而 1×1 卷积的滤波器则有 32 * 1 * 1 = 32 个权重。权重的总数为
    32 + 9 = 41。因此，与标准卷积相比，深度可分离卷积速度更快，且内存效率更高。
- en: Dilated convolutions
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 膨胀卷积
- en: 'The regular convolution applies an *n×n* filter over an *n×n* receptive field.
    With dilated convolutions, we apply the same filter sparsely over a receptive
    field of size *(n * l - 1) × (n * l - 1)*, where *l* is the **dilation factor**.
    We still multiply each filter weight by one input slice cell, but these cells
    are at a distance of *l* away from each other. The regular convolution is a special
    case of dilated convolution with *l = 1*. This is best illustrated with the following
    diagram:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 常规卷积在 *n×n* 感受野上应用 *n×n* 滤波器。而膨胀卷积则稀疏地在大小为 *(n * l - 1) × (n * l - 1)* 的感受野上应用相同的滤波器，其中
    *l* 是 **膨胀因子**。我们仍然将每个滤波器的权重与一个输入切片单元相乘，但这些单元之间的距离为 *l*。常规卷积是膨胀卷积的特例，*l = 1*。以下图示最好说明这一点：
- en: '![Figure 4.14 – A dilated convolution with a dilation factor of l=2\. Here,
    the ﬁrst two steps of the operation are displayed. The bottom layer is the input
    while the top layer is the output. Source: https://github.com/vdumoulin/conv_arithmetic](img/B19627_04_14.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – 一种膨胀卷积，膨胀因子为 l=2。这里展示了操作的前两步。底层是输入，顶层是输出。来源：[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)](img/B19627_04_14.jpg)'
- en: 'Figure 4.14 – A dilated convolution with a dilation factor of l=2\. Here, the
    ﬁrst two steps of the operation are displayed. The bottom layer is the input while
    the top layer is the output. Source: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 一种膨胀卷积，膨胀因子为 l=2。这里展示了操作的前两步。底层是输入，顶层是输出。来源：[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)
- en: Dilated convolutions can increase the receptive field’s size exponentially without
    losing resolution or coverage. We can also increase the receptive field with stride
    convolutions or pooling but at the cost of resolution and/or coverage. To understand
    this, let’s imagine that we have a stride convolution with stride *s > 1*. In
    this case, the output slice is *s* times smaller than the input (loss of resolution).
    If we increase *s > F* further (*F* is the size of either the pooling or convolutional
    kernel), we get a loss of coverage because some of the areas of the input slice
    will not participate in the output at all. Additionally, dilated convolutions
    don’t increase the computation and memory costs because the filter uses the same
    number of weights as the regular convolution.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积可以在不丧失分辨率或覆盖范围的情况下指数级地增大感受野的大小。我们也可以通过步幅卷积或池化来增大感受野，但会以分辨率和/或覆盖范围的损失为代价。为了理解这一点，我们假设有一个步幅大于1的步幅卷积*s
    > 1*。在这种情况下，输出切片的大小是输入的*s*倍（分辨率损失）。如果我们进一步增大*s > F*（*F*是池化或卷积核的大小），我们会失去覆盖范围，因为输入切片的某些区域根本不参与输出。此外，膨胀卷积不会增加计算和内存成本，因为滤波器使用的权重数量与常规卷积相同。
- en: Transposed convolutions
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转置卷积
- en: 'In the convolutional operations we’ve discussed so far, the output dimensions
    are either equal or smaller than the input dimensions. In contrast, transposed
    convolutions (first proposed in *Deconvolutional Networks by Matthew D. Zeiler,
    Dilip Krishnan, Graham W. Taylor, and Rob Fergus*: [https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf))
    allow us to upsample the input data (their output is larger than the input). This
    operation is also known as **deconvolution**, **fractionally strided convolution**,
    or **sub-pixel convolution**. These names can sometimes lead to confusion. To
    clarify things, note that the transposed convolution is, in fact, a regular convolution
    with a slightly modified input slice or convolutional filter.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到目前为止讨论的卷积操作中，输出维度要么与输入维度相同，要么小于输入维度。相比之下，转置卷积（最初由*Matthew D. Zeiler、Dilip
    Krishnan、Graham W. Taylor 和 Rob Fergus* 提出的 *Deconvolutional Networks*： [https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)）允许我们对输入数据进行上采样（其输出大于输入）。这种操作也被称为**反卷积**、**分数步幅卷积**或**子像素卷积**。这些名字有时会导致混淆。为了解释清楚，请注意，转置卷积实际上是一个常规卷积，只是输入切片或卷积滤波器略有修改。
- en: 'For the longer explanation, we’ll start with a 1D regular convolution over
    a single input and output slice:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更长的解释，我们将从一个1D常规卷积开始，处理单个输入和输出切片：
- en: '![Figure 4.15 – 1D regular convolution](img/B19627_04_15.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – 1D 常规卷积](img/B19627_04_15.jpg)'
- en: Figure 4.15 – 1D regular convolution
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 1D 常规卷积
- en: It uses a filter with *size = 4*, *stride = 2*, and *padding = 2* (denoted with
    gray in the preceding diagram). The input is a vector of size 6 and the output
    is a vector of size 4\. The filter, a vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">f</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/313.png),
    is always the same, but it’s denoted with different colors for each position we
    apply it to. The respective output cells are denoted with the same color. The
    arrows show which input cells contribute to one output cell.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用一个大小为*size = 4*，步幅为*stride = 2*，填充为*padding = 2*的滤波器（在前面的图示中用灰色表示）。输入是大小为6的向量，输出是大小为4的向量。滤波器是一个向量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">f</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"
    separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/313.png)始终是相同的，但每次应用时会用不同的颜色表示它的位置。对应的输出单元格也用相同的颜色表示。箭头指示哪些输入单元格贡献了一个输出单元格。
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The example that is being discussed in this section is inspired by the paper
    *Is the deconvolution layer the same as a convolutional* *layer?* ([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的示例灵感来源于论文 *Is the deconvolution layer the same as a convolutional* *layer?*
    ([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009))。
- en: 'Next, we’ll discuss the same example (1D, single input and output slices, and
    a filter with *size = 4*, *padding = 2*, and *stride = 2*), but for transposed
    convolution. The following diagram shows two ways we can implement it:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论相同的例子（1D，单输入输出切片，以及一个大小为*size = 4*，*padding = 2*，*stride = 2*的滤波器），但是是转置卷积。下图展示了我们可以实现它的两种方式：
- en: '![Figure 4.16 – A convolution with stride = 2, applied with the transposed
    ﬁlter f. The 2 pixels at the beginning and the end of the output are cropped (left);
    a convolution with stride 0.5, applied over input data, padded with subpixels.
    The input is ﬁlled with 0-valued pixels (gray) (right)](img/B19627_04_16.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16 – 使用步幅 = 2 的卷积，通过转置滤波器 f 应用。输出开始和结束时的两个像素被裁剪（左）；步幅为 0.5 的卷积，应用于输入数据，且用子像素进行填充。输入填充了值为
    0 的像素（灰色）（右）](img/B19627_04_16.jpg)'
- en: Figure 4.16 – A convolution with stride = 2, applied with the transposed ﬁlter
    f. The 2 pixels at the beginning and the end of the output are cropped (left);
    a convolution with stride 0.5, applied over input data, padded with subpixels.
    The input is ﬁlled with 0-valued pixels (gray) (right)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 使用步幅 = 2 的卷积，通过转置滤波器 f 应用。输出开始和结束时的两个像素被裁剪（左）；步幅为 0.5 的卷积，应用于输入数据，且用子像素进行填充。输入填充了值为
    0 的像素（灰色）（右）
- en: 'Let’s discuss them in detail:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论一下：
- en: 'In the first case, we have a regular convolution with *stride = 2* and a filter
    represented as a transposed row matrix (equivalent to a column matrix) with size
    4: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/314.png)
    (shown in the preceding diagram, left). Note that the stride is applied over the
    output layer as opposed to the regular convolution, where we stride over the input.
    By setting the stride larger than 1, we can increase the output size, compared
    to the input. Here, the size of the input slice is *I*, the size of the filter
    is *F*, the stride is *S*, and the input padding is *P*. Due to this, the size,
    *O*, of the output slice of a transposed convolution is given by the following
    formula: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mi>P</mml:mi></mml:math>](img/315.png).
    In this scenario, an input of size 4 produces an output of size 2 * (4 - 1) +
    4 - 2 * 2 = 6\. We also crop the two cells at the beginning and the end of the
    output vector because they only gather input from a single input cell.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一种情况下，我们有一个常规的卷积，步幅为*stride = 2*，滤波器表示为转置的行矩阵（相当于列矩阵），大小为 4：![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/314.png)（如前图所示，左边）。请注意，步幅应用于输出层，而不是常规卷积中的输入层。在设置步幅大于
    1 时，相比于输入，我们可以增加输出的大小。在这里，输入切片的大小是*I*，滤波器的大小是*F*，步幅是*S*，输入填充是*P*。因此，转置卷积的输出切片大小*O*由以下公式给出：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mi>P</mml:mi></mml:math>](img/315.png)。在这种情况下，大小为
    4 的输入会产生大小为 2 * (4 - 1) + 4 - 2 * 2 = 6 的输出。我们还会裁剪输出向量开始和结束时的两个单元格，因为它们只收集来自单个输入单元格的输入。
- en: In the second case, the input is filled with imaginary 0-valued subpixels between
    the existing ones (shown in the preceding diagram, right). This is where the name
    subpixel convolution comes from. Think of it as padding but within the image itself
    and not only along the borders. Once the input has been transformed in this way,
    a regular convolution is applied.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二种情况下，输入填充了存在像素之间的虚拟 0 值子像素（如前面的图所示，右侧）。这就是子像素卷积名称的由来。可以将其视为在图像内部进行的填充，而不仅仅是在边界上进行填充。一旦输入以这种方式变换，就会应用常规卷积。
- en: Let’s compare the two output cells, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/316.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/317.png),
    in both scenarios. As shown in the preceding diagram, in either case, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/318.png)
    receives input from the first and the second input cells and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/319.png)
    receives input from the second and third cells. The only difference between these
    two cases is the index of the weight, which participates in the computation. However,
    the weights are learned during training, and, because of this, the index is not
    important. Therefore, the two operations are equivalent.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较两个输出单元，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/316.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/317.png)，在这两种情况下，如前面的图所示，在任何情况下，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/318.png)
    都接收来自第一个和第二个输入单元的输入，而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/319.png)
    接收来自第二个和第三个单元的输入。这两种情况的唯一区别是参与计算的权重索引。然而，权重在训练过程中会被学习到，因此，索引并不重要。因此，这两个操作是等价的。
- en: 'Next, let’s take a look at a 2D transposed convolution from a subpixel point
    of view. As with the 1D case, we insert 0-valued pixels and padding in the input
    slice to achieve upsampling (the input is at the bottom):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们从子像素的角度来看 2D 转置卷积。与 1D 情况类似，我们在输入切片中插入值为 0 的像素和填充，以实现上采样（输入位于底部）：
- en: '![Figure 4.17 – The ﬁrst three steps of a 2D transpose convolution with padding
    = 1 and stride = 2\. Source: https://github.com/vdumoulin/conv_arithmetic, https://arxiv.org/abs/1603.07285](img/B19627_04_17.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图4.17 – 带填充 = 1 和步幅 = 2 的 2D 转置卷积的前三个步骤。来源: https://github.com/vdumoulin/conv_arithmetic,
    https://arxiv.org/abs/1603.07285](img/B19627_04_17.jpg)'
- en: 'Figure 4.17 – The ﬁrst three steps of a 2D transpose convolution with padding
    = 1 and stride = 2\. Source: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic),
    https://arxiv.org/abs/1603.07285'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.17 – 带填充 = 1 和步幅 = 2 的 2D 转置卷积的前三个步骤。来源: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic),
    https://arxiv.org/abs/1603.07285'
- en: The backpropagation operation of a regular convolution is a transposed convolution.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 常规卷积的反向传播操作是转置卷积。
- en: This concludes our extended introduction to the various types of convolutions.
    In the next section, we’ll learn how to build some advanced CNN architectures
    with the advanced convolutions we’ve learned about so far.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对各种类型卷积的扩展介绍。在下一部分，我们将学习如何利用迄今为止学到的高级卷积构建一些高级 CNN 架构。
- en: Advanced CNN models
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级 CNN 模型
- en: In this section, we’ll discuss some complex CNN models. They are available in
    both PyTorch and Keras, with pre-trained weights on the ImageNet dataset. You
    can import and use them directly, instead of building them from scratch. Still,
    it’s worth discussing their central ideas as an alternative to using them as black
    boxes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将讨论一些复杂的CNN模型。它们在PyTorch和Keras中都有提供，并且在ImageNet数据集上进行了预训练。你可以直接导入并使用它们，而不是从头开始构建。不过，作为替代方法，讨论它们的核心思想也是值得的，而不是把它们当作黑盒使用。
- en: 'Most of these models share a few architectural principles:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型大多共享一些架构原理：
- en: They start with an “entry” phase, which uses a combination of stride convolutions
    and/or pooling to reduce the input image size at least two to eight times, before
    propagating it to the rest of the network. This makes a CNN more computationally-
    and memory-efficient because the deeper layers work with smaller slices.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们从“入口”阶段开始，利用步幅卷积和/或池化的组合将输入图像的大小减少至少两到八倍，然后将其传播到网络的其余部分。这使得CNN在计算和内存上更加高效，因为更深层的网络处理的是较小的切片。
- en: The main network body comes after the entry phase. It is composed of multiple
    repeated composite modules. Each of these modules utilizes padded convolutions
    in such a way that its input and output slices are the same size. This makes it
    possible to stack as many modules as necessary to reach the desired depth. The
    deeper modules utilize a higher number of filters (output slices) per convolution,
    compared to the earlier ones.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主网络体位于入口阶段之后。它由多个重复的复合模块组成。每个模块都利用填充卷积，使其输入和输出的切片大小相同。这使得可以根据需要堆叠任意数量的模块，以达到所需的深度。相比早期的模块，更深的模块在每次卷积中使用更多的过滤器（输出切片）。
- en: The downsampling in the main body is handled by special modules with stride
    convolutions and/or pooling operations.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主体中的下采样由具有步幅卷积和/或池化操作的特殊模块处理。
- en: The convolutional phase usually ends with GAP over all slices.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积阶段通常以对所有切片进行GAP操作结束。
- en: The output of the GAP operation can serve as input for various tasks. For example,
    we can add an FC layer for classification.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAP操作的输出可以作为各种任务的输入。例如，我们可以添加一个全连接（FC）层进行分类。
- en: 'We can see a prototypical CNN built with these principles in the following
    figure:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到一个基于这些原理构建的典型CNN：
- en: '![Figure 4.18 – A prototypical CNN](img/B19627_04_18.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.18 – 一个典型的CNN](img/B19627_04_18.jpg)'
- en: Figure 4.18 – A prototypical CNN
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 – 一个典型的CNN
- en: With that, let’s dig deeper into deep CNNs (get it?).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，让我们更深入地了解深度CNN（懂了吗？）。
- en: Introducing residual networks
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入残差网络
- en: '**Residual networks** (**ResNets**, *Deep Residual Learning for Image Recognition*,
    [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)) were released
    in 2015 when they won all five categories of the ImageNet challenge that year.
    In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047), we discussed that the layers
    of an NN are not restricted to sequential order but form a directed graph instead.
    This is the first architecture we’ll learn about that takes advantage of this
    flexibility. This is also the first network architecture that has successfully
    trained a network with a depth of more than 100 layers.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差网络**（**ResNets**，*深度残差学习用于图像识别*，[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)）于2015年发布，当年他们在ImageNet挑战赛中赢得了所有五个类别的冠军。在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中，我们讨论了神经网络的层不是按顺序排列的，而是形成一个有向图。这是我们将要学习的第一种利用这种灵活性的架构。这也是第一种成功训练超过100层深度网络的架构。'
- en: Thanks to better weight initializations, new activation functions, as well as
    normalization layers, it’s now possible to train deep networks. However, the authors
    of the paper conducted some experiments and observed that a network with 56 layers
    had higher training and testing errors compared to a network with 20 layers. They
    argue that this should not be the case. In theory, we can take a shallow network
    and stack identity layers (these are layers whose output just repeats the input)
    on top of it to produce a deeper network that behaves in the same way as the shallow
    one. Yet, their experiments have been unable to match the performance of the shallow
    network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更好的权重初始化、新的激活函数以及归一化层，现在可以训练深度网络。然而，论文的作者进行了实验，观察到一个56层的网络在训练和测试中出现的错误比一个20层的网络要高。他们认为这种情况不应发生。从理论上讲，我们可以采用一个浅层网络，并在其上堆叠恒等层（这些层的输出只是重复输入）来生成一个更深的网络，使其行为与浅层网络相同。然而，他们的实验未能匹配浅层网络的表现。
- en: 'To solve this problem, they proposed a network constructed of residual blocks.
    A residual block consists of two or three sequential convolutional layers and
    a separate parallel **identity** (repeater) shortcut connection, which connects
    the input of the first layer and the output of the last one. We can see three
    types of residual blocks in the following figure:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，他们提出了由残差模块构成的网络。残差模块由两到三个连续的卷积层和一个单独的并行**恒等**（重复器）快捷连接组成，该连接将第一个层的输入与最后一层的输出相连。我们可以在下面的图中看到三种类型的残差模块：
- en: '![Figure 4.19 – From left to right – original residual block; original bottleneck
    residual block; pre-activation residual block; pre-activation bottleneck residual
    block](img/B19627_04_19.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.19 – 从左到右 – 原始残差模块；原始瓶颈残差模块；预激活残差模块；预激活瓶颈残差模块](img/B19627_04_19.jpg)'
- en: Figure 4.19 – From left to right – original residual block; original bottleneck
    residual block; pre-activation residual block; pre-activation bottleneck residual
    block
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19 – 从左到右 – 原始残差模块；原始瓶颈残差模块；预激活残差模块；预激活瓶颈残差模块
- en: Each block has two parallel paths. The left-hand path is similar to the other
    networks we’ve seen and consists of sequential convolutional layers and batch
    normalization. The right path contains the identity shortcut connection (also
    known as the **skip connection**). The two paths are merged via an element-wise
    sum – that is, the left and right tensors have the same shape, and an element
    of the first tensor is added to the element in the same position in the second
    tensor. The output is a single tensor with the same shape as the input. In effect,
    we propagate the features learned by the block forward, but also the original
    unmodified signal. In this way, we can get closer to the original scenario, as
    described by the authors. The network can decide to skip some of the convolutional
    layers thanks to the skip connections, in effect reducing its depth. The residual
    blocks use padding in such a way that the input and the output of the block have
    the same dimensions. Thanks to this, we can stack any number of blocks for a network
    with an arbitrary depth.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块都有两条并行路径。左侧路径与我们见过的其他网络类似，由连续的卷积层和批量归一化组成。右侧路径包含恒等快捷连接（也称为**跳跃连接**）。这两条路径通过元素级求和进行合并——也就是说，左右两边的张量形状相同，第一个张量的一个元素加到第二个张量中相同位置的元素上。输出是一个与输入形状相同的单一张量。实际上，我们将模块学到的特征向前传播，同时保留了原始的未修改信号。这样，我们就能更接近原始场景，正如作者所描述的那样。由于跳跃连接，网络可以选择跳过一些卷积层，实际上减少了其深度。残差模块使用填充，使得输入和输出的形状相同。得益于此，我们可以堆叠任意数量的模块，构建具有任意深度的网络。
- en: 'Now, let’s see how the blocks in the diagram differ:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看图中的模块有何不同：
- en: The first block contains two 3×3 convolutional layers. This is the original
    residual block, but if the layers are wide, stacking multiple blocks becomes computationally
    expensive.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个模块包含两个3×3卷积层。这是原始的残差模块，但如果层数过宽，堆叠多个模块会变得计算开销很大。
- en: The second block is equivalent to the first, but it uses a **bottleneck layer**.
    First, we use a 1×1 convolution to downsample the input volume depth (we discussed
    this in the *1×1 convolutions* section). Then, we apply a 3×3 (bottleneck) convolution
    to the reduced input. Finally, we expand the output back to the desired depth
    with another 1×1 upsampling convolution. This layer is less computationally expensive
    than the first.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个模块与第一个模块相同，但它使用了**瓶颈层**。首先，我们使用1×1卷积来下采样输入的深度（我们在*1×1卷积*章节中讨论过这个）。然后，我们对减少后的输入应用一个3×3的（瓶颈）卷积。最后，我们通过另一个1×1上采样卷积将输出扩展回所需的深度。这个层的计算开销比第一个要小。
- en: The third block is the latest revision of the idea, published in 2016 by the
    same authors (*Identity Mappings in Deep Residual Networks*, [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)).
    It uses pre-activations, and the batch normalization and the activation function
    come before the convolutional layer. This may seem strange at first, but thanks
    to this design, the skip connection path can run uninterrupted throughout the
    network. This is contrary to the other residual blocks, where at least one activation
    function is on the path of the skip connection. A combination of stacked residual
    blocks still has the layers in the right order.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个块是这一概念的最新修订版，由同一作者于 2016 年发布（*Identity Mappings in Deep Residual Networks*,
    [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)）。它使用了预激活，并且批归一化和激活函数位于卷积层之前。这一设计可能看起来有些奇怪，但得益于这一设计，跳跃连接路径可以在整个网络中不间断地运行。这与其他残差块不同，在那些块中，至少有一个激活函数位于跳跃连接的路径上。堆叠的残差块组合依然保持了正确的层次顺序。
- en: The fourth block is the bottleneck version of the third layer. It follows the
    same principle as the bottleneck residual layer v1.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四个块是第三层的瓶颈版本。它遵循与瓶颈残差层 v1 相同的原理。
- en: 'In the following table, we can see the family of networks proposed by the authors
    of the paper:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们可以看到论文作者提出的网络家族：
- en: '![Figure 4.20 – The family of the most popular residual networks. The residual
    blocks are represented by rounded rectangles. Inspired by https://arxiv. org/abs/1512.03385](img/B19627_04_20.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.20 – 最流行的残差网络家族。残差块以圆角矩形表示。灵感来源于 https://arxiv.org/abs/1512.03385](img/B19627_04_20.jpg)'
- en: Figure 4.20 – The family of the most popular residual networks. The residual
    blocks are represented by rounded rectangles. Inspired by [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20 – 最流行的残差网络家族。残差块以圆角矩形表示。灵感来源于 [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
- en: 'Some of their properties are as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的一些属性如下：
- en: They start with a 7×7 convolutional layer with *stride = 2*, followed by 3×3
    max pooling. This phase serves as a downsampling step, so the rest of the network
    can work with a much smaller slice of 56×56, compared to 224×224 of the input.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们以一个 7×7 卷积层开始，*stride = 2*，接着是 3×3 最大池化。这个阶段作为一个下采样步骤，使得网络的其余部分可以在一个更小的 56×56
    的切片上工作，相比于输入的 224×224。
- en: Downsampling in the rest of the network is implemented with a modified residual
    block with *stride =* *2*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络其余部分的下采样是通过一个修改过的残差块实现的，*stride =* *2*。
- en: GAP downsamples the output after all residual blocks and before the 1,000-unit
    FC softmax layer.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAP 在所有残差块之后、1,000 单元全连接 softmax 层之前进行下采样。
- en: The number of parameters for the various ResNets range from 25.6 million to
    60.4 million and their depth ranges from 18 to 152 layers.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种 ResNet 的参数数量从 2560 万到 6040 万不等，网络的深度从 18 层到 152 层不等。
- en: The ResNet family of networks is popular not only because of their accuracy
    but also because of their relative simplicity and the versatility of the residual
    blocks. As we mentioned previously, the input and output shape of the residual
    block can be the same due to the padding. We can stack residual blocks in different
    configurations to solve various problems with wide-ranging training set sizes
    and input dimensions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 网络家族之所以流行，不仅因为它们的准确性，还因为它们相对简洁且残差块具有很大的通用性。正如我们之前提到的，残差块的输入和输出形状由于填充可以相同。我们可以以不同的配置堆叠残差块，以解决具有广泛训练集规模和输入维度的各种问题。
- en: Inception networks
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Inception 网络
- en: '**Inception networks** (*Going Deeper with Convolutions*, [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    were introduced in 2014 when they won the ImageNet challenge of that year (there
    seems to be a pattern here). Since then, the authors have released multiple improvements
    (versions) of the architecture.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**Inception 网络** (*Going Deeper with Convolutions*, [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    于 2014 年首次提出，并赢得了当年的 ImageNet 挑战（似乎有一个规律）。此后，作者们发布了该架构的多个改进版本。'
- en: Fun fact
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: The name *inception* comes in part from the *We need to go deeper* internet
    meme, related to the movie Inception.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*Inception* 这一名称部分来源于 *We need to go deeper* 这一互联网迷因，后者与电影《盗梦空间》相关。'
- en: 'The idea behind inception networks started from the basic premise that the
    objects in an image have different scales. A distant object might take up a small
    region of the image, but the same object, once nearer, might take up a large part
    of the image. This presents a difficulty for standard CNNs, where the units in
    the different layers have a fixed receptive field size, as imposed on the input
    image. A regular network might be a good detector of objects at a certain scale
    but could miss them otherwise. To solve this problem, the authors of the paper
    proposed a novel architecture: one composed of inception blocks. An inception
    block starts with a common input and then splits it into different parallel paths
    (or towers). Each path contains either convolutional layers with a different-sized
    filter or a pooling layer. In this way, we apply different receptive fields to
    the same input data. At the end of the Inception block, the outputs of the different
    paths are concatenated.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: inception 网络的思想源于这样一个基本前提：图像中的物体具有不同的尺度。一个远距离的物体可能只占据图像中的一小部分，但同一个物体靠近时，可能会占据图像的大部分。这对于标准的
    CNN 来说是一个难题，因为不同层中的单元具有固定的感受野大小，这一大小被强加到输入图像上。一个常规的网络可能在某一尺度下能很好地检测物体，但在其他尺度下可能会错过物体。为了解决这个问题，论文的作者提出了一种新颖的架构：由
    inception 块组成。inception 块以一个共同的输入开始，然后将其分割成不同的并行路径（或塔）。每条路径包含具有不同尺寸滤波器的卷积层或池化层。通过这种方式，我们可以对相同的输入数据应用不同的感受野。在
    Inception 块的末端，来自不同路径的输出会被连接起来。
- en: In the next few sections, we’ll discuss the different variations of Inception
    networks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论 Inception 网络的不同变体。
- en: Inception v1
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Inception v1
- en: 'The following diagram shows the first version of the inception block, which
    is part of the **GoogLeNet** network architecture ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了**GoogLeNet**网络架构中 inception 块的第一个版本（[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)）：
- en: '![Figure 4.21 – Inception v1 block; inspired by https://arxiv.org/abs/1409.4842](img/B19627_04_21.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.21 – Inception v1 块；灵感来源于 https://arxiv.org/abs/1409.4842](img/B19627_04_21.jpg)'
- en: Figure 4.21 – Inception v1 block; inspired by https://arxiv.org/abs/1409.4842
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.21 – Inception v1 块；灵感来源于 https://arxiv.org/abs/1409.4842
- en: 'The v1 block has four paths:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: v1 块有四条路径：
- en: 1×1 convolution, which acts as a kind of repeater to the input
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，它作为输入的某种重复器
- en: 1×1 convolution, followed by a 3×3 convolution
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，后接 3×3 卷积
- en: 1×1 convolution, followed by a 5×5 convolution
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，后接 5×5 卷积
- en: 3×3 max pooling with *stride = 1*
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3×3 最大池化，*步幅 = 1*
- en: The layers in the block use padding in such a way that the input and the output
    have the same shape (but different depths). The padding is also necessary because
    each path would produce an output with a different shape, depending on the filter
    size. This is valid for all versions of inception blocks.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 块中的层使用填充，使得输入和输出具有相同的形状（但深度不同）。填充也是必要的，因为每个路径的输出形状会根据滤波器大小不同而有所不同。这适用于所有版本的
    inception 块。
- en: The other major innovation of this inception block is the use of downsampling
    1×1 convolutions. They are needed because the output of all paths is concatenated
    to produce the final output of the block. The result of the concatenation is an
    output with a quadrupled depth. If another inception block followed the current
    one, its output depth would quadruple again. To avoid such exponential growth,
    the block uses 1×1 convolutions to reduce the depth of each path, which, in turn,
    reduces the output depth of the block. This makes it possible to create deeper
    networks, without running out of resources.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该 inception 块的另一个主要创新是使用降采样 1×1 卷积。它们是必需的，因为所有路径的输出将被连接以产生该块的最终输出。连接的结果是输出深度被四倍增大。如果下一个块继续跟随当前块，它的输出深度将再次四倍增长。为了避免这种指数增长，块使用
    1×1 卷积来减少每条路径的深度，从而降低块的输出深度。这使得我们可以创建更深的网络，而不至于耗尽资源。
- en: 'The full GoogLeNet has the following properties:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 GoogLeNet 具有以下特性：
- en: Like ResNets, it starts with a downsampling phase, which utilizes two convolutional
    and two max pooling layers to reduce the input size from 224×224 to 56×56, before
    the inception blocks get involved.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 ResNets 类似，它首先进行降采样阶段，利用两个卷积层和两个最大池化层将输入尺寸从 224×224 降至 56×56，然后才开始使用 inception
    块。
- en: The network has nine inception v1 blocks.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该网络有九个 inception v1 块。
- en: The convolutional phase ends with global average pooling.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积阶段以全局平均池化结束。
- en: The network utilizes auxiliary classifiers—that is, it has two additional classification
    outputs (with the same ground truth labels) at various intermediate layers. During
    training, the total value of the loss is a weighted sum of the auxiliary losses
    and the real loss.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络利用了辅助分类器——即在不同的中间层有两个额外的分类输出（具有相同的真实标签）。在训练过程中，损失的总值是辅助损失和真实损失的加权和。
- en: The model has a total of 6.9 million parameters and a depth of 22 layers.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型共有 690 万个参数，深度为 22 层。
- en: Inception v2 and v3
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Inception v2 和 v3
- en: 'Inception v2 and v3 were released together and proposed several improved inception
    blocks over the original v1 (*Rethinking the Inception Architecture for Computer
    Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)).
    We can see the first new inception block, A, in the following diagram:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2 和 v3 是一起发布的，并提出了比原始 v1（*Rethinking the Inception Architecture for
    Computer Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)）更改进的多个
    Inception 块。我们可以在以下图示中看到第一个新 Inception 块 A：
- en: '![Figure 4.22 – Inception block A, inspired by https://arxiv.org/abs/1512.00567](img/B19627_04_22.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.22 – Inception 块 A，灵感来自于 https://arxiv.org/abs/1512.00567](img/B19627_04_22.jpg)'
- en: Figure 4.22 – Inception block A, inspired by https://arxiv.org/abs/1512.00567
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.22 – Inception 块 A，灵感来自于 https://arxiv.org/abs/1512.00567
- en: The first new property of block A is the factorization of the 5×5 convolution
    in two stacked 3×3 convolutions. This structure has several advantages.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 块 A 的第一个新特性是将 5×5 卷积因式分解为两个堆叠的 3×3 卷积。这种结构有几个优点。
- en: The receptive field of the units of the last stacked layer is equivalent to
    the receptive field of a single layer with a large convolutional filter. The stacked
    layers achieve the same receptive field size with fewer parameters, compared to
    a single layer with a large filter. For example, let’s replace a single 5×5 layer
    with two stacked 3×3 layers. For the sake of simplicity, we’ll assume that we
    have single input and output slices. The total number of weights (excluding biases)
    of the 5×5 layer is 5 * 5 = 25\.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层堆叠单元的感受野等同于一个大卷积滤波器的单层感受野。与使用单层大滤波器相比，堆叠的层能够以更少的参数实现相同的感受野大小。例如，我们可以将一个单独的
    5×5 层替换为两个堆叠的 3×3 层。为了简便起见，我们假设有单一的输入和输出切片。5×5 层的总权重（不包括偏差）是 5 * 5 = 25。
- en: 'On the other hand, the total weights of a single 3×3 layer is 3 * 3 = 9, and
    simply 2 * (3 * 3) = 18 for two layers, which makes this arrangement 28% more
    efficient (18/25 = 0.72). The efficiency gain is preserved even with multiple
    input and output slices for the two layers. The next improvement is the factorization
    of an *n×n* convolution in two stacked asymmetrical 1×*n* and *n*×1 convolutions.
    For example, we can split a single 3×3 convolution into two 1×3 and 3×1 convolutions,
    where the 3×1 convolution is applied over the output of the 1×3 convolution. In
    the first case, the filter size would be 3 * 3 = 9, while in the second case,
    we would have a combined size of (3 * 1) + (1 * 3) = 3 + 3 = 6, resulting in 33%
    efficiency, as seen in the following diagram:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，单个 3×3 层的总权重是 3 * 3 = 9，两个层的权重则是 2 * (3 * 3) = 18，这使得这种安排比单层的 5×5 层更高效
    28%（18/25 = 0.72）。即便在输入和输出切片为多个的情况下，这种效率提升也得以保持。接下来的改进是将一个 *n×n* 卷积因式分解为两个堆叠的不对称
    1×*n* 和 *n*×1 卷积。例如，我们可以将一个单一的 3×3 卷积分解为两个 1×3 和 3×1 卷积，其中 3×1 卷积应用于 1×3 卷积的输出。在第一个情况下，滤波器大小是
    3 * 3 = 9，而在第二种情况下，组合后的大小是 (3 * 1) + (1 * 3) = 3 + 3 = 6，达到了 33% 的效率提升，如下图所示：
- en: '![Figure 4.23 – Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions;
    inspired by https://arxiv.org/abs/1512.00567](img/B19627_04_23.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.23 – 3×3 卷积的因式分解为 1×3 和 3×1 卷积；灵感来自于 https://arxiv.org/abs/1512.00567](img/B19627_04_23.jpg)'
- en: Figure 4.23 – Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions;
    inspired by https://arxiv.org/abs/1512.00567
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.23 – 3×3 卷积的因式分解为 1×3 和 3×1 卷积；灵感来自于 https://arxiv.org/abs/1512.00567
- en: 'The authors introduced two new blocks that utilize factorized convolutions.
    The first of these blocks (and the second in total), inception block B, is equivalent
    to inception block A:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 作者引入了两个利用因式分解卷积的新块。其中第一个块（也是总的第二个块），Inception 块 B，等同于 Inception 块 A：
- en: '![Figure 4.24 – Inception block B. When n=3, it is equivalent to block A; inspired
    by https://arxiv.org/abs/1512.00567](img/B19627_04_24.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.24 – Inception 块 B。当 n=3 时，它等同于块 A；灵感来自于 https://arxiv.org/abs/1512.00567](img/B19627_04_24.jpg)'
- en: Figure 4.24 – Inception block B. When n=3, it is equivalent to block A; inspired
    by https://arxiv.org/abs/1512.00567
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.24 – Inception 块 B。当 n=3 时，它等同于块 A；灵感来源于 [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: 'The second (third in total) inception block, C, is similar, but the asymmetrical
    convolutions are parallel, resulting in a higher output depth (more concatenated
    paths). The hypothesis here is that the more features (different filters) the
    network has, the faster it learns. On the other hand, the wider layers take more
    memory and computation time. As a compromise, this block is only used in the deeper
    part of the network, after the other blocks:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个（总共是第三个）Inception 块 C 类似，但非对称卷积是并行的，从而导致更高的输出深度（更多的连接路径）。这里的假设是，网络拥有更多特征（不同的滤波器）时，它学习得更快。另一方面，更宽的层会占用更多的内存和计算时间。作为折衷，这个块仅在网络的较深部分使用，在其他块之后：
- en: '![Figure 4.25 – Inception block C; inspired by https://arxiv.org/abs/1512.00567](img/B19627_04_25.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.25 – Inception 块 C；灵感来源于 https://arxiv.org/abs/1512.00567](img/B19627_04_25.jpg)'
- en: Figure 4.25 – Inception block C; inspired by https://arxiv.org/abs/1512.00567
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.25 – Inception 块 C；灵感来源于 [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: Another major improvement in this version is the use of batch normalization,
    which was introduced by the same authors.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 该版本的另一个重大改进是使用批量归一化，这一技术由同一作者提出。
- en: 'These new blocks create two new inception networks: v2 and v3\. Inception v3
    uses batch normalization and is the more popular of the two. It has the following
    properties:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新模块创建了两个新的 Inception 网络：v2 和 v3。Inception v3 使用批量归一化，并且是这两者中更为流行的一个。它具有以下特点：
- en: The network starts with a downsampling phase, which utilizes stride convolutions
    and max pooling to reduce the input size from 299×299 to 35×35 before the inception
    blocks get involved
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络从下采样阶段开始，利用步幅卷积和最大池化将输入大小从 299×299 降低到 35×35，然后再进入 Inception 模块。
- en: The layers are organized into three inception blocks, A, five inception blocks,
    B, and two inception blocks, C
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层被组织为三个 Inception 块 A，五个 Inception 块 B 和两个 Inception 块 C。
- en: The convolutional phase ends with global average pooling
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积阶段以全局平均池化结束。
- en: It has 23.9 million parameters and a depth of 48 layers
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有 2390 万个参数，深度为 48 层。
- en: Inception v4 and Inception-ResNet
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Inception v4 和 Inception-ResNet
- en: 'The latest revisions of inception networks introduce three new streamlined
    inception blocks (**Inception-v4**, *Inception-v4, Inception-ResNet and the Impact
    of Residual Connections on Learning*, [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)).
    More specifically, the new versions introduce 7×7 asymmetric factorized convolutions
    average pooling instead of max pooling and new Inception-ResNet blocks with residual
    connections. We can see one such block in the following diagram:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 Inception 网络修订版引入了三个新的简化 Inception 块（**Inception-v4**，*Inception-v4, Inception-ResNet
    和残差连接对学习的影响*，[https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)）。更具体地说，新版本引入了
    7×7 非对称因式分解卷积、平均池化代替最大池化，以及具有残差连接的新 Inception-ResNet 块。我们可以在以下图示中看到其中一个这样的块：
- en: '![Figure 4.26 – An inception block (any kind) with a residual skip connection](img/B19627_04_26.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.26 – 带有残差跳跃连接的 Inception 块（任何类型）](img/B19627_04_26.jpg)'
- en: Figure 4.26 – An inception block (any kind) with a residual skip connection
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.26 – 带有残差跳跃连接的 Inception 块（任何类型）
- en: 'The Inception-ResNet family of models share the following properties:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-ResNet 系列模型具有以下特点：
- en: The networks start with a downsampling phase, which utilizes stride convolutions
    and max pooling to reduce the input size from 299×299 to 35×35 before the inception
    blocks get involved.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络从下采样阶段开始，利用步幅卷积和最大池化将输入大小从 299×299 降低到 35×35，然后再进入 Inception 模块。
- en: The main body of the model consists of three groups of four residual-inception-A
    blocks, seven residual-inception-B blocks, three residual inception-B blocks,
    and special reduction modules between the groups. The different models use slightly
    different variations of these blocks.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的主体由三组四个残差 Inception-A 块、七个残差 Inception-B 块、三个残差 Inception-B 块，以及各组之间的特殊缩减模块组成。不同的模型使用这些块的略微不同变体。
- en: The convolutional phase ends with global average pooling.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积阶段以全局平均池化结束。
- en: The models have around 56 million weights.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型约有 5600 万个权重。
- en: In this section, we discussed different types of inception networks and the
    different principles used in the various inception blocks. Next, we’ll talk about
    a newer CNN architecture that takes the inception concept to a new depth (or width,
    as it should be).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们讨论了不同类型的 inception 网络以及各个 inception 块中使用的不同原理。接下来，我们将介绍一种较新的 CNN 架构，它将
    inception 概念带到了一个新的深度（或者更准确地说，是宽度）。
- en: Introducing Xception
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入 Xception
- en: 'All inception blocks we’ve discussed so far start by splitting the input into
    several parallel paths. Each path continues with a dimensionality-reduction 1×1
    cross-channel convolution, followed by regular cross-channel convolutions. On
    one hand, the 1×1 connection maps cross-channel correlations, but not spatial
    ones (because of the 1×1 filter size). On the other hand, the subsequent cross-channel
    convolutions map both types of correlations. Let’s recall that earlier in this
    chapter, we introduced DSCs, which combine the following two operations:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有 inception 块都从将输入拆分成若干并行路径开始。每条路径接着执行一个降维的 1×1 跨通道卷积，然后是常规的跨通道卷积。一方面，1×1
    连接映射跨通道相关性，但不映射空间相关性（因为 1×1 滤波器大小）。另一方面，后续的跨通道卷积映射两种类型的相关性。回想一下，在本章前面，我们介绍了 DSC，它结合了以下两种操作：
- en: '**A depthwise convolution**: In a depthwise convolution, a single input slice
    produces a single output slice, so it only maps spatial (and not cross-channel)
    correlations'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积**：在深度卷积中，一个输入切片产生一个输出切片，因此它仅映射空间（而非跨通道）相关性。'
- en: '**A 1×1 cross-channel convolution**: With 1×1 convolutions, we have the opposite
    – that is, they only map cross-channel correlations'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1×1 跨通道卷积**：使用 1×1 卷积时，正好相反——它们只映射跨通道相关性。'
- en: 'The author of Xception (*Xception: Deep Learning with Depthwise Separable Convolutions*,
    [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)) argues that
    we can think of DSC as an extreme (hence the name) version of an inception block,
    where each depthwise input/output slice pair represents one parallel path. We
    have as many parallel paths as the number of input slices. The following diagram
    shows a simplified inception block and its transformation to an Xception block:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xception 的作者（*Xception: Deep Learning with Depthwise Separable Convolutions*,
    [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)）认为，我们可以将
    DSC 视为 inception 块的极端（因此得名）版本，其中每一对深度卷积输入/输出切片代表一条并行路径。我们有多少条并行路径，就有多少个输入切片。下图展示了一个简化版的
    inception 块及其转化为 Xception 块的过程：'
- en: '![Figure 4.27 –A simpliﬁed inception module (left); an Xception block (right);
    inspired by https://arxiv.org/abs/1610.02357](img/B19627_04_27.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.27 – 一个简化的 inception 模块（左）；一个 Xception 块（右）；灵感来源于 https://arxiv.org/abs/1610.02357](img/B19627_04_27.jpg)'
- en: Figure 4.27 –A simpliﬁed inception module (left); an Xception block (right);
    inspired by https://arxiv.org/abs/1610.02357
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.27 – 一个简化的 inception 模块（左）；一个 Xception 块（右）；灵感来源于 https://arxiv.org/abs/1610.02357
- en: 'The Xception block and the DSC have two differences:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Xception 块和 DSC 之间有两个区别：
- en: In Xception, the 1×1 convolution comes first, instead of last as in DSC. However,
    these operations are meant to be stacked anyway, and we can assume that the order
    is of no significance.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Xception 中，1×1 卷积位于最前面，而在 DSC 中位于最后。然而，这些操作本应被堆叠，因此我们可以假设顺序并不重要。
- en: The Xception block uses ReLU activations after each convolution, while the DSC
    doesn’t use non-linearity after the cross-channel convolution. According to the
    author’s experiments, networks with absent non-linearity depthwise convolution
    converged faster and were more accurate.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception 块在每个卷积后使用 ReLU 激活，而 DSC 在跨通道卷积之后不使用非线性激活。根据作者的实验结果，缺少非线性激活的深度卷积网络收敛速度更快且更为准确。
- en: 'The full Xception network has the following properties:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 Xception 网络具有以下特点：
- en: It starts with an entry flow of convolutional and pooling operations, which
    reduces the input size from 299×299 to 19×19.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从卷积和池化操作的输入流开始，将输入大小从 299×299 缩小到 19×19。
- en: It has 14 Xception modules, all of which have linear residual connections around
    them, except for the first and last modules.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含 14 个 Xception 模块，所有模块周围都有线性残差连接，除了第一个和最后一个模块。
- en: All convolutions and DSCs are followed by batch normalization. All DSCs have
    a depth multiplier of 1 (no depth expansion).
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有卷积和 DSC 操作后都紧跟着批量归一化。所有 DSC 的深度乘数为 1（没有深度扩展）。
- en: The convolutional phase ends with global average pooling.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积阶段以全局平均池化结束。
- en: A total of 23 million parameters and a depth of 36 convolutional layers.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总共 2300 万个参数，36 层卷积层的深度。
- en: This section concludes the series of inception-based models. In the next section,
    we’ll focus on a novel NN architectural element.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了基于 inception 的模型系列。在下一节中，我们将重点讨论一个新的神经网络架构元素。
- en: Squeeze-and-Excitation Networks
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩与激发网络
- en: '**Squeeze-and-Excitation Networks** (**SENet**, *Squeeze-and-Excitation Networks*,
    [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)) introduce
    a new NN architectural unit, which the authors call – you guessed it – the **Squeeze-and-Excitation**
    (**SE**) block. Let’s recall that the convolutional operation applies multiple
    filters across the input channels to produce multiple output feature maps (or
    channels). The authors of SENet observe that each of these channels has “equal
    weight” when it serves as input to the next layer. However, some channels could
    be more informative than others. To emphasize their importance, the authors propose
    the content-aware SE block, which weighs each channel adaptively. We can also
    think of the SE block as an **attention mechanism**. To understand how it works,
    let’s start with the following figure:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**压缩与激发网络**（**SENet**，*压缩与激发网络*，[https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)）引入了一个新的神经网络架构单元，作者称之为——你猜对了——**压缩与激发**（**SE**）块。我们回顾一下，卷积操作对输入通道应用多个滤波器以生成多个输出特征图（或通道）。SENet
    的作者观察到，当这些通道作为输入传递给下一层时，它们具有“相等的权重”。然而，有些通道可能比其他通道更有信息。为了强调它们的重要性，作者提出了内容感知的 SE
    块，它能够自适应地对每个通道进行加权。我们也可以将 SE 块看作是一种**注意力机制**。为了理解它是如何工作的，让我们从以下图像开始：'
- en: '![Figure 4.28 – The Squeeze-and-Excitation block](img/B19627_04_28.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.28 – 压缩与激发块](img/B19627_04_28.jpg)'
- en: Figure 4.28 – The Squeeze-and-Excitation block
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.28 – 压缩与激发块
- en: 'The block introduces a parallel path to the main NN data flow. Let’s see its
    steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 该块引入了一个与主神经网络数据流平行的路径。让我们来看一下它的步骤：
- en: '**Squeeze phase**: A GAP operation is applied across the channels. The output
    of the GAP is a single scalar value for each channel. For example, if the input
    is an RGB image, the unique GAP operations across each of the R, G, and B channels
    will produce a one-dimensional tensor with size 3\. Think of these scalar values
    as the distilled state of the channels.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**压缩阶段**：在各个通道上执行 GAP 操作。GAP 的输出是每个通道的一个标量值。例如，如果输入是 RGB 图像，那么在 R、G 和 B 通道上执行的独特
    GAP 操作将生成一个大小为 3 的一维张量。可以将这些标量值看作是通道的精炼状态。'
- en: '`FC layer -> ReLU -> FC layer -> sigmoid`. It resembles an autoencoder because
    the first hidden layer reduces the size of the input tensor and the second hidden
    layer upscales it to the original size (3 in the case of RGB input). The final
    sigmoid activation ensures that all values of the output are in the (0:1) range.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`全连接层 -> ReLU -> 全连接层 -> sigmoid`。它类似于自编码器，因为第一个隐藏层减少了输入张量的大小，第二个隐藏层将其扩展到原始大小（对于
    RGB 输入来说是 3）。最终的 sigmoid 激活函数确保输出值位于 (0:1) 范围内。'
- en: '**Scale**: The output values of the excitement NN serve as scaling coefficients
    of the channels of the original input tensor. All the values of a channel are
    scaled (or excited) by its corresponding coefficient produced by the excitement
    phase. In this way, the excitement NN can emphasize the importance of a given
    channel.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缩放**：兴奋神经网络的输出值作为原始输入张量通道的缩放系数。一个通道的所有值都由兴奋阶段生成的对应系数进行缩放（或激发）。通过这种方式，兴奋神经网络能够强调某个通道的重要性。'
- en: 'The authors added SE blocks to different existing models, which improved their
    accuracy. In the following figure, we can see how we can add SE blocks to inception
    and residual modules:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将 SE 块添加到不同的现有模型中，提升了它们的准确性。在下图中，我们可以看到如何将 SE 块添加到 inception 和残差模块中：
- en: '![Figure 4.29 – An SE-inception module (left) and an SE-ResNet module (right)](img/B19627_04_29.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.29 – 一个 SE-inception 模块（左）和一个 SE-ResNet 模块（右）](img/B19627_04_29.jpg)'
- en: Figure 4.29 – An SE-inception module (left) and an SE-ResNet module (right)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.29 – 一个 SE-inception 模块（左）和一个 SE-ResNet 模块（右）
- en: In the next section, we’ll see the SE block applied to a model, which prioritizes
    a small footprint and computational efficiency.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到 SE 块应用于一个优先考虑小尺寸和计算效率的模型。
- en: Introducing MobileNet
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 MobileNet
- en: 'In this section, we’ll discuss a lightweight CNN model called **MobileNet**
    (*MobileNetV3: Searching for MobileNetV3*, [https://arxiv.org/abs/1905.02244](https://arxiv.org/abs/1905.02244)).
    We’ll focus on the third revision of this idea (MobileNetV1 was introduced in
    *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications*,
    [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861) and MobileNetV2
    was introduced in *MobileNetV2: Inverted Residuals and Linear* *Bottlenecks*,
    [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '本节将讨论一种轻量级卷积神经网络模型，名为**MobileNet**（*MobileNetV3: Searching for MobileNetV3*，[https://arxiv.org/abs/1905.02244](https://arxiv.org/abs/1905.02244)）。我们将重点介绍这个想法的第三个版本（MobileNetV1在*MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications*中介绍，[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)；MobileNetV2在*MobileNetV2:
    Inverted Residuals and Linear* *Bottlenecks*中介绍，[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)）。'
- en: 'MobileNet is aimed at devices with limited memory and computing power, such
    as mobile phones (the name kind of gives it away). The NN introduces a new **inverted
    residual block** (or **MBConv**) with a reduced footprint. MBConv uses DSC, **linear
    bottlenecks**, and **inverted residuals**. V3 also introduces SE blocks. To understand
    all this, here’s the structure of the MBConv block:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet的目标是面向内存和计算能力有限的设备，例如手机（名字本身就揭示了这一点）。该神经网络引入了一种新的**反向残差块**（或称**MBConv**），具有较小的占用空间。MBConv使用了深度可分离卷积（DSC）、**线性瓶颈**和**反向残差**。V3版本还引入了SE模块。为了理解这些内容，下面是MBConv模块的结构：
- en: '![Figure 4.30 – MobileNetV3 building block. The shortcut connection exists
    only if the stride s=1](img/B19627_04_30.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.30 – MobileNetV3构建块。只有当步幅s=1时，才存在快捷连接](img/B19627_04_30.jpg)'
- en: Figure 4.30 – MobileNetV3 building block. The shortcut connection exists only
    if the stride s=1
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.30 – MobileNetV3构建块。只有当步幅s=1时，才存在快捷连接
- en: 'Let’s discuss its properties:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下它的属性：
- en: '**Linear bottlenecks**: We’ll assume that our input is an RGB image. As it’s
    propagated through the NN, each layer produces an activation tensor with multiple
    channels. It has long been assumed that the information encoded in these tensors
    can be compressed in the so-called “manifold of interest,” which is represented
    by a smaller tensor than the original. One way to force the NN to seek such manifolds
    is with 1×1 bottleneck convolutions. However, the authors of the paper argue that
    if this convolution is followed by non-linearity like ReLU, this might lead to
    a loss of manifold information because of the dying-ReLUs problem. To avoid this,
    MobileNet uses a 1×1 bottleneck convolution without non-linear activation.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性瓶颈**：我们假设输入是一个RGB图像。当图像通过神经网络（NN）传播时，每一层都会生成一个带有多个通道的激活张量。一直以来，人们认为这些张量中编码的信息可以在所谓的“感兴趣流形”中压缩，这个流形的张量比原始张量要小。强迫神经网络寻找这种流形的一种方法是使用1×1瓶颈卷积。然而，论文的作者认为，如果该卷积后接非线性激活函数，如ReLU，这可能会导致由于ReLU消失问题而丧失流形信息。为了解决这个问题，MobileNet使用没有非线性激活的1×1瓶颈卷积。'
- en: '`input -> 1×1 bottleneck conv -> 3×3 conv -> 1×1 unsampling conv`. In other
    words, it follows a `wide -> narrow -> wide` data representation. On the other
    hand, the inverted residual block follows a `narrow -> wide -> narrow` representation.
    Here, the bottleneck convolution expands its input with an **expansion** **factor**,
    *t*.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input -> 1×1瓶颈卷积 -> 3×3卷积 -> 1×1反采样卷积`。换句话说，它遵循一个`宽 -> 窄 -> 宽`的数据表示。另一方面，反向残差块遵循`窄
    -> 宽 -> 窄`的表示方式。在这里，瓶颈卷积通过**扩展** **因子** *t* 扩展其输入。'
- en: The authors argue that the bottlenecks contain all the necessary information,
    while an expansion layer acts merely as an implementation detail that accompanies
    a non-linear transformation of the tensor. Because of this, they propose having
    shortcut connections between the bottleneck connections instead.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者认为瓶颈包含了所有必要的信息，而扩展层仅作为一个实现细节，伴随着张量的非线性变换。因此，他们提出在瓶颈连接之间使用快捷连接。
- en: '**DSC**: We already introduced this operation earlier in this chapter. MobileNet
    V3 introduces **H-swish** activation in the DSC. H-swish resembles the swish function,
    which we introduced in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047). The V3
    architecture includes alternating ReLU and H-swish activations.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DSC**：我们在本章早些时候已经介绍过这个操作。MobileNet V3在DSC中引入了**H-swish**激活函数。H-swish类似于我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中介绍的swish函数。V3架构包括交替使用ReLU和H-swish激活函数。'
- en: '**SE blocks**: We’re already familiar with this block. The difference here
    is the **hard sigmoid** activation, which approximates the sigmoid but is computationally
    more efficient. The module is placed after the expanding depthwise convolution,
    so the attention can be applied to the largest representation. The SE block is
    a new addition to V3 and was not present in V2.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SE模块**：我们已经熟悉这个模块。这里的区别是**硬Sigmoid**激活函数，它近似Sigmoid函数，但在计算上更高效。该模块位于扩展深度卷积后，因此可以将注意力应用于最大的表示。SE模块是V3的新增加项，在V2中并不存在。'
- en: '**Stride** *s*: The block implements downsampling with stride convolutions.
    The shortcut connection exists only when *s*=1.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步幅** *s*：该模块使用步幅卷积实现下采样。当*s*=1时，才存在捷径连接。'
- en: 'MobileNetV3 introduces large and small variations of the network with the following
    properties:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV3引入了网络的大小变体，具有以下特点：
- en: Both networks start with a stride convolution that downsamples the input from
    224×224 to 112×112
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个网络都以步幅卷积开始，将输入从224×224下采样到112×112。
- en: The small and large variations have 11 and 15 MBConv blocks, respectively
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型和大型变体分别有11个和15个MBConv模块。
- en: The convolutional phase ends with global average pooling for both networks
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积阶段结束时，两个网络都会进行全局平均池化。
- en: The small and large networks have 3 and 5 million parameters, respectively
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型和大型网络分别有300万和500万个参数。
- en: In the next section, we’ll discuss an improved version of the MBConv block.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论MBConv模块的改进版。
- en: EfficientNet
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EfficientNet
- en: '**EfficientNet** (*EfficientNet: Rethinking Model Scaling for Convolutional
    Neural Networks*, [https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946),
    and *EfficientNetV2: Smaller Models and Faster Training*, [https://arxiv.org/abs/2104.00298](https://arxiv.org/abs/2104.00298))
    introduces the concept of **compound scaling**. It starts with a small baseline
    model and then simultaneously expands it in three directions: depth (more layers),
    width (more feature maps per layer), and higher input resolution. The compound
    scaling produces a series of new models. The EfficientNetV1 baseline model uses
    the MBConv building block of MobileNetV2\. EfficientNetV2 introduces the new **fused-MBConv**
    block, which replaces the expanding 1×1 bottleneck convolution and the 3×3 depthwise
    convolution of MBConv, with a single expanding 3×3 cross-channel convolution:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**EfficientNet**（*EfficientNet: Rethinking Model Scaling for Convolutional
    Neural Networks*，[https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)）和*EfficientNetV2:
    Smaller Models and Faster Training*，[https://arxiv.org/abs/2104.00298](https://arxiv.org/abs/2104.00298)）引入了**复合缩放**的概念。它从一个小的基线模型开始，然后在三个方向上同时扩展：深度（更多层）、宽度（每层更多特征图）以及更高的输入分辨率。复合缩放会生成一系列新模型。EfficientNetV1的基线模型使用了MobileNetV2的MBConv模块。EfficientNetV2引入了新的**融合MBConv**模块，它用单一的扩展3×3跨通道卷积替代了MBConv中的扩展1×1瓶颈卷积和3×3深度卷积：'
- en: '![Figure 4.31 – Fused-MBConv block](img/B19627_04_31.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图4.31 – 融合MBConv模块](img/B19627_04_31.jpg)'
- en: Figure 4.31 – Fused-MBConv block
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.31 – 融合MBConv模块
- en: The new 3×3 convolution handles both the expanding (with a factor of *t*) and
    the stride (1 or 2).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 新的3×3卷积同时处理扩展（通过因子*t*）和步幅（1或2）。
- en: The authors of EfficientNetV2 observed that a CNN, which uses a combination
    of fused-MBConv and MBConv blocks, trains faster compared to a CNN with MBConv
    blocks only. However, the
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 'EfficientNetV2的作者观察到，使用融合MBConv和MBConv模块组合的CNN，比仅使用MBConv模块的CNN训练速度更快。然而， '
- en: 'fused-MBConv block is computationally more expensive, compared to the plain
    MBConv block. Because of this, EfficientNetV2 replaces the blocks gradually, starting
    from the early stages. This makes sense because the earlier convolutions use a
    smaller number of filters (and hence slices), so the memory and computational
    penalty are less pronounced at this stage. Finding the right combination of the
    two blocks is not trivial, hence the need for compound scaling. This process produced
    multiple models with the following properties:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 融合MBConv模块在计算上比普通MBConv模块更为昂贵。因此，EfficientNetV2逐步替换这些模块，从早期阶段开始。这是有道理的，因为早期的卷积使用了更少的滤波器（因此切片较少），所以在这一阶段，内存和计算代价不那么显著。找到两种模块的正确组合并不简单，因此需要复合缩放。这一过程产生了多个具有以下特点的模型：
- en: The networks start with a stride convolution that downsamples the input twice
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个网络都以步幅卷积开始，将输入下采样两次。
- en: The early stages of the main body use fused-MBConv blocks, and the later stages
    use MBConv blocks
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主体的早期阶段使用融合MBConv模块，后期阶段使用MBConv模块。
- en: The convolutional phase ends with global average pooling for all networks
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有网络的卷积阶段最终都会通过全局平均池化结束
- en: The number of parameters ranges between 5.3 million and 119 million
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的数量范围从 530 万到 1.19 亿不等
- en: This concludes our introduction to advanced CNN models. We didn’t discuss all
    the available models, but we focused on some of the most popular ones. I hope
    that you now have sufficient knowledge to explore new models yourself. In the
    next section, we’ll demonstrate how to use these advanced models in PyTorch and
    Keras.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章介绍了高级 CNN 模型的内容。我们没有讨论所有可用的模型，而是重点介绍了其中一些最受欢迎的模型。我希望你现在已经掌握了足够的知识，可以自行探索新的模型。在下一节中，我们将演示如何在
    PyTorch 和 Keras 中使用这些高级模型。
- en: Using pre-trained models with PyTorch and Keras
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练模型与 PyTorch 和 Keras
- en: Both PyTorch and Keras have a collection of pre-trained ready-to-use models.
    All the models we discussed in the *Advanced network models* section are available
    in this way. The models are usually pre-trained on classifying the ImageNet dataset
    and can serve as backbones to various computer vision tasks, as we’ll see in [*Chapter
    5*](B19627_05.xhtml#_idTextAnchor146).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 和 Keras 都有一套预训练的即用型模型。我们在*高级网络模型*部分讨论的所有模型都可以通过这种方式使用。这些模型通常是在分类 ImageNet
    数据集上进行预训练的，可以作为各种计算机视觉任务的骨干网络，正如我们在 [*第 5 章*](B19627_05.xhtml#_idTextAnchor146)
    中将看到的那样。
- en: 'We can load a pre-trained model in PyTorch with the following code:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码在 PyTorch 中加载一个预训练的模型：
- en: '[PRE14]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The weights will be automatically downloaded. In addition, we can list all
    available models and load an arbitrary model using the following code:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 权重将会自动下载。此外，我们可以使用以下代码列出所有可用的模型并加载任意模型：
- en: '[PRE15]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Keras supports similar functionality. We can load a pre-trained model with
    the following code:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 也支持类似的功能。我们可以使用以下代码加载一个预训练的模型：
- en: '[PRE16]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: These short but very useful code examples conclude this chapter.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简短但非常有用的代码示例总结了本章的内容。
- en: Summary
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced CNNs. We talked about their main building blocks
    – convolutional and pooling layers – and we discussed their architecture and features.
    We paid special attention to the different types of convolutions. We also demonstrated
    how to use PyTorch and Keras to implement the CIFAR-10 classification CNN. Finally,
    we discussed some of the most popular CNN models in use today.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 CNN。我们讨论了它们的主要组成部分——卷积层和池化层，并讨论了它们的架构和特征。我们特别关注了不同类型的卷积。我们还演示了如何使用
    PyTorch 和 Keras 实现 CIFAR-10 分类 CNN。最后，我们讨论了目前一些最受欢迎的 CNN 模型。
- en: In the next chapter, we’ll build upon our new-found computer vision knowledge
    with some exciting additions. We’ll discuss how to train networks faster by transferring
    knowledge from one problem to another. We’ll also go beyond simple classification
    with object detection, or how to find the object’s location on the image. We’ll
    even learn how to segment each pixel of an image.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在新的计算机视觉知识基础上进行扩展，加入一些令人兴奋的内容。我们将讨论如何通过将知识从一个问题转移到另一个问题来加速网络训练。我们还将超越简单的分类，进行目标检测，或者如何找到图像中目标的位置。我们甚至会学习如何对图像的每个像素进行分割。
