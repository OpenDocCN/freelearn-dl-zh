- en: 1\. Building Blocks of Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 深度学习的构建模块
- en: Introduction
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, you will be introduced to deep learning and its relationship
    with artificial intelligence and machine learning. We will also learn about some
    of the important deep learning architectures, such as the multi-layer perceptron,
    convolutional neural networks, recurrent neural networks, and generative adversarial
    networks. As we progress, we will get hands-on experience with the TensorFlow
    framework and use it to implement a few linear algebra operations. Finally, we
    will be introduced to the concept of optimizers. We will understand their role
    in deep learning by utilizing them to solve a quadratic equation. By the end of
    this chapter, you will have a good understanding of what deep learning is and
    how programming with TensorFlow works.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解深度学习及其与人工智能和机器学习的关系。我们还将学习一些重要的深度学习架构，如多层感知器、卷积神经网络、递归神经网络和生成对抗网络。随着我们深入学习，你将通过实践体验TensorFlow框架，并使用它来实现一些线性代数操作。最后，我们将了解优化器的概念。通过利用它们来解决二次方程式，我们将理解优化器在深度学习中的作用。到本章结束时，你将对深度学习的概念和如何使用TensorFlow进行编程有一个清晰的了解。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: You have just come back from your yearly vacation. Being an avid social media
    user, you are busy uploading your photographs to your favorite social media app.
    When the photos get uploaded, you notice that the app automatically identifies
    your face and tags you in them almost instantly. In fact, it does that even in
    group photos. Even in some poorly lit photos, you notice that the app has, most
    of the time, tagged you correctly. How does the app learn how to do that?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚从每年的假期回来。作为一个活跃的社交媒体用户，你忙着将照片上传到你最喜欢的社交媒体应用。当照片上传后，你注意到应用会自动识别你的面部并几乎瞬间标记你。事实上，它甚至在群体照片中也能做到这一点。即使在一些光线较差的照片中，你也注意到应用大多数时候能正确标记你。那应用程序是如何学习做这些事情的呢？
- en: To identify a person in a picture, the app requires accurate information on
    the person's facial structure, bone structure, eye color, and many other details.
    But when you used that photo app, you didn't have to feed all these details explicitly
    to the app. All you did was upload your photos, and the app automatically began
    identifying you in them. How did the app know all these details?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要在照片中识别一个人，应用程序需要准确的信息，如此人的面部结构、骨骼结构、眼睛颜色以及许多其他细节。但当你使用这个照片应用程序时，你并不需要将所有这些细节明确地提供给应用程序。你所做的只是上传照片，应用程序就会自动开始识别你。那应用程序是如何知道这些细节的呢？
- en: When you uploaded your first photo to the app, the app would have asked you
    to tag yourself. When you manually tagged yourself, the app automatically "learned"
    all the information it needed to know about your face. Then, every time you upload
    a photo, the app uses the information it learned to identify you. It improves
    when you manually tag yourself in photos in which the app incorrectly tagged you.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次将照片上传到应用时，应用程序会要求你标记自己。当你手动标记自己时，应用程序会自动“学习”关于你面部的所有信息。然后，每次你上传照片时，应用程序就会利用它学到的信息来识别你。当你在应用错误标记你时，手动标记自己能够帮助它改进。
- en: This ability of the app to learn new details and improve itself with minimal
    human intervention is possible due to the power of **deep learning** (**DL**).
    Deep learning is a part of **artificial intelligence** (**AI**) that helps a machine
    learn by recognizing patterns from labeled data. But wait a minute, isn't that
    what **machine learning** (**ML**) does? Then what is the difference between deep
    learning and machine learning? What is the point of confluence among domains such
    as AI, machine learning, and deep learning? Let's take a quick look.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序能够在最小化人工干预的情况下学习新细节并自我改进，这得益于**深度学习**（**DL**）的强大功能。深度学习是**人工智能**（**AI**）的一部分，通过识别标记数据中的模式帮助机器学习。但等一下，这不就是**机器学习**（**ML**）的功能吗？那么，深度学习和机器学习之间有什么区别呢？人工智能、机器学习和深度学习等领域之间的交集点是什么？让我们快速了解一下。
- en: AI, Machine Learning, and Deep Learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能、机器学习与深度学习
- en: Artificial intelligence is the branch of computer science aimed at developing
    machines that can simulate human intelligence. Human intelligence, in a simplified
    manner, can be explained as decisions that are taken based on the inputs received
    from our five senses – sight, hearing, touch, smell, and taste. AI is not a new
    field and has been in vogue since the 1950s. Since then, there have been multiple
    waves of ecstasy and agony within this domain. The 21st century has seen a resurgence
    in AI following the big strides made in computing, the availability of data, and
    a better understanding of theoretical underpinnings. Machine learning and deep
    learning are subfields of AI and are increasingly used interchangeably.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是计算机科学的一个分支，旨在开发能够模拟人类智能的机器。人类智能可以简化为基于来自我们五感——视力、听力、触觉、嗅觉和味觉——的输入来做出决策。AI并不是一个新领域，自1950年代以来就已有发展。此后，这个领域经历了多次高潮与低谷。进入21世纪，随着计算能力的飞跃、数据的丰富和对理论基础的更好理解，AI迎来了复兴。机器学习和深度学习是AI的子领域，并且越来越多地被交替使用。
- en: 'The following figure depicts the relationship between AI, ML, and DL:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了AI、ML和DL之间的关系：
- en: '![Figure 1.1: Relationship between AI, ML, and DL'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：AI、ML和DL之间的关系'
- en: '](img/B15385_01_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_01.jpg)'
- en: 'Figure 1.1: Relationship between AI, ML, and DL'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：AI、ML和DL之间的关系
- en: Machine Learning
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习
- en: Machine learning is the subset of AI that performs specific tasks by identifying
    patterns within data and extracting inferences. The inferences that are derived
    from data are then used to predict outcomes on unseen data. Machine learning differs
    from traditional computer programming in its approach to solving specific tasks.
    In traditional computer programming, we write and execute specific business rules
    and heuristics to get the desired outcomes. However, in machine learning, the
    rules and heuristics are not explicitly written. These rules and heuristics are
    learned by providing a dataset. The dataset provided for learning the rules and
    heuristics is called a **training dataset**. The whole process of learning and
    inferring is called **training**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是AI的一个子集，通过识别数据中的模式并提取推论来执行特定任务。从数据中得出的推论随后用于预测未知数据的结果。机器学习与传统计算机编程在解决特定任务的方法上有所不同。在传统的计算机编程中，我们编写并执行特定的业务规则和启发式算法来获得期望的结果。然而，在机器学习中，这些规则和启发式算法并没有被明确编写。这些规则和启发式算法是通过提供数据集进行学习的。用于学习这些规则和启发式算法的数据集称为**训练数据集**。整个学习和推断的过程称为**训练**。
- en: 'Learning rules and heuristics is done using different algorithms that use statistical
    models for that purpose. These algorithms make use of many representations of
    data for learning. Each such representation of data is called an **example**.
    Each element within an example is called a **feature**. The following is an example
    of the famous IRIS dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    This dataset is a representation of different species of iris flowers based on
    different characteristics, such as the length and width of their sepals and petals:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 学习规则和启发式算法是通过使用不同的算法来完成的，这些算法采用统计模型来实现这一目的。这些算法利用多种数据表示方式进行学习。每种数据的表示方式称为**示例**。示例中的每个元素称为**特征**。以下是著名的IRIS数据集的一个示例（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)）。该数据集表示了不同种类的鸢尾花，基于不同的特征，如萼片和花瓣的长度与宽度：
- en: '![Figure 1.2: Sample data from the IRIS dataset'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2：IRIS数据集的样本数据'
- en: '](img/B15385_01_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_02.jpg)'
- en: 'Figure 1.2: Sample data from the IRIS dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：IRIS数据集的样本数据
- en: In the preceding dataset, each row of data represents an example, and each column
    is a feature. Machine learning algorithms make use of these features to draw inferences
    from the data. The veracity of the models, and thereby the outcomes that are predicted,
    depend a lot on the features of the data. If the features provided to the machine
    learning algorithm are a good representation of the problem statement, the chances
    of getting a good result are high. Some examples of machine learning algorithms
    are *linear regression*, *logistic regression*, *support vector machines*, *random
    forest*, and *XGBoost*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据集中，每一行数据代表一个例子，每一列是一个特征。机器学习算法利用这些特征从数据中推断出结论。模型的准确性，以及预测结果的可靠性，很大程度上依赖于数据的特征。如果提供给机器学习算法的特征能够很好地代表问题陈述，那么得到好结果的机会就会很高。一些常见的机器学习算法包括*线性回归*、*逻辑回归*、*支持向量机*、*随机森林*和*XGBoost*。
- en: Even though traditional machine learning algorithms are useful for a lot of
    use cases, they have a lot of dependence on the quality of the features to get
    superior outcomes. The creation of features is a time-consuming art and requires
    a lot of domain knowledge. However, even with comprehensive domain knowledge,
    there are still limitations on transferring that knowledge to derive features,
    thereby encapsulating the nuances of the data generating processes. Also, with
    the increasing complexity of the problems that are tackled with machine learning,
    particularly with the advent of unstructured data (images, voice, text, and so
    on), it can be almost impossible to create features that represent the complex
    functions, which, in turn, generate data. As a result, there is often a need to
    find a different approach to solving complex problems; that is where deep learning
    comes into play.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统的机器学习算法在许多应用场景中都很有用，但它们在获得优异结果时，非常依赖于特征的质量。特征的创建是一门耗时的艺术，且需要大量的领域知识。然而，即便拥有全面的领域知识，仍然存在将这些知识转化为特征的局限性，进而无法很好地封装数据生成过程中的细微差别。此外，随着机器学习所解决问题的复杂性增加，特别是非结构化数据（如图像、语音、文本等）的出现，几乎不可能创建能够表示复杂函数的特征，这些复杂函数反过来又生成数据。因此，往往需要找到一种不同的方法来解决复杂问题，这时深度学习就派上了用场。
- en: Deep Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习
- en: Deep learning is a subset of machine learning and an extension of a certain
    kind of algorithm called Artificial Neural Networks (ANNs). Neural networks are
    not a new phenomenon. Neural networks were created in the first half of the 1940s.
    The development of neural networks was inspired by the knowledge of how the human
    brain works. Since then, there have been several ups and downs in this field.
    One defining moment that renewed enthusiasm around neural networks was the introduction
    of an algorithm called backpropagation by stalwarts in the field such as Geoffrey
    Hinton. For this reason, Hinton is widely regarded as the 'Godfather of Deep Learning'.
    We will be discussing neural networks in depth in *Chapter 2*, *Neural Networks.*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，是一种称为人工神经网络（ANN）的算法的扩展。神经网络并不是一种新现象。神经网络的创建可以追溯到20世纪40年代的上半期。神经网络的开发灵感来自于对人类大脑运作方式的了解。从那时起，这一领域经历了几次高潮和低谷。一个重新激发人们对神经网络兴趣的关键时刻是由该领域的巨头们，如Geoffrey
    Hinton，提出的反向传播算法。正因为如此，Hinton被广泛认为是“深度学习的教父”。我们将在*第2章*《神经网络》中深入讨论神经网络。
- en: 'ANNs with multiple (deep) layers lie at the heart of deep learning. One defining
    characteristic of deep learning models is their ability to learn features from
    the input data. Unlike traditional machine learning, where there is the need to
    create features, deep learning excels in learning different hierarchies of features
    across multiple layers. Say, for example, we are using a deep learning model to
    detect faces. The initial layers of the model will learn low-level approximations
    of a face, such as the edges of the face, as shown in *Figure 1.3*. Each succeeding
    layer takes the lower layers'' features and puts them together to form more complex
    features. In the case of face detection, if the initial layer has learned to detect
    edges, the subsequent layers will put these edges together to form parts of a
    face such as the nose or eyes. This process continues with each successive layer,
    with the final layer generating an image of a human face:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多层（深层）人工神经网络（ANNs）是深度学习的核心。深度学习模型的一个显著特点是其能够从输入数据中学习特征。与传统的机器学习不同，后者需要手动创建特征，深度学习擅长从多个层次学习不同的特征层级。例如，假设我们使用一个深度学习模型来检测人脸。模型的初始层会学习面部的低级近似特征，如面部的边缘，如*图
    1.3*所示。每个后续层会将前一层的特征组合起来，形成更复杂的特征。在人脸检测的例子中，如果初始层学会了检测边缘，后续层将这些边缘组合起来，形成面部的一部分，如鼻子或眼睛。这个过程在每一层继续进行，直到最后一层生成一个完整的人脸图像：
- en: '![Figure 1.3: Deep learning model for detecting faces'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3：用于检测人脸的深度学习模型'
- en: '](img/B15385_01_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_03.jpg)'
- en: 'Figure 1.3: Deep learning model for detecting faces'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：用于检测人脸的深度学习模型
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding image is sourced from the popular research paper: Lee, Honglak
    & Grosse, Roger & Ranganath, Rajesh & Ng, Andrew. (2011). *Unsupervised Learning
    of Hierarchical Representations with Convolutional Deep Belief Networks.* Commun.
    ACM. 54\. 95-103\. 10.1145/2001269.2001295.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图片来自于一篇流行的研究论文：Lee, Honglak & Grosse, Roger & Ranganath, Rajesh & Ng, Andrew.
    (2011). *无监督学习层次表示与卷积深度置信网络.* Commun. ACM. 54\. 95-103\. 10.1145/2001269.2001295.
- en: Deep learning techniques have made great strides over the past decade. There
    are different factors that have led to the exponential rise of deep learning techniques.
    At the top of the list is the availability of large quantities of data. The digital
    age, with its increasing web of connected devices, has generated lots of data,
    especially unstructured data. This, in turn, has fueled the large-scale adoption
    of deep learning techniques as they are well-suited to handle large unstructured
    data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术在过去十年中取得了巨大的进步。多个因素促使了深度学习技术的指数增长，其中最重要的因素是大量数据的可用性。数字时代，随着越来越多设备的互联，产生了大量数据，特别是非结构化数据。这反过来促进了深度学习技术的大规模应用，因为它们非常适合处理大量的非结构化数据。
- en: Another major factor that has led to the rise in deep learning is the strides
    that have been made in computing infrastructure. Deep learning models that have
    large numbers of layers and millions of parameters necessitate great computing
    power. The advances in computing layers such as **Graphical Processing Units**
    (**GPUs**) and **Tensor Processing Units** (**TPUs**) at an affordable cost has
    led to the large-scale adoption of deep learning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习崛起的另一个重要因素是计算基础设施的进步。深度学习模型通常包含大量层次和数百万个参数，因此需要强大的计算能力。图形处理单元（**GPU**）和张量处理单元（**TPU**）等计算层次的进步，以合理的成本提供了强大的计算能力，从而推动了深度学习的广泛应用。
- en: The pervasiveness of deep learning was also accelerated by open sourcing different
    frameworks in order to build and implement deep learning models. In 2015, the
    Google Brain team open sourced the TensorFlow framework and since then TensorFlow
    has grown to be one of the most popular frameworks for deep learning. The other
    major frameworks available are PyTorch, MXNet, and Caffe. We will be using the
    TensorFlow framework in this book.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的普及还得益于不同框架的开源，这些框架用于构建和实现深度学习模型。2015年，Google Brain团队开源了TensorFlow框架，自那时以来，TensorFlow已成长为最受欢迎的深度学习框架之一。其他主要的框架包括PyTorch、MXNet和Caffe。本书将使用TensorFlow框架。
- en: Before we dive deep into the building blocks of deep learning, let's get our
    hands dirty with a quick demo that illustrates the power of deep learning models.
    You don't need to know any of the code that is presented in this demo. Simply
    follow the instructions and you'll be able to get a quick glimpse of the basic
    capabilities of deep learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨深度学习的构建块之前，让我们通过一个简短的演示来实际体验深度学习模型的强大功能。你不需要了解演示中的所有代码。只需按照指示操作，你就能快速了解深度学习的基本能力。
- en: Using Deep Learning to Classify an Image
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度学习分类图像
- en: In the exercise that follows, we will classify an image of a pizza and convert
    the resulting class text into speech. To classify the image, we will be using
    a pre-trained model. The conversion of text into speech will be done using a freely
    available API called **Google Text-to-Speech** (**gTTS**). Before we get into
    it, let's understand some of the key building blocks of this demo.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将分类一个披萨的图像，并将分类结果的文本转换为语音。为了对图像进行分类，我们将使用一个预训练的模型。文本转语音将使用一个免费提供的API——**Google文本转语音**（**gTTS**）来完成。在开始之前，让我们先了解一些这个演示的关键构建块。
- en: Pre-Trained Models
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Training a deep learning model requires a lot of computing infrastructure and
    time, with big datasets. However, to aid with research and learning, the deep
    learning community has also made models that have been trained on large datasets
    available. These pre-trained models can be downloaded and used for predictions
    or can be used for further training. In this demo, we will be using a pre-trained
    model called `ResNet50`. This model is available along with the Keras package.
    This pre-trained model can predict 1,000 different classes of objects that we
    encounter in our daily lives, such as birds, animals, automobiles, and more.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个深度学习模型需要大量的计算资源和时间，并且需要庞大的数据集。然而，为了促进研究和学习，深度学习社区也提供了在大数据集上训练好的模型。这些预训练模型可以下载并用于预测，或者用于进一步训练。在本次演示中，我们将使用一个名为`ResNet50`的预训练模型。这个模型与Keras包一起提供。这个预训练模型能够预测我们日常生活中遇到的1,000种不同类型的物体，比如鸟类、动物、汽车等。
- en: The Google Text-to-Speech API
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google文本转语音API
- en: Google has made its Text-to-Speech algorithm available for limited use. We will
    be using this algorithm to convert the predicted text into speech.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Google已经将其文本转语音算法开放供有限使用。我们将使用这个算法将预测的文本转换为语音。
- en: Prerequisite Packages for the Demo
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演示所需的先决条件包
- en: 'For this demo to work, you will need the following packages installed on your
    machine:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个演示正常工作，你需要在机器上安装以下包：
- en: TensorFlow 2.0
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0
- en: Keras
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras
- en: gTTS
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gTTS
- en: Please refer to the *Preface* to understand the process of installing the first
    two packages. Installing gTTS will be shown in the exercise. Let's dig into the
    demo.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*前言*以了解安装前两个包的过程。安装gTTS将在练习中展示。接下来，让我们深入了解演示。
- en: 'Exercise 1.01: Image and Speech Recognition Demo'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.01：图像和语音识别演示
- en: 'In this exercise, we will demonstrate image recognition and speech-to-text
    conversion using deep learning models. At this point, you will not be able to
    understand each and every line of the code. This will be explained later. For
    now, just execute the code and find out how easy it is to build deep learning
    and AI applications with TensorFlow. Follow these steps to complete this exercise:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将演示使用深度学习模型进行图像识别和语音转文本的转换。此时，你可能无法理解代码中的每一行，这将在后续讲解中解释。现在，只需执行代码，了解使用TensorFlow构建深度学习和人工智能应用程序有多么简单。按照以下步骤完成本次练习：
- en: Open a Jupyter Notebook and name it *Exercise 1.01.* For details on how to start
    a Jupyter Notebook, please refer to the preface.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Jupyter Notebook并命名为*练习1.01*。关于如何启动Jupyter Notebook的详细信息，请参阅前言。
- en: 'Import all the required libraries:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的库：
- en: '[PRE0]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is a brief description of the packages we''ll be importing:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里简要描述我们将要导入的包：
- en: '`load_img`: Loads the image into the Jupyter Notebook'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`load_img`：将图像加载到Jupyter Notebook中'
- en: '`img_to_array`: Converts the image into a NumPy array, which is the desired
    format for Keras'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`img_to_array`：将图像转换为NumPy数组，这是Keras所需的格式'
- en: '`preprocess_input`: Converts the input into a format that''s acceptable for
    the model'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`preprocess_input`：将输入转换为模型可以接受的格式'
- en: '`decode_predictions`: Converts the numeric output of the model prediction into
    text labels'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`decode_predictions`：将模型预测的数值输出转换为文本标签'
- en: '`Resnet50`: This is the pre-trained image classification model'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Resnet50`：这是一个预训练的图像分类模型'
- en: 'Create an instance of the pre-trained `Resnet` model:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个预训练的`Resnet`模型实例：
- en: '[PRE1]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should get a message similar to the following as it downloads:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下载过程中你应收到类似以下的消息：
- en: '![Figure 1.4: Loading Resnet50'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.4：加载Resnet50'
- en: '](img/B15385_01_04.jpg)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_01_04.jpg)'
- en: 'Figure 1.4: Loading Resnet50'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.4：加载Resnet50
- en: '`Resnet50` is a pre-trained image classification model. For first-time users,
    it will take some time to download the model into your environment.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Resnet50`是一个预训练的图像分类模型。对于首次使用者，下载模型到你的环境中需要一些时间。'
- en: Download an image of a pizza from the internet and store it in the same folder
    that you are running the Jupyter Notebook in. Name the image `im1.jpg`.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从互联网上下载一张披萨的图片，并将其保存在运行Jupyter Notebook的同一文件夹中。将图片命名为`im1.jpg`。
- en: Note
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can also use the image we are using by downloading it from this link: [https://packt.live/2AHTAC9](https://packt.live/2AHTAC9)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以通过此链接下载我们使用的图片：[https://packt.live/2AHTAC9](https://packt.live/2AHTAC9)
- en: 'Load the image to be classified using the following command:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令加载待分类的图片：
- en: '[PRE2]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you are storing the image in another folder, the complete path of the location
    where the image is located has to be given in place of the `im1.jpg` command.
    For example, if the image is stored in `D:/projects/demo`, the code should be
    as follows:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你将图片保存在另一个文件夹中，则必须提供图片所在位置的完整路径，代替`im1.jpg`命令。例如，如果图片保存在`D:/projects/demo`中，代码应如下所示：
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s display the image using the following command:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过以下命令来显示图片：
- en: '[PRE4]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding command will be as follows:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的输出将如下所示：
- en: '![Figure 1.5: Output displayed after loading the image'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.5：加载图片后显示的输出'
- en: '](img/B15385_01_05.jpg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_01_05.jpg)'
- en: 'Figure 1.5: Output displayed after loading the image'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.5：加载图片后显示的输出
- en: 'Convert the image into a `numpy` array as the model expects it in this format:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图片转换为`numpy`数组，因为模型期望它是这种格式：
- en: '[PRE5]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Reshape the image into a four-dimensional format since that''s what is expected
    by the model:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图片调整为四维格式，因为这是模型期望的格式：
- en: '[PRE6]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Prepare the image for submission by running the `preprocess_input()` function:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`preprocess_input()`函数准备图片以供提交：
- en: '[PRE7]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the prediction:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行预测：
- en: '[PRE8]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The prediction results in a number that needs to be converted into the corresponding
    label in text format:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测结果是一个数字，需要将其转换为相应的文本格式标签：
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, type in the following code to display the label:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，键入以下代码以显示标签：
- en: '[PRE10]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Print the label using the following code:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码打印标签：
- en: '[PRE11]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you have followed the steps correctly so far, the output will be as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果到目前为止你已正确按照步骤操作，输出结果将如下所示：
- en: '[PRE12]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The model has successfully identified our image. Interesting, isn't it? In the
    next few steps, we'll take this a step further and convert this result into speech.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型已成功识别我们的图片。很有趣，不是吗？接下来的几个步骤，我们将进一步处理，将这个结果转化为语音。
- en: Tip
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小贴士
- en: While we have used an image of a pizza here, you can use just about any image
    with this model. We urge you to try out this exercise multiple times with different
    images.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然我们在这里使用了一张披萨的图片，但你可以使用任何图片来进行模型测试。我们建议你多次尝试使用不同的图片进行此练习。
- en: 'Prepare the text to be converted into speech:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备要转换为语音的文本：
- en: '[PRE13]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Install the `gtts` package, which is required for converting text into speech.
    This can be implemented in the Jupyter Notebook, as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`gtts`包，该包用于将文本转换为语音。可以在Jupyter Notebook中按如下方式实现：
- en: '[PRE14]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Import the required libraries:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE15]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding code will import two libraries. One is `gTTS`, that is, Google
    Text-to-Speech, which is a cloud-based open source API for converting text into
    speech. Another is the `os` library that is used to play the resulting audio file.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将导入两个库。一个是`gTTS`，即Google文本转语音服务，这是一个基于云的开源API，用于将文本转换为语音。另一个是`os`库，用于播放生成的音频文件。
- en: 'Call the `gTTS` API and pass the text as a parameter:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`gTTS` API并将文本作为参数传递：
- en: '[PRE16]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You need to be online while running the preceding step.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行上述步骤时，你需要保持在线状态。
- en: Save the resulting audio file. This file will be saved in the home directory
    where the Jupyter Notebook is being run.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存生成的音频文件。该文件将保存在运行Jupyter Notebook的主目录中。
- en: '[PRE17]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can also specify the path where you want it to be saved by including the
    absolute path in front of the name; for example, `(myobj.save('D:/projects/prediction.mp3')`.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以通过在文件名之前指定绝对路径来设置保存位置；例如，`(myobj.save('D:/projects/prediction.mp3')`。
- en: 'Play the audio file:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 播放音频文件：
- en: '[PRE18]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you have correctly followed the preceding steps, you will hear the words
    `This is a pizza` being spoken.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你正确地遵循了前面的步骤，你将听到`This is a pizza`的语音。
- en: Note
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZPZx8B](https://packt.live/2ZPZx8B).
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2ZPZx8B](https://packt.live/2ZPZx8B)。
- en: You can also run this example online at [https://packt.live/326cRIu](https://packt.live/326cRIu).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/326cRIu](https://packt.live/326cRIu)在线运行这个示例。你必须执行整个笔记本才能获得预期的结果。
- en: In this exercise, we learned how to build a deep learning model by making use
    of publicly available models using a few lines of code in TensorFlow. Now that
    you have got a taste of deep learning, let's move forward and learn about the
    different building blocks of deep learning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何通过使用公共可用的模型并用几行代码在TensorFlow中构建深度学习模型。现在你已经体验了深度学习，让我们继续前进，了解深度学习的不同构建块。
- en: Deep Learning Models
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型
- en: At the heart of most of the popular deep learning models are ANNs, which are
    inspired by our knowledge of how the brain works. Even though no single model
    can be called perfect, different models perform better in different scenarios.
    In the sections that follow, we will learn about some of the most prominent models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数流行的深度学习模型的核心是人工神经网络（ANN），其灵感来源于我们对大脑工作原理的认识。虽然没有任何单一模型可以称为完美，但不同的模型在不同的场景下表现更好。在接下来的章节中，我们将了解一些最突出的模型。
- en: The Multi-Layer Perceptron
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'The **multi-layer perceptron** (**MLP**) is a basic type of neural network.
    An MLP is also known as a feed-forward network. A representation of an MLP can
    be seen in the following figure:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）是一种基本的神经网络类型。MLP也被称为前馈网络。以下图所示可以看到MLP的表示：'
- en: '![Figure 1.6: MLP representation'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.6：MLP表示'
- en: '](img/B15385_01_06.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_06.jpg)'
- en: 'Figure 1.6: MLP representation'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：MLP表示
- en: One of the basic building blocks of an MLP (or any neural network) is a neuron.
    A network consists of multiple neurons connected to successive layers. At a very
    basic level, an MLP will consist of an input layer, a hidden layer, and an output
    layer. The input layer will have neurons equal to the input data. Each input neuron
    will have a connection to all the neurons of the hidden layer. The final hidden
    layer will be connected to the output layer. The MLP is a very useful model and
    can be tried out on various classification and regression problems. The concept
    of an MLP will be covered in detail in *Chapter 2*, *Neural Networks.*
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP，或任何神经网络）的基本构建块之一是神经元。一个网络由多个神经元连接到后续的层。非常基础的MLP由输入层、隐藏层和输出层组成。输入层的神经元数量与输入数据相等。每个输入神经元将与隐藏层的所有神经元相连接。最终的隐藏层将与输出层连接。MLP是一个非常有用的模型，可以尝试应用于各种分类和回归问题。MLP的概念将在*第二章*，*神经网络*中详细介绍。
- en: Convolutional Neural Networks
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: A convolutional neural network (CNN) is a class of deep learning model that
    is predominantly used for image recognition. When we discussed the MLP, we saw
    that each neuron in a layer is connected to every other neuron in the subsequent
    layer. However, CNNs adopt a different approach and do not resort to such a fully
    connected architecture. Instead, CNNs extract local features from images, which
    are then fed to the subsequent layers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是一类深度学习模型，主要用于图像识别。当我们讨论MLP时，我们看到每一层的神经元都与后续层的每个神经元相连接。然而，CNN采用了不同的方法，并没有使用这种完全连接的架构。相反，CNN从图像中提取局部特征，然后将这些特征传递到后续层。
- en: 'CNNs rose to prominence in 2012 when an architecture called AlexNet won a premier
    competition called the **ImageNet Large-Scale Visual Recognition Challenge** **(ILSVRC)**.
    ILSVRC is a large-scale computer vision competition where teams from around the
    globe compete for the prize of the best computer vision model. Through the 2012
    research paper titled *ImageNet Classification with Deep Convolutional Neural
    Networks* ([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)),
    Alex Krizhevsky, et al. (University of Toronto) showcased the true power of CNN
    architectures, which eventually won them the 2012 ILSVRC challenge. The following
    figure depicts the structure of the *AlexNet* model, a CNN model whose high performance
    catapulted CNNs to prominence in the deep learning domain. While the structure
    of this model may look complicated to you, in *Chapter 3*, *Image Classification
    with Convolutional Neural Networks*, the working of such CNN networks will be
    explained to you in detail:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在2012年崭露头角，当时名为AlexNet的架构在一个名为**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）的顶级竞赛中获胜。ILSVRC是一个大规模计算机视觉竞赛，全球各地的团队竞相争夺最佳计算机视觉模型的奖项。在2012年的研究论文《*ImageNet
    分类与深度卷积神经网络*》([https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks))中，Alex
    Krizhevsky等人（多伦多大学）展示了CNN架构的真正强大力量，最终赢得了2012年ILSVRC挑战赛。下图展示了*AlexNet*模型的结构，这是一个CNN模型，其卓越的性能使得CNN在深度学习领域声名鹊起。尽管这个模型的结构看起来可能对你来说比较复杂，但在*第3章*《卷积神经网络图像分类》中，这种CNN网络的工作原理会被详细讲解：
- en: '![Figure 1.7: CNN architecture of the AlexNet model'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7：AlexNet模型的CNN架构'
- en: '](img/B15385_01_07.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_07.jpg)'
- en: 'Figure 1.7: CNN architecture of the AlexNet model'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：AlexNet模型的CNN架构
- en: Note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The aforementioned diagram is sourced from the popular research paper: Krizhevsky,
    Alex & Sutskever, Ilya & Hinton, Geoffrey. (2012). *ImageNet Classification with
    Deep Convolutional Neural Networks.* Neural Information Processing Systems. 25\.
    10.1145/3065386.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表来源于著名的研究论文：Krizhevsky, Alex & Sutskever, Ilya & Hinton, Geoffrey. (2012).
    *ImageNet 分类与深度卷积神经网络*。神经信息处理系统。25. 10.1145/3065386。
- en: Since 2012, there have been many breakthrough CNN architectures expanding the
    possibilities for computer vision. Some of the prominent architectures are ZFNet,
    Inception (GoogLeNet), VGG, and ResNet.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年以来，许多突破性的CNN架构扩展了计算机视觉的可能性。一些著名的架构有ZFNet、Inception（GoogLeNet）、VGG和ResNet。
- en: 'Some of the most prominent use cases where CNNs are put to use are as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: CNN应用最为显著的一些用例如下：
- en: Image recognition and **optical character recognition** (**OCR**)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别和**光学字符识别**（**OCR**）
- en: Face recognition on social media
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体上的人脸识别
- en: Text classification
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Object detection for self-driving cars
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车的物体检测
- en: Image analysis for health care
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗健康领域的图像分析
- en: Another great benefit of working with deep learning is that you needn't always
    build your models from scratch – you could use models built by others and use
    them for your own applications. This is known as "transfer learning", and it allows
    you to benefit from the active deep learning community.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习的另一个巨大好处是，你不必总是从零开始构建模型——你可以使用他人已经构建的模型，并将其用于自己的应用。这就是所谓的“迁移学习”，它使你能够从活跃的深度学习社区中受益。
- en: We will apply transfer learning to image processing and learn about CNNs and
    their dynamics in detail in *Chapter 3*, *Image Classification with Convolutional
    Neural Networks*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第3章*《卷积神经网络图像分类》中应用迁移学习于图像处理，并详细了解CNN及其动态。
- en: Recurrent Neural Networks
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'In traditional neural networks, the inputs are independent of the outputs.
    However, in cases such as language translation, where there is dependence on the
    words preceding and succeeding a word, there is a need to understand the dynamics
    of the sequences in which words appear. This problem was solved by a class of
    networks called **recurrent neural networks** (**RNNs**). RNNs are a class of
    deep learning networks where the output from the previous step is sent as input
    to the current step. A distinct characteristic of an RNN is a hidden layer, which
    remembers the information of other inputs in a sequence. A high-level representation
    of an RNN can be seen in the following figure. You''ll learn more about the inner
    workings of these networks in *Chapter 5,* *Deep Learning for Sequences*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的神经网络中，输入与输出是相互独立的。然而，在语言翻译等场景中，单词前后存在依赖关系，因此需要理解单词出现顺序的动态特性。这个问题通过一种被称为**循环神经网络**（**RNNs**）的网络类别得到了解决。RNNs是一类深度学习网络，其中前一步的输出作为当前步骤的输入。RNN的一个显著特点是隐藏层，它能够记住序列中其他输入的信息。以下图可以看到RNN的高级表示。你将在*第五章*，*深度学习与序列*中深入了解这些网络的内部工作原理：
- en: '![Figure 1.8: Structure of RNNs'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8：RNN的结构'
- en: '](img/B15385_01_08.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_08.jpg)'
- en: 'Figure 1.8: Structure of RNNs'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：RNN的结构
- en: There are different types of RNN architecture. Some of the most prominent ones
    are **long short-term memory** (**LSTM**) and **gated recurrent units** (**GRU**).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: RNN架构有不同的类型。其中一些最著名的类型是**长短时记忆网络**（**LSTM**）和**门控循环单元**（**GRU**）。
- en: 'Some of the important use cases for RNNs are as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的一些重要应用案例如下：
- en: Language modeling and text generation
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模与文本生成
- en: Machine translation
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Speech recognition
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Generating image descriptions
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成图像描述
- en: RNNs will be covered in detail in *Chapter 5*, *Deep Learning for Sequences*,
    and *Chapter 6,* *LSTMs, GRUs, and Advanced RNNs*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: RNN将在*第五章*，*深度学习与序列*和*第六章*，*LSTMs、GRUs及高级RNN*中详细讲解。
- en: Generative Adversarial Networks
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative adversarial networks** (**GANs**) are networks that are capable
    of generating data distributions similar to any real data distributions. One of
    the pioneers of deep learning, Yann LeCun, described GANs as one of the most promising
    ideas in deep learning in the last decade.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）是一种能够生成与任何真实数据分布相似的数据分布的网络。深度学习的先驱之一Yann LeCun曾表示，GANs是过去十年中深度学习领域最具前景的想法之一。'
- en: To give you an example, suppose we want to generate images of dogs from random
    noise data. For this, we train a GAN network with real images of dogs and the
    noise data until we generate data that looks like the real images of dogs. The
    following diagram explains the concept behind GANs. At this stage, you might not
    fully understand this concept. It will be explained in detail in *Chapter 7*,
    *Generative Adversarial Networks*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们想从随机噪声数据生成狗的图像。为此，我们训练一个GAN网络，使用真实的狗的图像和噪声数据，直到我们生成的图像看起来像真实的狗的图像。以下图解释了GAN的基本概念。在这个阶段，你可能还不完全理解这个概念。它将在*第七章*，*生成对抗网络*中详细讲解。
- en: '![Figure 1.9: Structure of GANs'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.9：GANs的结构'
- en: '](img/B15385_01_09.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_09.jpg)'
- en: 'Figure 1.9: Structure of GANs'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：GANs的结构
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The aforementioned diagram is sourced from the popular research paper: Barrios,
    Buldain, Comech, Gilbert & Orue (2019). *Partial Discharge Classification Using
    Deep Learning Methods—Survey of Recent Progress* ([https://doi.org/10.3390/en12132485](https://doi.org/10.3390/en12132485)).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表来源于一篇流行的研究论文：Barrios, Buldain, Comech, Gilbert & Orue (2019)。*利用深度学习方法进行局部放电分类——最近进展综述*（[https://doi.org/10.3390/en12132485](https://doi.org/10.3390/en12132485)）。
- en: 'GANs are a big area of research, and there are many use cases for them. Some
    of the useful applications of GANs are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是一个重要的研究领域，并且有许多应用案例。以下是一些GANs的有用应用：
- en: Image translation
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像翻译
- en: Text to image synthesis
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像合成
- en: Generating videos
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成视频
- en: The restoration of art
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艺术修复
- en: GANs will be covered in detail in *Chapter 7*, *Generative Adversarial Networks*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: GANs将在*第七章*，*生成对抗网络*中详细讲解。
- en: 'The possibilities and promises of deep learning are huge. Deep learning applications
    have become ubiquitous in our daily lives. Some notable examples are as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的可能性和前景是巨大的。深度学习应用已无处不在，成为我们日常生活的一部分。以下是一些显著的例子：
- en: Chatbots
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人
- en: Robots
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人
- en: Smart speakers (such as Alexa)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能音响（例如Alexa）
- en: Virtual assistants
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟助手
- en: Recommendation engines
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎
- en: Drones
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无人机
- en: Self-driving cars or autonomous vehicles
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车或自动化车辆
- en: This ever-expanding canvas of possibilities makes it a great toolset in the
    arsenal of a data scientist. This book will progressively introduce you to the
    amazing world of deep learning and make you adept at applying it to real-world
    scenarios.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不断扩展的可能性画布使它成为数据科学家工具箱中的一个重要工具。本书将逐步引导你进入深度学习的奇妙世界，并使你能够将其应用于现实世界的场景。
- en: Introduction to TensorFlow
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 简介
- en: 'TensorFlow is a deep learning library developed by Google. At the time of writing
    this book, TensorFlow is by far the most popular deep learning library. It was
    originally developed by a team within Google called the Google Brain team for
    their internal use and was subsequently open sourced in 2015\. The Google Brain
    team has developed popular applications such as Google Photos and Google Cloud
    Speech-to-Text, which are deep learning applications based on TensorFlow. TensorFlow
    1.0 was released in 2017, and within a short period of time, it became the most
    popular deep learning library ahead of other existing libraries, such as Caffe,
    Theano, and PyTorch. It is considered the industry standard, and almost every
    organization that is doing something in the deep learning space has adopted it.
    Some of the key features of TensorFlow are as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由 Google 开发的深度学习库。在撰写本书时，TensorFlow 是迄今为止最流行的深度学习库。最初，它由 Google
    内部的一个团队——Google Brain 团队开发，用于内部使用，并于 2015 年开源。Google Brain 团队开发了像 Google Photos
    和 Google Cloud Speech-to-Text 这样的流行应用，这些应用基于 TensorFlow，属于深度学习应用。TensorFlow 1.0
    于 2017 年发布，并在短时间内超越了其他现有的库，如 Caffe、Theano 和 PyTorch，成为最受欢迎的深度学习库。它被认为是行业标准，几乎每个从事深度学习的组织都在使用它。TensorFlow
    的一些关键特点如下：
- en: It can be used with all common programming languages, such as Python, Java,
    and R
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与所有常见的编程语言一起使用，如 Python、Java 和 R。
- en: It can be deployed on multiple platforms, including Android and Raspberry Pi
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以部署在多个平台上，包括 Android 和 Raspberry Pi。
- en: It can run in a highly distributed mode and hence is highly scalable
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以以高度分布的模式运行，因此具有高度的可扩展性。
- en: After being in Alpha/Beta release for a long time, the final version of TensorFlow
    2.0 was released on September 30, 2019\. The focus of TF2.0 was to make the development
    of deep learning applications easier. Let's go ahead and understand the basics
    of the TensorFlow 2.0 framework.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了长时间的 Alpha/Beta 发布后，TensorFlow 2.0 的最终版本于 2019 年 9 月 30 日发布。TF2.0 的重点是使深度学习应用的开发更加简便。接下来我们将一起了解
    TensorFlow 2.0 框架的基础知识。
- en: '**Tensors**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量**'
- en: Inside the TensorFlow program, every data element is called a **tensor**. A
    tensor is a representation of vectors and matrices in higher dimensions. The rank
    of a tensor denotes its dimensions. Some of the common data forms represented
    as tensors are as follows.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 程序中，每个数据元素都叫做**张量**。张量是向量和矩阵在更高维度下的表示。张量的秩表示其维度。以下是一些常见的数据形式，以张量的形式表示：
- en: '**Scalar**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**标量**'
- en: A scalar is a tensor of rank 0, which only has magnitude.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 标量是秩为 0 的张量，它只有大小。
- en: For example, `[ 12 ]` is a scalar of magnitude 12.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`[ 12 ]` 是一个大小为 12 的标量。
- en: '**Vector**'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量**'
- en: A vector is a tensor of rank 1.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是秩为 1 的张量。
- en: For example, `[ 10 , 11, 12, 13]`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`[ 10 , 11, 12, 13]`。
- en: '**Matrix**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵**'
- en: A matrix is a tensor of rank 2.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵是秩为 2 的张量。
- en: For example, `[ [10,11] , [12,13] ]`. This tensor has two rows and two columns.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`[ [10,11] , [12,13] ]`。这个张量有两行两列。
- en: '**Tensor of rank 3**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**秩为 3 的张量**'
- en: 'This is a tensor in three dimensions. For example, image data is predominantly
    a three-dimensional tensor with width, height, and the number of channels as its
    three dimensions. The following is an example of a tensor with three dimensions,
    that is, it has two rows, three columns, and three channels:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个三维张量。例如，图像数据通常是一个三维张量，具有宽度、高度和通道数作为其三个维度。以下是一个三维张量的例子，即它有两行、三列和三个通道：
- en: '![Figure 1.10: Tensor with three dimensions'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10：具有三维的张量'
- en: '](img/B15385_01_10.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_10.jpg)'
- en: 'Figure 1.10: Tensor with three dimensions'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：三维张量
- en: 'The shape of a tensor is represented by an array and indicates the number of
    elements in each dimension. For example, if the shape of a tensor is [2,3,5],
    it means the tensor has three dimensions. If this were to be image data, this
    shape would mean that this tensor has two rows, three columns, and five channels.
    We can also get the rank from the shape. In this example, the rank of the tensor
    is three, since there are three dimensions. This is further illustrated in the
    following diagram:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的形状由一个数组表示，表示每个维度中的元素个数。例如，如果一个张量的形状是 [2,3,5]，这意味着该张量有三个维度。如果这是图像数据，则此形状表示该张量有两行、三列和五个通道。我们还可以从形状中获取秩。在这个例子中，张量的秩是三，因为有三个维度。下面的图示进一步说明了这一点：
- en: '![Figure 1.11: Examples of Tensor rank and shape'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11：张量的秩和形状示例'
- en: '](img/B15385_01_11.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_11.jpg)'
- en: 'Figure 1.11: Examples of Tensor rank and shape'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：张量的秩和形状示例
- en: Constants
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常量
- en: 'Constants are used to store values that are not changed or modified during
    the course of the program. There are multiple ways in which a constant can be
    created, but the simplest way is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 常量用于存储在程序执行过程中不会被改变或修改的值。创建常量有多种方式，最简单的一种如下：
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This creates a tensor initialized to 10\. Keep in mind that a constant''s value
    cannot be updated or modified by reassigning a new value to it. Another example
    is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个初始化为 10 的张量。请记住，常量的值不能通过重新赋值来更新或修改。另一个示例如下：
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this line, we are instantiating a string as a constant.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行中，我们正在将一个字符串实例化为常量。
- en: Variables
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量
- en: 'A variable is used to store data that can be updated and modified during the
    course of the program. We will look at this in more detail in *Chapter 2*, *Neural
    Networks*. There are multiple ways of creating a variable, but the simplest way
    is as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 变量用于存储在程序执行过程中可以更新和修改的数据。我们将在*第二章*，*神经网络*中详细讨论这一点。创建变量有多种方式，最简单的一种如下：
- en: '[PRE21]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, the variable `b` is initialized to `20`. Note that in
    TensorFlow, unlike constants, the term `Variable` is written with an uppercase
    `V`.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，变量`b`被初始化为`20`。请注意，在 TensorFlow 中，与常量不同，`Variable` 这个术语的首字母是大写的。
- en: 'A variable can be reassigned a different value during the course of the program.
    Variables can be used to assign any type of object, including scalars, vectors,
    and multi-dimensional arrays. The following is an example of how an array whose
    dimensions are 3 x 3 can be created in TensorFlow:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 变量可以在程序执行过程中重新赋予不同的值。变量可以用于赋值任何类型的对象，包括标量、向量和多维数组。以下是如何在 TensorFlow 中创建一个维度为
    3 x 3 的数组的示例：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This variable can be initialized to a 3 x 3 matrix, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变量可以初始化为一个 3 x 3 的矩阵，如下所示：
- en: '![Figure 1.12: 3 x 3 matrix'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12：3 x 3 矩阵'
- en: '](img/B15385_01_12.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_12.jpg)'
- en: 'Figure 1.12: 3 x 3 matrix'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：3 x 3 矩阵
- en: Now that we know some of the basic concepts of TensorFlow, let's learn how to
    put them into practice.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 TensorFlow 的一些基本概念，接下来让我们学习如何将它们付诸实践。
- en: Defining Functions in TensorFlow
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义函数
- en: 'A function can be created in Python using the following syntax:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中可以使用以下语法创建函数：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A function is initiated using the special operator `def`, followed by the name
    of the function, `myfunc`, and the arguments for the function. In the preceding
    example, the body of the function is in the second line, and the last line returns
    the output.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特殊运算符`def`来初始化一个函数，接着是函数的名称`myfunc`，以及函数的参数。在前面的示例中，函数体位于第二行，最后一行返回输出。
- en: In the following exercise, we will learn how to implement a small function using
    the variables and constants we defined earlier.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将学习如何使用之前定义的变量和常量来实现一个简单的函数。
- en: 'Exercise 1.02: Implementing a Mathematical Equation'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.02：实现一个数学方程
- en: 'In this exercise, we will solve the following mathematical equation using TensorFlow:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 TensorFlow 求解以下数学方程：
- en: '![Figure 1.13: Mathematical equation to be solved using TensorFlow'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：使用 TensorFlow 求解的数学方程'
- en: '](img/B15385_01_13.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_13.jpg)'
- en: 'Figure 1.13: Mathematical equation to be solved using TensorFlow'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：使用 TensorFlow 求解的数学方程
- en: 'We will use TensorFlow to solve it, as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 TensorFlow 来求解它，如下所示：
- en: '[PRE24]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'While there are multiple ways of doing this, we will only explore one of the
    ways in this exercise. Follow these steps to complete this exercise:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有多种方法可以实现这一点，但在本练习中我们只会探索其中的一种方法。按照以下步骤完成此练习：
- en: Open a new Jupyter Notebook and rename it *Exercise 1.02*.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook，并将其重命名为*Exercise 1.02*。
- en: 'Import the TensorFlow library using the following command:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令导入TensorFlow库：
- en: '[PRE25]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let''s solve the equation. For that, you will need to create two variables,
    `X` and `Y`, and initialize them to the given values of `3` and `4`, respectively:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们解这个方程。为此，你需要创建两个变量，`X`和`Y`，并分别将它们初始化为给定的值`3`和`4`：
- en: '[PRE26]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In our equation, the value of `2` isn''t changing, so we''ll store it as a
    constant by typing the following code:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的方程中，`2`的值没有变化，因此我们将它作为常量存储，代码如下：
- en: '[PRE27]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define the function that will solve our equation:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来解决我们的方程：
- en: '[PRE28]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Call the function by passing `X`, `Y`, and `C` as parameters. We''ll be storing
    the output of this function in a variable called `result`:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过传递`X`，`Y`和`C`作为参数来调用该函数。我们将把该函数的输出存储在一个名为`result`的变量中：
- en: '[PRE29]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Print the result using the `tf.print()` function:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.print()`函数打印结果：
- en: '[PRE30]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output will be as follows:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE31]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ClXKjj](https://packt.live/2ClXKjj).
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 若要访问该部分的源代码，请参阅[https://packt.live/2ClXKjj](https://packt.live/2ClXKjj)。
- en: You can also run this example online at [https://packt.live/2ZOIN1C](https://packt.live/2ZOIN1C).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2ZOIN1C](https://packt.live/2ZOIN1C)上运行这个示例。你必须执行整个Notebook才能得到预期的结果。
- en: In this exercise, we learned how to define and use a function. Those familiar
    with Python programming will notice that it is not a lot different from normal
    Python code.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何定义和使用一个函数。熟悉Python编程的人会注意到，这与正常的Python代码没有太大区别。
- en: In the rest of this chapter, we will prepare ourselves by looking at some basic
    linear algebra and familiarize ourselves with some of the common vector operations,
    so that understanding neural networks in the next chapter will be much easier.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将通过学习一些基本的线性代数，并熟悉一些常见的向量运算，为下一章的神经网络做准备，这样理解神经网络就会更加容易。
- en: Linear Algebra with TensorFlow
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行线性代数
- en: The most important linear algebra topic that will be used in neural networks
    is matrix multiplication. In this section, we will explain how matrix multiplication
    works and then use TensorFlow's built-in functions to solve some matrix multiplication
    examples. This is essential in preparation for neural networks in the next chapter.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中使用的最重要的线性代数主题是矩阵乘法。在这一节中，我们将解释矩阵乘法的原理，并使用TensorFlow的内置函数解决一些矩阵乘法的示例。这对下一章神经网络的准备工作至关重要。
- en: How does matrix multiplication work? You might have studied this as part of
    high school, but let's do a quick recap.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是如何工作的？你可能在高中时学过这个内容，但让我们快速回顾一下。
- en: 'Let''s say we have to perform a matrix multiplication between two matrices,
    A and B, where we have the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要执行两个矩阵A和B之间的矩阵乘法，其中我们有以下内容：
- en: '![Figure 1.14: Matrix A'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.14：矩阵A'
- en: '](img/B15385_01_14.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_14.jpg)'
- en: 'Figure 1.14: Matrix A'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14：矩阵A
- en: '![Figure 1.15: Matrix B'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.15：矩阵B'
- en: '](img/B15385_01_15.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_15.jpg)'
- en: 'Figure 1.15: Matrix B'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15：矩阵B
- en: The first step would be to check whether multiplying a 2 x 3 matrix by a 3 x
    2 matrix is possible. There is a prerequisite for matrix multiplication. Remember
    that C=R, that is, the number of columns (C) in the first matrix should be equal
    to the number of rows (R) in the second matrix. And remember the sequence matters
    here, and that's why, A x B is not equal to B x A. In this example, C=3 and R=3\.
    So, multiplication is possible.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是检查一个2x3的矩阵乘以一个3x2的矩阵是否可能。矩阵乘法有一个前提条件。记住C=R，即第一个矩阵的列数(C)应当等于第二个矩阵的行数(R)。并且要记住顺序很重要，这也是为什么A
    x B不等于B x A。在这个例子中，C=3，R=3。所以，乘法是可能的。
- en: The resultant matrix would have the number of rows equal to that in A and the
    number of columns equal to that in B. So, in this case, the result would be a
    2 x 2 matrix.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵的行数将与A相同，列数将与B相同。因此，在这种情况下，结果将是一个2x2的矩阵。
- en: 'To begin multiplying the two matrices, take the elements of the first row of
    A (R1) and the elements of the first column of B (C1):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始乘法运算，取A的第一行（R1）和B的第一列（C1）的元素：
- en: '![Figure 1.16: Matrix A(R1)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.16：矩阵A(R1)'
- en: '](img/B15385_01_16.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_16.jpg)'
- en: 'Figure 1.16: Matrix A(R1)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16：矩阵A(R1)
- en: '![Figure 1.17: Matrix B(C1)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.17：矩阵B(C1)'
- en: '](img/B15385_01_17.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_17.jpg)'
- en: 'Figure 1.17: Matrix B(C1)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17：矩阵 B(C1)
- en: 'Get the sum of the element-wise products, that is, (1 x 7) + (2 x 9) + (3 x
    11) = 58\. This will be the first element in the resultant 2 x 2 matrix. We''ll
    call this incomplete matrix D(i) for now:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 获取按元素乘积的和，即 (1 x 7) + (2 x 9) + (3 x 11) = 58。这个将是结果 2 x 2 矩阵中的第一个元素。我们暂时称这个为不完整矩阵
    D(i)：
- en: '![Figure 1.18: Incomplete matrix D(i)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.18：不完整矩阵 D(i)'
- en: '](img/B15385_01_18.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_18.jpg)'
- en: 'Figure 1.18: Incomplete matrix D(i)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18：不完整矩阵 D(i)
- en: 'Repeat this with the first row of A(R1) and the second column of B (C2):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 重复此操作，使用 A 的第一行（R1）和 B 的第二列（C2）：
- en: '![Figure 1.19: First row of matrix A'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.19：矩阵 A 的第一行'
- en: '](img/B15385_01_19.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_19.jpg)'
- en: 'Figure 1.19: First row of matrix A'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19：矩阵 A 的第一行
- en: '![Figure 1.20: Second column of matrix B'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.20：矩阵 B 的第二列'
- en: '](img/B15385_01_20.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_20.jpg)'
- en: 'Figure 1.20: Second column of matrix B'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20：矩阵 B 的第二列
- en: 'Get the sum of the products of the corresponding elements, that is, (1 x 8)
    + (2 x 10) + (3 x 12) = 64\. This will be the second element in the resultant
    matrix:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 获取对应元素的乘积之和，即 (1 x 8) + (2 x 10) + (3 x 12) = 64。这个将是结果矩阵中的第二个元素：
- en: '![Figure 1.21: Second element of matrix D(i)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.21：矩阵 D(i) 的第二个元素'
- en: '](img/B15385_01_21.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_21.jpg)'
- en: 'Figure 1.21: Second element of matrix D(i)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21：矩阵 D(i) 的第二个元素
- en: 'Repeat the same with the second row to get the final result:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第二行重复相同的操作，以得到最终结果：
- en: '![Figure 1.22: Matrix D'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.22：矩阵 D'
- en: '](img/B15385_01_22.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_22.jpg)'
- en: 'Figure 1.22: Matrix D'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22：矩阵 D
- en: 'The same matrix multiplication can be performed in TensorFlow using a built-in
    method called `tf.matmul()`. The matrices that need to be multiplied must be supplied
    to the model as variables, as shown in the following example:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的矩阵乘法可以通过 TensorFlow 中的内置方法 `tf.matmul()` 来执行。需要相乘的矩阵必须作为变量传递给模型，如下例所示：
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding case, A and B are the matrices that we want to multiply. Let's
    practice this method by using TensorFlow to multiply the two matrices we multiplied
    manually.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，A 和 B 是我们要进行乘法运算的矩阵。我们通过使用 TensorFlow 来练习这个方法，进行我们手动计算过的两个矩阵的乘法。
- en: 'Exercise 1.03: Matrix Multiplication Using TensorFlow'
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.03：使用 TensorFlow 进行矩阵乘法
- en: 'In this exercise, we will use the `tf.matmul()` method to multiply two matrices
    using `tensorflow`. Follow these steps to complete this exercise:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 `tf.matmul()` 方法通过 `tensorflow` 进行两个矩阵的乘法运算。按照以下步骤完成此练习：
- en: Open a new Jupyter Notebook and rename it *Exercise 1.03*.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并将其重命名为 *Exercise 1.03*。
- en: 'Import the `tensorflow` library and create two variables, `X` and `Y`, as matrices.
    `X` is a 2 x 3 matrix and `Y` is a 3 x 2 matrix:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `tensorflow` 库并创建两个变量 `X` 和 `Y`，它们是矩阵。`X` 是一个 2 x 3 的矩阵，`Y` 是一个 3 x 2 的矩阵：
- en: '[PRE33]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Print and display the values of `X` and `Y` to make sure the matrices are created
    correctly. We''ll start by printing the value of `X`:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印并显示 `X` 和 `Y` 的值，确保矩阵正确创建。我们首先打印 `X` 的值：
- en: '[PRE34]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output will be as follows:'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE35]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, let''s print the value of `Y`:'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们打印 `Y` 的值：
- en: '[PRE36]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will be as follows:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE37]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perform matrix multiplication by calling the TensorFlow `tf.matmul()` function:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 TensorFlow 的 `tf.matmul()` 函数执行矩阵乘法：
- en: '[PRE38]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To display the result, print the value of `c1`:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了显示结果，打印 `c1` 的值：
- en: '[PRE39]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be as follows:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE40]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s perform matrix multiplication by changing the order of the matrices:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过改变矩阵的顺序来执行矩阵乘法：
- en: '[PRE41]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To display the result, let''s print the value of `c2`:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了显示结果，让我们打印 `c2` 的值：
- en: '[PRE42]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The resulting output will be as follows.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果输出如下：
- en: '[PRE43]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that the results are different since we changed the order.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，由于我们改变了顺序，结果不同。
- en: Note
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3eevyw4](https://packt.live/3eevyw4).
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/3eevyw4](https://packt.live/3eevyw4)。
- en: You can also run this example online at [https://packt.live/2CfGGvE](https://packt.live/2CfGGvE).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以在线运行这个示例，网址为 [https://packt.live/2CfGGvE](https://packt.live/2CfGGvE)。你必须执行整个
    Notebook 才能得到预期的结果。
- en: In this exercise, we learned how to create matrices in TensorFlow and how to
    perform matrix multiplication. This will come in handy when we create our own
    neural networks.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何在 TensorFlow 中创建矩阵，以及如何执行矩阵乘法。这在我们创建自己的神经网络时将会非常有用。
- en: The reshape Function
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: reshape 函数
- en: 'Reshape, as the name suggests, changes the shape of a tensor from its current
    shape to a new shape. For example, you can reshape a 2 × 3 matrix to a 3 × 2 matrix,
    as shown here:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如名称所示，`reshape` 函数可以改变张量的形状，将其从当前形状转换为新的形状。例如，你可以将一个 2 × 3 的矩阵重塑为 3 × 2 的矩阵，如下所示：
- en: '![Figure 1.23: Reshaped matrix'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.23：重塑后的矩阵'
- en: '](img/B15385_01_23.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_23.jpg)'
- en: 'Figure 1.23: Reshaped matrix'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.23：重塑后的矩阵
- en: 'Let''s consider the following 2 × 3 matrix, which we defined as follows in
    the previous exercise:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下 2 × 3 的矩阵，它是我们在前一个练习中定义的：
- en: '[PRE44]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can print the shape of the matrix using the following code:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码打印矩阵的形状：
- en: '[PRE45]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'From the following output, we can see the shape, which we already know:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，我们可以看到形状，这是我们已经知道的：
- en: '[PRE46]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, to reshape `X` into a 3 × 2 matrix, TensorFlow provides a handy function
    called `tf.reshape()`. The function is implemented with the following arguments:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要将 `X` 重塑为一个 3 × 2 的矩阵，TensorFlow 提供了一个方便的函数，叫做 `tf.reshape()`。该函数通过以下参数来实现：
- en: '[PRE47]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, `X` is the matrix that needs to be reshaped, and `[3,2]`
    is the new shape that the `X` matrix has to be reshaped to.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`X` 是需要重塑的矩阵，`[3,2]` 是 `X` 矩阵需要重塑成的新形状。
- en: 'Reshaping matrices is a handy operation when implementing neural networks.
    For example, a prerequisite when working with images using CNNs is that the image
    has to be of rank 3, that is, it has to have three dimensions: width, height,
    and depth. If our image is a grayscale image that has only two dimensions, the
    `reshape` operation will come in handy to add a third dimension. In this case,
    the third dimension will be 1:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑矩阵是实现神经网络时常用的操作。例如，在使用 CNN 处理图像时，图像必须是 3 维的，也就是说，它必须有三个维度：宽度、高度和深度。如果我们的图像是一个只有两个维度的灰度图像，那么
    `reshape` 操作就能派上用场，来添加第三个维度。在这种情况下，第三个维度的大小将是 1：
- en: '![Figure 1.24: Changing the dimension using reshape()'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.24：使用 reshape() 改变维度'
- en: '](img/B15385_01_24.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_24.jpg)'
- en: 'Figure 1.24: Changing the dimension using reshape()'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.24：使用 reshape() 改变维度
- en: In the preceding figure, we are reshaping a matrix of shape `[5,4]` to a matrix
    of shape `[5,4,1]`. In the exercise that follows, we will be using the `reshape()`
    function to reshape a `[5,4]` matrix.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们将一个形状为 `[5,4]` 的矩阵重新调整为形状为 `[5,4,1]` 的矩阵。在接下来的练习中，我们将使用 `reshape()`
    函数将一个 `[5,4]` 矩阵进行重塑。
- en: 'There are some important considerations when implementing the `reshape()` function:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现 `reshape()` 函数时，有一些重要的注意事项：
- en: The total number of elements in the new shape should be equal to the total number
    of elements in the original shape. For example, you can reshape a 2 × 3 matrix
    (a total of 6 elements) to a 3 × 2 matrix since the new shape also has 6 elements.
    However, you cannot reshape it to 3 × 3 or 3 × 4.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新形状中的元素总数应与原始形状中的元素总数相等。例如，你可以将一个 2 × 3 的矩阵（总共 6 个元素）重塑为 3 × 2 的矩阵，因为新形状也有 6
    个元素。但是，你不能将它重塑为 3 × 3 或 3 × 4。
- en: The `reshape()` function should not be confused with `transpose()`. In `reshape()`,
    the sequence of the elements of the matrix is retained and the elements are rearranged
    in the new shape in the same sequence. However, in the case of `transpose()`,
    the rows become columns and the columns become rows. Hence the sequence of the
    elements will change.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshape()` 函数不应与 `transpose()` 混淆。在 `reshape()` 中，矩阵元素的顺序保持不变，元素在新形状中按照相同的顺序重新排列。然而，在
    `transpose()` 的情况下，行变成列，列变成行。因此，元素的顺序会发生变化。'
- en: 'The `reshape()` function will not change the original matrix unless you assign
    the new shape to it. Otherwise, it simply displays the new shape without actually
    changing the original variable. For example, let''s say `x` has shape [2,3] and
    you simply run `tf.reshape(x,[3,2])`. When you check the shape of `x` again, it
    will remain as [2,3]. In order to actually change the shape, you need to assign
    the new shape to it, like this:'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshape()` 函数不会改变原始矩阵，除非你将新形状赋值给它。否则，它只是显示新形状，而并不会实际更改原始变量。例如，假设 `x` 的形状是
    [2,3]，你只是执行了 `tf.reshape(x,[3,2])`。当你再次检查 `x` 的形状时，它依然是 [2,3]。为了实际改变形状，你需要将新形状赋值给它，像这样：'
- en: '[PRE48]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Let's try implementing `reshape()` in TensorFlow in the exercise that follows.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的练习中尝试在 TensorFlow 中实现 `reshape()`。
- en: 'Exercise 1.04: Reshaping Matrices Using the reshape() Function in TensorFlow'
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.04：使用 TensorFlow 中的 reshape() 函数重塑矩阵
- en: 'In this exercise, we will reshape a `[5,4]` matrix into the shape of `[5,4,1]`
    using the `reshape()` function. This exercise will help us understand how `reshape()`
    can be used to change the rank of a tensor. Follow these steps to complete this
    exercise:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用`reshape()`函数将一个`[5,4]`的矩阵重塑为`[5,4,1]`的形状。这个练习将帮助我们理解如何使用`reshape()`来改变张量的秩。按照以下步骤完成此练习：
- en: 'Open a Jupyter Notebook and rename it *Exercise 1.04*. Then, import `tensorflow`
    and create the matrix we want to reshape:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Jupyter Notebook并将其重命名为*练习1.04*。然后，导入`tensorflow`并创建我们想要重塑的矩阵：
- en: '[PRE49]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'First, we''ll print the variable `A` to check whether it is created correctly,
    using the following command:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将打印变量`A`，以检查它是否已正确创建，使用以下命令：
- en: '[PRE50]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output will be as follows:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE51]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s print the shape of `A`, just to be sure:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印一下`A`的形状，以确保正确：
- en: '[PRE52]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be as follows:'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE53]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Currently, it has a rank of 2\. We'll be using the `reshape()` function to change
    its rank to 3.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前它的秩是2。我们将使用`reshape()`函数将其秩更改为3。
- en: 'Now, we will reshape `A` to the shape [5,4,1] using the following command.
    We''ve thrown in the `print` command just to see what the output looks like:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用以下命令将`A`重塑为形状`[5,4,1]`。我们加入了`print`命令，以便查看输出结果：
- en: '[PRE54]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We''ll get the following output:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE55]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: That worked as expected.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这按预期工作。
- en: 'Let''s see the new shape of `A`:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看`A`的新形状：
- en: '[PRE56]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output will be as follows:'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE57]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We can see that `A` still has the same shape. Remember that we discussed that
    in order to save the new shape, we need to assign it to itself. Let's do that
    in the next step.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到`A`仍然具有相同的形状。记得我们讨论过，为了保存新的形状，我们需要将其赋值给自己。我们将在下一步中执行此操作。
- en: 'Here, we''ll assign the new shape to `A`:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将新的形状赋给`A`：
- en: '[PRE58]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s check the new shape of `A` once again:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再检查一次`A`的新形状：
- en: '[PRE59]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We will see the following output:'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE60]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: With that, we have not just reshaped the matrix but also changed its rank from
    2 to 3\. In the next step, let's print out the contents of `A` just to be sure.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，我们不仅重新塑造了矩阵，还将其秩从2更改为3。在下一步中，让我们打印出`A`的内容，以确保无误。
- en: 'Let''s see what `A` contains now:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看`A`现在包含了什么：
- en: '[PRE61]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output, as expected, will be as follows:'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果，如预期的那样，将如下所示：
- en: '[PRE62]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gHvyGQ](https://packt.live/3gHvyGQ).
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3gHvyGQ](https://packt.live/3gHvyGQ)。
- en: You can also run this example online at [https://packt.live/2ZdjdUY](https://packt.live/2ZdjdUY).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/2ZdjdUY](https://packt.live/2ZdjdUY)上在线运行这个例子。您必须执行整个Notebook才能获得所需的结果。
- en: In this exercise, we saw how to use the `reshape()` function. Using `reshape()`,
    we can change the rank and shape of tensors. We also learned that reshaping a
    matrix changes the shape of the matrix without changing the order of the elements
    within the matrix. Another important thing that we learned was that the reshape
    dimension has to align with the number of elements in the matrix. Having learned
    about the `reshape` function, we will go ahead and learn about the next function,
    which is Argmax.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们学习了如何使用`reshape()`函数。通过`reshape()`，我们可以改变张量的秩和形状。我们还了解到，重塑矩阵会改变矩阵的形状，但不会改变矩阵中元素的顺序。另一个我们学到的重要内容是，重塑的维度必须与矩阵中的元素数量对齐。了解了`reshape`函数后，我们将继续学习下一个函数——`Argmax`。
- en: The argmax Function
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: argmax函数
- en: Now, let's understand the `argmax` function, which is frequently used in neural
    networks. `Argmax` returns the position of the maximum value along a particular
    axis in a matrix or tensor. It must be noted that it does not return the maximum
    value, but rather the index position of the maximum value.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解`argmax`函数，它在神经网络中经常使用。`argmax`返回矩阵或张量沿某个特定轴的最大值位置。需要注意的是，它并不会返回最大值本身，而是返回最大值的索引位置。
- en: For example, if `x` = `[1,10,3,5]`, then `tf.argmax(x)` will return 1 since
    the maximum value (which in this case is 10) is in the index position 1.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果`x` = `[1,10,3,5]`，那么`tf.argmax(x)`将返回1，因为最大值（在这种情况下是10）位于索引位置1。
- en: Note
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In Python, the index starts with 0\. So, considering the preceding example of
    `x`, the element 1 will have an index of 0, 10 will have an index of 1, and so
    on.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，索引是从0开始的。所以，考虑到前面的`x`例子，元素1的索引为0，10的索引为1，依此类推。
- en: 'Now, let''s say we have the following:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有以下内容：
- en: '![Figure 1.25: An example matrix'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.25：示例矩阵'
- en: '](img/B15385_01_25.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_25.jpg)'
- en: 'Figure 1.25: An example matrix'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.25：示例矩阵
- en: 'In this case, `argmax` has to be used with the `axis` parameter. When `axis`
    equals 0, it returns the position of the maximum value in each column, as shown
    in the following figure:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`argmax`必须与`axis`参数一起使用。当`axis`等于0时，它返回每列中最大值的位置，如下图所示：
- en: '![Figure 1.26: The argmax operation along axis 0'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.26：沿轴0进行的argmax操作'
- en: '](img/B15385_01_26.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_26.jpg)'
- en: 'Figure 1.26: The argmax operation along axis 0'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.26：沿轴0进行的argmax操作
- en: 'As you can see, the maximum value in the first column is 9, so the index, in
    this case, will be 2\. Similarly, if we move along to the second column, the maximum
    value is 5, which has an index of 0\. In the third column, the maximum value is
    8, and hence the index is 1\. If we were to run the `argmax` function on the preceding
    matrix with the `axis` as 0, we would get the following output:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，第一列的最大值是9，因此在这种情况下，索引为2。同样，若我们查看第二列，最大值是5，其索引为0。在第三列，最大值为8，因此索引为1。如果我们在前述矩阵上运行`argmax`函数并将`axis`设置为0，我们将得到以下输出：
- en: '[PRE63]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'When `axis` = 1, `argmax` returns the position of the maximum value across
    each row, like this:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 当`axis` = 1时，`argmax`返回每行最大值的位置，如下所示：
- en: '![Figure 1.27: The argmax operation along axis 1'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.27：沿轴1进行的argmax操作'
- en: '](img/B15385_01_27.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_27.jpg)'
- en: 'Figure 1.27: The argmax operation along axis 1'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.27：沿轴1进行的argmax操作
- en: 'Moving along the rows, we have 5 at index 1, 8 at index 2, and 9 at index 0\.
    If we were to run the `argmax` function on the preceding matrix with the axis
    as 1, we would get the following output:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着行移动，我们在索引1处有5，在索引2处有8，在索引0处有9。如果我们在前述矩阵上运行`argmax`函数，并将`axis`设置为1，我们将得到以下输出：
- en: '[PRE64]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: With that, let's try and implement `argmax` on a matrix.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试在矩阵上实现`argmax`。
- en: 'Exercise 1.05: Implementing the argmax() Function'
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.05：实现argmax()函数
- en: 'In this exercise, we are going to use the `argmax` function to find the position
    of the maximum value in a given matrix along axes 0 and 1\. Follow these steps
    to complete this exercise:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用`argmax`函数在给定矩阵的轴0和轴1上找到最大值的位置。请按照以下步骤完成此练习：
- en: 'Import `tensorflow` and create the following matrix:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`tensorflow`并创建以下矩阵：
- en: '[PRE65]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let''s print `X` and see what the matrix looks like:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印`X`并查看矩阵的样子：
- en: '[PRE66]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output will be as follows:'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE67]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Print the shape of `X`:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`X`的形状：
- en: '[PRE68]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output will be as follows:'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE69]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, let''s use `argmax` to find the positions of the maximum values while
    keeping `axis` as `0`:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`argmax`在保持`axis`为`0`的情况下找到最大值的位置：
- en: '[PRE70]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output will be as follows:'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE71]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Referring to the matrix in *Step 2*, we can see that, moving across the columns,
    the index of the maximum value (91) in the first column is 0\. Similarly, the
    index of the maximum value along the second column (88) is 1\. And finally, the
    maximum value across the third column (75) has index 2\. Hence, we have the aforementioned
    output.
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参考*步骤2*中的矩阵，我们可以看到，沿着列移动，第一列中最大值（91）的索引是0。同样，第二列中最大值（88）的索引是1。最后，第三列中最大值（75）的索引是2。因此，我们得到了上述输出。
- en: 'Now, let''s change the `axis` to `1`:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将`axis`改为`1`：
- en: '[PRE72]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output will be as follows:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE73]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Again, referring to the matrix in *Step 2*, if we move along the rows, the maximum
    value along the first row is 91, which is at index 0\. Similarly, the maximum
    value along the second row is 88, which is at index 1\. Finally, the third row
    is at index 0 again, with a maximum value of 75.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 再次参考*步骤2*中的矩阵，如果我们沿着行移动，第一行中的最大值是91，索引为0。同样，第二行中的最大值是88，索引为1。最后，第三行的最大值是75，索引又是0。
- en: Note
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZR5q5p](https://packt.live/2ZR5q5p).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/2ZR5q5p](https://packt.live/2ZR5q5p)。
- en: You can also run this example online at [https://packt.live/3eewhNO](https://packt.live/3eewhNO).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3eewhNO](https://packt.live/3eewhNO)在线运行此示例。您必须执行整个Notebook才能获得所需的结果。
- en: In this exercise, we learned how to use the `argmax` function to find the position
    of the maximum value along a given axis of a tensor. This will be used in the
    subsequent chapters when we perform classification using neural networks.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们学习了如何使用`argmax`函数来找到张量给定轴上最大值的位置。这将在后续章节中用于使用神经网络进行分类时。
- en: Optimizers
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: Before we look at neural networks, let's learn about one more important concept,
    and that is optimizers. Optimizers are extensively used for training neural networks,
    so it is important to understand their application. In this chapter, let's get
    a basic introduction to the concept of an optimizer. As you might already be aware,
    the purpose of machine learning is to find a function (along with its parameters)
    that maps inputs to outputs.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究神经网络之前，让我们先了解另一个重要的概念，那就是优化器。优化器广泛应用于训练神经网络，因此理解其应用非常重要。在本章中，让我们对优化器的概念做一个基本的介绍。正如你可能已经知道的，机器学习的目的是找到一个函数（以及它的参数），该函数将输入映射到输出。
- en: 'For example, let''s say the original function of a data distribution is a linear
    function (linear regression) of the following form:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设一个数据分布的原始函数是以下形式的线性函数（线性回归）：
- en: '[PRE74]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Here, `Y` is the dependent variable (label), `X` the independent variable (features),
    and `m` and `b` are the parameters of the model. Solving this problem with machine
    learning would entail learning the parameters `m` and `b` and thereby the form
    of the function that connects `X` to `Y`. Once the parameters have been learned,
    if we are given a new value for `X`, we can calculate or predict the value of
    `Y`. It is in learning these parameters that optimizers come into play. The learning
    process entails the following steps:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Y`是因变量（标签），`X`是自变量（特征），`m`和`b`是模型的参数。使用机器学习解决这个问题就是学习`m`和`b`这两个参数，从而得出将`X`与`Y`联系起来的函数形式。一旦这些参数被学习到，如果我们给定一个新的`X`值，我们就可以计算或预测`Y`的值。在学习这些参数的过程中，优化器发挥了作用。学习过程包括以下几个步骤：
- en: Assume some arbitrary random values for the parameters `m` and `b`.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设`m`和`b`是一些任意的随机值。
- en: With these assumed parameters, for a given dataset, estimate the values of `Y`
    for each `X` variable.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些假设的参数下，对于给定的数据集，估算每个`X`变量的`Y`值。
- en: Find the difference between the predicted value of `Y` and the actual value
    of `Y` associated with the `X` variable. This difference is called the **loss
    function** or **cost function**. The magnitude of loss will depend on the parameter
    values we initially assumed. If the assumptions were way off the actual values,
    then the loss will be high. The way to get toward the right parameter is by changing
    or altering the initial assumed values of the parameters in such a way that the
    loss function is minimized. This task of changing the values of the parameters
    to reduce the loss function is called optimization.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到`Y`的预测值和与`X`变量相关的实际值之间的差异。这个差异称为**损失函数**或**代价函数**。损失的大小将取决于我们最初假设的参数值。如果假设与实际值相差甚远，那么损失就会很大。通过改变或调整参数的初始假设值，使得损失函数最小化，就能接近正确的参数。这一改变参数值以减少损失函数的过程称为优化。
- en: 'There are different types of optimizers that are used in deep learning. Some
    of the most popular ones are stochastic gradient descent, Adam, and RMSprop. The
    detailed functionality and the internal workings of optimizers will be described
    in *Chapter 2, Neural Networks*, but here, we will see how they are applied in
    solving certain common problems, such as simple linear regression. In this chapter,
    we will be using an optimizer called Adam, which is a very popular optimizer.
    We can define the Adam optimizer in TensorFlow using the following code:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，有不同类型的优化器。一些最常用的优化器包括随机梯度下降、Adam和RMSprop。优化器的详细功能和内部工作原理将在*第2章，神经网络*中进行描述，但在这里，我们将看到它们如何应用于解决一些常见问题，比如简单线性回归。在本章中，我们将使用一个非常流行的优化器——Adam。我们可以使用以下代码在TensorFlow中定义Adam优化器：
- en: '[PRE75]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Once an optimizer has been defined, we can use it to minimize the loss using
    the following code:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了优化器，我们可以使用以下代码来最小化损失：
- en: '[PRE76]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The terms `[m,b]` are the parameters that will be changed during the optimization
    process. Now, let's use an optimizer to train a simple linear regression model
    using TensorFlow.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '`[m,b]`是优化过程中会被改变的参数。现在，让我们使用优化器通过TensorFlow训练一个简单的线性回归模型。'
- en: 'Exercise 1.06: Using an Optimizer for a Simple Linear Regression'
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.06：使用优化器进行简单线性回归
- en: 'In this exercise, we are going to see how to use an optimizer to train a simple
    linear regression model. We will start off by assuming arbitrary values for the
    parameters (`w` and `b`) in a linear equation `w*x + b`. Using the optimizer,
    we will observe how the values of the parameters change to get to the right parameter
    values, thus mapping the relationship between the input values (`x`) and output
    (`y`). Using the optimized parameter values, we will predict the output (`y`)
    for some given input values (`x`). After completing this exercise, we will see
    that the linear output, which is predicted by the optimized parameters, is very
    close to the real values of the output values. Follow these steps to complete
    this exercise:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何使用优化器训练一个简单的线性回归模型。我们将首先假设一个线性方程 `w*x + b` 中的任意值（`w` 和 `b`）。通过优化器，我们将观察这些参数值是如何变化的，以便得到正确的参数值，从而映射输入值（`x`）与输出（`y`）之间的关系。使用优化后的参数值，我们将预测一些给定输入值（`x`）的输出（`y`）。完成此练习后，我们将看到，由优化参数预测的线性输出与实际输出值非常接近。按照以下步骤完成此练习：
- en: Open a Jupyter Notebook and rename it *Exercise 1.06*.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个 Jupyter Notebook 并将其重命名为 *Exercise 1.06*。
- en: 'Import `tensorflow`, create the variables, and initialize them to 0\. Here,
    our assumed values are zero for both these parameters:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `tensorflow`，创建变量并将其初始化为 0。这里，我们假设这两个参数的值都为零：
- en: '[PRE77]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Define a function for the linear regression model. We learned how to create
    functions in TensorFlow earlier:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个线性回归模型的函数。我们之前学习了如何在 TensorFlow 中创建函数：
- en: '[PRE78]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Prepare the data in the form of features (`x`) and labels (`y`):'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据，以特征（`x`）和标签（`y`）的形式呈现：
- en: '[PRE79]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Define the `loss` function. In this case, this is the absolute value of the
    difference between the predicted value and the label:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `loss` 函数。在此案例中，`loss` 是预测值与标签值之间差的绝对值：
- en: '[PRE80]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Create an `Adam` optimizer instance with a learning rate of `.01`. The learning
    rate defines at what rate the optimizer should change the assumed parameters.
    We will discuss the learning rate in subsequent chapters:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个学习率为 `.01` 的 `Adam` 优化器实例。学习率定义了优化器应以多快的速度改变假设的参数。我们将在后续章节中讨论学习率：
- en: '[PRE81]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Train the model by running the optimizer for 1,000 iterations to minimize the
    loss:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行优化器 1,000 次迭代来训练模型，以最小化损失：
- en: '[PRE82]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Print the trained values of the `w` and `b` parameters:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印训练好的 `w` 和 `b` 参数的值：
- en: '[PRE83]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The output will be as follows:'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE84]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We can see that the values of the `w` and `b` parameters have been changed from
    their original values of 0, which were assumed. This is what is done during the
    optimizing process. These updated parameter values will be used for predicting
    the values of `Y`.
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，`w` 和 `b` 参数的值已经从假设的原始值 0 发生了变化。这正是优化过程中的操作。更新后的参数值将用于预测 `Y` 的值。
- en: Note
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The optimization process is stochastic in nature (having a random probability
    distribution), and you might get values for `w` and `b` that are different to
    the value that was printed here.
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化过程是随机的（具有随机概率分布），因此你可能得到与这里打印的值不同的 `w` 和 `b` 的值。
- en: 'Use the trained model to predict the output by passing in the `x` values. The
    model predicts the values, which are very close to the label values (`y`), which
    means the model was trained to a high level of accuracy:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型通过输入 `x` 值来预测输出。模型预测的值与标签值（`y`）非常接近，这意味着模型已经训练得非常精确：
- en: '[PRE85]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The output of the preceding command will be as follows:'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的输出将如下所示：
- en: '[PRE86]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Note
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gSBs8b](https://packt.live/3gSBs8b).
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/3gSBs8b](https://packt.live/3gSBs8b)。
- en: You can also run this example online at [https://packt.live/2OaFs7C](https://packt.live/2OaFs7C).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，链接地址是 [https://packt.live/2OaFs7C](https://packt.live/2OaFs7C)。你必须执行整个
    Notebook 才能得到期望的结果。
- en: In this exercise, we saw how to use an optimizer to train a simple linear regression
    model. During this exercise, we saw how the initially assumed values of the parameters
    were updated to get the true values. Using the true values of the parameters,
    we were able to get the predictions close to the actual values. Understanding
    how to apply the optimizer will help you later with training neural network models.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们学习了如何使用优化器来训练一个简单的线性回归模型。在此过程中，我们看到初始假设的参数值如何更新，以获得真实的参数值。通过使用真实的参数值，我们能够得到接近实际值的预测。理解如何应用优化器将帮助你后续训练神经网络模型。
- en: Now that we have seen the use of an optimizer, let's take what we've learned
    and apply the optimization function to solve a quadratic equation in the next
    activity.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过优化器的使用，让我们将学到的知识应用到下一个活动中，通过优化函数来解一个二次方程。
- en: 'Activity 1.01: Solving a Quadratic Equation Using an Optimizer'
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.01：使用优化器解二次方程
- en: 'In this activity, you will use an optimizer to solve the following quadratic
    equation:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，你将使用优化器来解下面的二次方程：
- en: '![Figure 1.28: A quadratic equation'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.28：一个二次方程'
- en: '](img/B15385_01_28.jpg)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_01_28.jpg)'
- en: 'Figure 1.28: A quadratic equation'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.28：一个二次方程
- en: 'Here are the high-level steps you need to follow to complete this activity:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本活动所需的高层次步骤如下：
- en: Open a new Jupyter Notebook and import the necessary packages, just as we did
    in the previous exercises.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并导入必要的包，就像我们在之前的练习中所做的那样。
- en: Initialize the variable. Please note that, in this example, `x` is the variable
    that you will need to initialize. You can initialize it to a value of 0.
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变量。请注意，在此示例中，`x` 是你需要初始化的变量。你可以将其初始化为 0。
- en: Construct the `loss` function using the `lambda` function. The `loss` function
    will be the quadratic equation that you are trying to solve.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `lambda` 函数构造 `loss` 函数。`loss` 函数将是你要解的二次方程。
- en: Use the `Adam` optimizer with a learning rate of `.01`.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习率为 `.01` 的 `Adam` 优化器。
- en: Run the optimizer for different iterations and minimize the loss. You can start
    the number of iterations at 1,000 and then increase it in subsequent trials until
    you get the result you desire.
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同的迭代中运行优化器并最小化损失。你可以从 1,000 次迭代开始，然后在随后的试验中增加迭代次数，直到获得你想要的结果。
- en: Print the optimized value of `x`.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印优化后的 `x` 值。
- en: 'The expected output is as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出如下：
- en: '[PRE87]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Please note that while your actual output might be a little different, it should
    be a value close to 5.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然你的实际输出可能略有不同，但它应该接近于5。
- en: Note
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 388.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤、解决方案和附加评论，请参见第388页。
- en: Summary
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: That brings us to the end of this chapter. Let's revisit what we have learned
    so far. We started off by looking at the relationship between AI, machine learning,
    and deep learning. Then, we implemented a demo of deep learning by classifying
    an image and then implementing a text to speech conversion using a Google API.
    This was followed by a brief description of different use cases and types of deep
    learning, such as MLP, CNN, RNN, and GANs.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 本章到此结束。让我们回顾一下我们迄今为止学到的内容。我们首先了解了 AI、机器学习和深度学习之间的关系。接着，我们通过分类一张图片实现了深度学习的演示，并利用
    Google API 实现了文本到语音的转换。随后，我们简要介绍了不同的深度学习应用场景和类型，如 MLP、CNN、RNN 和 GANs。
- en: In the next section, we were introduced to the TensorFlow framework and understood
    some of the basic building blocks, such as tensors and their rank and shape. We
    also implemented different linear algebra operations using TensorFlow, such as
    matrix multiplication. Later in the chapter, we performed some useful operations
    such as `reshape` and `argmax`. Finally, we were introduced to the concept of
    optimizers and implemented solutions for mathematical expressions using optimizers.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们介绍了 TensorFlow 框架，并了解了其中的一些基本构件，如张量及其秩和形状。我们还使用 TensorFlow 实现了不同的线性代数操作，如矩阵乘法。在本章后半部分，我们执行了一些有用的操作，如
    `reshape` 和 `argmax`。最后，我们介绍了优化器的概念，并使用优化器解决了数学表达式问题。
- en: Now that we have laid the foundations for deep learning and introduced you to
    the TensorFlow framework, the stage has been set for you to take a deep dive into
    the fascinating world of neural networks. In the next chapter, you will be introduced
    to neural networks, and in the successive chapters, we will take a look at more
    in-depth deep learning concepts. We hope you enjoy this fascinating journey.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为深度学习打下了基础，并向你介绍了TensorFlow框架，接下来你将可以深入探索神经网络的迷人世界。在下一章中，你将接触到神经网络，接下来的章节将深入探讨更多深度学习的概念。我们希望你能享受这段迷人的旅程。
