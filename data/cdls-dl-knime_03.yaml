- en: '*Chapter 2:* Data Access and Preprocessing with KNIME Analytics Platform'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第2章*：使用KNIME分析平台进行数据访问和预处理'
- en: Before deep-diving into neural networks and deep learning architectures, it
    might be a good idea to get familiar with KNIME Analytics Platform and its most
    important functions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入学习神经网络和深度学习架构之前，可能最好先熟悉KNIME分析平台及其最重要的功能。
- en: 'In this chapter, we will cover a few basic operations within KNIME Analytics
    Platform. Since every project needs data, we will first go through the basics
    of how to access data: from files or databases. In KNIME Analytics Platform, you
    can also access data from REST services, cloud repositories, specific industry
    formats, and more. We will leave the exploration of these other options to you.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖KNIME分析平台中的一些基本操作。由于每个项目都需要数据，我们将首先讲解如何访问数据：从文件或数据库中。在KNIME分析平台中，您还可以从REST服务、云存储库、特定行业格式等获取数据。我们将把这些其他选项的探索留给您自己。
- en: Data comes in a number of shapes and types. In the *Data Types and Conversions*
    section, we will briefly investigate the tabular nature of the KNIME data representation,
    the basic types of data in a data table, and how to convert from one type to another.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据有多种形状和类型。在*数据类型和转换*部分，我们将简要探讨KNIME数据表示的表格特性、数据表中的基本数据类型，以及如何将一种类型转换为另一种类型。
- en: At this point, after we have imported the data into a KNIME workflow, we will
    show some basic data operations, such as filtering, joining, concatenating, aggregating,
    and other commonly used data transformations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将数据导入到KNIME工作流中，接下来我们将展示一些基本的数据操作，如过滤、连接、拼接、聚合以及其他常用的数据转换。
- en: The parameterization of a static workflow will conclude this very quick overview
    of the basic operations you can perform with KNIME Analytics Platform on your
    data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 静态工作流的参数化将总结我们对KNIME分析平台进行的基本操作的快速概述。
- en: 'This chapter will take you through the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下主题：
- en: Accessing Data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问数据
- en: Data Types and Conversions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型和转换
- en: Transforming Data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Parameterizing the Workflow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流参数化
- en: Let's start with how to import data into a KNIME workflow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从如何将数据导入到KNIME工作流开始。
- en: Accessing Data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问数据
- en: 'Before starting with examples of how to access and import data into a KNIME
    workflow, let''s create the workflow:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始展示如何访问和导入数据到KNIME工作流之前，让我们先创建一个工作流：
- en: Click on the **File** item in the top menu or right-click on a folder, such
    as **LOCAL**, for example, in **KNIME Explorer**.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击顶部菜单中的**文件**项，或右键点击一个文件夹，例如在**KNIME Explorer**中的**LOCAL**文件夹。
- en: Then, select the **New KNIME Workflow** option.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择**新建KNIME工作流**选项。
- en: Give it a name – for example, `Ch2_Workflow_Examples` – and a destination.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给它命名——例如，`Ch2_Workflow_Examples`——并选择一个目标位置。
- en: 'An empty canvas will open in the central part of the KNIME workbench: the workflow
    editor.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个空白的画布将会在KNIME工作台的中央打开：工作流编辑器。
- en: 'For this chapter, we will use toy data already available at installation. A
    set of workflows is installed together with the core KNIME Analytics Platform.
    You can find them in the `Example Workflows` folder (*Figure 2.1*) in the `TheData`
    sub-folder contains some free toy datasets:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章内容，我们将使用安装时已经提供的示例数据。安装时会一起安装一套工作流，您可以在`Example Workflows`文件夹中找到它们（见*图2.1*），而`TheData`子文件夹包含一些免费的玩具数据集：
- en: '![Figure 2.1 – Structure of the Example Workflows folder in the KNIME Explorer
    panel](img/B16391_02_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 - KNIME Explorer面板中示例工作流文件夹的结构](img/B16391_02_001.jpg)'
- en: Figure 2.1 – Structure of the Example Workflows folder in the KNIME Explorer
    panel
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 - KNIME Explorer面板中示例工作流文件夹的结构
- en: We will mainly use the datasets in the `Misc` sub-folder.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要使用`Misc`子文件夹中的数据集。
- en: Tip
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In order to upload data to the **KNIME Explorer** panel, just copy it into a
    folder within the current workspace folder on your machine. The folder and its
    contents will then appear in **KNIME Explorer** in the list of workflows, servers,
    KNIME Hub spaces, and data available.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据上传到**KNIME Explorer**面板，只需将其复制到您计算机当前工作区文件夹中的一个文件夹中。然后，文件夹及其内容将在**KNIME
    Explorer**的工作流、服务器、KNIME Hub 空间和可用数据列表中出现。
- en: Reading Data from Files
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文件中读取数据
- en: 'Let''s start with a classic: reading a **CSV-formatted** text file. To read
    a CSV-formatted text file, you need the **File Reader** node or its simplified
    version, the **CSV Reader** node. Let''s focus on the File Reader node, which,
    though more complex, is also more powerful and flexible. There are now two ways
    to create and configure a File Reader node.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从经典的方法开始：读取**CSV格式**的文本文件。要读取CSV格式的文本文件，您需要使用**文件读取器**节点或其简化版本**CSV读取器**节点。让我们专注于文件读取器节点，尽管它更复杂，但也更强大和灵活。现在有两种创建和配置文件读取器节点的方法。
- en: In the long way, you search for the File Reader node in the Node Repository;
    drag and drop it into the workflow editor; double-click it to open its configuration
    window, or alternatively, right-click it and then select **Configure**; and set
    the required settings, which at the very least require the file path via the **Browse**
    button (*Figure 2.2*).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 长方法中，您可以在节点库中搜索文件读取器节点；将其拖放到工作流编辑器中；双击打开其配置窗口，或者选择右键然后选择**配置**；设置所需的设置，至少需要通过**浏览**按钮设置文件路径（*图
    2.2*）。
- en: In the short way, you just drag and drop your CSV-formatted file from the File
    Explorer panel into the workflow editor. This way automatically creates a File
    Reader node, fills up most of its configuration settings, including the file path,
    and keeps the configuration window open for further adjustments (*Figure 2.2*).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 短方法中，您只需将CSV格式的文件从文件浏览器面板拖放到工作流编辑器中。这种方式会自动创建文件读取器节点，并填充大部分配置设置，包括文件路径，并保持配置窗口打开以便进一步调整（*图
    2.2*）。
- en: 'Under the file path, there are some basic settings: whether to read the first
    row as column headers and/or the first column as `RowID`, the column delimiter
    for general text files, and how to deal with spaces and tabs.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件路径下方，有一些基本设置：是否将第一行读取为列标题和/或将第一列作为`行ID`，一般文本文件的列分隔符，以及如何处理空格和制表符。
- en: 'Notice two more things in this configuration window of the File Reader node:
    the data preview and the **Advanced** button. The data preview in the lower part
    of the window allows you to see whether the dataset is being read properly. The
    **Advanced** button takes you to more advanced settings, such as enabling shorter
    lines, character encoding, quotes, and other similar preferences.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件读取器节点的配置窗口中，还要注意两件事情：数据预览和**高级**按钮。窗口下部的数据预览允许您查看数据集是否被正确读取。**高级**按钮会带您到更高级的设置，例如启用更短的行，字符编码，引号和其他类似的偏好设置。
- en: 'When using the short way to create and configure a **File Reader** node, in
    the preview panel in the node configuration window (*Figure 2.2*), you can see
    whether the automatic settings were sufficient or whether additional customization
    is necessary:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用短方法创建和配置**文件读取器**节点时，在节点配置窗口的预览面板中（*图 2.2*），您可以看到自动设置是否足够或是否需要额外的定制：
- en: '![Figure 2.2 – The File Reader node and its configuration window](img/B16391_02_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 文件读取器节点及其配置窗口](img/B16391_02_002.jpg)'
- en: Figure 2.2 – The File Reader node and its configuration window
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 文件读取器节点及其配置窗口
- en: We drag and drop the `Demographics.csv` file from `Example Workflows/TheData/Misc`
    into the workflow editor. In the configuration window of the File Reader node,
    we see that the `CustomerKey` column is interpreted as the row ID of the data
    rows, rather than its own column. We need to disable the **read Row IDs** option
    to read the data properly. After the configuration is complete, we click **OK**;
    the node state moves to yellow and the node can now be executed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`Demographics.csv`文件从`示例工作流程/数据/其他`拖放到工作流编辑器中。在文件读取器节点的配置窗口中，我们看到`CustomerKey`列被解释为数据行的行ID，而不是其自己的列。我们需要禁用**读取行ID**选项以正确读取数据。配置完成后，点击**确定**；节点状态变为黄色，节点现在可以执行。
- en: Tip
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'The automatic creation of the node and the configuration of its settings by
    file drag and drop works only for specific file extensions: `.csv` for a File
    Reader node, `.table` for a Table Reader node, `.xls` and .`xlsx` for an Excel
    Reader node, and so on.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过文件拖放自动创建节点并配置其设置仅适用于特定的文件扩展名：`.csv`用于文件读取器节点，`.table`用于表格读取器节点，`.xls`和`.xlsx`用于Excel读取器节点等。
- en: 'Similarly, if we drag and drop the `ProductData2.xls` file from the KNIME Explorer
    panel to the workflow editor, an **Excel Reader** (**XLS**) node is created and
    automatically configured (*Figure 2.3*):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们从KNIME资源管理器面板中将`ProductData2.xls`文件拖放到工作流编辑器中，则会创建并自动配置**Excel读取器**（**XLS**）节点（*图
    2.3*）：
- en: '![Figure 2.3 – The Excel Reader (XLS) node and its configuration window](img/B16391_02_003.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – Excel Reader (XLS)节点及其配置窗口](img/B16391_02_003.jpg)'
- en: Figure 2.3 – The Excel Reader (XLS) node and its configuration window
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – Excel Reader (XLS)节点及其配置窗口
- en: 'The configuration window (*Figure 2.3*) is similar to the one of the **File
    Reader** node, but, of course, customized to deal with Excel files. Three items
    especially are different:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 配置窗口（*图2.3*）与**文件读取节点**的配置窗口相似，但当然是定制化的，用于处理Excel文件。特别有三个项是不同的：
- en: The preview part is activated by a **refresh** button. You need to click on
    **refresh** to update the preview.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预览部分由**刷新**按钮激活。你需要点击**刷新**来更新预览。
- en: Column headers and row IDs are extracted from spreadsheet cells, identified
    with an alphabet letter (the column with the row IDs) and a row number (the row
    with the column headers), according to the Excel standards.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列头和行ID是从电子表格单元格中提取的，通过字母（列头所在的列）和数字（行ID所在的行）来标识，符合Excel标准。
- en: On top of the URL path, there is a menu with a default choice, **Custom URL**.
    This menu allows you to express the file path as an absolute path (**local file
    system**), as a path relative to a mountpoint (**Mountpoint**), as a path relative
    to one of the current locations (data, workflow, or mountpoint), or as a custom
    path (**Custom URL**). This feature will be soon extended to other reader nodes.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在URL路径的顶部，有一个带有默认选择的菜单，**自定义URL**。此菜单允许你将文件路径表示为绝对路径（**本地文件系统**）、相对于挂载点的路径（**Mountpoint**）、相对于当前位置之一（数据、工作流或挂载点）的路径，或者自定义路径（**Custom
    URL**）。此功能很快将扩展到其他读取节点。
- en: In our case, the automated configuration process does not include the column
    headers. We can see this from the preview segment. So, because we have the column
    headers in the first row, we adjust the `1`, refresh the preview, and click **OK**
    to save the changes and close the window.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，自动配置过程并未包含列头。从预览段可以看出这一点。因此，由于我们在第一行中有列头，我们调整`1`，刷新预览，然后点击**确定**保存更改并关闭窗口。
- en: Next, let's read the `SentimentAnalysis.table` file. `.table` files contain
    binary content in a KNIME proprietary format optimized for speed and size. These
    files are read by the Table Reader node. Since all the information about the file
    is already included in the file itself, the configuration window of the `SentimentAnalysis.table`
    file automatically generates a Table Reader node with a pre-configured URL.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们读取`SentimentAnalysis.table`文件。`.table`文件包含以KNIME专有格式优化的二进制内容，旨在提高速度和减小文件大小。这些文件由表格读取节点（Table
    Reader node）读取。由于文件中的所有信息已经包含在文件本身，因此`SentimentAnalysis.table`文件的配置窗口会自动生成一个预配置URL的表格读取节点。
- en: To conclude this section, let's read the last files, `SentimentRating.csv` and
    `WebDataOldSystem.csv`, with two more File Reader nodes; then, let's add the name
    of the file in the comment under each node. Then, finally, let's group all these
    reader nodes inside an annotation explaining **Reading data from files** (*Figure
    2.9*).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本节内容，让我们通过两个额外的文件读取节点来读取最后两个文件，`SentimentRating.csv`和`WebDataOldSystem.csv`，然后在每个节点下的注释中添加文件名。最后，将所有这些读取节点组合在一个注释中，解释**从文件中读取数据**（*图2.9*）。
- en: '`Demographics.csv` contains the demographics of a number of customers, such
    as age and gender. Each customer is identified via a `CustomerKey` value. `ProductData2.xls`
    contains the products purchased by each customer, again identified via the `CustomerKey`
    value. `SentimentAnalysis.table` contains the sentiment expressed as text by the
    customer toward the company and the product, again identified via the `CustomerKey`
    value. `SentimentRating.csv` contains the mapping between the sentiment rating
    and the sentiment text. Finally, `WebdataOldSystem.csv` contains the old activity
    index by each customer, as classified in the old web system, before migration.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`Demographics.csv`包含多个客户的基本信息，如年龄和性别。每个客户通过`CustomerKey`值进行标识。`ProductData2.xls`包含每个客户购买的产品，同样通过`CustomerKey`值进行标识。`SentimentAnalysis.table`包含客户对公司和产品表达的情感（以文本形式），再次通过`CustomerKey`值进行标识。`SentimentRating.csv`包含情感评分与情感文本之间的映射。最后，`WebdataOldSystem.csv`包含在迁移前旧网站系统中，每个客户的旧活动指标。'
- en: Of course, if there is a dataset from before migration, we must have a newer
    dataset with data from the system after migration. This can be found in a database
    table in the `WebActivity.sqlite` SQLite database.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果有迁移之前的数据集，我们必须拥有一个包含迁移后系统数据的更新数据集。这个数据集可以在`WebActivity.sqlite` SQLite数据库中的一个数据库表中找到。
- en: This leads us to the next section, where we will learn how to read data from
    a database.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这引导我们进入下一部分，在那里我们将学习如何从数据库读取数据。
- en: Reading Data from Databases
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数据库读取数据
- en: 'In the Node Repository, there is a category named **DB**, dedicated to **database**
    operations. All database operations are performed according to the same sequence
    (*Figure 2.4*): connect to database, select the table to work on, build a SQL
    query, and import data according to the SQL query.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点库中，有一个名为 **DB** 的类别，专门用于 **数据库** 操作。所有数据库操作都按照相同的顺序执行（*图 2.4*）：连接数据库，选择要处理的表，构建
    SQL 查询，并根据 SQL 查询导入数据。
- en: 'There are nodes for each of these steps, as shown in *Figure 2.4*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 2.4* 所示，每个步骤都有相应的节点：
- en: '![Figure 2.4 – Importing data from databases: connect, select, build SQL query,
    and import](img/B16391_02_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 从数据库导入数据：连接、选择、构建 SQL 查询并导入](img/B16391_02_004.jpg)'
- en: 'Figure 2.4 – Importing data from databases: connect, select, build SQL query,
    and import'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 从数据库导入数据：连接、选择、构建 SQL 查询并导入
- en: 'Let''s check these nodes one by one:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一检查这些节点：
- en: '`WebActivity.sqlite` file. The configuration window only requires the database
    file path since SQLite is a file-based database. All other settings have been
    preset in the node. Indeed, it is common to have some preset settings in dedicated
    connectors, and therefore dedicated connectors need fewer settings than the generic
    DB Connector node. A drag and drop of the `.sqlite` file automatically generates
    the **SQLite DB Connector** node with preloaded configuration settings.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WebActivity.sqlite` 文件。配置窗口只需要数据库文件路径，因为 SQLite 是基于文件的数据库。所有其他设置已经在节点中预设。事实上，在专用连接器中预设一些设置是很常见的，因此专用连接器所需的设置比通用
    DB Connector 节点少。拖放 `.sqlite` 文件会自动生成带有预加载配置设置的 **SQLite DB Connector** 节点。'
- en: '**Selecting the Table**: The **DB Table Selector** node allows you to select
    the table from the connected database to work on. If you are a SQL expert, the
    **Custom Query** flag allows you to create your own query for the subset of data
    to extract.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择表**：**DB Table Selector** 节点允许你从连接的数据库中选择要操作的表。如果你是 SQL 专家，**自定义查询** 标志允许你为要提取的数据子集创建自己的查询。'
- en: '**Build SQL Query**: If you are not a SQL expert, you can still build your
    SQL query to extract the subset of data. The DB nodes in the **DB/Query** category
    take a SQL query as input and add one more SQL queries on top of it. The node
    GUI is completely codeless and therefore there is no need to know any SQL code.
    So, for example, the configuration window of the **DB Row Filter** node presents
    a graphical editor on the right to build a row-filtering SQL query.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建 SQL 查询**：如果你不是 SQL 专家，你仍然可以构建自己的 SQL 查询来提取数据子集。**DB/Query** 类别中的 DB 节点将
    SQL 查询作为输入，并在其基础上添加更多 SQL 查询。该节点的图形用户界面完全不需要编码，因此无需了解任何 SQL 代码。例如，**DB Row Filter**
    节点的配置窗口在右侧提供了一个图形编辑器，用于构建行过滤的 SQL 查询。'
- en: 'In the following screenshot (*Figure 2.5*), record(s) of **CustomerKey = 11177**
    have been excluded:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中（*图 2.5*），**CustomerKey = 11177** 的记录已被排除：
- en: '![Figure 2.5 – The GUI of the DB Row Filter node. This node builds a SQL query
    to filter out records without using any SQL script](img/B16391_02_005.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – DB 行过滤器节点的图形用户界面。该节点构建 SQL 查询以过滤掉没有使用任何 SQL 脚本的记录](img/B16391_02_005.jpg)'
- en: Figure 2.5 – The GUI of the DB Row Filter node. This node builds a SQL query
    to filter out records without using any SQL script
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – DB 行过滤器节点的图形用户界面。该节点构建 SQL 查询以过滤掉没有使用任何 SQL 脚本的记录
- en: '**Import Data**: Finally, the **DB Reader** node imports the data from the
    database connection according to the input SQL query. The DB Reader node has no
    configuration window since all the required SQL settings to import the data are
    contained in the SQL query at its input port. There are many other nodes, besides
    the DB Reader node, to import data from a database at the end of such a sequence.
    They are all in the **DB/Read/Write** category in the Node Configuration panel.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导入数据**：最后，**DB Reader** 节点根据输入的 SQL 查询从数据库连接中导入数据。DB Reader 节点没有配置窗口，因为导入数据所需的所有
    SQL 设置都包含在其输入端口的 SQL 查询中。除了 DB Reader 节点外，还有许多其他节点可以在这样的序列末尾从数据库中导入数据。它们都位于节点配置面板中的
    **DB/读/写** 类别下。'
- en: Important note
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Did you notice the node ports in *Figure 2.4*? We passed from the black triangle
    (data) to the red square (connection) to the brown square (SQL query). Only ports
    of the same type, transporting data of the same type, can be connected!
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您是否注意到*图 2.4*中的节点端口？我们从黑色三角形（数据）经过红色方块（连接）到棕色方块（SQL 查询）。只有相同类型的端口，传输相同类型的数据，才能连接！
- en: In order to inspect the results, after the successful execution of the DB Reader
    node, you can right-click the last node in the sequence – the one with a black
    triangle (data) port, in this case, the DB Reader node – and select the last item
    in the menu. This shows the output data table.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查结果，在成功执行 DB Reader 节点后，您可以右键点击序列中的最后一个节点——带有黑色三角形（数据）端口的节点，在此情况下是 DB Reader
    节点——并选择菜单中的最后一项。这将显示输出数据表。
- en: The database nodes only produce a SQL query. At the output port, you can still
    inspect the results of the query by right-clicking the node, selecting the last
    item in the menu, then clicking on the **Cache no of Rows** button in the **Table
    Preview** tab to temporarily visualize just the top rows in the selected number.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库节点只会生成一个 SQL 查询。在输出端口，您仍然可以通过右键点击节点，选择菜单中的最后一项，然后点击**缓存行数**按钮，在**表格预览**标签页中临时可视化所选数量的顶部行，以检查查询结果。
- en: At this point, we have also imported the last dataset, including customer web
    activity after migration to the new web system.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们也已经导入了最后一个数据集，包括迁移到新网站系统后的客户网页活动数据。
- en: Let's spend a bit of time now on the data structure and data types.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们花点时间了解数据结构和数据类型。
- en: Data Types and Conversions
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据类型和转换
- en: If you inspect any of the output data tables from any of the nodes described
    previously, you will see a table-like representation of the data. Here, each value
    is identified via `RowID`, the identification number for the record, and via a
    `CustomerKey 11000` is `M`, as identified via the `Gender` column header, and
    the row ID is `Row0`. In a reader node, the row ID and column header can be generated
    automatically or assigned from the values in a column or a row in the data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查之前描述的任何节点的输出数据表，您将看到数据的表格化表示。在这里，每个值是通过`RowID`（记录的标识号）来识别的，并且通过`CustomerKey
    11000`为`M`，如`Gender`列头所示，行 ID 为`Row0`。在读取节点中，行 ID 和列头可以自动生成，或者根据数据中的某列或某行的值分配。
- en: 'The following is a screenshot of the data table output by the File Reader node:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由 File Reader 节点输出的数据表的截图：
- en: '![Figure 2.6 – A KNIME data table. Here, a cell is identified via its RowID
    value and column header](img/B16391_02_006.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 一个 KNIME 数据表。在这里，一个单元格是通过其 RowID 值和列头来识别的](img/B16391_02_006.jpg)'
- en: Figure 2.6 – A KNIME data table. Here, a cell is identified via its RowID value
    and column header
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 一个 KNIME 数据表。在这里，一个单元格是通过其 RowID 值和列头来识别的。
- en: Each data column also has a data type, as you can see in *Figure 2.6* from the
    icons in the column headers. Basic data types are `true/false`), and **String**.
    However, more complex data types are also available, such as **Date&Time**, **Document**,
    **Image**, **Network**, and more. We will see some of these data types in the
    upcoming chapters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据列也都有一个数据类型，正如您在*图 2.6*中看到的那样，可以通过列头中的图标来识别。基本数据类型有`true/false`和**String**。但是，也有更多复杂的数据类型，如**Date&Time**、**Document**、**Image**、**Network**等。我们将在接下来的章节中看到这些数据类型的一些例子。
- en: Of course, a data column is not condemned to stay with that data type forever.
    If the condition exists, it can move to another data type. Some nodes are dedicated
    to conversions and can be found in the Node Repository under **Manipulation/Column/Convert
    & Replace**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一个数据列并不是注定永远保持该数据类型。如果条件允许，它可以转换为另一种数据类型。某些节点专门用于转换，您可以在节点库中的**Manipulation/Column/Convert
    & Replace**下找到它们。
- en: In the data that we have read, `CustomerKey` has been imported as a five-digit
    integer. However, it might be convenient to move from an integer type representation
    to a string type representation. For that, we use the **Number to String** node.
    The configuration window consists of an include/exclude framework to select those
    columns whose type needs changing. The opposite transformation is obtained with
    the **String to Number** node. The **Double to Int** node might also be useful
    for a transformation from double to integer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们读取的数据中，`CustomerKey`被导入为五位整数。但是，将其从整数类型表示转换为字符串类型表示可能会更方便。为此，我们使用**Number
    to String**节点。配置窗口由一个包含/排除框架组成，用于选择需要改变类型的列。相反的转换可以通过**String to Number**节点实现。**Double
    to Int**节点在从双精度浮点数转换为整数时也很有用。
- en: Tip
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The **String Manipulation** and **Math Formula** nodes, even though their primary
    task is data transformation, also present some conversion functionality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**字符串操作**和**数学公式**节点，虽然它们的主要任务是数据转换，但也提供一些转换功能。'
- en: We would like to draw your attention to the **Category To Number** node. This
    node comes in handy to discretize nominal classes and transform them into numbers,
    as neural networks only accept numbers as target classes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望引起你对**类别转数字**节点的注意。这个节点非常有用，可以将名义类别离散化，并将它们转换为数字，因为神经网络只接受数字作为目标类别。
- en: Special data types, such as **Image** or **Date&Time**, offer their own conversion
    nodes. A very helpful node for that is the **String to Date&Time** node. **Date**
    or **Time** objects are often read as **String**, and this node converts them
    into the appropriate type object.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊数据类型，如**图像**或**日期和时间**，提供了它们自己的转换节点。一个非常有用的节点是**字符串到日期和时间**节点。**日期**或**时间**对象通常被读取为**字符串**，而这个节点将它们转换为适当的类型对象。
- en: 'In the next section, we want to consolidate all this customer information,
    starting with the web activity before and after the migration. In these two datasets,
    the columns describing web activity have different names: `First_WebActivity_`
    and `First(WebActivity)`. Let''s standardize them to the same name: `First_WebActivity_`.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将整合所有这些客户信息，从迁移前后的网络活动开始。在这两个数据集中，描述网络活动的列名不同：`First_WebActivity_`
    和 `First(WebActivity)`。我们将它们标准化为相同的名称：`First_WebActivity_`。
- en: 'This is what the **Column Rename** node does:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是**列重命名**节点的功能：
- en: '![Figure 2.7 – The Column Rename node and its configuration window](img/B16391_02_007.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 列重命名节点及其配置窗口](img/B16391_02_007.jpg)'
- en: Figure 2.7 – The Column Rename node and its configuration window
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 列重命名节点及其配置窗口
- en: The configuration window of the **Column Rename** node lists all the columns
    from the input data table on the left. Double-clicking on a column opens a frame
    on the right showing the current column name and requiring the new name and/or
    new type. All the nodes we have introduced in this section can be seen in the
    workflow in *Figure 2.13*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**列重命名**节点的配置窗口列出了输入数据表中所有的列名，在左侧显示。双击某一列会在右侧打开一个框架，显示当前的列名，并要求输入新的列名和/或新的类型。我们在本节中介绍的所有节点可以在*图
    2.13*中的工作流中看到。'
- en: Now, we are ready to concatenate the two web activity datasets and join all
    the other datasets by their `CustomerKey` values.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好将两个网络活动数据集合并，并通过它们的`CustomerKey`值联接所有其他数据集。
- en: Transforming Data
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: We have read the data from files and databases. In this section, we will perform
    some operations to consolidate, filter, aggregate, and transform them. We will
    start with consolidation operations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从文件和数据库中读取了数据。在本节中，我们将执行一些操作来整合、筛选、聚合和转换这些数据。我们将从整合操作开始。
- en: Joining and Concatenating
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联接与连接
- en: 'The web activity dataset from the old system comes from a CSV file and, after
    column renaming, consists of two data columns: `CustomerKey` and `First_WebActivity_`.
    `First_WebActivity_` ranks how active a customer is on the company''s web site:
    `0` means `3` means **very active**.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 来自旧系统的网络活动数据集来自一个CSV文件，经过列重命名后，包含两列数据：`CustomerKey` 和 `First_WebActivity_`。`First_WebActivity_`表示客户在公司网站上的活跃度：`0`表示不活跃，`3`表示**非常活跃**。
- en: 'The web activity dataset from the new web system comes from the SQLite database
    and consists of three columns: `CustomerKey`, `First_WebActivity_`, and `Count`.
    `Count` is just a progressive number associated with the data rows. It is not
    important for the upcoming analysis. We can decide later whether to remove it
    or keep it.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 来自新网页系统的网络活动数据集来自SQLite数据库，包含三列：`CustomerKey`、`First_WebActivity_` 和 `Count`。`Count`只是与数据行相关的递增数字，对接下来的分析不重要。我们稍后可以决定是否删除它或保留它。
- en: It would be nice to have both rankings for the web activity, from the old and
    the new system, together in one single data table. For this, we use the **Concatenate**
    node. Two input data tables are placed together in the same output data table.
    Data cells belonging to columns with the same name are placed in the same output
    column. Data columns existing in only one of the tables can be retained (union
    of columns) or removed (intersection of columns), as set in the node configuration
    window. The node configuration window also offers a few strategies to deal with
    rows with the same row IDs existing in both input tables.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果能够将来自旧系统和新系统的网页活动排名放在同一个数据表中，那就太好了。为此，我们使用 **Concatenate** 节点。两个输入数据表被放置在同一个输出数据表中。属于同名列的数据单元被放置在相同的输出列中。仅存在于一个表中的数据列可以保留（列的联合）或移除（列的交集），具体取决于节点配置窗口中的设置。节点配置窗口还提供了一些策略，用于处理在两个输入表中都存在相同行
    ID 的行。
- en: We concatenated the two web activity data tables and kept the union of data
    columns in the output data table.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将两个网页活动数据表进行了合并，并在输出数据表中保留了数据列的联合。
- en: Important note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Concatenate node icon shows three dots in its lower-left corner. Clicking
    these three dots gives you the chance to add more input ports and therefore to
    concatenate more input data tables.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Concatenate 节点图标的左下角显示三个点。点击这三个点，您可以添加更多的输入端口，从而连接更多的输入数据表。
- en: 'Let''s now move on to the sentiment analysis data. `SentimentAnalysis.table`
    produced a data table with `CustomerKey` and `SentimentAnalysis` columns. `SentimentAnalysis`
    includes the customer''s sentiment toward the company and product, expressed as
    text. `SentimentRating.csv` produced a data table with two columns: `SentimentAnalysis`
    and `SentimentRating`. Both columns express the customer sentiment: one in text
    and one in ranking ordinals. This is a mapping data table, translating text into
    ranking sentiment and vice versa. Depending on the kind of analysis we will run,
    we might need the text expression or the ranking expression. So, to be on the
    safe side, let''s join these two data tables together to have them all, `CustomerKey`,
    `SentimentAnalysis` (text), and `SentimentRating` (ordinals), in one data table
    only. This is obtained with the **Joiner** node.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来处理情感分析数据。`SentimentAnalysis.table` 生成了一个包含 `CustomerKey` 和 `SentimentAnalysis`
    列的数据表。`SentimentAnalysis` 包含客户对公司和产品的情感，表达为文本。`SentimentRating.csv` 生成了一个包含两列的数据表：`SentimentAnalysis`
    和 `SentimentRating`。这两列都表达客户情感：一列为文本，另一列为排名序数。这是一个映射数据表，用于将文本转换为排名情感，反之亦然。根据我们要进行的分析类型，我们可能需要文本表达或排名表达。因此，为了确保安全，我们将这两个数据表连接在一起，将它们的
    `CustomerKey`、`SentimentAnalysis`（文本）和 `SentimentRating`（序数）都放在同一个数据表中。这是通过 **Joiner**
    节点实现的。
- en: 'The Joiner node joins data cells from two input data tables together into the
    same data row, according to a key value. In our case, the key values are provided
    by the `SentimentAnalysis` columns present in both input data tables. So, each
    customer (`CustomerKey`) will have the `SentimentAnalysis` text value and the
    corresponding `SentimentRating` value. The Joiner node offers four different join
    modes: **inner join** (intersection of key values in the two tables), **left outer
    join** (all key values from the left/top table), **right outer join** (all key
    values from the right/bottom table), and **full outer join** (all key values from
    both tables).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Joiner 节点根据键值将两个输入数据表中的数据单元合并到同一行。在我们的例子中，键值由两个输入数据表中都存在的 `SentimentAnalysis`
    列提供。因此，每个客户（`CustomerKey`）将拥有 `SentimentAnalysis` 文本值和相应的 `SentimentRating` 值。Joiner
    节点提供四种不同的连接模式：**内连接**（两个表中键值的交集）、**左外连接**（左/上表中的所有键值）、**右外连接**（右/下表中的所有键值）和 **全外连接**（两个表中的所有键值）。
- en: 'In *Figure 2.8*, you can find the two tabs of the configuration window of the
    Joiner node:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 2.8* 中，您可以找到 Joiner 节点配置窗口的两个选项卡：
- en: '![Figure 2.8 – Configuration window of the Joiner node: the Joiner Settings
    and Column Selection tabs](img/B16391_02_008.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – Joiner 节点的配置窗口：Joiner 设置和列选择选项卡](img/B16391_02_008.jpg)'
- en: 'Figure 2.8 – Configuration window of the Joiner node: the Joiner Settings and
    Column Selection tabs'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – Joiner 节点的配置窗口：Joiner 设置和列选择选项卡
- en: 'The configuration window of the Joiner node includes two tabs: **Joiner Settings**
    and **Column Selection**. The **Joiner Settings** tab exposes for selection the
    joiner mode and the data columns containing the key values for both input tables.
    The **Column Selection** tab sets the columns from both input tables to retain
    when building the final joint data rows. A few additional options are available
    to deal with columns with the same names in the two tables and to set what to
    do with the key columns after the joining is performed.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 连接器节点的配置窗口包含两个选项卡：**连接器设置**和**列选择**。**连接器设置**选项卡允许选择连接模式和包含关键值的数据列，适用于两个输入表格。**列选择**选项卡设置在构建最终连接数据行时，两个输入表格中要保留的列。还提供了一些附加选项，以处理两个表格中具有相同名称的列，并设置在连接执行后如何处理关键列。
- en: Important note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There can be more than one level of key columns for the join. Just select the
    **+** button in the **Joiner Settings** tab to add more key columns. If you have
    more than one level of key columns, you can decide whether a join is performed
    if all key values match (**Match all of the following**) or if just one key value
    matches (**Match any of the following**), as set in the top radio buttons (*Figure
    2.8* on the left).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 连接操作中可以有多个层级的关键列。只需在**连接器设置**选项卡中选择**+**按钮来添加更多关键列。如果有多个层级的关键列，您可以决定是否在所有关键值匹配时执行连接（**匹配所有以下条件**），或者仅当一个关键值匹配时执行连接（**匹配以下任意条件**），如左侧的单选按钮所设置（*图
    2.8*）。
- en: We joined the two sentiment tables using `SentimentAnalysis` as the key column
    in both tables and using a left outer join. The left outer join includes all key
    values from the left (upper) table (the customer table) and therefore makes sure
    that all sentiment values for all customers are retained in the output data table.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`SentimentAnalysis`作为两个表中的关键列，并使用左外连接将两个情感表连接在一起。左外连接包括左侧（上方）表格（客户表）中的所有关键值，因此确保所有客户的情感值都保留在输出数据表中。
- en: After joining `CustomerKey` with all the sentiment expressions, we will perform
    other similar join operations, multiple times, in cascade, using `CustomerKey`
    as the key column, to collect together the different pieces of data for the same
    customers in one single table (*Figure 2.13*).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在将`CustomerKey`与所有情感表达式连接后，我们将执行其他类似的连接操作，使用`CustomerKey`作为关键列，级联多次连接，以将同一客户的不同数据片段收集到一个表中（*图
    2.13*）。
- en: 'If we inspect the output produced by the `Demographics.csv` file, we notice
    two data columns that are also provided by other files: `WebActivity` and `SentimentRating`.
    They are old columns and should be substituted with the same columns from the
    `SentimentAnalysis.table` file and the web activity files. We could remove these
    two columns in the **Column Selection** tab of the **Joiner** node. Alternatively,
    we can just filter those two columns out with a dedicated node.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查由`Demographics.csv`文件生成的输出，我们会注意到有两个数据列也由其他文件提供：`WebActivity`和`SentimentRating`。这些是旧列，应当用`SentimentAnalysis.table`文件和网页活动文件中的相应列替换。我们可以在**连接器**节点的**列选择**选项卡中删除这两列。或者，我们也可以通过专用节点将这两列过滤掉。
- en: Let's see how to filter columns and rows out of a data table.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何从数据表中过滤列和行。
- en: Column and Row Filtering
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列和行过滤
- en: 'The **Column Filter** node is dedicated to filtering columns from the input
    data table. We can do that as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**列过滤器**节点专门用于过滤输入数据表中的列。我们可以按如下方式进行操作：'
- en: Manually select which columns to keep and which to exclude (**Manual Selection**).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动选择要保留和排除的列（**手动选择**）。
- en: Use a wildcard or a Regex expression to match the names of the columns to exclude
    or to keep (**Wildcard/Regex Selection**).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用通配符或正则表达式来匹配要排除或保留的列名（**通配符/正则表达式选择**）。
- en: Define one or more data types for the columns to include or exclude (**Type
    Selection**).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义要包含或排除的列的数据类型（**类型选择**）。
- en: All these options are available at the top of the configuration window of the
    **Column Filter** node. Selecting one of them changes the configuration window
    according to the required settings for that option. Here are the options.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项都可以在**列过滤器**节点的配置窗口顶部找到。选择其中一个选项后，配置窗口会根据该选项的设置进行调整。以下是这些选项。
- en: '**Manual Selection**: Provides an include/exclude framework to move columns
    from one frame to the other to include or exclude input columns from the output
    data table (*Figure 2.9*).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动选择**：提供一个包含/排除框架，用于将列从一个数据框移动到另一个数据框，以包含或排除输出数据表中的输入列（*图 2.9*）。'
- en: '`*` for joker characters; for example, `R*` indicates all words starting with
    `R`, `R*a` indicates all words starting with `R` and ending with `a`, and so on.
    Regex refers to regular expressions.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*` 表示通配符字符；例如，`R*` 表示所有以 `R` 开头的单词，`R*a` 表示所有以 `R` 开头并以 `a` 结尾的单词，依此类推。正则表达式（Regex）指的是正则表达式。'
- en: '**Type Selection**: This option provides a multiple choice for the data types
    of the columns to include.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型选择**：此选项提供了一个多项选择，用于选择列的数据类型。'
- en: 'The configuration window of the Column Filter node is shown in *Figure 2.9*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Column Filter 节点的配置窗口如*图 2.9*所示：
- en: '![Figure 2.9 – The Column Filter node and its configuration window](img/B16391_02_009.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – Column Filter 节点及其配置窗口](img/B16391_02_009.jpg)'
- en: Figure 2.9 – The Column Filter node and its configuration window
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – Column Filter 节点及其配置窗口
- en: 'So far, we have been filtering data by columns. The other flavor for data filtering
    is by rows. In this case, we want to remove or keep just some of the data rows
    in the table. For example, still working on the data from the `Demographics.csv`
    file, we might want to keep only the men in the dataset or remove all records
    with **CustomerKey 11177**. For this kind of filtering operation, there are many
    different nodes: **Row Filter**, **Row Filter (Labs)**, **Rule-based Row Filter**,
    **Reference Row Filter**, **Date&Time based Row Filter**, and more:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直是在按列过滤数据。数据过滤的另一种方式是按行过滤。在这种情况下，我们想要删除或保留表中的某些数据行。例如，在处理`Demographics.csv`文件中的数据时，我们可能只想保留数据集中的男性，或者删除所有**CustomerKey
    为 11177**的记录。对于这种类型的过滤操作，有许多不同的节点：**行过滤器**、**行过滤器（实验室版）**、**基于规则的行过滤器**、**参考行过滤器**、**基于日期和时间的行过滤器**等：
- en: 'The **Row Filter** node is very simple and very powerful: on the right, the
    filtering condition and on the left the filtering mode.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行过滤器**节点非常简单且非常强大：右侧是过滤条件，左侧是过滤模式。'
- en: '**Filtering Condition** matches the content of cells in a data column with
    a condition. The input data column to match is selected at the top. The condition
    can consist of **pattern matching**, including wildcards and regex in the pattern
    expression; **range checking**, which is useful for numerical columns; and **missing
    value matching**.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤条件**将数据列中的单元格内容与条件进行匹配。要匹配的输入数据列在顶部选择。条件可以包括**模式匹配**（包括通配符和正则表达式）、**范围检查**（对于数值列非常有用）和**缺失值匹配**。'
- en: '**Filtering Mode** on the left sets whether to include or exclude the matching
    rows, matching by attribute value, row number, or **RowID**:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**过滤模式**在左侧设置，确定是否包括或排除匹配的行，可以通过属性值、行号或 **RowID** 进行匹配：'
- en: '![Figure 2.10 – The Row Filter node and its configuration window](img/B16391_02_010.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.10 – 行过滤器节点及其配置窗口](img/B16391_02_010.jpg)'
- en: Figure 2.10 – The Row Filter node and its configuration window
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 – 行过滤器节点及其配置窗口
- en: Here, we filter out, using the `CustomerKey` attribute has a value of `11177`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`CustomerKey`属性过滤掉值为`11177`的数据。
- en: A similar result could have been obtained using a `11177` into the lower port
    of the **Reference Row Filter** node from a **Table Creator** node.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `11177` 输入到 **Table Creator** 节点的 **Reference Row Filter** 节点的下端口，也可以获得类似的结果。
- en: The **Table Creator** node is an interesting node for temporary small data.
    It covers the role of an internal spreadsheet, which is where to store a few lines
    of data.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格创建器**节点是一个非常有趣的节点，适用于临时的小数据。它充当内部电子表格的角色，可以存储几行数据。'
- en: Another group of very important nodes is the ones performing aggregations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组非常重要的节点是执行聚合操作的节点。
- en: Aggregations
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合
- en: 'Aggregations are a very important part of any data preparation. Whether for
    dashboard or machine learning algorithms, some aggregation operations are usually
    necessary. There are two commonly used nodes for aggregations: the **GroupBy**
    node and the **Pivoting** node.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是任何数据准备中非常重要的一部分。无论是用于仪表板还是机器学习算法，某些聚合操作通常是必要的。常用的聚合节点有两个：**GroupBy** 节点和
    **Pivoting** 节点。
- en: 'In *Figure 2.11*, you can see the two tabs in the configuration window of the
    GroupBy node:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 2.11*中，你可以看到 GroupBy 节点配置窗口中的两个标签页：
- en: '![Figure 2.11 – The two tabs of the GroupBy node''s configuration window:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11 – GroupBy 节点配置窗口的两个标签页](img/B16391_02_011.jpg)'
- en: Groups and Manual Aggregation](img/B16391_02_011.jpg)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[分组和手动聚合](img/B16391_02_011.jpg)'
- en: 'Figure 2.11 – The two tabs of the GroupBy node''s configuration window: Groups
    and Manual Aggregation'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 – GroupBy 节点配置窗口的两个标签页：分组和手动聚合
- en: The **GroupBy** node isolates groups of data and on these groups calculates
    some measures, such as simple count, average, variance, percentages, and others.
    Identification of the groups happens in the tab named **Groups** of the configuration
    window; measure setting happens in one of the other tabs (*Figure 2.11*).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**GroupBy** 节点将数据分组，并在这些组上计算一些度量，如简单计数、平均值、方差、百分比等。分组的识别发生在配置窗口中的 **Groups**
    标签页，度量设置则发生在其他标签页中（*图 2.11*）。'
- en: In the **Groups** tab, you select the data columns whose value combinations
    define the different groups of data. The node then creates one row for each group.
    For example, selecting the **Gender** column as the group column with distinct
    values of **male** and **female** means to identify those groups of data with
    **Gender** as **male** or **female**. Selecting the **Gender** (**male**/**female**)
    and **MaritalStatus** (**single**/**married**) columns as group columns means
    to identify the **single-female**, **single-male**, **married-female**, and **married-male**
    groups.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **Groups** 标签页中，选择那些其值组合定义不同数据组的数据列。节点然后为每个组创建一行。例如，选择 **Gender** 列，且其不同值为
    **male** 和 **female**，意味着识别出 **Gender** 为 **male** 或 **female** 的数据组。选择 **Gender**（**male**/**female**）和
    **MaritalStatus**（**single**/**married**）列作为分组列，意味着识别出 **single-female**、**single-male**、**married-female**
    和 **married-male** 数据组。
- en: 'Then, we need to select the measures we want to provide for these groups. Here
    we can proceed by doing the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要选择为这些组提供的度量。我们可以通过以下方式进行：
- en: Manually selecting the columns and the measures to apply one by one (**Manual
    Aggregation**)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动选择列并逐一应用度量（**手动聚合**）
- en: Selecting the columns based on a pattern, including wildcard or Regex expressions,
    and the measures to apply to each set of columns (**Pattern Based Aggregation**)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模式选择列，包括通配符或正则表达式，并为每组列应用的度量（**基于模式的聚合**）
- en: Selecting the columns by type and the measures to apply to each set of columns
    (**Type Based Aggregation**)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按类型选择列并为每组列应用的度量（**基于类型的聚合**）
- en: Each measure setting mode has its own tab in the configuration window (*Figure
    2.11*). In the `CustomerKey` column and the `Age` column. For `Gender` as the
    group column, we then get the number and the average age of women and men in the
    input table.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每种度量设置模式在配置窗口中都有自己的标签页（*图 2.11*）。在 `CustomerKey` 列和 `Age` 列中，选择 `Gender` 作为分组列后，我们得到输入表中女性和男性的数量以及平均年龄。
- en: Important note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The **GroupBy** node offers a large number of measures. We have seen **Count**
    and **Mean**. However, we could have also used percentage, median, variance, number
    of missing values, sum, mode, minimum, maximum, first, last, kurtosis, concatenation
    of (distinct) values, correlation, and more. It is worth taking some time to investigate
    all the measurement methods available within the **GroupBy** node.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**GroupBy** 节点提供了大量的度量方式。我们已经看到了 **Count** 和 **Mean**。然而，我们还可以使用百分比、中位数、方差、缺失值数量、总和、众数、最小值、最大值、首值、尾值、峰度、（不同）值的拼接、相关性等。值得花时间研究一下
    **GroupBy** 节点中所有可用的度量方法。'
- en: Like the `Gender` (`MaritalStatus` (`CustomerKey` data column. The final result
    is a table with **male**/**female** as the row IDs, **married**/**single** as
    the column headers, and the count of occurrences of each combination as the cell
    content.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `Gender`（`MaritalStatus`（`CustomerKey` 数据列）。最终结果是一个表格，其中 **male**/**female**
    为行 ID，**married**/**single** 为列标题，每个组合的出现次数为单元格内容。
- en: This means that the distinct values in the group columns generate rows and the
    distinct values in the pivoting columns generate columns.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着分组列中的不同值生成行，而透视列中的不同值生成列。
- en: 'The configuration window of the **Pivoting** node then has three tabs: **Groups**
    to select the group columns, **Pivots** to select the pivoting columns, and **Manual
    Aggregation** to manually select data columns and the measures to calculate on
    them. If more than one manual aggregation is used, the resulting pivoting table
    has one column for each combination of aggregation method and pivot value.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pivoting** 节点的配置窗口有三个标签：**Groups** 用于选择分组列，**Pivots** 用于选择透视列，**Manual Aggregation**
    用于手动选择数据列和计算度量。如果使用多个手动聚合，则结果透视表将为每个聚合方法和透视值的组合生成一列。'
- en: In addition, the node returns the total aggregation based on only the group
    columns on the second output port and the total aggregation based on only the
    pivoted columns at the third output port.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，节点会根据仅包含组列的第二个输出端口和仅包含透视列的第三个输出端口返回总聚合。
- en: Let's move on now to a few more very flexible and very powerful nodes to perform
    data transformation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续看几个非常灵活且强大的节点，用于执行数据转换。
- en: Math Formula and String Manipulation nodes
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学公式和字符串操作节点
- en: 'KNIME Analytics Platform offers many nodes for data transformation. We cannot
    describe all of them here. So, while we leave the enjoyment of their discovery
    to you, we will describe two very powerful nodes here: the **String Manipulation**
    and **Math Formula** nodes.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME分析平台提供了许多用于数据转换的节点。我们无法在此描述所有节点。因此，我们将把发现它们的乐趣留给您，在这里我们将描述两个非常强大的节点：**字符串操作**节点和**数学公式**节点。
- en: The **String Manipulation** node applies transformations on string values in
    data cells. The transformations are listed in the **Function** panel in the node
    configuration window (*Figure 2.12*). There, you can see the function and its
    possible syntaxes. If you select a function in the list, in the panel on the right,
    named **Description**, a full description of the function task and syntax appears.
    The transformation, however, is implemented in the **Expression** editor at the
    bottom of the window.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**字符串操作**节点对数据单元格中的字符串值进行转换。转换函数在节点配置窗口中的**功能**面板中列出（见*图2.12*）。在这里，您可以看到函数及其可能的语法。如果您选择列表中的某个函数，右侧名为**描述**的面板将显示该函数任务和语法的完整描述。然而，转换是在窗口底部的**表达式**编辑器中实现的。'
- en: First, you select (double-click) a transformation from the `""`, or values from
    other columns in the input data table. Values from columns are inserted automatically
    with the right syntax with a double-click on the column name in the **Column List**
    panel on the left.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从`""`或输入数据表中其他列的值中选择（双击）一个转换。通过双击左侧**列列表**面板中的列名称，列中的值会自动插入，且语法正确。
- en: 'Let''s take an example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个例子为例：
- en: In the data table resulting from the `M`) and one for female (`F`), containing
    the number of occurrences and the average age for each group (`M/F`). Let's change
    `"M"` to `"Male"` and `"F"` to `Female"`.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在由`M`和`F`生成的数据表中，分别为男性（`M`）和女性（`F`）创建了一个表格，包含每组的出现次数和平均年龄（`M/F`）。让我们将`"M"`更改为`"Male"`，将`"F"`更改为`"Female"`。
- en: 'Then, we would use the `replace(str, search, replace)` function, where `str`
    indicates the column to work on, `search` the string to search in the cell value,
    and `replace` the string to use as a replacement. Double-clicking on the **Gender**
    column in the **Column List** panel and completing the expression by hand, we
    end up with the following expression:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用`replace(str, search, replace)`函数，其中`str`表示要操作的列，`search`表示要在单元格值中搜索的字符串，`replace`表示要用作替换的字符串。双击**性别**列中的**列列表**面板，并手动完成表达式，最终得到以下表达式：
- en: '[PRE0]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We get the following in a subsequent node:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随后的节点中，我们得到了以下结果：
- en: '[PRE1]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The String Manipulation node and its configuration window are shown in *Figure
    2.12*:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**字符串操作**节点及其配置窗口如下所示（见*图2.12*）：'
- en: '![Figure 2.12 – The String Manipulation node and its configuration window](img/B16391_02_012.jpg)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图2.12 – 字符串操作节点及其配置窗口](img/B16391_02_012.jpg)'
- en: '[PRE2]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We would get a similar expression for `"F"` and `"Female"`.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们会得到类似的表达式来替换`"F"`和`"Female"`。
- en: Finally, we set to replace the original values in the `Gender` column with the
    new values, using the **Replace Column** option in the lower part of the configuration
    window.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过在配置窗口的下方使用**替换列**选项，将原始值替换为新值，以更新`性别`列。
- en: It is also possible to apply the same transformation to more than one input
    data column, with the **String Manipulation (Multi Column)** node. This node essentially
    works like the **String Manipulation** node. It just applies the set expression
    to all selected data columns. The lower part of its configuration window is the
    same as for the **String Manipulation** node. In the upper part, though, you can
    select all columns on which to apply the expression.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过**字符串操作（多列）**节点将相同的转换应用于多个输入数据列。该节点的功能与**字符串操作**节点相同，只不过它将设置的表达式应用于所有选定的数据列。其配置窗口的下半部分与**字符串操作**节点相同，而上半部分则允许您选择应用表达式的所有列。
- en: Important note
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: In the `$$CURRENTCOLUMN$$` general column name in the **Expression** editor.
    The very large number of string transformations in the **Function** list makes
    this node extremely powerful.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在`$$CURRENTCOLUMN$$`常规列名称的**表达式**编辑器中，**函数**列表中大量的字符串转换使得此节点极为强大。
- en: A node very similar to the String Manipulation node, even though working on
    a different task, is the **Math Formula** node. The Math Formula node implements
    a mathematical expression on the input data. Besides that, it works exactly the
    same as the String Manipulation node. In the configuration window, the available
    math functions are listed in the central **Function** panel. If a function from
    the list is selected, the description appears in the **Description** panel. The
    final expression is crafted in the **Expression** editor at the bottom. Insertion
    of column names in the **Expression** editor happens by double-clicking the column
    name in the **Column List** panel on the left. Nested mathematical functions are
    possible.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个与字符串操作节点非常相似的节点，尽管它处理的是不同的任务，那就是**数学公式**节点。数学公式节点对输入数据执行数学表达式。此外，它的工作方式与字符串操作节点完全相同。在配置窗口中，可用的数学函数列出了中央**函数**面板。如果从列表中选择了某个函数，描述将在**描述**面板中显示。最终的表达式是在底部的**表达式**编辑器中编写的。可以通过双击左侧**列列表**面板中的列名，将列名插入到**表达式**编辑器中。可以嵌套使用数学函数。
- en: The **Math Formula (Multi Column)** node extends the **Math Formula** node to
    apply the same formula onto many selected columns.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学公式（多列）**节点扩展了**数学公式**节点，使其能够将相同的公式应用于多个选定的列。'
- en: '*Figure 2.13* shows the final workflow containing all the operations described
    in this chapter, which is also available on the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.13* 显示了包含本章所描述的所有操作的最终工作流，且该工作流也可以在 KNIME Hub 上找到：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/)：'
- en: '![Figure 2.13 – Workflow that summarizes some data access, data conversion,
    and'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13 – 概述了某些数据访问、数据转换和'
- en: data transformation nodes available in KNIME Analytics Platform](img/B16391_02_013.jpg)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME Analytics Platform 中可用的数据转换节点](img/B16391_02_013.jpg)
- en: Figure 2.13 – Workflow that summarizes some data access, data conversion, and
    data transformation nodes available in KNIME Analytics Platform
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 – 概述了在 KNIME Analytics Platform 中可用的一些数据访问、数据转换和数据转换节点
- en: So far, we have seen static transformations on data. What about having a different
    transformation for different conditions? Let's take the **Row Filter** node. Today,
    I might want to filter out the female occurrences from the data table, while tomorrow
    the male ones. How can I do that without having to change the configuration settings
    for all involved nodes at every run? The time has come to introduce you to **Flow
    Variables**.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的是数据上的静态转换。那么，如何在不同的条件下使用不同的转换呢？以**行过滤器**节点为例。今天，我可能想要从数据表中过滤掉女性记录，而明天则是男性记录。我如何做到这一点，而不必在每次运行时都更改所有相关节点的配置设置呢？现在是时候向你介绍**流变量**了。
- en: Parameterizing the Workflow
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作流参数化
- en: 'Let''s consider a simple workflow: read the `Demographics.csv` file, filter
    all data rows with `Gender = M or F`, and replace `M` or `F` with `Male` or `Female`,
    respectively. Once we have decided whether to work on `M` or `F`, the workflow
    becomes quite simple and includes a `replace()` function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的工作流：读取`Demographics.csv`文件，筛选出所有`Gender = M 或 F`的数据行，并将`M`或`F`分别替换为`Male`或`Female`。一旦我们决定是处理`M`还是`F`，工作流就变得相当简单，并且包括一个`replace()`函数：
- en: 'Let''s add one node that allows us to choose whether to work on `M` or `F`
    records: the **String Configuration** node. This node generates a flow variable.
    A flow variable is a parameter that travels with the data flow along the workflow
    branch and it can be used to overwrite settings in other nodes.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们添加一个节点，允许我们选择是处理`M`还是`F`记录：**字符串配置**节点。此节点生成一个流变量。流变量是一个随着数据流在工作流分支中传递的参数，它可以用来覆盖其他节点中的设置。
- en: 'As far as we are concerned, for now, two settings are important in the configuration
    window of this node: the default value and the variable name. Let''s use default
    value `M` for now, to work with `Gender = M` records, and let''s name the flow
    variable `gender_variable`.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就我们而言，目前在此节点的配置窗口中，有两个设置非常重要：默认值和变量名称。暂时使用默认值`M`来处理`Gender = M`记录，并将流变量命名为`gender_variable`。
- en: Executing the node creates a Flow Variable named `gender_variable` with value
    `M`:![Figure 2.14 – This workflow shows how to use flow variables](img/B16391_02_014.jpg)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行节点会创建一个名为`gender_variable`的流变量，其值为`M`：![图2.14 – 该工作流展示了如何使用流变量](img/B16391_02_014.jpg)
- en: Figure 2.14 – This workflow shows how to use flow variables
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.14 – 该工作流展示了如何使用流变量
- en: Now, let's use the value of the `gender_variable` via the **V** button.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过**V**按钮使用`gender_variable`的值。
- en: Did you notice the red connection between the **String Configuration** node
    and the **Row Filter** node? This is a **Flow Variable** connection. Flow variables
    are injected into nodes and branches via these connections.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你注意到**字符串配置**节点和**行过滤器**节点之间的红色连接了吗？这是一条**流变量**连接。流变量通过这些连接注入到节点和分支中。
- en: Important note
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: All nodes have hidden red circle ports for the input and output of flow variables.
    Clicking on the flow variable port of a node and releasing on another node brings
    out the hidden flow variable port and connects the nodes. Alternatively, in the
    context menu of each node, the **Show Flow Variable Ports** option makes them
    visible.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有节点都有隐藏的红色圆形端口，用于流变量的输入和输出。点击节点的流变量端口并将其释放到另一个节点上，会显示出隐藏的流变量端口并连接这些节点。或者，在每个节点的上下文菜单中，**显示流变量端口**选项可以使它们可见。
- en: After that, we create a small table with two rows, [`M, Male`] and [`F, Female`].
    We select the row corresponding to the value in the `gender_variable` flow variable,
    and we aim to replace the `M` or `F` character with the text. For this last part,
    we need to replace the hardcoded strings in the `M` or `F` character as a **Flow
    Variable**.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们创建一个包含两行的小表格，[`M, 男性`] 和 [`F, 女性`]。我们选择与`gender_variable`流变量中的值对应的行，并旨在将`M`或`F`字符替换为文本。对于这最后一部分，我们需要将`M`或`F`字符中的硬编码字符串替换为**流变量**。
- en: Now, we transform the `Male`/`Female` text into a new flow variable. We do this
    via the `gender_variable`, generated by the `Row0`, generated by the `M, Male`]
    or [`F, Female`], depending on what has been selected in the `$Gender$`) and flow
    variables (`$${Sgender_variable}$$`). Also, flow variables can be inserted automatically
    and with the right syntax in the **Expression** editor, by double-clicking on
    the flow variable name in the **Flow Variable List** panel on the left of the
    String Manipulation node's configuration window (*Figure 2.12*).
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将`Male`/`Female`文本转换为一个新的流变量。我们通过`gender_variable`来实现，这个变量是由`Row0`生成的，取决于在`$Gender$`中选择了[`M,
    Male`]或[`F, Female`]，以及流变量（`$$Sgender_variable$$`）。此外，流变量可以自动插入并使用正确的语法，在**表达式**编辑器中，通过双击**流变量列表**面板中流变量名称（位于字符串操作节点配置窗口的左侧）来完成（*图2.12*）。
- en: The benefit of using flow variables is clear. When we decide to use `F` instead
    of `M`, we just change the setting in the **String Configuration** node instead
    of checking and changing the setting in every single node.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流变量的好处显而易见。当我们决定使用`F`而不是`M`时，只需在**字符串配置**节点中更改设置，而不需要在每个单独的节点中检查并更改设置。
- en: We have shown only a small fraction of the nodes dealing with flow variables.
    You can explore more of these nodes in the **Workflow Control/Variables** category
    in the Node Repository panel.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示的仅是处理流变量的节点中的一小部分。你可以在**工作流控制/变量**类别中进一步探索这些节点，位于节点库面板中。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We do not have space in this book to describe more of the many nodes available
    in KNIME Analytics Platform. We will leave this exploratory task to you.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本书没有足够的篇幅来描述KNIME Analytics Platform中更多的节点。我们将这项探索性任务留给你自己。
- en: KNIME Analytics Platform includes more than 2,000 nodes and covers a large variety
    of functionalities. However, the factotum nodes that work in most situations are
    much fewer in number, such as, for example, File Reader, Row Filter, GroupBy,
    Join, Concatenation, Math Formula, String Manipulation, Rule Engine, and more.
    We have described most of them in this chapter to give you a solid basis to build
    more complex workflows for deep learning, which we will do in the next chapter.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME Analytics Platform包括超过2,000个节点，并涵盖各种各样的功能。然而，大多数情况下工作的通用节点数量要少得多，例如，例如，文件读取器，行过滤器，GroupBy，Join，串联，数学公式，字符串操作，规则引擎等等。我们在本章中描述了大部分节点，为您构建更复杂的深度学习工作流程提供了坚实的基础，我们将在下一章中进行深入讨论。
- en: Questions and Exercises
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题和练习
- en: 'Check your level of understanding of the concepts presented in this chapter
    by answering the following questions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题来检查您对本章节中提出的概念的理解水平：
- en: How can I read a text file with lines of variable length?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何读取变长文本文件的行？
- en: a) By using the CSV Reader node
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 通过使用CSV读取节点
- en: b) By using the File Reader node
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 通过使用文件读取节点
- en: c) By using the File Reader node and the allow short lines enabled option
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 通过使用文件读取节点和启用短行选项
- en: d) By using the File Reader node and the Limit Rows enabled option
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 通过使用文件读取节点和启用的限制行选项
- en: How can I filter records to the `Age > 42` column?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何筛选记录以 `Age > 42` 列？
- en: a) By using the Column Filter node and selecting the `Age` column
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 通过使用列过滤节点并选择 `Age` 列
- en: b) By using the Row Filter node and pattern matching `=42` with the **Include**
    option on the right
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 通过使用行过滤节点和模式匹配 `=42`，在右侧使用 **包括** 选项
- en: c) By using the Row Filter node and range checking on, lower boundary *42*,
    with the **Include** option on the right
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 通过使用行过滤节点和范围检查，下限为 *42*，在右侧使用 **包括** 选项
- en: d) By using the Row Filter node and range checking on, lower boundary *42*,
    with the **Exclude** option on the right
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 通过使用行过滤节点和范围检查，下限为 *42*，在右侧使用 **排除** 选项
- en: How can I find the average sentiment rating for single women?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何找到单身女性的平均情感评分？
- en: a) By using a GroupBy node with `Gender` and `MaritalStatus` as group columns
    and the mean operation on the `SentimentRating` column
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 通过使用GroupBy节点，以 `Gender` 和 `MaritalStatus` 作为组列，并在 `SentimentRating` 列上执行均值操作
- en: b) By using a GroupBy node with `Gender` as the group column and a count operation
    on the `CustomerKey` column
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 通过使用GroupBy节点，以 `Gender` 作为组列，并在 `CustomerKey` 列上执行计数操作
- en: c) By using a GroupBy node with `CustomerKey` as the group column and a concatenate
    operation on the `SentimentAnalysis` column
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 通过使用GroupBy节点，以 `CustomerKey` 作为组列，并在 `SentimentAnalysis` 列上执行连接操作
- en: d) By using a GroupBy node with `MaritalStatus` as the group column and a percent
    operation on the `SentimentRating` column
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 通过使用GroupBy节点，以 `MaritalStatus` 作为组列，并在 `SentimentRating` 列上执行百分比操作
- en: Why do we need flow variables?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要流变量？
- en: a) To generate new values
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 生成新值
- en: b) To feed the necessary red connections
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 为了提供必要的红色连接
- en: c) To populate the flow variables list in configuration windows
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 用于在配置窗口中填充流变量列表
- en: d) To parameterize the workflow
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 为了参数化工作流程
