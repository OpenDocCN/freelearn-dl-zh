- en: 1\. Introduction to Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 强化学习简介
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces the **Reinforcement Learning** (**RL**) framework, which
    is one of the most exciting fields of machine learning and artificial intelligence.
    You will learn how to describe the characteristics and advanced applications of
    RL to show what can be achieved within this framework. You will also learn to
    differentiate between RL and other learning approaches. You will learn the main
    concepts of this discipline both from a theoretical point of view and from a practical
    point of view using Python and other useful libraries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**强化学习**（**RL**）框架，这是机器学习和人工智能领域中最令人兴奋的领域之一。你将学习如何描述RL的特征和高级应用，展示在这个框架下能够实现的目标。你还将学会如何区分RL与其他学习方法。你将从理论角度和实践角度（使用Python及其他有用的库）学习这门学科的主要概念。
- en: By the end of the chapter, you will understand what RL is and know how to use
    the Gym toolkit and Baselines, two popular libraries in this field, to interact
    with an environment and implement a simple learning loop.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解什么是RL，并了解如何使用Gym工具包和Baselines这两个在该领域中流行的库，来与环境进行互动并实现一个简单的学习循环。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Learning and adapting to new circumstances is a crucial process for humans and,
    in general, for all animals. Usually, learning is intended as a process of trial
    and error through which we improve our performance in particular tasks. Our life
    is a continuous learning process, that is, we start from simple goals (for example,
    walking), and we end up pursuing difficult and complex tasks (for example, playing
    a sport). As humans, we are always driven by our reward mechanism, which awards
    good behaviors and punishes bad ones.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和适应新环境是人类乃至所有动物的关键过程。通常，学习被视为一种通过反复试错的过程，通过这个过程我们在特定任务中提升表现。我们的生活是一个持续的学习过程，我们从简单的目标（例如，走路）开始，最终追求困难且复杂的任务（例如，参加体育运动）。作为人类，我们始终受到奖励机制的驱动，奖励好行为并惩罚不良行为。
- en: '**Reinforcement Learning** (**RL**), inspired by the human learning process,
    is a subfield of machine learning and deals with learning from interaction. With
    the term "interaction," we mean the process of trial and error through which we,
    as humans, understand the consequences of our actions and build up our own experiences.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**），受人类学习过程的启发，是机器学习的一个子领域，涉及通过互动进行学习。这里的“互动”指的是我们作为人类，通过试错过程来理解我们行为的后果，并积累我们的经验。'
- en: RL, in particular, considers sequential decision-making problems. These are
    problems in which an agent has to take a sequence of decisions, that is, actions,
    to maximize a certain performance measure.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: RL，特别是，关注的是序列决策问题。这些问题中，代理需要做出一系列决策，也就是一系列行动，以最大化某个性能指标。
- en: RL considers tasks to be **Markov Decision Processes** (**MDPs**), which are
    problems arising in many real-world scenarios. In this setting, the decision-maker,
    referred to as the agent, has to make decisions accounting for environmental uncertainty
    and experience. Agents are goal-directed; they need only a notion of a goal, such
    as a numerical signal, to be maximized. Unlike supervised learning, in RL, there
    is no need to provide good examples; it is the agent who learns how to map situations
    to actions. The mapping from situations (states) to actions is called "policy"
    in literature, and it represents the agent's behavior or strategy. Solving an
    MDP means finding the agent's policy by maximizing the desired outcome (that is,
    the total reward). We will study MDPs in more detail in future chapters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RL 将任务视为**马尔可夫决策过程**（**MDPs**），这些问题在许多现实世界场景中都会出现。在这种环境下，决策者（即代理）必须做出决策，考虑环境的不确定性和经验。代理是目标导向的；它们只需要一个目标的概念，比如一个需要最大化的数值信号。与监督学习不同，在RL中，不需要提供好的示例；是代理自己学习如何将情境映射到行动。情境（状态）到行动的映射在文献中被称为“策略”，它代表了代理的行为或策略。解决一个MDP意味着通过最大化期望的结果（即总奖励）来找到代理的策略。我们将在未来的章节中更详细地研究MDP。
- en: RL has been successfully applied to various kinds of problems and domains, showing
    exciting results. This chapter is an introduction to RL. It aims to explain some
    applications and describe concepts both from an intuitive perspective and from
    a mathematical point of view. Both of these aspects are very important when learning
    new disciplines. Without intuitive understanding, it is impossible to make sense
    of formulas and algorithms; without mathematical background, it is tough to implement
    existing or new algorithms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）已成功应用于各种问题和领域，取得了令人兴奋的结果。本章是强化学习的入门，旨在从直观的角度和数学的角度解释一些应用和概念。这两个方面在学习新学科时都非常重要。没有直观的理解，就无法理解公式和算法；没有数学背景，实施现有或新算法就会变得困难。
- en: In this chapter, we will first compare the three main machine learning paradigms,
    namely supervised learning, RL, and unsupervised learning. We will discuss their
    differences and similarities and define some example problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先比较三种主要的机器学习范式，即监督学习、强化学习（RL）和无监督学习。我们将讨论它们的异同，并定义一些示例问题。
- en: Second, we will move on to a section that contains the theory of RL and its
    notations. We will learn about concepts such as what an agent is, what an environment
    is, and how to parameterize different policies. This section represents the fundamentals
    of this discipline.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们将进入一个包含强化学习理论及其符号表示的部分。我们将学习像代理、环境以及如何参数化不同策略等概念。本节内容是这一学科的基础。
- en: Third, we will begin using two RL frameworks, namely Gym and Baselines. We will
    learn that interacting with a Gym environment is extremely simple, as is learning
    a task using Baselines algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们将开始使用两个强化学习框架，即Gym和Baselines。我们将学习与Gym环境的交互非常简单，使用Baselines算法学习任务也是如此。
- en: Finally, we will explore some RL applications to motivate you to study this
    discipline, showing various techniques that can be used to face real-world problems.
    RL is not bound to the academic world. However, it is still crucial from an industrial
    point of view, allowing you to solve problems that are almost impossible to solve
    using other techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探索一些强化学习应用，激励你学习这一学科，展示可用于应对现实世界问题的各种技术。强化学习不仅局限于学术界，但从工业角度来看，仍然至关重要，它使得你能够解决那些几乎无法通过其他技术解决的问题。
- en: Learning Paradigms
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习范式
- en: In this section, we will discuss the similarities and differences between the
    three main **learning paradigms** under the umbrella of machine learning. We will
    analyze some representative problems in order to understand the characteristics
    of these frameworks better.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在机器学习范畴下，三种主要**学习范式**的相似性与差异性。我们将分析一些代表性问题，以更好地理解这些框架的特征。
- en: Introduction to Learning Paradigms
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习范式简介
- en: 'For a learning paradigm, we implement a problem and a solution method. Usually,
    learning paradigms deal with data and rephrase the problem in a way that can be
    solved by finding parameters and maximizing an objective function. In these settings,
    the problem can be faced using mathematical and optimization tools, allowing a
    formal study. The term "learning" is often used to represent a dynamic process
    of adapting the algorithm''s parameters in such a way as to optimize their performance
    (that is, to learn) on a given task. Tom Mitchell defined learning in a precise
    way, as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个学习范式，我们实现一个问题和解决方法。通常，学习范式处理数据，并以一种可以通过寻找参数并最大化目标函数的方式重新表述问题。在这些设置中，问题可以通过数学和优化工具来解决，从而允许进行正式的研究。术语“学习”通常用于表示一种动态过程，在这个过程中，算法的参数以优化它们在给定任务上的表现（即“学习”）的方式进行调整。Tom
    Mitchell以以下精确的方式定义了学习：
- en: '*"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*“如果一个计算机程序在任务类别T和性能度量P下，随着经验E的积累，它在任务T中的表现通过P得到改善，那么我们说这个程序从经验E中学习。”*'
- en: 'Let''s rephrase the preceding definition more intuitively. To define whether
    a program is learning, we need to set a task; that is the goal of the program.
    The task can be everything we want the program to do, that is, play a game of
    chess, do autonomous driving, or carry out image classification. The problem should
    be accompanied by a performance measure, that is, a function that returns how
    well the program is performing on that task. For the chess game, a performance
    function can simply be represented by the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更直观地重新表述上述定义。要定义一个程序是否在学习，我们需要设置一个任务；这就是程序的目标。任务可以是我们希望程序完成的任何事情，比如下国际象棋、进行自动驾驶或执行图像分类。问题应当伴随一个表现度量，即一个函数，返回程序在该任务上的表现如何。对于国际象棋游戏，表现函数可以简单地通过以下方式表示：
- en: '![Figure 1.1: A performance function for a game of chess'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：国际象棋游戏的表现函数'
- en: '](img/B16182_01_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_01.jpg)'
- en: 'Figure 1.1: A performance function for a game of chess'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：国际象棋游戏的表现函数
- en: In this context, the experience is the amount of data collected by the program
    at a specific moment. For chess, the experience is represented by the set of games
    played by the program.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，经验是程序在特定时刻收集的数据量。对于国际象棋，经验可以通过程序进行的游戏集合来表示。
- en: The same input presented at the beginning of the learning phase or the end of
    the learning phase can result in different responses (that is, outputs) from the
    algorithm; the differences are caused by the algorithm's parameters being updated
    during the process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习阶段的开始或结束时呈现的相同输入，可能会导致算法产生不同的响应（即输出）；这些差异是由于算法的参数在过程中得到了更新。
- en: 'In the following table, we can see some examples of the experience, task, and
    performance tuples to better understand their concrete instantiations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们可以看到一些关于经验、任务和表现元组的例子，以便更好地理解它们的具体实例化：
- en: '![Figure 1.2: Table for instantiations'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2：实例化的表格'
- en: '](img/B16182_01_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_02.jpg)'
- en: 'Figure 1.2: Table for instantiations'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：实例化的表格
- en: It is possible to classify the learning algorithms based on the input they have
    and on the feedback they receive. In the following section, we will look at the
    three main learning paradigms in the context of machine learning based on this
    classification.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据学习算法的输入和它们收到的反馈来分类这些算法。在接下来的部分，我们将基于这种分类方法，探讨机器学习中的三种主要学习范式。
- en: Supervised versus Unsupervised versus RL
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习与强化学习
- en: 'The three main learning paradigms are supervised learning, unsupervised learning,
    and RL. The following figure represents the general schema of each of these learning
    paradigms:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 三种主要的学习范式是监督学习、无监督学习和强化学习（RL）。下图表示了每种学习范式的通用结构：
- en: '![Figure 1.3: Representation of learning paradigms'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3：学习范式的表示'
- en: '](img/B16182_01_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_03.jpg)'
- en: 'Figure 1.3: Representation of learning paradigms'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：学习范式的表示
- en: 'From the preceding figure, we can derive the following information:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下信息：
- en: Supervised learning minimizes the error of the output of the model with respect
    to a target specified in the training set.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习通过最小化模型输出相对于训练集中特定目标的误差来进行优化。
- en: RL maximizes the reward signal of the actions.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习（RL）通过最大化行为的奖励信号来进行优化。
- en: Unsupervised learning has no target and no reward; it tries to learn a data
    representation that can be useful.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习没有目标也没有奖励，它尝试学习一个可能有用的数据表示。
- en: Let's go more in-depth and elaborate on these concepts further, particularly
    from a mathematical perspective.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨并进一步阐述这些概念，特别是从数学角度来看。
- en: 'Supervised learning deals with learning a function by mapping an input to an
    output when the correspondences between the input and output (sample, label) are
    given by an external teacher (supervisor) and are contained in a training set.
    The objective of supervised learning is to generalize to unseen samples that are
    not included in the dataset, resulting in a system (for example, a function) that
    is able to respond correctly in new situations. Here, the correspondences between
    the sample and label are usually known (for example, in the training set) and
    given to the system. Examples of supervised learning tasks include regression
    and classification problems. In a regression task, the learner has to find a function,
    ![a](img/B16182_01_03d.png), of the input, ![b](img/B16182_01_03a.png), producing
    a (or ![e](img/B16182_01_03b.png), in general) real output,![c](img/B16182_01_03c.png).
    In mathematical notation, we have to find ![d](img/B16182_01_03e.png) such that:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习处理的是通过映射输入到输出来学习一个函数，当输入与输出（样本，标签）之间的对应关系由外部教师（监督者）给出，并包含在训练集中时。有监督学习的目标是能够推广到数据集中未包含的未见样本，从而使系统（例如，一个函数）能够在新情况下做出正确的响应。在这里，样本与标签之间的对应关系通常是已知的（例如，在训练集中），并且已给定给系统。监督学习任务的例子包括回归和分类问题。在回归任务中，学习者必须找到一个函数，![a](img/B16182_01_03d.png)，这个函数接收输入，![b](img/B16182_01_03a.png)，并生成一个（或一般是![e](img/B16182_01_03b.png)）实数输出，![c](img/B16182_01_03c.png)。用数学符号表示，我们需要找到![d](img/B16182_01_03e.png)，使得：
- en: '![Figure 1.4: Regression'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.4：回归'
- en: '](img/B16182_01_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_04.jpg)'
- en: 'Figure 1.4: Regression'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：回归
- en: 'Here, ![f](img/B16182_01_04a.png) is known for the examples in the training
    set. In a classification task, the function to be learned is a discrete mapping;
    ![7](img/B16182_01_04b.png) belongs to a finite and discrete set. Formalizing
    the problem, we search for a discrete-valued function, ![8](img/B16182_01_04c.png),
    such that:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![f](img/B16182_01_04a.png)表示训练集中的示例。在分类任务中，待学习的函数是一个离散的映射；![7](img/B16182_01_04b.png)属于一个有限的离散集合。通过将问题形式化，我们搜索一个离散值的函数，![8](img/B16182_01_04c.png)，使得：
- en: '![Figure 1.5: Classification'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.5：分类'
- en: '](img/B16182_01_05.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_05.jpg)'
- en: 'Figure 1.5: Classification'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：分类
- en: Here, the set, ![A picture containing clock
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，集合，![A picture containing clock
- en: Description automatically generated](img/B16182_01_05a.png), represents the
    set of possible classes or categories.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_05a.png)表示可能的类别或分类集合。
- en: Unsupervised learning deals with learning patterns in the data when the target
    label is not present or is unknown. The objective of unsupervised learning is
    to find a new, usually smaller, representation of data. Examples of unsupervised
    learning algorithms include clustering and **Principal Component Analysis (PCA)**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习处理的是在目标标签不存在或未知的情况下，学习数据中的模式。无监督学习的目标是找到数据的新表示，通常是更小的表示。无监督学习算法的例子包括聚类和**主成分分析（PCA）**。
- en: In a clustering task, the learner should split the dataset into clusters (a
    group of elements) according to some similarity measure. At first glance, clustering
    may seem very similar to classification; however, as an unsupervised learning
    task, the labels, or classes, are not given to the algorithm inside the training
    set. Indeed, it is the algorithm itself that should make sense of its inputs,
    by learning a representation of the input space in such a way that similar samples
    are close to each other.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类任务中，学习者应该根据某些相似性度量，将数据集划分为聚类（元素组）。乍一看，聚类可能与分类非常相似；然而，作为一种无监督学习任务，标签或类别并没有在训练集中给算法。实际上，应该是算法本身通过学习输入空间的表示，从而让相似的样本彼此靠近，来理解其输入。
- en: 'For example, in the following figure, we have the original data on the left;
    on the right, we have the possible output of a clustering algorithm. Different
    colors denote different clusters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下图中，我们在左侧看到的是原始数据；右侧是聚类算法的可能输出。不同的颜色代表不同的聚类：
- en: '![Figure 1.6: An example of a clustering application'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.6：聚类应用示例'
- en: '](img/B16182_01_06.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_06.jpg)'
- en: 'Figure 1.6: An example of a clustering application'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：聚类应用示例
- en: In the preceding example, the input space is composed of two dimensions, that
    is, ![g](img/B16182_01_06a.png), and the algorithm found three clusters or three
    groups of similar elements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述例子中，输入空间由两个维度组成，也就是![g](img/B16182_01_06a.png)，并且算法找到了三个聚类，或者说是三组相似的元素。
- en: PCA is an unsupervised algorithm used for dimensionality reduction and feature
    extraction. PCA tries to make sense of data by searching for a representation
    that contains most of the information from the given data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种用于降维和特征提取的无监督算法。PCA试图通过寻找一种表示方式来理解数据，该表示方式包含了给定数据中的大部分信息。
- en: RL is different from both supervised and unsupervised learning. RL deals with
    learning control actions in a sequential decision-making problem. The sequential
    structure of the problem makes RL challenging and different from the two other
    paradigms. Moreover, in supervised and unsupervised learning, the dataset is fixed.
    In RL, the dataset is continuously changing, and dataset creation is itself the
    agent's task. In RL, different from supervised learning, no teacher provides the
    correct value for a given sample or the right action for a given situation. RL
    is based on a different form of feedback, which is the environment's feedback
    evaluating the behavior of the agent. It is precisely the presence of feedback
    that also makes RL different from unsupervised learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）不同于监督学习和无监督学习。RL处理的是一个序列决策问题中的控制动作学习。问题的序列结构使得RL具有挑战性，并且与其他两种范式不同。此外，在监督学习和无监督学习中，数据集是固定的。而在RL中，数据集是不断变化的，数据集的创建本身就是智能体的任务。在RL中，不同于监督学习，没有教师为给定样本提供正确的值或为给定情境提供正确的行动。RL基于一种不同形式的反馈，即环境对智能体行为的反馈。正是这种反馈的存在，使得RL不同于无监督学习。
- en: 'We will explore these concepts in more detail in future sections:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的章节中更详细地探讨这些概念：
- en: '![Figure 1.7: Machine learning paradigms and their relationships'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.7：机器学习范式及其关系'
- en: '](img/B16182_01_07.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_07.jpg)'
- en: 'Figure 1.7: Machine learning paradigms and their relationships'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：机器学习范式及其关系
- en: RL and supervised learning can also be mixed up. A common technique (also used
    by AlphaGo Zero) is called **imitation learning** (or behavioral cloning). Instead
    of learning a task from scratch, we teach the agent in a supervised way how to
    behave (or which action to take) in a given situation. In this context, we have
    an expert (or multiple experts) demonstrating to the agent the desired behavior.
    In this way, the agent can start building its internal representation and its
    initial knowledge. Its actions won't be random at all when the RL part begins,
    and its behavior will be more focused on the actions shown by the expert.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习和监督学习也可能会混淆。一种常见的技术（也被AlphaGo Zero使用）被称为**模仿学习**（或行为克隆）。我们不是从头开始学习任务，而是以监督的方式教会智能体如何在给定情境中表现（或采取哪种行动）。在这种情况下，我们有一个专家（或多个专家），他们向智能体展示期望的行为。通过这种方式，智能体可以开始构建其内部表示和初始知识。当RL部分开始时，它的行动将不再是随机的，行为将更加专注于专家展示的行动。
- en: Let's now look at a few scenarios that will help us to classify the problems
    in a better manner.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看几个情境，帮助我们更好地分类这些问题。
- en: Classifying Common Problems into Learning Scenarios
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将常见问题分类为学习情境
- en: In this section, we will understand how it is possible to frame some common
    real-world problems into a learning framework by defining the required elements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何通过定义所需的元素，将一些常见的现实世界问题框架化为学习框架。
- en: Predicting Whether an Image Contains a Dog or a Cat
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测图像中是否包含狗或猫
- en: Predicting the content of an image is a standard classification example; therefore,
    it lies under the umbrella of supervised learning. Here, we are given a picture,
    and the algorithm should decide whether the image contains a dog or a cat. The
    input is the image, and the associated label can be 0 for cats and 1 for dogs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 预测图像内容是一个标准的分类示例，因此，它属于监督学习的范畴。在这里，我们给定一张图片，算法应该判断该图像中是狗还是猫。输入是图片，相关标签可以是0表示猫，1表示狗。
- en: For a human, this is a straightforward task, as we have an internal representation
    of dogs and cats (as well as an internal representation of the world), and we
    are trained extensively in our life to recognize dogs and cats. Despite this,
    writing an algorithm that is able to identify whether an image contains a dog
    or a cat is a difficult task without machine learning techniques. For a human,
    it is elementary to know whether the image is of a dog or cat; it is also easy
    to create a simple dataset of images of cats and dogs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，这是一个直接的任务，因为我们有关于猫狗的内部表征（以及对世界的内部表征），并且我们在一生中经过大量训练，能够识别猫狗。尽管如此，编写一个能够识别图像中是否包含猫狗的算法，在没有机器学习技术的情况下是一个困难的任务。对人类来说，知道图像是猫还是狗是很简单的；同时，创建一个简单的猫狗图像数据集也很容易。
- en: '*Why Not Unsupervised Learning?*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么不是无监督学习？*'
- en: Unsupervised learning is not suited to this type of task as we have a defined
    output we need to obtain from an input. Of course, supervised learning methods
    build an internal representation of the input data in which similarities are better
    exploited. This representation is only implicit; it is not the output of the algorithm
    as is the case in unsupervised learning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不适合这种类型的任务，因为我们有一个从输入中需要获得的定义输出。当然，监督学习方法会构建输入数据的内部表示，在这种表示中，相似性得到了更好的利用。这个表示是隐式的；它不是算法的输出，正如在无监督学习中那样。
- en: '*Why Not RL?*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么不是强化学习？*'
- en: RL, by definition, considers sequential decision-making problems. Predicting
    the content of an image is not a sequential problem, but instead a one-shot task.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习按定义考虑的是顺序决策问题。预测图像内容不是一个顺序问题，而是一个一次性任务。
- en: Detecting and Classifying All Dogs and Cats in an Image
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测与分类图像中的所有猫狗
- en: 'Detection and classification are two examples of supervised learning problems.
    However, this task is more complicated than the previous one. The detection part
    can be seen as both a regression and classification problem at the same time.
    The input is always the image we want to analyze, and the output is the coordinate
    of the bounding boxes for each dog or cat in the picture. Associated with each
    bounding box, we have a label to classify the content in the region of interest
    as a dog or a cat:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 检测与分类是监督学习问题的两个例子。然而，这个任务比前一个更复杂。检测部分可以同时看作是回归问题和分类问题。输入始终是我们想要分析的图像，输出是每个狗或猫的边界框坐标。与每个边界框相关联的是一个标签，用于将兴趣区域内的内容分类为狗或猫：
- en: '![Figure 1.8: Cat and dog detection and classification'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.8：猫狗检测与分类'
- en: '](img/B16182_01_08.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_08.jpg)'
- en: 'Figure 1.8: Cat and dog detection and classification'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：猫狗检测与分类
- en: '*Why Not Unsupervised Learning?*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么不是无监督学习？*'
- en: As in the previous example, here, we have a determined output given an input
    (an image). We do not want to extract unknown patterns in the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在这里，我们有一个给定输入（图像）后的确定输出。我们不想从数据中提取未知的模式。
- en: '*Why Not RL?*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么不是强化学习？*'
- en: Detection and classification are not tasks that are suited to the RL framework.
    We do not have a set of actions the agent should take to solve a problem. Also,
    in this case, the sequential structure is absent.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 检测与分类不是适合强化学习框架的任务。我们没有一套需要采取的行动来解决问题。此外，在这种情况下，缺乏顺序结构。
- en: Playing Chess
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下棋
- en: 'Playing chess can be seen as an RL problem. The program can perceive the current
    state of the board (for example, the positions and types of pawns), and, based
    on that, it should decide which action to take. Here, the number of possible actions
    is vast. Selecting an action means to understand and anticipate the consequences
    of the move to defeat the opponent:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下棋可以视为一个强化学习（RL）问题。程序可以感知棋盘的当前状态（例如，棋子的类型和位置），并基于此决定采取什么行动。在这里，可能的行动数量非常庞大。选择一个行动意味着要理解并预见这一动作的后果，以击败对手：
- en: '![Figure 1.9: Chess as an RL problem'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9：下棋作为强化学习问题'
- en: '](img/B16182_01_09.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_09.jpg)'
- en: 'Figure 1.9: Chess as an RL problem'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：下棋作为强化学习问题
- en: '*Why Not Supervised?*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么不是监督学习？*'
- en: We can think of playing chess as a supervised learning problem, but we would
    need to have a dataset, and we should incorporate the sequential structure of
    the game into the supervised learning problem. In RL, there is no need to have
    a dataset; it is the algorithm itself that builds up a dataset through interaction
    and, possibly, self-play.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将下棋视为一个监督学习问题，但我们需要一个数据集，并且应该将游戏的顺序结构融入到监督学习问题中。在强化学习中，不需要数据集；是算法本身通过交互并可能通过自我对弈来构建数据集。
- en: '*Why Not Unsupervised?*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么不是无监督学习？*'
- en: Unsupervised learning does not fit in this problem as we are not dealing with
    learning a representation of the data; we have a defined objective, which is winning
    the game.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不适用于这个问题，因为我们并不是在学习数据的表示；我们有一个明确的目标，那就是赢得游戏。
- en: In this section, we compared the three main learning paradigms. We saw the kind
    of data they have at their disposal, the type of interaction each algorithm has
    with the external world, and we analyzed some particular problems to understand
    which learning paradigm is best suited.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们比较了三种主要的学习范式。我们看到了它们所拥有的数据类型、每个算法与外部世界的交互方式，并分析了一些特定问题，以理解哪种学习范式最为适合。
- en: When facing a real-world problem, we always have to remember the distinction
    between these techniques, selecting the best one based on our goals, our data,
    and on the problem structure.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在面对现实世界问题时，我们总是要记住这些技术之间的区别，并根据我们的目标、数据以及问题结构选择最合适的技术。
- en: Fundamentals of Reinforcement Learning
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习基础
- en: In RL, the main goal is to learn from interaction. We want agents to learn a
    behavior, a way of selecting actions in given situations, to achieve some goal.
    The main difference between classical programming or planning is that we do not
    want to code the planning software explicitly on our own, as this would require
    a great effort; it can be very inefficient and even impossible. The RL discipline
    was born precisely for this reason.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，主要目标是通过交互来学习。我们希望智能体在给定的情境下学习一种行为，即选择行动的方式，以实现某个目标。与经典编程或规划的主要区别在于，我们不想显式地自己编码规划软件，因为这需要巨大的努力；它可能非常低效，甚至不可能完成。强化学习正是为了这个原因而诞生的。
- en: RL agents start (usually) with no idea of what to do. They typically do not
    know the goal, they do not know the game's rules, and they do not know the dynamics
    of the environment or how their actions influence the state.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）智能体通常一开始并不知道该做什么。它们通常不知道目标是什么，不知道游戏规则，也不了解环境的动态或自己的行为如何影响状态。
- en: 'There are three main components of RL: perception, actions, and goals.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习有三个主要组成部分：感知、行动和目标。
- en: Agents should be able to perceive the current environment state to deal with
    a task. This perception, also called observation, might be different from the
    actual environment state, can be subject to noise, or can be partial.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体应该能够感知当前的环境状态以完成任务。这种感知，也叫做观察，可能与实际的环境状态不同，可能受到噪音的干扰，或者可能是部分的。
- en: For example, think of a robot moving in an unknown environment. For robotic
    applications, usually, the robot perceives the environment using cameras. Such
    a perception does not represent the environment state completely; it can be subject
    to occlusions, poor lighting, or adverse conditions. The system should be able
    to deal with this incomplete representation and learn a way of moving in the environment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一个机器人在一个未知环境中移动。对于机器人应用，通常机器人通过摄像头感知环境。这种感知并不能完全代表环境状态，可能会受到遮挡、光线不足或不利条件的影响。系统应该能够处理这种不完整的表示，并学会在环境中移动的方式。
- en: The other main component of an agent is the ability to act; the agent should
    be able to take actions that affect the environment state or the agent's state.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的另一个主要组成部分是执行能力；智能体应该能够采取影响环境状态或自身状态的行动。
- en: Agents should also have a goal defined through the environment state. Goals
    are described using high-level concepts such as winning a game, moving in an environment,
    or driving correctly.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体还应该有一个通过环境状态定义的目标。目标通过高层次的概念来描述，例如赢得比赛、在环境中移动或正确驾驶。
- en: 'One of the challenges of RL, a challenge that does not arise in other types
    of learning, is the exploration-exploitation trade-off. In order to improve, the
    agent has to exploit its knowledge; it should prefer actions that have demonstrated
    themselves as useful in the past. There''s a problem here: to discover better
    actions, the agent should continue exploring, trying moves they have never done
    before. To estimate the effect of an action reliably, an agent has to perform
    each action many times. The critical thing to notice here is that neither exploration
    nor exploitation can be performed individually in order to learn a task.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的一个挑战是探索与利用的权衡，这个问题在其他类型的学习中并不存在。为了提高，智能体必须利用已有的知识；它应该偏好那些在过去证明有用的动作。这里有一个问题：为了发现更好的动作，智能体应该继续探索，尝试那些它以前从未做过的动作。为了可靠地估计一个动作的效果，智能体必须多次执行每个动作。需要注意的是，单独进行探索或利用都不能有效地学习任务。
- en: 'The aforementioned is very similar to the challenges we face as babies when
    we have to learn how to walk. At first, we try different types of movement, and
    we start from a simple movement yielding satisfactory results: crawling. Then,
    we want to improve our behavior to become more efficient. To learn a new behavior,
    we have to do movements we never did before: we try to walk. At first, we perform
    different actions yielding unsatisfactory results: we fall many times. Once we
    discover the correct way of moving our legs and balancing our body, we become
    more efficient in walking. If we did not explore further and we stopped at the
    first behavior that yields satisfactory results, we would crawl forever. By exploring,
    we learn that there can be different behaviors that are more efficient. Once we
    learn how to walk, we can stop exploring, and we can start exploiting our knowledge.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述情况非常类似于我们在婴儿时期学习走路时面临的挑战。最初，我们尝试不同类型的运动，并从一种简单且能带来令人满意结果的运动开始：爬行。然后，我们希望通过改进行为变得更加高效。为了学习新行为，我们不得不做一些以前从未做过的动作：我们尝试走路。刚开始，我们执行不同的动作，结果不尽如人意：我们摔倒了很多次。一旦我们发现了正确的腿部运动方式并学会了保持平衡，我们的走路效率就提高了。如果我们没有进一步探索，只停留在第一个带来满意结果的行为上，我们就会永远爬行。通过探索，我们学会了有些行为更高效。一旦我们学会了走路，就可以停止探索，开始利用已有的知识。
- en: Elements of RL
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的元素
- en: Let's introduce the main elements of the RL framework intuitively.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地介绍强化学习框架的主要元素。
- en: Agent
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智能体
- en: In RL, the agent is the abstract concept of the entity that moves in the world,
    takes actions, and achieves goals. An agent can be a piece of autonomous driving
    software, a chess player, a Go player, an algorithmic trader, or a robot. The
    agent is everything that can perceive and influence the state of the environment
    and, therefore, can be used to accomplish goals.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，智能体是指在世界中移动、执行动作并达成目标的实体的抽象概念。智能体可以是自动驾驶软件、象棋玩家、围棋玩家、算法交易员或机器人。智能体是能够感知并影响环境状态的所有事物，因此可以用来完成目标。
- en: Actions
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作
- en: An agent can perform actions based on the current situation. Actions can assume
    different forms depending on the specific task.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体可以根据当前的情况执行动作。动作的形式可以根据具体任务的不同而有所不同。
- en: Actions can be to steer, to push the accelerator pedal, or to push the brake
    pedal in an autonomous driving context. Other examples of actions include moving
    the horse to the H5 position or moving the king to the A5 position in a chess
    context.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶的背景下，动作可以是转向、踩下油门踏板或踩下刹车踏板。其他动作的例子包括在象棋中将马移到H5位置，或将国王移到A5位置。
- en: Actions can be low-level, such as controlling the voltage of the motors of a
    vehicle, but they can also be high-level, or planning actions, such as deciding
    where to go. The decision on the action level is the responsibility of the algorithm's
    designer. Actions that are too high-level can be challenging to implement at a
    lower level; they might require extensive planning at lower levels. At the same
    time, low-level actions make the problem difficult to learn.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 动作可以是低级的，例如控制车辆电机的电压，但也可以是高级的，或者是规划动作，例如决定去哪儿。动作层级的决定是算法设计师的责任。过于高级的动作可能难以在低层级实现；它们可能需要在较低层级进行大量规划。同时，低级动作使得问题学习变得更加困难。
- en: Environment
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境
- en: 'The environment represents the context in which the agent moves and takes decisions.
    An environment is composed of three main elements: states, dynamics, and rewards.
    They can be explained as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 环境代表了智能体移动并做出决策的上下文。一个环境由三个主要元素组成：状态、动态和奖励。它们可以如下解释：
- en: '**State**: This represents all of the information describing the environment
    at a particular timestep. The state is available to the agent through observations,
    which can be a partial or full representation.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：这表示描述在特定时间步环境的所有信息。状态通过观察可供智能体获取，观察可以是部分或完整的表示。'
- en: '**Dynamics**: The dynamics of an environment describe how actions influence
    the state of the environment. The environment dynamic is usually very complex
    or unknown. An RL algorithm using the information of the environment dynamic to
    learn how to achieve a goal belongs to the category of model-based RL, where the
    model represents the mathematical description of the environment. Most of the
    time, the environment dynamic is not available to the agent. In this case, the
    algorithm belongs to the model-free category. Even if the environment model is
    not available, too complicated, or too approximated, the agent can learn a model
    of the environment during training. Also, in this case, the algorithm is said
    to be model-based.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态**：环境的动态描述了行动如何影响环境的状态。环境的动态通常是非常复杂或未知的。使用环境动态信息来学习如何实现目标的RL算法属于基于模型的RL类别，其中模型表示环境的数学描述。大多数时候，环境动态对智能体是不可用的。在这种情况下，算法属于无模型类别。即使环境模型不可用、过于复杂或过于近似，智能体仍然可以在训练过程中学习到环境的模型。即使如此，这种算法也被认为是基于模型的。'
- en: '**Rewards**: Rewards are scalar values associated with each timestep describing
    the agent''s goal. Rewards can also be described as environmental feedback, providing
    information to an agent about its behavior; it is, therefore, necessary for making
    learning possible. If the agent receives a high reward, it means that it performed
    a good move, a move bringing it closer to its goal.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：奖励是与每个时间步相关的标量值，用于描述智能体的目标。奖励也可以描述为环境反馈，向智能体提供关于其行为的信息；因此，它对于使学习成为可能是必要的。如果智能体获得高奖励，意味着它做出了一个好的动作，一个将其带得更接近目标的动作。'
- en: Policy
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略
- en: A policy describes the behavior of the agent. Agents select actions by following
    their policies. Mathematically, a policy is a function mapping states to actions.
    What does this mean? Well, it means that the input of the policy is the current
    state, and its output is the action to take. A policy can have different forms.
    It can be a simple set of rules, a lookup table, a neural network, or any function
    approximator. A policy is the core of the RL framework, and the goal of all RL
    algorithms (implicit or explicit) is to improve the agent's policy to maximize
    the agent's performance on a task (or on a set of tasks). A policy can be stochastic,
    involving a distribution over actions, or it can be deterministic. In the latter
    case, the selected action is uniquely determined by the environment's state.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 策略描述了智能体的行为。智能体通过遵循策略来选择行动。从数学上讲，策略是一个将状态映射到动作的函数。这是什么意思呢？就是说，策略的输入是当前的状态，而它的输出是需要采取的动作。策略可以有不同的形式。它可以是简单的一组规则、查找表、神经网络或任何函数近似器。策略是强化学习（RL）框架的核心，所有RL算法（无论是隐式的还是显式的）的目标都是改进智能体的策略，以最大化智能体在任务（或一组任务）上的表现。策略可以是随机的，涉及到动作的分布，也可以是确定性的。在后者的情况下，所选动作是由环境的状态唯一决定的。
- en: An Example of an Autonomous Driving Environment
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个自动驾驶环境的例子
- en: 'To better understand the environment''s role and its characteristics in the
    RL framework, let''s formalize an autonomous driving environment, as shown in
    the following figure:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解环境在RL框架中的作用及其特性，让我们形式化一个自动驾驶环境，如下图所示：
- en: '![Figure 1.10: An autonomous driving scenario'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10：一个自动驾驶场景'
- en: '](img/B16182_01_10.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_10.jpg)'
- en: 'Figure 1.10: An autonomous driving scenario'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：一个自动驾驶场景
- en: 'Considering the preceding figure, let''s now look at each of the components
    of the environment:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前面的图，我们现在来看看环境的每个组成部分：
- en: '**State**: The state can be represented by the 360-degree image of the street
    around our car. In this case, the state is an image, that is, a matrix of pixels.
    It can also be represented by a series of images covering the whole space around
    the car. Another possibility is to describe the state using features and not images.
    The state can be the current velocity and acceleration of our vehicle, the distance
    from other cars, or the distance from the street border. In this case, we are
    using preprocessed information to represent the state more easily. These features
    can be extracted from images or other types of sensors (for example, **Light Detection
    and Ranging – LIDAR**).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：状态可以通过我们汽车周围 360 度的街景图像来表示。在这种情况下，状态是一个图像，即一个像素矩阵。它也可以通过一系列覆盖汽车周围整个空间的图像来表示。另一种可能性是使用特征而非图像来描述状态。状态可以是我们车辆的当前速度和加速度、与其他车辆的距离，或者与街道边缘的距离。在这种情况下，我们使用预处理过的信息来更轻松地表示状态。这些特征可以从图像或其他类型的传感器中提取（例如，**激光雷达（LIDAR）**）。'
- en: '**Dynamics**: The dynamics of the environment in an autonomous car scenario
    are represented by the equations describing how the system changes when the car
    accelerates, breaks, or steers. For instance, the vehicle is going at 30 km/h,
    and the next vehicle is 100 meters away from it. The state is represented by the
    car''s speed and the proximity information concerning the next vehicle. If the
    car accelerates, the speed changes according to the car''s properties (included
    in the environment dynamics). Also, the proximity information changes since the
    next vehicle can be closer or further away (according to the speed). In this situation,
    at the next timestep, the car''s speed can be 35 km/h, and the next vehicle can
    be closer, for example, only 90 meters away.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态**：在自动驾驶汽车场景中，环境的动态通过描述汽车加速、刹车或转向时系统如何变化的方程来表示。例如，车辆当前以 30 km/h 的速度行驶，前方的车辆距离其
    100 米。状态由汽车的速度和与前方车辆的距离信息表示。如果汽车加速，速度会根据汽车的属性（包含在环境动态中）发生变化。此外，由于速度的变化，距离信息也会发生变化，前方车辆可能会更近或更远。在这种情况下，下一个时间步，汽车的速度可能为
    35 km/h，前方车辆可能更近，例如仅 90 米远。'
- en: '**Reward**: The reward can represent how well the agent is driving. It''s not
    easy to formalize a reward function. A natural reward function should award states
    in which the car is aligned to the street and should avoid states in which the
    car crashes or goes off the road. The reward function definition is an open problem
    and researchers are putting efforts into developing algorithms where the reward
    function is not needed (self-motivation or curiosity-driven agents), where the
    agent learns from demonstrations (imitation learning), and where the agent recovers
    the reward function from demonstrations (**Inverse Reinforcement Learning or IRL**).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：奖励可以代表智能体驾驶的表现。形式化奖励函数并不容易。一个自然的奖励函数应该在汽车与街道对齐时给予奖励，并且应避免在汽车发生碰撞或驶出道路时给奖励。奖励函数的定义仍然是一个未解的问题，研究人员正在努力开发不需要奖励函数的算法（自我激励或好奇心驱动的智能体），让智能体通过示范学习（模仿学习）来学习，或者让智能体从示范中恢复奖励函数（**逆向强化学习或IRL**）。'
- en: Note
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'For further reading on curiosity-driven agents, please refer to the following
    paper: [https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf](https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf).'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关好奇心驱动的智能体的更多阅读，请参考以下论文：[https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf](https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf)。
- en: We are now ready to design and implement our first environment class using Python.
    We will demonstrate how to implement the state, the dynamics, and the reward of
    a toy problem in the following exercise.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备使用 Python 设计和实现我们的第一个环境类。在接下来的练习中，我们将展示如何实现一个玩具问题的状态、动态和奖励。
- en: 'Exercise 1.01: Implementing a Toy Environment Using Python'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.01：使用 Python 实现一个玩具环境
- en: In this exercise, we will implement a simple toy environment using Python. The
    environment is illustrated in [*Figure 1.11*](B16182_01_Final_SZ_ePub.xhtml#_idTextAnchor039).
    It is composed of three states (1, 2, 3) and two actions (A and B). The initial
    state is state 1\. States are represented by nodes. Edges represent transitions
    between states. On the edges, we have an action causing the transition and the
    associated reward.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 Python 实现一个简单的玩具环境。环境如 [*图 1.11*](B16182_01_Final_SZ_ePub.xhtml#_idTextAnchor039)
    所示。它由三个状态（1、2、3）和两个动作（A 和 B）组成。初始状态为状态 1。状态由节点表示。边表示状态之间的转换。在边上，我们有导致转换的动作和相关的奖励。
- en: 'The representation of the environment in *Figure 1.11* is the standard environment
    representation in the context of RL. In this exercise, we will become acquainted
    with the concept of the environment and its implementation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.11* 中环境的表示是强化学习中标准的环境表示。在这个练习中，我们将了解环境的概念及其实现：'
- en: '![Figure 1.11: A toy environment composed of three states (1, 2, 3) and two
    actions (A and B)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11: 由三个状态（1、2、3）和两个动作（A 和 B）组成的玩具环境'
- en: '](img/B16182_01_11.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_11.jpg)'
- en: 'Figure 1.11: A toy environment composed of three states (1, 2, 3) and two actions
    (A and B)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.11: 由三个状态（1、2、3）和两个动作（A 和 B）组成的玩具环境'
- en: In the preceding figure, the reward is associated with each state-action pair.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，奖励与每个状态-动作对相关联。
- en: 'The goal of this exercise is to implement an `Environment` class with a `step()`
    method that takes as input the agent''s actions and returns a state-action pair
    (next state, reward). In addition to this, we will write a `reset()` method that
    restarts the environment''s state:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习的目标是实现一个 `Environment` 类，其中包含一个 `step()` 方法，该方法接受代理的操作作为输入，并返回一个状态-动作对（下一个状态，奖励）。除此之外，我们将编写一个
    `reset()` 方法来重置环境的状态：
- en: Create a new Jupyter notebook or a simple Python script to enter the code.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 Jupyter 笔记本或简单的 Python 脚本来输入代码。
- en: 'Import the `Tuple` type from `typing`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `typing` 导入 `Tuple` 类型：
- en: '[PRE0]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the class constructor by initializing its properties:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过初始化其属性来定义类构造函数：
- en: '[PRE1]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the step function, which is responsible for updating the current state
    based on the previous state and the action taken by the agent:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义步骤函数，负责根据代理以前的状态和采取的动作更新当前状态：
- en: '[PRE2]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The `#` symbol in the code snippet above denotes a code comment. Comments are
    added into code to help explain specific bits of logic.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以上代码片段中的 `#` 符号表示代码注释。注释用于帮助解释特定逻辑的部分。
- en: We first check that the action is allowed. Then, we define the new current state
    and reward based on the action and the previous state by looking at the transition
    in the previous figure.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先检查动作是否允许。然后，我们根据前一个状态和动作查看上图中的转换来定义新的当前状态和奖励。
- en: 'Now, we need to define the `reset` function, which simply resets the environment
    state:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要定义 `reset` 函数，简单地重置环境状态：
- en: '[PRE3]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can use our environment class to understand whether our implementation is
    correct for the specified environment. We can do this with a simple loop, using
    a predefined set of actions to test the transitions of our environment. A possible
    action set, in this case, is `[0, 0, 1, 1, 0, 1]`. Using this set, we will test
    all of the environment''s transitions:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用我们的环境类来理解我们的实现是否适用于指定的环境。我们可以通过一个简单的循环来做到这一点，使用预定义的一组操作来测试环境的转换。在这种情况下，可能的操作集是
    `[0, 0, 1, 1, 0, 1]`。使用这个集合，我们将测试环境的所有转换：
- en: '[PRE4]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To understand this better, compare the output with *Figure 1.11* to discover
    whether the transitions and rewards are compatible with the selected actions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解这一点，请将输出与 *图 1.11* 进行比较，以发现所选操作的转换和奖励是否兼容。
- en: Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Arr9rO](https://packt.live/2Arr9rO).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/2Arr9rO](https://packt.live/2Arr9rO)。
- en: You can also run this example online at [https://packt.live/2zpMul0](https://packt.live/2zpMul0).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 [https://packt.live/2zpMul0](https://packt.live/2zpMul0) 上线运行此示例。
- en: In this exercise, we implemented a simple RL environment by defining the step
    function and the reset function. These functions are at the core of every environment,
    representing the interaction between the agent and the environment.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们通过定义步骤函数和重置函数来实现了一个简单的强化学习环境。这些函数是每个环境的核心，代表了代理与环境之间的交互。
- en: The Agent-Environment Interface
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理-环境接口
- en: 'RL considers sequential decision-making problems. In this context, we can refer
    to the agent as the "decision-maker." In sequential decision-making problems,
    actions taken by the decision-maker do not only influence the immediate reward
    and the immediate environment''s state, but they also affect future rewards and
    states. MDPs are a natural way of formalizing sequential decision-making problems.
    In MDPs, an agent interacts with an environment through actions and receives rewards
    based on the action, on the current state of the environment, and on the environment''s
    dynamics. The goal of the decision-maker is to maximize the cumulative sum of
    rewards given a horizon (which is possibly infinite). The task the agent has to
    learn is defined through the rewards it receives, as you can see in the following
    figure:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习考虑的是顺序决策问题。在这种背景下，我们可以将智能体称为“决策者”。在顺序决策问题中，决策者所采取的动作不仅会影响即时奖励和即时环境状态，还会影响未来的奖励和状态。马尔可夫决策过程（MDP）是形式化顺序决策问题的自然方式。在MDP中，智能体通过动作与环境交互，并根据动作、当前环境状态以及环境的动态来获得奖励。决策者的目标是最大化在给定时间跨度（可能是无限的）下的累积奖励。智能体必须学习的任务通过它收到的奖励来定义，正如你在下图中所看到的：
- en: '![Figure 1.12: The Agent-Environment interface'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.12：智能体-环境接口'
- en: '](img/B16182_01_12.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_12.jpg)'
- en: 'Figure 1.12: The Agent-Environment interface'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：智能体-环境接口
- en: 'In RL, an episode is divided into a sequence of discrete timesteps: ![11](img/B16182_01_12a.png).
    Here, ![a](img/B16182_01_12b.png) represents the horizon length, which is possibly
    infinite. The interaction between the agent and the environment happens at each
    timestep. At each timestep, the agent receives a representation of the current
    environment''s state, ![18](img/B16182_01_12c.png). Based on this state, it selects
    an action, ![14](img/B16182_01_12d.png), belonging to the action space given the
    current state,![15](img/B16182_01_12e.png). The action affects the environment.
    As a result, the environment changes its state, transitioning to the next state,
    ![16](img/B16182_01_12f.png), according to its dynamics. At the same time, the
    agent receives a scalar reward, ![17](img/B16182_01_12g.png) quantifying how good
    the action taken in that state was.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，一个回合被划分为一系列离散的时间步：![11](img/B16182_01_12a.png)。这里，![a](img/B16182_01_12b.png)表示可能是无限的时间跨度。智能体与环境的交互发生在每个时间步。在每个时间步，智能体接收到当前环境状态的表示，![18](img/B16182_01_12c.png)。基于这个状态，它选择一个动作，![14](img/B16182_01_12d.png)，这个动作属于给定当前状态下的动作空间，![15](img/B16182_01_12e.png)。该动作会影响环境，导致环境根据其动态变化状态，过渡到下一个状态，![16](img/B16182_01_12f.png)。同时，智能体会收到一个标量奖励，![17](img/B16182_01_12g.png)，用来量化在该状态下所采取的动作的效果。
- en: 'Let''s now try to understand the mathematical notations used in the preceding
    example:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来尝试理解前面例子中使用的数学符号：
- en: 'Time horizon ![19](img/B16182_01_12k.png): If a task has a finite time horizon,
    then ![20](img/B16182_01_12k.png) is an integer number representing the maximum
    duration of an episode. In infinite tasks, ![21](img/B16182_01_12q.png) can also
    be ![22](img/B16182_01_12o.png).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间跨度 ![19](img/B16182_01_12k.png)：如果任务具有有限的时间跨度，则![20](img/B16182_01_12k.png)是一个整数，表示回合的最大持续时间。在无限任务中，![21](img/B16182_01_12q.png)也可以是![22](img/B16182_01_12o.png)。
- en: Action ![23](img/B16182_01_12r.png) is the action taken by the agent in the
    timestep, t. The action belongs to the action space, ![24](img/B16182_01_12p.png),
    defined by the current state, ![25](img/B16182_01_12h.png).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作 ![23](img/B16182_01_12r.png)是智能体在时间步t采取的动作。该动作属于由当前状态![25](img/B16182_01_12h.png)定义的动作空间，![24](img/B16182_01_12p.png)。
- en: State ![26](img/B16182_01_12m.png) is the representation of the environment's
    state received by the agent at time t. It belongs to the state space, ![27](img/B16182_01_12i.png),
    defined by the environment. It can be represented by an image, a sequence of images,
    or a simple vector assuming different shapes. Note that the actual environment
    state can be different and more complex than the state perceived by the agent.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态 ![26](img/B16182_01_12m.png)是智能体在时间t接收到的环境状态的表示。它属于由环境定义的状态空间，![27](img/B16182_01_12i.png)。状态可以用图像、图像序列或假设有不同形状的简单向量表示。请注意，实际的环境状态可能与智能体感知到的状态不同，且更为复杂。
- en: Reward ![s](img/B16182_01_12j.png) is represented by a real number, describing
    how good the taken action was. A high reward corresponds to a good action. The
    reward is fundamental for the agent to understand how to achieve a goal.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励 ![s](img/B16182_01_12j.png) 由一个实数表示，描述所采取的行动有多好。高奖励对应于一个好的行动。奖励对于代理理解如何实现目标至关重要。
- en: In episodic RL, the agent-environment interaction is divided into episodes;
    the agent has to achieve the goal within the episode. The interaction is finalized
    to learn better behavior. After several episodes, the agent can decide to update
    its behavior by incorporating its knowledge of past interactions. Based on the
    effect of the action on the environment and the received rewards, the agent will
    perform more frequent actions yielding higher rewards.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在情节化的强化学习（episodic RL）中，代理与环境的交互被划分为多个情节；代理必须在情节内实现目标。交互结束后，代理可以通过结合过去交互的知识来更新其行为。经过若干情节后，代理会根据行动对环境的影响以及获得的奖励，执行更多频繁的行动以获得更高的奖励。
- en: What's the Agent? What's in the Environment?
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是代理？什么是环境？
- en: An important aspect to take into account when dealing with RL is the difference
    between the agent and the environment. This difference is not typically defined
    in terms of a physical distinction. Usually, we model the environment as everything
    that's not under the control of the agent. The environment can include physical
    laws, other agents, or an agent's properties or characteristics.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理强化学习（RL）时，必须考虑一个重要的方面，那就是代理和环境之间的区别。这个区别通常不是通过物理上的区分来定义的。通常，我们将环境建模为不受代理控制的所有事物。环境可以包括物理法则、其他代理，或者一个代理的属性或特征。
- en: However, this does not imply that the agent does not know the environment. The
    agent can also be aware of the environment and the effect of its actions on it,
    but it cannot change the way the environment reacts. Also, the reward computation
    belongs to the environment, as it must be entirely outside the agent's control.
    If this is not the case, the agent can learn how to modify the reward function
    in such a way as to maximize its performance without learning the task. The boundary
    between the agent and environment is a control boundary, meaning that the agent
    cannot control the reaction of the environment. It is not a knowledge boundary
    since the agent can know the environment model perfectly and still find difficulties
    in learning the task.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着代理不了解环境。代理也可以意识到环境以及其行动对环境的影响，但它无法改变环境的反应方式。此外，奖励计算属于环境，因为它必须完全在代理控制之外。如果不是这样，代理就可以学习如何修改奖励函数，以便在不学习任务的情况下最大化其表现。代理与环境之间的边界是一个控制边界，意味着代理无法控制环境的反应。这不是一个知识边界，因为代理可以完全了解环境模型，但仍然会在学习任务时遇到困难。
- en: Environment Types
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境类型
- en: In this section, we will examine some possible environment dichotomies. The
    characterization of the environment depends on the state space (finite or continuous),
    on the type of transitions (deterministic or stochastic), on the information available
    to the agent (fully or partially observable), and the number of agents involved
    in the learning problem (single versus multi-agent).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将探讨一些可能的环境二分法。环境的特征取决于状态空间（有限或连续）、转移类型（确定性或随机性）、代理可获得的信息（完全可观察或部分可观察）以及参与学习问题的代理数量（单一代理与多代理）。
- en: Finite versus Continuous
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有限与连续
- en: 'The state space gives the first distinction. The state space can be divided
    into two main categories: a finite state space and a continuous state space. A
    finite state space has a finite number of possible states in which the agent can
    be, and it''s the more straightforward case. An environment with a continuous
    state space has infinite possible states. In these types of environments, the
    generalization properties of the agent are fundamental to solve a task because
    the probability of arriving at the same state twice is almost zero. In continuous
    environments, an agent cannot use the experience due to the previous presence
    in that state; it has to generalize using some kind of similarity with respect
    to the previously experienced states. Note that generalization is also essential
    for finite state spaces with a considerable number of states (for example, when
    the state space is represented by the set of all possible images).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间给出了第一个区分。状态空间可以分为两大类：有限状态空间和连续状态空间。有限状态空间包含有限数量的可能状态，代理可以处于其中，这是较为简单的情况。而具有连续状态空间的环境则有无限多个可能的状态。在这些类型的环境中，代理的泛化能力对于解决任务至关重要，因为达到相同状态的概率几乎为零。在连续环境中，代理无法利用先前在该状态中的经验；它必须基于某种与先前经验状态的相似性进行泛化。请注意，对于具有大量状态的有限状态空间（例如，当状态空间由所有可能图像的集合表示时），泛化同样至关重要。
- en: 'Consider the following examples:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: Chess is finite. There is a finite number of possible states in which an agent
    can be. The state, for chess, is represented by the chessboard situation at a
    given time. We can calculate all the possible states by varying the situation
    of the chessboard. The number of states is very high but still finite.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋是有限的。代理可以处于的可能状态数量是有限的。对于国际象棋而言，状态由给定时刻的棋盘局面表示。我们可以通过变化棋盘局面来计算所有可能的状态。状态的数量非常高，但仍然是有限的。
- en: Autonomous driving can be defined as a continuous problem. If we describe the
    autonomous driving problem as a problem in which the agent has to make driving
    decisions based on the sensors' input, we obtain a continuous problem. The sensors
    provide continuous input in a given range. The agent state, in this case, can
    be represented by the agent's speed, the agent's acceleration, or the rotation
    of the wheels per minute.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶可以定义为一个连续问题。如果我们将自动驾驶问题描述为代理根据传感器输入做出驾驶决策的问题，那么我们得到的是一个连续问题。传感器提供给定范围内的连续输入。在这种情况下，代理的状态可以由代理的速度、加速度或每分钟车轮转速来表示。
- en: Deterministic versus Stochastic
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定性与随机性
- en: A deterministic environment is an environment in which, given a state, an action
    is performed by the agent; the following state is uniquely determined as well
    as the following reward. Deterministic environments are simple types of environments,
    but they are also rarely used due to their limited applicability in the real world.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性环境是指在给定一个状态的情况下，代理执行一个动作后，接下来的状态和奖励是唯一确定的。确定性环境是简单类型的环境，但由于其在现实世界中的适用性有限，因此很少使用。
- en: Almost all real-world environments are stochastic. In stochastic environments,
    a state and an action performed by the agent determines the probability distribution
    over the next state and the next reward. The following state is not uniquely determined,
    but it's uncertain. In these types of environments, the agent should act many
    times to obtain a reliable estimate of its consequences.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现实世界中的环境都是随机的。在随机环境中，一个状态和代理执行的动作决定了下一状态和下一奖励的概率分布。下一状态不是唯一确定的，而是不确定的。在这些类型的环境中，代理应该多次行动，以便获得可靠的后果估计。
- en: Notice that, in a deterministic environment, the agent could perform each action
    in each state exactly once, and based on the acquired knowledge, it can solve
    the task. Also, notice that solving the task does not mean taking actions that
    yield the highest immediate return, because this action can also bring the agent
    to an inconvenient part of the environment where future rewards are always low.
    To solve the task correctly, the agent should take actions with the highest associated
    future return (called a state-action value). The state-action value does not take
    into account only the immediate reward but also the future rewards, giving the
    agent a farsighted view. We will define later what a state-action value is.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在确定性环境中，智能体可以在每个状态下执行每个动作一次，基于获得的知识，它可以解决任务。同时，请注意，解决任务并不意味着采取能带来最高即时回报的动作，因为这种动作也可能将智能体带到环境中不方便的部分，在那里未来的回报始终很低。为了正确地解决任务，智能体应该采取与最高未来回报相关的动作（称为状态-动作值）。状态-动作值不仅考虑即时回报，还考虑未来的回报，给予智能体远见的视角。我们稍后将定义什么是状态-动作值。
- en: 'Consider the following examples:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: Rubik's Cube is deterministic. To a given action, it corresponds a defined state
    transition.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魔方是确定性的。对于一个给定的动作，它对应一个定义的状态转换。
- en: Chess is deterministic but opponent-dependent. The successive state does not
    depend only on the agent's action but also on the opponent's action.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋是确定性的，但依赖于对手。后续的状态不仅依赖于智能体的动作，还依赖于对手的动作。
- en: Texas Hold'em is stochastic and opponent-dependent. The transition to the next
    state is stochastic and depends on the deck, which is not known by the agent.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德州扑克牌是随机的且依赖于对手。到下一个状态的转换是随机的，并且依赖于扑克牌堆，而扑克牌堆是智能体无法知晓的。
- en: Fully Observable versus Partially Observable
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全可观察与部分可观察
- en: The agent, to plan actions, has to receive a representation of the environment
    state, ![29](img/B16182_01_12n.png) (refer to *Figure 1.12, The Agent-Environment
    interface*). If the state representation received by the agent completely defines
    the state of the environment, the environment is **Fully Observable**. If some
    parts of the environment are outside the representation observed by the agent,
    the environment is **Partially Observable**, also called the **Partially Observable
    Markov Decision Process** (**POMDP**). Partially observable environments are,
    for example, multi-agent environments. In the case of partially observable environments,
    the information perceived by the agents, together with the action taken, is not
    sufficient for determining the next state of the environment. A technique to improve
    the perception of the agent, making it more accurate, is to keep the history of
    taken actions and observations, but this requires some memory techniques (such
    as a **Recurrent Neural Network**, or **RNN**, or **Long Short-Term Memory**,
    or **LSTM**) embedded in the agent's policy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规划动作，智能体必须接收到环境状态的表示，![29](img/B16182_01_12n.png)（参见*图 1.12，智能体-环境接口*）。如果智能体接收到的状态表示完全定义了环境的状态，那么环境是**完全可观察**的。如果环境的某些部分超出了智能体观察到的范围，那么环境是**部分可观察**的，也叫做**部分可观察的马尔可夫决策过程**（**POMDP**）。部分可观察环境的例子有多智能体环境。在部分可观察环境的情况下，智能体所感知的信息和所采取的动作不足以确定环境的下一个状态。提高智能体感知准确性的一个方法是保持已执行动作和观察的历史，但这需要一些记忆技术（例如**递归神经网络**（**RNN**）或**长短期记忆网络**（**LSTM**））嵌入在智能体的策略中。
- en: Note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on LSTMs, please refer to [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 LSTM 的更多信息，请参考[https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)。
- en: POMDP versus MDP
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: POMDP 与 MDP
- en: 'Consider the following figure:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图示：
- en: '![Figure 1.13: A representation of a partially observable environment'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：部分可观察环境的示意图'
- en: '](img/B16182_01_13.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_13.jpg)'
- en: 'Figure 1.13: A representation of a partially observable environment'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：部分可观察环境的示意图
- en: In the preceding figure, the agent does not receive the full environment state
    but only an observation, ![A close up of a logo
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，智能体没有接收到完整的环境状态，而是只接收到一个观察，![一个徽标的特写
- en: Description automatically generated](img/B16182_01_13a.png).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_13a.png)。
- en: To better understand the differences between these two types of environments,
    let's look at *Figure 1.13*. In partially observable environments (POMDP), the
    representation given to the agent is only a part of the actual environment state,
    and it is not enough to understand the actual environment state without uncertainty.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这两种类型环境的区别，让我们看看*图 1.13*。在部分可观察环境（POMDP）中，给代理的表示只是实际环境状态的一部分，而且仅凭此表示无法在没有不确定性的情况下理解实际环境状态。
- en: In fully observable environments (MDPs), the state representation given to the
    agent is semantically equivalent to the state of the environment. Notice that,
    in this case, the state given to the agent can assume a different form (for example,
    an image, a vector, a matrix, or a tensor). However, from this representation,
    it is always possible to reconstruct the actual state of the environment. The
    meaning of the state is precisely the same, even if under a different form.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全可观察环境（MDP）中，给定代理的状态表示与环境的状态在语义上是等价的。请注意，在这种情况下，给代理的状态可以采用不同的形式（例如，图像、向量、矩阵或张量）。然而，从这个表示中，总是可以重建环境的实际状态。状态的意义是完全相同的，即使是在不同的形式下。
- en: 'Consider the following examples:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例：
- en: Chess (and, in general, board games) is fully observable. The agent can perceive
    the whole environment state. In a chess game, the environment state is represented
    by the chessboard, and the agent can exactly perceive the position of each pawn.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋（一般来说，棋盘游戏）是完全可观察的。代理可以感知整个环境状态。在国际象棋游戏中，环境状态由棋盘表示，代理可以精确感知每个棋子的的位置。
- en: Poker is partially observable. A poker agent cannot perceive the whole state
    of the game, which includes the opponent cards and deck cards.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扑克是部分可观察的。扑克代理无法感知游戏的全部状态，包括对手的牌和牌堆中的牌。
- en: Single Agents versus Multiple Agents
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单代理与多代理
- en: 'Another useful characteristic of environments is the number of agents involved
    in a task. If there is only one agent, the subject of our study, the environment
    is a single-agent environment. If the number of agents is more than one, the environment
    is a multi-agent environment. The presence of multiple agents increases the complexity
    of the problem since the action that influences the state becomes a joint action,
    the set of all the agents'' actions. Usually, agents only know their individual
    actions and do not know another agent''s actions. For this reason, the multi-agent
    environment is an instance of POMDP in which the partial visibility is due to
    the presence of other agents. Notice that each agent has its own observation,
    which can differ from the other agent''s observation, as shown in the following
    figure:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的另一个有用特征是参与任务的代理数量。如果只有一个代理，我们的研究对象，环境就是单代理环境。如果代理数量超过一个，环境就是多代理环境。多个代理的存在增加了问题的复杂性，因为影响状态的动作变成了联合动作，即所有代理的动作集合。通常，代理只知道自己个体的动作，而不知道其他代理的动作。因此，多代理环境是POMDP的一个实例，其中部分可见性是由于其他代理的存在。请注意，每个代理都有自己的观察，这可能与其他代理的观察不同，如下图所示：
- en: '![Figure 1.14: A schematic representation of the multi-agent decentralized
    MDP'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：多代理去中心化MDP的示意图'
- en: '](img/B16182_01_14.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_14.jpg)'
- en: 'Figure 1.14: A schematic representation of the multi-agent decentralized MDP'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：多代理去中心化MDP的示意图
- en: 'Consider the following examples:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例：
- en: Robot navigation is usually a single-agent task. We may have only one agent
    moving in a possible unknown environment. The goal of the agent can be to reach
    a given position in the environment while avoiding crashes as much as possible
    in the minimum amount of time.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人导航通常是单代理任务。我们可能只有一个代理在可能未知的环境中移动。代理的目标可以是在尽量避免碰撞的情况下，以最短的时间到达给定的位置。
- en: Poker is a multi-agent task where we have two agents competing against each
    other. The perceived state is different in this case and the perceived reward
    is also different.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扑克是一个多代理任务，其中有两个代理相互竞争。在这种情况下，感知到的状态不同，感知到的奖励也不同。
- en: An Action and Its Types
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个动作及其类型
- en: The action set of an agent in an environment can be finite or continuous. If
    the action set is finite, the agent has at its disposal a finite number of actions.
    Consider the MountainCar-v0 (discrete) example, described in more detail later.
    This has a discrete action set; the agent only has to select the direction in
    which to accelerate, and the acceleration is constant.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体在环境中的动作集可以是有限的，也可以是连续的。如果动作集是有限的，那么智能体可以使用的动作数量是有限的。以MountainCar-v0（离散）示例为例，稍后将详细描述。它具有离散的动作集；智能体只需要选择加速的方向，而加速度是恒定的。
- en: If the action set is continuous, the agent has at its disposal infinite actions
    from which it should select the best actions in a given state. Usually, tasks
    with continuous action sets are more challenging to solve than those with finite
    actions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作集是连续的，智能体可以从无限多的动作中选择最合适的动作。通常，具有连续动作集的任务比那些动作有限的任务更具挑战性。
- en: 'Let''s look at the example of MountainCar-v0:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下MountainCar-v0的例子：
- en: '![Figure 1.15: A MountainCar-v0 task'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15：MountainCar-v0任务'
- en: '](img/B16182_01_15.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_15.jpg)'
- en: 'Figure 1.15: A MountainCar-v0 task'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：MountainCar-v0任务
- en: As you can see in the preceding figure, a car is positioned in a valley between
    two mountains. The goal of the car is to arrive at the flag on the mountain to
    its right.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，一辆车位于两座山之间的山谷中。车的目标是到达右边山上的旗帜。
- en: 'The MountainCar-v0 example is a standard RL benchmark in which there is a car
    trying to ramp itself up a mountain. The car''s engine doesn''t have enough strength
    to ramp upward. For this reason, the car should use the inertia given from the
    shape of the valley, that is, it should go to the left to gain speed. The state
    is composed of the car velocity, acceleration, and *x* position. There are two
    versions of this task based on the action set we define, as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: MountainCar-v0例子是一个标准的强化学习基准任务，其中有一辆车试图将自己推上山坡。车的引擎没有足够的力量往上坡推，因此它应该利用山谷的形状提供的惯性，也就是它应该先向左移动以获得速度。状态由车的速度、加速度和*位置x*组成。根据我们定义的动作集，这个任务有两个版本，如下所示：
- en: '**MountainCar-v0 discrete**: We have only two possible actions, (-1, +1) or
    (0, 1), depending on the parameterization.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MountainCar-v0离散**：我们只有两个可能的动作，(-1, +1)或(0, 1)，取决于参数化。'
- en: '**MountainCar-v0 continuous**: A continuous set of actions from -1 to +1.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MountainCar-v0连续**：一个从-1到+1的连续动作集。'
- en: Policy
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略
- en: We define the policy as the behavior of the agent. Formally, a policy is a function
    that takes as input the history of the current episode and outputs the current
    action. The concept of policies has huge importance in RL; all RL algorithms focus
    on learning the best policy for a given task.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将策略定义为智能体的行为。严格来说，策略是一个函数，它以当前回合的历史为输入，并输出当前的动作。策略在强化学习中具有重要意义；所有强化学习算法都专注于为给定任务学习最佳策略。
- en: An example of a winning policy for the MountainCar-v0 task is a policy that
    brings the agent up on the left mountain and then uses the cumulated potential
    to ramp up the mountain on the right. For negative velocities, the optimal action
    is LEFT, as the agent should go as high as possible on the left mountain. For
    positive velocities, the agent should take the action RIGHT, as its goal is to
    ramp up the mountain on its right.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成功的MountainCar-v0任务策略的例子是，首先将智能体带到左边的山上，然后利用积累的势能爬升右边的山。对于负的速度，最佳的动作是向左（LEFT），因为智能体应该尽可能地往左山上爬。对于正的速度，智能体应该采取向右（RIGHT）的动作，因为它的目标是往右边的山上爬。
- en: A Markovian policy is simply a policy depending only on the current state and
    not the whole history.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科夫策略只是一个仅依赖于当前状态而非整个历史的策略。
- en: 'We denote a stationary Markovian policy with ![pi](img/B16182_01_15a.png) as
    follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用![pi](img/B16182_01_15a.png)表示一个静态的马尔科夫策略，如下所示：
- en: '![Figure 1.16: Stationary Markovian policy'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.16：静态马尔科夫策略'
- en: '](img/B16182_01_16.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_16.jpg)'
- en: 'Figure 1.16: Stationary Markovian policy'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：静态马尔科夫策略
- en: 'The Markovian policy goes from the state space to the action space. If we evaluate
    the policy in a given state, ![33](img/B16182_01_16b.png), we obtain the selected
    action, ![a](img/B16182_01_16a.png), in that state:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科夫策略从状态空间映射到动作空间。如果我们在给定状态下评估该策略，![33](img/B16182_01_16b.png)，我们得到在该状态下选择的动作![a](img/B16182_01_16a.png)：
- en: '![Figure 1.17: Stationary Markovian policy in state St'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.17：在状态St下的静态马尔科夫策略'
- en: '](img/B16182_01_17.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_17.jpg)'
- en: 'Figure 1.17: Stationary Markovian policy in state ![34](img/B16182_01_17b.png)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17：在状态![34](img/B16182_01_17b.png)下的静态马尔科夫策略
- en: A policy can be implemented in different ways. The most straightforward policy
    is just a rule-based policy, which is essentially a set of rules or heuristics.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 策略可以通过不同的方式实现。最简单的策略就是基于规则的策略，实际上就是一套规则或启发式方法。
- en: Policies that are a subject of interest in RL are usually parametric. Parametric
    policies are (differentiable) functions depending on a set of parameters. Usually,
    the policy parameters are identified as ![A picture containing object, clock
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，通常关注的策略是参数化的。参数化策略是依赖于一组参数的（可微分）函数。通常，策略参数被表示为！[一个包含物体的图片，时钟
- en: 'Description automatically generated](img/B16182_01_17a.png):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_01_17a.png)：
- en: '![Figure 1.18: Parametric policies'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.18：参数化策略'
- en: '](img/B16182_01_18.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_18.jpg)'
- en: 'Figure 1.18: Parametric policies'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18：参数化策略
- en: The set of policy parameters can be represented by a vector in a *d*-dimensional
    space. The selected action is determined by the policy structure (we will explore
    some possible policy structures later on), by the policy parameters, and, of course,
    by the current environment state.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 策略参数集可以通过一个 *d* 维空间中的向量来表示。所选的动作由策略结构（稍后我们将探讨一些可能的策略结构）、策略参数，当然，还有当前环境状态决定。
- en: Stochastic Policies
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机策略
- en: The policies presented so far are merely deterministic policies because the
    output is precisely an action. Stochastic policies are policies that output a
    distribution over the action space. Stochastic policies are usually powerful policies
    that mix both exploration and exploitation. With stochastic policies, it is possible
    to obtain complex behaviors.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止展示的策略仅仅是确定性策略，因为输出是一个确定的动作。随机策略则是输出一个关于动作空间的分布。随机策略通常是强大的策略，能够结合探索与利用。通过随机策略，能够获得复杂的行为。
- en: A stochastic policy assigns a certain probability to each action. The actions
    will be selected according to the associated probability.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略为每个动作分配一个特定的概率。动作将根据其关联的概率被选择。
- en: '*Figure 1.19* explains, graphically, and with an example, the differences between
    a stochastic policy and a deterministic policy. The policy in the figure has three
    possible actions.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.19* 通过图示和示例，解释了随机策略和确定性策略之间的差异。图中的策略有三种可能的动作。'
- en: The stochastic policy (upper part) assigns to actions, respectively, a probability
    of 0.2, 0.7, and 0.1\. The most probable action is the second action, which is
    associated with the highest probability. However, all of the actions could also
    be selected.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略（上部分）分别为动作分配了0.2、0.7和0.1的概率。最可能的动作是第二个动作，它与最高的概率相关联。然而，所有的动作也都有可能被选择。
- en: In the bottom part, we have the same set of actions with a deterministic policy.
    The policy, in this case, selects only one action (the second in the figure) with
    a probability of 1\. In this case, actions 1 and 3 will not be selected, having
    an associated probability of 0.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部部分，我们有相同的一组动作与确定性策略。在这种情况下，策略仅选择一个动作（图中的第二个动作），其概率为1\。此时，动作1和动作3将不会被选择，它们的关联概率为0。
- en: 'Note that we can obtain a deterministic policy from a stochastic one by taking
    the action associated with the highest probability:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以通过选择与最高概率关联的动作，从随机策略中得到确定性策略：
- en: '![Figure 1.19: Stochastic versus deterministic policies'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.19：随机策略与确定性策略'
- en: '](img/B16182_01_19.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_19.jpg)'
- en: 'Figure 1.19: Stochastic versus deterministic policies'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.19：随机策略与确定性策略
- en: Policy Parameterizations
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略参数化
- en: In this section, we will analyze some possible policy parameterizations. Parameterizing
    a policy means giving a structure to the policy function and considering how parameters
    affect our output actions. Based on the parameterization, it is possible to obtain
    simple policies or even complex stochastic policies starting from the same input
    state.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将分析一些可能的策略参数化。参数化策略意味着为策略函数赋予一个结构，并考虑参数如何影响我们的输出动作。根据参数化，能够从相同的输入状态出发，获得简单的策略，甚至是复杂的随机策略。
- en: '**Linear (Deterministic)**'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性（确定性）**'
- en: The resulting action is a linear combination of the state features, ![A picture
    containing drawing, animal
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 结果动作是状态特征的线性组合，！[一个包含画图、动物的图片
- en: 'Description automatically generated](img/B16182_01_20a.png):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_01_20a.png)：
- en: '![Figure 1.20: An expression of a linear policy'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.20：线性策略的表达'
- en: '](img/B16182_01_20.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_20.jpg)'
- en: 'Figure 1.20: An expression of a linear policy'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20：线性策略的表达式
- en: A linear policy is a very simple policy represented by a matrix multiplication.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 线性策略是一种非常简单的策略，通过矩阵乘法表示。
- en: 'Consider the example of MountainCar-v0\. The state space is represented by
    the position, speed, and acceleration: ![A close up of a sign'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑MountainCar-v0的例子。状态空间由位置、速度和加速度表示：![一个标志的特写
- en: Description automatically generated](img/B16182_01_20b.png). We usually add
    a constant, 1, that corresponds to the bias term. Therefore, ![A close up of a
    sign
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_20b.png)。我们通常会添加一个常数1，它对应偏置项。因此，![一个标志的特写
- en: Description automatically generated](img/B16182_01_20c.png) . Policy parameters
    are defined by ![A picture containing object, clock
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_20c.png)。策略参数由![一幅包含物体、时钟的画
- en: Description automatically generated](img/B16182_01_20d.png) . We can simply
    use as state features the identity function, ![40](img/B16182_01_20e.png).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_20d.png)。我们可以简单地使用恒等函数作为状态特征，![40](img/B16182_01_20e.png)。
- en: 'The resulting policy is as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的策略如下：
- en: '![Figure 1.21: A linear policy for MountainCar-v0'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.21：MountainCar-v0的线性策略'
- en: '](img/B16182_01_21.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_21.jpg)'
- en: 'Figure 1.21: A linear policy for MountainCar-v0'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.21：MountainCar-v0的线性策略
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: Using a comma, `,`, we can denote the column separator, and with a semicolon,
    `;`, we can denote the row separator.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逗号`,`，我们可以表示列分隔符，使用分号`;`，我们可以表示行分隔符。
- en: Therefore, ![41](img/B16182_01_21a.png) is a row vector, and ![42](img/B16182_01_21b.png)
    is a column vector that is equivalent to ![43](img/B16182_01_21c.png).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![41](img/B16182_01_21a.png)是行向量，![42](img/B16182_01_21b.png)是列向量，相当于![43](img/B16182_01_21c.png)。
- en: If the environment state is [1, 2, 0.1], the cart is in position ![b](img/B16182_01_21d.png)
    with velocity ![c](img/B16182_01_21e.png) and acceleration ![A picture containing
    drawing, clock
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境状态是[1, 2, 0.1]，则小车处于位置![b](img/B16182_01_21d.png)，速度为![c](img/B16182_01_21e.png)，加速度为![一幅画包含绘图、时钟
- en: Description automatically generated](img/B16182_01_21f.png), and the policy
    parameters are defined by [4, 5, 1, 1], we obtain an action, ![e](img/B16182_01_21g.png).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_21f.png)，并且策略参数由[4, 5, 1, 1]定义，我们得到一个动作，![e](img/B16182_01_21g.png)。
- en: 'Since the action space of MountainCar-v0 is defined in the interval, [-1, +1],
    we need to squash the resulting action using a squashing function such as ![f](img/B16182_01_21h.png)
    (hyperbolic tangent). In our case, ![g](img/B16182_01_21h.png) applied to the
    output of the multiplication results in approximately +1:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MountainCar-v0的动作空间定义在区间[-1, +1]内，我们需要使用像![f](img/B16182_01_21h.png)（双曲正切）这样的压缩函数来压缩得到的动作。在我们的案例中，应用![g](img/B16182_01_21h.png)到乘法的输出结果大约是+1：
- en: '![Figure 1.22: A hyperbolic tangent plot; the hyperbolic tangent squashes'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.22：双曲正切图；双曲正切将'
- en: the real numbers in the interval, [-1, +1]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 区间[-1, +1]中的实数
- en: '](img/B16182_01_22.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_22.jpg)'
- en: 'Figure 1.22: A hyperbolic tangent plot; the hyperbolic tangent squashes the
    real numbers in the interval, [-1, +1]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.22：双曲正切图；双曲正切将区间[-1, +1]中的实数压缩
- en: Even if linear policies are simple, they are usually enough to solve most tasks,
    given that the state features represent the problem.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 即使线性策略很简单，但通常足以解决大多数任务，因为状态特征已经代表了问题。
- en: '**Gaussian Policy**'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯策略**'
- en: In the case of Gaussian parameterization, the resulting action has a Gaussian
    distribution in which the mean, ![49](img/B16182_01_22a.png), and the variance,
    ![A drawing of a person
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯参数化的情况下，得到的动作遵循高斯分布，其中均值![49](img/B16182_01_22a.png)和方差![一个画着人的图
- en: 'Description automatically generated](img/B16182_01_22b.png), depend on state
    features:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_22b.png)，依赖于状态特征：
- en: '![Figure 1.23: Expression for a Gaussian policy'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.23：高斯策略的表达式'
- en: '](img/B16182_01_23.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_23.jpg)'
- en: 'Figure 1.23: Expression for a Gaussian policy'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.23：高斯策略的表达式
- en: Here, with the symbol ![c](img/B16182_01_23a.png), we denote the conditional
    distribution; therefore, with ![a](img/B16182_01_23b.png), we denote the distribution
    conditioned on state ![b](img/B16182_01_23c.png).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用符号![c](img/B16182_01_23a.png)，我们表示条件分布；因此，使用![a](img/B16182_01_23b.png)，我们表示在状态![b](img/B16182_01_23c.png)下的条件分布。
- en: Remember, the functional form of the Gaussian distribution, ![A picture containing
    drawing
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，高斯分布的函数形式，![一幅画包含图形
- en: 'Description automatically generated](img/B16182_01_23d.png), is as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_23d.png)，如下所示：
- en: '![Figure 1.24: A Gaussian distribution'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.24：高斯分布'
- en: '](img/B16182_01_24.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_24.jpg)'
- en: 'Figure 1.24: A Gaussian distribution'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.24：高斯分布
- en: 'In the case of a Gaussian policy, this becomes the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯策略的情况下，这变为以下形式：
- en: '![Figure 1.25: A Gaussian policy'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.25：高斯策略'
- en: '](img/B16182_01_25.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_25.jpg)'
- en: 'Figure 1.25: A Gaussian policy'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.25：高斯策略
- en: 'Gaussian parameterization is useful for continuous action spaces. Note that
    we are giving the agent the possibility of also changing the variance of the distribution.
    This means that it can decide to increase the variance, enabling it to explore
    scenarios where it''s not sure what the best action to take is, or it can reduce
    the variance by increasing the amount of exploitation when it''s very sure about
    which action to take in a given state. The effect of the variance can be visualized
    as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯参数化适用于连续动作空间。请注意，我们还赋予智能体改变分布方差的可能性。这意味着它可以决定增加方差，从而探索它不确定最佳动作的场景，或者当它对在给定状态下应采取的动作非常确定时，可以减少方差，从而增加利用。方差的影响可以通过以下方式可视化：
- en: '![Figure 1.26: The effect of the variance on a Gaussian policy'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.26：方差对高斯策略的影响'
- en: '](img/B16182_01_26.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_26.jpg)'
- en: 'Figure 1.26: The effect of the variance on a Gaussian policy'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.26：方差对高斯策略的影响
- en: In the preceding figure, if the variance increases (the lower curve), the policy
    becomes more exploratory. Additionally, actions that are very far from the mean
    have nonzero probabilities. When the variance is small (the higher curve), the
    policy is highly exploitative. This means that only actions that are very close
    to the mean have nonzero probabilities.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，如果方差增加（下曲线），策略变得更加探索性。此外，远离均值的动作具有非零概率。当方差较小（上曲线）时，策略高度偏向利用。这意味着只有那些非常接近均值的动作才具有非零概率。
- en: In the preceding diagram, the smaller Gaussian represents a highly explorative
    policy with respect to the larger policy. Here, we can see the effect of the variance
    on the policy exploration attitude.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，较小的高斯分布表示相对于较大的策略，具有高度探索性的策略。在这里，我们可以看到方差对策略探索态度的影响。
- en: While learning a task, in the first training episodes, the policy needs to have
    a high variance in order for it to explore different actions. The variance will
    be reduced once the agent gains some experience and becomes more and more confident
    about the best actions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习任务时，在前几个训练回合中，策略需要具有较高的方差，以便探索不同的动作。一旦智能体获得了一些经验，并且越来越有信心知道最佳动作是什么，方差将会减少。
- en: '**The Boltzmann Policy**'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**Boltzmann 策略**'
- en: 'Boltzmann parameterization is used for discrete action spaces. The resulting
    action is a softmax function acting on the weighted state features, as stated
    in the following expression:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 参数化用于离散动作空间。生成的动作是一个对加权状态特征作用的 softmax 函数，如下所示：
- en: '![Figure 1.27: Expression for a Boltzmann policy'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.27：Boltzmann 策略的表达式'
- en: '](img/B16182_01_27.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_27.jpg)'
- en: 'Figure 1.27: Expression for a Boltzmann policy'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.27：Boltzmann 策略的表达式
- en: Here, ![55](img/B16182_01_27a.png) is the set of parameters associated with
    action ![c](img/B16182_01_27b.png).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![55](img/B16182_01_27a.png) 是与动作 ![c](img/B16182_01_27b.png) 相关联的参数集。
- en: 'The Boltzmann policy is a stochastic policy. The motivation behind this is
    very simple; let''s sum the policy over all the actions (the denominator does
    not depend on the action, ![d](img/B16182_01_27c.png)), as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 策略是一个随机策略。其背后的动机非常简单；让我们对所有动作求和（分母与动作无关，![d](img/B16182_01_27c.png)），如下所示：
- en: '![Figure 1.28: A Boltzmann policy over all of the actions'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.28：所有动作的 Boltzmann 策略'
- en: '](img/B16182_01_28.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_28.jpg)'
- en: 'Figure 1.28: A Boltzmann policy over all of the actions'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.28：所有动作的 Boltzmann 策略
- en: The Boltzmann policy becomes deterministic if we select the action with the
    highest probability, which is equivalent to selecting the mean action in a Gaussian
    distribution. What the Boltzmann parameterization represents is simply a normalization
    of the value, ![59](img/B16182_01_28a.png), corresponding to the score of action
    ![e](img/B16182_01_27d.png). The score is thus normalized by considering the value
    of all the other actions obtaining a distribution.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择具有最高概率的动作，Boltzmann 策略变得确定性，这相当于在高斯分布中选择均值动作。Boltzmann 参数化所表示的仅仅是值的归一化，![59](img/B16182_01_28a.png)，对应于动作的得分
    ![e](img/B16182_01_27d.png)。因此，得分通过考虑所有其他动作的值来进行归一化，从而获得一个分布。
- en: In all of these parametrizations, the state features might be non-linear features
    depending on several parameters, for example, whether it is coming from a neural
    network, the **radial basis function (RBF)** features, or the tile coding features.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些参数化中，状态特征可能是非线性特征，这取决于多个参数，例如它是否来自神经网络、**径向基函数 (RBF)** 特征或平铺编码特征。
- en: 'Exercise 1.02: Implementing a Linear Policy'
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.02：实现线性策略
- en: 'In this exercise, we will practice with the implementation of a linear policy.
    The goal is to write the presented parameterizations in the case of a state composed
    of ![n](img/B16182_01_28b.png) components. In the first case, the features can
    be represented by the identity function; in the second case, the features are
    represented by a polynomial function of order 2:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将练习实现线性策略。目标是编写在由 ![n](img/B16182_01_28b.png) 组件组成的状态情况下呈现的参数化。第一种情况下，特征可以通过恒等函数表示；第二种情况下，特征通过二阶多项式函数表示：
- en: 'Open a new Jupyter notebook and import NumPy to implement all of the requested
    policies:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter 笔记本并导入 NumPy 以实现所有请求的策略：
- en: '[PRE5]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s now implement the linear policy. A linear policy can be efficiently
    represented by a dot product between the policy parameters and state features.
    The first step is to write the constructor:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来实现线性策略。线性策略可以通过策略参数和状态特征之间的点积来有效表示。第一步是编写构造函数：
- en: '[PRE6]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The constructor simply sets the attribute's parameters and features. The feature
    parameter is actually a callable that takes, as input, a NumPy array and returns
    another NumPy array. The input is the environment state, whereas the output is
    the state features.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构造函数仅设置属性的参数和特征。特征参数实际上是一个可调用的函数，它接受一个 NumPy 数组作为输入，并返回另一个 NumPy 数组。输入是环境状态，而输出是状态特征。
- en: 'Next, we will implement the call method. The `__call__` method takes as input
    the state, and returns the selected action according to the policy parameters.
    The call represents a real policy implementation. What we have to do in the linear
    case is to first apply the feature function and then compute the dot product between
    the parameters and the features. A possible implementation of the call function
    is as follows:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `__call__` 方法。`__call__` 方法以状态作为输入，并根据策略参数返回所选动作。调用代表了一个真实的策略实现。在线性情况下，我们需要先应用特征函数，然后计算参数和特征之间的点积。`call`
    函数的一个可能实现如下：
- en: '[PRE7]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s try the defined policy with a state composed of a 5-dimensional array.
    Sample a random set of parameters and a random state vector. Create the policy
    object. The constructor needs the callable features, which, in this case, is the
    identity function. Call the policy to obtain the resulting action:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试用一个由 5 维数组组成的状态来定义策略。随机采样一组参数和一个随机状态向量。创建策略对象。构造函数需要可调用的特征，在这种情况下是恒等函数。调用策略以获得结果动作：
- en: '[PRE8]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output will be as follows:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE9]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This value is the action selected by our agent, given the state and the policy
    parameters. In this case, the selected action is `[[1.33244481]]`. The meaning
    of the action depends on the RL task.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 该值是我们的智能体在给定状态和策略参数的情况下选择的动作。在此案例中，所选动作是 `[[1.33244481]]`。该动作的含义取决于强化学习任务。
- en: Of course, you will obtain different results based on the sampled parameters
    and sampled state. It is always possible to seed the NumPy random number generator
    to obtain reproducible results.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你将根据采样的参数和采样的状态获得不同的结果。始终可以为 NumPy 随机数生成器设置种子，以获得可复现的结果。
- en: Note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yvrku7](https://packt.live/2Yvrku7).
    You can also refer to the Gaussian and Boltzmann policies that are implemented
    in the same notebook.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/2Yvrku7](https://packt.live/2Yvrku7)。你还可以查看在同一笔记本中实现的高斯策略和玻尔兹曼策略。
- en: You can also run this example online at [https://packt.live/3dXc4Nc](https://packt.live/3dXc4Nc).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，网址为 [https://packt.live/3dXc4Nc](https://packt.live/3dXc4Nc)。
- en: In this exercise, we practiced with different policies and parameterizations.
    These are simple policies, but they are the building blocks of more complex policies.
    The trick is just to substitute the state features with a neural network or any
    other feature extractor.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们使用了不同的策略和参数化。这些是简单的策略，但它们是更复杂策略的构建块。关键在于将状态特征替换为神经网络或其他特征提取器。
- en: Goals and Rewards
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标和奖励
- en: In RL, the agent's goal is to maximize the total amount of reward it receives
    during an episode.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，代理的目标是最大化其在一个回合中收到的总奖励。
- en: 'This is based on the famous reward hypothesis in *Sutton & Barto 1998*:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这基于著名的*萨顿和巴托 1998*年的奖励假设：
- en: '*"That all of what we mean by goals and purposes can be well thought of as
    the maximization of the expected value of the cumulative sum of a received scalar
    signal (called reward)."*'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我们所说的目标和目的，可以很好地理解为期望值最大化，即接收到的标量信号（称为奖励）累积和的最大化。”*'
- en: The important thing here is that the reward should not describe how to achieve
    the goal; instead, it should describe the goal of the agent. The reward function
    is an element of the environment, but it can also be designed for a specific task.
    In principle, there are infinite reward functions for each task. Usually, reward
    functions that are characterized by a lot of information help the agent to learn.
    Sparse reward functions (with no information) makes learning difficult or, sometimes,
    impossible. Sparse reward functions are functions in which, most of the time,
    the reward is constant (or zero).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是，奖励不应该描述如何实现目标；而应该描述代理的目标。奖励函数是环境的一个元素，但它也可以针对特定任务进行设计。原则上，每个任务都有无限多的奖励函数。通常，包含大量信息的奖励函数有助于代理学习。稀疏奖励函数（没有信息）会使学习变得困难，甚至有时是不可能的。稀疏奖励函数是指大多数时候奖励是常数（或零）的函数。
- en: Sutton's hypothesis, which we explained earlier, is the basis of the RL framework.
    This hypothesis may be wrong; probably, a scalar reward signal (and its maximization)
    is not enough to define complex goals; however, still, this hypothesis is very
    flexible, simple, and it can be applied to a wide range of tasks. At the time
    of writing, the reward function design is more art than engineering; there are
    no formal practices regarding how to write a reward function, rather there are
    only best practices based on experience. Usually, a simple reward function works
    very well. Usually, we associate a positive value with good actions and behavior
    and negative values with bad actions or actions that are not important at that
    particular moment.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前解释过的萨顿假设是强化学习框架的基础。这个假设可能是错误的；可能一个标量奖励信号（及其最大化）不足以定义复杂目标；然而，尽管如此，这个假设仍然非常灵活、简单，且可以应用于广泛的任务。在写作时，奖励函数的设计更多的是一种艺术，而非工程；关于如何编写奖励函数没有正式的实践方法，只有基于经验的最佳实践。通常，一个简单的奖励函数效果很好。我们通常会将正值与良好的行为和动作关联，将负值与不好的行为或在特定时刻不重要的行为关联。
- en: 'In a locomotion task (for example, teaching a robot how to move), the reward
    may be defined as proportional to the robot''s forward movement. In chess, the
    reward may be defined as 0 for each timestep: +1 if the agent wins and -1 if the
    agent loses. If we want our agent to solve Rubik''s Cube, the reward may be defined
    similarly: 0 every step and +1 if the cube is solved.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个运动任务中（例如，教机器人如何移动），奖励可能被定义为与机器人前进的运动成正比。在国际象棋中，奖励可以定义为每个时间步长为0：如果代理获胜，则奖励+1；如果代理失败，则奖励-1。如果我们希望代理解决魔方，奖励可以类似地定义：每一步为0，且如果魔方被解开则奖励+1。
- en: Sometimes, as we learned earlier, defining a scalar reward function for a task
    is not easy, and, nowadays, it is more art than engineering or science.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，正如我们之前所学的，为任务定义一个标量奖励函数并不容易，今天，它更多的是一种艺术，而非工程或科学。
- en: 'In each of these tasks, the final objective is to learn a policy, a way of
    selecting actions, maximizing the total rewards received by the agent. Tasks can
    be episodic or continuous. Episodic tasks have a finite length, that is, a finite
    number of timesteps (for example, T is finite). Continuous tasks can last forever
    or until the agent reaches its goal. In the first case, we can simply define the
    total reward (**return**) received by an agent as the sum of the individual rewards:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些任务中，最终的目标是学习一个策略，即选择动作的方式，最大化代理收到的总奖励。任务可以是回合性的或连续的。回合性任务有一个有限的长度，即有限的时间步长（例如，T是有限的）。连续任务可以永远持续，直到代理达到目标。在第一种情况下，我们可以简单地将代理收到的总奖励（**回报**）定义为各个奖励的总和：
- en: '![Figure 1.29: Expression for a total reward'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.29：总奖励的表达式'
- en: '](img/B16182_01_29.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_29.jpg)'
- en: 'Figure 1.29: Expression for a total reward'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.29：总奖励的表达式
- en: 'Usually, we are interested in the return from a certain timestep, ![61](img/B16182_01_29a.png).
    In other words, the return, ![62](img/B16182_01_29b.png) , quantifies the agent''s
    performance in the long term, and it can be calculated as the sum of immediate
    rewards following time t until the end of the episode (timestep ![d](img/B16182_01_12b.png)):'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们关心的是从某个时间步![61](img/B16182_01_29a.png)开始的回报。换句话说，回报![62](img/B16182_01_29b.png)量化了代理的长期表现，并且可以计算为从时间t开始直到本集结束（时间步![d](img/B16182_01_12b.png)）的即时奖励之和：
- en: '![Figure 1.30: Expression for a return from timestep t'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.30：从时间步t开始的回报表达式
- en: '](img/B16182_01_30.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_30.jpg)'
- en: 'Figure 1.30: Expression for a return from timestep t'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.30：从时间步t开始的回报表达式
- en: It is straightforward to see that, with this formulation, the return for continuing
    tasks diverges to infinity.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，在这个公式下，连续任务的回报会趋向于无穷大。
- en: In order to deal with continuing tasks, we need to introduce the notion of a
    discounted return. This concept formalizes, in mathematical terms, the principle
    that the immediate reward (sometimes) is more valuable than the same amount of
    reward after many steps. This principle is widely known in economics. The discount
    factor, ![64](img/B16182_01_30a.png), quantifies the present value of future rewards.
    We are ready to present the unified notation for the return in episodic and continuing
    tasks.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理连续任务，我们需要引入折扣回报的概念。这个概念在数学上形式化了即时奖励（有时）比许多步骤后的相同奖励更有价值的原则。这个原则在经济学中被广泛知道。折扣因子![64](img/B16182_01_30a.png)量化了未来奖励的现值。我们已经准备好展示用于情景任务和连续任务的统一回报符号。
- en: 'The discounted return is the cumulative, discounted sum of rewards until the
    end of the episode. In mathematical terms, it can be formalized as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣回报是直到本集结束的奖励的累计折扣和。从数学上讲，可以形式化如下：
- en: '![Figure 1.31: Expression for the discounted return from timestep t'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.31：从时间步t开始的折扣回报表达式](img/B16182_01_31.jpg)'
- en: '](img/B16182_01_31.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_31.jpg)'
- en: 'Figure 1.31: Expression for the discounted return from timestep t'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.31：从时间步t开始的折扣回报表达式
- en: To understand how the discount affects the return, it is possible to see that
    the value of receiving reward ![e](img/B16182_01_31a.png) after a ![66](img/B16182_01_31b.png)
    timestep is ![67](img/B16182_01_31c.png), since ![68](img/B16182_01_31g.png) is
    less than or equal to ![69](img/B16182_01_31f.png). It is worth introducing the
    effect of the discount on the return. If ![A picture containing object, clock
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解折扣如何影响回报，可以看出，接收奖励![e](img/B16182_01_31a.png)后经过![66](img/B16182_01_31b.png)时间步的回报是![67](img/B16182_01_31c.png)，因为![68](img/B16182_01_31g.png)小于或等于![69](img/B16182_01_31f.png)。值得引入折扣对回报的影响。如果![A
    picture containing object, clock
- en: 'Description automatically generated](img/B16182_01_31d.png), the return, even
    if composed by an infinite sum, has a bounded value. If ![70](img/B16182_01_31e.png),
    the agent is myopic since it cares only about the immediate reward, and it does
    not care about future rewards. A myopic agent can cause problems: the only thing
    it learns is to select the action yielding the highest immediate return. A myopic
    chess player can, for example, eat the opponent''s pawn causing the game''s loss.
    Notice that, for some tasks, this isn''t always a problem. This includes tasks
    in which the current action does not affect the future reward and has no consequences
    for the agent''s future. These tasks can be solved by finding the action that
    causes a higher immediate reward for each state independently. Most of the time,
    the current action influences the future of the agent and its rewards. If the
    discount factor is near to 1, the agent is farsighted; it is possible for them
    to sacrifice an action yielding to a good immediate reward now for a higher reward
    in future steps.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_01_31d.png)，即使回报由一个无穷和组成，它也有一个有界值。如果![70](img/B16182_01_31e.png)，则代理是近视的，因为它只关心即时奖励，而不关心未来奖励。近视代理可能会引发问题：它所学到的唯一一件事就是选择产生最高即时回报的动作。例如，一个近视的国际象棋选手可能会吃掉对方的兵，导致游戏失利。请注意，对于某些任务，这并不总是问题。这些任务包括当前动作不会影响未来回报，也不会对代理的未来产生影响。这些任务可以通过为每个状态独立找到一个产生更高即时回报的动作来解决。大多数情况下，当前的动作会影响代理及其未来的奖励。如果折扣因子接近1，代理是远视的；它有可能为了未来更高的奖励，牺牲当前产生良好即时回报的动作。
- en: 'It is important to understand the relationship between returns at different
    timesteps, both from a theoretical point of view but also from an algorithmic
    point of view, because many RL algorithms are based on this principle:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同时间步的回报关系非常重要，不仅从理论角度看，也从算法角度看，因为许多强化学习（RL）算法都基于这一原理：
- en: '![Figure 1.32: Relationship between returns at different timesteps'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.32：不同时间步的回报关系'
- en: '](img/B16182_01_32.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_32.jpg)'
- en: 'Figure 1.32: Relationship between returns at different timesteps'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.32：不同时间步的回报关系
- en: By following these simple steps, we can see that the return from the timestep
    equals the immediate reward plus the return at the following step scaled by gamma.
    This simple relationship will be extensively used in RL algorithms.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下这些简单的步骤，我们可以看到某一时间步的回报等于即时奖励加上下一个时间步的回报乘以折扣因子。这一简单的关系将在强化学习算法中广泛使用。
- en: Why Discount?
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要折扣？
- en: 'The following describes the motivations as to why many RL problems are discounted:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了为什么许多强化学习问题需要折扣的动机：
- en: It is convenient from a mathematical perspective to have a bounded return, and
    also in the case of continuing tasks.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数学角度来看，拥有一个有界的回报是方便的，尤其是在处理持续任务时。
- en: If the task is a financial task, immediate rewards may gain more interest than
    delayed rewards.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任务是金融任务，即时奖励可能比延迟奖励更具吸引力。
- en: Animal and human behavior show a preference for immediate rewards.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动物和人类行为偏好即时奖励。
- en: A discounted reward may also represent uncertainty about the future.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣奖励也可以表示对未来的不确定性。
- en: It is also possible to use an undiscounted return ![d](img/B16182_01_32a.png)
    if all of the episodes terminate after a finite number of steps.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有的回合在有限步骤后终止，也可以使用未折扣的回报！[d](img/B16182_01_32a.png)
- en: This section introduced the main elements of RL, including agents, actions,
    environments, transition functions, and policies. In the next section, we will
    practice with these concepts by defining agents, environments, and measuring the
    performance of agents on some tasks.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了强化学习的主要元素，包括智能体、动作、环境、转移函数和策略。在接下来的章节中，我们将通过定义智能体、环境，并在一些任务上评估智能体的表现来实践这些概念。
- en: Reinforcement Learning Frameworks
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习框架
- en: In the previous sections, we learned the basic theory behind RL. In principle,
    an agent or an environment can be implemented in any way or any language. For
    RL, the primary language used by both academic and industrial people is Python,
    as it allows you to focus on the algorithms and not on the language details, making
    it very simple to use. Implementing, from scratch, an algorithm or a complex environment
    (that is, an autonomous driving environment) might be very difficult and error-prone.
    For this reason, several well-established and well-tested libraries make RL very
    easy for newcomers. In this section, we will explore the main Python RL libraries.
    We will present OpenAI Gym, a set of environments that is ready to use and easy
    to modify, and OpenAI Baselines, a set of high quality, state-of-the-art algorithms.
    By the end of this chapter, you will have learned about and practiced with environments
    and agents.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了强化学习背后的基本理论。原则上，智能体或环境可以用任何方式或任何语言来实现。对于强化学习，学术界和工业界使用的主要语言是 Python，因为它让你能够专注于算法，而不必关注语言的细节，从而使得使用变得非常简单。从零开始实现一个算法或复杂的环境（例如自动驾驶环境）可能非常困难且容易出错。为此，一些经过良好验证的库使得强化学习对于新手来说变得非常简单。在本节中，我们将探索主要的
    Python 强化学习库。我们将介绍 OpenAI Gym，这是一组现成可用且易于修改的环境，和 OpenAI Baselines，这是一组高质量、最前沿的算法。在本章结束时，你将学习并实践环境和智能体的使用。
- en: OpenAI Gym
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: OpenAI Gym ([https://gym.openai.com](https://gym.openai.com)) is a Python library
    that provides a set of RL environments ranging from toy environments to Atari
    environments and more complex environments, such as **MuJoCo** and **Robotics**
    environments. OpenAI Gym, besides providing this large set of tasks, also provides
    a unified interface for interacting with RL tasks and a set of interfaces that
    are useful for describing the environment's characteristics, such as the action
    space and the state space. An important property of Gym is that its only focus
    is on environments; it makes no assumption of the type of agent you have or the
    computational framework you use. We will not cover the installation details in
    this chapter for ease of presentation. Instead, we will focus on the main concepts
    and learn how to interact with these libraries.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym ([https://gym.openai.com](https://gym.openai.com))是一个Python库，提供了一套RL环境，从玩具环境到Atari环境，再到更复杂的环境，如**MuJoCo**和**Robotics**环境。除了提供这一大套任务外，OpenAI
    Gym还提供了一个统一的接口，用于与RL任务进行交互，并提供了一套有助于描述环境特性的接口，如动作空间和状态空间。Gym的一个重要特点是，它只专注于环境；它不对你使用的智能体类型或计算框架做任何假设。为了便于展示，我们在本章中不会详细介绍安装细节。相反，我们将专注于主要概念，并学习如何与这些库进行交互。
- en: Getting Started with Gym – CartPole
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gym入门 – CartPole
- en: 'CartPole is a classical control environment provided by Gym and used by researchers
    as a starting point of algorithms. It consists of a cart that moves along the
    horizontal axis (1-dimensional) and a pole anchored to the cart on one endpoint:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole是Gym提供的一个经典控制环境，研究人员常常用它作为算法的起点。它由一个沿水平轴（1维）移动的推车和一个固定在推车一端的杆组成：
- en: '![Figure 1.33: CartPole environment representation'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.33：CartPole环境表示'
- en: '](img/B16182_01_33.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_33.jpg)'
- en: 'Figure 1.33: CartPole environment representation'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.33：CartPole环境表示
- en: The agent has to learn how to move the cart to balance the pole (that is, to
    stop the pole from falling). The episode ends when the pole angle (![71](img/B16182_01_33a.png))
    becomes higher than a certain threshold (![72](img/B16182_01_33b.png)). The state
    space is represented by the position of the cart along the axis, ![73](img/B16182_01_33c.png);
    the velocity along the axis, ![74](img/B16182_01_33d.png); the pole angle, ![75](img/B16182_01_33e.png);
    and the pole angular velocity, ![76](img/B16182_01_33f.png). The state space is
    continuous in this case, but it can also be discretized to make learning simpler.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体必须学会如何移动推车以保持杆子的平衡（即防止杆子倒下）。当杆子的角度 (![71](img/B16182_01_33a.png)) 超过某个阈值
    (![72](img/B16182_01_33b.png)) 时，回合结束。状态空间由推车在轴上的位置表示！[73](img/B16182_01_33c.png)；轴上的速度，![74](img/B16182_01_33d.png)；杆子的角度，![75](img/B16182_01_33e.png)；和杆子的角速度，![76](img/B16182_01_33f.png)。在这种情况下，状态空间是连续的，但也可以离散化以简化学习。
- en: In the following steps, we will practice with Gym and its environments.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将实践Gym及其环境。
- en: 'Let''s create a CartPole environment using Gym and analyze its properties in
    a Jupyter notebook. Please refer to the *Preface* for Gym installation instructions:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Gym创建一个CartPole环境，并在Jupyter Notebook中分析它的属性。关于Gym安装的说明，请参阅*前言*部分：
- en: '[PRE10]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you run these lines, you will get the following output:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这些代码行，你将得到以下输出：
- en: '[PRE11]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Discrete(2)` means that the action space of CartPole is a discrete action
    space composed of two actions: Go Left and Go Right. These actions are the only
    actions available to the agent. The action of Go Left, in this case, is represented
    by action 0, and the action of Go Right by action 1.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discrete(2)`表示CartPole的动作空间是一个由两种动作组成的离散动作空间：向左移动和向右移动。这些动作是智能体可用的唯一动作。在这种情况下，向左移动的动作由动作0表示，向右移动的动作由动作1表示。'
- en: '`Box(4,)` means that the state space (the observation space) of the environment
    is represented by a 4-dimensional box, a subspace of ![d](img/B16182_01_33g.png).
    Formally, it is a Cartesian product of `n` intervals. The state space has a lower
    bound and an upper bound. The bounds may also be infinite, creating an unbounded
    box.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '`Box(4,)`表示环境的状态空间（观察空间）由一个4维的盒子表示，这是![d](img/B16182_01_33g.png)的一个子空间。形式上，它是`n`个区间的笛卡尔积。状态空间有一个下界和一个上界。界限也可以是无限的，从而创建一个无界盒子。'
- en: 'To inspect the observation space better, we can use the properties of `high`
    and `low`:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地检查观察空间，我们可以使用`high`和`low`的属性：
- en: '[PRE12]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will print the following:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下内容：
- en: '[PRE13]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, we can see that upper and lower bounds are arrays of 4 elements; one
    element for each state dimension. The following are some observations:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到上下边界是包含 4 个元素的数组；每个元素代表一个状态维度。以下是一些观察：
- en: The lower bound of the cart position (the first state dimension) is -4.8, while
    the upper bound is 4.8\.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车位置（第一个状态维度）的下限为 -4.8，上限为 4.8\。
- en: The lower bound of the velocity (the second state dimension) is -3.1038, basically
    ![77](img/B16182_01_33n.png); and the upper bound is +3.1038, basically ![78](img/B16182_01_33m.png).
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度（第二个状态维度）的下限为 -3.1038，基本上是 ![77](img/B16182_01_33n.png)；上限为 +3.1038，基本上是 ![78](img/B16182_01_33m.png)。
- en: The lower bound of the pole angle (the third state dimension) is -0.4 radians,
    representing an angle of -24 degrees. The upper bound is 0.4 radians, representing
    an angle of +24 degrees.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆角度（第三个状态维度）的下限为 -0.4 弧度，表示角度为 -24 度。上限为 0.4 弧度，表示角度为 +24 度。
- en: The lower and upper bounds of the pole angular velocity (the fourth state dimension)
    are, respectively, ![79](img/B16182_01_33p.png) and ![80](img/B16182_01_33m.png)
    , similar to the lower and upper bounds for the cart policy's angular velocity.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆角速度（第四个状态维度）的下限和上限分别为 ![79](img/B16182_01_33p.png) 和 ![80](img/B16182_01_33m.png)，类似于小车政策的角速度的下限和上限。
- en: Gym Spaces
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gym 空间
- en: The Gym `Space` class represents the way Gym describes actions and state spaces.
    The most used spaces are the `Discrete` and `Box` spaces.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Gym `Space` 类表示 Gym 描述动作和状态空间的方式。最常用的空间是 `Discrete` 和 `Box` 空间。
- en: A discrete space is composed of a fixed number of elements. It can represent
    both a state space but also an action space, and it describes the number of elements
    through the `n` attribute. Its elements range from 0 to `n-1`.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 离散空间由固定数量的元素组成。它既可以表示状态空间，也可以表示动作空间，并通过 `n` 属性描述元素的数量。其元素的范围从 0 到 `n-1`。
- en: A `Box` space describes its shape through the `shape` attribute. It can have
    an n-dimensional shape that corresponds to an `n`-dimensional box. A `Box` space
    can also be unbounded. Each interval has the form of one of ![81](img/B16182_01_33h.png).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '`Box` 空间通过 `shape` 属性描述其形状。它可以具有与 `n` 维盒子对应的 `n` 维形状。`Box` 空间也可以是无界的。每个区间的形式类似于
    ![81](img/B16182_01_33h.png)。'
- en: It is possible to sample from the action space to gain insight into the elements
    it is composed of using the `space.sample()` method.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从动作空间中采样，以了解其组成元素，使用 `space.sample()` 方法。
- en: Note
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the sampling distribution of box environments, to create a sample of the
    box, each coordinate is sampled according to the form of the interval in the following
    distributions:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于盒子环境的采样分布，为了创建一个盒子的样本，每个坐标是根据以下分布中的区间形式来采样的：
- en: '- ![82](img/B16182_01_33i.png) : A uniform distribution'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '- ![82](img/B16182_01_33i.png)：一个均匀分布'
- en: '-![83](img/B16182_01_33j.png): A shifted exponential distribution'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '- ![83](img/B16182_01_33j.png)：一个平移的指数分布'
- en: '- ![84](img/B16182_01_33k.png): A shifted negative exponential distribution'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '- ![84](img/B16182_01_33k.png)：一个平移的负指数分布'
- en: '- ![85](img/B16182_01_33l.png): A normal distribution'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '- ![85](img/B16182_01_33l.png)：一个正态分布'
- en: 'Let''s now demonstrate how to create simple spaces and how to sample from spaces:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们演示如何创建简单的空间以及如何从空间中采样：
- en: '[PRE14]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will print the samples from our spaces:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出我们空间中的样本：
- en: '[PRE15]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Of course, the samples will change according to your seeds.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，样本会根据你的种子值发生变化。
- en: As you can see, we have sampled element 4 from our discrete space composed of
    5 elements (from 0 to 4). We sampled a random 4 x 4 matrix with elements between
    0 and 1, the lower and the upper bound of our space.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们从由 5 个元素（从 0 到 4）组成的离散空间中抽取了元素 4。我们随机抽取了一个 4 x 4 的矩阵，其元素值介于 0 和 1 之间，分别对应我们空间的下限和上限。
- en: 'To obtain reproducible results, it is also possible to set the seed of an environment
    using the `seed` method:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得可复现的结果，也可以通过 `seed` 方法设置环境的种子：
- en: '[PRE16]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will print the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下内容：
- en: '[PRE17]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The previous statement will always print the same sample since we set the seed
    to 0\. Seeding an environment is very important in order to guarantee reproducible
    results.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将种子设置为 0，上述语句将始终打印相同的样本。为环境设置种子非常重要，以确保结果的可复现性。
- en: 'Exercise 1.03: Creating a Space for Image Observations'
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.03：为图像观测创建一个空间
- en: 'In this exercise, we will create a space to represent an image observation.
    Image-based observations are essential in RL since they allow the agent to learn
    from pixels and require minimal feature engineering or need to go through the
    feature extraction phase. The agent can focus on what is important for its task
    without being limited by manually decided heuristics. We will create a space representing
    RGB images with dimensions equal to 256 x 256:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将创建一个空间来表示图像观测。基于图像的观测在强化学习中至关重要，因为它们允许智能体从像素中学习，并且需要最少的特征工程，或者不需要经过特征提取阶段。智能体可以专注于对其任务重要的内容，而不受人工决策启发式限制。我们将创建一个代表
    RGB 图像的空间，尺寸为 256 x 256：
- en: 'Open a new Jupyter notebook and import the desired modules – `gym` and NumPy:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook 并导入所需的模块 – `gym` 和 NumPy：
- en: '[PRE18]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We are dealing with 256 x 256 RGB images, so the space has a shape of (256,
    256, 3). In addition, the images range from 0 to 255 (if we consider the `uint8`
    images):'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们处理的是 256 x 256 的 RGB 图像，因此空间的形状为 (256, 256, 3)。此外，图像的像素值范围是 0 到 255（如果我们考虑
    `uint8` 类型的图像）：
- en: '[PRE19]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We are now ready to create the space. An image is a `Box` space since it has
    defined bounds:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好创建空间了。图像是一个 `Box` 空间，因为它有定义的边界：
- en: '[PRE20]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will print the representation of our space:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印我们空间的表示：
- en: '[PRE21]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The first dimension is the image width, the second dimension is the image height,
    and the third dimension is the number of channels.
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个维度是图像宽度，第二个维度是图像高度，第三个维度是通道数。
- en: 'Here is a sample from the space:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是来自空间的一个样本：
- en: '[PRE22]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will return the space sample; in this case, it is a huge tensor of 256
    x 256 x 3 unsigned integers (between 0 and 255). The output (fewer lines are presented
    now) should be similar to the following:'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回空间样本；在这个例子中，它是一个 256 x 256 x 3 的大张量，包含无符号整数（介于 0 和 255 之间）。输出（此处仅显示部分行）应该类似于以下内容：
- en: '[PRE23]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To visualize the returned sample, use the following code:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要可视化返回的样本，可以使用以下代码：
- en: '[PRE24]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output will be as follows:'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.34: A sample from a Box space of (256, 256) RGB'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.34：来自 (256, 256) RGB 的 Box 空间样本'
- en: '](img/B16182_01_34.jpg)'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_01_34.jpg)'
- en: 'Figure 1.34: A sample from a Box space of (256, 256) RGB'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.34：来自 (256, 256) RGB 的 Box 空间样本
- en: The preceding is not very informative because it is a random image.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述内容并不非常有信息性，因为它是一张随机图像。
- en: 'Now, suppose we want to give our agent the opportunity to see the last `n=4`
    frames. By adding the temporal component, we can obtain a state representation
    composed of 4 dimensions. The first dimension is the temporal one, the second
    is the width, the third is the height, and the last one is the number of channels.
    This is a very useful technique that allows the agent to understand its movement:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，假设我们希望让我们的智能体看到最近的 `n=4` 帧。通过添加时间维度，我们可以获得由四个维度组成的状态表示。第一个维度是时间维度，第二个是宽度，第三个是高度，最后一个是通道数。这是一个非常有用的技巧，可以让智能体理解它的运动：
- en: '[PRE25]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will print the following:'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印以下内容：
- en: '[PRE26]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As you can see, we have successfully created a space and, on inspecting the
    space representation, we notice that we have another dimension: the temporal dimension.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们成功创建了一个空间，并且在检查空间的表示时，我们注意到它有另一个维度：时间维度。
- en: Note
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AwJm7x](https://packt.live/2AwJm7x).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2AwJm7x](https://packt.live/2AwJm7x)。
- en: You can also run this example online at [https://packt.live/2UzxoAY](https://packt.live/2UzxoAY).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/2UzxoAY](https://packt.live/2UzxoAY) 在线运行这个示例。
- en: Image-based environments are very important in RL. They allow the agent to learn
    salient features for solving the task directly from raw pixels, without any preprocessing.
    In this exercise, we learned how to create a Gym space for image observations
    and how to deal with image spaces.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的环境在强化学习（RL）中非常重要。它们允许智能体直接从原始像素中学习显著特征，从而解决任务，而不需要任何预处理。在本练习中，我们学习了如何为图像观测创建一个
    Gym 空间，以及如何处理图像空间。
- en: Rendering an Environment
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渲染环境
- en: In the [*Getting Started with Gym – CartPol*](B16182_01_Final_SZ_ePub.xhtml#_idTextAnchor071)*e*
    section, we saw a sample from the CartPole state space. However, visualizing or
    understanding the CartPole state from a vector representation is not an easy task,
    at least for a human. Gym also allows you to visualize a given task (if possible)
    through the `env.render()` function.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*Gym入门——CartPol*](B16182_01_Final_SZ_ePub.xhtml#_idTextAnchor071)*e*部分，我们看到了从CartPole状态空间中的一个样本。然而，从向量表示中可视化或理解CartPole状态并不是一件容易的事，至少对于人类来说是这样的。Gym还允许你通过`env.render()`函数可视化给定的任务（如果可能的话）。
- en: Note
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `env.render()` function is usually slow. Rendering an environment is done
    primarily to understand the behavior learned by the agent after the training or
    on intervals of many training steps. Usually, we train agents without rendering
    the environment state to improve the training speed.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.render()`函数通常较慢。渲染环境主要是为了理解智能体在训练后学习到的行为，或在多个训练步骤的间隔中进行观察。通常，我们在训练时不会渲染环境状态，以提高训练速度。'
- en: If we just call the `env.render()` function, we will always see the same scene,
    that is, the environment state does not change. To see the evolution of the environment
    in time, we must call the `env.step()` function, which takes as input an action
    belonging to the action space and applies the action in the environment.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只是调用`env.render()`函数，我们将始终看到相同的场景，也就是说，环境状态没有变化。为了查看环境随时间的演变，我们必须调用`env.step()`函数，它接受一个属于动作空间的动作作为输入，并在环境中应用该动作。
- en: Rendering CartPole
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渲染CartPole
- en: 'The following code demonstrates how to render the CartPole environment. The
    action is a sample from the action space. For RL algorithms, the action will be
    smartly selected from the policy:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何渲染CartPole环境。动作是从动作空间中采样的。对于强化学习算法，动作将根据策略智能地选择：
- en: '[PRE27]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you run this script, you will see that `gym` opens a window and displays
    the CartPole environment with random actions, as shown in the following figure:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个脚本，你会看到`gym`打开一个窗口，显示CartPole环境，并且执行随机动作，如下图所示：
- en: '![Figure 1.35: A CartPole environment rendered in Gym (the initial state)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.35：在Gym中渲染的CartPole环境（初始状态）]'
- en: '](img/B16182_01_35.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_35.jpg)'
- en: 'Figure 1.35: A CartPole environment rendered in Gym (the initial state)'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.35：在Gym中渲染的CartPole环境（初始状态）
- en: A Reinforcement Learning Loop with Gym
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Gym的强化学习循环
- en: 'To understand the consequences of an action, and to come up with a better policy,
    the agent observes its new state and a reward. Implementing this loop with `gym`
    is easy. The key element is the `env.step()` function. This function takes an
    action as input. It applies the action and returns four values, which are described
    as follows:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解一个动作的后果，并提出更好的策略，智能体观察其新的状态和奖励。使用`gym`实现这个循环非常简单。关键元素是`env.step()`函数。这个函数接受一个动作作为输入。它应用该动作并返回四个值，具体描述如下：
- en: '**Observation**: The observation is the next environmental state. This is represented
    as an element belonging to the observation space of the environment.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察**：观察是下一个环境状态。这是环境的观察空间中的一个元素。'
- en: '**Reward**: The reward associated with a step is a float value that is related
    to the action given as input to the function.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：与步骤相关的奖励是一个浮动值，它与输入到函数中的动作相关。'
- en: '`True` value when the episode is finished, and it''s time to call the `env.reset()`
    function to reset the environment state.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回合结束时返回`True`值，这时需要调用`env.reset()`函数来重置环境状态。
- en: '**Info**: This is a dictionary containing debugging information; usually, it
    is ignored.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息**：这是一个包含调试信息的字典；通常它会被忽略。'
- en: Let's now implement the RL loop within the Gym environment.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Gym环境中实现强化学习循环。
- en: 'Exercise 1.04: Implementing the Reinforcement Learning Loop with Gym'
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.04：使用Gym实现强化学习循环
- en: 'In this exercise, we will implement a basic RL loop with episodes and timesteps
    using the CartPole environment. You can change the environment and use other environments
    as well; nothing changes as the main goal of Gym is to unify the interfaces of
    all possible environments in order to build agents that are as environment-agnostic
    as possible. The transparency with respect to the environment is a very peculiar
    thing in RL: the algorithms are not usually suited to the task but are task-agnostic
    so that they can be applied successfully to a variety of environments and still
    solve them.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项练习中，我们将使用CartPole环境实现一个基本的强化学习循环，包含回合和时间步。你也可以更改环境，使用其他环境；没有任何变化，因为Gym的主要目标是统一所有可能环境的接口，以便构建尽可能与环境无关的智能体。对于环境的透明度是强化学习中的一个独特之处：算法通常不是专门为特定任务设计的，而是任务无关的，这样它们可以成功应用于多种环境并解决问题。
- en: 'We need to create the Gym CartPole environment as before using the `gym.make()`
    function. After that, we can loop for a defined number of episodes; for each episode,
    we loop for a defined number of steps or until the episode is terminated (by checking
    the `done` value). For each timestep, we have to call the `env.step()` function
    by passing an action (we will pass a random action for now), and then we collect
    the desired information:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要像之前一样使用`gym.make()`函数创建Gym的CartPole环境。之后，我们可以进行一个定义好的回合数循环；对于每个回合，我们进行一个定义好的步数循环，或者直到回合结束（通过检查`done`值）。对于每个时间步，我们必须调用`env.step()`函数并传入一个动作（目前我们会传入一个随机动作），然后我们收集所需的信息：
- en: 'Open a new Jupyter notebook and define the import, the environment, and the
    desired number of steps:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter笔记本，定义导入项、环境和所需的步数：
- en: '[PRE28]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Loop for each episode:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个回合进行循环：
- en: '[PRE29]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reset the environment and get the first observation:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并获取第一次观测：
- en: '[PRE30]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Loop for each timestep:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个时间步进行循环：
- en: '[PRE31]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Render the environment, select the action (randomly by using the `env.action_space.sample()`
    method), and then take the action:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境，选择动作（通过使用`env.action_space.sample()`方法随机选择），然后执行该动作：
- en: '[PRE32]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Check whether the episode has been terminated using the `done` variable:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`done`变量检查回合是否结束：
- en: '[PRE33]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After the episode loop, close the environment in order to release the associated
    memory:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在回合循环结束后，关闭环境以释放相关内存：
- en: '[PRE34]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you run the previous code, the output should, approximately, be like this:'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你运行之前的代码，输出应该大致如下：
- en: '[PRE35]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have the episode number and the number of steps taken in that episode. We
    can see that the average number of timesteps for an episode is approximately 17\.
    This means that, using the random policy, after 17 episodes on average, the pole
    falls and the episode finishes.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有回合号和该回合中采取的步数。我们可以看到，平均每个回合的时间步数大约是17。这意味着，使用随机策略后，平均经过17回合，杆子会倒下，回合结束。
- en: Note
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2MOs5t5](https://packt.live/2MOs5t5).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这个特定部分的源代码，请参考[https://packt.live/2MOs5t5](https://packt.live/2MOs5t5)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需要在本地运行。
- en: The goal of this exercise was to understand the bare bones of each RL algorithm.
    The only different thing here is that the action selection phase should take into
    account the environment state in order for it to be useful, and it should not
    be random.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这项练习的目标是理解每种强化学习算法的基本框架。唯一不同的地方是，动作选择阶段应考虑环境状态，这样才有用，而不是随机的。
- en: Let's now move toward completing an activity to measure the performance of an
    agent.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续完成一个活动，来测量智能体的表现。
- en: 'Activity 1.01: Measuring the Performance of a Random Agent'
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.01：测量随机智能体的表现
- en: The measurement of the performance and the design of an agent is an essential
    phase of every RL experiment. The goal of this activity is to practice with these
    two concepts by designing an agent that is able to interact with an environment
    using a random policy and then measure the performance.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 测量性能和设计智能体是每个强化学习实验中的重要阶段。此活动的目标是通过设计一个能够使用随机策略与环境交互的智能体，然后测量其表现，来练习这两个概念。
- en: 'You need to design a random agent using a Python class to modularize and keep
    the agent independent from the main loop. After that, you have to measure the
    mean and the variance of the discounted return using a batch of 100 episodes.
    You can use every environment you want, taking into account that the agent''s
    action should be compatible with the environment. You can design two different
    types of agents for discrete action spaces and continuous action spaces. The following
    steps will help you to complete the activity:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要设计一个随机代理，使用 Python 类进行模块化，并保持代理与主循环的独立性。之后，你需要通过一批 100 个回合来测量折扣回报的均值和方差。你可以使用任何环境，但要确保代理的动作与环境兼容。你可以为离散动作空间和连续动作空间设计两种不同类型的代理。以下步骤将帮助你完成此任务：
- en: 'Import the required libraries: `abc`, `numpy`, and `gym`.'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：`abc`、`numpy` 和 `gym`。
- en: Define the `Agent` abstract class in a very simple way, defining only the `pi()`
    function that represents the policy. The input should be an environment state.
    The `__init__` method should take as input the action space and build the distribution
    accordingly.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以非常简单的方式定义`Agent`抽象类，仅定义表示策略的`pi()`函数。输入应为环境状态。`__init__`方法应该以动作空间为输入，并根据此构建分布。
- en: Define a `ContinuousAgent` deriving from the `Agent` abstract class. The agent
    should check that the action space is coherent with it, and it should be a continuous
    action space. The agent should also initialize a probability distribution for
    sampling actions (you can use NumPy to define probability distributions). The
    continuous agent can change the distribution type according to the distributions
    defined by the Gym spaces.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个从`Agent`抽象类派生的`ContinuousAgent`。该代理应该检查动作空间是否与其一致，并且必须是连续动作空间。代理还应该初始化一个概率分布来进行动作采样（你可以使用
    NumPy 来定义概率分布）。连续代理可以根据 Gym 空间定义的分布类型改变分布类型。
- en: Define a `DiscreteAgent` deriving from the `Agent` abstract class. The discrete
    agent should, of course, initialize a uniform distribution.
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个从`Agent`抽象类派生的`DiscreteAgent`。离散代理应该初始化一个均匀分布。
- en: Implement the `pi()` function for both agents. This function is straightforward
    and should only sample from the distribution defined in the constructor and return
    it, ignoring the environment state. Of course, this is a simplification. You can
    also implement the `pi()` function in the `Agent` base class.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个代理实现`pi()`函数。这个函数非常简单，只需要从构造函数中定义的分布中采样并返回，忽略环境状态。当然，这是一个简化版本。你也可以在`Agent`基类中实现`pi()`函数。
- en: Define the main RL loop in another file by importing the agent.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个文件中定义主 RL 循环，并导入代理。
- en: Instantiate the correct agent according to the selected environment. Examples
    of environments are "CartPole-v1" or "MountainCar-Continuous-v0."
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据选定的环境实例化正确的代理。环境的示例包括“CartPole-v1”或“MountainCar-Continuous-v0”。
- en: Take actions according to the `pi` function of the agent.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据代理的`pi`函数采取行动。
- en: Measure the performance of the agent collecting (in a list or a NumPy array)
    the discounted return for each episode. Then, take the average and the standard
    deviation (you can use NumPy for this). Remember to apply the discount factor
    (user-defined) to the immediate reward. You have to keep a cumulated discount
    factor by multiplying the discount factor at each timestep.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过收集每个回合的折扣回报（可以存储在列表或 NumPy 数组中）来衡量代理的表现。然后，计算平均值和标准差（你可以使用 NumPy 来实现）。记得对即时奖励应用折扣因子（用户定义）。你需要通过在每个时间步乘以折扣因子来保持累积折扣因子。
- en: 'The output should be similar to the following:'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应类似于以下内容：
- en: '[PRE36]'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 680\.
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此任务的解决方案可以在第 680 页找到。
- en: OpenAI Baselines
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Baselines
- en: OpenAI Baselines ([https://github.com/openai/baselines](https://github.com/openai/baselines))
    is a set of state-of-the-art RL algorithms. The main goal of Baselines is to make
    it easier to reproduce results on a set of benchmarks, to evaluate new ideas,
    and to compare them to existing algorithms. In this section, we will learn how
    to use Baselines to run an existing algorithm on an environment taken from Gym
    (refer to the previous section) and how to visualize the behavior learned by the
    agent. As for Gym, we will not cover the installation instructions; these can
    be found in the *Preface* section. The implementation of the Baselines' algorithm
    is based on TensorFlow, one of the most popular libraries for machine learning.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Baselines（[https://github.com/openai/baselines](https://github.com/openai/baselines)）是一组最先进的强化学习算法。Baselines
    的主要目标是更轻松地重现一组基准结果，评估新想法，并将其与现有算法进行比较。在本节中，我们将学习如何使用 Baselines 在从 Gym 获取的环境上运行现有算法（参考前一节），以及如何可视化代理学到的行为。至于
    Gym，我们将不涉及安装说明；这些可以在*前言*部分找到。Baselines 算法的实现基于 TensorFlow，这是机器学习中最流行的库之一。
- en: Getting Started with Baselines – DQN on CartPole
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 入门 Baselines – 在 CartPole 上使用 DQN
- en: Training a **Deep Q Network (DQN)** on CartPole is straightforward with Baselines;
    we can do it with just one line of Bash.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Baselines 在 CartPole 上训练 **Deep Q Network (DQN)** 非常简单；我们只需用一行 Bash 命令就可以完成。
- en: 'Just use the terminal and run this command:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 只需使用终端并运行以下命令：
- en: '[PRE37]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s understand the parameters, as follows:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解以下参数：
- en: '`--alg=deepq` specifies the algorithm to be used to train our agent. In our
    case, we selected `deepq`, that is, DQN.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--alg=deepq` 指定要用于训练代理的算法。在我们的例子中，我们选择了 `deepq`，即 DQN。'
- en: '`--env=CartPole-v0` specifies the environment to be used. We selected CartPole,
    but we can also select many other environments.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--env=CartPole-v0` 指定要使用的环境。我们选择了 CartPole，但我们也可以选择许多其他环境。'
- en: '`--save_path=./cartpole_model.pkl` specifies where to save the trained agent.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--save_path=./cartpole_model.pkl` 指定了保存训练好的代理的位置。'
- en: '`--num_timesteps=1e5` is the number of training timesteps.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num_timesteps=1e5` 是训练步骤的数量。'
- en: 'After having trained the agent, it is also possible to visualize the learned
    behavior using the following:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完代理之后，还可以使用以下方法可视化学到的行为：
- en: '[PRE38]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: DQN is a very powerful algorithm; using it for a simple task such as CartPole
    is almost overkill. We can see that the agent has learned a stable policy, and
    the pole almost never falls. We will explore DQN in more detail in the following
    chapters.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 是一个非常强大的算法；在像 CartPole 这样简单的任务上使用它几乎有些杀鸡用牛刀的感觉。我们可以看到代理学习了一个稳定的策略，杆子几乎不会倒下。我们将在接下来的章节中更详细地探讨
    DQN。
- en: 'In the following steps, we will train a DQN agent on the CartPole environment
    using Baselines:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用 Baselines 在 CartPole 环境上训练一个 DQN 代理：
- en: 'First, we import `gym` and `baselines`:'
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入 `gym` 和 `baselines`：
- en: '[PRE39]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define a callback to inform `baselines` when to stop training. The callback
    should return `True` if the reward is satisfying:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个回调函数来告知 `baselines` 何时停止训练。如果奖励令人满意，则回调函数应返回 `True`：
- en: '[PRE40]'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s create the environment and prepare the algorithm''s parameters:'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建环境并准备算法的参数：
- en: '[PRE41]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can use the `deep.learn()` method to start the training and solve the task:'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `deep.learn()` 方法来开始训练并解决任务：
- en: '[PRE42]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: After some time, depending on your hardware (it usually takes a few minutes),
    the learning phase terminates, and you will have the CartPole agent saved to your
    current working directory.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，根据您的硬件（通常需要几分钟），学习阶段终止，并且您将在当前工作目录中保存 CartPole 代理。
- en: We should see the `baselines` logs reporting the agent's performance over time.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到 `baselines` 日志报告代理随时间的表现。
- en: 'Consider the following example:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE43]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following are the observations from the preceding logs:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从先前日志中的观察结果：
- en: The `episodes` parameter reports the episode number we are referring to.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`episodes` 参数报告了我们所指的回合数。'
- en: '`mean 100 episode reward` is the average return obtained in the last 100 episodes.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean 100 episode reward` 是最近100个回合的平均回报。'
- en: '`steps` is the number of training steps the algorithm has performed.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` 是算法执行的训练步数。'
- en: 'Now we can save our actor so that we can reuse it without retraining it:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以保存我们的 actor，这样我们就可以在不重新训练的情况下重复使用它：
- en: '[PRE44]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: After the `actor.save` function, the `"cartpole_model.pkl"` file contains the
    trained model.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '`actor.save` 函数之后，`"cartpole_model.pkl"` 文件包含了训练好的模型。'
- en: Now it is possible to use the model and visualize the agent's behavior.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用模型并可视化代理的行为。
- en: 'The actor returned by `deepq.learn` is actually a callable that returns the
    action given the current observation – it is the agent policy. We can use it by
    passing the current observation, and it returns the selected action:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '`deepq.learn` 返回的演员实际上是一个可调用对象，给定当前的观察，它返回一个动作 —— 这是代理的策略。我们可以通过传入当前观察来使用它，它会返回选择的动作：'
- en: '[PRE45]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: If you run the preceding code, you should see the agent's performance on the
    CartPole task.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码，你应该看到代理在 CartPole 任务上的表现。
- en: 'You should get, as output, the return for each episode; it should be something
    similar to the following:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到每一轮的回报输出；它应该类似于以下内容：
- en: '[PRE46]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This means our agent always reaches the maximum possible return for CartPole
    (`200.0`) and the maximum possible number of steps (`199`).
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的代理总是能够达到 CartPole 的最大回报（`200.0`）和最大步数（`199`）。
- en: We can compare the return obtained using a trained DQN agent with respect to
    the return obtained using a random agent (*Activity 1.01, Measuring the Performance
    of a Random Agent*). The random agent yields an average return of `20.0`, while
    DQN obtains the maximum return possible for CartPole, which is `200.0`.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以比较使用训练后的 DQN 代理获得的回报与使用随机代理获得的回报（*活动 1.01，衡量随机代理的表现*）。随机代理的平均回报是 `20.0`，而
    DQN 取得了 CartPole 的最大回报 `200.0`。
- en: In this section, we presented OpenAI Gym and OpenAI Baselines, the two main
    frameworks for RL research and experiments. There are many other frameworks for
    RL, each with their pros and cons. Gym is particularly suited due to its unified
    interface in the RL loop, while OpenAI Baselines is very useful for understanding
    how to implement sophisticated state-of-the-art RL algorithms and how to compare
    new algorithms with existing ones.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了 OpenAI Gym 和 OpenAI Baselines，这两个是强化学习（RL）研究和实验的主要框架。还有许多其他的 RL 框架，各有优缺点。Gym
    特别适合使用，因为它在 RL 循环中提供了统一的接口，而 OpenAI Baselines 对于理解如何实现先进的 RL 算法以及如何将新算法与现有算法进行比较非常有用。
- en: In the following section, we will explore some interesting RL applications in
    order to better understand the possibilities offered by the framework as well
    as its flexibility.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探索一些有趣的 RL 应用，以更好地理解该框架所提供的可能性以及其灵活性。
- en: Applications of Reinforcement Learning
  id: totrans-554
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的应用
- en: RL has exciting and useful applications in many different contexts. Recently,
    the usage of deep neural networks has augmented the number of possible applications
    considerably.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在许多不同的场景中具有令人兴奋且有用的应用。最近，深度神经网络的使用大大增加了可能应用的数量。
- en: When used in a deep learning context, RL can also be referred to as deep RL.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 当在深度学习背景下使用时，RL 也可以称为深度 RL。
- en: The applications vary from games and video games to real-world applications,
    such as robotics and autonomous driving. In each of these applications, RL is
    a game-changer, allowing you to solve tasks that are considered to be almost impossible
    (or, at least, very difficult) without these techniques.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用从游戏和视频游戏到现实世界的应用，如机器人技术和自动驾驶。在这些应用中，RL 是一项革命性的技术，使得没有这些技术的情况下几乎不可能（或至少非常困难）解决的任务变得可行。
- en: In this section, we will present some RL applications, describe the challenges
    of each application, and begin to understand why RL is preferred among other methods,
    along with its advantages and its drawbacks.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些 RL 应用，描述每个应用的挑战，并开始理解为什么 RL 在其他方法中被首选，以及它的优缺点。
- en: Games
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏
- en: Nowadays, RL is widely used in video games and board games.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，RL 在视频游戏和棋盘游戏中被广泛使用。
- en: 'Games are used to benchmark RL algorithms because, usually, they are very complex
    to solve yet easy to implement and to evaluate. Games also represent a simulated
    reality in which the agent can freely move and behave without affecting the real
    environment:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏被用来对 RL 算法进行基准测试，因为通常它们非常复杂，难以解决，但又容易实现和评估。游戏也代表了一个模拟的现实，代理可以在其中自由移动和行为，而不会影响真实环境：
- en: '![Figure 1.36:  Breakout – one of the most famous Atari games'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.36：打砖块 – 最著名的雅达利游戏之一](img/B16182_01_36.jpg)'
- en: '](img/B16182_01_36.jpg)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: 'Figure 1.36: Breakout – one of the most famous Atari games'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.36：打砖块 – 最著名的雅达利游戏之一
- en: Note
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding screenshot has been sourced from the official documentation of
    OpenAI Gym. Please refer to the following link for more examples: [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图来源于 OpenAI Gym 的官方文档。有关更多示例，请参阅以下链接：[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)。
- en: Despite appearing to be secondary or relatively limited-use applications, games
    represent a useful benchmark for RL and, in general, artificial intelligence algorithms.
    Very often, artificial intelligence algorithms are tested on games due to the
    significant challenges that arise in these scenarios.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管游戏看起来似乎是次要的或用途有限的应用，但它们为强化学习和一般的人工智能算法提供了一个有用的基准。由于这些场景中出现的重大挑战，人工智能算法常常在游戏中进行测试。
- en: The two main characteristics required to play games are **planning** and **real-time
    control**.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 玩游戏所需的两个主要特征是**规划**和**实时控制**。
- en: An algorithm that is not able to plan won't be able to win strategic games.
    Having a long-term plan is also fundamental in the early stages of a game. Planning
    is also fundamental in real-world applications in which taken actions may have
    long-term consequences.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 一个无法进行规划的算法将无法在战略性游戏中获胜。在游戏的早期阶段，拥有一个长期计划也是至关重要的。规划在现实世界应用中也非常关键，因为采取的行动可能会带来长期的后果。
- en: Real-time control is another fundamental challenge that requires an algorithm
    to be able to respond within a small timeframe. This challenge is similar to one
    an algorithm has to face when applied to real-world cases such as autonomous driving,
    robot control, and many others. In these cases, the algorithm can't evaluate all
    the possible actions or all the possible consequences of these actions; therefore,
    the algorithm should learn an efficient (and maybe compressed) state representation
    and should understand the consequences of its actions without simulating all of
    the possible scenarios.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 实时控制是另一个基本挑战，它要求算法能够在短时间内做出反应。这个挑战类似于算法在应用于现实世界的案例时所面临的挑战，例如自动驾驶、机器人控制等。在这些情况下，算法不能评估所有可能的行动或这些行动的所有可能后果；因此，算法应该学习一个高效的（可能是压缩的）状态表示，并且应理解其行动的后果，而无需模拟所有可能的情景。
- en: Recently, RL has been able to exceed human performance in games such as Go,
    and in video games such as Dota II and StarCraft, thanks to work done by DeepMind
    and OpenAI.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，得益于DeepMind和OpenAI的工作，强化学习在围棋、Dota II和星际争霸等电子游戏中已经超越了人类表现。
- en: Go
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 围棋
- en: Go is a very complex, highly strategic board game. In Go, two players are competing
    against each other. The aim is to use the game pieces, also called stones, to
    surround more territory than the opponent. At each turn, the player can place
    its stone in a vacant intersection on the board. At the end of the game, when
    no player can place a stone, the player surrounding more territories wins.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋是一款非常复杂、高度战略性的棋盘游戏。在围棋中，两名玩家相互竞争。目标是使用棋子（也叫做“石子”）围住比对方更多的领土。在每一回合，玩家可以在棋盘上的一个空交叉点放置自己的石子。游戏结束时，当没有玩家可以再放置石子时，围住更多领土的玩家获胜。
- en: Go has been studied for many years to understand the strategies and moves necessary
    to lead a player to victory. Until recently, no algorithm succeeded in producing
    strong players – even algorithms working very well for similar games, such as
    chess. This difficulty is due to Go's huge search space, the variety of possible
    moves, and the average length (in terms of moves) of Go games, which, for example,
    is longer than the average length of chess games. RL, and in particular **AlphaGo**
    by DeepMind, succeeded recently in beating a human player on a standard dimension
    board. AlphaGo is actually a mix of RL, supervised learning, and tree search algorithms
    trained on an extensive set of games from both human and artificial players. AlphaGo
    denoted a real milestone in artificial intelligence history, which was made possible
    mainly due to the advances in RL algorithms and their improved efficiency.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋已经被研究了很多年，以理解带领玩家走向胜利所需的策略和棋步。直到最近，没有算法能够成功地培养出强大的玩家——即使是那些在类似游戏（如国际象棋）中表现非常好的算法。这一困难源于围棋庞大的搜索空间、可行走棋的多样性以及围棋比赛的平均长度（按走棋步数计算），例如，围棋比赛的平均时长要长于国际象棋比赛的平均时长。强化学习，特别是由DeepMind开发的**AlphaGo**，最近成功击败了标准棋盘上的人类玩家。AlphaGo实际上是强化学习、监督学习和树搜索算法的结合，经过大量人类和人工玩家对弈的数据训练。AlphaGo标志着人工智能历史上的一个真正里程碑，这主要得益于强化学习算法的进展及其提高的效率。
- en: 'The successor of **AlphaGo** is **AlphaGo Zero**. AlphaGo Zero has been trained
    fully in a self-play fashion, learning from itself completely with no human intervention
    (Zero comes from this characteristic). It is currently the world''s top player
    at Go and Chess:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '**AlphaGo**的继任者是**AlphaGo Zero**。AlphaGo Zero完全通过自我对弈的方式进行训练，完全从自身学习，没有任何人类干预（Zero源自这一特性）。它目前是围棋和国际象棋领域的世界顶级选手：'
- en: '![Figure 1.37: The Go board'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.37：围棋棋盘'
- en: '](img/B16182_01_37.jpg)'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_37.jpg)'
- en: 'Figure 1.37: The Go board'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.37：围棋棋盘
- en: Both AlphaGo and AlphaGo Zero used a deep **Convolutional Neural Network (CNN)**
    to learn a suitable game representation starting from the "raw" board. This peculiarity
    shows that a deep CNN can also extract features starting from a sparse representation
    such as the Go board. One of the main strengths of RL is that it can use, in a
    transparent way, machine learning models that are widely studied in other fields
    or problems.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo 和 AlphaGo Zero 都使用了深度 **卷积神经网络（CNN）** 从“原始”棋盘开始学习合适的游戏表示。这一特点表明，深度 CNN
    也能够从稀疏表示（如围棋棋盘）中提取特征。强化学习的主要优势之一在于，它能够以透明的方式使用在其他领域或问题中广泛研究的机器学习模型。
- en: 'Deep convolutional networks are usually used for classification or segmentation
    problems that, at first glance, might seem very different from RL problems. Actually,
    the way CNNs are used in RL is very similar to a classification or a regression
    problem. The CNN of AlphaGo Zero, for example, takes the raw board representation
    and outputs the probabilities for each possible action together with the value
    of each action. It can be seen as a classification and regression problem at the
    same time. The difference is that the labels, or actions in the case of RL, are
    not given in the training set, rather it is the algorithm itself that has to discover
    the real labels through interaction. AlphaGo, the predecessor of AlphaGo Zero,
    used two different networks: one for action probabilities and another for value
    estimates. This technique is called actor-critic. The network tasked with predicting
    actions is called the actor, and the network that has to evaluate actions is called
    the critic.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积网络通常用于分类或分割问题，这些问题乍一看似乎与强化学习问题非常不同。实际上，CNN 在强化学习中的使用方式与分类或回归问题非常相似。例如，AlphaGo
    Zero 的 CNN 接受原始棋盘表示，并输出每个可能行动的概率以及每个行动的价值。它可以同时看作是一个分类和回归问题。不同之处在于，RL 中的标签或行动并没有在训练集中给出，而是算法本身需要通过交互发现真实标签。AlphaGo（AlphaGo
    Zero 的前身）使用了两种不同的网络：一个用于行动概率，另一个用于价值估计。这项技术被称为演员-评论家（actor-critic）。负责预测行动的网络称为演员（actor），负责评估行动的网络称为评论家（critic）。
- en: Dota 2
  id: totrans-581
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dota 2
- en: 'Dota 2 is a complex, real-time strategy game in which there are two teams of
    five players competing, with each player controlling a "hero." The characteristics
    of Dota, from an RL perspective, are as follows:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: Dota 2 是一款复杂的实时战略游戏，其中有两个五人队伍对抗，每个玩家控制一个“英雄”。从强化学习（RL）的角度来看，Dota 的特点如下：
- en: '**Long-Time Horizon**: A Dota game can have around 20,000 moves and can last
    for 45 minutes. As a reference, a chess game ends before 40 moves and a Go game
    ends before 150 moves.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长时间跨度**：一场 Dota 游戏大约有 20,000 步棋，并且可以持续 45 分钟。作为参考，一场国际象棋比赛在 40 步之前结束，而一场围棋比赛在
    150 步之前结束。'
- en: '**Partially Observed State**: In Dota, agents can only see a small portion
    of the full map, that is, only the portion around them. A strong player should
    make predictions about the position of the enemies and their actions. As a reference,
    Go and Chess are fully observable games where agents can see the whole situation
    and the actions taken by the opponents.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分可观察状态**：在 Dota 中，智能体只能看到完整地图的一小部分，也就是周围的区域。一位强大的玩家应该能够预测敌人的位置及其行动。作为参考，围棋和国际象棋是完全可观察的游戏，智能体可以看到整个局势和对手的行动。'
- en: '**High-Dimensional and Continuous Action Space**: Dota has a vast number of
    actions available to each player at each step. The possible actions have been
    discretized by researchers in around 170,000 actions, with an average of 1,000
    possible actions for each step. In comparison, the number of average actions in
    chess is 35, and in Go, it is 250\. With a huge action space, learning becomes
    very difficult.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维连续行动空间**：Dota 中每个玩家在每一步都可以选择大量的行动。研究人员已经将可能的行动离散化，约有 170,000 种行动，每一步平均有
    1,000 种可能的行动。相比之下，国际象棋的平均行动数为 35，围棋为 250。如此庞大的行动空间使得学习变得非常困难。'
- en: '**High-Dimensional and Continuous Observation Space**: While Chess and Go have
    a discretized observation space, Dota has a continuous state space with around
    20,000 dimensions. The state space, as we will learn later in the book, includes
    all of the information available to players that must be taken into consideration
    when selecting an action. In a video game, the state space is represented by the
    characteristics and position of the enemies, the state of the current player,
    including its ability, its equipment, and its health status, and other domain-specific
    features.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维连续观察空间**：虽然国际象棋和围棋有离散化的观察空间，但 Dota 拥有大约 20,000 维的连续状态空间。正如我们稍后在本书中将要学习的那样，状态空间包括所有玩家在选择行动时必须考虑的信息。在视频游戏中，状态空间由敌人的特征和位置、当前玩家的状态（包括其能力、装备和健康状况）以及其他领域特定的特征组成。'
- en: OpenAI Five, the RL algorithm able to exceed human performance at Dota, is composed
    of five neural networks collaborating together. The algorithm learns to play by
    itself through self-play, playing an equivalent of 180 years per day. The algorithm
    used for training the five neural networks is called **Proximal Policy Optimization**,
    representing the current state of the art of RL algorithms.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Five 是一个强化学习算法，能够在《Dota》游戏中超越人类表现，它由五个神经网络协同工作组成。该算法通过自我对弈进行学习，每天进行相当于
    180 年的对弈。用于训练这五个神经网络的算法叫做**近端策略优化（Proximal Policy Optimization）**，它代表了当前强化学习算法的最新技术。
- en: Note
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'To read more on OpenAI Five, refer to the following link: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于 OpenAI Five 的信息，请参考以下链接：[https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)
- en: StarCraft
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 星际争霸
- en: StarCraft has characteristics that make it very similar to Dota, including a
    huge number of moves per play, imperfect information available to players, and
    highly dimensional state and action spaces. **AlphaStar**, the player developed
    by DeepMind, is the first artificial intelligence agent able to reach the top
    league without any game restrictions. AlphaStar uses machine learning techniques
    such as neural networks, self-play through RL, multi-agent learning methods, and
    imitation learning to learn from other human players in a supervised way.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 星际争霸具有许多与 Dota 相似的特点，包括每场游戏中大量的操作、玩家可获取的信息不完全，以及高度维度的状态和动作空间。**AlphaStar**，由
    DeepMind 开发的玩家，是第一个能够在没有任何游戏限制的情况下进入顶级联赛的人工智能代理。AlphaStar 使用了机器学习技术，如神经网络、通过强化学习进行自我对弈、多智能体学习方法和模仿学习，从其他人类玩家那里以监督的方式进行学习。
- en: Note
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For further reading on AlphaStar, refer to the following paper: [https://arxiv.org/pdf/1902.01724.pdf](https://arxiv.org/pdf/1902.01724.pdf)'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 想要进一步了解 AlphaStar，请参考以下论文：[https://arxiv.org/pdf/1902.01724.pdf](https://arxiv.org/pdf/1902.01724.pdf)
- en: Robot Control
  id: totrans-594
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器人控制
- en: 'Robots are starting to become ubiquitous nowadays and are widely used in various
    industries because of their ability to perform repetitive tasks in a precise and
    efficient way. RL can be beneficial for robotics applications, by simplifying
    the development of complex behaviors. At the same time, robotics applications
    represent a set of benchmark and real-world validations for RL algorithms. Researchers
    test their algorithm on robotic tasks such as locomotion (for example, learning
    to move) or grasping (for example, learning how to grasp an object). Robotics
    offers unique challenges, such as the **curse of dimensionality**, the **effective
    usage of samples** (also called sample efficiency), the possibility of **transferring
    knowledge** from similar or simulated tasks, and the **need for safety**:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人正在逐渐变得无处不在，并广泛应用于各种行业，因为它们能够以精确和高效的方式执行重复任务。强化学习（RL）对于机器人应用非常有帮助，因为它可以简化复杂行为的开发。同时，机器人应用也为强化学习算法提供了一系列基准和现实世界的验证。研究人员将算法应用于机器人任务，如运动（例如，学习如何移动）或抓取（例如，学习如何抓取物体）。机器人技术提出了独特的挑战，比如**维度灾难**、**样本的有效使用**（也称为样本效率）、从相似或模拟任务中**转移知识**的可能性，以及**安全性需求**：
- en: '![Figure 1.38: A robotic task from the Gym robotics suite'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.38：来自 Gym 机器人套件的机器人任务](img/B16182_01_38.jpg)'
- en: '](img/B16182_01_38.jpg)'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_01_38.jpg)'
- en: 'Figure 1.38: A robotic task from the Gym robotics suite'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.38：来自 Gym 机器人套件的机器人任务
- en: Note
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding diagram has been sourced from the official documentation for
    OpenAI Gym: [https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics)'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表来自 OpenAI Gym 的官方文档：[https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics)
- en: Please refer to the link for more examples of robot control.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考链接了解更多机器人物控制的例子。
- en: The **curse of dimensionality** is a challenge that can also be found in supervised
    learning applications. Still, in these cases, it is softened by restricting the
    space of possible solutions to a limited class of functions or by injecting prior
    knowledge elements in the models through architectural decisions. Robots usually
    have many degrees of freedom, making the space of possible states and possible
    actions very large.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度灾难**是一个在监督学习应用中也能找到的挑战。然而，在这些情况下，通过限制可能解决方案的空间至有限的函数类别，或者通过在模型中注入先验知识元素，通常可以缓解这个问题。机器人通常有许多自由度，这使得可能的状态空间和动作空间非常庞大。'
- en: Robots interact with the physical environment by definition. The interaction
    of a real robot with an environment is usually time-consuming, and it can be dangerous.
    Usually, RL algorithms require millions of samples (or episodes) in order to become
    efficient. Sample efficiency is a problem in this field, as the required time
    may be impractical. The usage of collected samples in a smart way is the key to
    successful RL-based robotics applications. A technique that can be used in these
    cases is the so-called **sim2real**, in which an initial learning phase is practiced
    in a simulated environment that is usually safer and faster than the real environment.
    After this phase, the learned behavior is transferred to the real robot in the
    real environment. This technique requires a simulated environment that is very
    similar to the real environment or the generalization capabilities of the algorithm.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人本质上是通过与物理环境互动来进行工作的。真实机器人与环境的互动通常是耗时且可能危险的。通常，强化学习（RL）算法需要数百万个样本（或回合）才能变得高效。样本效率在这一领域是一个问题，因为所需的时间可能不切实际。以智能的方式使用收集到的样本是成功的基于RL的机器人应用的关键。可以在这些情况下使用的技术是所谓的**sim2real**，即初始学习阶段在模拟环境中进行，模拟环境通常比真实环境更安全、更快速。经过这一阶段，学习到的行为会被转移到真实环境中的真实机器人上。此技术需要一个与真实环境非常相似的模拟环境，或者要求算法具备较强的泛化能力。
- en: Autonomous Driving
  id: totrans-604
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动驾驶
- en: Autonomous driving is another exciting application of RL. The main challenge
    this task presents is the lack of precise specifications. In autonomous driving,
    it is challenging to formalize what it means to drive well, whether steering in
    a given situation is good or bad, or whether the driver should accelerate or break.
    As with robotic applications, autonomous driving can also be hazardous. Testing
    an RL algorithm, or, in general, a machine learning algorithm, on a driving task,
    is very problematic and raises many concerns.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶是RL的另一个令人兴奋的应用。这项任务的主要挑战在于缺乏精确的规格。在自动驾驶中，如何定义“驾驶得好”是一个难题，是否在某种情境下转向是好还是坏，驾驶员是否应该加速或刹车也难以界定。与机器人应用类似，自动驾驶也可能是危险的。在驾驶任务上测试RL算法，或者更广泛地说，机器学习算法，存在许多问题，并引发了诸多顾虑。
- en: Aside from the concerns, the autonomous driving scenario fits very well in the
    RL framework. As we will explore later in the book, we can think of the driver
    as the decision-maker. At each step, they receive an observation. The observation
    includes the road's state, the current velocity, the acceleration, and all of
    the car's characteristics. The driver, based on the current state, should make
    a decision corresponding to what to do with the car's commands, steering, brakes,
    and acceleration. Designing a rule-based system that is able to drive in real
    situations is complicated, due to the infinite number of different situations
    to confront. For this reason, a learning-based system would be far more efficient
    and effective in tasks such as this.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些顾虑，自动驾驶场景非常适合RL框架。正如我们将在本书中后续部分探讨的，我们可以把驾驶员看作是决策者。在每一步中，驾驶员都会接收到一个观测信息。这个观测信息包括道路的状态、当前的速度、加速度以及车辆的所有特征。驾驶员需要根据当前状态作出相应的决策，决定如何控制车辆的命令、转向、刹车和加速。设计一个基于规则的系统来在实际情况下进行驾驶是很复杂的，因为可能会遇到无穷多种不同的情形。因此，基于学习的系统在此类任务中会更高效、更有效。
- en: Note
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are many simulated environments available for developing efficient algorithms
    in the context of autonomous driving, listed as follows:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多模拟环境可供开发高效的自动驾驶算法，列举如下：
- en: '**Voyage Deepdrive:** [https://news.voyage.auto/introducing-voyage-deepdrive-69b3cf0f0be6](https://news.voyage.auto/introducing-voyage-deepdrive-69b3cf0f0be6)'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '**Voyage Deepdrive：** [https://news.voyage.auto/introducing-voyage-deepdrive-69b3cf0f0be6](https://news.voyage.auto/introducing-voyage-deepdrive-69b3cf0f0be6)'
- en: '**AWS DeepRacer:** [https://aws.amazon.com/fr/deepracer/](https://aws.amazon.com/fr/deepracer/)'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS DeepRacer:** [https://aws.amazon.com/fr/deepracer/](https://aws.amazon.com/fr/deepracer/)'
- en: In this section, we analyzed some interesting RL applications, the main challenges
    of them, and the main techniques used by researchers. Games, robotics, and autonomous
    driving are just some examples of real-world RL applications, but there are many
    others. In the remainder of this book, we will deep dive into RL; we will understand
    its components and the techniques presented in this chapter.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了一些有趣的强化学习应用、它们面临的主要挑战以及研究人员使用的主要技术。游戏、机器人技术和自动驾驶只是强化学习在现实世界中的一些应用示例，但还有许多其他应用。在本书的其余部分，我们将深入探讨强化学习；我们将了解其组成部分以及本章介绍的技术。
- en: Summary
  id: totrans-612
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: RL is one of the fundamental paradigms under the umbrella of machine learning.
    The principles of RL are very general and interdisciplinary, and they are not
    bound to a specific application.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习大伞下的基本范式之一。强化学习的原则非常普适和跨学科，并不局限于特定的应用领域。
- en: RL considers the interaction of an agent with an external environment, taking
    inspiration from the human learning process. RL explicitly targets the need to
    explore efficiently and the exploration-exploitation trade-off appearing in almost
    all human problems; this is a peculiarity that distinguishes this discipline from
    others.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习考虑了代理与外部环境的交互，灵感来自人类学习过程。强化学习明确目标是有效探索和几乎所有人类问题中出现的探索-利用权衡；这是区别于其他学科的一个特点。
- en: We started this chapter with a high-level description of RL, showing some interesting
    applications. We then introduced the main concepts of RL, describing what an agent
    is, what an environment is, and how an agent interacts with its environment. Finally,
    we implemented Gym and Baselines by showing how these libraries make RL extremely
    simple.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从高层次描述强化学习开始本章，展示了一些有趣的应用。然后，我们介绍了强化学习的主要概念，描述了代理是什么、环境是什么以及代理如何与其环境交互。最后，我们通过展示这些库如何使强化学习变得极其简单，实现了Gym和Baselines。
- en: In the next chapter, we will learn more about the theory behind RL, starting
    with Markov chains and arriving at MDPs. We will present the two functions at
    the core of almost all RL algorithms, namely the state-value function, which evaluates
    the goodness of states, and the action-value function, which evaluates the quality
    of the state-action pair.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入了解强化学习背后的理论，从马尔可夫链开始，到马尔可夫决策过程。我们将介绍几乎所有强化学习算法核心的两个函数：状态值函数，评估状态的好坏，以及动作值函数，评估状态-动作对的质量。
