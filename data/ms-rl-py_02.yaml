- en: '*Chapter 1*: Introduction to Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*：强化学习简介'
- en: '**Reinforcement Learning** (**RL**) aims to create **Artificial Intelligence**
    (**AI)**) agents that can make decisions in complex and uncertain environments,
    with the goal of maximizing their long-term benefit. These agents learn how to
    do this by interacting with their environments, which mimics the way we as humans
    learn from experience. As such, RL has an incredibly broad and adaptable set of
    applications, with the potential to disrupt and revolutionize global industries.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）旨在创建能够在复杂和不确定的环境中做出决策的**人工智能**（**AI**）代理，其目标是最大化代理的长期利益。这些代理通过与环境互动来学习如何做到这一点，这模仿了我们作为人类通过经验学习的方式。因此，强化学习具有极其广泛和可适应的应用领域，具有颠覆和革命全球行业的潜力。'
- en: This book will give you an advanced-level understanding of this field. We will
    go deeper into the theory behind some algorithms you may already know, and cover
    state-of-the-art RL. Moreover, this is a practical book. You will see examples
    inspired by real-world industry problems and learn expert tips along the way.
    By its conclusion, you will be able to model and solve your own sequential decision-making
    problems using Python.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将为你提供关于这一领域的高级理解。我们将深入探讨一些你可能已经知道的算法背后的理论，并覆盖最前沿的强化学习。此外，本书是一本实践性很强的书。你将看到来自现实行业问题的示例，并在过程中学习到专家技巧。到书的最后，你将能够使用Python建模并解决你自己的序列决策问题。
- en: 'So, let''s start our journey by refreshing your mind on RL concepts and get
    you set up for the advanced material coming up in the following chapters. Specifically,
    this chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从复习一下强化学习的概念开始，为接下来的高级内容做好准备。具体来说，本章将涵盖以下内容：
- en: Why RL?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择强化学习？
- en: The three paradigms of machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的三种范式
- en: RL application areas and success stories
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习应用领域和成功案例
- en: Elements of an RL problem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习问题的元素
- en: Setting up your RL environment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置你的强化学习环境
- en: Why RL?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择强化学习？
- en: Creating intelligent machines that make decisions at or superior to the human
    level is a dream of many scientists and engineers, and one that is gradually becoming
    closer to reality. In the seven decades since the Turing test, AI research and
    development have been on a roller coaster. The expectations were very high initially; in
    the 1960s, for example, Herbert Simon (who later received the Nobel Prize in Economics)
    predicted that machines would be capable of doing any work humans can do within
    20 years. It was this excitement that attracted big government and corporate funding
    flowing into AI research, only to be followed by big disappointments and a period
    called the "AI winter." Decades later, thanks to the incredible developments in
    computing, data, and algorithms, humankind is again very excited, more than ever
    before, in its pursuit of the AI dream.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 创建能够做出与人类相当或更优决策的智能机器是许多科学家和工程师的梦想，而这个梦想正逐渐变得触手可及。在过去七十年里，自从图灵测试以来，人工智能（AI）的研究和发展经历了过山车般的起伏。最初的期望值非常高；例如，20世纪60年代，赫伯特·西蒙（后来获得诺贝尔经济学奖）预测，机器将在20年内能够完成所有人类能做的工作。正是这种兴奋感吸引了大量政府和企业的资金流入人工智能研究，但接着迎来的是巨大的失望和一个被称为“人工智能寒冬”的时期。几十年后，得益于计算、数据和算法的惊人进展，人类再次感到前所未有的兴奋，追求人工智能梦想的脚步比以往任何时候都更加坚定。
- en: Note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: If you're not familiar with Alan Turing's instrumental work on the foundations
    of AI in 1950, it's worth learning more about the Turing Test here: [https://youtu.be/3wLqsRLvV-c](https://youtu.be/3wLqsRLvV-c).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉艾伦·图灵在1950年关于人工智能基础的开创性工作，那么值得了解一下图灵测试： [https://youtu.be/3wLqsRLvV-c](https://youtu.be/3wLqsRLvV-c)。
- en: The AI dream is certainly one of grandiosity. After all, the potential in intelligent
    autonomous systems is enormous. Think about how we are limited in terms of specialist
    medical doctors in the world. It takes years and significant intellectual and
    financial resources to educate them, which many countries don't have at sufficient
    levels. In addition, even after years of education, it is nearly impossible for
    a specialist to stay up to date with all of the scientific developments in their
    field, learn from the outcomes of the tens of thousands of treatments around the
    world, and effectively incorporate all this knowledge into practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的梦想无疑是宏伟的。毕竟，智能自主系统的潜力巨大。想想看，全球专科医生的数量是如何受到限制的。培养一位专科医生需要多年时间，并且要投入大量的智力和财力资源，而许多国家在这方面并不具备足够的水平。此外，即便经过多年的教育，专科医生也几乎不可能跟上自己领域中所有的科学进展，无法从世界各地成千上万的治疗结果中学习，并有效地将所有这些知识应用于实践。
- en: Conversely, an AI model could process and learn from all this data and combine
    it with a rich set of information about a patient (such as medical history, lab
    results, presenting symptoms, health profile, and so on) to make a diagnosis and
    suggest treatments. This model could serve in even the most rural parts of the
    world (as long as an internet connection and computer are available) and direct
    the local health personnel about the treatment. No doubt it would revolutionize
    international healthcare and improve the lives of millions of people.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一个AI模型可以处理并从所有这些数据中学习，并将其与有关患者的丰富信息（如病史、实验室结果、表现症状、健康档案等）结合起来，做出诊断并建议治疗方案。这个模型可以服务于世界上最偏远的地区（只要有互联网连接和计算机），并指导当地的医疗人员进行治疗。毫无疑问，它将彻底改变国际医疗体系，并改善数百万人的生活。
- en: Note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: AI is already transforming the healthcare industry. In a recent article, Google
    published results from an AI system surpassing human experts in breast cancer
    prediction using mammography readings (McKinney et al., 2020). Microsoft is collaborating
    with one of India's largest healthcare providers to detect cardiac illnesses using
    AI (Agrawal, 2018). IBM Watson for Clinical Trial Matching uses natural language
    processing to recommend potential treatments for patients from medical databases
    ([https://youtu.be/grDWR7hMQQQ](https://youtu.be/grDWR7hMQQQ)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能已经在改变医疗行业。在一篇最近的文章中，谷歌公布了其AI系统在乳腺癌预测中超越人类专家的成果，该系统使用了乳腺X光片（McKinney等，2020）。微软正在与印度最大的医疗服务提供商之一合作，利用AI检测心脏疾病（Agrawal，2018）。IBM的Watson临床试验匹配系统使用自然语言处理技术，从医疗数据库中为患者推荐潜在的治疗方案（[https://youtu.be/grDWR7hMQQQ](https://youtu.be/grDWR7hMQQQ)）。
- en: On our quest to develop AI systems that are at or superior to the human level,
    which is –somewhat controversially – called **Artificial General Intelligence**
    (**AGI**), it makes sense to develop a model that can learn from its own experience
    – without necessarily needing a supervisor. RL is the computational framework
    that enables us to create such intelligent agents. To better understand the value
    of RL, it is important to compare it with the other **Machine Learning** (**ML**) paradigms,
    which we'll look into next.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们追求开发与人类水平相当或超越人类水平的AI系统的过程中——这一目标有些争议，称之为**人工通用智能**（**AGI**）——开发一种可以从自身经验中学习的模型是有意义的，而不必依赖于监督者。强化学习（RL）是使我们能够创造这种智能体的计算框架。为了更好地理解RL的价值，有必要将其与其他**机器学习**（**ML**）范式进行比较，我们将在接下来的部分进行探讨。
- en: The three paradigms of machine learning
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的三种范式
- en: RL is a separate paradigm in ML, along with **Supervised Learning** (**SL**)
    and **Unsupervised Learning** (**UL**). It goes beyond what the other two paradigms
    involve – for example, perception, classification, regression, and clustering
    – and makes decisions. More importantly, however, RL utilizes the supervised and
    unsupervised ML methods in doing so. Therefore, RL is a distinct yet closely related
    field to SL and UL, and it's important to have a grasp of them.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习（ML）中的一个独立范式，和**监督学习**（**SL**）以及**无监督学习**（**UL**）并列。它超越了其他两个范式所涉及的内容——例如感知、分类、回归和聚类——并做出决策。然而，更重要的是，RL在实现这一点时，利用了监督学习和无监督学习的方法。因此，RL是一个独立但与SL和UL紧密相关的领域，掌握它们是很重要的。
- en: Supervised learning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'SL is about learning a mathematical function that maps a set of inputs to the
    corresponding outputs/labels as accurately as possible. The idea is that we don''t
    know the dynamics of the process that generates the output, but we try to figure
    it out using the data coming out of it. Consider the following examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习是学习一个数学函数，将一组输入映射到相应的输出/标签，并尽可能精确。其核心思想是，我们不知道生成输出的过程的动态，但我们尝试利用从中获得的数据来推测它。考虑以下示例：
- en: An image recognition model that classifies the objects on the camera of a self-driving
    car as a pedestrian, stop sign, truck, and so on
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个图像识别模型，它将自动驾驶汽车摄像头中的物体分类为行人、停车标志、卡车等。
- en: A forecasting model that predicts the customer demand of a product for a particular
    holiday season using past sales data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测特定假期季节产品客户需求的预测模型，使用的是过去的销售数据。
- en: 'It is extremely difficult to come up with the precise rules to visually differentiate
    objects, or what factors lead to customers demanding a product. Therefore, SL
    models infer them from labeled data. Here are some key points about how it works:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 确定如何在视觉上区分物体，或是什么因素导致客户需求某个产品，极其困难。因此，有监督学习模型从标记数据中推断这些规则。以下是它如何工作的关键要点：
- en: During training, models learn from ground truth labels/output provided by a
    supervisor (which could be a human expert or a process).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，模型从监督者（可以是人类专家或某个过程）提供的真实标签/输出中学习。
- en: During inference, models make predictions about what the output might be given
    the input.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，模型会根据输入预测输出可能是什么。
- en: Models use function approximators to represent the dynamics of the processes
    that generate the outputs.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型使用函数逼近器来表示生成输出的过程的动态。
- en: Unsupervised learning
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'UL algorithms identify patterns in data that were previously unknown. When
    using these models, we might have an idea of what to expect as a result, but we
    don''t supply the models with labels. Consider the following examples:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法识别数据中先前未知的模式。在使用这些模型时，我们可能有一定的预期结果，但不会给模型提供标签。考虑以下示例：
- en: Identifying homogenous segments on an image provided by the camera of a self-driving
    car. The model is likely to separate the sky, road, buildings, and so on based
    on the textures on the image.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别自动驾驶汽车摄像头提供的图像中的同质区域。模型可能会根据图像中的纹理将天空、道路、建筑物等区分开来。
- en: Clustering weekly sales data into three groups based on sales volume. The output
    is likely to be weeks with low, medium, and high sales volumes.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据销售量将每周销售数据分成三组。输出可能是销售量低、中、高的周次。
- en: 'As you can tell, this is quite different from how SL works, namely in the following
    ways:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这与有监督学习的工作方式非常不同，具体体现在以下几个方面：
- en: UL models don't know what the ground truth is, and there is no label to map
    the input to. They just identify the different patterns in the data. Even after
    doing so, for example, the model would not be aware that it separated the sky
    from the road, or a holiday week from a regular week.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习模型不知道什么是真实标签，也没有标签来映射输入。它们只是识别数据中的不同模式。即使这样做了，例如，模型也不会意识到它将天空与道路分开，或将假期周与常规周区分开来。
- en: During inference, the model would cluster the input into one of the groups it
    had identified, again, without knowing what that group represents.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，模型会将输入聚类到它已识别的某个组中，同样，它并不知道这个组代表的是什么。
- en: Function approximators, such as neural networks, are used in some UL algorithms,
    but not always.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数逼近器，如神经网络，在某些无监督学习算法中被使用，但并不总是如此。
- en: With SL and UL reintroduced, we'll now compare them to RL.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新引入有监督学习（SL）和无监督学习（UL）之后，我们将它们与强化学习（RL）进行比较。
- en: Reinforcement learning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'RL is a framework to learn how to make decisions under uncertainty to maximize
    a long-term benefit through trial and error. These decisions are made sequentially,
    and earlier decisions affect the situations and benefits that will be encountered
    later. This separates RL from both SL and UL, which don''t involve any decision-making.
    Let''s revisit the examples we provided earlier to see how an RL model would differ
    from SL and UL models in terms of what it tries to find out:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个框架，旨在通过试错法在不确定性下做出决策，以最大化长期收益。这些决策是按顺序做出的，早期的决策会影响后续将遇到的情况和收益。这使得强化学习与有监督学习（SL）和无监督学习（UL）有所不同，因为后者并不涉及任何决策。让我们重新回顾一下之前提供的示例，看看强化学习模型与有监督学习和无监督学习模型在它们尝试找出什么方面有何不同：
- en: For a self-driving car, given the types and positions of all the objects on
    the camera, and the edges of the lanes on the road, the model might learn how
    to steer the wheel and what the speed of the car should be to pass the car ahead
    safely and as quickly as possible.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于自驾车来说，给定摄像头捕捉到的所有物体的类型和位置，以及道路上的车道边缘，模型可能会学到如何转动方向盘以及应该以何种车速通过前方的车辆，以尽可能安全、快速地通过。
- en: Given the historical sales numbers for a product and the time it takes to bring
    the inventory from the supplier to the store, the model might learn when and how
    many units to order from the supplier so that seasonal customer demand is satisfied
    with high likelihood, while the inventory and transportation costs are minimized.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定产品的历史销售数据以及将库存从供应商送到商店所需的时间，模型可能会学到何时以及订购多少单位库存，以便以高概率满足季节性客户需求，同时将库存和运输成本最小化。
- en: 'As you will have noticed, the tasks that RL is trying to accomplish are of
    a different nature and more complex than those simply addressed by SL and UL alone.
    Let''s elaborate on how RL is different:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，RL试图完成的任务与SL和UL单独处理的任务性质不同，且更为复杂。我们来详细阐述一下RL的不同之处：
- en: The output of an RL model is a decision given the situation, not a prediction
    or clustering.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL模型的输出是在特定情况下的决策，而不是预测或聚类。
- en: There are no ground-truth decisions provided by a supervisor that tell the model
    what the ideal decisions are in different situations. Instead, the model learns
    the best decisions from the feedback from its own experience and the decisions
    it made in the past. For example, through trial and error, an RL model would learn
    that speeding too much while passing a car may lead to accidents, and ordering
    too much inventory before holidays will cause excess inventory later.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有由监督者提供的“真实”决策来告诉模型在不同情况下理想的决策是什么。相反，模型通过反馈自己的经验和过去做出的决策来学习最佳决策。例如，通过反复试验，RL模型会学到，超速超车可能会导致事故，而在假期前订购过多库存则会导致后期的库存过剩。
- en: RL models often use outputs of SL models as inputs to make decisions. For example,
    the output of an image recognition model in a self-driving car could be used to
    make driving decisions. Similarly, the output of a forecasting model is often
    used as input to an RL model that makes inventory replenishment decisions.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL模型通常使用SL模型的输出作为输入来做决策。例如，自驾车中图像识别模型的输出可以用来做驾驶决策。同样，预测模型的输出经常作为RL模型的输入，用来做库存补充决策。
- en: Even in the absence of such input from an auxiliary model, RL models, either
    implicitly or explicitly, predict what situations its decisions will lead to in
    the future.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即便没有来自辅助模型的这种输入，RL模型也会在隐式或显式的情况下预测其决策未来将导致的情况。
- en: RL utilizes many methods developed for SL and UL, such as various types of neural
    networks as function approximators.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL利用了许多为SL和UL开发的方法，例如将各种类型的神经网络作为函数近似器。
- en: So, what differentiates RL from other ML methods is that it is a decision-making
    framework. What makes it exciting and powerful, though, is its similarities to
    how we learn as humans to make decisions from experience. Imagine a toddler learning
    how to build a tower from toy blocks. Usually, the taller the tower, the happier
    the toddler is. Every increment in height is a success. Every collapse is a failure.
    They quickly discover that the closer the next block is to the center of the one
    beneath, the more stable the tower is. This is reinforced when a block that is
    placed too close to the edge more readily topples. With practice, they manage
    to stack several blocks on top of each other. They realize how they stack the
    earlier blocks creates a foundation that determines how tall of a tower they can
    build. Thus, they learn.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，RL与其他机器学习方法的不同之处在于，它是一个决策框架。然而，使其令人兴奋和强大的地方在于它与我们人类通过经验做决策的学习方式相似。试想一个学步的孩子如何学着用玩具积木搭建塔楼。通常，塔楼越高，孩子就越开心。每增加一块积木的高度都是一次成功，每次倒塌都是一次失败。他们很快发现，下一块积木越靠近下面积木的中心，塔楼就越稳定。当积木放得离边缘太近时，它更容易倒塌。这种经验得到了强化，经过练习，他们能将几块积木堆叠起来。他们意识到，早期积木的堆放方式创造了一个基础，决定了他们能建造多高的塔楼。由此，他们学会了。
- en: Of course, the toddler did not learn these architectural principles from a blueprint.
    They learned from the commonalities in their failure and success. The increasing
    height of the tower or its collapse provided a feedback signal upon which they
    refined their strategy accordingly. Learning from experience, rather than a blueprint,
    is at the center of RL. Just as the toddler discovers which block positions lead
    to taller towers, an RL agent identifies the actions with the highest long-term
    rewards through trial and error. This is what makes RL such a profound form of
    AI; it's unmistakably human.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，那个学步的小孩并不是从蓝图中学到这些建筑原理的。他们从失败和成功的共性中学习。塔楼高度的增加或倒塌为他们提供了反馈信号，基于这些信号他们调整了策略。从经验中学习，而非从蓝图中学习，这正是强化学习的核心。就像小孩发现哪些积木摆放的位置可以造出更高的塔一样，强化学习的智能体通过试错来识别出能带来最高长期回报的行动。这就是强化学习如此深刻的原因，它显然是人类的智慧体现。
- en: Over the past few years, there have been many amazing success stories proving
    the potential in RL. Moreover, there are many industries it is about to transform.
    So, before diving into the technical aspects of RL, let's further motivate ourselves
    by looking into what RL can do in practice.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，许多令人惊叹的成功故事证明了强化学习的潜力。此外，强化学习正在改变许多行业。所以，在深入探讨强化学习的技术细节之前，让我们通过了解强化学习在实际中的应用来进一步激励自己。
- en: RL application areas and success stories
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的应用领域和成功故事
- en: RL is not a new field. Many of the fundamental ideas in RL were introduced in
    the context of dynamic programming and optimal control over the past seven decades.
    However, successful RL implementations have taken off recently thanks to the breakthroughs
    in deep learning and more powerful computational resources. In this section, we
    will talk about some of the application areas of RL together with some famous
    success stories. We will go deeper into the algorithms behind these implementations
    in the following chapters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习并不是一个新兴领域。在过去的七十年里，强化学习的许多基本思想是在动态规划和最优控制的背景下提出的。然而，得益于深度学习的突破和更强大的计算资源，强化学习的成功实现最近才迎来了爆发。在这一节中，我们将讨论一些强化学习的应用领域以及一些著名的成功故事。我们将在接下来的章节中深入探讨这些实现背后的算法。
- en: Games
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏
- en: 'Board and video games have been a research lab for RL, leading to many famous
    success stories in this area. The reasons why games make good RL problems are
    as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 桌面游戏和视频游戏一直是强化学习的研究实验室，催生了许多著名的成功案例。游戏之所以成为良好的强化学习问题，原因如下：
- en: Games are naturally about sequential decision-making with uncertainty involved.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏本质上涉及到带有不确定性的顺序决策。
- en: They are available as computer software, making it possible for RL models to
    flexibly interact with them and generate billions of data points for training.
    Also, trained RL models are then also tested in the same computer environment.
    This is as opposed to many physical processes for which it is too complex to create
    accurate and fast simulators.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们以计算机软件的形式存在，使得强化学习模型可以灵活地与这些游戏互动，并生成数十亿的数据点用于训练。而且，训练过的强化学习模型也会在同样的计算环境中进行测试。这与许多物理过程不同，后者很难创建出准确且快速的模拟器。
- en: The natural benchmark in games are the best human players, making it an appealing
    battlefield for AI versus human comparisons.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏中的自然基准是最优秀的人类玩家，这使得它成为人工智能与人类比较的一个充满吸引力的战场。
- en: After this introduction, let's look into some of the most exciting pieces of
    RL work that have made it to the headlines.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本介绍之后，让我们来看看一些引人注目的强化学习（RL）工作，这些工作已经登上了头条。
- en: TD-Gammon
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TD-Gammon
- en: The first famous RL implementation is TD-Gammon, a model that learned how to
    play super-human-level backgammon – a two-player board game with 1,020 possible
    configurations. The model was developed by Gerald Tesauro at IBM Research in 1992\.
    TD-Gammon was so successful that it created great excitement in the backgammon
    community back then with the novel strategies it taught humans. Many methods used
    in that model (temporal-difference, self-play, and use of neural networks, for
    example) are still at the center of modern RL implementations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个著名的强化学习实现是TD-Gammon，这是一个学习如何玩超级人类水平的西洋双陆棋（backgammon）的模型——这是一种具有1,020种可能配置的双人棋盘游戏。该模型由IBM研究院的Gerald
    Tesauro于1992年开发。TD-Gammon的成功引起了当时西洋双陆棋社区的巨大关注，它为人类带来了许多新颖的策略。该模型中使用的许多方法（例如时序差分、自对弈和神经网络的应用）至今仍是现代强化学习实现的核心。
- en: Super-human performance in Atari games
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Atari游戏中超越人类表现
- en: One of the most impressive and seminal works in RL was that of Volodymry Mnih
    and his colleagues at Google DeepMind that came out in 2015\. The researchers
    trained RL agents that learned how to play Atari games better than humans by only
    using screen input and game scores, without any hand-crafted or game-specific
    features through deep neural networks. They named their algorithm the **Deep Q-Network** (**DQN**),
    which is one of the most popular RL algorithms today.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）领域最具影响力和开创性的研究之一是2015年由Volodymry Mnih及其在Google DeepMind的同事们完成的工作。研究人员训练了RL代理，使其仅通过屏幕输入和游戏得分，利用深度神经网络学习如何玩Atari游戏，超越人类表现，而无需任何手工设计或特定于游戏的特征。他们将这个算法命名为**深度Q网络**（**DQN**），它是今天最流行的RL算法之一。
- en: Beating the world champions in Go, chess, and shogi
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 击败围棋、国际象棋和将棋的世界冠军
- en: The RL implementation that perhaps brought the most fame to RL was Google DeepMind's
    AlphaGo. It was the first computer program to beat a professional player in the
    ancient board game of Go in 2015, and later the world champion Lee Sedol in 2016\.
    This story was later turned into a documentary film with the same name. The AlphaGo
    model was trained using data from human expert moves as well as with RL through
    self-play. A later version, AlphaGo Zero, reached a performance of defeating the
    original AlphaGo 100-0, which was trained via just self-play and without any human
    knowledge inserted into the model. Finally, the company released AlphaZero in
    2018, which was able to learn the games of chess, shogi (Japanese chess), and
    Go to become the strongest player in history for each, without any prior information
    about the games except the game rules. AlphaZero reached this performance after
    only several hours of training on **Tensor Processing Units** (**TPUs**). AlphaZero's
    unconventional strategies were praised by world-famous players, such as Garry
    Kasparov (chess) and Yoshiharu Habu (shogi).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 也许为RL带来最多声誉的RL实现是Google DeepMind的AlphaGo。它是第一个在2015年击败职业围棋选手的计算机程序，后来在2016年击败了世界冠军李世石。这一故事后来被拍成了同名纪录片。AlphaGo模型是通过人类专家棋局数据以及通过自我对弈与RL训练出来的。后来的版本AlphaGo
    Zero以100-0的成绩击败了原版AlphaGo，而这个模型只通过自我对弈训练，并且没有任何人类知识输入。最后，公司在2018年发布了AlphaZero，它能够学习国际象棋、将棋（日本象棋）和围棋，成为每个游戏历史上最强的玩家，且只了解游戏规则，没有任何关于游戏的先验信息。AlphaZero在仅数小时的**张量处理单元**（**TPU**）训练后达到了这个表现。AlphaZero的非常规策略得到了世界著名棋手的赞扬，比如加里·卡斯帕罗夫（国际象棋）和羽生善治（将棋）。
- en: Victories in complex strategy games
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在复杂战略游戏中的胜利
- en: RL's success later went beyond just Atari and board games, into Mario, Quake
    III Arena, Capture the Flag, Dota 2, and StarCraft II. Some of these games are
    exceptionally challenging for AI programs with the need for strategic planning,
    the involvement of game theory between multiple decision-makers, imperfect information,
    and a large number of possible actions and game states. Due to this complexity,
    it took an enormous amount of resources to train those models. For example, OpenAI
    trained the Dota 2 model using 256 GPUs and 128,000 CPU cores for months, giving
    900 years of game experience to the model per day. Google DeepMind's AlphaStar,
    which defeated top professional players in StarCraft II in 2019, required training
    hundreds of copies of a sophisticated model with 200 years of real-time game experience
    for each, although those models were initially trained on the real game data of
    human players.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: RL的成功后来超越了Atari和棋盘游戏，扩展到了马里奥、Quake III Arena、Capture the Flag、Dota 2和StarCraft
    II等游戏。这些游戏对AI程序极具挑战性，需要战略规划、多方决策者之间的博弈论、信息不完全以及大量的可能行动和游戏状态。由于这些复杂性，训练这些模型需要大量资源。例如，OpenAI使用256个GPU和128,000个CPU核心训练了Dota
    2模型，进行了数月的训练，每天为模型提供了900年的游戏经验。Google DeepMind的AlphaStar在2019年击败了星际争霸II的顶级职业选手，其训练过程也需要数百个精密模型的复制，每个模型有200年的实时游戏经验，尽管这些模型最初是基于人类玩家的真实游戏数据进行训练的。
- en: Robotics and autonomous systems
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器人学与自主系统
- en: Robotics and physical autonomous systems are challenging fields for RL. This
    is because RL agents are trained in simulation to gather enough data, but a simulation
    environment cannot reflect all the complexities of the real world. Therefore,
    those agents often fail in the actual task, which is especially problematic if
    the task is safety-critical. In addition, these applications often involve continuous
    actions, which require different types of algorithms than DQN. Despite these challenges,
    on the other hand, there are numerous RL success stories in these fields. In addition,
    there is a lot of research on using RL in exciting applications such as autonomous
    ground and air vehicles.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人技术和物理自主系统是强化学习领域中的挑战性领域。这是因为强化学习智能体通常是在仿真环境中训练的，以收集足够的数据，但仿真环境无法反映现实世界的所有复杂性。因此，这些智能体在实际任务中常常失败，尤其是在任务涉及到安全性时，这种失败尤为严重。此外，这些应用通常涉及连续的动作，这需要与DQN不同类型的算法。尽管面临这些挑战，但另一方面，强化学习在这些领域也有许多成功的案例。此外，使用强化学习应用于自动驾驶地面和空中车辆等令人兴奋的应用领域的研究也有很多。
- en: Elevator optimization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电梯优化
- en: An early success story that proved RL can create value for real-world applications
    was about elevator optimization in 1996 by Robert Crites and Andrew Barto. The
    researchers developed an RL model to optimize elevator dispatching in a 10-story
    building with 4 elevator cars. This was a much more challenging problem than the
    earlier TD-Gammon due to the possible number of situations the model can encounter,
    partial observability (for example, the number of people waiting at different
    floors was not observable to the RL model), and the possible number of decisions
    to choose from. The RL model substantially improved the best elevator control
    heuristics of the time across various metrics, such as average passenger wait
    time and travel time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个早期的成功案例证明了强化学习（RL）可以为现实世界的应用创造价值，这个案例发生在1996年，由Robert Crites和Andrew Barto进行的电梯优化研究。研究人员开发了一个强化学习模型，用于优化一栋有10层楼和4部电梯的建筑中的电梯调度。这比早期的TD-Gammon问题要具有更大的挑战性，因为模型可能会遇到的情境数量、部分可观察性（例如，不同楼层等待的人数对于强化学习模型不可观察）以及可选择的决策数量都大大增加。这个强化学习模型在多个指标上显著提高了当时最佳电梯控制启发式算法的表现，例如平均乘客等待时间和旅行时间。
- en: Humanoid robots and dexterous manipulation
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人形机器人与灵巧操作
- en: 'In 2017, Nicolas Heess et al. of Google DeepMind were able to teach different
    types of bodies (for example, humanoid and so on) various locomotion behaviors,
    such as how to run, jump, and so on in a computer simulation. In 2018, Marcin
    Andrychowicz et al. of OpenAI trained a five-fingered humanoid hand to manipulate
    a block from an initial configuration to a goal configuration. In 2019, again,
    researchers from OpenAI, Ilge Akkaya et al., were able to train a robot hand to
    solve a Rubik''s cube:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，谷歌DeepMind的Nicolas Heess等人能够在计算机仿真中教会不同类型的身体（例如，人形机器人等）各种运动行为，例如如何奔跑、跳跃等。2018年，OpenAI的Marcin
    Andrychowicz等人训练了一个五指人形机器人手臂，将一个方块从初始状态移动到目标状态。2019年，OpenAI的Ilge Akkaya等研究人员再次成功地训练了一个机器人手臂来解魔方：
- en: '![Figure 1.1 – OpenAI''s RL model that solved a Rubik''s cube is trained in
    simulation (a) and deployed on a physical robot (b) (image source: OpenAI Blog,
    2019)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 – OpenAI的强化学习模型，通过仿真训练（a），并部署到物理机器人上（b）（图像来源：OpenAI Blog，2019）'
- en: '](img/B14160_01_01.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_01_01.jpg)'
- en: 'Figure 1.1 – OpenAI''s RL model that solved a Rubik''s cube is trained in simulation
    (a) and deployed on a physical robot (b) (image source: OpenAI Blog, 2019)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – OpenAI的强化学习模型，通过仿真训练（a），并部署到物理机器人上（b）（图像来源：OpenAI Blog，2019）
- en: Both of the latter two models were trained in simulation and successfully transferred
    to physical implementation using domain randomization techniques (*Figure 1.1*).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 后两种模型都是在仿真中训练的，并成功地通过领域随机化技术转移到物理实现中（*图1.1*）。
- en: Emergency response robots
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 紧急响应机器人
- en: In the aftermath of a disaster, using robots could be extremely helpful, especially
    when operating in dangerous conditions. For example, robots could locate survivors
    in damaged structures, turn off gas valves, and so on. Creating intelligent robots
    that operate autonomously would allow scaling emergency response operations and
    provide the necessary support to many more people than is currently possible with
    manual operations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在灾难发生后，使用机器人可能会非常有帮助，特别是在危险环境下作业时。例如，机器人可以在损坏的建筑物中寻找幸存者，关闭燃气阀门等。创造能够自主操作的智能机器人将使紧急响应操作得到扩展，并为更多的人提供支持，这比目前人工操作能够提供的支持要多得多。
- en: Self-driving vehicles
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自主驾驶车辆
- en: Although a fully self-driving car is too complex to solve with an RL model alone,
    some of the tasks could be handled by RL. For example, we can train RL agents
    for self-parking and making decisions for when and how to pass a car on a highway.
    Similarly, we can use RL agents to execute certain tasks in an autonomous drone,
    such as how to take off, land, avoid collisions, and so on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管完全自动驾驶汽车的复杂性使得单靠RL模型无法解决，但其中一些任务可以由RL处理。例如，我们可以训练RL代理来实现自动停车，并决定何时及如何超车。此外，我们还可以使用RL代理来执行自动驾驶无人机中的某些任务，例如如何起飞、着陆、避免碰撞等。
- en: Supply chain
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 供应链
- en: 'Many decisions in a supply chain are of sequential nature and involve uncertainty,
    for which RL is a natural approach. Some of these problems are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 供应链中的许多决策具有顺序性质并涉及不确定性，因此RL是一个自然的解决方案。以下是一些这类问题的示例：
- en: '**Inventory planning** is about deciding when to place a purchase order to
    replenish the inventory of an item and at what quantity. Ordering less than necessary
    causes shortages and ordering more than necessary causes excess inventory costs,
    product spoilage, and inventory removal at reduced prices. RL models are used
    to make inventory planning decisions to decrease the cost of these operations.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库存规划**是关于决定何时下订单以补充商品库存以及订购多少数量的问题。订购不足会导致短缺，订购过多则会导致库存积压、产品腐烂以及以降价处理库存。RL模型被用来做出库存规划决策，以减少这些操作的成本。'
- en: '**Bin packing** is a common problem in manufacturing and supply chains where
    items arriving at a station are placed into containers to minimize the number
    of containers used and to ensure smooth operations in the facility. This is a
    difficult problem that can be solved using RL.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**装箱问题**是制造业和供应链中常见的问题，其中到达某一工作站的物品需要被放入容器中，以最小化使用的容器数量并确保工厂内操作顺利进行。这是一个难题，可以通过RL来解决。'
- en: Manufacturing
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制造业
- en: 'An area where RL will have a great impact is manufacturing, where a lot of
    manual tasks can potentially be carried out by autonomous agents at reduced costs
    and increased quality. As a result, many companies are looking into bringing RL
    to their manufacturing environment. Here are some examples of RL applications
    in manufacturing:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）将在制造业中产生巨大影响，许多人工任务可能会由自主代理以更低的成本和更高的质量完成。因此，许多公司正在研究将RL引入其制造环境。以下是一些RL在制造业中的应用示例：
- en: '**Machine calibration** is a task that is often handled by human experts in
    manufacturing environments, which is inefficient and error-prone. RL models are
    often capable of achieving these tasks at reduced costs and increased quality.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器校准**是制造环境中常由人类专家处理的任务，这种方式既低效又容易出错。RL模型通常能够以更低的成本和更高的质量完成这些任务。'
- en: '**Chemical plant operations** often involve sequential decision making, which
    is often handled by human experts or heuristics. RL agents are shown to effectively
    control these processes with better final product quality and less equipment wear
    and tear.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**化工厂操作**通常涉及顺序决策，这些决策通常由人类专家或启发式方法来处理。研究表明，RL代理能够有效地控制这些过程，从而获得更好的最终产品质量并减少设备磨损。'
- en: '**Equipment maintenance** requires planning downtimes to avoid costly breakdowns.
    RL models can effectively balance the cost of downtime and the cost of a potential
    breakdown.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备维护**需要规划停机时间以避免昂贵的设备故障。RL模型可以有效平衡停机成本与潜在故障的成本。'
- en: In addition to these examples, many successful RL applications in **robotics**
    can be transferred to manufacturing solutions.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了这些示例之外，许多成功的RL应用在**机器人技术**领域也可以转化为制造解决方案。
- en: Personalization and recommender systems
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个性化与推荐系统
- en: 'Personalization is arguably the area where RL has created the most business
    value so far. Big tech companies provide personalization as a service with RL
    algorithms running under the hood. Here are some examples:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化无疑是RL迄今为止创造最大商业价值的领域。大科技公司通过RL算法为个性化提供服务，算法在背后运行。以下是一些示例：
- en: In **advertising**, the order and content of promotional materials delivered
    to (potential) customers is a sequential decision-making problem that can be solved
    using RL, leading to increased customer satisfaction and conversion.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**广告**中，向（潜在）客户传递促销材料的顺序和内容是一个顺序决策问题，可以通过RL来解决，从而提高客户满意度和转化率。
- en: '**News recommendation** is an area where Microsoft News has famously applied
    RL and increased visitor engagement by improving the article selection and the
    order of recommendation.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新闻推荐**是微软新闻广为应用强化学习的一个领域，通过改进文章的选择和推荐顺序，成功增加了访客的参与度。'
- en: '**Personalization of the artwork** that you see for the titles on Netflix is
    handled by RL algorithms. With that, viewers better identify the titles relevant
    to their interests.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化的艺术作品**，比如Netflix中展示的标题，是由强化学习算法来处理的。通过这个方式，观众可以更好地识别与自己兴趣相关的标题。'
- en: '**Personalized healthcare** is becoming increasingly important as it provides
    more effective treatments at reduced costs. There are many successful applications
    of RL picking the right treatment for patients.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化医疗**变得越来越重要，因为它能以较低的成本提供更有效的治疗。强化学习在为患者选择合适治疗方案方面有许多成功的应用。'
- en: Smart cities
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智慧城市
- en: 'There are many areas where RL can help improve how cities operate. The following
    are a couple of examples:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以帮助改善城市运作的许多领域。以下是几个例子：
- en: In a traffic network with multiple intersections, the traffic lights should
    work in harmony to ensure the smooth flow of the traffic. It turns out that this
    problem can be modeled as a multi-agent RL problem and improve the existing systems
    for traffic light control.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个包含多个交叉口的交通网络中，交通信号灯应该协调工作，以确保交通流畅。事实证明，这个问题可以建模为一个多智能体强化学习问题，并改善现有的交通信号灯控制系统。
- en: Balancing the generation and demand in electricity grids in real time is an
    important problem to ensure grid safety. One way of achieving this is to control
    the demand, such as charging electric vehicles and turning on air conditioning
    systems when there is enough generation, without sacrificing the service quality,
    to which RL methods have successfully been applied.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时平衡电网中的电力生成和需求是一个重要的问题，以确保电网安全。实现这一目标的一种方式是控制需求，比如在电力生成充足时，充电电动车和开启空调系统，而不会影响服务质量，强化学习方法已经成功地应用于此。
- en: 'This list can go on for pages, but it should be enough to demonstrate the huge
    potential in RL. What Andrew Ng, a pioneer in the field, says about AI is very
    much true for RL as well:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表可以写好几页，但这已经足够展示强化学习的巨大潜力。领域先驱安德鲁·吴（Andrew Ng）关于人工智能的观点，对于强化学习同样适用：
- en: 'Just as electricity transformed almost everything 100 years ago, today I actually
    have a hard time thinking of an industry that I don''t think AI will transform
    in the next several years. (Andrew Ng: Why AI is the new electricity; Stanford
    News; March 15, 2017)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就像100年前电力几乎改变了所有行业一样，今天我很难想到一个行业，我认为人工智能在未来几年不会改变的。（安德鲁·吴：为什么人工智能是新的电力；斯坦福新闻；2017年3月15日）
- en: RL today is only at the beginning of its prime, and you are making a great investment
    by putting effort toward understanding what RL is and what it has to offer. Now,
    it is time to get more technical and formally define the elements in an RL problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的强化学习仍处于其黄金时代的初期，而你正在通过投入精力去理解强化学习的本质及其潜力，做出一个伟大的投资。现在，是时候更技术化地定义强化学习问题中的各个元素了。
- en: Elements of an RL problem
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习（RL）问题的元素
- en: So far, we have covered the types of problems that can be modeled using RL.
    In the next chapters, we will dive into state-of-the-art algorithms that will
    solve those problems. However, in between, we need to formally define the elements
    in an RL problem. This will lay the groundwork for more technical material by
    establishing our vocabulary. After providing these definitions, we will then look
    into what these concepts correspond to in a tic-tac-toe example.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了可以用强化学习建模的各种问题类型。在接下来的章节中，我们将深入探讨解决这些问题的最先进算法。然而，在此过程中，我们需要正式定义强化学习问题中的元素。这将为更技术化的内容奠定基础，帮助建立我们的词汇体系。在给出这些定义之后，我们将通过井字游戏的例子来解释这些概念的具体含义。
- en: RL concepts
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习概念
- en: 'Let''s start by defining the most fundamental components in an RL problem:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义强化学习问题中的最基本组成部分开始：
- en: At the center of an RL problem, there is the learner, which is called the **agent** in
    RL terminology. Most of the problem classes we deal with have a single agent.
    On the other hand, if there is more than one agent, that problem class is called
    a **multi-agent RL**, or **MARL** for short. In MARL, the relationship between
    the agents could be cooperative, competitive, or a mix of the two.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在强化学习问题的核心，是学习者，强化学习术语中称之为**智能体（agent）**。我们处理的大多数问题类只有一个智能体。另一方面，如果有多个智能体，那么这个问题类被称为**多智能体强化学习（multi-agent
    RL）**，简称**MARL**。在MARL中，智能体之间的关系可以是合作的、竞争的，或者两者的混合。
- en: The essence of an RL problem is the agent learning what to do – that is, which **action** to
    take – in different situations in the world it lives in. We call this world the **environment** and
    it refers to everything outside of the agent.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习问题的本质是智能体学习在其所处的世界中该做什么——也就是采取哪个**行动**——以应对不同的情况。我们将这个世界称为**环境**，它指的是智能体之外的所有事物。
- en: The set of all the information that precisely and sufficiently describes the
    situation in the environment is called the **state**. So, if the environment is
    in the same state at different points in time, it means everything about the environment
    is exactly the same – like a copy-paste.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有能够精确且充分描述环境中情况的信息集合称为**状态**。因此，如果环境在不同时间点处于相同的状态，意味着环境的所有情况完全相同——就像复制粘贴一样。
- en: In some problems, the knowledge of the state is fully available to the agent.
    In a lot of other problems, and especially in more realistic ones, the agent does
    not fully observe the state, but only part of it (or a derivation of a part of
    the state). In such cases, the agent uses its **observation** to take action.
    When this is the case, we say that the problem is **partially observable**. Unless
    we say otherwise, we assume that the agent is able to fully observe the state
    that the environment is in and is basing its actions on the state.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些问题中，智能体对状态的知识是完全可用的。在许多其他问题中，尤其是在更现实的情况中，智能体并不能完全观察到状态，而只能观察到其中的一部分（或某部分状态的推导）。在这种情况下，智能体使用其**观察**来采取行动。当情况如此时，我们称该问题为**部分可观察**。除非另有说明，我们假设智能体能够完全观察到环境所处的状态，并基于该状态来执行其行动。
- en: Info
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息
- en: The term *state*, and its notation ![](img/Formula_01_001.png), is more commonly
    used during abstract discussions, especially when the environment is assumed to
    be fully observable, although *observation* is a more general term; what the agent
    receives is always an observation, which is sometimes just the state itself, and
    sometimes a part of or a derivation from the state, depending on the environment.
    Don't get confused if you see them used interchangeably in some contexts.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*状态* 这一术语及其符号 ![](img/Formula_01_001.png) 在抽象讨论中更常被使用，尤其是在假设环境是完全可观察的情况下，尽管
    *观察* 是一个更广泛的术语；智能体所接收到的总是观察结果，有时是状态本身，有时是状态的一部分或从状态中推导出的内容，这取决于环境。如果你看到它们在某些语境中交替使用，不要感到困惑。'
- en: 'So far, we have not really defined what makes an action good or bad. In RL,
    every time the agent takes an action, it receives a **reward** from the environment
    (albeit it is sometimes zero). *Reward* could mean many things in general, but
    in RL terminology, its meaning is very specific: it is a scalar number. The greater
    the number is, the higher the reward also is. In an iteration of an RL problem,
    the agent observes the state the environment is in (fully or partially) and takes
    an action based on its observation. As a result, the agent receives a reward,
    and the environment transitions into a new state. This process is described in
    *Figure 1.2*, which is probably familiar to you:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有真正定义什么是好的或不好的行动。在强化学习中，每当智能体采取一个行动时，它都会从环境中获得一个**奖励**（尽管有时奖励为零）。*奖励*在一般意义上可以有很多含义，但在强化学习术语中，其含义非常具体：它是一个标量数字。数字越大，奖励也越高。在强化学习问题的每次迭代中，智能体观察到环境所处的状态（无论是完全观察还是部分观察），并根据其观察采取行动。结果是，智能体获得奖励，环境进入新的状态。这个过程在*图
    1.2*中有所描述，您可能已经很熟悉了：
- en: '![Figure 1.2 – RL process diagram'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2 – 强化学习过程图'
- en: '](img/B14160_01_02.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_01_02.jpg)'
- en: Figure 1.2 – RL process diagram
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 强化学习过程图
- en: 'Remember that in RL, the agent is interested in actions that will be beneficial
    over the long term. This means the agent must consider the long-term consequences
    of its actions. Some actions might lead the agent to immediate high rewards only
    to be followed by very low rewards. The opposite might also be true. So, the agent''s
    goal is to maximize the cumulative reward it receives. The natural follow-up question
    is over what time horizon? The answer depends on whether the problem of interest
    is defined over a finite or infinite horizon:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在强化学习中，智能体关心的是长期有益的行动。这意味着智能体必须考虑其行动的长期后果。一些行动可能会使智能体立即获得高奖励，但接下来却会迎来非常低的奖励，反之亦然。因此，智能体的目标是最大化其获得的累计奖励。自然的后续问题是：这个时间跨度是多长？答案取决于所关注的问题是在有限还是无限的时间范围内定义的：
- en: If it is the former, the problem is described as an **episodic task**, where
    an **episode** is defined as the sequence of interactions from an initial state
    to a **terminal state**. In episodic tasks, the agent's goal is to maximize the
    expected total cumulative reward collected over an episode.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是前者，那么问题被描述为一个**情节任务**，其中**情节**定义为从初始状态到**终止状态**的交互序列。在情节任务中，代理的目标是最大化在一个情节内收集到的期望总累计奖励。
- en: If the problem is defined over an infinite horizon, it is called a **continuing
    task**. In that case, the agent will try to maximize the average reward since
    the total reward would go up to infinity.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题是在无限时域内定义的，它被称为**持续任务**。在这种情况下，代理会尝试最大化平均奖励，因为总奖励将趋向于无穷大。
- en: So, how does an agent achieve this objective? The agent identifies the best
    action(s) to take given its observation of the environment. In other words, the
    RL problem is all about finding a **policy**, which maps a given observation to
    one (or more) of the actions, which maximizes the expected cumulative reward.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那么，代理如何实现这个目标呢？代理根据对环境的观察，确定最好的行动。换句话说，强化学习问题的核心是找到一种**策略**，它将给定的观察映射到一个（或多个）行动，从而最大化期望的累计奖励。
- en: All these concepts have concrete mathematical definitions, which we will cover
    in detail in later chapters. But for now, let's try to understand what these concepts
    would correspond to in a concrete example.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些概念都有具体的数学定义，我们将在后面的章节中详细讨论。但现在，让我们试着理解这些概念在具体例子中的含义。
- en: Casting tic-tac-toe as an RL problem
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将井字棋视为一个强化学习问题
- en: 'Tic-tac-toe is a simple game in which two players take turns to mark the empty
    spaces in a ![](img/Formula_01_002.png) grid. We will now cast this as an RL problem
    to map the definitions provided previously to the concepts in the game. The goal
    for a player is to place three of their marks in a vertical, horizontal, or diagonal
    row to become the winner. If none of the players are able to achieve this before
    running out of the empty spaces on the grid, the game ends in a draw. Mid-game,
    a tic-tac-toe board might look like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 井字棋是一个简单的游戏，两个玩家轮流在一个![](img/Formula_01_002.png)网格中标记空位。我们现在将其作为一个强化学习问题，将之前提供的定义映射到游戏中的概念。玩家的目标是将自己的标记排成一行（纵向、横向或对角线），以赢得比赛。如果没有任何一个玩家在所有空位用完之前完成这一目标，游戏将以平局结束。在游戏进行中，井字棋的棋盘可能看起来像这样：
- en: '![Figure 1.3 – An example board configuration in tic-tac-toe'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3 – 井字棋的示例棋盘配置'
- en: '](img/B14160_01_03.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_01_03.jpg)'
- en: Figure 1.3 – An example board configuration in tic-tac-toe
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 井字棋的示例棋盘配置
- en: 'Now, imagine that we have an RL agent playing against a human player:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有一个强化学习代理与一个人类玩家对战：
- en: The action the agent takes is to place its mark (say, a cross) in one of the
    empty spaces on the board when it is the agent's turn.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理采取的行动是在轮到代理的回合时，将其标记（比如叉）放置到棋盘上的空位之一。
- en: Here, the board is the entire environment, and the position of the marks on
    the board is the state, which is fully observable to the agent.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，棋盘就是整个环境，棋盘上标记的位置就是状态，代理可以完全观察到这些状态。
- en: In a 3 x 3 tic-tac-toe game, there are 765 states (unique board positions, excluding
    rotations and reflections) and the agent's goal is to learn a policy that will
    suggest an action for each of these states so as to maximize the chance of winning.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个3 x 3的井字棋游戏中，有765种状态（唯一的棋盘位置，排除了旋转和镜像），代理的目标是学习一种策略，能够为这些状态中的每一个建议一个行动，以最大化获胜的概率。
- en: The game can be defined as an episodic RL task. Why? Because the game will last
    for a maximum of 9 turns and the environment will reach a terminal state. A terminal
    state is one where either three Xs or Os make a row or one where no single mark
    makes a row and there is no space left on the board (that is, a draw).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个游戏可以定义为一个情节性强化学习任务。为什么？因为游戏最多会持续9轮，环境会达到一个终止状态。终止状态是指三个X或O排成一行，或者没有任何标记能排成一行且棋盘上没有空位（即平局）。
- en: Note that no reward is given as the players make their moves during the game,
    except at the very end if a player wins. So, the agent receives +1 reward if it
    wins, -1 if it loses, and 0 if the game is a draw. In all the iterations until
    the end, the agent receives 0 reward.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，除非在游戏结束时某个玩家获胜，否则在游戏过程中，玩家每进行一次操作都不会获得奖励。所以，如果代理获胜，它会得到+1奖励，失败则得到-1奖励，平局则得到0奖励。在所有迭代中，直到游戏结束，代理将获得0奖励。
- en: We can turn this into a multi-agent RL problem by replacing the human player
    with another RL agent to compete with the first one.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过用另一个 RL 代理替换人类玩家来将其转化为一个多智能体 RL 问题，让新代理与第一个代理竞争。
- en: Hopefully, this refreshes your mind on what agent, state, action, observation,
    policy, and reward mean. This was just a toy example, and rest assured that it
    will get much more advanced later. With this introductory context out of the way,
    what we need to do is to set up our computer environment to be able to run the
    RL algorithms we will cover in the following chapters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能刷新你对智能体、状态、动作、观察、策略和奖励含义的记忆。这只是一个简单的例子，放心，后面会有更先进的内容。在这个入门性的背景介绍完成后，我们需要做的就是设置计算机环境，以便能够运行我们在接下来的章节中将要讲解的
    RL 算法。
- en: Setting up your RL environment
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置你的 RL 环境
- en: RL algorithms utilize state-of-the-art ML libraries that require some sophisticated
    hardware. To follow along with the examples we will solve throughout the book,
    you will need to set up your computer environment. Let's go over the hardware
    and software you will need in your setup.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: RL 算法利用最先进的机器学习库，这些库需要一些复杂的硬件。为了跟随本书中我们将解决的示例，你需要设置你的计算机环境。接下来我们将介绍你在设置过程中所需的硬件和软件。
- en: Hardware requirements
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件要求
- en: 'As mentioned previously, state-of-the-art RL models are usually trained on
    hundreds of GPUs and thousands of CPUs. We certainly don''t expect you to have
    access to those resources. However, having multiple CPU cores will help you simultaneously
    simulate many agents and environments to collect data more quickly. Having a GPU will
    speed up training deep neural networks that are used in modern RL algorithms. In
    addition, to be able to efficiently process all that data, having enough RAM resources
    is important. But don''t worry; work with what you have, and you will still get
    a lot out of this book. For your reference, here are some specifications of the
    desktop we used to run the experiments:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，最先进的 RL 模型通常在数百个 GPU 和数千个 CPU 上进行训练。我们当然不期望你能访问这些资源。然而，拥有多个 CPU 核心将帮助你同时模拟多个智能体和环境，更快地收集数据。拥有一个
    GPU 将加速训练现代 RL 算法中使用的深度神经网络。此外，为了能够高效处理所有这些数据，拥有足够的内存资源也很重要。但不用担心，利用你现有的硬件，你仍然能够从本书中获得大量的知识。供参考，以下是我们用来运行实验的桌面配置：
- en: AMD Ryzen Threadripper 2990WX CPU with 32 cores
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AMD Ryzen Threadripper 2990WX CPU，拥有 32 核
- en: NVIDIA GeForce RTX 2080 Ti GPU
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA GeForce RTX 2080 Ti GPU
- en: 128 GB RAM
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 128 GB 内存
- en: 'As an alternative to building a desktop with expensive hardware, you can use **Virtual
    Machines** (**VMs**) with similar capabilities provided by various companies.
    The most famous ones are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作为构建带有昂贵硬件的桌面的替代方案，你可以使用由各家公司提供的具有相似能力的**虚拟机**（**VMs**）。最著名的几个如下：
- en: Amazon's AWS
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊 AWS
- en: Microsoft Azure
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Azure
- en: Google Cloud Platform
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云平台
- en: These cloud providers also provide data science images for your VMs during the
    setup and it saves the user from installing the necessary software for deep learning
    (for example, CUDA, TensorFlow, and so on). They also provide detailed guidelines
    on how to set up your VMs to which we will defer the details of the setup.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些云服务提供商还为你的虚拟机（VM）提供数据科学镜像，这样用户就不需要安装深度学习所需的软件（例如 CUDA、TensorFlow 等）。他们还提供了详细的指南，说明如何设置你的虚拟机，关于设置的细节我们将在后面讨论。
- en: A final option that would allow small-scale deep learning experiments with TensorFlow
    is Google's Colab, which provides VM instances readily accessible from your browser
    with the necessary software installed. You can start experimenting on a Jupyter
    Notebook-like environment right away, which is a very convenient option for quick
    experimentation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个最终选项是谷歌的 Colab，它可以让你在 TensorFlow 上进行小规模的深度学习实验。Colab 提供的虚拟机实例可以直接从浏览器访问，且所需的软件已经安装。你可以立即在类似
    Jupyter Notebook 的环境中开始实验，这是一个非常方便的快速实验选项。
- en: Operating system
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作系统
- en: When you develop data science models for educational purposes, there is often
    not a lot of difference between Windows, Linux, or macOS. However, we plan to
    do a bit more than that in this book, with advanced RL libraries running on a
    GPU. This setting is best supported on Linux, of which we use the Ubuntu 18.04.3
    LTS distribution. Another option is macOS, but that often does not come with a
    GPU on the machine. Finally, although the setup could be a bit convoluted, **Windows
    Subsystem for Linux** (**WSL**) 2 is an option you could explore.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当你为教育目的开发数据科学模型时，Windows、Linux 或 macOS 之间通常没有太大区别。然而，在本书中，我们计划做的事情不仅仅如此，还会使用运行在
    GPU 上的高级 RL 库。这个环境在 Linux 上得到最好的支持，我们使用的是 Ubuntu 18.04.3 LTS 发行版。另一个选择是 macOS，但它通常没有
    GPU。最后，尽管设置可能有些复杂，**Windows 子系统 Linux** (**WSL**) 2 是你可以探索的一个选项。
- en: Software toolbox
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件工具箱
- en: One of the first things people do when setting up the software environment for
    data science projects is to install **Anaconda**, which gives you a Python platform,
    along with many useful libraries.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 设置数据科学项目的软件环境时，人们通常首先安装 **Anaconda**，它为你提供了一个 Python 平台，并附带了许多有用的库。
- en: Tip
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The CLI tool called `virtualenv` is a lighter-weight tool compared to Anaconda
    to create virtual environments for Python, and preferable in most production environments.
    We, too, will use it in certain chapters. You can find the installation instructions
    for `virtualenv` at [https://virtualenv.pypa.io/en/latest/installation.html](https://virtualenv.pypa.io/en/latest/installation.html).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`virtualenv` 是一个比 Anaconda 更轻量的工具，用于创建 Python 虚拟环境，并且在大多数生产环境中更为推荐。我们也将在某些章节中使用它。你可以在
    [https://virtualenv.pypa.io/en/latest/installation.html](https://virtualenv.pypa.io/en/latest/installation.html)
    找到 `virtualenv` 的安装说明。'
- en: 'We will particularly need the following packages:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别需要以下几个包：
- en: '**Python 3.7**: Python is the *lingua franca* of data science today. We will
    use version 3.7.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 3.7**：Python 是今天数据科学的*共同语言*。我们将使用版本 3.7。'
- en: '**NumPy**: This is one of the most fundamental libraries used in scientific
    computing in Python.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：这是 Python 中用于科学计算的最基础的库之一。'
- en: '`pandas` is a widely used library that provides powerful data structures and
    analysis tools.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 是一个广泛使用的库，提供强大的数据结构和分析工具。'
- en: '**Jupyter Notebook**: This is a very convenient tool to run Python code, especially
    for small-scale tasks. It usually comes with your Anaconda installation by default.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**：这是一个非常方便的工具，特别适合运行小规模的 Python 代码任务。它通常默认随 Anaconda 安装一起提供。'
- en: '**TensorFlow 2.x**: This will be our choice as the deep learning framework.
    We use version 2.3.0 in this book. Occasionally, we will refer to repos that use
    TensorFlow 1.x as well.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 2.x**：这是我们选择的深度学习框架。在本书中，我们使用的是版本 2.3.0。有时，我们也会提到使用 TensorFlow
    1.x 的仓库。'
- en: '**Ray and RLlib**: Ray is a framework for building and running distributed
    applications and it is getting increasingly popular. RLlib is a library running
    on Ray that includes many popular RL algorithms. At the time of writing this book,
    Ray supports only Linux and macOS for production, and Windows support is in the
    alpha phase. We will use version 0.8.7\.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray 和 RLlib**：Ray 是一个用于构建和运行分布式应用的框架，正在越来越受欢迎。RLlib 是一个运行在 Ray 上的库，包含许多流行的
    RL 算法。在编写本书时，Ray 只支持 Linux 和 macOS 进行生产环境部署，Windows 支持仍处于 Alpha 阶段。我们将使用版本 0.8.7。'
- en: '**gym**: This is an RL framework created by OpenAI that you have probably interacted
    with before if you have ever touched RL. It allows us to define RL environments
    in a standard way and let them communicate with algorithms in packages such as
    RLlib.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gym**：这是一个由 OpenAI 创建的 RL 框架，如果你曾接触过 RL，你可能已经使用过它。它允许我们以标准方式定义 RL 环境，并让它们与像
    RLlib 这样的算法包进行交互。'
- en: '**OpenCV Python bindings**: We need this for some image processing tasks.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenCV Python 绑定**：我们需要这个库来进行一些图像处理任务。'
- en: '`Cufflinks` package to bind it to `pandas`.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cufflinks` 包将它绑定到 `pandas`。'
- en: 'You can use one of the following commands on your terminal to install a specific
    package. With Anaconda, we use the following command:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在终端中使用以下命令来安装特定的包。使用 Anaconda 时，我们使用以下命令：
- en: '[PRE0]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With `virtualenv` (also works with Anaconda in most cases), we use the following
    command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `virtualenv`（在大多数情况下也适用于 Anaconda），我们使用以下命令：
- en: '[PRE1]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sometimes, you are flexible with the version of the package, in which case you
    can omit the equals sign and what comes after.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你可以灵活选择包的版本，在这种情况下，你可以省略等号及其后面的内容。
- en: Tip
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: It is always a good idea to create a virtual environment specific to your experiments
    for this book and install all these packages in that environment. This way, you
    will not break dependencies for your other Python projects. There is comprehensive
    online documentation on how to manage your environments provided by Anaconda,
    available at [https://bit.ly/2QwbpJt](https://bit.ly/2QwbpJt).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实验的顺利进行，最好为本书创建一个专门的虚拟环境，并在该环境中安装所有相关的包。这样，你就不会破坏其他Python项目的依赖性。Anaconda提供了全面的在线文档，指导你如何管理环境，文档地址：[https://bit.ly/2QwbpJt](https://bit.ly/2QwbpJt)。
- en: That's it! With that, you are ready to start coding RL!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！有了这些，你就准备好开始编写强化学习（RL）代码了！
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This was our refresher on RL fundamentals! We began this chapter by discussing
    what RL is, and why it is such a hot topic and the next frontier in AI. We talked
    about some of the many possible applications of RL and the success stories that
    have made it to the news headlines over the past several years. We also defined
    the fundamental concepts we will use throughout the book. Finally, we covered
    the hardware and software you need to run the algorithms we will introduce in
    the next chapters. Everything so far was to refresh your mind about RL, motivate,
    and set you up for what is upcoming next: implementing advanced RL algorithms
    to solve challenging real-world problems. In the next chapter, we will dive right
    into it with multi-armed bandit problems, an important class of RL algorithms
    that has many applications in personalization and advertising.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们对强化学习基础知识的复习！我们从讨论强化学习是什么，以及为什么它是如此热门并成为人工智能的下一前沿话题开始。我们还探讨了强化学习的多种应用及其在过去几年成为新闻头条的成功案例。我们还定义了将在本书中使用的基本概念。最后，我们介绍了你需要的硬件和软件，以运行我们将在接下来的章节中介绍的算法。到目前为止的内容，都是为了让你刷新强化学习的记忆，激励你，并为接下来的内容做好准备：实现先进的强化学习算法，解决具有挑战性的实际问题。在下一章，我们将直接进入多臂赌博机问题，它是强化学习算法中的一个重要类别，在个性化和广告领域有着广泛的应用。
- en: References
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sutton, R. S., Barto, A. G. (2018). Reinforcement Learning: An Introduction. *The
    MIT Press*.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutton, R. S., Barto, A. G. (2018). 强化学习：导论。*MIT出版社*。
- en: Tesauro, G. (1992). Practical issues in temporal difference learning. *Machine
    Learning 8, 257–277*.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tesauro, G. (1992). 时序差分学习中的实际问题。*Machine Learning 8, 257–277*。
- en: Tesauro, G. (1995). Temporal difference learning and TD-Gammon. *Commun. ACM 38,
    3, 58-68*.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tesauro, G. (1995). 时序差分学习与TD-Gammon。*Commun. ACM 38, 3, 58-68*。
- en: Silver, D. (2018). Success Stories of Deep RL. Retrieved from [https://youtu.be/N8_gVrIPLQM](https://youtu.be/N8_gVrIPLQM).
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Silver, D. (2018). 深度强化学习的成功案例。来源：[https://youtu.be/N8_gVrIPLQM](https://youtu.be/N8_gVrIPLQM)。
- en: Crites, R. H., Barto, A.G. (1995). Improving elevator performance using reinforcement
    learning. *In Proceedings of the 8th International Conference on Neural Information
    Processing Systems (NIPS'95)*.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Crites, R. H., Barto, A.G. (1995). 使用强化学习提高电梯性能。*在第8届国际神经信息处理系统会议论文集（NIPS'95）中*。
- en: Mnih, V. et al. (2015). Human-level control through deep reinforcement learning. *Nature,
    518(7540), 529–533*.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mnih, V. 等人 (2015). 通过深度强化学习实现人类级控制。*Nature, 518(7540), 529–533*。
- en: Silver, D. et al. (2018). A general reinforcement learning algorithm that masters
    chess, shogi, and Go through self-play. *Science, 362(6419), 1140–1144*.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Silver, D. 等人 (2018). 一种通用的强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋。*Science, 362(6419), 1140–1144*。
- en: Vinyals, O. et al. (2019). Grandmaster level in StarCraft II using multi-agent
    reinforcement learning. *Nature*.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vinyals, O. 等人 (2019). 通过多智能体强化学习在《星际争霸II》上达到大师级水平。*Nature*。
- en: OpenAI. (2018). OpenAI Five. Retrieved from [https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/).
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI. (2018). OpenAI Five。来源：[https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/)。
- en: Heess, N. et al. (2017). Emergence of Locomotion Behaviours in Rich Environments. *ArXiv,
    abs/1707.02286*.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Heess, N. 等人 (2017). 在丰富环境中出现的运动行为。*ArXiv, abs/1707.02286*。
- en: OpenAI et al. (2018). Learning Dexterous In-Hand Manipulation. *ArXiv, abs/1808.00177*.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI 等人 (2018). 学习灵巧的手内操作。*ArXiv, abs/1808.00177*。
- en: OpenAI et al. (2019). Solving Rubik's Cube with a Robot Hand. *ArXiv, abs/1910.07113*.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI 等人 (2019). 用机器人手解决魔方。*ArXiv, abs/1910.07113*。
- en: OpenAI Blog (2019). Solving Rubik's Cube with a Robot Hand. Retrieved from [https://openai.com/blog/solving-rubiks-cube/](https://openai.com/blog/solving-rubiks-cube/).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI 博客 (2019). 用机器人手解决魔方。来源：[https://openai.com/blog/solving-rubiks-cube/](https://openai.com/blog/solving-rubiks-cube/)。
- en: 'Zheng, G. et al. (2018). DRN: A Deep Reinforcement Learning Framework for News
    Recommendation. *In Proceedings of the 2018 World Wide Web Conference (WWW ''18).
    International World Wide Web Conferences Steering Committee, Republic and Canton
    of Geneva, CHE, 167–176\. DOI:* [https://doi.org/10.1145/3178876.3185994](https://doi.org/10.1145/3178876.3185994).'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zheng, G. 等（2018）。DRN：一种用于新闻推荐的深度强化学习框架。*2018年全球信息网大会论文集（WWW '18）*。国际全球信息网会议指导委员会，瑞士日内瓦，167–176。DOI：[https://doi.org/10.1145/3178876.3185994](https://doi.org/10.1145/3178876.3185994)。
- en: Chandrashekar, A. et al. (2017). Artwork Personalization at Netflix. *The Netflix
    Tech Blog*. Retrieved from [https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76](https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76).
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chandrashekar, A. 等（2017）。Netflix中的艺术作品个性化。*The Netflix Tech Blog*。取自 [https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76](https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76)。
- en: McKinney, S. M. et al. (2020). International evaluation of an AI system for
    breast cancer screening. *Nature, 89-94*.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: McKinney, S. M. 等（2020）。国际评估用于乳腺癌筛查的人工智能系统。*Nature, 89-94*。
- en: Agrawal, R. (2018, March 8). *Microsoft News Center India*. Retrieved from [https://news.microsoft.com/en-in/features/microsoft-ai-network-healthcare-apollo-hospitals-cardiac-disease-prediction/](https://news.microsoft.com/en-in/features/microsoft-ai-network-healthcare-apollo-hospitals-cardiac-disease-prediction/).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Agrawal, R.（2018年3月8日）。*Microsoft News Center India*。取自 [https://news.microsoft.com/en-in/features/microsoft-ai-network-healthcare-apollo-hospitals-cardiac-disease-prediction/](https://news.microsoft.com/en-in/features/microsoft-ai-network-healthcare-apollo-hospitals-cardiac-disease-prediction/)。
