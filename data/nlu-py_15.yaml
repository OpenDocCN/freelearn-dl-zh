- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Summary and Looking to the Future
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与展望
- en: In this chapter, we will get an overview of the book and a look into the future.
    We will discuss where there is potential for improvement in performance as well
    as faster training, more challenging applications, and future directions for practical
    systems and research.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将概览本书内容并展望未来。我们将讨论在提高性能、加速训练、更具挑战性的应用程序以及实践系统和研究的未来方向方面的潜力。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of the book
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书概述
- en: Potential for better accuracy and faster training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高准确性和加速训练的潜力
- en: Other areas for improvement
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他改进领域
- en: Applications that are beyond the current state of the art
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越当前技术水平的应用
- en: Future directions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来方向
- en: The first section of this chapter is an overall summary of the topics covered
    in this book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分是本书所涉及主题的整体总结。
- en: Overview of the book
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书概述
- en: This book has covered the basics of **natural language understanding** (**NLU**),
    the technology that enables computers to process natural language and apply the
    results to a wide variety of practical applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍了自然语言理解（**NLU**）的基础，这项技术使计算机能够处理自然语言，并将结果应用于各种实际应用中。
- en: The goal of this book has been to provide a solid grounding in NLU using the
    Python programming language. This grounding will enable you not only to select
    the right tools and software libraries for developing your own applications but
    will also provide you with the background you need to independently make use of
    the many resources available on the internet. You can use these resources to expand
    your knowledge and skills as you take on more advanced projects and to keep up
    with the many new tools that are becoming available as this rapidly advancing
    technology continues to improve.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是通过使用Python编程语言提供坚实的自然语言理解（NLU）基础。这一基础不仅能帮助你选择正确的工具和软件库来开发自己的应用程序，还将为你提供所需的背景知识，使你能够独立使用互联网上的各种资源。你可以利用这些资源来扩展你的知识和技能，在进行更高级的项目时跟上快速发展的技术，并保持对新工具的关注，因为这些工具随着技术进步而不断涌现。
- en: 'In this book, we’ve discussed three major topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讨论了三个主要主题：
- en: In *Part 1*, we covered background information and how to get started
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第1部分*，我们讨论了背景信息以及如何开始
- en: In *Part 2*, we went over Python tools and techniques for accomplishing NLU
    tasks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第2部分*，我们讲解了实现NLU任务的Python工具和技术
- en: In *Part 3*, we discussed some practical considerations having to do with managing
    deployed applications
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第3部分*，我们讨论了一些与管理已部署应用程序相关的实际考虑因素
- en: 'Throughout this book, we have taken a step-by-step approach through the typical
    stages of an NLU project, starting from initial ideas through development, testing,
    and finally, to fine-tuning a deployed application. We can see these steps graphically
    in *Figure 15**.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们通过逐步的方法介绍了NLU项目的典型阶段，从初步构想到开发、测试，最后到对已部署应用程序的微调。我们可以在*图15.1*中以图形方式查看这些步骤：
- en: '![Figure 15.1 – The NLU project life cycle as covered in this book](img/B19005_15_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图15.1 – 本书所涵盖的自然语言理解（NLU）项目生命周期](img/B19005_15_01.jpg)'
- en: Figure 15.1 – The NLU project life cycle as covered in this book
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – 本书所涵盖的自然语言理解（NLU）项目生命周期
- en: In *Part 1*, you were introduced to the general topic of NLU and the kinds of
    tasks to which it can be applied.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第1部分*，你了解了自然语言理解（NLU）的一般主题以及它可以应用的任务类型。
- en: In *Part 2*, we started by covering foundational topics that support the most
    successful NLU applications, such as software development tools, data, visualization,
    and approaches to representing NLU data. The second general topic that we covered
    in *Part 2* was a set of five different approaches to processing language, including
    rules, traditional machine learning techniques, neural networks, transformers,
    and unsupervised learning. These topics are covered in the five chapters from
    [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159) through [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217),
    which covered the basics of NLU algorithms. Mastering this material will give
    you the background that you need to continue exploring NLU algorithms beyond the
    topics covered in this book. The final topic in *Part 2* was the very important
    subject of evaluation. Evaluation is critically important both for practical NLU
    deployments and for successful academic research. Our review of evaluation in
    [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), covers a variety of the most
    important NLU evaluation tools. With this background, you should now be prepared
    to evaluate your own NLU projects.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2部分*中，我们首先讨论了支持最成功的NLU应用的基础性主题，如软件开发工具、数据、可视化以及表示NLU数据的方法。我们在*第2部分*中覆盖的第二个主题是五种不同的语言处理方法，包括规则、传统机器学习技术、神经网络、变换器和无监督学习。这些主题在[*第8章*](B19005_08.xhtml#_idTextAnchor159)到[*第12章*](B19005_12.xhtml#_idTextAnchor217)的五个章节中进行了讨论，涵盖了NLU算法的基础。掌握这些内容将为你提供继续探索NLU算法所需的背景知识，超越本书所涵盖的内容。*第2部分*的最后一个主题是非常重要的评估问题。评估对于实际的NLU部署和成功的学术研究至关重要。我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中的评估回顾，涵盖了多种最重要的NLU评估工具。有了这些背景，你现在应该能够评估你自己的NLU项目。
- en: Finally, in *Part 3*, we turned to the topic of systems in action and focused
    (particularly in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248) on approaches
    to improving the performance of systems both before and after deployment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*第3部分*中，我们转向了系统应用的话题，并特别在[*第14章*](B19005_14.xhtml#_idTextAnchor248)中专注于提升系统性能的方法，涉及系统部署前后的改进措施。
- en: If you continue to work in the field of NLU, you will find that there are still
    many challenges, despite the fact that recent advances in **large language models**
    (**LLMs**) have dramatically improved the performance of NLU systems on many tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续从事NLU领域的工作，你会发现尽管近年来**大语言模型**（**LLMs**）在许多任务上显著提高了NLU系统的表现，但仍然面临许多挑战。
- en: We will look at some of these challenges in the next two sections, starting
    with the most important areas of improvement – better accuracy and faster training
    – followed by a section on other areas of improvement.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，我们将讨论其中的一些挑战，首先从最重要的改进领域——更好的准确性和更快的训练——开始，然后是关于其他改进领域的讨论。
- en: Potential for improvement – better accuracy and faster training
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进潜力——更好的准确性和更快的训练
- en: At the beginning of [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), we listed
    several criteria that can be used to evaluate NLU systems. The one that we usually
    think of first is accuracy – that is, given a specific input, did the system provide
    the right answer? Although in a particular application, we eventually may decide
    to give another criterion priority over accuracy, accuracy is essential.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第13章*](B19005_13.xhtml#_idTextAnchor226)的开始部分，我们列出了几个可以用来评估自然语言理解（NLU）系统的标准。我们通常首先想到的是准确性——也就是说，给定一个特定的输入，系统是否提供了正确的答案？尽管在某些特定应用中，我们可能最终会决定将另一个标准优先于准确性，但准确性仍然是至关重要的。
- en: Better accuracy
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的准确性
- en: 'As we saw in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), even our best-performing
    system, the large **Bidirectional Encoder Representations from Transformers**
    (**BERT**) model, only achieved an F1 score of *0.85* on the movie review dataset,
    meaning that 15% of its classifications were incorrect. State-of-the-art LLM-based
    research systems currently report an accuracy of *0.93* on this dataset, which
    still means that the system makes many errors (SiYu Ding, Junyuan Shang, Shuohuan
    Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021\. *ERNIE-Doc: A Retrospective
    Long-Document Modeling Transformer*), so we can see that there is still much room
    for accuracy improvements.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中看到的，即便是我们表现最好的系统——大型**双向编码器表示的变换器**（**BERT**）模型，在电影评论数据集上的F1得分也仅为*0.85*，这意味着有15%的分类是错误的。基于LLM的最新研究系统在该数据集上的准确率报告为*0.93*，但这仍然意味着系统会犯许多错误（SiYu
    Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang.
    2021\. *ERNIE-Doc: 一种回顾性长文档建模变换器*）。因此，我们可以看到，准确性的提升仍然有很大的空间。'
- en: 'LLMs represent the state of the art in NLU. However, there have been few formal
    studies of the accuracy of the most recent LLMs, so it is difficult to quantify
    how good they are. One study of a fairly challenging medical information task
    where physicians evaluated the accuracy of ChatGPT’s answers found that ChatGPT
    answers were considered to be largely accurate overall, receiving a mean score
    of *4.6* out of *6* (Johnson, D., et al. (2023). *Assessing the Accuracy and Reliability
    of AI-Generated Medical Responses: An Evaluation of the Chat-GPT Model*. Research
    Square, rs.3.rs-2566942\. [https://doi.org/10.21203/rs.3.rs-2566942/v1](https://doi.org/10.21203/rs.3.rs-2566942/v1)).
    However, the system still made many errors, and the authors cautioned that it
    was important for physicians to review medical advice supplied by ChatGPT or,
    in general, any LLMs at the current state of the art.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs代表了自然语言理解（NLU）领域的最新进展。然而，目前对最新LLMs的准确性进行的正式研究较少，因此很难量化它们的效果。有一项研究评估了一个相当具有挑战性的医学信息任务，在该任务中，医生评估了ChatGPT回答的准确性。结果发现，ChatGPT的回答总体上被认为是相当准确的，平均得分为*4.6*（满分为*6*）（Johnson,
    D., et al. (2023). *评估AI生成医学回应的准确性和可靠性：Chat-GPT模型评估*，Research Square, rs.3.rs-2566942\.
    [https://doi.org/10.21203/rs.3.rs-2566942/v1](https://doi.org/10.21203/rs.3.rs-2566942/v1)）。然而，该系统仍然存在许多错误，作者提醒医生在使用ChatGPT提供的医学建议时，或在当前技术水平下使用任何LLMs时，仍需谨慎，并对其进行审查。
- en: Better accuracy will always be a goal in NLU. Achieving better accuracy in future
    systems will include developing larger pretrained models as well as developing
    more effective fine-tuning techniques. In addition, there is a significant amount
    of work to be done in extending the current high performance of LLMs for the most
    widely studied languages to less well-studied languages.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的准确性始终是NLU领域的目标。未来系统中更高准确性的实现将包括开发更大规模的预训练模型以及更有效的微调技术。此外，扩展LLMs在最广泛研究语言上的高性能到研究较少的语言也需要大量的工作。
- en: Faster training
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更快的训练
- en: By working through some of the exercises in earlier chapters, you have found
    that training NLU models from scratch or fine-tuning an LLM did not take more
    than a few hours of computer time. For the purposes of this book, we intentionally
    selected smaller datasets so that you could get feedback from the training process
    more quickly. However, even larger problems that you may want to address in a
    practical setting should not take more than a few days of training time. On the
    other hand, training pretrained LLMs can take a very long time. One estimate for
    the training time for GPT-3 was that it took 355 GPU years to train on the 300
    billion token training dataset. In practice, the calendar time required was reduced
    by running the training on multiple GPUs in parallel ([https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)).
    Still, this training does involve tremendous amounts of computing power, along
    with the associated costs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成前几章的一些练习，你发现从零开始训练NLU模型或对LLM进行微调并不会花费超过几个小时的计算时间。为了本书的目的，我们故意选择了较小的数据集，以便你能更快地从训练过程中获得反馈。然而，即便是你可能在实际应用中需要解决的更大问题，训练时间也不应超过几天。另一方面，训练预训练LLM可能需要很长时间。有一个估计称，训练GPT-3的时间是355个GPU年，训练数据集包含3000亿个标记。在实践中，由于采用多GPU并行训练，所需的日历时间有所减少（[https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)）。然而，这种训练仍然涉及巨大的计算能力以及相关的成本。
- en: Since most pretrained models are trained by large organizations with extensive
    computing resources rather than by smaller organizations or researchers, long
    training times for large models don’t directly affect most of us because we will
    be using the pretrained models developed by large organizations. However, these
    long training times do affect us indirectly because long training times mean that
    it will take longer for new and improved models to be released for general use.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数预训练模型是由拥有大量计算资源的大型组织训练的，而不是由小型组织或研究人员训练的，因此大型模型的长时间训练并不会直接影响我们大多数人，因为我们将使用由大型组织开发的预训练模型。然而，这些长时间的训练间接地影响了我们，因为长时间的训练意味着新模型和改进模型需要更长时间才能发布供大众使用。
- en: In addition to better accuracy and faster training times, there are other areas
    where NLU technology can be improved. We will review some of these in the next
    section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更好的准确性和更快的训练时间外，NLU 技术还有其他可以改进的领域。我们将在下一节中回顾其中一些。
- en: Other areas for improvement
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他改进领域
- en: The areas for improvement that we’ll review in this section are primarily related
    to making NLU technology more practical in various ways, such as speeding up development
    and decreasing the number of computer resources needed during development and
    at runtime. These topics include smaller models, more explainability, and smaller
    amounts of fine-tuning data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中回顾的改进领域，主要与让 NLU 技术在多种方式上更具实用性相关，例如加速开发、减少开发和运行时所需的计算资源。这些话题包括更小的模型、更好的可解释性和更少的微调数据量。
- en: Smaller models
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更小的模型
- en: The BERT models we looked at in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193)
    and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), were relatively small. The
    reason for choosing these models was so that they could be downloaded and fine-tuned
    in a relatively short amount of time. However, as a rule of thumb, large models
    will be more accurate than smaller models. But we can’t always take advantage
    of large models because some models are too large to be fine-tuned on a single
    GPU, as pointed out on the TensorFlow site ([https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=dX8FtlpGJRE6](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=dX8FtlpGJRE6)).
    Because the larger models have better accuracy, it would be very helpful if high-accuracy
    performance could be obtained with smaller models. In addition, there are many
    situations where large models will not fit on resource-constrained devices such
    as mobile phones or even smartwatches. For those reasons, decreasing the size
    of models is an important goal in NLU research.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第 11 章*](B19005_11.xhtml#_idTextAnchor193)和[*第 13 章*](B19005_13.xhtml#_idTextAnchor226)中讨论的
    BERT 模型相对较小。选择这些模型的原因是它们可以在相对较短的时间内下载并进行微调。然而，作为经验法则，大型模型的准确性通常会比小型模型更高。但我们不能总是利用大型模型，因为有些模型过大，无法在单个
    GPU 上进行微调，正如 TensorFlow 网站指出的那样 ([https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=dX8FtlpGJRE6](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=dX8FtlpGJRE6))。由于较大的模型具有更好的准确性，如果能够在较小的模型上获得高准确性表现，将是非常有帮助的。此外，在许多情况下，大型模型无法适应资源有限的设备，例如手机甚至智能手表。因此，减少模型的大小是
    NLU 研究中的一个重要目标。
- en: Less data required for fine-tuning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调所需的更少数据
- en: For most NLU applications that use pretrained models, the pretrained model needs
    to be fine-tuned with application-specific data. We covered this process in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193) and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
    Clearly, reducing the amount of data required to fine-tune the system results
    in a reduction in the development time for the fine-tuned system. For example,
    in its discussion of the process of fine-tuning GPT-3, OpenAI states, “*The more
    training examples you have, the better. We recommend having at least a couple
    hundred examples. In general, we’ve found that each doubling of the dataset size
    leads to a linear increase in model quality*” ([https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)).
    As we learned in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), the process
    of finding and annotating data can be time-consuming and expensive, and it is
    clearly desirable to minimize this effort.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数使用预训练模型的NLU应用，预训练模型需要通过应用特定数据进行微调。我们在[*第11章*](B19005_11.xhtml#_idTextAnchor193)和[*第13章*](B19005_13.xhtml#_idTextAnchor226)中讨论过这一过程。显然，减少微调系统所需的数据量将缩短微调系统的开发时间。例如，在讨论如何微调GPT-3的过程中，OpenAI提到，“*你拥有的训练示例越多越好。我们建议至少准备几百个示例。通常，我们发现每次数据集大小翻倍都会带来模型质量的线性提升*”([https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning))。正如我们在[*第5章*](B19005_05.xhtml#_idTextAnchor107)中学到的，寻找和标注数据的过程可能既耗时又昂贵，因此显然希望最小化这一工作量。
- en: Explainability
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性
- en: For the most part, a result from an NLU system based on machine learning will
    simply be a number, such as a probability that a text falls into one of the categories
    on which the model was trained. We don’t have an easy way to understand how the
    system came up with that answer, whether the answer is correct or incorrect. If
    the answer is incorrect, we can try to improve the model by adding more data,
    adjusting the hyperparameters, or using some of the other techniques that we reviewed
    in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248), but it's hard to understand
    exactly why the system came up with the wrong answer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，基于机器学习的自然语言理解（NLU）系统的结果通常只是一个数字，例如文本属于模型训练时所设定类别的概率。我们没有简便的方法来理解系统是如何得出这个答案的，也无法判断答案是正确还是错误。如果答案是错误的，我们可以通过添加更多的数据、调整超参数，或者使用我们在[*第14章*](B19005_14.xhtml#_idTextAnchor248)中回顾的一些其他技术来尝试改善模型，但我们很难理解系统究竟为什么会得出错误的答案。
- en: In contrast, if a rule-based system such as those we went over in [*Chapter
    8*](B19005_08.xhtml#_idTextAnchor159) makes an error, it can normally be traced
    back to an incorrect rule, which can be fixed. However, since nearly all current
    systems are based on machine learning approaches rather than rules, it is very
    difficult to understand how they arrive at the answers that they do. Nevertheless,
    it is often important for users to understand how a system came up with a result.
    If the users don’t understand how the system came up with an answer, they might
    not trust the system. If a system gives a wrong answer or even a correct answer
    that the user doesn’t understand, it can undermine user confidence in the system.
    For that reason, explainability in NLU and in AI, in general, is an important
    research topic. You can read more about this topic at [https://en.wikipedia.org/wiki/Explainable_artificial_intelligence](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果是基于规则的系统（例如我们在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中讨论过的那些）发生错误，通常可以追溯到某条不正确的规则，并对其进行修正。然而，由于几乎所有当前的系统都基于机器学习方法，而不是基于规则，因此很难理解它们是如何得出当前答案的。然而，用户通常需要理解系统是如何得出结果的。如果用户不了解系统是如何得出答案的，他们可能不会信任该系统。如果系统给出了错误的答案，或者给出了用户无法理解的正确答案，都可能会削弱用户对系统的信任。因此，NLU和AI领域的可解释性是一个重要的研究课题。你可以在[https://en.wikipedia.org/wiki/Explainable_artificial_intelligence](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)上阅读更多相关内容。
- en: Timeliness of information
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息的时效性
- en: In [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248), we discussed how changes
    in the deployment context can result in system errors. The introduction of new
    product names, new movies, or even significant news events can lead to the system
    not knowing the answer to a user’s question or even giving the wrong answer. Because
    LLMs take so long to train, they are especially vulnerable to making errors due
    to a change in the deployment context.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 14 章*](B19005_14.xhtml#_idTextAnchor248)中，我们讨论了部署环境的变化如何导致系统错误。新产品名称、新电影，甚至是重大新闻事件的引入，可能导致系统无法回答用户的问题，甚至给出错误的答案。由于
    LLM 的训练时间非常长，它们特别容易因部署环境的变化而犯错。
- en: For example, the ChatGPT system has a knowledge cutoff date of September 2021,
    which means that it doesn’t have any knowledge of events that occurred after that.
    Because of this, it can make mistakes like the one shown in *Figure 15**.2*, which
    states that the current monarch of the United Kingdom is Elizabeth II. This was
    true in September 2021, but it is no longer true.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ChatGPT 系统的知识截止日期是 2021 年 9 月，这意味着它无法了解此日期之后发生的事件。因此，它可能会像*图 15.2*中所示的那样犯错误，该图表示英国现任君主是伊丽莎白二世。这在
    2021 年 9 月是正确的，但现在已经不再正确。
- en: '![Figure 15.2 – ChatGPT answer to “who is the current monarch of the united
    kingdom”](img/B19005_15_02.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.2 – ChatGPT 对“现任英国君主是谁”问题的回答](img/B19005_15_02.jpg)'
- en: Figure 15.2 – ChatGPT answer to “who is the current monarch of the united kingdom”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2 – ChatGPT 对“现任英国君主是谁”问题的回答
- en: Although the ChatGPT system acknowledges that its information may be out of
    date, this lack of timeliness can lead to errors if something changes in the deployment
    context. If you are developing your own application and something changes in the
    deployment context, you can either retrain the system from scratch with the new
    data or add new data to your existing model. However, if you are using a cloud-based
    LLM, you should be aware that the information it provides can be out of date.
    Note that this cutoff period can vary between different LLMs. For example, the
    Google Bard system was able to correctly answer the question in *Figure 15**.2*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ChatGPT 系统承认其信息可能已经过时，但这种时效性不足可能导致错误，特别是在部署环境发生变化时。如果你在开发自己的应用，且部署环境有所变化，你可以选择从头开始使用新数据重新训练系统，或者将新数据添加到现有模型中。然而，如果你使用的是基于云的
    LLM，你应该意识到它提供的信息可能会过时。请注意，这个截止期在不同的 LLM 之间可能会有所不同。例如，Google Bard 系统能够正确回答*图 15.2*中的问题。
- en: If your application uses an LLM and requires access to accurate time-dependent
    information, you should verify that the system you’re using is being kept up to
    date by its developers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用使用 LLM 且需要访问准确的时间依赖信息，你应该验证你所使用的系统是否由其开发者保持更新。
- en: The next section talks about a few blue sky applications that may be possible
    in the future.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节讨论了几种未来可能实现的蓝天应用。
- en: Applications that are beyond the current state of the art
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越当前技术水平的应用
- en: This section talks about several applications that are not yet possible, but
    that are theoretically feasible. In some cases, they could probably be achieved
    if the right training data and computing resources were available. In other cases,
    they might require some new algorithmic insights. In all of these examples, it
    is very interesting to think about how these and other futuristic applications
    might be accomplished.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了几种目前尚不可行但理论上可行的应用。在某些情况下，如果有合适的训练数据和计算资源，这些应用可能能够实现。在其他情况下，可能需要一些新的算法性见解。在所有这些例子中，思考这些及其他未来的应用如何实现是非常有趣的。
- en: Processing very long documents
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理非常长的文档
- en: Current LLMs have relatively small limits on the length of documents (or prompts)
    they can process. For example, GPT-4 can only handle texts of up to 8,192 tokens
    ([https://platform.openai.com/docs/models/gpt-4](https://platform.openai.com/docs/models/gpt-4)),
    which is around 16 single-spaced pages. Clearly, this means that many existing
    documents can’t be fully analyzed with these cloud systems. If you are doing a
    typical classification task, you can train your own model, for example, with a
    **Term frequency-inverse document frequency** (**TF-IDF**) representation, but
    this is not possible with a pretrained model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的大语言模型（LLMs）在能够处理的文档（或提示）的长度上有相对较小的限制。例如，GPT-4只能处理最多8,192个标记（[https://platform.openai.com/docs/models/gpt-4](https://platform.openai.com/docs/models/gpt-4)），大约是16页单倍行距的文本。显然，这意味着许多现有的文档无法通过这些云系统进行全面分析。如果你正在进行典型的分类任务，你可以训练自己的模型，例如使用**词频-逆文档频率**（**TF-IDF**）表示法，但这在预训练模型中是不可能的。
- en: In that case, the documents can be as long as you like, but you will lose the
    advantages of LLMs. Research systems such as Longformer have been able to process
    much longer documents through more efficient use of computational resources. If
    you have a use case for processing long documents, it would be worth looking into
    some of these research systems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，文档可以长到你喜欢的长度，但你会失去大语言模型（LLMs）的优势。像Longformer这样的研究系统已经能够通过更高效地使用计算资源来处理更长的文档。如果你有处理长文档的需求，值得研究一些这些研究系统。
- en: Understanding and creating videos
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和创建视频
- en: To understand videos, a system would need to be able to interpret both the video
    and audio streams and relate them to each other. If the system learns someone’s
    name in the early part of the video and that character appears in a later part
    of the video, it should be able to name the person based on recognizing the image
    of the character. It could then do tasks such as transcribing the script of a
    movie by simply watching it, complete with notations like “Character X smiles”.
    This is not a very difficult task for humans, who are quite good at recognizing
    a person that they’ve seen before, but it would be very difficult for automated
    systems. While they are quite good at identifying people in images, they are less
    capable of identifying people in videos. In contrast to understanding videos,
    generating videos seems to be an easier task. For example, there are currently
    systems available that generate videos from text, such as a system developed by
    Meta ([https://ai.facebook.com/blog/generative-ai-text-to-video/](https://ai.facebook.com/blog/generative-ai-text-to-video/)),
    although the videos don’t yet look very good.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解视频，系统需要能够解释视频和音频流，并将它们联系起来。如果系统在视频的早期部分学习到某人的名字，而该角色在视频后期出现，它应该能够根据识别该角色的图像来命名此人。然后，它可以执行诸如通过简单地观看电影来转录剧本的任务，并带有诸如“角色X微笑”的注释。这对于人类来说不是很难，毕竟人类很擅长识别曾经见过的人，但对于自动化系统来说却是非常困难的。虽然它们在识别图像中的人物方面相当出色，但在视频中识别人物的能力较差。与理解视频相比，生成视频似乎是一个更容易的任务。例如，目前有一些系统可以从文本生成视频，如Meta公司开发的一个系统（[https://ai.facebook.com/blog/generative-ai-text-to-video/](https://ai.facebook.com/blog/generative-ai-text-to-video/)），尽管生成的视频效果还不太理想。
- en: Interpreting and generating sign languages
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释和生成手语
- en: One application of understanding videos would be to understand sign languages
    such as American Sign Language and translate them into spoken languages. Combined
    with the reverse process of translating spoken language into sign language, this
    kind of technology could greatly simplify communication between signers and speakers
    of spoken languages. There have been some exploratory studies of interpreting
    and generating sign languages.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 理解视频的一个应用是理解手语，如美国手语，并将其翻译成口语。结合将口语翻译成手语的反向过程，这种技术可以大大简化手语使用者与口语使用者之间的沟通。已经有一些探索性研究在解释和生成手语方面取得了进展。
- en: For example, the work on [https://abdulhaim.github.io/6.S198-Assignments/final_project.html](https://abdulhaim.github.io/6.S198-Assignments/final_project.html)
    describes an approach to interpreting Argentinian Sign Language using **Convolutional
    Neural Networks** (**CNNs**). Although this is an interesting proof of concept,
    it only works with 64 signs from Argentinian Sign Language. In fact, there are
    thousands of signs used in actual sign languages, so handling 64 signs is only
    a small demonstration of the possibility of automatically interpreting sign languages.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，关于[https://abdulhaim.github.io/6.S198-Assignments/final_project.html](https://abdulhaim.github.io/6.S198-Assignments/final_project.html)的研究描述了一种使用**卷积神经网络**（**CNNs**）解释阿根廷手语的方法。尽管这是一种有趣的概念验证，但它只适用于阿根廷手语中的64个手势。实际上，实际手语中使用的手势有数千种，因此仅处理64个手势只是自动翻译手语可能性的一个小小展示。
- en: In addition, this research only used hand positions to recognize signs, while,
    in fact, signs are also distinguished by other body positions. More work needs
    to be done to demonstrate practical automatic sign language interpretation. This
    would be greatly aided by the availability of more sign language datasets.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这项研究仅使用手部姿势来识别手语，而实际上，手语还可以通过其他身体姿势来区分。需要更多的工作来展示实用的自动手语翻译。这将通过更多手语数据集的可用性得到极大帮助。
- en: Writing compelling fiction
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写作引人入胜的小说
- en: If you have experimented with ChatGPT or other LLMs, you may have noticed that
    the writing style is rather bland and boring. This is because it’s based on text
    from the internet and other existing sources and there is no way for it to be
    creative beyond the data that it is trained on. On the other hand, compelling
    fiction is unique and often contains insights and verbal images that have never
    appeared in writing before.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试过使用ChatGPT或其他大型语言模型（LLMs），你可能已经注意到，它们的写作风格相当单调乏味。这是因为它们基于互联网上的文本和其他现有资源，无法超越其训练数据来进行创作。另一方面，引人入胜的小说是独特的，通常包含前所未见的见解和语言形象。
- en: 'As an example, let’s look at an excerpt from one of the great poems of the
    English language, To a Skylark, by Percy Bysshe Shelley, which can be seen in
    *Figure 15**.3*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，让我们来看一下英文学中一首伟大诗歌的摘录，《致云雀》，由珀西·比希·雪莱（Percy Bysshe Shelley）所作，这可以在*图15**.3*中看到：
- en: '![Figure 15.3 – An excerpt from “To a Skylark”, a poem by Percy Bysshe Shelley
    (1820)](img/B19005_15_03.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图15.3 – 珀西·比希·雪莱（Percy Bysshe Shelley）《致云雀》中的摘录（1820年）](img/B19005_15_03.jpg)'
- en: Figure 15.3 – An excerpt from “To a Skylark”, a poem by Percy Bysshe Shelley
    (1820)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 – 珀西·比希·雪莱（Percy Bysshe Shelley）《致云雀》中的摘录（1820年）
- en: This poem includes novel figures of speech such as the simile that compares
    a bird to *a cloud of fire* and uses the metaphor *the blue deep* for the sky,
    which are probably unique in literature.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这首诗包含了新颖的修辞手法，比如将鸟类比作*一团火焰*的明喻，并用*蔚蓝深处*的隐喻来指代天空，这在文学中可能是独一无二的。
- en: Compare this to the poem generated by ChatGPT in *Figure 15**.4*. When prompted
    to write a poem about a skylark flying in the sky, the result seems flat and unoriginal
    compared to the Shelley poem and includes cliches such as *boundless sky* and
    *ascends* *on high*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将这与*图15**.4*中由ChatGPT生成的诗歌进行比较。当被要求写一首关于云雀在天空中飞翔的诗时，与雪莱的诗相比，结果显得平淡且缺乏原创性，还包含了*无边的天空*和*高高升起*等陈词滥调。
- en: '![Figure 15.4 – ChatGPT poem about a skylark](img/B19005_15_04.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图15.4 – ChatGPT生成的云雀诗歌](img/B19005_15_04.jpg)'
- en: Figure 15.4 – ChatGPT poem about a skylark
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 – ChatGPT生成的云雀诗歌
- en: Let’s think about how we might train an LLM to learn how to generate good poetry
    or interesting fiction. If we follow the standard NLU development paradigm of
    learning a model from training data, for an NLU system to write compelling fiction,
    we would need a dataset consisting of text examples of compelling and engaging
    writing and other examples of writing that are not compelling or engaging. Alternatively,
    we might be able to identify other features of compelling writing (use verbs,
    avoid passive voice, etc.) that could be used to train systems to evaluate writing
    or to produce good writing. You can understand how far we are from this kind of
    application by thinking about what an NLU system would need to be able to do to
    write insightful book reviews. It would have to be familiar with the author’s
    other books, other books in a similar genre, any relevant historical events mentioned
    in the book, and even the author’s biography. Then it would have to be able to
    pull all this knowledge together into a concise analysis of the book. All of this
    seems quite difficult.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下如何训练一个大语言模型（LLM）来学习生成优美的诗歌或有趣的小说。如果我们遵循标准的自然语言理解（NLU）开发范式，通过训练数据来训练模型，那么为了让一个NLU系统能够写出引人入胜的小说，我们需要一个包含引人入胜且富有吸引力的写作示例的语料库，以及一些不具吸引力或不引人入胜的写作示例。或者，我们也许能够识别出引人入胜的写作的其他特点（比如使用动词、避免被动语态等），这些特点可以用来训练系统来评估写作或创作好的写作。通过思考一个NLU系统需要能够做什么才能写出深刻的书评，你可以理解我们距离这种应用有多远。它必须熟悉作者的其他书籍、同类题材的其他书籍、书中提到的任何相关历史事件，甚至作者的传记。然后，它必须能够将所有这些知识整合成一篇简明的书评。所有这些看起来都相当困难。
- en: The next section will look at applications that are targets of current research
    and are much closer to realization than the ones we’ve just reviewed. We are likely
    to see advances in these kinds of applications in the next couple of years.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨一些当前研究的应用，这些应用离实现比我们刚才讨论的更近。我们可能会在接下来的几年里看到这些应用的进展。
- en: Future directions in NLU technology and research
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLU技术和研究的未来方向
- en: While the recent improvements in NLU technology based on transformers and LLMs,
    which we reviewed in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193), have resulted
    in very impressive capabilities, it is important to point out that there are many
    topics in NLU that are far from solved. In this section, we will look at some
    of the most active research areas – extending NLU to new languages, speech-to-speech
    translation, multimodal interaction, and avoiding bias.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于变压器和大语言模型（LLM）的最近进展，已经带来了非常令人印象深刻的能力，正如我们在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中回顾的那样，但值得指出的是，NLU领域还有许多问题远未解决。在本节中，我们将讨论一些最活跃的研究领域——将NLU扩展到新语言、语音到语音的翻译、多模态交互和避免偏见。
- en: Quickly extending NLU technologies to new languages
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速将NLU技术扩展到新语言
- en: A precise count of the number of currently spoken languages is difficult to
    obtain. However, according to *WorldData.info*, there are currently about 6,500
    languages spoken throughout the world ([https://www.worlddata.info/languages/index.php#:~:text=There%20are%20currently%20around%206%2C500,of%20Asia%2C%20Australia%20and%20Oceania](https://www.worlddata.info/languages/index.php#:~:text=There%20are%20currently%20around%206%2C500,of%20Asia%2C%20Australia%20and%20Oceania)).
    Some languages, such as Mandarin, English, Spanish, and Hindi, are spoken by many
    millions of people, while other languages are spoken by very few people and these
    languages are even in danger of dying out (for example, you can see a list of
    the endangered languages of North America alone at [https://en.wikipedia.org/wiki/List_of_endangered_languages_in_North_America](https://en.wikipedia.org/wiki/List_of_endangered_languages_in_North_America)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 目前很难精确统计世界上有多少种语言在使用。然而，根据*WorldData.info*的数据，目前全球大约有6,500种语言（[https://www.worlddata.info/languages/index.php#:~:text=There%20are%20currently%20around%206%2C500,of%20Asia%2C%20Australia%20and%20Oceania](https://www.worlddata.info/languages/index.php#:~:text=There%20are%20currently%20around%206%2C500,of%20Asia%2C%20Australia%20and%20Oceania)）。一些语言，如普通话、英语、西班牙语和印地语，有数百万人在使用，而其他一些语言的使用者极少，这些语言甚至面临灭绝的风险（例如，你可以查看[https://en.wikipedia.org/wiki/List_of_endangered_languages_in_North_America](https://en.wikipedia.org/wiki/List_of_endangered_languages_in_North_America)上的北美濒危语言列表）。
- en: Languages with many millions of speakers tend to be more economically important
    than languages with few speakers, and as a consequence, NLU technology for those
    languages is generally much more advanced than that for languages with few speakers.
    If you recall from our discussion of LLMs in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193),
    training an LLM such as BERT or GPT-3 for one language is a very expensive and
    time-consuming process that requires enormous amounts of text data. It would be
    impractical for this training process to be carried out for thousands of languages.
    For that reason, researchers have looked into adapting LLMs used for widely spoken
    languages to less widely spoken languages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 说话人数众多的语言往往比说话人数较少的语言更具经济重要性，因此，针对这些语言的自然语言理解（NLU）技术通常比针对说话人数较少的语言的技术更加先进。如果你还记得我们在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中关于大语言模型（LLMs）的讨论，训练一个如BERT或GPT-3这样的LLM来处理某种语言是一个非常昂贵且耗时的过程，需要大量的文本数据。为了成千上万种语言进行这样的训练是不现实的。因此，研究人员开始探索如何将用于广泛使用语言的LLMs适配到不那么广泛使用的语言。
- en: This is a very active research area that presents many challenges to NLU technology.
    One challenge, for example, is how to keep the original language from being forgotten
    when its language model is adapted to a new language – a process called **catastrophic
    forgetting**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常活跃的研究领域，给自然语言理解技术带来了许多挑战。一个挑战是，比如如何在将语言模型适配到新语言时避免原始语言被遗忘——这个过程被称为**灾难性遗忘**。
- en: Citation
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 引用
- en: 'This is an example of a recent research paper on this topic that you can read
    for more insight into the problem of adapting LLMs to new languages:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于该主题的近期研究论文示例，你可以阅读这篇论文以更深入了解将大语言模型（LLMs）适配到新语言的问题：
- en: 'Cahyawijaya, S., Lovenia, H., Yu, T., Chung, W., & Fung, P. (2023). *Instruct-Align:
    Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction*.
    arXiv preprint arXiv:2305.13627\. [https://arxiv.org/abs/2305.13627](https://arxiv.org/abs/2305.13627).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cahyawijaya, S., Lovenia, H., Yu, T., Chung, W., & Fung, P. (2023). *Instruct-Align:
    通过基于对齐的跨语言指令教导LLMs新语言*. arXiv预印本arXiv:2305.13627\. [https://arxiv.org/abs/2305.13627](https://arxiv.org/abs/2305.13627).'
- en: Real-time speech-to-speech translation
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时语音到语音翻译
- en: Anyone who has traveled to a foreign country whose language they do not know,
    or whose language they do not know well, has probably found communication very
    frustrating. Looking up words or even phrases in handheld apps or paper dictionaries
    is slow and can be inaccurate. A much better solution would be speech-to-speech
    translation. Speech-to-speech translation technology listens to speech in one
    language, translates it to another language, and the system speaks the output
    in a second language, which would be much faster than typing words into a mobile
    app.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 任何去过一个自己不懂或不熟悉语言的外国的人，可能都会觉得沟通非常困难。在手持设备的应用程序或纸质词典中查找单词甚至短语非常慢，并且可能不准确。一个更好的解决方案是语音到语音翻译。语音到语音翻译技术可以听取一种语言的语音，翻译成另一种语言，并将输出以第二种语言发音，这比在移动应用程序中输入单词要快得多。
- en: The base technologies underlying **speech-to-speech translation** are actually
    fairly advanced. For example, Microsoft Cognitive Services offers a speech-to-speech
    translation service ([https://azure.microsoft.com/en-us/products/cognitive-services/speech-translation/](https://azure.microsoft.com/en-us/products/cognitive-services/speech-translation/))
    with support for over 30 languages. The number of available language pairs continues
    to increase – for example, Speechmatics offers a translation service for 69 language
    pairs ([https://www.speechmatics.com/product/translation](https://www.speechmatics.com/product/translation)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 支撑**语音到语音翻译**的基础技术实际上已经相当先进。例如，微软认知服务提供语音到语音翻译服务（[https://azure.microsoft.com/en-us/products/cognitive-services/speech-translation/](https://azure.microsoft.com/en-us/products/cognitive-services/speech-translation/)），支持超过30种语言。可用的语言对数量还在不断增加——例如，Speechmatics提供69种语言对的翻译服务（[https://www.speechmatics.com/product/translation](https://www.speechmatics.com/product/translation)）。
- en: However, most of these services do their processing in the cloud. Because one
    of the most important use cases for speech-to-speech translation is for travel,
    users may not want to use a service that requires access to the cloud. They may
    not have a good internet connection, or they may not want to pay for data while
    traveling. It is much more difficult to translate speech offline, without sending
    it to the cloud, as mobile devices have far fewer computing resources than the
    cloud. The results are less accurate and not nearly as many languages are supported.
    For example, the Apple Translate app ([https://apps.apple.com/app/translate/id1514844618](https://apps.apple.com/app/translate/id1514844618))
    claims to support 30 languages but the reviews are very low, especially for offline
    use. There is significant room for improvement in the technology for offline speech-to-speech
    translation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些服务中的大多数在云端进行处理。由于语音到语音翻译的一个最重要的应用场景是旅行，用户可能不想使用需要访问云端的服务。他们可能没有良好的互联网连接，或者在旅行时不想为数据付费。在不将语音发送到云端的情况下，离线翻译语音要困难得多，因为移动设备的计算资源远不如云端。结果往往不够准确，支持的语言种类也远远较少。例如，Apple
    Translate 应用程序（[https://apps.apple.com/app/translate/id1514844618](https://apps.apple.com/app/translate/id1514844618)）声称支持
    30 种语言，但用户评价很低，尤其是在离线使用时。离线语音到语音翻译的技术仍有很大的改进空间。
- en: Multimodal interaction
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态交互
- en: '**Multimodal interaction** is a type of user-system interaction where the user
    interacts with a computer system in multiple ways (*modalities*) in addition to
    language. For example, multimodal interaction could include camera input that
    allows the system to interpret facial expressions in addition to speech input.
    This would let the system read the user’s body language to detect emotions such
    as happiness or confusion in addition to interpreting what the user says. As well
    as understanding multimodal user inputs, a multimodal system can also produce
    images, animations, videos, and graphics in addition to language in order to respond
    to users’ questions.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**多模态交互**是一种用户与系统交互的方式，其中用户除了语言之外，还通过多种方式（*模态*）与计算机系统进行交互。例如，多模态交互可以包括摄像头输入，使系统能够在语音输入的基础上解释面部表情。这将使系统能够读取用户的肢体语言，从而感知诸如快乐或困惑等情绪，除了理解用户所说的话之外。此外，除了语言，支持多模态输入的系统还可以生成图像、动画、视频和图形，以回应用户的问题。'
- en: Citation
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 引用
- en: 'Multimodal interaction has been used extensively in research projects such
    as the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态交互已广泛应用于以下研究项目：
- en: António Teixeira, Annika Hämäläinen, Jairo Avelar, Nuno Almeida, Géza Németh,
    Tibor Fegyó, Csaba Zainkó, Tamás Csapó, Bálint Tóth, André Oliveira, Miguel Sales
    Dias, *Speech-centric Multimodal Interaction for Easy-to-access Online Services
    – A Personal Life Assistant for the Elderly*, Procedia Computer Science, Volume
    27, 2014, Pages 389-397, ISSN 1877-0509, [https://doi.org/10.1016/j.procs.2014.02.043](https://doi.org/10.1016/j.procs.2014.02.043).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: António Teixeira, Annika Hämäläinen, Jairo Avelar, Nuno Almeida, Géza Németh,
    Tibor Fegyó, Csaba Zainkó, Tamás Csapó, Bálint Tóth, André Oliveira, Miguel Sales
    Dias, *面向老年人的便捷在线服务的语音中心多模态交互——个人生活助手*, 《计算机科学程序集》, 第27卷, 2014年, 页码389-397, ISSN
    1877-0509, [https://doi.org/10.1016/j.procs.2014.02.043](https://doi.org/10.1016/j.procs.2014.02.043)。
- en: However, multimodal interaction is far from widespread in practical applications.
    This may be due in part to the relative scarcity of training data for multimodal
    systems since training multimodal systems requires data for all the modalities
    being used in the system, not just language data. For example, if we wanted to
    develop an application that used a combination of facial expression recognition
    and NLU to understand users’ emotions, we would need a video dataset annotated
    with both facial expressions and NLU categories. There are a few existing datasets
    with this kind of information – for example, the datasets listed at [https://www.datatang.ai/news/60](https://www.datatang.ai/news/60)
    – but they are not nearly as abundant as the text datasets that we’ve been working
    with throughout this book. Multimodal interaction is a very interesting topic,
    and the availability of additional data will certainly stimulate some future groundbreaking
    work.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多模态交互在实际应用中仍然远未普及。这可能部分由于多模态系统的训练数据相对匮乏，因为训练多模态系统需要为系统中使用的所有模态提供数据，而不仅仅是语言数据。例如，如果我们想开发一个结合面部表情识别和自然语言理解（NLU）来理解用户情绪的应用程序，我们需要一个同时标注有面部表情和NLU类别的视频数据集。现有的一些数据集包含这类信息——例如，[https://www.datatang.ai/news/60](https://www.datatang.ai/news/60)
    列出的数据集——但它们远没有我们在本书中使用的文本数据集那样丰富。多模态交互是一个非常有趣的话题，额外数据的可用性无疑会激发未来一些突破性的研究工作。
- en: Detecting and correcting bias
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见的检测与纠正
- en: The training data for LLMs is based on existing text, primarily from the web.
    This text, in many cases, reflects cultural biases that we would not like to perpetuate
    in our NLU systems. It is easy to find this bias in current LLMs. For example,
    the article *ChatGPT insists that doctors are male and nurses female*, by Suzanne
    Wertheim, shows many examples of ChatGPT assuming that people in certain professions
    are male or female ([https://www.worthwhileconsulting.com/read-watch-listen/chatgpt-insists-that-doctors-are-male-and-nurses-female](https://www.worthwhileconsulting.com/read-watch-listen/chatgpt-insists-that-doctors-are-male-and-nurses-female)).
    This problem has been the topic of considerable research and is far from solved.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的训练数据基于现有的文本，主要来自网络。在许多情况下，这些文本反映了我们不希望在NLU系统中延续的文化偏见。在当前的LLM中，容易找到这种偏见。例如，Suzanne
    Wertheim撰写的文章《ChatGPT坚持认为医生是男性、护士是女性》展示了许多ChatGPT假设某些职业是男性或女性的例子（[https://www.worthwhileconsulting.com/read-watch-listen/chatgpt-insists-that-doctors-are-male-and-nurses-female](https://www.worthwhileconsulting.com/read-watch-listen/chatgpt-insists-that-doctors-are-male-and-nurses-female)）。这一问题已经成为大量研究的主题，且远未解决。
- en: Citation
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 引用
- en: 'You can find out more about how bias has been addressed in the following survey
    article:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下调查文章了解更多关于偏见如何得到解决的信息：
- en: Alfonso, L. (2021). *A Survey on Bias in Deep NLP*. Applied Sciences, 11(7),
    3184\. [https://doi.org/10.3390/app11073184](https://doi.org/10.3390/app11073184).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Alfonso, L. (2021). *《深度NLP中的偏见调查》*。应用科学，11(7)，3184。 [https://doi.org/10.3390/app11073184](https://doi.org/10.3390/app11073184)。
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have summarized the previous chapters in the book, reviewed
    some areas where NLU technology still faces challenges, and talked about some
    directions where it could improve in the future. NLU is an extremely dynamic and
    fast-moving field, and it will clearly continue to develop in many exciting directions.
    With this book, you have received foundational information about NLU that will
    enable you to decide not only how to build NLU systems for your current applications
    but also to take advantage of technological advances as NLU continues to evolve.
    I hope you will be able to build on the information in this book to create innovative
    and useful applications that use NLU to solve future practical as well as scientific
    problems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们总结了本书之前章节的内容，回顾了自然语言理解（NLU）技术仍面临的一些挑战，并讨论了未来可能改进的方向。NLU是一个极具活力且发展迅速的领域，显然会继续朝着许多令人兴奋的方向发展。通过这本书，您已经获得了有关NLU的基础信息，这将帮助您决定如何为当前应用构建NLU系统，并能够利用技术进步，跟随NLU的持续发展。我希望您能够在这本书的基础上，创建创新且有用的应用程序，利用NLU解决未来的实际问题和科学问题。
- en: Further reading
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'SiYu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng
    Wang. 2021\. *ERNIE-Doc: A Retrospective Long-Document Modeling Transformer*.
    In Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers), pages 2914–2927, Online. Association for Computational
    Linguistics'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '丁思宇, 尚俊远, 王硕欢, 孙昱, 田浩, 吴华, 王海峰. 2021. *ERNIE-Doc: 一种回顾性的长文档建模变换器*. 载于第59届计算语言学协会年会和第11届国际自然语言处理联合会议论文集（第1卷：长篇论文），第2914–2927页，线上出版.
    计算语言学协会'
- en: 'Beltagy, I., Peters, M.E., & Cohan, A. (2020). *Longformer: The Long-Document
    Transformer*. arXiv, abs/2004.05150'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'Beltagy, I., Peters, M.E., & Cohan, A. (2020). *Longformer: 长文档变换器*. arXiv,
    abs/2004.05150'
