- en: What is Machine Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: '**Machine learning** (**ML**) is an artificial intelligence branch where we
    define algorithms, with the aim of learning about a model that describes and extracts
    meaningful information from data.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是人工智能的一个分支，在这个领域，我们定义算法，目的是学习一个模型，该模型描述并提取数据中的有意义信息。'
- en: Exciting applications of ML can be found in fields such as predictive maintenance
    in industrial environments, image analysis for medical applications, time series
    forecasting for finance and many other sectors, face detection and identification
    for security purposes, autonomous driving, text comprehension, speech recognition,
    recommendation systems, and many other applications of ML are countless, and we
    probably use them daily without even knowing it!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的激动人心的应用可以在许多领域找到，比如工业环境中的预测性维护、医学应用的图像分析、金融领域的时间序列预测、面部检测和身份识别用于安全目的、自动驾驶、文本理解、语音识别、推荐系统，以及机器学习的许多其他应用，这些应用无数，我们可能每天都在使用它们，甚至都没意识到！
- en: Just think about the camera application on your smartphone— when you open the
    app and you point the camera toward a person, you see a square around the person's
    face. How is this possible? For a computer, an image is just a set of three stacked
    matrices. How can an algorithm detect that a specific subset of those pixels represents
    a face?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 想想你智能手机上的相机应用程序——当你打开应用并将相机对准一个人时，你会看到一个框框围绕着这个人的脸部。这是怎么做到的呢？对于计算机来说，一张图像只是一组三层叠加的矩阵。一个算法如何检测到这些像素的某个特定子集代表了人脸？
- en: There's a high chance that the algorithm (also called a **model**) used by the
    camera application has been trained to detect that pattern. This task is known
    as face detection. This face detection task can be solved using a ML algorithm
    that can be classified into the broad category of supervised learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 相机应用程序使用的算法（也称为**模型**）很可能已经经过训练，可以检测到这种模式。这项任务被称为人脸检测。这个人脸检测任务可以通过机器学习算法解决，这些算法可以归类为广泛的监督学习类别。
- en: 'ML tasks are usually classified into three broad categories, all of which we
    are going to analyze in the following sections:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务通常分为三大类，我们将在以下部分中分析这三类：
- en: Supervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Semi-supervised learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习
- en: 'Every group has its peculiarities and set of algorithms, but all of them share
    the same goal: learning from data. Learning from data is the goal of every ML
    algorithm and, in particular, learning about an unknown function that maps data
    to the (expected) response.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别都有其独特性和算法集，但它们都有一个共同的目标：从数据中学习。从数据中学习是每个机器学习算法的目标，特别是学习一个未知的函数，这个函数将数据映射到（预期的）响应上。
- en: The dataset is probably the most critical part of the entire ML pipeline; its
    quality, structure, and size are key to the success of deep learning algorithms,
    as we will see in upcoming chapters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可能是整个机器学习管道中最关键的部分；其质量、结构和大小是深度学习算法成功的关键，我们将在接下来的章节中看到这一点。
- en: For instance, the aforementioned face detection task can be solved by training
    a model, making it look at thousands and thousands of labeled examples so that the
    algorithm learns that a specific input corresponds with what we call a face.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，上述的人脸检测任务可以通过训练一个模型来解决，让它查看成千上万的标注示例，以便算法学习到特定输入对应我们所称之为人脸的东西。
- en: The same algorithm can achieve a different performance if it's trained on a
    different dataset of faces, and the more high-quality data we have, the better
    the algorithm's performance will be.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的算法在不同的人脸数据集上训练时，可能会表现出不同的性能，而我们拥有的高质量数据越多，算法的表现就会越好。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The importance of the dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的重要性
- en: Supervised learning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Semi-supervised learning
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习
- en: The importance of the dataset
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集的重要性
- en: Since the concept of the dataset is essential in ML, let's look at it in detail,
    with a focus on how to create the required splits for building a complete and
    correct ML pipeline.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集的概念在机器学习（ML）中至关重要，我们将详细探讨这一概念，重点介绍如何为构建完整且正确的机器学习管道创建所需的数据集划分。
- en: 'A **dataset** is nothing more than a collection of data. Formally, we can describe
    a dataset as a set of pairs, [![](img/d08ba150-f71a-4d8d-b983-9e51713bfb55.png)],
    where [![](img/68c3524a-0fe6-4179-bce7-b032e9b25bf5.png)] is the *i*-th example and [![](img/9b5f6392-3f2e-4600-8041-fa9f4ac225d5.png)] is its
    label, with a finite cardinality, [![](img/951dfeeb-0b2d-415e-88f2-dbfd9fe8f77c.png)]:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集**不过是数据的集合。正式来说，我们可以将数据集描述为一组对， [![](img/d08ba150-f71a-4d8d-b983-9e51713bfb55.png)]，其中
    [![](img/68c3524a-0fe6-4179-bce7-b032e9b25bf5.png)] 是第 *i* 个示例，[![](img/9b5f6392-3f2e-4600-8041-fa9f4ac225d5.png)]
    是其标签，且具有有限的基数 [![](img/951dfeeb-0b2d-415e-88f2-dbfd9fe8f77c.png)]：'
- en: '![](img/2745665d-1f20-4522-958c-4a94adb0894a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2745665d-1f20-4522-958c-4a94adb0894a.png)'
- en: A dataset has a finite number of elements, and our ML algorithm will loop over
    this dataset several times, trying to understand the data structure, until it
    solves the task it is asked to address. As shown in [Chapter 2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml),
    *Neural Networks and Deep Learning*, some algorithms will consider all the data
    at once, while other algorithms will iteratively look at a small subset of the
    data at each training iteration.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据集包含有限数量的元素，我们的机器学习算法会多次遍历这个数据集，试图理解数据结构，直到它解决所要求的任务。正如在[第2章](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml)《神经网络与深度学习》中所示，一些算法会一次性考虑所有数据，而其他算法则会在每次训练迭代时，逐步查看数据的一个小子集。
- en: A typical supervised learning task is the classification of the dataset. We
    train a model on the data, making it learn that a specific set of features extracted
    from the example [![](img/80609162-3140-45e1-830b-eec104e352fe.png)] (or the example, [![](img/80609162-3140-45e1-830b-eec104e352fe.png)],
    itself) corresponds to a label, [![](img/5aeb949d-b552-4bd6-8962-404d700fb887.png)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的监督学习任务是对数据集进行分类。我们在数据上训练一个模型，使其学会将从示例 [![](img/80609162-3140-45e1-830b-eec104e352fe.png)]（或该示例本身
    [![](img/80609162-3140-45e1-830b-eec104e352fe.png)]）中提取的一组特征与标签 [![](img/5aeb949d-b552-4bd6-8962-404d700fb887.png)]
    对应起来。
- en: It is worth familiarizing yourself with the concept of datasets, dataset splits,
    and epochs from the beginning of your journey into the ML world so that you are
    already familiar with these concepts when we talk about them in the chapters that
    follow.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在你踏入机器学习世界的初期，就有必要熟悉数据集、数据集拆分和训练轮次的概念，这样当我们在后续章节中讨论这些概念时，你就已经非常熟悉了。
- en: Right now, you already know, at a very high level, what a dataset is. But let's
    dig into the basic concepts of a dataset split. A dataset contains all the data
    that's at your disposal. As we mentioned previously, the ML algorithm needs to
    loop over the dataset several times and look at the data in order to learn how
    to solve a task (for example, the classification task).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经在非常高的层次上了解了数据集是什么。但让我们深入探讨数据集拆分的基本概念。一个数据集包含了你所有可用的数据。正如我们之前提到的，机器学习算法需要多次遍历数据集并查看数据，以便学习如何解决任务（例如分类任务）。
- en: If we use the same dataset to train and test the performance of our algorithm,
    how can we guarantee that our algorithm performs well, even on unseen data? Well,
    we can't.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用相同的数据集来训练和测试我们的算法性能，那么如何确保算法即使在未见过的数据上也能表现良好呢？嗯，我们无法做到这一点。
- en: 'The most common practice is to split the dataset into three parts:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的做法是将数据集划分为三个部分：
- en: '**Training set**: The subset to use to train the model.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：用于训练模型的子集。'
- en: '**Validation set**: The subset to measure the model''s performance during the
    training and also to perform hyperparameter tuning/searches.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：用于在训练过程中衡量模型性能，并执行超参数调整/搜索的子集。'
- en: '**Test set**: The subset to never touch during the training or validation phases.
    This is used only to run the final performance evaluation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：在训练或验证阶段**绝不触碰**的子集。它仅用于进行最终的性能评估。'
- en: 'All three parts are disjoint subsets of the dataset, as shown in the following
    Venn diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这三部分是数据集的互不相交的子集，如下图所示：
- en: '![](img/fce142ea-a73d-443a-b658-7958bb86375c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce142ea-a73d-443a-b658-7958bb86375c.png)'
- en: Venn diagram representing how a dataset should be divided no overlapping among
    the training, validation, and test sets is required
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表示数据集如何划分的维恩图，其中要求训练集、验证集和测试集之间没有重叠。
- en: The training set is usually the bigger subset since it must be a meaningful
    representation of the whole dataset. The validation and test sets are smaller
    and generally the same size—of course, this is just something general; there are
    no constraints about the dataset's cardinality. In fact, the only thing that matters
    is that they're big enough for the algorithm to be trained on and represented.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集通常是较大的子集，因为它必须能有效地代表整个数据集。验证集和测试集较小，并且一般大小相同——当然，这只是一般情况；数据集的基数没有严格限制。事实上，唯一重要的是它们足够大，能够让算法进行训练并有代表性。
- en: 'We will make our model learn from the training set, evaluate its performance
    during the training process using the validation set, and run the final performance
    evaluation on the test set: this allows us to correctly define and train supervised
    learning algorithms that could generalizewell, and therefore work well even on unseen data.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将让我们的模型从训练集学习，在训练过程中使用验证集评估其性能，并在测试集上进行最终的性能评估：这样可以让我们正确地定义和训练能够很好地泛化的监督学习算法，因此即使面对**未见**数据也能表现良好。
- en: An epoch is the processing of the entire training set that's done by the learning
    algorithm. Hence, if our training set has 60,000 examples, once the ML algorithm
    uses all of them to learn, then an epoch is passed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**epoch**是学习算法处理整个训练集的过程。因此，如果我们的训练集有60,000个样本，ML算法用完所有样本进行学习后，一个epoch就算完成。
- en: One of the most well-known datasets in the ML domain is the MNIST dataset. MNIST
    is a dataset of labeled pairs, where every example is a 28 x 28 binary image of
    a handwritten digit, and the label is the digit represented in the image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，最著名的数据集之一就是MNIST数据集。MNIST是一个标注数据对的数据集，其中每个样本是一个28 x 28的手写数字二值图像，标签则是图像中所表示的数字。
- en: 'However, we are not going to use the MNIST dataset in this book, for several
    reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出于几个原因，我们在本书中并不打算使用MNIST数据集：
- en: MNIST is too easy. Both traditional and recent ML algorithms can classify every
    digit of the dataset almost perfectly (> 97% accuracy).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST太简单了。无论是传统的还是现代的机器学习算法，几乎都能完美地分类数据集中的每一个数字（准确率>97%）。
- en: MNIST is overused. We're not going to make the same applications with the same
    datasets as everyone else.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST被过度使用了。我们不打算用相同的数据集做和别人一样的应用。
- en: MNIST cannot represent modern computer vision tasks.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST无法代表现代计算机视觉任务。
- en: The preceding reasons come from the description of a new dataset, called**fashion-MNIST**,
    which was released in 2017 by the researchers at Zalando Research. This is one
    of the datasets we are going to use throughout this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上述原因源自一个新数据集的描述，名为**fashion-MNIST**，它是由Zalando Research的研究人员于2017年发布的。这是我们在本书中将要使用的数据集之一。
- en: Fashion-MNIST is a drop-in replacement for the MNIST dataset, which means that
    they both have the same structure. For this reason, any source code that uses
    MNIST can be started using fashion-MNIST by changing the dataset path.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion-MNIST是MNIST数据集的**直接替代品**，这意味着它们的结构完全相同。因此，任何使用MNIST的数据集代码，只需更改数据集路径即可改为使用fashion-MNIST。
- en: 'It consists of a training set of 60,000 examples and a test set of 10,000 examples,
    just like the original MNIST dataset; even the image format (28 x 28) is the same.
    The main difference is in the subjects: there are no binary images of handwritten
    digits; this time, there''s grayscale images of clothing. Since they are grayscale
    and not binary, their complexity is higher (binary means only 0 for background
    and 255 for the foreground, while grayscale is the whole range [0,255]):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 它由60,000个样本的训练集和10,000个样本的测试集组成，和原始的MNIST数据集一样；甚至图像格式（28 x 28）也是相同的。主要的不同在于主题：这里没有手写数字的二值图像，而是衣物的灰度图像。由于它们是灰度图像而非二值图像，因此它们的复杂性更高（而二值图像只有背景为0，前景为255，而灰度图像是[0,255]的整个范围）：
- en: '![](img/49b1f5e8-bbf4-4583-b73f-dd9318dcc432.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49b1f5e8-bbf4-4583-b73f-dd9318dcc432.png)'
- en: Images sampled from the fashion-MNIST dataset on the left and from the MNIST
    dataset on the right. It's worth noting how the MNIST dataset is simpler since
    it's a binary images dataset, while the fashion-MNIST dataset is more complex
    because of the grayscale palette and the inherent complexity of the dataset elements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 左边是从fashion-MNIST数据集抽取的图像，右边是从MNIST数据集抽取的图像。值得注意的是，MNIST数据集较为简单，因为它是一个二值图像数据集，而fashion-MNIST数据集则更加复杂，原因在于灰度调色板和数据集元素的固有复杂性。
- en: A dataset such as fashion-MNIST is a perfect candidate to be used in supervised
    learning algorithms since they need annotated examples to be trained on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 像fashion-MNIST这样的数据集是监督学习算法的完美候选者，因为这些算法需要带有注释的示例进行训练。
- en: Before describing the different types of ML algorithms, it is worth becoming
    familiar with the concept of *n-*dimensional spaces, which are the daily bread
    of every ML practitioner.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述不同类型的机器学习算法之前，了解*n*维空间的概念是非常重要的，因为它是每个机器学习从业者的日常基础。
- en: n-dimensional spaces
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: n维空间
- en: '![](img/ef6bda75-d521-4e5c-97e1-57d12e7a52bc.png)-dimensional spaces are a
    way of modeling datasets whose examples have ![](img/ef6bda75-d521-4e5c-97e1-57d12e7a52bc.png)
    attributes each.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/ef6bda75-d521-4e5c-97e1-57d12e7a52bc.png) - 维空间是一种建模数据集的方法，这些数据集的每个示例有 ![](img/ef6bda75-d521-4e5c-97e1-57d12e7a52bc.png) 个属性。'
- en: 'Every example, ![](img/1c099531-b261-40ed-aa49-355af4d9a860.png), in the dataset
    is entirely described by its ![](img/ef6bda75-d521-4e5c-97e1-57d12e7a52bc.png) attributes, ![](img/bd462d2a-fa67-4b03-9750-f505987a1399.png):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个示例，![](img/1c099531-b261-40ed-aa49-355af4d9a860.png)，完全由其 ![](img/ef6bda75-d521-4e5c-97e1-57d12e7a52bc.png) 属性，![](img/bd462d2a-fa67-4b03-9750-f505987a1399.png)
    描述。
- en: '![](img/a7ac52ab-a340-4e36-8698-e42f7ea53cd2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7ac52ab-a340-4e36-8698-e42f7ea53cd2.png)'
- en: Intuitively, you can think about an example such as a row in a database table
    where the attributes are the columns. For example, an image dataset like the fashion-MNIST
    is a dataset of elements each with 28 x 28 = 284 attributes—there are no specific
    column names, but every column of this dataset can be thought of as a pixel position
    in the image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，你可以将其想象为数据库表中的一行，其中属性是列。例如，像fashion-MNIST这样的图像数据集是一个元素数据集，每个元素都有28 x 28
    = 784个属性——没有特定的列名，但该数据集的每一列可以被认为是图像中的一个像素位置。
- en: The concept of dimension arises when we start thinking about examples such as
    points in an n-dimensional space that are uniquely identified by their attributes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 维度的概念出现是因为我们开始将示例视为位于n维空间中的点，这些点通过它们的属性唯一标识。
- en: 'It is easy to visualize this representation when the number of dimensions is
    less than or equal to 3, and the attributes are numeric. To understand this concept,
    let''s take a look at the most common dataset in the data mining field: the Iris
    dataset.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当维度数小于或等于3且属性是数值型时，可视化这种表示方式很容易。为了理解这个概念，让我们来看一下数据挖掘领域最常见的数据集：鸢尾花数据集。
- en: 'What we are going to do here is explorative data analysis. Explorative data
    analysis is good practice when you''re starting to work with a new dataset: always
    visualize and try to understand the data before thinking about applying ML to
    it.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里要做的是探索性数据分析。探索性数据分析是当你开始处理一个新数据集时的良好实践：在考虑应用机器学习之前，始终先可视化并尝试理解数据。
- en: 'The dataset contains three classes of 50 instances each, where each class refers
    to a type of Iris plant. The attributes are all continuous, except for the label/class:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含三个类别，每个类别有50个实例，其中每个类别指代一种鸢尾花类型。所有属性都是连续的，除了标签/类别：
- en: Sepal length in cm
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度（单位：厘米）
- en: Sepal width in cm
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度（单位：厘米）
- en: Petal length in cm
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（单位：厘米）
- en: Petal width in cm
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（单位：厘米）
- en: Class—Iris Setosa, Iris Versicolor, Iris Virginica
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别——Iris Setosa, Iris Versicolor, Iris Virginica
- en: In this small dataset, we have four attributes (plus the class information),
    which means we have four dimensions that are already difficult to visualize all
    at once. What we can do to explore the dataset is pick pairs of features (sepal
    width, sepal length) and (petal width, petal length) and draw them in a 2D plane
    in order to understand how a feature is related (or not) with another and maybe
    find out whether there are some natural partitions in the data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小数据集中，我们有四个属性（加上类别信息），这意味着我们有四个维度，已经很难一次性将其可视化。我们可以做的就是选择特征对（萼片宽度，萼片长度）和（花瓣宽度，花瓣长度），并在二维平面中绘制它们，以便了解一个特征与另一个特征之间的关系（或没有关系），并可能发现数据中是否存在某些自然的分区。
- en: Using an approach such as visualizing the relation between two features only
    allows us to do some initial consideration on the dataset; it won't help us in
    a more complex scenario where the number of attributes is way more and not always
    numerical.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用诸如可视化两个特征之间关系的方法，只能帮助我们对数据集做一些初步的考虑；它无法帮助我们处理更复杂的场景，其中属性的数量远远更多，并且不总是数值型的。
- en: 'In the plots, we assign a different color to every class, (Setosa, Versicolor,
    Virginica) = (blue, green, red):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，我们为每个类别分配不同的颜色，（Setosa, Versicolor, Virginica）=（蓝色，绿色，红色）：
- en: '![](img/2d9e5d41-95c7-4658-9379-964ab40493d0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d9e5d41-95c7-4658-9379-964ab40493d0.png)'
- en: Scatter plot of the Iris dataset; every class has a different color and the
    two dimensions represented are sepal length (x axis) and sepal width (y axis)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集的散点图；每个类别都有不同的颜色，表示的两个维度分别是萼片长度（x 轴）和萼片宽度（y 轴）。
- en: 'As we can see, in this 2D space identified by the attributes (sepal width,
    sepal length) the blue dots are all close together, while the two other classes
    are still blended. All we can conclude by looking at this graph is that there
    could be a positive correlation between the sepal length and width of the Iris
    setosa, but nothing else. Let''s look at the petal relation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在由属性（萼片宽度、萼片长度）标识的这个二维空间中看到的，蓝色点都靠得很近，而其他两个类别仍然混合在一起。通过观察这张图表，我们唯一能得出的结论是，可能存在萼片长度和宽度之间的正相关关系，但别无其他。让我们看看花瓣之间的关系：
- en: '![](img/51bbe7ce-0913-471c-8e0d-122d134ee6c0.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51bbe7ce-0913-471c-8e0d-122d134ee6c0.png)'
- en: Scatter plot of the Iris dataset; every class has a different color and the
    two dimensions represented are petal length (x axis) and petal width (y axis).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集的散点图；每个类别都有不同的颜色，表示的两个维度分别是花瓣长度（x 轴）和花瓣宽度（y 轴）。
- en: This plot shows us that there are three partitions in this dataset. To find
    them, we can use the petal width and length attributes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表告诉我们，在这个数据集中有三个分区。为了找到它们，我们可以使用花瓣宽度和长度属性。
- en: 'The goal of classification algorithms is to get them to learn how to identify
    what features are discriminative in order to learn a function so that they can
    correctly separate elements of different classes. Neural networks have proven
    to be the right tool to use to avoid doing feature selection and a lot of data
    preprocessing: they''re so robust to the noise that they almost removed the need
    for data cleaning.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法的目标是让它们学会识别哪些特征具有区分性，以便学习一个函数，使它们能够正确地分离不同类别的元素。神经网络已经证明是用来避免进行特征选择和大量数据预处理的正确工具：它们对噪声非常强大，几乎不需要数据清理。
- en: '**Warning**: This is only valid on massive datasets, where the noise is overwhelmed
    by the correct data—for small datasets, it is always better to look at the features
    by plotting them and helping the ML algorithm by giving only the significant features
    as input.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**：此方法仅适用于大数据集，在这些数据集中，噪声被正确数据所压倒—对于小数据集，最好通过绘图查看特征，并通过仅输入显著特征来帮助 ML 算法。'
- en: The Iris dataset is the most straightforward dataset we could have used to describe
    an *n*-dimensional space. If we jump back to the fashion-MNIST dataset, things
    become way more interesting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集是我们用来描述 *n* 维空间最直接的数据集。如果我们转向 fashion-MNIST 数据集，情况就会变得更加有趣。
- en: 'A single example has 784 features: how can we visualize a 784-dimensional space?
    We can''t!'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 单个例子具有 784 个特征：我们如何可视化一个 784 维空间？我们做不到！
- en: The only thing we can do is perform a dimensionality reduction technique in
    order to reduce the number of dimensions that are needed for visualization and
    have a better understanding of the underlying data structure.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一能做的就是进行降维技术，以减少可视化所需的维度数量，并更好地理解底层数据结构。
- en: 'One of the simplest data reduction techniques—and usually meaningless on high-dimensional
    datasets—is the visualization of randomly picked dimensions of the data. We did
    it for the Iris dataset: we just chose two random dimensions among the four available
    and plotted the data in the 2D plane. Of course, for low-dimensional space, it
    could be helpful, but for a dataset such as fashion-MNIST, it is a complete waste
    of time. There are better dimensionality reduction techniques, such as **Principal
    Component Analysis** (**PCA**) or **t-distributed Stochastic Neighbor Embedding**
    (**t-SNE**), that we won''t cover in detail in this book, since the data visualization
    tool we are going to use in the upcoming chapters, that is, TensorBoard, already
    implements these algorithms for us.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的数据降维技术之一，通常在高维数据集上没有意义，是对数据中随机选择的维度进行可视化。我们在 Iris 数据集上做了这样的事情：我们只是从四个可用维度中随机选择了两个维度，并在二维平面上绘制了数据。当然，对于低维空间，这可能有所帮助，但对于诸如
    fashion-MNIST 等数据集来说，这是完全浪费时间的。还有更好的降维技术，例如**主成分分析**（**PCA**）或**t-分布随机近邻嵌入**（**t-SNE**），我们不会在本书中详细介绍，因为我们即将在后续章节中使用的数据可视化工具，即
    TensorBoard，已经为我们实现了这些算法。
- en: 'Moreover, there are specific geometrical properties that don''t work as we
    expect them to when we''re working in high-dimensional spaces: this fact is called
    the **curse of dimensionality**. In the next section, we''ll see how a simple
    geometrical example can be used to show how the Euclidean distances work differently
    as the number of dimensions increases.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当我们在高维空间中工作时，存在一些几何属性无法按预期工作：这个现象被称为**维度灾难**。在接下来的部分中，我们将通过一个简单的几何示例来展示，随着维度的增加，欧几里得距离的计算方式如何发生变化。
- en: The curse of dimensionality
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度灾难
- en: Let's take a hypercube unitary ![](img/75369d2a-8d75-4284-9ff6-83dca87908c4.png) with
    a center of [![](img/336758dd-dc2c-421a-8506-6d2cbe9ca23c.png)] in a ![](img/84a48931-fffe-44fe-ac18-8333c9be3591.png)-dimensional
    space.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个超立方体单位 ![](img/75369d2a-8d75-4284-9ff6-83dca87908c4.png) 为例，其中心在 [![](img/336758dd-dc2c-421a-8506-6d2cbe9ca23c.png)] 的位置，处于一个 ![](img/84a48931-fffe-44fe-ac18-8333c9be3591.png)-维空间中。
- en: Let's also take a ![](img/8d01ad76-1b79-4c8c-87c1-ba8f6a10cef8.png)-dimensional
    hypersphere, with ![](img/f19d223d-2c8b-4d49-9fc8-2d411a50eb0a.png) centered on
    the origin of the space, ![](img/11b6ec50-acdb-43b2-a8eb-2890a32e0fa7.png). Intuitively,
    the center of the hypercube, ![](img/412c96b6-52d3-41ae-971f-f4693cc56e58.png),
    is inside the sphere. Is this true for every value of ![](img/8708bf1c-15a3-4415-b0ee-07154ed6f4d9.png)?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑一个 ![](img/8d01ad76-1b79-4c8c-87c1-ba8f6a10cef8.png)-维超球体，球心位于空间的原点 ![](img/f19d223d-2c8b-4d49-9fc8-2d411a50eb0a.png) ，![](img/11b6ec50-acdb-43b2-a8eb-2890a32e0fa7.png)。直观地说，超立方体的中心 ![](img/412c96b6-52d3-41ae-971f-f4693cc56e58.png) 在球体内部。这对于所有的 ![](img/8708bf1c-15a3-4415-b0ee-07154ed6f4d9.png) 值都成立吗？
- en: 'We can verify this by measuring the Euclidean distance between the hypercube
    center and the origin:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过测量超立方体中心和原点之间的欧几里得距离来验证这一点：
- en: '![](img/24167f88-68ca-4f0e-877f-48239a2c60fa.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24167f88-68ca-4f0e-877f-48239a2c60fa.png)'
- en: Since the radius of the sphere is 1 in any dimension, we can conclude that,
    for a value of D greater than 4, the hypercube center is outside the hypersphere.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于球体的半径在任何维度下都是1，我们可以得出结论，对于D大于4的值，超立方体的中心位于超球体之外。
- en: With the curse of dimensionality, we refer to the various phenomena that arise
    only when we're working with data in high-dimensional spaces that do not occur
    in low-dimensional settings such as the 2D or 3D space.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难是指那些只有在我们处理高维空间中的数据时才会出现的各种现象，这些现象在低维空间（如二维或三维空间）中是不存在的。
- en: In practice, as the number of dimensions increases, some counterintuitive things
    start happening; this is the curse of dimensionality.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，随着维度的增加，一些违反直觉的现象开始发生；这就是维度灾难。
- en: Now, it should be clearer that working within high-dimensional spaces is not
    easy and not intuitive at all. One of the greatest strengths of deep neural networks—which is
    also one of the reasons for their widespread use—is that they make tractable problems
    in high dimensional spaces, thereby reducing dimensionality layer by layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，应该更清楚地认识到，在高维空间中工作既不容易也不直观。深度神经网络的最大优势之一——也是其广泛应用的原因之一——就是它们使高维空间中的问题变得可处理，从而层层减少维度。
- en: The first class of ML algorithms we are going to describe is the supervised
    learning family. These kinds of algorithms are the right tools to use when we
    aim to find a function that's able to separate elements of different classes in
    an *n*-dimensional space.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述的第一类机器学习算法是监督学习算法。此类算法是在我们希望在一个 *n* 维空间中找到一个能够区分不同类别元素的函数时，最合适的工具。
- en: Supervised learning
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning algorithms work by extracting knowledge from a **knowledge
    base** (**KB**), that is, the dataset that contains labeled instances of the concept
    we need to learn about.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法通过从**知识库**（**KB**）中提取知识来工作，也就是说，算法从包含我们需要学习的概念标签实例的数据集中提取信息。
- en: Supervised learning algorithms are two-phase algorithms. Given a supervised
    learning problem—let's say, a classification problem—the algorithm tries to solve
    it during the first phase, called the **training phase**, and its performance
    is measured in the second phase, called the **testing phase**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法是两阶段的算法。给定一个监督学习问题——比如一个分类问题——算法在第一阶段（称为**训练阶段**）尝试解决该问题，并在第二阶段（称为**测试阶段**）测量其性能。
- en: 'The three dataset splits (train, validation, and test), as defined in the previous
    section, and the two-phase algorithm should sound an alarm: why do we have a two-phase
    algorithm and three dataset splits?'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一部分所定义的，三个数据集划分（训练集、验证集和测试集）以及两阶段算法应当引起警觉：为什么我们需要一个两阶段算法和三个数据集划分？
- en: 'Because the first phase (should—in a well-made pipeline) uses two datasets.
    In fact, we can define the stages:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因为第一阶段（在一个精心设计的流程中）使用了两个数据集。事实上，我们可以定义以下阶段：
- en: '**Training and validation**: The algorithm analyzes the dataset to generate
    *a* theory that is valid for the data it has been trained on, but also for items
    it has never seen.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练和验证**：算法分析数据集以生成对其所训练数据有效的*理论*，同时也能适用于它从未见过的项。'
- en: The algorithm, therefore, tries to discover and generalize a concept that bonds
    the examples with the same label, with the examples themselves.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，该算法试图发现并概括一个概念，将具有相同标签的示例与示例本身联系起来。
- en: Intuitively, if you have a labeled dataset of cats and dogs, you want your algorithm
    to distinguish between them while being able to be robust to the variations that
    the examples with the same label can have (cats with different colors, positions,
    backgrounds, and so on).
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直观地说，如果你有一个包含猫和狗标签的数据集，你希望算法能够区分它们，同时能够对具有相同标签的示例的变化具有鲁棒性（例如不同颜色、位置、背景的猫等）。
- en: At the end of every training epoch, a performance evaluation using a metric on
    the validation set should be performed to select the model that reached the best
    performance on the validation set and to tune the algorithm hyperparameters to
    achieve the best possible result.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个训练周期结束时，应使用验证集上的度量标准进行性能评估，以选择在验证集上达到最佳性能的模型，并调优算法的超参数以获得最佳结果。
- en: '**Testing**: The learned theory is applied to labeled examples that were never
    seen during the training and validation phases. This allows us to test how the
    algorithm performs on data that has never been used to train or select the model
    hyperparameters—a real-life scenario.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：将学习到的理论应用于训练和验证阶段从未见过的标记示例。这使得我们能够测试算法在从未用于训练或选择模型超参数的数据上的表现——这是真实场景。'
- en: 'Supervised learning algorithms are a broad category, and all of them share
    the need for having a labeled dataset. Don''t be fooled by the concept of a label:
    it is not mandatory for the label to be a discrete value (cat, dog, house, horse);
    in fact, it can also be a continuous value. What matters is the existence of the
    association (example, value) in the dataset. More formally, the example is a predictor
    variable, while the value is the dependent variable, outcome, or target variable.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法是一个广泛的类别，所有这些算法都需要有标签的数据集。不要被标签的概念所误导：标签不一定是离散的值（如猫、狗、房子、马等）；事实上，它也可以是一个连续值。关键是数据集中存在着示例与值之间的关联。更正式地说，示例是预测变量，而值是因变量、结果或目标变量。
- en: 'Depending on the type of the desired outcome, supervised learning algorithms
    can be classified into two different families:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 根据期望结果的类型，监督学习算法可以分为两大类：
- en: '![](img/2d40c97e-e765-41ef-b82c-c5ea0d6a6d67.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d40c97e-e765-41ef-b82c-c5ea0d6a6d67.png)'
- en: The supervised learning family—the target variable defines the problem to solve
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习家族——目标变量定义了解决的问题
- en: '**Classification**: Where the label is discrete, and the aim is to classify
    the example and predict the label. The classification algorithm''s aim is to learn
    about classification boundaries. These boundaries are functions that divide the
    space where the examples live into regions.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：当标签是离散的，目标是对示例进行分类并预测标签。分类算法的目标是学习分类边界。这些边界是将示例所在空间划分为不同区域的函数。'
- en: '**Regression**: Where the target variable is continuous, and the aim is to
    learn to regress a continuous value given an example.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：当目标变量是连续的，目标是学会根据示例回归一个连续值。'
- en: A regression problem that we will see in the upcoming chapters is the regression
    of the bounding box corner coordinates around a face. The face can be anywhere
    in the input image, and the algorithm has learned to regress the eight coordinates
    of the bounding box.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中看到的回归问题是对人脸周围边界框角坐标的回归。人脸可以出现在输入图像的任何位置，算法已经学会回归边界框的八个坐标。
- en: 'Parametric and non-parametric algorithms are used to solve classification and
    regression problems; the most common non-parametric algorithm is the k-NN algorithm.
    This is used to introduce the fundamental concepts of distances and similarities:
    concepts that are at the basis of every ML application. We will cover the k-NN
    algorithm in the next section.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数和非参数算法用于解决分类和回归问题；最常见的非参数算法是 k-NN 算法。它用于介绍距离和相似性的基本概念：这些概念是每个机器学习应用的基础。我们将在下一节中讲解
    k-NN 算法。
- en: Distances and similarities – the k-NN algorithm
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离和相似性 — k-NN 算法
- en: The k-NN algorithm's goal is to find elements similar to a given one, rank them
    using a similarity score, and return the top-k similar elements (the first k elements,
    sorted by similarity) found.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN 算法的目标是找到与给定元素相似的元素，通过相似度评分对它们进行排名，并返回找到的前 k 个相似元素（按相似度排序的前 k 个元素）。
- en: 'To do this, you need to measure the similarity that''s required for a function
    that assigns a numerical score to two points: the higher the score, the more similar
    the elements should be.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，你需要衡量一个函数所需的相似性，该函数为两个点分配一个数值评分：评分越高，元素之间的相似性应该越强。
- en: Since we are modeling our dataset as a set of points in an *n*-dimensional space,
    we can use any ![](img/4239c62c-7f79-403f-9fe9-1be0e93315e6.png) norm, or any
    other score function, even if it's not a metric, to measure the distance between
    two points and consider similar elements that are close together and dissimilar
    elements that are far away. The choice of the norm/distance function is entirely
    arbitrary, and it should depend on the topology of the *n*-dimensional space (that
    is why we usually reduce the dimensionality of the input data, and we try to measure
    the distances in lower dimensional space—so the curse of dimensionality gives
    us less trouble).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将数据集建模为 *n* 维空间中的一组点，因此可以使用任何 ![](img/4239c62c-7f79-403f-9fe9-1be0e93315e6.png) 范数，或者任何其他评分函数，即使它不是度量函数，来衡量两点之间的距离，并认为接近的元素是相似的，而远离的元素是不相似的。范数/距离函数的选择完全是任意的，它应该依赖于
    *n* 维空间的拓扑结构（这就是为什么我们通常会减少输入数据的维度，并且尽量在低维空间中衡量距离——这样高维灾难给我们带来的困扰就少了）。
- en: 'Thus, if we want to measure the similarity of elements in a dataset with dimensionality
    *D*, given a point, *p*, we have to measure and collect the distance from *p*
    to every other point, *q*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想要衡量数据集中维度为 *D* 的元素之间的相似性，给定一个点 *p*，我们必须衡量并收集 *p* 到每个其他点 *q* 的距离：
- en: '![](img/b204cf6c-c47c-4bf5-b04a-f668262263a9.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b204cf6c-c47c-4bf5-b04a-f668262263a9.png)'
- en: The preceding example shows the general scenario of computing the generic *p*
    norm on the distance vector that connects *p* and *q*. In practice, setting *p=1*
    gives us the Manhattan distance, while setting *p=2* gives us the Euclidean distance.
    No matter what distance is chosen, the algorithm works by computing the distance
    function and sorting by closeness as a measure of similarity.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子展示了计算连接 *p* 和 *q* 的距离向量的通用 *p* 范数的常见场景。在实践中，设置 *p=1* 会给我们带来曼哈顿距离，而设置 *p=2*
    会给我们带来欧几里得距离。无论选择什么距离，算法的工作方式都是通过计算距离函数并按接近度排序来作为相似度的度量。
- en: When k-NN is applied to a classification problem, the point, *p*, is classified
    by the vote of its k neighbors, where the vote is their class. Thus, an object
    that is classified with a particular class depends on the class of the elements
    that surround it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当 k-NN 应用于分类问题时，点 *p* 会根据其 k 个邻居的投票来进行分类，投票的依据是它们的类别。因此，一个被分类为特定类别的对象依赖于其周围元素的类别。
- en: When k-NN is applied to regression problems, the output of the algorithm is
    the average of the values of the  k-NN.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当 k-NN 应用于回归问题时，算法的输出是 k-NN 的值的平均值。
- en: k-NN is only one among the various non-parametric models that has been developed
    over the years; however, parametric models usually show better performance. We'll
    look at these in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN 只是多年来发展出来的各种非参数模型中的一个；然而，参数模型通常表现出更好的性能。我们将在下一节中讨论这些模型。
- en: Parametric models
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数模型
- en: 'The ML models we are going to describe in this book are all parametric models:
    this means that a model can be described using a function, where the input and
    output are known (in the case of supervised learning, it is clear), and the aim
    is to change the model parameters so that, given a particular input, the model
    produces the expected output.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中将要描述的机器学习模型都是参数化模型：这意味着可以使用一个函数来描述模型，其中输入和输出是已知的（在监督学习的情况下，这是明确的），目标是改变模型参数，以便给定一个特定的输入，模型能产生预期的输出。
- en: Given an input sample, ![](img/9a34f9d1-234b-4d74-b83a-135c9223df52.png), and the
    desired outcome, ![](img/ee560d9d-abea-4460-bca0-2e7754dc7941.png), an ML model
    is a parametric function, ![](img/09fd850e-54f2-4dc2-89fa-e3ac5695b01f.png), where ![](img/7b49728e-7446-40dc-99f0-2b6c6e31f455.png) is the
    set of model parameters to change during the training in order to fit the data
    (or in other words, generating a hypothesis).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入样本，![](img/9a34f9d1-234b-4d74-b83a-135c9223df52.png)，以及期望的结果，![](img/ee560d9d-abea-4460-bca0-2e7754dc7941.png)，一个机器学习模型是一个参数化函数，![](img/09fd850e-54f2-4dc2-89fa-e3ac5695b01f.png)，其中![](img/7b49728e-7446-40dc-99f0-2b6c6e31f455.png)是训练过程中需要变化的模型参数，以便拟合数据（换句话说，生成一个假设）。
- en: The most intuitive and straightforward example we can give to clarify the concept
    of model parameters is linear regression.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明模型参数的概念，我们可以给出最直观和最简单的例子——线性回归。
- en: Linear regression attempts to model the relationship between two variables by
    fitting a linear equation to observed data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归试图通过将线性方程拟合到观测数据中来建模两个变量之间的关系。
- en: 'Linear regression models have the following equation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型具有以下方程：
- en: '![](img/2f27df43-72d7-4635-9961-51d570b56e53.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f27df43-72d7-4635-9961-51d570b56e53.png)'
- en: Here, ![](img/f34c86a0-57e0-4f17-928b-7ce8639cfb79.png) is the independent variable
    and ![](img/53f41198-69fc-454a-9f36-fa38a5eb83a5.png) is the dependent one. The
    parameter, ![](img/a63593eb-7c08-403e-9ebe-6f9352801cf7.png), is the scale factor,
    coefficient, or slope, and ![](img/3f137dca-9095-483d-9b1e-c90087047c5e.png) is the
    bias coefficient or intercept.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/f34c86a0-57e0-4f17-928b-7ce8639cfb79.png)是自变量，而![](img/53f41198-69fc-454a-9f36-fa38a5eb83a5.png)是因变量。参数![](img/a63593eb-7c08-403e-9ebe-6f9352801cf7.png)是比例因子、系数或斜率，而![](img/3f137dca-9095-483d-9b1e-c90087047c5e.png)是偏置系数或截距。
- en: Hence, the model parameters that must change during the training phase are ![](img/3160a492-4a4c-4810-bd0b-0a88449b7475.png).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练阶段必须变化的模型参数是![](img/3160a492-4a4c-4810-bd0b-0a88449b7475.png)。
- en: 'We''re talking about a single example in the training set, but the line should
    be the one that fits all the points of the training set the best. Of course, we
    are making a strong assumption about the dataset: we are using a model that, due
    to its nature, models a line. Due to this, before attempting to fit a linear model
    to the data, we should first determine whether or not there is a linear relationship
    between the dependent and independent variables (using a scatter plot is usually
    useful).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的是训练集中一个单独的示例，但这条线应该是最能拟合训练集所有点的线。当然，我们对数据集做出了一个强假设：我们使用的是一个由于其本质而建模为一条线的模型。由于这一点，在尝试将线性模型拟合到数据之前，我们应该首先确定因变量和自变量之间是否存在线性关系（使用散点图通常很有帮助）。
- en: The most common method for fitting a regression line is the method of least
    squares. This method calculates the best-fitting line for the observed data by
    minimizing the sum of the squares of the vertical deviations from each data point
    to the line (if a point lies on the fitted line exactly, then its vertical deviation
    is 0). This relationship between observed and predicted data is what we call the *loss
    function*, as we will see in [Chapter 2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml), *Neural
    Networks and Deep Learning*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合回归线的最常见方法是最小二乘法。该方法通过最小化每个数据点到拟合线的垂直偏差平方和，来计算观测数据的最佳拟合线（如果一个点恰好位于拟合线上的话，它的垂直偏差为0）。我们称这种观测数据和预测数据之间的关系为*损失函数*，正如我们将在[第2章](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml)《神经网络与深度学习》中看到的那样。
- en: The goal of the supervised learning algorithm is, therefore, to iterate over
    the data and to adjust the ![](img/7b49728e-7446-40dc-99f0-2b6c6e31f455.png) parameters iteratively
    so that ![](img/09fd850e-54f2-4dc2-89fa-e3ac5695b01f.png) correctly models the
    observed phenomena.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，监督学习算法的目标是遍历数据，并通过迭代调整![](img/7b49728e-7446-40dc-99f0-2b6c6e31f455.png)参数，以便![](img/09fd850e-54f2-4dc2-89fa-e3ac5695b01f.png)正确地建模观测现象。
- en: However, when using more complex models (with a considerable number of adjustable
    parameters, as in the case of neural networks), adjusting the parameters can lead
    to undesired results.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当使用更复杂的模型（如神经网络中具有大量可调参数的模型）时，调整参数可能会导致不希望的结果。
- en: If our model is composed of just two parameters and we are trying to model a
    linear phenomenon, there are no problems. But if we are trying to classify the
    Iris dataset, we can't use a simple linear model since it is easy to see that
    the function we have to learn about to separate the different classes is not a
    simple line.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型仅由两个参数组成，并且我们试图建模一个线性现象，则不会有问题。但如果我们试图对Iris数据集进行分类，我们不能使用简单的线性模型，因为很容易看出，用来分隔不同类别的函数并不是一条简单的直线。
- en: In cases like that, we can use models with a higher number of trainable parameters
    that can adjust their variables to almost perfectly fit the dataset. This may
    sound perfect, but in practice, it is not something that's desirable. In fact,
    the model is adapting its parameters only to fit the training data, almost *memorizing*
    the dataset and thus losing every generalization capability.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以使用具有更高数量可训练参数的模型，这些模型可以调整其变量以几乎完美地拟合数据集。这听起来完美，但实际上，这并不是我们期望的效果。事实上，模型只是调整其参数以适应训练数据，几乎是在*记忆*数据集，从而失去了所有的泛化能力。
- en: This pathological phenomenon is called **overfitting**, and it happens when
    we are using a model that's too complex to model a simple event. There's also
    an opposite scenario, called **underfitting**, that occurs when our model is too
    simple for the dataset and therefore is not able to capture all the complexity
    of the data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种病态现象被称为**过拟合**，它发生在我们使用一个过于复杂的模型来建模一个简单事件时。还有一种相反的情况，称为**欠拟合**，当我们的模型对于数据集过于简单，因此无法捕捉到数据的所有复杂性时，会发生这种情况。
- en: 'Every ML model aims to learn, and will adapt its parameters so that it''s robust
    to noise and generalize, which means to find a suitable approximate function representing
    the relationship between the predictors and the response:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习模型的目标都是学习，并会调整其参数，以使其对噪声具有鲁棒性并能够泛化，这意味着找到一个合适的近似函数来表示预测变量与响应之间的关系：
- en: '![](img/71a4bcf4-7d73-4f94-8cd2-fd6f11b0b321.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71a4bcf4-7d73-4f94-8cd2-fd6f11b0b321.png)'
- en: The dashed line represents the model's prediction. Underfitting, on the left,
    is a model with very poor generalization performance and therefore is unable to
    learn a good dataset approximation. The center image represents a good model that
    can generalize well in contrast to the model on the right that memorized the training
    set, overfitting it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线代表模型的预测。左侧的欠拟合是一个泛化性能很差的模型，因此无法学习到良好的数据集近似。中间的图像代表了一个可以很好地进行泛化的模型，而右侧的模型则记住了训练集，发生了过拟合。
- en: 'Several supervised learning algorithms have been developed over the years.
    This book, however, will focus on the ML model that demonstrated to be more versatile
    and that can be used to solve almost any supervised, unsupervised, and semi-supervised
    learning task: neural networks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已经开发出了多种监督学习算法。然而，本书将重点介绍一种已被证明更为通用，并可用于解决几乎所有监督、无监督和半监督学习任务的机器学习模型：神经网络。
- en: 'During the explanation of the training and validation phases, we talked about
    two concepts we haven''t introduced yet—hyperparameters and metrics:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和验证阶段的解释中，我们讨论了两个尚未介绍的概念——超参数和指标：
- en: '**Hyperparameters**: We talk about hyperparameters when our algorithm, which
    is to be fully defined, requires values to be assigned to a set of parameters.
    We call the parameters that define the algorithm itself hyperparameters. For example,
    the number of neurons in a neural network is a hyperparameter.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：当我们的算法（该算法需要完全定义）需要为一组参数分配值时，我们谈论的是超参数。我们将定义算法本身的参数称为超参数。例如，神经网络中的神经元数量就是一个超参数。'
- en: '**Metrics**: The functions that give the model prediction. The expected output
    produces a numerical score that measures the goodness of the model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**：给出模型预测的函数。期望的输出会产生一个数值评分，用来衡量模型的优劣。'
- en: Metrics are crucial components in every ML pipeline; they are so useful and
    powerful that they deserve their own section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是每个机器学习流程中的关键组成部分；它们如此有用且强大，以至于它们应当拥有自己的一节。
- en: Measuring model performance – metrics
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量模型性能——指标
- en: Evaluating a supervised learning algorithm during the evaluation and testing
    phases is an essential part of any well-made ML pipeline.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估和测试阶段评估监督学习算法是任何精心构建的机器学习管道中至关重要的一部分。
- en: 'Before we describe the various metrics that are available, there''s one last
    thing that''s worth noting: measuring the performance of a model is something
    that we can always do on every dataset split. During the training phase, usually
    at the end of every training epoch, we can measure the performance of the algorithm
    on the training set itself, as well as the validation set. Plotting how the curves
    change during the training and analyzing the relationships between the validation
    and training curve allow us to quickly identify the previously described pathological
    conditions of an ML model—overfitting and underfitting.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们描述可用的各种指标之前，还有一件值得注意的事：评估模型性能是我们在每个数据集拆分中都可以进行的操作。在训练阶段，通常在每个训练周期结束时，我们可以评估算法在训练集和验证集上的表现。通过绘制训练过程中曲线的变化，并分析验证曲线和训练曲线之间的关系，我们可以快速识别先前描述的机器学习模型病态条件——过拟合和欠拟合。
- en: Supervised learning algorithms have the significant advantage of having the
    expected outcome of the algorithm inside the dataset, and all the metrics hereby
    presented use the label information to evaluate "how well" the model performs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法具有一个显著的优势，那就是数据集中包含了算法的期望输出，所有这里呈现的指标都利用标签信息来评估模型的“表现如何”。
- en: There are metrics to measure the performance of classifiers and metrics to measure
    the performance of regressors; it is clear that it wouldn't make any sense to
    treat a classifier in the same way as a regressor, even if both are members of
    the supervised learning algorithm family.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有用于衡量分类器性能的指标，也有用于衡量回归器性能的指标；显然，即使分类器和回归器都属于监督学习算法家族，将它们视为相同的对象也是毫无意义的。
- en: The first metric and the most used metric for evaluating a supervised learning
    algorithm's performance is accuracy*.*
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 评估监督学习算法性能的第一个指标也是最常用的指标是准确率*。*
- en: Using accuracy
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用准确率
- en: Accuracy is the ratio of the number of correct predictions made to the number
    of all predictions made.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是指正确预测的数量与所有预测数量之比。
- en: Accuracy is used to measure classification performance on multiclass classification
    problems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率用于衡量多分类问题中的分类性能。
- en: 'Given ![](img/7bda140d-dee1-47f4-9474-01546f257b2b.png) as the label and ![](img/2aefc804-4ab5-499e-a6a3-0f1e0d14f47e.png) as
    the prediction, we can define the accuracy of the *i*-th example as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 给定![](img/7bda140d-dee1-47f4-9474-01546f257b2b.png)作为标签，![](img/2aefc804-4ab5-499e-a6a3-0f1e0d14f47e.png)作为预测，我们可以定义第*i*个样本的准确率如下：
- en: '![](img/6b23c635-4727-4eb3-addf-0dc4a90d15c6.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b23c635-4727-4eb3-addf-0dc4a90d15c6.png)'
- en: 'Therefore, for a whole dataset with *N* elements, the mean accuracy over all
    the samples is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于一个包含*N*个元素的完整数据集，所有样本的平均准确率如下：
- en: '![](img/c08d0d85-f13a-4f38-8ebd-6f7071db6d8b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c08d0d85-f13a-4f38-8ebd-6f7071db6d8b.png)'
- en: 'We have to pay attention to the structure of the dataset, *D*, when using this
    metric: in fact, it works well only when there is an equal number of samples belonging
    to each class (we need to be using a balanced dataset).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此指标时，我们必须关注数据集*D*的结构：实际上，只有在每个类别的样本数量相等时，它才会工作得很好（我们需要使用一个平衡的数据集）。
- en: In the case of an unbalanced dataset or when the error in predicting that an
    incorrect class is higher/lower than predicting another class, accuracy is not
    the best metric to use. To understand why, think about the case of a dataset with
    two classes only, where 80% of samples are of class 1, and 20% of samples are
    of class 2.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集不平衡的情况下，或者当预测错误类别的误差大于/小于预测其他类别时，准确率并不是最好的指标。为了理解这一点，考虑一个只有两个类别的数据集，其中80%的样本是类1，20%的样本是类2。
- en: If the classifier predicts only class 1, the accuracy that's measured in this
    dataset is 0.8, but of course, this is not a good measure of the performance of
    the classifier, since it always predicts the same class, no matter what the input
    is. If the same model is tested on a test set with 40% of samples from class 1
    and the remaining ones of class 2, the measurement will drop down to 0.4.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类器只预测类1，在这个数据集中测得的准确率是0.8，但显然这并不是一个好的分类器性能指标，因为无论输入是什么，它总是预测相同的类。如果在一个测试集上对同一模型进行测试，该测试集包含40%的类1样本和剩下的类2样本，那么准确率会降到0.4。
- en: Remembering that metrics can be used during the training phase to measure the
    model's performance, we can monitor how the training is going by looking at the
    validation accuracy and the training accuracy to detect if our model is overfitting
    or underfitting the training data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，度量标准可以在训练阶段用来衡量模型的表现，我们可以通过查看验证准确率和训练准确率来监控训练过程，检测模型是否出现过拟合或欠拟合。
- en: If the model can model the relationships present in the data, the training accuracy
    increases; if it doesn't, the model is too simple and we are underfitting the
    data. In this case, we have to use a complex model with a higher learning capacity
    (with a more significant number of trainable parameters).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型能够拟合数据中的关系，训练准确率会提高；如果不能，模型过于简单，出现欠拟合。这时，我们需要使用具有更高学习能力（更多可训练参数）的复杂模型。
- en: 'If our training accuracy increases, we can start looking at the validation
    accuracy (always at the end of every training epoch): if the validation accuracy
    stops growing or even starts decreasing, the model is overfitting the training
    data and we should stop the training (this is called an **early stop** and is
    a regularization technique).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的训练准确率提高了，我们可以开始查看验证准确率（每个训练周期结束时）：如果验证准确率停止增长甚至开始下降，说明模型正在过拟合训练数据，我们应该停止训练（这称为**提前停止**，是一种正则化技术）。
- en: Using the confusion matrix
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用混淆矩阵
- en: 'The confusion matrix is a tabular way of representing a classifier''s performance.
    It can be used to summarize how the classifier behaved on the test set, and it
    can be used only in the case of multi-class classification problems. Each row
    of the matrix represents the instances in a predicted class, while each column
    represents the instances in an actual class. For example, in a binary classification
    problem, we can have the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是表示分类器性能的表格方式。它可以用来总结分类器在测试集上的表现，并且仅适用于多类分类问题。矩阵的每一行代表预测类别中的实例，每一列代表实际类别中的实例。例如，在一个二分类问题中，我们可以有以下内容：
- en: '| Samples: 320 | Actual: YES | Actual: NO |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 样本: 320 | 实际: 是 | 实际: 否 |'
- en: '| Predicted: YES | 98 | 120 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 预测: 是 | 98 | 120 |'
- en: '| Predicted: NO | 150 | 128 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 预测: 否 | 150 | 128 |'
- en: It is worth noting that the confusion matrix is **not a metric**; in fact, the
    matrix alone does not measure the model's performance, but is the basis for computing
    several useful metrics, all of them based on the concepts of true positives, true
    negatives, false positives, and false negatives.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，混淆矩阵**不是一个度量标准**；实际上，单独的矩阵并不能衡量模型的性能，而是计算若干有用度量的基础，这些度量都基于真正正例、真正负例、假正例和假负例的概念。
- en: 'These terms all refer to a **single class**; this means you have to consider
    a multiclass classification problem as a binary classification problem when computing
    these terms. Given a multiclass classification problem, whose classes are A, B,
    ..., Z, we have, for example, the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语都指的是**单一类别**；这意味着在计算这些术语时，必须将多类分类问题视为二分类问题。给定一个多类分类问题，其中类别为 A、B、...、Z，我们例如可以得到以下内容：
- en: '(**TP**) **True positives of A**: All A instances that are classified as A'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (**TP**) **A 的真正正例**：所有被分类为 A 的 A 实例
- en: '(**TN**) **True negatives of A**: All non-A instances that are not classified
    as A'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (**TN**) **A 的真正负例**：所有未被分类为 A 的非 A 实例
- en: '(**FP**) **False positives of A**: All non-A instances that are classified
    as A'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (**FP**) **A 的假正例**：所有被分类为 A 的非 A 实例
- en: '(**FN**) **False negatives of A**: All A instances that are not classified
    as A'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (**FN**) **A 的假负例**：所有未被分类为 A 的 A 实例
- en: This, of course, can be applied to every class in the dataset so that we get
    these four values for every class.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这可以应用于数据集中的每个类别，从而为每个类别得到这四个值。
- en: The most important metrics we can compute that have the TP, TN, FP, and FN values
    are precision, recall, and the F1 score.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算的最重要的度量标准是精确度、召回率和 F1 分数，这些度量基于 TP、TN、FP 和 FN 的值。
- en: Precision
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度
- en: 'Precision is the number of correct positives results, divided by the number
    of positive results predicted:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是正确正例结果的数量，除以预测的正例结果的数量：
- en: '![](img/c3751881-70da-4d54-a113-fe41e878e2b7.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3751881-70da-4d54-a113-fe41e878e2b7.png)'
- en: 'The metric name itself describes what we measure here: a number in the [0,1]
    range that indicates how accurate the predictions of the classifier are: the higher,
    the better. However, as in the case of accuracy, the precision value alone can
    be misleading. High precision only means that, when we predict the positive class,
    we are precise in detecting it. But this does not mean that we are also accurate
    when we''re not detecting this class.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量名称本身描述了我们在这里测量的内容：一个[0,1]范围内的数字，表示分类器预测的准确度：数值越高越好。然而，与准确率类似，单独依赖精度值可能具有误导性。高精度仅意味着在我们预测为正类时，我们能精确地检测到它。但这并不意味着当我们未检测到该类别时，我们也能准确。
- en: The other metric that should always be measured to understand the complete behavior
    of a classifier is known as recall.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个应始终衡量的度量指标是召回率，以便理解分类器的完整行为。
- en: Recall
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 召回率
- en: 'The recall is the number of correct positive results, divided by the number
    of all relevant samples (for example, all the samples that should be classified
    as positive):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是正确的正类结果的数量，除以所有相关样本的数量（例如，所有应当分类为正类的样本）：
- en: '![](img/825994e6-c10c-4541-ac1c-818ebfd5e110.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/825994e6-c10c-4541-ac1c-818ebfd5e110.png)'
- en: Just like precision, recall is a number in the [0,1] range that indicates the
    percentage of correctly classified samples over all the samples of that class. The
    recall is an important metric, especially in problems such as object detection
    in images.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 像精度一样，召回率是一个[0,1]范围内的数字，表示正确分类的样本在该类别所有样本中的百分比。召回率是一个重要的度量，特别是在图像中的物体检测等问题中。
- en: Measuring the precision and recall of a binary classifier allows you to tune
    the classifier's performance, making it behave as needed.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 测量二分类器的精度和召回率可以帮助你调节分类器的性能，使其按需表现。
- en: Sometimes, precision is more important than recall, and vice versa. For this
    reason, it is worth dedicating a short section to the classifier regime.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，精度比召回率更重要，反之亦然。因此，值得为分类器的工作状态专门设立一个简短的部分。
- en: Classifier regime
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器工作状态
- en: Sometimes, it can be worth putting a classifier in the high-recall regime. This
    means that we prefer to have more false positives while also being sure that the
    true positives are detected.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将分类器放入高召回率状态可能是值得的。这意味着我们更愿意接受更多的假阳性，同时确保真正的阳性被检测出来。
- en: The high-recall regime is often required in computer vision industrial applications,
    where the production line needs to build a product. Then, at the end of the assembly
    process, a human controls whether the quality of the complete product reaches
    the required standard.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 高召回率工作状态通常要求在计算机视觉工业应用中使用，因为生产线需要构建产品。在组装过程结束时，人工检查整个产品的质量是否达到了要求标准。
- en: The computer vision applications that control the assembly robots usually work
    in a high-recall regime since the production line needs to have high throughput.
    Setting the computer vision applications in a high-precision regime would have
    stopped the line too often, reducing the overall throughput and making the company
    lose money.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 控制组装机器人操作的计算机视觉应用通常在高召回率状态下运行，因为生产线需要高吞吐量。如果将计算机视觉应用设置为高精度状态，生产线将经常停顿，减少整体吞吐量，并导致公司损失。
- en: The ability to change the working regime of a classifier is of extreme importance
    in real-life scenarios, where the classifiers are used as production tools that
    should adapt themselves to the business decisions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，改变分类器工作状态的能力至关重要，因为分类器作为生产工具被使用，必须能够根据商业决策进行自我适应。
- en: There are other cases where a high-precision regime is required. In industrial
    scenarios, there are also processes commanded by computer vision applications
    that are critical and for this reason, require high accuracy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些情况需要高精度的工作状态。在工业场景中，一些由计算机视觉应用控制的过程非常关键，因此要求具有高准确度。
- en: In an engine production line, classifiers could be used to decide on which part
    the camera sees is the correct one to pick and to assemble in the engine. In critical
    cases like this one, a high-precision regime is required and a high-recall regime
    is discouraged.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在发动机生产线中，分类器可以用于决定相机看到的零件是否是正确的部件，并将其挑选出来并安装到发动机中。在像这样的关键情况下，需要高精度的工作状态，而高召回率的工作状态则不推荐。
- en: A metric that combines both precision and recall is the F1 score.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一个结合了精确率和召回率的度量是F1分数。
- en: F1 score
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1分数
- en: The F1 score is the harmonic mean between precision and recall. This number,
    which is in the [0,1] range, indicates how precise the classifier is (precision)
    and how robust it is (recall).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是精度和召回率的调和平均数。这个值在 [0,1] 范围内，表示分类器的精度（precision）和它的健壮性（recall）。
- en: 'The greater the F1 score, the better the overall performance of the model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数越大，模型的整体表现越好：
- en: '![](img/10b27933-d943-4046-b469-e4ef8e6715ed.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10b27933-d943-4046-b469-e4ef8e6715ed.png)'
- en: Using the area under the ROC curve
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ROC 曲线下的面积
- en: The area under the  **Receiving Operating Characteristic** (**ROC**) curve is
    one of the most used metrics for the evaluation of binary classification problems.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收操作特征**（**ROC**）曲线下的面积是评估二元分类问题时最常用的指标之一。'
- en: Most classifiers produce a score in the [0,1] range and not directly as a classification
    label. The score must be thresholded to decide the classification. A natural threshold
    is to classify it as positive when the score is higher than 0.5 and negative otherwise,
    but this is not always what our application wants (think about the identification
    of people with a disease).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分类器生成的得分在 [0,1] 范围内，而不是直接作为分类标签。得分必须通过阈值来决定分类。一个自然的阈值是当得分大于 0.5 时分类为正，否则分类为负，但这并不总是我们的应用所需要的（例如，考虑到疾病患者的识别）。
- en: Varying the threshold will change the performance of the classifier, varying
    the number of TPs, FPs, TNs, and FNs, and thereby the overall classification performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 改变阈值会改变分类器的性能，改变 TP、FP、TN 和 FN 的数量，从而影响整体分类表现。
- en: 'The results of the threshold variations can be taken into account by plotting
    the ROC curve. The ROC curve takes into account the false positive rate (specificity)
    and the true positive rate (sensitivity): binary classification problems are a
    trade-off between these two values. We can describe these values as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值变化的结果可以通过绘制 ROC 曲线来考虑。ROC 曲线考虑了假阳性率（特异性）和真正率（敏感性）：二元分类问题是这两个值之间的权衡。我们可以按如下方式描述这些值：
- en: '**Sensitivity**: The true positive rate is defined as the proportion of positive
    data points that are correctly considered positive, with respect to all the positive
    data points:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感性**：真正率定义为被正确视为正类的正类数据点所占的比例，相对于所有正类数据点：'
- en: '![](img/9fd4d354-1d8e-4931-b226-5639f4ba82dd.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fd4d354-1d8e-4931-b226-5639f4ba82dd.png)'
- en: '**Specificity**: The false positive rate is defined as the proportion of negative
    data points that are considered positive, with respect to all the negative data
    points:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性**：假阳性率定义为被认为是正类的负类数据点所占的比例，相对于所有负类数据点：'
- en: '![](img/9525f147-d886-4cfd-97c7-cc5d8a2f0e0f.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9525f147-d886-4cfd-97c7-cc5d8a2f0e0f.png)'
- en: 'The **AUC** is the area under the ROC curve, and is obtained by varying the
    classification threshold:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**AUC** 是 ROC 曲线下的面积，通过改变分类阈值获得：'
- en: '![](img/147a46ca-852a-411c-9921-a970775035ad.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/147a46ca-852a-411c-9921-a970775035ad.png)'
- en: ROC curve obtained by varying the classification threshold. The dashed lines
    represent the expectation for random guessing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变分类阈值获得的 ROC 曲线。虚线表示随机猜测的预期。
- en: It is clear that both TPR and FPR have values in the [0,1] range, and the graph
    is drawn by varying the classification threshold of the classifier in order to
    get different pairs of TPR and FPR for every threshold value. The AUC is in the
    [0,1] range too and the greater the value, the better the model is.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，TPR 和 FPR 的值都在 [0,1] 范围内，图形是通过变化分类器的分类阈值来绘制的，以便在每个阈值下获得不同的 TPR 和 FPR 配对。AUC
    也在 [0,1] 范围内，值越大，模型越好。
- en: If we are interested in measuring the performance of a regressor's precision
    and recall and all the data that was gathered from the confusion matrix is useless,
    then we have to use other metrics to measure the regression error.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对回归器的精度和召回率的表现感兴趣，并且从混淆矩阵中收集的所有数据都没有用处，那么我们必须使用其他指标来衡量回归误差。
- en: Mean absolute error
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: '**Mean absolute error** (**MAE**) is the average of the absolute difference
    between the original and the predicted values. Since we are now interested in
    the measurement of the performance of a regressor, we have to take into account
    that the ![](img/570a994c-8139-422d-9a96-eaa44aee0d5c.png) and ![](img/158c6b30-6485-4e30-82bd-0f112cf80ca0.png) values
    are numerical values:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是原始值和预测值之间绝对差的平均值。由于我们现在关注的是回归器的性能衡量，我们必须考虑到![](img/570a994c-8139-422d-9a96-eaa44aee0d5c.png)和![](img/158c6b30-6485-4e30-82bd-0f112cf80ca0.png)的数值：'
- en: '![](img/a59c0cf9-6fa2-491a-81b8-0fd2b5d640f1.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a59c0cf9-6fa2-491a-81b8-0fd2b5d640f1.png)'
- en: The MAE value has no upper bound, and its lower bound is 0\. It should be evident
    that we want the MAE value to be as close as possible to 0.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: MAE值没有上限，下限为0。显然，我们希望MAE值尽可能接近0。
- en: MAE gives us an indication of how far the predictions are from the actual output;
    this metric is easily interpretable since its value is also on the same scale
    as the original response value.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: MAE为我们提供了预测值与实际输出之间距离的指示；这个指标容易解释，因为其值与原始响应值的量纲相同。
- en: Mean squared error
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: '**Mean squared error** (**MSE**) is the average of the squared difference between
    the original and the predicted values:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）是原始值和预测值之间平方差的平均值：'
- en: '![](img/84bc241c-27ac-4a48-b037-6f19cac05f5f.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84bc241c-27ac-4a48-b037-6f19cac05f5f.png)'
- en: Just like MAE, MSE has no upper bound and its lower bound is 0.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 和MAE一样，MSE没有上限，其下限为0。
- en: On the contrary, the presence of the square terms makes the metric less easy
    to interpret.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，平方项的存在使得该指标不易解释。
- en: A good practice to follow is to consider both metrics so that you get as much
    information as possible about the distribution of the errors.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一种良好的实践是同时考虑这两个指标，以便尽可能多地了解误差的分布。
- en: 'The ![](img/c15eff97-e297-4dad-944d-7e96ba0d778b.png) relation holds, and so
    the following is true:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该![](img/c15eff97-e297-4dad-944d-7e96ba0d778b.png)关系成立，因此以下内容为真：
- en: If MSE is close to MAE, the regressor makes small errors
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果MSE接近MAE，说明回归器的误差较小。
- en: If MSE is close to MAE², the regressor makes large errors
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果MSE接近MAE²，说明回归器的误差较大。
- en: 'Metrics are probably the most important part of the ML model selection and
    performance measurement tools: they express relations between the desired output
    and model output. This relation is fundamental since it is what we want to optimize
    our model for, as we will see in [Chapter 2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml), *Neural
    Networks and Deep Learning*, where we will introduce the concept of the *loss
    function*.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 指标可能是机器学习模型选择和性能衡量工具中最重要的部分：它们表示期望输出和模型输出之间的关系。这个关系是至关重要的，因为它是我们希望优化模型的目标，正如我们将在[第2章](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml)《神经网络与深度学习》中看到的那样，届时我们将介绍*损失函数*的概念。
- en: Moreover, since the models we are treating in this book are all parametric models,
    we can measure the metrics during/at the end of the training process and save
    the model parameters (and by definition, the model) that reached the best validation/test
    performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于本书中讨论的模型都是参数化模型，我们可以在训练过程中的任何时候/结束时衡量指标，并保存达到最佳验证/测试性能的模型参数（并根据定义，保存该模型）。
- en: Using parametric models allows us this kind of flexibility— we can freeze the
    status of a model when it reaches the desired performance and go ahead with training,
    changing hyperparameters, and experimenting with different configurations/training
    strategies, while also having the certainty of having stored a model that already
    has good performance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用参数化模型使我们能够拥有这种灵活性——我们可以在模型达到所需性能时冻结其状态，然后继续进行训练，调整超参数，并尝试不同的配置/训练策略，同时确信我们已经保存了一个具有良好性能的模型。
- en: Having metrics and the ability to measure them during the training process,
    together with the usage of parametric models that can be saved, gives us the power
    to evaluate different models and save only the one that fits our needs best. This
    process is called **model selection** and is fundamental in every well-made ML
    pipeline.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有指标并能够在训练过程中进行度量，同时使用可以保存的参数化模型，使我们能够评估不同的模型，并仅保存最符合我们需求的模型。这个过程被称为**模型选择**，在每个设计良好的机器学习管道中都至关重要。
- en: We've focused on the supervised learning family algorithm a lot, but of course,
    ML is much more than this (even tough supervised learning algorithms have the
    best performance when it comes to solving real-life problems).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入探讨了监督学习算法，但实际上，机器学习远不止如此（尽管在解决实际问题时，监督学习算法的表现最好）。
- en: The next family of algorithms we are briefly going to describe are from the
    unsupervised learning family.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将简要描述的算法来自无监督学习算法家族。
- en: Unsupervised learning
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In comparison to supervised learning, unsupervised learning does not need a
    dataset of labeled examples during the training phase–labels are only needed during
    the testing phase when we want to evaluate the performance of the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于监督学习，无监督学习在训练阶段不需要带标签的样本数据集——标签只在测试阶段需要，当我们想评估模型的表现时才需要。
- en: The purpose of unsupervised learning is to discover natural partitions in the
    training set. What does this mean? Think about the MNIST dataset—it has 10 classes,
    and we know this because every example has a different label in the [1,10] range.
    An unsupervised learning algorithm has to discover that there are 10 different
    objects inside the dataset and does this by looking at the examples without prior
    knowledge of the label.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的目的是发现训练集中的自然分区。这是什么意思呢？想想MNIST数据集——它有10个类别，我们知道这一点是因为每个样本都有一个不同的标签，标签范围在[1,10]之间。无监督学习算法必须发现数据集中有10种不同的对象，并且通过查看样本而不依赖于标签的先验知识来实现这一点。
- en: It is clear that unsupervised learning algorithms are challenging compared to
    supervised learning ones since they cannot rely on the label's information, but
    they have to discover features and learn about the concept of labels by themselves.
    Although challenging, their potential is huge since they discover patterns in
    data that humans can struggle to detect. Unsupervised learning algorithms are
    often used by decision-makers that need to extract meaning from data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，与监督学习算法相比，无监督学习算法更加具有挑战性，因为它们无法依赖标签的信息，必须自行发现特征并了解标签的概念。尽管具有挑战性，但它们的潜力巨大，因为它们能够发现数据中人类难以察觉的模式。无监督学习算法常常被需要从数据中提取意义的决策者所使用。
- en: 'Just think about the problem of fraud detection: you have a set of transactions,
    a huge volume of money exchanged between people, and you don''t know if there
    are fraudulent transactions inside them because there are no labels in the real
    world!'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 想想欺诈检测问题：你有一组交易，存在大量的资金交换，而你并不知道其中是否存在欺诈交易，因为现实世界中并没有标签！
- en: In this scenario, the application of unsupervised learning algorithms could
    help you find the big natural partition of normal transactions and help you discover
    the outliers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，应用无监督学习算法可以帮助你发现正常交易的大自然分区，并帮助你识别异常值。
- en: Outliers are the points outside, and usually far away, from any partition (also
    called a cluster) found in the data, or a partition itself with some particular
    characteristic that makes it different from the normal ones.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是指位于数据中任何分区（也称为簇）外部，且通常远离这些分区的点，或者是具有某些特殊特征的分区，使其不同于正常的分区。
- en: 'Unsupervised learning is, for this reason, used frequently in anomaly detection
    tasks, and in many different domains: not only fraud detection, but also quality
    control in images, video streams, streams of datasets coming from sensors in production
    environments, and much more.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无监督学习在异常检测任务中被广泛应用，并且涵盖了多个领域：不仅限于欺诈检测，还包括图像质量控制、视频流、来自生产环境传感器的数据流等。
- en: 'Unsupervised learning algorithms are two-phase algorithms as well:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法也是两阶段算法：
- en: '**Training and validation**: Since there are no labels inside the training
    set (and they should be discarded if present), the algorithm is trained to discover
    the existing patterns in the data. If there''s a validation set, that should contain
    labels; the model''s performance can be measured at the end of every training
    epoch.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练与验证**：由于训练集内没有标签（如果有标签，则应予以丢弃），所以算法被训练来发现数据中的现有模式。如果有验证集，它应该包含标签；模型的表现可以在每个训练周期结束时进行评估。'
- en: '**Testing**: A labeled dataset is given in the input to the algorithm (if such
    a dataset exists) and its results are compared with the label''s information.
    In this phase, we measure the performance of the algorithm using the label''s
    information in order to verify that the algorithm learned to extract patterns
    from data that humans have also been able to detect.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：将一个带标签的数据集输入到算法中（如果有此类数据集），并将其结果与标签信息进行比较。在此阶段，我们使用标签信息来衡量算法的性能，以验证算法是否学会了从数据中提取人类也能检测到的模式。'
- en: Working on these examples only, unsupervised learning algorithms are not classified
    on the basis of the label type (as the supervised learning algorithms), but on
    what they aim to discover.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅基于这些例子，无监督学习算法并不是按照标签类型进行分类的（如同监督学习算法），而是根据它们的目标进行分类。
- en: 'Unsupervised learning algorithms can be classified as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法可以分类如下：
- en: '**Clustering**: The aim is to discover clusters, that is, natural partitions
    of the data.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：目标是发现聚类，即数据的自然分区。'
- en: '**Association**: In this case, the aim is to discover rules that describe data
    and associations between them. These are usually used to give recommendations:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联**：在这种情况下，目标是发现描述数据及其之间关联的规则。这些规则通常用于提供推荐：'
- en: '![](img/fe4da198-a97a-425c-bdfa-d2fb23601c38.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe4da198-a97a-425c-bdfa-d2fb23601c38.png)'
- en: The unsupervised learning family—there are two main families of algorithms
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习家族——有两大类算法
- en: 'The association learning algorithms are powerful tools of the data mining world:
    they''re used to discover rules, such as "if a person is buying butter and bread,
    they will probably also buy milk". Learning about these rules can be a huge competitive
    advantage in business. By recalling the previous example, we can say that a store
    can place butter, bread, and milk together on the same shelf to maximize selling!'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 关联学习算法是数据挖掘领域的强大工具：它们用于发现规则，例如“如果一个人购买了黄油和面包，他们可能还会购买牛奶”。学习这些规则可以在商业中带来巨大的竞争优势。通过回顾前面的例子，我们可以说，商店可以将黄油、面包和牛奶放在同一货架上，以最大化销售！
- en: During the training phase of a clustering algorithm, we are interested in measuring
    the performance of the model, just like we do in the supervised learning case.
    Metrics, in the case of unsupervised learning algorithms, are more complex and
    task-dependent. What we usually do is exploit additional labels present in the
    dataset, but that aren't used during the training, and thus reconduct the problem
    to a supervised learning problem and use the usual metrics.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类算法的训练阶段，我们关心的是衡量模型的性能，就像在监督学习中一样。在无监督学习算法的情况下，度量标准更为复杂，且依赖于任务。我们通常做的是利用数据集中存在的附加标签，但这些标签在训练过程中不会被使用，从而将问题重新转化为一个监督学习问题，并使用常规的度量标准。
- en: As in the case of supervised learning, there are parametric and non-parametric
    models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习的情况一样，也有参数模型和非参数模型。
- en: Most non-parametric algorithms work by measuring the distance between a data
    point and every other data point in the dataset; then, they use the distance information
    to cluster the data space in different regions.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数非参数算法通过衡量数据点与数据集中其他每个数据点之间的距离来工作；然后，它们使用距离信息将数据空间划分为不同的区域。
- en: Like in the supervised learning case, a lot of algorithms have been developed
    over the years to find natural partitions and/or rules in non-labeled datasets.
    However, neural networks have been applied to solve unsupervised learning tasks
    and have achieved superior performance and shown to be very flexible. This is
    another reason why this book only focuses on neural networks.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习的情况类似，随着时间的推移，已经开发出许多算法，用于在无标签数据集中发现自然分区和/或规则。然而，神经网络已被应用于解决无监督学习任务，并取得了卓越的表现，且显示出极高的灵活性。这也是本书仅专注于神经网络的另一个原因。
- en: Unsupervised learning algorithms explicitly require to do not have any label
    information during the training phase. However, since labels could be present
    in the datasets, why not take advantage of their presence while still using a
    ML algorithm to discover other patterns in the data?
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法明确要求在训练阶段不使用任何标签信息。然而，由于数据集中可能存在标签，为什么不在仍使用机器学习算法发现数据中其他模式的同时，利用标签的存在呢？
- en: Semi-supervised learning
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Semi-supervised learning algorithms fall between supervised and unsupervised
    learning algorithms.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习算法介于监督学习和无监督学习算法之间。
- en: They rely upon the assumption that we can exploit the information of the labeled
    data to improve the result of unsupervised learning algorithms and vice versa.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 它们依赖于这样一个假设：我们可以利用有标签数据的信息来改进无监督学习算法的结果，反之亦然。
- en: 'Being able to use semi-supervised learning algorithms depends on the available
    data: if we have only labeled data, we can use supervised learning; if we don''t
    have any labeled data, we must go with unsupervised learning methods. However,
    let''s say we have the following:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 是否能够使用半监督学习算法取决于可用的数据：如果只有有标签的数据，可以使用监督学习；如果没有任何标签数据，则必须使用无监督学习方法。然而，假设我们有以下情况：
- en: Labeled and unlabeled examples
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有标签和无标签的示例
- en: Examples that are all labeled with the same class
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有标签为相同类别的示例
- en: If we have these, then we can use a semi-supervised approach to solve the problem.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有这些数据，那么我们可以使用半监督方法来解决这个问题。
- en: The scenario in which we have all the examples labeled with the same class could
    look like a supervised learning problem, but it isn't.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有示例的标签都属于同一类别，这种情况看起来像是一个监督学习问题，但实际上并不是。
- en: If the aim of the classification is to find a boundary that divides at least
    two regions, how can we define a boundary among regions if we only have a single
    region?
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类的目标是找到一个分割至少两个区域的边界，当我们只有一个区域时，如何定义区域之间的边界？
- en: We can't!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 不能！
- en: 'An unsupervised or semi-supervised learning approach is the way to go for these
    kinds of problems: the algorithm will learn how the input space is partitioned
    (hopefully, in one single cluster), its shape, and how the data is distributed
    in the space.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这类问题，无监督或半监督学习方法是最佳选择：算法将学习如何划分输入空间（希望能在一个单一的聚类中），其形状，以及数据在空间中的分布情况。
- en: An unsupervised learning approach could be used to learn that there is a single
    cluster in the data. By using the labels, and thereby switching to a semi-supervised
    learning approach, we can enforce some additional constraints on the space so
    that we lean toward a better representation of the data.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用无监督学习方法来学习数据中是否存在单一的聚类。通过使用标签，并因此转向半监督学习方法，我们可以对空间施加一些额外的约束，从而使数据的表示更加合理。
- en: Once the unsupervised/semi-supervised learning algorithm has learned about a
    representation of the data, we can test whether a new example—one that we have
    never seen during the training process—falls inside the cluster or not. Alternatively,
    we can calculate a numerical score that tells us "how much" the new example fits
    inside the learned representation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦无监督/半监督学习算法学习到数据的表示，我们就可以测试一个新示例——一个在训练过程中从未见过的示例——是否落入聚类中。或者，我们可以计算一个数值分数，告诉我们这个新示例“有多大”符合已学习的表示。
- en: Just like the unsupervised learning algorithms, the semi-supervised algorithm
    has two phases.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督学习算法一样，半监督算法也有两个阶段。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through the ML algorithm families from a general and
    theoretical point of view. It is essential to have good knowledge of what machine
    learning is, how algorithms are categorized, what kind of algorithms are used
    given a certain task, and how to become familiar with all the concepts and the
    terminology that's used among machine learning practitioners.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从一般和理论的角度讲解了机器学习算法的家族。了解机器学习是什么、算法如何分类、在特定任务下使用何种算法，以及如何熟悉机器学习从业者使用的所有概念和术语是非常重要的。
- en: In the next chapter, [Chapter 2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml), *Neural
    Networks and Deep Learning*, we will focus on neural networks. We will understand
    the strengths of machine learning models, how is it possible to make a network
    learn, and how, in practice, a model parameter update is performed.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第2章](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml)，*神经网络与深度学习*，我们将专注于神经网络。我们将理解机器学习模型的优点，如何让一个网络学习，以及在实践中如何执行模型参数更新。
- en: Exercises
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Answering the following questions is of extreme importance: you are building
    your ML foundations—do not skip this step!'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题至关重要：你正在建立你的机器学习基础——不要跳过这个步骤！
- en: Given a dataset of 1,000 labeled examples, what do you have to do if you want
    to measure the performance of a supervised learning algorithm during the training,
    validation, and test phases, while using accuracy as the unique metric?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个包含1000个标签示例的数据集，如果你想在训练、验证和测试阶段使用准确率作为唯一指标来衡量监督学习算法的表现，应该怎么做？
- en: What is the difference between supervised and unsupervised learning?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习有什么区别？
- en: What is the difference between precision and recall?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确度和召回率有什么区别？
- en: A model in a high-recall regime produces more or less false positives than a
    model in a low recall regime?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在高召回率模式下，模型产生的假阳性多还是少？与低召回率模式的模型相比，如何？
- en: Can the confusion matrix only be used in a binary classification problem? If
    not, how can we use it in a multiclass classification problem?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵只能用于二分类问题吗？如果不是，我们如何在多类分类问题中使用它？
- en: Is one-class classification a supervised learning problem? If yes, why? If no,
    why?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一类分类是监督学习问题吗？如果是，为什么？如果不是，为什么？
- en: If a binary classifier has an AUC of 0.5, what can you conclude from this?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个二分类器的AUC为0.5，你能从中得出什么结论？
- en: Write the formula of precision, recall, F1-score, and accuracy. Why is F1 important?
    Is there a relationship between accuracy, precision, and recall?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出精确度、召回率、F1分数和准确率的公式。为什么F1分数很重要？准确率、精确度和召回率之间有关系吗？
- en: 'The true positive rate and false positive rate are used to plot the ROC curve.
    What is the ROC curve''s purpose, and is there a relationship among the true positive
    rate/false positive rate and precision/recall? Hint: write the math.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真阳性率和假阳性率用于绘制ROC曲线。ROC曲线的目的是什么？真阳性率/假阳性率与精确度/召回率之间有关系吗？提示：写出数学公式。
- en: What is the curse of dimensionality?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是维度灾难？
- en: What are overfitting and underfitting?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是过拟合和欠拟合？
- en: What is the learning capacity of a model? Is it related to the condition of
    overfitting/underfitting?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的学习能力是什么？它是否与过拟合/欠拟合的情况相关？
- en: Write the *Lp* norm formula—is this the only way to measure the distance among
    points?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出*Lp*范数的公式——这是不是衡量点与点之间距离的唯一方式？
- en: How can we say that a data point is similar to another data point?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何判断一个数据点与另一个数据点相似？
- en: What is model selection? Why is it important?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是模型选择？它为什么重要？
