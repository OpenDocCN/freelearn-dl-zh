- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Solving Classification Problems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决分类问题
- en: In the previous chapters, we learned how to set up and run MXNet, how to work
    with Gluon and DataLoader, and how to visualize datasets for regression, classification,
    image, and text problems. We also discussed the different learning methodologies.
    In this chapter, we are going to focus on supervised learning with classification
    problems. We will learn why these problems are suitable for deep learning models
    with an overview of the equations that define these problems. We will learn how
    to create suitable models for them and how to train them, emphasizing the choice
    of hyperparameters. We will end each section by evaluating the models according
    to our data, as expected in supervised learning, and we will look at the different
    evaluation criteria for classification problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何设置和运行 MXNet，如何使用 Gluon 和 DataLoader，以及如何可视化回归、分类、图像和文本问题的数据集。我们还讨论了不同的学习方法。在本章中，我们将重点讨论带有分类问题的监督学习。我们将学习为什么这些问题适合深度学习模型，并概览定义这些问题的方程。我们将学习如何为这些问题创建合适的模型并进行训练，重点讲解超参数的选择。每一节结束时，我们都会根据我们的数据评估模型，这是监督学习中的标准做法，并且我们将查看分类问题的不同评估标准。
- en: 'Specifically, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下几个实例：
- en: Understanding math for classification models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类模型的数学原理
- en: Defining loss functions and evaluation metrics for classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义分类的损失函数和评估指标
- en: Training for classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型的训练
- en: Evaluating classification models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*前言*中指定的技术要求外，还需满足以下技术要求：
- en: Ensure that you have completed the first recipe, *Installing MXNet, Gluon, GluonCV
    and GluonNLP*, from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running*
    *with MXNet*.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您已完成第一个实例，*安装 MXNet、Gluon、GluonCV 和 GluonNLP*，该实例来自 [*第 1 章*](B16591_01.xhtml#_idTextAnchor016)，*使用
    MXNet 搭建环境*。
- en: 'Ensure that you have completed the second recipe, *Toy dataset for classification
    – Loading, Managing, and Visualizing Iris Dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader.*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您已经完成了第二个实例，*分类的玩具数据集——加载、管理和可视化鸢尾花数据集*，该实例来自 [*第 2 章*](B16591_02.xhtml#_idTextAnchor029)，*使用
    MXNet 和可视化数据集：Gluon 和 DataLoader*。
- en: Most of the concepts for the model, the loss and evaluation functions, and the
    training were introduced in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving
    Regression Problems*. Furthermore, as we will see in this chapter, classification
    can be seen as a special case of regression. Therefore, it is strongly recommended
    to complete [*Chapter* *3*](B16591_03.xhtml#_idTextAnchor052) first.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型、损失和评估函数以及训练的大部分概念已在 [*第 3 章*](B16591_03.xhtml#_idTextAnchor052)，*解决回归问题*
    中介绍。此外，正如我们将在本章中看到的，分类可以被视为回归的一个特例。因此，强烈建议先完成 [*第 3 章*](B16591_03.xhtml#_idTextAnchor052)。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub 链接找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04)。
- en: 'Furthermore, you can access each recipe directly from Google Colab; for example,
    for the first recipe of this chapter, visit the following link: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以直接从 Google Colab 访问每个实例；例如，访问本章第一个实例的链接：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb)。
- en: Understanding math for classification models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类模型的数学原理
- en: As we saw in the previous chapter, **classification** problems are **supervised
    learning** problems whose output is a class from a set of classes (categorical
    assignments) – for example, the *iris* class of a flower.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，**分类**问题是**监督学习**问题，其输出是一个类，该类来自一组类（分类分配）——例如，花朵的 *鸢尾花* 类。
- en: As we will see throughout this recipe, classification models can be seen as
    individual cases of regression models. We will start by exploring a binary classification
    model. This is a model that will output one of two classes. We will label these
    classes `[0, 1]` for simplicity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本食谱中看到的，分类模型可以看作是回归模型的个别案例。我们将从探索一个二元分类模型开始。这个模型将输出两个类别中的一个。为了简便起见，我们将这两个类别标记为`[0,
    1]`。
- en: The simplest model we can use for such a binary classification problem is a
    **linear regression** model. This model will output a number; therefore, to modify
    the output to satisfy our new classification criteria, we will modify the activation
    function to a more suitable one.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来解决这种二元分类问题的最简单模型是**线性回归**模型。这个模型将输出一个数字；因此，为了修改输出以满足我们新的分类标准，我们将修改激活函数为更合适的函数。
- en: 'As in the previous recipes, we will use a neural network as our model, and
    we will solve the iris dataset prediction problem we introduced in the second
    recipe, *Toy dataset for classification: Load, Manage and Visualize Iris Dataset*,
    in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029), *Working with MXNet and Visualizing
    Datasets: Gluon* *and DataLoader.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的食谱一样，我们将使用神经网络作为我们的模型，我们将解决在第二个食谱中介绍的鸢尾花数据集预测问题，*分类的玩具数据集：加载、管理和可视化鸢尾花数据集*，见
    [*第二章*](B16591_02.xhtml#_idTextAnchor029)，*使用 MXNet 和可视化数据集：Gluon* *和 DataLoader*。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be building upon the knowledge we gained in the previous recipes; therefore,
    it is highly recommended to read them. Moreover, as mentioned in the previous
    chapter, before jumping to understand our model, for the math part of this recipe,
    we will be using a little bit of matrix operations and linear algebra, but it
    will not be hard at all.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在之前食谱中获得的知识基础上进行构建，因此强烈建议阅读它们。此外，正如前一章所提到的，在深入理解我们的模型之前，本食谱中的数学部分将涉及一些矩阵运算和线性代数，但这并不难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将查看以下步骤：
- en: Defining a binary classification model
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个二元分类模型
- en: Defining a multi-label classification model
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个多标签分类模型
- en: Defining the features
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义特征
- en: Initializing the model
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型
- en: Evaluating the model
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Defining a binary classification model
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个二元分类模型
- en: 'This is the perceptron model we introduced in the previous recipes:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在之前的食谱中介绍的感知机模型：
- en: '![Figure 4.1 – Perceptron](img/B16591_04_1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 感知机](img/B16591_04_1.jpg)'
- en: Figure 4.1 – Perceptron
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 感知机
- en: This model could be mathematically described as *y = f(WX + b)*, where *W* is
    the weight vector *[W*1*, W*2*, …. W*n*]*, (*n* is the number of features), *X*
    is the feature vector *[X*1*, X*2*, …. X*n*]*, *b* is the bias term, and *f()*
    is the activation function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以数学地描述为 *y = f(WX + b)*，其中 *W* 是权重向量 *[W*1*, W*2*, …. W*n*]*，（*n* 是特征的数量），*X*
    是特征向量 *[X*1*, X*2*, …. X*n*]*，*b* 是偏置项，*f()* 是激活函数。
- en: In the regression use case, we chose the activation function as the identity
    function, which provided an output equal to the input; therefore, we had *y =
    WX +* *b*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归使用案例中，我们选择了恒等函数作为激活函数，这提供了一个等于输入的输出；因此，我们有了 *y = WX +* *b*。
- en: For our binary classification use case, we want to have an output that will
    help us classify our input data points into two classes (`0` and `1`). In the
    original Perceptron paper in 1958, Rosenblatt, who studied a binary classification
    problem, chose the step function, which provided 0 and 1 as its only possible
    output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的二元分类使用案例，我们希望有一个输出，帮助我们将输入数据点分类为两个类别（`0`和`1`）。在1958年原始的感知机论文中，Rosenblatt研究了一个二元分类问题，选择了阶跃函数，该函数只提供0和1作为其唯一可能的输出。
- en: If we recall from [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving
    Regression Problems*, in the third recipe, *Loss functions and evaluation metrics
    for regression*, we imposed certain properties on those functions. The fourth
    property, differentiability, was required due to the computations required by
    gradient descent. The same property applies to activation functions, and the step
    function Rosenblatt used does not comply with it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下 [*第三章*](B16591_03.xhtml#_idTextAnchor052)，*解决回归问题*，在第三个食谱中，*回归的损失函数和评估指标*，我们对这些函数施加了一些属性。第四个属性，可微性，是由于梯度下降所需的计算。这一属性同样适用于激活函数，而Rosenblatt使用的阶跃函数并不符合这一要求。
- en: Furthermore, if we could find a function that is also continuous between 0 and
    1, we could assess that number as the probability or confidence of the output
    being `1`, as assessed by the model. This approach has advantages that we will
    explore later in the recipe.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们能够找到一个在0到1之间连续的函数，我们就可以将这个数值评估为输出为`1`的概率或置信度，这是由模型评估得出的。这个方法具有一些优势，我们将在后面的配方中进行探索。
- en: 'Therefore, as the step function does not fulfill our properties, we need a
    new activation function. The most common activation function as the output of
    binary classification models is the **sigmoid** function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于阶跃函数不满足我们的特性，我们需要一个新的激活函数。最常用的二分类模型输出激活函数是**sigmoid**函数：
- en: '![Figure 4.2 – Sigmoid activation function](img/B16591_04_2.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – Sigmoid 激活函数](img/B16591_04_2.jpg)'
- en: Figure 4.2 – Sigmoid activation function
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – Sigmoid 激活函数
- en: The sigmoid function complies with all the required properties and the output
    quickly becomes `0` or `1`, which allows us to identify the output class suggested
    by the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数符合所有必需的属性，且输出会迅速变为`0`或`1`，这使得我们能够识别模型建议的输出类别。
- en: Defining a multi-label classification model
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义多标签分类模型
- en: What happens when we have multiple (let’s call this number *k*) classes instead
    of having just two classes to classify as we just saw? In this case, we need a
    different network architecture for our model. On the one hand, one output will
    not suffice now, as we need to have *k* different outputs. On the other hand,
    although we could use the sigmoid function as the activation function for each
    of the outputs, it is very useful if each output can be assessed as probabilities
    of each class, as we saw for the binary case. Using sigmoid will not enforce the
    condition for probabilities that the sum of all of them must be 1 (which means
    that each of the inputs must correspond to one of the classes).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有多个（我们称这个数字为*k*）类别，而不是像我们刚才看到的那样只有两个类别进行分类时，会发生什么呢？在这种情况下，我们需要为我们的模型设计一个不同的网络架构。一方面，单个输出将不再足够，因为我们需要*k*个不同的输出。另一方面，尽管我们可以为每个输出使用sigmoid函数作为激活函数，但如果每个输出都能被评估为每个类别的概率，那就非常有用，就像我们在二分类情况下看到的那样。使用sigmoid函数不会强制满足概率条件，即所有概率之和必须为1（这意味着每个输入必须对应于其中一个类别）。
- en: 'In this case, a function very similar to the sigmoid function that satisfies
    the conditions described is the softmax function:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个与sigmoid函数非常相似的函数，能够满足所描述的条件，是softmax函数：
- en: σ(x j) =  ⅇ x j _ Σ i ⅇ x i
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: σ(x j) =  ⅇ x j _ Σ i ⅇ x i
- en: 'The class that will be selected is the one that will output the maximum value:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将要选择的类别是输出最大值的类别：
- en: '![Figure 4.3 – Multi-label classification network](img/B16591_04_3.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 多标签分类网络](img/B16591_04_3.jpg)'
- en: Figure 4.3 – Multi-label classification network
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 多标签分类网络
- en: 'The full definition of our perceptron is *Y = f(WX + B)*, where *W* is now
    a weight matrix (with *n* x *k* shape: *n* being the number of features and *k*
    the number of outputs), *X* is the feature vector (*n* components), *B* is the
    bias vector (*k* components), and *f()* is the softmax activation function. *Y*
    is now a vector of *k* outputs, where the class with the maximum value will be
    the class assigned to the input.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的感知机的完整定义是*Y = f(WX + B)*，其中*W*现在是一个权重矩阵（形状为*n* x *k*：*n*是特征的数量，*k*是输出的数量），*X*是特征向量（*n*个分量），*B*是偏置向量（*k*个分量），而*f()*是softmax激活函数。*Y*现在是一个*k*个输出的向量，其中最大值所在的类别将被分配给该输入。
- en: Defining the features
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义特征
- en: So far, we have defined our model and its behavior theoretically; we did not
    use our problem framing or our dataset to define it. In this section, we will
    start working at a more practical level.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经理论上定义了我们的模型及其行为；我们没有使用问题框架或数据集来定义它。在本节中，我们将开始以更实际的方式进行工作。
- en: 'The next step to continue defining our model is to decide on which features
    (inputs) we are going to work with. We will continue using the Iris dataset we
    already know from the second recipe, *Toy dataset for classification: loading,
    managing, and visualizing the House Sales dataset* in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*. This dataset
    contained data for 150 flowers, including the class and 4 input features:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们模型的下一步是决定我们要使用哪些特征（输入）。我们将继续使用第二个配方中已知的Iris数据集，*分类的玩具数据集：加载、管理和可视化房屋销售数据集*，[*第2章*](B16591_02.xhtml#_idTextAnchor029)，*与MXNet一起工作并可视化数据集：Gluon和DataLoader*。该数据集包含150朵花的数据，包括类别和4个输入特征：
- en: Sepal length (in cm)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度（单位：厘米）
- en: Sepal width (in cm)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度（单位：厘米）
- en: Petal length (in cm)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（单位：厘米）
- en: Petal width (in cm)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（单位：厘米）
- en: 'If we show the first five flowers, we will see the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们展示前五朵花，我们将看到以下内容：
- en: '![Figure 4.4 – Features of flowers (Iris dataset)](img/B16591_04_4.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 花卉特征（鸢尾花数据集）](img/B16591_04_4.jpg)'
- en: Figure 4.4 – Features of flowers (Iris dataset)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 花卉特征（鸢尾花数据集）
- en: 'Furthermore, for the Iris dataset, we have different output classes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于鸢尾花数据集，我们有不同的输出类别：
- en: '`Setosa (0)`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Setosa (0)`'
- en: '`Versicolor (1)`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Versicolor (1)`'
- en: '`Virginica (2)`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Virginica (2)`'
- en: Initializing the model
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化模型
- en: 'Now that we have defined the input dimension (number of features) and output
    dimensions, we can initialize our model using random initialization:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了输入维度（特征数量）和输出维度，我们可以使用随机初始化来初始化我们的模型：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If we compare these values to the values we obtained in the first recipe, *Understanding
    math for regression models*, from [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052),
    *Solving Regression Problems*, we can see how the weights are now represented
    as a matrix as we have several outputs, not just one (as it was in the regression
    case), and the bias is a vector instead of a number, for the same reason.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些值与在第一章配方中获得的值进行比较，*理解回归模型的数学*，来自[*第3章*](B16591_03.xhtml#_idTextAnchor052)，*解决回归问题*，我们可以看到，随着输出的不再只有一个（如回归模型中的情况），权重现在被表示为矩阵，偏差是一个向量而不是一个数字，原因是相同的。
- en: Evaluating the model
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now that our model is initialized, we can use it to estimate the class of the
    first flower, which can be seen in *Figure 3**.24* to be *Setosa* (`0`). Here
    it is with our current model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经初始化，我们可以用它来估计第一朵花的类别。从*图 3.24*中可以看出它是*Setosa*（`0`）。以下是使用我们当前模型的结果：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Nicely done! Unfortunately, this was pure chance as the model was randomly initialized.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 干得漂亮！不幸的是，这完全是偶然的，因为模型是随机初始化的。
- en: In the next recipe, we will see how to properly evaluate our classification
    models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将学习如何正确评估我们的分类模型。
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Like regression, classification models can have as many layers (depth) as needed,
    stacking as many of them as the problem’s solution requires.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 像回归一样，分类模型可以根据需要具有任意多的层（深度），堆叠多个层，直到问题的解决方案所需。
- en: In this recipe, we described the modifications from the perceptron described
    in the first recipe, *Understanding maths for regression models*, in [*Chapter
    3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression Problems*. There were
    two main modifications. The first one is that, as in this case, we want to categorize
    each input into a set of classes, we need one output per class. Moreover, in order
    to be able to understand the outputs of our model as probabilities, we needed
    a new activation function, softmax.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们描述了从第一章配方中所述的感知器的修改，*理解回归模型的数学*，来自[*第3章*](B16591_03.xhtml#_idTextAnchor052)，*解决回归问题*。主要有两项修改。第一项是，由于在这种情况下我们希望将每个输入分类到一组类别中，我们需要每个类别一个输出。此外，为了能够理解我们模型的输出为概率，我们需要一个新的激活函数：softmax。
- en: To finalize, we learned how to initialize our model, the effect initialization
    has on the weights and bias, and how we can use our data to evaluate it. We will
    develop all these topics further in the later recipes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何初始化我们的模型、初始化对权重和偏差的影响，以及如何使用数据进行评估。我们将在后续的配方中进一步展开这些话题。
- en: There’s more…
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'At the beginning of the recipe, we walked through the activation function changes
    from Rosenblatt (step function) to regression (linear) to classification (sigmoid).
    One of the details we discussed was the non-differentiability of the step function.
    A deeper analysis can be found at the following link: [https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative](https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方的开始，我们回顾了从Rosenblatt（阶跃函数）到回归（线性）再到分类（sigmoid）的激活函数变化。我们讨论的一个细节是阶跃函数的不可微性。更深入的分析可以通过以下链接查看：[https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative](https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative)
- en: 'Using multi-layer architectures and/or sigmoid (or other activation functions)
    provides neural networks with the capability to approximate any function, which
    is known as the **Universal Approximation Theorem**. More details can be found
    here: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多层架构和/或 sigmoid（或其他激活函数）使得神经网络具备近似任何函数的能力，这被称为**普适逼近定理**。更多细节可以在这里找到：[https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
- en: Defining loss functions and evaluation metrics for classification
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义分类的损失函数和评估指标
- en: In the previous recipe, we defined our input features, described our model,
    and initialized it. At that point, we passed a features vector of a flower to
    predict its iris species, calculated the output, and compared it against the expected
    class.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们定义了输入特征，描述了我们的模型并进行了初始化。那时，我们传递了一朵花的特征向量来预测它的鸢尾花种类，计算了输出并与预期类别进行了比较。
- en: We also showed how those preliminary results did not represent a proper evaluation.
    In this recipe, we will explore the topic of evaluating our classification models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了如何这些初步结果并不能代表一个合适的评估。在本配方中，我们将探讨评估分类模型的话题。
- en: Furthermore, we will also understand which loss functions fit best for the binary
    and multi-label classification problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将理解哪些损失函数最适合二元和多标签分类问题。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: Loss functions and evaluation functions need to satisfy the same properties
    that are described in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving
    Regression Problems*, in the second recipe, *Defining Loss functions and evaluation
    metrics for regression*; therefore, I recommend reading that chapter first for
    a more thorough understanding.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和评估函数需要满足在[*第 3 章*](B16591_03.xhtml#_idTextAnchor052)中描述的相同属性，*解决回归问题*，在第二个配方中，*定义回归的损失函数和评估指标*；因此，我建议首先阅读该章节，以便更全面地理解。
- en: We will start developing our topics by analyzing the binary classification approach
    (two output classes), and we will generalize afterward to the multiple-label classification
    approach.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从分析二元分类方法（两个输出类别）开始，随后推广到多标签分类方法。
- en: How to do it...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s discuss some evaluation and loss functions and analyze their advantages
    and disadvantages. The functions we will describe are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些评估和损失函数，并分析它们的优缺点。我们将描述的函数如下：
- en: Cross-entropy loss function
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵损失函数
- en: Evaluation – confusion matrix
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 – 混淆矩阵
- en: Evaluation – metrics
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 – 指标
- en: Cross-entropy loss function
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉熵损失函数
- en: As we discussed in our previous recipe, once the model has output a *probability*
    for each of our classes, we want to select the class that has the maximum probability
    as the output of our model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的配方中讨论的，一旦模型为每个类别输出了*概率*，我们希望选择具有最大概率的类别作为我们模型的输出。
- en: When optimizing our model parameters, what we want is to find out which model
    parameters provide a maximum probability (`1`) for our desired class and a minimum
    probability for the rest (`0`). The derivation of the equation is out of the scope
    of the book, but you can find more information in *There’s more...* section at
    the end of the recipe.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化模型参数时，我们的目标是找出哪些模型参数为我们期望的类别提供最大的概率（`1`），并为其他类别提供最小的概率（`0`）。该方程的推导超出了本书的范围，但你可以在配方末尾的*更多信息...*部分找到更多内容。
- en: 'For the case of two output classes, we have the binary cross-entropy loss (for
    *N* samples):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个输出类别的情况，我们有二元交叉熵损失（对于*N*个样本）：
- en: BCE = −  1 _ N  ∑ i=0 N y i . log( ˆ y  i) + (1 − y i) . log(1 −  ˆ y  i)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: BCE = −  1 _ N  ∑ i=0 N y i . log( ˆ y  i) + (1 − y i) . log(1 −  ˆ y  i)
- en: 'We can plot this function for one sample, and assume the expected output to
    be *1 (yi =* *1)*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为一个样本绘制该函数，并假设预期输出为*1 (yi =* *1)*：
- en: '![Figure 4.5 – Binary cross-entropy loss graph (yi = 1)](img/B16591_04_5.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 二元交叉熵损失图 (yi = 1)](img/B16591_04_5.jpg)'
- en: Figure 4.5 – Binary cross-entropy loss graph (yi = 1)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 二元交叉熵损失图 (yi = 1)
- en: 'For the general case of multiple labels, the multi-label or categorical cross-entropy
    loss for *M* classes becomes the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多标签的常见情况，*M*个类别的多标签或类别交叉熵损失为以下形式：
- en: L (y, ŷ) = − ∑ j=0 M ∑ i=0  M ( y ij * log ( ŷ ij))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: L (y, ŷ) = − ∑ j=0 M ∑ i=0  M ( y ij * log ( ŷ ij))
- en: This equation yields the same graph as *Figure 4**.5* when comparing each pair
    of classes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程在比较每一对类别时，得到与*图 4.5*相同的图形。
- en: We will use this function in combination with an optimizer during the training
    loop in the next recipe.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节的训练循环中，将此功能与优化器结合使用。
- en: Evaluation – confusion matrix
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估 – 混淆矩阵
- en: 'The confusion matrix helps us measure the performance of our model, comparing
    the results between the expected values (ground truth) and the actual values our
    model will provide. For a binary classification problem (where we defined the
    classes as `Positive` and `Negative`), we have the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵帮助我们衡量模型的表现，通过将预期值（真实值）与模型实际提供的值进行比较。对于二分类问题（我们将类别定义为`Positive`和`Negative`），我们有以下公式：
- en: '![Figure 4.6 – Binary confusion matrix](img/B16591_04_6.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 二分类混淆矩阵](img/B16591_04_6.jpg)'
- en: Figure 4.6 – Binary confusion matrix
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 二分类混淆矩阵
- en: 'In *Figure 4**.6*, for each combination of predicted and actual class, we have
    the following terms (only valid in binary classification):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4.6*中，对于每种预测类别和实际类别的组合，我们有以下术语（仅在二分类中有效）：
- en: '**TP**: True positives.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TP**: 真阳性。'
- en: '**FP**: False positives. It is also known as a *type* *I* error.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP**: 假阳性。也称为*一型* *错误*。'
- en: '**FN**: False negatives. It is also known as a *type* *II* error.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FN**: 假阴性。也称为*二型* *错误*。'
- en: '**TN**: True negatives.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TN**: 真阴性。'
- en: Ideally, we want TP and TN to be as close to 100% as possible, and FP and FN
    as close to 0% as possible.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望TP和TN尽可能接近100%，而FP和FN尽可能接近0%。
- en: 'When we have a multi-label classification problem, in the confusion matrix,
    we will have one row and one column per class. For *K* classes, we will have a
    *KxK* matrix where, in the matrix main diagonal, we are looking for 100% probabilities
    and 0% in the rest of the values. For example, using our Iris dataset (three output
    classes), we can compute the confusion matrix (*3x3*) for a model just randomly
    initialized, similar to the one we computed in the previous recipe. In this case,
    we obtain the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们面临多标签分类问题时，在混淆矩阵中，每个类别都会对应一行一列。对于*K*个类别，我们将得到一个*KxK*的矩阵，在矩阵的主对角线上，我们期望看到100%的概率，其他位置的值为0%。例如，使用我们的鸢尾花数据集（三个输出类别），我们可以计算一个模型的混淆矩阵（*3x3*），该模型是随机初始化的，类似于我们在前一个示例中计算的矩阵。在这种情况下，我们得到以下结果：
- en: '![Figure 4.7 – Multi-label confusion matrix](img/B16591_04_7.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 多标签混淆矩阵](img/B16591_04_7.jpg)'
- en: Figure 4.7 – Multi-label confusion matrix
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 多标签混淆矩阵
- en: As expected, the results shown are quite bad. Notably, not a single versicolor
    flower was correctly classified; however, this example helps us visualize a multi-label
    confusion matrix.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，结果显示得非常糟糕。特别是，没有一朵变色鸢尾花被正确分类；然而，这个例子帮助我们可视化了一个多标签混淆矩阵。
- en: Evaluation – accuracy, precision, recall, specificity, and F1-score metrics
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估 – 准确率、精确率、召回率、特异性和F1分数指标
- en: 'To characterize the performance of a model for a given binary classification
    problem, there are several interesting metrics:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述一个模型在给定二分类问题上的表现，有几个有趣的指标：
- en: Precision =  TP _ TP + FP
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率 =  TP / (TP + FP)
- en: Recall =  TP _ TP + FN
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率 =  TP / (TP + FN)
- en: F1 =  2 × Precision × Recall  _______________  Precision + Recall
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: F1 =  2 × 精确率 × 召回率  / (精确率 + 召回率)
- en: Accuracy =  TP + TN ______________  TP + FN + TN + FP
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 =  (TP + TN) / (TP + FN + TN + FP)
- en: Specificity =  TN _ TN + FP
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性 =  TN / (TN + FP)
- en: 'Each of these metrics serves the purpose of helping us understand the performance
    of our models:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个指标的目的是帮助我们理解模型的表现：
- en: '**Accuracy**: From all the values, which ones were correctly classified (for
    both classes)?'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**: 在所有值中，哪些是被正确分类的（对于两个类别）？'
- en: '**Precision**: This is the rate of positive predictions that were correctly
    classified. However, it does not provide any information regarding negative predictions.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**: 这是正确分类的正预测的比率。然而，它不提供关于负预测的任何信息。'
- en: '**Recall**: This is the rate of positive labels that were correctly classified
    by the model. This figure does not include any information regarding negative
    labels.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**: 这是正确分类的正标签的比率。此数字不包括有关负标签的信息。'
- en: '**Specificity**: Similar to recall but for negative labels. This is the rate
    of negative labels that were correctly classified by the model. This figure does
    not include any information regarding positive labels. This metric is seldom used.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性**: 类似于召回率，但针对负标签。这是模型正确分类的负标签的比率。此指标不包括任何有关正标签的信息。此指标很少使用。'
- en: '**F1 score**: This is the harmonic mean of precision and recall. By combining
    both metrics, this metric provides a better assessment of the model considering
    positive and negative classes. To achieve a high F1 score, the model needs to
    have a high precision and a high recall. A low F1 score will indicate that either
    precision, recall, or both are low.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1得分**：这是精确率和召回率的调和平均数。通过结合这两个指标，这个指标提供了一个更好的模型评估，考虑了正类和负类。要获得高F1得分，模型需要具备高精确率和高召回率。低F1得分则意味着精确率、召回率或两者都很低。'
- en: 'These metrics can be computed for the multi-label scenario. For example, for
    our randomly initialized model and the Iris datasets, these were the figures computed
    (except specificity, which does not have a metrics function in **scikit-learn**):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标可以用于多标签场景。例如，对于我们的随机初始化模型和鸢尾花数据集，计算出的结果如下（除了特异度，它在**scikit-learn**中没有对应的指标函数）：
- en: '[PRE2]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The values are very close to average results, as expected from a randomly initialized
    model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值非常接近平均结果，这是由于随机初始化模型所预期的表现。
- en: Evaluation – Area Under the Curve (AUC)
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估 – 曲线下面积（AUC）
- en: For binary classification problems, the output we want to provide is a class
    (`Positive` or `Negative`); however, the output of our model is a number (the
    probability of a positive result). To transform this result into a class, we need
    to apply a threshold.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，我们想要提供的输出是一个类别（`Positive`或`Negative`）；然而，模型的输出是一个数字（表示正类结果的概率）。为了将这个结果转化为类别，我们需要应用一个阈值。
- en: 'For example, if we define our threshold as `0.5`, every probability larger
    than 0.5 will get assigned the `Positive` class. By decreasing our threshold,
    more values will be considered positive, increasing the number of TPs (**True
    Positive Rate**, or **TPR**) and the number of FPs (**False Positive Rate**, or
    **FPR**). If the threshold is increased, the effect is the contrary: fewer values
    will be considered positive, hence a smaller TPR and FPR.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将阈值定义为`0.5`，那么所有大于0.5的概率都会被归类为`Positive`。通过降低阈值，更多的值会被认为是正类，从而增加真正例（**True
    Positive Rate**，或**TPR**）和假正例（**False Positive Rate**，或**FPR**）的数量。如果增加阈值，效果则相反：较少的值会被视为正类，因此TPR和FPR都较小。
- en: 'As we modify the value of the threshold, we will have different TPR and FPR
    values. If we plot those values in a graph, we obtain the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调整阈值的值时，TPR和FPR的值会有所不同。如果我们将这些值绘制在图表中，会得到如下结果：
- en: '![Figure 4.8 – AUC](img/B16591_04_8.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – AUC](img/B16591_04_8.jpg)'
- en: Figure 4.8 – AUC
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – AUC
- en: If we calculate the area covered between the curve, the *x* axis, the *y = 0*
    axis, and the *y = 1* axis, we obtain a parameter that is not dependent on the
    threshold value; it defines the performance of our model for the given data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算曲线与*X*轴、*Y = 0*轴和*Y = 1*轴之间所覆盖的区域，我们可以得到一个不依赖于阈值的参数，它定义了模型在给定数据上的表现。
- en: 'TPR and FPR are only defined for binary classification cases. For the multi-label
    classification case, we can emulate binary classification cases. There are two
    possible approaches:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: TPR和FPR仅适用于二元分类情况。对于多标签分类情况，我们可以模拟二元分类的情况。有两种可能的方法：
- en: One versus one
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一
- en: One versus rest
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多
- en: If you are interested, you can find more information in the *There’s more...*
    section of this recipe. These curves are also known as **receiver operating**
    **characteristic** curves.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，可以在本教程的*还有更多...*部分找到更多信息。这些曲线也被称为**接收者操作特征**曲线。
- en: How it works...
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'After understanding the differences between a regression model and a classification
    model, including the activation functions, in this recipe, we focused on the loss
    functions (useful for training) and the metrics (useful for evaluation). We explored
    both cases: binary classification and multi-label classification.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了回归模型和分类模型之间的差异后，包括激活函数的不同，本教程着重讲解了损失函数（用于训练）和评估指标（用于评估）。我们探讨了二元分类和多标签分类两种情况。
- en: We computed the most common loss function for classification, the binary/categorical
    cross-entropy loss function, and we defined several evaluation metrics such as
    accuracy, precision, recall, and F1 score. Furthermore, we learned about the confusion
    matrix as an easy way to look at the per-class performance of our models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了分类的最常见损失函数——二元/分类交叉熵损失函数，并定义了多个评估指标，如准确率、精确率、召回率和F1得分。此外，我们了解了混淆矩阵，它是一种方便查看模型每类表现的方式。
- en: We ended the recipe by taking a look at the AUC, which provides a visualization
    agnostic of thresholds.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过查看AUC结束了这个食谱，它提供了一个与阈值无关的可视化。
- en: There’s more...
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'The mathematical formulas for cross-entropy loss were not derived. In these
    links, you can find more information:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失的数学公式没有推导出来。在这些链接中，你可以找到更多信息：
- en: 'Binary Cross-Entropy Loss: [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Sigmoid](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元交叉熵损失：[https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Sigmoid](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross)
- en: 'Categorical Cross-Entropy Loss: [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类交叉熵损失：[https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax)
- en: 'For a better understanding of how to compute multi-label classification metrics,
    I recommend the following link: [https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解如何计算多标签分类指标，我推荐以下链接：[https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2)。
- en: 'To conclude, reading this AUC explanation can provide further insight: [https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，阅读这个AUC的解释可以提供更多的见解：[https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)。
- en: 'For the multi-label case, these examples can help understand the one versus
    one/one versus rest approaches: [https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多标签情况，这些示例有助于理解一对一/一对多方法：[https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)。
- en: Training for classification models
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型的训练
- en: In this recipe, we will visit the basic concepts of training a model to solve
    a classification problem. We will apply them to optimize the classification model
    we previously defined in this chapter, combined with the usage of the loss functions
    and evaluation metrics we discussed.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将探讨训练一个模型解决分类问题的基本概念。我们将把这些概念应用到优化我们在本章之前定义的分类模型，结合我们讨论过的损失函数和评估指标的使用。
- en: 'We will predict the iris class of flowers using the dataset seen in the second
    recipe, *Toy dataset for classification – load, manage, and visualize Iris dataset*,
    from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029), *Working with MXNet and
    Visualizing Datasets: Gluon* *and DataLoader.*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第二个食谱中看到的数据集 *分类用玩具数据集 - 加载、管理和可视化鸢尾花数据集* 来预测鸢尾花的类别，该食谱位于[ *第2章* ](B16591_02.xhtml#_idTextAnchor029)中，*与MXNet协作和数据集可视化：Gluon*
    和 DataLoader。
- en: Getting ready
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will follow a similar pattern as we did in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052),
    *Solving Regression Problems*, in the third recipe, *Training for regression models*,
    so it will be interesting to revisit the concepts of the loss function, optimizer,
    dataset split, epochs, and batch size.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将遵循与[ *第3章* ](B16591_03.xhtml#_idTextAnchor052)中 *解决回归问题* 类似的模式，即第三个食谱
    *回归模型训练*，因此，重新审视损失函数、优化器、数据集划分、训练轮次和批量大小的概念将非常有趣。
- en: How to do it...
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will create our own training loop and we will evaluate how
    each hyperparameter influences the training. To achieve this, we will follow these
    steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将创建自己的训练循环，并评估每个超参数如何影响训练。为此，我们将按照以下步骤进行：
- en: Improve the model.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进模型。
- en: Define the loss function and optimizer.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和优化器。
- en: Split our dataset and analyze fairness and diversity.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割我们的数据集并分析公平性和多样性。
- en: Put everything together for a training loop.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起，形成训练循环。
- en: Improving the model
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改进模型
- en: 'To solve this problem, given the limited amount of data the dataset contains
    (150 samples), we will define a **Multi-Layer Perceptron** (**MLP**) network architecture,
    as we saw in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression
    Problems*, in the third recipe, *Training for regression models*. This will have
    2 hidden layers fully connected (dense) and **ReLU** activation function with
    10 neurons in each, and an output layer with the corresponding 3 outputs (1 per
    class). The last layer is left without an activation function, although softmax
    was expected. In the next section, we will understand why. For this network, the
    necessary code is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，考虑到数据集包含的有限数据量（150 个样本），我们将定义一个**多层感知机**（**MLP**）网络架构，正如我们在 [*第 3 章*](B16591_03.xhtml#_idTextAnchor052)
    *解决回归问题* 中第三个食谱 *回归模型的训练* 中所看到的那样。该网络将有 2 个隐藏层，每个隐藏层包含 10 个神经元，使用全连接（密集）结构和 **ReLU**
    激活函数，并且输出层有相应的 3 个输出（每个类一个）。最后一层没有激活函数，尽管本应使用 softmax。在下一节中，我们将理解为何如此。对于这个网络，所需的代码如下：
- en: '[PRE3]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have also applied scaling to our input features. The number of parameters
    for the model is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对输入特征进行了缩放。模型的参数数量如下：
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The number of trainable parameters is ~200\. For our (small) dataset, we have
    4 features for each of the 150 rows; therefore, our dataset is ~3 times the number
    of parameters of the model. Typically, this is the minimum for a successful model
    and, ideally, we would like to work with a dataset size of ~10 times the number
    of the parameters of the model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 可训练参数的数量约为 200。对于我们的（小型）数据集，每一行有 4 个特征，共 150 行；因此，我们的数据集大约是模型参数数量的 3 倍。通常，这是成功模型的最小要求，理想情况下，我们希望数据集的大小是模型参数数量的
    10 倍左右。
- en: Important Note
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Even though the comparison between the data points available and the number
    of parameters of the model is a very useful one, different architectures have
    different requirements in terms of data. As usual, experimentation (trial and
    error) is the key to finding the right balance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可用数据点与模型参数数量之间的比较非常有用，但不同的架构在数据方面有不同的要求。像往常一样，实验（试错法）是找到正确平衡的关键。
- en: Defining the loss function and optimizer
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义损失函数和优化器
- en: As discussed in the previous recipe, we will compute the **Categorial Cross-Entropy**
    (**CCE**) loss function. However, there is an optimization detail when computing
    the CCE loss with the softmax activation function; therefore, the computation
    for the softmax function during training is included in the loss function. For
    **inference**, we need to add it externally. Similar to what we did for regression
    problems, we will focus our analysis on the **Stochastic Gradient Descent** (**SGD**)
    and Adam optimizers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一个食谱中所讨论的，我们将计算 **类别交叉熵**（**CCE**）损失函数。然而，在使用 softmax 激活函数计算 CCE 损失时，有一个优化细节；因此，在训练过程中，softmax
    函数的计算包含在损失函数中。对于**推理**，我们需要在外部添加它。类似于我们在回归问题中所做的，我们将重点分析 **随机梯度下降**（**SGD**）和
    Adam 优化器。
- en: Splitting our dataset
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集划分
- en: One of the strongest disadvantages of the Iris dataset is its size; with 150
    samples, it is a small dataset. For this reason, we will apply a 50/40/10 split
    for the training, validation, and test sets.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集的最大缺点之一是其大小；只有 150 个样本，它是一个较小的数据集。因此，我们将采用 50/40/10 的划分比例来分配训练集、验证集和测试集。
- en: 'If we analyze the splits to verify fairness and diversity, we obtain the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析数据划分以验证公平性和多样性，得到如下结果：
- en: '![Figure 4.9 – Distribution for a training set (left), a validation set (middle
    ), and a test set (right)](img/B16591_04_9.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 训练集（左）、验证集（中）、测试集（右）分布](img/B16591_04_9.jpg)'
- en: Figure 4.9 – Distribution for a training set (left), a validation set (middle
    ), and a test set (right)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 训练集（左）、验证集（中）、测试集（右）分布
- en: We can see that each of the classes is well represented across all features,
    the small size being the only reason for the discrepancies.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个类别在所有特征中都有良好的代表性，唯一的原因是样本量小导致的差异。
- en: Putting it all together for a training loop
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合成一个训练循环
- en: 'The training loop for the classification case is very similar to the regression
    case, and we will follow a similar analysis as done earlier in this chapter: we
    will compare each of the hyperparameters, keeping the others constant (unless
    otherwise noted).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题的训练循环与回归问题非常相似，我们将进行与本章早些时候相似的分析：我们将比较每个超参数，保持其他参数不变（除非另有说明）。
- en: Optimizer and learning rate
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化器和学习率
- en: As discussed earlier, the chosen optimizer for the training loop and the learning
    rate are related because, for some optimizers (such as SGD), the learning rate
    is kept constant, whereas for others (such as Adam), it varies from a starting
    point.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练循环中选择的优化器与学习率是相关的，因为对于某些优化器（如 SGD），学习率保持不变，而对于其他优化器（如 Adam），它从一个起始点开始变化。
- en: Tip
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The best optimizer depends on several factors, and nothing trumps trial and
    error; I strongly suggest trying a few to see which one fits best. In my experience,
    typically, SGD and Adam are the ones that work best, including in this problem.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的优化器依赖于多个因素，没有什么比尝试和错误更重要；我强烈建议尝试几个，看看哪个最合适。根据我的经验，通常来说，SGD 和 Adam 是最有效的，包括在这个问题中。
- en: 'Let’s analyze how the training loss and validation loss vary for the SGD optimizer
    when we change the **learning rate** (**LR**), keeping the other parameters constant:
    epochs = 100, batch size = 64, and loss function = softmax cross-entropy:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析当我们改变 SGD 优化器的**学习率**（**LR**）时，如何观察训练损失和验证损失的变化，同时保持其他参数不变：epoch 数量 = 100，批量大小
    = 64，损失函数 = softmax 交叉熵：
- en: '![Figure 4.10 – Loss for SGD optimizer with different LRs](img/B16591_04_10.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 使用不同 LR 值的 SGD 优化器损失](img/B16591_04_10.jpg)'
- en: Figure 4.10 – Loss for SGD optimizer with different LRs
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 使用不同 LR 值的 SGD 优化器损失
- en: From *Figure 4**.10*, we can conclude that for the SGD optimizer, an LR value
    between 1.0 and 3.0 is optimal. Furthermore, we can see that for very large values
    of LR (> 2.0), the algorithm still converges, whereas for the regression case,
    SGD and very large LRs made the model diverge.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.10*，我们可以得出结论，对于 SGD 优化器，LR 值在 1.0 到 3.0 之间最为理想。此外，我们可以看到，对于 LR 值非常大的情况（>
    2.0），算法仍然收敛，而对于回归问题，SGD 和非常大的 LR 值则导致模型发散。
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    when we change the LR, keeping the other parameters constant: epochs = 100, batch
    size = 64, and loss function = softmax cross-entropy:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析当我们改变 Adam 优化器的学习率（LR）时，如何观察训练损失和验证损失的变化，同时保持其他参数不变：epoch 数量 = 100，批量大小
    = 64，损失函数 = softmax 交叉熵：
- en: '![Figure 4.11 – Loss for the Adam optimizer with different LRs](img/B16591_04_11.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 使用不同 LR 值的 Adam 优化器损失](img/B16591_04_11.jpg)'
- en: Figure 4.11 – Loss for the Adam optimizer with different LRs
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 使用不同 LR 值的 Adam 优化器损失
- en: From *Figure 4**.11*, we can conclude that for the Adam optimizer, an LR value
    between 10-2 and 10-1 is optimal.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.11*，我们可以得出结论，对于 Adam 优化器，LR 值在 10-2 和 10-1 之间最为理想。
- en: Although, in this case, SGD with an LR of 3.0 is yielding the best results (smallest
    loss), the evolution of the optimization process is much noisier than for Adam,
    possibly due to the limited amount of data available (the batch size did not influence
    this). A smooth optimization process is also an indication of how well a model
    can generalize; hence, we will choose Adam as our optimizer for the rest of our
    tests.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这种情况下，SGD 优化器以 LR = 3.0 得到最佳结果（最小损失），但优化过程的演变要比 Adam 更加嘈杂，这可能是由于数据量有限（批量大小对这一点没有影响）。平滑的优化过程也表明了模型的泛化能力；因此，在接下来的测试中，我们将选择
    Adam 作为优化器。
- en: Batch size
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量大小
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by changing the batch size, keeping the other parameters constant: epochs = 100,
    LR = 10-2, and loss function = softmax cross-entropy:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析当我们改变 Adam 优化器的批量大小时，如何观察训练损失和验证损失的变化，同时保持其他参数不变：epoch 数量 = 100，LR = 10-2，损失函数
    = softmax 交叉熵：
- en: '![Figure 4.12 – Loss for the Adam optimizer by varying the batch size](img/B16591_04_12.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 通过改变批量大小来观察 Adam 优化器的损失](img/B16591_04_12.jpg)'
- en: Figure 4.12 – Loss for the Adam optimizer by varying the batch size
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 通过改变批量大小来观察 Adam 优化器的损失
- en: From *Figure 4**.12*, we can conclude that for the Adam optimizer, a batch size
    value between 32 and 64 provides the best results.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.12*，我们可以得出结论，对于 Adam 优化器，批量大小在 32 到 64 之间提供了最佳结果。
- en: Epochs
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Epoch 数量
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by varying the epochs, keeping the other parameters constant: LR = 10-2, batch
    size = 32, and loss function = softmax cross-entropy:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析当我们改变 Adam 优化器的训练损失和验证损失时，如何通过改变 epoch 的数量来观察其变化，同时保持其他参数不变：学习率（LR）= 10-2，批量大小（batch
    size）= 32，损失函数（loss function）= softmax 交叉熵（cross-entropy）：
- en: '![Figure 4.13 – Loss for the Adam optimizer by varying the number of epochs](img/B16591_04_13.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – 通过改变 epoch 数量来观察 Adam 优化器的损失](img/B16591_04_13.jpg)'
- en: Figure 4.13 – Loss for the Adam optimizer by varying the number of epochs
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 通过改变 epoch 数量来观察 Adam 优化器的损失
- en: From *Figure 4**.13*, we can conclude that a range of 200–300 epochs is good
    for our problem. With these values, it is very likely the best result will be
    achieved earlier than that.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.13*中，我们可以得出结论，200到300个周期适合我们的任务。使用这些值，很可能会在更短的时间内获得最佳结果。
- en: How it works...
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: On our path to solving our classification problem, in this recipe, we learned
    how to update our model hyperparameters optimally. We revisited the role that
    each hyperparameter plays in the training loop and we performed some ablation
    studies for each individual hyperparameter. This helped us understand how our
    training and validation losses behaved when we modified each hyperparameter individually.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决分类问题的过程中，在本食谱中，我们学习了如何最优地更新模型的超参数。我们重新审视了每个超参数在训练循环中的作用，并对每个超参数进行了消融实验。这帮助我们了解了在单独修改每个超参数时，训练和验证损失是如何变化的。
- en: 'For our current problem and the chosen model, we verified that the best set
    of hyperparameters was as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们当前的问题和选择的模型，我们验证了最佳超参数集如下：
- en: 'Optimizer: Adam'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器：Adam
- en: 'LR: 10-2'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：10^-2
- en: 'Batch size: 32'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小：32
- en: 'Number of epochs: 300'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮数：300
- en: At the end of the training loop, these hyperparameters gave us a training loss
    of `0.01` and a validation loss of `0.1`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环结束时，这些超参数给出了`0.01`的训练损失和`0.1`的验证损失。
- en: There’s more...
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we mostly put together concepts we have been learning about
    in the previous recipes and chapters.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们主要结合了在前面几个食谱和章节中学习到的概念。
- en: 'We did pass through how, in our model definition, we did not explicitly use
    the softmax activation function. This is due to how cross-entropy loss and the
    softmax activation function work together during training (its joint derivative).
    A good reference to understand this point is the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也已经讨论了在模型定义中，我们并没有显式使用 softmax 激活函数。这是由于交叉熵损失函数和 softmax 激活函数在训练过程中是如何共同作用的（它们的联合导数）。要理解这一点的一个好参考是：
- en: '[https://peterroelants.github.io/posts/cross-entropy-softmax/](https://peterroelants.github.io/posts/cross-entropy-softmax/)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://peterroelants.github.io/posts/cross-entropy-softmax/](https://peterroelants.github.io/posts/cross-entropy-softmax/)'
- en: Evaluating classification models
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: 'In the previous recipe, we learned how to choose our training hyperparameters
    to optimize our training. We also verified how those choices affected the training
    and validation losses. In this recipe, we are going to explore how those choices
    affect our actual evaluation in the real world. You will have noticed that we
    split the dataset into three different sets: training, validation, and test sets.
    However, during our training, we only used the training set and the validation
    set. In this recipe, we will emulate real-world behavior by using the unseen data
    from our model, the test set.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个食谱中，我们学习了如何选择训练超参数以优化训练。我们还验证了这些选择如何影响训练和验证损失。在这个食谱中，我们将探索这些选择如何影响我们在现实世界中的实际评估。你可能已经注意到，我们将数据集分成了三部分：训练集、验证集和测试集。然而，在训练过程中，我们只使用了训练集和验证集。在这个食谱中，我们将通过使用我们模型中未见过的数据——测试集，来模拟现实世界的行为。
- en: Getting ready
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: When evaluating a model, we can perform qualitative evaluation and quantitative
    evaluation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，我们可以执行定性评估和定量评估。
- en: '**Qualitative evaluation** is the selection of one or more random (or not so
    random, depending on what we are looking for) samples and analyzing the result,
    verifying whether it matches our expectations.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**定性评估**是选择一个或多个随机（或不那么随机，取决于我们在寻找什么）样本，并分析结果，验证其是否符合我们的预期。'
- en: In this recipe, we will compute the evaluation metrics we defined in the second
    recipe, *Defining loss functions and evaluation metrics for classification models*,
    in this chapter.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将计算在第二个食谱中定义的评估指标，*定义分类模型的损失函数和评估指标*，并在本章中进行计算。
- en: Furthermore, we are going to take a look at how training can have a large influence
    on the evaluation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将探讨训练如何对评估产生重大影响。
- en: How to do it...
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'Before jumping into model evaluation, we will discuss how we can measure our
    model training performance. Therefore, the steps of this recipe are as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始模型评估之前，我们将讨论如何衡量模型训练的性能。因此，本食谱的步骤如下：
- en: Measuring training performance – losses and accuracy
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量训练性能——损失和准确率
- en: Qualitative evaluation
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定性评估
- en: Quantitative evaluation
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定量评估
- en: Measuring training performance – losses and accuracy
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量训练性能——损失和准确率
- en: 'As we saw for regression, a good way to prevent overfitting was early stopping.
    When training our classification model in the previous recipe, we stored the training
    loss, validation loss, and validation accuracy. Let’s see how the training loss,
    validation loss, and validation accuracy evolve as the training progresses:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在回归中看到的，防止过拟合的一个好方法是提前停止。当我们在前一个食谱中训练分类模型时，我们存储了训练损失、验证损失和验证准确率。让我们看看随着训练进展，训练损失、验证损失和验证准确率是如何变化的：
- en: "![Figure 4.14 – Losses \uFEFFand accuracy versus epochs (Adam)](img/B16591_04_14.jpg)"
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – 损失与准确率与 epoch 的关系（Adam）](img/B16591_04_14.jpg)'
- en: Figure 4.14 – Losses and accuracy versus epochs (Adam)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 损失与准确率与 epoch 的关系（Adam）
- en: As we can see in *Figure 4**.14*, at around epoch 50, the validation loss starts
    increasing although the training loss continues to decrease. Moreover, the validation
    accuracy also seems to plateau (close to 1.0/100%) around that epoch. We saved
    the model for the best accuracy and those are the values reported during training
    in the third recipe, *Training for classification models*, in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图 4.14*中所见，约在第 50 个 epoch 时，验证损失开始增加，尽管训练损失持续下降。此外，验证准确率似乎也在该 epoch 附近趋于平稳（接近
    1.0/100%）。我们保存了最佳准确率的模型，这些值是在本章第三个食谱《分类模型的训练》中报告的训练过程中使用的值。
- en: Important Note
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'As a reminder, if you want to use early stopping as is, MXNet provides a callback
    for it: [https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html](https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，如果你想像原样使用提前停止，MXNet 提供了一个回调：[https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html](https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html)。
- en: 'An important thing to mention is that, in the previous recipe, we mentioned
    that SGD did not provide very smooth training. If we plot the values, we obtain
    the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提到的一个重要事项是，在前一个食谱中，我们提到过 SGD 并没有提供非常平稳的训练。如果我们绘制这些值，得到的结果如下：
- en: '![Figure 4.15 – Losses versus epochs (SGD)](img/B16591_04_15.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – 损失与 epoch 的关系（SGD）](img/B16591_04_15.jpg)'
- en: Figure 4.15 – Losses versus epochs (SGD)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 损失与 epoch 的关系（SGD）
- en: As we can see, the training is not stable at all.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，训练过程并不稳定。
- en: Qualitative evaluation
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定性评估
- en: 'To verify that our model is behaving similarly to what we expect (yielding
    a high accuracy when predicting the iris species of a flower), a simple qualitative
    approach is to run our model for a random input from the test set (unseen data).
    In our case, this is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的模型是否与预期相似（在预测花卉的鸢尾花种类时能够获得高准确率），一种简单的定性方法是对测试集中的随机输入（未见过的数据）运行我们的模型。在我们的案例中，结果如下：
- en: '[PRE5]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For this example, as this was just one random input, accuracy can be either
    100% or 0% (accurate or not), and we got the right class.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子来说，因为它只是一个随机输入，准确率可以是 100% 或 0%（准确或不准确），而我们得到了正确的类别。
- en: Important Note
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Although I tried to keep the code as reproducible as possible (including setting
    the seeds for all random processes), there might be some sources of randomness.
    This means that your results might be different, but typically, the order of magnitude
    of errors will be similar.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我尽力让代码尽可能可重复（包括为所有随机过程设置种子），但仍可能存在一些随机性来源。这意味着你的结果可能会有所不同，但通常误差的数量级会相似。
- en: Quantitative evaluation – confusion matrix
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量评估 – 混淆矩阵
- en: 'For the stored results, the confusion matrix obtained is as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于存储的结果，得到的混淆矩阵如下：
- en: '![Figure 4.16 – Confusion matrix](img/B16591_04_16.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16 – 混淆矩阵](img/B16591_04_16.jpg)'
- en: Figure 4.16 – Confusion matrix
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 混淆矩阵
- en: These results are excellent, as there are only values different from zero in
    the main diagonal. This means our model yielded perfect results for the test set.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果非常优秀，因为主对角线上的值除了零以外都是不同的。这意味着我们的模型在测试集上取得了完美的结果。
- en: Quantitative evaluation – accuracy, precision, recall, and F1 score
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量评估 – 准确率、精确率、召回率和 F1 分数
- en: 'During our previous recipes, we defined these metrics and worked with them
    to optimize the training. These evaluations were done for the training set and
    the validation set. For the test set, we obtained the following values:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们定义了这些指标并使用它们来优化训练。这些评估是对训练集和验证集进行的。对于测试集，我们获得了以下值：
- en: 'Accuracy: 1.0'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率：1.0
- en: 'Precision: 1.0'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确率：1.0
- en: 'Recall: 1.0'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率：1.0
- en: 'F1 score: 1.0'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1 分数：1.0
- en: How it works...
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we explored how to evaluate our classification model. To properly
    do this, we revisited the right decision of splitting our full dataset into a
    training set, a validation set, and a test set.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们探索了如何评估我们的分类模型。为了正确做到这一点，我们重新审视了将完整数据集分割成训练集、验证集和测试集的决策。
- en: During training, we used the training set to calculate the gradients to update
    our model parameters, and the validation set to confirm the real-world behavior.
    Afterward, to evaluate our model performance, we used the test set, which was
    the only remaining set of unseen data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们使用训练集计算梯度来更新模型参数，并使用验证集来确认模型在真实世界中的表现。之后，为了评估我们的模型性能，我们使用了测试集，它是唯一剩下的未见数据集。
- en: We discovered the value of qualitatively describing our model behavior by calculating
    the output of random samples, and of quantitatively describing our model performance
    by exploring several numbers and graphs including the confusion matrix, accuracy,
    precision, recall, and F1 score.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，通过计算随机样本的输出来定性描述模型行为，以及通过探索包括混淆矩阵、准确率、精确度、召回率和F1分数等多个数字和图表来定量描述模型性能的价值。
- en: There’s more...
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'In this recipe, we computed the most important evaluation metrics for balanced
    classification datasets. However, when the dataset is imbalanced, we need to be
    careful. This is a good tutorial I like about this topic:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们计算了平衡分类数据集最重要的评估指标。然而，当数据集不平衡时，我们需要小心。这是我喜欢的一篇关于这个话题的教程：
- en: '[https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/)。'
- en: 'At this stage, we have completed our path through a complete classification
    problem: we explored our classification dataset, decided on our evaluation metrics,
    and defined and initialized our model. We understood the best hyperparameter combination
    of optimizer, learning rate, batch size, and epochs, and trained it with early
    stopping. We ended by evaluating it qualitatively and quantitatively.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了一个完整的分类问题的流程：我们探索了分类数据集，决定了评估指标，定义并初始化了模型。我们理解了优化器、学习率、批量大小和训练周期的最佳超参数组合，并使用早停法进行了训练。最后，我们从定性和定量两个方面对模型进行了评估。
