- en: Perceptron Neural Network Modeling – Basic Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机神经网络建模 – 基本模型
- en: So far, we have seen the basics of neural networks and how the learning portion
    works. In this chapter, we take a look at one of the basic and simple forms of
    neural network architecture, the perceptron.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了神经网络的基础知识以及学习部分的工作原理。在本章中，我们将研究神经网络架构的基本且简单的形式——感知机。
- en: 'A **perceptron** is defined as a basic building block of a neural network.
    In machine learning, a perceptron is an algorithm for supervised learning of binary
    classifiers. They classify an output as binary: `TRUE`/`FALSE` or `1`/`0`.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机**被定义为神经网络的基本构建模块。在机器学习中，感知机是一种用于二分类器的监督学习算法。它们将输出分类为二元：`TRUE`/`FALSE`
    或 `1`/`0`。'
- en: 'This chapter helps understand the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章帮助理解以下主题：
- en: Explanation of the perceptron
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机的解释
- en: Linear separable classifier
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性可分分类器
- en: Simple perceptron implementation function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的感知机实现函数
- en: '**Multi-Layer Perceptrons** (**MLPs**)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知机**（**MLPs**）'
- en: By the end of the chapter, we will understand the basic concepts of perceptrons
    and how they are used in neural network algorithm. We will discover the linear
    separable classifier. We will learn a simple perceptron implementation function
    in R environment. We will know how to train and model an MLP.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将理解感知机的基本概念及其在神经网络算法中的应用。我们将发现线性可分分类器。我们将学习在R环境中的简单感知机实现函数。我们将了解如何训练和建模一个多层感知机（MLP）。
- en: Perceptrons and their applications
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机及其应用
- en: A perceptron can be understood as anything that takes multiple inputs and produces
    one output. It is the simplest form of a neural network. The perceptron was proposed
    by Frank Rosenblatt in 1958 as an entity with an input and output layer and a
    learning rule based on minimizing the error. This learning function called **error
    backpropagation** alters connective weights (synapses) based on the actual output
    of the network with respect to a given input, as the difference between the actual
    output and the desired output.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机可以理解为任何接收多个输入并产生一个输出的实体。它是最简单的神经网络形式。感知机由Frank Rosenblatt于1958年提出，作为具有输入层和输出层的实体，并基于最小化误差的学习规则。这个学习函数叫做**误差反向传播**，它根据网络的实际输出与给定输入之间的差异，改变连接权重（突触）。
- en: The enthusiasm was enormous and the cybernetics industry was born. But later,
    scientists Marvin Minsky and Seymour Papert (1969) demonstrated the limits of
    the perceptron. Indeed, a perceptron is able to recognize, after a suitable training,
    only linearly separable functions. For example, the XOR logic function cannot
    be implemented by a perceptron.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当时的热情极高，控制论行业也随之诞生。但后来，科学家Marvin Minsky和Seymour Papert（1969年）展示了感知机的局限性。事实上，感知机经过适当的训练后只能识别线性可分函数。例如，XOR逻辑函数无法通过感知机实现。
- en: 'The following image showns Frank Rosenblatt at the Cornell Aeronautical Laboratory
    (1957-1959), while working on the Mark I Perceptron classifier:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了Frank Rosenblatt在康奈尔航空实验室（1957-1959）时，研究Mark I感知机分类器的情景：
- en: '![](img/00080.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpeg)'
- en: Potentially, a multilevel network of percetters could solve more complex problems,
    but the increasing computational complexity of training made this path impracticable.
    Only in recent times have we started to consider the utility of this operational
    entity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在地，一个多层感知机网络可以解决更复杂的问题，但训练的计算复杂性增加使得这条路径变得不可行。直到最近，我们才开始重新考虑这种操作实体的实用性。
- en: In the single form, a perceptron has one neuron unit accepting inputs and producing
    a set of outputs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在单一形式中，感知机有一个神经元单元，接受输入并生成一组输出。
- en: 'For example, let us take a look at the following image:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看以下图像：
- en: '![](img/00081.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.jpeg)'
- en: Here **x[1], x[2],.., x[n]** are the set of inputs and **x[0]** is the bias.
    **x[0]** is set to **1**. The output **y** is the sum product of **w[i]x[i]**.
    The **signum** function is applied after the sum product has been executed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里**x[1], x[2],.., x[n]**是输入集合，**x[0]**是偏置。**x[0]**被设置为**1**。输出**y**是**w[i]x[i]**的加权和。**符号函数**在加权和计算后应用。
- en: 'It separates the output as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出分隔为：
- en: If **y>0**, the output is **1**
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**y>0**，输出为**1**
- en: If **y<=0**, the output is **-1**
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**y<=0**，输出为**-1**
- en: The bias is constant and is associated with weight **w[0]**. This perceptron
    functions as a linear separator, splitting the output into one category, **-1**
    or **+1**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置是常数，与权重**w[0]**相关。这个感知机充当线性分隔器，将输出分为**-1**或**+1**两类。
- en: Note that here this is no backpropagation and the weight update updates through
    steps we will soon see. There is a threshold setup which determines the value
    of the output. The output here is binary (either **-1** or **+1**), which can
    be set as zero or one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里没有反向传播，权重更新是通过我们很快会看到的步骤进行的。这里有一个阈值设置，它决定了输出的值。输出是二值的（**-1** 或 **+1**），可以设置为零或一。
- en: Hence, a perceptron is a simple classification function that directly makes
    its prediction. The core of the functionality lives in the weights and how we
    update the weights to the best possible prediction of **y**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，感知机是一个简单的分类函数，直接做出预测。其功能的核心在于权重，以及我们如何更新权重，以便对 **y** 做出最佳预测。
- en: 'This case is a **simple perceptron** or basic perceptron, and the outputs are
    binary in nature: *0/1* *true/false* *+1/-1*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子是 **简单感知机** 或基础感知机，输出是二进制的：*0/1* *真/假* *+1/-1*。
- en: There is another type of perceptron called the **multi-class perceptron**, which
    can classify many possible labels for an animal, such as dog, cat, or bird.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类型的感知机叫做 **多类感知机**，它可以对动物进行多种可能标签的分类，比如狗、猫或鸟。
- en: 'In the following figure is shown a simple perceptron architecture versus multi-class
    perceptron architecture:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中展示了一个简单感知机架构与多类感知机架构的对比：
- en: '![](img/00082.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.jpeg)'
- en: By modifying the weighting vector, we can modify the output of a perceptron
    to improve the learning or storage properties. For example, we can try to instruct
    a perceptron such that given an input *x*, output *y* is as close as possible
    to a given a priori chosen *y* actual value. The computational capabilities of
    a single perceptron are, however, limited, and the performance that can be obtained
    depends heavily on both the input choice and the choice of function that you want
    to implement.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改权重向量，我们可以修改感知机的输出，以提高学习或存储特性。例如，我们可以尝试指导感知机，使得给定输入 *x* 时，输出 *y* 尽可能接近预先选定的
    *y* 实际值。然而，单个感知机的计算能力是有限的，其性能很大程度上依赖于输入选择和你想要实现的函数选择。
- en: In fact, inputs can be limited to a subset of all the possible inputs, or be
    randomly extracted according to a certain predetermined probability distribution.
    To a lesser extent, the performance of such a system also depends on how the distance
    between the actual outputs and the expected outputs is quantified.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，输入可以限制为所有可能输入的一个子集，或者根据某种预定的概率分布随机提取。较小程度上，系统的性能也取决于实际输出与预期输出之间的距离如何量化。
- en: Once you have identified the problem of learning, you can try to find the optimal
    weight assignment for the given problem.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你识别出了学习问题，你就可以尝试为给定的问题找到最佳的权重分配。
- en: Simple perceptron – a linear separable classifier
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单感知机 – 线性可分分类器
- en: 'As we saw, a simple perceptron is a single layer neural unit which is a linear
    classifier. It is a neuron capable of producing only two output patterns, which
    can be synthesized in *active* or *inactive*. Its decision rule is implemented
    by a *threshold* behavior: if the sum of the activation patterns of the individual
    neurons that make up the input layer, weighted for their weights, exceeds a certain
    threshold, then the output neuron will adopt the output pattern *active*. Conversely,
    the output neuron will remain in the *inactive* state.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，简单感知机是一个单层神经单元，它是一个线性分类器。它是一个只能生成两种输出模式的神经元，这些模式可以合成为 *激活* 或 *非激活*。其决策规则通过
    *阈值* 行为实现：如果组成输入层的各个神经元的激活模式的总和，按它们的权重加权后，超过某个阈值，那么输出神经元将采用输出模式 *激活*。反之，输出神经元将保持在
    *非激活* 状态。
- en: 'As mentioned, the output is the sum of *weights*inputs* and a function applied
    on top of it; output is *+1 (y>0)* or *-1(y<=0)*, as shown in the following figure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，输出是 *权重*输入*的总和，并在其上应用一个函数；输出是 *+1 (y>0)* 或 *-1(y<=0)*，如下面的图所示：
- en: '![](img/00083.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.jpeg)'
- en: We can see the linear interaction here; the output *y* is linearly dependent
    on the inputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这里的线性交互；输出 *y* 与输入是线性相关的。
- en: As with most neural network models, it is possible to realize a learning function
    based on the modification of synaptic connective weights, even in perceptors.
    At the beginning of the training phase, weights *w* of perceptron synaptic connections
    assume completely random values. For training, we have a number of examples with
    its relative, correct, classification. The network is presented in turn, the different
    cases to be classified and the network processes each time its response (greater
    than the threshold or less than the threshold). If the classification is correct
    (network output is the same as expected), the training algorithm does not make
    any changes. On the contrary, if the classification is incorrect, the algorithm
    changes the synaptic weights in an attempt to improve the classification performance
    of the network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数神经网络模型一样，即使在感知器中，也可以通过修改突触连接权重来实现学习功能。在训练阶段开始时，感知器突触连接的权重*w*是完全随机的。对于训练，我们有若干个示例及其相关的正确分类。网络依次呈现待分类的不同案例，每次网络处理其响应（大于阈值或小于阈值）。如果分类正确（网络输出与预期一致），则训练算法不会做出任何更改。相反，如果分类不正确，算法会改变突触权重，以期提高网络的分类性能。
- en: 'The single perceptron is an online learner. The weight updates happen through
    the following steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 单一感知机是一个在线学习者。权重更新通过以下步骤发生：
- en: Get *x* and output label *y*.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取*x*并输出标签*y*。
- en: Update *w* for *f(x)*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新*f(x)*的*w*。
- en: If *f(x)=y*, mark as completed; else, fix it
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*f(x)=y*，则标记为完成；否则，修正它。
- en: 'Now adjust score based on error:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在根据错误调整评分：
- en: '*f(x)= sign(sum of weights*inputs)*, the errors are possible'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x) = sign(权重*输入之和)*，此时可能出现错误。'
- en: if *y=+1* and *f(x)=-1, w*x* is too small, make it bigger
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*y=+1*且*f(x)=-1*，则*w*x*太小，增大它。
- en: if *y=-1* and *f(x)=+1, w*x* is too large make it smaller
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*y=-1*且*f(x)=+1*，则*w*x*太大，减小它。
- en: 'Apply the following rules:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用以下规则：
- en: make *w=w-x* if *f(f)=+1* and *y=-1*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f(f)=+1*且*y=-1*，则*w=w-x*。
- en: make *w=w+x* if *f(f)=-1* and *y=+1*
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f(f)=-1*且*y=+1*，则*w=w+x*。
- en: '*w=w* if *f(x)=y*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f(x)=y*，则*w=w*。
- en: Or simply, *w=w+yx* if *f(x)!=y*
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或简单地说，*w=w+yx*，如果*f(x)!=y*。
- en: Repeat steps 3 to 5, until *f(x) = y*.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3到5，直到*f(x) = y*。
- en: 'The perceptron is guaranteed to satisfy all our data, but only for a binary
    classifier with a single neuron. In step 5, we brought a term called **learning
    rate**. This helps our model converge. In step 5, *w* is written as: *w=w+αyx
    if f(x) != y*, where *α* is the learning rate chosen.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机保证能够满足我们所有的数据，但仅适用于具有单一神经元的二分类器。在步骤5中，我们引入了一个叫做**学习率**的术语。这有助于我们的模型收敛。在步骤5中，*w*被写为：*w=w+αyx*，如果*f(x)
    != y*，其中*α*是选定的学习率。
- en: The bias is also updated as *b=b+ αy* if *f(x) != y*. The *b* is actually our
    *w[0]*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f(x) != y*，则偏置也会更新为*b=b+ αy*。*b* 实际上就是我们的 *w[0]*。
- en: If the Boolean function is a linear threshold function (that is, if it is linearly
    separable), then the local perceptron rule can find a set of weights capable of
    achieving it in a finite number of steps.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果布尔函数是一个线性阈值函数（即它是线性可分的），那么局部感知机规则可以在有限步骤内找到一组能够实现它的权重。
- en: This theorem, known as the **perceptron theorem**, is also applicable in the
    case of the global rule, which modifies the vector of synaptic weights *w*, not
    at a single input vector, but depending on the behavior of the perceptron on the
    whole set of input vectors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理，称为**感知机定理**，在全局规则的情况下也适用，该规则修改突触权重的向量*w*，而不是在单个输入向量上，而是根据感知机在整个输入向量集上的行为进行修改。
- en: We just mentioned the linearly separable function, but what is meant by this
    term? We will understand it in the following section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提到了线性可分函数，那么这个术语是什么意思呢？我们将在接下来的章节中理解它。
- en: Linear separation
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性分离
- en: 'When a set of output values can be split by a straight line, the output values
    are said to be linearly separable. Geometrically, this condition describes the
    situation in which there is a hyperplane that separates, in the vector space of
    inputs, those that require positive output from those that require a negative
    output, as shown in the following figure:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当一组输出值可以通过一条直线分开时，称这些输出值是线性可分的。从几何学上看，这个条件描述了在输入的向量空间中存在一个超平面，它将需要正输出的与需要负输出的分开，如下图所示：
- en: '![](img/00084.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.jpeg)'
- en: Here, one side of the separator are those predicted to belong to one class whilst
    those on the other side are predicted to belong to a different class. The decision
    rule of the Boolean neuron corresponds to the breakdown of the input features
    space, operated by a hyperplane.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，分隔符的一侧是预测属于一个类别的点，而另一侧是预测属于不同类别的点。布尔神经元的决策规则对应于由超平面操作的输入特征空间的划分。
- en: If, in addition to the output neuron, even the input of the neural network is
    Boolean, then using the neural network to perform a classification is equivalent
    to determining a Boolean function of the input vector. This function takes the
    value 1 where it exceeds the threshold value, 0 otherwise. For example, with two
    input and output Boolean neurons, it is possible to represent, in an extremely
    intuitive way, the *AND* and *OR* functions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果除了输出神经元外，神经网络的输入也是布尔值，那么使用神经网络进行分类相当于确定输入向量的布尔函数。该函数在超过阈值时取值为1，否则取值为0。例如，对于两个输入和输出布尔神经元，可以以一种非常直观的方式表示*AND*和*OR*函数。
- en: Indeed, the *AND* gate and *OR* gate are linearly separable. Let's test it in
    practice by first listing the possible cases in a table and then representing
    them on a two-dimensional plane.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，*AND*门和*OR*门是线性可分的。让我们通过实际测试来验证，首先列出可能的情况并将其表示在二维平面上。
- en: 'Let''s first do this for the *AND* function. In the following table are listed
    all the possible cases with the logical results:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来做*AND*函数的例子。下表列出了所有可能的情况及其逻辑结果：
- en: '| **x1** | **x2** | **y** (**AND gate**) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **x1** | **x2** | **y** (**AND门**) |'
- en: '| *1* | *1* | *1* |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *1* | *1* |'
- en: '| *1* | *0* | *0* |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *0* | *0* |'
- en: '| *0* | *1* | *0* |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| *0* | *1* | *0* |'
- en: '| *0* | *0* | *0* |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| *0* | *0* | *0* |'
- en: 'The following figure shows all the four cases in a two-dimensional plane:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在二维平面中的所有四种情况：
- en: '![](img/00085.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpeg)'
- en: All points above the hyperplane are assumed to be *1/TRUE*, while the ones below
    are assumed to be *0/FALSE*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所有超平面上的点假设为*1/TRUE*，而其下方的点假设为*0/FALSE*。
- en: 'Let''s do it now for the *OR* function. In the following table are listed all
    the possible cases with the logical results:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来做*OR*函数的例子。下表列出了所有可能的情况及其逻辑结果：
- en: '| **x1** | **x2** | **y** (**OR gate**) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **x1** | **x2** | **y** (**或门**) |'
- en: '| *1* | *1* | *1* |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *1* | *1* |'
- en: '| *1* | *0* | *1* |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *0* | *1* |'
- en: '| *0* | *1* | *1* |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| *0* | *1* | *1* |'
- en: '| *0* | *0* | *0* |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| *0* | *0* | *0* |'
- en: 'The following figure shows all the four cases in a two-dimensional plane:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在二维平面中的所有四种情况：
- en: '![](img/00086.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.jpeg)'
- en: In this case also, all the points above the hyperplane are assumed to be *1/TRUE*,
    while the ones below are assumed to be *0/FALSE*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所有超平面上的点假设为*1/TRUE*，而其下方的点假设为*0/FALSE*。
- en: 'However, some Boolean functions cannot be replicated through a network structure,
    such as that seen up to here. The *XOR* and identity functions, for example, are
    not separable: to isolate them, two lines would be needed, which can be implemented
    only through the use of a more complex network structure.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些布尔函数无法通过网络结构来复制，比如这里看到的函数。*XOR*和恒等函数就是不可分的：要将它们隔离，需要两条线，而这只能通过更复杂的网络结构来实现。
- en: 'In the following table are listed all the possible cases with the logical results,
    for the *XOR* function:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了*XOR*函数的所有可能情况及其逻辑结果：
- en: '| **x1** | **x2** | **y (XOR gate)** |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **x1** | **x2** | **y (XOR门)** |'
- en: '| *1* | *1* | *0* |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *1* | *0* |'
- en: '| *1* | *0* | *1* |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *0* | *1* |'
- en: '| *0* | *1* | *1* |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| *0* | *1* | *1* |'
- en: '| *0* | *0* | *0* |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| *0* | *0* | *0* |'
- en: 'In the following figure are shown all the four cases in a two-dimensional plane:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在二维平面中的所有四种情况：
- en: '![](img/00087.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpeg)'
- en: As anticipated, such a function requires two lines to group all possible cases.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，这样的函数需要两条线来划分所有可能的情况。
- en: After understanding the basics of perceptron theory, we can study a practical
    case.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了感知机理论的基础之后，我们可以研究一个实际的案例。
- en: The perceptron function in R
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R中的感知机函数
- en: In the previous sections, we understood the fundamental concepts underlying
    the use of a perceptron as a classifier. The time has come to put into practice
    what has been studied so far. We will do it by analyzing an example in which we
    will try to classify the floral species on the basis of the size of the petals
    and sepals of an Iris. As you will recall, the `iris` dataset has already been
    used in [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4), *Deep
    Learning Using Multilayer Neural Networks*. The reason for its re-use is not only
    due to the quality of the data contained in it that allows the reader to easily
    understand the concepts outlined, but also, and more importantly, to be able to
    compare the different algorithms.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们理解了感知器作为分类器的基本概念。现在，实践的时刻到了，我们将通过分析一个示例来应用迄今为止所学的内容，在这个示例中，我们将尝试根据鸢尾花的花瓣和萼片的大小来对花卉物种进行分类。如你所记得，`iris`数据集已经在[第3章](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4)中使用过，*使用多层神经网络的深度学习*。重新使用这个数据集的原因不仅是因为其中的数据质量使读者能够轻松理解所阐述的概念，而且更重要的是，能够比较不同的算法。
- en: 'As you will recall, the dataset contains 50 samples from each of three species
    of Iris (Iris `setosa`, Iris `virginica`, and Iris `versicolor`). Four features
    were measured from each sample: the length and the width of the sepals and petals,
    in centimeters.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所记得，数据集包含了来自三种鸢尾花物种（Iris `setosa`、Iris `virginica` 和 Iris `versicolor`）的50个样本。从每个样本中测量了四个特征：萼片和花瓣的长度和宽度，单位为厘米。
- en: 'The following variables are contained:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 包含以下变量：
- en: Sepal length in cm
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度（单位：厘米）
- en: Sepal width in cm
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度（单位：厘米）
- en: Petal length in cm
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（单位：厘米）
- en: Petal width in cm
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（单位：厘米）
- en: 'Class: `setosa`, `versicolour`, `virginica`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别：`setosa`、`versicolour`、`virginica`
- en: In the example presented, we will try to classify the `setosa` and `versicolor`
    species through linear separation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在所示的示例中，我们将尝试通过线性分隔来分类`setosa`和`versicolor`物种。
- en: 'Let us implement a perceptron function in R for the `iris` dataset. The code
    is presented next:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在R语言中为`iris`数据集实现一个感知器函数。代码如下：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let us go through the code line-by-line. Following the style in the rest
    of this book, we will present a portion of the code first as follows and then
    explain it in detail:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐行分析代码。按照本书其余部分的风格，我们首先呈现一部分代码如下，然后详细解释：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first command loads the `iris` dataset, which is contained in the datasets
    library, and saves it in a given dataframe. Then we use the `head` function to
    display the first `20` lines of the dataset. Remember, the `head` function returns
    the first or last parts of a vector, matrix, table, dataframe, or function. In
    this case, we specify the number of lines that must be displayed (`n=20`). The
    following is the result:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条命令加载了`iris`数据集，该数据集包含在datasets库中，并将其保存在给定的数据框中。然后，我们使用`head`函数显示数据集的前`20`行。请记住，`head`函数返回向量、矩阵、表格、数据框或函数的前部分或后部分。在这里，我们指定了要显示的行数（`n=20`）。以下是结果：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s go back to the code. We will get the binary output by extracting only
    *100* rows of the `iris` dataset, and extracting only `sepal` length and `petal`
    length with `species`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到代码。我们将通过提取`iris`数据集中的*100*行，并仅提取`sepal`长度和`petal`长度以及`species`来获取二元输出：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, only the first `100` rows of the `iris` dataset are taken and columns
    `1`,`3`, and `5` are chosen. This is because the first `100` lines contain the
    data for the two species (`setosa` and `versicolor`) we are interested in, in
    the following example. The three columns are `sepal.length(x1)`, `petal.length(x2)`,
    and `species(y - output)`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们仅取`iris`数据集中的前`100`行，并选择第`1`、`3`和`5`列。这是因为前`100`行包含了我们感兴趣的两个物种（`setosa`和`versicolor`）的数据。三列分别是`sepal.length(x1)`、`petal.length(x2)`和`species(y
    - output)`。
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: First we load the `ggplot2` library, and then we use `ggplot()` to get the scatterplot
    of the distribution of species with respect to `sepal.length` and `petal.length`.
    Of course, the library should have been installed beforehand.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载`ggplot2`库，然后使用`ggplot()`绘制出物种分布与`sepal.length`和`petal.length`的散点图。当然，库应该事先安装。
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，要安装一个不在R初始分发版中的库，你必须使用`install.package`函数。这是安装包的主要函数。它接受一个名称向量和一个目标库，从存储库中下载并安装这些包。
- en: 'The objective of the `perceptron` function is to find a linear separation of
    the `setosa` and `versicolor` species. The following figure shows **Sepal length**
    versus **Petal length** for the two species of Iris flower:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`perceptron`函数的目标是找到`setosa`和`versicolor`物种的线性分离。下图展示了**萼片长度**与**花瓣长度**之间的关系，分别对应两种鸢尾花：'
- en: '![](img/00088.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.jpeg)'
- en: 'As can be seen, the two species are placed in distinct areas of the plane,
    so linear separation is possible. At this point, we need to define functions to
    do the perceptron processing:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，两种物种位于平面的不同区域，因此可以进行线性分离。此时，我们需要定义函数来进行感知器处理：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We define the `perceptron` function as discussed in the algorithm for perceptron
    training. We apply `learning.rate` as `1` and try to update the weights in each
    loop. Once we have the output and the function *(weights*inputs)* equal, we stop
    the training and move out. The updated weights are returned by the function. The
    objective of the function is to get a set of optimal weights needed for the model
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了`perceptron`函数，如感知器训练算法中所讨论的那样。我们将`learning.rate`设置为`1`，并在每个循环中尝试更新权重。一旦输出和函数*(weights*inputs)*相等，我们就停止训练并退出。更新后的权重将由函数返回。该函数的目标是获得模型所需的最优权重集，如下所示：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With the first line, we set the `x` inputs as the `sepal` and `petal` lengths.
    `sepal.length` and `petal.length` form the input matrix. In the second line, we
    set label output as positive for `setosa` and the rest as negative. The output
    is either `setosa` or not (`+1` or `-1`). In the third line, we run the `perceptron`
    function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，我们将`x`输入设置为`萼片`和`花瓣`长度。`sepal.length`和`petal.length`构成输入矩阵。在第二行中，我们将标签输出设置为`setosa`为正，其余为负。输出是`setosa`或非`setosa`（`+1`或`-1`）。在第三行中，我们运行`perceptron`函数。
- en: 'We call the `perceptron` function with `x` and `y`, which gives the optimal
    weights for the perceptron as shown in the following code sample:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`perceptron`函数，传入`x`和`y`，它返回感知器的最优权重，如下所示的代码示例：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The previous lines of code plot `x` and `y`, highlighting `setosa` and `versicolor`
    as `+` and `*` points in the graph. We then find the intercept and slope of the
    `p` variable (perceptron), returned by the perceptron. Plotting the linear separation
    line gives the following graph:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码行绘制了`x`和`y`，并在图中将`setosa`和`versicolor`分别标记为`+`和`*`点。然后我们找到了`p`变量（感知器）的截距和斜率，并绘制了线性分离线，得到如下图表：
- en: '![](img/00089.gif)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.gif)'
- en: To summarize, we have implemented the perceptron using R code and found optimal
    weights. The linear separation has been achieved using the perceptron.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们使用R代码实现了感知器并找到了最优权重。通过感知器实现了线性分离。
- en: Multi-Layer Perceptron
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'We saw that the *AND* and *OR* gate outputs are linearly separable and perceptron
    can be used to model this data. However, not all functions are separable. In fact,
    there are very few and their proportion to the total of achievable functions tends
    to zero as the number of bits increases. Indeed, as we anticipated, if we take
    the *XOR* gate, the linear separation is not possible. The crosses and the zeros
    are in different locations and we cannot put a line to separate them, as shown
    in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，*AND*和*OR*门的输出是线性可分的，感知器可以用来建模这些数据。然而，并不是所有的函数都是可分的。实际上，能够分开的函数非常少，而且随着比特数的增加，它们在所有可实现函数中的比例趋近于零。正如我们预期的那样，如果我们考虑*XOR*门，线性分离是不可能的。交叉点和零点的位置不同，我们无法画一条线将它们分开，如下图所示：
- en: '![](img/00090.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00090.jpeg)'
- en: We could think of parsing more perceptrons. The resulting structure could thus
    learn a greater number of functions, all of which belong to the subset of linearly
    separable functions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑解析更多的感知器。这样，得到的结构可以学习更多的函数，这些函数都属于线性可分函数的子集。
- en: In order to achieve a wider range of functions, intermediate transmissions must
    be introduced into the perceptron between the input layer and the output layer,
    allowing for some kind of internal representation of the input. The resulting
    perceptron is called MLP.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更广泛的功能，必须在输入层和输出层之间引入中间传输，允许对输入进行某种形式的内部表示。由此产生的感知器称为 MLP。
- en: 'We have already seen this as feed forward networks in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*. MLP consists of at least
    three layers of nodes: input, hidden, and output nodes. Except for the input nodes,
    each node is a neuron using a non-linear activation function. MLP uses a supervised
    learning technique and back propagation for training. The multiple layers and
    non-linear nature distinguishes MLP from simple perceptrons. MLP is specifically
    used when the data is not linearly separable.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第一章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)，*神经网络与人工智能概念*中看到过它作为前馈网络的应用。MLP
    至少由三层节点组成：输入层、隐藏层和输出层。除输入节点外，每个节点都是使用非线性激活函数的神经元。MLP 使用监督学习技术和反向传播进行训练。多层结构和非线性特性将
    MLP 与简单的感知器区分开来。MLP 特别用于数据不能线性分割的情况。
- en: 'For example, an MLP, such as that shown in the following figure, is able to
    realize the **XOR** function, which we have previously seen cannot be achieved
    through a simple perceptron:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面图示中的 MLP 能够实现 **XOR** 函数，这一点我们之前已经看到，单一的感知器无法实现：
- en: '![](img/00091.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.jpeg)'
- en: '**XOR** is achieved using a three layer network and is a combination of **OR**
    and **AND** perceptrons. The output layer contains one neuron which gives the
    **XOR** output. A configuration of this kind allows the two neurons to specialize
    each on a particular logic function. For example, in the case of **XOR,** the
    two neurons can respectively carry out the **AND** and **OR** logic functions.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**XOR** 使用三层网络实现，它是 **OR** 和 **AND** 感知器的组合。输出层包含一个神经元，给出 **XOR** 输出。这种配置允许两个神经元分别专注于特定的逻辑功能。例如，在
    **XOR** 的情况下，两个神经元可以分别执行 **AND** 和 **OR** 逻辑功能。'
- en: The term MLP does not refer to a single perceptron that has multiple layers.
    Rather, it contains many perceptrons that are organized into layers. An alternative
    is an MLP network.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 这个术语并不是指具有多层的单一感知器。相反，它包含多个感知器，这些感知器被组织成不同的层。另一种形式是 MLP 网络。
- en: 'Applications of MLP are:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 的应用包括：
- en: MLPs are extremely useful for complex problems in research.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 对于研究中的复杂问题极为有用。
- en: MLPs are universal function approximators and they can be used to create mathematical
    models by regression analysis. MLPs also make good classifier algorithms.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 是通用的函数逼近器，可用于通过回归分析创建数学模型。MLP 也是很好的分类算法。
- en: MLPs are used in diverse fields, such as speech recognition, image recognition,
    and language translation. They form the basis for deep learning.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP 被广泛应用于语音识别、图像识别和语言翻译等领域，它们是深度学习的基础。
- en: We will now implement an MLP using the R package SNNS.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 R 包 SNNS 来实现 MLP。
- en: MLP R implementation using RSNNS
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RSNNS 实现的 MLP R 实现
- en: 'The package `RSNNS` is taken from CRAN for this example of `mlp()` model build.
    The SNNS is a library written in C++ and contains many standard implementations
    of neural networks. This `RSNNS` package wraps the SNNS functionality to make
    it available from within R. Using the `RSNNS` low-level interface, all the algorithmic
    functionality and flexibility of SNNS can be accessed. The package contains a
    high-level interface for most commonly used neural network topologies and learning
    algorithms, which integrate seamlessly into R. A brief description of the `RSNNS`
    package, extracted from the official documentation, is shown in the following
    table:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例中的 `RSNNS` 包来自 CRAN，用于 `mlp()` 模型构建。SNNS 是一个用 C++ 编写的库，包含了许多标准的神经网络实现。这个
    `RSNNS` 包封装了 SNNS 的功能，使其可以在 R 内部使用。通过 `RSNNS` 的低级接口，可以访问 SNNS 的所有算法功能和灵活性。该包还包含一个高级接口，用于最常用的神经网络拓扑和学习算法，能够与
    R 无缝集成。以下表格展示了从官方文档中提取的 `RSNNS` 包的简要描述：
- en: '| **RSNNS package** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| **RSNNS 包** |'
- en: '| **Description**: |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **描述**： |'
- en: '| The SNNS is a library containing many standard implementations of neural
    networks. This package wraps the SNNS functionality to make it available from
    within R. Using the `RSNNS` low-level interface, all the algorithmic functionality
    and flexibility of SNNS can be accessed. Furthermore, the package contains a convenient
    high-level interface, so that the most common neural network topologies and learning
    algorithms integrate seamlessly into R. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| SNNS 是一个包含许多神经网络标准实现的库。此包封装了 SNNS 的功能，使其能够在 R 中使用。通过 `RSNNS` 低级接口，可以访问 SNNS
    的所有算法功能和灵活性。此外，包还包含一个方便的高级接口，最常见的神经网络拓扑结构和学习算法能够无缝集成到 R 中。|'
- en: '| **Details**: |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| **详细信息**: |'
- en: '| Package: `RSNNS` Type: Package'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '| 包: `RSNNS` 类型: 包'
- en: 'Version: 0.4-9'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '版本: 0.4-9'
- en: 'Date: 2016-12-16'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2016-12-16'
- en: 'License: LGPL (>=2) |'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '许可证: LGPL (>=2) |'
- en: '| **Authors**: |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **作者**: |'
- en: '| *Christoph Bergmeir* *José M. Benítez* |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| *Christoph Bergmeir* *José M. Benítez* |'
- en: '| **Usage**: |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **用法**: |'
- en: '| `mlp(x, y,` `size = c(5),`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '| `mlp(x, y,` `size = c(5),`'
- en: '`maxit = 100,`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxit = 100,`'
- en: '`initFunc = "Randomize_Weights",`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`initFunc = "随机初始化权重",`'
- en: '`initFuncParams = c(-0.3, 0.3),`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`initFuncParams = c(-0.3, 0.3),`'
- en: '`learnFunc = "Std_Backpropagation",`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`learnFunc = "标准反向传播",`'
- en: '`learnFuncParams = c(0.2, 0),`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`learnFuncParams = c(0.2, 0),`'
- en: '`updateFunc = "Topological_Order",`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`updateFunc = "拓扑顺序",`'
- en: '`updateFuncParams = c(0),`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`updateFuncParams = c(0),`'
- en: '`hiddenActFunc = "Act_Logistic",`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`hiddenActFunc = "激活_逻辑函数",`'
- en: '`shufflePatterns = TRUE,`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`shufflePatterns = TRUE,`'
- en: '`linOut = FALSE,`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`linOut = FALSE,`'
- en: '`outputActFunc = if (linOut) "Act_Identity" else "Act_Logistic",`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`outputActFunc = if (linOut) "激活_恒等函数" else "激活_逻辑函数",`'
- en: '`inputsTest = NULL,`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputsTest = NULL,`'
- en: '`targetsTest = NULL,`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`targetsTest = NULL,`'
- en: '`pruneFunc = NULL,`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`pruneFunc = NULL,`'
- en: '`pruneFuncParams = NULL, ...)` |'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`pruneFuncParams = NULL, ...)` |'
- en: We use the `mlp()` function which creates an MLP and trains it. Training is
    usually performed by backpropagation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `mlp()` 函数来创建和训练 MLP。训练通常通过反向传播完成。
- en: 'The most commonly used parameters are listed in the following table:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的参数列在下表中：
- en: '| `x` | A matrix with training inputs for the network |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `x` | 用于网络的训练输入矩阵 |'
- en: '| `y` | The corresponding targets values |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `y` | 对应的目标值 |'
- en: '| `size` | Number of units in the hidden layers |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `size` | 隐藏层中单元的数量 |'
- en: '| `maxit` | Maximum iterations to learn |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `maxit` | 学习的最大迭代次数 |'
- en: '| `hiddenActFunc` | The activation function of all hidden units |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `hiddenActFunc` | 所有隐藏单元的激活函数 |'
- en: '| `outputActFunc` | The activation function of all output units |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `outputActFunc` | 所有输出单元的激活函数 |'
- en: '| `inputsTest` | A matrix with inputs to test the network |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `inputsTest` | 用于测试网络的输入矩阵 |'
- en: '| `targetsTest` | The corresponding targets for the test input |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `targetsTest` | 测试输入对应的目标值 |'
- en: 'Let''s see the code for building a SNNS MLP using the full Iris dataset:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下使用完整的 Iris 数据集构建 SNNS MLP 的代码：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's go through the code step-by-step.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步分析代码。
- en: 'This command loads the iris dataset, which is contained in the datasets library,
    and saves it in a given dataframe. Considering the many times we have used it,
    I do not think it''s necessary to add anything. This loads the `RSNNS` library
    for the program:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令加载包含在 datasets 库中的 iris 数据集，并将其保存在给定的数据框中。考虑到我们已经多次使用它，我认为不需要再做什么。此命令加载 `RSNNS`
    库以供程序使用：
- en: '[PRE9]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，要安装 R 初始分发中没有的库，你必须使用 `install.package` 函数。这是安装包的主要函数。它接收一个名称向量和目标库，从仓库中下载包并安装它们。
- en: In our case, we must use the command `install.packages("RSNNS")`. The install
    package is required only the first time, to install the `RSNNS` package from CRAN.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们必须使用命令 `install.packages("RSNNS")`。只需第一次使用安装包，将 `RSNNS` 包从 CRAN 安装。
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this preceding line, the `iris` dataset is shuffled within rows. This operation
    makes the order of the rows in the dataset random. In fact, in the original dataset,
    the observations are ordered by floral species: the first *50* occurrences of
    the `setosa` species, followed by the *50* occurrences of the `versicolor` species,
    and finally the *50* occurrences of the virginica species. After this operation,
    the rows happen randomly. To verify this, we print the first `20` lines of the
    modified dataset:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的这一行中，`iris` 数据集在行内进行了洗牌。此操作使得数据集中的行顺序变得随机。事实上，在原始数据集中，观察值是按照花卉种类排序的：首先是*50*个
    `setosa` 物种的出现次数，其次是*50*个 `versicolor` 物种的出现次数，最后是*50*个 `virginica` 物种的出现次数。经过这一操作后，行的位置变得随机。为了验证这一点，我们打印修改后的数据集的前`20`行：
- en: '[PRE11]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The numbers in the first column are the row numbers of the original dataset.
    How can we notice the shuffling flawed perfectly. To compare with the original
    sequence, see the previous example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列中的数字是原始数据集的行号。我们可以注意到洗牌操作是完美地完成的。为了与原始顺序进行比较，请参见前面的示例：
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The independent and target variables are set up and assigned to `irisValues`
    and `irisTargets` respectively:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自变量和目标变量被设置并分别分配给 `irisValues` 和 `irisTargets`：
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the first line, the training data and the test data is split up through
    the `splitForTrainingAndTest()` function. This function splits the input and target
    values to a training and a testing set. A test set is taken from the end of the
    data. If the data is to be shuffled, this should be done before calling this function.
    In particular, the data is split as follows: *85* percent for training and *15*
    percent for testing. In the second line, the data is normalized. To do this, the
    `normTrainingAndTestSet()` function is used. This function normalizes the training
    and test set in the following way: The `inputsTrain` member is normalized using
    `normalizeData` with the parameters given in type. The normalization parameters
    obtained during this normalization are then used to normalize the `inputsTest`
    member. If the `dontNormTargets` argument is not set, then the targets are normalized
    in the same way:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，训练数据和测试数据通过 `splitForTrainingAndTest()` 函数进行拆分。此函数将输入值和目标值分配到训练集和测试集。测试集是从数据的末尾提取的。如果数据需要洗牌，应该在调用该函数之前完成此操作。具体来说，数据的拆分如下：*85*
    百分比用于训练，*15* 百分比用于测试。在第二行中，数据被标准化。为此，使用了 `normTrainingAndTestSet()` 函数。该函数以如下方式对训练集和测试集进行标准化：使用
    `normalizeData` 函数对 `inputsTrain` 成员进行标准化，并使用类型中给定的参数。标准化过程中获得的标准化参数随后用于标准化 `inputsTest`
    成员。如果未设置 `dontNormTargets` 参数，则目标值也将以相同的方式进行标准化：
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `mlp()` function is called with the training dataset to build the model.
    This function creates an MLP and trains it. MLPs are fully connected feed-forward
    networks, and probably the most common network architecture in use. Training is
    usually performed by error backpropagation or a related procedure. The test dataset
    is also passed to provide the test results:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlp()` 函数与训练数据集一起调用，以构建模型。此函数创建一个 MLP 并对其进行训练。MLP 是完全连接的前馈神经网络，可能是当前使用最广泛的网络架构。训练通常通过误差反向传播或相关程序进行。测试数据集也被传递以提供测试结果：'
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'These lines of code allow us to extract useful information from the newly created
    model. The `summary()` function prints out a summary of the network. The printed
    information can be either all information of the network in the original SNNS
    file format, or the information given by `extractNetInfo`. This behavior is controlled
    with the parameter `origSnnsFormat`, while the `weightMatrix()` function extracts
    the weight matrix of an `rsnns` object. The following figure shows a screenshot
    of the summary results:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码行使我们能够从新创建的模型中提取有用信息。`summary()` 函数打印出网络的摘要信息。打印的信息可以是网络的所有信息，采用原始 SNNS
    文件格式，也可以是由 `extractNetInfo` 提供的信息。这一行为由参数 `origSnnsFormat` 控制，而 `weightMatrix()`
    函数则提取 `rsnns` 对象的权重矩阵。下图显示了摘要结果的截图：
- en: '![](img/00092.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.jpeg)'
- en: 'Now we measure the performance of the algorithm in model training:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来衡量算法在模型训练中的表现：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `plotIterativeError()` function plots the iterative training and test error
    of the net of the model. The results are shown in the following figure:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`plotIterativeError()` 函数绘制了模型网络的迭代训练和测试误差。结果显示在下图中：'
- en: '![](img/00093.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpeg)'
- en: The previous figure showns the iterative fit error as a black line and the iterative
    test error as a red line. As can be seen, both lines have a strongly decreasing
    trend, demonstrating that the algorithm quickly converges.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了迭代拟合误差作为黑线，迭代测试误差作为红线。从图中可以看出，两条线都有明显的下降趋势，证明算法迅速收敛。
- en: 'After properly training the model, it is time to use it to make predictions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确训练好模型之后，是时候用它来进行预测了：
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this case, we have used the `predict()` function. This is a generic function
    for predictions from the results of various model fitting functions. The function
    invokes particular methods which depend on the class of the first argument. We
    have both the predictions and the actual data; we just have to compare them through
    the regression error calculus:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们使用了`predict()`函数。这是一个通用的预测函数，适用于从各种模型拟合函数的结果中进行预测。该函数调用特定的方法，这些方法取决于第一个参数的类。我们既有预测值也有实际数据，我们只需要通过回归误差计算将它们进行比较：
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To plot the regression error, we have used the `plotRegressionError()` function.
    This function shows target values on the *X* axis and fitted/predicted values
    on the *Y* axis. The optimal fit would yield a line through zero with gradient
    one. This optimal line is shown in black in the following figure. A linear fit
    to the actual data is shown in red. The following figure shows the regression
    error for the model which we previously trained:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制回归误差，我们使用了`plotRegressionError()`函数。该函数在*X*轴上显示目标值，在*Y*轴上显示拟合/预测值。最佳拟合应该是通过零点的直线，且斜率为一。这条最佳线在下图中以黑色表示。对实际数据的线性拟合则用红色显示。下图展示了我们之前训练的模型的回归误差：
- en: '![](img/00094.gif)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.gif)'
- en: 'Let''s now evaluate the model performance in predicting data by computing the
    confusion matrix:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过计算混淆矩阵来评估模型在预测数据方面的表现：
- en: '[PRE19]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To compute the confusion matrix, we have used the `confusionMatrix()` function.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算混淆矩阵，我们使用了`confusionMatrix()`函数。
- en: Remember, the confusion matrix shows how many times a pattern with the real
    class `x` was classified as class `y`. A perfect method should result in a diagonal
    matrix. All values not on the diagonal are errors of the method.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，混淆矩阵显示了一个真实类别`x`的模式被分类为类别`y`的次数。完美的方法应该得到一个对角线矩阵。所有不在对角线上的值都是该方法的错误。
- en: 'In the first line of the code, we calculated the confusion matrix for the data
    used in the training (which is *85* percent of the data), while in the second
    line, we calculated the confusion matrix for the data used in the test (which
    is the remaining *15* percent of data). The results are as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第一行，我们计算了用于训练的数据的混淆矩阵（这些数据占*85*百分比），而在第二行，我们计算了用于测试的数据的混淆矩阵（这些数据占剩余的*15*百分比）。结果如下：
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As can be seen, there were four mistakes in the training phase, that only concerned
    the `versicolor` and `virginica` species. Remember, we obtained the same result
    in the example presented in [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4),
    *Deep Learning Using Multilayer Neural Networks*. In the test, however, we did
    not make any mistakes. I would say very good results, although the processed data
    is actually small. We graphically evaluate these results:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，在训练阶段有四个错误，只涉及`versicolor`和`virginica`两个物种。记住，我们在[第3章](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4)中展示的示例中得到了相同的结果，*使用多层神经网络的深度学习*。然而，在测试中，我们没有犯任何错误。我会说这是非常好的结果，尽管处理的数据实际上较少。我们以图形方式评估这些结果：
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To evaluate network performance, we have plotted the receiver operating characteristic.
    The previous commands plot the ROC for both the phases (training and testing).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估网络性能，我们绘制了接收者操作特征曲线。前面的命令绘制了两个阶段（训练和测试）的ROC曲线。
- en: 'The ROC is a metric used to check the quality of classifiers. For each class
    of a classifier, ROC applies threshold values across the interval *[0,1]* to outputs.
    The ROC curve is a plot of the TPR versus the FPR, as the threshold is varied.
    A perfect test would show points in the upper-left corner, with *100* percent
    sensitivity and *100* percent specificity. The better the lines approach the upper-left
    corner, the better is the network performance. The following figure shows the
    ROC curves for both the phases (training to the left and test to the right):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 是一种用于检查分类器质量的度量标准。对于每个分类器的类别，ROC 会对输出应用 *[0,1]* 区间内的阈值。ROC 曲线是 TPR 与 FPR
    之间的关系图，随着阈值的变化。完美的测试将在左上角显示点，具有 *100* 百分比的灵敏度和 *100* 百分比的特异性。线条越接近左上角，网络性能就越好。下图显示了两个阶段的
    ROC 曲线（训练阶段在左，测试阶段在右）：
- en: '![](img/00095.gif)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.gif)'
- en: As already mentioned, in the training phase there were errors that are absent
    in the test.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在训练阶段出现了错误，但在测试阶段则没有。
- en: Note, we used the `par()` function to display both the charts in a single window.
    In it we have set to display the graphs as a matrix with a row and two columns.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了 `par()` 函数在一个窗口中显示这两个图表。在其中，我们设置了将图形以矩阵形式显示，行数为 1，列数为 2。
- en: 'There is no `plot` function within `RSNNS`, hence we use a `plot` function
    from GitHub to plot the following MLP for the neural network we just built. There
    are three classes of output and there are four input nodes:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`RSNNS` 中没有 `plot` 函数，因此我们使用来自 GitHub 的 `plot` 函数来绘制我们刚刚构建的神经网络的 MLP。这里有三个输出类别和四个输入节点：'
- en: '![](img/00096.jpeg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.jpeg)'
- en: We have seen a simple implementation of an `iris` dataset neural network using
    `RSNNS`. The same `mlp()` function can be used for any MLP neural network architecture.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到使用 `RSNNS` 对 `iris` 数据集神经网络的简单实现。同样的 `mlp()` 函数可以用于任何 MLP 神经网络架构。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced you the concept of perceptrons, which are the
    basic building blocks of a neural network. We also saw multi-layer perceptrons
    and an implementation using `RSNNS`. The simple perceptron is useful only for
    a linear separation problem and cannot be used where the output data is not linearly
    separable. These limits are exceeded by the use of the MLP algorithm.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了感知机的概念，它是神经网络的基本构建块。我们还看到了多层感知机以及使用 `RSNNS` 的实现。简单的感知机仅对线性分离问题有效，对于输出数据不线性可分的情况则无法使用。这些局限性通过使用
    MLP 算法得到了克服。
- en: We understood the basic concepts of perceptron and how they are used in neural
    network algorithms. We discovered the linear separable classifier and the functions
    this concept applies to. We learned a simple perceptron implementation function
    in R environment and then we learnt how to train and model an MLP.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解了感知机的基本概念及其在神经网络算法中的应用。我们发现了线性可分的分类器及其适用的函数。我们学习了在 R 环境中实现简单感知机的函数，并且接着学习了如何训练和建模
    MLP。
- en: In the next chapter, we will understand how to train, test, and evaluate a dataset
    using the neural network model. We will learn how to visualize the neural network
    model in R environment. We will cover concepts like early stopping, avoiding overfitting,
    generalization of neural network, and scaling of neural network parameters.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将理解如何使用神经网络模型来训练、测试和评估数据集。我们将学习如何在 R 环境中可视化神经网络模型。我们将涵盖诸如早停法、避免过拟合、神经网络的泛化和神经网络参数的缩放等概念。
