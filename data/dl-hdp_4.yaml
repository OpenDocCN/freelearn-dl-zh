- en: Chapter 4. Recurrent Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：循环神经网络
- en: '|   | *I think the brain is essentially a computer and consciousness is like
    a computer program. It will cease to run when the computer is turned off. Theoretically,
    it could be re-created on a neural network, but that would be very difficult,
    as it would require all one''s memories.* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *我认为大脑本质上是一个计算机，意识就像一个计算机程序。当计算机关机时，程序就会停止运行。从理论上讲，它可以在神经网络上重新创建，但这将非常困难，因为这需要重建一个人的所有记忆。*
    |   |'
- en: '|   | --*Stephen Hawking* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*斯蒂芬·霍金* |'
- en: To solve every problem, people do not initiate their thinking process from scratch.
    Our thoughts are non-volatile, and it is persistent just like the **Read Only
    Memory** (**ROM**) of a computer. When we read an article, we understand the meaning
    of every word from our understanding of earlier words in the sentences.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 解决每个问题时，人们并不是从零开始思考。我们的思维是非易失性的，它是持久的，就像计算机的**只读存储器**（**ROM**）一样。当我们阅读一篇文章时，我们从对句子中早期单词的理解中理解每个单词的含义。
- en: Let us take a real life example to explain this context a bit more. Let us assume
    we want to make a classification based on the events happening at every point
    in a video. As we do not have the information of the earlier events of the video,
    it would be a cumbersome task for the traditional deep neural networks to find
    some distinguishing reasons to classify those. Traditional deep neural networks
    cannot perform this operation, and hence, it has been one of the major limitations
    for them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个现实生活中的例子来进一步解释这个问题。假设我们想基于视频中每个时刻发生的事件进行分类。由于我们没有视频早期事件的信息，对于传统深度神经网络来说，要找到一些区分这些事件的理由将是一个繁琐的任务。传统的深度神经网络无法执行这个操作，因此，这是它们的一个主要限制。
- en: '**Recurrent neural networks** (**RNN**) [103] are a special type of neural
    network, which provides many enigmatic solutions for these difficult machine learning
    and deep learning problems. In the last chapter, we discussed convolutional neural
    networks, which is specialized in processing a set of values *X* (For example,
    an image). Similarly, RNNs are magical while processing a sequence of values,
    *x (0)*, *x (1)*,*x(2)*,*...*, *x(τ-1)*. To start with RNNs in this chapter, let
    us first place this network next to convolutional neural networks so that you
    can get an idea of its basic functionalities, and to basically know about this
    network.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）[103]是一种特殊类型的神经网络，能够为这些困难的机器学习和深度学习问题提供许多神秘的解决方案。在上一章，我们讨论了卷积神经网络，它专门处理一组值*X*（例如，一张图像）。类似地，RNN在处理一系列值时具有神奇的能力，*x
    (0)*，*x (1)*，*x(2)*，*...*，*x(τ-1)*。为了在本章开始讨论RNN，我们首先将这个网络与卷积神经网络进行对比，这样你就能对其基本功能有所了解，并大致了解这个网络。'
- en: Convolutional neural networks can easily scale to images with large width, height,
    and depth. Moreover, some convolutional neural networks can also process images
    with variable sizes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络可以轻松扩展到具有大宽度、高度和深度的图像。此外，一些卷积神经网络也可以处理变尺寸的图像。
- en: In contrast, recurrent networks can readily scale to long sequences; also, most
    of those can also process variable length sequences. To process these arbitrary
    sequences of inputs, RNN uses their internal memory.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，循环网络可以轻松扩展到长序列；此外，其中大多数还可以处理变长序列。为了处理这些任意长度的输入序列，RNN利用其内部记忆。
- en: RNNs generally operate on mini-batches of sequences, and contain vectors *x
    (t)* with the time-step index *t* ranging from *0* to *(τ-1)*. The sequence length
    *τ* can also vary for each member of the mini-batch. This time-step index should
    not always refer to the time intervals in the real world, but can also point to
    the position inside the sequence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通常在小批量序列上操作，并包含时间步索引*t*范围从*0*到*(τ-1)*的向量*x (t)*。序列长度*τ*对于小批量中的每个成员也可以有所不同。这个时间步索引不一定总是指代现实世界中的时间间隔，还可以指代序列内部的位置。
- en: A RNN, when unfolded in time, can be seen as a deep neural network with indefinite
    number of layers. However, compared to common deep neural networks, the basic
    functionalities and architecture of RNNs are somewhat different. For RNNs, the
    main function of the layers is to bring in memory, and not hierarchical processing.
    For other deep neural networks, the input is only provided in the first layer,
    and the output is produced at the final layer. However, in RNNs, the inputs are
    generally received at each time step, and the corresponding outputs are computed
    at those intervals. With every network iteration, fresh information is integrated
    into every layer, and the network can go along with this information for an indefinite
    number of network updates. However, during the training phase, the recurrent weights
    need to learn which information they should pass onwards, and what they must reject.
    This feature generates the primary motivation for a special form of RNN, called
    **Long short-term memory** (**LSTM**).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当RNN展开时，可以看作是一个深度神经网络，具有不定数量的层。然而，与常见的深度神经网络相比，RNN的基本功能和架构有所不同。对于RNN来说，层的主要功能是带来记忆，而非层级处理。对于其他深度神经网络，输入仅在第一层提供，输出则在最后一层生成。然而，在RNN中，输入通常在每个时间步接收，并在这些时间间隔内计算相应的输出。随着每次网络迭代，新的信息被整合进每一层，网络可以根据这些信息进行无限次的更新。然而，在训练阶段，循环权重需要学习该传递哪些信息，哪些信息应该被拒绝。这个特点促使了一个特殊形式的RNN的出现，称为**长短期记忆（LSTM）**。
- en: RNNs started its journey a few decades back [104], but recently, it has significantly
    become a popular choice for modeling sequences of variable length. As of now,
    RNN has been successfully implemented in various problems such as learning word
    embedding [105], language modelling [106] [107] [108], speech recognition [109],
    and online handwritten recognition [110].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的起源可以追溯到几十年前[104]，但近年来，它已成为建模变长序列的热门选择。到目前为止，RNN已经成功应用于许多问题，如学习词嵌入[105]、语言建模[106][107][108]、语音识别[109]和在线手写识别[110]。
- en: In this chapter, we will discuss everything you need to know about RNN and the
    associated core components. We will introduce Long short-term memory later in
    the chapter, which is a special type of RNN.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将讨论你需要了解的关于RNN及其核心组件的所有内容。稍后，我们将介绍一种特殊类型的RNN——长短期记忆（LSTM）。
- en: 'The topic-wise organization of this chapter is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题组织结构如下：
- en: What makes recurrent networks distinctive from others?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络与其他神经网络有何不同？
- en: Recurrent neural networks(RNNs)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）
- en: Backpropagation through time (BPTT)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: Long short-term memory (LSTM)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: Bi-directional RNNs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向RNN
- en: Distributed deep RNNs
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式深度RNN
- en: RNNs with Deeplearning4j
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Deeplearning4j的RNN
- en: What makes recurrent networks distinctive from others?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络与其他神经网络有何不同？
- en: You might be curious to know the specialty of RNNs. This section of the chapter
    will discuss these things, and from the next section onwards, we will talk about
    the building blocks of this type of network.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能好奇RNN的特别之处。本章的这一部分将讨论这些内容，接下来的章节将介绍这种类型网络的构建模块。
- en: From [Chapter 3](ch03.html "Chapter 3.  Convolutional Neural Network") , *Convolutional
    Neural Network*, you have probably got a sense of the harsh limitation of convolutional
    networks and that their APIs are too constrained; the network can only take an
    input of a fixed-sized vector, and also generates a fixed-sized output. Moreover,
    these operations are performed through a predefined number of intermediate layers.
    The primary reason that makes RNNs distinctive from others is their ability to
    operate over long sequences of vectors, and produce different sequences of vectors
    as the output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第3章](ch03.html "第3章 卷积神经网络")的*卷积神经网络*中，你可能已经感受到卷积网络的严峻限制，它们的API过于受限；网络只能接收固定大小的输入向量，并且输出也是固定大小的。此外，这些操作是通过预定义数量的中间层来执行的。使得RNN与其他神经网络区别开来的主要原因是它们能够处理长序列的向量，并生成不同的向量序列作为输出。
- en: '|   | *"If training vanilla neural nets is optimization over functions, training
    recurrent nets is optimization over programs"* |   |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|   | *"如果训练传统神经网络是对函数的优化，那么训练循环网络就是对程序的优化"* |   |'
- en: '|   | --*Alex Lebrun* |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|   | --*亚历克斯·勒布伦* |'
- en: 'We show different types of input-output relationships of the neural networks
    in *Figure 4.1* to portray the differences. We show five types of input-output
    relations as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图4.1*中展示了神经网络的不同类型输入输出关系，以描绘它们的差异。我们展示了以下五种输入输出关系：
- en: '**One to one**: This input-output relationship is for traditional neural network
    processing without the involvement of a RNN. Mostly used for image classification,
    where the mapping is from fixed-sized input to fixed-sized output.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一**：这种输入输出关系适用于没有涉及RNN的传统神经网络处理。主要用于图像分类，其中映射是从固定大小的输入到固定大小的输出。'
- en: '**One to many**: In this kind of relationship, the input and output maintain
    a one-to-many relationship. The model generates a sequence of outputs with one
    fixed-sized input. Often observed where the model takes an image (image captioning),
    and produces a sentence of words.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：在这种关系中，输入和输出保持一对多的关系。模型通过一个固定大小的输入生成一系列输出。常见于模型接受图像（图像描述），并生成一串单词的句子。'
- en: '**Many to one**: In this type of relationship, the model takes a sequence of
    inputs, and outputs one single observation. For example, in case of sentiment
    analysis, a sentence or reviews are provided to the model; it classifies the sentence
    as either a positive or negative sentiment.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：在这种关系中，模型接受一系列输入，并输出一个单一的观测值。例如，在情感分析中，模型接受句子或评论，并将句子分类为积极或消极情感。'
- en: '**Many to many (Variable intermedia states)**: The model receives a sequence
    of inputs, and a corresponding sequence of outputs are generated. In this type,
    the RNN reads a sentence in English, and then translates and outputs a sentence
    in German. Used in case of Machine Translation.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多（可变中间状态）**：模型接收一系列输入，并生成相应的一系列输出。在这种类型中，RNN读取英文句子，然后将其翻译并输出德文句子。用于机器翻译的场景。'
- en: '**Many to many (Fixed number of intermedia state)**: The model receives a synced
    sequence of input, and generates a sequence of outputs. For example, in video
    classification, we might wish to classify every event of the movie.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多（固定数量的中间状态）**：模型接收一个同步的输入序列，并生成一系列输出。例如，在视频分类中，我们可能希望对电影的每个事件进行分类。'
- en: '![What makes recurrent networks distinctive from others?](img/image_04_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![递归网络与其他网络的不同之处](img/image_04_001.jpg)'
- en: 'Figure 4.1: The rectangles in the figure represent each element of the sequence
    vector, the arrows signify the functions. Input vectors are shown in red, and
    output vectors are in blue. The green color represents the intermediate RNN''s
    state. Image taken from [111].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：图中的矩形表示序列向量的每个元素，箭头表示函数。输入向量以红色显示，输出向量以蓝色显示。绿色表示中间RNN的状态。图片来自[111]。
- en: Operations that involve sequences are generally more powerful and appealing
    than networks with fixed-sized inputs and outputs. These models are used to build
    more intelligent systems. In the next sections, we will see how RNNs are built,
    and how the network unites the input vectors with their state vector with a defined
    function to generate a new state vector.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及序列的操作通常比具有固定大小输入和输出的网络更强大和更具吸引力。这些模型用于构建更智能的系统。在接下来的章节中，我们将看到如何构建RNN，以及网络如何通过定义的函数将输入向量与其状态向量结合，从而生成新的状态向量。
- en: Recurrent neural networks(RNNs)
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）
- en: In this section, we will discuss the architecture of the RNN. We will talk about
    how time is unfolded for the recurrence relation, and used to perform the computation
    in RNNs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论RNN的架构。我们将讨论时间如何展开以进行递归关系，并用于执行RNN中的计算。
- en: Unfolding recurrent computations
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展开递归计算
- en: This section will explain how unfolding a recurrent relation results in sharing
    of parameters across a deep network structure, and converts it into a computational
    model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释展开递归关系如何导致深度网络结构中参数的共享，并将其转换为计算模型。
- en: 'Let us consider a simple recurrent form of a dynamical system:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的动态系统递归形式：
- en: '![Unfolding recurrent computations](img/image_04_002-1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![展开递归计算](img/image_04_002-1.jpg)'
- en: In the preceding equation, *s ^((t))* represents the state of the system at
    time *t*, and *θ* is the same parameter shared across all the iterations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*s ^((t))*表示系统在时间*t*的状态，*θ*是所有迭代中共享的相同参数。
- en: This equation is called a recurrent equation, as the computation of *s ^((t))*
    requires the value returned by *s ^((t-1))*, the value of *s ^((t-1))* will require
    the value of *s ^((t-2))*, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为递归方程，因为计算*s ^((t))*需要*s ^((t-1))*返回的值，*s ^((t-1))*的值将需要*s ^((t-2))*的值，依此类推。
- en: 'This is a simple representation of a dynamic system for understanding purpose.
    Let us take one more example, where the dynamic system is driven by an external
    signal *x ^((t))*, and produces output *y ^((t))*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的动态系统表示，目的是帮助理解。让我们再举一个例子，其中动态系统由外部信号*x ^((t))*驱动，并产生输出*y ^((t))*：
- en: '![Unfolding recurrent computations](img/image_04_003-1.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![展开递归计算](img/image_04_003-1.jpg)'
- en: RNNs, ideally, follow the second type of equation, where the intermediate state
    retains the information about the whole past sequence. However, any equation that
    involves recurrence can be used to model the RNN.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，RNN遵循第二种类型的方程式，其中中间状态保留了关于整个过去序列的信息。然而，任何涉及递归的方程式都可以用来建模RNN。
- en: 'Therefore, similar to the feed-forward neural networks, the state of the hidden
    (intermediate) layers of RNNs can be defined using the variable *h* at time *t*,
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与前馈神经网络类似，RNN的隐藏（中间）层状态可以通过时间*t*的变量*h*来定义，如下所示：
- en: '![Unfolding recurrent computations](img/image_04_004-1.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![展开递归计算](img/image_04_004-1.jpg)'
- en: We will explain the functionality of this preceding equation in a RNN in the
    next part of this section. As of now, to illustrate the functionality of this
    hidden layer, *Figure 4.2* shows a simple recurrent network with no output. The
    left side of the figure shows a network whose current state influences the next
    state. The box in the middle of the loop represents the delay between two successive
    time steps.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节的下一部分解释这个前面方程在RNN中的功能。到目前为止，为了说明这个隐藏层的功能，*图 4.2*展示了一个没有输出的简单递归网络。图的左侧显示了一个网络，其当前状态影响下一个状态。循环中间的框表示两个连续时间步之间的延迟。
- en: As shown in the preceding recurrent equation, we can unfold or unroll the hidden
    states in time. The right side of the image shows the unfolded structure of the
    recurrent network. There, the network can be converted to a feed-forward network
    by unfolding over time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的递归方程所示，我们可以展开或展开时间中的隐藏状态。图像的右侧显示了递归网络的展开结构。在那里，通过在时间上展开，网络可以转换为前馈网络。
- en: In an unfolded network, each variable for each time step can be shown as a separate
    node of the network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在展开的网络中，每个时间步的每个变量可以作为网络的一个独立节点展示。
- en: '![Unfolding recurrent computations](img/image_04_005.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![展开递归计算](img/image_04_005.jpg)'
- en: 'Figure 4.2: The left part of the figure shows the recurrent network where the
    information passes through multiple times through the hidden layer with each time
    step. On the right, we have the unfolded structure of the same network. Each node
    of this network is associated with one timestamp.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：图的左侧显示了递归网络，其中信息在每个时间步通过隐藏层多次传递。在右侧，我们有相同网络的展开结构。该网络的每个节点都与一个时间戳相关联。
- en: So, from *Figure 4.2*, the unfolding operation can be defined as an operation
    that performs the mapping of the circuit on the left-hand side to a computational
    model split into multiple states on the right-hand side.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，来自*图 4.2*的展开操作可以定义为一个操作，它将左侧电路的映射执行到右侧分割成多个状态的计算模型。
- en: Advantages of a model unfolded in time
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在时间上展开的模型的优势
- en: 'Unfolding a network in time provides a few major advantages, which are listed
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间上展开一个网络带来了几个主要优势，列举如下：
- en: A model without a parameter would require many training examples for learning
    purposes. However, learning a shared single model helps to generalize the sequence
    lengths, even those that are not present in the training set. This allows the
    model to estimate the upcoming sequence data with fewer training examples.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个没有参数的模型将需要大量的训练示例用于学习。然而，学习一个共享的单一模型有助于推广序列长度，甚至是那些在训练集中不存在的序列。这使得模型能够使用较少的训练示例来估计即将到来的序列数据。
- en: Irrespective of the length of the sequence, the input size of the model will
    always remain the same. The input size in an unfolded model is specified in terms
    of transition from the hidden state to the other. However, for other cases, it
    is specified in terms of undefined length of the history of states.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不论序列的长度如何，模型的输入大小始终保持不变。在展开模型中，输入大小是指从隐藏状态到其他状态的过渡。然而，对于其他情况，它是根据状态历史的未定义长度来指定的。
- en: Due to parameter sharing, the same transition function *f*, with the same parameter
    can be used at every time step.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于参数共享，相同的过渡函数*f*可以在每个时间步使用相同的参数。
- en: Memory of RNNs
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN的记忆
- en: As of now, you might have got some idea that the primary difference between
    a feed forward neural network and recurrent network is the feedback loop. The
    feedback loop is ingested into its own intermediate outcome as the input to the
    next state. The same task is performed for every element of the input sequence.
    Hence, the output of each hidden state depends on the previous computations. In
    a practical situation, each hidden state is not only concerned about the current
    input sequence in action, but also about what they perceived one step back in
    time. So, ideally, every hidden state must have all the information of the previous
    step's outcome.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能已经对前馈神经网络和递归网络之间的主要区别有了一些了解。反馈循环作为其自己中间结果的输入被摄入到下一个状态中。每个输入序列的每个元素都执行相同的任务。因此，每个隐藏状态的输出取决于先前计算的结果。在实际情况中，每个隐藏状态不仅关注当前活动的输入序列，还关注它们在时间上一步感知到的内容。因此，理想情况下，每个隐藏状态必须具有上一步结果的所有信息。
- en: Due to this requirement of persistent information, it is said that RNNs have
    their *own memory*. The sequential information is preserved as memory in the recurrent
    network's hidden state. This helps to handle the upcoming time steps as the network
    cascades forward to update the processing with each new sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要持久信息，据说RNN具有其*自身的内存*。顺序信息作为循环网络隐藏状态中的记忆得以保留。这有助于处理随着每个新序列向前级联更新处理的即将到来的时间步骤。
- en: '*Figure 4.3* shows the concept of *simple recurrent neural networks* proposed
    by Elman back in 1990 [112]; it shows the illustration of persistent memory for
    a RNN.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.3*显示了Elman在1990年提出的*简单递归神经网络*的概念[112]；它展示了为RNN提供持久内存的插图。'
- en: In the next figure, a part of the word sequence AYSXWQF at the bottom represents
    the input example currently under consideration. Each box of this input example
    represents a pool of units. The forward arrow shows the complete set of trainable
    mapping from each sending input unit to each output unit for the next time step.
    The context unit, which can be considered as the persistent memory unit, preserves
    the output of the previous steps. The backward arrow, directed from the hidden
    layer to the context unit shows a copy operation of the output, used for evaluating
    the outcome for the next time step.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个图中，底部的字序列AYSXWQF的一部分代表当前正在考虑的输入示例。此输入示例的每个框表示一个单元池。向前箭头显示了从每个发送输入单元到下一个时间步的每个输出单元的完整可训练映射。上下文单元可以被视为持久内存单元，保留了前几步的输出。从隐藏层到上下文单元的向后箭头显示了输出的复制操作，用于评估下一个时间步的结果。
- en: The decision where a RNN reaches at time step *t*, depends mostly on its last
    decision of the time step at *(t-1)*. Therefore, it can be inferred that unlike
    traditional neural networks, RNNs have two sources of input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在时间步*t*的决策主要取决于其在*(t-1)*时间步的上一个决策。因此，可以推断出，与传统神经网络不同，RNN有两个输入源。
- en: One is the current input unit under consideration, which is *X* in the following
    figure, and the other one is the information received from the recent past, which
    is taken from the context units in the figure. The two sources, in combination,
    decide the output of the current time step. More about this will be discussed
    in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个是考虑中的当前输入单元，即下图中的*X*，另一个是从图中的上下文单元获取的来自最近过去的信息。这两个源结合起来决定当前时间步的输出。关于这点将在下一节中讨论。
- en: '![Memory of RNNs](img/image_04_006.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![RNN的记忆](img/image_04_006.jpg)'
- en: 'Figure 4.3: A simple recurrent neural networks with the concept of memory of
    RNN is shown in this figure.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.3: 显示了具有RNN记忆概念的简单递归神经网络。'
- en: Architecture
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: So, we have come to know that RNNs have their memory, which collects information
    about what has been computed so far. In this section, we will discuss the general
    architecture of RNNs and their functioning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们了解到 RNN 有它们的记忆，用于收集到目前为止计算的信息。在本节中，我们将讨论 RNN 的一般结构及其工作原理。
- en: A typical RNN, unfolded (or unrolled) at the time of the calculation involved
    in its forward computation is shown in *Figure 4.4*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 RNN，在进行前向计算时的展开（或展开）过程如 *图 4.4* 所示。
- en: 'Unrolling or unfolding a network means to write out the network for the complete
    sequences of input. Let us take an example before we start explaining the architecture.
    If we have a sequence of 10 words, the RNN would then be unfolded into a 10-layer
    deep neural network, one layer for each word, as depicted by the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 展开或展开一个网络意味着将网络写成完整的输入序列。让我们在开始解释结构之前举个例子。如果我们有一个包含 10 个单词的序列，那么 RNN 将展开成一个
    10 层深的神经网络，每一层对应一个单词，如下图所示：
- en: '![Architecture](img/image_04_007.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/image_04_007.jpg)'
- en: 'Figure 4.4: The figure shows a RNN being unrolled or unfolded into a full network.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：该图展示了一个 RNN 被展开或展开成完整网络的过程。
- en: The time period to reach from input *x* to output *o* is split into several
    timestamps given by *(t-1)*, *t*, *(t+1)*, and so on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入 *x* 到输出 *o* 的时间段被分割成多个时间戳，分别为 *(t-1)*、*t*、*(t+1)*，以此类推。
- en: 'The computational steps and formulae for an RNN are listed as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的计算步骤和公式列举如下：
- en: In the preceding figure, *x[t]* is the input at time step *t*. The figure shows
    computations for three timestamps *(t-1)*, *t*, and *(t+1)*, where the inputs
    are *x[(t-1)]*, *x[t]*, and *x[(t+1)],* respectively. For example, *x[1]* and
    *x[2]* are the vectors that correspond to the second and third word of the sequence.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前面的图中，*x[t]* 是时间步 *t* 的输入。图中展示了三个时间戳 *(t-1)*、*t* 和 *(t+1)* 的计算，其中输入分别为 *x[(t-1)]*、*x[t]*
    和 *x[(t+1)]*。例如，*x[1]* 和 *x[2]* 是对应于序列中第二个和第三个单词的向量。
- en: '*s[t]* represents the hidden state at time step *t*. Conceptually, this state
    defines the memory of the neural network. Mathematically, the formulation for
    *s[t]* or the process of carrying memory can be written as follows:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s[t]* 表示时间步 *t* 的隐藏状态。从概念上讲，这个状态定义了神经网络的记忆。从数学角度来看，*s[t]* 的公式或记忆传递过程可以写成如下形式：'
- en: '![Architecture](img/image_04_008-1.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/image_04_008-1.jpg)'
- en: So, the hidden state is a function of the input at time step *x[t]*, multiplied
    by the weight *U*, and addition of the hidden state of the last time step *s[t-1]*,
    which is multiplied by its own hidden-state-to-hidden-state matrix *W*. This hidden-state-to-hidden-state
    is often termed as a transition matrix, and is similar to a Markov chain. The
    weight matrices behave as filters, which primarily decide the importance of both,
    the past hidden state and the current input. The error generated for the current
    state would be sent back via backpropagation to update these weights until the
    error is minimized to the desired value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，隐藏状态是时间步 *x[t]* 输入与权重 *U* 的乘积，再加上上一个时间步的隐藏状态 *s[t-1]*，它与自身的隐藏状态到隐藏状态的矩阵 *W*
    相乘。这种隐藏状态到隐藏状态的转换通常称为转移矩阵，类似于马尔可夫链。权重矩阵像滤波器一样起作用，主要决定过去隐藏状态和当前输入的重要性。当前状态产生的误差会通过反向传播返回，用于更新这些权重，直到误差被最小化到期望值。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To calculate the first hidden state, we would require determining the value
    *s-1*, which is generally initialized to all zeroes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算第一个隐藏状态，我们需要确定值 *s-1*，通常初始化为全零。
- en: Unlike a traditional deep neural network, where different parameters are used
    for the computation at each layer, a RNN shares the same parameters (here, *U*,
    *V*, and *W*) across all the time steps to calculate the value of the hidden layer.
    This makes the life of a neural network much easier, as we need to learn fewer
    number of parameters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的深度神经网络不同，在传统深度神经网络中，每一层的计算都使用不同的参数，而 RNN 在所有时间步中共享相同的参数（这里是 *U*、*V* 和 *W*），用来计算隐藏层的值。这使得神经网络的训练过程更为简便，因为我们需要学习的参数数量较少。
- en: 'This sum of the weight input and hidden state is squeezed by the function *f*,
    which usually is a nonlinearity such as a logistic sigmoid function, *tan h*,
    or ReLU:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这份输入权重和隐藏状态的总和通过函数 *f* 被压缩，通常 *f* 是一个非线性函数，如逻辑Sigmoid函数、*tan h* 或 ReLU：
- en: 'In the last figure, *o[t]* is represented as the output at time step t. The
    output at step *o[t]* is solely computed based on the memory available for the
    network at time t. Theoretically, although RNNs can persist memory for arbitrarily
    long sequences, in practice, it''s a bit complicated, and they are limited to
    looking back only a few time steps. Mathematically, this can be represented as
    follows:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一张图中，*o[t]* 表示在时间步 *t* 上的输出。步骤 *o[t]* 的输出仅基于时间 *t* 时刻网络可用的记忆进行计算。从理论上讲，虽然
    RNN 可以持续记忆任意长的序列，但实际上这有点复杂，它们仅限于回溯几个时间步。数学上，这可以表示如下：
- en: '![Architecture](img/image_04_009-1.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/image_04_009-1.jpg)'
- en: The next section shall discuss how to train a RNN through back propagation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将讨论如何通过反向传播训练一个 RNN。
- en: Backpropagation through time (BPTT)
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: You have already learnt that the primary requirement of RNNs is to distinctly
    classify the sequential inputs. The backpropagation of error and gradient descent
    primarily help to perform these tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学过，RNN 的主要需求是明确地分类顺序输入。误差的反向传播和梯度下降主要有助于执行这些任务。
- en: 'In case of feed forward neural networks, backpropagation moves in the backward
    direction from the final error outputs, weights, and inputs of each hidden layer.
    Backpropagation assigns the weights responsible for generating the error, by calculating
    their partial derivatives: ![Backpropagation through time (BPTT)](img/B05883_04_16.jpg)
    where *E* denotes the error and *w* is the respective weights. The derivatives
    are applied on the learning rate, and the gradient decreases to update the weights
    so as to minimize the error rate.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前馈神经网络，反向传播是从最终的误差输出、权重和每个隐藏层的输入反向传递的。反向传播通过计算每个权重的偏导数来分配导致误差的权重：![时间反向传播（BPTT）](img/B05883_04_16.jpg)，其中
    *E* 表示误差，*w* 是相应的权重。偏导数作用于学习率，梯度下降更新权重，以最小化误差率。
- en: However, a RNN, without using backpropagation directly, uses an extension of
    it, termed as **backpropagation through time** (**BPTT**). In this section, we
    will discuss BPTT to explain how the training works for RNNs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个 RNN 在没有直接使用反向传播的情况下，使用它的扩展，称为**时间反向传播**（**BPTT**）。在本节中，我们将讨论 BPTT，解释 RNN
    的训练是如何进行的。
- en: Error computation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 误差计算
- en: The **backpropagation through time** (**BPTT**) learning algorithm is a natural
    extension of the traditional backpropagation method, which computes the gradient
    descent on a complete unrolled neural network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间反向传播**（**BPTT**）学习算法是传统反向传播方法的自然扩展，它在一个完整展开的神经网络上计算梯度下降。'
- en: '*Figure 4.5* shows the errors associated with each hidden state for an unrolled
    RNN. Mathematically, the errors associated with each state can be given as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.5* 显示了展开的 RNN 中每个隐藏状态的相关误差。从数学上讲，每个状态相关的误差可以表示如下：'
- en: '![Error computation](img/image_04_011-1.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![误差计算](img/image_04_011-1.jpg)'
- en: where *o[t]* represents the correct output, and *ô[t]* represents the predicted
    word at time step *t*. The total error (cost function) of the whole network is
    calculated as the summation of all the intermediate errors at each time step.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *o[t]* 代表正确的输出，*ô[t]* 代表在时间步 *t* 上的预测词。整个网络的总误差（成本函数）是通过对每个时间步的所有中间误差求和来计算的。
- en: 'If the RNN is unfolded into multiple time steps, starting from *t[0]* to *t[n-1]*,
    the total error can be written as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 RNN 展开为多个时间步，从 *t[0]* 到 *t[n-1]*，则总误差可以表示如下：
- en: '![Error computation](img/image_04_012-1.jpg)![Error computation](img/image_04_013.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![误差计算](img/image_04_012-1.jpg)![误差计算](img/image_04_013.jpg)'
- en: 'Figure 4.5: The figure shows errors associated with every time step for a RNN.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：该图显示了 RNN 每个时间步的相关误差。
- en: In the backpropagation through time method, unlike the traditional method, the
    gradient descent weight is updated in each time step.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间反向传播方法中，与传统方法不同，梯度下降的权重在每个时间步都会更新。
- en: 'Let *w[ij]* denote the connection of weight from neuron *i* to neuron *j*.
    *η* denotes the learning rate of the network. So, mathematically, the weight update
    with gradient descent at every time step is given by the following equation:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 令 *w[ij]* 表示从神经元 *i* 到神经元 *j* 的权重连接。*η* 表示网络的学习率。因此，从数学上讲，梯度下降的权重更新在每个时间步可以通过以下方程表示：
- en: '![Error computation](img/B05883_04.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![误差计算](img/B05883_04.jpg)'
- en: Long short-term memory
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: In this section, we will discuss a special unit called **Long short-term memory**
    (**LSTM**), which is integrated into RNN. The main purpose of LSTM is to prevent
    a significant problem of RNN, called the vanishing gradient problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一个特殊的单元，称为**长短期记忆**（**LSTM**），它被集成到RNN中。LSTM的主要目的是防止RNN的一个重要问题——梯度消失问题。
- en: Problem with deep backpropagation with time
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度反向传播在时间上的问题
- en: Unlike the traditional feed forward network, due to unrolling of a RNN with
    narrow time steps, the feed forward network generated this way could be aggressively
    deep. This sometimes makes it extremely difficult to train via backpropagation
    through the time procedure.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的前馈网络不同，由于RNN在狭窄时间步长下的展开，生成的前馈网络可能会非常深。这有时使得通过时间反向传播程序进行训练变得极其困难。
- en: In the first chapter, we discussed the vanishing gradient problem. An unfolded
    RNN suffers from the vanishing gradient problem of exploding while performing
    backpropagation through time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讨论了梯度消失问题。展开的RNN在进行反向传播时会遭遇梯度消失或梯度爆炸问题。
- en: Every state of a RNN depends on its input and its previous output multiplied
    by the current hidden state vector. The same operations happen to the gradient
    in the reverse direction during backpropagation through time. The layers and numerous
    time steps of the unfolded RNN relate to each other through multiplication, hence
    the derivatives are susceptible to vanish with every pass.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的每个状态都依赖于其输入和上一个输出与当前隐藏状态向量的乘积。反向传播过程中，梯度在逆向传播时执行相同的操作。展开的RNN的各层和多个时间步通过乘法相互关联，因此导数在每次传递时容易消失。
- en: On the other hand, a small gradient tends to get smaller, while a large gradient
    gets even larger while passing through every time step. This creates the vanishing
    or exploding gradient problem respectively for a RNN.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，小梯度趋向于变得更小，而大梯度在每次时间步传递时会变得更大。这分别导致了RNN的梯度消失或梯度爆炸问题。
- en: Long short-term memory
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: In the mid-90s, an updated version of RNNs with a special unit, called **Long
    short-term memory** (**LSTM**) units, was proposed by German researchers Sepp
    Hochreiter and Juergen Schmidhuber [116] to protect against the exploding or vanishing
    gradient problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在90年代中期，德国研究人员Sepp Hochreiter和Juergen Schmidhuber提出了一种更新版的RNN，加入了一个特殊单元，称为**长短期记忆**（**LSTM**）单元，以防止梯度爆炸或梯度消失问题[116]。
- en: LSTM helps to maintain a constant error, which can be propagated though time
    and through each layer of the network. This preservation of constant error allows
    the unrolled recurrent networks to learn on an aggressively deep network, even
    unrolled by a thousand time steps. This eventually opens a channel to link the
    causes of effects remotely.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM有助于保持恒定的误差，这些误差可以在时间和网络的每一层中传播。恒定误差的保持使得展开的循环神经网络能够在极深的网络中学习，甚至展开到千个时间步。这最终打开了一条通道，可以远程关联因果关系。
- en: 'The architecture of LSTM maintains a constant error flow through the internal
    state of special memory units. The following figure (*Figure 4.6*) shows a basic
    block diagram of a LSTM for easy understanding:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的架构通过特殊的记忆单元保持恒定的误差流。下图（*图4.6*）展示了LSTM的基本框图，便于理解：
- en: '![Long short-term memory](img/image_04_015.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/image_04_015.jpg)'
- en: 'Figure 4.6: The figure shows a basic model of the Long-short term memorys'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：该图显示了长短期记忆的基本模型。
- en: As shown in the preceding figure, an LSTM unit is composed of a memory cell
    that primarily stores information for long periods of time. Three specialized
    gate neurons-- write gate, read gate, and forget gate-protect the access to this
    memory cell. Unlike the digital storage of computers, the gates are analog in
    nature, with a range of 0 to 1\. Analog devices have an added advantage over digital
    ones, as they are differentiable, and hence, serve the purpose for the backpropagation
    method. The gate cell of LSTM, instead of forwarding the information as inputs
    to the next neurons, sets the associated weights connecting the rest of the neural
    network to the memory cell. The memory cell is, basically, a self-connected linear
    neuron. When the forget cell is reset (turned **0**), the memory cell writes its
    content to itself and remembers the last content of the memory. For a memory write
    operation though, the forget gate and write get should be set (turned **1**).
    Also, when the forget gate outputs something close to **1**, the memory cell effectively
    forgets all the previous contents that it had stored. Now, when the write gate
    is set, it allows any information to write into its memory cell. Similarly, when
    the read gate outputs a **1**, it will allow the rest of the network to read from
    its memory cell.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，LSTM单元由一个主要用于长期存储信息的记忆单元组成。三个专门的门神经元——写入门、读取门和遗忘门——保护着对该记忆单元的访问。与计算机的数字存储不同，门是模拟的，范围从0到1。模拟设备相对于数字设备具有额外的优势，因为它们是可微分的，因此，适合反向传播方法的需要。LSTM的门单元并不将信息作为输入传递给下一个神经元，而是设置与神经网络其他部分连接到记忆单元的相关权重。记忆单元基本上是一个自连接的线性神经元。当遗忘单元被重置（设置为**0**）时，记忆单元会将其内容写入自身并记住记忆的最后内容。对于写入操作，遗忘门和写入门应被设置（设置为**1**）。此外，当遗忘门输出接近**1**时，记忆单元实际上会忘记它之前存储的所有内容。现在，当写入门被设置时，它允许任何信息写入到记忆单元中。同样，当读取门输出**1**时，它将允许网络其他部分从记忆单元中读取数据。
- en: 'As explained earlier, the problem with computing the gradient descent for traditional
    RNNs is that the error gradient vanishes rapidly while propagating through the
    time steps in the unfolded network. Adding an LSTM unit, the error values when
    backpropagated from the output are collected in the memory cell of the LSTM units.
    This phenomenon is also known as *error carousel*. We will use the following example
    to describe how LSTM overcomes the vanishing gradient problem for RNNs:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，传统RNN计算梯度下降的问题在于，误差梯度在展开的网络中传播时迅速消失。加入LSTM单元后，从输出反向传播的误差值被收集到LSTM单元的记忆单元中。这一现象也被称为*误差旋转木马*。我们将通过以下示例来描述LSTM如何克服RNN的梯度消失问题：
- en: '![Long short-term memory](img/B05883_04_07-2.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![长短时记忆](img/B05883_04_07-2.jpg)'
- en: 'Figure 4.7: The figure shows a Long-short term memory unfolded in time. It
    also depicts how the content of the memory cell is protected with the help of
    three gates.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：该图显示了一个展开的长短期记忆单元，展示了如何通过三个门的帮助保护记忆单元的内容。
- en: '*Figure 4.7* shows a Long short-term memory unit unrolled through time. We
    will start with initializing the value of the forget gate to **1** and write gate
    to **1**. As shown in the preceding figure, this will write an information **K**
    into the memory cell. After writing, this value is retained in the memory cell
    by setting the value of the forget gate to **0**. We then set the value of the
    read gate as **1**, which reads and outputs the value **K** from the memory cell.
    From the point of loading **K** into the memory cell to the point of reading the
    same from the memory cell backpropagation through time is followed.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.7* 显示了一个通过时间展开的长短期记忆单元。我们将从初始化遗忘门的值为**1**和写入门的值为**1**开始。如前图所示，这将把信息**K**写入记忆单元。写入后，通过将遗忘门的值设置为**0**，该值将被保留在记忆单元中。然后我们将读取门的值设置为**1**，从记忆单元中读取并输出值**K**。从将**K**加载到记忆单元开始，到从记忆单元读取该值时，都会遵循时间上的反向传播过程。'
- en: The error derivatives received from the read point backpropagate through the
    network with some nominal changes, until the write point. This happens because
    of the linear nature of the memory neuron. Thus, with this operation, we can maintain
    the error derivatives over hundreds of steps without going into the trap of the
    vanishing gradient problems.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从读取点接收到的误差导数通过网络进行反向传播，直到写入点，并且在过程中进行一些微小的变化。这是因为记忆神经元的线性特性。因此，通过这种操作，我们可以在不陷入梯度消失问题的情况下，保持数百步的误差导数。
- en: So there are many reasons for why Long short-term memory outperforms standard
    RNNs. LSTM was able to achieve the best known result in unsegmented connected
    handwriting recognition [117]; also, it is equally successfully applied to automated
    speech recognition. As of now, the major technological companies such as Apple,
    Microsoft, Google, Baidu, and so on have started to widely use LSTM networks as
    a primary component for their latest products [118].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，长期短期记忆（LSTM）之所以优于标准 RNN，有很多原因。LSTM 在未分割的连接手写识别中达到了目前已知的最佳结果[117]；同时，它也成功地应用于自动语音识别。到目前为止，主要的科技公司，如苹果、微软、谷歌、百度等，已经开始广泛使用
    LSTM 网络作为其最新产品的主要组成部分[118]。
- en: Bi-directional RNNs
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向 RNN
- en: This section of the chapter will discuss the major limitations of RNNs and how
    bi-directional RNN, a special type of RNN helps to overcome those shortfalls.
    Bi-directional neural networks, apart from taking inputs from the past, takes
    the information from the future context for its required prediction.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论 RNN 的主要局限性，以及如何通过双向 RNN（一种特殊类型的 RNN）克服这些不足。与传统的 RNN 不同，双向神经网络除了从过去获取输入，还能够从未来的上下文中获取信息以进行所需的预测。
- en: Shortfalls of RNNs
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 的不足之处
- en: The computation power of standard or unidirectional RNNs has constraints, as
    the current state cannot reach its future input information. In many cases, the
    future input information coming up later becomes extremely useful for sequence
    prediction. For example, in speech recognition, due to linguistic dependencies,
    the appropriate interpretation of the voice as a phoneme might depend on the next
    few spoken words. The same situation might also arise in handwriting recognition.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 标准或单向 RNN 的计算能力受到限制，因为当前状态无法获得其未来的输入信息。在许多情况下，后续的未来输入信息对于序列预测变得极为重要。例如，在语音识别中，由于语言依赖性，作为音素的语音的适当解释可能依赖于接下来几句话中的词汇。手写识别中也可能出现类似的情况。
- en: 'In some modified versions of RNN, this feature is partially attained by inserting
    some delay of a certain amount (*N*) of time steps in the output. This delay helps
    to capture the future information to predict the data. Although, theoretically,
    in order to capture most of the available future information, the value of *N*
    can be set as very large, but in a practical scenario, the prediction power of
    the model actually reduces with a large value of *N*. The paper [113] has put
    some logical explanation for this inference. As the value of *N* increases, most
    of the computational power of a RNN only focuses on remembering the input information
    for ![Shortfalls of RNNs](img/image_04_017.jpg) (from *Figure 4.8*) to predict
    the outcome, *y[tc]*. (*t[c]* in figure denotes the current time step in consideration).
    Therefore, the model will have less processing power to combine the prediction
    knowledge received from different input vectors. The following *Figure 4.8* shows
    an illustration of the amount of input information needed for different types
    of RNNs:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些修改版本的 RNN 中，这一特性通过在输出中插入一定数量的时间步（*N*）的延迟部分实现。这个延迟有助于捕捉未来的信息，以预测数据。虽然理论上，为了捕捉大部分可用的未来信息，*N*
    的值可以设定得非常大，但在实际应用中，模型的预测能力实际上会随着 *N* 值的增大而减弱。文献 [113] 对这一推论给出了一些逻辑解释。随着 *N* 值的增大，RNN
    的大部分计算能力仅仅集中在记忆输入信息上，以便从![RNN的不足之处](img/image_04_017.jpg)（*图 4.8* 中）预测结果，*y[tc]*。
    (*t[c]* 在图中表示当前的时间步)。因此，模型会减少处理来自不同输入向量的预测知识的能力。以下 *图 4.8* 显示了不同类型的 RNN 所需的输入信息量：
- en: '![Shortfalls of RNNs](img/image_04_018-1.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![RNN的不足之处](img/image_04_018-1.jpg)'
- en: 'Figure 4.8: The figure shows visualizations of input information used by different
    types of RNNs. [113]'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：该图展示了不同类型 RNN 使用的输入信息的可视化。[113]
- en: Solutions to overcome
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服不足的解决方案
- en: To subjugate the limitations of a unidirectional RNN explained in the last section,
    **bidirectional recurrent network** (**BRNN**) was invented in 1997 [113].
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服上一节中解释的单向 RNN 的局限性，**双向递归网络**（**BRNN**）于 1997 年发明[113]。
- en: The basic idea behind bidirectional RNN is to split the hidden state of a regular
    RNN into two parts. One part is responsible for the forward states (positive time
    direction), and the other part for the backward states (negative time direction).
    Outputs generated from the forward states are not connected to the inputs of the
    backward states, and vice versa. A simple version of a bidirectional RNN, unrolled
    in three time steps is shown in *Figure 4.9*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 双向RNN的基本思想是将常规RNN的隐藏状态分为两部分。一部分负责前向状态（正时间方向），另一部分负责后向状态（负时间方向）。从前向状态生成的输出与后向状态的输入不连接，反之亦然。一个简单的双向RNN版本，展开为三个时间步骤，如*图4.9*所示。
- en: With this structure, as both the time directions are taken care of, the currently
    evaluated time frame can easily use the input information from the past and the
    future. So, the objective function of the current output will eventually minimize,
    as we do not need to put further delays to include the future information. This
    was necessary for regular RNNs as stated in the last section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种结构，由于考虑了两个时间方向，当前评估的时间框架可以轻松使用来自过去和未来的输入信息。因此，当前输出的目标函数最终会最小化，因为我们不需要再增加延迟来包含未来的信息。对于常规的RNN来说，这在上一节中已经提到，是必须的。
- en: '![Solutions to overcome](img/image_04_019.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![解决方法](img/image_04_019.jpg)'
- en: 'Figure 4.9: The figure shows the conventional structure of a bidirectional
    neural network unrolled in three time steps.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：该图展示了一个常规双向神经网络在三个时间步骤中的展开结构。
- en: So far, bidirectional RNNs have been found to be extremely useful in applications
    such as speech recognition [114], handwriting recognition, bioinformatics [115],
    and so on.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，双向RNN在语音识别[114]、手写识别、生物信息学[115]等应用中被发现极为有用。
- en: Distributed deep RNNs
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度RNN
- en: As you now have an understanding of a RNN, its applications, features, and architecture,
    we can now move on to discuss how to use this network as distributed architecture.
    Distributing RNN is not an easy task, and hence, only a few researchers have worked
    on this in the past. Although the primary concept of data parallelism is similar
    for all the networks, distributing RNNs among multiple servers requires some brainstorming
    and a bit tedious work too.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你已经理解了RNN的应用、特点和结构，我们可以继续讨论如何将该网络作为分布式架构来使用。分布式RNN并不是一项容易的任务，因此，过去只有少数研究者在这方面进行了研究。尽管数据并行的基本概念对于所有网络都是相似的，但将RNN分布到多个服务器上需要一些头脑风暴，并且工作也相对繁琐。
- en: Recently, one work from Google [119] has tried to distribute recurrent networks
    in many servers in a speech recognition task. In this section, we will discuss
    this work on distributed RNNs with the help of Hadoop.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，谷歌的一项研究[119]尝试在语音识别任务中将递归网络分布到多个服务器上。在这一节中，我们将讨论谷歌在分布式RNN方面的工作，并借助Hadoop来实现。
- en: '**Asynchronous stochastic gradient descent** (**ASGD**) can be used for large-scale
    training of a RNN. ASGD has particularly shown success in sequence discriminative
    training of the deep neural networks.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**异步随机梯度下降** (**ASGD**) 可以用于大规模训练循环神经网络（RNN）。ASGD在深度神经网络的序列判别训练中表现出了特别的成功。'
- en: A two-layer deep Long short-term memory RNN is used to build the Long short-term
    memory network. Each Long short-term memory consists of 800 memory cells. The
    paper uses 13 million parameters for the LSTM network. For cell input and output
    units tan h (hyperbolic tangent activation) is used, and for the write, read,
    and forget gates, the logistic sigmoid function is used.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个两层深度的长短期记忆（LSTM）RNN用于构建长短期记忆网络。每个LSTM包含800个记忆单元。该论文使用了1300万个LSTM网络参数。对于单元的输入和输出，使用的是tan
    h（双曲正切激活函数），对于写入、读取和遗忘门，使用的是逻辑 sigmoid 函数。
- en: For training purposes, the input speech training data can be split and randomly
    shuffled across multiple DataNodes of the Hadoop framework. The Long short-term
    memory is put across all these DataNodes, and distributed training is performed
    on those datasets in parallel. Asynchronous stochastic gradient descent is used
    for this distributed training. One parameter server, dedicated for maintaining
    the current state of all model parameters, is used.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行训练，输入的语音训练数据可以被拆分并随机打乱，分布到Hadoop框架的多个DataNode上。长短期记忆（LSTM）被部署到所有这些DataNode上，并在这些数据集上进行并行分布式训练。异步随机梯度下降用于此分布式训练。使用一个参数服务器，专门用于维护所有模型参数的当前状态。
- en: To implement this procedure on Hadoop, each DataNode has to perform asynchronous
    stochastic gradient descent operations on the partitioned data. Each worker, running
    on each block of the DataNodes works on the partitions, one utterance at a time.
    For each utterance of the speech, the model parameter *P* is fetched from the
    parameter server mentioned earlier. The workers compute the current state of every
    frame; decipher the speech utterance to calculate the final outer gradients. The
    updated parameter is then sent back to the parameter server. The workers then
    repeatedly request the parameter server to provide the latest parameters. Backpropagation
    through time is then performed to calculate the updated parameter gradient for
    the next set of frames, which is again sent back to the parameter server.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Hadoop上实现这个过程，每个DataNode必须对分区数据执行异步随机梯度下降操作。每个工作节点在每个DataNode的块上运行，逐个处理分区中的数据。对于每个语音片段，模型参数*P*会从前面提到的参数服务器中获取。工作节点计算每一帧的当前状态；解码语音片段以计算最终的外部梯度。更新后的参数然后被发送回参数服务器。工作节点随后会反复请求参数服务器提供最新的参数。然后，执行时间反向传播来计算下一组帧的更新参数梯度，并再次将其发送回参数服务器。
- en: RNNs with Deeplearning4j
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Deeplearning4j的RNN
- en: Training a RNN is not a simple task, and it can be extremely computationally
    demanding sometimes. With long sequences of training data involving many time
    steps, the training, sometimes becomes extremely difficult. As of now, you have
    got a better theoretical understanding of how and why backpropagation through
    time is primarily used for training a RNN. In this section, we will consider a
    practical example of the use of a RNN and its implementation using Deeplearning4j.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个RNN并非易事，有时它可能会非常消耗计算资源。当训练数据涉及多个时间步长的长序列时，训练过程有时会变得异常困难。到目前为止，你已经对为何以及如何通过时间反向传播（backpropagation
    through time）来训练RNN有了更好的理论理解。在本节中，我们将考虑RNN应用的一个实际示例，并使用Deeplearning4j实现它。
- en: We now take an example to give an idea of how to do the sentiment analysis of
    a movie review dataset using RNN. The main problem statement of this network is
    to take some raw text of a movie review as input, and classify that movie review
    as either positive or negative based on the contents present. Each word of the
    raw review text is converted to vectors using the Word2Vec model, and then fed
    into a RNN. The example uses a large-scale dataset of raw movie reviews taken
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    .
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过一个示例来说明如何使用RNN进行电影评论数据集的情感分析。该网络的主要问题是，将一段电影评论的原始文本作为输入，并根据内容将该评论分类为正面或负面。原始评论文本中的每个单词都使用Word2Vec模型转换为向量，然后输入到RNN中。该示例使用了一个大规模的原始电影评论数据集，数据来自[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)。
- en: 'The whole implementation of this model using DL4J can be split into the following
    few steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DL4J实现该模型的完整过程可以分为以下几个步骤：
- en: Download and extract the raw movie reviews data.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并提取原始电影评论数据。
- en: Configure the network configuration needed for training, and evaluate the performance.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置训练所需的网络配置，并评估性能。
- en: Load each review and convert the words to vectors using the Word2Vec model.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载每个评论并使用Word2Vec模型将单词转换为向量。
- en: Perform training for multiple predefined epochs. For each epoch, the performance
    is evaluated on the test set.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为多个预定义的训练周期（epoch）执行训练。对于每个周期，都会在测试集上评估性能。
- en: 'To download and extract the movie reviews'' data, we need to set up the download
    configuration first. The following code snippet sets all the things needed to
    do so:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了下载并提取电影评论数据，我们首先需要设置下载配置。以下代码片段设置了完成此操作所需的一切：
- en: '[PRE0]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Location to save and extract the training and testing data in the local file
    path is set as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地文件路径中设置训练和测试数据保存与提取的位置如下所示：
- en: '[PRE1]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Location of the local filesystem for the Google News vectors is given as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google News向量在本地文件系统中的位置如下所示：
- en: '[PRE2]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following code helps to download the data from the web URL to the local
    file path:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码有助于将数据从Web URL下载到本地文件路径：
- en: '[PRE3]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, as we have downloaded the raw movie reviews'' data, we can now move to
    set up our RNN to perform the training of this data. The downloaded data is split
    on a number of examples used in each mini batch to work on each worker of Hadoop
    for distributed training purposes. We need to declare a variable, `batchSize`,
    for this purpose. Here, as a sample, we use each batch of 50 examples, which will
    be split across multiple blocks of Hadoop, where the workers will run in parallel:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，随着我们下载了原始电影评论数据，我们可以开始设置我们的RNN进行数据训练。下载的数据被分割成多个例子，每个迷你批次会在Hadoop的每个工作节点上进行分配，以便进行分布式训练。为此，我们需要声明一个变量`batchSize`。这里，作为示例，我们使用每个批次包含50个例子，这些例子将被分配到多个Hadoop块中，工作节点将并行运行：
- en: '[PRE4]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we set the network configuration for a RNN, we can now move on to the training
    operation as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们为RNN设置网络配置后，接下来我们可以继续进行训练操作，如下所示：
- en: '[PRE5]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The testing of the network is performed by creating an object of the `Evaluation`
    class as follows:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络的测试通过创建`Evaluation`类的对象来执行，如下所示：
- en: '[PRE6]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: RNNs are special compared to other traditional deep neural networks because
    of their capability to work over long sequences of vectors, and to output different
    sequences of vectors. RNNs are unfolded over time to work like a feed-forward
    neural network. The training of RNNs is performed with backpropagation of time,
    which is an extension of the traditional backpropagation algorithm. A special
    unit of RNNs, called Long short-term memory, helps to overcome the limitations
    of the backpropagation of time algorithm.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他传统的深度神经网络相比，RNN具有特殊性，因为它们能够处理长时间序列的向量，并输出不同的向量序列。RNN会随着时间展开，像前馈神经网络一样工作。RNN的训练是通过时间反向传播（Backpropagation
    Through Time，简称BPTT）来进行的，这是一种传统反向传播算法的扩展。RNN的一个特殊单元，称为长短期记忆（Long Short-Term Memory，LSTM），有助于克服时间反向传播算法的局限性。
- en: We also talked about the bidirectional RNN, which is an updated version of the
    unidirectional RNN. Unidirectional RNNs sometimes fail to predict correctly because
    of lack of future input information. Later, we discussed distribution of deep
    RNNs and their implementation with Deeplearning4j. Asynchronous stochastic gradient
    descent can be used for the training of the distributed RNN. In the next chapter,
    we will discuss another model of deep neural network, called the Restricted Boltzmann
    machine.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了双向RNN，它是单向RNN的升级版本。单向RNN有时由于缺乏未来输入信息，无法正确预测。后来，我们讨论了深度RNN的分布式处理及其在Deeplearning4j中的实现。可以使用异步随机梯度下降（Asynchronous
    stochastic gradient descent）来训练分布式RNN。在下一章中，我们将讨论另一种深度神经网络模型，称为限制玻尔兹曼机（Restricted
    Boltzmann machine）。
