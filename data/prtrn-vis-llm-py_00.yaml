- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: So, you want to work with foundation models? That is an excellent place to begin!
    Many of us in the machine learning community have followed these curious creatures
    for years, from their earliest onset in the first days of the Transformer models,
    to their expansion in computer vision, to the near ubiquitous presence of text
    generation and interactive dialogue we see in the world today.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你想使用基础模型工作吗？那是一个非常好的起点！我们中许多机器学习领域的从业者多年来一直关注这些有趣的“生物”，从最早期的 Transformer
    模型诞生，到它们在计算机视觉领域的扩展，再到如今在文本生成和互动对话中几乎无处不在的存在。
- en: But where do foundation models come from? How do they work? What makes them
    tick, and when should you pretrain and fine-tune them? How can you eke out performance
    gains on your datasets and applications? How many accelerators do you need? What
    does an end-to-end application look like, and how can you use foundation models
    to master this new surge of interest in generative AI?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，基础模型是从哪里来的呢？它们是如何工作的？是什么驱动它们的运作，什么时候应该进行预训练和微调？如何在你的数据集和应用程序中挖掘出性能提升？你需要多少加速器？一个端到端的应用是什么样的，如何使用基础模型来掌握这个生成式
    AI 的新兴趣浪潮？
- en: These pages hope to provide answers to these very important questions. As you
    are no doubt aware, the pace of innovation in this space is truly breathtaking,
    with more foundation models coming online every day from both open-source and
    proprietary model vendors. To grapple with this reality, I’ve tried to focus on
    the most important conceptual fundamentals throughout the book. This means your
    careful study here should pay off for at least a few more years ahead.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些页面希望能为这些非常重要的问题提供答案。正如你无疑已经意识到的，这个领域的创新速度真是令人瞩目，每天都有更多的基础模型从开源和专有模型供应商处上线。为了应对这一现实，我尽力在本书中专注于最重要的概念基础。这意味着，你在这里的细心学习至少会对未来几年产生回报。
- en: In terms of practical applications and guidance, I’ve overwhelmingly focused
    on cloud computing options available through AWS and especially Amazon SageMaker.
    I’ve spent more than the last five years very happily at AWS and enjoy sharing
    all of my knowledge and experience with you! Please do note that all thoughts
    and opinions shared in this book are my own, and do not represent those of Amazon’s.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践应用和指导方面，我主要聚焦于通过 AWS 提供的云计算选项，尤其是 Amazon SageMaker。我在 AWS 已经愉快工作了超过五年，很高兴与大家分享我所有的知识和经验！请注意，本书中所有的观点和意见仅代表我个人的看法，并不代表亚马逊的立场。
- en: The following chapters focus on concepts, not code. This is because software
    changes rapidly, while fundamentals change very slowly. You’ll find in the repository
    with the book links to my go-to resources for all of the key topics mentioned
    throughout these fifteen chapters, which you can use right away to get hands-on
    with everything you’re learning here. Starting July 1, 2023, you’ll also find
    in the repository a set of new pretraining and fine-tuning examples from yours
    truly to complete all of the topics.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将重点讨论概念，而不是代码。这是因为软件变化迅速，而基本原理变化缓慢。你会在与本书相关的代码库中找到我在这十五章中提到的所有关键主题的资源链接，你可以立即使用它们来亲自实践你在这里学到的内容。从2023年7月1日起，你还将在代码库中找到一套新的预训练和微调示例，由我亲自提供，以完成所有相关主题。
- en: 'You might find this hard to believe, but in my early twenties I wasn’t actually
    coding: I was exploring the life of a Buddhist monastic. I spent five years living
    at a meditation retreat center in Arizona, the Garchen Institute. During this
    time, I learned how to meditate, focus my mind, watch my emotions and develop
    virtuous habits. After my master’s degree at the University of Chicago years later,
    and now at Amazon, I can see that these traits are extremely useful in today’s
    world as well!'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能很难相信，但在我二十多岁的时候，我其实并没有在编程：我在探索佛教僧侣的生活。我曾在亚利桑那州的一个冥想静修中心——Garchen Institute——生活了五年。在这段时间里，我学会了冥想、集中精力、观察情绪并培养良好的习惯。多年后在芝加哥大学获得硕士学位，以及现在在亚马逊工作，我发现这些品质在今天的世界中也是非常有用的！
- en: I mention this so that you can take heart. Machine learning, artificial intelligence,
    cloud computing, economics, application development, none of these topics are
    straightforward. But if you apply yourself, if you really stretch your mind to
    consider the core foundations of the topics at hand, if you keep yourself coming
    back to the challenge again and again, there’s truly nothing you can’t do. That
    is the beauty of humanity! And if a meditating yogi straight from the deep silence
    of a retreat hut can eventually learn what it takes to pretrain and fine-tune
    foundation models, then so can you!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到这一点是为了让你振作起来。机器学习、人工智能、云计算、经济学、应用开发，这些话题都不简单。但只要你付出努力，真正拓宽思维，去考虑这些话题的核心基础，不断迎接挑战，实际上没有什么是你做不到的。这就是人类的美丽所在！如果一个从静谧的禅修小屋中走出的冥想瑜伽士，最终也能学会预训练和微调基础模型，那么你也一定可以做到！
- en: With that in mind, let’s learn more about the book itself!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些概念，接下来让我们更多地了解这本书本身！
- en: Note
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Most of the concepts mentioned here will be accompanied by scripting examples
    in the repository starting July 1, 2023\. However, to get you started even earlier,
    you can find a list of resources in the repository today with links to useful
    hands-on examples elsewhere for demonstration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提到的大部分概念，将会伴随脚本示例在2023年7月1日开始发布在仓库中。然而，为了帮助你提前入门，你今天就可以在仓库中找到一些资源列表，其中包含指向其他地方的实用示例链接，供展示使用。
- en: Who is this book for?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书适合谁阅读？
- en: If you’re a machine learning researcher or enthusiast who wants to start a foundation
    modelling project, this book is for you. Applied scientists, data scientists,
    machine learning engineers, solution architects, product managers, and students
    will all benefit from this book. Intermediate Python is a must, along with introductory
    concepts of cloud computing. A strong understanding of deep learning fundamentals
    is needed, while advanced topics will be explained. The content covers advanced
    machine learning and cloud techniques, explaining them in an actionable, easy-to-understand
    way.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是希望启动基础模型项目的机器学习研究人员或爱好者，那么这本书非常适合你。应用科学家、数据科学家、机器学习工程师、解决方案架构师、产品经理以及学生都将从本书中受益。要求具备中级Python知识，以及云计算的基础概念。需要对深度学习基础有扎实的理解，同时本书也会解释高级话题。内容涵盖了先进的机器学习和云计算技术，以一种可操作、易于理解的方式进行讲解。
- en: What this book covers
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概述
- en: '[*Chapter 1*](B18942_01.xhtml#_idTextAnchor016)*, An Introduction to Pretraining
    Foundation Models* In this chapter you’ll be introduced to foundation models,
    the backbone of many artificial intelligence and machine learning systems today.
    We will dive into their creation process, also called pretraining, and understand
    where it’s competitive to improve the accuracy of your models. We will discuss
    the core transformer architecture underpinning state of the art models like Stable
    Diffusion, BERT, Vision Transformers, CLIP, Flan-T5 and more. You will learn about
    the encoder and decoder frameworks that work to solve a variety of use cases.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第一章*](B18942_01.xhtml#_idTextAnchor016)*，基础模型预训练概述* 本章将介绍基础模型，这是当今许多人工智能和机器学习系统的支柱。我们将深入了解它们的创建过程，也就是预训练，了解在哪些方面提升模型准确性具有竞争力。我们将讨论支撑当前先进模型（如
    Stable Diffusion、BERT、Vision Transformers、CLIP、Flan-T5 等）的核心变换器架构。你将学习解决各种应用场景的编码器和解码器框架。'
- en: '[*Chapter 2*](B18942_02.xhtml#_idTextAnchor034)*, Dataset Preparation: Part
    One* In this chapter, we begin to discuss what you’ll need in your dataset to
    start a meaningful pretraining project. This is the first of two parts on dataset
    preparation. It opens with some business guidance on finding a good use case for
    foundation modeling, where the data become instrumental. Then, focusing on the
    content of your dataset, we use qualitative and quantitative measures to compare
    it with datasets used in pretraining other top models. You’ll learn how to use
    the scaling laws to determine if your datasets are “large enough” and “good enough”
    to boost accuracy while pretraining. We discuss bias identification and mitigation,
    along with multilingual and multimodal solutions.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第2章*](B18942_02.xhtml#_idTextAnchor034)*, 数据集准备：第一部分* 在本章中，我们开始讨论在数据集中需要什么内容来启动一个有意义的预训练项目。这是关于数据集准备的两部分中的第一部分。开篇介绍了关于如何为基础模型找到合适用例的一些业务指导，其中数据变得至关重要。然后，聚焦于数据集的内容，我们使用定性和定量指标将其与用于预训练其他顶级模型的数据集进行比较。你将学习如何使用规模定律来判断你的数据集是否“足够大”和“足够好”，以提高预训练过程中的准确性。我们还讨论了偏差识别与缓解，以及多语言和多模态的解决方案。'
- en: '[*Chapter 3*](B18942_03.xhtml#_idTextAnchor050)*, Model Preparation* In this
    chapter you’ll learn how to pick which model will be most useful to serve as a
    basis for your pretraining regime. You’ll learn how to think about the size of
    the model in parameters, along with the key loss functions and how they determine
    performance in production. You’ll combine the scaling laws with your expected
    dataset size to select ceiling and basement model sizes, which you’ll use to guide
    your experiments.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第3章*](B18942_03.xhtml#_idTextAnchor050)*, 模型准备* 在本章中，你将学习如何选择哪个模型最适合作为你预训练方案的基础。你将学习如何从参数的角度思考模型的大小，以及关键的损失函数如何决定模型在生产中的表现。你将结合规模定律和预期的数据集大小，选择模型的上限和下限，这些将帮助你指导实验的设计。'
- en: '[*Chapter 4*](B18942_04.xhtml#_idTextAnchor066)*, Containers and Accelerators
    on the Cloud* In this chapter, you’ll learn how to containerize your scripts and
    optimize them for accelerators on the cloud. We’ll learn about a range of accelerators
    for foundation models, including tradeoffs around cost and performance across
    the entire machine learning lifecycle. You’ll learn key aspects of Amazon SageMaker
    and AWS to train models on accelerators, optimize performance, and troubleshoot
    common issues. If you’re already familiar with using accelerators on AWS, feel
    free to skip this chapter.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B18942_04.xhtml#_idTextAnchor066)*, 云上的容器和加速器* 在本章中，你将学习如何将脚本容器化并优化它们以适应云上的加速器。我们将了解适用于基础模型的一系列加速器，包括在整个机器学习生命周期中的成本与性能的权衡。你将学习Amazon
    SageMaker和AWS的关键方面，以便在加速器上训练模型、优化性能并排除常见问题。如果你已经熟悉在AWS上使用加速器，可以跳过本章。'
- en: '[*Chapter 5*](B18942_05.xhtml#_idTextAnchor085)*, Distribution Fundamentals*
    In this chapter, you’ll learn conceptual fundamentals for the distribution techniques
    you need to employ for large scale pretraining and fine-tuning. First, you’ll
    master top distribution concepts for machine learning, notably model and data
    parallel. Then, you’ll learn how Amazon SageMaker integrates with distribution
    software to run your job on as many GPUs as you need. You’ll learn how to optimize
    model and data parallel for large-scale training especially with techniques like
    sharded data parallelism. Then, you’ll learn how to reduce your memory consumption
    with advanced techniques like optimizer state sharding, activation checkpointing,
    compilation, and more. Lastly, we’ll look at a few examples across language, vision,
    and more to bring all of these concepts together.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第5章*](B18942_05.xhtml#_idTextAnchor085)*, 分布式基础* 在本章中，你将学习用于大规模预训练和微调的分布式技术的概念基础。首先，你将掌握机器学习的主要分布式概念，特别是模型并行和数据并行。接下来，你将了解Amazon
    SageMaker如何与分布式软件集成，将任务运行在你需要的任意数量的GPU上。你将学习如何优化模型并行和数据并行，以便进行大规模训练，尤其是使用像分片数据并行这样的技术。然后，你将学习如何通过高级技术来减少内存消耗，例如优化器状态分片、激活检查点、编译等。最后，我们将通过一些语言、视觉等方面的示例，将所有这些概念结合起来。'
- en: '[*Chapter 6*](B18942_06.xhtml#_idTextAnchor106)*, Dataset Preparation: Part
    Two, the Data Loader* In this chapter, you’ll learn how to prepare your dataset
    to immediately use with your chosen models. You’ll master the concept of a data
    loader, knowing why it’s a common source of error in training large models. You’ll
    learn about creating embeddings, using tokenizers and other methods to featurize
    your raw data for your preferred neural network. Following these steps, you’ll
    be able to prepare your entire dataset, using methods for both vision and language.
    Finally, you’ll learn about data optimizations on AWS and Amazon SageMaker to
    efficiently send datasets large and small to your training cluster. Throughout
    this chapter we’ll work backwards from the training loop; giving you incrementally
    all the steps you need to have functional deep neural networks training at scale.
    You’ll also follow a case study from how I trained on 10TB for Stable Diffusion
    on SageMaker!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第6章*](B18942_06.xhtml#_idTextAnchor106)*，数据集准备：第二部分，数据加载器* 本章将教您如何准备数据集，以便立即与您选择的模型一起使用。您将掌握数据加载器的概念，了解为什么它是大规模训练中常见的错误源。您将学习如何创建嵌入、使用分词器以及其他方法，将原始数据转换为适用于您的神经网络的特征。按照这些步骤，您将能够准备好整个数据集，适用于视觉和语言模型。最后，您将学习在AWS和Amazon
    SageMaker上的数据优化技术，以高效地将大小不一的数据集发送到训练集群。在本章中，我们将从训练循环开始，逐步完成您所需的所有步骤，使您能够在大规模训练中成功实现深度神经网络。您还将跟随一个案例研究，了解我是如何在SageMaker上训练10TB的Stable
    Diffusion模型的！'
- en: '[*Chapter 7*](B18942_07.xhtml#_idTextAnchor116)*, Finding the Right Hyperparameters*
    In this chapter, you’ll dive into the key hyperparameters that govern performance
    for top vision and language models, such as batch size, learning rate, and more.
    First, we’ll start with a quick overview of hyperparameter tuning for those who
    are new or new a light refresh, including key examples in vision and language.
    Then we’ll explore hyperparameter tuning in foundation models, both what is possible
    today and where trends might emerge. Finally, we’ll learn how to do this on Amazon
    SageMaker, taking incremental steps in a cluster size and changing each hyperparameter
    as we do.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第7章*](B18942_07.xhtml#_idTextAnchor116)*，找到合适的超参数* 本章将深入探讨影响顶级视觉和语言模型性能的关键超参数，如批量大小、学习率等。首先，我们将为那些新手或需要轻度复习的读者快速回顾超参数调优，包括视觉和语言领域的关键示例。接着，我们将探讨基础模型中的超参数调优，了解今天可以实现的技术和未来可能出现的趋势。最后，我们将学习如何在Amazon
    SageMaker上进行这些操作，通过逐步增加集群大小并调整每个超参数。'
- en: '[*Chapter 8*](B18942_08.xhtml#_idTextAnchor127)*, Large-Scale Training on SageMaker*
    In this chapter, we cover key features and functionality available with Amazon
    SageMaker for running highly optimized distributed training. You’ll learn how
    to optimize your script for SageMaker training, along with key usability features.
    You’ll also learn about backend optimizations for distributed training with SageMaker,
    like GPU health checks, resilient training, checkpointing, script mode, and more.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第8章*](B18942_08.xhtml#_idTextAnchor127)*，SageMaker上的大规模训练* 本章将介绍使用Amazon
    SageMaker进行高度优化的分布式训练的关键功能和特性。您将学习如何优化脚本以适应SageMaker训练，同时掌握关键的可用性特性。您还将了解SageMaker分布式训练的后端优化，如GPU健康检查、抗干扰训练、检查点保存、脚本模式等。'
- en: '[*Chapter 9*](B18942_09.xhtml#_idTextAnchor138)*, Advanced Training Concepts*
    In this chapter, we will cover advanced training concepts at scale, like evaluating
    throughput, calculating model TFLOPS per device, compilation, and using the scaling
    laws to determine the right length of training time. In the last chapter you learned
    about how to do large-scale training on SageMaker generally speaking. In this
    chapter you’ll learn about particularly complex and sophisticated techniques you
    can use to drive down the overall cost of your job. This lower cost directly translates
    to higher model performance, because it means you can train for longer on the
    same budget.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第9章*](B18942_09.xhtml#_idTextAnchor138)*，高级训练概念* 本章将讨论大规模训练中的高级概念，如评估吞吐量、计算每个设备的模型TFLOPS、编译以及利用扩展法则确定合适的训练时间长度。在上一章中，您学习了如何在SageMaker上进行大规模训练的基本方法。本章您将学习一些特别复杂且高效的技术，帮助您降低工作的总体成本。这种降低的成本直接转化为更高的模型性能，因为它意味着您可以在相同的预算下训练更长时间。'
- en: '[*Chapter 10*](B18942_10.xhtml#_idTextAnchor152)*, Fine-Tuning and Evaluating*
    In this chapter, you’ll learn how to fine-tune your model on use case-specific
    datasets, comparing its performance to that of off-the-shelf public models. You
    should be able to see quantitative and qualitative boost from your pretraining
    regime. You’ll dive into some examples from language, text, and everything in-between.
    You’ll also learn how to think about and design a human-in-the-loop evaluation
    system, including the same RLHF that makes ChatGPT tick! This chapter focuses
    on *updating the trainable weights of the model*. For techniques that mimic learning
    but don’t update the weights, such as prompt tuning and standard retrieval augmented
    generation, see [*Chapter 13*](B18942_13.xhtml#_idTextAnchor198) on prompt engineering
    or [*Chapter 15*](B18942_15.xhtml#_idTextAnchor229) on future trends.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第10章*](B18942_10.xhtml#_idTextAnchor152)*，微调与评估* 本章将教你如何在特定用例数据集上微调模型，并将其性能与现成的公共模型进行比较。你应能看到来自预训练过程的定量和定性提升。我们将深入探讨语言、文本及其中的各种示例。你还将学会如何设计一个人类在环的评估系统，包括使得ChatGPT运作的相同RLHF！本章重点介绍*更新模型的可训练权重*。对于模拟学习但不更新权重的技术，如提示调优和标准的检索增强生成，请参见[*第13章*](B18942_13.xhtml#_idTextAnchor198)的提示工程或[*第15章*](B18942_15.xhtml#_idTextAnchor229)的未来趋势。'
- en: '[*Chapter 11*](B18942_11.xhtml#_idTextAnchor167)*, Detecting, Mitigating, and
    Monitoring Bias* In this chapter, we’ll analyze leading bias identification and
    mitigation strategies for large vision, language, and multimodal models. You’ll
    learn about the concept of bias, both in a statistical sense and how it impacts
    human beings in critical ways. You’ll understand key ways to quantify and remedy
    this in vision and language models, eventually landing on monitoring strategies
    that enable you to reduce any and all forms of harm when applying your foundation
    models.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第11章*](B18942_11.xhtml#_idTextAnchor167)*，偏见的检测、缓解和监控* 本章将分析针对大型视觉、语言和多模态模型的偏见识别和缓解策略。你将了解偏见的概念，包括其统计意义以及它如何以关键的方式影响人类。你将掌握量化和解决视觉及语言模型中偏见的关键方法，最终学习能够减少所有形式危害的监控策略，以便在应用基础模型时避免伤害。'
- en: '[*Chapter 12*](B18942_12.xhtml#_idTextAnchor178)*, How to Deploy Your Model*
    In this chapter, we’ll introduce you to a variety of techniques for deploying
    your model, including real-time endpoints, serverless, batch options and more.
    These concepts apply to many compute environments, but we’ll focus on capabilities
    available on AWS within Amazon SageMaker. We’ll talk about why you should try
    to shrink the size of your model before deploying, along with techniques for this
    across vision and language. We’ll also cover distributed hosting techniques, for
    scenarios when you can’t or don’t need to shrink your model. Lastly, we’ll explore
    model serving techniques and concepts that can help you optimize the end-to-end
    performance of your model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第12章*](B18942_12.xhtml#_idTextAnchor178)*，如何部署你的模型* 本章将介绍多种部署模型的技术，包括实时端点、无服务器计算、批处理选项等。这些概念适用于许多计算环境，但我们将重点讨论在Amazon
    SageMaker上的AWS可用功能。我们将讨论为什么在部署前应尝试缩小模型的大小，以及如何在视觉和语言领域应用相关技术。我们还将讨论分布式托管技术，适用于无法或不需要缩小模型的场景。最后，我们将探索模型服务技术和概念，帮助你优化模型的端到端性能。'
- en: '[*Chapter 13*](B18942_13.xhtml#_idTextAnchor198)*, Prompt Engineering* In this
    chapter, we’ll dive into a special set of techniques called prompt engineering.
    You’ll learn about this technique at a high level, including how it is similar
    to and different from other learning-based topics throughout this book. We’ll
    explore examples across vision and language, and dive into key terms and success
    metrics. In particular this chapter covers all of the tips and tricks for improving
    performance without updating the model weights. This means we’ll be mimicking
    the learning process, without necessarily changing any of the model parameters.
    This includes some advanced techniques like prompt and prefix tuning.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第13章*](B18942_13.xhtml#_idTextAnchor198)*，提示工程* 本章将深入探讨一组特殊的技术——提示工程。你将了解这一技术的高级概念，包括它与本书其他学习方法的相似性与差异。我们将探讨视觉和语言领域的示例，并深入了解关键术语和成功标准。特别是，本章涵盖了无需更新模型权重即可提高性能的所有技巧和窍门。这意味着我们将在不改变任何模型参数的情况下模拟学习过程。这包括一些高级技术，如提示和前缀调优。'
- en: '[*Chapter 14*](B18942_14.xhtml#_idTextAnchor217)*, MLOps for Vision and Language*
    In this chapter, we’ll introduce core concepts of operations and orchestration
    for machine learning, also known as MLOps. This includes building pipelines, continuous
    integration and deployment, promotion through environments, and more. We’ll explore
    options for monitoring and human-in-the-loop auditing of model predictions. We’ll
    also identify unique ways to support large vision and language models in your
    MLOps pipelines.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第14章*](B18942_14.xhtml#_idTextAnchor217)*, 视觉与语言的 MLOps* 在本章中，我们将介绍机器学习的操作和编排的核心概念，也称为
    MLOps。包括构建管道、持续集成与部署、在不同环境中进行推广等内容。我们将探讨监控选项以及对模型预测进行人工审核的方式。我们还将识别在 MLOps 管道中支持大型视觉与语言模型的独特方法。'
- en: '[*Chapter 15*](B18942_15.xhtml#_idTextAnchor229)*, Future Trends in Pretraining
    Foundation Models* In this chapter, we’ll close out the book by pointing to where
    trends are headed for all relevant topics presented in this book. We’ll explore
    trends in foundation model application development, like using LangChain to build
    interactive dialogue applications, along with techniques like retrieval augmented
    generation to reduce LLM hallucination. We’ll explore ways to use generative models
    to solve classification tasks, human-centered design, and other generative modalities
    like code, music, product documentation, PowerPoints, and more! We’ll talk through
    AWS offerings like SageMaker JumpStart Foundation Models, Amazon Bedrock, Amazon
    Titan, and Amazon Code Whisperer, and top trends in the future of foundation models
    and pretraining itself.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第15章*](B18942_15.xhtml#_idTextAnchor229)*, 预训练基础模型的未来趋势* 在本章中，我们将通过展望本书中所有相关主题的未来趋势来结束本书。我们将探讨基础模型应用开发的趋势，如使用
    LangChain 构建交互式对话应用程序，以及使用检索增强生成（RAG）等技术减少大语言模型的幻觉问题。我们还将探讨如何利用生成模型解决分类任务、人本设计，以及其他生成模式，如代码、音乐、产品文档、PPT
    等！我们将讲解 AWS 提供的服务，如 SageMaker JumpStart 基础模型、Amazon Bedrock、Amazon Titan 和 Amazon
    Code Whisperer，并讨论基础模型和预训练本身的未来趋势。'
- en: To get the most out of this book
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要从本书中获得最大收益
- en: As mentioned earlier, you want to be very happy in Python development to absolutely
    maximize your time in this book. The pages don’t spend a lot of time focusing
    on the software, but again, everything in the GitHub repository is Python. If
    you’re already using a few key AWS services, like Amazon SageMaker, S3 buckets,
    ECR images, and FSx for Lustre, that will speed you up tremendously in applying
    what you’ve learned here. If you’re new to these, that’s ok, we’ll include introductions
    to each of these.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了充分利用本书的内容，你需要在 Python 开发中非常得心应手。书中的页面并不会花太多时间聚焦在软件上，但 GitHub 仓库中的所有内容都是
    Python。如果你已经在使用一些关键的 AWS 服务，如 Amazon SageMaker、S3 存储桶、ECR 镜像和 FSx for Lustre，那么你在应用本书中学到的知识时将会大大提速。如果你对这些服务不熟悉也没关系，我们将提供这些服务的介绍。
- en: '| **AWS Service or Open-source** **software framework** | **What we’re using**
    **it for** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **AWS 服务或开源** **软件框架** | **我们使用它的目的** |'
- en: '| Amazon SageMaker | Studio, notebook instances, training jobs, endpoints,
    pipelines |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Amazon SageMaker | Studio、笔记本实例、训练任务、端点、管道 |'
- en: '| S3 buckets | Storing objects and retrieving metadata |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| S3 buckets | 存储对象并检索元数据 |'
- en: '| Elastic Container Registry | Storing Docker images |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Elastic Container Registry | 存储 Docker 镜像 |'
- en: '| FSx for Lustre | Storing large-scale data for model training loops |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| FSx for Lustre | 用于模型训练循环的大规模数据存储 |'
- en: '| Python | General scripting: including managing and interacting with services,
    importing other packages, cleaning your data, defining your model training and
    evaluation loops, etc |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Python | 通用脚本：包括管理和与服务交互、导入其他包、清洗数据、定义模型训练与评估循环等 |'
- en: '| PyTorch and TensorFlow | Deep learning frameworks to define your neural networks
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch 和 TensorFlow | 定义神经网络的深度学习框架 |'
- en: '| Hugging Face | Hub with more than 100,000 open-source pretrained models and
    countless extremely useful and reliable methods for NLP and increasingly CV |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Hugging Face | 拥有超过100,000个开源预训练模型的 hub，以及无数在自然语言处理（NLP）和计算机视觉（CV）中非常有用且可靠的方法
    |'
- en: '| Pandas | Go-to library for data analysis |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Pandas | 数据分析的首选库 |'
- en: '| Docker | Open-source framework for building and managing containers |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Docker | 构建和管理容器的开源框架 |'
- en: '**If you are using the digital version of this book, we advise you to access
    the code from the book’s GitHub repository (a link is available in the next section),
    step through the examples, and type the code yourself. Doing so will help you
    avoid any potential errors related to the copying and pasting** **of code.**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您使用的是本书的数字版本，我们建议您通过本书GitHub仓库访问代码（下一个章节中会提供链接），逐步完成示例，并自己键入代码。这样做有助于您避免因复制和粘贴代码而可能出现的错误。**'
- en: Download the example code files
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python](https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python).
    If there’s an update to the code, it will be updated in the GitHub repository.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从GitHub上下载本书的示例代码文件，网址为[https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python](https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python)。如果代码有更新，GitHub仓库中会同步更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了其他代码包，来自我们丰富的书籍和视频目录，您可以在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)查看。
- en: Conventions used
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本约定。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “Mount the downloaded `WebStorm-10*.dmg` disk image
    file as another disk in your system.”'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`文中的代码`：表示文本中的代码单词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter用户名。这里有一个例子：“将下载的`WebStorm-10*.dmg`磁盘镜像文件挂载为系统中的另一个磁盘。”'
- en: 'A block of code is set as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块的设置如下：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    “Select **System info** from the **Administration** panel.”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要单词或您在屏幕上看到的单词。例如，菜单或对话框中的单词会以**粗体**显示。这里有一个例子：“从**管理**面板中选择**系统信息**。”'
- en: Tips or important notes
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或重要说明
- en: Appear like this.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显示如下。
- en: Get in touch
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '**General feedback**: If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](http://customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：如果您对本书的任何内容有疑问，请通过[customercare@packtpub.com](http://customercare@packtpub.com)给我们发邮件，并在邮件主题中注明书名。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：虽然我们已经尽力确保内容的准确性，但难免会出现错误。如果您发现本书中的错误，我们将非常感激您向我们报告。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)，并填写表单。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packt.com](http://copyright@packt.com)
    with a link to the material.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果您在互联网上遇到我们作品的任何非法复制形式，我们将非常感激您能提供相应的地址或网站名称。请通过[copyright@packt.com](http://copyright@packt.com)与我们联系，并附上相关内容的链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个主题方面具有专业知识，并且有兴趣撰写或参与书籍的编写，请访问[authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share Your Thoughts
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you’ve read Pretrain Vision and Large Language Models in Python, we’d love
    to hear your thoughts! [Please click here to go straight to the Amazon review
    page for this book](https://packt.link/r/1-804-61825-X) and share your feedback.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您阅读了《Pretrain Vision and Large Language Models in Python》，我们非常希望听到您的反馈！[请点击这里直接访问该书的亚马逊评论页面](https://packt.link/r/1-804-61825-X)，并分享您的意见。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您的反馈对我们和技术社区都非常重要，将帮助我们确保提供优质的内容。
- en: Download a free PDF copy of this book
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载本书的免费PDF副本
- en: Thanks for purchasing this book!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你购买这本书！
- en: Do you like to read on the go but are unable to carry your print books everywhere?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢随时随地阅读，但又无法携带纸质书籍吗？
- en: Is your eBook purchase not compatible with the device of your choice?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你的电子书购买是否无法兼容你选择的设备？
- en: Don’t worry, now with every Packt book you get a DRM-free PDF version of that
    book at no cost.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，现在购买每一本Packt书籍，你都能免费获得该书的无DRM PDF版本。
- en: Read anywhere, any place, on any device. Search, copy, and paste code from your
    favorite technical books directly into your application.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随时随地，在任何设备上阅读。直接从你最喜欢的技术书籍中搜索、复制并粘贴代码到你的应用程序中。
- en: The perks don’t stop there, you can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 优惠不止于此，你还可以独享折扣、新闻通讯和每日免费内容，直接发送到你的邮箱。
- en: 'Follow these simple steps to get the benefits:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些简单步骤即可享受优惠：
- en: Scan the QR code or visit the link below
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描二维码或访问下面的链接
- en: '![](img/B18942_QR_Free_PDF.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18942_QR_Free_PDF.jpg)'
- en: '[https://packt.link/free-ebook/9781804618257](https://packt.link/free-ebook/9781804618257)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/free-ebook/9781804618257](https://packt.link/free-ebook/9781804618257)'
- en: Submit your proof of purchase
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交你的购买证明
- en: That’s it! We’ll send your free PDF and other benefits to your email directly
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就是这样！我们将直接把你的免费PDF和其他福利发送到你的邮箱。
- en: 'Part 1: Before Pretraining'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分：预训练之前
- en: In part 1, you’ll learn how to get ready to pretrain a large vision and/or language
    model, including dataset and model preparation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1部分，你将学习如何准备预训练一个大型视觉和/或语言模型，包括数据集和模型准备。
- en: 'This section has the following chapters:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 1*](B18942_01.xhtml#_idTextAnchor016), *An Introduction to Pretraining
    Foundation Models*'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第1章*](B18942_01.xhtml#_idTextAnchor016)，*预训练基础模型介绍*'
- en: '[*Chapter 2*](B18942_02.xhtml#_idTextAnchor034), *Dataset Preparation: Part
    One*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第2章*](B18942_02.xhtml#_idTextAnchor034)，*数据集准备：第一部分*'
- en: '[*Chapter 3*](B18942_03.xhtml#_idTextAnchor050), *Model Preparation*'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B18942_03.xhtml#_idTextAnchor050)，*模型准备*'
