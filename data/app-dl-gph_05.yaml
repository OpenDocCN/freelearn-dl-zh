- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Graph Deep Learning Challenges
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图深度学习挑战
- en: As deep learning on graphs has gained significant attention in recent years,
    researchers and practitioners have encountered numerous challenges that complicate
    the application of traditional deep learning techniques to graph data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着近年来图上的深度学习获得了广泛关注，研究人员和从业者遇到了许多挑战，这些挑战使得将传统深度学习技术应用于图数据变得复杂。
- en: This chapter aims to give you a comprehensive overview of key challenges faced
    in graph learning, spanning from fundamental data issues to advanced model architectures
    and domain-specific problems. We will explore how the unique properties of graphs—such
    as their irregular structure, variable size, and complex dependencies—pose significant
    hurdles for conventional machine learning approaches.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为您提供图学习中面临的关键挑战的全面概述，涵盖从基础数据问题到高级模型架构及领域特定问题的各个方面。我们将探讨图的独特特性——如其不规则结构、可变大小和复杂的依赖关系——如何为传统机器学习方法带来重大障碍。
- en: By addressing these challenges, we aim to provide you with a solid foundation
    for understanding the current limitations and future directions of deep learning
    with respect to graphs. This chapter will serve as a roadmap, highlighting areas
    that require further investigation and innovation to advance the field of graph
    learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些挑战，我们旨在为您提供一个扎实的基础，帮助您理解当前深度学习在图上的局限性和未来的发展方向。本章将作为一个路线图，突出那些需要进一步研究和创新的领域，以推动图学习领域的发展。
- en: As we delve into each of these challenges, we will discuss current approaches,
    limitations, and potential avenues for future research. Understanding these challenges
    is crucial for developing more robust, efficient, and effective graph learning
    algorithms and applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入探讨这些挑战时，我们将讨论当前的解决方法、局限性以及未来研究的潜在方向。理解这些挑战对于开发更加稳健、高效和有效的图学习算法及应用至关重要。
- en: 'The challenges discussed in this chapter can be broadly categorized into several
    key areas:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的挑战大体上可以分为几个关键领域：
- en: Data-related challenges
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据相关的挑战
- en: Model architecture challenges
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型架构挑战
- en: Computational challenges
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算挑战
- en: Task-specific challenges
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定任务的挑战
- en: Interpretability and explainability
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性和可解释性
- en: Data-related challenges
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据相关的挑战
- en: Graph data presents unique challenges due to its inherent complexity and diverse
    nature. In this section, we explore three key data-related challenges that significantly
    impact the development and application of graph learning algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据由于其固有的复杂性和多样性，带来了独特的挑战。在这一部分，我们将探讨三个与数据相关的关键挑战，它们对图学习算法的发展和应用产生了重大影响。
- en: Heterogeneity in graph structures
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图结构的异质性
- en: 'Graphs in different domains can have vastly different structural properties:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不同领域的图具有截然不同的结构特性：
- en: '**Node and edge types** : Many real-world graphs are heterogeneous, containing
    multiple types of nodes and edges. For instance, in an academic network, *nodes*
    could represent authors, papers, and conferences, while *edges* could represent
    authorship, citations, or attendance.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点和边的类型**：许多现实世界中的图是异质的，包含多种类型的节点和边。例如，在学术网络中，*节点*可能代表作者、论文和会议，而*边*则可能代表作者关系、引用或参与情况。'
- en: '**Attribute diversity** : Nodes and edges may have associated attributes of
    various types (numerical, categorical, textual), adding another layer of complexity
    to the learning process.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性多样性**：节点和边可能具有各种类型的关联属性（数值型、类别型、文本型），为学习过程增添了另一层复杂性。'
- en: '**Structural variations** : Graphs can exhibit different global structures
    (for example, scale-free, small-world, random) and local patterns (for example,
    communities, motifs), requiring models that can adapt to these variations.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构变化**：图可以表现出不同的全局结构（例如，尺度无关、小世界、随机图）和局部模式（例如，社区、模体），需要能够适应这些变化的模型。'
- en: Dynamic and evolving graphs
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态和演变中的图
- en: 'Many real-world graphs are not static but change over time:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的图不是静态的，而是随时间变化的：
- en: '**Temporal evolution** : Nodes and edges may appear or disappear over time,
    changing the graph structure dynamically. In a social media platform, the network
    of user connections evolves constantly as new friendships form and others dissolve.
    For instance, a user might connect with new colleagues after starting a job while
    losing touch with old classmates, causing nodes and edges to appear and disappear
    over time.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间演化**：节点和边可能会随着时间的推移出现或消失，动态地改变图结构。在社交媒体平台上，用户连接的网络不断演化，因为新的友谊形成，旧的友谊消失。例如，用户可能在开始新工作后与新同事建立联系，同时与老同学失去联系，导致节点和边随着时间的推移出现和消失。'
- en: '**Attribute changes** : Node and edge attributes may also change over time,
    reflecting evolving properties or states. On a professional networking site such
    as LinkedIn, user profiles and connections undergo frequent updates. A user might
    change their job title, add new skills, or relocate, altering node attributes.
    Similarly, the strength of connections between professionals might increase as
    they collaborate on more projects, modifying edge attributes dynamically.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性变化**：节点和边的属性也可能随着时间变化，反映出不断变化的属性或状态。在像 LinkedIn 这样的职业社交网站上，用户的个人资料和连接会频繁更新。用户可能会更改职位名称、增加新技能或搬迁，改变节点属性。同样，随着专业人士在更多项目上合作，连接的强度可能增加，从而动态地修改边的属性。'
- en: '**Concept drift** : Underlying patterns or rules governing the graph structure
    may change, requiring models that can adapt to these shifts. In an e-commerce
    recommendation system, the underlying patterns of user preferences can shift over
    time. Initially, the system might suggest products based on similar categories,
    but as consumer behavior evolves toward prioritizing sustainability or ethical
    sourcing, the recommendation algorithm needs to adapt its rules to reflect these
    changing preferences.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念漂移**：支配图结构的基础模式或规则可能会发生变化，要求能够适应这些变化的模型。在电子商务推荐系统中，用户偏好的基础模式可能随时间变化。最初，系统可能会根据相似类别推荐产品，但随着消费者行为向优先考虑可持续性或伦理采购转变，推荐算法需要调整规则以反映这些变化的偏好。'
- en: '**Streaming data** : In some applications, graph data arrives as a continuous
    stream, necessitating online learning algorithms that can process and update models
    incrementally. A real-time fraud detection system for a bank processes transaction
    data as a continuous stream. Each new transaction creates a node in the graph,
    instantly connecting to account holders and merchants. The system must analyze
    this incoming data on the fly, updating the graph structure and running fraud
    detection algorithms without interruption, all while adapting to emerging patterns
    of fraudulent behavior.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式数据**：在某些应用中，图数据以连续流的形式到达，要求使用在线学习算法，能够逐步处理和更新模型。银行的实时欺诈检测系统处理交易数据作为连续流。每笔新交易都会在图中创建一个节点，并立即与账户持有者和商户连接。系统必须即时分析这些传入数据，更新图结构并运行欺诈检测算法，确保不间断地适应新出现的欺诈行为模式。'
- en: Noisy and incomplete graph data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声和不完整的图数据
- en: 'Real-world graph data often suffers from quality issues:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的图数据通常存在质量问题：
- en: '**Missing data** : Graphs may have missing nodes, edges, or attributes due
    to data collection limitations or privacy concerns.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失数据**：由于数据收集的限制或隐私问题，图可能缺少节点、边或属性。'
- en: '**Noisy connections** : Some edges in the graph may be erroneous or irrelevant,
    potentially misleading learning algorithms.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声连接**：图中的一些边可能是错误的或不相关的，可能会误导学习算法。'
- en: '**Uncertain attributes** : Node and edge attributes may be uncertain, imprecise,
    or subject to measurement errors.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不确定的属性**：节点和边的属性可能是不确定的、不精确的，或受测量误差的影响。'
- en: '**Sampling bias** : The observed graph may be a biased sample of a larger population,
    leading to potential inaccuracies in learned models.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样偏差**：观察到的图可能是较大总体的偏倚样本，从而导致学习模型中可能出现不准确的情况。'
- en: Addressing these data-related challenges is crucial for developing robust and
    effective graph learning algorithms. Practitioners must consider these issues
    when designing models, choosing evaluation metrics, and interpreting results.
    Future advancements in graph learning will likely focus on developing techniques
    that can handle larger, more complex, and dynamic graphs while being resilient
    to noise and incompleteness in the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些与数据相关的挑战对于开发稳健且有效的图学习算法至关重要。实践者在设计模型、选择评估指标和解释结果时必须考虑这些问题。未来的图学习进展可能会集中在开发能够处理更大、更复杂和动态图形的技术，同时能够抵御噪声和数据不完整的影响。
- en: Let’s look into major architecture challenges faced by modern **graph neural**
    **networks** ( **GNNs** ).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看现代**图神经** **网络**（**GNNs**）面临的主要架构挑战。
- en: Model architecture challenges
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构挑战
- en: GNNs have shown remarkable success in various graph learning tasks. However,
    they face several architectural challenges that limit their effectiveness in certain
    scenarios. Here, we investigate four key model architecture challenges in graph
    learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GNN在各种图学习任务中取得了显著成功。然而，在某些场景中，它们面临着一些限制其效果的架构挑战。这里，我们研究了图学习中四个关键的模型架构挑战。
- en: Capturing long-range dependencies
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 捕捉长程依赖
- en: GNNs often struggle to capture dependencies between distant nodes in the graph,
    as information typically propagates only to immediate neighbors in each layer.
    For instance, in scenarios such as citation networks, a paper might be influenced
    by another paper several citation links away. Standard GNNs might fail to capture
    this influence if it extends beyond their receptive field.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GNN通常难以捕捉图中远距离节点之间的依赖关系，因为信息通常只传播到每一层的直接邻居。例如，在引用网络中，一篇论文可能受到另一篇论文的影响，而两篇论文之间可能有多个引用链接。标准的GNN可能无法捕捉到这种影响，如果这种影响超出了它们的感受野。
- en: '**Graph** **attention mechanisms** and **higher-order graph convolutions**
    represent two sophisticated approaches to enhancing GNNs’ long-range capabilities.
    Graph attention mechanisms introduce a dynamic weighting system that allows the
    model to intelligently focus on the most significant connections within the graph,
    particularly those spanning longer distances. By assigning learnable weights to
    neighboring nodes, these mechanisms enable the model to automatically identify
    and prioritize influential nodes, even when they are distant in the graph structure.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图** **注意力机制**和**高阶图卷积**代表了两种增强图神经网络（GNN）长程能力的复杂方法。图注意力机制引入了一种动态加权系统，使得模型能够智能地关注图中的最重要连接，特别是那些跨越较远距离的连接。通过为邻居节点分配可学习的权重，这些机制使得模型能够自动识别并优先考虑有影响力的节点，即使它们在图结构中相距较远。'
- en: This is complemented by higher-order graph convolutions, which take the traditional
    concept of graph convolution a step further. Instead of being limited to immediate
    neighbors, these advanced convolutions can process and aggregate information from
    nodes that are multiple hops away in a single operation. This means the model
    can directly capture complex relationships and patterns that exist across extended
    neighborhoods, leading to a more comprehensive understanding of the graph’s structure
    and underlying relationships. Together, these approaches significantly improve
    the model’s ability to process and understand complex graph-structured data by
    effectively managing both local and global information flow.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这一机制得到了高阶图卷积的补充，它将传统的图卷积概念进一步推进。与仅限于直接邻居节点的传统卷积不同，这些先进的卷积能够在一次操作中处理并汇总来自多个跳数远节点的信息。这意味着模型能够直接捕捉跨越广泛邻域的复杂关系和模式，从而更全面地理解图的结构和潜在关系。结合这两种方法，模型能够有效地管理局部和全局信息流，从而显著提高处理和理解复杂图结构数据的能力。
- en: Depth limitation in GNNs
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GNN中的深度限制
- en: Unlike traditional **deep neural networks** ( **DNNs** ), GNNs often do not
    benefit from increased depth and may even suffer performance degradation with
    too many layers. This issue is noticeable in tasks such as molecule property prediction,
    where a deep GNN might not perform better than a shallow one, limiting the model’s
    ability to learn complex hierarchical features of the molecular structure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的**深度神经网络**（**DNNs**）不同，GNN在增加深度时往往不会从中受益，甚至在层数过多时可能会导致性能下降。这个问题在诸如分子属性预测等任务中尤为明显，深度GNN可能不会比浅层的GNN表现更好，从而限制了模型学习分子结构复杂层次特征的能力。
- en: 'To overcome this limitation, several architectural modifications have been
    developed:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一限制，已经开发了几种架构修改方法：
- en: '**Residual connections** , inspired by **Residual Network** ( **ResNet** )
    architectures in **computer vision** ( **CV** ), allow information to skip intermediate
    layers, facilitating gradient flow in deeper networks. These connections can be
    implemented by adding the input of each layer to its output, enabling the network
    to learn residual functions.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差连接**，受到**残差网络**（**ResNet**）架构在**计算机视觉**（**CV**）中的启发，允许信息跳过中间层，从而促进深层网络中的梯度流动。这些连接可以通过将每层的输入添加到其输出中来实现，使网络能够学习残差函数。'
- en: '**Jump connections** extend this concept by allowing information to jump across
    multiple layers, providing shorter paths for gradient propagation. This can be
    achieved through techniques such as **Jumping Knowledge** **Networks** ( **JKNets**
    ).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳跃连接**通过允许信息跨越多层跳跃，提供更短的梯度传播路径，从而扩展了这一概念。可以通过如**Jumping Knowledge** **Networks**（**JKNets**）等技术实现。'
- en: '**Adaptive depth mechanisms** dynamically adjust the effective depth of the
    network for each node, allowing different parts of the graph to be processed at
    different depths as needed. This approach can be implemented using techniques
    such as **DropEdge** , which stochastically removes edges or layers during training
    to create networks of varying effective depths.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应深度机制**动态地调整每个节点的有效深度，使图的不同部分可以根据需要在不同深度下进行处理。这个方法可以通过使用如**DropEdge**等技术实现，该技术在训练过程中随机移除边或层，以创建具有不同有效深度的网络。'
- en: Over-smoothing and over-squashing
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过度平滑与过度压缩
- en: As the number of GNN layers increases, node representations tend to converge
    to similar values ( **over-smoothing** ), and information from distant nodes may
    be “ *squashed* ” as it propagates through the graph ( **over-squashing** ). In
    a protein-protein interaction network, for instance, over-smoothing might cause
    the model to lose distinctive features of individual proteins, while over-squashing
    could prevent information about important distant interactions from influencing
    the final representation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GNN层数的增加，节点表示趋向于收敛到相似的值（**过度平滑**），并且远程节点的信息可能在传播过程中被“*压缩*”（**过度压缩**）。例如，在蛋白质-蛋白质相互作用网络中，过度平滑可能导致模型失去个体蛋白质的独特特征，而过度压缩则可能阻止远程重要相互作用的信息影响最终的表示。
- en: 'To combat these issues, several techniques have been proposed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，提出了几种技术：
- en: '**Normalization** methods such as **PairNorm** ( [https://arxiv.org/abs/1909.12223](https://arxiv.org/abs/1909.12223)
    ) or **DiffGroupNorm** ( [https://arxiv.org/abs/2006.06972](https://arxiv.org/abs/2006.06972)
    ) help maintain the diversity of node representations across layers by normalizing
    pairwise distances between node features. These methods adjust the scale of node
    representations to prevent them from converging to a single point.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**方法，如**PairNorm**（[https://arxiv.org/abs/1909.12223](https://arxiv.org/abs/1909.12223)）或**DiffGroupNorm**（[https://arxiv.org/abs/2006.06972](https://arxiv.org/abs/2006.06972)），通过归一化节点特征之间的成对距离来帮助维持不同层之间节点表示的多样性。这些方法调整节点表示的尺度，以防止它们收敛到单一点。'
- en: '**Adaptive edge pruning** techniques dynamically remove less important edges
    during message passing, reducing redundant information flow and mitigating over-smoothing.
    This can be implemented using attention mechanisms or learned edge importance
    scores.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应边修剪**技术在消息传递过程中动态移除不重要的边，减少冗余信息流并缓解过度平滑。可以通过使用注意力机制或学习的边重要性得分来实现这一点。'
- en: '**Hierarchical pooling** strategies progressively coarsen the graph, reducing
    its size while preserving its global structure. Methods such as **DiffPool** can
    be used to create hierarchical representations that capture information at different
    scales, helping to prevent over-squashing by providing more direct paths for information
    flow between distant nodes.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次池化**策略逐步简化图的结构，在保留全局结构的同时减少其大小。像**DiffPool**这样的算法可以用来创建层次化表示，在不同尺度上捕捉信息，帮助通过提供更直接的信息流路径，防止过度压缩。'
- en: Balancing local and global information
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平衡局部和全局信息
- en: GNNs need to effectively combine local structural information with global graph
    properties, but finding the right balance is often difficult. This challenge is
    evident in tasks such as traffic prediction on a road network, where the model
    needs to consider both the immediate surroundings of a road segment ( *local*
    ) and overall traffic flow patterns in the city ( *global* ).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 需要有效地结合局部结构信息与全局图属性，但找到合适的平衡往往是困难的。这个挑战在诸如交通预测这类任务中尤为明显，在这些任务中，模型需要同时考虑道路段的周围环境（*局部*）以及整个城市的交通流量模式（*全局*）。
- en: 'To achieve this balance, several approaches have been developed:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一平衡，已经开发出几种方法：
- en: '**Graph pooling techniques** aggregate node information hierarchically, creating
    a multi-scale representation of the graph. Methods such as **Self-Attention Graph
    Pooling** ( **SAGPool** ) or **TopKPool** use learnable pooling operations to
    select and combine important nodes at each level of the hierarchy.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图池化技术**通过分层聚合节点信息，创建图的多尺度表示。像**自注意力图池化**（**SAGPool**）或**TopKPool**这样的技术使用可学习的池化操作，在每个层次选择并结合重要的节点。'
- en: '**Combining local GNN layers with global readout functions** allows the model
    to explicitly incorporate graph-level information. This can be achieved by using
    techniques such as **Set2Set** or **SortPool** to create fixed-size graph representations
    that capture global structure. Set2Set is a recurrent-based method that aggregates
    node representations by iteratively applying attention mechanisms, ensuring a
    dynamic and order-invariant set representation. SortPool, on the other hand, sorts
    node embeddings based on a chosen criterion (for example, node degree) and then
    selects the top- *k* nodes to form a fixed-size graph representation. Both methods
    help in summarizing entire graphs while maintaining important structural information,
    thus ensuring better performance in tasks requiring both local and global understanding
    of the graph.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结合局部 GNN 层与全局读出函数**可以让模型显式地融入图级信息。这可以通过使用像**Set2Set**或**SortPool**这样的技术来实现，生成固定大小的图表示，从而捕捉全局结构。Set2Set
    是一种基于递归的方法，通过迭代地应用注意力机制聚合节点表示，确保动态且顺序不变的集合表示。另一方面，SortPool 根据选定标准（例如节点度数）对节点嵌入进行排序，然后选择排名前
    *k* 的节点来形成固定大小的图表示。这两种方法都帮助总结整个图，同时保持重要的结构信息，从而确保在需要局部和全局图理解的任务中获得更好的表现。'
- en: '**Attention mechanisms** that span different scales, such as those used in
    graph transformer architectures, allow the model to selectively focus on both
    local and global graph properties. These mechanisms can be implemented using **multi-head
    attention** ( **MHA** ), where different heads can attend to information at different
    scales, from immediate neighbors to distant nodes or even global graph properties.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨尺度的注意力机制**，例如图 Transformer 架构中使用的注意力机制，使模型能够有选择地关注图的局部和全局属性。这些机制可以通过**多头注意力**（**MHA**）实现，其中不同的头可以关注不同尺度的信息，从直接邻居到远程节点，甚至是全局图属性。'
- en: Facing a model architecture challenge – an example
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面临模型架构挑战——举个例子
- en: To illustrate these challenges with a concrete example, let’s consider a large
    social network analysis task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过具体例子来说明这些挑战，假设我们面临一个大型社交网络分析任务。
- en: 'Imagine developing a GNN model to predict user interests on a platform such
    as X (formerly Twitter). The graph consists of millions of users ( *nodes* ) connected
    by follower relationships ( *edges* ), with tweets and hashtags as additional
    features. The following are some challenges that may arise:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在开发一个 GNN 模型，用于预测平台（例如 X（前身为 Twitter））上的用户兴趣。图包含数百万个用户（*节点*），这些用户通过粉丝关系（*边*）相连接，并且推文和标签作为附加特征。以下是可能出现的一些挑战：
- en: '**Long-range dependencies** : The model needs to capture influences from popular
    users or trending topics that might be several hops away in the follower graph.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长程依赖**：模型需要捕捉来自流行用户或热门话题的影响，这些影响可能位于粉丝图中相隔多个跳数的地方。'
- en: '**Depth limitation** : Simply stacking more GNN layers doesn’t necessarily
    improve the model’s ability to understand complex user interaction patterns.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度限制**：单纯堆叠更多的 GNN 层不一定能提高模型理解复杂用户交互模式的能力。'
- en: '**Over-smoothing and over-squashing** : With a deep GNN, users with diverse
    interests might end up with similar representations, losing important distinctions.
    Information about niche interests from distant parts of the network might get
    lost as it propagates through the graph.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度平滑和过度压缩**：在深层GNN中，兴趣多样的用户可能会得到相似的表示，从而丧失重要的区分性。来自网络远端的关于小众兴趣的信息可能会随着传播在图中逐渐丧失。'
- en: '**Balancing local and global information** : The model must combine a user’s
    immediate network ( *local* ) with platform-wide trends and influential users
    ( *global* ) to make accurate interest predictions.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡局部和全局信息**：模型必须将用户的即时网络（*局部*）与平台范围内的趋势和有影响力的用户（*全局*）结合，以做出准确的兴趣预测。'
- en: By addressing these architectural challenges using the strategies mentioned
    previously, practitioners can develop more powerful and flexible GNN models capable
    of handling a wide range of graph learning tasks across various domains.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用前面提到的策略来解决这些架构挑战，实践者可以开发出更强大、更灵活的图神经网络（GNN）模型，能够处理跨多个领域的各种图学习任务。
- en: As GNNs continue to evolve and find applications in increasingly large-scale
    graph datasets, practitioners face significant computational challenges that push
    the boundaries of existing algorithms and computing infrastructures. The following
    section explores three primary computational challenges that researchers and developers
    must navigate to unlock the full potential of GNNs across various domains.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图神经网络（GNN）技术的不断发展，并在越来越大规模的图数据集上找到应用，实践者面临着重大的计算挑战，这些挑战推动着现有算法和计算基础设施的边界。以下部分将探讨研究人员和开发人员必须应对的三大计算挑战，以解锁GNN在多个领域的全部潜力。
- en: Computational challenges
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算挑战
- en: 'As graph learning techniques continue to evolve and find applications in increasingly
    complex domains, they face significant computational hurdles. The sheer scale
    and intricacy of real-world graphs pose formidable challenges to existing algorithms
    and computing infrastructures. Here, we delve into three primary computational
    challenges that researchers and practitioners encounter when working with large-scale
    graph data: scalability issues, memory constraints, and the need for parallel
    and distributed computing solutions.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图学习技术的不断发展并在越来越复杂的领域中找到应用，它们面临着巨大的计算挑战。现实世界图的规模和复杂性为现有算法和计算基础设施带来了巨大的挑战。在这里，我们深入探讨了研究人员和实践者在处理大规模图数据时遇到的三大主要计算挑战：扩展性问题、内存限制和对并行与分布式计算解决方案的需求。
- en: Scalability issues for large graphs
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大规模图的扩展性问题
- en: As graph data continues to grow in size and complexity, scalability has become
    a critical challenge for graph learning algorithms. This issue is particularly
    evident in scenarios such as social network analysis or web-scale graphs, where
    billions of nodes and edges are common. Traditional graph algorithms often have
    time complexities that scale poorly with graph size, making them impractical for
    large-scale applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图数据在规模和复杂性上的不断增长，扩展性已成为图学习算法的一个关键挑战。这一问题在社交网络分析或Web规模图等场景中尤为明显，这些场景中数十亿的节点和边是常见现象。传统的图算法往往在图的规模增大时时间复杂度急剧上升，使得它们在大规模应用中变得不可行。
- en: To address this challenge, several approaches have been developed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，已经开发出多种方法。
- en: Sampling-based methods, such as **GraphSAGE** , which we explored in [*Chapter
    4*](B22118_04.xhtml#_idTextAnchor078) , or **FastGCN** , reduce computational
    complexity by operating on subsets of the graph. These techniques randomly sample
    neighborhoods or nodes during training, allowing the model to scale to large graphs
    by approximating full-graph computations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于采样的方法，如**GraphSAGE**，我们在[*第4章*](B22118_04.xhtml#_idTextAnchor078)中探讨过，或者**FastGCN**，通过在图的子集上操作来减少计算复杂度。这些技术在训练过程中随机采样邻域或节点，使得模型能够通过近似全图计算来扩展到大规模图。
- en: Another approach is the use of simplified propagation rules, as seen in models
    such as **Simple Graph Convolution** ( **SGC** ) or **Scalable Inception Graph
    Neural Networks** ( **SIGNs** ). These methods reduce the number of nonlinear
    operations and parameter updates required in each iteration, significantly speeding
    up training and inference on large graphs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用简化的传播规则，如**简单图卷积**（**SGC**）或**可扩展的图神经网络**（**SIGNs**）。这些方法减少了每次迭代中所需的非线性操作和参数更新的数量，从而显著加速了大规模图的训练和推理过程。
- en: Memory constraints in graph processing
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图处理中的内存限制
- en: 'Processing large graphs often requires holding substantial amounts of data
    in memory, which can exceed the capacity of single machines. This challenge is
    particularly acute in tasks involving large knowledge graphs or molecular datasets
    with millions of compounds. To address this, several techniques have been developed:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大规模图通常需要在内存中存储大量数据，这可能超过单台机器的容量。这一挑战在涉及到大规模知识图谱或包含数百万化合物的分子数据集的任务中尤为突出。为了解决这个问题，已经开发了几种技术：
- en: '**Out-of-core computing techniques** , such as those used in **GraphChi** or
    **X-Stream** , allow the processing of graphs that don’t fit in main memory by
    efficiently managing data on disk. These methods carefully organize graph data
    and computations to minimize random access to secondary storage.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外存计算技术**，如**GraphChi**或**X-Stream**中使用的技术，允许处理那些无法完全存入主内存的图，通过高效管理磁盘上的数据来进行处理。这些方法精心组织图数据和计算，尽量减少对二级存储的随机访问。'
- en: '**Graph compression techniques** , such as those employed in **k2-tree** representations,
    reduce the memory footprint of large graphs by exploiting structural regularities
    and redundancies. These approaches can significantly reduce storage requirements
    while still allowing efficient query operations.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图压缩技术**，例如**k2-tree**表示法中使用的技术，通过利用结构性规律和冗余来减少大图的内存占用。这些方法可以显著减少存储需求，同时仍然允许高效的查询操作。'
- en: Another effective approach is the use of **mini-batch training strategies**
    , as seen in **Cluster-GCN** ( [https://arxiv.org/abs/1905.07953](https://arxiv.org/abs/1905.07953)
    ) or **GraphSAINT** ( [https://arxiv.org/abs/1907.04931](https://arxiv.org/abs/1907.04931)
    ). These methods process the graph in small, manageable subgraphs or batches,
    allowing training on much larger graphs than would be possible with full-graph
    approaches.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种有效的方法是使用**小批量训练策略**，例如在**Cluster-GCN**（[https://arxiv.org/abs/1905.07953](https://arxiv.org/abs/1905.07953)）或**GraphSAINT**（[https://arxiv.org/abs/1907.04931](https://arxiv.org/abs/1907.04931)）中所见。这些方法通过将图划分为小的、可管理的子图或批次来处理图，从而可以在比全图方法更大的图上进行训练。
- en: Parallel and distributed computing for graphs
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图的并行和分布式计算
- en: 'The scale of many real-world graphs necessitates the use of parallel and distributed
    computing techniques to achieve reasonable processing times. This is crucial in
    applications such as analyzing internet-scale networks or processing large-scale
    scientific simulation data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 许多实际图的规模需要使用并行和分布式计算技术来实现合理的处理时间。这在分析互联网级网络或处理大规模科学模拟数据等应用中至关重要：
- en: '**Graph-parallel frameworks** , such as **Pregel** , **GraphLab** , or **PowerGraph**
    , provide programming models specifically designed for distributed graph computations.
    These frameworks often use a **“think like a vertex” paradigm** , where computations
    are expressed from the perspective of individual nodes, facilitating parallelization
    across a cluster of machines.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图并行框架**，如**Pregel**、**GraphLab**或**PowerGraph**，提供专门为分布式图计算设计的编程模型。这些框架通常使用**“像顶点一样思考”范式**，在这种范式中，计算从单个节点的角度来表达，从而便于在集群中的并行化。'
- en: '**Distributed GNN training techniques** , such as those used in **PinSage**
    or **AliGraph** , allow GNN models to be trained on massive graphs spread across
    multiple machines. These approaches often combine **data parallelism** (distributing
    the graph across machines) with **model parallelism** (distributing the **neural
    network** ( **NN** ) itself).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式GNN训练技术**，例如**PinSage**或**AliGraph**中使用的技术，允许在分布在多台机器上的庞大图上训练GNN模型。这些方法通常将**数据并行性**（将图分布到不同机器上）与**模型并行性**（将**神经网络**（**NN**）本身分布到不同机器上）结合使用。'
- en: '**GPU-accelerated graph processing** , exemplified by frameworks such as **Gunrock**
    or **cuGraph** , leverages the massive parallelism of GPUs to speed up graph algorithms.
    These approaches often require careful algorithm redesign to match GPU architectures,
    such as using warp-centric programming models or optimizing memory access patterns.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU加速的图处理**，例如**Gunrock**或**cuGraph**等框架，利用GPU的强大并行计算能力来加速图算法。这些方法通常需要精心重新设计算法，以适应GPU架构，如使用基于warp的编程模型或优化内存访问模式。'
- en: By addressing these computational challenges, we can develop graph learning
    systems capable of handling the scale and complexity of real-world graph data,
    opening up new possibilities for applications in areas such as social network
    analysis, recommender systems, and scientific computing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些计算挑战，我们可以开发出能够处理现实世界图数据规模和复杂性的图学习系统，开启在社交网络分析、推荐系统、科学计算等领域的新应用可能性。
- en: While addressing these broad computational challenges is crucial, it’s equally
    important to consider specific issues that arise in different graph-related tasks.
    Let’s explore some of these task-specific challenges, starting with node classification
    in imbalanced graphs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管解决这些广泛的计算挑战至关重要，但同样重要的是要考虑不同图相关任务中出现的具体问题。让我们从不平衡图中的节点分类开始，探讨一些这些任务特定的挑战。
- en: Task-specific challenges
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特定任务的挑战
- en: While graph learning algorithms face general challenges, certain tasks present
    unique difficulties that require specialized approaches. In this section, we consider
    four common task-specific challenges in graph learning, each with its own set
    of complexities and proposed solutions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图学习算法面临普遍的挑战，但某些任务呈现出独特的困难，需要专门的方法。在本节中，我们考虑了图学习中的四个常见任务特定挑战，每个挑战都有其自身的复杂性和提出的解决方案。
- en: Node classification in imbalanced graphs
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不平衡图中的节点分类
- en: Node classification in real-world graphs often suffers from class imbalance,
    where some classes are significantly underrepresented. This issue is prevalent
    in scenarios such as fraud detection in financial transaction networks, where
    fraudulent transactions are typically rare compared to legitimate ones. The imbalance
    can lead to biased models that perform poorly on minority classes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界图中的节点分类通常存在类不平衡问题，其中某些类的样本显著较少。这个问题在金融交易网络中的欺诈检测等场景中尤为突出，在这些场景中，欺诈交易通常比合法交易少得多。这种不平衡可能导致模型偏向多数类，表现不佳。
- en: 'Some approaches to mitigate this include the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一些缓解这种问题的方法包括：
- en: '**Re-sampling techniques** , such as over-sampling minority classes or under-sampling
    majority classes, can be adapted for graph data. For instance, **GraphSMOTE**
    extends the **Synthetic Minority Over-sampling TEchnique** ( **SMOTE** ) algorithm
    to graph-structured data, generating synthetic samples for minority classes by
    interpolating between existing nodes in the feature and graph space.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重采样技术**，例如过采样少数类或欠采样多数类，可以适应图数据。例如，**GraphSMOTE** 将 **合成少数类过采样技术**（**SMOTE**）算法扩展到图结构数据，通过在特征和图空间中插值现有节点来为少数类生成合成样本。'
- en: '**Cost-sensitive learning approaches** assign higher penalties for misclassifications
    of minority class nodes during training. This can be implemented by modifying
    the loss function to weight errors on minority classes more heavily.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本敏感学习方法** 在训练过程中对少数类节点的误分类赋予更高的惩罚。这可以通过修改损失函数来加重少数类的错误权重。'
- en: Another effective approach is to use **meta-learning techniques** , such as
    **few-shot learning** ( **FSL** ) algorithms adapted for graphs. These methods,
    such as **Meta-GNN** or **graph prototypical networks** ( **GPNs** ), aim to learn
    generalizable representations that can perform well on underrepresented classes
    with limited samples.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种有效的方法是使用 **元学习技术**，例如为图定制的 **小样本学习**（**FSL**）算法。这些方法，如 **Meta-GNN** 或 **图原型网络**（**GPNs**），旨在学习可泛化的表示，从而能够在样本稀缺的情况下，针对少数类表现良好。
- en: Link prediction in sparse graphs
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏图中的链接预测
- en: Link prediction in sparse graphs presents unique challenges, as the vast majority
    of potential edges are absent, leading to extreme class imbalance in the link
    prediction task. This is common in biological networks, where only a small fraction
    of possible interactions between entities are observed. The sparsity can make
    it difficult to learn meaningful patterns.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏图中的链接预测面临独特的挑战，因为绝大多数潜在的边缘都缺失，导致链接预测任务中的类不平衡。这在生物网络中很常见，在这些网络中，只有少数实体之间的可能交互被观察到。稀疏性使得很难学习到有意义的模式。
- en: To tackle this issue, several specialized approaches have been proposed. **Negative
    sampling** techniques carefully select a subset of non-existent edges as negative
    examples during training, balancing the dataset without introducing too much noise.
    Advanced methods such as **knowledge-based generative adversarial networks** (
    **KBGANs** ) use adversarial training to generate high-quality negative samples.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，已经提出了几种专门的方法。**负采样**技术在训练过程中仔细选择一部分不存在的边作为负样本，从而平衡数据集而不引入过多噪声。像**基于知识的生成对抗网络**（**KBGANs**）这样的先进方法通过对抗训练生成高质量的负样本。
- en: Graph generation and reconstruction
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图生成与重建
- en: Graph generation and reconstruction tasks aim to create new graphs or complete
    partial graphs, which is challenging due to the discrete nature of graphs and
    the need to maintain complex structural properties. This is crucial in applications
    such as drug discovery, where generating valid molecular graphs is essential.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图生成和重建任务旨在创建新的图形或完成部分图形，由于图形的离散性质以及需要保持复杂结构属性，这一任务充满挑战。这对于药物发现等应用尤为关键，其中生成有效的分子图形至关重要。
- en: One major approach to this challenge is the use of **variational autoencoders**
    ( **VAEs** ) adapted for graphs, such as **GraphVAE** or **variational graph autoencoders**
    ( **VGAE** ). These models learn a continuous latent space representation of graphs,
    allowing for the generation of new graphs by sampling from this space.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 应对这一挑战的一个主要方法是使用适应图形的**变分自编码器**（**VAEs**），如**GraphVAE**或**变分图自编码器**（**VGAE**）。这些模型学习图形的连续潜在空间表示，从而通过从这个空间中采样生成新的图形。
- en: However, ensuring the validity of generated graphs remains a challenge. Another
    powerful approach is the use of autoregressive models for graph generation, as
    seen in **GraphRNN** or **graph recurrent attention networks** ( **GRANs** ).
    These models generate graphs sequentially, one node or edge at a time, capturing
    complex dependencies in the graph structure.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确保生成的图形的有效性仍然是一个挑战。另一种强大的方法是使用自回归模型进行图形生成，如**GraphRNN**或**图形递归注意力网络**（**GRANs**）。这些模型逐节点或逐边生成图形，捕捉图结构中的复杂依赖关系。
- en: Graph matching and alignment
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图匹配与对齐
- en: Graph matching and alignment involve finding correspondences between nodes of
    different graphs, which is crucial in tasks such as network alignment in systems
    biology or matching 3D objects in CV. This task is computationally challenging
    due to the combinatorial nature of the problem and the need to consider both structural
    and attribute similarities.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图匹配与对齐涉及找到不同图形之间节点的对应关系，这对于如系统生物学中的网络对齐或计算机视觉中的3D物体匹配等任务至关重要。由于问题的组合性质以及需要考虑结构和属性相似性，这一任务在计算上非常具有挑战性。
- en: To overcome this challenge for larger graphs, approximate methods based on graph
    embeddings have gained popularity. Models such as **REpresentation learning-based
    Graph Alignment** ( **REGAL** ) learn node embeddings that preserve local and
    global graph structure, allowing for efficient alignment by matching nodes in
    the embedding space. Recent advances also include the application of GNNs to the
    matching task, such as the **cross-graph attention network** , which learns to
    match nodes by attending to their local neighborhoods across graphs. By addressing
    these task-specific challenges, we can develop more effective and robust graph
    learning models tailored to the unique requirements of different applications.
    As the field progresses, we can expect to see further innovations that combine
    insights from multiple approaches to tackle these complex problems.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服较大图形的这一挑战，基于图嵌入的近似方法变得越来越流行。**基于表示学习的图对齐**（**REGAL**）等模型学习保留局部和全局图结构的节点嵌入，从而通过匹配嵌入空间中的节点实现高效对齐。最近的进展还包括将图神经网络（GNNs）应用于匹配任务，如**跨图注意力网络**，它通过关注图中的局部邻域来学习匹配节点。通过解决这些任务特定的挑战，我们可以开发出更有效、更稳健的图学习模型，以适应不同应用的独特需求。随着该领域的发展，我们可以预见到将出现更多创新，结合多种方法的见解来解决这些复杂问题。
- en: 'As we move from discussing technical innovations in graph learning, it’s crucial
    to consider how these complex models can be understood and explained, especially
    when applied to high-stakes domains. This leads us to our next important topic:
    the interpretability and explainability of graph learning models.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从讨论图学习中的技术创新转向，考虑这些复杂模型如何被理解和解释变得至关重要，尤其是在高风险领域应用时。这引出了我们下一个重要的话题：图学习模型的可解释性和可解释性。
- en: Interpretability and explainability
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性和可解释性
- en: As graph learning models become increasingly complex and are applied to critical
    domains such as healthcare, finance, and social sciences, the need for interpretable
    and explainable models has grown significantly. Here, we explore two key aspects
    of interpretability and explainability in graph learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图学习模型变得越来越复杂，并且被应用于诸如医疗保健、金融和社会科学等关键领域，对可解释和可解释模型的需求显著增长。在这里，我们探讨图学习中可解释性和可解释性的两个关键方面。
- en: Explaining GNN decisions
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释GNN决策
- en: 'GNNs often act as black boxes, making it challenging to understand why they
    make certain predictions. This lack of transparency can be problematic in high-stakes
    applications such as drug discovery or financial fraud detection. To address this,
    several approaches have been developed to explain GNN decisions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GNN通常作为黑盒运行，使得理解其做出某些预测的原因变得困难。这种透明度缺乏在药物发现或金融欺诈检测等高风险应用中可能带来问题。为了解决这个问题，已经开发了几种方法来解释GNN的决策：
- en: One prominent method is **GNNExplainer** , which identifies important subgraphs
    and features that influence a model’s predictions. It does this by optimizing
    a mutual information objective between a conditional distribution of the GNN’s
    predictions and a simplified explanation.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个重要的方法是**GNNExplainer**，它识别影响模型预测的重要子图和特征。通过优化GNN预测的条件分布与简化解释之间的互信息目标来实现这一点。
- en: Another approach is **GraphLIME** , an extension of the **Local Interpretable
    Model-agnostic Explanation** ( **LIME** ) framework for graph-structured data.
    It explains individual predictions by learning an interpretable model locally
    around the prediction.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是**GraphLIME**，它是**局部可解释模型无关解释**（**LIME**）框架的一个扩展，专为图结构数据设计。它通过在预测周围局部学习一个可解释的模型来解释单个预测。
- en: Gradient-based methods, such as **Grad-CAM** adapted for graphs, provide explanations
    by visualizing the gradients of the output with respect to intermediate feature
    maps, highlighting important regions of the input graph. Some recent works also
    focus on counterfactual explanations for GNNs, generating minimal changes to the
    input graph that would alter the model’s prediction.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于梯度的方法，如适用于图的**Grad-CAM**，通过可视化输出相对于中间特征图的梯度提供解释，突出显示输入图的重要区域。一些近期的研究还集中于图神经网络（GNN）的反事实解释，生成最小的输入图变化，以改变模型的预测。
- en: These approaches help in understanding model decisions and identify potential
    biases or vulnerabilities in the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法有助于理解模型决策并识别模型中的潜在偏差或脆弱性。
- en: Visualizing graph embeddings
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化图嵌入
- en: 'Graph embeddings, which represent nodes or entire graphs as vectors in a low-dimensional
    space, are fundamental to many graph learning tasks. However, interpreting these
    embeddings can be challenging due to their high-dimensional nature. Various techniques
    have been developed to visualize and understand these embeddings:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入将节点或整个图表示为低维空间中的向量，这是许多图学习任务的基础。然而，由于其高维性质，解释这些嵌入可能具有挑战性。已经开发了各种技术来可视化和理解这些嵌入：
- en: '**Dimensionality reduction techniques** , such as **t-distributed Stochastic
    Neighbor Embedding** ( **t-SNE** ) or **Uniform Manifold Approximation and Projection**
    ( **UMAP** ), are commonly used to project high-dimensional embeddings into 2D
    or 3D spaces for visualization. These methods aim to preserve local relationships
    between points, allowing for the identification of clusters and patterns in the
    embedding space.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维技术**，如**t-分布随机邻域嵌入**（**t-SNE**）或**均匀流形近似与投影**（**UMAP**），通常用于将高维嵌入投影到二维或三维空间以进行可视化。这些方法旨在保持点之间的局部关系，从而能够在嵌入空间中识别集群和模式。'
- en: '**Interactive visualization tools** , such as **TensorBoard Projector** or
    **Embedding Projector** , allow users to explore embeddings dynamically, zooming
    in on specific regions and examining relationships between nodes. Some advanced
    approaches combine embedding visualization with the original graph structure.
    For instance, **GraphTSNE** integrates graph structural information into the t-SNE
    algorithm, producing layouts that reflect both the embedding similarity and the
    graph topology.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式可视化工具**，例如**TensorBoard Projector**或**Embedding Projector**，允许用户动态地探索嵌入，放大特定区域并检查节点之间的关系。一些先进的方法将嵌入可视化与原始图结构相结合。例如，**GraphTSNE**将图结构信息集成到t-SNE算法中，生成的布局反映了嵌入的相似性和图的拓扑结构。'
- en: Another innovative approach is the use of **graph generation techniques** to
    visualize embeddings. By training a graph generative model on embeddings and original
    graphs, one can generate synthetic graphs that represent different regions of
    the embedding space, providing intuitive visualizations of what the embeddings
    have captured.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种创新方法是使用**图生成技术**来可视化嵌入。通过在嵌入和原始图上训练图生成模型，可以生成代表嵌入空间不同区域的合成图，从而直观地展示嵌入所捕捉到的内容。
- en: By addressing these aspects of interpretability and explainability, researchers
    aim to bridge the gap between the performance of complex graph learning models
    and the need for transparent, trustworthy AI systems. As the field progresses,
    we can expect to see further integration of these techniques into mainstream graph
    learning frameworks, making them more accessible to practitioners across various
    domains. The development of interpretable and explainable graph learning models
    not only enhances trust and adoption but also opens new avenues for scientific
    discovery and knowledge extraction from complex graph-structured data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些可解释性和可解释性的方面，研究人员旨在弥合复杂图学习模型的性能与透明、可信赖的人工智能系统需求之间的差距。随着该领域的进展，我们可以预期看到这些技术进一步融入主流的图学习框架，使其更易于被各个领域的从业者使用。可解释和可解释的图学习模型的开发不仅增强了信任和采纳，还为从复杂的图结构数据中进行科学发现和知识提取开辟了新的途径。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explored the multifaceted challenges that define the
    current landscape of graph learning. From fundamental issues of handling large-scale,
    heterogeneous, and dynamic graph data to intricate problems of designing effective
    GNN architectures, each challenge presents unique obstacles and opportunities
    for innovation. We’ve examined the computational hurdles of processing massive
    graphs, nuanced difficulties in specific tasks such as node classification and
    link prediction, and the growing demand for interpretable and explainable models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了定义当前图学习领域的多方面挑战。从处理大规模、异构和动态图数据的基本问题到设计有效的GNN架构的复杂问题，每个挑战都带来了独特的障碍和创新机会。我们还研究了处理大规模图数据的计算难题、在节点分类和链接预测等特定任务中的细微困难，以及对可解释和可解释模型日益增长的需求。
- en: These challenges are not isolated; they intersect and compound each other, creating
    a complex ecosystem of problems that researchers and practitioners must navigate.
    As graph learning continues to evolve and find applications in critical domains
    such as healthcare, finance, and social sciences, addressing these challenges
    becomes not just an academic pursuit but a practical necessity.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战并非孤立存在；它们交织在一起，相互叠加，创造出一个复杂的生态系统，研究人员和从业者必须在其中导航。随着图学习的不断发展，并在医疗、金融和社会科学等关键领域找到应用，解决这些挑战不仅是学术追求，更是实际的必要性。
- en: The future of graph learning lies in developing holistic solutions that can
    handle the scale, complexity, and dynamism of real-world graphs while providing
    robust, efficient, and interpretable models. By confronting these challenges head-on,
    you are now poised to unlock new possibilities and drive innovations that can
    transform how we understand and interact with the interconnected world around
    us.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图学习的未来在于开发能够处理现实世界图的规模、复杂性和动态性的整体解决方案，同时提供强大、高效和可解释的模型。通过正面应对这些挑战，您现在已准备好解锁新的可能性，并推动创新，改变我们理解和与周围互联世界互动的方式。
- en: As we look to the future of graph learning, one promising avenue is the integration
    of **large language models** ( **LLMs** ) with graph-based approaches. The next
    chapter explores how LLMs can be leveraged to enhance graph learning techniques,
    potentially addressing some of the challenges discussed here while opening up
    new possibilities for more sophisticated graph analysis and understanding.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们展望图学习的未来时，一个有前景的方向是将**大语言模型**（**LLMs**）与基于图的方法相结合。下一章将探讨如何利用LLMs来增强图学习技术，可能解决此处讨论的一些挑战，同时为更复杂的图分析和理解开辟新的可能性。
