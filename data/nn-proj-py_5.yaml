- en: Removing Noise from Images Using Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器去除图像噪声
- en: In this chapter, we will study a class of neural networks known as autoencoders,
    which have gained traction in recent years. In particular, the ability of autoencoders
    to remove noise from images has been greatly studied. In this chapter, we will
    build and train an autoencoder that is able to denoise and restore corrupted images.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究一类神经网络，称为自编码器，这些网络近年来得到了广泛关注。特别是，自编码器去除图像噪声的能力已经得到了大量研究。在本章中，我们将构建并训练一个能够去噪并恢复损坏图像的自编码器。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What are autoencoders?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是自编码器？
- en: Unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Types of autoencoders—basic autoencoders, deep autoencoders and convolutional
    autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的类型——基础自编码器、深度自编码器和卷积自编码器
- en: Autoencoders for image compression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于图像压缩的自编码器
- en: Autoencoders for image denoising
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于图像去噪的自编码器
- en: Step-by-step guide to build and train an autoencoder in Keras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练自编码器的逐步指南（在 Keras 中）
- en: Analysis of our results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们结果的分析
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The Python libraries required for this chapter are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的 Python 库有：
- en: matplotlib 3.0.2
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib 3.0.2
- en: Keras 2.2.4
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 2.2.4
- en: Numpy 1.15.2
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Numpy 1.15.2
- en: PIL 5.4.1
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PIL 5.4.1
- en: 'The code and dataset for this chapter can be found in the GitHub repository
    for the book at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和数据集可以在本书的 GitHub 仓库找到：[https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python)
- en: 'To download the code into your computer, you may run the following `git clone`
    command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要将代码下载到计算机中，您可以运行以下 `git clone` 命令：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the process is complete, there will be a folder titled `Neural-Network-Projects-with-Python`.
    Enter the folder by running:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完成后，将会有一个名为 `Neural-Network-Projects-with-Python` 的文件夹。通过运行以下命令进入该文件夹：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要在虚拟环境中安装所需的 Python 库，请运行以下命令：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that you should have installed Anaconda in your computer first, before
    running this command. To enter the virtual environment, run the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在运行此命令之前，您应该首先在计算机中安装 Anaconda。要进入虚拟环境，请运行以下命令：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Navigate to the `Chapter05` folder by running the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令导航到 `Chapter05` 文件夹：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following files are located in the folder:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文件位于文件夹中：
- en: '`autoencoder_image_compression.py`: This is the code for the *Building a simple
    autoencoder* section in this chapter'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`autoencoder_image_compression.py`：这是本章 *构建一个简单的自编码器* 部分的代码'
- en: '`basic_autoencoder_denoise_MNIST.py` and `conv_autoencoder_denoise_MNIST.py`:
    These are the code for the *Denoising autoencoder* section in this chapter'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`basic_autoencoder_denoise_MNIST.py` 和 `conv_autoencoder_denoise_MNIST.py`：这些是本章
    *去噪自编码器* 部分的代码'
- en: '`basic_autoencoder_denoise_documents.py` and `deep_conv_autoencoder_denoise_documents.py`:
    These are the code for the *Denoising documents with autoencoders* section in
    this chapter'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`basic_autoencoder_denoise_documents.py` 和 `deep_conv_autoencoder_denoise_documents.py`：这些是本章
    *用自编码器去噪文档* 部分的代码'
- en: 'To run the code in each file, simply execute each Python file, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行每个文件中的代码，只需执行每个 Python 文件，如下所示：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What are autoencoders?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自编码器？
- en: So far in this book, we have looked at the applications of neural networks for
    supervised learning. Specifically, in each project, we have a labeled dataset
    (that is, features **x** and label **y***) *and our goal is to train a neural
    network using this dataset, so that the neural network is able to predict label **y**
    from any new instance **x**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经看过了神经网络在监督学习中的应用。具体来说，在每个项目中，我们都有一个带标签的数据集（即特征**x**和标签**y**），*我们的目标是利用这个数据集训练一个神经网络，使得神经网络能够从任何新的实例**x**中预测标签**y**。*
- en: 'A typical feedforward neural network is shown in the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的前馈神经网络如下面的图所示：
- en: '![](img/802d112e-32ad-4fe9-9966-c86bb791ad3b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/802d112e-32ad-4fe9-9966-c86bb791ad3b.png)'
- en: In this chapter, we will study a different class of neural networks, known as
    autoencoders. Autoencoders represent a paradigm shift from the conventional neural
    networks we have seen so far. The goal of autoencoders is to learn a **Latent** **Representation**
    of the input. This representation is usually a compressed representation of the
    original input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究一类不同的神经网络，称为自编码器。自编码器代表了迄今为止我们所见过的传统神经网络的一种范式转变。自编码器的目标是学习输入的**潜在**
    **表示**。这种表示通常是原始输入的压缩表示。
- en: All autoencoders have an **Encoder **and a **Decoder.** The role of the encoder
    is to encode the input to a learned, compressed representation, and the role of
    the decoder is to reconstruct the original input using the compressed representation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的自编码器都有一个**编码器**和一个**解码器**。编码器的作用是将输入编码为学习到的压缩表示，解码器的作用是使用压缩表示重构原始输入。
- en: 'The following diagram illustrates the architecture of a typical autoencoder:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了典型自编码器的架构：
- en: '![](img/e2ccadaf-b45a-494f-8193-48b3fcd8914c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2ccadaf-b45a-494f-8193-48b3fcd8914c.png)'
- en: Notice that, in the preceding diagram, we do not require a label *y*, unlike
    in CNNs. This distinction means that autoencoders are a form of unsupervised learning,
    while CNNs fall within the realm of supervised learning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的图表中，与CNNs不同，我们不需要标签*y*。这个区别意味着自编码器是一种无监督学习形式，而CNNs属于监督学习的范畴。
- en: Latent representation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在表示
- en: At this point, you might wonder what is the purpose of autoencoders. Why do
    we bother learning a representation of the original input, only to reconstruct
    a similar output? The answer lies in the learned representation of the input.
    By forcing the learned representation to be compressed (that is, having smaller
    dimensions compared to the input), we essentially force the neural network to
    learn the most salient representation of the input. This ensures that the learned
    representation only captures the most relevant characteristics of the input, known
    as the **latent representation**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会想知道自编码器的目的是什么。为什么我们要学习原始输入的表示，然后再重建一个类似的输出？答案在于输入的学习表示。通过强制学习到的表示被压缩（即与输入相比具有较小的维度），我们实质上迫使神经网络学习输入的最显著表示。这确保了学习到的表示仅捕捉输入的最相关特征，即所谓的**潜在表示**。
- en: 'As a concrete example of latent representations, take, for example, an autoencoder
    trained on the cats and dogs dataset, as shown in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为潜在表示的一个具体例子，例如，一个在猫和狗数据集上训练的自编码器，如下图所示：
- en: '![](img/f25ef79d-d1de-4ad3-866d-e7ee135cdfa7.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f25ef79d-d1de-4ad3-866d-e7ee135cdfa7.png)'
- en: An autoencoder trained on this dataset will eventually learn that the salient
    characteristics of cats and dogs are the the shape of the ears, the length of
    whiskers, the snout size, and the length of the tongue visible. These salient
    characteristics are captured by the latent representation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上训练的自编码器最终将学习到猫和狗的显著特征是耳朵的形状、胡须的长度、吻的大小和可见的舌头长度。这些显著特征被潜在表示捕捉到。
- en: 'With this latent representation learned by the autoencoder, we can then do
    the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个由自编码器学习到的潜在表示，我们可以做以下工作：
- en: Reduce the dimensionality of the input data. The latent representation is a
    natural reduced representation of the input data.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少输入数据的维度。潜在表示是输入数据的自然减少表示。
- en: Remove any noise from the input data (known as denoising). Noise is not a salient
    characteristic and therefore should be easily identifiable by using the latent
    representation.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从输入数据中去除任何噪音（称为去噪）。噪音不是显著特征，因此应该通过使用潜在表示轻松识别。
- en: In the sections to follow, we shall create and train autoencoders for each of
    the preceding purposes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将为每个前述目的创建和训练自编码器。
- en: Note that, in the previous example, we have used examples such as shape of ears
    and snout size as descriptions for the latent representation. In reality, latent
    representations are simply a matrix of numbers and it is impossible to assign
    meaningful labels for them (nor do we need to). The descriptions that we used
    here simply provide an intuitive explanation for latent representations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的例子中，我们已经使用了如耳朵形状和吻大小等描述作为潜在表示的描述。实际上，潜在表示只是一组数字的矩阵，不可能为其分配有意义的标签（也不需要）。我们在这里使用的描述仅仅为潜在表示提供了直观的解释。
- en: Autoencoders for data compression
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于数据压缩的自编码器
- en: So far, we have seen how autoencoders are able to learn a reduced representation
    of the input data. It is natural to think that autoencoders can do a good job
    at generalized data compression. However, that is not the case. Autoencoders are
    poor at generalized data compression, such as image compression (that is, JPEG)
    and audio compression (that is, MP3), because the learned latent representation
    only represents the data on which it was trained. In other words, autoencoders
    only work well for images similar to those on which it was trained.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到自编码器能够学习输入数据的简化表示。自然地，我们会认为自编码器在通用数据压缩方面做得很好。然而，事实并非如此。自编码器在通用数据压缩方面表现不佳，比如图像压缩（即
    JPEG）和音频压缩（即 MP3），因为学习到的潜在表示仅代表它所训练过的数据。换句话说，自编码器只对与其训练数据相似的图像有效。
- en: Furthermore, autoencoders are a "lossy" form of data compression, which means
    that the output from autoencoders will have less information when compared to
    the original input. These characteristics mean that autoencoders are poor at being
    generalized data compression techniques. Other forms of data compression, such
    as JPEG and MP3, are superior when compared to autoencoders.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自编码器是一种“有损”数据压缩形式，这意味着自编码器的输出相较于原始输入会丢失一些信息。这些特性使得自编码器在作为通用数据压缩技术时效果较差。其他数据压缩形式，如
    JPEG 和 MP3，相较于自编码器更为优越。
- en: The MNIST handwritten digits dataset
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 手写数字数据集
- en: One of the datasets that we'll use for this chapter is the MNIST handwritten
    digits dataset. The MNIST dataset contains 70,000 samples of handwritten digits,
    each of size 28 x 28 pixels. Each sample contains only one digit within the image,
    and all samples are labeled.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将使用的一个数据集是 MNIST 手写数字数据集。MNIST 数据集包含 70,000 个手写数字样本，每个样本的大小为 28 x 28 像素。每个样本图像只包含一个数字，且所有样本都有标签。
- en: 'The MNIST dataset is provided directly in Keras, and we can import it by simply
    running the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集在 Keras 中直接提供，我们只需运行以下代码即可导入：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/d22c2165-a1ac-4a11-953d-c2c4e12aef71.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d22c2165-a1ac-4a11-953d-c2c4e12aef71.png)'
- en: We can see that the digits are definitely handwritten, and each 28 x 28 image
    captures only one digit. The autoencoder should be able to learn the compressed
    representation of these digits (smaller than 28 x 28), and to reproduce the images
    using this compressed representation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这些数字确实是手写的，每个 28 x 28 的图像只包含一个数字。自编码器应该能够学习这些数字的压缩表示（小于 28 x 28），并使用这种压缩表示重建图像。
- en: 'The following diagram illustrates this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这一点：
- en: '![](img/49774e97-83b3-4391-9d3c-cfe3f4776937.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49774e97-83b3-4391-9d3c-cfe3f4776937.png)'
- en: Building a simple autoencoder
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个简单的自编码器
- en: 'To cement our understanding, let''s start off by building the most basic autoencoder,
    as shown in the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固我们的理解，让我们从构建最基本的自编码器开始，如下图所示：
- en: '![](img/10feca38-7f7b-4816-9733-322fe50ab617.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10feca38-7f7b-4816-9733-322fe50ab617.png)'
- en: So far, we have emphasized that the hidden layer (**Latent** **Representation**)
    should be of a smaller dimension than the input data. This ensures that the latent
    representation is a compressed representation of the salient features of the input.
    But how small should it be?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们强调过隐藏层（**潜在** **表示**）的维度应该小于输入数据的维度。这样可以确保潜在表示是输入显著特征的压缩表示。那么，隐藏层应该有多小呢？
- en: 'Ideally, the size of the hidden layer should balance between being:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，隐藏层的大小应当在以下两者之间保持平衡：
- en: Sufficiently *small* enough to represent a compressed representation of the
    input features
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 足够*小*，以便表示输入特征的压缩表示
- en: Sufficiently *large* enough for the decoder to reconstruct the original input
    without too much loss
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 足够*大*，以便解码器能够重建原始输入而不会有过多的损失
- en: In other words, the size of the hidden layer is a hyperparameter that we need
    to select carefully to obtain the best results. We shall see how we can define
    the size of the hidden layer in Keras.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，隐藏层的大小是一个超参数，我们需要仔细选择以获得最佳结果。接下来，我们将看看如何在 Keras 中定义隐藏层的大小。
- en: Building autoencoders in Keras
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中构建自编码器
- en: First, let's start building our basic autoencoder in Keras. As always, we'll
    use the `Sequential` class in Keras to build our model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们开始在 Keras 中构建我们的基本自编码器。和往常一样，我们将使用 Keras 中的 `Sequential` 类来构建我们的模型。
- en: 'We''ll start by importing and defining a new `Sequential` class in Keras:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从导入并定义一个新的 Keras `Sequential` 类开始：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we'll add the hidden layer to our model. From the previous diagram, we
    can clearly see that the hidden layer is a fully connected layer (that is, a `Dense`
    layer). From the `Dense` class in Keras, we can define the size of the hidden
    layer through the `units` parameter. The number of units is a hyperparameter that
    we will be experimenting with. For now, let's use a single node (units=1) as the
    hidden layer. The `input_shape` to the `Dense` layer is a vector of size `784`
    (since we are using 28 x 28 images) and the `activation` function is the `relu`
    activation function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向模型中添加隐藏层。从前面的图示中，我们可以清楚地看到隐藏层是一个全连接层（即一个`Dense`层）。在Keras的`Dense`类中，我们可以通过`units`参数定义隐藏层的大小。单元数量是一个超参数，我们将进行实验。现在，我们暂时使用一个节点（units=1）作为隐藏层。`Dense`层的`input_shape`是一个大小为`784`的向量（因为我们使用的是28
    x 28的图像），`activation`函数是`relu`激活函数。
- en: 'The following code adds a `Dense` layer with a single node to our model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码为我们的模型添加了一个包含单个节点的`Dense`层：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Lastly, we'll add the output layer. The output layer is also a fully connected
    layer (that is, a `Dense` layer), and the size of the output layer should naturally
    be `784`, since we are trying to output the original 28 x 28 image. We use a `Sigmoid`
    activation function for the output to constrain the output values (value per pixel)
    between 0 and 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加输出层。输出层也是一个全连接层（即一个`Dense`层），输出层的大小应该是`784`，因为我们试图输出原始的28 x 28图像。我们使用`Sigmoid`激活函数来约束输出值（每个像素的值）在0和1之间。
- en: 'The following code adds an output `Dense` layer with `784` units to our model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码为我们的模型添加了一个包含`784`个单元的输出`Dense`层：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Before we train our model, let's check the structure of our model and make sure
    that it is consistent with our diagram.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的模型之前，让我们检查模型的结构，确保它与我们的图示一致。
- en: 'We can do this by calling the `summary()` function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用`summary()`函数来实现这一点：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We get the following output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/ad6c0a37-f66e-4c5d-a474-a9a77f619831.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad6c0a37-f66e-4c5d-a474-a9a77f619831.png)'
- en: Before we move on to the next step, let's create a function that encapsulates
    the model creation process that we just went through. Having such a function is
    useful, as it allows us to easily create different models with different hidden
    layer sizes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一步之前，让我们创建一个封装模型创建过程的函数。拥有这样的函数是有用的，因为它可以让我们轻松创建具有不同隐藏层大小的不同模型。
- en: 'The following code defines a function that creates a basic autoencoder with
    a `hidden_layer_size` variable:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了一个创建基本自编码器的函数，其中包含`hidden_layer_size`变量：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The next step is to preprocess our data. There are two preprocessing steps
    required:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是预处理我们的数据。需要两个预处理步骤：
- en: Reshape the images from a 28 x 28 vector to a 784 x 1 vector.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像从28 x 28的向量重塑为784 x 1的向量。
- en: Normalize the values of the vector between 0 and 1 from the current 0 to 255\.
    This smaller range of values makes it easier to train our neural network using
    the data.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量的值从当前的0到255规范化到0和1之间。这个较小的值范围使得使用数据训练神经网络变得更加容易。
- en: 'To reshape the images from 28 x 28 to 784 x 1, we simply run the following
    code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将图像从28 x 28的大小重塑为784 x 1，我们只需运行以下代码：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that first dimension, `X_train.shape[0]`, refers to the number of samples.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个维度，`X_train.shape[0]`，表示样本的数量。
- en: 'To normalize the values of the vector between 0 and 1 (from the original range
    of 0 to 255), we run the following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将向量的值从0到255规范化到0和1之间，我们运行以下代码：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With that done, we can start to train our model. We'll first compile our model
    using the `adam` optimizer and `mean_squared_error` as the `loss` function. The
    `mean_squared_error` is useful in this case because we need a `loss` function
    that quantifies the pixel-wise discrepancy between the input and the output.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们可以开始训练我们的模型。我们将首先使用`adam`优化器并将`mean_squared_error`作为`loss`函数来编译我们的模型。`mean_squared_error`在这种情况下是有用的，因为我们需要一个`loss`函数来量化输入与输出之间逐像素的差异。
- en: 'The following code compiles our model using the aforementioned parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用上述参数编译我们的模型：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Finally, let's train our model for `10` epochs. Note that we use `X_train_reshaped`
    as both the input (*x*) and output (*y*). This makes sense because we are trying
    to train the autoencoder to produce output that is identical to the input.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们训练我们的模型`10`个周期。请注意，我们使用`X_train_reshaped`作为输入(*x*)和输出(*y*)。这是合理的，因为我们试图训练自编码器使输出与输入完全相同。
- en: 'We train our autoencoder with the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码训练我们的自编码器：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''ll see the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/2cc71cc5-daf2-4546-9267-0f0b77e61671.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cc71cc5-daf2-4546-9267-0f0b77e61671.png)'
- en: 'With our model trained, let''s apply it on our testing set:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练完成后，我们将其应用于测试集：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We would like to plot the output, and see how closely it matches with the original
    input. Remember, the autoencoder should produce output images that are close to
    the original input images.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望绘制输出图像，并看看它与原始输入的匹配程度。记住，自编码器应该生成与原始输入图像相似的输出图像。
- en: 'The following code selects five random images from the testing set and plots
    them on the top row. It then plots the output images for these five randomly selected
    inputs on the bottom row:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从测试集中随机选择五个图像，并将它们绘制在顶部行。然后，它将这五个随机选择的输入的输出图像绘制在底部行：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We''ll see the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/445943f4-d849-4f17-99f4-59ca4d2b5694.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/445943f4-d849-4f17-99f4-59ca4d2b5694.png)'
- en: 'Top: Original images provided to the autoencoder as input; bottom: images output
    from the autoencoder'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上图：提供给自编码器的原始图像作为输入；下图：自编码器输出的图像
- en: 'Wait a minute: the output images look terrible! They look like a blurry white
    scribble and they look nothing like our original input images. Clearly, an autoencoder
    with a hidden layer size of one node is insufficient to encode this dataset. This
    latent representation is too small for our autoencoder to sufficiently capture
    the salient features of our data.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下：输出图像看起来糟透了！它们看起来像模糊的白色涂鸦，完全不像我们的原始输入图像。显然，隐藏层节点数为一个节点的自编码器不足以对这个数据集进行编码。这个潜在表示对于我们的自编码器来说太小，无法充分捕捉我们数据的显著特征。
- en: Effect of hidden layer size on autoencoder performance
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层大小对自编码器性能的影响
- en: Let's try training more autoencoders with different hidden layer sizes and see
    how they fare.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练更多具有不同隐藏层大小的自编码器，看看它们的表现如何。
- en: 'The following code creates and trains five different models with `2`, `4`,
    `8`, `16`, and `32` nodes in the hidden layer:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建并训练五个不同的模型，隐藏层节点数分别为`2`、`4`、`8`、`16`和`32`：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice how each successive model has twice the number of nodes in the hidden
    layer as the preceding model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个连续模型的隐藏层节点数是前一个模型的两倍。
- en: 'Now, let''s train all five of our models together. We use the `verbose=0` argument
    in the `fit()` function to hide the output, as shown in the following code snippet:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们一起训练所有五个模型。我们在`fit()`函数中使用`verbose=0`参数来隐藏输出，如以下代码片段所示：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once training is complete, we apply the trained models on the testing set:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们将训练好的模型应用于测试集：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s plot five randomly selected outputs from each model and see how
    they compare to the original input image:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制每个模型随机选择的五个输出，并看看它们与原始输入图像的对比：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We get the following output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/32751ee1-662f-4725-8b4f-603a5fecce27.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32751ee1-662f-4725-8b4f-603a5fecce27.png)'
- en: Isn't it beautiful? We can clearly see a nice transition as we double the number
    of nodes in the hidden layer. Gradually, we see that the output images become
    clearer and closer to the original input as we increase the number of nodes in
    the hidden layer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 难道这不美吗？我们可以清楚地看到，当我们将隐藏层节点数加倍时，输出图像逐渐变得更清晰，并且越来越接近原始输入图像。
- en: At 32 nodes in the hidden layer, the output becomes very close (though not perfect)
    to the original input. Interestingly, we have shrunk the original input by 24.5
    times (784÷32) and still managed to produce a satisfactory output. That's a pretty
    impressive compression ratio!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐藏层节点数为32时，输出变得非常接近（尽管不完美）原始输入。有趣的是，我们将原始输入压缩了24.5倍（784÷32），仍然能够生成令人满意的输出。这是一个相当令人印象深刻的压缩比！
- en: Denoising autoencoders
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Another interesting application of autoencoders is image denoising. Image noise
    is defined as a random variations of brightness in an image. Image noise may originate
    from the sensors of digital cameras. Although digital cameras these days are capable
    of capturing high quality images, image noise may still occur, especially in low
    light conditions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的另一个有趣应用是图像去噪。图像噪声定义为图像中亮度的随机变化。图像噪声可能源自数字相机的传感器。尽管如今的数字相机能够捕捉高质量的图像，但图像噪声仍然可能发生，尤其是在低光条件下。
- en: Denoising images has been a challenge for researchers for many years. Early
    methods include applying some sort of image filter (that is, mean averaging filter,
    where the pixel value is replaced with the average pixel value of its neighbors)
    over the image. However, such methods can sometimes fall short and the effects
    can be less than ideal.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，去噪图像一直是研究人员的难题。早期的方法包括对图像应用某种图像滤波器（例如，均值平滑滤波器，其中像素值被其邻居的平均像素值替换）。然而，这些方法有时会失效，效果可能不尽如人意。
- en: 'A few years ago, researchers discovered that we can train autoencoders for
    image denoising. The idea is simple. Instead of using the same input and output
    when training conventional autoencoders (as described in the previous section),
    we use a noisy image as the input and a clean reference image for the autoencoder
    to compare its output against. This is illustrated in the following diagram:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，研究人员发现我们可以训练自动编码器进行图像去噪。这个想法很简单。在训练传统自动编码器时（如上一节所述），我们使用相同的输入和输出，而在这里我们使用一张带噪声的图像作为输入，并使用一张干净的参考图像来与自动编码器的输出进行比较。以下图展示了这一过程：
- en: '![](img/1eab68ab-bf04-407d-af27-ff529c41dc0f.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1eab68ab-bf04-407d-af27-ff529c41dc0f.png)'
- en: During the training process, the autoencoder will learn that the noises in the
    image should not be part of the output, and will learn to output a clean image.
    Essentially, we are training our autoencoder to remove noise from images!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，自动编码器将学习到图像中的噪声不应成为输出的一部分，并将学会输出干净的图像。从本质上讲，我们是在训练自动编码器去除图像中的噪声！
- en: 'Let''s start by introducing noise to the MNIST dataset. We''ll add a random
    value between `-0.5` and `0.5` to each pixel in the original images. This has
    the effect of increasing and decreasing the intensity of pixels at random. The
    following code does this using `numpy`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先给MNIST数据集引入噪声。我们将在原始图像的每个像素上加上一个介于`-0.5`和`0.5`之间的随机值。这将随机增加或减少像素的强度。以下代码使用`numpy`来实现这一操作：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we clip the noisy images between `0` and `1` to normalize the images:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将带噪声的图像裁剪到`0`和`1`之间，以便对图像进行归一化：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let's define a basic autoencoder just like we did in the previous section. This
    basic autoencoder has a single hidden layer with `16` nodes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像在上一节那样定义一个基础的自动编码器。这个基础的自动编码器有一个包含`16`个节点的单隐层。
- en: 'The following code creates this autoencoder using the function that we defined
    in the previous section:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用我们在上一节中定义的函数来创建这个自动编码器：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we train our denoising autoencoder. Remember, the input to the denoising
    autoencoder is a noisy image and the output is a clean image. The following code
    trains our basic denoising autoencoder:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练我们的去噪自动编码器。记住，去噪自动编码器的输入是带噪声的图像，输出是干净的图像。以下代码训练我们的基础去噪自动编码器：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once training is done, we apply our denoising autoencoder on the test images:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们将去噪自动编码器应用于测试图像：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We plot the output and compare it with the original image and the noisy image:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制输出并将其与原始图像和带噪声的图像进行比较：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We get the following output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '![](img/595e6fac-c11b-4147-bda1-fb09ccbca711.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/595e6fac-c11b-4147-bda1-fb09ccbca711.png)'
- en: How does it do? Well, it could definitely be better! This basic denoising autoencoder
    is perfectly capable of removing noise, but it doesn't do a very good job at reconstructing
    the original image. We can see that this basic denoising autoencoder sometimes
    fails to separate noise from the digits, especially near the center of the image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 它表现如何？嗯，肯定可以更好！这个基础的去噪自动编码器完全能够去除噪声，但在重建原始图像时并不做得很好。我们可以看到，这个基础的去噪自动编码器有时未能有效区分噪声和数字，尤其是在图像的中心部分。
- en: Deep convolutional denoising autoencoder
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积去噪自动编码器
- en: Can we do better than the basic, one-hidden layer autoencoder? We saw in the
    previous chapter, [Chapter 4](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml), *Cats
    Versus Dogs – Image Classification Using CNNs*, that deep CNNs perform well for
    image classification tasks. Naturally, we can apply the same concept for autoencoders
    too. Instead of using only one hidden layer, we use multiple layers (that is,
    a deep network) and instead of a fully connected dense layer, we use convolutional
    layers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得比基础的单隐层自动编码器更好吗？我们在上一章中看到，[第4章](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml)，*猫与狗
    – 使用CNN进行图像分类*，深度CNN在图像分类任务中表现出色。自然，我们也可以将相同的概念应用于自动编码器。我们不再仅使用一个隐层，而是使用多个隐层（即深度网络），并且不使用全连接的稠密层，而是使用卷积层。
- en: 'The following diagram illustrates the architecture of a deep convolutional
    autoencoder:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了深度卷积自编码器的架构：
- en: '![](img/d03d2b8c-5e5d-486f-81f8-f5a6c1b8213d.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d03d2b8c-5e5d-486f-81f8-f5a6c1b8213d.png)'
- en: Constructing a deep convolutional autoencoder in Keras is simple. Once again,
    we'll use the `Sequential` class in Keras to construct our model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中构建深度卷积自编码器非常简单。我们再次使用Keras中的`Sequential`类来构建我们的模型。
- en: 'First, we define a new `Sequential` class:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个新的`Sequential`类：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let''s add the first two convolutional layers, which act as the encoder
    in our model. There are several parameters we need to define while using the `Conv2D`
    class in Keras:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加作为编码器的前两层卷积层。在使用Keras中的`Conv2D`类时，有几个参数需要定义：
- en: '**Number of filters**: Typically, we use a decreasing number of filters for
    each layer in the encoder. Conversely, we use an increasing number of filters
    for each layer in the decoder. Let''s use 16 filters for the first convolutional
    layer in the encoder and eight filters for the second convolutional layer in the
    encoder. Conversely, let''s use eight filters for the first convolutional layer
    in the decoder and 16 filters for the second convolutional layer in the decoder.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器数量：**通常，在编码器的每一层中，我们使用递减数量的滤波器。相反，在解码器的每一层中，我们使用递增数量的滤波器。我们可以为编码器的第一层卷积层使用16个滤波器，为第二层卷积层使用8个滤波器。相反，我们可以为解码器的第一层卷积层使用8个滤波器，为第二层卷积层使用16个滤波器。'
- en: '**Filter size: **As shown in the previous chapter, [Chapter 4](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml),
    *Cats Versus Dogs – Image Classification Using CNNs*, a filter size of 3 x 3 is
    typical for convolutional layers.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器大小：**如上一章所示，[第4章](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml)，*猫狗对战——使用CNN进行图像分类*，卷积层通常使用3
    x 3的滤波器大小。'
- en: '**Padding: **For autoencoders, we use a same padding. This ensures that the
    height and width of successive layers remains the same. This is useful because
    we need to ensure that the dimensions of the final output is the same as the input.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充：**对于自编码器，我们使用相同的填充方式。这确保了连续层的高度和宽度保持不变。这一点非常重要，因为我们需要确保最终输出的维度与输入相同。'
- en: 'The following code snippet adds the first two convolutional layers with the
    aforementioned parameters to our model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将前述参数添加到模型中，包含前两层卷积层：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we'll add the decoder layers onto our model. Just like the encoder layers,
    the decoder layers are also convolutional layers. The only difference is that,
    in the decoder layers, we use an increasing number of filters after each successive
    layer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解码器层添加到模型中。与编码器层类似，解码器层也是卷积层。唯一的不同是，在解码器层中，我们在每一层后使用递增数量的滤波器。
- en: 'The following code snippet adds the next two convolutional layers as the decoder:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段添加了作为解码器的两个卷积层：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finally, we add the output layer to our model. The output layer should be a
    convolutional layer with only one filter, as we are trying to output a 28 x 28
    x 1 image. The `Sigmoid` function is used as the activation function for the output
    layer.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们向模型中添加输出层。输出层应该是一个只有一个滤波器的卷积层，因为我们要输出一个28 x 28 x 1的图像。`Sigmoid`函数被用作输出层的激活函数。
- en: 'The following code adds the final output layer:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码添加了最终的输出层：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s take a look at the structure of the model to make sure that it is consistent
    with what was shown in the diagram earlier. We can do so by calling the `summary()`
    function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型的结构，以确保它与前面图示中展示的一致。我们可以通过调用`summary()`函数来实现：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We get the following output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/dbaf5d0f-67fe-4d7b-a471-6805b614af25.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dbaf5d0f-67fe-4d7b-a471-6805b614af25.png)'
- en: 'We are now ready to train our deep convolutional autoencoder. As usual, we
    define the training process under the `compile` function and call the `fit` function,
    as shown in the following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练我们的深度卷积自编码器。像往常一样，我们在`compile`函数中定义训练过程，并调用`fit`函数，如下所示：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once training is done, we''ll get the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们将得到以下输出：
- en: '![](img/12f069db-0d87-4491-8924-82ef829e91e8.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12f069db-0d87-4491-8924-82ef829e91e8.png)'
- en: 'Let''s use the trained model on the testing set:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试集上使用训练好的模型：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: It will be interesting to see how this deep convolutional autoencoder performs
    on the testing set. Remember, the testing set represents images that the model
    has never seen before.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这个深度卷积自编码器在测试集上的表现将会很有趣。记住，测试集代表了模型从未见过的图像。
- en: 'We plot the output and compare it with the original image and the noisy image:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制输出并将其与原始图像和噪声图像进行比较：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We get the following output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/c61e9229-97c3-4945-afd2-a897ad928f41.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c61e9229-97c3-4945-afd2-a897ad928f41.png)'
- en: Isn't that amazing? The denoised output from our deep convolutional autoencoder
    is so good that we can barely differentiate the original images and the denoised
    output.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这难道不令人惊讶吗？我们的深度卷积自编码器的去噪输出效果如此出色，以至于我们几乎无法分辨原始图像和去噪后的输出。
- en: Despite the impressive results, it is important to keep in mind that the convolutional
    model that we used is pretty simple. The advantage of deep neural networks is
    that we can always increase the complexity of the model (that is, more layers
    and more filters per layer) and use it on more complex datasets. This ability
    to scale is one of the main advantages of deep neural networks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果令人印象深刻，但需要记住的是，我们使用的卷积模型相对简单。深度神经网络的优势在于我们总是可以增加模型的复杂性（即更多的层和每层更多的滤波器），并将其应用于更复杂的数据集。这个扩展能力是深度神经网络的主要优势之一。
- en: Denoising documents with autoencoders
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器去噪文档
- en: So far, we have applied our denoising autoencoder on the MNIST dataset, which
    is a pretty simple dataset. Let's take a look now at a more complicated dataset,
    which better represents the challenges of denoising documents in real life.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在MNIST数据集上应用了去噪自编码器，该数据集相对简单。现在让我们看看一个更复杂的数据集，它更好地代表了现实生活中去噪文档所面临的挑战。
- en: The dataset that we will be using is provided for free by the **University of
    California Irvine** (**UCI**). For more information on the dataset, you can visit
    UCI's website at [https://archive.ics.uci.edu/ml/datasets/NoisyOffice](https://archive.ics.uci.edu/ml/datasets/NoisyOffice).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是由**加利福尼亚大学欧文分校**（**UCI**）免费提供的。有关数据集的更多信息，请访问UCI的网站：[https://archive.ics.uci.edu/ml/datasets/NoisyOffice](https://archive.ics.uci.edu/ml/datasets/NoisyOffice)。
- en: The dataset can be found in the accompanying GitHub repository for this book.
    For more information on downloading the code and dataset for this chapter from
    the GitHub repository, please refer to the *Technical requirements* section earlier
    in the chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在本书的配套GitHub仓库中找到。有关如何从GitHub仓库下载本章的代码和数据集的更多信息，请参阅本章前面的*技术要求*部分。
- en: The dataset consists of 216 different noisy images. The noisy images are scanned
    office documents that are tainted by coffee stains, wrinkled marks, and other
    sorts of defects that are typical in office documents. For every noisy image,
    a corresponding reference clean image is provided, which represents the office
    document in an ideal noiseless state.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含216张不同的噪声图像。这些噪声图像是扫描的办公室文档，受到咖啡渍、皱痕和其他典型的办公室文档缺陷的污染。对于每张噪声图像，提供了一张对应的干净图像，表示理想的无噪声状态下的办公室文档。
- en: 'Let''s take a look at the dataset to have a better idea of what we are working
    with. The dataset is located at the following folder:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下数据集，深入了解我们正在处理的内容。数据集位于以下文件夹：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `Noisy_Documents` folder contains two subfolders (`noisy` and `clean`),
    which contains the noisy and clean images, respectively.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`Noisy_Documents`文件夹包含两个子文件夹（`noisy`和`clean`），分别包含噪声图像和干净图像。'
- en: To load the `.png` images into Python, we can use the `load_img` function provided
    by Keras. To convert the loaded images into a `numpy` array, we use the `img_to_array`
    function in Keras.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要将`.png`图像加载到Python中，我们可以使用Keras提供的`load_img`函数。为了将加载的图像转换为`numpy`数组，我们使用Keras中的`img_to_array`函数。
- en: 'The following code imports the noisy `.png` images in the `/Noisy_Documents/noisy/`
    folder into a `numpy` array:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将位于`/Noisy_Documents/noisy/`文件夹中的噪声`.png`图像导入到`numpy`数组中：
- en: '[PRE38]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To verify that our images are loaded properly into the `numpy` array, let''s
    print the dimensions of the array:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的图像是否正确加载到`numpy`数组中，让我们打印数组的维度：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We get the following output:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/72d59ff5-918d-4594-90f7-acc0d445a78a.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72d59ff5-918d-4594-90f7-acc0d445a78a.png)'
- en: We can see that there are 216 images in the array, each with dimensions 420
    x 540 x 1 (width x height x number of channels for each image).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，数组中有216张图像，每张图像的维度为420 x 540 x 1（宽度 x 高度 x 每张图像的通道数）。
- en: 'Let''s do the same for the clean images. The following code imports the clean `.png` images
    in the `/Noisy_Documents/clean/` folder into a `numpy` array:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对干净的图像执行相同操作。以下代码将位于`/Noisy_Documents/clean/`文件夹中的干净`.png`图像导入到`numpy`数组中：
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s display the loaded images to have a better idea of the kind of images
    we are working with. The following code randomly selects `3` images and plots
    them, as shown in the following code:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示加载的图像，以便更好地了解我们正在处理的图像类型。以下代码随机选择`3`张图像并绘制它们，如下所示：
- en: '[PRE41]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We get the output as shown in the following screenshot:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下截图中的输出：
- en: '![](img/b9cc19ed-e4e3-4d64-b155-b223743a8158.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9cc19ed-e4e3-4d64-b155-b223743a8158.png)'
- en: We can see that the kind of noise in this dataset is markedly different from
    what we saw in the MNIST dataset. The noise in this dataset are random artifacts
    that appear throughout the image. Our autoencoder model needs to have a strong
    understanding of signal versus noise in order to successfully denoise this dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个数据集中噪声的类型与MNIST数据集中看到的显著不同。这个数据集中的噪声是随机的伪影，遍布整个图像。我们的自编码器模型需要能够清楚地理解信号与噪声的区别，才能成功去噪这个数据集。
- en: 'Before we proceed to train our model, let''s split our dataset into a training
    and testing set, as shown in the following code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练模型之前，让我们将数据集分成训练集和测试集，如下代码所示：
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Basic convolutional autoencoder
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本卷积自编码器
- en: We're now ready to tackle the problem. Let's start with a basic model to see
    how far we can go with it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好解决这个问题了。让我们从一个基本模型开始，看看能走多远。
- en: 'As always, we define a new `Sequential` class:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们定义一个新的`Sequential`类：
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we add a single convolutional layer as our encoder layer:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个卷积层作为编码器层：
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We add a single convolutional layer as our decoder layer:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加一个卷积层作为解码器层：
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we add an output layer:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一个输出层：
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s check the structure of the model:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看模型的结构：
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We get the output as shown in the following screenshot:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下截图中的输出：
- en: '![](img/77fcef33-83cf-442d-8617-b55c2142bb42.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77fcef33-83cf-442d-8617-b55c2142bb42.png)'
- en: 'Here''s the code to train our basic convolutional autoencoder:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练我们基本卷积自编码器的代码：
- en: '[PRE48]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once the training is done, we apply our model on the testing set:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们将模型应用到测试集上：
- en: '[PRE49]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s plot the output and see what kind of results we got. The following code
    plots the original noisy images in the left column, the original clean images
    in the middle column, and the denoised image output from our model in the right
    column:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制输出，看看得到的结果。以下代码在左列绘制原始噪声图像，在中列绘制原始干净图像，并在右列绘制从我们模型输出的去噪图像：
- en: '[PRE50]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We get the output as shown in the following screenshot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下截图中的输出：
- en: '![](img/a3bd8afd-8513-4ca9-88ed-9f890c713566.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3bd8afd-8513-4ca9-88ed-9f890c713566.png)'
- en: Well, our model can certainly do a better job. The denoised images tend to have
    a gray background rather than a white background in the true `Clean Images`. The
    model also does a poor job at removing the coffee stains from the `Noisy Images`.
    Furthermore, the words in the denoised images are faint, showing that the model
    struggles at this task.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我们的模型确实能做得更好。去噪后的图像通常有灰色背景，而不是`Clean Images`中的白色背景。模型在去除`Noisy Images`中的咖啡渍方面也做得不太好。此外，去噪后的图像中的文字较为模糊，显示出模型在此任务上的困难。
- en: Deep convolutional autoencoder
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积自编码器
- en: Let's try denoising the images with a deeper model and more filters in each
    convolutional layer.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用更深的模型和每个卷积层中更多的滤波器来去噪图像。
- en: 'We start by defining a new `Sequential` class:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个新的`Sequential`类：
- en: '[PRE51]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we add three convolutional layers as our encoder, with `32`, `16`, and
    `8` filters:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加三个卷积层作为编码器，使用`32`、`16`和`8`个滤波器：
- en: '[PRE52]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Similarly for the decoder, we add three convolutional layers with `8`, `16`,
    and `32` filters:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于解码器，我们添加三个卷积层，使用`8`、`16`和`32`个滤波器：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Finally, we add an output layer:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一个输出层：
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s check the structure of our model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下模型的结构：
- en: '[PRE55]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We get the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/4451482c-aee1-4535-ba4c-85239ccd27fb.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4451482c-aee1-4535-ba4c-85239ccd27fb.png)'
- en: From the preceding output, we can see that there are 12,785 parameters in our
    model, which is approximately 17 times more than the basic model we used in the
    previous section.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到模型中有12,785个参数，大约是我们在上一节使用的基本模型的17倍。
- en: 'Let''s train the model and apply it on the testing images:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型并将其应用于测试图像：
- en: Caution
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The following code may take some time to run if you are not using Keras with
    a GPU. If the model is taking too long to train, you may reduce the number of
    filters in each convolutional layer in the model.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可能需要一些时间运行，如果你没有使用带GPU的Keras。若模型训练时间过长，你可以减少每个卷积层中的滤波器数量。
- en: '[PRE56]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, we plot the output to see the kind of results we get. The following
    code plots the original noisy images in the left column, the original clean images
    in the middle column, and the denoised image output from our model in the right
    column:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制输出结果，以查看得到的结果类型。以下代码将原始噪声图像显示在左列，原始干净图像显示在中列，模型输出的去噪图像显示在右列：
- en: '[PRE57]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We get the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/03364997-57db-4feb-b88c-3ce300cf7b0d.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03364997-57db-4feb-b88c-3ce300cf7b0d.png)'
- en: The result looks amazing! In fact, the output denoised images look so good that
    we can barely differentiate them from the true clean images. We can see that the
    coffee stain has been almost entirely removed and the noise from the crumpled
    paper is non-existent in the denoised image. Furthermore, the words in the denoised
    images look sharp and clear, and we can easily read the words in the denoised
    images.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来很棒！实际上，去噪后的图像看起来非常好，我们几乎无法区分它们与真实的干净图像。我们可以看到，咖啡渍几乎完全被去除了，揉皱纸张的噪声在去噪图像中消失了。此外，去噪图像中的文字清晰锐利，我们可以轻松地阅读去噪图像中的文字。
- en: This dataset truly demonstrates the power of autoencoders. By adding on additional
    complexity in the form of deeper convolutional layers and more filters, the model
    is able to differentiate the signal from the noise, allowing it to successfully
    denoise images that are heavily corrupted.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集真正展示了自编码器的强大功能。通过增加更多复杂性，例如更深的卷积层和更多的滤波器，模型能够区分信号与噪声，从而成功去除严重损坏的图像噪声。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at autoencoders, a class of neural networks that
    learn the latent representation of input images. We saw that all autoencoders
    have an encoder and decoder component. The role of the encoder is to encode the
    input to a learned, compressed representation and the role of the decoder is to
    reconstruct the original input using the compressed representation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了自编码器，这是一类学习输入图像潜在表示的神经网络。我们看到所有自编码器都有一个编码器和解码器组件。编码器的作用是将输入编码成一个学习到的压缩表示，而解码器的作用是使用这个压缩表示重构原始输入。
- en: We first looked at autoencoders for image compression. By training an autoencoder
    with identical input and output, the autoencoder learns the most salient features
    of the input. Using MNIST images, we constructed an autoencoder with a 24.5 times
    compression rate. Using this learned 24.5x compressed representation, the autoencoder
    is able to successfully reconstruct the original input.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先研究了用于图像压缩的自编码器。通过训练一个输入和输出相同的自编码器，自编码器能够学习输入的最显著特征。使用MNIST图像，我们构建了一个压缩率为24.5倍的自编码器。利用这个学习到的24.5倍压缩表示，自编码器能够成功地重构原始输入。
- en: Next, we looked at denoising autoencoders. By training an autoencoder with noisy
    images as input and clean images as output, the autoencoder is able to pick out
    the signal from the noise, and is able to successfully denoise the noisy image.
    We trained a deep convolutional autoencoder, and the autoencoder was able to successfully
    denoise documents with coffee stains and other sorts of image corruptions. The
    results were impressive, with the autoencoder removing almost all of the noise
    in the noisy documents, producing an output that is almost identical to the true
    clean images.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了去噪自编码器。通过训练一个以噪声图像为输入、干净图像为输出的自编码器，自编码器能够从噪声中提取信号，并成功地去除噪声图像的噪声。我们训练了一个深度卷积自编码器，该自编码器能够成功去除咖啡渍和其他类型的图像损坏。结果令人印象深刻，去噪自编码器几乎去除了所有噪声，输出几乎与真实的干净图像完全相同。
- en: In the next chapter, [Chapter 6](21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml), *Sentiment
    Analysis of Movie Reviews Using LSTM* we'll use a **long short-term memory** (**LSTM**)
    neural network to predict the sentiment of movie reviews.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第6章](21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml)，*使用LSTM进行电影评论情感分析*，我们将使用**长短期记忆**（**LSTM**）神经网络来预测电影评论的情感。
- en: Questions
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How are autoencoders different from a conventional feed forward neural network?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器与传统的前馈神经网络有什么不同？
- en: Autoencoders are neural networks that learn a compressed representation of the
    input, known as the latent representation. They are different from conventional
    feed forward neural networks because their structure consists of an encoder and
    a decoder component, which is not present in CNNs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是学习输入压缩表示的神经网络，这种表示被称为潜在表示。它们不同于传统的前馈神经网络，因为自编码器的结构包含一个编码器和一个解码器组件，而这些组件在CNN中是不存在的。
- en: What happens when the latent representation of the autoencoder is too small?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当自编码器的潜在表示过小时会发生什么？
- en: The size of the latent representation should be sufficiently *small* enough
    to represent a compressed representation of the input, and also be sufficiently *large*
    enough for the decoder to reconstruct the original image without too much loss.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在表示的大小应该足够*小*，以便表示输入的压缩表示，同时又要足够*大*，使解码器能够重建原始图像而不会丢失太多信息。
- en: What are the input and output when training a denoising autoencoder?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练去噪自编码器时的输入和输出是什么？
- en: The input to a denoising autoencoder should be a noisy image and the output
    should be a reference clean image. During the training process, the autoencoder
    learns that the output should not contain any noise (through the `loss` function),
    and the latent representation of the autoencoder should only contain the signals
    (that is, non-noise elements)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器的输入应为一张带噪声的图像，输出应为一张干净的参考图像。在训练过程中，自编码器通过`loss`函数学习输出不应包含任何噪声，并且自编码器的潜在表示应该只包含信号（即非噪声元素）。
- en: What are some of the ways we can improve the complexity of denoising autoencoders?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过哪些方法提高去噪自编码器的复杂度？
- en: For denoising autoencoders, convolutional layers always work better than dense
    layers, just as CNNs work better than conventional feed forward neural networks
    for image classification tasks. We can also improve the complexity of our model
    by building a deeper network with more layers, and by using more filters in each
    convolutional layer.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于去噪自编码器，卷积层总是比全连接层表现更好，就像卷积神经网络（CNN）在图像分类任务中比传统的前馈神经网络表现更佳一样。我们还可以通过构建一个更深的网络（增加更多的层数），并在每个卷积层中使用更多的滤波器，来提高模型的复杂度。
