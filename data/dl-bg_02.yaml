- en: Introduction to Machine Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: You have probably heard the term **Machine Learning** (**ML**) or **Artificial
    Intelligence** (**AI**) frequently in recent years, especially **Deep Learning**
    (**DL**). It may be the reason you decided to invest in this book and get to know
    more. Given some new, exciting developments in the area of neural networks, DL
    has come to be a hot area in ML. Today, it is difficult to imagine a world without
    quick text translation between languages, or without fast song identification.
    These, and many other things, are just the tip of the iceberg when it comes to
    the potential of DL to change your world. When you finish this book, we hope you
    will join the bus and ride along with amazing new applications and projects based
    on DL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能近年来常常听到**机器学习**（**ML**）或**人工智能**（**AI**）这个术语，尤其是**深度学习**（**DL**）。这可能是您决定投资本书并深入了解更多的原因。随着神经网络领域一些新的激动人心的进展，深度学习已成为机器学习中的热门领域。如今，难以想象没有快速的语言间文本翻译，或者没有快速的歌曲识别的世界。这些，以及许多其他事情，都是深度学习改变世界潜力的冰山一角。当您读完本书后，我们希望您能加入这个行列，搭乘基于深度学习的新应用和项目的快车。
- en: This chapter briefly introduces the field of ML and how it is used to solve
    common problems. Throughout this chapter, you will be driven to understand the
    basic concepts of ML, the research questions addressed, and their significance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了机器学习领域及其如何用于解决常见问题。在本章中，您将深入理解机器学习的基本概念、所涉及的研究问题以及它们的重要性。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Diving into the ML ecosystem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解机器学习生态系统
- en: Training ML algorithms from data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据中训练机器学习算法
- en: Introducing deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Why is deep learning important today?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么深度学习在今天如此重要？
- en: Diving into the ML ecosystem
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解机器学习生态系统
- en: 'From the typical ML application process depicted in *Figure 1.1*, you can see
    that ML has a broad range of applications. However, ML algorithms are only a small
    part of a bigger ecosystem with a lot of moving parts, and yet ML is transforming
    lives around the world today:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图1.1*中描述的典型机器学习应用流程来看，您可以看到机器学习有广泛的应用。然而，机器学习算法只是更大生态系统中一个小部分，这个生态系统有许多活动的组件，但机器学习今天正在改变世界各地的生活：
- en: '![](img/238398f2-bdc7-42c4-a046-5fb6057cf064.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/238398f2-bdc7-42c4-a046-5fb6057cf064.png)'
- en: Figure 1.1 - ML ecosystem. ML interacts with the world through several stages
    of data manipulation and interpretation to achieve an overall system integration
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 - 机器学习生态系统。机器学习通过多个数据处理和解释阶段与世界互动，以实现整体系统集成
- en: Deployed ML applications usually start with a process of data collection that
    uses sensors of different types, such as cameras, lasers, spectroscopes, or other
    types of direct access to data, including local and remote databases, big or small.
    In the simplest of cases, input can be gathered through a computer keyboard or
    smartphone screen taps. At this stage, the data collected or sensed is considered
    to be raw data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的机器学习应用通常从数据收集过程开始，使用不同类型的传感器，例如摄像头、激光器、光谱仪或其他直接获取数据的方式，包括本地和远程数据库，不论其大小。在最简单的情况下，输入可以通过计算机键盘或智能手机屏幕点击收集。在此阶段，收集到或感知到的数据被视为原始数据。
- en: 'Raw data is usually preprocessed before presenting it to an ML model. Raw data is
    rarely the actual input to ML algorithms, unless the ML model is meant to find
    a rich representation of the raw data, and later be used as input to another ML
    algorithm. In other words, there are some ML algorithms that are specifically
    used as preprocessing agents and they are not at all related to a main ML model
    that will classify or regress on the preprocessed data. In a general sense, this
    data preprocessing stage aims to convert raw data into arrays or matrices with
    specific data types. Some popular preprocessing strategies include the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据通常会在输入到机器学习模型之前进行预处理。原始数据很少是机器学习算法的实际输入，除非该机器学习模型旨在从原始数据中找到丰富的表示形式，然后将其用作另一个机器学习算法的输入。换句话说，有些机器学习算法专门作为预处理工具使用，它们与将要对预处理数据进行分类或回归的主机器学习模型无关。一般来说，这一数据预处理阶段的目标是将原始数据转换为具有特定数据类型的数组或矩阵。一些常见的预处理策略包括：
- en: Word-to-vector conversions, for example, using GloVe or Word2Vec
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词向量转换，例如，使用GloVe或Word2Vec
- en: Sequence-to-vector or sequence-to-matrix strategies
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到向量或序列到矩阵策略
- en: Value range normalization, for example, (0, 255) to (0.0, 1.0)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值范围归一化，例如，将(0, 255)转换为(0.0, 1.0)
- en: Statistical value normalization, for example, to have zero mean and unit variance
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计值归一化，例如，调整为零均值和单位方差
- en: Once these preprocessing measures take place, most ML algorithms can use the
    data. However, it must be noted that the preprocessing stage is not trivial, it
    requires advanced knowledge and skills with respect to operating systems and sometimes
    even electronics. In a general sense, a real ML application has a long pipeline
    touching different aspects of computer science and engineering.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些预处理措施完成，大多数机器学习算法就可以使用这些数据。然而，必须指出，预处理阶段并非简单，它需要对操作系统的高级知识和技能，有时甚至需要电子学方面的知识。从一般意义上讲，一个真正的机器学习应用有一个长期的流程，涉及计算机科学和工程的不同方面。
- en: The processed data is what you will usually see in books like the one you are
    reading right now. The reason is that we need to focus on deep learning instead
    of data processing. If you wish to be more knowledgeable in this area, you could
    read data science materials such as Ojeda, T. *et.al.* 2014 or Kane, F. 2017.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的数据通常是你在像现在你正在阅读的这本书中看到的内容。原因是我们需要关注深度学习，而不是数据处理。如果你希望在这个领域有更深入的了解，可以阅读数据科学方面的资料，如Ojeda,
    T. *et.al.* 2014 或 Kane, F. 2017。
- en: 'Mathematically speaking, the processed data as a whole is referred to using
    the uppercase, bold font, letter ***X***, which has ***N*** rows (or data points).
    If we want to refer to the specific *i*-th element (or row) of the dataset, we
    would do that by writing: ***X[i]***. The dataset will have *d* columns and they
    are usually called features*.* One way to think about the features is as dimensions.
    For example, if the dataset has two features, height and weight, then you could
    represent the entire dataset using a two-dimensional plot. The first dimension, ***x[1]***,
    (height) can be the horizontal axis, while the second dimension, ***x[2]***, (weight)
    can be the vertical axis, as depicted in *Figure 1.2*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，处理过的数据整体上用大写粗体字母***X***表示，其中有***N***行（或数据点）。如果我们想引用数据集中的特定*i*元素（或行），我们可以写作：***X[i]***。该数据集有*d*列，通常被称为特征*。*
    一种理解特征的方法是将其看作维度。例如，如果数据集有两个特征，身高和体重，那么你可以用二维图表示整个数据集。第一维，***x[1]***（身高），可以作为横轴，而第二维，***x[2]***（体重），可以作为纵轴，如*图1.2*所示：
- en: '![](img/c6a8a698-ba57-462e-9c70-0d97094963cd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6a8a698-ba57-462e-9c70-0d97094963cd.png)'
- en: Figure 1.2 - Sample two-dimensional data
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 - 示例二维数据
- en: During production, when the data is presented to an ML algorithm, a series of
    tensor products and additions will be executed. Such vectorial operations are
    usually transformed or normalized using non-linear functions. This is then followed
    by more products and additions, more non-linear transformations, temporary storage
    of intermediate values, and finally producing the desired output that corresponds
    to the input. For now, you can think of this process as an ML black box that will
    be revealed as you continue reading.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产过程中，当数据呈现给机器学习算法时，会执行一系列张量乘法和加法。这些向量运算通常通过非线性函数进行转换或归一化。接下来会进行更多的乘法和加法，更多的非线性变换，临时存储中间值，最后产生与输入相对应的期望输出。现在，你可以将这个过程看作是一个机器学习的“黑箱”，随着你继续阅读，黑箱会逐步揭示出来。
- en: The output that the ML produces in correspondence to the input usually requires
    some type of interpretation, for example, if the output is a vector of probabilities
    of objects being classified to belong to a group or to another, then that may
    need to be interpreted. You may need to know how low the probabilities are in
    order to account for uncertainty, or you may need to know how different are the
    probabilities to account for even more uncertainty. The output processing serves
    as the connecting factor between ML and the decision-making world through the
    use of business rules. These rules can be, for example, *if-then* rules such as,
    "If the predicted probability of the maximum is twice as large as the second maximum,
    then issue a prediction; otherwise, do not proceed to make a decision." Or they
    can be formula-based rules or more complex systems of equations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习根据输入产生的输出通常需要某种形式的解释。例如，如果输出是一个对象分类为某一组或另一组的概率向量，那么可能需要进行解释。你可能需要了解概率有多低，以便考虑不确定性，或者你可能需要了解概率之间有多大差异，以便考虑更大的不确定性。输出处理充当了机器学习与决策世界之间的连接因素，通过业务规则的使用。这些规则可以是*if-then*规则，例如：“如果最大预测概率是第二大预测概率的两倍，那么发出预测；否则，不继续做出决策。”或者它们可以是基于公式的规则，或更复杂的方程组系统。
- en: Finally, in the decision-making stage, the ML algorithm is ready to interact
    with the world by turning on a light bulb through an actuator, or to buy stock
    if the prediction is not uncertain, by alerting a manager that the company will
    run out of inventory in three days and they need to buy more items, or by sending
    an audio message to a smartphone speaker saying, "Here is the route to the movie
    theater" and opening a maps application through an **application programming interface**
    (**API**) call or **operating system** (**OS**) commands.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在决策阶段，机器学习算法准备与世界互动，通过执行器开启一个灯泡，或者在预测不确定时购买股票，警告经理公司将在三天内用完库存，需要购买更多商品，或者通过**应用程序编程接口**（**API**）调用或**操作系统**（**OS**）命令向智能手机扬声器发送一条音频信息，说“这里是去电影院的路线”，并打开地图应用程序。
- en: This is a broad overview of the world of ML systems when they are in production.
    However, this assumes that the ML algorithm is properly trained and tested, which
    is the easy part, trust me. At the end of the book, you will be skilled in training
    highly complex, deep learning algorithms but, for now, let us introduce the generic
    training process.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习系统在生产环境中的广泛概述。然而，这假设机器学习算法已经正确训练和测试过，而这其实是最容易的部分，相信我。书的结尾，你将能够熟练训练高度复杂的深度学习算法，但现在，让我们先介绍通用的训练过程。
- en: Training ML algorithms from data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中训练机器学习算法
- en: 'A typical preprocessed dataset is formally defined as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的预处理数据集正式定义如下：
- en: '![](img/92150313-7b8e-4825-ace3-647ebae682c9.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92150313-7b8e-4825-ace3-647ebae682c9.png)'
- en: Where *y* is the desired output corresponding to the input vector **x**. So,
    the motivation of ML is to use the data to find linear and non-linear transformations
    over **x** using highly complex tensor (vector) multiplications and additions,
    or to simply find ways to measure similarities or distances among data points,
    with the ultimate purpose of predicting ***y*** given **x**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*是与输入向量**x**对应的期望输出。所以，机器学习的动机是利用数据，通过复杂的张量（向量）乘法和加法，或者简单地通过测量数据点之间的相似度或距离，来找到对**x**的线性和非线性变换，最终的目的是根据**x**预测***y***。
- en: 'A common way of thinking about this is that we want to approximate some unknown
    function over **x**:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的思考方式是，我们希望对**x**进行某个未知函数的近似：
- en: '![](img/8cc3eeec-c6d9-4f63-84e3-00b63c6dfad2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cc3eeec-c6d9-4f63-84e3-00b63c6dfad2.png)'
- en: Where ***w*** is an unknown vector that facilitates the transformation of **x**
    along with ***b***. This formulation is very basic, linear, and is simply an illustration
    of what a simple learning model would look like. In this simple case, the ML algorithms
    revolve around finding the best ***w*** and ***b*** that yields the closest (if
    not perfect) approximation to ***y***, the desired output. Very simple algorithms
    such as the perceptron (Rosenblatt, F. 1958) try different values for ***w***
    and *b* using past mistakes in the choices of **w** and ***b*** to make the next
    selection in proportion to the mistakes made.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***w***是一个未知的向量，帮助**x**与***b***一起进行变换。这个公式非常基础、线性，仅仅是展示一个简单学习模型的样子。在这个简单的例子中，机器学习算法的核心是找到最佳的***w***和***b***，使得它们能够提供最接近（如果不是完美的话）***y***，即期望输出的结果。像感知机（Rosenblatt,
    F. 1958）这样非常简单的算法，通过过去在选择**w**和***b***时犯的错误，尝试不同的***w***和***b***值，从而根据错误的比例来选择下一步的参数。
- en: A combination of perceptron-like models that look at the same input, intuitively,
    turned out to be better than single ones. Later, people realized that having them
    stacked may be the next logical step leading to multilayer perceptrons, but the
    problem was that the learning process was rather complicated for people in the
    1970s. These kinds of multilayered systems were analog to brain neurons, which
    is the reason we call them neural networks today. With some interesting discoveries
    in ML, new specific kinds of neural networks and algorithms were created known
    as deep learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一种结合感知机模型的方式，它们从直觉上看同一输入，结果证明比单一模型更好。后来，人们意识到将它们堆叠在一起可能是通向多层感知机的下一步，但问题是，1970年代的人们认为学习过程过于复杂。这些多层系统类似于大脑神经元，这也是我们今天称它们为神经网络的原因。随着机器学习领域的一些有趣发现，出现了新的特定种类的神经网络和算法，被称为深度学习。
- en: Introducing deep learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入深度学习
- en: While a more detailed discussion of learning algorithms will be addressed in
    [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning from Data*,
    in this section, we will deal with the fundamental concept of a neural network
    and the developments that led to deep learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关于学习算法的更详细讨论将在 [第 4 章](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml)《从数据中学习》中进行，本节将讨论神经网络的基本概念以及促使深度学习发展的相关内容。
- en: The model of a neuron
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元的模型
- en: The human brain has input connections from other neurons (synapses) that receive
    stimuli in the form of electric charges, and then has a nucleus that depends on
    how the input stimulates the neuron that can trigger the neuron's activation**. **At
    the end of the neuron, the output signal is propagated to other neurons through
    dendrites, thus forming a network of neurons.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑通过其他神经元（突触）接收来自外界的电荷刺激，然后具有一个核，根据输入的刺激来触发神经元的激活**。** 在神经元的末端，输出信号通过树突传播到其他神经元，从而形成神经元网络。
- en: 'The analogy of the human neuron is depicted in *Figure 1.3*, where the input
    is represented with the vector ***x***, the activation of the neuron is given
    by some function **z(.)**, and the output is ***y***. The parameters of the neuron
    are **w** and ***b***:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 人类神经元的类比如图 *1.3* 所示，其中输入由向量 ***x*** 表示，神经元的激活由某个函数 **z(.)** 给出，输出为 ***y***。神经元的参数为
    **w** 和 ***b***：
- en: '![](img/f17064ac-8bd1-46e3-832f-ecdc2b9dad6b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f17064ac-8bd1-46e3-832f-ecdc2b9dad6b.png)'
- en: Figure 1.3 - The basic model of a neuron
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 - 神经元的基本模型
- en: 'The trainable parameters of a neuron are ***w*** and ***b***, and they are
    unknown. Thus, we can use training data ![](img/c380f821-4bbe-426d-9996-1819f9a61000.png) to
    determine these parameters using some learning strategy. From the picture, **x**[**1** ]multiplies ***w[1]***,
    then ***x***[**2** ]multiplies ***w[2]***, and ***b***is multiplied by 1; all
    these products are added, which can be simplified as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的可训练参数是 ***w*** 和 ***b***，它们是未知的。因此，我们可以使用训练数据 ![](img/c380f821-4bbe-426d-9996-1819f9a61000.png)
    来通过某种学习策略来确定这些参数。从图中可以看出，**x**[**1**] 乘以 ***w[1]***，然后 ***x***[**2**] 乘以 ***w[2]***，并且
    ***b*** 乘以 1；所有这些乘积相加，可以简化为：
- en: '![](img/a57b2bc2-0ad9-44cd-bbc2-42618a80eb84.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a57b2bc2-0ad9-44cd-bbc2-42618a80eb84.png)'
- en: 'The activation function operates as a way to ensure the output is within the
    desired output range. Let''s say that we want a simple linear activation, then
    the function **z****(.)** is non-existing or can be bypassed, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的作用是确保输出在所需的输出范围内。假设我们希望使用简单的线性激活，那么函数 **z**(.) 就不存在，或者可以跳过，如下所示：
- en: '![](img/6182f132-99da-4278-9242-57133e23fb53.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6182f132-99da-4278-9242-57133e23fb53.png)'
- en: 'This is usually the case when we want to solve a regression problem and the
    output data can have a range from -∞ to +∞. However, we may want to train the
    neuron to determine whether a vector ***x*** belongs to one of two classes, say
    -1 and +1\. Then we would be better suited using a function called a sign activation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常发生在我们想要解决回归问题时，输出数据的范围可以从 -∞ 到 +∞。然而，我们可能希望训练神经元来判断一个向量 ***x*** 是否属于两类之一，比如
    -1 和 +1。那么，使用一种叫做符号激活的函数可能更为合适：
- en: '![](img/1a254e0a-49c7-42fb-aeb6-0fe282bd0c34.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a254e0a-49c7-42fb-aeb6-0fe282bd0c34.png)'
- en: 'Where the *sign*(.) function is denoted as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *sign*(.) 函数表示如下：
- en: '![](img/d0c512a7-95b3-40f9-b4cc-d3e4fdb35a88.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0c512a7-95b3-40f9-b4cc-d3e4fdb35a88.png)'
- en: There are many other activation functions, but we will introduce those later
    on. For now, we will briefly show one of the simplest learning algorithms, the **perceptron
    learning algorithm** (**PLA**).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的激活函数，但我们稍后会介绍。现在，我们将简要展示其中一个最简单的学习算法，即 **感知机学习算法** (**PLA**)。
- en: The perceptron learning algorithm
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机学习算法
- en: 'The PLA begins from the assumption that you want to classify data, **X**, into
    two different groups, the positive group (+) and the negative group (-). It will
    find *some ***w **and *b* by training to predict the corresponding correct labels
    ***y**.* The PLA uses the *sign*( . ) function as the activation. Here are the
    steps that the PLA follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PLA 从假设你希望将数据 **X** 分类为两类：正类 (+) 和负类 (-) 开始。它将通过训练找到 *某些 ***w*** 和 *b* 来预测相应的正确标签
    ***y***。PLA 使用 *sign*(.) 函数作为激活函数。以下是 PLA 执行的步骤：
- en: Initialize **w** to zeros, and iteration counter *t* = 0
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 **w** 为零，迭代计数器 *t* = 0
- en: 'While there are any incorrectly classified examples:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当存在任何分类错误的例子时：
- en: Pick an incorrectly classified example, call it **x**^*, whose true label is
    *y*^*
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个分类错误的例子，称其为**x**^*，其真实标签是*y*^*
- en: 'Update **w** as follows: **w***[t+1]* = **w***[t]* + *y*^***x**^*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新**w**如下：**w***[t+1]* = **w***[t]* + *y*^***x**^*
- en: Increase iteration counter *t*++ and repeat
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加迭代计数器*t*++并重复
- en: Notice that, for the PLA to work as we want, we have to make an adjustment.
    What we want is for ![](img/851a5760-3a74-45bb-81b2-14bf338e8848.png) to be implied
    in the expression ![](img/d1ac8002-3b39-47be-945c-fdff2b35ae12.png). The only
    way this could work is if we set ![](img/7f9e85b0-b9bf-441d-a0ca-5d17632a508a.png) and ![](img/34e97ef0-ae8a-4fb4-9692-c9dfe8d0e8dc.png).
    The previous rule seeks **w**, which implies the search for *b.*
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使PLA按我们预期的方式工作，我们必须进行调整。我们希望的是，![](img/851a5760-3a74-45bb-81b2-14bf338e8848.png)
    在表达式中隐含了![](img/d1ac8002-3b39-47be-945c-fdff2b35ae12.png)。只有在我们设置了![](img/7f9e85b0-b9bf-441d-a0ca-5d17632a508a.png)
    和 ![](img/34e97ef0-ae8a-4fb4-9692-c9dfe8d0e8dc.png) 时，这种方式才会起作用。之前的规则寻找**w**，这意味着我们在寻找*b*。
- en: 'To illustrate the PLA, consider the case of the following linearly separable
    dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明PLA，考虑以下线性可分数据集的情况：
- en: '![](img/05cdb81d-1ef5-4004-a9fd-786de7689fa1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05cdb81d-1ef5-4004-a9fd-786de7689fa1.png)'
- en: A linearly separable dataset is one whose data points are sufficiently apart
    such that at least one hypothetical line exists that can be used to separate the
    data groups into two. Having a linearly separable dataset is the dream of all
    ML scientists, but it is seldom the case that we will find such datasets naturally.
    In further chapters, we will see that neural networks transform the data into
    a new feature space where such a line may exist.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线性可分数据集是指数据点足够分开，以至于至少存在一条假设的线可以用来将数据分为两组。拥有线性可分的数据集是所有机器学习科学家的梦想，但自然界中很少能找到这样的数据集。在后续章节中，我们将看到神经网络如何将数据转换到一个新的特征空间，在这个空间中可能存在这样的分割线。
- en: This two-dimensional dataset was produced at random using Python tools that
    we will discuss later on. For now, it should be self-evident that you can draw
    a line between the two groups and divide them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个二维数据集是使用Python工具随机生成的，我们将在后面讨论这些工具。目前，显而易见的是，你可以在两组数据之间画一条线并将它们分开。
- en: 'Following the steps outlined previously, the PLA can find ***a*** solution,
    that is, a separating line that satisfies the training data target outputs completely
    in only four iterations in this particular case. The plots after each update are
    depicted in the following plots with the corresponding line found at every update:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 按照之前的步骤，PLA可以找到***a***解，即仅在四次迭代内，在这个特定情况下，可以找到一个完全满足训练数据目标输出的分隔线。每次更新后的图示以及每次更新时找到的相应线条如下：
- en: '![](img/a4fa4db2-1cc1-46e1-a1b1-7d60035a1cf0.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4fa4db2-1cc1-46e1-a1b1-7d60035a1cf0.png)'
- en: 'At iteration zero, all 100 points are misclassified, but after randomly choosing
    one misclassified point to make the first update, the new line only misses four
    points:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第零次迭代时，所有100个点都被误分类，但在随机选择一个被误分类的点进行第一次更新后，新线仅错过了四个点：
- en: '![](img/e868771c-3d6d-407b-9930-cc09476afb8e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e868771c-3d6d-407b-9930-cc09476afb8e.png)'
- en: 'After the second update, the line only misses one data point:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次更新后，线仅错过一个数据点：
- en: '![](img/310f03fc-6726-48f1-a2ad-8e43469e6987.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/310f03fc-6726-48f1-a2ad-8e43469e6987.png)'
- en: Finally, after update number three, all data points are correctly classified.
    This is just to show that a simple learning algorithm can successfully learn from
    data. Also, the perceptron model led to much more complicated models such as a
    neural network. We will now introduce the concept of a shallow network and its
    basic complexities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在第三次更新后，所有数据点都被正确分类。这只是为了展示一个简单的学习算法如何能够成功地从数据中学习。而且，感知机模型为更复杂的模型（如神经网络）奠定了基础。接下来，我们将介绍浅层网络的概念及其基本复杂性。
- en: Shallow networks
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浅层网络
- en: 'A neural network consists of multiple networks connected in different layers.
    In contrast, a perceptron has only one neuron and its architecture consists of
    an input layer and an output layer. In neural networks, there are additional layers
    between the input and output layer, as shown in *Figure 1.4*, and they are known
    ashidden layers:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由多个网络层连接而成。与此不同，感知机只有一个神经元，其结构包括输入层和输出层。在神经网络中，输入层和输出层之间还有额外的层，如*图1.4*所示，这些层被称为隐藏层：
- en: '![](img/eaa06b01-7c6d-491f-9dae-45554fc092b1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eaa06b01-7c6d-491f-9dae-45554fc092b1.png)'
- en: Figure 1.4 - Example of a shallow neural network
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 - 浅层神经网络示例
- en: The example in the figure shows a neural network that has a hidden layer with
    eight neurons in it. The input size is 10-dimensional, and the output layer has
    four dimensions (four neurons). This intermediate layer can have as many neurons
    as your system can handle during training, but it is usually a good idea to keep
    things to a reasonable number of neurons.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的示例展示了一个神经网络，它的隐藏层包含八个神经元。输入层的维度是 10 维，输出层有四个维度（四个神经元）。这个中间层可以根据你的系统在训练时处理的能力，包含任意数量的神经元，但通常来说，保持神经元数量在合理范围内是个不错的选择。
- en: If this is your first time using neural networks, it is recommended that your
    hidden layer size, that is, the number of neurons, is greater than or equal to
    the input layer, and less than or equal to the output size. However, although
    this is good advice for absolute beginners, this is not an absolute scientific
    fact since finding the optimal number of neurons in neural networks is an art,
    rather than a science, and it is usually determined through a great deal of experimentation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你第一次使用神经网络，建议你的隐藏层大小，也就是神经元的数量，应该大于或等于输入层的大小，且小于或等于输出层的大小。然而，虽然这是对初学者的好建议，但这并不是一个绝对的科学事实，因为找到神经网络的最佳神经元数量更像是一门艺术，而非科学，通常需要通过大量的实验来确定。
- en: Neural networks can solve more difficult problems than without a network, for
    example*,* with a single neural unit such as the perceptron. This must feel intuitive
    and must be easy to conceive. A neural network can solve problems including and beyond
    those that are linearly separable. For linearly separable problems, we can use
    both the perceptron model and a neural network. However, for more complex and
    non-linearly separable problems, the perceptron cannot offer a high-quality solution,
    while a neural network does.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络能够解决比没有网络更困难的问题，例如，*仅仅*依靠像感知机这样的单一神经单元。这个概念应该是直观的，而且很容易理解。神经网络能够解决包括线性可分问题在内的更复杂问题。对于线性可分问题，我们可以使用感知机模型和神经网络。但是，对于更复杂和不可线性分割的问题，感知机无法提供高质量的解决方案，而神经网络则能够做到。
- en: 'For example, if we consider the sample two-class dataset and we bring the data
    groups closer together, the perceptron will fail to terminate with a solution
    and some other strategy can be used to stop it from going forever. Or, we can
    switch to a neural network and train it to find the best solution it can possibly
    find. *Figure 1.5* shows an example of training a neural network with 100 neurons
    in the hidden layer over a two-class dataset that is not linearly separable:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑一个二分类数据集，并将数据组拉得更近，感知机将无法得出解决方案，这时可以采用其他策略来阻止其陷入无穷循环。或者，我们可以切换到神经网络，并训练它找到尽可能好的解决方案。*图
    1.5* 展示了一个在一个不可线性分割的二分类数据集上，训练一个隐藏层包含 100 个神经元的神经网络的示例：
- en: '![](img/23682c14-ee9e-4535-a5a0-4a37007f3993.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23682c14-ee9e-4535-a5a0-4a37007f3993.png)'
- en: Figure 1.5 - Non-separable data and a non-linear solution using a neural network
    with 100 neurons in the hidden layer
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 - 使用包含 100 个神经元的隐藏层的神经网络对非可分数据进行非线性求解
- en: 'This neural network has 100 neurons in the hidden layer. This was a choice
    done by experimentation and you will learn strategies on how to find such instances
    in further chapters. However, before we go any further, there are two new terms
    introduced that require further explanation: non-separable data and non-linear
    models, which are defined as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络在隐藏层有 100 个神经元。这个选择是通过实验得出的，之后的章节你将学习到如何找到这些实例的策略。然而，在我们继续之前，有两个新术语需要进一步解释：不可分数据和非线性模型，它们的定义如下：
- en: Non-separable data is such that there is no line that can separate groups of
    data (or classes) into two groups.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非可分数据是指没有一条线能够将数据组（或类别）分成两个组的数据。
- en: Non-linear models, or solutions, are those that naturally and commonly occur
    when the best solution to a classification problem is not a line. For example,
    it can be some curve described by some polynomial of any degree greater than one.
    For an example, see *Figure 1.5*.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性模型或解决方案是那些在分类问题的最佳解决方案不是一条线时，通常会自然地出现的。例如，它可以是由任何高于一次的多项式描述的曲线。一个例子请参见*图
    1.5*。
- en: A non-linear model is usually what we will be working with throughout this book,
    and the reason is that this is most likely what you will encounter out there in
    the real world. Also, it is non-linear, in a way, because the problem is non-separable.
    To achieve this non-linear solution, the neural network model goes through the
    following mathematical operations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性模型通常是我们在本书中所要使用的模型，原因是这在现实世界中更为常见。而且，它是非线性的，从某种程度上说，是因为问题是不可分的。为了实现这种非线性解，神经网络模型会经历以下数学操作。
- en: The input-to-hidden layer
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入层到隐藏层
- en: 'In a neural network, the input vector ***x*** is connected to a number of neurons
    through weights ***w*** for each neuron, which can be now thought of as a number
    of weight vectors forming a matrix ***W***. The matrix ***W* **has as many columns
    as neurons as the layer has, and as many rows as the number of features (or dimensions) ***x***
    has. Thus, the output of the hidden layer can be thought of as the following vector:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，输入向量***x***通过权重***w***连接到多个神经元，对于每个神经元，可以将其视为由多个权重向量组成的矩阵***W***。矩阵***W***的列数等于层中神经元的数量，行数则等于输入向量***x***的特征数（或维度）。因此，隐藏层的输出可以被看作以下向量：
- en: '![](img/515143a9-56db-43e7-99fd-56f26ae62c95.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/515143a9-56db-43e7-99fd-56f26ae62c95.png)'
- en: Where **b** is a vector of biases, whose elements correspond to one neural unit,
    and the size of **h** is proportional to the number of hidden units. For example,
    eight neurons in *Figure 1.4*, and 100 neurons in *Figure 1.5*. However, the activation
    function z(.) does not have to be the *sign*(.)function, in fact, it usually never
    is. Instead, most people use functions that are easily differentiable*.*
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，**b**是一个偏置向量，其元素对应于一个神经单元，**h**的大小与隐藏单元的数量成正比。例如，*图1.4*中的八个神经元，以及*图1.5*中的100个神经元。然而，激活函数z(.)不一定是*sign*(.)函数，事实上，它通常不是。相反，大多数人使用的是那些易于*可微分*的函数。
- en: A differentiable activation function is one that has a mathematical derivative
    that can be computed with traditional numerical methods or that is clearly defined.
    The opposite would be a function that does not have a defined derivative, it does
    not exist, or is nearly impossible to calculate.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可微分的激活函数是指具有数学导数的函数，这个导数可以通过传统的数值方法计算，或者函数的导数被明确定义。相反，不具有定义导数的函数就是不可计算的，甚至几乎不可能计算。
- en: The hidden-to-hidden layer
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层到隐藏层
- en: In a neural network, we could have more than one single hidden layer, and we
    will work with this kind a lot in this book. In such case, the matrix ***W***
    can be expressed as a three-dimensional matrix that will have as many elements
    in the third dimension and as many hidden layers as the network has. In the case
    of the *i*-th layer, we will refer to that matrix as **W[*i*]** for convenience.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，我们可以有多个隐藏层，而我们将在本书中大量使用这种情况。在这种情况下，矩阵***W***可以表示为一个三维矩阵，其第三维的元素个数与网络的隐藏层数量相等。对于第*i*层，我们将该矩阵称为**W[*i*]**，以便于表示。
- en: 'Therefore, we can refer to the output of the *i*-th hidden layer as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将第*i*个隐藏层的输出表示为：
- en: '![](img/dd6555c4-470b-422b-852e-773f99cadef2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd6555c4-470b-422b-852e-773f99cadef2.png)'
- en: For *i* = 2, 3, ..., *k*-1, where ***k*** is the total number of layers, and
    the case of ***h[1]*** is computed with the equation given for the first layer
    (see previous section), which uses ***x*** directly, and does not go all the way
    to the last layer, ***h[k]***, because that is computed as discussed next.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*i* = 2, 3, ..., *k*-1，其中***k***是总层数，并且***h[1]***是通过前一层给定的方程计算的（见前一节），该方程直接使用***x***，并且不需要经过最后一层***h[k]***，因为后者将在下文中讨论时计算。
- en: The hidden-to-output layer
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层到输出层
- en: 'The overall output of the network is the output at the last layer:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的整体输出是最后一层的输出：
- en: '![](img/995792ee-3b1c-4cf6-b1e8-2058af172a6c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/995792ee-3b1c-4cf6-b1e8-2058af172a6c.png)'
- en: Here, the last activation function is usually different from the hidden layer
    activations. The activation function in the last layer (output) traditionally
    depends on the type of problem we are trying to solve. For example, if we want
    to solve a regression problem, we would use a linear function, or sigmoid activations
    for classification problems. We will discuss those later on. For now, it should
    be evident that the perceptron algorithm will no longer work in the training phase.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最后的激活函数通常与隐藏层的激活函数不同。最后一层（输出层）的激活函数通常取决于我们尝试解决的问题类型。例如，如果我们想解决回归问题，我们会使用线性函数；如果是分类问题，则会使用sigmoid激活函数。我们稍后会讨论这些问题。现在，应该显而易见的是，感知机算法在训练阶段将不再有效。
- en: While the learning still has to be in terms of the mistakes the neural network
    makes, the adjustments cannot be in direct proportion to the data point that is
    incorrectly classified or predicted. The reason is that the neurons in the last
    layer are responsible for making the predictions, but they depend on a previous
    layer, and those may depend on more previous layers, and when making adjustments
    to ***W*** and ***b***, the adjustment has to be made differently for each neuron.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然学习过程仍然需要基于神经网络所犯的错误，但调整不能直接与被错误分类或预测的数据点成比例。原因是，最后一层的神经元负责进行预测，但它们依赖于前一层，而前一层可能又依赖于更前面的层，在对***W***和***b***进行调整时，必须为每个神经元做出不同的调整。
- en: One approach to do this is to apply gradient descent techniques on the neural
    network. There are many of these techniques and we will discuss the most popular
    of these in further chapters. In general, a gradient descent algorithm is one
    that uses the notion that, if you take the derivative of a function and that reaches
    a value of zero, then you have found the maximum (or minimum) value you can get
    for the set of parameters on which you are taking the derivatives. For the case
    of scalars, we call them derivatives, but for vectors or matrices (**W**, **b**),
    we call them gradients.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是应用梯度下降技术来训练神经网络。梯度下降技术有很多种，我们将在后面的章节中讨论其中最流行的一些。通常，梯度下降算法是这样一种算法：如果你对一个函数求导，并且导数的值为零，那么你就找到了可以获得的最大值（或最小值），即你正在对其求导的参数集的极值。对于标量，我们称之为导数；但对于向量或矩阵（**W**，**b**），我们称之为梯度。
- en: The function we can use is called a loss function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的函数被称为损失函数。
- en: A loss function is usually one that is differentiable so that we can calculate
    its gradient using some gradient descent algorithm.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数通常是可微分的，这样我们就可以使用一些梯度下降算法来计算它的梯度。
- en: 'We can define a loss function, for example, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个损失函数，例如，如下所示：
- en: '![](img/54f2b837-debb-482a-8c0f-18e7002c1435.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54f2b837-debb-482a-8c0f-18e7002c1435.png)'
- en: This loss is known as the **mean squared error** (**MSE**); it is meant to measure
    how different the target output ***y*** is from the predicted output in the output
    layer ***h***[***k***]in terms of the square of its elements, and averaged. This
    is a good loss because it is differentiable and it is easy to compute.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失被称为**均方误差**（**MSE**）；它旨在衡量目标输出***y***与输出层中预测输出***h***[***k***]之间的差异，差异是通过其元素的平方来度量并进行平均。这是一个好的损失函数，因为它是可微的，并且计算起来非常容易。
- en: A neural network such as this introduced a great number of possibilities, but
    relied heavily on a gradient descent technique for learning them called backpropagation
    (Hecht-Nielsen, R. 1992). Rather than explaining backpropagation here (we will
    reserve that for later), we rather have to remark that it changed the world of
    ML, but did not make much progress for a number of years because it had some practical
    limitations and the solutions to these paved the way for deep learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的神经网络引入了大量的可能性，但在学习过程中依赖于一种称为反向传播（Hecht-Nielsen, R. 1992）的梯度下降技术。我们这里不详细解释反向传播（我们将在后面讲解），而是要指出，反向传播改变了机器学习的世界，但由于其一些实际限制，很多年里并没有取得很大进展，解决这些限制的问题为深度学习铺平了道路。
- en: Deep networks
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度网络
- en: On March 27, 2019, an announcement was published by the ACM saying that three
    computer scientists were awarded the Nobel Prize in computing, that is, the ACM
    Turing Award, for their achievements in deep learning. Their names are Yoshua
    Bengio, Yann LeCun, and Geoffrey Hinton; all are very accomplished scientists.
    One of their major contributions was in the learning algorithm known as backpropagation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年3月27日，ACM发布了一则公告，宣布三位计算机科学家因其在深度学习方面的成就获得了计算机领域的诺贝尔奖——即ACM图灵奖。他们的名字分别是Yoshua
    Bengio、Yann LeCun和Geoffrey Hinton；他们都是非常杰出的科学家。他们的主要贡献之一就是反向传播学习算法。
- en: 'In [the official communication](https://www.acm.org/media-center/2019/march/turing-award-2018),
    the ACM wrote the following about Dr. Hinton and one of his seminal papers (Rumelhart,
    D. E. 1985):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在[官方通讯](https://www.acm.org/media-center/2019/march/turing-award-2018)中，ACM这样写道，关于Dr.
    Hinton和他的其中一篇开创性论文（Rumelhart, D. E. 1985）：
- en: In a 1986 paper, “Learning Internal Representations by Error Propagation,” co-authored
    with David Rumelhart and Ronald Williams, Hinton demonstrated that the backpropagation
    algorithm allowed neural nets to discover their own internal representations of
    data, making it possible to use neural nets to solve problems that had previously
    been thought to be beyond their reach. The backpropagation algorithm is standard
    in most neural networks today.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在1986年的一篇论文《通过误差传播学习内部表示》中，Hinton与David Rumelhart和Ronald Williams共同合作，展示了反向传播算法使神经网络能够发现其数据的内部表示，从而使神经网络能够解决之前认为超出其能力范围的问题。反向传播算法今天已成为大多数神经网络的标准算法。
- en: 'Similarly, they wrote the following about Dr. LeCun''s paper (LeCun, Y., *et.al.,* 1998):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，他们在Dr. LeCun的论文（LeCun, Y., *et.al.,* 1998）中写道：
- en: LeCun proposed an early version of the backpropagation algorithm (backprop),
    and gave a clean derivation of it based on variational principles. His work to
    speed up backpropagation algorithms included describing two simple methods to
    accelerate learning time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: LeCun提出了反向传播算法（backprop）的早期版本，并基于变分原理给出了清晰的推导。他在加速反向传播算法方面的工作包括描述了两种简单的方法来加速学习时间。
- en: Dr. Hinton was able to show that there was a way to minimize a loss function
    in neural networks using biologically inspired algorithms such as the backward
    and forward adjustment of connections by modifying its importance for particular
    neurons. Usually, backpropagation is related to feed-forward neural networks,
    while backward-forward propagation is related to Restricted Boltzmann Machines(covered
    in [Chapter 10](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml), *Restricted Boltzmann
    Machines*).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Dr. Hinton能够证明，有一种方法可以通过使用生物启发式算法（如通过调整神经元之间连接的重要性来进行前向和反向调整）来最小化神经网络中的损失函数。通常，反向传播与前馈神经网络相关，而前后传播则与限制玻尔兹曼机（在[第10章](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml)中介绍，*限制玻尔兹曼机*）相关。
- en: A feed-forward neural network is one whose input is pipelined directly toward
    the output layer through intermediate layers that have no backward connections,
    as shown in *Figure 1.4*, and we will talk about these all the time in this book.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是一种其输入直接通过没有反向连接的中间层传递到输出层的网络，如*图1.4*所示，在本书中我们将经常讨论这些内容。
- en: It is usually safe to assume that, unless you are told otherwise, all neural
    networks have a feed-forward architecture. Most of this book will talk about deep
    neural networks and the great majority are feed-forward-like, with the exception
    of Restricted Boltzmann Machines or recurrent neural networks, for example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常可以安全假设，除非另有说明，所有神经网络都是前馈结构。本书的大部分内容将讨论深度神经网络，其中绝大多数是前馈类型的，例外的情况包括限制玻尔兹曼机或递归神经网络等。
- en: Backpropagation enabled people to train neural networks in a way that was never
    seen before; however, people had problems training neural networks on large datasets,
    and on larger (deeper) architectures. If you go ahead and look at neural network
    papers in the late '80s and early '90s, you will notice that architectures were
    small in size; networks usually had no more than two or three layers, and the
    number of neurons usually did not exceed the order of hundreds. These are (today)
    known as shallow neural networks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播使得人们能够以一种前所未见的方式训练神经网络；然而，人们在大数据集和更大（更深）架构上训练神经网络时遇到了问题。如果你查看80年代末和90年代初的神经网络论文，你会注意到当时的架构很小；网络通常不超过两三层，神经元的数量通常也不会超过几百个。这些今天被称为浅层神经网络。
- en: The major problems were with convergence time for larger datasets, and convergence
    time for deeper architectures. Dr. LeCun's contributions were precisely in this
    area as he envisioned different ways to speed up the training process. Other advances
    such as vector (tensor) computations over **graphics processing units** (**GPUs**)
    increased training speeds dramatically.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题在于较大数据集的收敛时间，以及更深架构的收敛时间。LeCun博士的贡献正是在这一领域，他设想了加速训练过程的不同方法。其他进展，例如在**图形处理单元**（**GPUs**）上进行向量（张量）计算，显著提高了训练速度。
- en: Thus, over the last few years, we have seen the rise of deep learning, that
    is, the ability to train deeper neural networks, with more than three or four
    layers, in fact with tens and hundreds of layers. Further, we have a wide variety
    of architectures that can accomplish things that we were not able in the last
    decade.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在过去的几年中，我们见证了深度学习的崛起，即训练更深神经网络的能力，实际上是具有三层或四层，甚至是数十层和数百层的网络。此外，我们还拥有各种各样的架构，可以完成过去十年无法完成的任务。
- en: 'The deep network shown in *Figure 1.6* would have been impossible to train
    30 years ago, and it is not that deep anyway:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图1.6*所示的深度网络，30年前是无法训练的，而实际上它并不算深：
- en: '![](img/e261302a-8ab2-44f4-a52c-e856ad8bc42e.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e261302a-8ab2-44f4-a52c-e856ad8bc42e.png)'
- en: Figure 1.6 - A deep and fully connected feed-forward neural network with eight
    layers
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 - 一个有八层的深度全连接前馈神经网络
- en: In this book, we will consider a deep neural network any network that has more
    than three or four layers overall. However, there is no standard definition as
    to exactly how deep is considered deep out there. Also, you need to consider that
    what we consider deep today, at the time of writing this book in 2020, will probably
    not be considered deep in 20 or 30 years from now.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将认为任何具有三层或四层以上的网络都是深度神经网络。然而，并没有一个标准定义明确说明深度网络究竟有多深。此外，您还需要考虑到，今天我们认为的“深度”——在2020年写这本书时的标准——在20或30年后可能不再被认为是深度网络。
- en: Regardless of the future of DL, let us now discuss what makes DL so important
    today.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 无论深度学习的未来如何，让我们现在来讨论一下为什么今天深度学习如此重要。
- en: Why is deep learning important today?
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么今天深度学习如此重要？
- en: 'Today, we enjoy the benefits of algorithms and strategies that we did not have
    20 or 30 years ago, which enable us to have amazing applications that are changing
    lives. Allow me to summarize some of the great and important things about deep
    learning today:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们享受着20或30年前没有的算法和策略带来的好处，这些算法和策略让我们能够拥有正在改变生活的惊人应用。让我总结一下今天深度学习的一些伟大且重要的内容：
- en: '**Training in mini-batches**: This strategy allows us today to have very large
    datasets and train a deep learning model little by little. In the past, we would
    have to load the entire dataset into memory, making it computationally impossible
    for some large datasets. Today, yes, it may take a little longer, but we at least
    can actually perform training on finite time.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量训练**：这种策略使得我们今天能够使用非常大的数据集，并且一点一点地训练深度学习模型。在过去，我们必须将整个数据集加载到内存中，这对于一些大数据集来说在计算上是不可能的。今天，虽然可能需要更长的时间，但我们至少可以在有限的时间内进行训练。'
- en: '**Novel activation functions**: **Rectified linear units** (**ReLUs**), for
    example, are a relatively new kind of activation that solved many of the problems
    with large-scale training with backpropagation strategies. These new activations
    enable training algorithms to converge on deep architectures when, in the past,
    we would get stuck on non-converging training sessions that would end up having
    exploding or vanishing gradients.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新型激活函数**：例如，**修正线性单元**（**ReLUs**）是一种相对较新的激活函数，它解决了许多使用反向传播策略进行大规模训练时遇到的问题。这些新的激活函数使得训练算法能够在深度架构上收敛，而在过去，我们往往会在非收敛的训练过程中卡住，最终导致梯度爆炸或消失。'
- en: '**Novel neural network architectures**: Convolutional or recurrent networks,
    for example, have been transforming the world by opening the possibilities of
    things we can do with neural networks. Convolutional networks are widely applied
    in computer vision applications or other areas in which the convolution operation
    is a natural thing to do, for example, multi-dimensional signal or audio analysis.
    Recurrent neural networks with memory are widely used to analyze sequences of
    text, thus enabling us to have networks that understand words, sentences, and
    paragraphs, and we can use them to translate between languages, and many more
    things.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新颖的神经网络架构**：例如，卷积神经网络或循环神经网络通过开辟我们能用神经网络做的事情的可能性，正在改变世界。卷积网络广泛应用于计算机视觉领域或其他卷积运算自然适用的领域，如多维信号或音频分析。带有记忆的循环神经网络广泛应用于文本序列分析，使我们能够拥有理解单词、句子和段落的网络，我们可以用它们进行语言翻译等诸多任务。'
- en: '**Interesting loss functions**: These losses play an interesting role in deep
    learning because, in the past, we only used the same standard losses over and
    over again; losses such as the MSE. Today, we can minimize the MSE and, at the
    same time, minimize the norm of the weights or the output of some neurons, which
    leads to sparser weights and solutions that, in turn, make the produced model
    much more efficient when it is deployed into production.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有趣的损失函数**：这些损失在深度学习中起到了有趣的作用，因为过去我们只是一遍又一遍地使用相同的标准损失，如均方误差（MSE）。今天，我们不仅可以最小化MSE，同时还可以最小化权重的范数或某些神经元的输出，这导致了更稀疏的权重和解决方案，反过来使得模型在生产环境中部署时更高效。'
- en: '**Novel strategies resembling biology**: Things such as missing or dropping
    connections between neurons, rather than having them fully connected all the time,
    is more realistic, or comparable to biological neural network design. Also, dropping
    or removing neurons altogether is a new strategy that can push some neurons to
    excel when others are removed, learning richer representations, while at the same
    time reducing the computations during training and when deployed. The sharing
    of parameters between different and specialized neural networks also has proven
    to be interesting and effective today.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类似生物学的新策略**：例如，缺失或丢弃神经元之间的连接，而不是让它们始终完全连接，这更符合现实，或者说更像生物神经网络的设计。此外，完全丢弃或移除神经元是一个新的策略，可以促使某些神经元在其他神经元被移除时表现出色，学习更丰富的表示，同时在训练时和部署时减少计算量。不同且专业化的神经网络之间共享参数，今天也被证明是一个有趣且有效的策略。'
- en: '**Adversarial training**: Making a neural network compete against another network
    whose sole purpose is to generate fraudulent, noisy, and confusing data points
    trying to make the network fail has proven to be an excellent strategy for networks
    to learn better from the data and be robust against noisy environments when deployed
    into production.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗训练**：让一个神经网络与另一个网络竞争，后者的唯一目的是生成虚假的、嘈杂的和混乱的数据点，试图让网络失败，已被证明是一种优秀的策略，能够让网络从数据中学习得更好，并且在生产环境中具备更强的抗干扰能力。'
- en: There are many other interesting facts and points that make deep learning an
    exciting area and justify the writing of this book. I hope you are as excited
    as we all are and begin reading this book knowing that we are going to code some
    of the most exciting and incredible neural networks of our time. Our ultimate
    purpose will be to make deep neural networks that can generalize.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习有许多其他有趣的事实和要点，使其成为一个令人兴奋的领域，也为编写本书提供了正当理由。我希望你和我们一样激动，开始阅读本书时，你将知道我们将编写一些这个时代最令人兴奋和不可思议的神经网络。我们的最终目标是构建能够泛化的深度神经网络。
- en: Generalization is the ability of a neural network to correctly make predictions
    on data that has never been seen before. This is the ultimate purpose of all machine
    and deep learning practitioners, and requires a great deal of skill and knowledge
    of the data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化是指神经网络在从未见过的数据上做出正确预测的能力。这是所有机器学习和深度学习从业者的终极目标，且需要极大的技能和对数据的深刻理解。
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This introductory chapter presented an overview of ML. It introduced the motivation
    behind ML and the terminology that is commonly used in the field. It also introduced
    deep learning and how it fits in the realm of artificial intelligence. At this
    point, you should feel confident that you know enough about what a neural network
    is to be curious about how big it can be. You should also feel very intrigued
    about the area of deep learning and all the new things that are coming out every
    week.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节介绍了机器学习的概况。它阐述了机器学习背后的动机以及该领域常用的术语。它还介绍了深度学习以及深度学习在人工智能领域中的位置。到此为止，你应该已经对神经网络的基本概念有了足够的了解，足以好奇它能够有多大。你也应该对深度学习领域以及每周都会有新进展的各类新技术感到非常好奇。
- en: At this point, you must be a bit anxious to begin your deep learning coding
    journey; for that reason, the next logical step is to go to [Chapter 2](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml), *Setup
    and Introduction to Deep Learning Frameworks.* In this chapter, you will get ready
    for the action by setting up your system and making sure you have access to the
    resources you will need to be a successful deep learning practitioner. But before
    you go there, please try to quiz yourself with the following questions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你应该有点迫不及待开始你的深度学习编程之旅了；因此，接下来的逻辑步骤是转到[第二章](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml)，*深度学习框架的设置与介绍*。在这一章中，你将通过设置系统并确保你能访问成功成为深度学习从业者所需的资源，为实际操作做好准备。但在你继续之前，请先用以下问题测试一下自己。
- en: Questions and answers
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Can a perceptron and/or a neural network solve the problem of classifying
    data that is linearly separable?**'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**感知器和/或神经网络能否解决可线性分割的数据分类问题？**'
- en: Yes, both can.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，两个都可以。
- en: '**Can a perceptron and/or a neural network solve the problem of classifying
    data that is non-separable? **'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**感知器和/或神经网络能否解决不可分割数据的分类问题？**'
- en: Yes, both can. However, the perceptron will go on forever unless we specify
    a stopping condition such as a maximum number of iterations (updates), or stopping
    if the number of misclassified points does not decrease after a number of iterations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，两个都可以。然而，感知器将永远运行下去，除非我们指定停止条件，例如最大迭代次数（更新次数），或在若干次迭代后，如果误分类点数没有减少，则停止。
- en: '**What are the changes in the ML filed that have enabled us to have deep learning
    today?**'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在机器学习领域中，哪些变化使我们今天能够拥有深度学习？**'
- en: (A) backpropagation algorithms, batch training, ReLUs, and so on;
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (A) 反向传播算法，批量训练，ReLU等；
- en: (B) computing power, GPUs, cloud, and so on.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (B) 计算能力，GPU，云计算等等。
- en: '**Why is generalization a good thing?**'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么泛化是好事？**'
- en: Because deep neural networks are most useful when they can function as expected
    when they are given data that they have not seen before, that is, data on which
    they have not been trained.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因为深度神经网络在处理它们从未见过的数据时最为有效，即它们没有在这些数据上进行训练的数据。
- en: References
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Hecht-Nielsen, R. (1992). *Theory of the backpropagation neural network*. In *Neural
    networks for perception* (pp. 65-93). *Academic Press*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hecht-Nielsen, R. (1992). *反向传播神经网络的理论*。在 *感知神经网络*（第65-93页）。 *Academic Press*。
- en: Kane, F. (2017). *Hands-On Data Science and Python ML*. *Packt Publishing Ltd*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kane, F. (2017). *动手实践数据科学与Python机器学习*. *Packt Publishing Ltd*。
- en: 'LeCun, Y., Bottou, L., Orr, G., and Muller, K. (1998). *Efficient backprop
    in neural networks: Tricks of the trade* (Orr, G. and Müller, K., eds.). *Lecture
    Notes in Computer Science*, 1524(98), 111.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun, Y., Bottou, L., Orr, G., 和 Muller, K. (1998). *神经网络中的高效反向传播：行内技巧*（Orr,
    G. 和 Müller, K. 编）。 *计算机科学讲义笔记*，1524(98), 111。
- en: Ojeda, T., Murphy, S. P., Bengfort, B., and Dasgupta, A. (2014). *Practical
    Data Science Cookbook*. *Packt* *Publishing Ltd*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojeda, T., Murphy, S. P., Bengfort, B., 和 Dasgupta, A. (2014). *实用数据科学食谱*。*Packt
    Publishing Ltd*。
- en: 'Rosenblatt, F. (1958). *The perceptron: a probabilistic model for information
    storage and organization in* the *brain*. *Psychological Review*, 65(6), 386.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenblatt, F. (1958). *感知器：大脑中信息存储和组织的概率模型*。 *心理学评论*，65(6), 386。
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). *Learning internal
    representations by error* *propagation* (No. ICS-8506). *California Univ San Diego
    La Jolla Inst for Cognitive Science*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., 和 Williams, R. J. (1985). *通过误差传播学习内部表示*（No.
    ICS-8506）。 *加利福尼亚大学圣地亚哥分校认知科学研究所*。
