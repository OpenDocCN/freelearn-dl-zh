- en: Sentiment Analysis in Your Browser Using TensorFlow.js
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在浏览器中使用 TensorFlow.js 进行情感分析
- en: Sentiment analysis is a popular problem in machine learning. People are constantly
    trying to understand the sentiment of a product or movie review. Currently, for
    sentiment analysis, we extract the text from a client/browser, pass it on to a
    server that runs a machine learning model to predict sentiment of the text, and
    the server then sends the result back to the client.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是机器学习中的一个热门问题。人们不断尝试理解产品或电影评论的情感。目前，对于情感分析，我们从客户端/浏览器提取文本，将其传递给运行机器学习模型的服务器来预测文本的情感，服务器然后将结果返回给客户端。
- en: This is perfectly fine if we don't care about the latency in the system. However,
    there are many applications, such as stock trading, customer support conversations
    where it might be helpful to predict sentiment of the text with low latency. One
    obvious bottleneck in reducing latency is the server call.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不关心系统的延迟，这样做是完全可以接受的。然而，很多应用场景（如股票交易、客户支持对话）中，预测文本的情感低延迟可能会非常有帮助。减少延迟的一个明显瓶颈就是服务器调用。
- en: If sentiment analysis could be achieved on the browser/client itself, we can
    get rid of the server call and can predict the sentiment in real time. Google
    recently released TensorFlow.js, which enables us to do the model training and
    inference on a browser/client.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果情感分析能够在浏览器/客户端本身完成，我们就可以摆脱服务器调用，并且可以实时预测情感。谷歌最近发布了 TensorFlow.js，它使我们能够在浏览器/客户端上进行模型训练和推理。
- en: 'Additionally, training the models on the client side opens up a world of opportunities.
    A brief summary of all the advantages of doing this are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在客户端训练模型也打开了一个全新的机会。以下是这样做的所有优势的简要总结：
- en: '**Priv****acy**: Since data will never leave the client side, we are able to
    provide the magical experience of machine learning without compromising on data
    privacy'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：由于数据永远不会离开客户端，我们能够提供机器学习的神奇体验而不妥协于数据隐私'
- en: '**No hassle ML**: Since the code runs on a browser, the user doesn''t need
    to install any libraries or dependencies'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无烦恼的机器学习**：由于代码在浏览器中运行，用户不需要安装任何库或依赖项'
- en: '**Latency**: Since there is no need to transfer data to servers for training/predictions,
    we can deploy ML models for low latency applications'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低延迟**：由于不需要将数据传输到服务器进行训练/预测，我们可以为低延迟应用部署机器学习模型'
- en: '**Device agnostic**: A web page can be opened on any device (laptop, mobile.
    and so on) and so TensorFlow.js can take advantage of any hardware (GPU) or device
    sensors, such as accelerometers in mobile devices, to train ML models'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备无关性**：一个网页可以在任何设备（笔记本、手机等）上打开，因此 TensorFlow.js 可以利用任何硬件（GPU）或设备传感器，如手机中的加速度计，来训练机器学习模型'
- en: 'Let''s learn how to incorporate sentiment analysis in the browser using TensorFlow.js
    by covering the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下主题来学习如何使用 TensorFlow.js 在浏览器中整合情感分析：
- en: Understanding TensorFlow.js
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 TensorFlow.js
- en: Understanding Adam Optimization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Adam 优化算法
- en: Understanding Categorical Cross Entropy Loss
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解类别交叉熵损失
- en: Understanding Word Embeddings
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: Setting up the sentiment analysis problem and building a model in Python
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中设置情感分析问题并构建模型
- en: Deploying the model using TensorFlow.js in a browser
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow.js 在浏览器中部署模型
- en: Find the code for this chapter and Installation instructions are also present
    in a **README** file in repository for this project.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中查找代码，并且安装说明也在此项目的 **README** 文件中。
- en: Understanding TensorFlow.js
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 TensorFlow.js
- en: TensorFlow recently open sourced TensorFlow.js. This is an open source library
    that helps us define and train deep learning models entirely on the browser using
    JavaScript as well as through a high-level layered API. We can use this to train
    deep learning models entirely on the client side. A server GPU is not required
    to train these models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 最近开源了 TensorFlow.js。这是一个开源库，帮助我们完全在浏览器中使用 JavaScript 定义和训练深度学习模型，同时提供高级分层
    API。我们可以利用它在客户端完全训练深度学习模型，不需要服务器 GPU 来训练这些模型。
- en: 'The following diagram illustrates the API overview of TensorFlow.js:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 TensorFlow.js 的 API 概览：
- en: '![](img/1f074dbe-54eb-40d5-9042-6201fe2a9397.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f074dbe-54eb-40d5-9042-6201fe2a9397.png)'
- en: 'This is powered by **WebGL**. It also provides a high-level layered API. It
    has a support for **Eager** execution, too. You can achieve three things using
    TensorFlow.js:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这由 **WebGL** 提供支持。它还提供了一个高级分层 API，并且支持 **Eager** 执行。你可以使用 TensorFlow.js 实现以下三项功能：
- en: Load existing TensorFlow/Keras models for in browser predictions
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在浏览器中加载现有的TensorFlow/Keras模型以进行预测
- en: Retrain existing models using client data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用客户数据重新训练现有模型
- en: Define and train Deep Learning models from scratch on the **Browser**
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**浏览器**上定义和训练深度学习模型
- en: Understanding Adam Optimization
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Adam优化
- en: Before we look at Adam optimization, let's try to first understand the concept
    of gradient descent.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究Adam优化之前，让我们先试着理解梯度下降的概念。
- en: 'Gradient descent is an iterative optimization algorithm to find the minimum
    of a function. An analogous example could be as follows: let''s say we are stuck
    on somewhere in middle of a mountain and we want to reach the ground in fastest
    possible manner.  As a first step, we will observe the slope of mountain in all
    directions around us and decide to take the the direction with steepest slope
    down.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种迭代优化算法，用于寻找函数的最小值。类比的例子可以是：假设我们被困在山的中部某处，我们想以最快的方式到达地面。作为第一步，我们会观察周围所有方向山的坡度，并决定沿最陡的坡度方向向下前进。
- en: We re-evaluate our choice of direction after every step we take. Also, the size
    of our walking also depends on the steepness of the downward slope. If the slope
    is very steep, we take bigger steps as it can help us to reach faster to the ground.
    This way after a few/large number of steps we can reach the ground safely. Similarly,
    in machine learning, we want to minimize some error/cost function by updating
    the weights of the algorithm. To find minimum of cost function using gradient,
    we update the weights of the algorithm proportional to the gradient in the direction
    of steepest descent. The proportionality constant is also known as Learning Rate
    in neural network literature.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每走一步后，我们重新评估我们的方向选择。此外，我们步行的距离也取决于下坡的陡峭程度。如果坡度很陡，我们会迈更大的步伐，因为这有助于我们更快地到达地面。这样，在经过几步或大量步骤后，我们可以安全地到达地面。类似地，在机器学习中，我们希望通过更新算法的权重来最小化某些错误/成本函数。为了使用梯度找到成本函数的最小值，我们根据最陡下降的方向中的梯度更新算法的权重。在神经网络文献中，这个比例常数也被称为学习率。
- en: However, in large scale machine learning, doing gradient descent optimization
    is pretty costly because we take only one step after a single pass on our entire
    training dataset. So, the time to converge to a minimum of cost function is huge
    if we were to take several thousand steps to converge.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大规模机器学习中，进行梯度下降优化是相当昂贵的，因为我们在整个训练数据集通过后只能进行一步。因此，如果我们要花费几千步来收敛到成本函数的最小值，那么收敛时间将会非常长。
- en: A solution to this problem is Stochastic Gradient Descent (SGD), an approach
    to update weights of the algorithm after each training example and not wait for
    entire training dataset to pass through the algorithm for the update. We use the
    term, **stochastic** to denote the approximate nature of gradient as it is only
    computed after every training example. However, it is shown in the literature
    that after many iterations, SGD almost surely converges to the true local/global
    minimum of the function. Generally, in deep learning algorithms, we can observe
    that we tend to use mini-batch SGD where we update the weights after every mini
    batch and not after every training example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个解决方案是随机梯度下降（SGD），这是一种在每个训练示例后更新算法权重的方法，而不是等待整个训练数据集通过算法进行更新。我们使用术语“随机”来表示梯度的近似性，因为它仅在每个训练示例后计算。然而，文献中表明，经过许多迭代，SGD几乎肯定会收敛到函数的真实局部/全局最小值。一般来说，在深度学习算法中，我们观察到我们倾向于使用小批量SGD，在每个小批量后更新权重，而不是在每个训练示例后更新。
- en: Adam optimization is a variant of SGD where we maintain per parameter (weight)
    learning rate and update it based on the mean and variance of previous gradients
    of that parameter. Adam has been proven to be extremely good and fast for many
    deep learning problems. For more details on Adam optimization, please refer to
    the original paper ([https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化是SGD的一个变体，我们在每个参数（权重）上维护学习率，并根据该参数之前梯度的均值和方差进行更新。已经证明Adam对许多深度学习问题非常高效和快速。有关Adam优化的更多细节，请参阅原始论文（[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)）。
- en: Understanding categorical cross entropy loss
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类交叉熵损失
- en: Cross entropy loss, or log loss, measures the performance of the classification
    model whose output is a probability between 0 and 1\. Cross entropy increases
    as the predicted probability of a sample diverges from the actual value. Therefore,
    predicting a probability of 0.05 when the actual label has a value of 1 increases
    the cross entropy loss.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失，或称对数损失，衡量分类模型的性能，该模型的输出是介于0和1之间的概率。当预测概率与实际值偏离时，交叉熵增加。因此，当实际标签值为1时，如果预测概率为0.05，交叉熵损失就会增加。
- en: 'Mathematically, for a binary classification setting, cross entropy is defined
    as the following equation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，对于二分类设置，交叉熵定义为以下方程：
- en: '![](img/513a42dd-d240-46c7-8a74-dbd70a09d276.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/513a42dd-d240-46c7-8a74-dbd70a09d276.png)'
- en: Here, ![](img/d8664366-e696-42ef-8019-5c5a534384a3.png) is the binary indicator
    (0 or 1) denoting the class for the sample ![](img/3c66d06e-4997-4db5-aa30-89812b374e7a.png),
    while ![](img/e6887a84-0522-4518-8e83-6448f23b9c49.png) denotes the predicted
    probability between 0 and 1 for that sample.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/d8664366-e696-42ef-8019-5c5a534384a3.png) 是一个二进制指示符（0或1），表示样本 ![](img/3c66d06e-4997-4db5-aa30-89812b374e7a.png)
    的类别，而 ![](img/e6887a84-0522-4518-8e83-6448f23b9c49.png) 表示该样本的预测概率，范围是0到1。
- en: 'Alternatively, if there are more than two classes, we define a new term known
    as categorical cross entropy. It is calculated as a sum of separate loss for each
    class label per observation. Mathematically, it is given as the following equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种情况是，如果类别数超过两个，我们定义一个新的术语，称为类别交叉熵。它是通过对每个类别标签在每个观察值上的单独损失求和来计算的。从数学上讲，它由以下方程给出：
- en: '![](img/710d38ef-dfc4-4150-8b23-cebe30073ab3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/710d38ef-dfc4-4150-8b23-cebe30073ab3.png)'
- en: Here, ![](img/5d45ec5a-62bb-4442-84dd-dd65ce7722e1.png) denotes the number of
    classes, ![](img/fc17a134-4c08-4d89-baf4-4d06c4309bd2.png) is a binary indicator
    (0 or 1) that indicates whether ![](img/a8c647e5-8fe0-41c9-be72-a1bf309f9a0a.png) is
    the correct class for the observation, ![](img/84984180-ad05-465c-84d4-b217cb23f6d6.png),
    and ![](img/ddf45d66-3f33-4d35-8c57-1d5cf205fee3.png)denotes the probability of
    observation ![](img/c6141838-e2a2-484d-b836-86931ea042ef.png) for class ![](img/61a34d4d-531a-4306-bae2-b62d653c96de.png).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/5d45ec5a-62bb-4442-84dd-dd65ce7722e1.png) 表示类别的数量，![](img/fc17a134-4c08-4d89-baf4-4d06c4309bd2.png)
    是一个二进制指示符（0或1），表示 ![](img/a8c647e5-8fe0-41c9-be72-a1bf309f9a0a.png) 是否是该观察的正确类别，![](img/84984180-ad05-465c-84d4-b217cb23f6d6.png)，以及
    ![](img/ddf45d66-3f33-4d35-8c57-1d5cf205fee3.png) 表示该观察属于类别 ![](img/61a34d4d-531a-4306-bae2-b62d653c96de.png)
    的概率。
- en: In this chapter, as we perform binary classification on reviews, we will only
    use binary cross entropy as our classification loss.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，由于我们对评论进行二分类，因此我们只会使用二进制交叉熵作为分类损失。
- en: Understanding word embeddings
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: Word embeddings refer to the class of feature learning techniques in **Natural
    Language Processing** (**NLP**) that are used to generate a real valued vector
    representation of a word, sentence, or document.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是指**自然语言处理**（**NLP**）中用于生成单词、句子或文档的实数值向量表示的特征学习技术类别。
- en: Many machine learning tasks today involve text. For example, Google's language
    translation or spam detection in Gmail both use text as input to their models
    to perform the tasks of translation and spam detection. However, modern day computers
    can only take real valued numbers as input and can't understand strings or text
    unless we encode them into numbers or vectors.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在许多机器学习任务都涉及文本。例如，谷歌的语言翻译或Gmail中的垃圾邮件检测都将文本作为输入，使用模型执行翻译和垃圾邮件检测任务。然而，现代计算机只能接受实值数字作为输入，无法理解字符串或文本，除非我们将其编码为数字或向量。
- en: 'For example, let''s consider a sentence, "*I like Football"*, for which we
    want a representation of all of the words. A brute force method to generate the
    embeddings of the three words "*I"*, *"like"*, and *"Football"* is done through
    the one hot representation of words. In this case, the embeddings are given as
    follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们有一句话 "*I like Football*"，我们希望获取所有单词的表示。生成三词 "*I*"、*"like*" 和 *"Football"*
    的嵌入的粗暴方法是通过单词的独热表示来实现的。在这种情况下，嵌入结果如下：
- en: '"I" = [1,0,0]'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"I" = [1,0,0]'
- en: '"like" = [0,1,0]'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"like" = [0,1,0]'
- en: '"Football" = [0,0,1]'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"Football" = [0,0,1]'
- en: 'The idea is to create a vector that has a dimension equal to the number of
    unique words in the sentence and to assign a 1 to the position where a word exists
    and zero everywhere else. There are two issues with this approach:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的思路是创建一个向量，其维度等于句子中唯一单词的数量，并且在单词出现的地方赋值为1，在其他地方为0。这个方法存在两个问题：
- en: The number of vector dimension scales with the number of words in the corpus.
    Let's say we have 100,000 unique words in a document. Therefore, we represent
    each word with a vector that has a dimension of 100,000\. This increases the memory
    required to represent words, making our system inefficient.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量维度的数量随着语料库中单词数量的增加而增加。假设我们有100,000个独特的单词在文档中。因此，我们用一个维度为100,000的向量来表示每个单词。这增加了表示单词所需的内存，使我们的系统变得低效。
- en: 'One hot representations like this fail to capture the similarity between words.
    For example, there are two words in a sentence, *"like"* and *"love"*. We know
    that *"like"* is more similar to *"love"* than it is to *"Football"*. However,
    in our current one hot representation, the dot product of any two vectors is zero.
    Mathematically, the dot product of *"like"* and *"Football"* is represented as
    follows:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像这种独热编码表示无法捕捉到单词之间的相似性。例如，在句子中有两个单词，*“like”* 和 *“love”*。我们知道*“like”*比*“love”*更接近，而比*“Football”*要相似。然而，在当前的独热编码表示中，任何两个向量的点积都是零。数学上，*“like”*和*“Football”*的点积如下表示：
- en: '*"like" * "Football" = Transpose([0,1,0]) *[0,0,1] = 0*0 + 1*0 + 0*1 = 0*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*"like" * "Football" = 转置([0,1,0]) *[0,0,1] = 0*0 + 1*0 + 0*1 = 0*'
- en: This is because we have a separate position in the vector for each word and
    only have 1 in that position.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们为每个单词在向量中分配了一个独立的位置，并且该位置只有一个1。
- en: For both spam detection and language translation problems, understanding the
    similarity between words is quite crucial. For this reason, there are several
    other ways (both supervised and unsupervised) in which we can learn about word
    embeddings.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于垃圾邮件检测和语言翻译问题，理解单词之间的相似性非常重要。为此，有几种方法（包括监督学习和无监督学习）可以让我们学习词嵌入。
- en: You can learn more about how to use word embeddings with TensorFlow from this official
    tutorial. ([https://www.tensorflow.org/tutorials/representation/word2vec](https://www.tensorflow.org/tutorials/representation/word2vec))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过这个[官方教程](https://www.tensorflow.org/tutorials/representation/word2vec)了解如何使用TensorFlow的词嵌入。
- en: This project uses the embedding layer from Keras to map the words in our movie
    reviews to real valued vector representations. In this project, we learn the vector
    representation of words in a supervised manner. Essentially, we initialize the
    word embeddings randomly and then use back propagation in neural networks to update
    the embeddings such that total network loss is minimized. Training them in a supervised
    manner helps to generate task specific embeddings. For example, we expect a similar
    representation of words such as *Awesome* and *Great* since they both signify
    a positive sentiment. Once we have encoded words in movie reviews, we can use
    them as input in our neural network layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目使用Keras的嵌入层将我们的电影评论中的单词映射到实际值的向量表示中。在这个项目中，我们通过监督学习的方式来学习单词的向量表示。基本上，我们随机初始化词嵌入，然后通过神经网络中的反向传播来更新嵌入，以最小化总的网络损失。以监督的方式训练它们有助于生成任务特定的嵌入。例如，我们期望*Awesome*和*Great*这样的单词具有相似的表示，因为它们都表示积极的情感。一旦我们对电影评论中的单词进行了编码，就可以将它们作为输入传递到神经网络层中。
- en: Building the sentiment analysis model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建情感分析模型
- en: In this section, we will learn how to build a sentiment analysis model from
    scratch using Keras. To perform sentiment analysis, we will use sentiment analysis
    data from the University of Michigan that is available at [https://www.kaggle.com/c/si650winter11/data](https://www.kaggle.com/c/si650winter11/data).
    This dataset contains 7,086 movie reviews with labels. Label `1` denotes a positive
    sentiment, while `0` denotes a negative sentiment. In the repository, the dataset
    is stored in the file named `sentiment.txt`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何使用Keras从头开始构建情感分析模型。为了进行情感分析，我们将使用来自密歇根大学的情感分析数据，数据可以在[https://www.kaggle.com/c/si650winter11/data](https://www.kaggle.com/c/si650winter11/data)找到。这个数据集包含了7,086条带标签的电影评论。标签`1`表示积极情感，而`0`表示消极情感。在这个代码库中，数据集保存在名为`sentiment.txt`的文件中。
- en: Pre-processing data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Once you have installed the requisite packages (can be found in a `requirements.txt`
    file with the code) to run this project and read the data, the next step is to
    preprocess the data:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你安装了运行此项目并读取数据所需的包（可以在`requirements.txt`文件中找到），下一步就是对数据进行预处理：
- en: 'The first step is to get the tokens/word list from the reviews. Remove any
    punctuation and make sure that all of the tokens are in lowercase:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是从评论中获取词元/单词列表。去除任何标点符号，并确保所有词元都是小写字母：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For example, if we have an input `This is a GREAT movie!!!!`, our output should
    be `this is a great movie`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们输入`This is a GREAT movie!!!!`，那么输出应该是`this is a great movie`。
- en: 'Create a `token_idx` dictionarythat maps tokens to integers to create embeddings.
    Note that the number of unique tokens (words) present in a dictionary can be very
    large, so we must filter out the ones that occur less than the threshold (the
    default value for this is `5` in the code) in the training set. This is because
    it is difficult to learn any relationship between movie sentiment and words that
    don''t occur much in the dataset:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`token_idx`字典，将词汇映射到整数，以创建嵌入。请注意，字典中可能包含非常多的独特词汇（tokens），因此我们必须过滤掉在训练集中出现次数少于阈值的词汇（代码中默认值为`5`）。这是因为学习电影情感和在数据集中出现频率较低的词汇之间的关系非常困难：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Map each review in the dataset to a sequence of integers (based on the `token_idx`dictionary
    we created in the last step). However, before doing that, find the review with
    the largest number of tokens:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集中的每个评论映射到一个整数序列（基于我们在上一步创建的`token_idx`字典）。但是，在执行此操作之前，首先找到具有最多词汇的评论：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To create the sequences that will be fed into the model to learn the embeddings,
    we must create a fixed-length sequence of `(max_tokens)` for each review in the
    dataset. We pre-pad the sequences with zeros if they are less than the maximum
    length to ensure that all of the sequences are of the same length. Pre-padding
    a sequence is preferred over post padding as it helps to achieve a more accurate
    result:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了创建输入模型学习嵌入的序列，我们必须为数据集中的每个评论创建一个固定长度的`(max_tokens)`序列。如果评论的长度小于最大长度，我们将用零进行前填充，以确保所有序列的长度一致。与后填充相比，前填充被认为能帮助获得更准确的结果：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Building the model
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'This model will consist of an embedding layer, followed by three layers of GRU and
    a fully connected layer with sigmoid activation. For the optimization and accuracy
    metric, we will use an `Adam` optimizer and `binary_crossentropy`, respectively:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将包括一个嵌入层，接着是三层GRU和一个带有sigmoid激活函数的全连接层。对于优化和准确度评估，我们将分别使用`Adam`优化器和`binary_crossentropy`损失函数：
- en: 'The model is defined using the following parameters:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用以下参数定义：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Train the model with the following parameters:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下参数训练模型：
- en: Epochs = 15
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮次 = 15
- en: Validation split = 0.05
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证分割 = 0.05
- en: Batch size = 32
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小 = 32
- en: Embedding Size = 8
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入大小 = 8
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Test the model trained on a few random review sentences to verify its performance:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试在一些随机评论句子上训练的模型，以验证其性能：
- en: '| **Text** | **Predicted Score** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **文本** | **预测得分** |'
- en: '| Awesome movie | 0.9957 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 很棒的电影 | 0.9957 |'
- en: '| Terrible Movie | 0.0023 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 糟糕的电影 | 0.0023 |'
- en: '| That movie really sucks  | 0.0021 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 那部电影真糟糕 | 0.0021 |'
- en: '| I like that movie  | 0.9469 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 我喜欢那部电影 | 0.9469 |'
- en: The predicted score is close to 1 for positive sentences and close to 0 for
    negative ones. This validates our random checks on the performance of our model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 预测得分对于正面句子接近1，对于负面句子接近0。这验证了我们对模型性能的随机检查。
- en: Note that the actual scores might vary a little if you train your model on different
    hardware types.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你在不同的硬件上训练模型，实际得分可能会略有不同。
- en: Running the model on a browser using TensorFlow.js
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在浏览器中使用TensorFlow.js运行模型
- en: In this section, we are going to deploy the model on a browser.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将把模型部署到浏览器上。
- en: 'The following steps demonstrate how to save the model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤演示了如何保存模型：
- en: 'Install TensorFlow.js, which will help us format our trained model in accordance
    with what can be consumed by the browser:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装TensorFlow.js，它将帮助我们按照浏览器可使用的格式来转换训练好的模型：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Save the model in the TensorFlow.js format:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型保存为TensorFlow.js格式：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will create a json file called `model.json`, which will contain the meta-variables
    and some other files, such as `group1-shard1of1`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`model.json`的json文件，该文件将包含元变量和一些其他文件，例如`group1-shard1of1`。
- en: 'Good job! Deploying the model in the HTML file is a little trickier, however:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！不过，将模型部署到HTML文件中稍微有点复杂：
- en: For running the code mentioned in the repository, please follow the `README.md` documentation carefully
    (note the troubleshooting part, if required) regarding the settings before running
    the `Run_On_Browser.html` file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 若要运行库中提到的代码，请仔细遵循`README.md`文档中的说明（如有必要，请注意故障排除部分），确保在运行`Run_On_Browser.html`文件之前正确设置。
- en: 'Incorporate TensorFlow.js in your JavaScript through script tags:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过脚本标签将TensorFlow.js集成到你的JavaScript中：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load the model and our `token_idx` dictionary. This will help us to load the
    relevant data before processing any review from the browser:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型和我们的`token_idx`字典。这将帮助我们在处理任何来自浏览器的评论之前，加载相关数据：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add some helper functions to process the review input from the browser. This
    includes processing text, mapping words to `token_idx`, and creating sequences
    for model predictions:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一些辅助函数来处理浏览器中的评论输入。这包括处理文本、将单词映射到`token_idx`，并为模型预测创建序列：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Incorporate the predict function that processes the input sentence and uses
    the model''s predict function to return a tensor with a predicted score, as shown
    in the previous section:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集成预测函数，处理输入句子并使用模型的预测函数返回一个包含预测得分的张量，正如上一节所示：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To illustrate the entire process from a user''s point of view, open the `Run_on_Browser.html` file.
    You will see something similar to what''s shown in the following screenshot:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从用户的角度说明整个过程，打开`Run_on_Browser.html`文件。你会看到类似下面截图中的内容：
- en: '![](img/c3ecaa16-a349-4418-ad6f-00e0fa621d61.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3ecaa16-a349-4418-ad6f-00e0fa621d61.png)'
- en: The left-hand side of the screenshot denotes the website's layout, while the
    right-hand side shows the console and outputs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 截图的左侧表示网站的布局，而右侧显示了控制台和输出内容。
- en: Note that we can load the dictionary and model beforehand to make the predictions
    faster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以提前加载字典和模型，以加速预测。
- en: 'Type a review into the box provided and click submit to see the model''s predicted
    score. Try running the application with the `Awesome Movie` text:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提供的框中输入评论并点击提交按钮，以查看模型的预测得分。尝试用`Awesome Movie`文本运行应用程序：
- en: '![](img/29713226-a4ef-434b-9450-98be88b3e10f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29713226-a4ef-434b-9450-98be88b3e10f.png)'
- en: The predicted score is quite high, which indicates a positive sentiment. You
    can play around with different text to see the results.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预测得分相当高，表明情感是积极的。你可以尝试不同的文本来看结果。
- en: Note that this is mainly for illustration purposes and that if you are up for
    it, you can improve the UI through JavaScript.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这主要是为了演示目的，如果你愿意，你可以通过JavaScript改进用户界面。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter was a brief introduction to how to build an en- to-end system that
    trains a sentiment analysis model using Keras and deploys it in JavaScript using
    TensorFlow.js. The process of deploying the model in production is pretty seamless.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了如何构建一个端到端系统，使用Keras训练情感分析模型并通过TensorFlow.js将其部署到JavaScript中。将模型部署到生产环境中的过程非常顺畅。
- en: A potential next step would be to modify the JavaScript to predict sentiment
    as soon as a word is typed. As we mentioned previously, by deploying the model
    using TensorFlow.js, you can enable low- latency applications like real-time sentiment
    prediction without having to interact with the server.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的一个潜在步骤是修改JavaScript，以便在输入单词时立即预测情感。正如我们之前提到的，通过使用TensorFlow.js部署模型，你可以启用低延迟应用，如实时情感预测，而无需与服务器交互。
- en: Finally, we built a neural network in Python and deployed it in JavaScript.
    However, you can try building the entire model in JavaScript using TensorFlow.js.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在Python中构建了一个神经网络并将其部署在JavaScript中。不过，你也可以尝试使用TensorFlow.js在JavaScript中构建整个模型。
- en: In the next chapter, we will learn about Google's new library, TensorFlow Lite.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将了解谷歌的新库——TensorFlow Lite。
- en: Questions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can you evaluate the accuracy of the model if you use LSTM instead of GRUs?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用LSTM代替GRU，你能评估模型的准确性吗？
- en: What happens to the accuracy and training time if you increase Embedding Size?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你增加了嵌入大小（Embedding Size），会对准确性和训练时间产生什么影响？
- en: Can you incorporate more layers in the model? What will happen to the training
    time?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在模型中加入更多层吗？训练时间会发生什么变化？
- en: Can you modify the code  to train the model inside a browser instead of loading
    the trained model?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能修改代码，以便在浏览器内训练模型，而不是加载已训练的模型吗？
