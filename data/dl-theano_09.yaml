- en: Chapter 9. Selecting Relevant Inputs or Memories with the Mechanism of Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 使用注意力机制选择相关的输入或记忆
- en: This chapter introduces a mechanism of attention to neural network performance,
    and enables networks to improve their performance by focusing on relevant parts
    of their inputs or memories.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一种注意力机制，通过这种机制，神经网络能够通过专注于输入或记忆的相关部分来提升其性能。
- en: With such a mechanism, translations, annotations, explanations, and segmentations,
    as seen in previous chapter, enjoy greater accuracy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种机制，翻译、注释、解释和分割等，在前一章中看到的，都能获得更高的准确性。
- en: Inputs and outputs of a neural network may also be connected to *reads* and
    *writes* to an external memory. These networks, **memory networks**, are enhanced
    with an external memory and capable of deciding what information, and from where,
    to store or retrieve.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输入和输出也可以与*读取*和*写入*外部记忆相关联。这些网络，**记忆网络**，通过外部记忆增强，并能够决定存储或检索哪些信息，以及从哪里存储或检索。
- en: 'In this chapter, we''ll discuss:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论：
- en: The mechanism of attention
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制
- en: Aligning translations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对齐翻译
- en: Focus in images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中的焦点
- en: Neural Turing Machines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经图灵机
- en: Memory networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆网络
- en: Dynamic memory networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态记忆网络
- en: Differentiable mechanism of attention
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可微分的注意力机制
- en: When translating a sentence, describing the content of an image, annotating
    a sentence, or transcribing an audio, it sounds natural to focus on one part at
    a time of the input sentence or image, to get the sense of the block and transform
    it, before moving to the next part, under a certain order for global understanding.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在翻译句子、描述图像内容、注释句子或转录音频时，自然的做法是一次专注于输入句子或图像的某一部分，在理解该部分并转换后，再转向下一部分，按照一定的顺序进行全局理解。
- en: For example, in the German language, under certain conditions, verbs come at
    the end of the sentence, so, when translating to English, once the subject has
    been read and translated, a good machine translation neural network could move
    its focus to the end of the sentence to find the verb and translate it into English.
    This process of matching input positions to current output predictions is possible
    through the *mechanism of attention*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在德语中，在某些条件下，动词出现在句子的末尾。因此，在翻译成英语时，一旦主语被读取和翻译，好的机器翻译神经网络可以将注意力转向句子末尾以找到动词并将其翻译成英语。这种将输入位置与当前输出预测匹配的过程是通过*注意力机制*实现的。
- en: 'First, let''s come back to classification networks that have been designed
    with a softmax layer (see [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network*) that outputs a non-negative weight
    vector ![Differentiable mechanism of attention](img/00130.jpeg) that sums to *1*
    given an input X:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回到设计了 softmax 层的分类网络（见 [第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "第2章. 使用前馈网络分类手写数字"), *使用前馈网络分类手写数字*），该层输出一个非负权重向量 ![可微分的注意力机制](img/00130.jpeg)，对于输入
    X，该向量的和为*1*：
- en: '![Differentiable mechanism of attention](img/00131.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![可微分的注意力机制](img/00131.jpeg)'
- en: 'Then:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '![Differentiable mechanism of attention](img/00132.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![可微分的注意力机制](img/00132.jpeg)'
- en: The objective of classification is to have ![Differentiable mechanism of attention](img/00133.jpeg)
    as close as possible to *1* for the correct class *k*, and near zero for the other
    classes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的目标是使 ![可微分的注意力机制](img/00133.jpeg) 尽可能接近*1*（对于正确的类别*k*），并对其他类别接近零。
- en: 'But ![Differentiable mechanism of attention](img/00133.jpeg) is a probability
    distribution, and can also be used as a weight vector to pay attention to some
    values of a memory vector ![Differentiable mechanism of attention](img/00134.jpeg)
    at a position *k*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 ![可微分的注意力机制](img/00133.jpeg) 是一个概率分布，也可以作为一个权重向量，用来关注在位置*k*的记忆向量的某些值 ![可微分的注意力机制](img/00134.jpeg)：
- en: '![Differentiable mechanism of attention](img/00135.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![可微分的注意力机制](img/00135.jpeg)'
- en: It returns ![Differentiable mechanism of attention](img/00136.jpeg) if the weights
    focus on position *k*. Depending on the sharpness of the weights, the output will
    be more or less blurry.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重集中在位置*k*，则返回 ![可微分的注意力机制](img/00136.jpeg)。根据权重的锐度，输出将更清晰或更模糊。
- en: 'This mechanism of addressing the value of the vector *m* at a particular position
    is an **attention mechanism**: that is, it''s linear, differentiable, and has
    a back-propagation gradient descent for training on specific tasks.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定位置处理向量*m*值的这个机制就是**注意力机制**：也就是说，它是线性的、可微的，并且具有反向传播梯度下降，用于特定任务的训练。
- en: Better translations with attention mechanism
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的使用注意力机制进行翻译
- en: 'The applications for attention mechanisms are very large. To get a better understanding,
    let us first illustrate it with the example of machine translation. Attention
    mechanism aligns the source sentence and the target sentence (predicted translation),
    and avoids translation degradation for long sentences:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的应用范围非常广泛。为了更好地理解，首先让我们通过机器翻译的例子来说明它。注意力机制对齐源句子和目标句子（预测翻译），并避免长句子的翻译退化：
- en: '![Better translations with attention mechanism](img/00137.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制进行翻译](img/00137.jpeg)'
- en: 'In the previous chapter, we addressed the machine translation with an encoder-decoder
    framework and a fixed-length encoded vector *c* provided by the encoder to the
    decoder. With the attention mechanism, if each step of the encoding recurrent
    network produces a hidden state *h* *i*, the vector provided to the decoder at
    each decoding time step *t* will be variable and given by:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了使用编码器-解码器框架的机器翻译，编码器提供给解码器一个固定长度的编码向量*c*。有了注意力机制，如果每一步的编码循环网络产生一个隐藏状态*h*
    *i*，那么在每个解码时间步*t*提供给解码器的向量将是可变的，并由以下公式给出：
- en: '![Better translations with attention mechanism](img/00138.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制进行翻译](img/00138.jpeg)'
- en: 'With ![Better translations with attention mechanism](img/00139.jpeg) the alignment
    coefficients produced by a softmax function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用![更好的使用注意力机制进行翻译](img/00139.jpeg)通过softmax函数产生的对齐系数：
- en: '![Better translations with attention mechanism](img/00140.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制进行翻译](img/00140.jpeg)'
- en: 'Depending on the previous hidden state of the decoder ![Better translations
    with attention mechanism](img/00141.jpeg) and the encoding hidden states ![Better
    translations with attention mechanism](img/00142.jpeg), the embedded dot product
    between the previous decoder hidden state and each encoder hidden state produces
    a weight that describes how they should match:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据解码器的先前隐藏状态![更好的使用注意力机制进行翻译](img/00141.jpeg)和编码器的隐藏状态![更好的使用注意力机制进行翻译](img/00142.jpeg)，前一个解码器隐藏状态与每个编码器隐藏状态之间的嵌入式点积产生一个权重，描述它们应该如何匹配：
- en: '![Better translations with attention mechanism](img/00143.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制进行翻译](img/00143.jpeg)'
- en: 'After a few epochs of training, the model predicts each next word by focusing
    on a part of the input:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几个训练周期后，模型通过聚焦输入的某个部分来预测下一个词：
- en: '![Better translations with attention mechanism](img/00144.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制进行翻译](img/00144.jpeg)'
- en: To learn to align better, it is possible to use the alignment annotations present
    in the dataset, and add a cross entropy loss for the weights produced by the attention
    mechanism, to be used in the first epochs of training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地学习对齐，可以使用数据集中存在的对齐注释，并为由注意力机制产生的权重添加交叉熵损失，这可以在训练的前几个周期中使用。
- en: Better annotate images with attention mechanism
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的使用注意力机制对图像进行注释
- en: The same mechanism of attention can be applied to the tasks of annotating images
    or transcribing audio.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的注意力机制可以应用于图像注释或音频转录任务。
- en: 'For images, the attention mechanism focuses on the relevant part of the features
    at each predicting time step:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，注意力机制在每个预测时间步聚焦于特征的相关部分：
- en: '![Better annotate images with attention mechanism](img/00145.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制对图像进行注释](img/00145.jpeg)'
- en: 'Show, attend and tell: neural image caption generation with visual attention'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 展示、关注和讲述：带有视觉注意力的神经图像字幕生成
- en: 'Let''s have a look at the point of attention on images for a trained model:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下经过训练的模型在图像上的注意力点：
- en: '![Better annotate images with attention mechanism](img/00146.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![更好的使用注意力机制对图像进行注释](img/00146.jpeg)'
- en: '(*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*,
    by Kelvin Xu et al., 2015)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '(*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*，Kelvin
    Xu等，2015年)'
- en: Store and retrieve information in Neural Turing Machines
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在神经图灵机中存储和检索信息
- en: Attention mechanism can be used as an access to a part of memory in the memory-augmented
    networks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制可以作为在记忆增强网络中访问部分记忆的方式。
- en: The concept of memory in Neural Turing Machines has been inspired by both neuroscience
    and computer hardware.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图灵机中的记忆概念受到了神经科学和计算机硬件的启发。
- en: RNN hidden states to store information is not capable of storing sufficiently
    large amounts of data and retrieving it, even when the RNN is augmented with a
    memory cell, such as in the case of LSTM.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的隐藏状态用来存储信息，但它无法存储足够大的数据量并进行检索，即使RNN已被增强了一个记忆单元，如LSTM中的情况。
- en: To solve this problem, **Neural Turing Machines** (**NTM**) have been first
    designed with an **external memory bank** and read/write heads, whilst retaining
    the magic of being trained via gradient descent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，**神经图灵机**（**NTM**）首先设计了一个**外部记忆库**和读/写头，同时保留了通过梯度下降进行训练的神奇之处。
- en: 'Reading the memory bank is given by an attention on the variable memory bank
    as the attention on inputs in the previous examples:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 读取记忆库是通过对变量记忆库的注意力进行控制，类似于前面例子中对输入的注意力：
- en: '![Store and retrieve information in Neural Turing Machines](img/00147.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![在神经图灵机中存储和检索信息](img/00147.jpeg)'
- en: 'Which can be illustrated the following way:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下方式进行说明：
- en: '![Store and retrieve information in Neural Turing Machines](img/00148.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![在神经图灵机中存储和检索信息](img/00148.jpeg)'
- en: 'While writing a value to the memory bank consists of assigning our new value
    to part of the memory, thanks to another attention mechanism:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 而写入记忆库则通过另一个注意力机制将我们的新值分配到记忆的一部分：
- en: '![Store and retrieve information in Neural Turing Machines](img/00149.jpeg)![Store
    and retrieve information in Neural Turing Machines](img/00150.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![在神经图灵机中存储和检索信息](img/00149.jpeg)![在神经图灵机中存储和检索信息](img/00150.jpeg)'
- en: 'describes the information to store, and ![Store and retrieve information in
    Neural Turing Machines](img/00151.jpeg) the information to erase, and are each
    the size of the memory bank:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 描述需要存储的信息，并且![在神经图灵机中存储和检索信息](img/00151.jpeg)是需要删除的信息，并且它们的大小与记忆库相同：
- en: '![Store and retrieve information in Neural Turing Machines](img/00152.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![在神经图灵机中存储和检索信息](img/00152.jpeg)'
- en: The read and write heads are designed as in a hard drive and their mobility
    is imagined by the attention weights ![Store and retrieve information in Neural
    Turing Machines](img/00153.jpeg) and ![Store and retrieve information in Neural
    Turing Machines](img/00154.jpeg).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 读写头的设计类似于硬盘，其移动性由注意权重![在神经图灵机中存储和检索信息](img/00153.jpeg)和![在神经图灵机中存储和检索信息](img/00154.jpeg)来想象。
- en: The memory ![Store and retrieve information in Neural Turing Machines](img/00155.jpeg)
    will evolve at every timestep as the cell memory of a LSTM; but, since the memory
    bank is designed to be large, the network tends to store and organize the incoming
    data at every timestep with less interference than for any classical RNN.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆![在神经图灵机中存储和检索信息](img/00155.jpeg)将在每个时间步演变，就像LSTM的单元记忆一样；但是，由于记忆库设计得很大，网络倾向于在每个时间步将传入的数据进行存储和组织，干扰比任何经典RNN都要小。
- en: 'The process to work with the memory is naturally been driven with a recurrent
    neural network acting as a **controller** at each time step:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与记忆相关的处理过程自然是通过一个递归神经网络（RNN）在每个时间步充当**控制器**来驱动的：
- en: '![Store and retrieve information in Neural Turing Machines](img/00156.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![在神经图灵机中存储和检索信息](img/00156.jpeg)'
- en: 'The controller network outputs at each timestep:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器网络在每个时间步输出：
- en: The positioning or attention coefficients for each write/read head
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个读/写头的定位或注意系数
- en: The value to store or erase for the write heads
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写头需要存储或删除的值
- en: 'The original NTM proposes two approaches to define the *head positioning*,
    also named *addressing*, defined by the weights ![Store and retrieve information
    in Neural Turing Machines](img/00157.jpeg):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的NTM提出了两种定义*头部定位*（也称为*寻址*）的方法，定义为权重![在神经图灵机中存储和检索信息](img/00157.jpeg)：
- en: A content-based positioning, to place similar content in the same area of the
    memory, which is useful for retrieval, sorting or counting tasks:![Store and retrieve
    information in Neural Turing Machines](img/00158.jpeg)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于内容的定位，用于将相似的内容放置在记忆的同一区域，这对于检索、排序或计数任务非常有用：![在神经图灵机中存储和检索信息](img/00158.jpeg)
- en: A location-based positioning, which is based on previous position of the head,
    and can be used in copy tasks. A gate ![Store and retrieve information in Neural
    Turing Machines](img/00159.jpeg) defines the influence of the previous weights
    versus newly generated weights to compute the position of the head. A shift weight
    ![Store and retrieve information in Neural Turing Machines](img/00160.jpeg) defines
    how much to translate from the position with respect to this position.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于位置的定位，它依赖于头部的先前位置，可以在复制任务中使用。一个门控![在神经图灵机中存储和检索信息](img/00159.jpeg)定义了先前权重与新生成权重之间的影响，以计算头部的位置。一个偏移权重![在神经图灵机中存储和检索信息](img/00160.jpeg)定义了相对于该位置的位移量。
- en: 'Last, a sharpening weight ![Store and retrieve information in Neural Turing
    Machines](img/00161.jpeg) reduces the blur on the head position:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个锐化权重![在神经图灵机中存储和检索信息](img/00161.jpeg)减少了头部位置的模糊：
- en: '![Store and retrieve information in Neural Turing Machines](img/00162.jpeg)![Store
    and retrieve information in Neural Turing Machines](img/00163.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![在神经图灵机中存储和检索信息](img/00162.jpeg)![在神经图灵机中存储和检索信息](img/00163.jpeg)'
- en: All operations are differentiable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有操作都是可微分的。
- en: Many more than two heads are possible, in particular for tasks such as the addition
    of two stored values where a single read head would be limiting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不止两个头，特别是在一些任务中，如对两个存储值的加法运算，单个读取头将会受到限制。
- en: These NTM have demonstrated better capability than LSTM in tasks such as retrieving
    the next item in an input sequence, repeating the input sequence many times, or
    sampling from distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些NTM在任务中表现出比LSTM更强的能力，比如从输入序列中检索下一个项目、重复输入序列多次或从分布中采样。
- en: Memory networks
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆网络
- en: Answering questions or resolving problems given a few facts or a story have
    led to the design of a new type of networks, **memory networks**. In this case,
    the facts or the story are embedded into a memory bank, as if they were inputs.
    To solve tasks that require the facts to be ordered or to create transitions between
    the facts, memory networks use a recurrent reasoning process in multiple steps
    or hops on the memory banks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一些事实或故事来回答问题或解决问题，促使设计出一种新型网络——**记忆网络**。在这种情况下，事实或故事被嵌入到一个记忆库中，就像它们是输入一样。为了完成需要排序事实或在事实之间创建转换的任务，记忆网络使用递归推理过程，在多个步骤或跳跃中操作记忆库。
- en: 'First, the query or question *q* is converted into a constant input embedding:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，查询或问题*q*被转换成常量输入嵌入：
- en: '![Memory networks](img/00164.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00164.jpeg)'
- en: 'While, at each step of the reasoning, the facts *X* to answer the question
    are embedded into two memory banks, where the embedding coefficients are a function
    of the timestep:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 而在每个推理步骤中，回答问题的事实*X*被嵌入到两个记忆库中，其中嵌入系数是时间步长的函数：
- en: '![Memory networks](img/00165.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00165.jpeg)'
- en: 'To compute attention weights:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算注意力权重：
- en: '![Memory networks](img/00166.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00166.jpeg)'
- en: 'And:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 并且：
- en: '![Memory networks](img/00167.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00167.jpeg)'
- en: 'Selected with the attention:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 选择了带有注意力机制：
- en: '![Memory networks](img/00168.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00168.jpeg)'
- en: 'The output at each reasoning time step is then combined with the identity connection,
    as seen previously to improve the efficiency of the recurrency:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个推理时间步骤的输出随后与身份连接组合，如前所述，以提高递归效率：
- en: '![Memory networks](img/00169.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00169.jpeg)'
- en: 'A linear layer and classification softmax layer are added to the last ![Memory
    networks](img/00170.jpeg):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个线性层和分类softmax层被添加到最后的![记忆网络](img/00170.jpeg)：
- en: '![Memory networks](img/00171.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![记忆网络](img/00171.jpeg)'
- en: Episodic memory with dynamic memory networks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有动态记忆网络的情节记忆
- en: 'Another design has been introduced with dynamic memory networks. First, the
    N facts are concatenated with a separator token and then encoded with a RNN: the
    output of the RNN at each separation ![Episodic memory with dynamic memory networks](img/00172.jpeg)
    is used as input embedding. This way to encode facts is more natural and also
    preserves time dependency. The question is also encoded with an RNN to produce
    a vector *q*.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种设计通过动态记忆网络被引入。首先，N个事实与分隔符令牌连接在一起，然后通过RNN编码：RNN在每个分隔符处的输出![具有动态记忆网络的情节记忆](img/00172.jpeg)被用作输入嵌入。这样的编码方式更加自然，同时也保留了时间依赖性。问题也通过RNN进行编码以生成向量*q*。
- en: 'Secondly, the memory bank is replaced with an episodic memory, relying on an
    attention mechanism mixed with an RNN, in order to preserve time dependency between
    the facts as well:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，记忆库被替换为情节记忆，依赖于混合了RNN的注意力机制，以便保留事实之间的时间依赖关系：
- en: '![Episodic memory with dynamic memory networks](img/00173.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![带有动态记忆网络的情节记忆](img/00173.jpeg)'
- en: The gates ![Episodic memory with dynamic memory networks](img/00174.jpeg) are
    given by a multilayer perceptron depending on the previous state of reasoning
    ![Episodic memory with dynamic memory networks](img/00175.jpeg), the question
    and the input embedding ![Episodic memory with dynamic memory networks](img/00176.jpeg)
    as inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 门控 ![带有动态记忆网络的情节记忆](img/00174.jpeg)由多层感知器提供，依赖于推理的前一个状态 ![带有动态记忆网络的情节记忆](img/00175.jpeg)、问题和输入嵌入
    ![带有动态记忆网络的情节记忆](img/00176.jpeg)作为输入。
- en: 'The reasoning occurs the same way with a RNN:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程与RNN相同：
- en: '![Episodic memory with dynamic memory networks](img/00177.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![带有动态记忆网络的情节记忆](img/00177.jpeg)'
- en: 'The following picture illustrates the interactions between inputs and outputs
    to compute the episodic memories:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了输入和输出之间的相互作用，以计算情节记忆：
- en: '![Episodic memory with dynamic memory networks](img/00178.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![带有动态记忆网络的情节记忆](img/00178.jpeg)'
- en: 'Ask Me Anything: dynamic memory networks for natural language processing'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 问我任何事：自然语言处理的动态记忆网络
- en: To benchmark these networks, Facebook research has synthetized the bAbI dataset,
    using NLP tools to create facts, questions, and answers for some random modeled
    stories. The dataset is composed of different tasks to test different reasoning
    skills, such as reasoning on one, two, or three facts, in time, size, or position,
    counting, listing, or understanding relations between arguments, negations, motivations,
    and finding paths.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这些网络进行基准测试，Facebook研究通过合成bAbI数据集，使用NLP工具为一些随机建模的故事创建事实、问题和答案。该数据集由不同的任务组成，用于测试不同的推理技能，例如基于时间、大小或位置的单个、两个或三个事实推理、计数、列举或理解论点之间的关系、否定、动机以及路径查找。
- en: 'As for guided alignment in machine translation, when the dataset also contains
    the annotations for the facts leading to the answer, it is also possible to use
    supervised training for:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 至于机器翻译中的引导对齐，当数据集也包含了导致答案的事实注释时，也可以使用监督训练：
- en: The attention mechanism
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制
- en: When to stop the reasoning loop, producing a stop token, when the number of
    facts used is sufficient to answer the question
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当推理循环停止时，生成一个停止标记，判断使用的事实数量是否足够回答问题
- en: Further reading
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to these topics for more insights:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下主题以获取更多见解：
- en: '*Ask Me Anything: Dynamic Memory Networks for Natural Language Processing*,Ankit
    Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani,
    Victor Zhong, Romain Paulus, Richard Socher, 2015'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问我任何事：自然语言处理的动态记忆网络*，Ankit Kumar，Ozan Irsoy，Peter Ondruska，Mohit Iyyer，James
    Bradbury，Ishaan Gulrajani，Victor Zhong，Romain Paulus，Richard Socher，2015年'
- en: '*Attention and Augmented Recurrent Neural Networks*, Chris Olah, Shan Carter,
    Sept 2016 [http://distill.pub/2016/augmented-rnns/](http://distill.pub/2016/augmented-rnns/)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力与增强型循环神经网络*，Chris Olah，Shan Carter，2016年9月 [http://distill.pub/2016/augmented-rnns/](http://distill.pub/2016/augmented-rnns/)'
- en: '*Guided Alignment training for Topic Aware Neural Machine Translation*, Wenhu
    Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter, Jul 2016'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面向话题的神经机器翻译的引导对齐训练*，陈文虎，Evgeny Matusov，Shahram Khadivi，Jan-Thorsten Peter，2016年7月'
- en: '*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*,
    Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard
    Zemel, Yoshua Bengio, Fev 2015'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*展示、注意与叙述：具有视觉注意力的神经图像字幕生成*，Kelvin Xu，Jimmy Ba，Ryan Kiros，Kyunghyun Cho，Aaron
    Courville，Ruslan Salakhutdinov，Richard Zemel，Yoshua Bengio，2015年2月'
- en: '*Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks*,
    Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer,
    Armand Joulin, Tomas Mikolov,2015'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迈向AI完全问题回答：一组先决条件玩具任务*，Jason Weston，Antoine Bordes，Sumit Chopra，Alexander
    M. Rush，Bart van Merriënboer，Armand Joulin，Tomas Mikolov，2015年'
- en: '*Memory Networks*, Jason Weston, Sumit Chopra, Antoine Bordes,2014'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆网络*，Jason Weston，Sumit Chopra，Antoine Bordes，2014年'
- en: '*End-To-End Memory Networks*, Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
    Rob Fergus, 2015'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*端到端记忆网络*，Sainbayar Sukhbaatar，Arthur Szlam，Jason Weston，Rob Fergus，2015年'
- en: '*Neural Turing Machines*, Alex Graves, Greg Wayne, Ivo Danihelka, 2014'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经图灵机*，Alex Graves，Greg Wayne，Ivo Danihelka，2014年'
- en: '*Deep Visual-Semantic Alignments for Generating Image Descriptions*, Andrej
    Karpathy, Li Fei-Fei, 2014'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度视觉-语义对齐用于生成图像描述*，Andrej Karpathy，Li Fei-Fei，2014'
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The attention mechanism is a smart option to help neural networks select the
    right information and focus to produce the correct output. It can be placed either
    directly on the inputs or the features (inputs processed by a few layers). Accuracies
    in the cases of translation, image annotation, and speech recognition, are increased,
    in particular when the dimension of the inputs is important.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是帮助神经网络选择正确信息并集中精力以生成正确输出的聪明选择。它可以直接应用于输入或特征（输入经过几层处理）。在翻译、图像标注和语音识别等任务中，尤其是在输入维度很重要时，准确率得到了提升。
- en: 'Attention mechanism has led to new types of networks enhanced with external
    memory, working as an input/output, from which to read or to which to write. These
    networks have proved to be very powerful in question-answering challenges, into
    which most tasks in natural language processing can can be cast: tagging, classification,
    sequence-to-sequence, or question answering tasks.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制引入了增强了外部记忆的新型网络，作为输入/输出，可以从中读取或写入。这些网络已被证明在问答挑战中非常强大，几乎所有自然语言处理任务都可以转化为此类任务：标注、分类、序列到序列，或问答任务。
- en: In the next chapter, we'll see more advanced techniques and their application
    to the more general case of recurrent neural networks, to improve accuracy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍更高级的技巧及其在更一般的递归神经网络中的应用，以提高准确性。
