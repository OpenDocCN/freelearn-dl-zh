- en: Building Speech Recognition with DeepSpeech2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DeepSpeech2构建语音识别
- en: It's been a great journey, building awesome deep learning projects in Python
    using image, text, and sound data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段很棒的旅程，在Python中使用图像、文本和声音数据构建了出色的深度学习项目。
- en: We've been working quite heavily on language models in building chatbots in
    our previous chapters. Chatbots are a powerful tool for customer engagement and
    the automation of a wide range of business processes from customer service to
    sales. Chatbots enable the automation of repetitive and/or redundant interactions
    such as frequently asked questions or product-ordering workflows. This automation
    saves time and money for businesses and enterprises. If we've done our job well
    as deep-learning engineers, it also means that the consumers are receiving a much-improved
    **user experience** (**UX**) as a result.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们在构建聊天机器人中相当重视语言模型。聊天机器人是客户参与和各种业务流程自动化的强大工具，从客户服务到销售。聊天机器人实现了重复和/或多余交互的自动化，如常见问题或产品订购工作流程。这种自动化为企业节省了时间和金钱。如果我们作为深度学习工程师做得很好，这也意味着消费者因此而获得了更好的**用户体验**（**UX**）。
- en: The new interaction between a business and its customers via a chatbot is very
    effective in each party receiving value. Let's look at the interaction scenario
    and see if we can identify any constraints that should be the focus of our next
    project. Up until now, all of our chat interactions have been through text. Let's
    think about what this means for the consumer. Text interactions are often (but
    not exclusively) initiated via mobile devices. Secondly, chatbots open up a new
    **user interaction** (**UI**)—for conversational UI. Part of the power of conversational
    UI is that it can remove the constraint of the physical keyboard and open the
    range of locations and devices that are now possible for this interaction to take
    place.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聊天机器人进行新的业务与客户之间的互动非常有效，每一方都能获得价值。让我们看看互动场景，并确定我们下一个项目应该关注的任何约束。到目前为止，我们所有的聊天互动都是通过文本进行的。让我们思考这对消费者意味着什么。文本互动通常（但不限于）通过移动设备发起。其次，聊天机器人开辟了一种新的**用户界面**（**UI**）—对话式UI。对话式UI的强大之处在于它可以消除物理键盘的限制，并打开了现在可以进行此类互动的位置和设备范围。
- en: Conversational UI is made possible by speech recognition systems working through
    popular devices, such as your smartphone with Apple's Siri, Amazon's Echo, and
    Google Home. It's very cool technology, consumers love it, and businesses that
    adopt this technology gain an advantage over those in their industry that do not
    keep up with the times.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过流行设备（如您的智能手机与苹果的Siri、亚马逊的Echo和谷歌Home）进行的语音识别系统使得会话界面成为可能。这是非常酷的技术，消费者喜爱它，采用这项技术的企业在其行业中获得了优势。
- en: In this chapter, we will build a system that recognizes English speech, using
    the **DeepSpeech2** (**DS2**) model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个使用**DeepSpeech2**（**DS2**）模型识别英语语音的系统。
- en: 'You will learn the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学到以下内容：
- en: To work with speech and spectrograms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了处理语音和频谱图
- en: To build an end-to-end speech recognition system
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个端到端的语音识别系统
- en: The **Connectionist Temporal Classification** (**CTC**) loss function
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接主义时序分类**（**CTC**）损失函数'
- en: Batch normalization and SortaGrad for **recurrent neural networks** (**RNNs**)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于**递归神经网络**（**RNNs**）的批归一化和SortaGrad
- en: Let's get started and deep dive into the speech data, learn to feature engineer
    the speech data, extract various kinds of features from it, and then build a speech
    recognition system that can detect your or a registered user's voice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始深入研究语音数据，学习如何从中提取特征工程的语音数据，提取各种类型的特征，然后构建一个可以检测您或注册用户声音的语音识别系统。
- en: '**Define the goal**: The goal of this project is to build and train an **automatic
    speech recognition** (**ASR**) system to take in and convert an audio call to
    text that could then be used as input for a text-based chatbot that could understand
    and respond.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义目标**：本项目的目标是构建和训练一个**自动语音识别**（**ASR**）系统，用于接收并转换音频呼叫为文本，然后可以作为文本聊天机器人的输入，能够理解并作出响应。'
- en: Data preprocessing
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: In this project, we will use *LibriSpeech ASR corpus* ([http://www.openslr.org/12/](http://www.openslr.org/12/)),
    which is 1,000 hours of 16 kHz-read English speech.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们将使用*LibriSpeech ASR corpus*（[http://www.openslr.org/12/](http://www.openslr.org/12/)），这是1,000小时的16
    kHz英语语音数据。
- en: 'Let''s use the following commands to download the corpus and unpack the LibriSpeech
    data:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令来下载语料库并解压缩LibriSpeech数据：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will take a while and once the process is completed, we will have the
    `data` folder structure, as shown in the following screenshot:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程需要一些时间，完成后我们将得到如下截图所示的`data`文件夹结构：
- en: '![](img/6808431a-97af-4fd3-ba24-a023f5ad024a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6808431a-97af-4fd3-ba24-a023f5ad024a.png)'
- en: We now have three folders named as `train-clean-100`, `dev-clean`, and `test-clean`.
    Each folder will have subfolders that are the associated IDs used for mapping
    the small segment of the transcript and the audio. All the audio files are in
    the `.flac` extension, and all the folders will have one `.txt` file, which is
    the transcript for the audio files.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有三个文件夹，分别名为`train-clean-100`、`dev-clean`和`test-clean`。每个文件夹都有一些子文件夹，这些子文件夹包含与成绩单和音频的小段落映射相关的ID。所有音频文件都为`.flac`扩展名，每个文件夹中都会有一个`.txt`文件，这是音频文件的成绩单。
- en: Corpus exploration
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库探索
- en: 'Let''s explore the dataset in detail. First, let''s look into the audio file
    by reading it from the file and plotting it. To read the audio file, we will use
    the `pysoundfile` package with the following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨数据集。首先，我们通过从文件读取音频文件并绘制它来查看音频文件。为了读取音频文件，我们将使用`pysoundfile`包，命令如下：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will import the modules, read the audio files, and plot them with
    the following code block:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将导入模块，读取音频文件，并通过以下代码块绘制它们：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the frequency representation of each segment of speech:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个语音段的频率表示：
- en: '![](img/3b80d1ea-0131-486a-9078-b725e20bcfe2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b80d1ea-0131-486a-9078-b725e20bcfe2.png)'
- en: The raw audio signal plot from the audio MIDI file
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来自音频MIDI文件的原始音频信号图
- en: 'Now let''s look into the content of the transcript text file. It''s a clean
    version of the text with the audio file IDs in the beginning and the associated
    text following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下成绩单文本文件的内容。它是文本的清洁版本，开头是音频文件ID，后面是相关的文本：
- en: '![](img/9f89089e-8ebc-49c0-adcb-2439d9151ad6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f89089e-8ebc-49c0-adcb-2439d9151ad6.png)'
- en: '![](img/2f46caaa-5309-4215-a84d-e86145b25560.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f46caaa-5309-4215-a84d-e86145b25560.png)'
- en: The transcript data is stored a specific format. Left numbers are the midi file
    name and the right part is the actually transcript. This helps in building the
    mapping between the midi file and its respective transcript.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 成绩单数据以特定格式存储。左边的数字是midi文件名，右边是实际的成绩单。这有助于建立midi文件与其相应成绩单之间的映射。
- en: What we see is that each audio file is the narration of the transcript contained
    in the file. Our model will try to learn this sequence pattern. But before we
    work on the model, we need to extract some features from the audio file and convert
    the text into one-hot encoding format.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每个音频文件都是该文件中成绩单的叙述。我们的模型将尝试学习这个序列模式。但在我们处理模型之前，我们需要从音频文件中提取一些特征，并将文本转换为独热编码格式。
- en: Feature engineering
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: So, before we feed the raw audio data into our model, we need to transform the
    data into numerical representations that are features. In this section, we will
    explore various techniques to extract features from the speech data that we can
    use to feed into the model. The accuracy and performance of the model vary based
    on the type of features we use. As an inquisitive deep-learning engineer, it's
    your opportunity to explore and learn the features with these techniques and use
    the best one for the use case at hand.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在将原始音频数据输入我们的模型之前，我们需要将数据转化为数值表示，即特征。在本节中，我们将探索从语音数据中提取特征的各种技术，这些特征可以用于输入到模型中。模型的准确性和性能取决于我们使用的特征类型。作为一个富有好奇心的深度学习工程师，这是你探索和学习特征的机会，并使用最适合当前用例的特征。
- en: 'The following table gives us a list of techniques and their properties:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格给出了技术及其属性的列表：
- en: '| **Techniques** | **Properties** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **属性** |'
- en: '| **Principal component analysis** (**PCA**) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **主成分分析** (**PCA**) |'
- en: Eigenvector-based method
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征向量的方法
- en: Non-linear feature extraction method
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性特征提取方法
- en: Supported to linear map
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持线性映射
- en: Faster than other techniques
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比其他技术更快
- en: Good for Gaussian data
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适合高斯数据
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Linear discriminate analysis** (**LDA**) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **线性判别分析** (**LDA**) |'
- en: Linear feature extraction method
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性特征提取方法
- en: Supported to the supervised linear map
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持监督式线性映射
- en: Faster than other techniques
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比其他技术更快
- en: Better than PCA for classification
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分类来说优于PCA
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Independent component analysis** (**ICA**) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **独立成分分析** (**ICA**) |'
- en: Blind course separation method
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盲源分离方法
- en: Support to linear map
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持线性映射
- en: Iterative in nature
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本质上是迭代的
- en: Good for non-Gaussian data
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适合非高斯数据
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Cepstral analysis |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 倒谱分析 |'
- en: Static feature extraction method
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态特征提取方法
- en: Power spectrum method
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功率谱方法
- en: Used to represent spectral envelope
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于表示谱包络
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Mel-frequency scale analysis |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 梅尔频率尺度分析 |'
- en: Static feature extraction method
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态特征提取方法
- en: Spectral analysis method
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱分析方法
- en: Mel scale is calculated
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算梅尔尺度
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Mel-frequency cepstral coefficient** (**MFFCs**) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **梅尔频率倒谱系数** (**MFCCs**) |'
- en: Power spectrum is computed by performing Fourier Analysis
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功率谱是通过进行傅里叶分析来计算的
- en: Robust and dynamic method for speech feature extraction
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音特征提取的稳健且动态的方法
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Wavelet technique |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 小波技术 |'
- en: Better time resolution than Fourier transform
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比傅里叶变换更好的时间分辨率
- en: Real-time factor is minimum
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时因子最小
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'The MFCC technique is the most efficient and is often used for the extraction
    of speech features for speech recognition. The MFCC is based on the known variation
    of the human ear''s critical bandwidth frequencies, with filters spaced linearly
    at low frequencies. The process of MFCC is shown in the following diagram:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MFCC技术是最有效的，通常用于提取语音识别的语音特征。MFCC基于人耳的临界带宽频率的已知变化，低频部分的滤波器是线性间隔的。MFCC的过程如下图所示：
- en: '![](img/0d01cbae-4f9a-4440-b02d-f7d41ce7de6d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d01cbae-4f9a-4440-b02d-f7d41ce7de6d.png)'
- en: Block diagram of MFCC process
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MFCC 过程的框图
- en: For our implementation purposes, we are not going to perform each step; instead,
    we will use a Python package called `python_speech_features` that provides common
    speech features for ASR, including MFCCs and filterbank energies.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实现目的，我们不打算执行每个步骤；相反，我们将使用一个名为`python_speech_features`的Python包，它提供了ASR常用的语音特征，包括MFCC和滤波器组能量。
- en: 'Let''s `pip install` the package with the following command:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令`pip install`安装该包：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So, let''s define a function that will normalize the audio time series data
    and extract the MFCC features:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们定义一个函数来归一化音频时间序列数据并提取MFCC特征：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s plot the audio and MFCC features and visualize them:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制音频和MFCC特征并可视化它们：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is the output of the spectrogram:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是频谱图的输出：
- en: '![](img/ed8d2416-9bc1-41d2-89dc-4e54c4bc9f39.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed8d2416-9bc1-41d2-89dc-4e54c4bc9f39.png)'
- en: Data transformation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: Once we have all the features that we need to feed into the model, we will transform
    the raw NumPy tensors into the TensorFlow specific format called `TFRecords`*. *
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了需要输入模型的所有特征，我们将把原始的NumPy张量转换为TensorFlow特定的格式——`TFRecords`*。*
- en: 'In the following code snippet, we are creating the folders to store all the
    processed records. The `make_example()` function creates the sequence example
    for a single utterance given the sequence length, MFCC features, and corresponding
    transcript. Multiple sequence records are then written into TFRecord files using
    the `tf.python_io.TFRecordWriter()` function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们正在创建文件夹以存储所有已处理的记录。`make_example()`函数根据序列长度、MFCC特征和相应的转录创建单一发音的序列示例。然后，使用`tf.python_io.TFRecordWriter()`函数将多个序列记录写入TFRecord文件：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'All the data-processing code is written in the `preprocess_LibriSpeech.py`
    file, which will perform all the previously mentioned data manipulation part,
    and once the operation is complete, the resulting processed data gets stored at
    the `data/librispeech/processed/` location. Use the following command to run the
    file:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据处理代码都写在`preprocess_LibriSpeech.py`文件中，该文件将执行所有先前提到的数据处理部分，操作完成后，处理后的数据将存储在`data/librispeech/processed/`位置。使用以下命令运行该文件：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: DS2 model description and intuition
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DS2模型描述与直觉
- en: 'DS2 architecture is composed of many layers of recurrent connections, convolutional
    filters, and non-linearities, as well as the impact of a specific instance of
    batch normalization, applied to RNNs, as shown here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: DS2架构由多个递归连接层、卷积滤波器和非线性层组成，还包括批量归一化对RNN的特定实例的影响，如下所示：
- en: '![](img/67764589-524c-4e06-b44b-8b70180284fc.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67764589-524c-4e06-b44b-8b70180284fc.png)'
- en: To learn from datasets with a large amount of data, DS2 model's capacity is
    increased by adding more depth. The architectures are made up to 11 layers of
    many bidirectional recurrent layers and convolutional layers. To optimize these
    models successfully, batch normalization for RNNs and a novel optimization curriculum
    called SortaGrad were used.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从包含大量数据的数据集进行学习，DS2模型通过增加更多深度来提升容量。架构由最多11层的双向递归层和卷积层组成。为了成功优化这些模型，使用了RNN的批量归一化和一种名为SortaGrad的新优化课程。
- en: 'The training data is a combination of input sequence `x(i)` and the transcript
    `y(i)`, whereas the goal of the RNN layers is to learn the features between `x(i)`
    and `y(i)`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据是输入序列`x(i)`和转录`y(i)`的组合，而RNN层的目标是学习`x(i)`和`y(i)`之间的特征：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The spectrogram of power normalized audio clips are used as the features to
    the system and the outputs of the network are the graphemes of each language.
    In terms of adding non-linearity, the clipped **rectified linear unit** (**ReLU**)
    function *σ(x) = min{max{x, 0}, 20}* was used. After the bidirectional recurrent
    layers, one or more fully connected layers are placed and the output layer *L*
    is a softmax, computing a probability distribution over characters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 用于系统特征的输入是功率归一化后的音频剪辑的谱图，网络的输出是每种语言的字形。为了增加非线性，使用了**修正线性单元**（**ReLU**）函数，*σ(x)
    = min{max{x, 0}, 20}*。在双向循环层之后，放置一个或多个全连接层，输出层*L*是softmax层，计算字符的概率分布。
- en: Now let's look into the implementation of the DS2 architecture. You can find
    the full code [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter07](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter07).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解DS2架构的实现。你可以在这里找到完整代码：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter07](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter07)。
- en: 'The following is what the model looks like in TensorBoard:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型在TensorBoard中的显示：
- en: '![](img/aa7975fa-5aa0-439f-b254-8d78dcde3663.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa7975fa-5aa0-439f-b254-8d78dcde3663.png)'
- en: For the convolution layers, we have the kernel of size `[11, input_seq_length,
    number_of_filter]` followed by the 2D convolution operation on the input sequence,
    and then `dropout` is applied to prevent overfitting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积层，我们有一个大小为`[11, input_seq_length, number_of_filter]`的卷积核，接着对输入序列进行2D卷积操作，然后应用`dropout`以防止过拟合。
- en: 'The following code segment executes these steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段执行这些步骤：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we next have the recurrent layer, where we reshape the output of the convolution
    layer to fit the data into the RNN layer. Then, the custom RNN cells are created
    based on the hyperparameter called `rnn_type`, which can be of two types, uni-directional
    or bi-directional, followed by the dropout cells.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入循环层，在这里我们将卷积层的输出重塑，以便将数据适配到RNN层。然后，根据一个叫做`rnn_type`的超参数创建自定义的RNN单元，它可以是单向的或双向的，之后是dropout单元。
- en: 'The following code block creates the RNN part of the model:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块创建了模型的RNN部分：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Further more, the linear layer is created to perform the CTC loss function
    and output from the softmax layer:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，创建了线性层来执行CTC损失函数，并从softmax层输出结果：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Production scale tip**: Training a single model at these scales requires
    tens of exaFLOPs that would take three to six weeks to execute on a single GPU.
    This makes model exploration a very time-consuming exercise, so the developers
    of DeepSpeech have built a highly optimized training system that uses eight or
    16 GPUs to train one model, as well as synchronous **stochastic gradient descent**
    (**SGD**), which is easier to debug while testing new ideas, and also converges
    faster for the same degree of data parallelism.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**生产规模提示**：在这些规模下训练单个模型需要数十个exaFLOP，单GPU执行这些任务需要三到六周的时间。这使得模型探索变得非常耗时，因此DeepSpeech的开发者们构建了一个高度优化的训练系统，使用八个或十六个GPU来训练一个模型，并且使用同步**随机梯度下降**（**SGD**），这种方法在测试新想法时更容易调试，同时对于相同的数据并行度也能更快地收敛。'
- en: Training the model
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now that we understand the data that we are using and the DeepSpeech model architecture,
    let's set up the environment to train the model. There are some preliminary steps
    to create a virtual environment for the project that are optional, but always
    recommended to use. Also, it's recommended to use GPUs to train these models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了使用的数据和DeepSpeech模型架构，接下来让我们设置环境以训练模型。创建项目的虚拟环境有一些预备步骤，这些步骤是可选的，但强烈建议使用。另外，建议使用GPU来训练这些模型。
- en: 'Along with Python Version 3.5 and TensorFlow version 1.7+, the following are
    some of the prerequisites:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Python版本3.5和TensorFlow版本1.7+，以下是一些先决条件：
- en: '`python-Levenshtein`: To compute **character error rate** (**CER**), basically
    the distance'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python-Levenshtein`：用来计算**字符错误率**（**CER**），基本上是计算距离'
- en: '`python_speech_features`: To extract MFCC features from raw data'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python_speech_features`：用来从原始数据中提取MFCC特征'
- en: '`pysoundfile`: To read FLAC files'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pysoundfile`：用来读取FLAC文件'
- en: '`scipy`: Helper functions for windowing'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scipy`：用于窗口化的辅助函数'
- en: '`tqdm`: For displaying a progress bar'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tqdm`: 用于显示进度条'
- en: 'Let''s create the virtual environment and install all the dependencies:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建虚拟环境并安装所有依赖项：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Install the following dependencies:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 安装以下依赖项：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Install TensorFlow with GPU support:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 安装支持GPU的TensorFlow：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you see a `sndfile` error, use the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果看到`sndfile`错误，请使用以下命令：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now you will need to clone the repository that contains all the code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要克隆包含所有代码的仓库：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s move the TFRecord files that we created in the *Data transformation*
    section. The computed MFCC features are stored inside the `data/librispeech/processed/` directory:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们移动在*数据转换*部分中创建的TFRecord文件。计算得到的MFCC特征存储在`data/librispeech/processed/`目录中：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once we have all the data files in place, it''s time to train the model. We
    are defining four hyperparameters as `num_rnn_layers` set to `3`, `rnn_type` set
    to `bi-dir`, `max_steps` is set to `30000`, and `initial_lr` is set to `3e-4`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将所有数据文件准备好，就可以开始训练模型。我们定义了四个超参数：`num_rnn_layers`设置为`3`，`rnn_type`设置为`bi-dir`，`max_steps`设置为`30000`，`initial_lr`设置为`3e-4`：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Also, if you want to resume the training using the pre-trained models from
    [https://drive.google.com/file/d/1E65g4HlQU666RhgY712Sn6FuU2wvZTnQ/view](https://drive.google.com/file/d/1E65g4HlQU666RhgY712Sn6FuU2wvZTnQ/view),
    you can download and unzip them to the `logs` folder:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想使用[https://drive.google.com/file/d/1E65g4HlQU666RhgY712Sn6FuU2wvZTnQ/view](https://drive.google.com/file/d/1E65g4HlQU666RhgY712Sn6FuU2wvZTnQ/view)中的预训练模型继续训练，可以下载并解压到`logs`文件夹中：
- en: '`(SpeechRecog)$python deepSpeech_train.py --checkpoint_dir ./logs/ --max_steps
    40000`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`(SpeechRecog)$python deepSpeech_train.py --checkpoint_dir ./logs/ --max_steps
    40000`'
- en: Note that during the first epoch, the cost will increase and it will take longer
    to train on later steps because the utterances are presented in a sorted order
    to the network.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第一个epoch期间，成本将增加，且随着后续步骤的训练，训练时间会变得更长，因为语音样本是按照排序顺序呈现给网络的。
- en: 'The following are the steps involved during the training process:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练过程中涉及的步骤：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'While the training process happens, we can see significant improvements, as
    shown in the following plots. Following graph shows the accuracy of the plot after
    50k steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们可以看到显著的改进，如下图所示。以下图表显示了50k步之后的准确率：
- en: '![](img/b30c4288-74b4-44ab-8445-3713db09822e.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b30c4288-74b4-44ab-8445-3713db09822e.png)'
- en: 'Here are the loss plots over 50k steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是50k步骤的损失图：
- en: '![](img/df80ad19-b089-4b41-b243-14b2d50b00fc.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df80ad19-b089-4b41-b243-14b2d50b00fc.png)'
- en: 'The learning rate is slowing down over the period of time:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率随着时间的推移逐渐减小：
- en: '![](img/58d9769f-6c43-4ede-834a-b32a644cbef8.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58d9769f-6c43-4ede-834a-b32a644cbef8.png)'
- en: Testing and evaluating the model
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和评估模型
- en: 'Once the model is trained, you can perform the following command to execute
    the `test` steps using the `test` dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，你可以执行以下命令，使用`test`数据集执行`test`步骤：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We evaluate its performance by testing it on previously unseen utterances from
    a `test` set. The model generates sequences of probability vectors as outputs,
    so we need to build a decoder to transform the model''s output into word sequences. Despite
    being trained on character sequences, DS2 models are still able to learn an implicit
    language model and are already quite adept at spelling out words phonetically,
    as shown in the following table. The model''s spelling performance is typically
    measured using CERs calculated using the Levenshtein distance ([https://en.wikipedia.org/wiki/Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance)) at
    the character level:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在先前未见过的语音样本上测试模型来评估其性能。模型生成概率向量的序列作为输出，因此我们需要构建解码器，将模型的输出转换为单词序列。尽管在字符序列上训练，DS2模型仍能学习到一个隐式的语言模型，并且已经相当擅长根据发音拼写单词，正如以下表格所示。模型的拼写性能通常通过计算基于Levenshtein距离（[https://en.wikipedia.org/wiki/Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance)）的字符级CER来衡量：
- en: '| Ground truth | Model output |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 真实值 | 模型输出 |'
- en: '| --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| This had some effect in calming him | This had some offectind calming him
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 这对他起到了安抚作用 | 这对他起到了安抚作用 |'
- en: '| He went in and examined his letters but there was nothing from carrier |
    He went in an examined his letters but there was nothing from carry |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 他进去查看了信件，但没有来自快递员的信 | 他进去查看了信件，但没有来自快递员的信 |'
- en: '| The design was different but the thing was clearly the same | The design
    was differampat that thing was clarly the same |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 设计不同，但东西显然是一样的 | 设计不同，但东西显然是一样的 |'
- en: Although the model exhibit excellent CERs, their tendency to spell out words
    phonetically results in relatively high word-error rates. You can improve the
    model's performance **word-error rate** (**WER**) by allowing the decoder to incorporate
    constraints from an external lexicon and language model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型展示了优秀的CER（字符错误率），但它们倾向于按音标拼写单词，导致相对较高的词错误率（WER）。你可以通过允许解码器结合外部词典和语言模型的约束来改善模型的**词错误率**（**WER**）。
- en: We have observed that many of the errors in the model's predictions occur in
    words that do not appear in the training set. It is thus reasonable to expect
    that the overall CER would continue to improve as we increased the size of the
    training set and training steps. It achieved 15% CERs after 30k steps or training.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，模型预测中的许多错误发生在训练集中没有出现的单词上。因此，随着我们增加训练集的大小和训练步骤，整体CER有望持续改善。经过30k步训练后，它达到了15%的CER。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We dove right into this deep-learning project in Python, creating and training
    an ASR model that understands speech data. We learned to feature engineer the
    speech data to extract various kinds of features from it and then build a speech
    recognition system that could detect a user's voice.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接进入了这个Python深度学习项目，创建并训练了一个能够理解语音数据的ASR模型。我们学习了如何对语音数据进行特征工程，从中提取各种特征，然后构建一个能够识别用户声音的语音识别系统。
- en: We're happy to have achieved our stated goal!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很高兴达成了我们设定的目标！
- en: In this chapter, we built a system that recognizes English speech, using the
    DS2 model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了一个识别英语语音的系统，使用了DS2模型。
- en: 'You learned following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习了以下内容：
- en: To work with speech and spectrograms
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语音和声谱图进行工作
- en: To build an end-to-end speech recognition system
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个端到端的语音识别系统
- en: The CTC loss function
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CTC损失函数
- en: Batch normalization and SortaGrad for RNNs
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化和用于RNN的SortaGrad
- en: This caps off a major section of the deep-learning projects in this Python book
    that explores chatbots, NLP, and speech recognition with RNNs (uni and bi-directional,
    with and without LSTM components), and CNNs. We've seen the power of these technologies
    to provide intelligence to existing business processes and to create entirely
    new and smart systems. This is exciting work at the cutting edge of applied AI
    using deep learning! In the remaining half of the book, we'll explore deep-learning
    projects in Python that are generally grouped into computer vision technologies.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着本书中深度学习项目的一个主要部分的结束，这些项目涉及聊天机器人、自然语言处理（NLP）和使用RNN（单向和双向，包括或不包括LSTM组件）以及CNN的语音识别。我们已经看到了这些技术为现有业务流程提供智能，以及创造全新智能系统的潜力。这是应用AI领域中令人兴奋的前沿工作，利用深度学习推动创新！在本书的下半部分，我们将探索通常归类为计算机视觉技术的Python深度学习项目。
- en: Let's turn the page and get started!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们翻到下一页，开始吧！
