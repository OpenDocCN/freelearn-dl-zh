- en: '*Chapter 6*: AI Painter'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：AI画家'
- en: In this chapter, we are going to look at two **generative adversarial networks**
    (**GANs**) that could be used to generate and edit images interactively; they
    are iGAN and GauGAN . The **iGAN** (**interactive GAN**) was the first network
    to demonstrate how to use GANs for interactive image editing and transformation,
    back in 2016\. As GANs were still in fancy at that time, the generated image quality
    was not impressive as that of today's networks, but the door was opened to the
    incorporation of GANs into mainstream image editing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论两种可以用来生成和编辑图像的**生成对抗网络**（**GANs**）；它们分别是iGAN和GauGAN。**iGAN**（**互动GAN**）是第一个展示如何利用GANs进行互动图像编辑和转换的网络，最早出现在2016年。那时，GANs尚处于兴起阶段，生成的图像质量不如今天的网络那样令人印象深刻，但它为GANs进入主流图像编辑开辟了道路。
- en: In this chapter, you will be introduced to the concepts behind iGANs and some
    websites that feature video demonstrations of them. There won't be any code in
    that section. Then, we will go over a more recent award-winning application called
    **GauGAN**, produced by Nvidia in 2019, that gives impressive results in converting
    semantic segmentation masks into real landscape photos.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将了解iGAN的概念，并浏览一些展示视频的相关网站。该部分不会包含代码。接着，我们将介绍一个由Nvidia于2019年推出的更为先进的获奖应用——**GauGAN**，该应用能够将语义分割掩码转换为真实的风景照片，产生令人印象深刻的效果。
- en: We will implement GauGAN from scratch, starting with a new normalization technique
    known as **spatially adaptive normalization**. We will also learn about a new
    loss known as **hinge loss** and will go on to build a full-size GauGAN. The quality
    of the image generated by GauGAN is far superior to that of the general-purpose
    image-to-image translation networks that we covered in previous chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从零开始实现GauGAN，首先介绍一种新的归一化技术——**空间自适应归一化**。我们还将了解一种新的损失函数——**铰链损失**，并最终构建一个完整的GauGAN。GauGAN生成的图像质量远远优于我们在前几章中讲解的通用图像到图像转换网络。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to iGAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iGAN简介
- en: Segmentation map-to-image translation with GauGAN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GauGAN进行分割图到图像的转换
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The relevant Jupyter notebooks and code can be found here:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的Jupyter笔记本和代码可以在这里找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06)'
- en: The notebook used in this chapter is `ch6_gaugan.ipynb`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的笔记本是`ch6_gaugan.ipynb`。
- en: Introduction to iGAN
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iGAN简介
- en: We are now familiar with using generative models such as pix2pix (see [*Chapter
    4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*, Image-to-Image Translation*)to
    generate images from sketch or segmentation masks. However, as most of us are
    not skilled artists, we are only able to draw simple sketches, and as a result,
    our generated images also have simple shapes. What if we could use a real image
    as input and use sketches to change the appearance of the real image?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经熟悉了使用生成模型，如pix2pix（见[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)《图像到图像的转换》）从草图或分割掩码生成图像。然而，由于大多数我们并非专业画家，画出的草图通常很简单，因此生成的图像形状也较为简单。如果我们能使用一张真实图像作为输入，并通过草图来改变其外观呢？
- en: 'In the early days of GANs, a paper titled *Generative Visual Manipulation on
    the Natural Image Manifold* by J-Y. Zhu (inventor of CycleGAN) et al. was published
    that explored how to use a learned latent representation to perform image editing
    and morphing. The authors made a website, [http://efrosgans.eecs.berkeley.edu/iGAN/](http://efrosgans.eecs.berkeley.edu/iGAN/),
    that contains videos that demonstrate a few of the following use cases:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在GANs的早期，有一篇名为*Generative Visual Manipulation on the Natural Image Manifold*的论文，作者J-Y.
    Zhu（CycleGAN的发明者）等人，探讨了如何利用学习到的潜在表示进行图像编辑和变形。作者们建立了一个网站，[http://efrosgans.eecs.berkeley.edu/iGAN/](http://efrosgans.eecs.berkeley.edu/iGAN/)，其中包含展示以下几个应用场景的视频：
- en: '**Interactive image generation**: This involves generating images from sketches
    in real time, as shown here:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互动图像生成**：这涉及到实时从草图生成图像，示例如下：'
- en: '![Figure 6.1 – Interactive image generation, where an image is generated only
    from simple brush strokes (Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation
    on the Natural Image Manifold", https://arxiv.org/abs/1609.03552)](img/B14538_06_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 交互式图像生成，通过简单的笔触生成图像（来源：J-Y. Zhu 等，2016，“在自然图像流形上的生成视觉操作”，https://arxiv.org/abs/1609.03552)](img/B14538_06_01.jpg)'
- en: 'Figure 6.1 – Interactive image generation, where an image is generated only
    from simple brush strokes (Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation
    on the Natural Image Manifold", [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.1 – 交互式图像生成，通过简单的笔触生成图像（来源：J-Y. Zhu 等，2016，“在自然图像流形上的生成视觉操作”，[https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552)）  '
- en: '**Interactive image editing**: A picture is imported and we perform image editing
    using a GAN. Early GANs generated images uses only noise as input. Even BicycleGAN
    (which was invented a few years after the iGAN) could only change the appearance
    of generated images randomly without direct manipulation. iGANs allow us to specify
    changes in color and texture, which is impressive.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式图像编辑**：导入一张图片，我们使用 GAN 进行图像编辑。早期的 GAN 生成的图像仅使用噪声作为输入。即使是 BicycleGAN（在
    iGAN 发明几年后出现的技术），也只能随机地改变生成图像的外观，而无法进行直接操作。iGAN 让我们能够指定颜色和纹理的变化，令人印象深刻。'
- en: '**Interactive image transformation** (**morphing**): Given two images, an iGAN
    can create sequences of images that show a morphing process from one image to
    the other, as follows:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式图像变换**（**形变**）：给定两张图像，iGAN 可以创建一系列图像，展示从一张图像到另一张图像的形变过程，如下所示：'
- en: '![Figure 6.2 – Interactive image transformation (morphing). Given two images,
    sequences of intermediates images can be generated'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 交互式图像变换（形变）。给定两张图像，可以生成一系列中间图像'
- en: '(Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation on the Natural
    Image Manifold", https://arxiv.org/abs/1609.03552)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：J-Y. Zhu 等，2016，“在自然图像流形上的生成视觉操作”，https://arxiv.org/abs/1609.03552）
- en: '](img/B14538_06_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_02.jpg)'
- en: 'Figure 6.2 – Interactive image transformation (morphing). Given two images,
    sequences of intermediates images can be generated (Source: J-Y. Zhu et al., 2016,
    "Generative Visual Manipulation on the Natural Image Manifold", [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 交互式图像变换（形变）。给定两张图像，可以生成一系列中间图像（来源：J-Y. Zhu 等，2016，“在自然图像流形上的生成视觉操作”，[https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552)）
- en: The term **manifold** appears in the paper a lot. It also appears in other machine
    learning literature, so let's spend some time understanding it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**流形**在论文中出现频繁。它也出现在其他机器学习文献中，因此我们花点时间来理解它。
- en: Understanding manifold
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解流形
- en: We can understand manifold from the perspective of a natural image. A color
    pixel can be represented by 8-bit or 256-bit number; a single RGB pixel alone
    can have 256x256x256 = 1.6 million different possible combinations! Using the
    same logic, the total possibilities for all pixels in an image is astronomically
    high!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从自然图像的角度来理解流形。一个颜色像素可以通过 8 位或 256 位数字表示；单个 RGB 像素本身就可以有 256x256x256 = 160
    万种不同的组合！用同样的逻辑，图像中所有像素的总可能性是天文数字！
- en: However, we know that the pixels are not independent of each other; for example,
    the pixels of grassland are confined to the green color range. Thus, the high
    dimensionality of an image is not as daunting as it seems. In other words, the
    dimension space is a lot smaller than we might think at first. Thus, we can say
    that a high-dimensional image space is supported by a low-dimensional manifold.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们知道像素之间并非独立；例如，草地的像素限制在绿色范围内。因此，图像的高维度并不像看起来那么令人生畏。换句话说，维度空间比我们最初想象的要小得多。因此，我们可以说，高维图像空间是由低维流形支持的。
- en: 'Manifold is a term in physics and mathematics that''s used to describe smooth
    geometric surfaces. They can exist in any dimension. One-dimensional manifolds
    include lines and circles; two-dimensional manifolds are called **surfaces**.
    A *sphere* is a three-dimensional manifold that is smooth everywhere. In contrast,
    *cubes* are not manifolds as they are not smooth at the vertices. In fact, we
    saw in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Variational
    Autoencoder*, that a latent space of an autoencoder with a latent dimension of
    2 was a 2D manifold of the MNIST digits projected. The following diagram shows
    a 2D latent space of digits:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 流形是物理学和数学中用来描述光滑几何表面的术语。流形可以存在于任何维度中。一维流形包括直线和圆；二维流形被称为**曲面**。*球体*是一个三维流形，它在任何地方都是光滑的。相对而言，*立方体*不是流形，因为它在顶点处并不光滑。事实上，我们在[*第二章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*，变分自编码器*中看到，一个具有二维潜在维度的自编码器的潜在空间是MNIST数字的二维流形投影。以下图示显示了数字的二维潜在空间：
- en: '![Figure 6.3 – Illustration of a 2D manifold of digits.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – 数字的二维流形示意图。'
- en: '(Source: https://scikit-learn.org/stable/modules/manifold.html)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: https://scikit-learn.org/stable/modules/manifold.html)'
- en: '](img/B14538_06_03.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_03.jpg)'
- en: 'Figure 6.3 – Illustration of a 2D manifold of digits. (Source: https://scikit-learn.org/stable/modules/manifold.html)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '图6.3 – 数字的二维流形示意图。(来源: https://scikit-learn.org/stable/modules/manifold.html)'
- en: 'A good resource to visualize manifolds in GAN is the interactive tool at [https://poloclub.github.io/ganlab/](https://poloclub.github.io/ganlab/).
    In the following example, a GAN is trained to map uniformly distributed 2D samples
    into 2D samples that have a circular distribution:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的资源来可视化GAN中的流形是[https://poloclub.github.io/ganlab/](https://poloclub.github.io/ganlab/)上的交互式工具。在以下示例中，一个GAN被训练用于将均匀分布的二维样本映射到具有圆形分布的二维样本：
- en: '![Figure 6.4 – The generator''s data transformation is visualized as a manifold,
    which turns input noise (on the left) into fake samples (on the right).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – 生成器的数据转换被可视化为流形，将输入噪声（左侧）转化为假样本（右侧）。'
- en: '(Source: M. Kahng, 2019, "GAN Lab: Understanding Complex Deep Generative Models
    using Interactive Visual Experimentation," IEEE Transactions on Visualization
    and Computer Graphics, 25(1) (VAST 2018) https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: M. Kahng, 2019, "GAN Lab: 使用交互式可视化实验理解复杂的深度生成模型," IEEE Transactions on
    Visualization and Computer Graphics, 25(1) (VAST 2018) https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf)'
- en: '](img/B14538_06_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_04.jpg)'
- en: 'Figure 6.4 – The generator''s data transformation is visualized as a manifold,
    which turns input noise (on the left) into fake samples (on the right).(Source:
    M. Kahng, 2019, "GAN Lab: Understanding Complex Deep Generative Models using Interactive
    Visual Experimentation," IEEE Transactions on Visualization and Computer Graphics,
    25(1) (VAST 2018) [https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf](https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图6.4 – 生成器的数据转换被可视化为流形，将输入噪声（左侧）转化为假样本（右侧）。(来源: M. Kahng, 2019, "GAN Lab: 使用交互式可视化实验理解复杂的深度生成模型,"
    IEEE Transactions on Visualization and Computer Graphics, 25(1) (VAST 2018) [https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf](https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf))'
- en: We can visualize this mapping using a manifold, where the input is represented
    as a uniform square grid. The generator wraps the high-dimensional input grid
    into a warped version with fewer dimensions. The output shown at the top right
    of the figure is the manifold approximated by the generator. The generator output,
    or the fake image (bottom right in the figure), is the samples sampled from the
    manifold, where an area with smaller grid blocks means a higher sampling probability.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用流形来可视化这种映射，其中输入被表示为一个均匀的方形网格。生成器将高维输入网格转换为一个低维的扭曲版本。图中右上方显示的输出是生成器近似的流形。生成器输出，或者说假图像（图中右下角），是从流形上采样的样本，其中网格块较小的区域意味着更高的采样概率。
- en: The assumption of the paper is that a GAN's output sampled from random noise
    **z**, **G(z)**, lies in on a smooth manifold. Therefore, given two images on
    the manifold, **G(z**0**)** and **G(z**N**)**, we could get a sequence of *N*
    + 1 images *[G(z*0*) , G(z*0*), ..., G(z*N*)]* with a smooth transition by interpolation
    in the latent space. This approximation of the manifold of natural images is used
    for performing *realistic image editing*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的假设是，来自随机噪声**z**的GAN输出**G(z)**，位于一个平滑流形上。因此，给定流形上的两张图像，**G(z**0**)**和**G(z**N**)**，我们可以通过在潜在空间中插值得到一个包含平滑过渡的*N*
    + 1张图像序列*[G(z*0*) , G(z*0*), ..., G(z*N*)]*。这种自然图像流形的近似用于执行*真实感图像编辑*。
- en: Image editing
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像编辑
- en: Now that we know what a manifold is, let's see how to use that knowledge to
    perform image editing. The first step in image editing is to project an image
    onto the manifold.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道什么是流形，接下来我们看看如何利用这些知识进行图像编辑。图像编辑的第一步是将图像投影到流形上。
- en: Projecting an image onto a manifold
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将图像投影到流形上
- en: What projecting an image onto a manifold means is using a pre-trained GAN to
    generate an image that is close to the given image. In this book, we will use
    a pre-trained DCGAN where the input to the generator is a 100-dimension latent
    vector. Therefore, we will need to find a latent vector that generates an image
    manifold that is as close as possible to the original image. One way to do this
    is to use optimization such as **style transfer**, a topic we covered in detail
    in [*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*, Style Transfer*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像投影到流形上意味着使用预训练的GAN生成一张接近给定图像的图像。在本书中，我们将使用预训练的DCGAN，其中生成器的输入是一个100维的潜在向量。因此，我们需要找到一个潜在向量，以生成尽可能接近原始图像的图像流形。实现这一目标的一种方法是使用优化，例如**风格迁移**，这一主题我们在[*第5章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*，风格迁移*中详细讲解过。
- en: We first extract the features of the original image using a pretrained **convolutional
    neural network** (**CNN**), such as the output of the *block5_conv1* layer in
    VGG (see [*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*, Style
    Transfer*), and use it as a target.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用预训练的**卷积神经网络**（**CNN**），例如VGG中的*block5_conv1*层的输出（见[*第5章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*，风格迁移*），提取原始图像的特征，并将其作为目标。
- en: Then, we use the pre-trained DCGAN's generator with frozen weights and optimize
    on the input latent vector by minimizing the L2 loss between the features.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用预训练的DCGAN生成器，并冻结权重，通过最小化特征之间的L2损失来优化输入的潜在向量。
- en: As we have learned regarding style transfer, optimization can be slow to run
    and hence not responsive when it comes to interactive drawing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在风格迁移中所学到的，优化可能运行缓慢，因此在交互式绘图时响应较慢。
- en: Another method is to train a feedforward network to predict the latent vector
    from the image, which is a lot faster. If the GAN is to translate a segmentation
    mask into an image, then we can use a network such as U-Net to predict the segmentation
    mask from the image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是训练一个前馈网络来预测图像的潜在向量，这种方法速度更快。如果GAN是将分割掩码转换为图像，那么我们可以使用诸如U-Net之类的网络来从图像中预测分割掩码。
- en: 'Manifold projection using a feedforward network looks similar to using an autoencoder.
    The encoder encodes (predicts) the latent variable from the original image, then
    the decoder (the generator, in the case of a GAN) projects the latent variable
    onto the image manifold. However, this method is not always perfect. This is where
    the *hybrid* method comes in. We use the feedforward network to predict the latent
    variables, which we then fine-tune using optimization. The following figure shows
    the images generated using different techniques:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前馈网络进行流形投影看起来类似于使用自编码器。编码器从原始图像中编码（预测）潜在变量，然后解码器（在GAN中是生成器）将潜在变量投影到图像流形上。然而，这种方法并不总是完美的。这时，*混合*方法派上了用场。我们使用前馈网络来预测潜在变量，然后通过优化进行微调。下图展示了使用不同技术生成的图像：
- en: '![Figure 6.5 – Projecting real photos onto an image manifold using a GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    https://arxiv.org/abs/1609.03552)](img/B14538_06_05.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 使用GAN将真实照片投影到图像流形上。（来源：J-Y. Zhu等人，2016年，“在自然图像流形上的生成视觉操作”，https://arxiv.org/abs/1609.03552)](img/B14538_06_05.jpg)'
- en: 'Figure 6.5 – Projecting real photos onto an image manifold using a GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 使用GAN将真实照片投影到图像流形上。（来源：J-Y. Zhu等人，2016年，《自然图像流形上的生成视觉操作》，[https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552)）
- en: As we have now obtained the latent vector, we will use it to edit the manifold.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经获得了潜在向量，我们将使用它来编辑流形。
- en: Editing the manifold using latent vector
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用潜在向量编辑流形
- en: Now that we have obtained the latent variable *z*0 and the image manifold *x*0
    *= G(z*0*)*, the next step is to manipulate *z*0 to modify the image. Now, let's
    say the image is a red shoe and we want to change the color to black – how can
    we do that? The simplest and crudest method is to open an image editing software
    package to select all the red pixels in the picture and change them to black.
    The resulting picture is likely to not look very natural as some details may be
    lost. Traditional image editing tools' algorithms tend to not work that well on
    natural images with complex shapes and fine texture details.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了潜在变量 *z*0 和图像流形 *x*0 *= G(z*0*)*，下一步是操作 *z*0 来修改图像。假设图像是一只红色鞋子，我们想将其颜色改为黑色——我们该如何做呢？最简单也是最粗糙的方法是打开图像编辑软件，选择图中所有的红色像素并将它们改为黑色。生成的图像可能看起来不太自然，因为可能会丢失一些细节。传统的图像编辑工具算法通常不太适用于具有复杂形状和细致纹理细节的自然图像。
- en: On the other hand, we know we probably could change the latent vector and feed
    that to the generator and change the color. In practice, we do not know how to
    modify the latent variables to get the results we want.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们知道我们可能可以改变潜在向量，并将其输入生成器来改变颜色。在实际操作中，我们并不知道如何修改潜在变量以获得我们想要的结果。
- en: Therefore, instead of changing the latent vector directly, we could attack the
    problem from a different direction. We can edit the manifold, for example, by
    drawing a black stripe on the shoes, then use that to optimize the latent variables,
    and then project that to generate another image on the manifold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以从另一个方向来解决这个问题，而不是直接改变潜在向量。我们可以编辑流形，例如，通过在鞋子上画一条黑色条纹，然后用它来优化潜在变量，再将其投影以在流形上生成另一张图像。
- en: 'Essentially, we are performing optimization as previously described for manifold
    projection but with different loss functions. We want to find an image manifold
    *x* that minimizes the following equation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们执行的是如前所述的流形投影优化，但使用的是不同的损失函数。我们希望找到一个图像流形 *x*，使得以下方程最小化：
- en: '![](img/Formula_06_001.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_001.jpg)'
- en: 'Let''s start with the second loss term *S(x, x*0*)* for manifold smoothness.
    It is L2 loss, used to encourage the new manifold to not deviate too much from
    the original manifold. This loss term keeps the global appearance of the image
    in check. The first loss term is the data term, which sums up all the editing
    operation loss. This is best described using the following images:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第二个损失项 *S(x, x*0*)* 开始，这是用于流形平滑性的L2损失，旨在鼓励新的流形不要偏离原始流形太多。这个损失项保持图像的整体外观。第一个损失项是数据项，它将所有编辑操作的损失进行求和。这个最好通过以下图像来描述：
- en: '![Figure 6.6 – Projecting real photos onto the image manifold using GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    https://arxiv.org/abs/1609.03552)](img/B14538_06_06.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 使用GAN将真实照片投影到图像流形上。（来源：J-Y. Zhu等人，2016年，《自然图像流形上的生成视觉操作》，https://arxiv.org/abs/1609.03552)](img/B14538_06_06.jpg)'
- en: 'Figure 6.6 – Projecting real photos onto the image manifold using GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 使用GAN将真实照片投影到图像流形上。（来源：J-Y. Zhu等人，2016年，《自然图像流形上的生成视觉操作》，[https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552)）
- en: This example uses a color brush to change the color of the shoe. The color change
    may not be obvious in the grayscale print of this book and you are encouraged
    to check out the color version of the paper, which you can download from [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552).
    The top row of the preceding figure shows the brush stroke as the constraint *v*g
    and *f*g as the editing operation. We want every manifold pixel in the brush stroke
    *f*g*(x)* to be as close to *v*g as possible.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例使用色彩画笔来改变鞋子的颜色。由于本书的灰度打印中颜色变化不明显，建议查看论文的彩色版本，您可以从 [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552)
    下载。前面图形的顶行显示了作为约束的画笔笔触 *v*g 和 *f*g 作为编辑操作。我们希望画笔笔触 *f*g*(x)* 中的每个流形像素尽可能接近 *v*g。
- en: 'In other words, if we put a black stroke on the shoe, we want that part in
    the image manifold to be black. That is the intention, but to execute it, we will
    need to do the optimization of the latent variables. Thus, we reformulate the
    preceding equation from pixel space into latent space. The equation is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们在鞋子上涂上黑色笔触，我们希望图像流形中的那部分是黑色的。这就是我们的意图，但要实现它，我们需要对潜在变量进行优化。因此，我们将前面的方程从像素空间重新表述为潜在空间。方程如下：
- en: '![](img/Formula_06_002.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_002.jpg)'
- en: The last term ![](img/Formula_06_003.png) is the *GAN's adversarial loss*. This
    is used for making the manifold look real and improve the visual quality slightly.
    By default, this term is not used for increasing the frame rate. With all the
    loss terms defined, we can use a TensorFlow optimizer such as Adam to run the
    optimization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项术语 ![](img/Formula_06_003.png) 是 *GAN的对抗损失*。这个损失用于让流形看起来更真实，并略微提高视觉质量。默认情况下，这个项不会用于提高帧率。定义了所有损失项后，我们可以使用TensorFlow优化器，如Adam，来运行优化。
- en: Edit transfer
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑传递
- en: '**Edit transfer** is the last step of image editing. Now we have two manifolds,
    *G(z*0*)* and *G(z*1*)*, and we can generate sequences of intermediate images
    using linear interpolation in latent space between *z*0 and *z*1\. Due to the
    capacity limitations of the DCGAN, the generated manifolds can appear blurry and
    may not look as realistic as we hoped them to be.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**编辑传递**是图像编辑的最后一步。现在我们有两个流形，*G(z*0*)* 和 *G(z*1*)*，我们可以通过在潜在空间中对 *z*0 和 *z*1
    进行线性插值，生成一系列的中间图像。由于DCGAN的容量限制，生成的流形可能会显得模糊，且可能没有我们预期的那样逼真。'
- en: The way the authors of the previously mentioned paper address this problem is
    to not use the manifold as the final image but instead to estimate the color and
    geometric changes between the manifolds and apply the changes to the original
    image. The estimation of color and motion flow is performed using optical flow;
    this is a traditional computer vision technique that is beyond the scope of this
    book.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述论文的作者解决这个问题的方式是：不使用流形作为最终图像，而是估计流形之间的颜色和几何变化，并将这些变化应用到原始图像上。颜色和运动流的估计是通过光流进行的；这是一种传统的计算机视觉技术，超出了本书的讨论范围。
- en: Using the preceding shoe example, if all we are interested in is the color change,
    we estimate the color change of the pixel between manifolds, then transfer the
    color changes on the pixels in the original image. Similarly, if the transformation
    involves warping, that is, a change in shape, we measure the motion of pixels
    and apply them to original image to perform morphing. The demonstration video
    on the website was created using both motion and color flow.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以鞋子示例为例，如果我们关注的是颜色变化，我们会估计流形间像素的颜色变化，然后将颜色变化转移到原始图像中的像素上。类似地，如果变换涉及扭曲，即形状变化，我们就需要衡量像素的运动并将它们应用到原始图像上进行形态变化。网站上的示范视频是通过运动和颜色流共同创建的。
- en: To recap, we have now learned that an iGAN is not a GAN but a method of using
    a GAN to perform image editing. This is done by first projecting a real image
    onto a manifold using either optimization or a feedforward network. Next, we use
    brush strokes as constraints to modify the manifold generated by the latent vector.
    Finally, we transfer the color and motion flow of the interpolated manifolds onto
    the real image to complete the image editing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们现在已经了解到，iGAN并不是一个GAN，而是一种使用GAN进行图像编辑的方法。首先通过优化或前馈网络将真实图像投影到流形上。接着，我们使用画笔笔触作为约束，修改由潜在向量生成的流形。最后，我们将插值流形的颜色和运动流应用到真实图像上，从而完成图像编辑。
- en: As there aren't any new GAN architectures, we will not be implementing an iGAN.
    Instead, we are going to implement GauGAN, which includes some new innovations
    that are exciting for code implementation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有新的GAN架构，我们将不会实现iGAN。相反，我们将实现GauGAN，其中包含一些令人兴奋的新创新，适合于代码实现。
- en: Segmentation map-to-image translation with GauGAN
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GauGAN进行分割图到图像的转换
- en: '**GauGAN** (named after 19th-century painter Paul Gauguin) is a GAN from **Nvidia**.
    Speaking of Nvidia, it is one of the handful of companies that has invested heavily
    in GANs. They have achieved several breakthroughs in this space, including **ProgressiveGAN**
    (we''ll cover that in [*Chapter 7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)*,
    High Fidelity Face Generation*), to generate high-resolution images, and **StyleGAN**
    for high-fidelity faces.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**GauGAN**（以19世纪画家保罗·高更命名）是来自**Nvidia**的一种生成对抗网络（GAN）。说到Nvidia，它是为数不多的几家在GAN领域进行大规模投资的公司之一。他们在这一领域取得了若干突破，包括**ProgressiveGAN**（我们将在[*第7章*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)中介绍，*高保真面部生成*），用于生成高分辨率图像，以及**StyleGAN**，用于高保真面孔生成。'
- en: Their main business is in making graphics chips rather than AI software. Therefore,
    unlike some other companies, who keep their code and trained models as closely
    guarded secrets, Nvidia tends to open source their software code to the general
    public. They have built a web page ([http://nvidia-research-mingyuliu.com/gaugan/](http://nvidia-research-mingyuliu.com/gaugan/))
    to showcase GauGAN, which can generate photorealistic landscape photos from segmentation
    maps. The following screenshot is taken from their web page.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的主要业务是制造图形芯片，而非AI软件。因此，与一些其他公司不同，后者将其代码和训练模型视为严格保密，Nvidia倾向于将其软件代码开源给公众。他们建立了一个网页（[http://nvidia-research-mingyuliu.com/gaugan/](http://nvidia-research-mingyuliu.com/gaugan/)），展示GauGAN，这个工具可以根据分割图生成逼真的景观照片。以下截图取自他们的网页。
- en: 'Feel free to pause this chapter for a bit and have a play with the application
    to see how good it is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随时暂停本章并尝试一下该应用程序，看看它有多好：
- en: '![Figure 6.7 –From brush stroke to photo with GauGAN'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – 通过GauGAN从画笔笔触到照片'
- en: '](img/B14538_06_07.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_07.jpg)'
- en: Figure 6.7 –From brush stroke to photo with GauGAN
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 通过GauGAN从画笔笔触到照片
- en: We will now learn about pix2pixHD.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习pix2pixHD。
- en: Introduction to pix2pixHD
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pix2pixHD简介
- en: 'GauGAN uses **pix2pixHD** as a base and adds new features to it. pix2pixHD
    is an upgraded version of pix2pix that can generate **high-definition** (**HD**)
    images. As we haven''t covered pix2pixHD in this book and we won''t be using an
    HD dataset, we will build our GauGAN base on pix2pix''s architecture and the code
    base that we are already familiar with. Nevertheless, it is good to know the high-level
    architecture of pix2pixHD, and I''ll walk you through some of the high-level concepts.
    The following diagram shows the architecture of pix2pixHD''s generators:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GauGAN以**pix2pixHD**为基础，并在其上增加了新特性。pix2pixHD是pix2pix的升级版本，能够生成**高清**（**HD**）图像。由于本书中未涉及pix2pixHD，且我们不会使用高清数据集，因此我们将在pix2pix的架构和我们已熟悉的代码基础上构建我们的GauGAN基础。尽管如此，了解pix2pixHD的高级架构还是很有帮助的，我将带你了解一些高层次的概念。下图展示了pix2pixHD生成器的架构：
- en: '![Figure 6.8 – The network architecture of the pix2pixHD generator.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 – pix2pixHD生成器的网络架构'
- en: '(Source: T-C. W et al., 2018, "High-Resolution Image Synthesis and Semantic
    Manipulation with Conditional GANs," https://arxiv.org/abs/1711.11585)](img/B14538_06_08.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：T-C. W等人，2018年，《使用条件生成对抗网络进行高分辨率图像合成和语义操作》，https://arxiv.org/abs/1711.11585)](img/B14538_06_08.jpg)
- en: 'Figure 6.8 – The network architecture of the pix2pixHD generator. (Source:
    T-C. W et al., 2018, "High-Resolution Image Synthesis and Semantic Manipulation
    with Conditional GANs," [https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585))'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – pix2pixHD生成器的网络架构。（来源：T-C. W等人，2018年，《使用条件生成对抗网络进行高分辨率图像合成和语义操作》，[https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585)）
- en: In order to generate high-resolution images, pix2pixHD uses two generators at
    different image resolutions at coarse- and fine-scale. The coarse generator **G1**
    works at half the image resolution; that is, the input and target images are downsampled
    into half the resolution. When that is trained, we start training the coarse generator
    **G**1 together with the fine generator **G2**, which works on the full-image
    scale. From the preceding architecture diagram, we can see that **G**1's encoder
    output concatenates with **G**1's features and feeds into the decoder part of
    **G**2 to generate high-resolution images. This setting is also known as a **coarse-to-fine
    generator**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成高分辨率图像，pix2pixHD在不同图像分辨率下使用两个生成器，分别在粗尺度和细尺度上工作。粗生成器**G1**工作在图像分辨率的一半；也就是说，输入和目标图像被下采样到原图像分辨率的一半。当粗生成器训练完成后，我们开始训练粗生成器**G1**与细生成器**G2**，后者在全图像尺度上工作。从前面的架构图中，我们可以看到，**G1**的编码器输出与**G1**的特征连接，并输入到**G2**的解码器部分，生成高分辨率图像。这种设置也被称为**粗到细生成器**。
- en: pix2pixHD uses three PatchGAN discriminators that operate at different image
    scales. A new loss, known as feature matching loss, is used to match the layer
    features between the real and fake images. This is used in style transfer, where
    we use a pre-trained VGG for feature extraction and optimize on the style features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pixHD使用三个PatchGAN判别器，这些判别器在不同的图像尺度上工作。一个新的损失函数，称为特征匹配损失，用于匹配真实图像和假图像之间的层特征。这在风格迁移中得到应用，我们使用预训练的VGG进行特征提取，并优化风格特征。
- en: Now that we've had a quick introduction to pix2pixHD, we can move on to GauGAN.
    But first, we will implement a normalization technique that demonstrates GauGAN.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经简单介绍了pix2pixHD，我们可以继续讲解GauGAN。但在此之前，我们将实现一种归一化技术来展示GauGAN。
- en: Spatial Adaptive Normalization (SPADE)
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空间自适应归一化（SPADE）
- en: The main innovation in GauGAN is a layer normalization method for segmentation
    map known as **Spatial-Adaptive Normalization** (**SPADE**). That's right, another
    entry into the already-long list of normalization techniques in the GAN's toolbox.
    We will dive deep into SPADE, but before that, we should learn about the format
    of the network input – the **semantic segmentation map**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: GauGAN的主要创新是为分割图采用一种层归一化方法，称为**空间自适应归一化**（**SPADE**）。没错，又一个进入GAN工具箱的归一化技术。我们将深入探讨SPADE，但在此之前，我们应了解网络输入的格式——**语义分割图**。
- en: One-hot encoded segmentation masks
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独热编码分割掩码
- en: We will use the `facades` dataset to train our GauGAN. In previous experiments,
    the segmentation map was encoded as different colors in an RGB image; for example,
    a wall was represented by a purple mask and a door was green. This representation
    is visually easy for us to understand but it is not that helpful for the neural
    network to learn. This is because the colors do not have semantic meaning.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`facades`数据集来训练我们的GauGAN。在之前的实验中，分割图被编码为RGB图像中的不同颜色；例如，墙面用紫色掩码表示，门则是绿色。这种表示方式对我们来说视觉上很容易理解，但对神经网络学习并没有太大帮助。因为这些颜色没有语义意义。
- en: Colors being closer in color space does not mean they are close in semantic
    meaning. We could use light green to represent grass and dark green to represent
    an airplane, and their semantic meanings would not be related even though the
    segmentation maps would be close in color shade.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色在色彩空间中接近，并不意味着它们在语义上也接近。我们可以用浅绿色表示草地，用深绿色表示飞机，尽管它们在分割图中颜色接近，但它们的语义并没有关联。
- en: 'For this reason, instead of labeling pixels using colors, we should use class
    labels. However, this still does not solve the problem as the class labels are
    numbers assigned randomly and they also do not have semantic meaning. Therefore,
    a better way is to use a **segmentation mask** with a label of 1 when an object
    is present in that pixel and a label of 0 otherwise. In other words, we one-hot
    encode the labels in a segmentation map into a segmentation mask with the shape
    (H, W, number of classes). The following figure shows an example of semantic segmentation
    masks for a building image:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该使用类别标签而非颜色来标注像素。但这仍然没有解决问题，因为类别标签是随机分配的数字，它们也没有语义意义。因此，更好的方法是使用**分割掩码**，当某个像素处有物体时，标签为1，否则为0。换句话说，我们将分割图中的标签进行独热编码，得到形状为(H,
    W, 类别数)的分割掩码。下图展示了建筑图像的语义分割掩码示例：
- en: '![Figure 6.9 – On the left is a segmentation map encoded in RGB. On the right,
    the segmentation maps are separated into individual classes of window, façade,
    and pillar ](img/B14538_06_09.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 左侧是使用RGB编码的分割图。右侧是分割图，被分为单独的窗口、立面和柱子类别](img/B14538_06_09.jpg)'
- en: Figure 6.9 – On the left is a segmentation map encoded in RGB. On the right,
    the segmentation maps are separated into individual classes of window, façade,
    and pillar
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 左侧是使用RGB编码的分割图。右侧是分割图，被分为单独的窗口、立面和柱子类别
- en: 'The data in the `facades` dataset that we used in the previous chapters was
    encoded as JPEG, so we cannot use that to train GauGAN. In JPEG encoding, some
    visual information that is less important to the visuals is removed in the compression
    process. The resulting pixels may have different values even if they should belong
    to the same class and appear to be the same color. As a result, we cannot map
    the colors in a JPEG image to classes. To tackle this problem, I got the raw dataset
    from the original source and created a new dataset that includes three different
    image file types for each sample as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中使用的`facades`数据集是通过JPEG编码的，因此我们无法使用它来训练GauGAN。在JPEG编码过程中，某些对视觉效果不太重要的视觉信息会在压缩过程中被移除。即使某些像素应该属于同一类别并且看起来是相同颜色，压缩后的像素值可能也会不同。因此，我们不能将JPEG图像中的颜色映射到类别。为了解决这个问题，我从原始数据源获得了原始数据集，并创建了一个新的数据集，每个样本包含三种不同的图像文件类型，如下所示：
- en: JPEG – real photo
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JPEG – 真实照片
- en: PNG – segmentation map using RGB color
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PNG – 使用RGB颜色的分割图
- en: BMP – segmentation map using class labels
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BMP – 使用类别标签的分割图
- en: BMP is uncompressed. We can think of a BMP image as the image in RGB format
    in the preceding figure, except that the pixel values are 1-channel class labels
    rather than 3-channel RGB colors. In image loading and pre-processing, we will
    load all three files and convert them from BMP into one-hot encoded segmentation
    masks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: BMP是无压缩的。我们可以将BMP图像视为前面图示中的RGB格式图像，不同之处在于像素值是1通道的类别标签，而不是3通道的RGB颜色。在图像加载和预处理过程中，我们将加载这三个文件，并将它们从BMP格式转换为独热编码的分割掩码。
- en: Sometimes, TensorFlow's basic image pre-processing APIs are not able to do some
    of the more complex tasks, so we need to resort to using other Python libraries.
    Luckily, `tf.py_function` allows us to run a generic Python function within a
    TensorFlow training pipeline.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，TensorFlow的基本图像预处理API无法完成一些更复杂的任务，因此我们需要借助其他Python库。幸运的是，`tf.py_function`允许我们在TensorFlow训练管道中运行一个通用的Python函数。
- en: 'In this file-loading function, as shown in the following code, we use `.numpy()`
    to convert TensorFlow tensors into Python objects. The function name is a bit
    misleading as it applies not only to numerical values but also string values:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件加载函数中，如下代码所示，我们使用`.numpy()`将TensorFlow张量转换为Python对象。函数名有点误导，因为它不仅适用于数值值，还适用于字符串值：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we understand the format of one-hot encoded semantic segmentation masks,
    we will look at how SPADE can help us generate better images from segmentation
    masks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了独热编码的语义分割掩码的格式，我们将看看SPADE如何帮助我们从分割掩码生成更好的图像。
- en: Implementing SPADE
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现SPADE
- en: Instance normalization has become popular in image generation, but it tends
    to wash away the semantic meaning of segmentation masks. What does that mean?
    Let's assume an input image consists of only one single segmentation label; for
    example, say the entire image is of the sky. As the input has uniform values,
    the output, after passing through the convolution layer, will also have uniform
    values.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实例归一化在图像生成中已经变得流行，但它往往会抹去分割掩码的语义信息。这意味着什么呢？假设输入图像仅包含一个单一的分割标签；例如，假设整张图像都是天空。当输入图像具有统一的值时，经过卷积层处理后的输出也将是统一的值。
- en: Recall that instance normalization calculates the mean across dimensions (H,
    W) for each channel. Therefore, the mean for that channel will be the same uniform
    value, and the normalized activation after subtraction with mean will become zero.
    Obviously, the semantic meaning is lost and the sky has just vanished into thin
    air. This is an extreme example, but using the same logic, we can see that a segmentation
    mask loses its semantic meaning as its area grows larger.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，实例归一化是通过对每个通道的（H，W）维度计算均值来完成的。因此，该通道的均值将是相同的统一值，经过均值减法后的归一化激活将变为零。显然，语义信息丢失了，天空仿佛在一瞬间消失了。这是一个极端的例子，但使用相同的逻辑，我们可以看到随着分割掩码区域的增大，它的语义意义会丧失。
- en: 'To solve this problem, SPADE normalizes on local areas confined by the segmentation
    mask rather than on the entire mask. The following diagram shows the high-level
    architecture of SPADE:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，SPADE在由分割掩码限制的局部区域上进行归一化，而不是在整个掩码上进行。下图展示了SPADE的高层架构：
- en: '![Figure 6.10 – High-level SPADE architecture.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10 – 高层次SPADE架构。'
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_10.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: （重绘自：T. Park等，2019年，《具有空间自适应归一化的语义图像合成》，https://arxiv.org/abs/1903.07291)](img/B14538_06_10.jpg)
- en: 'Figure 6.10 – High-level SPADE architecture. (Redrawn from: T. Park et al.,
    2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 高层次SPADE架构。（重绘自：T. Park等，2019年，《具有空间自适应归一化的语义图像合成》，[https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))
- en: 'In batch normalization, we calculate the means and standard deviations of channels
    across dimensions (N, H, W). This is the same for SPADE, as shown in the preceding
    figure. The difference is that gamma and beta for each channel are no longer scalar
    values (or vectors of C channels) but two-dimensional (H, W). In other words,
    there is a gamma and a beta for every activation that is learned from the semantic
    segmentation map. So, normalization is applied differently to different segmentation
    areas. These two parameters are learned by using two convolutional layers, as
    shown in the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在批归一化中，我们计算跨维度（N，H，W）上的通道的均值和标准差。这对于SPADE也是一样，如前图所示。不同之处在于，每个通道的gamma和beta不再是标量值（或C通道的向量），而是二维的（H，W）。换句话说，每个激活都有一个gamma和一个beta，它们从语义分割图中学习。因此，归一化是以不同的方式应用于不同的分割区域。这两个参数通过使用两个卷积层进行学习，如下图所示：
- en: '![Figure 6.11 – SPADE design diagram, where k denotes the number of convolutional
    filters'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11 – SPADE设计图，其中k表示卷积滤波器的数量'
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_11.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: （重绘自：T. Park等，2019年，《具有空间自适应归一化的语义图像合成》，https://arxiv.org/abs/1903.07291)](img/B14538_06_11.jpg)
- en: 'Figure 6.11 – SPADE design diagram, where k denotes the number of convolutional
    filters (Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – SPADE设计图，其中k表示卷积滤波器的数量（重绘自：T. Park等，2019年，《具有空间自适应归一化的语义图像合成》，[https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))
- en: SPADE is used not only at the network input stage but also in the internal layers.
    The resize layer is to resize the segmentation map to match the dimensions of
    the layer's activation. We can now implement a TensorFlow custom layer for SPADE.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SPADE不仅在网络输入阶段使用，而且在内部层中也有应用。resize层用于调整分割图的大小，以匹配该层的激活尺寸。我们现在可以实现一个TensorFlow自定义层来实现SPADE。
- en: 'We will first define the convolutional layers in the `__init__` constructor
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在`__init__`构造函数中定义卷积层，如下所示：
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will get the activation map dimensions to be used in resizing later:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将获取激活图的尺寸，稍后用于调整大小：
- en: '[PRE2]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we will connect the layers and operations together in `call()` as
    follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在`call()`中将层和操作连接起来，如下所示：
- en: '[PRE3]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is a straightforward implementation based on the SPADE design diagram.
    Next, we will look at how to make use of SPADE.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于SPADE设计图的直接实现。接下来，我们将看看如何使用SPADE。
- en: Inserting SPADE into residual blocks
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将SPADE插入残差块
- en: 'GauGAN uses **residual blocks** in the generator. We will now look at how to
    insert SPADE into residual blocks:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GauGAN在生成器中使用**残差块**。接下来，我们将看看如何将SPADE插入残差块：
- en: '![Figure 6.12 – SPADE residual blocks'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12 – SPADE残差块'
- en: '(Redrawn from: T. Park et. al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_12.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: （重绘自：T. Park等，2019年，《具有空间自适应归一化的语义图像合成》，https://arxiv.org/abs/1903.07291)](img/B14538_06_12.jpg)
- en: 'Figure 6.12 – SPADE residual blocks (Redrawn from: T. Park et. al., 2019, "Semantic
    Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – SPADE残差块（重绘自：T. Park等，2019年，《具有空间自适应归一化的语义图像合成》，[https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))
- en: The basic building block within the SPADE residual block is the **SPADE-ReLU-Conv
    layer**. Each SPADE takes two inputs – the activation from the previous layer
    and the semantic segmentation map.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: SPADE 残差块中的基本构建块是 **SPADE-ReLU-Conv 层**。每个 SPADE 接收两个输入——来自前一层的激活值和语义分割图。
- en: 'As with the standard residual block, there are two convolution-ReLU layers
    and a skip path. Whenever there is a change in the number of channels before and
    after the residual block, the skip connection is learned via the sub-block in
    the dashed-line box shown in the preceding diagram. When this happens, the activation
    maps at the inputs of the two SPADEs in forward path will have different dimensions.
    That is alright as we have built-in resizing within SPADE block. The following
    is the code for the SPADE residual block to build the needed layers:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准残差块一样，它包含两个卷积-ReLU 层和一个跳跃连接路径。每当残差块前后的通道数量发生变化时，跳跃连接通过前面图中虚线框内的子块进行学习。当这种情况发生时，前向路径中两个
    SPADE 的输入激活图会具有不同的维度。没关系，因为我们在 SPADE 块内已构建了内置的调整大小功能。以下是构建所需层的 SPADE 残差块代码：
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we connect the layers up in `call()`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 `call()` 中连接各个层：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the original GauGAN implementation, spectral normalization is applied after
    the convolutional layer. It is yet another normalization that we will cover in
    [*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*, Self-Attention
    for Image Generation*, when we talk about self-attention GANs. Therefore, we will
    skip over that and put the residual blocks together to implement GauGAN.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 GauGAN 实现中，谱归一化应用于卷积层之后。这是另一种归一化方法，我们将在[*第 8 章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)
    *图像生成的自注意力机制*中讨论它，届时会讲解自注意力 GAN。因此，我们将跳过这一部分，直接将残差块组合在一起实现 GauGAN。
- en: Implementing GauGAN
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 GauGAN
- en: We will first build the generator, followed by the discriminator. Finally, we
    will implement the loss functions and start training GauGAN.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先构建生成器，然后是判别器。最后，我们将实现损失函数并开始训练 GauGAN。
- en: Building the GauGAN generator
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建 GauGAN 生成器
- en: Before diving into the GauGAN generator, let's revise what we know about some
    of its predecessors. In pix2pix, the generator takes in only one input – the semantic
    segmentation map. As there is no randomness in the network, given the same input,
    it will always generate building facades with the same color and texture. The
    naive way of concatenating the input with random noise doesn't work.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解 GauGAN 生成器之前，让我们复习一下它的一些前辈。在 pix2pix 中，生成器只有一个输入——语义分割图。由于网络中没有随机性，给定相同的输入，它将始终生成具有相同颜色和纹理的建筑外立面。简单地将输入与随机噪声连接起来的方法是不可行的。
- en: One of the two methods used by **BicycleGAN** ([*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*) to address this problem is using an encoder to encode
    the target image (the real photo) into latent vectors, which are then used to
    sample random noise for the generator input. This **cVAE-GAN** structure is used
    in the GauGAN generator. There are two inputs to the generator – the semantic
    segmentation mask and the real photo.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**BicycleGAN**（[*第 4 章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084) *图像到图像翻译*）为解决这个问题使用的两种方法之一是使用编码器将目标图像（真实照片）编码为潜向量，然后用它来采样随机噪声作为生成器输入。这个
    **cVAE-GAN** 结构在 GauGAN 生成器中得到了应用。生成器有两个输入——语义分割掩码和真实照片。'
- en: 'In the GauGAN web application, we can select a photo (the generated image will
    resemble the style of the photo). This is made possible by using the encoder to
    encode style information into latent variables. The code for the encoder is the
    same as that which we used in the previous chapters, so we will move on to look
    at the generator architecture. Feel free to revisit [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*, to refresh the encoder implementation. In the following
    diagram, we can see the GauGAN generator architecture:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GauGAN 的 Web 应用程序中，我们可以选择一张照片（生成的图像将类似于照片的风格）。这是通过使用编码器将风格信息编码为潜变量来实现的。编码器的代码与我们在前几章中使用的相同，因此我们将继续查看生成器架构。可以随时回顾[*第
    4 章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084) *图像到图像翻译*，以复习编码器的实现。在下图中，我们可以看到
    GauGAN 生成器的架构：
- en: '![Figure 6.13 – GauGAN generator architecture'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.13 – GauGAN 生成器架构'
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: （重绘自：T. Park 等人，2019，《利用空间自适应归一化进行语义图像合成》，https://arxiv.org/abs/1903.07291）
- en: '](img/B14538_06_13.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_13.jpg)'
- en: 'Figure 6.13 – GauGAN generator architecture (Redrawn from: T. Park et al.,
    2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – GauGAN生成器架构（重绘自：T. Park等，2019年，“带有空间自适应归一化的语义图像合成”，[https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)）
- en: The generator is a decoder-like architecture. The main difference is that the
    segmentation mask goes into every residual block via SPADE. The latent variable
    dimension chosen for GauGAN is 256\.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是一个类似解码器的架构。主要的不同之处在于，分割掩码通过SPADE进入每个残差块。为GauGAN选择的潜在变量维度为256\。
- en: Note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The encoder is not an integral part of the generator; we can choose not to use
    any style image and sample from a standard multivariate Gaussian distribution.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器不是生成器的一个核心部分；我们可以选择不使用任何样式图像，而是从标准的多元高斯分布中进行采样。
- en: 'The following is the code to build the generator using the residual block that
    we wrote previously:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们之前编写的使用残差块构建生成器的代码：
- en: '[PRE6]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You now know about everything that makes GauGAN work – SPADE and the generator.
    The rest of the network architecture are ideas borrowed from other GANs that we
    have previously learned. Next, we will look at how to build the discriminator.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了让GauGAN工作的所有要素——SPADE和生成器。网络架构的其余部分是从我们之前学习过的其他GAN中借鉴来的。接下来，我们将探讨如何构建判别器。
- en: Building the discriminator
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建判别器
- en: 'The discriminator is PatchGAN, where the input is a concatenation of the segmentation
    map and the generated image. The segmentation map has to have the same number
    of channels as the generated RGB image; therefore, we will use the RGB segmentation
    map instead of a one-hot encoded segmentation mask. The architecture of the GauGAN
    discriminator is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器是PatchGAN，其中输入是分割图和生成图像的连接。分割图必须与生成的RGB图像具有相同的通道数；因此，我们将使用RGB分割图，而不是使用one-hot编码的分割掩码。GauGAN判别器的架构如下：
- en: '![Figure 6.14 – GauGAN discriminator architecture'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14 – GauGAN判别器架构'
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: （重绘自：T. Park等，2019年，“带有空间自适应归一化的语义图像合成”，[https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)）](img/B14538_06_14.jpg)
- en: 'Figure 6.14 – GauGAN discriminator architecture (Redrawn from: T. Park et al.,
    2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – GauGAN判别器架构（重绘自：T. Park等，2019年，“带有空间自适应归一化的语义图像合成”，[https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)）
- en: 'Except for the last layer, the discriminator layers consist of the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了最后一层，判别器层由以下部分组成：
- en: A convolutional layer with a kernel size of 4x4 and a stride of 2 for downsampling
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用4x4的卷积层，步幅为2，用于下采样
- en: Instance normalization (except for the first layer)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例归一化（第一层除外）
- en: Leaky ReLU
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: GauGAN uses multiple discriminators at different scales. Since our dataset image
    has a low resolution of 256x256, one discriminator is sufficient. If we were to
    use multiple discriminators, all we would need to do is downsample the input size
    by half for the next discriminator and calculate the average loss from all the
    discriminators.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: GauGAN在不同的尺度上使用多个判别器。由于我们的数据集图像分辨率较低，为256x256，单个判别器就足够了。如果我们使用多个判别器，我们需要做的就是将输入大小下采样一半，用于下一个判别器，并计算所有判别器的平均损失。
- en: 'The code implementation for a single PatchGAN is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 单个PatchGAN的代码实现如下：
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is identical to pix2pix, except that the discriminator returns all downsampling
    blocks' output. Why do we need that? Well, this brings us to a discussion about
    loss functions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这与pix2pix完全相同，唯一的不同是判别器返回所有下采样块的输出。为什么我们需要这样做呢？嗯，这就引出了关于损失函数的讨论。
- en: Feature matching loss
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征匹配损失
- en: '**Feature matching loss** has been successfully used in style transfer. The
    content and style features are extracted using a pre-trained VGG and the losses
    are calculated between the target image and the generated image. The content features
    are simply the outputs from multiple convolutional blocks in VGG. GauGAN employs
    content loss to replace L1 reconstruction loss, which was common among GANs. The
    reason is that reconstruction loss makes comparisons pixel by pixel, and the loss
    can be large if the image shifts in location despite still looking the same to
    human eyes.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征匹配损失**已成功用于风格转移。使用预训练的VGG提取内容和样式特征，并计算目标图像与生成图像之间的损失。内容特征简单地来自VGG中多个卷积块的输出。GauGAN使用内容损失来取代GAN中常见的L1重建损失。原因在于重建损失是逐像素进行比较的，如果图像位置发生变化但在人眼看来仍然相同，则损失可能很大。'
- en: On the other hand, the content features of convolutional layers are spatially
    invariant. As a result, when using the content loss to train on the `facades`
    dataset, our generated buildings will look a lot less blurry and the lines will
    look straighter. The content loss in style transfer literature is sometimes known
    as **VGG loss** in code, as people love to use VGG for feature extraction.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，卷积层的内容特征是空间不变的。因此，在使用内容损失在`facades`数据集上进行训练时，我们生成的建筑物看起来模糊得多，线条看起来更直。在风格转移文献中，内容损失有时被称为代码中的**VGG损失**，因为人们喜欢使用VGG进行特征提取。
- en: Why do people still love using the old VGG?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么人们仍然喜欢使用老旧的VGG？
- en: Newer CNN architectures such as ResNet have long surpassed VGG's performance
    and achieve much higher accuracy in image classification. So, why do people still
    use VGG for feature extraction? Some have tried Inception and ResNet for neural
    style transfer but have found that the results generated using VGG were more visually
    pleasant. This is likely due to the hierarchy of VGG's architecture, with its
    monotonically increasing channel numbers across layers. This allows feature extraction
    to happen smoothly from low-level to high-level representation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 新的CNN架构如ResNet在图像分类方面的性能早已超过了VGG，并实现了更高的准确率。那么，为什么人们仍然使用VGG进行特征提取呢？一些人尝试使用Inception和ResNet进行神经风格转移，但发现使用VGG生成的结果在视觉上更加愉悦。这可能是由于VGG架构的层次结构，其通道数在各层间单调递增。这使得从低级到高级表示的特征提取能够顺利进行。
- en: In contrast, the residual blocks of ResNet have a bottleneck design that squeezes
    the input activation channels (say, 256) to a lower number (say, 64), before restoring
    it back to a higher number (256 again). Residual blocks also have a skip connection
    that could *smuggle* information for the classification task and bypass feature
    extraction in the convolutional layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，ResNet的残差块具有瓶颈设计，将输入激活通道（例如256）压缩到较低数量（例如64），然后再恢复到较高数量（再次是256）。残差块还具有跳过连接，可以为分类任务*夹带*信息并绕过卷积层中的特征提取。
- en: 'The code to calculate the VGG feature loss is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 计算VGG特征损失的代码如下：
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When calculating VGG loss, we first convert the images from `[-1, +1]` to `[0,
    255]` and from `RGB` to `BGR`, which is the image format expected by Keras' VGG
    `preprocess` function. GauGAN gives more weights to higher layers to emphasize
    structural accuracy. This is to align the generated image with the segmentation
    mask. Anyway, this is not set in stone and you are welcome to try different weights.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 计算VGG损失时，我们首先将图像从`[-1, +1]`转换为`[0, 255]`，并从`RGB`转换为`BGR`，这是Keras VGG `preprocess`函数期望的图像格式。GauGAN对更高层次给予更多权重，以强调结构准确性。这是为了使生成的图像与分割掩模对齐。总之，这并非铁板一块，欢迎您尝试不同的权重。
- en: 'Feature matching is also used on the discriminator, where we extract the discriminator
    layer outputs of real and fake images. The following code is used to calculate
    the L1 feature matching loss in the discriminator:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 特征匹配还用于鉴别器，我们在真实和虚假图像的鉴别器层输出中提取特征。以下代码用于计算鉴别器中的L1特征匹配损失：
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Apart from this, we will also have **KL divergence loss** for the encoder. The
    last loss is **hinge loss** as the new adversarial loss.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，编码器还将具有**KL散度损失**。最后一种损失是新的对抗损失**铰链损失**。
- en: Hinge loss
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 铰链损失
- en: 'Hinge loss may be a newcomer in the GAN world, but it has long been used in
    **support vector machines** (**SVMs**) for classification. It maximizes the margin
    of the decision boundary. The following plots show the hinge loss for positive
    (real) and negative (fake) labels:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 铰链损失可能是 GAN 领域的新来者，但它早已在**支持向量机**（**SVM**）中用于分类。它最大化决策边界的间隔。下图显示了正（真实）和负（假）标签的铰链损失：
- en: '![Figure 6.15 – Hinge loss for a discriminator'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15 – 判别器的铰链损失'
- en: '](img/B14538_06_15.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_15.jpg)'
- en: Figure 6.15 – Hinge loss for a discriminator
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 判别器的铰链损失
- en: 'On the left is the hinge loss for the discriminator when the image is real.
    When we use hinge loss for the discriminator,the loss is bounded to 0 when prediction
    is over 1\. If it is anything under 1, the loss increases to penalize for not
    predicting the image as real. It''s similar for fake images but in the opposite
    direction: the hinge loss is 0 when the prediction of fake image is under -1 and
    it increases linearly once above that threshold.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是当图像为真实图像时，判别器的铰链损失。当我们为判别器使用铰链损失时，当预测值大于 1 时，损失被限制为 0；如果预测值小于 1，损失则会增加，以惩罚未将图像预测为真实的情况。对于假图像也是类似的，只不过方向相反：当预测假图像小于
    -1 时，铰链损失为 0，且当预测值超过该阈值时，损失会线性增加。
- en: 'We can implement the hinge loss using basic mathematic operations as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下基本的数学运算来实现铰链损失：
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another way of doing this is by using TensorFlow''s hinge loss API:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种做法是使用 TensorFlow 的铰链损失 API：
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The loss for the generator isn''t really hinge loss; it is simply a negative
    mean of prediction. This is unbounded, so the loss is lower when the prediction
    score is higher:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失并不是真正的铰链损失；它只是一个负的预测均值。这是无界的，所以当预测分数越高时，损失越低：
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now have everything we need to train GauGAN using a training framework as
    we did in the previous chapter. The following figure shows the images generated
    using the segmentation mask:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了训练 GauGAN 所需的一切，就像在上一章中所做的那样，使用训练框架进行训练。下图展示了使用分割掩码生成的图像：
- en: '![Figure 6.16 – Example of images generated by our GauGAN implementation'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.16 – 我们的 GauGAN 实现生成的图像示例'
- en: '](img/B14538_06_16.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_06_16.jpg)'
- en: Figure 6.16 – Example of images generated by our GauGAN implementation
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 我们的 GauGAN 实现生成的图像示例
- en: They look a lot better than the pix2pix and CycleGAN results! If we encode the
    ground truth image's style into random noise, the generated images will be almost
    indistinguishable from the ground truth. It is really impressive to look at on
    the computer!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 它们看起来比 pix2pix 和 CycleGAN 的结果好得多！如果我们将真实图像的风格编码为随机噪声，生成的图像几乎无法与真实图像区分开来。用计算机查看时，效果非常令人印象深刻！
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Using AI in image editing is already prevalent now, and all this started at
    around the time that the iGAN was introduced. We learned about the key principle
    of the iGAN being to first project an image onto a manifold and then directly
    perform editing on the manifold. We then optimize this on the latent variables
    and generate an edited image that is natural-looking. This is in contrast with
    previous methods that could only change generated images indirectly by manipulating
    latent variables.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，AI 在图像编辑中的应用已经很普遍，所有这一切大约是在 iGAN 被引入的时候开始的。我们了解了 iGAN 的关键原理，即首先将图像投影到流形上，然后直接在流形上进行编辑。接着我们优化潜在变量，生成自然逼真的编辑图像。这与之前只能通过操作潜在变量间接改变生成图像的方法不同。
- en: GauGAN incorporates many advanced techniques to generate crisp images from semantic
    segmentation masks. This includes the use of hinge loss and feature matching loss.
    However, the key ingredient is SPADE, which provides superior performance when
    using a segmentation mask as input. SPADE performs normalization on a local segmentation
    map to preserve its semantic meaning, which helps us to produce high-quality images.
    So far, we have been using images with up to 256x256 resolution to train our networks.
    We now have techniques that are mature enough to generate high-resolution images,
    as we briefly discussed when introducing pix2pixHD.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: GauGAN 融合了许多先进的技术，通过语义分割掩码生成清晰的图像。这包括使用铰链损失和特征匹配损失。然而，关键成分是 SPADE，它在使用分割掩码作为输入时提供了更优的性能。SPADE
    对局部分割图进行归一化，以保持其语义含义，这有助于我们生成高质量的图像。到目前为止，我们一直使用分辨率为 256x256 的图像来训练我们的网络。我们现在拥有成熟的技术，能够生成高分辨率的图像，正如我们在介绍
    pix2pixHD 时简要讨论过的那样。
- en: In the next chapter, we will move to the realm of high-resolution images with
    advanced models such as ProgressiveGAN and StyleGAN.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进入高分辨率图像的领域，使用诸如ProgressiveGAN和StyleGAN等高级模型。
