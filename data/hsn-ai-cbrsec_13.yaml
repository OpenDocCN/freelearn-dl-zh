- en: Evaluating Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估算法
- en: As we have seen in the previous chapters, several AI solutions are available
    to achieve certain cybersecurity goals, so it is important to learn how to evaluate
    the effectiveness of various alternative solutions, using appropriate analysis
    metrics. At the same time, it is important to prevent phenomena such as overfitting,
    which can compromise the reliability of forecasts when switching from training
    data to test data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中所看到的，为了实现某些网络安全目标，有多种AI解决方案可以选择，因此，学习如何评估各种替代解决方案的效果是非常重要的，同时，使用适当的分析指标也同样关键。同时，防止过拟合等现象也很重要，因为这些现象可能会影响从训练数据转到测试数据时预测的可靠性。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Feature engineering best practices in dealing with raw data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理原始数据时的特征工程最佳实践
- en: How to evaluate a detector's performance using the ROC curve
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用ROC曲线评估检测器的性能
- en: How to appropriately split sample data into training and test sets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何适当地将样本数据拆分为训练集和测试集
- en: How to manage algorithms' overfitting and bias–variance trade-offs with cross
    validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过交叉验证来管理算法的过拟合和偏差–方差权衡
- en: Now, let's begin our discussion of we need feature engineering by examining
    the very nature of raw data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从原始数据的本质开始，讨论为什么我们需要特征工程。
- en: Best practices of feature engineering
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程的最佳实践
- en: In the previous chapters, we looked at different **artificial intelligence**
    (**AI**) algorithms, analyzing their application to the different scenarios and
    their use cases in a cybersecurity context. Now, the time has come to learn how
    to evaluate these algorithms, starting from the assumption that algorithms are
    the foundation of data-driven learning models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们探讨了不同的**人工智能**（**AI**）算法，分析了它们在不同场景中的应用及其在网络安全背景下的使用案例。现在，到了学习如何评估这些算法的时刻，前提是算法是数据驱动学习模型的基础。
- en: We will therefore have to deal with the very nature of the data, which is the
    basis of the algorithm learning process, which aims to make generalizations in
    the form of predictions based on the samples received as input in the training
    phase.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须处理数据的本质，数据是算法学习过程的基础，算法旨在根据训练阶段接收的样本输入，进行基于预测的泛化。
- en: The choice of algorithm will therefore fall on the one that is best for generalizing
    beyond the training data, thereby obtaining the best predictions when facing new
    data. In fact, it is relatively simple to identify an algorithm that fits the
    training data; the problem becomes more complicated when the algorithm must correctly
    make predictions on data that has never been seen before. In fact, we will see
    that the tendency to optimize the accuracy of the algorithm's predictions on training
    data gives rise to the phenomenon known as **overfitting**, where predictions
    become worse when dealing with new test data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，算法的选择将落在能够最好地进行泛化的算法上，从而在面对新数据时获得最佳预测。实际上，识别一个适合训练数据的算法是相对简单的；当算法必须对从未见过的数据做出正确预测时，问题变得更加复杂。事实上，我们将看到，优化算法在训练数据上的预测准确性会导致一个名为**过拟合**的现象，即在处理新测试数据时，预测结果变得更差。
- en: It therefore becomes important to understand how to correctly perform the algorithm
    training, from the selection of the training dataset up to the correct tuning
    of the learning parameters characterizing the chosen algorithm.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理解如何正确执行算法训练变得非常重要，从选择训练数据集到正确调整特征所选算法的学习参数。
- en: There are several methods available for performing algorithm training, such
    as using the same training dataset (for example, by dividing the training dataset
    into two separate subsets, one for training and one for testing) and choosing
    a suitable percentage of the original training dataset to be assigned to the two
    distinct subsets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以用于执行算法训练，比如使用相同的训练数据集（例如，通过将训练数据集划分为两个独立的子集，一个用于训练，一个用于测试），并选择一个适当的原始训练数据集百分比来分配到这两个不同的子集。
- en: Another strategy is based on cross validation, which, as we will see, consists
    of randomly dividing the training dataset into a certain number of subsets on
    which to train the algorithm and calculate the average of the results obtained
    in order to verify the accuracy of predictions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略基于交叉验证，正如我们将看到的，它是通过随机将训练数据集划分为若干子集，然后在这些子集上训练算法并计算所得结果的平均值，从而验证预测的准确性。
- en: Better algorithms or more data?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更好的算法还是更多的数据？
- en: 'While it is true that, in order to make correct predictions (which, in turn,
    are nothing more than generalizations starting from sample data), the data alone
    is not enough; you need to combine the data with algorithms (which, in turn, are
    nothing more than data representations). In practice, however, we are often faced
    with a dilemma when improving our predictions: should we design a better algorithm,
    or do we just need to collect more data? The answer to this question has not always
    been the same over time, since, when research in the field of AI began, the emphasis
    was on the quality of the algorithms, since the availability of data was dictated
    by the cost of storage.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然确实如此，为了做出正确的预测（这些预测其实也不过是从样本数据出发的泛化），单凭数据是不够的；你需要将数据与算法结合（算法实际上不过是数据的表示）。然而，在实践中，我们常常面临一个困境：是应该设计更好的算法，还是仅仅需要收集更多数据？这个问题随着时间的推移并非始终如一的答案，因为当人工智能领域的研究起步时，重点在于算法的质量，因为数据的可用性受制于存储成本。
- en: With the reduction in costs associated with storage, in recent years, we have
    witnessed an unprecedented explosion in the availability of data, which has given
    rise to new analytical techniques based on big data, and the emphasis has consequently
    shifted to the availability of data. However, as the amount of data available
    increases, the amount of time required to analyze it increases accordingly, so,
    in choosing between the quality of the algorithms and the amount of training data,
    we must face a trade-off.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着存储成本的降低，近年来我们见证了数据可用性前所未有的爆炸式增长，这催生了基于大数据的新分析技术，因此，焦点逐渐转向了数据的可用性。然而，随着可用数据量的增加，所需分析的时间也相应增加，因此，在选择算法的质量和训练数据量之间，我们必须面对一个权衡。
- en: In general, practical experience shows us that even a dumb algorithm powered
    by large amounts of data is able to produce better predictions than a clever algorithm
    fed with less data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，实践经验告诉我们，即便是一个由大量数据驱动的简单算法，也能比一个处理较少数据的复杂算法产生更好的预测结果。
- en: However, the very nature of the data is often the element that makes the difference.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据的本质往往是决定性因素。
- en: The very nature of raw data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始数据的本质
- en: The emphasis given to the relevance of data often resonates in the motto that
    states let the data speak for itself. In reality, data is almost never able to
    speak for itself, and when it does, it usually deceives us. The raw data is nothing
    more than fragments of information that behave like pieces of a puzzle where we
    do not (yet) know the bigger picture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据相关性的重视常常体现在一句格言中：“让数据为自己说话”。然而，现实中，数据几乎从来不能为自己说话，而且当它确实这么做时，通常会误导我们。原始数据不过是一些信息碎片，像拼图的碎片一样，我们（还）无法看到完整的图景。
- en: To make sense of the raw data, we therefore need models that help us to distinguish
    the necessary pieces (the signal) from the useless pieces (the noise), in addition
    to identifying the missing pieces to complete our puzzle.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了理解原始数据，我们需要模型帮助我们区分必要的部分（信号）与无用的部分（噪声），并且识别出缺失的部分，以完成我们的拼图。
- en: The models, in the case of AI, take the form of mathematical relations between
    features, through which we are able to show the different aspects and the different
    functions that the data represents, based on the purpose we intend to achieve
    with our analysis. In order for the raw data to be inserted in to our mathematical
    models, it must first be treated appropriately, thereby becoming the feature of
    our models. A feature, in fact, is nothing but the numerical representation of
    the raw data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的案例中，模型表现为特征之间的数学关系，通过这些关系，我们能够根据分析目的，展示数据所代表的不同方面和不同功能。为了将原始数据输入到我们的数学模型中，它必须首先经过适当处理，从而成为我们模型的特征。事实上，特征不过是原始数据的数值表示。
- en: For example, raw data often does not occur in numerical form. However, the representation
    of data in numerical form is a necessary prerequisite, in order to get processed
    by algorithms. Therefore, we must convert the raw data into numerical form before
    feeding it to our algorithms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，原始数据往往不是以数字形式出现。然而，以数字形式表示数据是必要的前提，以便能够通过算法进行处理。因此，我们必须在将原始数据输入到算法之前，将其转换为数字形式。
- en: Feature engineering to the rescue
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程来拯救我们
- en: Therefore, in implementing our predictive models, we must not simply limit ourselves
    to specifying the choice of the algorithm(s), but we must also define the features
    required to power them. As such, the correct definition of features is an essential
    step, both for the achievement of our goals and for the efficiency of the implementation
    of our predictive model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实施我们的预测模型时，我们不能仅仅局限于指定算法的选择，还必须定义支持这些算法所需的特征。因此，正确地定义特征是一个关键步骤，既是实现我们目标的必要步骤，也是高效实施预测模型的前提。
- en: As we have said, the features constitute the numerical representations of the
    raw data. There are obviously different ways to convert raw data into numerical
    form, and these are distinguished according to the varying nature of the raw data,
    in addition to the types of algorithms of choice. Different algorithms, in fact,
    require different features in order to work.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，特征是原始数据的数值表示。显然，将原始数据转换为数值形式的方式有很多种，这些方式根据原始数据的性质以及所选择的算法类型有所不同。不同的算法实际上需要不同的特征才能工作。
- en: The number of features is also equally important for the predictive performance
    of our model. Choosing the quality and quantity of features therefore constitutes
    a preliminary process, known as feature engineering.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的数量对我们模型的预测性能同样至关重要。因此，选择特征的质量和数量构成了一个初步过程，这被称为特征工程。
- en: Dealing with raw data
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理原始数据
- en: A first screening is conducted on the basis of the nature of the numerical values
    that are to be associated with our models. We should ask ourselves whether the
    number values we require are only positive or negative, or just Boolean values,
    whether we can limit ourselves to certain orders of magnitude, whether we can
    determine in advance the maximum value and the minimum value that the features
    can assume, and so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，根据将要与我们的模型关联的数值性质进行筛选。我们应该问自己，我们所需的数值是仅为正数还是负数，还是仅为布尔值，是否可以仅限于某些数量级，是否能够预先确定特征可以假定的最大值和最小值，等等。
- en: We can also artificially create complex features, starting from simple features,
    in order to increase the explanatory, as well as the predictive, capacity of our
    models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过从简单特征出发，人工创建复杂特征，以提高我们模型的解释能力和预测能力。
- en: 'Here are some of the most common transformations applicable to transforming
    raw data into model features:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最常见的适用于将原始数据转化为模型特征的转化方法：
- en: Data binarization
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据二值化
- en: Data binning
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分箱
- en: Logarithmic data transformation
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数数据转化
- en: We will now examine each of these transformations in detail.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细讨论这些转化方法。
- en: Data binarization
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据二值化
- en: One of the most basic forms of transformation, based on raw data counting, is
    binarization, which consists of assigning the value `1` to all the counts greater
    than `0`, and assigning the value `0` in the remaining cases. To understand the
    usefulness of binarization, we only need to consider the development of a predictive
    model whose goal is to predict user preferences based on video visualizations.
    We could therefore decide to assess the preferences of the individual users simply
    by counting their respective visualizations of videos; however, the problem is
    that the order of magnitude of the visualizations varies according to the habits
    of the individual users.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于原始数据计数的最基本的转化形式之一是二值化，它是将大于`0`的所有计数值赋值为`1`，其余情况赋值为`0`。为了理解二值化的实用性，我们只需考虑开发一个预测模型，其目标是根据视频视觉化来预测用户的偏好。因此，我们可以简单地通过计算每个用户观看视频的次数来评估他们的偏好；然而，问题在于视觉化的数量级因用户的个人习惯而有所不同。
- en: Therefore, the absolute value of the visualizations—that is, the raw count—does
    not constitute a reliable measure of the greater or lesser preference accorded
    to each video. In fact, some users have the habit of repeatedly visualizing the
    same video, without paying particular attention to it, while other users prefer
    to focus their attention, thereby reducing the number of visualizations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，视觉化的绝对值——即原始计数——并不能作为衡量每个视频所获得偏好程度的可靠标准。事实上，一些用户有反复观看同一视频的习惯，而不特别关注它，而其他用户则倾向于集中注意力，从而减少了视觉化的次数。
- en: Moreover, the different orders of magnitude associated with video visualizations
    by each user, varying from tens to hundreds, or even thousands of views, based
    on a user's habits, makes some statistical measurements, such as the arithmetic
    average, less representative of individual preferences.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于每个用户的视频可视化所关联的不同量级，视图数从几十到几百，甚至几千不等，这与用户的习惯有关，这使得某些统计度量，如算术平均值，较难代表个体偏好。
- en: Instead of using the raw count of visualizations, we can binarize the counts,
    associating the value `1` with all the videos that obtained a number of visualizations
    greater than `0` and the value `0` otherwise. Obtaining the results in this way
    is more efficient and robust measure of individual preferences.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将视图计数二值化，将值 `1` 关联到所有获取了大于 `0` 次视图的视频，将值 `0` 关联到其他视频。以这种方式获得结果是衡量个体偏好的更高效和稳健的方式。
- en: Data binning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分箱
- en: Managing the different orders of magnitude of the counts is a problem that occurs
    in different situations, and there are many algorithms that behave badly when
    faced with data that exhibits a wide ranges of values, such as clustering algorithms
    that measure similarity on the basis of Euclidean distance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 管理计数的不同量级是一个在不同场景中都会遇到的问题，许多算法在处理具有广泛值范围的数据时表现不佳，例如基于欧几里得距离衡量相似度的聚类算法。
- en: In a similar way to binarization, it is possible to reduce the dimensional scale
    by grouping the raw data counts into containers called **bins**, with fixed amplitude
    (fixed-with binning), sorted in ascending order, thereby scaling their absolute
    values linearly or exponentially.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与二值化类似，可以通过将原始数据计数分组到称为**箱**的容器中来减少维度尺度，箱具有固定的幅度（固定宽度分箱），按升序排序，从而线性或指数地缩放其绝对值。
- en: Logarithmic data transformation
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数数据转换
- en: Similarly, it is possible to reduce the magnitude of raw data counts by replacing
    their absolute values with logarithms.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可以通过将原始数据计数的绝对值替换为对数值，从而减少数据的量级。
- en: A peculiar feature of the logarithmic transformation is precisely that of reducing
    the relevance of greater values, and, at the same time, of amplifying smaller
    values, thereby achieving greater uniformity of value distribution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对数转换的一个独特特性正是减少大值的重要性，同时放大小值，从而实现更均匀的值分布。
- en: In addition to logarithms, it is possible to use other power functions, which
    allow the stabilization of the variance of a data distribution (such as the **Box–Cox
    transformation**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对数之外，还可以使用其他幂函数，这些函数可以稳定数据分布的方差（例如**Box–Cox变换**）。
- en: Data normalization
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据归一化
- en: Also known as feature normalization or feature scaling, data normalization improves
    the performance of algorithms that can be influenced by the scale of input values.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化也称为特征归一化或特征缩放，它能提高受输入值规模影响的算法的性能。
- en: The following are the most common examples of feature normalization.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最常见的特征归一化示例。
- en: Min–max scaling
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小–最大缩放
- en: With the min–max scaling transformation, we let the data fall within a limited
    range of values: `0` and `1`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最小–最大缩放转换时，我们将数据限制在一个有限的值范围内：`0` 和 `1`。
- en: 'The transformation of the data involves replacing the original values ![](img/b50a1285-4c07-480b-9570-0cfe78242d22.png)
    with the values calculated with the following formula:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的转换涉及用以下公式计算的值替换原始值 ![](img/b50a1285-4c07-480b-9570-0cfe78242d22.png)：
- en: '![](img/0f345394-1d4d-40bd-a45e-4812c056ff5e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f345394-1d4d-40bd-a45e-4812c056ff5e.png)'
- en: Here, ![](img/b040dc4a-5be0-4b03-b363-0cd1a90eb0f8.png) represents the minimum
    value of the entire distribution, and ![](img/61bfae4a-ae32-4882-8498-a06ebf8fe84f.png)
    the maximum value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/b040dc4a-5be0-4b03-b363-0cd1a90eb0f8.png) 代表整个分布的最小值，![](img/61bfae4a-ae32-4882-8498-a06ebf8fe84f.png)
    代表最大值。
- en: Variance scaling
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差缩放
- en: Another very common data normalization method involves subtracting the mean
    of the distribution from each single ![](img/627215c2-d54f-4636-840d-349182d572f7.png)
    value, and then dividing the result obtained by the variance of the distribution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非常常见的数据归一化方法是从每个单独的值中减去分布的均值，然后将得到的结果除以分布的方差 ![](img/627215c2-d54f-4636-840d-349182d572f7.png)。
- en: Following normalization (also known as **standardization**), the distribution
    of the recalculated data shows a mean equal to `0` and a variance equal to `1`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 经过归一化（也称为**标准化**）后，重新计算的数据分布显示均值为 `0`，方差为 `1`。
- en: 'The formula for variance scaling is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 方差缩放的公式如下：
- en: '![](img/d352f705-2937-4bb7-b122-f20007038cbe.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d352f705-2937-4bb7-b122-f20007038cbe.png)'
- en: How to manage categorical variables
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何管理类别变量
- en: Raw data can be represented by categorical variables that take non-numeric values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据可以通过取非数字值的类别变量来表示。
- en: A typical example of a categorical variable is nationality. In order to mathematically
    manage categorical variables, we need to use some form of category transformation
    in numerical values, also called encoding.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量的典型示例是国籍。为了数学上管理类别变量，我们需要使用某种形式的类别转换为数值，也称为编码。
- en: The following are the most common methods of categorical encoding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最常见的几种类别编码方法。
- en: Ordinal encoding
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序数编码
- en: 'An intuitive approach to encoding could be to assign a single progressive value
    to the individual categories:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观的编码方法是为各个类别分配一个逐步递增的值：
- en: '![](img/8f0f4e40-c79a-4776-b3da-ce4424d745db.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f0f4e40-c79a-4776-b3da-ce4424d745db.png)'
- en: The advantage and disadvantage of this encoding method is that the transformed
    values may be numerically ordered, even when this numerical ordering has no real
    meaning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码方法的优点和缺点是，尽管这些转换后的值可能在数值上有顺序，但这种数值顺序实际上没有实际意义。
- en: One-hot encoding
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: With the one-hot encoding method, a set of bits is assigned to each variable,
    with each bit representing a distinct category.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码方法，为每个变量分配一组位，每个位表示一个不同的类别。
- en: 'The set of bits enables us to distinguish the variables that cannot belong
    to more than one category, resulting in only one bit data set:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这组位使我们能够区分不能属于多个类别的变量，结果只会有一个位的数据集：
- en: '![](img/05b97bc3-bed1-4916-a6bc-322d838ca09a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05b97bc3-bed1-4916-a6bc-322d838ca09a.png)'
- en: Dummy encoding
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟编码
- en: 'The one-hot encoding method actually wastes a bit (that, in fact, is not strictly
    necessary), which can be eliminated using the dummy encoding method:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码方法实际上有些浪费（事实上，并不是绝对必要的），这一点可以通过使用虚拟编码方法来消除：
- en: '![](img/715eddd8-8a7f-4a82-949a-97112dabc963.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/715eddd8-8a7f-4a82-949a-97112dabc963.png)'
- en: Feature engineering examples with sklearn
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sklearn 进行特征工程的示例
- en: Now let's look at some examples of feature engineering implementation using
    the NumPy library and the preprocessing package of the `scikit-learn` library.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看使用 NumPy 库和`scikit-learn`库的预处理包实现特征工程的一些示例。
- en: Min–max scaler
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小-最大缩放器
- en: 'In the following code, we see an example of feature engineering using the `MinMaxScaler`
    class of `scikit-learn`, aimed at scaling features to lie between a given range
    of values (minimum and maximum), such as `0` and `1`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们看到一个使用`scikit-learn`的`MinMaxScaler`类进行特征工程的示例，旨在将特征缩放到给定的值范围（最小值和最大值）之间，例如`0`和`1`：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Standard scaler
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化缩放器
- en: 'The following example shows the `StandardScaler` class of `scikit-learn` in
    action, used to compute the mean and standard deviation on a training set by leveraging
    the `transform()` method:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了`scikit-learn`的`StandardScaler`类的应用，使用`transform()`方法计算训练集的均值和标准差：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Power transformation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幂变换
- en: 'In the following example, we see the `PowerTransformer` class of `scikit-learn` in
    action, applying the zero-mean, unit-variance normalization to the transformed
    output using a Box–Cox transformation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们看到`scikit-learn`的`PowerTransformer`类在应用 Box-Cox 变换时，使用零均值、单位方差归一化对变换后的输出进行处理：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Ordinal encoding with sklearn
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sklearn 进行序数编码
- en: 'In the following example, we see how to encode categorical features into integers
    using the `OrdinalEncoder` class of `scikit-learn` and its `transform()` method:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们看到如何使用`scikit-learn`的`OrdinalEncoder`类及其`transform()`方法将类别特征编码为整数：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One-hot encoding with sklearn
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sklearn 进行独热编码
- en: 'The following example shows how to transform categorical features into binary
    representation, making use of the `OneHotEncoder` class of `scikit-learn`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用`scikit-learn`的`OneHotEncoder`类将类别特征转换为二进制表示：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After having described feature engineering best practices, we can move on to
    evaluating the performance of our models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述完特征工程的最佳实践后，我们可以继续评估我们模型的性能。
- en: Evaluating a detector's performance with ROC
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ROC 曲线评估检测器的性能
- en: We have previously encountered the ROC curve and AUC measure ([Chapter 5](a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml), *Network
    Anomaly Detection with AI**,* and [Chapter 7](98ce7db1-f53d-47ca-b6ca-ec0e5f882566.xhtml),
    *Fraud Prevention with Cloud AI Solutions*) to evaluate and compare the performance
    of different classifiers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经遇到过ROC曲线和AUC度量（[第5章](a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml)，*基于AI的网络异常检测*，以及[第7章](98ce7db1-f53d-47ca-b6ca-ec0e5f882566.xhtml)，*基于云AI解决方案的欺诈预防*），用于评估和比较不同分类器的性能。
- en: 'Now let''s explore the topic in a more systematic way, introducing the confusion
    matrix associated with all the possible results returned by a fraud-detection
    classifier, comparing the predicted values with the real values:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们以更系统的方式探索这个主题，引入与欺诈检测分类器返回的所有可能结果相关的混淆矩阵，将预测值与实际值进行比较：
- en: '![](img/da832600-40a3-4ae9-9307-cd648dae232d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da832600-40a3-4ae9-9307-cd648dae232d.png)'
- en: 'We can then calculate the following values (listed with their interpretation)
    based on the previous confusion matrix:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以基于之前的混淆矩阵计算以下值（附带其解释）：
- en: '**Sensitivity** = **Recall** = **Hit rate** = ***TP/(TP + FP)***: This value
    measures the rate of correctly labeled fraudsters and represents the **true positive
    rate** (**TPR**)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感度** = **召回率** = **命中率** = ***TP/(TP + FP)***：该值衡量正确标记为欺诈者的比例，表示**真阳性率**（**TPR**）。'
- en: '**False Positive Rate** ***(FPR) = FP/(FP + TN)***: FPR is also calculated
    as 1 – Specificity'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性率** ***(FPR) = FP/(FP + TN)***：FPR也可以通过1 – 特异性来计算。'
- en: '**Classification accuracy** = ***(TP + TN)/(TP + FP + FN + TN)***: This value
    represents the percentage of correctly classified observations'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类准确率** = ***(TP + TN)/(TP + FP + FN + TN)***：该值表示正确分类的观察结果所占的百分比。'
- en: '**Classification error** = ***(FP + FN)/(TP + FP + FN + TN)***: This value
    represents the misclassification rate'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类错误率** = ***(FP + FN)/(TP + FP + FN + TN)***：该值表示误分类率。'
- en: '**Specificity** = ***TN/(FP + TN)***: This value measures the rate of correctly
    labeled nonfraudsters'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性** = ***TN/(FP + TN)***：该值衡量正确标记为非欺诈者的比例。'
- en: '**Precision** = ***TP/(TP + FP)***: This value measures how many of the predicted
    fraudsters are actually fraudsters'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率** = ***TP/(TP + FP)***：该值衡量预测为欺诈者的个体中，实际为欺诈者的比例。'
- en: '***F*** - ***measure*** = ***2*** x **(*Precision*** x ***Recall*)/(*Precision***
    + ***Recall*)**: This value represents the weighted harmonic mean of the precision
    and recall. The *F*-measure varies from 0 (worst score) to 1 (best value)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***F*** - ***度量*** = ***2*** x **(*精确率*** x ***召回率*)/(*精确率*** + ***召回率*)**：该值表示精确率和召回率的加权调和平均数。*F*度量的范围从0（最差评分）到1（最佳值）。'
- en: We are now able to analyze in detail the ROC curve and its associated AUC measure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以详细分析ROC曲线及其相关的AUC度量。
- en: ROC curve and AUC measure
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线和AUC度量
- en: 'Among the techniques most commonly used to compare the performance of different
    classifiers, we have the **receiving operating characteristic** (**ROC**) curve,
    which describes the relationship between the TPR, or sensitivity, and the FPR,
    or 1 – Specificity, associated with each classifier:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较不同分类器性能时最常用的技术之一是**接收操作特征**（**ROC**）曲线，它描述了每个分类器的TPR（真阳性率或敏感度）与FPR（假阳性率或1-特异性）之间的关系：
- en: '![](img/8318045e-fdbd-4639-8182-2f3d23ffe968.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8318045e-fdbd-4639-8182-2f3d23ffe968.png)'
- en: '*(Image Credits: http://scikit-learn.org/)*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图片来源：http://scikit-learn.org/)*'
- en: How can the performances of the different classifiers be compared?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如何比较不同分类器的性能？
- en: Let's start by taking into consideration the characteristics that the best possible
    classifier should have. Its curve would correspond to the pair of values, ​​***x***
    = ***0*** and ***y*** = ***1***, in the ROC space. In other words, the best possible
    classifier is the one that correctly identifies all cases of fraud without generating
    any false positives, meaning that ***FPR ***= ***0*** and ***TPR*** = ***1***
    are ideal.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑最佳分类器应具备的特性。其曲线应对应于ROC空间中的坐标对***x*** = ***0***和***y*** = ***1***。换句话说，最佳分类器是能够正确识别所有欺诈案件且没有产生任何假阳性的分类器，即理想情况下***FPR***
    = ***0***和***TPR*** = ***1***。
- en: Similarly, the performance of a random classifier, which makes predictions at
    random, would fall on the diagonal described by the pair of coordinates [***x***
    = ***0***, ***y*** = ***0***] and [***x*** = ***1***, ***y*** = ***1***].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，随机分类器的性能（即随机预测）会落在由坐标对[***x*** = ***0***，***y*** = ***0***]和[***x*** = ***1***，***y***
    = ***1***]描述的对角线上。
- en: The comparison between the performances of different classifiers would therefore
    involve verifying how much their curves deviate from the *L*-curve (corresponding
    to the best classifier).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，不同分类器性能之间的比较需要验证它们的曲线与*L*-曲线（对应最佳分类器）之间的偏差。
- en: To achieve a more precise measure of performance, we can calculate the **area
    under the ROC curve **(**AUC**) metric associated with the individual classifiers.
    The values ​​that the AUC metric can assume fall in the range between 0 and 1.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更精确地衡量性能，我们可以计算每个分类器的**ROC曲线下面积（AUC）**指标。AUC指标的值介于0到1之间。
- en: The best classifier's AUC metric is equal to 1, corresponding to the maximum
    value of AUC. The AUC metric can also be interpreted as a measure of the probability.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳分类器的AUC指标等于1，对应于AUC的最大值。AUC指标也可以解释为概率度量。
- en: In fact, a random classifier, whose curve corresponds to the diagonal in the
    ROC space, would have an AUC value of 0.5\. Consequently, the performance of any
    other classifier should fall between a minimum AUC value of 0.5 and a maximum
    value of 1\. Values ​​of ***AUC*** < ***0.5*** indicate that the chosen classifier
    behaves worse than the random classifier.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一个随机分类器，其曲线对应于ROC空间中的对角线，将具有0.5的AUC值。因此，任何其他分类器的性能应当介于最小AUC值0.5和最大值1之间。***AUC***值小于***0.5***表明所选分类器的表现比随机分类器更差。
- en: To correctly evaluate the quality of the estimated probabilities of the individual
    classifiers, we can use the **Brier score** (**BS**), which measures the average
    of the differences between the estimated probabilities and the actual values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确评估单个分类器的估计概率质量，我们可以使用**Brier分数**（**BS**），它衡量估计概率与实际值之间差异的平均值。
- en: 'Here is the BS formula:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是BS公式：
- en: '![](img/e252f1d0-4e6a-421d-8e08-7fd7071da4d2.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e252f1d0-4e6a-421d-8e08-7fd7071da4d2.png)'
- en: Here, *P**[i]* is the estimated probability for observation *i*, and ![](img/5d83a679-ee27-42df-b126-418635c4971a.png)
    is a binary estimator (that assumes values of 0 or 1) for the actual value, *i*. Also,
    the value of *BS* falls in the interval between 0 and 1, but, unlike the AUC,
    smaller values of *BS* (that is, *BS* values closer to 0) correspond to more accurate
    probability estimates.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*P**[i]*是观察值*i*的估计概率，![](img/5d83a679-ee27-42df-b126-418635c4971a.png)是一个二进制估算器（假定为0或1），用于表示实际值*i*。此外，*BS*的值介于0和1之间，但与AUC不同，较小的*BS*值（即接近0的*BS*值）对应于更准确的概率估计。
- en: The following are some examples of calculations of the ROC curves and of the
    metrics associated with them, using the `scikit-learn` library.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些使用`scikit-learn`库计算ROC曲线及其相关指标的示例。
- en: Examples of ROC metrics
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC指标示例
- en: 'In the following code, we can see an example of a calculation of ROC metrics,
    using scikit-learn''s methods, such as `precision_recall_curve()`, `average_precision_score()`,
    `recall_score()`, and `f1_score()`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们可以看到使用`scikit-learn`的各种方法（如`precision_recall_curve()`、`average_precision_score()`、`recall_score()`和`f1_score()`）计算ROC指标的示例：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ROC curve example
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线示例
- en: 'The following code shows how to calculate an ROC curve using the `roc_curve()`
    method of `scikit-learn`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用`scikit-learn`的`roc_curve()`方法计算ROC曲线：
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: AUC score example
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AUC分数示例
- en: 'In the following example code, we can see how to calculate the AUC curve using
    the `roc_auc_score()` method of `scikit-learn`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例代码中，我们可以看到如何使用`scikit-learn`的`roc_auc_score()`方法计算AUC曲线：
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Brier score example
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Brier分数示例
- en: 'In the following example, we evaluate the quality of estimated probabilities
    using the `brier_score_loss()` method of `scikit-learn`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用`scikit-learn`的`brier_score_loss()`方法评估估计概率的质量：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let's continue the evaluation of model performances by introducing the
    effects deriving from the splitting of the sample dataset into training and testing
    subsets.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过引入从将样本数据集分割为训练集和测试集子集得到的影响，继续评估模型的性能。
- en: How to split data into training and test sets
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将数据分割为训练集和测试集
- en: One of the most commonly used methods to evaluate the learning effectiveness
    of our models is to test the predictions made by the algorithms on data it has
    never seen before. However, it is not always possible to feed fresh data into
    our models. One alternative involves subdividing the data at our disposal into
    training and testing subsets, varying the percentages of data to be assigned to
    each subset. The percentages usually chosen vary between 70% and 80% for the training
    subset, with the remaining 20–30% assigned to the testing subset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 评估我们模型学习效果的最常用方法之一是测试算法在从未见过的数据上的预测。然而，并非总能将新数据输入到我们的模型中。一种替代方法是将现有数据划分为训练集和测试集，并调整分配给每个子集的数据比例。通常，训练集占70%到80%，剩余的20%到30%分配给测试集。
- en: 'The subdivision of the original sample dataset into two subsets for training
    and testing can be easily performed using the `scikit-learn` library, as we have
    done several times in our examples:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始样本数据集划分为训练集和测试集可以通过`scikit-learn`库轻松完成，正如我们在多个例子中所做的那样：
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By invoking the `train_test_split()` method of the `sklearn.model_selection`
    package and setting the `test_size = 0.2` parameter, we are splitting the original
    sample dataset into training and testing subsets, reserving a percentage equal
    to 20% of the original dataset for the testing dataset and assigning the remaining
    80% to the training dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`sklearn.model_selection`包中的`train_test_split()`方法，并设置`test_size = 0.2`参数，我们将原始样本数据集划分为训练集和测试集，保留原始数据集20%的比例作为测试数据集，剩余的80%分配给训练数据集。
- en: This technique, however simple, may nonetheless have important effects on the
    learning effectiveness of our algorithms.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术虽然简单，但仍可能对我们算法的学习效果产生重要影响。
- en: Algorithm generalization error
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法泛化误差
- en: 'As we have seen, the purpose of the algorithms is to learn to make correct
    predictions by generalizing from the training samples. All the algorithms, as
    a result of this learning process, manifest a generalization error that can be
    expressed as the following formula:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，算法的目的是通过对训练样本的泛化来学习做出正确的预测。因此，所有算法在这一学习过程中都会表现出一定的泛化误差，可以用以下公式表示：
- en: '![](img/0d7a09e9-ab03-4acf-a353-122d729b597b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d7a09e9-ab03-4acf-a353-122d729b597b.png)'
- en: By *Bias*, we mean the systematic error made by the algorithm in carrying out
    its predictions, and by *Variance*, we mean the sensitivity of the algorithm to
    the variations affecting the analyzed data. Finally, *Noise* is an irreducible
    component that characterizes the data being analyzed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*偏差*（Bias），我们指的是算法在执行预测时所犯的系统性错误，通过*方差*（Variance），我们指的是算法对影响分析数据的变化的敏感度。最后，*噪声*（Noise）是一个不可减少的成分，它表征了被分析数据的特点。
- en: The following diagram shows different estimators that are characterized by their
    ability to adapt to data. Starting from the simplest estimator and moving up to
    the most complex estimator, we can see how the components of *Bias* and *Variance*
    vary. A lower complexity of the estimator usually corresponds to a higher *Bias*
    (systematic error) and a reduced variance—that is, sensitivity to data change.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了不同估算器的能力，这些估算器根据其适应数据的能力而有所不同。从最简单的估算器开始，逐渐到最复杂的估算器，我们可以看到*偏差*和*方差*的变化。估算器的复杂度越低，通常对应着更高的*偏差*（系统性误差）和较低的方差，即对数据变化的敏感度减少。
- en: 'Conversely, as the complexity of the model increases, the *Bias* is reduced
    but the *Variance* is increased, so that more complex models tend to over-adapt
    (overfit) their predictions to training data, thereby producing inferior predictions
    when switching from training data to testing data:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，随着模型复杂度的增加，*偏差*（Bias）减少，但*方差*（Variance）增加，因此更复杂的模型往往会过度适应（过拟合）其预测，导致从训练数据到测试数据的预测效果变差：
- en: '![](img/6e2393d9-95d0-4e22-936e-27a7d913053d.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e2393d9-95d0-4e22-936e-27a7d913053d.png)'
- en: '*(Image Credits: http://scikit-learn.org/)*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图片来源：http://scikit-learn.org/)*'
- en: A method to reduce the *Variance* associated with the complexity of the algorithm
    involves increasing the amount of data constituting the training dataset; however,
    it is not always easy to distinguish which component (the *Bias* or the *Variance*)
    assumes greater importance in the determination of the generalization error. Therefore,
    we must use appropriate tools to distinguish the role played by the various components
    in the generalization error determination.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一种减少与算法复杂度相关的*方差*的方法是增加构成训练数据集的数据量；然而，并不是总能轻松区分哪个成分（*偏差*还是*方差*）在确定泛化误差时更为重要。因此，我们必须使用适当的工具来区分各个成分在泛化误差决定中的作用。
- en: Algorithm learning curves
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法学习曲线
- en: 'A useful tool for identifying a component between the *Bias* and the *Variance* is
    important in determining the generalization error of an algorithm. This is the
    learning curve, through which the predictive performance of the algorithm is compared
    with the amount of training data. This way, it is possible to evaluate how the
    training score and the testing score of an algorithm vary as the training dataset
    changes:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于识别*偏差*（Bias）和*方差*（Variance）之间成分的有用工具，对于确定算法的泛化误差非常重要。这就是学习曲线，通过它可以比较算法的预测性能与训练数据的数量。通过这种方式，可以评估算法的训练得分和测试得分如何随着训练数据集的变化而变化：
- en: '![](img/c82113c4-7be1-4048-af6a-e548e938369c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c82113c4-7be1-4048-af6a-e548e938369c.png)'
- en: '*(Image Credits: Wikipedia https://commons.wikimedia.org/wiki/File:Variance-bias.svg)*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图片来源：维基百科 https://commons.wikimedia.org/wiki/File:Variance-bias.svg)*'
- en: If the training score and the testing score tend to converge when the training
    dataset grows (as shown in the preceding diagram), in order to improve our predictions,
    we will have to increase the complexity of our algorithm, thereby reducing the
    *Bias* component.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当训练数据集增长时，训练得分和测试得分趋于收敛（如上图所示），为了提高我们的预测效果，我们必须增加算法的复杂度，从而减少*偏差*成分。
- en: If instead the training score is constantly higher than the testing score, an
    increase in the training dataset would improve the predictive performance of our
    algorithm, thereby reducing the *Variance* component. Finally, in the case where
    the training score and the testing score do not converge, our model is characterized
    by high variance, and we will therefore have to act both on the complexity of
    the algorithm and on the size of the training dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练得分始终高于测试得分，那么增加训练数据集的规模将提高我们算法的预测性能，从而减少*方差*成分。最后，如果训练得分和测试得分无法收敛，那么我们的模型就表现出高方差特征，因此我们必须同时调整算法的复杂度和训练数据集的大小。
- en: 'In the following example, we can see how to use the `learning_curve()` method
    of the `sklearn.model_selection` package to obtain the values necessary to design
    a learning curve combined with a **support vector classifier** (**SVC**), based
    on different training dataset sizes:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们可以看到如何使用`sklearn.model_selection`包的`learning_curve()`方法，基于不同的训练数据集大小，获取设计学习曲线所需的值，并结合**支持向量分类器**（**SVC**）进行分析：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In conclusion, we can say that our choice of the size of the training dataset
    affects on the learning effectiveness of our algorithms. In principle, by reducing
    the percentage assigned to the training dataset, we are increasing the *Bias*
    error component. If instead we increase the size of the training dataset while
    maintaining the original sample dataset's size constant, we risk over-adapting
    the algorithm to the training data, which results in inferior predictions when
    we feed our algorithm new data, not to mention the fact that some highly informative
    samples could be excluded from the training dataset, due to the simple effect
    of the case, in relation to the specific splitting strategy we choose.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以说，训练数据集大小的选择会影响我们算法的学习效果。原则上，通过减少分配给训练数据集的比例，我们会增加*偏差*误差成分。如果我们在保持原始样本数据集大小不变的情况下，增加训练数据集的规模，我们可能会导致算法过度拟合训练数据，这会导致当算法接收新数据时的预测效果较差，更不用说由于具体拆分策略的影响，某些高信息量样本可能会被排除在训练数据集之外。
- en: Furthermore, if the training dataset is characterized by high dimensionality,
    the similarities between the testing and training data might only be apparent,
    thus making the learning process more difficult.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果训练数据集具有高维度，那么测试数据与训练数据之间的相似性可能只是表面现象，从而使得学习过程变得更加困难。
- en: Therefore, the simple strategy of splitting the sample dataset according to
    fixed percentages is not always the best solution, especially when it comes to
    evaluating and fine-tuning the performance of algorithms.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，按照固定百分比拆分样本数据集的简单策略并不总是最优解，特别是在评估和微调算法性能时。
- en: An alternative solution is to use cross validation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代解决方案是使用交叉验证。
- en: Using cross validation for algorithms
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证进行算法训练
- en: The type of cross validation most commonly used is known as k-folds cross validation,
    and it involves randomly dividing the sample dataset into a number of folds, *k*
    corresponding to equal portions (if possible) of data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的交叉验证类型是k折交叉验证，它涉及将样本数据集随机分成若干个折叠，*k*对应于尽可能相等的数据部分。
- en: 'The learning process is performed in an iterative way, based on the different
    compositions of folds, used both as a training dataset and as a testing dataset.
    In this way, each fold is used in turn as a training dataset or as a testing dataset:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程是以迭代方式进行的，基于不同的折叠组成，既用作训练数据集，也用作测试数据集。这样，每个折叠轮流作为训练数据集或测试数据集：
- en: '![](img/75f8f430-1f26-4878-a2c8-caea432fc8f3.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75f8f430-1f26-4878-a2c8-caea432fc8f3.png)'
- en: '*(Image Credits: http://scikit-learn.org/)*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图片来源：http://scikit-learn.org/)*'
- en: In practice, the different folds (randomly generated) alternate in the role
    of training and testing datasets, and the iterative process ends when all the
    k-folds have been used both as training and testing datasets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，不同的折叠（随机生成的）轮流作为训练数据集和测试数据集，迭代过程在所有k个折叠都作为训练和测试数据集使用后结束。
- en: Since, at each iteration, the generalization error generated is different (as
    the algorithm is trained and tested with different datasets), we can calculate
    the average of these errors as a representative measure of the cross validation
    strategy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在每次迭代中产生的泛化误差是不同的（因为算法使用不同的数据集进行训练和测试），我们可以计算这些误差的平均值，作为交叉验证策略的代表性度量。
- en: K-folds cross validation pros and cons
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证的优缺点
- en: 'K-folds cross validation has several advantages, including the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证具有多个优点，包括以下几点：
- en: It enables all the available data to be used, both for training and testing
    goals
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使得所有可用的数据都可以用于训练和测试目的
- en: The specific composition of the individual folds is irrelevant since each fold
    is used at most once for training, and once for testing
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个折叠的具体组成是无关紧要的，因为每个折叠最多用于一次训练和一次测试
- en: We can increase the number of folds by increasing the *k* value, thereby increasing
    the size of the training dataset to reduce the *Bias* component of the generalization
    error
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过增加*k*值来增加折叠的数量，从而增加训练数据集的大小，以减少泛化误差中的*偏差*成分
- en: In terms of disadvantages, we must stress that k-folds cross validation considers
    the order that characterizes our original sample dataset to be irrelevant. In
    case the order of the data constitutes relevant information (as in the case of
    time series datasets), we will have to use a different strategy that takes into
    account the original sequence—perhaps by splitting the data in accordance with
    the oldest data—to be used as a training dataset, thereby reserving the most recent
    data for testing purposes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺点方面，我们必须强调，k折交叉验证认为原始样本数据集的顺序是无关的。如果数据的顺序包含重要信息（如时间序列数据集），我们将不得不使用另一种策略，考虑到原始顺序—可能通过按最旧数据拆分数据—用于训练数据集，从而将最新的数据保留用于测试。
- en: K-folds cross validation example
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证示例
- en: In the following example, we will use the k-folds cross validation implemented
    by the `scikit-learn` package, `sklearn.model_selection`. For simplicity, we will
    assign the value `2` to the variable `k`, thereby obtaining a 2-folds cross validation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用`scikit-learn`包中实现的k折交叉验证，`sklearn.model_selection`。为了简化，我们将变量`k`的值设为`2`，从而得到一个2折交叉验证。
- en: 'The sample dataset consists of just four samples. Therefore, each fold will
    contain two arrays to be used in turn, one for training and the other for testing.
    Finally, note how it is possible to associate the different folds with training
    and testing data, using the syntax provided by `numpy` indexing:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 样本数据集仅包含四个样本。因此，每个折叠将包含两个数组，交替使用，一个用于训练，另一个用于测试。最后，注意如何使用`numpy`索引语法将不同的折叠与训练数据和测试数据关联起来：
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have looked at the different techniques commonly adopted
    to evaluate the predictive performances of different algorithms. We looked at
    how to transform raw data into features, following feature engineering best practices,
    thereby allowing algorithms to use data that does not have a numeric form, such
    as categorical variables. We then focused on the techniques needed to correctly evaluate
    the various components (such as bias and variance) that constitute the generalization
    error associated with the algorithms, and finally, we learned how to perform the
    cross validation of the algorithms to improve the training process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了常用的不同技术，用于评估不同算法的预测性能。我们研究了如何将原始数据转化为特征，遵循特征工程的最佳实践，从而使算法能够使用没有数值形式的数据，如类别变量。接着，我们重点介绍了评估算法所涉及的各种组成部分（如偏差和方差）所需的技术，以正确评估算法的泛化误差，最后，我们学习了如何进行算法的交叉验证，以改进训练过程。
- en: In the next chapter, we will learn how to assess your AI arsenal.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何评估你的AI工具库。
