- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer
- en: Transformer models changed the playing field for most machine learning problems
    that involve sequential data. They have advanced the state of the art by a significant
    margin compared to the previous leaders, RNN-based models. One of the primary
    reasons that the Transformer model is so performant is that it has access to the
    whole sequence of items (e.g. sequence of tokens), as opposed to RNN-based models,
    which look at one item at a time. The term Transformer has come up several times
    in our conversations as a method that has outperformed other sequential models
    such as LSTMs and GRUs. Now, we will learn more about Transformer models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型改变了大多数涉及顺序数据的机器学习问题的游戏规则。与之前的 RNN 基模型相比，它们显著提高了技术水平。Transformer
    模型之所以如此高效的一个主要原因是，它能够访问整个序列（例如，令牌序列），而不像 RNN 基模型那样一次只查看一个元素。Transformer 这一术语在我们的讨论中多次出现，它是一种超越其他顺序模型（如
    LSTM 和 GRU）的方法。现在，我们将深入了解 Transformer 模型。
- en: In this chapter, we will first learn about the Transformer model in detail.
    Then we will discuss the details of a specific model from the Transformer family
    known as **Bidirectional Encoder Representations from Transformers** (**BERT**).
    We will see how we can use this model to complete a question-answering task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将首先详细学习 Transformer 模型。然后，我们将讨论 Transformer 家族中的一个特定模型，称为 **双向编码器表示模型（BERT）**。我们将了解如何使用该模型来完成问答任务。
- en: 'Specifically, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主要内容：
- en: Transformer architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: Understanding BERT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 BERT
- en: 'Use case: Using BERT to answer questions'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：使用 BERT 回答问题
- en: Transformer architecture
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: A Transformer is a type of Seq2Seq model (discussed in the previous chapter).
    Transformer models can work with both image and text data. The Transformer model
    takes in a sequence of inputs and maps that to a sequence of outputs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是一种 Seq2Seq 模型（在上一章中讨论过）。Transformer 模型可以处理图像和文本数据。Transformer 模型接受一系列输入并将其映射到一系列输出。
- en: 'The Transformer model was initially proposed in the paper *Attention is all
    you need* by Vaswani et al. ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    Just like a Seq2Seq model, the Transformer consists of an encoder and a decoder
    (*Figure 10.1*):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型最初在 Vaswani 等人提出的论文 *Attention is all you need* 中提出（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）。与
    Seq2Seq 模型类似，Transformer 包含一个编码器和一个解码器（*图 10.1*）：
- en: '![Intuition behind NMT](img/B14070_10_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![NMT 背后的直觉](img/B14070_10_01.png)'
- en: 'Figure 10.1: The encoder-decoder architecture'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：编码器-解码器架构
- en: Let’s understand how the Transformer model works using the previously studied
    Machine Translation task. The encoder takes in a sequence of source language tokens
    and produces a sequence of interim outputs. Then the decoder takes in a sequence
    of target language tokens and predicts the next token for each time step (the
    teacher forcing technique). Both the encoder and the decoder use attention mechanisms
    to improve performance. For example, the decoder uses attention to inspect all
    the past encoder states and previous decoder inputs. The attention mechanism is
    conceptually similar to Bahdanau attention, which we discussed in the last chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过之前学习过的机器翻译任务来理解 Transformer 模型是如何工作的。编码器接受一系列源语言的令牌并生成一系列中间输出。然后，解码器接受一系列目标语言的令牌并预测每个时间步的下一个令牌（教师强迫技术）。编码器和解码器都使用注意力机制来提高性能。例如，解码器使用注意力机制检查所有过去的编码器状态和先前的解码器输入。该注意力机制在概念上类似于我们在上一章中讨论过的
    Bahdanau 注意力。
- en: The encoder and the decoder
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器和解码器
- en: Now let’s discuss in detail what the encoder and the decoder consist of. They
    have more or less the same architecture with a few differences. Both the encoder
    and the decoder are designed to consume a sequence of input items at a time. But
    their goals during the task differ; the encoder produces a latent representation
    with the inputs, whereas the decoder produces a target output with the inputs
    and the encoder’s outputs. To perform these computations, these inputs are propagated
    through several stacked layers. Each layer within these models takes in a sequence
    of elements and outputs another sequence of elements. Each layer is also made
    from several sub-layers that encapsulate different computations performed on a
    sequence of input tokens to produce a sequence of outputs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细讨论编码器和解码器的组成部分。它们的架构大致相同，但也有一些差异。编码器和解码器都被设计为一次性处理一个输入序列。但它们在任务中的目标不同；编码器使用输入生成潜在表示，而解码器则使用输入和编码器的输出生成目标输出。为了执行这些计算，这些输入会通过多个堆叠的层进行传播。每一层都接收一个元素序列并输出另一个元素序列。每一层也由几个子层组成，这些子层对输入的令牌序列执行不同的计算，从而生成输出序列。
- en: 'A layer found in the Transformer mainly comprises the following two sub-layers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 中的一个层主要由以下两个子层组成：
- en: A self-attention layer
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自注意力层
- en: A fully connected layer
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: The self-attention layer produces its output using matrix multiplications and
    activation functions (this is similar to a fully connected layer, which we will
    discuss in a minute). The self-attention layer takes in a sequence of inputs and
    produces a sequence of outputs. However, a special characteristic of the self-attention
    layer is that, when producing an output at each time step, it has access to all
    the other inputs in that sequence [(](Chapter_10.xhtml)**Figure 10.2*). This makes
    learning and remembering long sequences of inputs trivial for this layer. For
    comparison, RNNs struggle to remember long sequences of inputs as they need to
    go through each input sequentially. Additionally, by design, the self-attention
    layer can select and combine different inputs at each time step based on the task
    it’s solving. This makes Transformers very powerful in sequential learning tasks.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层通过矩阵乘法和激活函数生成输出（这与我们稍后将讨论的全连接层类似）。自注意力层接收一系列输入并生成一系列输出。然而，自注意力层的一个特殊特点是，在每个时间步生成输出时，它可以访问该序列中的所有其他输入[(](Chapter_10.xhtml)**图
    10.2*）。这使得该层能够轻松学习和记住长序列的输入。相比之下，RNN 在记住长序列输入时会遇到困难，因为它们需要依次处理每个输入。此外，按设计，自注意力层可以根据所解决的任务，在每个时间步选择并组合不同的输入。这使得
    Transformer 在序列学习任务中非常强大。*
- en: '*Let’s discuss why it’s important to selectively combine different input elements
    this way. In an NLP context, the self-attention layer enables the model to peek
    at other words while processing a certain word. This means that while the encoder
    is processing the word *it* in the sentence *I kicked the ball and it disappeared*,
    the model can attend to the word *ball*. By doing this, the Transformer can learn
    dependencies and disambiguate words, which leads to better language understanding.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们讨论一下为什么以这种方式选择性地组合不同的输入元素很重要。在 NLP 领域，自注意力层使得模型在处理某个词时能够“窥视”其他词。这意味着，当编码器在处理句子
    *I kicked the ball and it disappeared* 中的词 *it* 时，模型可以关注词 *ball*。通过这种方式，Transformer
    能够学习依赖关系并消除歧义，从而提升语言理解能力。'
- en: 'We can even understand how self-attention helps us to solve a task conveniently
    through a real-world example. Assume you are playing a game with two other people:
    person A and person B. Person A holds a question written on a board, and you need
    to answer that question. Say person A reveals one word of the question at a time,
    and after the last word of the question is revealed, you answer it. For long and
    complex questions, this would be challenging as you cannot physically see the
    complete question and would have to heavily rely on memory. This is what it feels
    like to perform computations without self-attention for a Transformer. On the
    other hand, say person B reveals the full question on the board in one go instead
    of word by word. Now it is much easier to answer the question as you can see the
    complete question at once. If the question is a complex question requiring a complex
    answer, you can look at different parts of the question as you are providing various
    sections of the full answer. This is what the self-attention layer enables.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以通过一个现实世界的例子理解自注意力如何帮助我们方便地解决任务。假设你正在和另外两个人玩一个游戏：A 人和 B 人。A 人手里有一个写在板子上的问题，而你需要回答这个问题。假设
    A 人每次揭示一个问题的单词，在问题的最后一个单词揭示出来后，你才回答它。对于长且复杂的问题，这会变得具有挑战性，因为你无法在物理上看到完整的问题，必须依赖记忆。这就是没有自注意力的
    Transformer 执行计算时的感觉。另一方面，假设 B 人一次性将完整的问题揭示在板子上，而不是一个个字地揭示。现在，你可以一次性看到完整的问题，因此回答问题变得容易得多。如果问题很复杂，需要复杂的答案，你可以在给出不同部分的答案时查看问题的不同部分。这就是自注意力层的作用。
- en: The self-attention layer is followed by a fully connected layer. A fully connected
    layer has all the input nodes connected to all the output nodes, optionally followed
    by a non-linear activation function. It takes the output elements produced by
    the self-attention sub-layer and produces a hidden representation for each output
    element. Unlike the self-attention layer, the fully connected layer treats individual
    sequence items independently, performing computations on them in an element-wise
    fashion.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层后跟一个全连接层。全连接层将所有输入节点与所有输出节点连接，通常后面跟着一个非线性激活函数。它将自注意力子层产生的输出元素作为输入，生成每个输出元素的隐藏表示。与自注意力层不同，全连接层独立地处理每个序列项，按逐元素的方式进行计算。
- en: 'They introduce non-linear transformations while making the model deeper, thus
    allowing the model to perform better:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在使模型更深的同时引入了非线性变换，从而使模型能够更好地执行任务：
- en: '![](img/B14070_10_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_02.png)'
- en: 'Figure 10.2: The difference between the self-attention sub-layer and the fully
    connected sub-layer. The self-attention sub-layer looks at all the inputs in the
    sequence, whereas the fully-connected sub-layer only looks at the input that is
    processed.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：自注意力子层与全连接子层的区别。自注意力子层查看序列中的所有输入，而全连接子层只查看正在处理的输入。
- en: Now that we understand the basic building blocks of a Transformer layer, let’s
    look at the encoder and the decoder separately. Before diving in, let’s establish
    some basics. The encoder takes in an input sequence and the decoder takes in an
    input sequence as well (a different sequence to the encoder input). Then the decoder
    produces an output sequence. Let’s call a single item in these sequences a *token*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了 Transformer 层的基本构建块，接下来我们将分别看看编码器和解码器。在深入之前，我们先建立一些基础知识。编码器接收一个输入序列，解码器也接收一个输入序列（与编码器输入的序列不同）。然后解码器产生一个输出序列。我们将这些序列中的每个单项称为
    *token*。
- en: 'The encoder consists of a stack of layers, where each layer consists of two
    sub-layers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由一堆层组成，每一层由两个子层构成：
- en: A self-attention layer – Generates a latent representation for each encoder
    input token in the sequence. For each input token, this layer looks at the whole
    sequence and selects other tokens in the sequence that enrich the semantics of
    the generated hidden output for that token (that is, ‘attended’ representation).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力层 – 为序列中的每个编码器输入标记生成潜在表示。对于每个输入标记，该层查看整个序列并选择序列中的其他标记，丰富该标记的隐藏输出（即“注意到的”表示）的语义。
- en: A fully-connected layer – Generates an element-wise deeper hidden representation
    of the attended representation
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层 – 生成注意到的表示的逐元素更深隐藏表示。
- en: 'The decoder layer consists of three sub-layers:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器层由三个子层组成：
- en: A masked self-attention layer – For each decoder input, a token looks at all
    the tokens to the left of it. The decoder needs to mask words to the right to
    prevent the model from seeing words in the future. Having access to successive
    words during prediction can make the prediction task trivial for the decoder.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩蔽自注意力层 – 对于每个解码器输入，一个令牌会查看它左侧的所有令牌。解码器需要掩蔽右侧的词语，以防止模型看到未来的词语。在预测过程中，如果能够访问到后续的词语，解码器的预测任务可能会变得非常简单。
- en: An attention layer – For each input token in the decoder, it looks at both the
    encoder’s outputs and the decoder’s masked attended output to generate a semantically
    rich hidden output. Since this layer is not only focused on decoder inputs, we’ll
    call this an attention layer.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力层 – 对于解码器中的每个输入令牌，它会查看编码器的输出和解码器的掩蔽关注输出，以生成语义丰富的隐藏输出。由于该层不仅关注解码器输入，我们将其称为注意力层。
- en: A fully-connected layer – Generates an element-wise hidden representation of
    the attended representation of the decoder.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层 – 生成解码器关注表示的逐元素隐藏表示。
- en: 'This is shown in *Figure 10.3*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.3*所示：
- en: '![](img/B14070_10_03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_03.png)'
- en: 'Figure 10.3: How a Transformer model is used to translate an English sentence
    to French. The diagram shows various layers in the encoder and the decoder, and
    various connections formed within the encoder, within the decoder, and between
    the encoder and the decoder. The squares represent the inputs and outputs of the
    models. The rectangular shaded boxes represent interim outputs of the sub-layers.
    The <sos> token represents the beginning of the decoder’s input.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：Transformer模型如何用于将英文句子翻译成法语。该图展示了编码器和解码器中的各个层，以及编码器内部、解码器内部和编码器与解码器之间的各种连接。方框表示模型的输入和输出。矩形阴影框表示子层的临时输出。`<sos>`符号表示解码器输入的开始。
- en: Next, let’s learn about the computational mechanics of the self-attention layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习自注意力层的计算机制。
- en: Computing the output of the self-attention layer
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算自注意力层的输出
- en: 'There is no doubt that the self-attention layer is at the center of the Transformer.
    The computations that govern the self-attention mechanism can be difficult to
    understand. Therefore, this section is dedicated to understanding the self-attention
    technique in detail. There are three key concepts to understand: query, key, and
    value. The query and the key are used to generate an affinity matrix. For the
    decoder’s attention layer, the affinity matrix’s position *i,j* represents how
    similar the encoder state (key) *i* is to the decoder input *j* (query). Then,
    we create a weighted average of encoder states (value) for each position, where
    the weights are given by the affinity matrix.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，自注意力层是Transformer的核心。支配自注意力机制的计算可能比较难以理解。因此，本节将详细解释自注意力技术。要理解的三个关键概念是：查询、键和值。查询和键用于生成亲和力矩阵。对于解码器的注意力层，亲和力矩阵中的位置
    *i,j* 表示编码器状态（键）*i* 与解码器输入（查询）*j* 之间的相似度。接着，我们为每个位置创建一个加权平均的编码器状态（值），权重由亲和力矩阵给出。
- en: 'To reinforce our understanding, let’s imagine a scenario where the decoder
    is generating a self-attention output. Say we have an English to French machine
    translation task. Take the example sentence *Dogs are great*, which becomes *Les
    chiens sont super* in French. Say we are at time step 2, trying to produce the
    word *chiens*. Let’s represent each word with a single floating point number (such
    as a simplified embedding representation of words):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加深我们的理解，让我们假设一个情境，解码器正在生成自注意力输出。假设我们有一个英法机器翻译任务。以句子*Dogs are great*为例，翻译成法语就是*Les
    chiens sont super*。假设我们处在第2时间步，尝试生成单词*chiens*。我们将每个单词用一个浮动点数表示（例如，简化版的单词嵌入表示）：
- en: '*Dogs -> 0.8*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dogs -> 0.8*'
- en: '*are -> 0.3*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*are -> 0.3*'
- en: '*great -> -0.2*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*great -> -0.2*'
- en: '*chiens -> 0.5*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*chiens -> 0.5*'
- en: 'Now let’s compute the affinity matrix (to be specific, the affinity vector
    since we are only considering a single decoder input). The query would be 0.5
    and the key (i.e. the encoder state sequence) would be `[0.8, 0.3, -0.2]`. If
    we take the dot product, we have:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算亲和力矩阵（具体来说，是亲和力向量，因为我们只考虑单个解码器输入）。查询值为0.5，键（即编码器状态序列）为`[0.8, 0.3, -0.2]`。如果我们进行点积运算，结果为：
- en: '`[0.4, 0.15, -0.1]`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`[0.4, 0.15, -0.1]`'
- en: 'Let’s understand what this affinity matrix is saying. With respect to the word
    *chiens*, the word *Dogs* has the highest similarity, and the word *are* also
    has a positive similarity (since *chiens* is plural, carrying a reference to the
    word *are* in English). However, the word *great* has a negative similarity to
    the word *chiens*. Then, we can compute the final attended output for that time
    step as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下这个亲和矩阵所表达的含义。相对于单词*chiens*，单词*Dogs*具有最高的相似度，单词*are*也有较高的相似度（因为*chiens*是复数，指代的是英文中的*are*）。然而，单词*great*与单词*chiens*的相似度是负值。然后，我们可以计算该时间步的最终注意力输出，计算方式如下：
- en: '`[0.4 * 0.8, 0.15 * 0.3, -0.1 * -0.2] = [0.32 + 0.45 + 0.02] = 0.385`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`[0.4 * 0.8, 0.15 * 0.3, -0.1 * -0.2] = [0.32 + 0.45 + 0.02] = 0.385`'
- en: We have ended up with a final output somewhere in the middle of matching words
    from the English language, where the word *great* has the highest distance. This
    example was presented to show how the query, key, and value come into play to
    compute the final attended output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到的输出位于英语单词匹配的一部分，其中单词*great*的距离最大。这个例子展示了查询、键和值是如何发挥作用的，以计算最终的注意力输出。
- en: 'Now let’s look at the actual computation that transpires in the layer. To compute
    the query, key, and value, we use a linear projection of the actual inputs provided
    using weight matrices. The three weight matrices are:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下实际在该层中发生的计算。为了计算查询、键和值，我们使用权重矩阵对实际输入进行线性投影。三个权重矩阵是：
- en: Query weights matrix (![](img/B14070_10_001.png))
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询权重矩阵 (![](img/B14070_10_001.png))
- en: Key weights matrix (![](img/B14070_10_002.png))
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键权重矩阵 (![](img/B14070_10_002.png))
- en: Value weights matrix (![](img/B14070_10_003.png))
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值权重矩阵 (![](img/B14070_10_003.png))
- en: 'Each of these weight matrices produces three outputs for a given token (at
    position ![](img/B14070_10_004.png)) in a given input sequence by multiplying
    with the weight matrix as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个权重矩阵通过与权重矩阵相乘，为给定输入序列中某个位置的标记（位置 ![](img/B14070_10_004.png)）产生三个输出，计算方式如下：
- en: '![](img/B14070_10_005.png), ![](img/B14070_10_006.png), and ![](img/B14070_10_007.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_005.png), ![](img/B14070_10_006.png), 和 ![](img/B14070_10_007.png)'
- en: '*Q*, *K*, and *V* are *[B, T, d]* sized tensors, where *B* is the batch size,
    *T* is the number of time steps, and *d* is a hyperparameter that defines the
    dimensionality of the latent representation. These are then used to compute the
    affinity matrix, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*，*K*，和*V*是大小为*[B, T, d]*的张量，其中*B*是批量大小，*T*是时间步数，*d*是一个超参数，用于定义潜在表示的维度。这些张量随后用于计算亲和矩阵，计算方式如下：'
- en: '![](img/B14070_10_04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_04.png)'
- en: 'Figure 10.4: The computations in the self-attention layer. The self-attention
    layer starts with an input sequence and computes sequences of query, key, and
    value vectors. Then the queries and keys are converted to a probability matrix,
    which is used to compute a weighted sum of values'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：自注意力层中的计算过程。自注意力层从输入序列开始，计算查询、键和值向量序列。然后，将查询和键转换为概率矩阵，该矩阵用于计算值的加权和。
- en: 'The affinity matrix *P* is computed as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和矩阵*P*的计算方式如下：
- en: '![](img/B14070_10_008.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_008.png)'
- en: 'Then the final attended output of the self-attention layer is computed as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，计算自注意力层的最终注意力输出，计算方式如下：
- en: '![](img/B14070_10_009.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_009.png)'
- en: Here, *Q* represents the queries tensor, *K* represents the keys tensor, and
    *V* represents the values tensor. This is what makes Transformer models so powerful;
    unlike LSTM models, Transformer models aggregate all tokens in a sequence to a
    single matrix multiplication, making these models highly parallelizable. *Figure
    10.4* also depicts the computations that take place within the self-attention
    layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Q*表示查询张量，*K*表示键张量，*V*表示值张量。这就是Transformer模型如此强大的原因；与LSTM模型不同，Transformer模型将序列中的所有标记聚合成一个矩阵乘法，使这些模型具有很高的并行性。*图10.4*还展示了自注意力层内发生的计算过程。
- en: Embedding layers in the Transformer
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer中的嵌入层
- en: Word embeddings provide a semantic-preserving representation of words based
    on the context in which words are used. In other words, if two words are used
    in the same context, they will have similar word vectors. For example, the words
    *cat* and *dog* will have similar representations, whereas *cat* and *volcano*
    will have vastly different representations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入提供了一种语义保留的词语表示，基于词语使用的上下文。换句话说，如果两个词语在相同的上下文中使用，它们将具有相似的词向量。例如，*cat*和*dog*将具有相似的表示，而*cat*和*volcano*将具有截然不同的表示。
- en: 'Word vectors were initially introduced in the paper titled *Efficient Estimation
    of Word Representations in Vector Space* by Mikolov et al. ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)).
    It came in two variants: skip-gram and continuous bag-of-words. Embeddings work
    by first defining a large matrix of size *V* x *E*, where *V* is the size of the
    vocabulary and *E* is the size of the embeddings. *E* is a user-defined hyperparameter;
    a larger *E* typically leads to more powerful word embeddings. In practice, you
    do not need to increase the size of embeddings beyond 300.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量最早在Mikolov等人发表的论文*Efficient Estimation of Word Representations in Vector
    Space*中被提出（[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)）。它有两种变体：skip-gram和连续词袋（CBOW）。嵌入通过首先定义一个大小为*V*
    x *E*的大矩阵来工作，其中*V*是词汇表的大小，*E*是嵌入的大小。*E*是用户定义的超参数；较大的*E*通常会导致更强大的词嵌入。实际上，你不需要将嵌入的大小增大到超过300。
- en: 'Motivated by the original word vector algorithms, modern deep learning models
    use embedding layers to represent words/tokens. The following general approach
    (along with pre-training later to fine-tune these embeddings) is taken to incorporate
    word embeddings into a machine learning model:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 受原始词向量算法的启发，现代深度学习模型使用嵌入层来表示词语/标记。以下通用方法（以及后续的预训练以微调这些嵌入）用于将词嵌入整合到机器学习模型中：
- en: Define a randomly initialized word embedding matrix (or pre-trained embeddings,
    available to download for free)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个随机初始化的词嵌入矩阵（或预训练的嵌入，可以免费下载）
- en: Define the model (randomly initialized) that uses word embeddings as the inputs
    and produces an output (for example, sentiment, or a language translation)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义使用词嵌入作为输入并产生输出的模型（例如情感分析或语言翻译）
- en: Train the whole model (embeddings and the model) end-to-end on the task
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对整个模型（嵌入和模型）进行端到端训练，完成任务
- en: 'The same technique is used in Transformer models. However, in Transformer models,
    there are two different embeddings:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer模型中使用相同的技术。然而，在Transformer模型中，有两种不同的嵌入：
- en: Token embeddings (provide a unique representation for each token seen by the
    model in an input sequence)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记嵌入（为模型在输入序列中看到的每个标记提供唯一表示）
- en: Positional embeddings (provide a unique representation for each position in
    the input sequence)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入（为输入序列中的每个位置提供唯一表示）
- en: The token embeddings have a unique embedding vector for each token (such as
    character, word, and sub-word), depending on the model’s tokenizing mechanism.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 标记嵌入为每个标记（如字符、词语和子词）提供一个唯一的嵌入向量，这取决于模型的标记化机制
- en: 'The positional embeddings are used to signal the model where a token is appearing.
    The primary purpose of the positional embeddings server is to inform the Transformer
    model where a word is appearing. This is because, unlike LSTMs/GRUs, Transformer
    models don’t have a notion of sequence, as it processes the whole text in one
    go. Furthermore, a change to the position of a word can alter the meaning of a
    sentence/or a word. For example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入用于指示模型一个标记出现的位置。位置嵌入的主要作用是告诉Transformer模型一个词语出现的位置。这是因为，与LSTM/GRU不同，Transformer模型没有序列的概念，它一次性处理整个文本。此外，改变词语的位置可能会改变句子的含义/词义。例如：
- en: '*Ralph loves his tennis ball.* **It** *likes to chase the ball*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ralph loves his tennis ball.* **It** *likes to chase the ball*'
- en: '*Ralph loves his tennis ball. Ralph likes to chase* **it**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ralph loves his tennis ball. Ralph likes to chase* **it**'
- en: 'In the sentences above, the word *it* refers to different things and the position
    of the word *it* can be used as a cue to identify this difference. The original
    Transformer paper uses the following equations to generate positional embeddings:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述句子中，*it*一词指代不同的事物，*it*的位置可以作为线索来识别这种差异。原始的Transformer论文使用以下方程来生成位置嵌入：
- en: '![](img/B14070_10_010.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_010.png)'
- en: '![](img/B14070_10_011.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_011.png)'
- en: where *pos* denotes the position in the sequence and ![](img/B14070_10_012.png)
    denotes the ![](img/B14070_10_013.png) feature dimension (![](img/B14070_10_014.png)).
    Even-numbered features use a sine function and odd numbered features use a cosine
    function. *Figure 10.5* presents how positional embeddings change as the time
    step and the feature position change. It can be seen that feature positions with
    higher indices have lower-frequency sinusoidal waves. It is not entirely clear
    how the authors came up with the exact equation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*pos* 表示序列中的位置，![](img/B14070_10_012.png) 表示 ![](img/B14070_10_013.png) 特征维度（![](img/B14070_10_014.png)）。偶数编号的特征使用正弦函数，奇数编号的特征使用余弦函数。*图
    10.5* 展示了随着时间步和特征位置的变化，位置嵌入是如何变化的。可以看到，特征位置索引较高的位置具有较低频率的正弦波。尚不完全清楚作者是如何得出该精确方程的。
- en: However, they do mention that they did not see a significant performance difference
    between the above equation and letting the model learn positional embeddings jointly
    during the training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他们确实提到，尽管使用上述方程与在训练过程中让模型联合学习位置嵌入之间没有明显的性能差异。
- en: '![](img/B14070_10_05.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_05.png)'
- en: 'Figure 10.5: How positional embeddings change with the time step and the feature
    position. Even-numbered feature positions use the sine function and odd-numbered
    positions use the cosine function. Additionally, the frequency of the signals
    decreases as the feature position increases'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：随着时间步和特征位置的变化，位置嵌入是如何变化的。偶数编号的特征位置使用正弦函数，奇数编号的位置使用余弦函数。此外，随着特征位置的增加，信号的频率降低。
- en: 'It is important to note that both token and positional embeddings will have
    the same dimensionality ![](img/B14070_10_015.png), making it possible to perform
    element-wise addition. Finally, as the input to the model, the token embeddings
    and the positional embeddings are summed to form a single hybrid embedding vector
    (*Figure 10.6*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，token 嵌入和位置嵌入将具有相同的维度 ![](img/B14070_10_015.png)，这使得逐元素相加成为可能。最后，作为模型的输入，token
    嵌入和位置嵌入相加，形成一个单一的混合嵌入向量（*图 10.6*）：
- en: '![](img/B14070_10_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_06.png)'
- en: 'Figure 10.6: The embeddings generated in a Transformer model and how the final
    embeddings are computed'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：Transformer 模型中生成的嵌入以及最终嵌入是如何计算的
- en: 'Let’s now discuss two optimization techniques used in each layer of the Transformer:
    residual connections and layer normalizations.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论 Transformer 中每一层使用的两种优化技术：残差连接和层归一化。
- en: Residuals and normalization
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差与归一化
- en: Another important characteristic of the Transformer models is the existence
    of the residual connections and the normalization layers in between the individual
    layers of the Transformer model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型的另一个重要特性是，模型中各层之间存在残差连接和归一化层。
- en: Residual connections are formed by adding a given layer’s output to the output
    of one or more layers ahead. This in turn forms shortcut connections through the
    model and provides a stronger gradient flow by reducing the changes of the phenomenon
    known as vanishing gradients (*Figure 10.7*). The vanishing gradients problem
    causes the gradients in the layers closest to the inputs to be very small so that
    the training in those layers is hindered. The residual connections for deep learning
    models were popularized by the paper “*Deep Residual Learning for Image Recognition*”
    by Kaiming He et.al. ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf))
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接通过将给定层的输出加到一个或多个前面层的输出上形成。这反过来通过模型形成快捷连接，并通过减少梯度消失现象的发生来提供更强的梯度流（*图 10.7*）。梯度消失问题导致最接近输入层的梯度非常小，从而妨碍了这些层的训练。残差连接在深度学习模型中的应用，由
    Kaiming He 等人在论文“*Deep Residual Learning for Image Recognition*”中推广（[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)）
- en: '![](img/B14070_10_07.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_07.png)'
- en: 'Figure 10.7: How residual connections work'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：残差连接的工作原理
- en: 'In Transformer models, in each layer, residual connections are created the
    following way:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 模型中，每一层的残差连接是通过以下方式创建的：
- en: Input to the self-attention sub-layer is added to the output of the self-attention
    sub-layer.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入自注意力子层的输入会加到自注意力子层的输出上。
- en: Input to the fully-connected sub-layer is added to the output of the fully-connected
    sub-layer.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入全连接子层的输入会加到全连接子层的输出上。
- en: Next, the output reinforced by residual connections goes through a layer normalization
    layer. Layer normalization, similar to batch normalization, is a way to reduce
    the covariate shift in neural networks, allowing them to be trained faster and
    achieve better performance. Covariate shift refers to changes in the distribution
    of neural network activations (caused by changes in the data distribution), which
    transpires as the model goes through model training. These changes in the distribution
    damage consistency during model training and negatively impact the model. It was
    introduced in the paper *Layer Normalization* by Ba et al. ([https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，经过残差连接强化的输出通过一个层归一化层。层归一化类似于批量归一化，它是一种减少神经网络中协变量偏移的方法，使得神经网络能够更快地训练并取得更好的性能。协变量偏移是指神经网络激活值的分布变化（由于数据分布变化引起的），这种变化会在模型训练过程中发生。这些分布的变化破坏了训练过程中的一致性，并对模型产生负面影响。该方法在Ba等人发表的论文*Layer
    Normalization*中被提出([https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf))。
- en: Batch normalization computes the mean and variance of activations as an average
    over the samples in the batch, causing its performance to rely on mini-batches
    used to train the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化通过计算激活值的均值和方差，并以批次样本的平均值为基础，从而使其性能依赖于训练模型时使用的迷你批次。
- en: However, layer normalization computes the mean and variance (that is, the normalization
    terms) of the activations in such a way that the normalization terms are the same
    for every hidden unit. In other words, layer normalization has a single mean and
    a variance value for all the hidden units in a layer. This is in contrast to batch
    normalization, which maintains individual mean and variance values for each hidden
    unit in a layer. Moreover, unlike batch normalization, layer normalization does
    not average over the samples in the batch; instead, it leaves the averaging out
    and has different normalization terms for different inputs. By having a mean and
    variance per-sample, layer normalization gets rid of the dependency on the mini-batch
    size. For more details about this method, please refer to the original paper by
    Ba et al.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，层归一化以一种方式计算激活值的均值和方差（即归一化项），使得每个隐藏单元的归一化项相同。换句话说，层归一化对层中的所有隐藏单元有一个共同的均值和方差值。这与批量归一化形成对比，后者为每个隐藏单元维持单独的均值和方差值。此外，不同于批量归一化，层归一化不会对批次中的样本求均值；相反，它跳过了平均值计算，并为不同的输入提供不同的归一化项。通过为每个样本单独计算均值和方差，层归一化摆脱了对迷你批次大小的依赖。如需了解该方法的更多细节，请参阅Ba等人发表的原始论文。
- en: TensorFlow provides a convenient implementation of the layer normalization algorithm
    at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization).
    You can simply use this layer with any model you define using TensorFlow Keras
    APIs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow在[https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization)提供了方便的层归一化算法实现。你可以简单地在使用TensorFlow
    Keras API定义的任何模型中使用该层。
- en: '*Figure 10.8* depicts how residual connections and layer normalization are
    used in Transformer models:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.8* 展示了残差连接和层归一化如何在Transformer模型中使用：'
- en: '![](img/B14070_10_08.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_08.png)'
- en: 'Figure 10.8: How residual connections and layer normalization layers are used
    in the transformer model'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：残差连接和层归一化层在Transformer模型中的使用方式
- en: With that, we end our discussion on the components of the Transformer model.
    We have discussed all the bells and whistles of the Transformer model. The Transformer
    model is an encoder-decoder based model. Both the encoder and the decoder have
    the same structure, apart from a few small differences. The Transformer uses self-attention,
    a powerful parallelizable attention mechanism to attend to other inputs at every
    time step. The Transformer also uses several embedding layers, such as token embeddings
    and positional embeddings, to inject information about tokens and their positioning.
    The Transformer also uses residual connections and layer normalization to improve
    the performance of the model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们结束了对Transformer模型组件的讨论。我们已经讨论了Transformer模型的所有关键组件。Transformer模型是一个基于编码器-解码器的模型。编码器和解码器具有相同的结构，除了少数几个小差异。Transformer使用自注意力机制，这是一种强大的并行化注意力机制，用于在每个时间步骤关注其他输入。Transformer还使用多个嵌入层，例如词汇嵌入和位置嵌入，以注入有关词汇和其位置的信息。Transformer还使用残差连接和层归一化，以提高模型的性能。
- en: Next, we will discuss a specific Transformer model known as BERT, which we’ll
    be using to solve a question-answering problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一个特定的 Transformer 模型，称为 BERT，我们将使用它来解决一个问答问题。
- en: Understanding BERT
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 BERT
- en: '**BERT** (**Bidirectional Encoder Representation from Transformers**) is a
    Transformer model among a plethora of Transformer models that have come to light
    over the past few years.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**（**来自 Transformer 的双向编码器表示**）是近年来众多 Transformer 模型中的一个。'
- en: 'BERT was introduced in the paper *BERT: Pre-training of Deep Bidirectional
    Transformers for Language Understanding* by Delvin et al. ([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)).
    The Transformer models are divided into two main factions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'BERT 在 Delvin 等人发表的论文 *BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding* 中提出（[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)）。Transformer
    模型分为两大类：'
- en: Encoder-based models
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于编码器的模型
- en: Decoder-based (autoregressive) models
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于解码器（自回归）模型
- en: In other words, either the encoder or the decoder part of the Transformer provides
    the foundation for these models, compared to using both the encoder and the decoder.
    The main difference between the two is how attention is used. Encoder-based models
    use bidirectional attention, whereas decoder-based models use autoregressive (that
    is, left to right) attention.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，与使用 Transformer 的编码器和解码器相比，Transformer 的编码器或解码器部分为这些模型提供了基础。两者之间的主要区别在于注意力机制的使用方式。基于编码器的模型使用双向注意力，而基于解码器的模型使用自回归（即从左到右）注意力。
- en: 'BERT is an encoder-based Transformer model. It takes an input sequence (a collection
    of tokens) and produces an encoded output sequence. *Figure 10.9* depicts the
    high-level architecture of BERT :'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个基于编码器的 Transformer 模型。它接收一个输入序列（一组标记）并生成一个编码的输出序列。*图 10.9* 描述了 BERT
    的高层次架构：
- en: '![](img/B14070_10_09.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_09.png)'
- en: 'Figure 10.9: The high-level architecture of BERT. It takes a set of input tokens
    and produces a sequence of hidden representations generated using several hidden
    layers'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：BERT 的高层次架构。它接收一组输入标记并生成通过多个隐藏层生成的隐藏表示序列。
- en: Now let’s discuss a few details pertinent to BERT, such as inputs consumed by
    BERT and the tasks it is designed to solve.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一些与 BERT 相关的细节，比如 BERT 消耗的输入和它设计用来解决的任务。
- en: Input processing for BERT
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 的输入处理
- en: When BERT takes an input, it inserts some special tokens into the input. First,
    at the beginning, it inserts a `[CLS]` (an abbreviated form of the term classification)
    token that is used to generate the final hidden representation for certain types
    of tasks (such as sequence classification). It represents the output after attending
    to all the tokens in the sequence. Next, it also inserts a `[SEP]` (meaning ‘separation’)
    token depending on the type of input. The `[SEP]` token marks the end and beginning
    of different sequences in the input. For example, in question-answering, the model
    takes a question and a context (such as a paragraph) that may have the answer
    as an input, and `[SEP]` is used in between the question and the context. Additionally,
    we have the `[PAD]` token, which can be used to pad short sequences to a required
    length.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当 BERT 接收输入时，它会在输入中插入一些特殊的标记。首先，在开头，它插入一个 `[CLS]`（分类的缩写）标记，用于生成某些任务（如序列分类）的最终隐藏表示。它代表在处理序列中所有标记后的输出。接下来，根据输入类型，它还会插入一个
    `[SEP]`（意为“分隔”）标记。`[SEP]` 标记用于标记输入中不同序列的开始和结束。例如，在问答中，模型将问题和可能包含答案的上下文（如段落）作为输入，`[SEP]`
    用于问题和上下文之间。此外，还有 `[PAD]` 标记，可用于将短序列填充至所需长度。
- en: The `[CLS]` token is appended to any input sequence fed to BERT. This denotes
    the beginning of the input. It also forms the basis for the input fed into the
    classification head used on top of BERT to solve your NLP task. As you know, BERT
    produces a hidden representation for each input token in the sequence. As a convention,
    the hidden representation corresponding to the `[CLS]` token is used as the input
    to the classification model that sits on top of BERT.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`[CLS]` 标记会附加到输入的每个序列中，表示输入的开始。它也是输入到 BERT 上层分类头的基础，用于解决您的 NLP 任务。如您所知，BERT
    会为序列中的每个输入标记生成隐藏表示。根据惯例，`[CLS]` 标记对应的隐藏表示将作为输入，传递给位于 BERT 之上的分类模型。'
- en: Next, the final embedding of the tokens is generated using three different embedding
    spaces. The token embedding has a unique vector for each token in the vocabulary.
    The positional embeddings encode the position of each token, as discussed earlier.
    Finally, the segment embedding provides a distinct representation for each sub-component
    in the input, when the input consists of multiple components. For example, in
    question-answering, the question will have a unique vector as its segment embedding
    vector and the context will have a different embedding vector. This is done by
    having ![](img/B14070_10_016.png) embedding vectors for the ![](img/B14070_10_017.png)
    different components in the input sequence. Depending on the component index specified
    for each token in the input, the corresponding segment embedding vector is retrieved.
    ![](img/B14070_10_018.png) needs to be specified in advance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用三种不同的嵌入空间生成最终的令牌嵌入。每个词汇表中的令牌都有一个独特的向量表示。位置嵌入编码了每个令牌的位置，如前所述。最后，段落嵌入为输入中的每个子组件提供了一个独特的表示，当输入由多个组件组成时。例如，在问答任务中，问题将拥有一个独特的向量作为其段落嵌入向量，而上下文将具有不同的嵌入向量。这是通过为输入序列中的不同组件提供![](img/B14070_10_016.png)嵌入向量来实现的。根据输入中每个令牌指定的组件索引，检索相应的段落嵌入向量。需要提前指定![](img/B14070_10_018.png)。
- en: Tasks solved by BERT
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 解决的任务
- en: 'The task-specific NLP tasks solved by BERT can be classified into four different
    categories. These are motivated by the tasks found in the **General Language Understanding
    Evaluation** (**GLUE**) benchmark task suite ([https://gluebenchmark.com](https://gluebenchmark.com)):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 解决的特定任务可以分为四个不同的类别。这些类别受 **通用语言理解评估** (**GLUE**) 基准任务套件的启发（[https://gluebenchmark.com](https://gluebenchmark.com)）：
- en: Sequence classification – Here, a single input sequence is given and the model
    is asked to predict a label for the whole sequence (for example, sentiment analysis
    or spam identification).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列分类 – 在这里，给定一个输入序列，模型被要求为整个序列预测一个标签（例如，情感分析或垃圾邮件识别）。
- en: Token classification – Here, a single input sequence is given and the model
    is asked to predict a label for each token in the sequence (for example, named
    entity recognition or part-of-speech tagging).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令牌分类 – 在这里，给定一个输入序列，模型被要求为序列中的每个令牌预测一个标签（例如，命名实体识别或词性标注）。
- en: 'Question-answering – Here, the input consists of two sequences: a question
    and a context. The question and the context are separated by a `[SEP]` token.
    The model is trained to predict the starting and ending indices of the span of
    tokens belonging to the answer.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答任务 – 在这里，输入由两个序列组成：一个问题和一个上下文。问题和上下文由一个 `[SEP]` 令牌分隔。模型被训练以预测答案所属的令牌跨度的起始和结束索引。
- en: Multiple choice – Here, the input consists of multiple sequences; a question
    followed by multiple candidates that may or may not be the answer to the question.
    These multiple sequences are separated by the token `[SEP]` and provided as a
    single input sequence to the model. The model is trained to predict the correct
    answer (that is, the class label) for that question.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项选择 – 在这里，输入由多个序列组成；一个问题后面跟着多个候选答案，这些候选答案可能是也可能不是问题的答案。这些多个序列由令牌 `[SEP]` 分隔，并作为一个单一的输入序列提供给模型。模型被训练以预测该问题的正确答案（即，类别标签）。
- en: '*Figure 10.10* depicts how BERT is used to solve these different tasks:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.10* 描述了 BERT 如何用于解决这些不同的任务：'
- en: '![](img/B14070_10_10.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_10.png)'
- en: 'Figure 10.10: How BERT is used for different NLP tasks'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：BERT 如何用于不同的 NLP 任务
- en: BERT is designed in such a way that it can be used to complete these tasks without
    any modifications to the base model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的设计使其能够在不修改基础模型的情况下完成这些任务。
- en: 'In tasks that involve multiple sequences (such as multiple-choice questions),
    you need the model to tell different inputs belonging to different segments apart
    (that is, which tokens are the question and which tokens are the context in a
    question-answering task). In order to make that distinction, the `[SEP]` token
    is used. A `[SEP]` token is inserted between the different sequences. For example,
    if you are solving a question-answering problem, you might have the following
    input:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及多个序列的任务中（例如多项选择题），你需要模型区分属于不同段落的不同输入（即，在问答任务中，哪些令牌是问题，哪些令牌是上下文）。为了做出这个区分，使用了
    `[SEP]` 令牌。一个 `[SEP]` 令牌插入在不同序列之间。例如，如果你正在解决一个问答问题，输入可能如下所示：
- en: '*Question: What color is the ball?*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题：球的颜色是什么？*'
- en: '*Paragraph: Tippy is a dog. She loves to play with her red ball.*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*段落：Tippy 是一只狗。她喜欢玩她的红色球。*'
- en: 'Then the input to BERT might look like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输入到 BERT 的内容可能如下所示：
- en: '`[CLS]` *What color is the ball* `[SEP]` *Tippy is a dog She loves to play
    with her red ball* `[SEP]`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`[CLS]` *球的颜色是什么* `[SEP]` *Tippy 是一只狗，她喜欢玩她的红色球* `[SEP]`'
- en: 'Now that we have discussed all the elements of BERT so we can use it successfully
    to solve a downstream NLP task, let’s reiterate the key points about BERT:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了 BERT 的所有元素，因此我们可以成功地使用它来解决下游的 NLP 任务，接下来让我们重述关于 BERT 的关键点：
- en: BERT is an encoder-based Transformer
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 是一个基于编码器的 Transformer
- en: BERT outputs a hidden representation for every token in the input sequence
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 对输入序列中的每个标记输出一个隐藏表示
- en: 'BERT has three embedding spaces: token embedding, positional embedding, and
    segment embedding'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 有三个嵌入空间：标记嵌入、位置嵌入和片段嵌入
- en: BERT uses a special token `[CLS]` to denote the beginning of an input and is
    used as the input to a downstream classification model
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 使用特殊的标记 `[CLS]` 来表示输入的开始，并作为下游分类模型的输入
- en: 'BERT is designed to solve four types of NLP tasks: sequence classification,
    token classification, free-text question-answering, and multiple-choice question-answering'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 被设计用来解决四种类型的 NLP 任务：序列分类、标记分类、自由文本问答和多项选择问答
- en: BERT uses the special token `[SEP]` to separate between sequence A and sequence
    B
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 使用特殊的标记 `[SEP]` 来分隔序列 A 和序列 B
- en: The power within BERT doesn’t just lie within its structure. BERT is pre-trained
    on a large corpus of text using a few different pre-training techniques. In other
    words, BERT already comes with a solid understanding of the language, making downstream
    NLP tasks easier to solve. Next, let’s discuss how BERT is pre-trained.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的强大不仅体现在其结构上。BERT 在一个庞大的文本语料库上进行预训练，使用几种不同的预训练技术。换句话说，BERT 已经具备了对语言的扎实理解，这使得下游的自然语言处理任务更容易解决。接下来，让我们讨论
    BERT 是如何进行预训练的。
- en: How BERT is pre-trained
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 是如何进行预训练的
- en: 'The real value of BERT comes from the fact that it has been pre-trained on
    a large corpus of data in a self-supervised fashion. In the pre-training stage,
    BERT is trained on two different tasks:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的真正价值在于它已经在一个庞大的数据语料库上进行了自监督的预训练。在预训练阶段，BERT 被训练于两个不同的任务：
- en: '**Masked language modeling** (sometimes abbreviated as **MLM**)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩码语言建模**（有时缩写为 **MLM**）'
- en: '**Next sentence prediction** (sometimes abbreviated as **NSP**)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个句子预测**（有时缩写为 **NSP**）'
- en: Let’s now discuss the details of the above two tasks and how they provide language
    understanding for BERT.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论上述两个任务的细节，以及它们是如何为 BERT 提供语言理解的。
- en: Masked Language Modeling (MLM)
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）
- en: 'The MLM task is inspired by the Cloze task, or the Cloze test, where a student
    is given a sentence with one or more blanks and is asked to fill the blanks. Similarly,
    given a text corpus, words are masked from sentences and then the model is asked
    to predict the masked tokens. For example, the sentence:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: MLM 任务的灵感来源于 Cloze 任务或 Cloze 测试，学生会得到一个句子，其中有一个或多个空白，需要填充这些空白。同样地，给定一个文本语料库，句子中的词被掩码，然后模型需要预测这些被掩码的标记。例如，句子：
- en: '*I went to the bakery to buy bread*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*我去面包店买面包*'
- en: 'might become:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可能变成：
- en: '*I went to the* `[MASK]` *to buy bread*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*我去* `[MASK]` *买面包*'
- en: 'BERT uses a special token, `[MASK]`, to represent masked words. Then the target
    for the model will be the word *bakery*. But this introduces a practical issue
    to the model. The special `[MASK]` token does not appear in the actual text. This
    means that the text the model will see during the finetuning phase (that is, when
    training on a classification problem) will be different to what it will see during
    pre-training. This is sometimes referred to as the **pre-training-finetuning discrepancy**.
    Therefore, the authors of BERT suggest the following approach to cope with the
    issue. When masking a word, do one of the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 使用特殊的标记 `[MASK]` 来表示被掩码的词。然后，模型的目标词将是 *bakery*（面包店）。但是，这为模型引入了一个实际问题。特殊的
    `[MASK]` 标记在实际文本中并不会出现。这意味着模型在微调阶段（即在分类问题上训练时）看到的文本与预训练时看到的文本会有所不同。这有时被称为 **预训练-微调不一致**。因此，BERT
    的作者提出了以下方法来应对这个问题。当掩码一个词时，执行以下操作之一：
- en: Use the `[MASK]` token as it is (with 80% probability)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照原样使用 `[MASK]` 标记（80% 的概率）
- en: Use a random word (with 10% probability)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个随机词（10% 的概率）
- en: Use the true word (with 10% probability)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实的词（10% 的概率）
- en: In other words, instead of always seeing `[MASK]`, the model will see actual
    words on certain occasions, alleviating the discrepancy.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，模型在某些情况下会看到实际的单词，而不总是看到 `[MASK]`，从而缓解了这种差异。
- en: Next Sentence Prediction (NSP)
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下一句预测（NSP）
- en: In the NSP task, the model is given a pair of sentences, A and B (in that order),
    and is asked to predict whether the B is the next sentence after A. This can be
    done by fitting a binary classifier onto BERT and training the whole model from
    end to end on selected pairs of sentences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NSP 任务中，模型会接收到一对句子 A 和 B（按此顺序），并被要求预测 B 是否是 A 后面的下一句。这可以通过在 BERT 上拟合一个二分类器并对选定的句子对进行端到端训练来实现。
- en: 'Generating pairs of sentences as inputs for the model is not hard and can be
    done in an unsupervised manner:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 生成句子对作为模型的输入并不难，可以以无监督的方式进行：
- en: A sample with the label TRUE is generated by picking two sentences that are
    adjacent to each other
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择相邻的两句话，生成一个标签为 TRUE 的样本。
- en: A sample with the label FALSE is generated by picking two sentences randomly
    that are not adjacent to each other
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过随机选择两句不相邻的句子，生成一个标签为 FALSE 的样本。
- en: Following this approach, a labeled dataset is generated for the next sentence
    prediction task. Then BERT, along with the binary classifier, is trained from
    end to end using the labeled dataset to solve a downstream task. To see this in
    action, we’ll be using Hugging Face’s `transformers` library.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这种方法，我们生成一个用于下一个句子预测任务的标注数据集。然后，BERT 和二分类器一起，使用该标注数据集进行端到端的训练，以解决下游任务。为了看到这一过程的实际应用，我们将使用
    Hugging Face 的 `transformers` 库。
- en: 'Use case: Using BERT to answer questions'
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用场景：使用 BERT 回答问题。
- en: Now let’s learn how to implement BERT, train it on a question-answer dataset,
    and ask the model to answer a given question.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习如何实现 BERT，在一个问答数据集上训练它，并让模型回答给定的问题。
- en: Introduction to the Hugging Face transformers library
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face `transformers` 库简介
- en: We will use the `transformers` library built by Hugging Face. The `transformers`
    library is a high-level API that is built on top of TensorFlow, PyTorch, and JAX.
    It provides easy access to pre-trained Transformer models that can be downloaded
    and fine-tuned with ease. You can find models in the Hugging Face’s model registry
    at [https://huggingface.co/models](https://huggingface.co/models). You can filter
    models by task, examine the underlying deep learning frameworks, and more.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由 Hugging Face 构建的 `transformers` 库。`transformers` 库是一个高层次的 API，建立在 TensorFlow、PyTorch
    和 JAX 之上。它提供了便捷的访问预训练 Transformer 模型的方式，这些模型可以轻松下载并进行微调。你可以在 Hugging Face 的模型注册表中找到模型，网址为
    [https://huggingface.co/models](https://huggingface.co/models)。你可以按任务筛选模型，查看底层的深度学习框架等。
- en: 'The `transformers` library was designed with the aim of providing a very low
    barrier for entry to using complex Transformer models. For this reason, there’s
    only a handful of concepts that you need to learn in order to hit the ground running
    with the library. Three important classes are required to load and use a model
    successfully:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers` 库的设计目的是提供一个非常低的入门门槛，使用复杂的 Transformer 模型。因此，使用该库时你只需要学习少数几个概念，就能快速上手。成功加载和使用模型需要三类重要的类：'
- en: Model class (such as `TFBertModel`) – Contains the trained weights of the model
    in the form of `tf.keras.models.Model` or the PyTorch equivalent.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型类（如 `TFBertModel`）– 包含模型的训练权重，形式为 `tf.keras.models.Model` 或 PyTorch 等效类。
- en: Configuration (such as `BertConfig`) – Stores various parameters and hyperparameters
    needed to load the model. If you’re using the pre-trained model as is, you don’t
    need to explicitly define its configuration.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置（如 `BertConfig`）– 存储加载模型所需的各种参数和超参数。如果你直接使用预训练模型，则不需要显式定义其配置。
- en: Tokenizer (such as `BertTokenizerFast`) – Contains the vocabulary and token-to-ID
    mapping needed to tokenize the words for the model.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tokenizer（如 `BertTokenizerFast`）– 包含模型所需的词汇和词到 ID 的映射，用于对文本进行分词。
- en: 'All of these classes can be used with two straightforward functions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些类都可以通过两个简单的函数来使用：
- en: '`from_pretrained()` – Provides a way to instantiate a model/configuration/tokenizer
    available from the model repository or locally'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_pretrained()` – 提供一种从模型库或本地实例化模型/配置/分词器的方法。'
- en: '`save_pretrained()` – Provides a way to save the model/configuration/tokenizer
    so that it can be reloaded later'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_pretrained()` – 提供一种保存模型/配置/分词器的方法，以便以后重新加载。'
- en: TensorFlow hosts a variety of Transformer models (released by both TensorFlow
    and third parties) in TensorFlow Hub (at [https://tfhub.dev/](https://tfhub.dev/)).
    If you would like to know how to use TensorFlow Hub and the raw TensorFlow API
    to implement a model such as BERT, please visit [https://www.tensorflow.org/text/tutorials/classify_text_with_bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 在 TensorFlow Hub（[https://tfhub.dev/](https://tfhub.dev/)）托管了多种 Transformer
    模型（由 TensorFlow 和第三方发布）。如果你想了解如何使用 TensorFlow Hub 和原始 TensorFlow API 实现像 BERT
    这样的模型，请访问 [https://www.tensorflow.org/text/tutorials/classify_text_with_bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)。
- en: We will soon see how these classes and functions are used in an actual use case.
    It is also important to note the side-effects of having such an easy-to-grasp
    interface for using models. Due to serving the very specific purpose of providing
    a way to use Transformer models built with TensorFlow, PyTorch, or Jax, you don’t
    have the modularity or flexibility found in TensorFlow, for example. In other
    words, you cannot use the `transformers` library in the same way you would use
    TensorFlow to build a `tf.keras.models.Model` using `tf.keras.layers.Layer` objects.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会看到这些类和函数如何在实际用例中使用。同样，重要的是要注意，尽管使用模型的界面非常简单易懂，但这也带来了一些副作用。由于它专门用于提供一种使用
    TensorFlow、PyTorch 或 Jax 构建的 Transformer 模型的方法，你在使用时无法享受 TensorFlow 等框架所提供的模块化或灵活性。换句话说，你不能像使用
    TensorFlow 构建 `tf.keras.models.Model`，并使用 `tf.keras.layers.Layer` 对象那样使用 `transformers`
    库。
- en: Exploring the data
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'The dataset we are going to use for this task is a popular question-answering
    dataset called SQUAD. Each datapoint consists of four items:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于此任务的数据集是一个流行的问答数据集，名为 SQUAD。每个数据点由四个部分组成：
- en: A question
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个问题
- en: A context that may contain the answer to the question
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能包含问题答案的上下文
- en: The start index of the answer
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案的起始索引
- en: The answer
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案
- en: 'We can download the dataset using Hugging Face’s `datasets` library and call
    the `load_dataset()` function with the `"squad"` argument:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Hugging Face 的 `datasets` 库下载数据集，并通过传入 `"squad"` 参数来调用 `load_dataset()`
    函数：
- en: '[PRE0]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let’s print some examples using:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用以下方法打印一些示例：
- en: '[PRE1]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'which will output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出：
- en: '[PRE2]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, `answer_start` indicates the character index at which this answer starts
    in the context provided. With a good understanding of what’s available in the
    dataset, we’ll perform a simple processing step. When training the model, we will
    be asking the model to predict the start and end indices of the answer. In its
    original form, only the `answer_start` is present. We will need to manually add
    `answer_end` to our dataset. The following function does this. Furthermore, it
    does a few sanitary checks on the dataset:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`answer_start` 表示答案在提供的上下文中开始的字符索引。通过对数据集中可用内容的充分理解，我们将执行一个简单的处理步骤。在训练模型时，我们将要求模型预测答案的起始和结束索引。在其原始形式中，仅存在
    `answer_start`。我们需要手动将 `answer_end` 添加到数据集中。以下函数实现了这一功能。此外，它还对数据集进行了几个检查：
- en: '[PRE3]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we will download a pre-trained BERT model from the Hugging Face repository
    and learn about the model in depth.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从 Hugging Face 仓库下载一个预训练的 BERT 模型，并深入了解该模型。
- en: Implementing BERT
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 BERT
- en: 'To use a pre-trained Transformer model from the Hugging Face repository, we
    need three components:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Hugging Face 仓库中的预训练 Transformer 模型，我们需要三个组件：
- en: '`Tokenizer` – Responsible for splitting a long bit of text (such as a sentence)
    into smaller tokens'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tokenizer` – 负责将长文本（例如句子）拆分成更小的标记'
- en: '`config` – Contains the configuration of the model'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` – 包含模型的配置'
- en: '`Model` – Takes in the tokens, looks up the embeddings, and produces the final
    output(s) using the provided inputs'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Model` – 接收标记，查找嵌入，并使用提供的输入生成最终输出'
- en: We can ignore the `config` as we are using the pre-trained model as is. However,
    to paint a full picture, we will use the configuration nevertheless.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以忽略 `config`，因为我们将直接使用预训练模型。但是，为了完整展示，我们还是会使用配置。
- en: Implementing and using the Tokenizer
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现和使用 Tokenizer
- en: 'First, we will look at how to download the Tokenizer. You can download the
    Tokenizer using the `transformers` library. Simply call the `from_pretrained()`
    function provided by the `PreTrainedTokenizerFast` base class:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看如何下载 Tokenizer。你可以使用 `transformers` 库下载 Tokenizer。只需调用 `PreTrainedTokenizerFast`
    基类提供的 `from_pretrained()` 函数：
- en: '[PRE4]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will be using a Tokenizer called `bert-base-uncased`. It is the Tokenizer
    developed for the BERT base model and is uncased (that is, there’s no distinction
    between uppercase and lowercase characters). Next, let’s see the Tokenizer in
    action:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用名为 `bert-base-uncased` 的分词器。它是为 BERT 基础模型开发的分词器，并且是不区分大小写的（即不区分大写字母和小写字母）。接下来，让我们看看分词器的实际应用：
- en: '[PRE5]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s understand the arguments we’ve provided to the tokenizer’s call:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来理解一下我们传递给分词器的参数：
- en: '`text` – A single or batch of text sequences to be encoded by the tokenizer.
    Each text sequence is a string.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` – 单个或一批文本序列，供分词器进行编码。每个文本序列都是一个字符串。'
- en: '`text_pair` – An optional single or batch of text sequences to be encoded by
    the tokenizer. It’s useful in situations where the model takes a multi-part input
    (such as a question and a context in question-answering).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` – 一个可选的单个或一批文本序列，供分词器进行编码。在模型接受多部分输入的情况下（例如，在问答任务中，包含问题和上下文），它非常有用。'
- en: '`padding` – Indicates the padding strategy. If set to `True`, it will be padded
    to the maximum sequence length in the dataset. If set to `max_length`, it will
    be padded to the length specified by the `max_length` argument. If set to `False`,
    no padding will be done.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` – 表示填充策略。如果设置为 `True`，则将填充到数据集中的最大序列长度。如果设置为 `max_length`，则将填充到由
    `max_length` 参数指定的长度。如果设置为 `False`，则不进行填充。'
- en: '`return_tensors` – An argument that defines the type of tensors returned. It
    could be either `pt` (PyTorch) or `tf` (TensorFlow). Since we want TensorFlow
    tensors, we define it as `''tf''`.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` – 定义返回张量类型的参数。它可以是 `pt`（PyTorch）或 `tf`（TensorFlow）。由于我们需要使用
    TensorFlow 张量，因此将其定义为 `''tf''`。'
- en: 'This prints:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE6]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This outputs a `transformers.tokenization_utils_base.BatchEncoding` object,
    which is essentially a dictionary. It has three keys and tensors as values:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出一个 `transformers.tokenization_utils_base.BatchEncoding` 对象，它本质上是一个字典。它包含三个键，每个键对应一个张量值：
- en: '`input_ids` – Provides the IDs of the tokens for the tokens found in the text
    sequences. Additionally, it introduces the `[CLS]` token ID at the beginning of
    the sequence and two instances of the `[SEP]` token ID, one between the question
    and context, and the other one at the end.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` – 提供文本序列中标记的 ID。此外，它会在序列开头插入 `[CLS]` 标记的 ID，并在问题与上下文之间以及序列末尾插入两个
    `[SEP]` 标记的 ID。'
- en: '`token_type_ids` – This is the segment ID we use for the segment embedding.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` – 用于段落嵌入的段落 ID。'
- en: '`attention_mask` – The attention mask represents the words that are allowed
    to be attended to during the forward pass. Since BERT is an encoder model, any
    token can pay attention to any other token. The only exception is the padded tokens
    that will be ignored during the attention mechanism.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` – 注意力掩码表示在前向传播过程中可以被注意到的词。由于 BERT 是一个编码器模型，任何标记都可以关注任何其他标记。唯一的例外是填充的标记，它们会在注意力机制中被忽略。'
- en: 'We could also convert these token IDs to actual tokens to know what they represent.
    To do that, we use the `convert_ids_to_tokens()` function:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将这些标记 ID 转换为实际的标记，以了解它们代表什么。为此，我们使用 `convert_ids_to_tokens()` 函数：
- en: '[PRE7]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will print:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE8]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can see how the tokenizer inserts special tokens like `[CLS]` and `[SEP]`
    into the text sequence. With the functionality of the tokenizer understood, let’s
    use it to encode the train and test datasets:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到分词器如何将特殊标记如 `[CLS]` 和 `[SEP]` 插入到文本序列中。在理解了分词器的功能后，接下来让我们使用它来编码训练集和测试集：
- en: '[PRE9]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can check the size of the train encodings by running:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来检查训练编码的大小：
- en: '[PRE10]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'which will give:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE11]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The maximum sequence length in our dataset is 512\. Therefore, we see that
    the maximum length of the sequences is 512\. Once we tokenize our data, we need
    to perform one more data processing step. Our `answer_start` and `answer_end`
    indices are character-based. However, since we are working with tokens, we need
    to convert our character-based indices to token-based indices. We will define
    a function for that:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的最大序列长度为 512。因此，我们看到序列的最大长度为 512。一旦我们将数据进行分词，我们还需要进行一步数据处理。我们的 `answer_start`
    和 `answer_end` 索引是基于字符的。然而，由于我们处理的是标记，我们需要将基于字符的索引转换为基于标记的索引。我们将为此定义一个函数：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This function takes in a set of `BatchEncodings` called `encodings` generated
    by the tokenizer and a set of answers (a list of dictionaries). Then it updates
    the provided encodings with two new keys: `start_positions` and `end_positions`.
    These keys respectively hold the token-based indices denoting the start and end
    of the answer. If the answer is not found, we set the start and end indices to
    the last token. To convert our existing character-based indices to token-based
    indices, we use a function called `char_to_token()` provided by the `BatchEncodings`
    class. It takes a character index as the input and provides the corresponding
    token index as the output. With the function defined, let’s call it on our training
    and testing data:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接收由分词器生成的一组`BatchEncodings`（称为`encodings`）和一组答案（字典列表）。然后，它通过两个新键`start_positions`和`end_positions`更新提供的编码。这两个键分别保存表示答案开始和结束的基于
    token 的索引。如果没有找到答案，我们将开始和结束索引设置为最后一个 token。为了将现有的基于字符的索引转换为基于 token 的索引，我们使用一个名为`char_to_token()`的函数，该函数由`BatchEncodings`类提供。它以字符索引为输入，输出相应的
    token 索引。定义好该函数后，让我们在训练和测试数据上调用它：
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With the clean data, we will now define a TensorFlow dataset. Note that this
    function modifies the encodings in place.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用清理后的数据，我们现在将定义一个 TensorFlow 数据集。请注意，此函数会原地修改编码。
- en: Defining a TensorFlow dataset
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义 TensorFlow 数据集
- en: 'Next, let’s implement a TensorFlow dataset to generate the data for the model.
    Our data will consist of two tuples: one containing inputs and the other containing
    the targets. The input tuple contains:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个 TensorFlow 数据集，以生成模型所需的数据。我们的数据将包含两个元组：一个包含输入，另一个包含目标。输入元组包含：
- en: Input token IDs – A batch of padded token IDs of size `[batch size, sequence
    length]`
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入 token ID – 一批填充的 token ID，大小为`[batch size, sequence length]`
- en: Attention mask – A batch of attention masks of size `[batch size, sequence length]`
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力掩码 – 一批注意力掩码，大小为`[batch size, sequence length]`
- en: 'The output tuple contains:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出元组包含：
- en: Start index of the answer – A batch of start indices of the answer
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案的起始索引 – 一批答案的起始索引
- en: End index of the answer – A batch of end indices of the answer
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案的结束索引 – 一批答案的结束索引
- en: 'We will first define a generator that generates the data in this format:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义一个生成器，生成这种格式的数据：
- en: '[PRE14]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since we have already processed the data, it’s a matter of reorganizing the
    already existing data to return using the code above.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经处理了数据，因此只需重新组织已有的数据即可使用上面的代码返回结果。
- en: 'Next, we will define a partial function that we can simply call without passing
    any arguments:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个部分函数，可以在不传递任何参数的情况下直接调用：
- en: '[PRE15]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This function is then passed to the `tf.data.Dataset.from_generator()` function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将此函数传递给`tf.data.Dataset.from_generator()`函数：
- en: '[PRE16]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then shuffle the data in our training dataset. When shuffling a TensorFlow
    dataset we need to provide a buffer size. The buffer size defines how many samples
    are chosen to shuffle. Here we set that to 1,000 samples:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将训练数据集中的数据打乱。在打乱 TensorFlow 数据集时，我们需要提供缓冲区大小。缓冲区大小定义了用于打乱的样本数量。这里我们将其设置为
    1,000 个样本：
- en: '[PRE17]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we split our dataset into two: a training set and a validation dataset.
    We will use the first 10,000 samples as the validation set. The rest of the data
    is used as the training set. Both datasets will be batched using a batch size
    of 4:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集分为两部分：训练集和验证集。我们将使用前 10,000 个样本作为验证集，其余的数据作为训练集。两个数据集都将使用批量大小为 4 的批处理：
- en: '[PRE18]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we follow the same procedure to create the test dataset:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按照相同的过程创建测试数据集：
- en: '[PRE19]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now let’s see how BERT’s architecture can be used to answer questions.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 BERT 的架构如何用于回答问题。
- en: BERT for answering questions
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于回答问题的 BERT
- en: There are a few modifications introduced on top of the pre-trained BERT model
    to leverage it for question-answering. First, the model takes in a question, followed
    by a context. As we discussed before, the context may or may not contain the answer
    to the question. The input has the format `[CLS] <question tokens> [SEP] <context
    tokens> [SEP]`. Then, for each token position of the context, we have two classification
    heads predicting a probability. One head predicts the probability of each context
    token being the start of the answer, whereas the other one predicts the probability
    of each context token being the end of the answer.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练的BERT模型基础上，为了使其适应问答任务，进行了几项修改。首先，模型输入一个问题，后接一个上下文。如前所述，上下文可能包含也可能不包含问题的答案。输入格式为`[CLS]
    <问题标记> [SEP] <上下文标记> [SEP]`。然后，对于上下文中的每个标记位置，我们有两个分类头预测概率。一个头部预测每个上下文标记作为答案开始的概率，另一个头部预测每个上下文标记作为答案结束的概率。
- en: Once we figure out the start and end indices of the answer, we can simply extract
    the answer from the context using those indices.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了答案的起始和结束索引，我们就可以使用这些索引从上下文中提取答案。
- en: '![](img/B14070_10_11.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_10_11.png)'
- en: 'Figure 10.11: Using BERT for question-answering. The model takes in a question
    followed by a context. The model has two heads: one to predict the probability
    of each token in the context being the start of the answer and another to predict
    the end of the answer for each context token.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：使用BERT进行问答。模型输入一个问题，后接一个上下文。模型有两个头部：一个预测上下文中每个标记作为答案开始的概率，另一个预测上下文中每个标记作为答案结束的概率。
- en: Defining the config and the model
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义配置和模型
- en: 'In Hugging Face, you have several variants of each Transformer model. These
    variants are based on different tasks solved by these models. For example, for
    BERT we have:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face中，每种Transformer模型有多个变种。这些变种是基于这些模型解决的不同任务。例如，对于BERT，我们有：
- en: '`TFBertForPretraining` – The pre-trained model without a task-specific head'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForPretraining` – 没有任务特定头部的预训练模型'
- en: '`TFBertForSequenceClassification` – Used for classifying a sequence of text'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForSequenceClassification` – 用于对文本序列进行分类'
- en: '`TFBertForTokenClassification` – Used for classifying each token in the sequence
    of text'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForTokenClassification` – 用于对文本序列中的每个标记进行分类'
- en: '`TFBertForMultipleChoice` – Used for answering multiple-choice questions'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForMultipleChoice` – 用于回答多项选择题'
- en: '`TFBertForQuestionAnswering` – Used for extracting answers to a question from
    a given context'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForQuestionAnswering` – 用于从给定上下文中提取问题的答案'
- en: '`TFBertForMaskedLM` – Used for pre-training BERT on the masked language modeling
    task'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForMaskedLM` – 用于在掩蔽语言模型任务上预训练BERT'
- en: '`TFBertForNextSentencePrediction` – Used for pre-training BERT to predict the
    next sentence'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TFBertForNextSentencePrediction` – 用于预训练BERT预测下一句'
- en: 'Here, we are interested in `TFBertForQuestionAnswering`. Let’s import this
    class along with the `BertConfig` class, which we will extract important hyperparameters
    from:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们感兴趣的是`TFBertForQuestionAnswering`。让我们导入这个类以及`BertConfig`类，我们将从中提取重要的超参数：
- en: '[PRE20]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To get the pre-trained `config`, we call the `from_pretrained()` function of
    `BertConfig` with the model we’re interested in. Here, we’ll use the `bert-base-uncased`
    model:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取预训练的`config`，我们调用`BertConfig`的`from_pretrained()`函数，并传入我们感兴趣的模型。在这里，我们将使用`bert-base-uncased`模型：
- en: '[PRE21]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can print the `config` and see what’s in there:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以打印`config`并查看其中的内容：
- en: '[PRE22]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we get the model by calling the same function `from_pretrained()`
    from the `TFBertForQuestionAnswering` class and pass the `config` we just obtained:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过调用`TFBertForQuestionAnswering`类中的相同函数`from_pretrained()`并传入我们刚刚获得的`config`来获取模型：
- en: '[PRE23]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When you run this, you will get a warning saying:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个时，你会收到一个警告，内容如下：
- en: '[PRE24]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This is expected and totally fine. It’s saying that there are some layers that
    have not been initialized from the pre-trained model; the output heads of the
    model need to be introduced as new layers, thus they are not pre-initialized.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预期的，并且完全正常。它表示有些层尚未从预训练模型中初始化；模型的输出头需要作为新层引入，因此它们没有预初始化。
- en: 'After that, we will define a function that will wrap the returned model as
    a `tf.keras.models.Model` object. We need to perform this step because if we try
    to use the model as it is, TensorFlow returns the following error:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将定义一个函数，将返回的模型包装为`tf.keras.models.Model`对象。我们需要执行这一步，因为如果我们直接使用模型，TensorFlow会返回以下错误：
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Therefore, we will define two input layers: one takes in the input token IDs
    and the other takes the attention mask and passes it to the model. Finally, we
    get the output of the model. We then define a `tf.keras.models.Model` using these
    inputs and output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将定义两个输入层：一个输入令牌 ID，另一个输入 attention mask 并将其传递给模型。最后，我们得到模型的输出。然后，我们使用这些输入和输出定义一个`tf.keras.models.Model`：
- en: '[PRE26]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we learned when studying the structure of the model, the question-answering
    BERT has two heads: one to predict the starting index of the answer and the other
    to predict the end. Therefore, we have to optimize two losses coming from the
    two heads. This means we need to add the two losses to get the final loss. When
    we have a multi-output model such as this, we can pass multiple loss functions
    aimed at each output head. Here, we define a single loss function. This means
    the same loss will be used across both heads and will be summed to generate the
    final loss:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在学习模型结构时所了解到的，问答 BERT 有两个头：一个用于预测答案的起始索引，另一个用于预测结束索引。因此，我们需要优化来自这两个头的两个损失。这意味着我们需要将两个损失相加以获得最终的损失。当我们有一个多输出模型时，我们可以为每个输出头传递多个损失函数。在这里，我们定义了一个单一的损失函数。这意味着两个头会使用相同的损失，并将它们加起来生成最终损失：
- en: '[PRE27]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We will now see how we can train and evaluate our model on the question-answering
    task.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何在问答任务中训练和评估我们的模型。
- en: Training and evaluating the model
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: 'We already have the data prepared and the model defined. Training the model
    is quite easy, and is just a one-liner:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好了数据并定义了模型。训练模型非常简单，只需要一行代码：
- en: '[PRE28]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You should see an output as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下输出：
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should see the accuracy on the validation set reaching an accuracy between
    ~73 and 75%. This is quite high, given we only trained the model for two epochs.
    This performance can be attributed to the high level of language understanding
    the pre-trained model already had when we downloaded it. Let’s evaluate the model
    on our test data:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到验证集的准确率达到大约 73% 到 75% 之间。这是相当高的，考虑到我们只训练了模型两个周期。这个表现可以归功于我们下载的预训练模型已经具备了很高的语言理解能力。让我们在测试数据上评估模型：
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'It should output the following:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该输出以下内容：
- en: '[PRE31]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We see that it performs comparably well on the test dataset as well. Finally,
    we can save the model. We will save the `TFBertForQuestionAnswering` component
    of the model. We’ll also save the tokenizer:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到它在测试数据集上的表现也相当不错。最后，我们可以保存模型。我们将保存模型的`TFBertForQuestionAnswering`组件。我们还将保存分词器：
- en: '[PRE32]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We have trained our model and evaluated it to ensure the model performs well.
    Once we confirmed that the model is performing well, we finally saved it for future
    use. Next, let’s discuss how we can use this model to generate answers for a given
    question.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练并评估了模型，以确保其表现良好。确认模型表现良好后，我们最终将其保存以供将来使用。接下来，让我们讨论如何使用这个模型生成给定问题的答案。
- en: Answering questions with Bert
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Bert 回答问题
- en: 'Let’s now write a simple script to generate answers to questions from the trained
    model. First, let’s define a sample question to generate an answer for. We’ll
    also store the inputs and the ground truth answer to compare:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个简单的脚本，从训练好的模型中生成问题的答案。首先，我们定义一个示例问题来生成答案。我们还将存储输入和真实答案以进行比较：
- en: '[PRE33]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we’ll define the inputs to the model. The input to the model needs to
    have a batch dimension. Therefore we use the `[i:i+1]` syntax to make sure the
    batch dimension is not flattened:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义模型的输入。模型的输入需要有一个批量维度。因此，我们使用`[i:i+1]`语法来确保批量维度不会被压扁：
- en: '[PRE34]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Let’s now define a simple function called `ask_bert` to find an answer from
    the context for a given question. This function takes in an input, a tokenizer,
    and a model.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个简单的函数`ask_bert`，用于从给定的问题中找到答案。这个函数接收输入、分词器和模型作为参数。
- en: 'Then it generates the token IDs from the tokenizer, passes them to the model,
    outputs the start and end indices for the answer, and finally extracts the corresponding
    answer from the text of the context:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它从分词器生成令牌 ID，将它们传递给模型，输出答案的起始和结束索引，最后从上下文的文本中提取相应的答案：
- en: '[PRE35]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s execute the following lines to print the answer given by our model:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下代码行来打印模型给出的答案：
- en: '[PRE36]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'which will print:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出：
- en: '[PRE37]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can see that BERT has answered the question correctly. We have learned a
    lot about Transformers in general as well as BERT-specific architecture. We then
    used this knowledge to adapt BERT to solve a question-answering problem. Here
    we end our discussion about Transformers and BERT.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 BERT 已经正确回答了问题。我们学习了很多关于 Transformer 的内容，以及 BERT 特有的架构。然后，我们运用这些知识，将
    BERT 调整为解决问答问题。至此，我们结束了关于 Transformer 和 BERT 的讨论。
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about Transformer models. First, we looked at the
    Transformer at a microscopic level to understand the inner workings of the model.
    We saw that Transformers use self-attention, a powerful technique to attend to
    other inputs in the text sequences while processing one input. We also saw that
    Transformers use positional embeddings to inform the model about the relative
    position of tokens in addition to token embeddings. We also discussed that Transformers
    leverage residual connections (that is, shortcut connections) and layer normalization
    in order to improve model training.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 Transformer 模型。首先，我们从微观角度看了 Transformer，以理解模型的内部工作原理。我们看到，Transformer
    使用了自注意力机制，这是一种强大的技术，可以在处理一个输入时，关注文本序列中的其他输入。我们还看到，Transformer 使用位置编码来告知模型 token
    在序列中的相对位置，除了 token 编码之外。我们还讨论了，Transformer 利用残差连接（即快捷连接）和层归一化，以提高模型的训练效果。
- en: 'We then discussed BERT, an encoder-based Transformer model. We looked at the
    format of the data accepted by BERT and the special tokens it uses in the input.
    Next, we discussed four different types of task BERT can solve: sequence classification,
    token classification, multiple-choice, and question-answering.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了 BERT，一个基于编码器的 Transformer 模型。我们查看了 BERT 接受的数据格式和它在输入中使用的特殊 token。接下来，我们讨论了
    BERT 可以解决的四种不同任务类型：序列分类、token 分类、多选题和问答。
- en: Finally, we looked at how BERT is pre-trained on a large corpus of text.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们查看了 BERT 如何在大量文本语料库上进行预训练。
- en: 'After that, we started on a use case: answering questions with BERT. To implement
    the solution, we used the `transformers` library by Hugging Face. It’s an extremely
    useful high-level library built on top of deep learning frameworks such as TensorFlow,
    PyTorch, and Jax. The `transformers` library is specifically designed for quickly
    loading and using pre-trained Transformer models. In this use case, we first processed
    the data and created a `tf.data.Dataset` to stream the data in batches. Then we
    trained the model on that data and evaluated it on a test set. Finally, we used
    the model to infer answers to a sample question given to the model.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们开始了一个使用案例：用 BERT 回答问题。为了实现这个解决方案，我们使用了 Hugging Face 的 `transformers` 库。这是一个非常有用的高级库，建立在
    TensorFlow、PyTorch 和 Jax 等深度学习框架之上。`transformers` 库专为快速加载和使用预训练的 Transformer 模型而设计。在这个使用案例中，我们首先处理了数据，并创建了一个
    `tf.data.Dataset`，用于分批流式传输数据。然后，我们在这些数据上训练了模型，并在测试集上进行了评估。最后，我们使用该模型推断给定示例问题的答案。
- en: 'In the next chapter, we will learn a bit more about Transformers and how they
    can be used in a more complicated task that involves both images and text: image
    caption generation.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地了解 Transformer 及其在一个更复杂任务中的应用——图像与文本结合的任务：图像标题生成。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起学习，超过 1000 名成员等你加入：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)*'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/QR_Code5143653472357468031.png)*'
