- en: Maths for Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的数学
- en: Neural network users need to have a fair understanding of neural network concepts,
    algorithms, and the underlying mathematics. Good mathematical intuition and understanding
    of many techniques is necessary for a solid grasp of the inner functioning of
    the algorithms and for getting good results. The amount of maths required and
    the level of maths needed to understand these techniques is multidimensional and
    also depends on interest. In this chapter, you will learn neural networks by understanding
    the maths used to solve complex computational problems. This chapter covers the
    basics of linear algebra, calculus, and optimization for neural networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络用户需要对神经网络的概念、算法及其背后的数学有一定的了解。良好的数学直觉和对多种技术的理解对于深入掌握算法的内部运作并获得良好的结果是必要的。理解这些技术所需的数学量及数学水平是多维度的，并且也取决于兴趣。在本章中，你将通过理解用于解决复杂计算问题的数学，来学习神经网络。本章涵盖了神经网络所需的线性代数、微积分和优化的基础知识。
- en: The main purpose of this chapter is to set up the fundamentals of mathematics
    for the upcoming chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目的是为接下来的章节设置数学基础。
- en: 'Following topics will be covered in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding linear algebra
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线性代数
- en: Understanding Calculus
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解微积分
- en: Optimization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化
- en: Understanding linear algebra
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线性代数
- en: Linear algebra is a key branch of mathematics. An understanding of linear algebra
    is crucial for **deep learning**, that is, neural networks. Throughout this chapter,
    we will go through the key and fundamental linear algebra prerequisites. Linear
    Algebra deals with linear systems of equations. Instead of working with scalars,
    we start working with matrices and vectors. Using linear algebra, we can describe
    complicated operations in deep learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是数学的一个重要分支。理解线性代数对于**深度学习**（即神经网络）至关重要。在本章中，我们将介绍线性代数的关键基础知识。线性代数处理线性方程组。我们不再只处理标量，而是开始处理矩阵和向量。通过线性代数，我们可以描述深度学习中的复杂运算。
- en: Environment setup
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: Before we jump into the field of mathematics and its properties, it's essential
    for us to set up the development environment as it will provide us settings to
    execute the concepts we learn, meaning installing the compiler, dependencies,
    and **IDE** (**Integrated Development Environment**) to run our code base.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究数学及其性质之前，设置开发环境至关重要，因为它将为我们提供执行所学概念所需的设置，意味着安装编译器、依赖项以及**IDE**（**集成开发环境**）来运行我们的代码库。
- en: Setting up the Python environment in Pycharm
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Pycharm 中设置 Python 环境
- en: It is best to use an IDE like Pycharm to edit Python code as it provides development
    tools and built-in coding assistance. Code inspection makes coding and debugging
    faster and simpler, ensuring that you focus on the end goal of learning maths
    for neural networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最好使用像 Pycharm 这样的集成开发环境（IDE）来编辑 Python 代码，因为它提供了开发工具和内置的代码辅助功能。代码检查使编码和调试变得更快、更简便，确保你能专注于学习神经网络所需的数学知识这一最终目标。
- en: 'The following steps show you how to set up local Python environment in Pycharm:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何在 Pycharm 中设置本地 Python 环境：
- en: 'Go to Preferences and verify that the TensorFlow library is installed. If not,
    follow the instructions at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)
    to install TensorFlow:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到首选项，验证 TensorFlow 库是否已安装。如果没有，请按照[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)上的说明安装
    TensorFlow：
- en: '![](img/80d45f03-9991-4b89-a3f8-78695a3e62f6.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80d45f03-9991-4b89-a3f8-78695a3e62f6.png)'
- en: Keep the default options of TensorFlow and click on OK.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持 TensorFlow 的默认选项并点击确定。
- en: 'Finally, right-click on the source file and click on Run ''matrices'':'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，右键点击源文件并点击运行 'matrices'：
- en: '![](img/adef8686-9dd0-4c7c-93a1-38bdbb9f497e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adef8686-9dd0-4c7c-93a1-38bdbb9f497e.png)'
- en: Linear algebra structures
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数结构
- en: In the following section, we will describe the fundamental structures of linear
    algebra.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将描述线性代数的基本结构。
- en: Scalars, vectors, and matrices
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标量、向量和矩阵
- en: 'Scalars, vectors, and matrices are the fundamental objects of mathematics.
    Basic definitions are listed as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 标量、向量和矩阵是数学中的基本对象。基本定义如下：
- en: Scalar is represented by a single number or numerical value called **magnitude**.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量是通过一个称为**大小**的单一数字或数值来表示的。
- en: Vector is an array of numbers assembled in order. A unique index identifies
    each number. Vector represents a point in space, with each element giving the
    coordinate along a different axis.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量是按顺序排列的数字数组。每个数字都有一个唯一的索引来标识。向量表示空间中的一个点，每个元素给出了沿不同轴的坐标。
- en: Matrices is a two-dimensional array of numbers where each number is identified
    using two indices (*i*, *j*).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵是一个二维数字数组，其中每个数字通过两个索引（*i*、*j*）来标识。
- en: Tensors
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: An array of numbers with a variable number of axes is known as a **tensor**.
    For example, for three axes, it is identified using three indices (*i*, *j*, *k*).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有可变数量轴的数字数组称为**张量**。例如，对于三个轴，它通过三个索引（*i*、*j*、*k*）来标识。
- en: 'The following image summaries a tensor, it describes a second-order tensor
    object. In a three-dimensional Cartesian coordinate system, tensor components
    will form the matrix:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像总结了一个张量，描述了一个二阶张量对象。在三维笛卡尔坐标系中，张量的分量将形成矩阵：
- en: '![](img/713cf813-f618-4301-bcf0-94bb9d530429.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/713cf813-f618-4301-bcf0-94bb9d530429.png)'
- en: Image reference is taken from tensor wiki [https://en.wikipedia.org/wiki/Tensor](https://en.wikipedia.org/wiki/Tensor)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片参考来自张量维基 [https://en.wikipedia.org/wiki/Tensor](https://en.wikipedia.org/wiki/Tensor)
- en: Operations
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作
- en: The following topics will describe the various operations of linear algebra.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下主题将描述线性代数的各种操作。
- en: Vectors
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量
- en: The `Norm` function is used to get the size of the vector; the norm of a vector
    *x* measures the distance from the origin to the point *x*. It is also known as
    the ![](img/c656e3fe-f16d-4703-b95d-18dd3dc02fca.jpg)norm, where *p=2* is known
    as the **Euclidean norm**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`Norm`函数用于获取向量的大小；向量*x*的范数衡量了从原点到点*x*的距离。它也被称为 ![](img/c656e3fe-f16d-4703-b95d-18dd3dc02fca.jpg)范数，其中*p=2*
    被称为**欧几里得范数**。'
- en: 'The following example shows you how to calculate the ![](img/c656e3fe-f16d-4703-b95d-18dd3dc02fca.jpg)norm
    of a given vector:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何计算给定向量的 ![](img/c656e3fe-f16d-4703-b95d-18dd3dc02fca.jpg)范数：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output of the listing is 8.77496.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出为8.77496。
- en: Matrices
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵
- en: A matrix is a two-dimensional array of numbers where each element is identified
    by two indices instead of just one. If a real matrix *X* has a height of *m* and
    a width of *n*, then we say that *X ∈ Rm × n*. Here, *R* is a set of real numbers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵是一个二维的数字数组，其中每个元素由两个索引标识，而不仅仅是一个。如果一个实数矩阵*X*的高度为*m*，宽度为*n*，则我们说*X ∈ Rm × n*。这里的*R*是实数集。
- en: 'The following example shows how different matrices are converted to tensor
    objects:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何将不同的矩阵转换为张量对象：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the listing is shown in the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下所示：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Matrix multiplication
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'Matrix multiplication of matrices *A* and *B* is a third matrix, *C*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵*A*和矩阵*B*的矩阵乘法是第三个矩阵*C*：
- en: '*C = AB*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*C = AB*'
- en: The element-wise product of matrices is called a **Hadamard** product and is
    denoted as *A.B*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的逐元素积称为**哈达玛积**，表示为*A.B*。
- en: 'The dot product of two vectors *x* and *y* of the same dimensionality is the
    matrix product *x* transposing *y*. Matrix product *C = AB* is like computing
    *C[i,j]* as the dot product between row *i* of matrix *A* and column *j* of matrix
    *B*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 两个维度相同的向量*x*和*y*的点积是矩阵乘积*x*转置*y*。矩阵乘积*C = AB*就像计算*C[i,j]*，即矩阵*A*的第*i*行与矩阵*B*的第*j*列的点积：
- en: '![](img/39075617-fcce-4cf3-aa3f-33fd65df9288.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39075617-fcce-4cf3-aa3f-33fd65df9288.jpg)'
- en: 'The following example shows the Hadamard product and dot product using tensor
    objects:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用张量对象的哈达玛积和点积：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the listing is shown as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下所示：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Trace operator
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迹操作符
- en: 'The trace operator *Tr(A)* of matrix *A* gives the sum of all of the diagonal
    entries of a matrix. The following example shows how to use a trace operator on
    tensor objects:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵*A*的迹操作符*Tr(A)*给出矩阵所有对角线元素的和。以下示例展示了如何在张量对象上使用迹操作符：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output of the listing is *12.0*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出为*12.0*。
- en: Matrix transpose
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵转置
- en: 'Transposition of the matrix is the mirror image of the matrix across the main
    diagonal. A symmetric matrix is any matrix that is equal to its own transpose:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的转置是矩阵沿主对角线的镜像。对称矩阵是任何等于其自身转置的矩阵：
- en: '![](img/8ab26f97-c7f1-4046-9494-9aba9b6d4ac7.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ab26f97-c7f1-4046-9494-9aba9b6d4ac7.jpg)'
- en: 'The following example shows how to use a transpose operator on tensor objects:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何在张量对象上使用转置操作符：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the listing is shown as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下所示：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Matrix diagonals
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵对角线
- en: Matrices that are diagonal in nature consist mostly of zeros and have non-zero
    entries only along the main diagonal. Not all diagonal matrices need to be square.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对角矩阵通常由零组成，只有主对角线上的元素非零。并非所有对角矩阵都必须是方阵。
- en: 'Using the diagonal part operation, we can get the diagonal of a given matrix,
    and to create a matrix with a given diagonal, we use the `diag` operation from
    `tensorflow`. The following example shows how to use diagonal operators on tensor
    objects:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对角部分操作，我们可以获得给定矩阵的对角线，为了创建一个具有给定对角线的矩阵，我们可以使用 `tensorflow` 的 `diag` 操作。以下示例展示了如何在张量对象上使用对角操作：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of this is shown as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Identity matrix
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单位矩阵
- en: An identity matrix is a matrix *I* that does not change any vector, like *V*,
    when multiplied by *I*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 单位矩阵是一个矩阵 *I*，它在与任何向量相乘时不会改变该向量，例如 *V*，当与 *I* 相乘时。
- en: 'The following example shows how to get the identity matrix for a given size:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何为给定大小计算单位矩阵：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of this is shown as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Inverse matrix
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆矩阵
- en: 'The matrix inverse of *I* is denoted as ![](img/9ce156ad-e34e-4287-bbbf-4fa1480b7702.jpg).
    Consider the following equation; to solve it using inverse and different values
    of *b*, there can be multiple solutions for *x*. Note the property:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*I* 的矩阵逆表示为 ![](img/9ce156ad-e34e-4287-bbbf-4fa1480b7702.jpg)。考虑以下方程式；使用逆矩阵和不同的
    *b* 值来求解时，*x* 可以有多个解。请注意以下性质：'
- en: '![](img/4264ad0b-4dd1-4598-a383-b37799179877.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4264ad0b-4dd1-4598-a383-b37799179877.jpg)'
- en: '**![](img/80ef6668-7388-44e3-9008-cb946ffc07b4.jpg)**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/80ef6668-7388-44e3-9008-cb946ffc07b4.jpg)**'
- en: 'The following example shows how to calculate the inverse of a matrix using
    the `matrix_inverse` operation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 `matrix_inverse` 操作计算矩阵的逆：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Solving linear equations
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求解线性方程
- en: TensorFlow can solve a series of linear equations using the `solve` operation.
    Let's first explain this without using the library and later use the `solve` function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 可以使用 `solve` 操作求解一系列线性方程。我们首先解释不使用库的情况，然后再使用 `solve` 函数。
- en: 'A linear equation is represented as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个线性方程的表示如下：
- en: '*ax + b = yy - ax = b*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*ax + b = yy - ax = b*'
- en: '*y - ax = b*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*y - ax = b*'
- en: '*y/b - a/b(x) = 1*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*y/b - a/b(x) = 1*'
- en: Our job is to find the values for *a* and *b* in the preceding equation, given
    our observed points. First, create the matrix points. The first column represents
    *x* values, while the second column represents *y* values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是根据观察到的点来求解前述方程中的 *a* 和 *b* 值。首先，创建矩阵点。第一列表示 *x* 值，第二列表示 *y* 值。
- en: Consider that *X* is the input matrix and *A* is the parameters that we need
    to learn; we set up a system like *AX=B*, therefore, ![](img/a545637a-63f7-46c5-b731-43b1fcae9fab.png).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *X* 是输入矩阵，*A* 是我们需要学习的参数；我们设立一个类似于 *AX=B* 的系统，因此， ![](img/a545637a-63f7-46c5-b731-43b1fcae9fab.png)。
- en: 'The following example, with code, shows how to solve the linear equation:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过代码展示了如何求解线性方程：
- en: '*3x+2y = 15*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*3x+2y = 15*'
- en: '*4x−y = 10*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*4x−y = 10*'
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the listing is shown as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出结果如下所示：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The canonical equation for a circle is *x2+y2+dx+ey+f=0*; to solve this for
    the parameters *d*, *e*, and *f*, we use TensorFlow''s solve operation as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 圆的标准方程是 *x2+y2+dx+ey+f=0*；为了求解其中的参数 *d*、*e* 和 *f*，我们使用 TensorFlow 的 solve 操作，如下所示：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the listing is shown in the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出结果如下所示：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Singular value decomposition
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: When we decompose an integer into its prime factors, we can understand useful
    properties about the integer. Similarly, when we decompose a matrix, we can understand
    many functional properties that are not directly evident. There are two types
    of decomposition, namely eigenvalue decomposition and singular value decomposition.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将一个整数分解为它的质因数时，我们可以了解该整数的一些有用性质。同样，当我们分解一个矩阵时，我们也能理解许多并非直接显现的功能性属性。矩阵有两种分解方式，分别是特征值分解和奇异值分解。
- en: All real matrices have singular value decomposition, but the same is not true
    for Eigenvalue decomposition. For example, if a matrix is not square, the Eigen
    decomposition is not defined and we must use singular value decomposition instead.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实数矩阵都有奇异值分解，但特征值分解则并非所有矩阵都适用。例如，如果一个矩阵不是方阵，那么特征值分解就不可定义，我们必须使用奇异值分解。
- en: '**Singular Value Decomposition** (**SVD**) in mathematical form is the product
    of three matrices *U*, *S*, and *V*, where *U* is *m*r*, *S* is *r*r* and *V*
    is *r*n*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解** (**SVD**) 在数学形式上是三个矩阵 *U*、*S* 和 *V* 的乘积，其中 *U* 是 *m*r*，*S* 是 *r*r*，*V*
    是 *r*n*：'
- en: '![](img/301bd7b9-1332-435f-96c7-96a7d51f1373.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/301bd7b9-1332-435f-96c7-96a7d51f1373.jpg)'
- en: 'The following example shows SVD using a TensorFlow `svd` operation on textual
    data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用TensorFlow `svd`操作对文本数据进行SVD：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output of this is shown as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出如下所示：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the plot for the SVD of the preceding dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是前述数据集的SVD图示：
- en: '![](img/f312b60a-53ca-4019-b53b-800ec1019c19.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f312b60a-53ca-4019-b53b-800ec1019c19.png)'
- en: Eigenvalue decomposition
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征值分解
- en: Eigen decomposition is one of the most famous decomposition techniques in which
    we decompose a matrix into a set of eigenvectors and eigenvalues.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值分解是最著名的分解技术之一，我们将矩阵分解为一组特征向量和特征值。
- en: 'For a square matrix, Eigenvector is a vector *v* such that multiplication by
    *A* alters only the scale of *v*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于方阵，特征向量* v *是一个向量，使得乘以* A *后只改变* v *的尺度：
- en: '*Av = λv*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*Av = λv*'
- en: The scalar *λ* is known as the eigenvalue corresponding to this eigenvector.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 标量* λ *称为与此特征向量对应的特征值。
- en: 'Eigen decomposition of *A* is then given as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵* A *的特征值分解如下所示：
- en: '![](img/9fb009f4-4519-4f74-af40-922704ca0d3d.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fb009f4-4519-4f74-af40-922704ca0d3d.jpg)'
- en: Eigen decomposition of a matrix describes many useful details about the matrix.
    For example, the matrix is singular if, and only if, any of the eigenvalues are
    zero.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的特征值分解描述了许多关于矩阵的有用细节。例如，当且仅当任何特征值为零时，矩阵是奇异的。
- en: Principal Component Analysis
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '**Principal Component Analysis** (**PCA**) projects the given dataset onto
    a lower dimensional linear space so that the variance of the projected data is
    maximized. PCA requires the eigenvalues and eigenvectors of the covariance matrix,
    which is the product where *X* is the data matrix.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）将给定的数据集投影到一个低维线性空间，以使得投影数据的方差最大化。PCA需要协方差矩阵的特征值和特征向量，协方差矩阵是*
    X *的乘积，其中* X *是数据矩阵。'
- en: 'SVD on the data matrix *X* is given as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据矩阵* X *进行的SVD如下所示：
- en: '![](img/6384841b-c4e6-4684-8e8d-bd4f8848d008.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6384841b-c4e6-4684-8e8d-bd4f8848d008.jpg)'
- en: 'The following example shows PCA using SVD:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用SVD进行PCA：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of the listing is shown as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下所示：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Calculus
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微积分
- en: Topics in the previous sections are covered as part of standard linear algebra;
    something that wasn't covered is basic calculus. Despite the fact that the calculus
    that we use is relatively simple, the mathematical form of it may look very complex.
    In this section, we present some basic forms of matrix calculus with a few examples.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节中的主题涵盖了标准线性代数的内容；但未涉及的是基础微积分。尽管我们使用的微积分相对简单，但其数学形式可能看起来非常复杂。本节将介绍一些矩阵微积分的基本形式，并附有几个例子。
- en: Gradient
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度
- en: 'Gradient for functions with respect to a real-valued matrix *A* is defined
    as the matrix of partial derivatives of *A* and is denoted as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 函数关于实值矩阵* A *的梯度定义为* A *的偏导数矩阵，表示如下：
- en: '![](img/d459ff93-d4b1-45a4-98ec-09be902ff273.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d459ff93-d4b1-45a4-98ec-09be902ff273.jpg)'
- en: '![](img/57cf3a28-13d4-4e38-9564-c0625d794ef5.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57cf3a28-13d4-4e38-9564-c0625d794ef5.jpg)'
- en: TensorFlow does not do numerical differentiation; rather, it supports automatic
    differentiation. By specifying operations in a TensorFlow graph, it can automatically
    run the chain rule through the graph and, as it knows the derivatives of each
    operation we specify, it can combine them automatically.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow不进行数值微分，而是支持自动微分。通过在TensorFlow图中指定操作，它可以自动在图中运行链式法则，并且由于它知道我们指定的每个操作的导数，因此能够自动将它们组合起来。
- en: The following example shows training a network using MNIST data, the MNIST database
    consists of handwritten digits. It has a training set of 60,000 examples and a
    test set of 10,000 samples. The digits are size-normalized.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用MNIST数据训练网络，MNIST数据库由手写数字组成。它有一个包含60,000个样本的训练集和一个包含10,000个样本的测试集。数字已进行尺寸归一化。
- en: Here backpropagation is performed without any API usage and derivatives are
    calculated manually. We get 913 correct out of 1,000 tests. This concept will
    be introduced in the next chapter.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里执行了反向传播，没有使用任何API，并且导数是手动计算的。我们在1,000次测试中获得了913个正确结果。这个概念将在下一章介绍。
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now define a two-layered network with a nonlinear `sigmoid` function; a
    squared loss function is applied and optimized using a backward propagation algorithm,
    as shown in the following snippet:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一个具有非线性`sigmoid`函数的两层网络；应用平方损失函数并使用反向传播算法进行优化，如下所示：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of this is shown as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出如下所示：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, let's use automatic differentiation with TensorFlow. The following example
    demonstrates the use of `GradientDescentOptimizer`. We get 924 correct out of
    1,000 tests.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 TensorFlow 进行自动微分。以下示例展示了使用 `GradientDescentOptimizer`。我们在 1,000 次测试中得到了
    924 次正确的结果。
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of this is shown as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出如下所示：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following example shows linear regression using gradient descent:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用梯度下降的线性回归：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of this is shown as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出如下所示：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The plots are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图表如下：
- en: '![](img/733b0660-6367-48b9-bf7d-b597645e9649.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/733b0660-6367-48b9-bf7d-b597645e9649.png)'
- en: 'The following image shows the fitted line on testing data using the model:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用模型在测试数据上的拟合曲线：
- en: '![](img/e6782be0-fb03-4c47-9af1-2e5769266908.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6782be0-fb03-4c47-9af1-2e5769266908.png)'
- en: Hessian
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hessian
- en: 'Gradient is the first derivative for functions of vectors, whereas hessian
    is the second derivative. We will go through the notation now:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是向量函数的第一导数，而 Hessian 是第二导数。我们现在来讲解一下符号表示：
- en: '![](img/98c65983-8474-4932-8848-2cb3d21ea642.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98c65983-8474-4932-8848-2cb3d21ea642.jpg)'
- en: Similar to the gradient, the hessian is defined only when *f(x)* is real-valued.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与梯度类似，Hessian 仅在 *f(x)* 是实值时才定义。
- en: The algebraic function used is ![](img/21afe5dc-d4ba-4fbb-87c8-6d7819dfd8cb.jpg).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的代数函数是 ![](img/21afe5dc-d4ba-4fbb-87c8-6d7819dfd8cb.jpg)。
- en: 'The following example shows the hessian implementation using TensorFlow:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用 TensorFlow 实现 Hessian：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output of this is shown as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出如下所示：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Determinant
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行列式
- en: Determinant shows us information about the matrix that is helpful in linear
    equations and also helps in finding the inverse of a matrix.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 行列式为我们提供了关于矩阵的信息，这在求解线性方程和寻找矩阵的逆矩阵时非常有用。
- en: 'For a given matrix *X*, the determinant is shown as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的矩阵 *X*，行列式如下所示：
- en: '![](img/04084fab-c318-4f21-960f-c326eee557b0.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04084fab-c318-4f21-960f-c326eee557b0.jpg)'
- en: '![](img/ce5ff6f2-7d96-45d5-ab7a-c1e956c44634.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce5ff6f2-7d96-45d5-ab7a-c1e956c44634.jpg)'
- en: 'The following example shows how to get a determinant using TensorFlow:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 TensorFlow 获取行列式：
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of this is shown as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出如下所示：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Optimization
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: As part of deep learning, we mostly would like to optimize the value of a function
    that either minimizes or maximizes *f(x)* with respect to *x*. A few examples
    of optimization problems are least-squares, logistic regression, and support vector
    machines. Many of these techniques will get examined in detail in later chapters.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 作为深度学习的一部分，我们通常希望优化一个函数的值，使其最小化或最大化 *f(x)* 相对于 *x*。一些优化问题的例子包括最小二乘法、逻辑回归和支持向量机。许多这些技术将在后续章节中详细讨论。
- en: Optimizers
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器
- en: We will study `AdamOptimizer` here; TensorFlow `AdamOptimizer` uses Kingma and
    Ba's Adam algorithm to manage the learning rate. Adam has many advantages over
    the simple `GradientDescentOptimizer`. The first is that it uses moving averages
    of the parameters, which enables Adam to use a larger step size, and it will converge
    to this step size without any fine-tuning.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里研究 `AdamOptimizer`；TensorFlow `AdamOptimizer` 使用 Kingma 和 Ba 的 Adam 算法来管理学习率。Adam
    相较于简单的 `GradientDescentOptimizer` 有许多优点。首先，它使用参数的移动平均，这使得 Adam 可以使用较大的步长，并且能够在没有精细调整的情况下收敛到该步长。
- en: The disadvantage of Adam is that it requires more computation to be performed
    for each parameter in each training step. `GradientDescentOptimizer` can be used
    as well, but it would require more hyperparameter tuning before it would converge
    as quickly.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 的缺点是它需要为每个训练步骤中的每个参数执行更多的计算。`GradientDescentOptimizer` 也可以使用，但它需要更多的超参数调优，才能更快地收敛。
- en: 'The following example shows how to use `AdamOptimizer`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 `AdamOptimizer`：
- en: '`tf.train.Optimizer` creates an optimizer'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.train.Optimizer` 创建一个优化器'
- en: '`tf.train.Optimizer.minimize(loss, var_list)` adds the optimization operation
    to the computation graph'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.train.Optimizer.minimize(loss, var_list)` 将优化操作添加到计算图中'
- en: 'Here, automatic differentiation computes gradients without user input:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，自动微分计算梯度时不需要用户输入：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the scatter plot for the dataset:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集的散点图：
- en: '![](img/fd9bc39b-f5b5-481c-8932-8765e2538d70.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd9bc39b-f5b5-481c-8932-8765e2538d70.png)'
- en: 'This is the plot of the learned model on the data:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在数据上学习到的模型的图示：
- en: '![](img/c5c47a2e-cde3-4932-a685-86788ec140f0.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5c47a2e-cde3-4932-a685-86788ec140f0.png)'
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've introduced the mathematical concepts that are key to
    the understanding of neural networks and reviewed some maths associated with tensors.
    We also demonstrated how to perform mathematical operations within TensorFlow.
    We will repeatedly be applying these concepts in the following chapters.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了理解神经网络所需的数学概念，并回顾了与张量相关的一些数学内容。我们还展示了如何在 TensorFlow 中执行数学运算。在接下来的章节中，我们将反复应用这些概念。
