- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Essentials of NLP
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP基础
- en: Language has been a part of human evolution. The development of language allowed
    better communication between people and tribes. The evolution of written language,
    initially as cave paintings and later as characters, allowed information to be
    distilled, stored, and passed on from generation to generation. Some would even
    say that the hockey stick curve of advancement is because of the ever-accumulating
    cache of stored information. As this stored information trove becomes larger and
    larger, the need for computational methods to process and distill the data becomes
    more acute. In the past decade, a lot of advances were made in the areas of image
    and speech recognition. Advances in **Natural Language Processing** (**NLP**)
    are more recent, though computational methods for NLP have been an area of research
    for decades. Processing textual data requires many different building blocks upon
    which advanced models can be built. Some of these building blocks themselves can
    be quite challenging and advanced. This chapter and the next focus on these building
    blocks and the problems that can be solved with them through simple models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 语言一直是人类进化的一部分。语言的发展使得人们和部落之间的交流变得更加顺畅。书面语言的演变，最初是洞穴壁画，后来变成字符，使信息能够被提炼、存储并从一代传递到另一代。有些人甚至会说，技术进步的曲线之所以呈现曲棍球棒形态，是因为信息的累积存储不断增长。随着这个存储的信息库越来越大，使用计算方法处理和提炼数据的需求变得更加迫切。在过去的十年里，图像和语音识别领域取得了许多进展。**自然语言处理**（**NLP**）的进展则较为近期，尽管NLP的计算方法已经是几十年的研究领域。处理文本数据需要许多不同的构建模块，基于这些模块可以构建高级模型。这些构建模块本身可能相当具有挑战性和复杂性。本章和下一章将重点介绍这些构建模块以及通过简单模型可以解决的问题。
- en: 'In this chapter, we will focus on the basics of pre-processing text and build
    a simple spam detector. Specifically, we will learn about the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍文本预处理的基础知识，并构建一个简单的垃圾邮件检测器。具体来说，我们将学习以下内容：
- en: The typical text processing workflow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的文本处理工作流
- en: Data collection and labeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集与标注
- en: Text normalization, including case normalization, text tokenization, stemming,
    and lemmatization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本规范化，包括大小写规范化、文本分词、词干提取和词形还原
- en: Modeling datasets that have been text normalized
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模已进行文本规范化的数据集
- en: Vectorizing text
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本向量化
- en: Modeling datasets with vectorized text
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用向量化文本的建模数据集
- en: Let's start by getting to grips with the text processing workflow most NLP models use.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从掌握大多数NLP模型所使用的文本处理工作流开始。
- en: A typical text processing workflow
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个典型的文本处理工作流
- en: 'To understand how to process text, it is important to understand the general
    workflow for NLP. The following diagram illustrates the basic steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解如何处理文本，首先要了解NLP的通用工作流。下图说明了基本步骤：
- en: '![](img/B16252_01_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_01.png)'
- en: 'Figure 1.1: Typical stages of a text processing workflow'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：文本处理工作流的典型阶段
- en: The first two steps of the process in the preceding diagram involve collecting
    labeled data. A supervised model or even a semi-supervised model needs data to
    operate. The next step is usually normalizing and featurizing the data. Models
    have a hard time processing text data as is. There is a lot of hidden structure
    in a given text that needs to be processed and exposed. These two steps focus
    on that. The last step is building a model with the processed inputs. While NLP
    has some unique models, this chapter will use only a simple deep neural network
    and focus more on the normalization and vectorization/featurization. Often, the
    last three stages operate in a cycle, even though the diagram may give the impression
    of linearity. In industry, additional features require more effort to develop
    and more resources to keep running. Hence, it is important that features add value.
    Taking this approach, we will use a simple model to validate different normalization/vectorization/featurization
    steps. Now, let's look at each of these stages in detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图表的前两步涉及收集标注数据。监督模型甚至半监督模型需要数据来操作。接下来的步骤通常是对数据进行规范化和特征化处理。模型处理文本数据时往往很困难，文本中存在许多隐藏的结构，需要经过处理和揭示。这两步正是聚焦于此。最后一步是基于处理过的输入数据构建模型。尽管NLP有一些独特的模型，本章仅使用一个简单的深度神经网络，更多关注规范化和向量化/特征化。通常，最后三个阶段会在一个循环中操作，尽管图表可能给人线性进行的印象。在工业界，额外的特性需要更多的开发工作和资源来维持运行。因此，特性必须带来价值。采用这种方法，我们将使用一个简单的模型来验证不同的规范化/向量化/特征化步骤。现在，让我们详细看看这些阶段。
- en: Data collection and labeling
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集和标记
- en: The first step of any **Machine Learning** (**ML**) project is to obtain a dataset.
    Fortunately, in the text domain, there is plenty of data to be found. A common
    approach is to use libraries such as `scrapy` or Beautiful Soup to scrape data
    from the web. However, data is usually unlabeled, and as such can't be used in
    supervised models directly. This data is quite useful though. Through the use
    of transfer learning, a language model can be trained using unsupervised or semi-supervised
    methods and can be further used with a small training dataset specific to the
    task at hand. We will cover transfer learning in more depth in *Chapter 3*, *Named
    Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding*, when we look
    at transfer learning using BERT embeddings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）项目的第一步是获取数据集。在文本领域，幸运的是，可以找到大量的数据。一个常见的方法是使用诸如`scrapy`或Beautiful
    Soup之类的库从网络上爬取数据。然而，这些数据通常是未标记的，因此不能直接在监督模型中使用。尽管如此，这些数据非常有用。通过使用迁移学习，可以使用无监督或半监督的方法训练语言模型，并且可以进一步使用特定于手头任务的小训练数据集。在第3章中，*使用BiLSTMs、CRFs和Viterbi解码进行命名实体识别（NER）*，我们将更深入地讨论迁移学习使用BERT嵌入的问题。'
- en: In the labeling step, textual data sourced in the data collection step is labeled
    with the right classes. Let's take some examples. If the task is to build a spam
    classifier for emails, then the previous step would involve collecting lots of
    emails. This labeling step would be to attach a *spam* or *not spam* label to
    each email. Another example could be sentiment detection on tweets. The data collection
    step would involve gathering a number of tweets. This step would label each tweet
    with a label that acts as a ground truth. A more involved example would involve
    collecting news articles, where the labels would be summaries of the articles.
    Yet another example of such a case would be an email auto-reply functionality.
    Like the spam case, a number of emails with their replies would need to be collected.
    The labels in this case would be short pieces of text that would approximate replies.
    If you are working on a specific domain without much public data, you may have
    to do these steps yourself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记步骤中，从数据收集步骤获取的文本数据会被标记为正确的类别。让我们举几个例子。如果任务是构建一个用于电子邮件的垃圾邮件分类器，那么前一步将涉及收集大量的电子邮件。这个标记步骤将是给每封电子邮件附上一个*垃圾*或*非垃圾*的标签。另一个例子可能是在推特上进行情感检测。数据收集步骤将涉及收集一些推特。这一步将会给每条推特打上一个充当事实依据的标签。一个更复杂的例子可能涉及收集新闻文章，其中标签将是文章的摘要。另一个这种情况的例子可能是电子邮件自动回复功能。与垃圾邮件案例类似，需要收集一些带有回复的电子邮件。在这种情况下，标签将是一些短文本，用来近似回复。如果您在一个没有太多公开数据的特定领域工作，您可能需要自己完成这些步骤。
- en: Given that text data is generally available (outside of specific domains like
    health), labeling is usually the biggest challenge. It can be quite time consuming
    or resource intensive to label data. There has been a lot of recent focus on using
    semi-supervised approaches to labeling data. We will cover some methods for labeling
    data at scale using semi-supervised methods and the **snorkel** library in *Chapter
    7*, *Multi-modal Networks and Image Captioning with ResNets and Transformer*,
    when we look at weakly supervised learning for classification using Snorkel.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于文本数据通常是可用的（除了像健康这样的特定领域），标记通常是最大的挑战。标记数据可能非常耗时或资源密集。最近已经有很多关注使用半监督方法来标记数据。在第7章中，*多模态网络和使用ResNets和Transformer进行图像字幕生成*，我们将介绍使用半监督方法和**snorkel**库来大规模标记数据的一些方法。
- en: There is a number of commonly used datasets that are available on the web for
    use in training models. Using transfer learning, these generic datasets can be
    used to prime ML models and then you can use a small amount of domain-specific
    data to fine-tune the model. Using these publicly available datasets gives us
    a few advantages. First, all the data collection has been already performed. Second,
    labeling has already been done. Lastly, using such a dataset allows the comparison
    of results with the state of the art; most papers use specific datasets in their
    area of research and publish benchmarks. For example, the **Stanford Question
    Answering Dataset** (or **SQuAD** for short) is often used as a benchmark for
    question-answering models. It is a good source to train on as well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 网络上有许多常用的数据集可以用来训练模型。通过迁移学习，这些通用数据集可以用来为机器学习模型提供初步训练，然后你可以使用少量特定领域的数据来微调模型。使用这些公开可用的数据集为我们带来了一些优势。首先，所有的数据收集工作已经完成。其次，标签已经做好。最后，使用这些数据集可以与当前最先进的技术进行结果比较；大多数论文会在其研究领域使用特定的数据集，并发布基准测试。例如，**斯坦福问答数据集**（简称**SQuAD**）通常作为问答模型的基准。这也是一个很好的训练数据源。
- en: Collecting labeled data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集标注数据
- en: In this book, we will rely on publicly available datasets. The appropriate datasets
    will be called out in their respective chapters along with instructions on downloading
    them. To build a spam detection system on an email dataset, we will be using the
    SMS Spam Collection dataset made available by University of California, Irvine.
    This dataset can be downloaded using instructions available in the tip box below.
    Each SMS is tagged as "SPAM" or "HAM," with the latter indicating it is not a
    spam message.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将依赖公开的数据集。适当的数据集将在各自的章节中指出，并附带下载说明。为了构建一个垃圾邮件检测系统，我们将使用加利福尼亚大学欧文分校提供的SMS垃圾邮件数据集。这个数据集可以通过下面提示框中的说明进行下载。每条短信都被标记为“SPAM”或“HAM”，其中“HAM”表示这不是垃圾邮件。
- en: University of California, Irvine, is a great source of machine learning datasets.
    You can see all the datasets they provide by visiting [http://archive.ics.uci.edu/ml/datasets.php](http://archive.ics.uci.edu/ml/datasets.php).
    Specifically for NLP, you can see some publicly available datasets on [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学欧文分校是机器学习数据集的一个重要来源。你可以访问[http://archive.ics.uci.edu/ml/datasets.php](http://archive.ics.uci.edu/ml/datasets.php)查看他们提供的所有数据集。特别是对于自然语言处理（NLP），你可以在[https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets)找到一些公开的数据集。
- en: Before we start working with the data, the development environment needs to
    be set up. Let's take a quick moment to set up the development environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始处理数据之前，需要先设置开发环境。让我们快速设置开发环境。
- en: Development environment setup
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发环境设置
- en: 'In this chapter, we will be using Google Colaboratory, or Colab for short,
    to write code. You can use your Google account, or register a new account. Google
    Colab is free to use, requires no configuration, and also provides access to GPUs.
    The user interface is very similar to a Jupyter notebook, so it should seem familiar.
    To get started, please navigate to [colab.research.google.com](http://colab.research.google.com)
    using a supported web browser. A web page similar to the screenshot below should
    appear:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Google Colaboratory，简称Colab，来编写代码。你可以使用你的Google账户，或者注册一个新账户。Google
    Colab是免费的，不需要配置，同时也提供GPU访问。用户界面与Jupyter笔记本非常相似，因此应该很熟悉。要开始使用，请使用支持的浏览器访问[colab.research.google.com](http://colab.research.google.com)。页面应类似于下面的截图：
- en: '![](img/B16252_01_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_02.png)'
- en: 'Figure 1.2: Google Colab website'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：Google Colab网站
- en: The next step is to create a new notebook. There are a couple of options. The
    first option is to create a new notebook in Colab and type in the code as you
    go along in the chapter. The second option is to upload a notebook from the local
    drive into Colab. It is also possible to pull in notebooks from GitHub into Colab,
    the process for which is detailed on the Colab website. For the purposes of this
    chapter, a complete notebook named `SMS_Spam_Detection.ipynb` is available in
    the GitHub repository of the book in the `chapter1-nlp-essentials` folder. Please
    upload this notebook into Google Colab by clicking **File | Upload Notebook**.
    Specific sections of this notebook will be referred to at the appropriate points
    in the chapter in tip boxes. The instructions for creating the notebook from scratch
    are in the main description.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个新的笔记本。这里有几种选择。第一个选项是在 Colab 中创建一个新的笔记本，并按照章节中的步骤输入代码。第二个选项是将本地磁盘中的笔记本上传到
    Colab。也可以从 GitHub 拉取笔记本到 Colab，具体过程可以参考 Colab 网站。为了本章目的，书中的 GitHub 仓库中，`chapter1-nlp-essentials`
    文件夹下有一个完整的名为 `SMS_Spam_Detection.ipynb` 的笔记本。请通过点击**文件 | 上传笔记本**将该笔记本上传到 Google
    Colab。笔记本的特定部分将在章节的提示框中提到。创建笔记本的详细步骤已在主要描述中列出。
- en: Click on the **File** menu option at the top left and click on **New Notebook**.
    A new notebook will open in a new browser tab. Click on the notebook name at the
    top left, just above the **File** menu option, and edit it to read `SMS_Spam_Detection`.
    Now the development environment is set up. It is time to begin loading in data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 点击左上角的**文件**菜单选项，然后点击**新建笔记本**。一个新的笔记本将在新的浏览器标签页中打开。点击左上角的笔记本名称，位于**文件**菜单选项正上方，将其编辑为
    `SMS_Spam_Detection`。现在，开发环境已经设置完成，可以开始加载数据了。
- en: 'First, let us edit the first line of the notebook and import TensorFlow 2\.
    Enter the following code in the first cell and execute it:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编辑笔记本的第一行并导入 TensorFlow 2。请输入以下代码到第一个单元格并执行：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of running this cell should look like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此单元格后输出应如下所示：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This confirms that version 2.4.0 of the TensorFlow library was loaded. The highlighted
    line in the preceding code block is a magic command for Google Colab, instructing
    it to use TensorFlow version 2+. The next step is to download the data file and
    unzip to a location in the Colab notebook on the cloud.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这确认了 TensorFlow 库的 2.4.0 版本已加载。前面代码块中突出显示的那行是 Google Colab 的魔法命令，指示它使用 TensorFlow
    2+ 版本。下一步是下载数据文件并将其解压到 Colab 笔记本的云端位置。
- en: The code for loading the data is in the *Download Data* section of the notebook.
    Also note that as of writing, the release version of TensorFlow was 2.4.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据的代码位于笔记本的*下载数据*部分。还要注意，本文写作时，TensorFlow 的发布版本为 2.4。
- en: 'This can be done with the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码完成这一操作：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following output confirms that the data was downloaded and extracted:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出确认了数据已经下载并解压：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Reading the data file is trivial:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据文件非常简单：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The last line of code shows a sample line of data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行代码显示了一条数据示例：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This example is labeled as not spam. The next step is to split each line into
    two columns – one with the text of the message and the other as the label. While
    we are separating these labels, we will also convert the labels to numeric values.
    Since we are interested in predicting spam messages, we can assign a value of
    `1` to the spam messages. A value of `0` will be assigned to legitimate messages.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例标记为非垃圾邮件。下一步是将每一行分成两列——一列是消息的文本，另一列是标签。在分离这些标签的同时，我们还将标签转换为数值。由于我们要预测垃圾邮件，所以我们可以为垃圾邮件分配值
    `1`。合法邮件将分配值 `0`。
- en: The code for this part is in the *Pre-Process Data* section of the notebook.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分的代码在笔记本的*数据预处理*部分。
- en: 'Please note that the following code is verbose for clarity:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以下代码为清晰起见，做了详细描述：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now the dataset is ready for further processing in the pipeline. However, let's
    take a short detour to see how to configure GPU access in Google Colab.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据集已经准备好可以进一步在管道中处理了。不过，让我们稍作绕行，看看如何在 Google Colab 中配置 GPU 访问。
- en: Enabling GPUs on Google Colab
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Google Colab 上启用 GPU
- en: 'One of the advantages of using Google Colab is access to free GPUs for small
    tasks. GPUs make a big difference in the training time of NLP models, especially
    ones that use **Recurrent Neural Networks** (**RNNs**). The first step in enabling
    GPU access is to start a runtime, which can be done by executing a command in
    the notebook. Then, click on the **Runtime** menu option and select the **Change
    Runtime** option, as shown in the following screenshot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Google Colab 的一个优势是可以访问免费的 GPU 来处理小任务。GPU 对于 NLP 模型的训练时间影响很大，尤其是使用**递归神经网络**（**RNNs**）的模型。启用
    GPU 访问的第一步是启动一个运行时，这可以通过在笔记本中执行一个命令来完成。然后，点击 **Runtime** 菜单选项并选择 **Change Runtime**
    选项，如下图所示：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_03.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  描述自动生成](img/B16252_01_03.png)'
- en: 'Figure 1.3: Colab runtime settings menu option'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：Colab 运行时设置菜单选项
- en: 'Next, a dialog box will show up, as shown in the following screenshot. Expand
    the **Hardware Accelerator** option and select **GPU**:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将弹出一个对话框，如下图所示。展开 **Hardware Accelerator** 选项并选择 **GPU**：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  描述自动生成](img/B16252_01_04.png)'
- en: 'Figure 1.4: Enabling GPUs on Colab'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4：在 Colab 上启用 GPU
- en: Now you should have access to a GPU in your Colab notebook! In NLP models, especially
    when using RNNs, GPUs can shave a lot of minutes or hours off the training time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该可以在 Colab 笔记本中使用 GPU 了！在 NLP 模型中，特别是使用 RNN 时，GPU 可以大幅减少训练时间，节省数分钟或数小时。
- en: For now, let's turn our attention back to the data that has been loaded and
    is ready to be processed further for use in models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力重新集中在已经加载并准备好进一步处理的数据上，以便用于模型。
- en: Text normalization
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本归一化
- en: Text normalization is a pre-processing step aimed at improving the quality of
    the text and making it suitable for machines to process. Four main steps in text normalization
    are case normalization, tokenization and stop word removal, **Parts-of-Speech**
    (**POS**) tagging, and stemming.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 文本归一化是一个预处理步骤，旨在提高文本质量，使其适合机器处理。文本归一化的四个主要步骤包括大小写归一化、标记化和停用词移除、**词性**（**POS**）标注和词干提取。
- en: Case normalization applies to languages that use uppercase and lowercase letters.
    All languages based on the Latin alphabet or the Cyrillic alphabet (Russian, Mongolian,
    and so on) use upper- and lowercase letters. Other languages that sometimes use
    this are Greek, Armenian, Cherokee, and Coptic. In case normalization, all letters
    are converted to the same case. It is quite helpful in semantic use cases. However,
    in other cases, this may hinder performance. In the spam example, spam messages
    may have more words in all-caps compared to regular messages.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 大小写归一化适用于使用大写字母和小写字母的语言。所有基于拉丁字母或西里尔字母的语言（如俄语、蒙古语等）都使用大小写字母。其他偶尔使用这种方式的语言包括希腊语、亚美尼亚语、切罗基语和科普特语。在大小写归一化中，所有字母都会转换为相同的大小写。这在语义应用中非常有帮助。然而，在其他情况下，这可能会影响性能。在垃圾邮件示例中，垃圾邮件的消息通常包含更多的大写字母。
- en: Another common normalization step removes punctuation in the text. Again, this
    may or may not be useful given the problem at hand. In most cases, this should
    give good results. However, in some cases, such as spam or grammar models, it
    may hinder performance. It is more likely for spam messages to use more exclamation
    marks or other punctuation for emphasis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的归一化步骤是移除文本中的标点符号。同样，这可能对当前问题有用，也可能没有用。在大多数情况下，这应该会产生良好的结果。然而，在某些情况下，比如垃圾邮件或语法模型，它可能会影响性能。垃圾邮件更有可能使用更多的感叹号或其他标点符号来进行强调。
- en: The code for this part is in the *Data Normalization* section of the notebook.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分的代码在笔记本的 *Data Normalization* 部分。
- en: 'Let''s build a baseline model with three simple features:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来构建一个基准模型，使用三个简单的特征：
- en: Number of characters in the message
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息中的字符数量
- en: Number of capital letters in the message
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息中的大写字母数量
- en: Number of punctuation symbols in the message
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息中的标点符号数量
- en: 'To do so, first, we will convert the data into a `pandas` DataFrame:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，首先，我们将数据转换为一个`pandas` DataFrame：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, let''s build some simple functions that can count the length of the message,
    and the numbers of capital letters and punctuation symbols. Python''s regular
    expression package, `re`, will be used to implement these:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建一些简单的函数，计算消息的长度、首字母大写字母的数量和标点符号的数量。Python 的正则表达式包 `re` 将用于实现这些功能：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the `num_capitals()` function, substitutions are performed for the capital
    letters in English. The `count` of these substitutions provides the count of capital
    letters. The same technique is used to count the number of punctuation symbols.
    Please note that the method used to count capital letters is specific to English.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在`num_capitals()`函数中，针对英语中的大写字母进行替换操作。这些替换的`count`值即为大写字母的数量。相同的技术也用于统计标点符号的数量。请注意，计数大写字母的方法仅适用于英语。
- en: 'Additional feature columns will be added to the DataFrame, and then the set
    will be split into test and train sets:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将会向DataFrame添加额外的特征列，然后将数据集拆分为测试集和训练集：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This should generate the following output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该生成以下输出：
- en: '![](img/B16252_01_05.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_05.png)'
- en: 'Figure 1.5: Base dataset for initial spam model'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：初始垃圾邮件模型的基础数据集
- en: 'The following code can be used to split the dataset into training and test
    sets, with 80% of the records in the training set and the rest in the test set.
    Further more, labels will be removed from both the training and test sets:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可用于将数据集拆分为训练集和测试集，其中80%的记录用于训练集，剩余的用于测试集。此外，还会从训练集和测试集中删除标签：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we are ready to build a simple classifier to use this data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建一个简单的分类器来使用这些数据。
- en: Modeling normalized data
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模规范化数据
- en: Recall that modeling was the last part of the text processing pipeline described
    earlier. In this chapter, we will use a very simple model, as the objective is
    to show different basic NLP data processing techniques more than modeling. Here,
    we want to see if three simple features can aid in the classification of spam.
    As more features are added, passing them through the same model will help in seeing
    if the featurization aids or hampers the accuracy of the classification.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆建模是前面描述的文本处理流程的最后一步。在本章中，我们将使用一个非常简单的模型，因为我们的目标更多的是展示不同的基础NLP数据处理技术，而不是建模。在这里，我们想看看三个简单特征是否能帮助垃圾邮件的分类。随着更多特征的加入，通过相同的模型进行测试将帮助我们了解这些特征化是否有助于提高分类的准确性，或者是否会影响准确性。
- en: The *Model Building* section of the workbook has the code shown in this section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 工作簿的*模型构建*部分包含本节所示的代码。
- en: 'A function is defined that allows the construction of models with different
    numbers of inputs and hidden units:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个函数，允许构建具有不同输入和隐藏单元数量的模型：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This model uses binary cross-entropy for computing loss and the Adam optimizer
    for training. The key metric, given that this is a binary classification problem,
    is accuracy. The default parameters passed to the function are sufficient as only
    three features are being passed in.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用二元交叉熵计算损失，并使用Adam优化器进行训练。由于这是一个二分类问题，关键指标是准确率。传递给函数的默认参数足够，因为只传递了三个特征。
- en: 'We can train our simple baseline model with only three features like so:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以仅使用三个特征来训练我们的简单基线模型，如下所示：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is not bad as our three simple features help us get to 93% accuracy. A
    quick check shows that there are 592 spam messages in the test set, out of a total
    of 4,459\. So, this model is doing better than a very simple model that guesses
    everything as not spam. That model would have an accuracy of 87%. This number
    may be surprising but is fairly common in classification problems where there
    is a severe class imbalance in the data. Evaluating it on the training set gives
    an accuracy of around 93.4%:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不差，因为我们的三个简单特征帮助我们达到了93%的准确率。快速检查显示测试集中有592条垃圾邮件，所有邮件总数为4,459条。因此，这个模型比一个非常简单的模型要好，后者会将所有邮件都预测为非垃圾邮件。那个模型的准确率是87%。这个数字可能让人吃惊，但在数据中存在严重类别不平衡的分类问题中，这种情况是相当常见的。在训练集上的评估给出了大约93.4%的准确率：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Please note that the actual performance you see may be slightly different due
    to the data splits and computational vagaries. A quick verification can be performed
    by plotting the confusion matrix to see the performance:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您看到的实际性能可能会因为数据拆分和计算偏差而略有不同。可以通过绘制混淆矩阵来快速验证性能：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|  | Predicted Not Spam | Predicted Spam |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 预测为非垃圾邮件 | 预测为垃圾邮件 |'
- en: '| Actual Not Spam | 3,771 | 96 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 实际非垃圾邮件 | 3,771 | 96 |'
- en: '| Actual Spam | 186 | 406 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 实际垃圾邮件 | 186 | 406 |'
- en: This shows that 3,771 out of 3,867 regular messages were classified correctly,
    while 406 out of 592 spam messages were classified correctly. Again, you may get
    a slightly different result.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了3,771条常规消息中的3,867条被正确分类，而592条垃圾邮件中的406条被正确分类。同样，您可能会得到略有不同的结果。
- en: To test the value of the features, try re-running the model by removing one
    of the features, such as punctuation or a number of capital letters, to get a
    sense of their contribution to the model. This is left as an exercise for the
    reader.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试特征的价值，尝试通过去除某个特征（如标点符号或多个大写字母）重新运行模型，以了解这些特征对模型的贡献。这留给读者作为练习。
- en: Tokenization
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: 'This step takes a piece of text and converts it into a list of tokens. If the
    input is a sentence, then separating the words would be an example of tokenization.
    Depending on the model, different granularities can be chosen. At the lowest level,
    each character could become a token. In some cases, entire sentences of paragraphs
    can be considered as a token:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤将一段文本转换成一个标记的列表。如果输入的是句子，那么将句子分割成单词就是分词的一个例子。根据模型的不同，可以选择不同的粒度。在最低级别，每个字符都可以成为一个标记。在某些情况下，整句或段落可以作为一个标记：
- en: '![A close up of a logo  Description automatically generated](img/B16252_01_06.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![一个徽标的特写，描述自动生成](img/B16252_01_06.png)'
- en: 'Figure 1.6: Tokenizing a sentence'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：分词一个句子
- en: The preceding diagram shows two ways a sentence can be tokenized. One way to
    tokenize is to chop a sentence into words. Another way is to chop into individual
    characters. However, this can be a complex proposition in some languages such
    as Japanese and Mandarin.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示展示了两种分词方法。一种方法是将句子拆分成单词。另一种方法是将句子拆分成单独的字符。然而，在一些语言中，如日语和普通话，这可能是一个复杂的任务。
- en: Segmentation in Japanese
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日语中的分割
- en: Many languages use a word separator, a space, to separate words. This makes
    the task of tokenizing on words trivial. However, there are other languages that
    do not use any markers or separators between words. Some examples of such languages
    are Japanese and Chinese. In such languages, the task is referred to as *segmentation*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 许多语言使用空格作为词的分隔符，这使得基于词的分词任务变得简单。然而，也有一些语言不使用任何标记或分隔符来区分单词。例如，日语和中文就是这样的语言。在这些语言中，这一任务被称为*分割*。
- en: 'Specifically, in Japanese, there are mainly three different types of characters
    that are used: *Hiragana*, *Kanji*, and *Katakana*. Kanji is adapted from Chinese
    characters, and similar to Chinese, there are thousands of characters. Hiragana
    is used for grammatical elements and native Japanese words. Katakana is mostly
    used for foreign words and names. Depending on the preceding characters, a character
    may be part of an existing word or the start of a new word. This makes Japanese
    one of the most complicated writing systems in the world. Compound words are especially
    hard. Consider the following compound word that reads *Election Administration
    Committee*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在日语中，主要有三种不同类型的字符：*平假名*、*汉字*和*片假名*。汉字源自中文字符，和中文一样，汉字有成千上万的字符。平假名用于语法元素和日语本土词汇。片假名主要用于外来词和人名。根据前面的字符，某个字符可能是现有单词的一部分，或者是新词的开始。这使得日语成为世界上最复杂的书写系统之一。复合词尤其难。考虑以下复合词，表示*选举管理委员会*：
- en: '![](img/B16252_01_003.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_003.png)'
- en: 'This can be tokenized in two different ways, outside of the entire phrase being
    considered one word. Here are two examples of tokenizing (from the Sudachi library):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过两种不同的方式进行分词，除了将整个短语视为一个词。以下是使用 Sudachi 库进行分词的两个示例：
- en: '![](img/B16252_01_004.png) (Election / Administration / Committee)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_004.png) (选举 / 管理 / 委员会)'
- en: '![](img/B16252_01_005.png) (Election / Administration / Committee / Meeting)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_005.png) (选举 / 管理 / 委员会 / 会议)'
- en: Common libraries that are used specifically for Japanese segmentation or tokenization
    are MeCab, Juman, Sudachi, and Kuromoji. MeCab is used in Hugging Face, spaCy,
    and other libraries.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 专门用于日语分割或分词的常见库有 MeCab、Juman、Sudachi 和 Kuromoji。MeCab 被 Hugging Face、spaCy 和其他库使用。
- en: The code shown in this section is in the *Tokenization and Stop Word Removal*
    section of the notebook.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示的代码位于笔记本的*分词和停用词移除*部分。
- en: 'Fortunately, most languages are not as complex as Japanese and use spaces to
    separate words. In Python, splitting by spaces is trivial. Let''s take an example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数语言不像日语那样复杂，使用空格来分隔单词。在 Python 中，通过空格分割是非常简单的。让我们看一个例子：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding split operation results in the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的分割操作的输出结果如下：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The two highlighted lines in the preceding output show that the naïve approach
    in Python will result in punctuation being included in the words, among other
    issues. Consequently, this step is done through a library like StanfordNLP. Using
    `pip`, let''s install this package in our Colab notebook:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出中两个高亮的行显示，Python 中的简单方法会导致标点符号被包含在单词中，等等。因此，这一步是通过像 StanfordNLP 这样的库来完成的。使用
    `pip`，我们可以在 Colab 笔记本中安装此包：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The StanfordNLP package uses PyTorch under the hood as well as a number of
    other packages. These and other dependencies will be installed. By default, the
    package does not install language files. These have to be downloaded. This is
    shown in the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: StanfordNLP 包使用 PyTorch 底层支持，同时还使用了一些其他包。这些包和其他依赖项将会被安装。默认情况下，该包不会安装语言文件，这些文件需要手动下载。以下代码展示了这一过程：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The English file is approximately 235 MB. A prompt will be displayed to confirm
    the download and the location to store it in:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 英文文件大约为 235 MB。在下载前会显示提示，确认下载并选择存储位置：
- en: '![](img/B16252_01_07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_07.png)'
- en: 'Figure 1.7: Prompt for downloading English models'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：下载英文模型的提示
- en: Google Colab recycles the runtimes upon inactivity. This means that if you perform
    commands in the book at different times, you may have to re-execute every command
    again from the start, including downloading and processing the dataset, downloading
    the StanfordNLP English files, and so on. A local notebook server would usually
    maintain the state of the runtime but may have limited processing power. For simpler
    examples as in this chapter, Google Colab is a decent solution. For the more advanced
    examples later in the book, where training may run for hours or days, a local
    runtime or one running on a cloud **Virtual Machine** (**VM**) would be preferred.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 在不活动时会回收运行时。这意味着如果你在不同的时间执行书中的命令，你可能需要重新执行每个命令，从头开始，包括下载和处理数据集、下载
    StanfordNLP 的英文文件等等。一个本地的笔记本服务器通常会保持运行时的状态，但可能处理能力有限。对于本章中的简单示例，Google Colab 是一个不错的解决方案。对于书中后续的更高级示例，其中训练可能需要几个小时或几天，本地运行时或运行在云端的**虚拟机**（**VM**）会更合适。
- en: 'This package provides capabilities for tokenization, POS tagging, and lemmatization
    out of the box. To start with tokenization, we instantiate a pipeline and tokenize
    a sample text to see how this works:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包提供了开箱即用的分词、词性标注和词形还原功能。为了开始分词，我们实例化一个管道并对一个示例文本进行分词，看看这个过程是如何工作的：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `lang` parameter is used to indicate that an English pipeline is desired.
    The second parameter, `processors`, indicates the type of processing that is desired
    in the pipeline. This library can also perform the following processing steps
    in the pipeline:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`lang` 参数用于指示所需的英文管道。第二个参数 `processors` 指示管道中所需的处理类型。这个库还可以执行以下处理步骤：'
- en: '`pos` labels each token with a POS token. The next section provides more details
    on POS tags.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos` 为每个词元标注一个词性标签（POS）。下一部分将提供更多关于词性标签的细节。'
- en: '`lemma`, which can convert different forms of verbs, for example, to the base
    form. This will be covered in detail in the *Stemming and lemmatization* section
    later in this chapter.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lemma`，它可以将动词的不同形式转换为基本形式。例如，这将在本章后面的*词干提取与词形还原*部分中详细介绍。'
- en: '`depparse` performs dependency parsing between words in a sentence. Consider
    the following example sentence, "Hari went to school." *Hari* is interpreted as
    a noun by the POS tagger, and becomes the governor of the word *went*. The word
    *school* is dependent on *went* as it describes the object of the verb.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depparse` 执行句子中单词的依存句法分析。考虑以下示例句子：“Hari went to school。”*Hari* 被词性标注器解释为名词，并成为动词
    *went* 的主词。*school* 是 *went* 的依赖词，因为它描述了动词的宾语。'
- en: 'For now, only tokenization of text is desired, so only the tokenizer is used:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，只需要进行文本分词，所以只使用分词器：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This shows that the tokenizer correctly divided the text into two sentences.
    To investigate what words were removed, the following code can be used:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明分词器正确地将文本分成了两个句子。为了调查删除了哪些单词，可以使用以下代码：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note the highlighted words in the preceding output. Punctuation marks were separated
    out into their own words. Text was split into multiple sentences. This is an improvement
    over only using spaces to split. In some applications, removal of punctuation
    may be required. This will be covered in the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意上述输出中标记的单词。标点符号被分离为独立的单词。文本被拆分为多个句子。这比仅使用空格进行拆分有所改进。在某些应用中，可能需要删除标点符号。这个问题将在下一节中讲解。
- en: 'Consider the preceding example of Japanese. To see the performance of StanfordNLP
    on Japanese tokenization, the following piece of code can be used:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑上述日语示例。为了查看 StanfordNLP 在日语分词上的表现，可以使用以下代码片段：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is the first step, which involves downloading the Japanese language model,
    similar to the English model that was downloaded and installed previously. Next,
    a Japanese pipeline will be instantiated and the words will be processed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一步，涉及下载日语语言模型，类似于之前下载并安装的英语模型。接下来，将实例化一个日语处理管道，单词将会被处理：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You may recall that the Japanese text reads Election Administration Committee.
    Correct tokenization should produce three words, where first two should be two
    characters each, and the last word is three characters:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，日文文本的内容是“选举管理委员会”。正确的分词应该生成三个单词，其中前两个单词各包含两个字符，最后一个单词包含三个字符：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This matches the expected output. StanfordNLP supports 53 languages, so the
    same code can be used for tokenizing any language that is supported.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这与预期输出匹配。StanfordNLP 支持 53 种语言，因此相同的代码可以用于分词任何受支持的语言。
- en: Coming back to the spam detection example, a new feature can be implemented
    that counts the number of words in the message using this tokenization functionality.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 回到垃圾邮件检测的例子，可以实现一个新特征，使用此分词功能来计算消息中的单词数。
- en: This word count feature is implemented in the *Adding Word Count Feature* section
    of the notebook.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字数特征是在笔记本中的 *添加字数特征* 部分实现的。
- en: 'It is possible that spam messages have different numbers of words than regular
    messages. The first step is to define a method to compute the number of words:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 可能垃圾邮件的单词数量与常规邮件不同。第一步是定义一个方法来计算单词数：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, using the train and test splits, add a column for the word count feature:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用训练和测试数据集的划分，添加一个字数特征列：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The last line in the preceding code block creates a new model with four input
    features.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块中的最后一行创建了一个具有四个输入特征的新模型。
- en: '**PyTorch warning**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch 警告**'
- en: 'When you execute functions in the StanfordNLP library, you may see a warning
    like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 StanfordNLP 库中执行函数时，可能会看到如下警告：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Internally, StanfordNLP uses the PyTorch library. This warning is due to StanfordNLP
    using an older version of a function that is now deprecated. For all intents and
    purposes, this warning can be ignored. It is expected that maintainers of StanfordNLP
    will update their code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，StanfordNLP 使用了 PyTorch 库。这个警告是因为 StanfordNLP 使用了一个现在已经弃用的函数的旧版本。对于所有实际目的而言，这个警告可以忽略。预计
    StanfordNLP 的维护者会更新他们的代码。
- en: Modeling tokenized data
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对分词数据进行建模
- en: 'This model can be trained like so:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以通过以下方式进行训练：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'There is only a marginal improvement in accuracy. One hypothesis is that the
    number of words is not useful. It would be useful if the average number of words
    in spam messages were smaller or larger than regular messages. Using pandas, this
    can be quickly verified:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度的提高只是微乎其微。一种假设是，单词的数量没有太大用处。如果垃圾邮件的平均单词数比常规消息的单词数要小或大，那将是有用的。使用 pandas 可以快速验证这一点：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_08.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图 自动生成的描述](img/B16252_01_08.png)'
- en: 'Figure 1.8: Statistics for spam message features'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：垃圾邮件特征的统计数据
- en: 'Let''s compare the preceding results to the statistics for regular messages:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将上述结果与常规消息的统计数据进行比较：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_09.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图 自动生成的描述](img/B16252_01_09.png)'
- en: 'Figure 1.9: Statistics for regular message features'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：常规消息特征的统计数据
- en: Some interesting patterns can quickly be seen. Spam messages usually have much
    less deviation from the mean. Focus on the **Capitals** feature column. It shows
    that regular messages use far fewer capitals than spam messages. At the 75^(th)
    percentile, there are 3 capitals in a regular message versus 21 for spam messages.
    On average, regular messages have 4 capital letters while spam messages have 15\.
    This variation is much less pronounced in the number of words category. Regular
    messages have 17 words on average, while spam has 29\. At the 75^(th) percentile,
    regular messages have 22 words while spam messages have 35\. This quick check
    yields an indication as to why adding the word features wasn't that useful. However,
    there are a couple of things to consider still. First, the tokenization model
    split out punctuation marks as words. Ideally, these words should be removed from
    the word counts as the punctuation feature is showing that spam messages use a
    lot more punctuation characters. This will be covered in the *Parts-of-speech
    tagging* section. Secondly, languages have some common words that are usually
    excluded. This is called stop word removal and is the focus of the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有趣的模式可以迅速被发现。垃圾邮件通常与平均值的偏差较小。关注**大写字母**特征列。它显示正常邮件使用的首字母大写远少于垃圾邮件。在第75百分位，正常邮件有3个大写字母，而垃圾邮件有21个。平均而言，正常邮件有4个大写字母，而垃圾邮件有15个。在单词数量类别中，这种变化就不那么明显了。正常邮件平均有17个单词，而垃圾邮件有29个。在第75百分位，正常邮件有22个单词，而垃圾邮件有35个。这次快速检查表明，为什么添加单词特征没有那么有用。然而，仍然有几点需要考虑。首先，分词模型将标点符号分割成单词。理想情况下，这些标点符号应该从单词计数中移除，因为标点特征显示垃圾邮件使用了更多的标点符号字符。这将在*词性标注*部分中介绍。其次，语言中有一些常见的词汇通常会被排除。这被称为停用词移除，也是下一部分的重点。
- en: Stop word removal
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Stop word removal involves removing common words such as articles (the, an)
    and conjunctions (and, but), among others. In the context of information retrieval
    or search, these words would not be helpful in identifying documents or web pages
    that would match the query. As an example, consider the query "Where is Google
    based?". In this query, *is* is a stop word. The query would produce similar results
    irrespective of the inclusion of *is*. To determine the stop words, a simple approach
    is to use grammar clues.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词移除涉及去除常见的词汇，如冠词（the, an）和连接词（and, but）等。在信息检索或搜索的上下文中，这些词汇对于识别与查询匹配的文档或网页没有帮助。例如，考虑查询“Google的总部在哪里？”。在这个查询中，*is*是一个停用词。无论是否包括*is*，查询结果都会是相似的。为了确定停用词，一种简单的方法是使用语法线索。
- en: In English, articles and conjunctions are examples of classes of words that
    can usually be removed. A more robust way is to consider the frequency of occurrence
    of words in a corpus, set of documents, or text. The most frequent terms can be
    selected as candidates for the stop word list. It is recommended that this list
    be reviewed manually. There can be cases where words may be frequent in a collection
    of documents but are still meaningful. This can happen if all the documents in
    the collection are from a specific domain or on a specific topic. Consider a set
    of documents from the Federal Reserve. The word *economy* may appear quite frequently
    in this case; however, it is unlikely to be a candidate for removal as a stop
    word.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，冠词和连接词是通常可以移除的词类的例子。一种更强大的方法是考虑词汇在语料库、文档集或文本中的出现频率。最常出现的词汇可以被选为停用词列表的候选词。建议手动审查这个列表。有些情况下，某些词汇在文档集合中可能出现频繁，但仍然具有意义。如果所有文档来自特定领域或特定主题，这种情况可能会发生。考虑来自联邦储备的文档集。词汇*economy*在这种情况下可能会出现得非常频繁；然而，它不太可能作为停用词被移除。
- en: In some cases, stop words may actually contain information. This may be applicable
    to phrases. Consider the fragment "flights to Paris." In this case, *to* provides
    valuable information, and its removal may change the meaning of the fragment.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，停用词可能实际上包含有用的信息。这可能适用于短语。例如，考虑“飞往巴黎的航班”这个片段。在这种情况下，*to*提供了有价值的信息，移除它可能会改变这个片段的含义。
- en: Recall the stages of the text processing workflow. The step after text normalization
    is vectorization. This step is discussed in detail later in the *Vectorizing text*
    section of this chapter, but the key step in vectorization is to build a vocabulary
    or dictionary of all the tokens. The size of this vocabulary can be reduced by
    removing stop words. While training and evaluating models, removing stop words
    reduces the number of computation steps that need to be performed. Hence, the
    removal of stop words can yield benefits in terms of computation speed and storage
    space. Modern advances in NLP see smaller and smaller stop words lists as more
    efficient encoding schemes and computation methods evolve. Let's try and see the
    impact of stop words on the spam problem to develop some intuition about its usefulness.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾文本处理工作流的各个阶段。文本规范化后的步骤是向量化。这个步骤将在本章的*文本向量化*部分中详细讨论，但向量化的关键步骤是构建所有词元的词汇表或字典。通过移除停用词，可以减少词汇表的大小。在训练和评估模型时，去除停用词会减少需要执行的计算步骤。因此，去除停用词在计算速度和存储空间方面可以带来好处。现代NLP的进展使得停用词列表越来越小，随着更高效的编码方案和计算方法的发展，停用词列表也变得更加精简。让我们尝试并查看停用词对垃圾邮件问题的影响，以便更好地理解其有用性。
- en: Many NLP packages provide lists of stop words. These can be removed from the
    text after tokenization. Tokenization was done through the StanfordNLP library
    previously. However, this library does not come with a list of stop words. NLTK
    and spaCy supply stop words for a set of languages. For this example, we will
    use an open source package called `stopwordsiso`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 许多NLP包提供停用词的列表。可以在分词后从文本中移除这些停用词。分词之前是通过StanfordNLP库完成的。然而，这个库并没有提供停用词的列表。NLTK和spaCy为多种语言提供停用词。在这个例子中，我们将使用一个叫做`stopwordsiso`的开源包。
- en: The *Stop Word Removal* section of the notebook contains the code for this section.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的*停用词移除*部分包含了这一部分的代码。
- en: This Python package takes the list of stop words from the stopwords-iso GitHub
    project at [https://github.com/stopwords-iso/stopwords-iso](https://github.com/stopwords-iso/stopwords-iso).
    This package provides stop words in 57 languages. The first step is to install
    the Python package that provides access to the stop words lists.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Python包从[https://github.com/stopwords-iso/stopwords-iso](https://github.com/stopwords-iso/stopwords-iso)的stopwords-iso
    GitHub项目中获取停用词列表。该包提供了57种语言的停用词。第一步是安装提供停用词列表的Python包。
- en: 'The following command will install the package through the notebook:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将通过笔记本安装该包：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Supported languages can be checked with the following commands:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令检查支持的语言：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'English language stop words can be checked as well to get an idea of some of
    the words:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 英语的停用词也可以检查，以了解其中的一些词汇：
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Given that tokenization was already implemented in the preceding `word_counts()`
    method, the implementation of that method can be updated to include removing stop
    words. However, all the stop words are in lowercase. Case normalization was discussed
    earlier, and capital letters were a useful feature for spam detection. In this
    case, tokens need to be converted to lowercase to effectively remove them:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于分词已经在前面的`word_counts()`方法中实现，可以更新该方法的实现，加入移除停用词的步骤。然而，所有的停用词都是小写字母。之前讨论过的大小写规范化对垃圾邮件检测来说是一个有用的特征。在这种情况下，词元需要转换为小写字母才能有效地去除它们：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'A consequence of using stop words is that a message such as "When are you going
    to ride your bike?" counts as only 3 words. When we see if this has had any effect
    on the statistics for word length, the following picture emerges:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用停用词的一个后果是，像“你什么时候骑自行车？”这样的消息只算作3个词。当我们查看这是否对单词长度的统计数据产生了影响时，出现了以下情况：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_10.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_10.png)'
- en: 'Figure 1.10: Word counts for spam messages after removing stop words'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10：移除停用词后的垃圾邮件单词计数
- en: Compared to the word counts prior to stop word removal, the average number of
    words has been reduced from 29 to 18, almost a 30% decrease. The 25^(th) percentile
    changed from 26 to 14\. The maximum has also reduced from 49 to 33\.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与移除停用词之前的单词计数相比，单词的平均数量从29减少到18，几乎减少了30%。第25百分位数从26变化为14。最大值也从49减少到33。
- en: 'The impact on regular messages is even more dramatic:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对普通消息的影响更加显著：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_11.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_11.png)'
- en: 'Figure 1.11: Word counts for regular messages after removing stop words'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：去除停用词后的常规消息的单词计数
- en: Comparing these statistics to those from before stop word removal, the average
    number of words has more than halved to almost 8\. The maximum number of words
    has also reduced from 209 to 147\. The standard deviation of regular messages
    is about the same as its mean, indicating that there is a lot of variation in
    the number of words in regular messages. Now, let's see if this helps us train
    a model and improve its accuracy.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些统计数据与去除停用词前的数据进行比较，平均单词数已经减半，接近 8 个。最大单词数也从 209 减少到了 147。常规消息的标准差与其均值差不多，表明常规消息中单词数量变化较大。现在，让我们看看这是否有助于我们训练一个模型并提高其准确度。
- en: Modeling data with stop words removed
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 去除停用词后的数据建模
- en: 'Now that the feature without stop words is computed, it can be added to the
    model to see its impact:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，去除停用词后的特征已经计算出来，可以将其加入模型中，看看它的影响：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This accuracy reflects a slight improvement over the previous model:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这一准确度相较于之前的模型有了轻微的提升：
- en: '[PRE44]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In NLP, stop word removal used to be standard practice. In more modern applications,
    stop words may actually end up hindering performance in some use cases, rather
    than helping. It is becoming more common not to exclude stop words. Depending
    on the problem you are solving, stop word removal may or may not help.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，停用词去除曾经是标准做法。在现代应用中，停用词可能实际上会在某些使用场景中阻碍性能，而不是帮助提升。现在不排除停用词已经变得更加常见。根据你解决的问题，停用词去除可能会有帮助，也可能没有帮助。
- en: Note that StanfordNLP will separate words like *can't* into *ca* and *n't*.
    This represents the expansion of the short form into its constituents, *can* and
    *not*. These contractions may or may not appear in the stop word list. Implementing
    a more robust stop word detector is left to the reader as an exercise.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，StanfordNLP 会将像*can't*这样的词分开成*ca*和*n't*。这表示将缩写词扩展为其组成部分，即*can*和*not*。这些缩写词可能出现在停用词列表中，也可能不在。实现更健壮的停用词检测器是留给读者的一个练习。
- en: StanfordNLP uses a supervised RNN with **Bi-directional Long Short-Term Memory**
    (**BiLSTM**) units. This architecture uses a vocabulary to generate embeddings
    through the vectorization of the vocabulary. The vectorization and generation
    of embeddings is covered later in the chapter, in the *Vectorizing text* section.
    This architecture of BiLSTMs with embeddings is often a common starting point
    in NLP tasks. This will be covered and used in successive chapters in detail.
    This particular architecture for tokenization is considered the state of the art
    as of the time of writing this book. Prior to this, **Hidden Markov Model** (**HMM**)-based
    models were popular.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: StanfordNLP 使用带有 **双向长短期记忆**（**BiLSTM**）单元的监督 RNN。该架构使用词汇表通过词汇的向量化生成词嵌入。词汇的向量化和词嵌入的生成将在本章后面的*向量化文本*部分讲解。这种带有词嵌入的
    BiLSTM 架构通常是 NLP 任务的常见起点。后续章节中将详细讲解并使用该架构。这种特定的分词架构被认为是截至本书写作时的最先进技术。在此之前，基于**隐马尔可夫模型**（**HMM**）的模型曾经很流行。
- en: Depending on the languages in question, regular expression-based tokenization
    is also another approach. The NLTK library provides the Penn Treebank tokenizer
    based on regular expressions in a `sed` script. In future chapters, other tokenization
    or segmentation schemes such as **Byte Pair Encoding** (**BPE**) and WordPiece
    will be explained.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所涉及的语言，基于正则表达式的分词也是一种方法。NLTK 库提供了基于正则表达式的 Penn Treebank 分词器，该分词器使用 `sed` 脚本实现。在后续章节中，将会解释其他分词或分段方案，如**字节对编码**（**BPE**）和
    WordPiece。
- en: The next task in text normalization is to understand the structure of a text
    through POS tagging.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标准化的下一个任务是通过词性标注理解文本的结构。
- en: Part-of-speech tagging
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词性标注
- en: 'Languages have a grammatical structure. In most languages, words can be categorized
    primarily into verbs, adverbs, nouns, and adjectives. The objective of this part
    of the processing step is to take a piece of text and tag each word token with
    a POS identifier. Note that this makes sense only in the case of word-level tokens.
    Commonly, the Penn Treebank POS tagger is used by libraries including StanfordNLP
    to tag words. By convention, POS tags are added by using a code after the word,
    separated by a slash. As an example, `NNS` is the tag for a plural noun. If the
    words `goats` was encountered, it would be represented as `goats/NNS`. In the
    StandfordNLP library, **Universal POS** (**UPOS**) tags are used. The following
    tags are part of the UPOS tag set. More details on mapping of standard POS tags
    to UPOS tags can be seen at [https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html](https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html).
    The following is a table of the most common tags:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 语言具有语法结构。在大多数语言中，单词可以主要分为动词、副词、名词和形容词。这个处理步骤的目标是对一段文本中的每个单词进行词性标注。请注意，这只有在处理词级标记时才有意义。常见的，Penn
    Treebank 词性标注器被包括 StanfordNLP 在内的库用来标注单词。根据惯例，词性标注通过在单词后加上一个斜杠后缀的代码来添加。例如，`NNS`
    是复数名词的标记。如果遇到 `goats` 这个词，它会被表示为 `goats/NNS`。在 StanfordNLP 库中，使用的是 **通用词性** (**UPOS**)
    标记。以下是 UPOS 标记集的一部分。有关标准词性标记与 UPOS 标记映射的更多细节，请参见 [https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html](https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html)。以下是最常见的标记表：
- en: '| Tag | Class | Examples |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 类别 | 示例 |'
- en: '| ADJ | **Adjective**: Usually describes a noun. Separate tags are used for
    comparatives and superlatives. | Great, pretty |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ADJ | **形容词**：通常描述名词。比较级和最高级使用不同的标记。 | 很棒，漂亮 |'
- en: '| ADP | **Adposition**: Used to modify an object such as a noun, pronoun, or
    phrase; for example, "Walk **up** the stairs." Some languages like English use
    prepositions while others such as Hindi and Japanese use postpositions. | Up,
    inside |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ADP | **介词**：用来修饰名词、代词或短语等对象；例如，“**上**楼”。像英语这样的语言使用介词，而像印地语和日语等语言使用后置词。 |
    上，里面 |'
- en: '| ADV | **Adverb**: A word or phrase that modifies or qualifies an adjective,
    verb, or another adverb. | Loudly, often |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| ADV | **副词**：修饰或限定形容词、动词或其他副词的词或短语。 | 大声地，经常 |'
- en: '| AUX | **Auxiliary verb**: Used in forming mood, voice, or tenses of other
    verbs. | Will, can, may |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| AUX | **助动词**：用来构成其他动词的语气、语态或时态。 | 将，会，可能 |'
- en: '| CCONJ | **Co-ordinating conjunction**: Joins two phrases, clauses, or sentences.
    | And, but, that |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| CCONJ | **并列连词**：连接两个短语、从句或句子。 | 和，但，那 |'
- en: '| INTJ | **Interjection**: An exclamation, interruption, or sudden remark.
    | Oh, uh, lol |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| INTJ | **感叹词**：表达惊讶、打断或突然的评论。 | 哦，呃，哈哈 |'
- en: '| NOUN | **Noun**: Identifies people, places, or things. | Office, book |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| NOUN | **名词**：指代人、地点或事物。 | 办公室，书 |'
- en: '| NUM | **Numeral**: Represents a quantity. | Six, nine |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| NUM | **数词**：表示数量。 | 六，九 |'
- en: '| DET | **Determiner**: Identifies a specific noun, usually as a singular.
    | A, an, the |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| DET | **限定词**：确定特定的名词，通常是单数形式。 | 一个，某个，那个 |'
- en: '| PART | **Particle**: Parts of speech outside of the main types. | To, n''t
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| PART | **助词**：属于主要词性之外的词类。 | 到，不能 |'
- en: '| PRON | **Pronoun**: Substitutes for other nouns, especially proper nouns.
    | She, her |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| PRON | **代词**：替代其他名词，尤其是专有名词。 | 她，她的 |'
- en: '| PROPN | **Proper noun**: A name for a specific person, place, or thing. |
    Gandhi, US |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| PROPN | **专有名词**：指代特定的人员、地点或事物的名称。 | 甘地，美国 |'
- en: '| PUNCT | Different punctuation symbols. | , ? / |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| PUNCT | 不同的标点符号。 | ，？ / |'
- en: '| SCONJ | **Subordinating conjunction**: Connects independent clause to a dependent
    clause. | Because, while |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| SCONJ | **从属连词**：连接独立从句和依赖从句。 | 因为，虽然 |'
- en: '| SYM | Symbols including currency signs, emojis, and so on. | $, #, % :) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| SYM | 符号，包括货币符号、表情符号等。 | $，#，% :) |'
- en: '| VERB | **Verb**: Denotes action or occurrence. | Go, do |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| VERB | **动词**：表示动作或发生的事情。 | 去，做 |'
- en: '| X | **Other**: That which cannot be classified elsewhere. | Etc, 4\. (a numbered
    list bullet) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| X | **其他**：无法归类到其他类别的内容。 | 等等，4.（编号列表项） |'
- en: 'The best way to understand how POS tagging works is to try it out:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的理解词性标注工作原理的方法是尝试一下：
- en: The code for this section is in the *POS Based Features* section of the notebook.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分的代码位于笔记本的 *POS 基础特征* 部分。
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code instantiates an English pipeline and processes a sample
    piece of text. The next piece of code is a reusable function to print back the
    sentence tokens with the POS tags:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码实例化了一个英语处理流程并处理了一段示例文本。下一段代码是一个可重用的函数，用于打印带有词性标记的句子标记：
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This method can be used to investigate the tagging for the preceding example
    sentence:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法可以用来调查前面示例句子的标记：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Most of these tags would make sense, though there may be some inaccuracies.
    For example, the word *lookin* is miscategorized as a noun. Neither StanfordNLP,
    nor a model from another package, will be perfect. This is something that we have
    to account for in building models using such features. There are a couple of different
    features that can be built using these POS. First, we can update the `word_counts()`
    method to exclude the punctuation from the count of words. The current method
    is unaware of the punctuation when it counts the words. Additional features can
    be created that look at the proportion of different types of grammatical elements
    in the messages. Note that so far, all features are based on the structure of
    the text, and not on the content itself. Working with content features will be
    covered in more detail as this book continues.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分词性标记是合理的，尽管可能存在一些不准确之处。例如，单词*lookin*被错误地标记为名词。无论是StanfordNLP，还是其他包中的模型，都不可能做到完美。这是我们在构建使用这些特征的模型时必须考虑到的。可以通过这些词性标记构建几种不同的特征。首先，我们可以更新`word_counts()`方法，排除标点符号在单词计数中的影响。当前方法在计算单词时未考虑标点符号。还可以创建其他特征，分析消息中不同类型语法元素的比例。需要注意的是，到目前为止，所有的特征都是基于文本结构，而非内容本身。关于内容特征的工作将在本书的后续部分详细讲解。
- en: 'As a next step, let''s update the `word_counts()` method and add a feature
    to show the percentages of symbols and punctuation in a message – with the hypothesis
    that maybe spam messages use more punctuation and symbols. Other features around
    types of different grammatical elements can also be built. These are left to you
    to implement. Our `word_counts()` method is updated as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，让我们更新`word_counts()`方法，添加一个特征，用于显示消息中符号和标点符号的百分比——假设垃圾邮件消息可能使用更多标点符号和符号。还可以构建关于不同语法元素类型的其他特征。这些留给你自己实现。我们更新后的`word_counts()`方法如下：
- en: '[PRE50]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This function is a little different compared to the previous one. Since there
    are multiple computations that need to be performed on the message in each row,
    these operations are combined and a `Series` object with column labels is returned.
    This can be merged with the main DataFrame like so:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数与之前的稍有不同。由于每一行的消息需要执行多个计算，这些操作被组合起来并返回一个带有列标签的`Series`对象。这样可以像下面这样与主数据框合并：
- en: '[PRE51]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'A similar process can be performed on the test set:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上可以执行类似的过程：
- en: '[PRE52]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'A quick check of the statistics for spam and non-spam messages in the training
    set shows the following, first for non-spam messages:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对训练集中垃圾邮件和非垃圾邮件消息的统计进行快速检查，首先是非垃圾邮件消息：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_12.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  自动生成的描述](img/B16252_01_12.png)'
- en: 'Figure 1.12: Statistics for regular messages after using POS tags'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：使用词性标记后的常规消息统计
- en: 'And then for spam messages:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是垃圾邮件消息：
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_13.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  自动生成的描述](img/B16252_01_13.png)'
- en: 'Figure 1.13: Statistics for spam messages after using POS tags'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13：使用词性标记后的垃圾邮件消息统计
- en: In general, word counts have been reduced even further after stop word removal.
    Further more, the new `Punct` feature computes the ratio of punctuation tokens
    in a message relative to the total tokens. Now we can build a model with this
    data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在去除停用词后，单词计数进一步减少。此外，新的`Punct`特征计算了消息中标点符号令牌相对于总令牌的比例。现在我们可以用这些数据构建一个模型。
- en: Modeling data with POS tagging
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用词性标记进行数据建模
- en: 'Plugging these features into the model, the following results are obtained:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些特征插入模型后，得到以下结果：
- en: '[PRE55]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The accuracy shows a slight increase and is now up to 94.66%. Upon testing,
    it seems to hold:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率略有提高，目前达到了94.66%。测试结果显示，它似乎成立：
- en: '[PRE57]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The final part of text normalization is stemming and lemmatization. Though we
    will not be building any features for the spam model using this, it can be quite
    useful in other cases.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 文本规范化的最后一步是词干提取和词形还原。虽然我们不会使用此方法为垃圾邮件模型构建任何特征，但在其他情况下它非常有用。
- en: Stemming and lemmatization
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: 'In certain languages, the same word can take a slightly different form depending
    on its usage. Consider the word *depend* itself. The following are all valid forms
    of the word *depend*: *depends*, *depending*, *depended*, *dependent*. Often,
    these variations are due to tenses. In some languages like Hindi, verbs may have
    different forms for different genders. Another case is derivatives of the same
    word such as *sympathy*, *sympathetic*, *sympathize*, and *sympathizer*. These
    variations can take different forms in other languages. In Russian, proper nouns
    take different forms based on usage. Suppose there is a document talking about
    London (Лондон). The phrase *in London* (в Лондоне) spells *London* differently
    than *from London* (из Лондона). These variations in the spelling of *London*
    can cause issues when matching some input to sections or words in a document.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些语言中，相同的词可以根据其用法呈现略有不同的形式。以单词*depend*为例，以下都是有效的*depend*形式：*depends*、*depending*、*depended*、*dependent*。这些变化通常与时态相关。在一些语言（如印地语）中，动词可能会根据不同的性别而变化。另一个例子是同一词汇的派生词，如*sympathy*、*sympathetic*、*sympathize*和*sympathizer*。这些变化在其他语言中可能有不同的形式。在俄语中，专有名词会根据用法变化其形式。假设有一篇文档讨论伦敦（Лондон）。短语*in
    London*（в Лондоне）中的*London*拼写方式与*from London*（из Лондона）中的*London*不同。这些*London*拼写上的变化可能会在将输入与文档中的某些部分或单词匹配时产生问题。
- en: When processing and tokenizing text to construct a vocabulary of words appearing
    in the corpora, the ability to identify the root word can reduce the size of the
    vocabulary while expanding the accuracy of matches. In the preceding Russian example,
    any form of the word London can be matched to any other form if all the forms
    are normalized to a common representation post-tokenization. This process of normalization
    is called stemming or lemmatization.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理和标记文本以构建语料库中出现的单词词汇表时，识别根词的能力可以减少词汇表的大小，同时提高匹配的准确性。在前面的俄语示例中，如果所有形式的单词都在标记化后归一化为统一的表示形式，那么*London*的任何形式都可以与其他形式匹配。这个归一化过程被称为词干提取或词形还原。
- en: Stemming and lemmatization differ in their approach and sophistication but serve
    the same objective. Stemming is a simpler, heuristic rule-based approach that
    chops off the affixes of words. The most famous stemmer is called the Porter stemmer,
    published by Martin Porter in 1980\. The official website is [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/),
    where various versions of the algorithm implemented in various languages are linked.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取和词形还原在方法和复杂性上有所不同，但服务于相同的目标。词干提取是一种更简单的启发式基于规则的方法，它去除单词的词缀。最著名的词干提取器叫做Porter词干提取器，由Martin
    Porter于1980年发布。官方网站是[https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)，这里链接了用不同语言实现的算法的各种版本。
- en: 'This stemmer only works for English and has rules including removing *s* at
    the end of the words for plurals, and removing endings such as *-ed* or *-ing*.
    Consider the following sentence:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这个词干提取器仅适用于英语，并有规则，包括移除名词复数形式的* s *，以及移除像* -ed *或* -ing *的词尾。考虑以下句子：
- en: '"Stemming is aimed at reducing vocabulary and aid understanding of morphological
    processes. This helps people understand the morphology of words and reduce size
    of corpus."'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: “词干提取的目的是减少词汇量并帮助理解形态学过程。这有助于人们理解单词的形态，并减少语料库的大小。”
- en: 'After stemming using Porter''s algorithm, this sentence will be reduced to
    the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Porter算法进行词干提取后，这个句子将被简化为以下内容：
- en: '"Stem is aim at reduce vocabulari and aid understand of morpholog process .
    Thi help peopl understand the morpholog of word and reduc size of corpu ."'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: “Stem is aim at reduce vocabulari and aid understand of morpholog process .
    Thi help peopl understand the morpholog of word and reduc size of corpu .”
- en: Note how different forms of *morphology*, *understand*, and *reduce* are all
    tokenized to the same form.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*morphology*、*understand*和*reduce*的不同形式都被标记化为相同的形式。
- en: Lemmatization approaches this task in a more sophisticated manner, using vocabularies
    and morphological analysis of words. In the study of linguistics, a morpheme is
    a unit smaller than or equal to a word. When a morpheme is a word in itself, it
    is called a root or a free morpheme. Conversely, every word can be decomposed
    into one or more morphemes. The study of morphemes is called morphology. Using
    this morphological information, a word's root form can be returned post-tokenization.
    This base or dictionary form of the word is called a *lemma*, hence the process
    is called lemmatization. StanfordNLP includes lemmatization as part of processing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 词元化以更复杂的方式处理这个任务，利用词汇和单词的形态学分析。在语言学研究中，形态素是小于或等于一个单词的单位。当形态素本身就是一个单词时，它被称为根或自由形态素。相反，每个单词都可以分解为一个或多个形态素。形态素的研究被称为形态学。利用这些形态学信息，可以在分词后返回单词的根形式。这个单词的基础或字典形式被称为*词元*，因此这个过程被称为词元化。StanfordNLP将词元化作为处理的一部分。
- en: The *Lemmatization* section of the notebook has the code shown here.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的*词元化*部分展示了这里的代码。
- en: 'Here is a simple piece of code to take the preceding sentences and parse them:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的代码片段，用于处理前述句子并解析它们：
- en: '[PRE59]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: After processing, we can iterate through the tokens to get the lemma of each
    word. This is shown in the following code fragment. The lemma of a word is exposed
    as the `.lemma` property of each word inside a token. For the sake of brevity
    of code, a simplifying assumption is made here that each token has only one word.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后，我们可以通过迭代令牌来获取每个单词的词元。如下代码片段所示，单词的词元以每个令牌内单词的`.lemma`属性呈现。为了简化代码，这里做了一个简化假设，即每个令牌只有一个单词。
- en: 'The POS for each word is also printed out to help us understand how the process
    was performed. Some key words in the following output are highlighted:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词的词性（POS）也会被打印出来，帮助我们理解过程是如何进行的。以下输出中一些关键字已被突出显示：
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Compare this output to the output of the Porter stemmer earlier. One immediate
    thing to notice is that lemmas are actual words as opposed to fragments, as was
    the case with the Porter stemmer. In the case of *reduce*, the usage in both sentences
    is in the form of a verb, so the choice of lemma is consistent. Focus on the words
    *understand* and *understanding* in the preceding output. As the POS tag shows,
    it is used in two different forms. Consequently, it is not reduced to the same
    lemma. This is different from the Porter stemmer. The same behavior can be observed
    for *morphology* and *morphological*. This is a quite sophisticated behavior.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 将此输出与之前的Porter词干提取器的输出进行对比。一个立即可以注意到的点是，词元是真正的单词，而不像Porter词干提取器中那样是碎片。在*reduce*的情况下，两句话中的用法都是动词形式，因此词元选择是一致的。重点关注前面输出中的*understand*和*understanding*。如POS标签所示，它们分别是两种不同的形式。因此，它们没有还原为相同的词元。这与Porter词干提取器不同。对于*morphology*和*morphological*，也可以观察到相同的行为。这是一种非常复杂的行为。
- en: Now that text normalization is completed, we can begin the vectorization of
    text.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文本标准化已完成，我们可以开始文本的向量化处理。
- en: Vectorizing text
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本向量化
- en: While building models for the SMS message spam detection thus far, only aggregate
    features based on counts or distributions of lexical or grammatical features have
    been considered. The actual words in the messages have not been used thus far.
    There are a couple of challenges in using the text content of messages. The first
    is that text can be of arbitrary lengths. Comparing this to image data, we know
    that each image has a fixed width and height. Even if the corpus of images has
    a mixture of sizes, images can be resized to a common size with minimal loss of
    information by using a variety of compression mechanisms. In NLP, this is a bigger
    problem compared to computer vision. A common approach to handle this is to truncate
    the text. We will see various ways to handle variable-length texts in various
    examples throughout the book.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在迄今为止构建短信垃圾信息检测模型的过程中，仅考虑了基于词汇或语法特征计数或分布的聚合特征。至今为止，消息中的实际单词尚未被使用。使用消息的文本内容有几个挑战。首先，文本的长度是任意的。与图像数据相比，我们知道每张图像都有固定的宽度和高度。即使图像集合包含不同尺寸的图像，通过使用各种压缩机制，图像可以被调整为统一大小且信息损失最小。而在自然语言处理（NLP）中，这比计算机视觉要复杂得多。处理这个问题的一种常见方法是截断文本。我们将在本书中的不同示例中看到处理可变长度文本的各种方法。
- en: The second issue is that of the representation of words with a numerical quantity
    or feature. In computer vision, the smallest unit is a pixel. Each pixel has a
    set of numerical values indicating color or intensity. In a text, the smallest
    unit could be a word. Aggregating the Unicode values of the characters does not
    convey or embody the meaning of the word. In fact, these character codes embody
    no information at all about the character, such as its prevalence, whether it
    is a consonant or a vowel, and so on. However, averaging the pixels in a section
    of an image could be a reasonable approximation of that region of the image. It
    may represent how that region would look if seen from a large distance. A core
    problem then is to construct a numerical representation of words. Vectorization
    is the process of converting a word to a vector of numbers that embodies the information
    contained in the word. Depending on the vectorization technique, this vector may
    have additional properties that may allow comparison with other words, as will
    be shown in the *Word vectors* section later in this chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是如何用数值或特征来表示单词。在计算机视觉中，最小的单位是像素。每个像素都有一组数值表示颜色或强度。在文本中，最小的单位可能是单词。将字符的Unicode值聚合并不能传达或体现单词的意义。实际上，这些字符编码并不包含关于字符的任何信息，比如它的普遍性、它是辅音还是元音，等等。然而，平均化图像某一部分的像素值可能是对该区域的合理近似。这可能代表从远距离看到该区域时的样子。那么，一个核心问题就是构建单词的数值表示。向量化是将单词转换为数值向量的过程，目的是体现单词所包含的信息。根据不同的向量化技术，这些向量可能有额外的特性，使得它们可以与其他单词进行比较，正如本章后面*词向量*部分所展示的那样。
- en: The simplest approach for vectorizing is to use counts of words. The second
    approach is more sophisticated, with its origins in information retrieval, and
    is called TF-IDF. The third approach is relatively new, having been published
    in 2013, and uses RNNs to generate embeddings or word vectors. This method is
    called Word2Vec. The newest method in this area as of the time of writing was
    BERT, which came out in the last quarter of 2018\. The first three methods will
    be discussed in this chapter. BERT will be discussed in detail in *Chapter 3*,
    *Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding.*
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的向量化方法是使用单词计数。第二种方法更为复杂，起源于信息检索，称为TF-IDF。第三种方法相对较新，发表于2013年，使用RNN生成嵌入或单词向量。此方法称为Word2Vec。截止到目前为止，最先进的方法是BERT，它于2018年最后一个季度发布。本章将讨论前三种方法，BERT将在*第3章*中详细讨论，*命名实体识别（NER）与BiLSTMs、CRFs及维特比解码*。
- en: Count-based vectorization
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于计数的向量化
- en: 'The idea behind count-based vectorization is really simple. Each unique word
    appearing in the corpus is assigned a column in the vocabulary. Each document,
    which would correspond to individual messages in the spam example, is assigned
    a row. The counts of the words appearing in that document are entered in the relevant
    cell corresponding to the document and the word. With `n` unique documents containing
    `m` unique words, this results in a matrix of `n` rows by `m` columns. Consider
    a corpus like so:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计数的向量化思想其实很简单。语料库中每个唯一出现的单词都会被分配一个词汇表中的列。每个文档（对应垃圾邮件示例中的个别信息）都会分配一行。文档中出现的单词的计数会填写在对应文档和单词的单元格中。如果有`n`个独特的文档包含`m`个独特的单词，那么就会得到一个`n`行`m`列的矩阵。假设有如下的语料库：
- en: '[PRE62]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: There are three documents in this corpus of text. The `scikit-learn` (`sklearn`)
    library provides methods for undertaking count-based vectorization.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语料库中有三个文档。`scikit-learn`（`sklearn`）库提供了进行基于计数的向量化的方法。
- en: Modeling after count-based vectorization
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模拟基于计数的向量化
- en: 'In Google Colab, this library should already be installed. If it is not installed
    in your Python environment, it can be installed via the notebook like so:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Colab中，该库应该已经安装。如果在你的Python环境中没有安装，可以通过笔记本以如下方式安装：
- en: '[PRE63]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The `CountVectorizer` class provides a built-in tokenizer that separates the
    tokens of two or more characters in length. This class takes a variety of options
    including a custom tokenizer, a stop word list, the option to convert characters
    to lowercase prior to tokenization, and a binary mode that converts every positive
    count to 1\. The defaults provide a reasonable choice for an English language
    corpus:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 类提供了一个内置的分词器，用于分隔长度大于或等于两个字符的标记。这个类提供了多种选项，包括自定义分词器、停用词列表、将字符转换为小写字母后再进行分词的选项，以及一个二进制模式，将每个正向计数转换为1。默认选项对于英语语料库来说提供了一个合理的选择：'
- en: '[PRE64]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In the preceding code, a model is fit to the corpus. The last line prints out
    the tokens that are used as columns. The full matrix can be seen as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，模型被拟合到语料库上。最后一行输出的是作为列使用的标记。完整的矩阵可以如下所示：
- en: '[PRE66]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This process has now converted a sentence such as "I like fruits. Fruits like
    bananas" into a vector (`0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0`). This is an example
    of **context-free** vectorization. Context-free refers to the fact that the order
    of the words in the document did not make any difference in the generation of
    the vector. This is merely counting the instances of the words in a document.
    Consequently, words with multiple meanings may be grouped into one, for example,
    *bank*. This may refer to a place near the river or a place to keep money. However,
    it does provide a method to compare documents and derive similarity. The cosine
    similarity or distance can be computed between two documents, to see which documents
    are similar to which other documents:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程已经将像 "I like fruits. Fruits like bananas" 这样的句子转化为一个向量（`0, 0, 0, 1, 0, 0,
    0, 2, 0, 2, 0, 0`）。这是**无关上下文**向量化的一个例子。无关上下文是指文档中单词的顺序在生成向量时没有任何影响。这仅仅是对文档中单词出现的次数进行计数。因此，具有多重含义的单词可能会被归为一个类别，例如
    *bank*。它可以指代河岸边的地方或存钱的地方。然而，它确实提供了一种比较文档并推导相似度的方法。可以计算两个文档之间的余弦相似度或距离，以查看哪些文档与其他文档相似：
- en: '[PRE68]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This shows that the first sentence and the second sentence have a 0.136 similarity
    score (on a scale of 0 to 1). The first and third sentence have nothing in common.
    The second and third sentence have a similarity score of 0.308 – the highest in
    this set. Another use case of this technique is to check the similarity of the
    documents with given keywords. Let''s say that the query is *apple and bananas*.
    This first step is to compute the vector of this query, and then compute the cosine
    similarity scores against the documents in the corpus:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明第一句和第二句的相似度分数为0.136（0到1的范围）。第一句和第三句没有任何相似之处。第二句和第三句的相似度分数为0.308——这是这个集合中的最高值。这个技术的另一个应用场景是检查给定关键词的文档相似度。假设查询是
    *apple and bananas*。第一步是计算这个查询的向量，然后计算它与语料库中各文档的余弦相似度分数：
- en: '[PRE69]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This shows that this query matches the second sentence in the corpus the best.
    The third sentence would rank second, and the first sentence would rank lowest.
    In a few lines, a basic search engine has been implemented, along with logic to
    serve queries! At scale, this is a very difficult problem, as the number of words
    or columns in a web crawler would top 3 billion. Every web page would be represented
    as a row, so that would also require billions of rows. Computing a cosine similarity
    in milliseconds to serve an online query and keeping the content of this matrix
    updated is a massive undertaking.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明该查询与语料库中的第二个句子匹配最佳。第三个句子排在第二，第一个句子排名最低。通过几行代码，基本的搜索引擎已经实现，并且具备了服务查询的逻辑！在大规模的情况下，这是一个非常难的问题，因为网页爬虫中的单词或列数将超过30亿。每个网页都会作为一行表示，因此也需要数十亿行。计算余弦相似度并在毫秒级别内响应在线查询，同时保持矩阵内容的更新，是一项巨大的工作。
- en: The next step from this rather simple vectorization scheme is to consider the
    information content of each word in constructing this matrix.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相对简单的向量化方案的下一步是考虑每个单词的信息内容，在构建这个矩阵时加以考虑。
- en: Term Frequency-Inverse Document Frequency (TF-IDF)
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词频-逆文档频率（TF-IDF）
- en: In creating a vector representation of the document, only the presence of words
    was included – it does not factor in the importance of a word. If the corpus of
    documents being processed is about a set of recipes with fruits, then one may
    expect words like *apples*, *raspberries*, and *washing* to appear frequently.
    **Term Frequency** (**TF**) represents how often a word or token occurs in a given
    document. This is exactly what we did in the previous section. In a set of documents
    about fruits and cooking, a word like *apple* may not be terribly specific to
    help identify a recipe. However, a word like *tuile* may be uncommon in that context.
    Therefore, it may help to narrow the search for recipes much faster than a word
    like *raspberry*. On a side note, feel free to search the web for raspberry tuile
    recipes. If a word is rare, we want to give it a higher weight, as it may contain
    more information than a common word. A term can be upweighted by the inverse of
    the number of documents it appears in. Consequently, words that occur in a lot
    of documents will get a smaller score compared to terms that appear in fewer documents.
    This is called the **Inverse Document Frequency** (**IDF**).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建文档的向量表示时，只考虑了词的出现情况——并未考虑词的重要性。如果处理的文档语料库是关于水果的食谱，那么像*苹果*、*覆盆子*和*清洗*这样的词可能会频繁出现。**术语频率**（**TF**）表示某个词或标记在给定文档中出现的频率。这正是我们在前一节所做的。在一个关于水果和烹饪的文档集里，像*苹果*这样的词可能不太特定，难以帮助识别食谱。然而，像*薄脆饼*（*tuile*）这样的词在该语境中可能比较少见。因此，它可能比像*覆盆子*这样的词更快地帮助缩小食谱搜索范围。顺便说一句，不妨上网搜索一下覆盆子薄脆饼的食谱。如果一个词比较稀有，我们希望给予它更高的权重，因为它可能包含比常见词更多的信息。术语可以通过它出现的文档数量的倒数来提升权重。因此，出现在大量文档中的词将得到较小的分数，而出现在较少文档中的词则得分较高。这被称为**逆文档频率**（**IDF**）。
- en: 'Mathematically, the score of each term in a document can be computed as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，文档中每个术语的分数可以按如下方式计算：
- en: '![](img/B16252_01_006.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_006.png)'
- en: Here, *t* represents the word or term, and *d* represents a specific document.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*t*表示词或术语，*d*表示特定的文档。
- en: It is common to normalize the TF of a term in a document by the total number
    of tokens in that document.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，通过文档中所有标记的总数来规范化一个术语在文档中的TF值。
- en: 'The IDF is defined as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: IDF的定义如下：
- en: '![](img/B16252_01_007.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_01_007.png)'
- en: Here, *N* represents the total number of documents in the corpus, and *n*[t]
    represents the number of documents where the term is present. The addition of
    1 in the denominator avoids the divide-by-zero error. Fortunately, `sklearn` provides
    methods to compute TF-IDF.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N*代表语料库中文档的总数，*n*[t]代表术语出现的文档数量。分母中加1可以避免除以零的错误。幸运的是，`sklearn`提供了计算TF-IDF的方法。
- en: The *TF-IDF Vectorization* section of the notebook contains the code for this
    section.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的*TF-IDF向量化*部分包含此部分的代码。
- en: 'Let''s convert the counts from the previous section into their TF-IDF equivalents:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前一部分中的计数转换为它们的TF-IDF等效值：
- en: '[PRE70]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This produces the following output:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生如下输出：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_14.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_14.png)'
- en: This should give some intuition on how TF-IDF is computed. Even with three toy
    sentences and a very limited vocabulary, many of the columns in each row are 0\.
    This vectorization produces **sparse representations**.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该能帮助理解TF-IDF是如何计算的。即使只有三句话和一个非常有限的词汇表，每行中的许多列都是0。这种向量化产生了**稀疏表示**。
- en: 'Now, this can be applied to the problem of detecting spam messages. Thus far,
    the features for each message have been computed based on some aggregate statistics
    and added to the `pandas` DataFrame. Now, the content of the message will be tokenized
    and converted into a set of columns. The TF-IDF score for each word or token will
    be computed for each message in the array. This is surprisingly easy to do with
    `sklearn`, as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这可以应用于检测垃圾信息的问题。到目前为止，每条信息的特征已经基于一些聚合统计值进行计算，并添加到`pandas` DataFrame中。接下来，信息的内容将被标记化，并转换为一组列。每个词或标记的TF-IDF分数将被计算为数组中每条信息的值。使用`sklearn`，这实际上是非常简单的，代码如下：
- en: '[PRE71]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The second parameter shows that 7,741 tokens were uniquely identified. These
    are the columns of features that will be used in the model later. Note that the
    vectorizer was created with the binary flag. This implies that even if a token
    appears multiple times in a message, it is counted as one. The next line trains
    the TF-IDF model on the training dataset. Then, it converts the words in the test
    set according to the TF-IDF scores learned from the training set. Let's train
    a model on just these TF-IDF features.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数显示已唯一标识了7,741个标记。这些是稍后将在模型中使用的特征列。请注意，向量化器是通过二进制标志创建的。这意味着即使一个标记在一条消息中出现多次，它也只会被计为一次。接下来的行在训练数据集上训练TF-IDF模型。然后，它会根据从训练集中学到的TF-IDF分数转换测试集中的单词。让我们只用这些TF-IDF特征训练一个模型。
- en: Modeling using TF-IDF features
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TF-IDF特征建模
- en: 'With these TF-IDF features, let''s train a model and see how it does:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些TF-IDF特征，让我们训练一个模型并看看它的表现：
- en: '[PRE73]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Whoa – we are able to classify every one correctly! In all honesty, the model
    is probably overfitting, so some regularization should be applied. The test set
    gives this result:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 哇—我们能够正确分类每一个！说实话，模型可能存在过拟合问题，因此应该应用一些正则化。测试集给出的结果如下：
- en: '[PRE75]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'An accuracy rate of 98.39% is by far the best we have gotten in any model so
    far. Checking the confusion matrix, it is evident that this model is indeed doing
    very well:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 98.39%的准确率迄今为止是我们在任何模型中获得的最佳结果。查看混淆矩阵，可以明显看出这个模型确实表现得非常好：
- en: '[PRE77]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Only 2 regular messages were classified as spam, while only 16 spam messages
    were classified as being not spam. This is indeed a very good model. Note that
    this dataset has Indonesian (or Bahasa) words as well as English words in it.
    Bahasa uses the Latin alphabet. This model, without using a lot of pretraining
    and knowledge of language, vocabulary, and grammar, was able to do a very reasonable
    job with the task at hand.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 仅有2条常规消息被分类为垃圾邮件，而只有16条垃圾邮件被分类为非垃圾邮件。这确实是一个非常好的模型。请注意，这个数据集中包含了印尼语（或巴哈萨语）单词以及英语单词。巴哈萨语使用拉丁字母。这个模型没有使用大量的预训练和对语言、词汇、语法的了解，却能够在手头的任务中表现得非常合理。
- en: However, this model ignores the relationships between words completely. It treats
    the words in a document as unordered items in a set. There are better models that
    vectorize the tokens in a way that preserves some of the relationships between
    the tokens. This is explored in the next section.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种模型完全忽略了单词之间的关系。它将文档中的单词视为集合中的无序项。实际上，有更好的模型通过向量化方式保留了一些单词之间的关系，这将在下一节中进行探讨。
- en: Word vectors
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词向量
- en: In the previous example, a row vector was used to represent a document. This
    was used as a feature for the classification model to predict spam labels. However,
    no information can be gleaned reliably from the relationships between words. In
    NLP, a lot of research has been focused on learning the words or representations
    in an unsupervised way. This is called representation learning. The output of
    this approach is a representation of a word in some vector space, and the word
    can be considered **embedded** in that space. Consequently, these word vectors
    are also called embeddings.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，使用了一个行向量来表示文档。这被作为分类模型的特征来预测垃圾邮件标签。然而，从单词之间的关系中并不能可靠地提取到信息。在自然语言处理（NLP）中，很多研究集中在无监督地学习单词或表示。这被称为表示学习。这种方法的输出是单词在某个向量空间中的表示，单词可以被视为**嵌入**在该空间中。因此，这些词向量也被称为嵌入向量。
- en: The core hypothesis behind word vector algorithms is that words that occur near
    each other are related to each other. To see the intuition behind this, consider
    two words, *bake* and *oven*. Given a sentence fragment of five words, where one
    of these words is present, what would be the probability of the other being present
    as well? You would be right in guessing that the probability is likely quite high.
    Suppose now that words are being mapped into some two-dimensional space. In that
    space, these two words should be closer to each other, and probably further away
    from words like *astronomy* and *tractor*.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量算法背后的核心假设是，相邻出现的单词之间是相关的。为了理解这一点，考虑两个单词，*bake*（烤）和*oven*（烤箱）。假设一个包含五个单词的句子片段，其中一个单词是上述两个单词之一，另一个单词出现在其中的概率是多少？你猜测其概率可能会很高，这是正确的。现在假设单词被映射到某个二维空间中。在这个空间里，这两个单词应该彼此靠得更近，可能会远离像*astronomy*（天文学）和*tractor*（拖拉机）这样的单词。
- en: The task of learning these embeddings for the words can be then thought of as
    adjusting words in a giant multidimensional space where similar words are closer
    to each other and dissimilar words are further apart from each other.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 学习这些单词的嵌入表示的任务可以被看作是在一个巨大的多维空间中调整单词，使得相似的单词彼此接近，不相似的单词彼此远离。
- en: 'A revolutionary approach to do this is called Word2Vec. This algorithm was
    published by Tomas Mikolov and collaborators from Google in 2013\. This approach
    produces dense vectors of the order of 50-300 dimensions generally (though larger
    are known), where most of the values are non-zero. In contrast, in our previous
    trivial spam example, the TF-IDF model had 7,741 dimensions. The original paper
    had two algorithms proposed in it: **continuous bag-of-words** and **continuous
    skip-gram**. On semantic tasks and overall, the performance of skip-gram was state
    of the art at the time of its publication. Consequently, the continuous skip-gram
    model with negative sampling has become synonymous with Word2Vec. The intuition
    behind this model is fairly straightforward.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 一种革命性的做法称为Word2Vec。这一算法由Tomas Mikolov及其Google团队的合作伙伴于2013年发布。该方法通常生成50-300维的密集向量（虽然也有更大维度的向量），其中大多数值为非零值。相比之下，在我们之前简单的垃圾邮件示例中，TF-IDF模型有7,741维。原始论文中提出了两种算法：**连续词袋模型**和**连续Skip-Gram模型**。在语义任务和整体表现上，Skip-Gram模型在其发布时是最先进的。因此，带有负采样的连续Skip-Gram模型已经成为Word2Vec的代名词。该模型的直觉非常简单。
- en: 'Consider this sentence fragment from a recipe: "Bake until the cookie is golden
    brown all over." Under the assumption that a word is related to the words that
    appear near it, a word from this fragment can be picked and a classifier can be
    trained to predict the words around it:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个来自食谱的句子片段：“烤至饼干表面金黄”。假设一个单词与其附近的单词相关联，可以从这个片段中选取一个单词，并训练一个分类器来预测其周围的单词：
- en: '![A close up of a logo  Description automatically generated](img/B16252_01_15.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![一个 logo 的特写图像，描述自动生成](img/B16252_01_15.png)'
- en: 'Figure 1.14: A window of 5 centered on cookie'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14：以饼干为中心的5个单词窗口
- en: Taking an example of a window of five words, the word in the center is used
    to predict two words before and two words after it. In the preceding figure, the
    fragment is *until the cookie is golden*, with the focus on the word *cookie*.
    Assuming that there are 10,000 words in the vocabulary, a network can be trained
    to predict binary decisions given a pair of words. The training objective is that
    the network predicts `true` for pairs like (*cookie*, *golden*) while predicting
    `false` for (*cookie*, *kangaroo*). This particular approach is called **Skip-Gram
    Negative Sampling** (**SGNS**) and it considerably reduces the training time required
    for large vocabularies. Very similar to the single-layer neural model in the previous
    section, a model can be trained with a one-to-many as the output layer. The sigmoid
    activation would be changed to a `softmax` function. If the hidden layer has 300
    units, then its dimensions would be 10,000 x 300, that is, for each of the words,
    there will be a set of weights. The objective of the training is to learn these
    weights. In fact, these weights become the embedding for that word once training
    is complete.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个包含五个单词的窗口为例，窗口中心的单词用于预测其前后各两个单词。在前面的图中，片段是*直到饼干变金黄*，焦点是单词*饼干*。假设词汇表中有10,000个单词，可以训练一个网络，根据一对单词预测二元决策。训练目标是，网络对像(*饼干*,
    *金黄*)这样的词对预测`true`，而对像(*饼干*, *袋鼠*)这样的词对预测`false`。这种方法被称为**Skip-Gram负采样**（**SGNS**），它大大减少了大词汇量训练所需的时间。与上一节中的单层神经网络模型非常相似，可以通过一对多的输出层训练一个模型。sigmoid激活函数将被更改为`softmax`函数。如果隐藏层有300个单元，那么其维度将是10,000
    x 300，即每个单词都会有一组权重。训练的目标是学习这些权重。实际上，这些权重会在训练完成后成为该单词的嵌入表示。
- en: The choice of units in the hidden layer is a hyperparameter that can be adapted
    for specific applications. 300 is commonly found as it is available through pretrained
    embeddings on the Google News dataset. Finally, the error is computed as the sum
    of the categorical cross-entropy of all the word pairs in negative and positive
    examples.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中单元数的选择是一个超参数，可以根据特定应用进行调整。300个单元通常被采用，因为它可以通过Google新闻数据集上的预训练嵌入获得。最后，误差被计算为所有负样本和正样本中词对的分类交叉熵之和。
- en: The beauty of this model is that it does not require any supervised training
    data. Running sentences can be used to provide positive examples. For the model
    to learn effectively, it is important to provide negative samples as well. Words
    are randomly sampled using their probability of occurrence in the training corpus
    and fed as negative examples.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的优点在于它不需要任何监督式的训练数据。运行的句子可以用来提供正面示例。为了让模型有效学习，提供负面样本也非常重要。词汇是根据它们在训练语料库中的出现概率随机抽样，并作为负面示例输入。
- en: To understand how the Word2Vec embeddings work, let's download a set of pretrained
    embeddings.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 Word2Vec 嵌入是如何工作的，我们来下载一组预训练的嵌入。
- en: The code shown in the following section can be found in the *Word Vectors* section
    of the notebook.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分显示的代码可以在笔记本的*词向量*部分找到。
- en: Pretrained models using Word2Vec embeddings
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Word2Vec 嵌入的预训练模型
- en: 'Since we are only interested in experimenting with a pretrained model, we can
    use the Gensim library and its pretrained embeddings. Gensim should already be
    installed in Google Colab. It can be installed like so:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只对实验预训练模型感兴趣，可以使用 Gensim 库及其预训练嵌入。Gensim 应该已经安装在 Google Colab 中，可以通过以下方式安装：
- en: '[PRE78]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'After the requisite imports, pretrained embeddings can be downloaded and loaded.
    Note that these particular embeddings are approximately 1.6 GB in size, so may
    take a very long time to load (you may encounter some memory issues as well):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行必要的导入之后，可以下载并加载预训练嵌入。请注意，这些嵌入的大小大约是 1.6 GB，因此可能需要很长时间才能加载（你也可能会遇到一些内存问题）：
- en: '[PRE79]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Another issue that you may run into is the Colab session expiring if left alone
    for too long while waiting for the download to finish. This may be a good time
    to switch to a local notebook, which will also be helpful in future chapters.
    Now, we are ready to inspect the similar words:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到的另一个问题是，如果下载等待时间过长，Colab 会话可能会过期。此时可能是切换到本地笔记本的好时机，这对后续章节也会有所帮助。现在，我们准备好检查相似词汇了：
- en: '[PRE80]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This is pretty good. Let''s see how this model does at a word analogy task:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常不错。让我们看看这个模型在词类比任务中的表现：
- en: '[PRE82]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The model is able to guess that compared to the other words, which are all
    countries, Tokyo is the odd one out, as it is a city. Now, let''s try a very famous
    example of mathematics on these word vectors:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够猜测，与其他所有国家名称相比，东京是唯一的异常词，因为它是一个城市。现在，让我们在这些词向量上试试一个非常著名的数学例子：
- en: '[PRE84]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Given that *King* was provided as an input to the equation, it is simple to
    filter the inputs from the outputs and *Queen* would be the top result. SMS spam
    classification could be attempted using these embeddings. However, future chapters
    will cover the use of GloVe embeddings and BERT embeddings for sentiment analysis.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*King*作为输入提供给方程式，简单地从输入和输出中筛选，就能得到*Queen*作为最优结果。可以尝试使用这些嵌入进行短信垃圾分类。然而，后续章节将介绍如何使用
    GloVe 嵌入和 BERT 嵌入进行情感分析。
- en: A pretrained model like the preceding can be used to vectorize a document. Using
    these embeddings, models can be trained for specific purposes. In later chapters,
    newer methods of generating contextual embeddings, such as BERT, will be discussed
    in detail.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 像前面提到的预训练模型可以用于将文档向量化。使用这些嵌入，可以训练特定用途的模型。在后续章节中，我们将详细讨论生成上下文嵌入的新方法，例如 BERT。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we worked through the basics of NLP, including collecting and
    labeling training data, tokenization, stop word removal, case normalization, POS
    tagging, stemming, and lemmatization. Some vagaries of these in languages such
    as Japanese and Russian were also covered. Using a variety of features derived
    from these approaches, we trained a model to classify spam messages, where the
    messages had a combination of English and Bahasa Indonesian words. This got us
    to a model with 94% accuracy.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讲解了自然语言处理的基础，包括收集和标注训练数据、分词、停用词去除、大小写标准化、词性标注、词干提取和词形还原。还涉及了日语和俄语等语言中的一些特殊性。通过使用从这些方法中衍生出来的多种特征，我们训练了一个模型来分类垃圾信息，该信息包含英语和印尼语的混合词汇。这让我们得到了一个94%准确率的模型。
- en: However, the major challenge in using the content of the messages was in defining
    a way to represent words as vectors such that computations could be performed
    on them. We started with a simple count-based vectorization scheme and then graduated
    to a more sophisticated TF-IDF approach, both of which produced sparse vectors.
    This TF-IDF approach gave a model with 98%+ accuracy in the spam detection task.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用消息内容时的主要挑战在于如何定义一种方法，将词语表示为向量，以便进行计算。我们从一种简单的基于计数的向量化方案开始，然后过渡到更复杂的TF-IDF方法，前者生成了稀疏向量。这个TF-IDF方法在垃圾邮件检测任务中实现了98%以上的准确率。
- en: Finally, we saw a contemporary method of generating dense word embeddings, called
    Word2Vec. This method, though a few years old, is still very relevant in many
    production applications. Once the word embeddings are generated, they can be cached
    for inference and that makes an ML model using these embeddings run with relatively
    low latency.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了一个生成密集词向量的现代方法，叫做Word2Vec。这个方法虽然已经有几年历史，但在许多生产应用中依然非常相关。一旦生成了词向量，它们可以被缓存用于推理，这使得使用这些词向量的机器学习模型在运行时具有相对较低的延迟。
- en: We used a very basic deep learning model for solving the SMS spam classification
    task. Like how **Convolutional Neural Networks** (**CNNs**) are the predominant
    architecture in computer vision, **Recurrent Neural Networks** (**RNNs**), especially
    those based on **Long Short-Term Memory** (**LSTM**) and **Bi-directional LSTMs**
    (**BiLSTMs**), are most commonly used to build NLP models. In the next chapter,
    we cover the structure of LSTMs and build a sentiment analysis model using BiLSTMs.
    These models will be used extensively in creative ways to solve different NLP
    problems in future chapters.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个非常基础的深度学习模型来解决短信垃圾分类任务。就像**卷积神经网络**（**CNNs**）是计算机视觉中的主流架构一样，**循环神经网络**（**RNNs**），特别是基于**长短时记忆网络**（**LSTM**）和**双向LSTM**（**BiLSTM**）的网络，是构建自然语言处理（NLP）模型时最常用的架构。在下一章中，我们将介绍LSTM的结构，并使用BiLSTM构建情感分析模型。这些模型将在未来的章节中以创造性的方式广泛应用于解决不同的NLP问题。
