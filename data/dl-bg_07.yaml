- en: Training Multiple Layers of Neurons
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 训练多个神经元层
- en: Previously, in [Chapter 6](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml), *Training
    a Single Neuron*, we explored a model involving a single neuron and the concept
    of the perceptron. A limitation of the perceptron model is that, at best, it can
    only produce linear solutions on a multi-dimensional hyperplane. However, this
    limitation can be easily solved by using multiple neurons and multiple layers
    of neurons in order to produce highly complex non-linear solutions for separable
    and non-separable problems. This chapter introduces you to the first challenges
    of deep learning using the **Multi-Layer Perceptron** (**MLP**) algorithm, such
    as a gradient descent technique for error minimization, followed by hyperparameter
    optimization experiments to determine trustworthy accuracy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的[第6章](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml)中，*单神经元训练*一节，我们探讨了涉及单一神经元和感知器概念的模型。感知器模型的一个限制是，最多只能在多维超平面上产生线性解。然而，通过使用多个神经元和多个神经元层来产生高度复杂的非线性解，针对可分和不可分问题，这一限制可以轻松解决。本章将带你了解深度学习的第一个挑战，使用**多层感知器**（**MLP**）算法，例如用于误差最小化的梯度下降技术，接着是超参数优化实验，以确定可靠的准确性。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The MLP model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP模型
- en: Minimizing the error
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化误差
- en: Finding the best hyperparameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找最佳超参数
- en: The MLP model
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP模型
- en: We have previously seen, in [Chapter 5](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml),
    *Training a Single Neuron*, that Rosenblatt's perceptron model is simple and powerful
    for some problems (Rosenblatt, F. 1958). However, for more complicated and highly
    non-linear problems, Rosenblatt did not give enough attention to his models that
    connected many more neurons in different architectures, including deeper models
    (Tappert, C. 2019).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在[第5章](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml)中，*单神经元训练*一节中，已经看到Rosenblatt的感知器模型对于某些问题来说既简单又强大（Rosenblatt,
    F. 1958）。然而，对于更复杂和高度非线性的问题，Rosenblatt没有充分关注他连接了更多神经元并采用不同架构的模型，包括更深的模型（Tappert,
    C. 2019）。
- en: Years later, in the 1990s, Prof. Geoffrey Hinton, the 2019 Turing Award winner,
    continued working to connect more neurons together since this is more brain-like
    than simple neurons (Hinton, G. 1990). Most people today know this type of approach
    as *connectionist*.The main idea is to connect neurons in different ways that
    will resemble brain connections. One of the first successful models was the MLP, which
    uses a supervised gradient descent-based learning algorithm that learns to approximate
    a function, ![](img/f8bc26a8-f3ac-430b-b3a0-0bf4dab209f0.png), using labeled data, ![](img/a99bcb2f-d199-4000-afb9-9665eeb321ce.png).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多年后，在1990年代，2019年图灵奖得主Geoffrey Hinton教授继续致力于将更多神经元连接在一起，因为这种方式比单一神经元更像大脑（Hinton,
    G. 1990）。如今，大多数人知道这种方法被称为*联结主义*。其主要思想是以不同的方式连接神经元，从而模拟大脑中的连接。第一个成功的模型之一是MLP，它使用基于监督梯度下降的学习算法，通过标记数据学习逼近一个函数，![](img/f8bc26a8-f3ac-430b-b3a0-0bf4dab209f0.png)，![](img/a99bcb2f-d199-4000-afb9-9665eeb321ce.png)。
- en: '*Figure 6.1* depicts an MLP with one layer of multiple neurons that indicate
    how the input connects to all neurons through weights, which stimulate a neuron
    to produce a large (non-zero) numerical response, depending on the variable weights
    that need to be *learned*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1*展示了一个包含多个神经元的MLP，它指示了输入是如何通过权重与所有神经元连接的，这些权重激活神经元以产生一个较大的（非零）数值响应，具体取决于需要*学习*的变量权重：'
- en: '![](img/23b21eea-b260-4f2d-bbc7-02be5377d5b1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23b21eea-b260-4f2d-bbc7-02be5377d5b1.png)'
- en: Figure 6.1 – Multiple perceptrons in one hidden layer
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 单隐藏层中的多个感知器
- en: 'For completeness, *Figure 6.2* depicts the same architecture but vertically;
    it also shows positive weights in light gray and negative weights in darker gray.
    *Figure 6.2* aims to show that some features might stimulate some neurons more
    than others:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，*图6.2*展示了相同的架构，但以竖直方向呈现；它还用浅灰色表示正权重，用深灰色表示负权重。*图6.2*旨在展示某些特征可能比其他特征更能激活某些神经元：
- en: '![](img/1ffd2130-584b-49fa-8742-da9b062fc576.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ffd2130-584b-49fa-8742-da9b062fc576.png)'
- en: 'Figure 6.2 – MLP with weights that are grayscale-coded: lighter grays denote
    positive weights, darker grays denote negative weights'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – MLP，权重采用灰度编码：浅灰色表示正权重，深灰色表示负权重
- en: Based on *Figure 6.2*, the layer of neurons at the top is known as the **input
    layer**. These features are connected to different neurons in a layer known as
    a **hidden layer**. This layer usually consists of at least one layer of neurons,
    but in deep learning, it may contain many more.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于*图6.2*，顶部的神经元层被称为**输入层**。这些特征与称为**隐藏层**的不同神经元连接。这个层通常至少包含一层神经元，但在深度学习中，它可能包含更多层。
- en: '**On the interpretation of the weights close to the input layer**:One of the
    key differences between the MLP and the perceptron is that the interpretation
    of the weights in the input layer is lost in the MLP unless the hidden layer contains
    only one neuron. Usually, in a perceptron, you can argue that the importance of
    certain features is directly correlated to the value (weight) directly associated
    with those features. For example, the feature associated with the most negative
    weight is said to negatively influence the outcome, and the feature associated
    with the most positive weight is also influencing the outcome in a significant
    manner. Therefore, looking into the absolute value of the weights in a perceptron
    (and in linear regression) can inform us about feature importance. Not so much
    in the MLP; the more neurons are involved and the more layers are involved, the
    chances of interpreting weights and feature importance is reduced significantly.
    You must not rely heavily on the first-layer weights to deduce feature importance.
    Be careful.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于输入层附近权重的解释**：MLP和感知机之间的一个关键区别是，除非隐藏层仅包含一个神经元，否则输入层中权重的解释在MLP中会丧失。通常，在感知机中，你可以认为某些特征的重要性与直接与这些特征相关联的值（权重）有直接的关系。例如，最负权重相关联的特征被认为会对结果产生负面影响，而最正权重相关联的特征也会显著地影响结果。因此，在感知机（和线性回归）中，查看权重的绝对值可以帮助我们了解特征的重要性。在MLP中则不然；涉及的神经元越多，层数越多，解释权重和特征重要性的可能性就越小。你不应过于依赖第一层的权重来推断特征的重要性。要小心。'
- en: 'From *Figure 6.1*, we can see that neurons, ![](img/ce8c7434-1308-4835-9eef-56f1500898e1.png),
    are simplified to imply that there is some non-linear activation function, [![](img/5f7fc36f-88a3-48dd-a180-fce9b6554ca7.png)],
    over the scalar, resulting from adding the products of the features and the weights
    associated with those features and that neuron, ![](img/83f57141-0dcf-4762-8213-a0c10510bb30.png).
    In deeper MLP layers, the input is no longer data from the input layer, ![](img/d2fab579-f5ed-47a4-9f99-17afa1dba090.png),
    but are rather outputs from previous layers: ![](img/023944b5-18a7-4fec-b8bc-92a950d346a6.png).
    We will make some changes to the notation in the next section to describe this
    process more formally.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图6.1*中，我们可以看到神经元，![](img/ce8c7434-1308-4835-9eef-56f1500898e1.png)，被简化为意味着有某种非线性激活函数，[![](img/5f7fc36f-88a3-48dd-a180-fce9b6554ca7.png)]，作用在标量上，标量是通过加上特征和与这些特征及神经元相关的权重的乘积得到的，![](img/83f57141-0dcf-4762-8213-a0c10510bb30.png)。在更深的MLP层中，输入不再是来自输入层的数据，![](img/d2fab579-f5ed-47a4-9f99-17afa1dba090.png)，而是来自前一层的输出：![](img/023944b5-18a7-4fec-b8bc-92a950d346a6.png)。我们将在下一节对符号进行一些更改，以更正式地描述这个过程。
- en: For now, what you need to know is that the MLP is a lot better than the perceptron
    in that is has the ability to learn highly complex non-linear models. The perceptron
    is only able to provide linear models. But with this power comes great responsibility.
    The MLP has a non-convex and non-smooth loss function that limits how the learning
    process is achieved, and although there has been much progress, their problems
    still persist. Another disadvantage is that the learning algorithms may need other
    hyperparameters to assure the success (convergence) of the algorithm. Finally,
    it is worth noting that the MLP requires preprocessing of the input features (normalization)
    to mitigate neurons overfitting on specific features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你需要知道的是，MLP比感知机要好得多，因为它能够学习高度复杂的非线性模型。而感知机只能提供线性模型。但这种强大的能力也伴随着巨大的责任。MLP有一个非凸且不平滑的损失函数，这限制了学习过程的实现，尽管已经取得了很多进展，但这些问题仍然存在。另一个缺点是，学习算法可能需要其他超参数来确保算法的成功（收敛）。最后，值得注意的是，MLP需要对输入特征进行预处理（归一化），以减轻神经元在特定特征上过拟合的问题。
- en: Now, let's examine how the learning process actually happens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看学习过程是如何实际发生的。
- en: Minimizing the error
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小化误差
- en: Learning from data using an MLP was one of the major problems since its conception.
    As we pointed out before, one of the major problems with neural networks was the
    computational tractability of deeper models, and the other was stable learning
    algorithms that would converge to a reasonable minimum. One of the major breakthroughs
    in machine learning, and what paved the way for deep learning, was the development
    of the learning algorithm based on backpropagation. Many scientists independently
    derived and applied forms of backpropagation in the 1960s; however, most of the
    credit has been given to Prof. G. E. Hinton and his group (Rumelhart, D. E., et.al.
    1986). In the next few paragraphs, we will go over this algorithm, whose sole
    purpose is to **minimize the error** caused by incorrect predictions made during
    training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MLP 从数据中学习是其诞生以来的一个主要问题。正如我们之前指出的，神经网络面临的一个主要问题是更深层模型的计算可行性，另一个问题是稳定的学习算法，能够收敛到合理的最小值。机器学习的一个重大突破，也是为深度学习铺平道路的，是基于反向传播的学习算法的开发。许多科学家在1960年代独立推导并应用了不同形式的反向传播；然而，大多数功劳归于G.
    E. Hinton教授及其团队（Rumelhart, D. E. 等，1986年）。在接下来的几段中，我们将详细介绍这个算法，其唯一目的是**最小化错误**，以减少在训练过程中由于预测不准确而导致的误差。
- en: 'To begin, we will describe the dataset, which is called **spirals***. *This
    is a widely known benchmark dataset that has two classes that are separable, yet
    highly non-linear. The positive and negative classes go around each other on opposite
    sides of a two-dimensional space as they grow from the center outward, as shown
    in *Figure 6.3*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将描述这个名为**螺旋**（spirals）的数据集。*这是一个广为人知的基准数据集，具有两个可分的类别，但这些类别是高度非线性的。正负类别在二维空间的相对两侧盘绕，随着从中心向外扩展，如*图
    6.3*所示：*
- en: '![](img/97403252-c001-4e6a-b531-65055aded079.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97403252-c001-4e6a-b531-65055aded079.png)'
- en: Figure 6.3 – Sample data from the two-spiral benchmark
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 - 来自双螺旋基准数据集的示例数据
- en: 'The dataset can be produced using the following function in Python:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下Python函数生成该数据集：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code fragment, we will receive in `X` a two-column matrix whose rows
    are samples of the spiral dataset, and `y` contains the corresponding target class
    in the ![](img/20c250a1-a4d1-42b6-81a5-a0a4224ba882.png) set. *Figure 6.3* was
    produced based on the preceding code fragment, which contains 300 samples.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们将接收一个`X`的两列矩阵，其行是螺旋数据集的样本，而`y`包含相应的目标类别，来自![](img/20c250a1-a4d1-42b6-81a5-a0a4224ba882.png)集合。*图
    6.3*是基于前面的代码片段生成的，包含300个样本。
- en: 'We will also use a very simple MLP architecture with only three neurons in
    a single hidden layer; this is only to explain *backpropagation* as clearly as
    possible. The proposed MLP is shown in *Figure 6.4*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用一个非常简单的多层感知机（MLP）架构，该架构仅包含一个隐藏层的三个神经元；这只是为了尽可能清晰地解释*反向传播*。所提出的 MLP 如*图
    6.4*所示：
- en: Backpropagation is known among the professionals today as **backprop***. *If
    you read any recent online discussions about it, it will most likely be referred
    to as backprop, for short.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播在业内人士中今天被称为**反向传播**（backprop）。*如果你阅读最近的在线讨论，它很可能会被简称为 backprop。*
- en: '![](img/681215c7-cbf6-4b98-b884-8fd0ddb2e2d4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/681215c7-cbf6-4b98-b884-8fd0ddb2e2d4.png)'
- en: Figure 6.4 - Simple MLP architecture for backpropagation-based learning on the
    spiral dataset
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 - 用于基于反向传播学习的简单多层感知机（MLP）架构，应用于螺旋数据集
- en: 'The architecture of the network shown in *Figure 6.4* assumes that there is
    a well-defined input vector containing multiple vectors, ![](img/ade78908-fbaa-441d-95df-05554ba3fa62.png) (a
    matrix), represented as ![](img/ac0661e9-7c97-4ce4-acc3-c76ebead6fa6.png), and
    multiple individual targets represented as a vector, ![](img/91f42c43-effa-40ab-b990-29ae9d78b06b.png).
    Also, each layer, ![](img/d16b47e7-3267-4767-822c-a2317f7bc349.png), has a matrix
    of weights, ![](img/2f1013b9-18cc-496b-8e0d-87e17bce2e02.png), which is the case
    with the first layer. For example, from *Figure 6.4*, the weight matrices would
    be as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.4*所示的网络架构假设存在一个定义良好的输入向量，包含多个向量，![](img/ade78908-fbaa-441d-95df-05554ba3fa62.png)（一个矩阵），表示为
    ![](img/ac0661e9-7c97-4ce4-acc3-c76ebead6fa6.png)，以及多个独立的目标向量，![](img/91f42c43-effa-40ab-b990-29ae9d78b06b.png)。此外，每一层，![](img/d16b47e7-3267-4767-822c-a2317f7bc349.png)，都有一个权重矩阵，![](img/2f1013b9-18cc-496b-8e0d-87e17bce2e02.png)，这在第一层也是如此。例如，从*图
    6.4*中，权重矩阵将如下所示：
- en: '![](img/20f071f2-4395-4dd8-a4d1-14ce57fc7b06.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20f071f2-4395-4dd8-a4d1-14ce57fc7b06.png)'
- en: '![](img/92af53a3-84d9-4d77-968b-e8745ef359ad.png).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/92af53a3-84d9-4d77-968b-e8745ef359ad.png)。'
- en: 'These matrices have real values initialized at random. The hidden layer, ![](img/e7bdbf92-3a53-4f0f-a836-88df5a020e1e.png), consists
    of three neurons. Each neuron in receives as input, ![](img/d305bc5a-7909-4ecc-ab93-76328e67de8c.png),
    a weighted sum of observations consisting of the inner product of the features
    and the weights leading to the *i*th neuron; for example, for the first neuron
    it would be as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些矩阵的值是随机初始化的实际值。隐藏层 ![](img/e7bdbf92-3a53-4f0f-a836-88df5a020e1e.png) 由三个神经元组成。每个神经元接收作为输入的
    ![](img/d305bc5a-7909-4ecc-ab93-76328e67de8c.png)，这是特征和权重的内积，得到加权的观察值，指向第 *i*
    个神经元；例如，对于第一个神经元，计算方式如下：
- en: '![](img/68263946-1dec-4a7b-88af-95e0423c887f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68263946-1dec-4a7b-88af-95e0423c887f.png)'
- en: Here, ![](img/9eb3fd4a-2973-460a-9eb4-d53ef2db7fb3.png) denotes the output of
    the activation function of the first neuron in the first layer, which in this
    case would be a sigmoid.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/9eb3fd4a-2973-460a-9eb4-d53ef2db7fb3.png) 表示第一层中第一个神经元的激活函数的输出，在本例中将是一个
    sigmoid 函数。
- en: The sigmoid activation function is denoted as ![](img/57a4a874-66bc-40f8-92bf-e2182fb49c42.png).
    This function is interesting because it squashes whatever value it receives as
    input and maps it to values between 0 and 1\. It is also a nice function to use
    in gradient calculation since its derivative is well known and easy to compute: ![](img/8df4cea8-a438-4497-a673-8887fe5d4439.png).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数表示为 ![](img/57a4a874-66bc-40f8-92bf-e2182fb49c42.png)。这个函数很有趣，因为它会将输入值压缩，并将其映射到
    0 和 1 之间的值。它也是一个很好的用于梯度计算的函数，因为其导数是已知的，且易于计算：![](img/8df4cea8-a438-4497-a673-8887fe5d4439.png)。
- en: 'In Python, we could easily code the sigmoid as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以很容易地编写以下 sigmoid 函数代码：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Finally, the output layer consists of two neurons that, in this case, we will
    use to model each of the target classes, the positive spiral, and the negative
    spiral.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层由两个神经元组成，在本例中我们将用它们来建模每个目标类别，即正螺旋和负螺旋。
- en: With this in mind, we can do backprop to correct the weights based on the direction
    of the gradient that minimizes the error for a given set of labeled samples; for
    more details, refer to this tutorial (Florez, O. U. 2017). We will be following
    the steps outlined in the following sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以通过反向传播（backprop）来根据梯度的方向调整权重，从而最小化给定标签样本集的误差；更多详细信息，请参考此教程（Florez,
    O. U. 2017）。我们将按照接下来的步骤进行操作。
- en: Step 1 –** I**nitialization
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步 – **初始化**
- en: 'We will perform an initial step in which we *randomly initialize* the network
    weights. In our example, we will use the following values:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行初始步骤，在此步骤中我们 *随机初始化* 网络权重。在我们的示例中，我们将使用以下值：
- en: '![](img/3d2e1892-0403-49d9-83f3-999d370f3f82.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d2e1892-0403-49d9-83f3-999d370f3f82.png)'
- en: '![](img/12423628-7956-4956-92bd-bb2f26ca49f6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12423628-7956-4956-92bd-bb2f26ca49f6.png)'
- en: 'In Python, we can generate these weights between `-1` and `1` by using the
    following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以通过以下方式生成介于 `-1` 和 `1` 之间的这些权重：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Step 2 – The forward pass
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步 – 前向传播
- en: 'The next step would be the **forward pass**. In this step, the input, ![](img/ac0661e9-7c97-4ce4-acc3-c76ebead6fa6.png),
    is presented at the input layer and propagated forward into the network until
    we observe the resulting vector in the output layer. The forward pass in our small
    example would be as follows. We first begin with a linear transformation of a
    single sample, ![](img/c8374d95-3dae-4f96-bdb1-b0346cc68a35.png), using weight ![](img/a3f1d43f-f209-430f-8494-f86c66078dd5.png) in
    the first layer:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是 **前向传播**。在此步骤中，输入 ![](img/ac0661e9-7c97-4ce4-acc3-c76ebead6fa6.png) 被传递到输入层，并向前传播到网络中，直到我们在输出层观察到结果向量。我们的小示例中的前向传播如下所示。我们首先对单个样本
    ![](img/c8374d95-3dae-4f96-bdb1-b0346cc68a35.png) 进行线性变换，使用第一层中的权重 ![](img/a3f1d43f-f209-430f-8494-f86c66078dd5.png)：
- en: '![](img/e2b5afa8-5943-4bbc-9b45-388f78621b69.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2b5afa8-5943-4bbc-9b45-388f78621b69.png)'
- en: 'Thus, for some cases of [![](img/76b30968-7c2c-49fc-bf0c-153e1850d087.png)],
    we calculate the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于某些情况下的 [![](img/76b30968-7c2c-49fc-bf0c-153e1850d087.png)]，我们计算如下：
- en: '![](img/494ef2a0-670b-4f9e-ad84-f12556367caa.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/494ef2a0-670b-4f9e-ad84-f12556367caa.png)'
- en: 'This would result in the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下结果：
- en: '![](img/773c3be4-8bd4-44d9-87e3-59b2eeeed61a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/773c3be4-8bd4-44d9-87e3-59b2eeeed61a.png)'
- en: 'Then, we pass ![](img/21bd032a-0138-413e-9733-354daa3b2bc6.png) through the
    sigmoid function and obtain ![](img/9cce632f-89d6-4696-a577-11600b215a51.png),
    which is the output of the three neurons in the first hidden layer. This results
    in the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将 ![](img/21bd032a-0138-413e-9733-354daa3b2bc6.png) 传递通过 sigmoid 函数并得到
    ![](img/9cce632f-89d6-4696-a577-11600b215a51.png)，这就是第一隐藏层中三个神经元的输出。结果如下：
- en: '![](img/209f362a-3c70-43a8-b01c-6d846a062c5e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/209f362a-3c70-43a8-b01c-6d846a062c5e.png)'
- en: '![](img/8bb3dfca-4b6d-4d08-a799-ca3301dd95e8.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb3dfca-4b6d-4d08-a799-ca3301dd95e8.png)'
- en: 'This could be implemented as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以这样实现：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One interesting way to look at what we have accomplished so far in the first
    layer is that we have mapped the input data, which was in two dimensions, into
    three dimensions, which will be now processed to observe the output back in two
    dimensions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以一种有趣的方式来看待我们在第一层所取得的成果，我们已经将二维的输入数据映射到三维空间，现在这些数据将被处理以便观察输出再回到二维空间。
- en: 'The same process is repeated for any subsequent layers in the group of hidden
    layers. In our example, we will do this only one more time for the output layer.
    We calculate the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的过程会在后续的隐藏层中重复。在我们的例子中，我们只会为输出层再做一次。我们计算如下：
- en: '![](img/20021201-4248-4189-866b-7f574e564d0c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20021201-4248-4189-866b-7f574e564d0c.png)'
- en: 'This results in the following calculation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下计算结果：
- en: '![](img/9811422c-83a4-45a6-bf05-14f1acbbe4d0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9811422c-83a4-45a6-bf05-14f1acbbe4d0.png)'
- en: 'This leads to the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下结果：
- en: '![](img/02620605-1286-4589-990a-108cea5d8da7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02620605-1286-4589-990a-108cea5d8da7.png)'
- en: 'Again, we pass ![](img/47d30185-f322-4978-b271-65d1317e2fc4.png) through the
    sigmoid function and obtain ![](img/0b3f0d9a-2f04-4ab4-903a-e68566f7d410.png),
    which is the output of the two neurons in the output layer. This results in the
    following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将 ![](img/47d30185-f322-4978-b271-65d1317e2fc4.png) 通过sigmoid函数传递，并得到 ![](img/0b3f0d9a-2f04-4ab4-903a-e68566f7d410.png)，这是输出层中两个神经元的输出。这导致了以下结果：
- en: '![](img/cd3a9429-e4b9-46db-a9e1-9e39fafac287.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd3a9429-e4b9-46db-a9e1-9e39fafac287.png)'
- en: '![](img/26ce4485-4520-44e6-b87c-2e9a6f3f589b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26ce4485-4520-44e6-b87c-2e9a6f3f589b.png)'
- en: 'We implement this as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现方式如下：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this point, we need to give some meaning to this output so that we can determine
    the next step. What we would like to model in these two last neurons is the probability
    of the input data, ![](img/01f2e877-eafd-4a0d-ab5a-f136d34f193e.png), belonging
    to the positive class in ![](img/11d943f1-5879-4507-b3d3-29b4bf9cf0f8.png), and
    the probability of it belonging to the negative class in ![](img/f61d8b20-a71c-42c9-8d70-bc8e929c32d3.png).
    The next step is to establish an error metric in order to learn.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要为这个输出赋予一些意义，以便确定下一步的操作。我们希望在这两个神经元中建模的是输入数据 ![](img/01f2e877-eafd-4a0d-ab5a-f136d34f193e.png) 属于正类在 ![](img/11d943f1-5879-4507-b3d3-29b4bf9cf0f8.png)
    中的概率，以及属于负类在 ![](img/f61d8b20-a71c-42c9-8d70-bc8e929c32d3.png) 中的概率。下一步是建立一个误差度量来进行学习。
- en: Error metrics, or error functions, are also known as **loss** functions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 误差度量，或称误差函数，也叫做**损失**函数。
- en: Step 3 – Calculating loss
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 计算损失
- en: 'The next step is to define and **calculate the total loss**. In [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml),
    *Learning from Data*, we discussed some error metrics (or losses), such as the
    **Mean Squared Error** (**MSE**):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义并**计算总损失**。在[第4章](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml)《从数据中学习》中，我们讨论了一些误差度量（或损失），例如**均方误差**（**MSE**）：
- en: '![](img/557a0c88-66bc-44cf-bfa2-6e2951d1efe6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/557a0c88-66bc-44cf-bfa2-6e2951d1efe6.png)'
- en: 'It is important to think about this loss in terms of its derivative since we
    want to adjust the weights of the network in terms of the gradient as given by
    this loss function. Thus, we can do small changes that do not affect at all the
    overall result of the learning process but can result in nice derivatives. For
    example, if we take the derivative of ![](img/4ccc438c-c376-4129-933f-88b8f8071a1f.png),
    the square will imply a multiplication by a factor of 2, but we could nullify
    the effect of that by slightly modifying the MSE, introducing a division by 2,
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 思考这个损失函数的导数非常重要，因为我们希望根据该损失函数提供的梯度调整网络的权重。因此，我们可以进行小的调整，这些调整不会影响学习过程的整体结果，但能得到很好的导数。例如，如果我们对 ![](img/4ccc438c-c376-4129-933f-88b8f8071a1f.png)
    取导数，平方将导致乘以2的因子，但我们可以通过稍微修改MSE，加入除以2的操作，来抵消这一影响，具体如下：
- en: '![](img/dddcf0c5-0e8f-44c0-aea5-50e5a1d0fd75.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dddcf0c5-0e8f-44c0-aea5-50e5a1d0fd75.png)'
- en: 'This loss, therefore, can be used to determine how "wrong" the predictions
    are from the actual target outcome. In the preceding example, the desired outcome
    was as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个损失可以用来确定预测与实际目标结果的“偏差”有多大。在前面的例子中，期望的结果如下：
- en: '![](img/19fb4378-3d63-470b-9df5-289c8b3d8de9.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19fb4378-3d63-470b-9df5-289c8b3d8de9.png)'
- en: 'The predicted response was as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的响应如下所示：
- en: '![](img/87a96822-6701-4595-b8e3-a825c19c20e9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87a96822-6701-4595-b8e3-a825c19c20e9.png)'
- en: 'This is normal since the weights were initialized at random; thus, it is expected
    from the model to perform poorly. The network can be further improved by using
    a modern approach that penalizes weights from taking on very large values. In
    neural networks, there is always a risk of having *exploding* or *vanishing* gradients,
    and a simple technique to reduce the effects of large gradients is to put a limit
    on the scale of the numbers that the weights can take. This is widely known as **regularization***.*
    It leads to other nice properties, such as *sparse *models. We can achieve this regularization
    by modifying the loss as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正常的，因为权重是随机初始化的；因此，模型的表现较差是可以预期的。可以通过使用现代方法进一步改进网络，这些方法对权重采取过大值进行惩罚。在神经网络中，总是存在*梯度爆炸*或*梯度消失*的风险，而减少大梯度影响的一种简单方法是限制权重能够取的数值范围。这被广泛称为**正则化**。它还带来了其他良好的特性，例如*sparse*（稀疏）模型。我们可以通过以下方式修改损失函数来实现这种正则化：
- en: '![](img/130cef99-5d09-40e4-8ab4-dfc60e4ee288.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/130cef99-5d09-40e4-8ab4-dfc60e4ee288.png)'
- en: 'This loss can be implemented as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数可以如下实现：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The added regularization term adds up all the weights in each layer and large
    weights are penalized according to the [![](img/db43bf9a-01db-41ef-9b36-e1ef465034dd.png)] parameter.
    This is a hyperparameter that needs to be fine-tuned by ourselves. A large [![](img/06a4486b-5db9-45b3-919d-207024d4c9f8.png)]
    value penalizes heavily any large weights, and a small [![](img/b2c50ffc-d77b-4ca8-8302-f1fffbdc908c.png)] value
    ignores any effects of the weights in the learning process. This is the loss function
    we will use in this model, and note that the regularization term is also easily
    differentiable.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的正则化项将每层中的所有权重相加，并根据[![](img/db43bf9a-01db-41ef-9b36-e1ef465034dd.png)] 参数对大的权重进行惩罚。这是一个超参数，需要我们自己进行微调。一个大的[![](img/06a4486b-5db9-45b3-919d-207024d4c9f8.png)]值会对任何大权重进行重罚，而一个小的[![](img/b2c50ffc-d77b-4ca8-8302-f1fffbdc908c.png)]值则忽略权重在学习过程中的任何影响。这就是我们将在此模型中使用的损失函数，值得注意的是，正则化项也是容易求导的。
- en: Step 4 – The backward pass
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 反向传播
- en: The next step is to perform the **backward pass**. The goal is to adjust the
    weights in proportion to the loss and in a direction that reduces it. We start
    by calculating the partial derivative of ![](img/249ffbfe-f393-437b-bd72-7a54f1c8039d.png) with
    respect to the weights in the output layer, ![](img/c953fad4-6078-47d9-9fe2-bca5e1dbdb72.png),
    and then with respect to the first layer,![](img/ad16f9ee-de9c-4e84-b2ea-fa99247469ad.png).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是执行**反向传播**。目标是根据损失的大小调整权重，并朝着减少损失的方向进行调整。我们首先计算关于输出层权重的偏导数 ![](img/249ffbfe-f393-437b-bd72-7a54f1c8039d.png)，然后计算关于第一层权重的偏导数
    ![](img/ad16f9ee-de9c-4e84-b2ea-fa99247469ad.png)。
- en: 'Let''s begin the *backward pass* by solving the first partial derivative. We
    can do so by using the well-known chain rule that allows us to decompose the main
    derivative in pieces that represent the same process; we do that as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过解决第一个偏导数来开始*反向传播*。我们可以通过使用著名的链式法则来做到这一点，该法则允许我们将主导数分解成表示相同过程的多个部分；我们可以按如下方式进行：
- en: '![](img/168170fe-622b-46db-9d3d-9bcb86e1ee2d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/168170fe-622b-46db-9d3d-9bcb86e1ee2d.png)'
- en: 'Here, ![](img/e40ad2d2-af44-43ac-bbc2-822951f2f39a.png) for all cases of ![](img/fb3adba7-f666-4239-97c6-576e47a83bcd.png).
    If we define each piece of these partial derivatives independently, we arrive
    at the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， ![](img/e40ad2d2-af44-43ac-bbc2-822951f2f39a.png) 适用于所有的 ![](img/fb3adba7-f666-4239-97c6-576e47a83bcd.png)。如果我们独立定义这些偏导数的每一部分，我们就得到了以下结果：
- en: '![](img/b94ddeac-2053-40a2-b653-47cc064ef75b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b94ddeac-2053-40a2-b653-47cc064ef75b.png)'
- en: '![](img/fb13fb48-5849-40a3-a9f1-69bbe20a5f61.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb13fb48-5849-40a3-a9f1-69bbe20a5f61.png)'
- en: '![](img/ab902ca0-b5c6-404f-9bde-6cc193f959b6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab902ca0-b5c6-404f-9bde-6cc193f959b6.png)'
- en: 'These three partial derivatives have an exact solution every time. In our example,
    their values would be as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个偏导数每次都有一个精确的解。在我们的例子中，它们的值将如下所示：
- en: '![](img/399a46ae-839d-47bf-acf3-d691b35bbc1f.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/399a46ae-839d-47bf-acf3-d691b35bbc1f.png)'
- en: '![](img/076e587b-de5f-44e5-96e4-f742f263be96.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/076e587b-de5f-44e5-96e4-f742f263be96.png)'
- en: '![](img/7dbe6388-5a1c-49da-aa72-7eb361e876c5.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dbe6388-5a1c-49da-aa72-7eb361e876c5.png)'
- en: 'Now, since we need to update the weights, ![](img/0a204667-c6e2-48b4-9b72-cacff3d9de90.png),
    we need a 3 x 2 matrix, and, therefore, we can get this update by multiplying
    the vectors of the partial derivatives, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们需要更新权重， ![](img/0a204667-c6e2-48b4-9b72-cacff3d9de90.png)，我们需要一个3 x 2的矩阵，因此，我们可以通过以下方式将偏导数的向量相乘来获得这个更新：
- en: '![](img/2789ffb5-2f1c-41db-8937-c4e706cc7551.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2789ffb5-2f1c-41db-8937-c4e706cc7551.png)'
- en: 'To get this result, we first need to perform an element-wise multiplication
    of the two small vectors on the right, and then perform an ordinary multiplication
    by the left transposed vector. In Python, we could do this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到这个结果，我们首先需要对右侧的两个小向量进行逐元素乘法，然后通过左侧转置向量执行常规乘法。在Python中，我们可以这样做：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have calculated the derivative, we can perform an update of the
    weights using a traditional scaling factor on the gradient known as the **learning
    rate**. We calculate the new ![](img/60a35181-ab53-4305-9cfe-8f5abc430dfe.png) value,
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了导数，我们可以使用传统的梯度缩放因子来更新权重，这个因子被称为**学习率**。我们计算新的![](img/60a35181-ab53-4305-9cfe-8f5abc430dfe.png)值，如下所示：
- en: '![](img/549867f0-5708-447b-934e-bb0e66cd71cd.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/549867f0-5708-447b-934e-bb0e66cd71cd.png)'
- en: '![](img/e7219868-e0c4-4927-81b5-2cb7d6dd3731.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7219868-e0c4-4927-81b5-2cb7d6dd3731.png)'
- en: The **learning rate** is a mechanism that we use in machine learning to limit
    the influence of the derivatives in the update process. Remember that the derivative
    is interpreted as the rate of change of the weights given some input data. A *large*
    learning rate values too much the direction and magnitude of the derivatives and
    has the risk of skipping a good local minimum. A *small* learning rate only partially
    considers the information of the derivative at the risk of making very slow progress
    toward a local minimum. The learning rate is another hyperparameter that needs
    to be tuned.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率**是我们在机器学习中用来限制导数在更新过程中影响的一种机制。记住，导数被解释为在给定输入数据下，权重变化的速率。一个*大*的学习率过多地决定了导数的方向和幅度，存在跳过良好局部最小值的风险。一个*小*的学习率则仅部分考虑了导数的信息，可能导致朝局部最小值的进展非常缓慢。学习率是需要调整的另一个超参数。'
- en: 'Now, we proceed to calculate the next derivative, ![](img/ad16f9ee-de9c-4e84-b2ea-fa99247469ad.png),
    which will allow us to calculate the update on ![](img/d34676f3-5d66-40b6-9a7f-fa10370f8fe6.png).
    We begin by defining the partial derivative and attempt to simplify its calculation,
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续计算下一个导数，![](img/ad16f9ee-de9c-4e84-b2ea-fa99247469ad.png)，这将使我们能够计算关于![](img/d34676f3-5d66-40b6-9a7f-fa10370f8fe6.png)的更新。我们首先定义偏导数并尝试简化其计算，如下所示：
- en: '![](img/709107ac-e03d-46ec-95c8-9e42c7b2eb1b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/709107ac-e03d-46ec-95c8-9e42c7b2eb1b.png)'
- en: 'If we pay close attention to the first partial derivative, ![](img/13627e0b-94a6-4a58-88d2-c8481dc86166.png),
    we can notice that its derivative is defined as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细关注第一个偏导数，![](img/13627e0b-94a6-4a58-88d2-c8481dc86166.png)，我们可以注意到其导数定义如下：
- en: '![](img/cd8c817a-371a-4817-abb3-59a86126e61f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd8c817a-371a-4817-abb3-59a86126e61f.png)'
- en: 'But the underlined term has already been calculated before! Notice that the
    underlined term is equivalent to the underlined term in the previously defined
    equation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但下划线部分已经在之前计算过了！注意，下划线部分等价于在先前定义方程中的下划线部分：
- en: '![](img/1d1e6ffe-8f5f-4dc1-b1ce-7d2708de3be0.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d1e6ffe-8f5f-4dc1-b1ce-7d2708de3be0.png)'
- en: This is a nice property that is possible due to the chain rule in differentiation
    and allows us to *recycle* computations and have a much more efficient learning
    algorithm. This nice property also tells us that we are indeed incorporating information
    of deeper layers into layers closer to the input. Let's now proceed to the individual
    calculation of each partial derivative knowing that we have done some of the work
    already.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的性质，得益于微分链式法则，它使我们能够*重复利用*计算，并拥有一个更加高效的学习算法。这个优良的性质还告诉我们，实际上我们正在将更深层的信息融入到靠近输入的层中。现在，我们继续进行每个偏导数的单独计算，因为我们已经完成了一部分工作。
- en: 'Since ![](img/83fa979c-2b29-4a23-b18f-1bd464a3a6ac.png), then the first term
    can be expressed as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于![](img/83fa979c-2b29-4a23-b18f-1bd464a3a6ac.png)，那么第一项可以表示为：
- en: '![](img/7ea41332-0f82-4bd3-bdb1-f0fa00ab77b2.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ea41332-0f82-4bd3-bdb1-f0fa00ab77b2.png)'
- en: 'In our example, this leads to the following result:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，这会导致以下结果：
- en: '![](img/bb58611e-b0c9-45b9-b2fd-97e409c8f584.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb58611e-b0c9-45b9-b2fd-97e409c8f584.png)'
- en: 'Now, the second term in the partial derivative can be calculated as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，偏导数中的第二项可以如下计算：
- en: '![](img/d4314651-4f29-4dcf-9ce7-75532d6d1ce6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4314651-4f29-4dcf-9ce7-75532d6d1ce6.png)'
- en: 'This leads to the following vector:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下向量：
- en: '![](img/2da31a23-bb5c-43bb-9293-c6b49399ed30.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2da31a23-bb5c-43bb-9293-c6b49399ed30.png)'
- en: 'After this, we are now able to calculate the last term, which can be directly
    computed as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们现在能够计算最后一项，可以直接按如下方式计算：
- en: '![](img/5894600e-742e-4e45-945c-c075eca06b7b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5894600e-742e-4e45-945c-c075eca06b7b.png)'
- en: 'Finally, we can replace the results of the individual partial derivatives into
    the products of the chain rule:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将每个偏导数的结果代入链式法则的乘积中：
- en: '![](img/74ed95e2-363a-43a8-b0b0-54f5c24ab4b2.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74ed95e2-363a-43a8-b0b0-54f5c24ab4b2.png)'
- en: 'This is obtained by rearranging the vectors to obtain a resulting matrix consistent
    with the weight matrix dimensions, ![](img/91f5bfed-c333-4cf4-ab43-616158f52c3a.png).
    The multiplications lead to the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新排列向量以获得与权重矩阵维度一致的结果矩阵，![](img/91f5bfed-c333-4cf4-ab43-616158f52c3a.png)即可获得此结果。乘法运算得到以下结果：
- en: '![](img/4f483e72-8f66-4e58-aa68-e822e281194b.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f483e72-8f66-4e58-aa68-e822e281194b.png)'
- en: 'In Python, we do this like so:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们这样做：
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, the corresponding ![](img/0c08586d-1664-4776-adea-550af80cbb2e.png)
    update is calculated as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，相应的![](img/0c08586d-1664-4776-adea-550af80cbb2e.png)更新计算如下：
- en: '![](img/93769393-a7a5-48b0-bee5-dc4ce5605f54.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93769393-a7a5-48b0-bee5-dc4ce5605f54.png)'
- en: '![](img/39fcb173-a44a-4dbd-8a0e-04dd4175e45e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39fcb173-a44a-4dbd-8a0e-04dd4175e45e.png)'
- en: 'This concludes the backprop algorithm by assigning [![](img/c339651e-da7e-44c6-98d5-a09c2c9e7846.png)] at
    iteration ![](img/9d9a52b0-eb5f-436e-9623-c6b32f0993b3.png) (or *epoch*), which
    we implement as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在迭代（*epoch*）时分配[![](img/c339651e-da7e-44c6-98d5-a09c2c9e7846.png)]来完成反向传播算法，具体实现如下：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The process repeats for as many epochs as we wish. We could let the algorithm
    run with the following parameters:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会根据我们希望的时期（epochs）重复进行。我们可以使用以下参数让算法运行：
- en: '![](img/e328a616-4152-4492-9b54-5da84e242069.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e328a616-4152-4492-9b54-5da84e242069.png)'
- en: 'Then, the resulting separating hyperplane would look like that in the following
    figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，得到的分离超平面将如下图所示：
- en: '![](img/86648a66-1ff6-40fc-9fef-cd5461a595b3.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86648a66-1ff6-40fc-9fef-cd5461a595b3.png)'
- en: Figure 6.5 - Separating the hyperplane of the sample three-neuron MLP
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 - 分离三神经元MLP的超平面
- en: This figure shows that there are many samples that are misclassified, which
    are depicted as black dots. The total accuracy is 62%. Clearly, three neurons
    are good enough to produce a classifier better than random chance; however, this
    is not the best possible outcome. What we must do now is tune-up the classifier
    by changing the hyperparameters and the number of neurons or layers. This is what
    we will discuss next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示有许多样本被错误分类，这些错误分类的样本以黑点表示。总准确率为62%。显然，三个神经元足以产生一个比随机猜测更好的分类器；然而，这并不是最理想的结果。接下来，我们必须通过调整超参数和神经元或层的数量来调优分类器。这就是我们接下来要讨论的内容。
- en: Finding the best hyperparameters
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最佳超参数
- en: 'There is a simpler way of coding what we coded in the previous section using
    Keras. We can rely on the fact that the backprop is coded correctly and is improved
    for stability and there is a richer set of other features and algorithms that
    can improve the learning process. Before we begin the process of optimizing the
    set of hyperparameters of the MLP, we should indicate what would be the equivalent
    implementation using Keras. The following code should reproduce the same model,
    almost the same loss function, and almost the same backprop methodology:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras编码，我们可以用一种更简单的方式实现我们在上一节中编写的代码。我们可以依赖于反向传播算法已经被正确编码，并且经过改进以增强稳定性，同时还有一套丰富的其他特性和算法可以提升学习过程。在我们开始优化MLP的超参数之前，应该指出，使用Keras时的等效实现是什么。以下代码应该重现相同的模型、几乎相同的损失函数和几乎相同的反向传播方法：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This would produce an error of 62.3% and a decision boundary like the one shown
    in *Figure 6.7*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生62.3%的错误率和如*图6.7*所示的决策边界：
- en: '![](img/616deaf2-cbe7-4c52-b3b8-d2cc511a89bb.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/616deaf2-cbe7-4c52-b3b8-d2cc511a89bb.png)'
- en: Figure 6.6 – Keras-based MLP for the same model as in *Figure 6.5*
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 基于Keras的MLP，和*图6.5*中的模型相同
- en: The figure is very similar to *Figure 6.6*, which is expected since they are
    the same model. But let's review briefly the meaning of the model described in
    the code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该图与*图6.6*非常相似，预期如此，因为它们是相同的模型。但让我们简要回顾一下代码中描述的模型含义。
- en: As explained before, `from tensorflow.keras.models import Sequential` imports
    the Sequential library, which allows us to create a *sequential *model as opposed
    to a *functional* approach to model creation, `mlp = Sequential()`, and it also
    allows us to add elements to the model, `mlp.add()`, such as multiple layers of
    neurons (dense layers): `Dense(...)`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`from tensorflow.keras.models import Sequential`导入了Sequential库，它允许我们创建一个*顺序模型*，而不是*函数式*模型创建方法，`mlp
    = Sequential()`，它还允许我们向模型中添加元素，`mlp.add()`，例如多个神经元层（全连接层）：`Dense(...)`。
- en: The first layer of the sequential model must specify the dimension of the input
    (input layer size), which in this case is `2`, and the activation function, which
    is a sigmoid: `mlp.add(Dense(3, input_dim=2, activation='sigmoid'))`. In this
    case, the number `3` indicates how many neurons this model will have in the first
    hidden layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模型的第一层必须指定输入的维度（输入层大小），在本例中为 `2`，并指定激活函数为 sigmoid：`mlp.add(Dense(3, input_dim=2,
    activation='sigmoid'))`。这里的数字 `3` 表示该模型在第一隐藏层中将有多少个神经元。
- en: The second (and last) layer is similar but denotes the two neurons in the output
    layer: `mlp.add(Dense(2, activation='sigmoid'))`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层（也是最后一层）类似，但表示输出层中的两个神经元：`mlp.add(Dense(2, activation='sigmoid'))`。
- en: 'Once the sequential model has been specified, we must compile it, `mlp.compile(...)`,
    defining the loss to be minimized, `loss=''mean_squared_error''`, the optimization
    (backprop) algorithm to be used, `optimizer=''sgd''`, and also a list of what
    metrics to report after each training epoch, `metrics=[''accuracy'']`. The mean
    squared loss defined here does not include the regularization term described before,
    but this should not have a greater impact here; the loss is, therefore, something
    we have seen before:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦指定了顺序模型，我们必须对其进行编译，`mlp.compile(...)`，定义要最小化的损失函数，`loss='mean_squared_error'`，要使用的优化（反向传播）算法，`optimizer='sgd'`，以及在每个训练周期后报告的度量列表，`metrics=['accuracy']`。这里定义的均方损失函数不包括前面提到的正则化项，但这不应对结果产生更大影响；因此，损失函数是我们之前见过的：
- en: '![](img/557a0c88-66bc-44cf-bfa2-6e2951d1efe6.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/557a0c88-66bc-44cf-bfa2-6e2951d1efe6.png)'
- en: The `sgd` optimizer defines an algorithm known as **stochastic gradient descent**.
    This is a robust way of calculating the gradient and updating the weights accordingly
    and has been around since the 1950s [Amari, S. I. 1993]. In Keras, it has a default
    *learning rate* of ![](img/be921a2e-9df9-4a81-8421-690fd18bdd18.png); however,
    this rate has a decay strategy that allows the learning rate to adapt to the learning
    process.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`sgd` 优化器定义了一种算法，称为 **随机梯度下降**。这是一种计算梯度并相应更新权重的稳健方法，自20世纪50年代以来就已经出现了[Amari,
    S. I. 1993]。在 Keras 中，默认的 *学习率* 是 ![](img/be921a2e-9df9-4a81-8421-690fd18bdd18.png)；然而，该学习率有衰减策略，使学习率能够根据学习过程进行调整。'
- en: 'With this in mind, what we will do is vary the following hyperparameters:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们将调整以下超参数：
- en: The learning rate, ![](img/ca7c445c-d7a6-406b-b680-84ed8102ad32.png), is adaptive.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率，![](img/ca7c445c-d7a6-406b-b680-84ed8102ad32.png)，是自适应的。
- en: The number of layers, between 2, 3, and 4, with 16 neurons each (except the
    output layer).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数为2、3或4，每层16个神经元（输出层除外）。
- en: The activation function, either ReLU or sigmoid.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数，可以是 ReLU 或 sigmoid。
- en: 'This can be achieved by running several experiments with cross-validation,
    as explained before in [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml),
    *Learning from Data*. The following table shows a comprehensive list of the experiments
    performed under five-fold cross-validation and the corresponding results:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过进行多次交叉验证实验来实现，正如在[第4章](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml)《从数据中学习》中所解释的那样。下表展示了在五折交叉验证下执行的实验和相应的结果：
- en: '| **Exp.** | **Hyperparameters** | **Mean Accuracy** | **Std.** |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| **实验** | **超参数** | **平均准确率** | **标准差** |'
- en: '| `a` | (16-Sigmoid, 2-Sigmoid) | 0.6088 | 0.004 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `a` | (16-Sigmoid, 2-Sigmoid) | 0.6088 | 0.004 |'
- en: '| `b` | (16-ReLU, 2-Sigmoid) | 0.7125 | 0.038 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `b` | (16-ReLU, 2-Sigmoid) | 0.7125 | 0.038 |'
- en: '| `c` | (16-Sigmoid, 16-Sigmoid, 2-Sigmoid) | 0.6128 | 0.010 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `c` | (16-Sigmoid, 16-Sigmoid, 2-Sigmoid) | 0.6128 | 0.010 |'
- en: '| `d` | (16-ReLU, 16-Sigmoid, 2-Sigmoid) | 0.7040 | 0.067 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `d` | (16-ReLU, 16-Sigmoid, 2-Sigmoid) | 0.7040 | 0.067 |'
- en: '| `e` | (16-Sigmoid, 16-Sigmoid, 16-Sigmoid, 2-Sigmoid) | 0.6188 | 0.010 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `e` | (16-Sigmoid, 16-Sigmoid, 16-Sigmoid, 2-Sigmoid) | 0.6188 | 0.010 |'
- en: '| `f` | (16-ReLU, 16-Sigmoid, 16-ReLU, 2-Sigmoid) | 0.7895 | 0.113 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| `f` | (16-ReLU, 16-Sigmoid, 16-ReLU, 2-Sigmoid) | 0.7895 | 0.113 |'
- en: '| `g` | (16-ReLU, 16-ReLU, 16-Sigmoid, 2-Sigmoid) | **0.9175** | 0.143 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `g` | (16-ReLU, 16-ReLU, 16-Sigmoid, 2-Sigmoid) | **0.9175** | 0.143 |'
- en: '| `h` | (16-ReLU, 16-ReLU, 16-ReLU, 2-Sigmoid) | 0.9050 | 0.094 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `h` | (16-ReLU, 16-ReLU, 16-ReLU, 2-Sigmoid) | 0.9050 | 0.094 |'
- en: '| `i` | (16-ReLU, 16-Sigmoid, 16-Sigmoid, 2-Sigmoid) | 0.6608 | 0.073 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `i` | (16-ReLU, 16-Sigmoid, 16-Sigmoid, 2-Sigmoid) | 0.6608 | 0.073 |'
- en: 'Note that other experiments were performed with an additional fifth layer,
    but the results were not much better in terms of average performance and variability.
    It appears that four layers with as little as 16 neurons in each layer (except
    the output layer, with 2) are sufficient to produce adequate class separation.
    *Figure 6.8* shows a sample run from experiment `g`, which achieved the highest
    performance with 99% accuracy:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，另有一些实验是加上了第五层进行的，但在平均性能和变异性方面并未显著提高。看来，四层，每层仅有 16 个神经元（除了输出层为 2 个神经元），就足以产生足够的类别分离。*图
    6.8* 展示了来自实验 `g` 的一个样本运行，达到了 99% 的最高准确率：
- en: '![](img/ec897d46-8327-46cb-8928-07f4081803f9.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec897d46-8327-46cb-8928-07f4081803f9.png)'
- en: Figure 6.7 – Classification boundaries for the two-spirals dataset using a four-layered
    (16,16,16,2) neural network. Corresponds to experiment g in Table 1
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 使用四层（16,16,16,2）神经网络对两个螺旋数据集的分类边界。对应于表 1 中的实验 g。
- en: A visual inspection of *Figure 6.8* reveals that the largest margin of confusion
    is in the center area where the spirals originate and are very close to each other.
    Notice also that the separating hyperplane seems to be non-smooth in some areas,
    which is typical of the MLP. Some suggest that this phenomenon is due to the fact
    that neurons in the input layer are using a linear function to approximate a function,
    and deeper layers are mixtures of linear functions that produce non-linear functions
    based on such linear functions. Of course, it is much more complicated than that,
    but it is interesting to note here.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *图 6.8* 进行视觉检查可以发现，最大的混淆边缘位于螺旋的起始中心区域，那里两条螺旋非常接近。还可以注意到，分隔超平面在某些区域似乎是不平滑的，这通常是
    MLP 的特点。有些人认为，这一现象是由于输入层的神经元使用线性函数来近似某个函数，而更深层的神经元则是线性函数的混合体，这些混合体基于这些线性函数生成非线性函数。当然，情况要复杂得多，但这里值得注意的是这一点。
- en: Before we conclude this chapter, note that there are other hyperparameters that
    we could have optimized empirically. We could have chosen different optimizers,
    such as `adam` or `rmsprop`; we could have tried other activation functions such
    as `tanh`, or `softmax`; we could have tried more layers; or we could have tried
    more (or less) and different numbers of neurons in increasing, decreasing, or
    mixed order. However, for now, these experiments are sufficient to make the point
    that experimentation with different things is key in finding what works best for
    us in our particular application or the problem we are trying to solve.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这一章之前，请注意还有其他超参数是我们可以通过经验进行优化的。我们本可以选择不同的优化器，例如 `adam` 或 `rmsprop`；我们本可以尝试其他激活函数，例如
    `tanh` 或 `softmax`；我们本可以尝试更多的层数；或者我们本可以尝试更多（或更少）且不同数量的神经元，以递增、递减或混合的顺序。不过，现阶段，这些实验足以表明，实验不同的选项是找到最适合我们特定应用或我们要解决的问题的方法的关键。
- en: This concludes our introductory chapters, and the coming ones will look at specific
    types of architecture that have a specific purpose, as opposed to the MLP, which
    is usually considered a multipurpose, fundamental neural network. Our next chapter
    will deal with autoencoders; they can be seen as a special type of neural network
    aiming to encode input data into a smaller dimensional space and then reconstructing
    it back to the original input space, minimizing the loss of information in the
    reconstructed data. An autoencoder allows us to compress data, and learn from
    the data without the label associated with it. The latter makes the autoencoder
    a special kind of neural network that learns using what is categorized as **unsupervised learning***.*
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们的介绍章节，接下来的章节将讨论具有特定用途的架构类型，而不是通常被认为是多用途、基础神经网络的 MLP。我们的下一章将讨论自编码器；它们可以看作是一个特殊类型的神经网络，旨在将输入数据编码成一个更小的维度空间，然后再将其重构回原始输入空间，最小化重构数据中的信息丢失。自编码器允许我们压缩数据，并在没有与数据相关标签的情况下进行学习。后者使得自编码器成为一种特殊的神经网络，采用被归类为**无监督学习**的方法进行学习。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This intermediate-introductory chapter showed the design of an MLP and the paradigms
    surrounding its functionality. We covered the theoretical framework behind its
    elements and we had a full discussion and treatment of the widely known backprop
    mechanism to perform gradient descent on a loss function. Understanding the backprop
    algorithm is key for further chapters since some models are designed specifically
    to overcome some potential difficulties with backprop. You should feel confident
    that what you have learned about backprop will serve you well in knowing what
    deep learning is all about. This backprop algorithm, among other things, is what
    makes deep learning an exciting area. Now, you should be able to understand and
    design your own MLP with different layers and different neurons. Furthermore,
    you should feel confident in changing some of its parameters, although we will
    cover more of this in the further reading.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是一个中级入门章节，展示了MLP的设计及其功能的相关范式。我们讨论了其元素背后的理论框架，并对广为人知的反向传播机制进行了全面的讨论，用于对损失函数进行梯度下降。理解反向传播算法对于后续章节至关重要，因为一些模型专门设计用来克服反向传播可能遇到的一些困难。你应该对你学到的反向传播知识有信心，这将帮助你更好地理解深度学习的本质。反向传播算法，除此之外，是深度学习成为一个令人兴奋的领域的原因之一。现在，你应该能够理解并设计具有不同层和不同神经元的MLP。此外，你应该有信心去改变其一些参数，尽管我们会在后续阅读中详细讲解更多内容。
- en: '[Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders,* will
    continue with an architecture very similar to the MLP that is widely used today
    for many different learning tasks associated with learning representations of
    data. This chapter begins a new part that is dedicated to *unsupervised* *learning*
    algorithms and models based on the type of learning where you can learn from data
    even if it is not labeled.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml)，*自编码器*，将继续介绍一种与MLP非常相似的架构，这种架构今天被广泛应用于许多不同的学习任务，尤其是与数据表示学习相关的任务。本章开启了一个新的部分，专门讨论基于无监督学习的算法和模型，这种学习方式让你即使在数据没有标签的情况下，也能从中学习。'
- en: Questions and answers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Why is the MLP better than the perceptron model?**'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么MLP优于感知器模型？**'
- en: The larger number and layers of neurons give the MLP the advantage over the
    perceptron to model non-linear problems and solve much more complicated pattern
    recognition problems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元数量和层数更多的MLP使其相对于感知器在建模非线性问题和解决更复杂的模式识别问题上具有优势。
- en: '**Why is backpropagation so important to know about? **'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么反向传播如此重要？**'
- en: Because it is what makes neural networks learn in the era of big data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它是让神经网络在大数据时代得以学习的关键。
- en: '**Does the MLP always converge?**'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MLP总是会收敛吗？**'
- en: Yes and no. It does always converge to a local minimum in terms of the loss
    function; however, it is not guaranteed to converge to a global minimum since,
    usually, most loss functions are non-convex and non-smooth.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，也不是。它总是会收敛到损失函数的局部最小值；然而，它不能保证收敛到全局最小值，因为通常大多数损失函数都是非凸的和非平滑的。
- en: '**Why should we try to optimize the hyperparameters of our models?**'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们为什么要尝试优化模型的超参数？**'
- en: Because anyone can train a simple neural network; however, not everyone knows
    what things to change to make it better. The success of your model depends heavily
    on you trying different things and proving to yourself (and others) that your
    model is the best that it can be. This is what will make you a better learner
    and a better deep learning professional.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因为任何人都可以训练一个简单的神经网络；然而，并不是每个人都知道应该改变哪些内容来使其更好。模型的成功在很大程度上取决于你尝试不同的方案，并向自己（和他人）证明你的模型已经是最好的。这将使你成为更好的学习者，也成为更优秀的深度学习专业人员。
- en: References
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological Review*, 65(6), 386.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenblatt, F. (1958). 感知器：一种用于大脑信息存储和组织的概率模型。*Psychological Review*, 65(6),
    386。
- en: Tappert, C. C. (2019). Who is the Father of Deep Learning? *Symposium on Artificial
    Intelligence.*
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tappert, C. C. (2019). 谁是深度学习的奠基人？*人工智能研讨会*。
- en: Hinton, G. E. (1990). Connectionist learning procedures. *Machine learning*.
    Morgan Kaufmann, 555-610.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G. E. (1990). 连接主义学习程序。*Machine learning*。Morgan Kaufmann, 555-610。
- en: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations
    by back-propagating errors. *Nature*, 323(6088), 533-536.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). 通过反向传播误差学习表示。*Nature*,
    323(6088), 533-536。
- en: 'Florez, O. U. (2017). One LEGO at a time: Explaining the Math of How Neural
    Networks Learn. *Online*: [https://omar-florez.github.io/scratch_mlp/](https://omar-florez.github.io/scratch_mlp/).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Florez, O. U. (2017). 一次一个乐高：解释神经网络学习的数学原理。*在线*: [https://omar-florez.github.io/scratch_mlp/](https://omar-florez.github.io/scratch_mlp/).'
- en: Amari, S. I. (1993). Backpropagation and stochastic gradient descent method.
    *Neurocomputing*, 5(4-5), 185-196.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari, S. I. (1993). 反向传播与随机梯度下降方法。*神经计算*, 5(4-5), 185-196.
