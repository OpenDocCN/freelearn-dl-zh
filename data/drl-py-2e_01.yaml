- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Fundamentals of Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的基本原理
- en: '**Reinforcement Learning** (**RL**) is one of the areas of **Machine Learning**
    (**ML**). Unlike other ML paradigms, such as supervised and unsupervised learning,
    RL works in a trial and error fashion by interacting with its environment.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是**机器学习**（**ML**）的一个领域。与其他机器学习范式（如监督学习和无监督学习）不同，强化学习通过与环境互动，以试验和错误的方式进行工作。'
- en: RL is one of the most active areas of research in artificial intelligence, and
    it is believed that RL will take us a step closer towards achieving artificial
    general intelligence. RL has evolved rapidly in the past few years with a wide
    variety of applications ranging from building a recommendation system to self-driving
    cars. The major reason for this evolution is the advent of deep reinforcement
    learning, which is a combination of deep learning and RL. With the emergence of
    new RL algorithms and libraries, RL is clearly one of the most promising areas
    of ML.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是人工智能领域最活跃的研究方向之一，被认为它将使我们更接近实现人工通用智能。强化学习在过去几年中迅速发展，应用范围广泛，从构建推荐系统到自动驾驶汽车都有涉及。这个发展迅速的主要原因是深度强化学习的出现，它是深度学习与强化学习的结合。随着新型强化学习算法和库的涌现，强化学习显然是机器学习领域最具前景的方向之一。
- en: In this chapter, we will build a strong foundation in RL by exploring several
    important and fundamental concepts involved in RL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过探讨强化学习中的几个重要且基本的概念，建立强化学习的坚实基础。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Key elements of RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的关键元素
- en: The basic idea of RL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的基本概念
- en: The RL algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习算法
- en: How RL differs from other ML paradigms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习与其他机器学习范式的区别
- en: The Markov Decision Processes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Fundamental concepts of RL
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的基本概念
- en: Applications of RL
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的应用
- en: RL glossary
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习术语
- en: We will begin the chapter by defining *Key elements of RL*. This will help explain
    *The basic idea of RL*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从定义 *强化学习的关键元素* 开始这一章节。这将有助于解释 *强化学习的基本概念*。
- en: Key elements of RL
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的关键元素
- en: Let's begin by understanding some key elements of RL.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来理解强化学习中的一些关键元素。
- en: Agent
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能体
- en: An agent is a software program that learns to make intelligent decisions. We
    can say that an agent is a learner in the RL setting. For instance, a chess player
    can be considered an agent since the player learns to make the best moves (decisions)
    to win the game. Similarly, Mario in a Super Mario Bros video game can be considered
    an agent since Mario explores the game and learns to make the best moves in the
    game.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体是一个学习做出智能决策的软件程序。我们可以说智能体是强化学习环境中的学习者。例如，棋手可以视为一个智能体，因为他学习如何做出最佳的棋步（决策）以赢得比赛。同样，超级马里奥游戏中的马里奥也可以被视为智能体，因为马里奥在游戏中探索并学习如何做出最佳的行动。
- en: Environment
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: The environment is the world of the agent. The agent stays within the environment.
    For instance, coming back to our chess game, a chessboard is called the environment
    since the chess player (agent) learns to play the game of chess within the chessboard
    (environment). Similarly, in Super Mario Bros, the world of Mario is called the
    environment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是智能体的世界。智能体存在于环境中。例如，在我们的棋局示例中，棋盘就称为环境，因为棋手（智能体）在棋盘（环境）内学习如何下棋。同样，在超级马里奥游戏中，马里奥的世界被称为环境。
- en: State and action
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态与动作
- en: A state is a position or a moment in the environment that the agent can be in.
    We learned that the agent stays within the environment, and there can be many
    positions in the environment that the agent can stay in, and those positions are
    called states. For instance, in our chess game example, each position on the chessboard
    is called the state. The state is usually denoted by *s*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 状态是智能体能够处于的环境中的某个位置或时刻。我们了解到，智能体始终存在于环境中，并且环境中有许多位置，智能体可以停留在这些位置上，这些位置被称为状态。例如，在我们的棋局示例中，棋盘上的每个位置都称为状态。状态通常用
    *s* 来表示。
- en: The agent interacts with the environment and moves from one state to another
    by performing an action. In the chess game environment, the action is the move
    performed by the player (agent). The action is usually denoted by *a*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体通过与环境互动，从一个状态移动到另一个状态，执行一个动作。在棋局环境中，动作是玩家（智能体）执行的棋步。动作通常用 *a* 来表示。
- en: Reward
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励
- en: We learned that the agent interacts with an environment by performing an action
    and moves from one state to another. Based on the action, the agent receives a
    reward. A reward is nothing but a numerical value, say, +1 for a good action and
    -1 for a bad action. How do we decide if an action is good or bad?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，代理者通过执行动作与环境交互，并从一个状态移动到另一个状态。根据动作，代理者会得到一个奖励。奖励只是一个数值，比如，执行一个好的动作得到+1，执行一个坏的动作得到-1。我们如何判断一个动作是好还是坏？
- en: In our chess game example, if the agent makes a move in which it takes one of
    the opponent's chess pieces, then it is considered a good action and the agent
    receives a positive reward. Similarly, if the agent makes a move that leads to
    the opponent taking the agent's chess piece, then it is considered a bad action
    and the agent receives a negative reward. The reward is denoted by *r*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的象棋游戏例子中，如果代理者走了一步吃掉对手的棋子，那么这被认为是一个好动作，代理者会得到积极奖励。同样地，如果代理者的一步棋导致对手吃掉代理者的棋子，那么这被认为是一个坏动作，代理者会得到负面奖励。奖励用*r*来表示。
- en: The basic idea of RL
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的基本思想
- en: Let's begin with an analogy. Let's suppose we are teaching a dog (agent) to
    catch a ball. Instead of teaching the dog explicitly to catch a ball, we just
    throw a ball and every time the dog catches the ball, we give the dog a cookie
    (reward). If the dog fails to catch the ball, then we do not give it a cookie.
    So, the dog will figure out what action caused it to receive a cookie and repeat
    that action. Thus, the dog will understand that catching the ball caused it to
    receive a cookie and will attempt to repeat catching the ball. Thus, in this way,
    the dog will learn to catch a ball while aiming to maximize the cookies it can
    receive.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个类比开始。假设我们正在教一只狗（代理者）接住一个球。我们不是明确地教狗去接球，而是只是把球扔出去，每次狗接住球时我们给它一块饼干（奖励）。如果狗没有接住球，我们就不给它饼干。因此，狗将会找出是哪个动作让它得到了饼干并重复那个动作。这样，狗就会理解接住球让它得到了饼干，并会尝试重复接住球。通过这种方式，狗将学会接住球，并且会努力最大化它可以得到的饼干数量。
- en: Similarly, in an RL setting, we will not teach the agent what to do or how to
    do it; instead, we will give a reward to the agent for every action it does. We
    will give a positive reward to the agent when it performs a good action and we
    will give a negative reward to the agent when it performs a bad action. The agent
    begins by performing a random action and if the action is good, we then give the
    agent a positive reward so that the agent understands it has performed a good
    action and it will repeat that action. If the action performed by the agent is
    bad, then we will give the agent a negative reward so that the agent will understand
    it has performed a bad action and it will not repeat that action.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在强化学习设置中，我们不会教代理者要做什么或如何做；相反，我们会为代理者的每个动作给予奖励。当代理者执行一个好的动作时，我们会给予积极奖励，这样代理者就能理解它已经执行了一个好的动作，并会重复这个动作。如果代理者执行的动作是坏的，那么我们会给予负面奖励，这样代理者就能理解它执行了一个坏的动作，并且不会重复这个动作。
- en: Thus, RL can be viewed as a trial and error learning process where the agent
    tries out different actions and learns the good action, which gives a positive
    reward.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强化学习可以被视为一个试错学习过程，代理者尝试不同的动作并学习到给予积极奖励的好动作。
- en: In the dog analogy, the dog represents the agent, and giving a cookie to the
    dog upon it catching the ball is a positive reward and not giving a cookie is
    a negative reward. So, the dog (agent) explores different actions, which are catching
    the ball and not catching the ball, and understands that catching the ball is
    a good action as it brings the dog a positive reward (getting a cookie).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在狗的类比中，狗代表着代理者，当狗接住球时给它一块饼干是积极奖励，不给饼干则是负面奖励。因此，狗（代理者）探索不同的动作，即接住球和不接住球，并且理解接住球是一个好的动作，因为这会给狗带来积极奖励（得到一块饼干）。
- en: 'Let''s further explore the idea of RL with one more simple example. Let''s
    suppose we want to teach a robot (agent) to walk without hitting a mountain, as
    *Figure 1.1* shows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个更简单的例子进一步探讨强化学习的概念。假设我们想教一个机器人（代理者）在不撞到山的情况下行走，如*图 1.1* 所示：
- en: '![](img/B15558_01_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_01.png)'
- en: 'Figure 1.1: Robot walking'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.1: 机器人行走'
- en: 'We will not teach the robot explicitly to not go in the direction of the mountain.
    Instead, if the robot hits the mountain and gets stuck, we give the robot a negative
    reward, say -1\. So, the robot will understand that hitting the mountain is the
    wrong action, and it will not repeat that action:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会明确教机器人避免朝着山的方向走。相反，如果机器人撞到山并被卡住，我们会给机器人一个负奖励，例如-1。这样，机器人就能明白撞到山是错误的行为，它将不会再重复这一行为：
- en: '![](img/B15558_01_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_02.png)'
- en: 'Figure 1.2: Robot hits mountain'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：机器人撞到山脉
- en: 'Similarly, when the robot walks in the right direction without hitting the
    mountain, we give the robot a positive reward, say +1\. So, the robot will understand
    that not hitting the mountain is a good action, and it will repeat that action:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当机器人朝着正确的方向走且没有撞到山时，我们会给机器人一个正奖励，例如+1。这样，机器人就会明白不撞到山是一个好的动作，它将会重复这一动作：
- en: '![](img/B15558_01_03.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_03.png)'
- en: 'Figure 1.3: Robot avoids mountain'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：机器人避开山脉
- en: Thus, in the RL setting, the agent explores different actions and learns the
    best action based on the reward it gets.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在强化学习的环境中，智能体探索不同的动作，并根据获得的奖励学习最佳动作。
- en: Now that we have a basic idea of how RL works, in the upcoming sections, we
    will go into more detail and also learn the important concepts involved in RL.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对强化学习的基本原理有了一个大致的了解，接下来的章节中，我们将进一步深入，学习强化学习中涉及的重要概念。
- en: The RL algorithm
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习算法
- en: 'The steps involved in a typical RL algorithm are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的强化学习算法的步骤如下：
- en: First, the agent interacts with the environment by performing an action.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，智能体通过执行一个动作与环境进行交互。
- en: By performing an action, the agent moves from one state to another.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行一个动作，智能体从一个状态转移到另一个状态。
- en: Then the agent will receive a reward based on the action it performed.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，智能体会根据它所执行的动作获得一个奖励。
- en: Based on the reward, the agent will understand whether the action is good or
    bad.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于奖励，智能体将理解该行为是好是坏。
- en: If the action was good, that is, if the agent received a positive reward, then
    the agent will prefer performing that action, else the agent will try performing
    other actions in search of a positive reward.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果某个动作是好的，也就是说，智能体收到了正奖励，那么智能体会倾向于执行该动作；否则，智能体会尝试其他动作，寻找能获得正奖励的行动。
- en: RL is basically a trial and error learning process. Now, let's revisit our chess
    game example. The agent (software program) is the chess player. So, the agent
    interacts with the environment (chessboard) by performing an action (moves). If
    the agent gets a positive reward for an action, then it will prefer performing
    that action; else it will find a different action that gives a positive reward.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）基本上是一个试错学习过程。现在，让我们回顾一下我们的国际象棋游戏示例。智能体（软件程序）就是下棋的玩家。因此，智能体通过执行一个动作（走棋）与环境（棋盘）进行交互。如果智能体因为某个动作获得正奖励，那么它会倾向于执行该动作；否则，它会寻找其他能够获得正奖励的动作。
- en: Ultimately, the goal of the agent is to maximize the reward it gets. If the
    agent receives a good reward, then it means it has performed a good action. If
    the agent performs a good action, then it implies that it can win the game. Thus,
    the agent learns to win the game by maximizing the reward.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，智能体的目标是最大化它所获得的奖励。如果智能体得到好的奖励，那就意味着它做出了好的动作。如果智能体做出了好的动作，那就意味着它能够赢得比赛。因此，智能体通过最大化奖励来学习如何赢得比赛。
- en: RL agent in the grid world
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格世界中的强化学习智能体
- en: 'Let''s strengthen our understanding of RL by looking at another simple example.
    Consider the following grid world environment:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过另一个简单的例子来进一步巩固对强化学习的理解。考虑以下的网格世界环境：
- en: '![](img/B15558_01_04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_04.png)'
- en: 'Figure 1.4: Grid world environment'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：网格世界环境
- en: The positions **A** to **I** in the environment are called the states of the
    environment. The goal of the agent is to reach state **I** by starting from state
    **A** without visiting the shaded states (**B**, **C**, **G**, and **H**). Thus,
    in order to achieve the goal, whenever our agent visits a shaded state, we will
    give a negative reward (say -1) and when it visits an unshaded state, we will
    give a positive reward (say +1). The actions in the environment are moving *up*,
    *down*, *right* and *left*. The agent can perform any of these four actions to
    reach state **I** from state **A**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中的位置**A**到**I**被称为环境的状态。智能体的目标是从状态**A**出发，达到状态**I**，并且不经过阴影状态（**B**、**C**、**G**和**H**）。因此，为了实现目标，每当智能体访问阴影状态时，我们会给予一个负奖励（例如-1），而当它访问非阴影状态时，我们会给予一个正奖励（例如+1）。环境中的动作包括*向上*、*向下*、*向右*和*向左*。智能体可以执行这四个动作中的任何一个，以从状态**A**到达状态**I**。
- en: The first time the agent interacts with the environment (the first iteration),
    the agent is unlikely to perform the correct action in each state, and thus it
    receives a negative reward. That is, in the first iteration, the agent performs
    a random action in each state, and this may lead the agent to receive a negative
    reward. But over a series of iterations, the agent learns to perform the correct
    action in each state through the reward it obtains, helping it achieve the goal.
    Let us explore this in detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体第一次与环境互动时（第一轮），它不太可能在每个状态中执行正确的动作，因此它会得到负奖励。也就是说，在第一轮中，智能体在每个状态中执行了一个随机动作，这可能导致智能体收到负奖励。但经过一系列迭代后，智能体通过获得的奖励学会在每个状态中执行正确的动作，从而帮助它实现目标。让我们详细探讨这个过程。
- en: '**Iteration 1**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一轮**'
- en: 'As we learned, in the first iteration, the agent performs a random action in
    each state. For instance, look at the following figure. In the first iteration,
    the agent moves *right* from state **A** and reaches the new state **B**. But
    since **B** is the shaded state, the agent will receive a negative reward and
    so the agent will understand that moving *right* is not a good action in state
    **A**. When it visits state **A** next time, it will try out a different action
    instead of moving *right*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学，在第一轮中，智能体在每个状态中执行了一个随机动作。例如，看看下面的图。在第一轮中，智能体从状态**A**向*右*移动，达到了新状态**B**。但是由于**B**是一个阴影状态，智能体将收到负奖励，因此智能体会明白在状态**A**时，*向右*并不是一个好的动作。当它下次访问状态**A**时，它会尝试不同的动作，而不是向*右*移动：
- en: '![](img/B15558_01_05.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_05.png)'
- en: 'Figure 1.5: Actions taken by the agent in iteration 1'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：第一轮中智能体执行的动作
- en: As *Figure 1.5* shows, from state **B**, the agent moves *down* and reaches
    the new state **E**. Since **E** is an unshaded state, the agent will receive
    a positive reward, so the agent will understand that moving *down* from state
    **B** is a good action.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*图1.5*所示，从状态**B**，智能体向*下*移动，达到了新状态**E**。由于**E**是一个非阴影状态，智能体将获得正奖励，因此智能体明白了从状态**B**向下移动是一个好的动作。
- en: From state **E**, the agent moves *right* and reaches state **F**. Since **F**
    is an unshaded state, the agent receives a positive reward, and it will understand
    that moving *right* from state **E** is a good action. From state **F**, the agent
    moves *down* and reaches the goal state **I** and receives a positive reward,
    so the agent will understand that moving down from state **F** is a good action.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从状态**E**，智能体向*右*移动，达到了状态**F**。由于**F**是一个非阴影状态，智能体获得了正奖励，它会明白从状态**E**向*右*移动是一个好的动作。从状态**F**，智能体向*下*移动，达到了目标状态**I**并获得了正奖励，因此智能体会明白从状态**F**向下移动是一个好的动作。
- en: '**Iteration 2**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二轮**'
- en: In the second iteration, from state **A**, instead of moving *right*, the agent
    tries out a different action as the agent learned in the previous iteration that
    moving *right* is not a good action in state **A**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二轮中，智能体从状态**A**出发，不再选择*向右*移动，而是尝试了一种不同的动作，因为智能体在上一轮学到，*向右*在状态**A**并不是一个好的动作。
- en: 'Thus, as *Figure 1.6* shows, in this iteration the agent moves *down* from
    state **A** and reaches state **D**. Since **D** is an unshaded state, the agent
    receives a positive reward and now the agent will understand that moving *down*
    is a good action in state **A**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如*图1.6*所示，在这一轮中，智能体从状态**A**向*下*移动，达到了状态**D**。由于**D**是一个非阴影状态，智能体获得了正奖励，现在智能体明白了在状态**A**时，*向下*是一个好的动作：
- en: '![](img/B15558_01_06.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_06.png)'
- en: 'Figure 1.6: Actions taken by the agent in iteration 2'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：第二轮中智能体执行的动作
- en: As shown in the preceding figure, from state **D**, the agent moves *down* and
    reaches state **G**. But since **G** is a shaded state, the agent will receive
    a negative reward and so the agent will understand that moving *down* is not a
    good action in state **D**, and when it visits state **D** next time, it will
    try out a different action instead of moving *down*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，从状态**D**，智能体向*下*移动并到达状态**G**。但由于**G**是一个阴影状态，智能体将获得负奖励，因此智能体会明白，在状态**D**时向*下*移动不是一个好的动作，当它下次访问状态**D**时，它会尝试不同的动作，而不是向*下*移动。
- en: From **G**, the agent moves *right* and reaches state **H**. Since **H** is
    a shaded state, it will receive a negative reward and understand that moving *right*
    is not a good action in state **G**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从状态**G**，智能体向*右*移动并到达状态**H**。由于**H**是一个阴影状态，智能体会收到负奖励，并理解在状态**G**时向*右*移动不是一个好的动作。
- en: From **H** it moves *right* and reaches the goal state **I** and receives a
    positive reward, so the agent will understand that moving *right* from state **H**
    is a good action.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从状态**H**，智能体向*右*移动并到达目标状态**I**，并获得正奖励，因此智能体会明白，在状态**H**时向*右*移动是一个好的动作。
- en: '**Iteration 3**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 3**'
- en: 'In the third iteration, the agent moves *down* from state **A** since, in the
    second iteration, our agent learned that moving *down* is a good action in state
    **A**. So, the agent moves *down* from state **A** and reaches the next state,
    **D**,as *Figure 1.7* shows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三次迭代中，智能体从状态**A**向*下*移动，因为在第二次迭代中，我们的智能体学到在状态**A**时向*下*移动是一个好的动作。因此，智能体从状态**A**向*下*移动并到达下一个状态**D**，如*图
    1.7*所示：
- en: '![](img/B15558_01_07.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_07.png)'
- en: 'Figure 1.7: Actions taken by the agent in iteration 3'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：智能体在迭代 3 中采取的动作
- en: Now, from state **D**, theagent tries a different action instead of moving *down*
    since in the second iteration our agent learned that moving *down* is not a good
    action in state **D**. So, in this iteration, the agent moves *right* from state
    **D** and reaches state **E**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从状态**D**，智能体尝试了一个不同的动作，而不是向*下*移动，因为在第二次迭代中，我们的智能体已经学到在状态**D**时向*下*移动不是一个好的动作。所以，在这一迭代中，智能体从状态**D**向*右*移动并到达状态**E**。
- en: From state **E**, the agent moves *right* as the agent already learned in the
    first iteration that moving *right* from state **E** is a good actionand reaches
    state **F**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从状态**E**，智能体向*右*移动，因为智能体已经在第一次迭代中学到在状态**E**时向*右*移动是一个好的动作，并到达状态**F**。
- en: Now, from state **F**, theagent moves *down* since the agent learned in the
    first iteration that moving *down* is a good action in state **F**, and reaches
    the goal state **I**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从状态**F**，智能体向*下*移动，因为智能体在第一次迭代中学到在状态**F**时向*下*移动是一个好的动作，并到达目标状态**I**。
- en: '*Figure 1.8* shows the result of the third iteration:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.8* 显示了第三次迭代的结果：'
- en: '![](img/B15558_01_08.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_08.png)'
- en: 'Figure 1.8: The agent reaches the goal state without visiting the shaded states'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：智能体成功到达目标状态，未经过阴影状态
- en: As we can see, our agent has successfully learned to reach the goal state **I**
    from state **A** without visiting the shaded states based on the rewards.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的智能体已经成功学会了从状态**A**到达目标状态**I**，且没有经过阴影状态，这是基于奖励的学习结果。
- en: In this way, the agent will try out different actions in each state and understand
    whether an action is good or bad based on the reward it obtains. The goal of the
    agent is to maximize rewards. So, the agent will always try to perform good actions
    that give a positive reward, and when the agent performs good actions in each
    state, then it ultimately leads the agent to achieve the goal.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，智能体将在每个状态中尝试不同的动作，并根据获得的奖励理解某个动作是好还是坏。智能体的目标是最大化奖励。因此，智能体总是会尝试执行能够获得正奖励的好动作，当智能体在每个状态中执行好动作时，最终会帮助它实现目标。
- en: Note that these iterations are called episodes in RL terminology. We will learn
    more about episodes later in the chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些迭代在强化学习术语中被称为“回合”。我们将在本章稍后部分详细学习回合。
- en: How RL differs from other ML paradigms
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RL与其他机器学习范式的区别
- en: 'We can categorize ML into three types:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将机器学习（ML）分为三种类型：
- en: Supervised learning
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: RL
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习（RL）
- en: In supervised learning, the machine learns from training data. The training
    data consists of a labeled pair of inputs and outputs. So, we train the model
    (agent) using the training data in such a way that the model can generalize its
    learning to new unseen data. It is called supervised learning because the training
    data acts as a supervisor, since it has a labeled pair of inputs and outputs,
    and it guides the model in learning the given task.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，机器通过训练数据进行学习。训练数据由输入和输出的标签对组成。因此，我们使用训练数据训练模型（智能体），使得模型能够将其学习泛化到新的、未见过的数据上。这叫做监督学习，因为训练数据充当了监督者的角色，因为它有标签对输入和输出，并指导模型完成给定的任务。
- en: Now, let's understand the difference between supervised and reinforcement learning
    with an example. Consider the dog analogy we discussed earlier in the chapter.
    In supervised learning, to teach the dog to catch a ball, we will teach it explicitly
    by specifying turn left, go right, move forward seven steps, catch the ball, and
    so on in the form of training data. But in RL, we just throw a ball, and every
    time the dog catches the ball, we give it a cookie (reward). So, the dog will
    learn to catch the ball while trying to maximize the cookies (reward) it can get.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个例子来理解监督学习和强化学习之间的区别。考虑我们在本章前面讨论的狗的类比。在监督学习中，为了教狗抓球，我们将通过指定左转、右转、向前走七步、抓住球等方式明确地教它，形成训练数据。但是在强化学习中，我们只会扔一个球，每当狗抓住球时，我们就给它一块饼干（奖励）。因此，狗会通过尽量获取更多饼干（奖励）来学习抓球。
- en: Let's consider one more example. Say we want to train the model to play chess
    using supervised learning. In this case, we will have training data that includes
    all the moves a player can make in each state, along with labels indicating whether
    it is a good move or not. Then, we train the model to learn from this training
    data, whereas in the case of RL, our agent will not be given any sort of training
    data; instead, we just give a reward to the agent for each action it performs.
    Then, the agent will learn by interacting with the environment and, based on the
    reward it gets, it will choose its actions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑一个例子。假设我们想用监督学习训练模型下棋。在这种情况下，我们将拥有训练数据，数据中包括玩家在每种状态下可以进行的所有棋步，并且有标签指示每步是否是好棋。然后，我们训练模型从这些训练数据中学习，而在强化学习（RL）中，我们的智能体不会获得任何形式的训练数据；相反，我们只会为智能体的每个行动给予奖励。接着，智能体通过与环境互动来学习，并根据获得的奖励选择其行动。
- en: Similar to supervised learning, in unsupervised learning, we train the model
    (agent) based on the training data. But in the case of unsupervised learning,
    the training data does not contain any labels; that is, it consists of only inputs
    and not outputs. The goal of unsupervised learning is to determine hidden patterns
    in the input. There is a common misconception that RL is a kind of unsupervised
    learning, but it is not. In unsupervised learning, the model learns the hidden
    structure, whereas, in RL, the model learns by maximizing the reward.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于监督学习，在无监督学习中，我们也根据训练数据训练模型（智能体）。但在无监督学习中，训练数据不包含任何标签；也就是说，它仅包含输入而没有输出。无监督学习的目标是确定输入中的隐藏模式。有一种常见的误解认为强化学习是一种无监督学习，但实际上并不是。在无监督学习中，模型学习隐藏的结构，而在强化学习中，模型通过最大化奖励来学习。
- en: For instance, consider a movie recommendation system. Say we want to recommend
    a new movie to the user. With unsupervised learning, the model (agent) will find
    movies similar to the movies the user (or users with a profile similar to the
    user) has viewed before and recommend new movies to the user.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个电影推荐系统。假设我们想给用户推荐一部新电影。在无监督学习中，模型（智能体）会根据用户（或与用户有相似档案的用户）之前观看过的电影，找出与之相似的电影，并推荐新电影给用户。
- en: With RL, the agent constantly receives feedback from the user. This feedback
    represents rewards (a reward could be ratings the user has given for a movie they
    have watched, time spent watching a movie, time spent watching trailers, and so
    on). Based on the rewards, an RL agent will understand the movie preference of
    the user and then suggest new movies accordingly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，智能体不断接收来自用户的反馈。这些反馈代表奖励（奖励可以是用户对其看过的电影评分、看电影的时间、看预告片的时间等等）。基于这些奖励，RL智能体将理解用户的电影偏好，然后根据这些信息推荐新电影。
- en: Since the RL agent is learning with the aid of rewards, it can understand if
    the user's movie preference changes and suggest new movies according to the user's
    changed movie preference dynamically.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习代理是通过奖励来学习的，它能够理解用户的电影偏好是否发生变化，并根据用户变化后的电影偏好动态地推荐新电影。
- en: Thus, we can say that in both supervised and unsupervised learning the model
    (agent) learns based on the given training dataset, whereas in RL the agent learns
    by directly interacting with the environment. Thus, RL is essentially an interaction
    between the agent and its environment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说，在监督学习和无监督学习中，模型（代理）是基于给定的训练数据集进行学习的，而在强化学习中，代理是通过直接与环境互动来学习的。因此，强化学习本质上是代理与其环境之间的互动。
- en: Before moving on to the fundamental concepts of RL, we will introduce a popular
    process to aid decision-making in an RL environment.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入学习强化学习的基本概念之前，我们将介绍一个常用的过程，帮助在强化学习环境中做出决策。
- en: Markov Decision Processes
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: The **Markov Decision Process** (**MDP**) provides a mathematical framework
    for solving the RL problem. Almost all RL problems can be modeled as an MDP. MDPs
    are widely used for solving various optimization problems. In this section, we
    will understand what an MDP is and how it is used in RL.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）提供了解决强化学习问题的数学框架。几乎所有的强化学习问题都可以被建模为一个MDP。MDP广泛应用于解决各种优化问题。在本节中，我们将了解MDP是什么以及它如何在强化学习中被使用。'
- en: To understand an MDP, first, we need to learn about the Markov property and
    Markov chain.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解MDP，首先我们需要了解马尔可夫性质和马尔可夫链。
- en: The Markov property and Markov chain
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫性质和马尔可夫链
- en: The Markov property states that the future depends only on the present and not
    on the past. The Markov chain, also known as the Markov process, consists of a
    sequence of states that strictly obey the Markov property; that is, the Markov
    chain is the probabilistic model that solely depends on the current state to predict
    the next state and not the previous states, that is, the future is conditionally
    independent of the past.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质声明，未来只依赖于现在，而不依赖于过去。马尔可夫链，也叫做马尔可夫过程，由一系列严格遵循马尔可夫性质的状态组成；也就是说，马尔可夫链是一个仅依赖当前状态来预测下一个状态的概率模型，而不依赖于之前的状态。换句话说，未来是条件独立于过去的。
- en: For example, if we want to predict the weather and we know that the current
    state is cloudy, we can predict that the next state could be rainy. We concluded
    that the next state is likely to be rainy only by considering the current state
    (cloudy) and not the previous states, which might have been sunny, windy, and
    so on.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想预测天气，并且我们知道当前状态是“多云”，我们可以预测下一个状态可能是“雨天”。我们仅通过考虑当前状态（多云），而不是考虑之前的状态（如晴天、风天等），得出下一个状态很可能是“雨天”。
- en: However, the Markov property does not hold for all processes. For instance,
    throwing a dice (the next state) has no dependency on the previous number that
    showed up on the dice (the current state).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不是所有过程都符合马尔可夫性质。例如，掷骰子（下一个状态）与骰子上显示的前一个数字（当前状态）无关。
- en: 'Moving from one state to another is called a transition, and its probability
    is called a transition probability. We denote the transition probability by ![](img/B15558_01_001.png).
    It indicates the probability of moving from the state *s* to the next state ![](img/B15558_01_002.png).
    Say we have three states (cloudy, rainy, and windy) in our Markov chain. Then
    we can represent the probability of transitioning from one state to another using
    a table called a Markov table, as shown in *Table 1.1*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个状态过渡到另一个状态叫做转移，其概率称为转移概率。我们用 ![](img/B15558_01_001.png) 来表示转移概率。它表示从状态 *s*
    过渡到下一个状态 ![](img/B15558_01_002.png) 的概率。假设我们的马尔可夫链中有三个状态（多云、雨天和风天）。然后，我们可以通过一个叫做马尔可夫表的表格来表示从一个状态到另一个状态的转移概率，如*表1.1*所示：
- en: '![](img/B15558_01_09.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_09.png)'
- en: 'Table 1.1: An example of a Markov table'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1：一个马尔可夫表的示例
- en: 'From *Table 1.1*, we can observe that:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从*表1.1*中，我们可以观察到：
- en: From the state cloudy, we transition to the state rainy with 70% probability
    and to the state windy with 30% probability.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态“多云”出发，我们以70%的概率过渡到“雨天”状态，并以30%的概率过渡到“风天”状态。
- en: From the state rainy, we transition to the same state rainy with 80% probability
    and to the state cloudy with 20% probability.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态“雨天”出发，我们以80%的概率过渡到相同的“雨天”状态，并以20%的概率过渡到“多云”状态。
- en: From the state windy, we transition to the state rainy with 100% probability.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态“风天”出发，我们以100%的概率过渡到“雨天”状态。
- en: 'We can also represent this transition information of the Markov chain in the
    form of a state diagram, as shown in *Figure 1.9*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将马尔可夫链的转移信息表示为状态图，如 *图 1.9* 所示：
- en: '![](img/B15558_01_10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_10.png)'
- en: 'Figure 1.9: A state diagram of a Markov chain'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：马尔可夫链的状态图
- en: 'We can also formulate the transition probabilities into a matrix called the
    transition matrix, as shown in *Figure 1.10*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将转移概率表示为一个矩阵，称为转移矩阵，如 *图 1.10* 所示：
- en: '![](img/B15558_01_11.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_11.png)'
- en: 'Figure 1.10: A transition matrix'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：转移矩阵
- en: Thus, to conclude, we can say that the Markov chain or Markov process consists
    of a set of states along with their transition probabilities.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最后我们可以说，马尔可夫链或马尔可夫过程由一组状态及其转移概率组成。
- en: The Markov Reward Process
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫奖励过程
- en: The **Markov Reward Process** (**MRP**) is an extension of the Markov chain
    with the reward function. That is, we learned that the Markov chain consists of
    states and a transition probability. The MRP consists of states, a transition
    probability, and also a reward function.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫奖励过程** (**MRP**) 是马尔可夫链的扩展，增加了奖励函数。也就是说，我们已经知道马尔可夫链包括状态和转移概率。MRP 包括状态、转移概率以及奖励函数。'
- en: A reward function tells us the reward we obtain in each state. For instance,
    based on our previous weather example, the reward function tells us the reward
    we obtain in the state cloudy, the reward we obtain in the state windy, and so
    on. The reward function is usually denoted by *R*(*s*).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数告诉我们在每个状态中获得的奖励。例如，基于我们之前的天气示例，奖励函数告诉我们在“多云”状态下获得的奖励，在“有风”状态下获得的奖励，依此类推。奖励函数通常表示为
    *R*(*s*)。
- en: Thus, the MRP consists of states *s*, a transition probability ![](img/B15558_01_003.png),
    and a reward function *R*(*s*).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MRP 包括状态 *s*、一个转移概率 ![](img/B15558_01_003.png) 和一个奖励函数 *R*(*s*)。
- en: The Markov Decision Process
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: The **Markov Decision Process** (**MDP**) is an extension of the MRP with actions.
    That is, we learned that the MRP consists of states, a transition probability,
    and a reward function. The MDP consists of states, a transition probability, a
    reward function, and also actions. We learned that the Markov property states
    that the next state is dependent only on the current state and is not based on
    the previous state. Is the Markov property applicable to the RL setting? Yes!
    In the RL environment, the agent makes decisions only based on the current state
    and not based on the past states. So, we can model an RL environment as an MDP.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程** (**MDP**) 是包含动作的 MRP 的扩展。也就是说，我们已经知道 MRP 包括状态、转移概率和奖励函数。MDP 包括状态、转移概率、奖励函数，以及动作。我们了解到，马尔可夫性质表明下一个状态仅依赖于当前状态，而不依赖于之前的状态。那么，马尔可夫性质是否适用于
    RL 环境？是的！在 RL 环境中，智能体仅基于当前状态做出决策，而不依赖于过去的状态。因此，我们可以将 RL 环境建模为一个 MDP。'
- en: 'Let''s understand this with an example. Given any environment, we can formulate
    the environment using an MDP. For instance, let''s consider the same grid world
    environment we learned earlier. *Figure 1.11* shows the grid world environment,
    and the goal of the agent is to reach state **I** from state **A** without visiting
    the shaded states:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这个概念。在任何环境下，我们都可以使用 MDP 来建模环境。例如，考虑我们之前学习过的网格世界环境。*图 1.11* 显示了网格世界环境，智能体的目标是从状态
    **A** 到达状态 **I**，而不经过阴影状态：
- en: '![](img/B15558_01_12.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_12.png)'
- en: 'Figure 1.11: Grid world environment'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：网格世界环境
- en: 'An agent makes a decision (action) in the environment only based on the current
    state the agent is in and not based on the past state. So, we can formulate our
    environment as an MDP. We learned that the MDP consists of states, actions, transition
    probabilities, and a reward function. Now, let''s learn how this relates to our
    RL environment:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体仅根据当前所在的状态做出决策（行动），而不依赖于过去的状态。因此，我们可以将环境建模为一个 MDP。我们知道，MDP 包括状态、动作、转移概率和奖励函数。现在，让我们了解这如何与
    RL 环境相关：
- en: '**States** – A set of states present in the environment. Thus, in the grid
    world environment, we have states **A** to **I**.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态** – 环境中存在的状态集合。因此，在网格世界环境中，我们有状态 **A** 到 **I**。'
- en: '**Actions** – A set of actions that our agent can perform in each state. An
    agent performs an action and moves from one state to another. Thus, in the grid
    world environment, the set of actions is *up*, *down*, *left*, and *right*.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作** - 代理在每个状态下可以执行的一组动作。代理执行某个动作，并从一个状态移动到另一个状态。因此，在网格世界环境中，动作集包括*up*、*down*、*left*和*right*。'
- en: '**Transition probability** – The transition probability is denoted by ![](img/B15558_01_004.png).
    It implies the probability of moving from a state *s* to the next state ![](img/B15558_01_005.png)
    while performing an action *a*. If you observe, in the MRP, the transition probability
    is just ![](img/B15558_01_003.png), that is, the probability of going from state
    *s* to state ![](img/B15558_01_005.png), and it doesn''t include actions. But
    in the MDP, we include the actions, and thus the transition probability is denoted
    by ![](img/B15558_01_004.png).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**转移概率** - 转移概率用![](img/B15558_01_004.png)表示。它表示在执行动作*a*时，从状态*s*转移到下一个状态![](img/B15558_01_005.png)的概率。如果你观察，在MRP中，转移概率仅是![](img/B15558_01_003.png)，即从状态*s*转移到状态![](img/B15558_01_005.png)的概率，并不包括动作。但在MDP中，我们包含了动作，因此转移概率用![](img/B15558_01_004.png)表示。'
- en: 'For example, in our grid world environment, say the transition probability
    of moving from state **A** to state **B** while performing an action *right* is
    100%. This can be expressed as *P*(*B*|*A*, right) = 1.0\. We can also view this
    in the state diagram, as shown in *Figure 1.12*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的网格世界环境中，假设在执行*right*动作时，从**A**状态转移到**B**状态的转移概率为100%。这可以表示为*P*(*B*|*A*,
    right) = 1.0。我们还可以在状态图中查看此情况，如*图1.12*所示：
- en: '![](img/B15558_01_13.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_13.png)'
- en: 'Figure 1.12: Transition probability of moving right from A to B'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：从A到B执行右移的转移概率
- en: 'Suppose our agent is in state **C** and the transition probability of moving
    from state **C** to state **F** while performing the action *down* is 90%, then
    it can be expressed as *P*(*F*|*C*, down) = 0.9\. We can also view this in the
    state diagram, as shown in *Figure 1.13*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的代理处于**C**状态，并且在执行*down*动作时，从**C**状态转换到**F**状态的转移概率为90%，则可以表示为*P*(*F*|*C*,
    down) = 0.9。我们还可以在状态图中查看此情况，如*图1.13*所示：
- en: '![](img/B15558_01_14.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_14.png)'
- en: 'Figure 1.13: Transition probability of moving down from C to F'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13：从C到F执行下移的转移概率
- en: '**Reward function** - The reward function is denoted by ![](img/B15558_01_009.png).
    It represents the reward our agent obtains while transitioning from state *s*
    to state ![](img/B15558_01_005.png) while performing an action *a*.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励函数** - 奖励函数用![](img/B15558_01_009.png)表示。它表示我们的代理在执行动作*a*时，从状态*s*转换到状态![](img/B15558_01_005.png)时获得的奖励。'
- en: 'Say the reward we obtain while transitioning from state **A** to state **B**
    while performing the action *right* is -1, then it can be expressed as *R*(*A*,
    right, *B*) = -1\. We can also view this in the state diagram, as shown in *Figure
    1.14*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在执行*right*动作时，从**A**状态转换到**B**状态时，我们获得的奖励是-1，则可以表示为*R*(*A*, right, *B*) =
    -1。我们还可以在状态图中查看此情况，如*图1.14*所示：
- en: '![](img/B15558_01_15.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_15.png)'
- en: 'Figure 1.14: Reward of moving right from A to B'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14：从A到B执行右移的奖励
- en: 'Suppose our agent is in state **C** and say the reward we obtain while transitioning
    from state **C** to state **F** while performing the action *down* is +1, then
    it can be expressed as *R*(*C*, down, *F*) = +1\. We can also view this in the
    state diagram, as shown in *Figure 1.15*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的代理处于**C**状态，并且假设在执行*down*动作时，从**C**状态转换到**F**状态时，我们获得的奖励是+1，则可以表示为*R*(*C*,
    down, *F*) = +1。我们还可以在状态图中查看此情况，如*图1.15*所示：
- en: '![](img/B15558_01_16.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_16.png)'
- en: 'Figure 1.15: Reward of moving down from C to F'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15：从C到F执行下移的奖励
- en: Thus, an RL environment can be represented as an MDP with states, actions, transition
    probability, and the reward function. But wait! What is the use of representing
    the RL environment using the MDP? We can solve the RL problem easily once we model
    our environment as the MDP. For instance, once we model our grid world environment
    using the MDP, then we can easily find how to reach the goal state **I** from
    state **A** without visiting the shaded states. We will learn more about this
    in the upcoming chapters. Next, we will go through more essential concepts of
    RL.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个强化学习环境可以表示为一个具有状态、动作、转移概率和奖励函数的马尔可夫决策过程（MDP）。但是，等等！为什么要用 MDP 来表示强化学习环境呢？一旦我们将环境建模为
    MDP，就可以轻松解决强化学习问题。例如，一旦我们用 MDP 建模了我们的网格世界环境，那么就可以轻松地找到如何从状态 **A** 到达目标状态 **I**，而不经过阴影状态。我们将在接下来的章节中学习更多内容。接下来，我们将探讨更多强化学习的基本概念。
- en: Fundamental concepts of RL
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的基本概念
- en: In this section, we will learn about several important fundamental RL concepts.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习几个重要的强化学习基本概念。
- en: Math essentials
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学基础
- en: Before going ahead, let's quickly recap expectation from our high school days,
    as we will be dealing with expectation throughout the book.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们快速回顾一下高中时学习的期望值，因为我们将在本书中多次涉及期望值。
- en: Expectation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 期望值
- en: Let's say we have a variable *X* and it has the values 1, 2, 3, 4, 5, 6\. To
    compute the average value of *X*, we can just sum all the values of *X* divided
    by the number of values of *X*. Thus, the average of *X* is (1+2+3+4+5+6)/6 =
    3.5.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个变量 *X*，它的取值为 1, 2, 3, 4, 5, 6。为了计算 *X* 的平均值，我们只需将 *X* 的所有值相加，然后除以 *X*
    的取值个数。因此，*X* 的平均值是 (1+2+3+4+5+6)/6 = 3.5。
- en: 'Now, let''s suppose *X* is a random variable. The random variable takes values
    based on a random experiment, such as throwing dice, tossing a coin, and so on.
    The random variable takes different values with some probabilities. Let''s suppose
    we are throwing a fair dice, then the possible outcomes (*X*) are 1, 2, 3, 4,
    5, and 6 and the probability of occurrence of each of these outcomes is 1/6, as
    shown in *Table 1.2*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设 *X* 是一个随机变量。随机变量的取值基于随机实验，例如掷骰子、抛硬币等。随机变量会以一定的概率取不同的值。假设我们正在掷一颗公平的骰子，那么可能的结果（*X*）是
    1、2、3、4、5 和 6，每个结果出现的概率都是 1/6，如 *表 1.2* 所示：
- en: '![](img/B15558_01_17.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_17.png)'
- en: 'Table 1.2: Probabilities of throwing a dice'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.2：掷骰子的概率
- en: 'How can we compute the average value of the random variable *X*? Since each
    value has a probability of an occurrence, we can''t just take the average. So,
    instead, we compute the weighted average, that is, the sum of values of *X* multiplied
    by their respective probabilities, and this is called expectation. The expectation
    of a random variable *X* can be defined as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算随机变量 *X* 的平均值呢？由于每个值都有发生的概率，我们不能直接取平均值。所以，我们要计算加权平均值，也就是将 *X* 的各个值与它们对应的概率相乘后求和，这就是期望值。随机变量
    *X* 的期望值可以定义为：
- en: '![](img/B15558_01_011.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_011.png)'
- en: Thus, the expectation of the random variable *X* is *E*(*X*) = 1(1/6) + 2(1/6)
    + 3(1/6) + 4(1/6) + 5 (1/6) + 6(1/6) = 3.5.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机变量 *X* 的期望值是 *E*(*X*) = 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6)
    = 3.5。
- en: The expectation is also known as the expected value. Thus, the expected value
    of the random variable *X* is 3.5\. Thus, when we say expectation or the expected
    value of a random variable, it basically means the weighted average.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值也称为期望值。因此，随机变量 *X* 的期望值为 3.5。所以，当我们提到期望或随机变量的期望值时，本质上是指加权平均值。
- en: 'Now, we will look into the expectation of a function of a random variable.
    Let ![](img/B15558_01_012.png), then we can write:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨一个随机变量函数的期望值。设 ![](img/B15558_01_012.png)，然后我们可以写成：
- en: '![](img/B15558_01_18.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_18.png)'
- en: 'Table 1.3: Probabilities of throwing a dice'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3：掷骰子的概率
- en: 'The expectation of a function of a random variable can be computed as:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量函数的期望值可以通过以下方式计算：
- en: '![](img/B15558_01_013.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_013.png)'
- en: Thus, the expected value of *f*(*X*) is given as *E*(*f*(*X*)) = 1(1/6) + 4(1/6)
    + 9(1/6) + 16(1/6) + 25(1/6) + 36(1/6) = 15.1.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*f*(*X*) 的期望值为 *E*(*f*(*X*)) = 1(1/6) + 4(1/6) + 9(1/6) + 16(1/6) + 25(1/6)
    + 36(1/6) = 15.1。
- en: Action space
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作空间
- en: 'Consider the grid world environment shown in *Figure 1.16*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑 *图 1.16* 中所示的网格世界环境：
- en: '![](img/B15558_01_19.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_19.png)'
- en: 'Figure 1.16: Grid world environment'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：网格世界环境
- en: In the preceding grid world environment, the goal of the agent is to reach state
    **I** starting from state **A** without visiting the shaded states. In each of
    the states, the agent can perform any of the four actions—*up*, *down*, *left,*
    and *right*—to achieve the goal. The set of all possible actions in the environment
    is called the action space. Thus, for this grid world environment, the action
    space will be [*up*, *down*, *left*, *right*].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述网格世界环境中，智能体的目标是从状态**A**出发，达到状态**I**，并且不经过阴影状态。在每个状态中，智能体可以执行四种动作中的任何一种——*向上*、*向下*、*向左*和*向右*——来实现目标。环境中所有可能动作的集合被称为动作空间。因此，对于这个网格世界环境，动作空间为[*向上*，*向下*，*向左*，*向右*]。
- en: 'We can categorize action spaces into two types:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将动作空间分为两种类型：
- en: Discrete action space
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散动作空间
- en: Continuous action space
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续动作空间
- en: '**Discrete action space**: When our action space consists of actions that are
    discrete, then it is called a discrete action space. For instance, in the grid
    world environment, our action space consists of four discrete actions, which are
    up, down, left, right, and so it is called a discrete action space.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散动作空间**：当我们的动作空间由离散的动作组成时，它被称为离散动作空间。例如，在网格世界环境中，我们的动作空间由四个离散动作组成，即向上、向下、向左、向右，因此它被称为离散动作空间。'
- en: '**Continuous action space**: When our action space consists of actions that
    are continuous, then it is called a continuous action space. For instance, let''s
    suppose we are training an agent to drive a car, then our action space will consist
    of several actions that have continuous values, such as the speed at which we
    need to drive the car, the number of degrees we need to rotate the wheel, and
    so on. In cases where our action space consists of actions that are continuous,
    it is called a continuous action space.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续动作空间**：当我们的动作空间由连续的动作组成时，它被称为连续动作空间。例如，假设我们正在训练一个智能体驾驶汽车，那么我们的动作空间将包含几个具有连续值的动作，比如我们需要驾驶汽车的速度、我们需要转动方向盘的角度等。当我们的动作空间由连续动作组成时，这种情况称为连续动作空间。'
- en: Policy
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 政策
- en: A policy defines the agent's behavior in an environment. The policy tells the
    agent what action to perform in each state. For instance, in the grid world environment,
    we have states **A** to **I** and four possible actions. The policy may tell the
    agent to move *down* in state **A**, move *right* in state **D**, and so on.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 策略定义了智能体在环境中的行为。策略告诉智能体在每个状态下应该执行什么动作。例如，在网格世界环境中，我们有状态**A**到**I**，以及四个可能的动作。策略可能告诉智能体在状态**A**执行*向下*的动作，在状态**D**执行*向右*的动作，依此类推。
- en: To interact with the environment for the first time, we initialize a random
    policy, that is, the random policy tells the agent to perform a random action
    in each state. Thus, in an initial iteration, the agent performs a random action
    in each state and tries to learn whether the action is good or bad based on the
    reward it obtains. Over a series of iterations, an agent will learn to perform
    good actions in each state, which gives a positive reward. Thus, we can say that
    over a series of iterations, the agent will learn a good policy that gives a positive
    reward.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了首次与环境交互，我们初始化一个随机策略，即随机策略告诉智能体在每个状态下执行一个随机动作。因此，在初始迭代中，智能体在每个状态下执行一个随机动作，并根据它获得的奖励来判断该动作是好是坏。经过一系列迭代后，智能体将学会在每个状态下执行能带来正奖励的好动作。因此，我们可以说，通过一系列迭代，智能体将学习到一个能够获得正奖励的好策略。
- en: This good policy is called the optimal policy. The optimal policy is the policy
    that gets the agent a good reward and helps the agent to achieve the goal. For
    instance, in our grid world environment, the optimal policy tells the agent to
    perform an action in each state such that the agent can reach state **I** from
    state **A** without visiting the shaded states.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个好策略被称为最优策略。最优策略是能让智能体获得好奖励并帮助智能体实现目标的策略。例如，在我们的网格世界环境中，最优策略告诉智能体在每个状态下执行某个动作，使得智能体能够从状态**A**到达状态**I**，并且不经过阴影状态。
- en: 'The optimal policy is shown in *Figure 1.17*. As we can observe, the agent
    selects the action in each state based on the optimal policy and reaches the terminal
    state **I** from the starting state **A** without visiting the shaded states:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略如*图1.17*所示。正如我们所观察到的，智能体根据最优策略在每个状态下选择动作，并从起始状态**A**到达终止状态**I**，而不会经过阴影状态：
- en: '![](img/B15558_01_20.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_20.png)'
- en: 'Figure 1.17: The optimal policy in the grid world environment'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17：网格世界环境中的最优策略
- en: Thus, the optimal policy tells the agent to perform the correct action in each
    state so that the agent can receive a good reward.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最优策略告诉智能体在每个状态下执行正确的动作，以便智能体可以获得良好的奖励。
- en: 'A policy can be classified as the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 策略可以分为以下几种：
- en: A deterministic policy
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种确定性策略
- en: A stochastic policy
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种随机策略
- en: Deterministic policy
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定性策略
- en: 'The policy that we just covered is called a deterministic policy. A deterministic
    policy tells the agent to perform one particular action in a state. Thus, the
    deterministic policy maps the state to one particular action and is often denoted
    by ![](img/B15558_01_014.png). Given a state *s* at a time *t*, a deterministic
    policy tells the agent to perform one particular action *a*. It can be expressed
    as:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才讨论的策略叫做确定性策略。确定性策略告诉智能体在某个状态下执行一个特定的动作。因此，确定性策略将状态映射到一个特定的动作，通常用 ![](img/B15558_01_014.png)
    表示。给定某个时刻 *t* 的状态 *s*，确定性策略告诉智能体执行特定的动作 *a*。它可以表示为：
- en: '![](img/B15558_01_015.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_015.png)'
- en: 'For instance, consider our grid world example. Given state **A**, the deterministic
    policy ![](img/B15558_01_016.png) tells the agent to perform the action *down*.
    This can be expressed as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们网格世界的例子。给定状态 **A**，确定性策略 ![](img/B15558_01_016.png) 告诉智能体执行动作 *下*。这可以表示为：
- en: '![](img/B15558_01_017.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_017.png)'
- en: Thus, according to the deterministic policy, whenever the agent visits state
    **A**, it performs the action *down*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据确定性策略，每当智能体访问状态 **A** 时，它会执行动作 *下*。
- en: Stochastic policy
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机策略
- en: Unlike a deterministic policy, a stochastic policy does not map a state directly
    to one particular action; instead, it maps the state to a probability distribution
    over an action space.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与确定性策略不同，随机策略并不直接将一个状态映射到某个特定的动作，而是将状态映射到动作空间上的概率分布。
- en: That is, we learned that given a state, the deterministic policy will tell the
    agent to perform one particular action in the given state, so whenever the agent
    visits the state it always performs the same particular action. But with a stochastic
    policy, given a state, the stochastic policy will return a probability distribution
    over an action space. So instead of performing the same action every time the
    agent visits the state, the agent performs different actions each time based on
    a probability distribution returned by the stochastic policy.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们了解到，给定一个状态，确定性策略会告诉智能体在该状态下执行一个特定的动作，因此，每次智能体访问该状态时，它总是执行相同的特定动作。而在随机策略中，给定一个状态，随机策略会返回一个关于动作空间的概率分布。因此，智能体每次访问该状态时，不再总是执行相同的动作，而是基于随机策略返回的概率分布执行不同的动作。
- en: Let's understand this with an example; we know that our grid world environment's
    action space consists of four actions, which are [*up, down, left, right*]. Given
    a state **A**, the stochastic policy returns the probability distribution over
    the action space as [0.10,0.70,0.10,0.10]. Now, whenever the agent visits state
    **A**, instead of selecting the same particular action every time, the agent selects
    *up* 10% of the time, *down* 70% of the time, *left* 10% of the time, and *right*
    10% of the time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这个问题；我们知道，我们的网格世界环境的动作空间包含四个动作，分别是 [*上、下、左、右*]。给定状态 **A**，随机策略返回关于动作空间的概率分布
    [0.10, 0.70, 0.10, 0.10]。因此，每次智能体访问状态 **A** 时，它不会每次都选择相同的特定动作，而是有 10% 的概率选择 *上*，70%
    的概率选择 *下*，10% 的概率选择 *左*，以及 10% 的概率选择 *右*。
- en: 'The difference between the deterministic policy and stochastic policy is shown
    in *Figure 1.18*. As we can observe, the deterministic policy maps the state to
    one particular action, whereas the stochastic policy maps the state to the probability
    distribution over an action space:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性策略和随机策略之间的区别如 *图 1.18* 所示。正如我们所观察到的，确定性策略将状态映射到一个特定的动作，而随机策略将状态映射到动作空间上的概率分布：
- en: '![](img/B15558_01_21.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_21.png)'
- en: 'Figure 1.18: The difference between deterministic and stochastic policies'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18：确定性策略和随机策略之间的区别
- en: 'Thus, the stochastic policy maps the state to a probability distribution over
    the action space and is often denoted by ![](img/B15558_01_018.png). Say we have
    a state *s* and action *a* at a time *t*, then we can express the stochastic policy
    as:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机策略将状态映射到动作空间上的一个概率分布，通常用 ![](img/B15558_01_018.png) 表示。假设在时刻 *t* 有一个状态
    *s* 和一个动作 *a*，那么我们可以将随机策略表示为：
- en: '![](img/B15558_01_019.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_019.png)'
- en: Or it can also be expressed as ![](img/B15558_01_020.png).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 或者它也可以表示为 ![](img/B15558_01_020.png)。
- en: 'We can categorize the stochastic policy into two types:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将随机策略分为两种类型：
- en: Categorical policy
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别策略
- en: Gaussian policy
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯策略
- en: Categorical policy
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类别策略
- en: 'A stochastic policy is called a categorical policy when the action space is
    discrete. That is, the stochastic policy uses a categorical probability distribution
    over the action space to select actions when the action space is discrete. For
    instance, in the grid world environment from the previous example, we select actions
    based on a categorical probability distribution (discrete distribution) as the
    action space of the environment is discrete. As *Figure 1.19* shows, given state
    **A**, we select an action based on the categorical probability distribution over
    the action space:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当动作空间是离散时，随机策略被称为类别策略。也就是说，当动作空间是离散时，随机策略使用类别概率分布来选择动作。例如，在之前例子中的网格世界环境中，由于环境的动作空间是离散的，我们根据类别概率分布（离散分布）选择动作。正如*图
    1.19*所示，给定状态**A**，我们根据动作空间上的类别概率分布选择一个动作：
- en: '![](img/B15558_01_22.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_22.png)'
- en: 'Figure 1.19: Probability of next move from state A for a discrete action space'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19：从状态 A 出发的下一步动作概率（离散动作空间）
- en: Gaussian policy
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高斯策略
- en: 'A stochastic policy is called a Gaussian policy when our action space is continuous.
    That is, the stochastic policy uses a Gaussian probability distribution over the
    action space to select actions when the action space is continuous. Let''s understand
    this with a simple example. Suppose we are training an agent to drive a car and
    say we have one continuous action in our action space. Let the action be the speed
    of the car, and the value of the speed of the car ranges from 0 to 150 kmph. Then,
    the stochastic policy uses the Gaussian distribution over the action space to
    select an action, as *Figure 1.20* shows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的动作空间是连续时，随机策略被称为高斯策略。也就是说，当动作空间是连续时，随机策略使用高斯概率分布来选择动作。我们通过一个简单的例子来理解这一点。假设我们正在训练一个智能体驾驶汽车，并且假设在我们的动作空间中有一个连续的动作。让这个动作是汽车的速度，汽车速度的取值范围从
    0 到 150 公里每小时。然后，随机策略使用高斯分布在动作空间上选择动作，如*图 1.20*所示：
- en: '![](img/B15558_01_23.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_23.png)'
- en: 'Figure 1.20: Gaussian distribution'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20：高斯分布
- en: We will learn more about the Gaussian policy in the upcoming chapters.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更深入地了解高斯策略。
- en: Episode
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回合
- en: The agent interacts with the environment by performing some actions, starting
    from the initial state and reaches the final state. This agent-environment interaction
    starting from the initial state until the final state is called an episode. For
    instance, in a car racing video game, the agent plays the game by starting from
    the initial state (the starting point of the race) and reaches the final state
    (the endpoint of the race). This is considered an episode. An episode is also
    often called a trajectory (the path taken by the agent) and it is denoted by ![](img/B15558_01_021.png).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体通过执行一些动作与环境进行交互，从初始状态出发，直到最终状态为止。这个从初始状态到最终状态的智能体与环境的交互过程被称为回合。例如，在一款赛车视频游戏中，智能体通过从初始状态（比赛起点）开始，直到最终状态（比赛终点）为止来进行游戏。这被视为一个回合。回合通常也被称为轨迹（智能体所经过的路径），并且表示为！[](img/B15558_01_021.png)。
- en: An agent can play the game for any number of episodes, and each episode is independent
    of the others. What is the use of playing the game for multiple episodes? In order
    to learn the optimal policy, that is, the policy that tells the agent to perform
    the correct action in each state, the agent plays the game for many episodes.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体可以玩任意数量的回合，并且每个回合彼此独立。那么，玩多个回合有什么意义呢？为了学习最优策略，也就是告诉智能体在每个状态下执行正确动作的策略，智能体需要玩多个回合。
- en: For example, let's say we are playing a car racing game; the first time, we
    may not win the game, so we play the game several times to understand more about
    the game and discover some good strategies for winning the game. Similarly, in
    the first episode, the agent may not win the game and it plays the game for several
    episodes to understand more about the game environment and good strategies to
    win the game.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在玩一款赛车游戏；第一次玩时，可能不会赢得比赛，所以我们会玩几次游戏，逐步了解游戏并发现一些有效的获胜策略。类似地，在第一轮中，智能体可能无法赢得比赛，它会玩多个回合，逐步了解游戏环境，并找出一些有效的获胜策略。
- en: Say we begin the game from an initial state at a time step *t* = 0 and reach
    the final state at a time step *T*, then the episode information consists of the
    agent-environment interaction, such as state, action, and reward, starting from
    the initial state until the final state, that is, (*s*[0], *a*[0], *r*[0], *s*[1],
    *a*[1], *r*[1],…,*s*[T]).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从时间步长 *t* = 0 的初始状态开始游戏，并在时间步长 *T* 时达到最终状态，那么 episode 信息包含了从初始状态到最终状态的智能体与环境交互，例如状态、动作和奖励，即
    (*s*[0], *a*[0], *r*[0], *s*[1], *a*[1], *r*[1], …,*s*[T])。
- en: '*Figure 1.21* shows an example of an episode/trajectory:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.21* 显示了一个 episode/轨迹的示例：'
- en: '![](img/B15558_01_24.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_24.png)'
- en: 'Figure 1.21: An example of an episode'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21：一个 episode 的示例
- en: Let's strengthen our understanding of the episode and the optimal policy with
    the grid world environment. We learned that in the grid world environment, the
    goal of our agent is to reach the final state **I** starting from the initial
    state **A** without visiting the shaded states. An agent receives a +1 reward
    when it visits the unshaded states and a -1 reward when it visits the shaded states.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过网格世界环境来加深对 episode 和最优策略的理解。我们了解到，在网格世界环境中，智能体的目标是从初始状态 **A** 到达最终状态 **I**，而不经过阴影状态。当智能体访问未阴影状态时，它会获得
    +1 奖励；当它访问阴影状态时，它会获得 -1 奖励。
- en: When we say generate an episode, it means going from the initial state to the
    final state. The agent generates the first episode using a random policy and explores
    the environment and over several episodes, it will learn the optimal policy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说生成一个 episode 时，它意味着从初始状态到最终状态的过程。智能体使用随机策略生成第一个 episode，探索环境，并在多个 episode
    后，学习到最优策略。
- en: '**Episode 1**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**Episode 1**'
- en: 'As the *Figure 1.22* shows, in the first episode, the agent uses a random policy
    and selects a random action in each state from the initial state until the final
    state and observes the reward:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 1.22*所示，在第一个 episode 中，智能体使用随机策略，在每个状态中从初始状态到最终状态随机选择动作并观察奖励：
- en: '![](img/B15558_01_25.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_25.png)'
- en: 'Figure 1.22: Episode 1'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22：Episode 1
- en: '**Episode 2**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**Episode 2**'
- en: 'In the second episode, the agent tries a different policy to avoid the negative
    rewards it received in the previous episode. For instance, as we can observe in
    the previous episode, the agent selected the action *right* in state **A** and
    received a negative reward, so in this episode, instead of selecting the action
    *right* in state **A**, it tries a different action, say *down,* as shown in *Figure
    1.23*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个 episode 中，智能体尝试使用不同的策略来避免在上一个 episode 中获得的负奖励。例如，如我们在上一个 episode 中观察到的，智能体在状态
    **A** 选择了动作 *右* 并获得了负奖励，因此在本 episode 中，智能体没有在状态 **A** 选择动作 *右*，而是尝试了不同的动作，比如 *下*，正如
    *图 1.23* 所示：
- en: '![](img/B15558_01_26.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_26.png)'
- en: 'Figure 1.23: Episode 2'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.23：Episode 2
- en: '**Episode n**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**Episode n**'
- en: 'Thus, over a series of episodes, the agent learns the optimal policy, that
    is, the policy that takes the agent to the final state **I** from state **A**
    without visiting the shaded states, as *Figure 1.24* shows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在一系列 episode 后，智能体会学习到最优策略，也就是将智能体从状态 **A** 带到最终状态 **I**，并且不经过阴影状态的策略，正如
    *图 1.24* 所示：
- en: '![](img/B15558_01_27.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_27.png)'
- en: 'Figure 1.24: Episode n'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.24：Episode n
- en: Episodic and continuous tasks
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合任务与连续任务
- en: 'An RL task can be categorized as:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强化学习任务可以被分类为：
- en: An episodic task
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个集合任务
- en: A continuous task
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个连续任务
- en: '**Episodic task**: As the name suggests, an episodic task is one that has a
    terminal/final state. That is, episodic tasks are tasks made up of episodes and
    thus they have a terminal state. For example, in a car racing game, we start from
    the starting point (initial state) and reach the destination (terminal state).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**集合任务**：顾名思义，集合任务是具有终止/最终状态的任务。也就是说，集合任务由多个 episode 组成，因此它们有一个终止状态。例如，在一场赛车游戏中，我们从起点（初始状态）开始，达到终点（终止状态）。'
- en: '**Continuous task**: Unlike episodic tasks, continuous tasks do not contain
    any episodes and so they don''t have any terminal state. For example, a personal
    assistance robot does not have a terminal state.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续任务**：与集合任务不同，连续任务不包含任何 episode，因此没有终止状态。例如，个人助理机器人没有终止状态。'
- en: Horizon
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Horizon
- en: 'Horizon is the time step until which the agent interacts with the environment.
    We can classify the horizon into two categories:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Horizon 是智能体与环境交互的时间步长。我们可以将 Horizon 分为两类：
- en: Finite horizon
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限 Horizon
- en: Infinite horizon
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无限 Horizon
- en: '**Finite horizon**: If the agent-environment interaction stops at a particular
    time step, then the horizon is called a finite horizon. For instance, in episodic
    tasks, an agent interacts with the environment by starting from the initial state
    at time step *t* = 0 and reaches the final state at time step *T*. Since the agent-environment
    interaction stops at time step *T*, it is considered a finite horizon.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**有限时域**：如果智能体与环境的交互在某个特定时间步停止，则称之为有限时域。例如，在回合任务中，智能体从时间步 *t* = 0 的初始状态开始，并在时间步
    *T* 达到最终状态。由于智能体与环境的交互在时间步 *T* 处停止，因此被视为有限时域。'
- en: '**Infinite horizon**: If the agent-environment interaction never stops, then
    it is called an infinite horizon. For instance, we learned that a continuous task
    has no terminal states. This means the agent-environment interaction will never
    stop in a continuous task and so it is considered an infinite horizon.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**无限时域**：如果智能体与环境的交互永不停止，则称之为无限时域。例如，我们了解到，连续任务没有终止状态。这意味着在连续任务中，智能体与环境的交互将永不停止，因此被视为无限时域。'
- en: Return and discount factor
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回报和折扣因子
- en: 'A return can be defined as the sum of the rewards obtained by the agent in
    an episode. The return is often denoted by *R* or *G*. Say the agent starts from
    the initial state at time step *t* = 0 and reaches the final state at time step
    *T*, then the return obtained by the agent is given as:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 回报可以定义为智能体在一个回合中获得的奖励之和。回报通常用 *R* 或 *G* 来表示。假设智能体从初始状态开始，在时间步 *t* = 0 处，并在时间步
    *T* 处达到最终状态，那么智能体获得的回报可以表示为：
- en: '![](img/B15558_01_022.png)![](img/B15558_01_023.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_022.png)![](img/B15558_01_023.png)'
- en: 'Let''s understand this with an example; consider the trajectory (episode) ![](img/B15558_01_024.png):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点；考虑轨迹（回合） ![](img/B15558_01_024.png)：
- en: '![](img/B15558_01_28.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_28.png)'
- en: 'Figure 1.25: Trajectory/episode ![](img/B15558_01_025.png)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.25：轨迹/回合 ![](img/B15558_01_025.png)
- en: The return of the trajectory is the sum of the rewards, that is, ![](img/B15558_01_026.png).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹的回报是奖励的总和，也就是 ![](img/B15558_01_026.png)。
- en: Thus, we can say that the goal of our agent is to maximize the return, that
    is, maximize the sum of rewards (cumulative rewards) obtained over the episode.
    How can we maximize the return? We can maximize the return if we perform the correct
    action in each state. Okay, how can we perform the correct action in each state?
    We can perform the correct action in each state by using the optimal policy. Thus,
    we can maximize the return using the optimal policy. Thus, the optimal policy
    is the policy that gets our agent the maximum return (sum of rewards) by performing
    the correct action in each state.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说智能体的目标是最大化回报，也就是最大化在回合中获得的奖励总和（累计奖励）。我们如何最大化回报呢？如果我们在每个状态下都采取正确的行动，就能最大化回报。那么，我们如何在每个状态下执行正确的行动呢？我们可以通过使用最优策略在每个状态下执行正确的行动。这样，我们就能通过最优策略最大化回报。因此，最优策略就是通过在每个状态下采取正确的行动来获得最大回报（奖励总和）的策略。
- en: 'Okay, how can we define the return for continuous tasks? We learned that in
    continuous tasks there are no terminal states, so we can define the return as
    a sum of rewards up to infinity:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何为连续任务定义回报呢？我们了解到，连续任务中没有终止状态，因此我们可以将回报定义为奖励的总和，直到无穷大：
- en: '![](img/B15558_01_027.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_027.png)'
- en: 'But how can we maximize the return that just sums to infinity? We introduce
    a new term called discount factor ![](img/B15558_01_028.png) and rewrite our return
    as:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何最大化一个求和到无穷大的回报呢？我们引入一个新的术语，叫做折扣因子 ![](img/B15558_01_028.png)，并将回报重新写为：
- en: '![](img/B15558_01_029.png)![](img/B15558_01_030.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_029.png)![](img/B15558_01_030.png)'
- en: Okay, but how is this discount factor ![](img/B15558_01_031.png) helping us?
    It helps us in preventing the return from reaching infinity by deciding how much
    importance we give to future rewards and immediate rewards. The value of the discount
    factor ranges from 0 to 1\. When we set the discount factor to a small value (close
    to 0), it implies that we give more importance to immediate rewards than to future
    rewards. When we set the discount factor to a high value (close to 1), it implies
    that we give more importance to future rewards than to immediate rewards. Let's
    understand this with an example with different discount factor values.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，那么这个折扣因子！[](img/B15558_01_031.png)是如何帮助我们的呢？它通过决定我们给未来奖励和即时奖励赋予多少重要性来帮助我们防止回报值达到无穷大。折扣因子的值范围从0到1。当我们将折扣因子设置为小值（接近0）时，意味着我们更重视即时奖励而非未来奖励。当我们将折扣因子设置为较大值（接近1）时，意味着我们更重视未来奖励而非即时奖励。让我们通过一个具有不同折扣因子值的例子来理解这一点。
- en: Small discount factor
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小折扣因子
- en: 'Let''s set the discount factor to a small value, say 0.2, that is, let''s set
    ![](img/B15558_01_032.png), then we can write:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将折扣因子设置为一个小值，比如0.2，也就是将！[](img/B15558_01_032.png)，然后我们可以写成：
- en: '![](img/B15558_01_033.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_033.png)'
- en: 'From this equation, we can observe that the reward at each time step is weighted
    by a discount factor. As the time steps increase, the discount factor (weight)
    decreases and thus the importance of rewards at future time steps also decreases.
    That is, from the equation, we can observe that:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程中，我们可以观察到每个时间步长的奖励都受到折扣因子的加权。随着时间步长的增加，折扣因子（权重）下降，因此未来时间步长的奖励的重要性也会减少。也就是说，从方程中我们可以观察到：
- en: At time step 0, the reward *r*[0] is weighted by a discount factor of 1.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步长0时，奖励*r*[0]受到折扣因子1的加权。
- en: At time step 1, the reward *r*[1] is weighted by a heavily decreased discount
    factor of 0.2.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步长1时，奖励*r*[1]受到一个大幅降低的折扣因子0.2的加权。
- en: At time step 2, the reward *r*[2] is weighted by a heavily decreased discount
    factor of 0.04.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步长2时，奖励*r*[2]受到一个大幅降低的折扣因子0.04的加权。
- en: As we can observe, the discount factor is heavily decreased for the subsequent
    time steps and more importance is given to the immediate reward *r*[0] than the
    rewards obtained at the future time steps. Thus, when we set the discount factor
    to a small value, we give more importance to the immediate reward than future
    rewards.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，随后的时间步长中的折扣因子大幅降低，因此比起未来时间步长的奖励，当前奖励*r*[0]的重要性被赋予更多。因此，当我们将折扣因子设置为小值时，我们更重视即时奖励而非未来奖励。
- en: Large discount factor
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大折扣因子
- en: 'Let''s set the discount factor to a high value, say 0.9, that is, let''s set,
    ![](img/B15558_01_034.png), then we can write:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将折扣因子设置为一个较大值，比如0.9，也就是将，![](img/B15558_01_034.png)，然后我们可以写成：
- en: '![](img/B15558_01_035.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_035.png)'
- en: 'From this equation, we can infer that as the time step increases the discount
    factor (weight) decreases; however, it is not decreasing heavily (unlike the previous
    case) since here we started off with ![](img/B15558_01_036.png). So, in this case,
    we can say that we give more importance to future rewards. That is, from the equation,
    we can observe that:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程中，我们可以推断出，随着时间步长的增加，折扣因子（权重）会下降；然而，它并不像前一种情况那样大幅下降，因为这里我们一开始就设定了！[](img/B15558_01_036.png)。因此，在这种情况下，我们可以说我们更重视未来奖励。也就是说，从方程中我们可以观察到：
- en: At time step 0, the reward *r*[0] is weighted by a discount factor of 1.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步长0时，奖励*r*[0]受到折扣因子1的加权。
- en: At time step 1, the reward *r*[1] is weighted by a slightly decreased discount
    factor of 0.9.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步长1时，奖励*r*[1]受到一个稍微降低的折扣因子0.9的加权。
- en: At time step 2, the reward *r*[2] is weighted by a slightly decreased discount
    factor of 0.81.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间步长2时，奖励*r*[2]受到一个稍微降低的折扣因子0.81的加权。
- en: As we can observe, the discount factor is decreased for subsequent time steps
    but unlike the previous case, the discount factor is not decreased heavily. Thus,
    when we set the discount factor to a high value, we give more importance to future
    rewards than the immediate reward.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，折扣因子对于随后的时间步长有所下降，但与之前的情况不同，折扣因子的下降幅度并不大。因此，当我们将折扣因子设置为较大值时，我们更重视未来奖励而非即时奖励。
- en: What happens when we set the discount factor to 0?
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当我们将折扣因子设置为0时，会发生什么？
- en: 'When we set the discount factor to 0, that is ![](img/B15558_01_037.png), it
    implies that we consider only the immediate reward *r*[0] and not the reward obtained
    from the future time steps. Thus, when we set the discount factor to 0, then the
    agent will never learn as it will consider only the immediate reward *r*[0], as
    shown here:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将折扣因子设置为0时，即 ![](img/B15558_01_037.png)，这意味着我们只考虑即时回报 *r*[0]，而不考虑未来时间步获得的回报。因此，当我们将折扣因子设置为0时，代理将永远不会学习，因为它只会考虑即时回报
    *r*[0]，如这里所示：
- en: '![](img/B15558_01_038.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_038.png)'
- en: As we can observe, when we set ![](img/B15558_01_039.png), our return will be
    just the immediate reward *r*[0].
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，当我们设置 ![](img/B15558_01_039.png) 时，我们的回报将仅为即时回报 *r*[0]。
- en: What happens when we set the discount factor to 1?
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当我们将折扣因子设置为1时会发生什么？
- en: 'When we set the discount factor to 1, that is ![](img/B15558_01_040.png), it
    implies that we consider all the future rewards. Thus, when we set the discount
    factor to 1, then the agent will learn forever, looking for all the future rewards,
    which may lead to infinity, as shown here:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将折扣因子设置为1时，即 ![](img/B15558_01_040.png)，这意味着我们考虑所有未来的回报。因此，当我们将折扣因子设置为1时，代理将永远学习，寻找所有未来的回报，这可能导致无限大，如下所示：
- en: '![](img/B15558_01_041.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_041.png)'
- en: As we can observe, when we set ![](img/B15558_01_042.png), then our return will
    be the sum of rewards up to infinity.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，当我们设置 ![](img/B15558_01_042.png) 时，我们的回报将是到无限的回报之和。
- en: Thus, we have learned that when we set the discount factor to 0, the agent will
    never learn, considering only the immediate reward, and when we set the discount
    factor to 1 the agent will learn forever, looking for the future rewards that
    lead to infinity. So, the optimal value of the discount factor lies between 0.2
    and 0.8.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经了解到，当我们将折扣因子设置为0时，代理将永远不会学习，只考虑即时回报，而当我们将折扣因子设置为1时，代理将永远学习，寻找导致无限大的未来回报。所以，折扣因子的最佳值介于0.2和0.8之间。
- en: But the question is, why should we care about immediate and future rewards?
    We give importance to immediate and future rewards depending on the tasks. In
    some tasks, future rewards are more desirable than immediate rewards, and vice
    versa. In a chess game, the goal is to defeat the opponent's king. If we give
    more importance to the immediate reward, which is acquired by actions such as
    our pawn defeating any opposing chessman, then the agent will learn to perform
    this sub-goal instead of learning the actual goal. So, in this case, we give greater
    importance to future rewards than the immediate reward, whereas in some cases,
    we prefer immediate rewards over future rewards. Would you prefer chocolates if
    I gave them to you today or 13 days later?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题是，为什么我们应该关心即时和未来的回报呢？我们根据任务的不同，给予即时回报和未来回报不同的权重。在某些任务中，未来的回报比即时回报更为重要，反之亦然。在象棋游戏中，目标是击败对方的国王。如果我们更重视即时回报，例如通过我们的兵击败对方的棋子，那么代理将学会执行这个子目标，而不是学习实际的目标。所以，在这种情况下，我们给予未来回报更大的重要性，而在某些情况下，我们更倾向于即时回报而不是未来回报。如果今天给你巧克力和13天后给你巧克力，你更愿意选择哪一种？
- en: In the following two sections, we'll analyze the two fundamental functions of
    RL.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将分析强化学习的两个基本函数。
- en: The value function
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值函数
- en: 'The value function, also called the state value function, denotes the value
    of the state. The value of a state is the return an agent would obtain starting
    from that state following policy ![](img/B15558_01_043.png). The value of a state
    or value function is usually denoted by *V*(*s*) and it can be expressed as:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数，也称为状态值函数，表示状态的值。状态的值是代理从该状态开始，遵循策略 ![](img/B15558_01_043.png) 所获得的回报。状态的值或值函数通常用
    *V*(*s*) 表示，可以表示为：
- en: '![](img/B15558_01_044.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_044.png)'
- en: where *s*[0] = *s* implies that the starting state is *s*. The value of a state
    is called the state value.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *s*[0] = *s* 表示起始状态是 *s*。状态的值称为状态值。
- en: 'Let''s understand the value function with an example. Let''s suppose we generate
    the trajectory ![](img/B15558_01_045.png) following some policy ![](img/B15558_01_046.png)
    in our grid world environment, as shown in *Figure 1.26*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解值函数。假设我们在网格世界环境中，按照某个策略 ![](img/B15558_01_046.png) 生成了轨迹 ![](img/B15558_01_045.png)，如
    *图1.26* 所示：
- en: '![](img/B15558_01_29.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_29.png)'
- en: 'Figure 1.26: A value function example'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.26：值函数示例
- en: 'Now, how do we compute the value of all the states in our trajectory? We learned
    that the value of a state is the return (sum of rewards) an agent would obtain
    starting from that state following policy ![](img/B15558_01_046.png). The preceding
    trajectory is generated using policy ![](img/B15558_01_047.png), thus we can say
    that the value of a state is the return (sum of rewards) of the trajectory starting
    from that state:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何计算我们轨迹中所有状态的值呢？我们了解到，状态的值是从该状态出发，遵循策略 ![](img/B15558_01_046.png) 获得的回报（奖励之和）。前述轨迹是通过策略
    ![](img/B15558_01_047.png) 生成的，因此我们可以说，状态的值是从该状态开始的轨迹的回报（奖励之和）：
- en: The value of state **A** is the return of the trajectory starting from state
    **A**. Thus, *V*(*A*) = 1+1+ -1+1 = 2.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态 **A** 的值是从状态 **A** 开始的轨迹的回报。因此，*V*(*A*) = 1+1+ -1+1 = 2。
- en: The value of state **D** is the return of the trajectory starting from state
    **D**. Thus, *V*(*D*) = 1-1+1= 1.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态 **D** 的值是从状态 **D** 开始的轨迹的回报。因此，*V*(*D*) = 1-1+1 = 1。
- en: The value of state **E** is the return of the trajectory starting from state
    **E**. Thus, *V*(*E*) = -1+1 = 0.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态 **E** 的值是从状态 **E** 开始的轨迹的回报。因此，*V*(*E*) = -1+1 = 0。
- en: The value of state **H** is the return of the trajectory starting from state
    **H**. Thus, *V*(*H*) = 1.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态 **H** 的值是从状态 **H** 开始的轨迹的回报。因此，*V*(*H*) = 1。
- en: What about the value of the final state **I**? We learned the value of a state
    is the return (sum of rewards) starting from that state. We know that we obtain
    a reward when we transition from one state to another. Since **I** is the final
    state, we don't make any transition from the final state, so there is no reward
    and thus no value for the final state **I**.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，最终状态 **I** 的值是多少呢？我们知道，状态的值是从该状态开始的回报（奖励之和）。我们知道，在从一个状态过渡到另一个状态时会获得奖励。由于
    **I** 是最终状态，我们从最终状态没有进行任何过渡，因此没有奖励，也没有最终状态 **I** 的值。
- en: In a nutshell, the value of a state is the return of the trajectory starting
    from that state.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，状态的值是从该状态开始的轨迹的回报。
- en: 'Wait! There is a small change here: instead of taking the return directly as
    a value of a state, we will use the expected return. Thus, the value function
    or the value of state *s* can be defined as the expected return that the agent
    would obtain starting from state *s* following policy ![](img/B15558_01_046.png).
    It can be expressed as:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！这里有一个小变化：我们不再直接将回报作为状态的值，而是使用期望回报。因此，值函数或状态 *s* 的值可以定义为智能体按照策略 ![](img/B15558_01_046.png)
    从状态 *s* 开始所获得的期望回报。它可以表示为：
- en: '![](img/B15558_01_050.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_050.png)'
- en: Now, the question is why expected return? Why we can't we just compute the value
    of a state as a return directly? Because our return is the random variable and
    it takes different values with some probability.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，为什么是期望回报？为什么我们不能直接将状态的值计算为回报呢？因为我们的回报是一个随机变量，它会根据某些概率取不同的值。
- en: Let's understand this with a simple example. Suppose we have a stochastic policy
    ![](img/B15558_01_047.png). We learned that unlike the deterministic policy, which
    maps the state to the action directly, the stochastic policy maps the state to
    the probability distribution over the action space. Thus, the stochastic policy
    selects actions based on a probability distribution.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来理解这个概念。假设我们有一个随机策略 ![](img/B15558_01_047.png)。我们了解到，不像将状态直接映射到动作的确定性策略，随机策略将状态映射到动作空间上的概率分布。因此，随机策略根据概率分布选择动作。
- en: Let's suppose we are in state **A** and the stochastic policy returns the probability
    distribution over the action space as [0.0,0.80,0.00,0.20]. It implies that with
    the stochastic policy, in state **A**, we perform the action *down* 80% of the
    time, that is, ![](img/B15558_01_052.png), and the action *right* 20% of the time,
    that is ![](img/B15558_01_053.png).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在状态 **A** 中，随机策略返回的动作空间概率分布为 [0.0, 0.80, 0.00, 0.20]。这意味着，在状态 **A** 中，按照随机策略，我们80%的时间执行动作
    *down*，即 ![](img/B15558_01_052.png)，20%的时间执行动作 *right*，即 ![](img/B15558_01_053.png)。
- en: Thus, in state **A**, our stochastic policy ![](img/B15558_01_047.png) selects
    the action *down* 80% of the time and the action *right* 20% of the time, and
    say our stochastic policy selects the action *right* in states **D** and **E**
    and the action *down* in states **B** and **F** 100% of the time.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在状态 **A** 中，我们的随机策略 ![](img/B15558_01_047.png) 80%的时间选择动作 *down*，20%的时间选择动作
    *right*，并且假设我们的随机策略在状态 **D** 和 **E** 中选择动作 *right*，在状态 **B** 和 **F** 中选择动作 *down*
    100%的时间。
- en: 'First, we generate an episode ![](img/B15558_01_055.png) using our stochastic
    policy ![](img/B15558_01_056.png), as shown in *Figure 1.27*:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用随机策略 ![](img/B15558_01_056.png) 生成一个情景 ![](img/B15558_01_055.png)，如*图
    1.27*所示：
- en: '![](img/B15558_01_30.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_30.png)'
- en: 'Figure 1.27: Episode ![](img/B15558_01_057.png)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.27：情景 ![](img/B15558_01_057.png)
- en: For better understanding, let's focus only on the value of state **A**. The
    value of state **A** is the return (sum of rewards) of the trajectory starting
    from state **A**. Thus, ![](img/B15558_01_058.png).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们只关注状态**A**的值。状态**A**的值是从状态**A**开始的轨迹的回报（奖励的总和）。因此，![](img/B15558_01_058.png)。
- en: 'Say we generate another episode ![](img/B15558_01_059.png) using the same given
    stochastic policy ![](img/B15558_01_047.png), as shown in *Figure 1.28*:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用相同的随机策略 ![](img/B15558_01_047.png) 生成另一个情景 ![](img/B15558_01_059.png)，如*图
    1.28*所示：
- en: '![](img/B15558_01_31.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_31.png)'
- en: 'Figure 1.28: Episode ![](img/B15558_01_061.png)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.28：情景 ![](img/B15558_01_061.png)
- en: The value of state **A** is the return (sum of rewards) of the trajectory from
    state **A**. Thus, ![](img/B15558_01_062.png).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 状态**A**的值是从状态**A**开始的轨迹的回报（奖励的总和）。因此，![](img/B15558_01_062.png)。
- en: As you may observe, although we use the same policy, the values of state **A**
    in trajectories ![](img/B15558_01_063.png) and ![](img/B15558_01_064.png) are
    different. This is because our policy is a stochastic policy and it performs the
    action *down* in state **A** 80% of the time and the action *right* in state **A**
    20% of the time. So, when we generate a trajectory using policy ![](img/B15558_01_065.png),
    the trajectory ![](img/B15558_01_066.png) will occur 80% of the time and the trajectory
    ![](img/B15558_01_067.png) will occur 20% of the time. Thus, the return will be
    4 for 80% of the time and 2 for 20% of the time.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，尽管我们使用相同的策略，但轨迹 ![](img/B15558_01_063.png) 和 ![](img/B15558_01_064.png)
    中状态**A**的值是不同的。这是因为我们的策略是随机策略，它在状态**A**下80%的时间执行动作*down*，20%的时间执行动作*right*。因此，当我们使用策略
    ![](img/B15558_01_065.png) 生成轨迹时，轨迹 ![](img/B15558_01_066.png) 会在80%的时间发生，轨迹 ![](img/B15558_01_067.png)
    会在20%的时间发生。因此，回报在80%的时间为4，在20%的时间为2。
- en: 'Thus, instead of taking the value of the state as a return directly, we will
    take the expected return, since the return takes different values with some probability.
    The expected return is basically the weighted average, that is, the sum of the
    return multiplied by their probability. Thus, we can write:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不会直接将状态的值作为回报，而是将期望回报作为参考，因为回报有一定的概率取不同的值。期望回报本质上是加权平均数，即回报与其概率的乘积之和。因此，我们可以写作：
- en: '![](img/B15558_01_068.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_068.png)'
- en: 'The value of a state **A** can be obtained as:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 状态**A**的值可以表示为：
- en: '![](img/B15558_01_069.png)![](img/B15558_01_070.png)![](img/B15558_01_071.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_069.png)![](img/B15558_01_070.png)![](img/B15558_01_071.png)'
- en: Thus, the value of a state is the expected return of the trajectory starting
    from that state.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态的值是从该状态开始的轨迹的期望回报。
- en: 'Note that the value function depends on the policy, that is, the value of the
    state varies based on the policy we choose. There can be many different value
    functions according to different policies. The optimal value function *V**(*s*)
    yields the maximum value compared to all the other value functions. It can be
    expressed as:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，值函数依赖于策略，也就是说，状态的值会根据我们选择的策略而变化。根据不同的策略，可以有许多不同的值函数。最优值函数*V**(*s*)与其他所有值函数相比，能够产生最大的值。它可以表示为：
- en: '![](img/B15558_01_072.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_072.png)'
- en: For example, let's say we have two policies ![](img/B15558_01_073.png) and ![](img/B15558_01_074.png).
    Let the value of state *s* using policy ![](img/B15558_01_075.png) be ![](img/B15558_01_076.png)
    and the value of state *s* using policy ![](img/B15558_01_077.png) be ![](img/B15558_01_078.png).
    Then the optimal value of state *s* will be ![](img/B15558_01_079.png) as it is
    the maximum. The policy that gives the maximum state value is called the optimal
    policy ![](img/B15558_01_080.png). Thus, in this case, ![](img/B15558_01_075.png)
    is the optimal policy as it gives the maximum state value.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有两个策略 ![](img/B15558_01_073.png) 和 ![](img/B15558_01_074.png)。假设使用策略
    ![](img/B15558_01_075.png) 时状态*s*的值为 ![](img/B15558_01_076.png)，使用策略 ![](img/B15558_01_077.png)
    时状态*s*的值为 ![](img/B15558_01_078.png)。那么状态*s*的最优值将是 ![](img/B15558_01_079.png)，因为它是最大的。给出最大状态值的策略称为最优策略
    ![](img/B15558_01_080.png)。因此，在这种情况下，![](img/B15558_01_075.png) 是最优策略，因为它给出了最大的状态值。
- en: 'We can view the value function in a table called a value table. Let''s say
    we have two states *s*[0] and *s*[1], then the value function can be represented
    as:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个叫做值表（value table）的表格来查看值函数。假设我们有两个状态 *s*[0] 和 *s*[1]，那么值函数可以表示为：
- en: '![](img/B15558_01_32.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_32.png)'
- en: 'Table 1.4: Value table'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.4：值表
- en: From the value table, we can tell that it is better to be in state *s*[1] than
    state *s*[0] as *s*[1] has a higher value. Thus, we can say that state *s*[1]
    is the optimal state.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 从值表中，我们可以看出处于状态 *s*[1] 比处于状态 *s*[0] 更好，因为 *s*[1] 的值更高。因此，我们可以说状态 *s*[1] 是最优状态。
- en: Q function
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q 函数
- en: 'A Q function, also called the state-action value function, denotes the value
    of a state-action pair. The value of a state-action pair is the return the agent
    would obtain starting from state *s* and performing action *a* following policy
    ![](img/B15558_01_082.png). The value of a state-action pair or Q function is
    usually denoted by *Q*(*s*,*a*) and is known as the *Q* value or state-action
    value. It is expressed as:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Q 函数，也称为状态-动作值函数，表示一个状态-动作对的值。状态-动作对的值是智能体从状态 *s* 开始并执行动作 *a*，根据策略 ![](img/B15558_01_082.png)
    所获得的回报。状态-动作对的值或 Q 函数通常用 *Q*(*s*,*a*) 表示，称为 *Q* 值或状态-动作值。其表达式为：
- en: '![](img/B15558_01_083.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_083.png)'
- en: 'Note that the only difference between the value function and Q function is
    that in the value function we compute the value of a state, whereas in the Q function
    we compute the value of a state-action pair. Let''s understand the Q function
    with an example. Consider the trajectory in *Figure 1.29* generated using policy
    ![](img/B15558_01_046.png):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，值函数和 Q 函数之间唯一的区别在于，在值函数中我们计算的是一个状态的值，而在 Q 函数中我们计算的是一个状态-动作对的值。让我们通过一个例子来理解
    Q 函数。考虑使用策略 ![](img/B15558_01_046.png) 生成的轨迹 *图 1.29*：
- en: '![](img/B15558_01_33.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_33.png)'
- en: 'Figure 1.29: A trajectory/episode example'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.29：轨迹/回合示例
- en: 'We learned that the Q function computes the value of a state-action pair. Say
    we need to compute the Q value of state-action pair **A**-*down*. That is the
    Q value of moving *down* in state **A**. Then the Q value will be the return of
    our trajectory starting from state **A** and performing the action *down*:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到 Q 函数计算的是一个状态-动作对的值。假设我们需要计算状态-动作对 **A**-*down* 的 Q 值。也就是说，计算在状态 **A**
    下执行动作 *down* 的 Q 值。那么 Q 值就是从状态 **A** 开始并执行动作 *down* 的轨迹的回报：
- en: '![](img/B15558_01_085.png)![](img/B15558_01_086.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_085.png)![](img/B15558_01_086.png)'
- en: 'Let''s suppose we need to compute the Q value of the state-action pair **D**-*right*.
    That is the Q value of moving *right* in state **D**. The Q value will be the
    return of our trajectory starting from state **D** and performing the action *right*:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要计算状态-动作对 **D**-*right* 的 Q 值。也就是说，计算在状态 **D** 下执行动作 *right* 的 Q 值。Q 值将是从状态
    **D** 开始并执行动作 *right* 的轨迹的回报：
- en: '![](img/B15558_01_087.png)![](img/B15558_01_088.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_087.png)![](img/B15558_01_088.png)'
- en: 'Similarly, we can compute the Q value for all the state-action pairs. Similar
    to what we learned about the value function, instead of taking the return directly
    as the Q value of a state-action pair, we use the expected return because the
    return is the random variable and it takes different values with some probability.
    So, we can redefine our Q function as:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以计算所有状态-动作对的 Q 值。与我们学习的值函数类似，不是直接将回报作为状态-动作对的 Q 值，而是使用期望回报，因为回报是一个随机变量，并且它的值有一定的概率会发生变化。因此，我们可以将
    Q 函数重新定义为：
- en: '![](img/B15558_01_089.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_089.png)'
- en: It implies that the Q value is the expected return the agent would obtain starting
    from state *s* and performing action *a* following policy ![](img/B15558_01_090.png).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 Q 值是智能体从状态 *s* 开始并执行动作 *a*，根据策略 ![](img/B15558_01_090.png) 所获得的期望回报。
- en: 'Similar to the value function, the Q function depends on the policy, that is,
    the Q value varies based on the policy we choose. There can be many different
    Q functions according to different policies. The optimal Q function is the one
    that has the maximum Q value over other Q functions, and it can be expressed as:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 与值函数类似，Q 函数依赖于策略，即 Q 值会根据我们选择的策略而变化。根据不同的策略，可能会有许多不同的 Q 函数。最优 Q 函数是所有 Q 函数中具有最大
    Q 值的那个，可以表示为：
- en: '![](img/B15558_01_091.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_091.png)'
- en: The optimal policy ![](img/B15558_01_092.png) is the policy that gives the maximum
    Q value.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略 ![](img/B15558_01_092.png) 是给予最大 Q 值的策略。
- en: 'Like the value function, the Q function can be viewed in a table. It is called
    a Q table. Let''s say we have two states *s*[0] and *s*[1], and two actions 0
    and 1; then the Q function can be represented as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 像价值函数一样，Q 函数也可以以表格的形式展示。它被称为 Q 表。假设我们有两个状态 *s*[0] 和 *s*[1]，以及两个动作 0 和 1；那么 Q
    函数可以表示如下：
- en: '![](img/B15558_01_34.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_34.png)'
- en: 'Table 1.5: Q table'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.5：Q 表
- en: 'As we can observe, the Q table represents the Q values of all possible state-action
    pairs. We learned that the optimal policy is the policy that gets our agent the
    maximum return (sum of rewards). We can extract the optimal policy from the Q
    table by just selecting the action that has the maximum Q value in each state.
    Thus, our optimal policy will select action 1 in state *s*[0] and action 0 in
    state *s*[1] since they have a high Q value, as shown in *Table 1.6*:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，Q 表表示所有可能的状态-动作对的 Q 值。我们了解到，最优策略是能够为我们的智能体带来最大回报（奖励的总和）的策略。我们可以通过从
    Q 表中选择每个状态下具有最大 Q 值的动作来提取最优策略。因此，我们的最优策略将在状态 *s*[0] 选择动作 1，在状态 *s*[1] 选择动作 0，因为它们的
    Q 值较高，如 *表 1.6* 所示：
- en: '![](img/B15558_01_35.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_35.png)'
- en: 'Table 1.6: Optimal policy extracted from the Q table'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.6：从 Q 表中提取的最优策略
- en: Thus, we can extract the optimal policy by computing the Q function.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过计算 Q 函数来提取最优策略。
- en: Model-based and model-free learning
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于模型的学习与无模型学习
- en: Now, let's look into two different types of learning called model-based and
    model-free learning.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看两种不同的学习类型：基于模型的学习和无模型学习。
- en: '**Model-based learning**: In model-based learning, an agent will have a complete
    description of the environment. We know that the transition probability tells
    us the probability of moving from state *s* to the next state ![](img/B15558_01_002.png)
    by performing action *a*. The reward function tells us the reward we would obtain
    while moving from state *s* to the next state ![](img/B15558_01_002.png) by performing
    action *a*. When the agent knows the model dynamics of its environment, that is,
    when the agent knows the transition probability of its environment, then the learning
    is called model-based learning. Thus, in model-based learning, the agent uses
    the model dynamics to find the optimal policy.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于模型的学习**：在基于模型的学习中，智能体将拥有环境的完整描述。我们知道，转移概率告诉我们通过执行动作 *a* 从状态 *s* 转移到下一个状态
    ![](img/B15558_01_002.png) 的概率。奖励函数告诉我们，在执行动作 *a* 从状态 *s* 转移到下一个状态 ![](img/B15558_01_002.png)
    时所获得的奖励。当智能体知道其环境的模型动态时，即智能体知道其环境的转移概率时，这种学习被称为基于模型的学习。因此，在基于模型的学习中，智能体利用模型动态来找到最优策略。'
- en: '**Model-free learning**: Model-free learning is when the agent does not know
    the model dynamics of its environment. That is, in model-free learning, an agent
    tries to find the optimal policy without the model dynamics.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**无模型学习**：无模型学习是指智能体不知道其环境的模型动态。也就是说，在无模型学习中，智能体尝试在没有模型动态的情况下找到最优策略。'
- en: Next, we'll discover the different types of environment an agent works within.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索智能体所处的不同类型的环境。
- en: Different types of environments
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同类型的环境
- en: At the beginning of the chapter, we learned that the environment is the world
    of the agent and the agent lives/stays within the environment. We can categorize
    the environment into different types.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们了解到环境是智能体的世界，智能体生活/停留在环境中。我们可以将环境分类为不同的类型。
- en: Deterministic and stochastic environments
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定性环境与随机环境
- en: '**Deterministic environment**: In a deterministic environment, we are certain
    that when an agent performs action *a* in state *s*, then it always reaches state
    ![](img/B15558_01_002.png). For example, let''s consider our grid world environment.
    Say the agent is in state **A**, and when it moves *down* from state **A**, it
    always reaches state **D**. Hence the environment is called a deterministic environment:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '**确定性环境**：在确定性环境中，我们可以确定当智能体在状态 *s* 执行动作 *a* 时，它总是到达状态 ![](img/B15558_01_002.png)。例如，假设我们有一个网格世界环境。假设智能体处于状态
    **A**，当它从状态 **A** 向 *下* 移动时，它总是到达状态 **D**。因此，环境被称为确定性环境：'
- en: '![](img/B15558_01_36.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_36.png)'
- en: 'Figure 1.30: Deterministic environment'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.30：确定性环境
- en: '**Stochastic environment**: In a stochastic environment, we cannot say that
    by performing action *a* in state *s* the agent always reaches state ![](img/Image59404.png)
    because there will be some randomness associated with the stochastic environment.
    For example, let''s suppose our grid world environment is a stochastic environment.
    Say our agent is in state **A**; now if it moves *down* from state **A**, then
    the agent doesn''t always reach state **D**. Instead, it reaches state **D** 70%
    of the time and state **B** 30% of the time. That is, if the agent moves *down*
    in state **A**, then the agent reaches state **D** with 70% probability and state
    **B** with 30% probability, as *Figure 1.31* shows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机环境**：在随机环境中，我们不能说通过在状态 *s* 执行动作 *a* 代理总是到达状态 ![](img/Image59404.png)，因为在随机环境中会有一定的随机性。例如，假设我们的网格世界环境是一个随机环境。假设我们的代理处于状态
    **A**，现在如果它从状态 **A** 向下移动，那么代理并不总是到达状态 **D**。相反，它70%的概率到达状态 **D**，30%的概率到达状态 **B**。也就是说，如果代理在状态
    **A** 下移，那么它以70%的概率到达状态 **D**，以30%的概率到达状态 **B**，正如*图1.31*所示：'
- en: '![](img/B15558_01_37.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_01_37.png)'
- en: 'Figure 1.31: Stochastic environment'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.31：随机环境
- en: Discrete and continuous environments
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离散环境与连续环境
- en: '**Discrete environment**: A discrete environment is one where the environment''s
    action space is discrete. For instance, in the grid world environment, we have
    a discrete action space, which consists of the actions [*up*, *down*, *left*,
    *right*] and thus our grid world environment is discrete.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散环境**：离散环境是指环境的动作空间是离散的。例如，在网格世界环境中，我们的动作空间是离散的，包含了动作[*上*，*下*，*左*，*右*]，因此我们的网格世界环境是离散的。'
- en: '**Continuous environment**: A continuous environment is one where the environment''s
    action space is continuous. For instance, suppose we are training an agent to
    drive a car, then our action space will be continuous, with several continuous
    actions such as changing the car''s speed, the number of degrees the agent needs
    to rotate the wheel, and so on. In such a case, our environment''s action space
    is continuous.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续环境**：连续环境是指环境的动作空间是连续的。例如，假设我们正在训练一个代理来驾驶一辆车，那么我们的动作空间将是连续的，其中包括多个连续的动作，如改变车速、旋转方向盘所需的角度等。在这种情况下，我们环境的动作空间是连续的。'
- en: Episodic and non-episodic environments
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步环境与非逐步环境
- en: '**Episodic environment**: In an episodic environment, an agent''s current action
    will not affect future actions, and thus an episodic environment is also called
    a non-sequential environment.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**逐步环境**：在逐步环境中，代理的当前动作不会影响未来的动作，因此逐步环境也被称为非顺序环境。'
- en: '**Non-episodic environment**: In a non-episodic environment, an agent''s current
    action will affect future actions, and thus a non-episodic environment is also
    called a sequential environment. For example, a chessboard is a sequential environment
    since the agent''s current action will affect future actions in a chess match.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**非逐步环境**：在非逐步环境中，代理的当前动作会影响未来的动作，因此非逐步环境也被称为顺序环境。例如，国际象棋棋盘是一个顺序环境，因为代理的当前动作会影响未来的动作。'
- en: Single and multi-agent environments
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单代理与多代理环境
- en: '**Single-agent environment**: When our environment consists of only a single
    agent, then it is called a single-agent environment.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单代理环境**：当我们的环境只包含一个代理时，它被称为单代理环境。'
- en: '**Multi-agent environment**: When our environment consists of multiple agents,
    then it is called a multi-agent environment.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多代理环境**：当我们的环境包含多个代理时，它被称为多代理环境。'
- en: We have covered a lot of concepts of RL. Now, we'll finish the chapter by looking
    at some exciting applications of RL.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了强化学习的许多概念。现在，我们将通过看看一些令人兴奋的强化学习应用来结束本章。
- en: Applications of RL
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的应用
- en: 'RL has evolved rapidly over the past couple of years with a wide range of applications
    ranging from playing games to self-driving cars. One of the major reasons for
    this evolution is due to **Deep Reinforcement Learning** (**DRL**), which is a
    combination of RL and deep learning. We will learn about the various state-of-the-art
    deep RL algorithms in the upcoming chapters, so be excited! In this section, we
    will look at some real-life applications of RL:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）在过去几年中发展迅速，应用范围广泛，从玩游戏到自动驾驶汽车。导致这一发展的主要原因之一是**深度强化学习**（**DRL**），它是RL和深度学习的结合。我们将在接下来的章节中学习各种最先进的深度RL算法，所以请期待！在这一节中，我们将探讨一些RL的现实应用：
- en: '**Manufacturing**: In manufacturing, intelligent robots are trained using RL
    to place objects in the right position. The use of intelligent robots reduces
    labor costs and increases productivity.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制造业**：在制造业中，智能机器人通过强化学习被训练成能够将物品放置到正确的位置。使用智能机器人能够减少人工成本并提高生产力。'
- en: '**Dynamic pricing**: One of the popular applications of RL is dynamic pricing.
    Dynamic pricing implies that we change the price of products based on demand and
    supply. We can train the RL agent for the dynamic pricing of products with the
    goal of maximizing revenue.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态定价**：强化学习的一个热门应用是动态定价。动态定价意味着我们根据需求和供应来调整产品的价格。我们可以训练强化学习代理来实现产品的动态定价，目标是最大化收入。'
- en: '**Inventory management**: RL is used extensively in inventory management, which
    is a crucial business activity. Some of these activities include supply chain
    management, demand forecasting, and handling several warehouse operations (such
    as placing products in warehouses to manage space efficiently).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库存管理**：强化学习广泛应用于库存管理，这是一个至关重要的业务活动。这些活动包括供应链管理、需求预测以及处理多个仓库操作（例如将产品放置在仓库中以高效管理空间）。'
- en: '**Recommendation system**: RL is widely used in building a recommendation system
    where the behavior of the user constantly changes. For instance, in music recommendation
    systems, the behavior or the music preferences of the user changes from time to
    time. So, in those cases using an RL agent can be very useful as the agent constantly
    learns by interacting with the environment.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**：强化学习广泛应用于构建推荐系统，在这些系统中，用户的行为不断变化。例如，在音乐推荐系统中，用户的行为或音乐偏好会随时间变化。因此，在这些情况下，使用强化学习代理非常有用，因为代理会通过与环境的交互不断学习。'
- en: '**Neural architecture search**: In order for a neural network to perform a
    given task with good accuracy, the architecture of the network is very important,
    and it has to be properly designed. With RL, we can automate the process of complex
    neural architecture search by training the agent to find the best neural architecture
    for a given task with the goal of maximizing the accuracy.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经架构搜索**：为了让神经网络以良好的准确性执行给定任务，网络的架构非常重要，必须正确设计。通过强化学习，我们可以自动化复杂神经架构搜索的过程，通过训练代理来寻找给定任务的最佳神经架构，目标是最大化准确性。'
- en: '**Natural Language Processing (NLP)**: With the increase in popularity of deep
    reinforcement algorithms, RL has been widely used in several NLP tasks, such as
    abstractive text summarization, chatbots, and more.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理（NLP）**：随着深度强化学习算法的普及，强化学习已广泛应用于多个自然语言处理任务，如抽象文本摘要、聊天机器人等。'
- en: '**Finance**: RL is widely used in financial portfolio management, which is
    the process of constant redistribution of a fund into different financial products.
    RL is also used in predicting and trading in commercial transaction markets. JP
    Morgan has successfully used RL to provide better trade execution results for
    large orders.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融**：强化学习广泛应用于金融投资组合管理，即不断将资金重新分配到不同的金融产品中。强化学习也被用于预测和交易商业交易市场。摩根大通成功地利用强化学习为大宗订单提供更好的交易执行结果。'
- en: RL glossary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习术语
- en: We have learned several important and fundamental concepts of RL. In this section,
    we revisit several important terms that are very useful for understanding the
    upcoming chapters.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了强化学习的几个重要基本概念。在本节中，我们将回顾一些对于理解接下来章节非常有用的术语。
- en: '**Agent**: The agent is the software program that learns to make intelligent
    decisions, such as a software program that plays chess intelligently.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理**：代理是学习做出智能决策的软件程序，例如一个能够智能下棋的软件程序。'
- en: '**Environment**: The environment is the world of the agent. If we continue
    with the chess example, a chessboard is the environment where the agent plays
    chess.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境**：环境是代理的世界。如果我们继续使用象棋的例子，棋盘就是代理进行象棋游戏的环境。'
- en: '**State**: A state is a position or a moment in the environment that the agent
    can be in. For example, all the positions on the chessboard are called states.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态**：状态是代理可能处于的环境中的一个位置或时刻。例如，棋盘上的所有位置都被称为状态。'
- en: '**Action**: The agent interacts with the environment by performing an action
    and moves from one state to another, for example, moves made by chessmen are actions.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作**：代理通过执行动作与环境交互，并从一个状态转移到另一个状态，例如棋子所做的移动就是动作。'
- en: '**Reward**: A reward is a numerical value that the agent receives based on
    its action. Consider a reward as a point. For instance, an agent receives +1 point
    (reward) for a good action and -1 point (reward) for a bad action.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励**：奖励是代理根据其动作获得的数值。可以将奖励视为一个分数。例如，代理因执行一个好动作而获得 +1 分（奖励），因执行一个坏动作而获得 -1
    分（奖励）。'
- en: '**Action space**: The set of all possible actions in the environment is called
    the action space. The action space is called a discrete action space when our
    action space consists of discrete actions, and the action space is called a continuous
    action space when our actions space consists of continuous actions.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作空间**：环境中所有可能动作的集合称为动作空间。当我们的动作空间由离散动作组成时，称为离散动作空间；当我们的动作空间由连续动作组成时，称为连续动作空间。'
- en: '**Policy**: The agent makes a decision based on the policy. A policy tells
    the agent what action to perform in each state. It can be considered the brain
    of an agent. A policy is called a deterministic policy if it exactly maps a state
    to a particular action. Unlike a deterministic policy, a stochastic policy maps
    the state to a probability distribution over the action space. The optimal policy
    is the one that gives the maximum reward.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略**：代理根据策略做出决策。策略告诉代理在每个状态下应该执行什么动作。它可以被看作是代理的大脑。如果一个策略能精确地将一个状态映射到一个特定动作，则称之为确定性策略。与确定性策略不同，随机策略将状态映射到动作空间上的概率分布。最优策略是能带来最大奖励的策略。'
- en: '**Episode**: The agent-environment interaction from the initial state to the
    terminal state is called an episode. An episode is often called a trajectory or
    rollout.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '**回合**：代理与环境从初始状态到终止状态的交互称为一个回合。回合通常被称为轨迹或展开。'
- en: '**Episodic and continuous task**: An RL task is called an episodic task if
    it has a terminal state, and it is called a continuous task if it does not have
    a terminal state.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**回合任务和连续任务**：如果一个 RL 任务有终止状态，则称为回合任务；如果任务没有终止状态，则称为连续任务。'
- en: '**Horizon**: The horizon can be considered an agent''s lifespan, that is, the
    time step until which the agent interacts with the environment. The horizon is
    called a finite horizon if the agent-environment interaction stops at a particular
    time step, and it is called an infinite horizon when the agent environment interaction
    continues forever.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间跨度**：时间跨度可以视为代理的生命周期，即代理与环境交互的时间步长。如果代理-环境交互在某一特定时间步长结束，则称为有限时间跨度；如果代理与环境的交互持续到永远，则称为无限时间跨度。'
- en: '**Return**: Return is the sum of rewards received by the agent in an episode.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**回报**：回报是代理在一个回合中获得的所有奖励的总和。'
- en: '**Discount factor**: The discount factor helps to control whether we want to
    give importance to the immediate reward or future rewards. The value of the discount
    factor ranges from 0 to 1\. A discount factor close to 0 implies that we give
    more importance to immediate rewards, while a discount factor close to 1 implies
    that we give more importance to future rewards than immediate rewards.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**折扣因子**：折扣因子帮助我们控制是否要重视即时奖励还是未来奖励。折扣因子的值范围从 0 到 1。当折扣因子接近 0 时，意味着我们更重视即时奖励；而折扣因子接近
    1 时，意味着我们更重视未来奖励而非即时奖励。'
- en: '**Value function**: The value function or the value of the state is the expected
    return that an agent would get starting from state *s* following policy ![](img/B15558_01_018.png).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**价值函数**：价值函数或状态的价值是指代理在按照策略![](img/B15558_01_018.png)从状态 *s* 开始时所期望的回报。'
- en: '**Q function**: The Q function or the value of a state-action pair implies
    the expected return an agent would obtain starting from state *s* and performing
    action *a* following policy ![](img/B15558_01_046.png).'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q函数**：Q函数或状态-动作对的价值意味着代理在按照策略![](img/B15558_01_046.png)从状态 *s* 开始并执行动作 *a*
    时所期望的回报。'
- en: '**Model-based and model-free learning**: When the agent tries to learn the
    optimal policy with the model dynamics, then it is called model-based learning;
    and when the agent tries to learn the optimal policy without the model dynamics,
    then it is called model-free learning.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于模型和无模型学习**：当代理尝试通过模型动力学学习最优策略时，这称为基于模型的学习；而当代理尝试在没有模型动力学的情况下学习最优策略时，这称为无模型学习。'
- en: '**Deterministic and stochastic environment**: When an agent performs action
    *a* in state *s* and it reaches state ![](img/B15558_01_002.png) every time, then
    the environment is called a deterministic environment. When an agent performs
    action *a* in state *s* and it reaches different states every time based on some
    probability distribution, then the environment is called a stochastic environment.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**确定性和随机性环境**：当一个智能体在状态*s*下执行动作*a*，并且每次都到达状态 ![](img/B15558_01_002.png) 时，环境被称为确定性环境。当一个智能体在状态*s*下执行动作*a*，并且每次根据某些概率分布到达不同的状态时，环境被称为随机性环境。'
- en: Summary
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding the basic idea of RL. We learned that
    RL is a trial and error learning process and the learning in RL happens based
    on a reward. We then explored the difference between RL and the other ML paradigms,
    such as supervised and unsupervised learning. Going ahead, we learned about the
    MDP and how the RL environment can be modeled as an MDP. Next, we understood several
    important fundamental concepts involved in RL, and at the end of the chapter we
    looked into some real-life applications of RL.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解强化学习的基本思想开始。本章中，我们了解到强化学习是一个基于试错的学习过程，学习依赖于奖励。接着，我们探讨了强化学习与其他机器学习范式（如监督学习和无监督学习）的不同。随后，我们了解了马尔可夫决策过程（MDP），并学习了如何将强化学习的环境建模为MDP。接着，我们理解了强化学习中的几个重要基本概念，在本章结尾时，我们还探讨了强化学习的一些实际应用。
- en: Thus, in this chapter, we have learned several fundamental concepts of RL. In
    the next chapter, we will begin our *Hands-on reinforcement learning* journey
    by implementing all the fundamental concepts we have learned in this chapter using
    the popular toolkit called Gym.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们学习了强化学习的几个基本概念。在下一章，我们将通过使用一个流行的工具包Gym，实施我们在本章中学习的所有基本概念，开始我们的*实践强化学习*之旅。
- en: Questions
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our newly acquired knowledge by answering these questions:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们新获得的知识：
- en: How does RL differ from other ML paradigms?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习（RL）与其他机器学习（ML）范式有何不同？
- en: What is called the environment in the RL setting?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在强化学习的设定中，什么被称为环境？
- en: What is the difference between a deterministic and a stochastic policy?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定性策略和随机性策略有什么区别？
- en: What is an episode?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是一个回合（episode）？
- en: Why do we need a discount factor?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要折扣因子？
- en: How does the value function differ from the Q function?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值函数与Q函数有什么区别？
- en: What is the difference between deterministic and stochastic environments?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定性环境和随机性环境有何不同？
- en: Further reading
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For further information, refer to the following link:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的信息，请参阅以下链接：
- en: '**Reinforcement Learning**: A Survey by *L. P. Kaelbling, M. L. Littman, A.
    W. Moore*, available at [https://arxiv.org/abs/cs/9605103](https://arxiv.org/abs/cs/9605103)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**：由*L. P. Kaelbling, M. L. Littman, A. W. Moore*编写的调查报告，访问链接：[https://arxiv.org/abs/cs/9605103](https://arxiv.org/abs/cs/9605103)'
