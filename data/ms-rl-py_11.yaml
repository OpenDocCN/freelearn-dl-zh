- en: '*Chapter 9*: Multi-Agent Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：多智能体强化学习'
- en: If there is something more exciting than training a **reinforcement** **learning**
    (**RL**) agent to exhibit intelligent behavior, it is to train multiple of them
    to collaborate or compete. **Multi-agent** **RL** (**MARL**) is where you will
    really feel the potential in artificial intelligence. Many famous RL stories,
    such as AlphaGo or OpenAI Five, stemmed from MARL, which we introduce you to in
    this chapter. Of course, there is no free lunch, and MARL comes with lots of challenges
    along with its opportunities, which we will also explore. At the end of the chapter,
    we will train a bunch of tic-tac-toe agents through competitive self-play. So,
    at the end, you will have some companions to play some game against.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有比训练一个**强化学习**（**RL**）智能体表现出智能行为更令人兴奋的事情，那就是训练多个智能体进行合作或竞争。**多智能体RL**（**MARL**）是你真正能感受到人工智能潜力的地方。许多著名的RL故事，例如AlphaGo或OpenAI
    Five，都源自MARL，我们将在这一章中介绍它们。当然，天下没有免费的午餐，MARL也伴随着许多挑战与机遇，我们也将对此进行探索。在本章的最后，我们将通过竞争性自我博弈训练一群井字游戏智能体。所以，最后你将有一些伙伴一起玩游戏。
- en: 'This will be a fun chapter, and specifically we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章将会很有趣，我们将具体涵盖以下主题：
- en: Introducing multi-agent reinforcement learning,
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍多智能体强化学习，
- en: Exploring the challenges in multi-agent reinforcement learning,
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索多智能体强化学习中的挑战，
- en: Training policies in multi-agent settings,
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多智能体环境中训练策略，
- en: Training tic-tac-toe agents through self-play.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自我博弈训练井字游戏智能体。
- en: Let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Introducing multi-agent reinforcement learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍多智能体强化学习
- en: All of the problems and algorithms we have covered in the book so far involved
    a single agent being trained in an environment. On the other hand, in many applications
    from games to autonomous vehicle fleets, there are multiple decision-makers, agents,
    which train concurrently, but execute local policies (i.e., without a central
    decision-maker). This leads us to MARL, which involves a much richer set of problems
    and challenges than single-agent RL does. In this section, we give an overview
    of MARL landscape.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们书中讨论的所有问题和算法都涉及在环境中训练单一智能体。另一方面，在从游戏到自动驾驶车队的许多应用中，存在多个决策者，即多个智能体，这些智能体并行训练，但执行的是本地策略（即没有中央决策者）。这引导我们进入MARL，它涉及比单一智能体RL更为丰富的问题和挑战。在本节中，我们将概述MARL的整体格局。
- en: Collaboration and competition between MARL agents
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MARL智能体之间的协作与竞争
- en: MARL problems can be classified into three different groups with respect to
    the structure of collaboration and competition between agents. Let's look into
    what those groups are and what types of applications fit into each group.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MARL问题可以根据智能体之间的协作和竞争结构分为三大类。让我们看看这三类是什么，以及每类适合哪些应用。
- en: Fully cooperative environments
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全合作环境
- en: In this setting, all of the agents in the environment work towards a common
    long-term goal. The agents are credited equally for the return the environment
    reveals, so there is no incentive for any of the agents to deviate from the common
    goal.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种环境下，所有智能体都朝着一个共同的长期目标努力。环境所反馈的回报会平等地分配给所有智能体，因此没有任何智能体有动机偏离共同目标。
- en: 'Here are some examples to fully cooperative environments:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些完全合作环境的例子：
- en: '**Autonomous vehicle / robot fleets**: There are many applications where a
    fleet of autonomous vehicles / robots could work towards accomplishing a common
    mission. One example is disaster recovery / emergency response / rescue missions,
    where the fleet tries to achieve tasks such as delivering emergency supplies to
    first responders, shutting of gas valves, removing debris from roads etc. Similarly,
    transportation problems as in supply chains or in the form of transporting a big
    object using multiple robots are good examples in this category.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶车辆/机器人车队**：有许多应用场景，自动驾驶车辆/机器人车队可以协同完成一个共同的任务。一个例子是灾后恢复/紧急响应/救援任务，车队尝试完成如向急救人员运送紧急物资、关闭气阀、清理道路上的杂物等任务。类似地，在供应链中出现的运输问题，或通过多个机器人运输大物体的场景也属于这一类。'
- en: '**Manufacturing**: The whole idea behind Industry 4.0 is to have interconnected
    equipment and cyber-physical systems to achieve efficient production and service.
    If you think of a single manufacturing floor, in which there are usually many
    decision-making equipment, MARL is a natural fit to model such control problems.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制造业**：工业4.0背后的整体理念是实现设备和网络物理系统的互联，以达到高效的生产和服务。如果你想象一个单一的制造车间，里面通常有许多决策设备，MARL是建模此类控制问题的自然选择。'
- en: '**Smart grid**: In the emerging field of smart grid, many problems can be modeled
    in this category. An example is the problem of cooling a data center that involves
    many cooling units. Similarly, control of traffic lights in an intersection is
    another good example in this area. In fact, in [*Chapter 17*](B14160_17_Final_SK_ePub.xhtml#_idTextAnchor365)*,
    Smart City and Cybersecurity*, we will model and solve this problem using MARL.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能电网**：在新兴的智能电网领域，许多问题可以归类为这一类。例如，涉及许多冷却单元的数据中心冷却问题。类似地，交叉路口交通信号灯的控制也是这一领域的另一个好例子。事实上，在[*第17章*](B14160_17_Final_SK_ePub.xhtml#_idTextAnchor365)*《智能城市与网络安全》*中，我们将使用MARL来建模和解决这个问题。'
- en: Before moving into discussing other types of MARL environments, while we are
    at MARL for autonomous vehicles, let's briefly mention a useful platform, MACAD-Gym,
    for you to experiment with.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论其他类型的MARL环境之前，在我们讨论自动驾驶车辆的MARL时，让我们简要提到一个有用的平台，MACAD-Gym，供你实验使用。
- en: MACAD-Gym for multi-agent connected autonomous driving
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MACAD-Gym用于多代理连接的自动驾驶
- en: MACAD-Gym is a Gym-based library for connected and autonomous driving applications
    in multi-agent settings, built on top of the famous CARLA simulator.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MACAD-Gym是一个基于Gym的库，用于多代理设置中的连接与自动驾驶应用，建立在著名的CARLA模拟器之上。
- en: '![Figure 9.1: MACAD-Gym platform (source: MACAD-Gym GitHub repo)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1：MACAD-Gym平台（来源：MACAD-Gym GitHub仓库）'
- en: '](img/B14160_09_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_001.jpg)'
- en: 'Figure 9.1: MACAD-Gym platform (source: MACAD-Gym GitHub repo)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：MACAD-Gym平台（来源：MACAD-Gym GitHub仓库）
- en: 'The platform provides a rich set of scenarios involving cars, pedestrians,
    traffic lights, bikes etc., depicted in Figure 9.1\. In more detail, MACAD-Gym
    environments contain a variety of MARL configurations as in the following example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该平台提供了一套丰富的场景，包括汽车、行人、交通信号灯、自行车等，如图9.1所示。更详细地说，MACAD-Gym环境包含各种MARL配置，如以下示例所示：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To see what you can do with MACAD-Gym, check out its Github repo, developed
    and maintained by Praveen Palanisamy, at [https://github.com/praveen-palanisamy/macad-gym](https://github.com/praveen-palanisamy/macad-gym).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看你可以用MACAD-Gym做什么，请访问其由Praveen Palanisamy开发和维护的Github仓库：[https://github.com/praveen-palanisamy/macad-gym](https://github.com/praveen-palanisamy/macad-gym)。
- en: After this short detour, let's proceed to fully competitive settings in MARL.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一短暂的绕道之后，我们继续讨论MARL中的完全竞争环境。
- en: Fully competitive environments
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全竞争环境
- en: 'In fully competitive environments, the success of one of the agents means failure
    for the others. Therefore, such settings are modelled as zero-sum games:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全竞争的环境中，一个代理的成功意味着其他代理的失败。因此，这些环境被建模为零和博弈：
- en: '![](img/Formula_09_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_001.jpg)'
- en: where ![](img/Formula_09_002.png) is the reward for the ![](img/Formula_09_003.png)
    agent.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/Formula_09_002.png)是！[](img/Formula_09_003.png)代理的奖励。
- en: 'Some examples to fully competitive environments are the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 完全竞争环境的一些例子如下：
- en: '**Board games**: This is the classic example for such environments, such as
    chess, Go, and tic-tac-toe.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**棋盘游戏**：这是这种环境的经典例子，例如国际象棋、围棋和井字游戏。'
- en: '**Adversarial settings**: In situations where we want to minimize the risk
    of failure for an agent in real-life, we might train it against adversarial agents.
    This creates a fully competitive environment.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗性设置**：在我们希望最小化代理在现实生活中失败风险的情况下，我们可能会让它与对抗性代理进行训练。这就创建了一个完全竞争的环境。'
- en: Finally, let's take a look at mixed cooperative-competitive environments.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看一下混合合作-竞争环境。
- en: Mixed cooperative-competitive environments
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合合作-竞争环境
- en: 'A third type of environments involves both collaboration and cooperation between
    agents. These environments are usually modelled as general-sum games:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类环境涉及代理之间的合作与竞争。这些环境通常被建模为一般和博弈：
- en: '![](img/Formula_09_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_004.jpg)'
- en: Here ![](img/Formula_09_005.png) is the reward for the ![](img/Formula_09_006.png)
    agent and ![](img/Formula_09_007.png) is some fixed total reward that can be collected
    by the agents.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里！[](img/Formula_09_005.png)是！[](img/Formula_09_006.png)代理的奖励，！[](img/Formula_09_007.png)是代理可以收集的固定总奖励。
- en: 'Here are some examples to mixed environments:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些混合环境的例子：
- en: '**Team competitions**: When there are teams of agents competing against each
    other, the agents within a team collaborate to defeat the other teams.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**团队竞争**：当智能体分成多个团队并相互竞争时，每个团队内的智能体会合作，以击败其他团队。'
- en: '**Economy**: If you think about the economic activities we are involved in,
    it is a mix of competition and cooperation. A nice example to this is how tech
    companies such as Microsoft, Google, Facebook, and Amazon compete against each
    other for certain businesses while collaborating on some open-source projects
    to advance the software technology.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经济**：如果你思考我们所参与的经济活动，它是竞争与合作的结合。一个很好的例子是像微软、谷歌、Facebook和亚马逊这样的科技公司，它们在某些业务上相互竞争，但也在一些开源项目上合作，以推动软件技术的发展。'
- en: Info
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息
- en: At this point, it is worth taking a pause to watch OpenAI's demo on agents playing
    hide-and-seek in teams. The agents develop very cool cooperation and competition
    strategies after playing against each other for a large number of episodes, inspiring
    us for the potential in RL towards artificial general intelligence. See Figure
    9.2 for a quick snapshot and the link to the video.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一点上，值得暂停一下，观看OpenAI的演示，展示了智能体在团队中玩捉迷藏的场景。智能体在经历了大量对局后，发展出了非常酷的合作与竞争策略，这激发了我们对强化学习（RL）在实现人工通用智能（AGI）方面的潜力的思考。参见图9.2，获取快速截图和视频链接。
- en: '![Figure 9.2: OpenAI''s agents playing hide-and-seek (Source: https://youtu.be/kopoLzvh5jY)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.2：OpenAI的智能体玩捉迷藏（来源：https://youtu.be/kopoLzvh5jY）'
- en: '](img/B14160_09_002.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_002.jpg)'
- en: 'Figure 9.2: OpenAI''s agents playing hide-and-seek (Source: [https://youtu.be/kopoLzvh5jY](https://youtu.be/kopoLzvh5jY))'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：OpenAI的智能体玩捉迷藏（来源：[https://youtu.be/kopoLzvh5jY](https://youtu.be/kopoLzvh5jY)）
- en: Now that we have covered the fundamentals, next, let's look into some of the
    challenges with MARL.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了基础知识，接下来让我们探讨一下多智能体强化学习（MARL）面临的一些挑战。
- en: Exploring the challenges in multi-agent reinforcement learning
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索多智能体强化学习中的挑战
- en: In the earlier chapters in this book, we discussed many challenges in reinforcement
    learning. In particular, the dynamic programming methods we initially introduced
    are not able to scale to problems with complex and large observation and action
    spaces. Deep reinforcement learning approaches, on the other hand, although capable
    of handling complex problems, lack theoretical guarantees and therefore required
    many tricks to stabilize and converge. Now that we talk about problems in which
    there are more than one agent learning, interacting with each other, and affecting
    the environment; the challenges and complexities of single-agent RL are multiplied.
    For this reason, many results in MARL are empirical.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们讨论了强化学习中的许多挑战。特别是，我们最初介绍的动态规划方法无法扩展到具有复杂和大规模观察与动作空间的问题。另一方面，深度强化学习方法尽管能够处理复杂问题，但缺乏理论保证，因此需要许多技巧来稳定和收敛。现在，我们谈论的是在多个智能体共同学习、相互作用并影响环境的情况下；单智能体强化学习中的挑战和复杂性被成倍增加。因此，MARL中的许多结果都是经验性的。
- en: In this section, we discuss what makes MARL specifically complex and challenging.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了为什么MARL特别复杂且充满挑战。
- en: Non-stationarity
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非平稳性
- en: The mathematical framework behind single-agent RL is the Markov decision process
    (MDP), which establishes that the environment dynamics depend on the state it
    is in and not the history. This suggests that the environment is stationary, on
    which many approaches rely for convergence. Now that there are multiple agents
    in the environment that are learning hence changing their behavior over time,
    that fundamental assumption falls apart and prevents us analyzing MARL the same
    way we analyze single-agent RL.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 单智能体强化学习背后的数学框架是马尔可夫决策过程（MDP），它表明环境动态依赖于当前状态，而非历史。这表明环境是平稳的，许多方法依赖于此假设来实现收敛。但现在环境中有多个智能体在学习，因此随着时间的推移改变其行为，这一基本假设被打破，阻碍了我们像分析单智能体强化学习那样分析MARL。
- en: As an example, consider off-policy methods such as Q-learning with replay buffer.
    In MARL, using such approaches is especially challenging because the experience
    that was collected a while ago might be wildly different than how the environment
    (in part, the other agents) responds to the actions of a single agent.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，考虑使用带有重放缓冲区的Q学习等脱政策方法。在MARL中，使用这种方法尤其具有挑战性，因为较早收集的经验可能与环境（部分由其他智能体构成）对单个智能体行动的反应截然不同。
- en: Scalability
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: One possible solution to non-stationarity is to account for the actions of the
    other agents, such as using a joint action space. As the number of agents increases,
    this becomes increasingly intractable, which makes scalability an issue in MARL.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决非平稳性的一个可能方法是考虑其他智能体的行为，例如使用联合行动空间。随着智能体数量的增加，这种方法会变得越来越不可行，这使得MARL的可扩展性成为一个问题。
- en: Having said this, it is somewhat easier to analyze how the agent behavior could
    converge when there are only two agents in the environment. If you are familiar
    with game theory, a common way to look at such systems is to understand equilibrium
    points where neither of the agents benefit from changing their policies.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，当环境中只有两个智能体时，分析智能体行为如何收敛相对容易。如果你熟悉博弈论，一种常见的看待这种系统的方式是理解均衡点，在这些点上，智能体不会通过改变他们的策略而受益。
- en: Info
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: If you need a brief introduction to game theory and Nash equilibrium, check
    out this video at [https://www.youtube.com/watch?v=0i7p9DNvtjk](https://www.youtube.com/watch?v=0i7p9DNvtjk)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要对博弈论和纳什均衡的简要介绍，可以查看这个视频：[https://www.youtube.com/watch?v=0i7p9DNvtjk](https://www.youtube.com/watch?v=0i7p9DNvtjk)
- en: This analysis gets significantly harder when there are more than two agents
    in the environment, which makes large-scale MARL very difficult to understand.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当环境中有超过两个智能体时，这种分析会变得非常困难，这使得大规模MARL变得非常难以理解。
- en: Unclear reinforcement learning objective
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不明确的强化学习目标
- en: 'In single-agent RL, the objective is clear: To maximize the expected cumulative
    return. On the other hand, there is no such a unique objective defined MARL.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在单一智能体强化学习（RL）中，目标很明确：最大化预期的累积回报。另一方面，在多智能体强化学习（MARL）中并没有一个唯一明确的目标。
- en: Think about a chess game in which we try to train a very good chess agent. To
    this end, we train many agents competing with each other using **self-play**.
    How would you set the objective of this problem? First thought could be to maximize
    the reward of the best agent. But this could result in making all the agents terrible
    players but one. This would certainly not what we would want.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个棋局，我们试图训练一个非常强大的棋手。为此，我们训练许多智能体，让它们通过**自我对弈**相互竞争。你会如何设定这个问题的目标？最初的想法可能是最大化最佳智能体的奖励。但是，这可能导致除了一个以外所有智能体都成为糟糕的棋手。这显然不是我们所希望的结果。
- en: A popular objective in MARL is to achieve convergence to Nash equilibrium. This
    often works well, but it also has disadvantages when the agents are not fully
    rational. Moreover, Nash equilibrium naturally implies overfitting to the policies
    of the other agents, which is not necessarily desirable.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MARL中一个流行的目标是实现纳什均衡的收敛。这个方法通常很有效，但当智能体不完全理性时，它也有缺点。此外，纳什均衡自然意味着对其他智能体策略的过拟合，这不一定是我们想要的。
- en: Information sharing
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息共享
- en: 'Another important challenge in MARL is to design the information sharing structure
    between the agents. There are three alternative information structures we can
    consider:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MARL中的另一个重要挑战是设计智能体之间的信息共享结构。我们可以考虑三种信息结构：
- en: '**Fully centralized**: In this structure, all of the information collected
    by the agents are processed by a central mechanism and the local policies would
    leverage this centralized knowledge. The advantage of this structure is the full
    coordination between the agents. On the other hand, this could lead to an optimization
    problem that won''t scale as the number of the agents grow.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全中心化**：在这种结构下，所有智能体收集的信息都由一个中央机制处理，局部策略将利用这一集中知识。该结构的优势在于智能体之间的完全协调。另一方面，这可能导致一个优化问题，随着智能体数量的增加，它的扩展性变差。'
- en: '**Fully decentralized**: In this structure, no information is exchanged between
    the agents and each agent would act based on their local observations. The obvious
    benefit here is that there is no burden of a centralized coordinator. On the flip
    side, the actions of the agents would be suboptimal due to their limited information
    about the environment. In addition, RL algorithms might have a hard time to converge
    when the training is fully independent due to high partial observability.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全去中心化**：在这种结构下，智能体之间不交换信息，每个智能体将基于他们的本地观察做出决策。显而易见的好处是没有中心化协调器的负担。另一方面，由于智能体对环境的信息有限，智能体的行为可能是次优的。此外，当训练完全独立时，由于高度部分可观测性，强化学习算法可能会很难收敛。'
- en: '**Decentralized but networked agents**: This structure would allow information
    exchange between small groups of (neighbor) agents. In turn, this would help the
    information spread among them. The challenge here would be to create a robust
    communication structure that would work under different conditions of the environment.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去中心化但网络化的代理**：这种结构允许小群体（邻居）代理之间交换信息。反过来，这有助于信息在它们之间传播。这里的挑战是创建一个能够在不同环境条件下有效工作的强健通信结构。'
- en: Depending on the objective of the RL problem as well as the availability of
    computational resources, different approaches could be preferred. Consider a cooperative
    environment in which a large swarm of robots are trying to achieve a common goal.
    In this problem, fully centralized or networked control might make sense. In a
    fully competitive environment, such as a strategy video game, a fully decentralized
    structure might be preferred since there would be no common goal between the agents.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据强化学习问题的目标以及计算资源的可用性，不同的方式可能会更为优选。考虑一个合作环境，其中大量的机器人群体试图达成一个共同的目标。在这种问题中，完全集中式或网络化的控制可能是合理的。而在完全竞争的环境中，比如策略类视频游戏，完全去中心化的结构可能更为适合，因为在代理之间没有共同的目标。
- en: After this much theory, it is now time to go into practice! Soon, we will train
    a tic-tac-toe agent that you can play against during your meetings or classes.
    Let's first describe how we will do the training and then go into implementation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这么多理论之后，现在是时候进入实践了！很快，我们将训练一个井字游戏代理，你可以在会议或课堂上与其对战。首先，我们将描述如何进行训练，然后再进入实现部分。
- en: Training policies in multi-agent settings
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多代理环境中训练策略
- en: There are many algorithms and approaches designed for MARL, which can be classified
    in the following two broad categories.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多针对MARL设计的算法和方法，可以大致分为以下两大类。
- en: '**Independent learning**: This approach suggests training agents individually
    while treating the other agents in the environment as part of the environment.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立学习**：这种方法建议独立训练代理，并将环境中的其他代理视为环境的一部分。'
- en: '**Centralized training and decentralized execution**: In this approach, there
    is a centralized controller that uses information from multiple agents during
    training. At the time of execution (inference), the agents locally execute the
    policies, without relying on a central mechanism.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中式训练与去中心化执行**：在这种方法中，有一个集中式控制器，在训练过程中使用来自多个代理的信息。在执行（推理）时，代理将独立地执行策略，而不依赖于中央机制。'
- en: Generally speaking, we can take any of the algorithms we covered in one of the
    previous chapters and use it in a multi-agent setting to train policies via independent
    learning, which, as it turns out, is a very competitive alternative to specialized
    MARL algorithms. So rather than dumping more theory and notation on you, in this
    chapter, we will skip discussing the technical details of any specific MARL algorithm
    and refer you to literature for that.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以选择之前章节中提到的任何算法，并将其应用于多代理环境中，通过独立学习来训练策略，事实证明，这是一种非常有竞争力的替代方法，比专门的多代理强化学习（MARL）算法还要有效。因此，在本章中，我们将跳过讨论任何特定MARL算法的技术细节，并将相关资料交给你参考文献部分。
- en: Info
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: An easy and highly recommended read on a comparison of deep MARL algorithms
    is by (Papoudakis et al. 2020). Just visit the references section to find the
    link to the paper.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度MARL算法比较的简明且强烈推荐的阅读资料可以参考(Papoudakis et al. 2020)。只需访问参考文献部分，找到论文的链接。
- en: 'So, we will use independent learning. How does it work, though? Well, it requires
    us to:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将采用独立学习。那么它是如何工作的呢？嗯，它要求我们：
- en: Have an environment with multiple agents,
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有多个代理的环境，
- en: Maintain policies to support the agents,
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维持支持代理的策略，
- en: Appropriately assign the rewards coming out of the environment to the agents.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当地将来自环境的奖励分配给代理。
- en: It would be tricky for us to come up with a proper framework here to handle
    the points above. Fortunately, RLlib has a multi-agent environment to come to
    the rescue. Next, let's see how it works.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，设计一个合适的框架来处理上述问题可能会有些棘手。幸运的是，RLlib提供了一个多代理环境来解决这个问题。接下来，让我们看看它是如何工作的。
- en: RLlib multi-agent environment
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLlib多代理环境
- en: 'RLlib''s multi-agent environment flexibly allows us to hook up with one of
    the algorithms that you already know to use for MARL. In fact, RLlib documentation
    conveniently shows which algorithms are compatible with this environment type:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 的多智能体环境灵活地允许我们连接到一个你已经熟悉的算法，用于 MARL。事实上，RLlib 文档方便地展示了哪些算法与这种环境类型兼容：
- en: '![Figure 9.3: RLlib''s algorithm list shows multi-agent compatibility (Source:
    https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3：RLlib 的算法列表显示了多智能体的兼容性（来源：[https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html](https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html)）'
- en: '](img/B14160_09_003.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_003.jpg)'
- en: 'Figure 9.3: RLlib''s algorithm list shows multi-agent compatibility (Source:
    [https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html](https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html))'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：RLlib 的算法列表显示了多智能体的兼容性（来源：[https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html](https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html)）
- en: In that list, you will also see a separate section for MARL-specific algorithms.
    In this chapter, we will use PPO.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在该列表中，你还会看到一个单独的部分，列出了针对 MARL 特定算法的内容。在本章中，我们将使用 PPO。
- en: 'Of course, the next step is to understand how to use an algorithm we selected
    with a multi-agent environment. At this point, we need to make a key differentiation:
    *Using RLlib, we will train policies, not the agents (at least directly). An agent
    will be mapped to one of the policies that are being trained to retrieve actions.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，下一步是理解如何将我们选择的算法与多智能体环境一起使用。此时，我们需要做一个关键区分：*使用 RLlib 时，我们将训练策略，而不是智能体（至少不是直接训练）。一个智能体将映射到正在训练的某个策略，以获取动作。*
- en: 'RLlib documentation illustrates this with the following figure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 文档通过以下图示说明了这一点：
- en: '![Figure 9.4: Relationship between agents and policies in RLlib (source: https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4：RLlib 中智能体和策略之间的关系（来源：[https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical](https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical)）'
- en: '](img/B14160_09_004.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_004.jpg)'
- en: 'Figure 9.4: Relationship between agents and policies in RLlib (source: [https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical](https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical))'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：RLlib 中智能体和策略之间的关系（来源：[https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical](https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical)）
- en: In case you have not realized, this gives us a very powerful framework to model
    a MARL environment. For example, we can flexibly add agents to the environment,
    remove them, and train multiple policies for the same task. All is fine as far
    as we specify the mapping between the policies and agents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有意识到，这为我们提供了一个非常强大的框架，用来建模多智能体强化学习（MARL）环境。例如，我们可以灵活地向环境中添加智能体、移除智能体，并为同一任务训练多个策略。只要我们指定了策略与智能体之间的映射，一切都能顺利进行。
- en: 'With that, let''s look into what the training loop in RLlib requires to be
    used with a multi-agent environment:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，让我们看看 RLlib 中的训练循环在与多智能体环境配合使用时需要哪些要求：
- en: List of policies with corresponding ids. These are what will be trained.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有相应 id 的策略列表。这些是将要被训练的策略。
- en: A function that maps a given agent id to a policy id, so that RLlib knows where
    the actions for a given agent will come from.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个将给定智能体 id 映射到策略 id 的函数，这样 RLlib 就知道为给定智能体生成的动作来自哪里。
- en: 'Once this is set, the environment will use the Gym convention communicate with
    RLlib. The difference will be that observations, rewards, and terminal statements
    will be emitted for multiple agents in the environment. For example, a reset function
    will return a dictionary of observations like the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好，环境将使用 Gym 规范与 RLlib 进行通信。不同之处在于，观察、奖励和终止语句将为环境中的多个智能体发出。例如，重置函数将返回如下所示的观察字典：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Similarly, the actions by the policies will be passed to the agents from which
    we received an observation, similar to:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由策略生成的动作会传递给我们从中收到观察的智能体，类似于：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This means that if the environment returns an observation for an agent, it is
    asking an action back.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果环境返回一个智能体的观察，它就会要求返回一个动作。
- en: So far so good! This should suffice to give you some idea about how it works.
    Things will be clearer when we go into the implementation!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利！这些内容应该能给你一些关于它如何工作的初步了解。等我们进入实现部分时，事情会变得更加清晰！
- en: Soon, we will train tic-tac-toe policies, as we mentioned. The agents that will
    use these policies will compete against each other to learn how to play the game.
    This is called **competitive self-play**, which we discuss next.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，我们将训练井字棋策略，正如我们之前提到的。使用这些策略的智能体将相互竞争，学习如何玩游戏。这就是**竞争性自我对弈**，我们接下来会讨论这个话题。
- en: Competitive self-play
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 竞争性自我对弈
- en: Self-play is a great tool to train RL agents for competitive tasks, which include
    things like board games, multi-player video games, adversarial scenarios etc.
    Many of the famous RL agents you have heard about are trained this way, such as
    AlphaGo, OpenAI Five for Dota 2, and the StarCraft II agent of DeepMind.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈是训练RL智能体进行竞争性任务的一个很好的工具，这些任务包括棋类游戏、多人视频游戏、对抗性场景等。你听说过的许多著名RL智能体，都是通过这种方式训练的，比如AlphaGo、OpenAI
    Five（用于Dota 2）以及DeepMind的《星际争霸II》智能体。
- en: Info
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The story of OpenAI Five is a very interesting one, showing how the project
    started and evolved to what it is today. The blog posts on the project gives many
    beneficial information, from hyperparameters used in the models to how the OpenAI
    team overcame interesting challenges throughout the work. You can find the project
    page at [https://openai.com/projects/five/](https://openai.com/projects/five/).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Five的故事非常有趣，展示了该项目是如何开始并发展成今天的模样的。关于该项目的博客文章提供了许多有用的信息，从模型中使用的超参数到OpenAI团队如何克服工作中的各种挑战。你可以在[https://openai.com/projects/five/](https://openai.com/projects/five/)找到该项目页面。
- en: One of the drawbacks of vanilla self-play is that the agents, who only see the
    other agents trained the same way, tend to overfit to each other's strategies.
    To overcome this, it makes sense to train multiple policies and pit them against
    each other, which is also what we will do in this chapter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 传统自我对弈的一个缺点是，智能体仅看到以相同方式训练的其他智能体，因此容易过拟合到彼此的策略。为了解决这个问题，训练多个策略并让它们彼此对抗是有意义的，这也是我们在本章中要做的事情。
- en: Info
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Overfitting is such a challenge in self-play that even training multiple policies
    and simply setting them against each other is not enough. DeepMind created a "League"
    of agents/policies, like a basketball league, to obtain a really competitive training
    environment, which led to the success of their StarCraft II agents. They explain
    their approach in a nice blog at [https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合在自我对弈中是一个挑战，甚至训练多个策略并将它们彼此对抗也不足以解决问题。DeepMind创建了一个代理/策略的“联赛”，就像篮球联赛一样，以获得一个真正具有竞争性的训练环境，这也促成了他们在《星际争霸II》上的成功。他们在一个很棒的博客中解释了他们的方法，[https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)。
- en: Finally, it is time to experiment with multi-agent reinforcement learning!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，到了尝试多智能体强化学习的时候了！
- en: Training tic-tac-toe agents through self-play
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自我对弈训练井字棋智能体
- en: In this section, we will provide you with some key explanations of the code
    in our Github repo to get a better grasp of MARL with RLlib while training tic-tac-toe
    agents on a 3x3 board. For the full code, you can refer to [https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为您提供一些关于我们Github仓库中代码的关键解释，以便您更好地掌握使用RLlib进行MARL的技巧，同时在3x3的井字棋棋盘上训练智能体。完整代码可以参考[https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python)。
- en: '![Figure 9.5: A 3x3 tic-tac-toe. For the image credit and to learn how it is
    played, see https://en.wikipedia.org/wiki/Tic-tac-toe'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.5：一个3x3的井字棋。图片来源以及如何玩游戏的说明，见 https://en.wikipedia.org/wiki/Tic-tac-toe'
- en: '](img/B14160_09_005.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_005.jpg)'
- en: 'Figure 9.5: A 3x3 tic-tac-toe. For the image credit and to learn how it is
    played, see [https://en.wikipedia.org/wiki/Tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：一个3x3的井字棋。图片来源以及如何玩游戏的说明，见[https://en.wikipedia.org/wiki/Tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe)。
- en: Let's started with designing the multi-agent environment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从设计多智能体环境开始。
- en: Designing the multi-agent tic-tac-toe environment
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计多智能体井字棋环境
- en: 'In the game, we have two agents, X and O, playing the game. We will train four
    policies for the agents to pull their actions from, and each policy can play either
    an X or O. We construct the environment class as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏中，我们有两个代理，X和O，它们在玩游戏。我们将为代理训练四个策略，每个策略可以选择X或O的行动。我们构建环境类如下：
- en: Chapter09/tic_tac_toe.py
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter09/tic_tac_toe.py
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, 9 refers to the number of squares on the board, each of which can be filled
    by either X, O, or none.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，9代表棋盘上的方格数量，每个方格可以被X、O或空白填充。
- en: 'We reset this environment as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式重置这个环境：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, we don't pass this state directly to the policy as it is just full
    of characters. We process it so that, for the player that is about to play, its
    own marks are always represented as 1s and the other player's marks are always
    represented as 2s.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不会将这个状态直接传递给策略，因为它仅仅是充满字符。我们会处理它，以便在即将进行的玩家面前，他们自己的标记总是用1表示，而对手的标记总是用2表示。
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This processed observation is what is passed to the policy:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理后的观察结果就是传递给策略的数据：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, the `step` method processes the action for the player and proceeds
    the environment to the next step. For a win, the player gets a ![](img/Formula_09_008.png),
    and ![](img/Formula_09_009.png) for a loss. Notice that the policies may suggest
    putting the mark on an already-occupied square, a behavior that is penalized by
    a ![](img/Formula_09_010.png) points.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`step` 方法会处理玩家的操作，并将环境推进到下一步。如果赢了，玩家会得到一个 ![](img/Formula_09_008.png)，如果输了，则得到
    ![](img/Formula_09_009.png)。注意，策略可能会建议在已占据的方格上放置标记，这种行为会被惩罚并扣除 ![](img/Formula_09_010.png)
    分。
- en: Configuring the trainer
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置训练器
- en: 'We create 4 policies to train, assign them some ids, and specify their observation
    and action spaces. Here is how we do it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了4个策略来训练，给它们分配一些ID，并指定它们的观察空间和行动空间。我们是这样做的：
- en: Chapter09/ttt_train.py
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter09/ttt_train.py
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: While creating the config dictionary to pass to the trainer, we map the agents
    to the policies. To mitigate overfitting, rather than assigning a specific policy
    to a given agent, we randomly pick the policy to retrieve the action from for
    the agent that is about to play.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建传递给训练器的配置字典时，我们将代理映射到策略上。为了减少过拟合，而不是为某个特定代理分配特定策略，我们随机选择一个策略来获取即将执行的代理的行动。
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: During training, we save the models as they improve. Since there are multiple
    policies involved, as a proxy to measure the progress, we check if the episodes
    are getting longer with valid moves. Our hope is that as the agents get competitive,
    more and more games will result in draws, at which point the board is full of
    marks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们会保存模型，以便跟踪它们的改进。由于涉及多个策略，作为进度的代理，我们检查回合是否因有效的操作而变长。我们希望随着代理变得更具竞争力，越来越多的游戏将以平局结束，这时棋盘上将充满标记。
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That's it! Now the fun of watching the training!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在开始观看训练的乐趣吧！
- en: Observing the results
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察结果
- en: 'Initially in the game, there will be lots of invalid moves, resulting in extended
    episode lengths and excessive penalties for the agents. Therefore, the mean agent
    reward plot will look like the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏开始时，会有很多无效操作，导致回合长度延长，并且代理受到过多惩罚。因此，代理的平均奖励图像将如下所示：
- en: '![Figure 9.6: Average agent reward'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6：平均代理奖励'
- en: '](img/B14160_09_006.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_006.jpg)'
- en: 'Figure 9.6: Average agent reward'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：平均代理奖励
- en: 'Notice how this starts in deep negatives to converge to zero, indicating draws
    as the common result. In the meantime, you should see the episode length converging
    to 9:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到它如何从深度负值开始，逐渐趋近于零，表明平局成为了常见的结果。与此同时，你应该看到回合长度趋近于9：
- en: '![Fig 9.7: Episode length progress'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7：回合长度进展'
- en: '](img/B14160_09_007.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_09_007.jpg)'
- en: 'Fig 9.7: Episode length progress'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：回合长度进展
- en: When you see the competition on fire, you can stop the training! What will be
    even more interesting is to play against the AI by running the script `ttt_human_vs_ai.py`
    or watch them compete by running `ttt_ai_vs_ai.py`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到竞争愈演愈烈时，你可以停止训练！更有趣的是，运行 `ttt_human_vs_ai.py` 脚本与AI对战，或者运行 `ttt_ai_vs_ai.py`
    观察它们的对战。
- en: With that, we conclude this chapter. This was a fun one, wasn't it? Let's summarize
    the learnings from this chapter next.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们结束了这一章。这是一个有趣的章节，不是吗？接下来我们来总结一下本章的学习内容。
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered multi-agent reinforcement learning. This branch
    of RL is more challenging than others due to multiple decision-makers influencing
    the environment and also evolving over time. After introducing some MARL concepts,
    we explored these challenges in detail. We then proceeded to train tic-tac-toe
    agents through competitive self-play using RLlib. And they were so competitive
    that they kept coming to a draw at the end of the training!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讲解了多智能体强化学习（MARL）。与其他强化学习分支相比，它更具挑战性，因为有多个决策者影响环境，而且这些决策者在时间上也会发生变化。在介绍了MARL的一些概念后，我们详细探讨了这些挑战。随后，我们通过RLlib进行竞争性自我博弈来训练井字游戏智能体。它们竞争激烈，训练结束时总是以平局告终！
- en: In the next chapter, we switch gears to discuss an emerging approach in reinforcement
    learning, called Machine Teaching, which brings the subject matter expert, you,
    more actively into the process to guide the training. Hoping to see you there
    soon!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向讨论一种新兴的强化学习方法——机器教学，它将让领域专家（也就是你）更积极地参与到训练过程中，指导训练进展。期待在那里见到你！
- en: References
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Mosterman, P. J. et al. (2014). A heterogeneous fleet of vehicles for automated
    humanitarian missions. Computing in Science & Engineering, vol. 16, issue 3, pg.
    90-95\. URL: [http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf](http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf)'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mosterman, P. J. 等（2014）。用于自动化人道主义任务的异构车队。计算科学与工程，16卷，第3期，页90-95。网址：[http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf](http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf)
- en: Papoudakis, Georgios, et al. (2020). Comparative Evaluation of Multi-Agent Deep
    Reinforcement Learning Algorithms. arXiv.org, [http://arxiv.org/abs/2006.07869](http://arxiv.org/abs/2006.07869)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Papoudakis, Georgios, 等（2020）。多智能体深度强化学习算法的比较评估。arXiv.org，[http://arxiv.org/abs/2006.07869](http://arxiv.org/abs/2006.07869)
- en: Palanisamy, Praveen. (2019). Multi-Agent Connected Autonomous Driving Using
    Deep Reinforcement Learning. arxiv.org, [https://arxiv.org/abs/1911.04175v1](https://arxiv.org/abs/1911.04175v1)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Palanisamy, Praveen.（2019）。基于深度强化学习的多智能体自动驾驶。arxiv.org，[https://arxiv.org/abs/1911.04175v1](https://arxiv.org/abs/1911.04175v1)
