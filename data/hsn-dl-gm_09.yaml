- en: Agent and the Environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体与环境
- en: Playing with and exploring experimental reinforcement learning environments
    is all well and good, but, at the end of the day, most game developers want to
    develop their own learning environment. To do that, we need to understand a lot
    more about training deep reinforcement learning environments, and, in particular,
    how an agent receives and processes input. Therefore, in this chapter, we will
    take a very close look at training one of the more difficult sample environments
    in Unity. This will help us understand many of the intricate details of how important
    input and state is to training an agent, and the many features in the Unity ML-Agents
    toolkit that make it easy for us to explore multiple options. This will be a critical
    chapter for anyone wanting to build their own environments and use the ML-Agents
    in their game. So, if you need to work through this chapter a couple of times
    to understand the details, please do so.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 玩转和探索实验性的强化学习环境是很有趣的，但最终，大多数游戏开发者希望开发自己的学习环境。为了做到这一点，我们需要更深入地了解训练深度强化学习环境，特别是一个智能体如何接收和处理输入。因此，在本章中，我们将仔细研究如何在Unity中训练一个更为复杂的示例环境。这将帮助我们理解输入和状态对训练智能体的重要性，以及Unity
    ML-Agents工具包中许多使我们能够探索多种选项的特性。本章对于任何希望在自己的游戏中构建环境并使用ML-Agents的人来说都至关重要。所以，如果你需要反复阅读本章以理解细节，请务必这样做。
- en: 'In this chapter, we are going to cover many details related to how agents process
    input/state, and how you can adapt this to fit your agent training. Here is a
    summary of what we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖许多与智能体如何处理输入/状态相关的细节，以及你如何调整这些内容以适应你的智能体训练。以下是本章内容的总结：
- en: Exploring the training environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索训练环境
- en: Understanding state
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解状态
- en: Understanding visual state
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解视觉状态
- en: Convolution and visual state
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积与视觉状态
- en: Recurrent networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Ensure that you have read, understood, and ran some of the sample exercises
    from the last chapter, [Chapter 6](b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml),
    *Unity ML-Agents*. It is essential that you have Unity and the ML-Agents toolkit
    configured and running correctly before continuing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已经阅读、理解并运行了上一章的部分示例练习，[第6章](b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml)，*Unity
    ML-Agents*。在继续之前，确保你已经正确配置并运行了Unity和ML-Agents工具包。
- en: Exploring the training environment
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索训练环境
- en: 'One of the things that often pushes us to success, or pushes us to learn, is
    failure. As humans, when we fail, one of two things happens: we try harder or
    we quit. Interestingly, this is not unlike a negative reward in reinforcement
    learning. In RL, an agent that gets a negative reward may quit exploring a path
    if it sees no future value, or that it predicts will not give enough benefit.
    However, if the agent feels like more exploration is needed, or it hasn''t exhausted
    the path fully, it will push on and, often, this leads it to the right path. Again,
    this is certainly not unlike us humans. Therefore, in this section, we are going
    to train one of the more difficult example agents to push ourselves to learn how
    to fail and fix training failures.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 经常推动我们成功或推动我们学习的因素之一就是失败。作为人类，当我们失败时，通常会发生两件事：我们要么更加努力，要么选择放弃。有趣的是，这与强化学习中的负奖励很相似。在RL中，智能体如果获得负奖励，可能会因为看不到未来的价值或预测无法带来足够的好处而放弃探索某条路径。然而，如果智能体认为需要更多的探索，或者它没有完全探索完这条路径，它就会继续前进，并且通常这会引导它走上正确的道路。同样，这与我们人类的情况非常相似。因此，在本节中，我们将训练一个较为复杂的示例智能体，促使我们学习如何面对失败并修正训练中的问题。
- en: Unity is currently in the process of building a multi-level bench marking tower
    environment that features multiple levels of difficulty. This will allow DRL enthusiasts,
    practitioners, and researchers to test their skills/models on baseline environments.
    The author has been told, on reasonably good authority, that this environment
    should be completed by early/mid 2019.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Unity目前正在构建一个多级基准塔环境，具有多个难度等级。这将允许深度强化学习爱好者、从业者和研究人员在基准环境上测试他们的技能和模型。作者获得的相对可靠的消息是，这个环境应该会在2019年年初或年中完成。
- en: We will need to use many of the advanced features of the Unity ML-Agents toolkit
    ultimately get this example working. This will require you to have a good understanding
    of the first five chapters of this book. If you skipped those chapters to get
    here, please go back and review them as needed. In many places in this chapter,
    helpful links have been provided to previous relevant chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终需要使用Unity ML-Agents工具包的许多高级功能来使这个示例正常工作。这要求你对本书的前五章有良好的理解。如果你跳过了这些章节来到这里，请根据需要回去复习。在本章的许多地方，我们提供了指向之前相关章节的有用链接。
- en: The training sample environment we will focus on is the VisualHallway, not to
    be confused with the standard Hallway example. The VisualHallway differs in that
    it uses the camera as the complete input state into the model, while the other
    Unity examples we previously looked at used some form of multi-aware sensor input,
    often allowing the agent to see 90 to 360 degrees at all times, and be given other
    useful information. This is fine for most games, and, in fact, many games still
    allow  such cheats or intuition for NPC or computer opponents as part of their
    AI. Putting these cheats in for a game's AI has been an accepted practice for
    many years, but perhaps that will soon change.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注的训练示例环境是VisualHallway，不要与标准的Hallway示例混淆。VisualHallway的不同之处在于它使用摄像头作为模型的完整输入状态，而我们之前看到的其他Unity示例则使用某种形式的多传感器输入，通常允许代理在任何时候看到90度到360度的视角，并提供其他有用的信息。这对大多数游戏来说是可以接受的，事实上，许多游戏仍然允许这样的“作弊”或直觉作为NPC或计算机对手AI的一部分。将这些“作弊”加入游戏AI一直是一个被接受的做法，但也许这很快就会发生改变。
- en: After all, good games are fun to play, and make sense to the player. Games of
    the not so distant past could get away with giving the AI cheats. However, now,
    players are expecting more, they want their AI to play by the same rules as them.
    The previous perception that computer AI was hindered by technological limitations
    is gone, and now a game AI must play by the same rules as the player, which makes
    our focus on getting the VisualHallway sample working/training more compelling.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，好游戏是有趣的，并且对玩家来说是合乎逻辑的。过去不久的游戏可能能让AI作弊，但现在，玩家对AI的期望更高了，他们希望AI和他们遵循相同的规则。之前认为计算机AI受到技术限制的观念已经消失，现在游戏AI必须遵循与玩家相同的规则，这使得我们专注于使VisualHallway示例正常工作和训练变得更加有意义。
- en: There is, of course, another added benefit to teaching an AI to play/learn like
    a player, and that is the ability to transfer that capability to play in other
    environments using a concept called transfer learning. We will explore transfer
    learning in [Chapter 10](1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml), *Imitation
    and Transfer Learning*, where we will learn how to adapt pretrained models/parameters
    and apply them to other environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，教AI像玩家一样玩/学习还有另一个额外的好处，那就是能够将这种能力转移到其他环境中，这个概念叫做迁移学习。我们将在[第10章](1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml)《模仿与迁移学习》中探索迁移学习，学习如何调整预训练模型/参数，并将其应用于其他环境。
- en: The VisualHallway/Hallway samples start by dropping the agent into a long room
    or hallway at random. In the center of this space is a colored block, and at one
    end of the hallway in each corner is a colored square covering the floor. The
    block is either red or gold (orange/yellow) and is used to inform the agent of
    the target square that is the same color. The goal is for the agent to move to
    the correct colored square. In the standard Hallway example, the agent is given
    360 degree sensor awareness. In the Visual Hallway example, the agent is only
    shown a camera view of the room, exactly as the player version of the game would
    see. This puts our agent on equal footing with a player.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: VisualHallway/Hallway 示例首先会将代理随机放入一个长房间或走廊。在这个空间的中央是一个彩色方块，每个走廊的两端角落都有一个覆盖地板的彩色方形区域。这个方块的颜色要么是红色，要么是金色（橙色/黄色），用来告诉代理目标方块的颜色与之相同。目标是让代理移动到正确的彩色方块。在标准的Hallway示例中，代理拥有360度的传感器感知。而在VisualHallway示例中，代理只能看到房间的摄像机视图，就像玩家在游戏中看到的一样。这使得我们的代理与玩家站在了同一起跑线。
- en: 'Before we get to training, let''s open up the example and play it as a player
    would, and see how we do. Follow this exercise to open the VisualHallway sample:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，让我们像玩家一样打开示例并玩一下，看看我们能做得怎么样。按照这个练习打开VisualHallway示例：
- en: Ensure you have a working installation of ML-Agents and can train a brain externally
    in Python before continuing. Consult the previous chapter if you need help.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，确保你已经正确安装了 ML-Agents，并且能够在 Python 中外部训练大脑。如果需要帮助，请参考上一章。
- en: Open the VisualHallway scene from the Assets | ML-Agents | Examples | Hallway | Scenes
    folder in the Project window.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从项目窗口的 Assets | ML-Agents | Examples | Hallway | Scenes 文件夹中打开 VisualHallway
    场景。
- en: 'Make sure that Agent | Hallway Agent | Brain is set to VisualHallwayPlayer,
    as shown in the following screenshot:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保 Agent | Hallway Agent | Brain 设置为 VisualHallwayPlayer，如下图所示：
- en: '![](img/259082a8-06ae-4583-9189-fa851ca24130.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/259082a8-06ae-4583-9189-fa851ca24130.png)'
- en: Hallway Agent | Brain set to player
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Hallway Agent | Brain 设置为 player
- en: Press Play in the editor to run the scene, and use the *W*, *A*, *S*, and *D*
    keys to control the agent. Remember, the goal is to move to the square that is
    the same color as the center square.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中按下播放按钮运行场景，并使用 *W*、*A*、*S* 和 *D* 键来控制代理。记住，目标是移动到与中心方块颜色相同的方块。
- en: Play the game and move to both color squares to see what happens when a reward
    is given, either negative or positive. The game screen will flash with green or
    red when a reward square is entered.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩游戏并移动到两个颜色方块，观察在进入奖励方块时，正向或负向奖励给予时会发生什么。游戏画面在进入奖励方块时会闪烁绿色或红色。
- en: This game environment is typical of a first person shooter, and perfect for
    training an agent to play in first person as well. Training an agent to play as
    a human would be the goal of many an AI practitioner, and one you may or may not
    strive to incorporate in your game. As we will see, depending on the complexity
    of your game, this type of learning/training may not even be a viable option.
    At this point, we should look at how to set up and train the agent visually.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏环境典型地模拟了第一人称射击游戏，非常适合训练代理以第一人称视角进行游戏。训练代理以类似人类的方式玩游戏是许多 AI 从业者的目标，虽然这可能是你是否会在游戏中实现的功能。正如我们所见，根据你游戏的复杂性，这种学习/训练可能甚至不是一个可行的选项。此时，我们应该了解如何设置并通过视觉训练代理。
- en: Training the agent visually
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观地训练代理
- en: 'Fortunately, setting up the agent to train it visually is quite straightforward,
    especially if you worked through the exercises in the last chapter. Open the Unity
    editor to the VisualHallway scene, have a Python command or Anaconda window ready,
    and let''s begin:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，设置代理进行视觉训练相当简单，特别是如果你已经完成了上一章的练习。打开 Unity 编辑器并加载 VisualHallway 场景，准备好 Python
    命令行或 Anaconda 窗口，我们就可以开始了：
- en: 'In Unity, change Agent | Hallway Agent | Brain to VisualHallwayLearning, as
    shown in the following screenshot:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Unity 中，将 Agent | Hallway Agent | Brain 更改为 VisualHallwayLearning，如下图所示：
- en: '![](img/3f960c28-6e36-45f1-b6c7-aeff1738ac2a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f960c28-6e36-45f1-b6c7-aeff1738ac2a.png)'
- en: Changing that the Brain to learning
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将大脑更改为学习模式
- en: Click on the VisualHallwayLearning brain to locate it in the Project window.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 VisualHallwayLearning 大脑，在项目窗口中定位它。
- en: 'Click on the VisualHallwayLearning brain to view its properties in the Inspector
    window, and as shown in the following screen excerpt:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 VisualHallwayLearning 大脑，在检查器窗口中查看其属性，如下图所示：
- en: '![](img/c8bbaee9-105f-4b1f-8982-64535af4acf8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8bbaee9-105f-4b1f-8982-64535af4acf8.png)'
- en: Confirming the properties are set correctly on the learning brain
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确认学习大脑的属性设置正确
- en: Make sure that the Brain parameters are set to accept a single Visual Observation
    at a resolution of `84` x `84` pixels, and are not using Gray scale. Gray is simply
    the removal of the color channels, which makes the input one channel instead of
    three. Recall our discussion of CNN layers in [Chapter 2](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml), *Convolutional
    and Recurrent Networks*. Also, be sure that the Vector Observation | Space Size
    is 0, as shown in the preceding screenshot.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保大脑参数设置为接受分辨率为 `84` x `84` 像素的单一视觉观察，并且不使用灰度。灰度仅是去除颜色通道，使输入变为一个通道，而非三个通道。回顾我们在[第
    2 章](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml)中讨论的卷积神经网络（CNN）层，*卷积和递归网络*。同时，确保
    Vector Observation | Space Size 设置为 0，如前图所示。
- en: From the Menu, select File | Save and File | Save Project to save all your changes.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在菜单中选择文件 | 保存和文件 | 保存项目，以保存所有更改。
- en: 'Switch to your Python window or Anaconda prompt, make sure you are in the `ML-Agents/ml-agents`
    directory, and run the following command:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到你的 Python 窗口或 Anaconda 提示符，确保你在 `ML-Agents/ml-agents` 目录下，并运行以下命令：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After the command runs, wait for the prompt to start the editor. Then, run the
    editor when prompted and let the sample run to completion, or however long you
    have the patience for.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令执行后，等待提示以启动编辑器。然后，在提示时运行编辑器，并让示例运行完成，或者运行到你有耐心的时长为止。
- en: 'After you run the sample to completion, you should see something like the following:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在示例运行完成后，你应该会看到如下所示的内容：
- en: '![](img/7856918a-91b1-4722-9212-9e1b1f86e07e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7856918a-91b1-4722-9212-9e1b1f86e07e.png)'
- en: Full training run to completion
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的训练运行直到完成
- en: Assuming you trained your agent to the end of the run that is, for 500 K iterations,
    then you can confirm that the agent does, in fact, learn nothing. So, why would
    Unity put an example like that in their samples? Well, you could argue that it
    was an intentional challenge, or perhaps just an oversight on their part. Either
    way, we will take it as a challenge to better understand reinforcement learning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你训练智能体直到运行结束，即训练了 500K 次迭代，那么你可以确认智能体确实什么也没学到。那么，为什么 Unity 会在他们的示例中加入这样的示例呢？嗯，你可以认为这是一个故意设计的挑战，或者只是他们的一次疏忽。无论如何，我们将其视为一个挑战，借此更好地理解强化学习。
- en: Before we tackle this challenge, let's take a step back and reaffirm our understanding
    of this environment by looking at the easier to train Hallway example in the next
    section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们应对这个挑战之前，先回过头来重新确认我们对这个环境的理解，通过查看下一个部分中更易于训练的 Hallway 示例。
- en: Reverting to the basics
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归基础
- en: 'Often, when you get stuck on a problem, it helps to go back to the beginning
    and reaffirm that your understanding of everything works as expected. Now, to
    be fair, we have yet to explore the internals of ML-Agents and really understand
    DRL, so we never actually started at the beginning, but, for the purposes of this
    example, we will take a step back and look at the Hallway example in more detail.
    Jump back into the editor and follow this exercise:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在问题上卡住时，回到最基础的地方确认一切是否按预期工作是很有帮助的。公平地说，我们还没有深入探索 ML-Agents 的内部机制，也没有真正理解深度强化学习（DRL），因此我们实际上并没有从一开始就出发。但为了本示例的目的，我们将回过头来，详细查看
    Hallway 示例。返回编辑器并执行以下操作：
- en: Open the Hallway sample scene in the editor. Remember, the scene is located
    in the Assets | ML-Agents | Examples | Hallway | Scenes folder.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中打开 Hallway 示例场景。记住，场景位于 Assets | ML-Agents | Examples | Hallway | Scenes
    文件夹中。
- en: This example is configured to use several concurrent training environments.
    We are able to train multiple concurrent training environments with the same brain,
    because **Proximal Policy Optimization** (**PPO**), the RL algorithm powering
    this agent, trains to a policy and not a model. We will cover the fundamentals
    of policy and model-based learning when we get to the internals of PPO in [Chapter
    8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding PPO,* for RL. For
    our purposes and for simplicity, we will disable these additional environments
    for now.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个示例配置为使用多个并发训练环境。我们能够使用相同的大脑训练多个并发训练环境，因为 **近端策略优化**（**PPO**），支持这个智能体的强化学习算法，是基于策略进行训练，而不是基于模型。我们将在
    [第8章](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml)《理解 PPO》中详细讲解基于策略和基于模型的学习。为了简化操作，目前我们将暂时禁用这些额外的环境。
- en: Press *Shift* and then select all the numbered HallwayArea (1-15) objects in
    the Hierarchy.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下 *Shift* 键，然后在层级面板中选择所有编号的 HallwayArea（1-15）对象。
- en: 'With all the extra HallwayArea objects selected, disable them all by clicking
    the Active checkbox, as shown in the following screenshot:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选中所有额外的 HallwayArea 对象，点击 "Active" 复选框将其禁用，如下图所示：
- en: '![](img/bc31838a-bed8-4a26-aff8-f888099a6171.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc31838a-bed8-4a26-aff8-f888099a6171.png)'
- en: Disabling all the extra training hallways
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用所有额外的训练走廊
- en: Open the remaining active HallwayArea in the Hierarchy window and select the
    Agent.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开层级窗口中剩下的活动 HallwayArea 并选择 Agent。
- en: Set the Brain agents to use the HallwayLearning brain. It may be set to use
    the player brain by default.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Brain 智能体设置为使用 HallwayLearning brain。默认情况下，它可能设置为使用玩家的大脑。
- en: Select the Academy object back in the Hierarchy window, and make sure the Hallway
    Academy component has its brain set to Learning and that the Control checkbox
    is enabled.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口中重新选择 Academy 对象，确保 Hallway Academy 组件的大脑设置为 Learning，并且启用了 Control 复选框。
- en: 'Open a Python or Anaconda window to the `ML-Agents/ml-agents` folder. Make
    sure your ML-Agents virtual environment is active and run the following command:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Python 或 Anaconda 窗口，进入 `ML-Agents/ml-agents` 文件夹。确保你的 ML-Agents 虚拟环境已激活，并运行以下命令：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let the trainer start up and prompt you to click Play in the editor. Watch the
    agent run and compare its performance to the VisualHallway example.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让训练器启动并提示你点击编辑器中的Play按钮。观察代理运行，并将其表现与VisualHallway示例进行对比。
- en: Generally, you will notice some amount of training activity from the agent before
    50,000 iterations, but this may vary. By training activity, we mean the agent
    is responding with a Mean Reward greater than -1.0 and a Standard Reward not equal
    to zero. Even if you let the example run to completion, that is, 500,000 iterations
    again, it is unlikely that the sample will train to a positive Mean Reward. We
    generally want our rewards to range from -1.0 to +1.0, with some amount of variation
    to show learning activity. If you recall from the VisualHallway example, the agent
    showed no learning activity for the duration of the training. We could have extended
    the training iterations, but it is unlikely we would have seen any stable training
    emerge. The reason for this has to do with the increased state space and handling
    of rewards. We will expand our understanding of state and how it pertains to RL
    in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在50,000次迭代之前，你会注意到代理有一些训练活动，但这可能会有所不同。所谓训练活动，是指代理的平均奖励（Mean Reward）大于-1.0，标准奖励（Standard
    Reward）不等于零。即使你让示例运行完成，即500,000次迭代，它也不太可能训练到正的平均奖励。我们通常希望奖励范围从-1.0到+1.0，并且有一定的变化来展示学习活动。如果你还记得VisualHallway示例，代理在整个训练过程中没有显示任何学习活动。我们本可以延长训练迭代次数，但不太可能看到任何稳定的训练成果。原因在于状态空间的增加和奖励的处理。我们将在下一节扩展对状态的理解，并讨论它与强化学习的关系。
- en: Understanding state
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解状态
- en: The Hallway and VisualHallway examples are essentially the same game problem,
    but provide a different perspective, or what we may refer to in reinforcement
    learning as environment or game state. In the Hallway example, the agent learns
    by sensor input, which is something we will look at shortly, while in the VisualHallway
    example, the agent learns by a camera or player view. What will be helpful at
    this point is to understand how each example handles state, and how we can modify
    it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Hallway和VisualHallway示例本质上是相同的游戏问题，但提供了不同的视角，或者我们在强化学习中所说的环境或游戏状态。在Hallway示例中，代理通过传感器输入进行学习，这一点我们将很快讨论，而在VisualHallway示例中，代理通过摄像头或玩家视角进行学习。此时，理解每个示例如何处理状态，以及我们如何修改状态，将会非常有帮助。
- en: 'In the following exercise, we will modify the Hallway input state and see the
    results:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将修改Hallway输入状态并查看结果：
- en: Jump back into the Hallway scene with learning enabled as we left it at the
    end of the last exercise.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳回到上一个练习结束时启用学习的Hallway场景。
- en: We will need to modify a few lines of C# code, nothing very difficult, but it
    may be useful to install Visual Studio (Community or another version) as this
    will be our preferred editor. You can, of course, use any code editor you like
    as long as it works with Unity.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要修改几行C#代码，没什么难的，但安装Visual Studio（Community版或其他版本）会比较有用，因为这是我们推荐的编辑器。当然，你也可以使用任何你喜欢的代码编辑器，只要它与Unity兼容。
- en: 'Locate the Agent object in the Hierarchy window, and then, in the Inspector
    window, click the Gear icon over the Hallway Agent component, as shown in the
    following screenshot:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口中找到Agent对象，然后在检视窗口中点击Hallway Agent组件上的齿轮图标，如下图所示：
- en: '![](img/62b836fb-b5d0-405a-aaef-059a8205417c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62b836fb-b5d0-405a-aaef-059a8205417c.png)'
- en: Opening the HallwayAgent.cs script
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 打开HallwayAgent.cs脚本
- en: From the context menu, select the Edit Script option, as shown in the previous
    screenshot. This will open the script in your code editor of choice.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从上下文菜单中选择编辑脚本选项，如上图所示。这将会在你选择的代码编辑器中打开脚本。
- en: 'Locate the following section of C# code in your editor:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中找到以下C#代码部分：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `CollectObservations` method is where the agent collects its observations
    or inputs its state. In the Hallway example, the agent has `useVectorObs` set
    to `true`, meaning that it detects state by using the block of code that's internal
    to the `if` statement. All this code does is cast a ray or line from the agent
    in angles of `20f`, `60f`, `120f`, and `160f` degrees at a distance defined by
    `rayDistance` and detect objects defined in `detectableObjects`. The ray perception
    is done with a helper component called `rayPer` of the `RayPerception` type, and
    it executes `rayPer.Percieve` to collect the environment state it perceives. This,
    along with the ratio of steps, is added to the vector observations or state the
    agent will input. At this point, the state is 36 vectors in length. As of this
    version, this needs to be constructed in code, but this will likely change in
    the future.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`CollectObservations` 方法是智能体收集观察或输入状态的地方。在 Hallway 示例中，智能体将 `useVectorObs`
    设置为 `true`，意味着它通过 `if` 语句内部的代码块来检测状态。所有这些代码做的事情就是从智能体发射一束射线，角度分别为 `20f`、`60f`、`120f`
    和 `160f` 度，距离由 `rayDistance` 定义，并检测在 `detectableObjects` 中定义的物体。这些射线感知是通过一个名为
    `rayPer` 的辅助组件完成的，`rayPer` 的类型是 `RayPerception`，并执行 `rayPer.Percieve` 来收集它所感知到的环境状态。这些信息与步骤的比例一起，添加到智能体输入的向量观察或状态中。此时，状态是长度为
    36 的向量。根据这个版本，必须在代码中构造它，但未来可能会有所变化。'
- en: 'Alter the `rayAngles` line of code so that it matches the following:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `rayAngles` 这一行代码，使其与以下内容匹配：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This has the effect of narrowing the agent's vision or perception dramatically
    from 180 to 60 degrees. Another way to think of it is reducing the input state.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样做的效果是显著缩小了智能体的视野或感知范围，从 180 度缩小到 60 度。换句话说，就是减少了输入状态。
- en: After you finish the edit, save the file and return to Unity. Unity will recompile
    the code when you return to the editor.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成编辑后，保存文件并返回 Unity。当你返回编辑器时，Unity 会重新编译代码。
- en: 'Locate the HallwayLearning brain in the Assets | ML-Agents | Examples | Hallway
    | Brains folder and change the Vector Observation | Space Size to `15`, as shown
    in the following screenshot:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Assets | ML-Agents | Examples | Hallway | Brains 文件夹中找到 HallwayLearning 大脑，并将
    Vector Observation | Space Size 修改为 `15`，如以下截图所示：
- en: '![](img/39bc0790-79d7-4128-ac3d-1fedb99babb0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39bc0790-79d7-4128-ac3d-1fedb99babb0.png)'
- en: Setting the Vector Observation Space Size
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 设置向量观察空间大小
- en: The reason we reduce this to 15 is that the input now consists of two angle
    inputs, plus one steps input. Each angle input consists of five detectable objects,
    plus two boundaries for seven total perceptions or inputs. Thus, two angles times
    seven perceptions plus one for steps, equals 15\. Previously, we had five angles
    times seven perceptions plus one step, which equals 35.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将其减少到 15 的原因是：现在的输入由两个角度输入加一个步骤输入组成。每个角度输入包括五个可检测的物体，再加上两个边界，总共七个感知或输入。因此，两个角度乘以七个感知，再加上一个步骤，等于
    15。之前，我们有五个角度乘以七个感知，再加上一个步骤，等于 35。
- en: Make sure that you save the project after modifying the Brain scriptable objects.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在修改 Brain 可编程对象后，请确保保存项目。
- en: Run the example again in training and watch how the agent trains. Take some
    time and pay attention to the actions the agent takes and how it learns. Be sure
    to let this example run as long as you let the other Hallway sample run for, hopefully
    to completion.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例进行训练，并观察智能体如何训练。花些时间关注智能体采取的动作以及它是如何学习的。务必让这个示例运行的时间与其他 Hallway 示例相同，希望能够完整运行。
- en: Were you surprised by the results? Yes, our agent with a smaller view of the
    world actually trained quicker. This may seem completely counter-intuitive, but
    think about this in terms of mathematics. A smaller input space or state means
    the agent has less paths to explore, and so should train quicker. This is indeed
    what we saw in this example when we reduced the input space by more than half.
    At this point, we definitely need to see what happens when we reduce the visual
    state space in the VisualHallway example.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果让你感到惊讶吗？是的，我们的智能体在较小的视野下实际上训练得更快。这个结果可能看起来完全不合常理，但从数学角度考虑，小的输入空间或状态意味着智能体有更少的路径可供探索，因此应该训练得更快。这正是我们在减少输入空间超过一半后在这个示例中看到的情况。在此时，我们肯定需要观察在
    VisualHallway 示例中，减少视觉状态空间会发生什么。
- en: Understanding visual state
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解视觉状态
- en: RL is a very powerful algorithm, but can become very computationally complex
    when we start to look at massive state inputs. To account for massive states,
    many powerful RL algorithms use the concept of model-free or policy-based learning,
    something we will cover in a later chapter. As we already know, Unity uses a policy-based
    algorithm that allows it to learn any size of state space by generalizing to a
    policy. This allows us to easily input a state space of 15 vectors in the example
    we just ran to something more massive, as in the VisualHallway example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种非常强大的算法，但当我们开始处理大量状态输入时，计算复杂性会变得非常高。为了应对庞大的状态，许多强大的强化学习算法使用无模型或基于策略的学习概念，这一点我们将在后面的章节中讨论。如我们所知，Unity
    使用基于策略的算法，允许它通过推广到策略来学习任何大小的状态空间。这使得我们可以轻松地将我们刚才运行的示例中的 15 个向量输入转变为更大规模的状态空间，就像在
    VisualHallway 示例中那样。
- en: 'Let''s open up Unity to the VisualHallway example scene and look at how to
    reduce the visual input space in the following exercise:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开 Unity 到 VisualHallway 示例场景，并看看如何在接下来的练习中减少视觉输入空间：
- en: With the VisualHallway scene open, locate the HallwayLearningBrain in the Assets
    | ML-Agents | Examples | Hallway | Brains folder and select it.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打开 VisualHallway 场景的同时，找到位于 Assets | ML-Agents | Examples | Hallway | Brains
    文件夹中的 HallwayLearningBrain 并选择它。
- en: 'Modify the Brain Parameters **|** Visual Observation first camera observable
    to an input of `32` x `32` Gray scale. An example of this is shown in the following
    screenshot:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Brain 参数 **|** 视觉观察的第一个相机可观察输入修改为 `32` x `32` 灰度。如下截图所示：
- en: '![](img/6655a1f7-7595-454c-863c-60b2c4107101.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6655a1f7-7595-454c-863c-60b2c4107101.png)'
- en: Setting up the visual observation space for the agent
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 设置代理的视觉观察空间
- en: When Visual Observations are set on a brain, then every frame is captured from
    the camera at the resolution selected. Previously, the captured image was 84 x
    84 pixels large, by no means as large as the game screen in player mode, but still
    significantly larger than 35 vector inputs. By reducing our image size and making
    it gray, scale we reduced one input frame from 84 x 84 x 3 = 20,172 inputs to
    32 x 32 x 1 =1,024\. In turn, this greatly reduces our required model input space
    and the complexity of the network that's needed to learn.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当视觉观察设置在大脑上时，每一帧都会以所选的分辨率从相机中捕捉。之前，捕捉的图像大小为 84 x 84 像素，虽然远不如玩家模式下的游戏屏幕那么大，但仍然明显大于
    35 个向量输入。通过减小图像大小并将其转换为灰度，我们将输入框架从 84 x 84 x 3 = 20,172 个输入，减少到 32 x 32 x 1 =
    1,024 个输入。反过来，这大大减少了所需的模型输入空间以及学习所需的网络复杂度。
- en: Save the project and the scene.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存项目和场景。
- en: 'Run the VisualHallway in learning mode again using the following command:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令再次以学习模式运行 VisualHallway：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice how we are changing the `--run-id` parameter with every run. Recall that,
    if we want to use TensorBoard, then each of our runs needs a unique name, otherwise
    it just writes over previous runs.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们在每次运行时都在更改 `--run-id` 参数。回想一下，如果我们要使用 TensorBoard，那么每次运行都需要一个唯一的名称，否则它会覆盖之前的运行。
- en: Let the sample train for as long as you ran the earlier VisualHallway exercise,
    as this will give you a good comparison of the change we made in state.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让示例训练的时间与之前运行 VisualHallway 练习的时间一样，因为这样你可以很好的比较我们在状态上所做的变化。
- en: Are the results what you expected? Yeah, the agent still doesn't learn, even
    after reducing the state. The reason for this is because the smaller visual state
    actually works against the agent in this particular case. Not unlike the results,
    we would expect us humans to have when trying to solve a task by looking through
    a pinhole. However, there is another way to reduce visual state into feature sets
    using convolution. As you may recall, we covered convolution and CNN in [Chapter
    2](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml), *Convolutional and Recurrent Networks*,
    at some length. In the next section, we will look at how we can reduce the visual
    state of our example by adding convolutional layers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是否如你所预期？是的，代理仍然没有学会，即使在减少了状态后。原因在于，较小的视觉状态实际上在这种情况下对代理是有害的。就像我们人类在尝试通过针孔看事物时的效果一样。然而，还有另一种方法可以通过卷积将视觉状态减少为特征集。正如你可能记得的，我们在[第
    2 章](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml)《卷积神经网络和循环神经网络》中详细讨论了卷积。在接下来的章节中，我们将研究如何通过添加卷积层来减少示例的视觉状态。
- en: Convolution and visual state
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积与视觉状态
- en: 'The visual state an agent uses in the ML-Agents toolkit is defined by a process
    that takes a screenshot at a specific resolution and then feeds that into a convolutional
    network to train some form of embedded state. In the following exercise, we will
    open up the ML-Agents training code and enhance the convolution code for better
    input state:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML-Agents工具包中，代理使用的视觉状态是通过一个过程定义的，这个过程在特定的分辨率下截取截图，然后将截图输入到卷积网络中，以训练某种形式的嵌入状态。在接下来的练习中，我们将打开ML-Agents的训练代码，并增强卷积代码以获得更好的输入状态：
- en: Use a file browser to open the ML-Agents `trainers` folder located at `ml-agents.6\ml-agents\mlagents\trainers`**. **Inside
    this folder, you will find several Python files that are used to train the agents.
    The file we are interested in is called `models.py`.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文件浏览器打开ML-Agents `trainers`文件夹，路径为`ml-agents.6\ml-agents\mlagents\trainers`**。**在这个文件夹中，你会找到几个用于训练代理的Python文件。我们感兴趣的文件叫做`models.py`。
- en: Open the `models.py` file in your Python editor of choice. Visual Studio with
    the Python data extensions is an excellent platform, and also provides the ability
    to interactively debug code.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你选择的Python编辑器中打开`models.py`文件。Visual Studio与Python数据扩展是一个非常好的平台，并且还提供了交互式调试代码的功能。
- en: 'Scroll down in the file to locate the `create_visual_observation_encoder` function,
    which looks as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动文件，找到`create_visual_observation_encoder`函数，其内容如下：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The code is Python using TensorFlow, but you should be able to identify the `conv1`
    and `conv2` convolution layers. Notice how the kernel and stride is defined for
    layers and the missing pooling layers as well. Unity does not use pooling in order
    to avoid loss of spatial relationships in data. However, as we discussed earlier,
    this is not always so cut-and-dry, and really varies by the type of visual features
    you are trying to identify.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码使用的是Python和TensorFlow，但你应该能够识别`conv1`和`conv2`卷积层。注意层的卷积核和步幅是如何定义的，以及缺少的池化层。Unity没有使用池化，以避免丢失数据中的空间关系。然而，正如我们之前讨论的，这并非总是那么简单，实际上，这取决于你要识别的视觉特征类型。
- en: 'Add the following lines of code after the two convolution layers and modify
    the `hidden` layer setup, as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个卷积层后添加以下代码行，并修改`hidden`层的设置，如下所示：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will have the effect of adding another layer of convolution to extract
    finer details in the agents game view. As we saw in [Chapter 2](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml),
    *Convolutional and Recurrent Networks*, adding extra layers of convolution will
    increase training time, but does increase training performance – at least on image
    classifiers, anyway.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生在代理的游戏视图中添加另一个卷积层的效果，以提取更细节的内容。正如我们在[第二章](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml)中看到的，*卷积神经网络与递归网络*，增加额外的卷积层会增加训练时间，但确实会提高训练表现——至少在图像分类器上是这样。
- en: 'Jump back to your command or Anaconda window and run the sample in learning
    mode with the following command:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳回你的命令行或Anaconda窗口，并使用以下命令以学习模式运行示例：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Observe the training and watch how the agent performs—be sure to watch the agent's
    movements in the Game window as the sample runs. Is the agent doing what you expected?
    Compare your results with the previous runs and notice the differences.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察训练过程并查看代理的表现——在示例运行时，一定要在游戏窗口中观察代理的动作。代理是否按你预期的方式执行？将结果与之前的运行进行比较，并注意其中的差异。
- en: One thing you will certainly notice is the agent becoming slightly more graceful
    and being able to perform finer movements. While the training may take much longer
    overall, this agent will be able to observe finer changes in the environment,
    and so will make finer movements. You could, of course, swap the entire CNN architecture
    of ML-Agents to use more well-defined architectures. However, be aware that most
    image classification networks ignore spatial relevance that, as we will see in
    the next section, is very relevant to game agents.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定会注意到，代理变得稍微更优雅，能够执行更精细的动作。虽然训练过程可能需要更长的时间，但这个代理能够观察到环境中的细微变化，因此会做出更精细的动作。当然，你也可以将整个CNN架构替换为使用更明确的架构。然而，需要注意的是，大多数图像分类网络忽略空间相关性，而正如我们在下一节中看到的，空间相关性对于游戏代理非常重要。
- en: To pool or not to pool
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是否使用池化
- en: 'As we discussed in [Chapter 2](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml),
    *Convolutional and Recurrent Networks*, ML-Agents does not use any pooling in
    order to avoid any loss of spatial relationships in data. However, as we saw in
    our self-driving vehicle example, a single pooling layer or two up at the higher
    feature level extraction (convolutional layers) can in fact help. Although our
    example was tested on a much more complex network, it will be helpful to see how
    this applies to a more complex ML-Agents CNN embedding. Let''s try this out, and
    apply a layer of pooling to the last example by completing the following exercise:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第二章](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml)中讨论的，*卷积和递归网络*，ML-Agents不使用任何池化操作，以避免数据中的空间关系丢失。然而，正如我们在自动驾驶车辆的例子中看到的那样，实际上在更高特征级别的提取（卷积层）上，加入一个或两个池化层是有帮助的。虽然我们的例子在一个更复杂的网络上进行了测试，但它有助于了解这对更复杂的ML-Agents
    CNN嵌入的应用。让我们尝试一下，通过完成以下练习，在上一个例子中添加一个池化层：
- en: Open the `models.py` file in your Python editor of choice. Visual Studio with
    the Python data extensions is an excellent platform, and also provides the ability
    to interactively debug code.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你选择的Python编辑器中的`models.py`文件。Visual Studio 配合Python数据扩展是一个很好的平台，同时也提供了交互式调试代码的功能。
- en: 'Locate the following block of code, which is as we last left it in the previous
    exercise:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到以下代码块，这是我们在上一个练习中留下的样子：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will now inject a layer of pooling by modifying the block of code, like
    so:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过修改代码块来注入一个池化层，代码如下：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This now sets up our previous sample to use a single layer of pooling. You can
    think of this as extracting all the upper features, such as the sky, wall, or
    floor, and pooling the results together. When you think about it, how much spatial
    information does the agent need to know regarding one sky patch versus another?
    All the agent really needs to know is that the sky is always up.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，这将设置我们的前一个示例，使用单层池化。你可以将其视为提取所有上层特征，比如天空、墙壁或地板，并将结果池化在一起。仔细想想，代理需要知道多少空间信息才能区分一个天空区域和另一个天空区域？代理实际上只需要知道天空总是在上面。
- en: 'Open your command shell or Anaconda window and train the sample by running
    the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的命令行窗口或Anaconda窗口，通过运行以下代码来训练示例：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As always, watch the performance of the agent and notice how the agent moves
    as it trains. Watch the training until completion, or as much as you observed
    others.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，观察代理的表现，注意代理在训练过程中是如何移动的。观察训练直到完成，或者观察你之前看到的其他人的训练过程。
- en: Now, depending on your machine or environment you may have noticed a substantial
    improvement in training time, but actual performance suffered slightly. This means
    that each training iteration executed much quicker, two to three times or more,
    but the agent needs slightly more interactions. In this case, the agent will train
    quicker time-wise, but in other environments, pooling at higher levels maybe more detrimental.
    When it comes down to it, it will depend on the visuals of your environment, how
    well you want your agent to perform, and, ultimately, your patience.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据你的机器或环境，你可能已经注意到训练时间有了显著的改进，但实际表现略有下降。这意味着每次训练迭代执行得更快了，可能快了两到三倍甚至更多，但代理需要更多的交互。在这种情况下，代理训练的时间会更短，但在其他环境中，高级别的池化可能会更具破坏性。最终，这取决于你环境中的视觉效果、你希望代理表现得有多好，以及你个人的耐心。
- en: In the next section, we will look at another characteristic of state – memory,
    or sequencing. We will look at how recurrent networks are used to capture the
    importance of remembering sequences or event series.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨状态的另一个特征——记忆，或者序列。我们将了解如何使用递归网络来捕捉记住序列或事件系列的重要性。
- en: Recurrent networks for remembering series
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆序列的递归网络
- en: 'The sample environments we have been running in this chapter use a form of
    recurrent memory by default to remember past sequences of events. This recurrent
    memory is constructed of **Long Short-Term Memory** (**LSTM**) layers that allow
    the agent to remember beneficial sequences that may encourage some amount of future
    reward. Remember that we extensively covered LSTM networks in [Chapter 2](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml), *Convolutional
    and Recurrent Networks*. For example, an agent may see the same sequence of frames
    repeatedly, perhaps moving toward the target goal, and then associate that sequence
    of states with an increased reward. A diagram showing the original form of this
    network, taken from the paper *Training an Agent for FPS Doom Game using Visual
    Reinforcement Learning and VizDoom* by *Khan Aduil et a**l.,* is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们运行的示例环境默认使用一种递归记忆形式来记住过去的事件序列。这种递归记忆由**长短期记忆**（**LSTM**）层构成，允许代理记住可能有助于未来奖励的有益序列。请记住，我们在[第二章](391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml)中深入讲解了LSTM网络，*卷积与递归网络*。例如，代理可能反复看到相同的帧序列，可能是朝着目标移动，然后将这一状态序列与增加的奖励关联起来。以下是摘自*Khan
    Aduil等人*的论文*Training an Agent for FPS Doom Game using Visual Reinforcement Learning
    and VizDoom*中的图示，展示了这种网络的原始形式：
- en: '![](img/42b80820-40ef-424c-8c27-8651f58f377d.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42b80820-40ef-424c-8c27-8651f58f377d.png)'
- en: DQRN Architecture
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DQRN架构
- en: The authors referred to the network architecture as DQRN, which stands for deep
    Q recurrent network. It is perhaps strange they did not call it DQCRN, since the
    diagram clearly shows the addition of convolution. While the ML-Agents implementation
    is slightly different, the concept is very much the same. Either way, the addition
    of LSTM layers can be a huge benefit to agent training, but, at this stage, we
    have yet to see the affect of not being used in training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将该网络架构称为DQRN，代表深度Q递归网络。可能有点奇怪的是，他们没有称之为DQCRN，因为图示清楚地显示了卷积的加入。虽然ML-Agents的实现略有不同，但概念基本相同。无论如何，添加LSTM层对代理训练有很大帮助，但在这一阶段，我们还没有看到不使用LSTM的训练效果。
- en: 'Therefore, in the following exercise, we will learn how to disable recurrent
    networks and see what effect this has on training:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在接下来的练习中，我们将学习如何禁用递归网络，并查看这对训练的影响：
- en: Open the standard Hallway example scene, the one without visual learning, from
    the `Assets/ML-Agents/Examples/Hallway/Scenes` folder.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开标准的走廊示例场景，即没有视觉学习的那个，位置在`Assets/ML-Agents/Examples/Hallway/Scenes`文件夹中。
- en: Open a command shell or Anaconda window and make sure your ML-Agent's virtual
    Python environment is active.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令行窗口或Anaconda窗口，并确保你的ML-Agent虚拟Python环境已激活。
- en: Locate and open the `trainer_config.xml` file located in the `ML-Agents/ml-agents/config`
    folder in a text or XML editor of your choice.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到并打开位于`ML-Agents/ml-agents/config`文件夹中的`trainer_config.xml`文件，使用你喜欢的文本或XML编辑器。
- en: 'Locate the configuration block, as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到如下所示的配置块：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The named configuration block, called `HallwayLearning`, matches the name of
    the brain we set up in the Academy within the scene. If you need to confirm this,
    go ahead.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 名为`HallwayLearning`的配置块与我们在场景中的Academy中设置的大脑名称相匹配。如果你需要确认这一点，可以继续检查。
- en: We generally refer to all these configuration parameters as hyperparameters,
    and they can have a considerable effect on training, especially if set incorrectly.
    If you scroll to the top of the file, you will notice a set of default parameters,
    followed by exceptions for each of the named brains. Each section of brain parameters
    for each brain override the default settings.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通常将所有这些配置参数称为超参数，它们对训练有很大的影响，尤其是在设置不正确时。如果你滚动到文件的顶部，你会注意到一组默认参数，接着是每个命名大脑的例外设置。每个大脑的参数部分将覆盖默认设置。
- en: 'Disable the `use_recurrent` networks by modifying the code, as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过如下修改代码来禁用`use_recurrent`网络：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Setting `use_recurrent` to `false` disables the use of recurrent encoding. We
    can now see what effect this has on training.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`use_recurrent`设置为`false`可以禁用递归编码的使用。现在我们可以看到这对训练的影响。
- en: Save the configuration file.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存配置文件。
- en: Run the sample on learning as you normally would. You should be able to run
    a training sample in your sleep by now.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照平常的方式运行学习示例。现在你应该已经能够轻松地运行一个训练示例了。
- en: As always, watch how the agent performs and be sure to pay attention to the
    agent's movements as well.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，观察代理的表现，并确保关注代理的动作。
- en: As you can see, the agent performs considerably worse in this example, and it
    is obvious that the use of recurrent networks to capture sequences of important
    moves made a big difference. In fact, in most repetitive game environments, such
    as the Hallway and VisualHallway, the addition of recurrent state works quite
    well. However, there will be other environments that may not benefit, or may indeed
    suffer, from the use of state sequencing. Environments that feature extensive
    exploration or new content may, in fact, suffer. Since the agent may prefer shorter
    action sequences, this is limited by the amount of memory that is configured for
    the agent. Try to keep that in mind when you develop a new environment.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这个示例中，代理的表现显著较差，显然使用循环网络来捕捉重要动作序列起到了很大的作用。事实上，在大多数重复性游戏环境中，比如Hallway和VisualHallway，增加循环状态非常有效。然而，也有一些环境可能不会受益，或者实际上会因使用状态序列而受到影响。那些需要广泛探索或包含新内容的环境，可能会受到影响。由于代理可能更倾向于使用较短的动作序列，这会受到为代理配置的内存量的限制。在开发新环境时，记得考虑这一点。
- en: Now that we have a comparison for how our samples run without recurrent or LSTM
    layers, we can test the sample again by tweaking some of the relevant recurrent
    hyperparameters in the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个没有循环或LSTM层时样本运行的比较，我们可以通过在下一节调整一些相关的循环超参数，重新测试样本。
- en: Tuning recurrent hyperparameters
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整循环超参数
- en: 'As we learned in our discussion of recurrent networks, LSTM layers may receive
    variable input, but we still need to define a maximum sequence length that we
    want the network to remember. There are two critical hyperparameters we need to
    play with when using recurrent networks. A description of these parameters, at
    the time of writing, and as listed in the ML-Agents docs, is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在讨论循环网络时了解到的，LSTM层可能接收可变输入，但我们仍然需要定义希望网络记住的最大序列长度。使用循环网络时，我们需要调整两个关键的超参数。以下是这些参数的描述（截至本文撰写时，按ML-Agents文档中的列表）：
- en: '`sequence_length`: *C*orresponds to the length of the sequences of experience
    that are passed through the network during training. This should be long enough
    to capture whatever information your agent might need to remember over time. For
    example, if your agent needs to remember the velocity of objects, then this can
    be a small value. If your agent needs to remember a piece of information that''s
    given only once at the beginning of an episode, then this should be a larger value:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_length`：*C*对应于在训练过程中传递通过网络的经验序列的长度。这个长度应该足够长，以捕捉代理可能需要记住的任何信息。例如，如果你的代理需要记住物体的速度，那么这个值可以是一个小数值。如果你的代理需要记住一条在回合开始时只给定一次的信息，那么这个值应该更大：'
- en: 'Typical Range: 4 – 128'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型范围：4 – 128
- en: '`memory_size`: Corresponds to the size of the array of floating point numbers
    that are used to store the hidden state of the recurrent neural network. This
    value must be a multiple of four, and should scale with the amount of information
    you expect the agent will need to remember to successfully complete the task:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory_size`：对应于用于存储循环神经网络隐藏状态的浮点数数组的大小。该值必须是四的倍数，并且应根据你预期代理需要记住的任务信息量进行缩放：'
- en: 'Typical Range: 64 – 512'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型范围：64 – 512
- en: The description of the recurrent `sequence_length` and `memory_size` hyperparameters
    was extracted directly from the Unity ML-Agents documentation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 循环的`sequence_length`和`memory_size`超参数的描述直接来自Unity ML-Agents文档。
- en: 'If we look at our VisualHallway example configuration in the `trainer_config.yaml`
    file, we can see that the parameters are defined as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`trainer_config.yaml`文件中的VisualHallway示例配置，可以看到这些参数定义如下：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This effectively means that our agent will remember 64 frames or states of
    input using a memory size of 256\. The documentation is unclear as to how much
    memory a single input takes, so we can only assume that the default visual convolutional
    encoding network, the original two layer model, requires four per frame. We can
    assume that, by increasing our convolutional encoding in the previous examples,
    the agent may have not been able to remember every frame of state. Therefore,
    let''s modify the configuration in the VisualHallway example to account for that
    increase in memory, and see the effect it has in the following exercise:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上意味着我们的代理将使用256的内存大小记住64帧或输入状态。文档并未明确说明单个输入占用多少内存，因此我们只能假设默认的视觉卷积编码网络——原始的两层模型——每帧需要四个内存单元。我们可以假设，通过增加我们之前示例中的卷积编码，代理可能无法记住每一帧状态。因此，让我们修改
    VisualHallway 示例中的配置，以适应这种内存增加，并查看它在以下练习中的效果：
- en: Open up the VisualHallway example to where we last left it in the previous exercises,
    with or without pooling enabled. Just be sure to remember if you are or are not
    using pooling, as this will make a difference to the required memory.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 VisualHallway 示例，回到我们上次在之前的练习中离开的地方，无论是否启用池化。只要记住你是否启用了池化，因为这将影响所需的内存。
- en: Open the `trainer_config.yaml` file located in the `ML-Agents/ml-agents/config`
    folder.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `ML-Agents/ml-agents/config` 文件夹中的 `trainer_config.yaml` 文件。
- en: 'Modify the `VisualHallwayLearning` config section, as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `VisualHallwayLearning` 配置部分，如下所示：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are increasing the agent's memory from 64 to 128 sequences, thus doubling
    its memory. Then, we are increasing the memory to 2,048 when not using pooling,
    and 1,024 when using pooling. Remember that pooling collects features and reduces
    the number of feature maps that are produced at every step of convolution.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将代理的记忆从64个序列增加到128个序列，从而使其记忆翻倍。接着，当不使用池化时，我们将记忆增加到2,048，而使用池化时为1,024。记住，池化会收集特征并减少每次卷积步骤中生成的特征图数量。
- en: Save the file after you finish editing it.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑完成后，保存文件。
- en: 'Open your command or Anaconda window and start training with the following
    command:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的命令行或 Anaconda 窗口，使用以下命令开始训练：
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When prompted, start the training session in the editor by pressing Play and
    watch the action unfold.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示时，通过按下播放按钮开始编辑器中的训练会话，并观看操作的展开。
- en: Wait for the agent to train, like you did for the other examples we ran. You
    should notice another increase in training performance, as well as the choice
    of actions the agent makes, which should look better coordinated.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待代理训练完成，像我们之前运行的其他示例一样。你应该能注意到训练性能的再次提高，以及代理选择的动作，应该显得更加协调。
- en: As we can see, a slight tweaking of hyperparameters allowed us to improve the
    performance of the agent. Understanding the use of the many parameters that are
    used in training will be critical to your success in building remarkable agents.
    In the next section, we will look at further exercises you can use to improve
    your understanding and skill.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，轻微调整超参数使得我们能够改善代理的性能。理解在训练中使用的众多参数的作用，对于你成功构建出色的代理至关重要。在下一部分，我们将介绍一些进一步的练习，帮助你提高理解和技能。
- en: Exercises
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: As always, try and complete a minimum of two to three of these exercises on
    your own, and for your own benefit. While this is a hands-on book, it always helps
    to spend a little more time applying your knowledge to new problems.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，尽量独立完成两到三个练习，为了你自己的利益。虽然这是一本实践书籍，但花一些额外时间将你的知识应用于新问题总是有益的。
- en: 'Complete the following exercises on your own:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 独立完成以下练习：
- en: Go through and explore the VisualPushBlock example. This example is quite similar
    to the Hallway, and is a good analog to play with.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览并探索 VisualPushBlock 示例。这个示例与走廊示例非常相似，是一个不错的类比，可以进行尝试。
- en: Modify the Hallway example's HallwayAgent script to use more scanning angles,
    and thus more vector observations.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改走廊示例中的 HallwayAgent 脚本，以使用更多的扫描角度，从而获得更多的向量观察。
- en: Modify the Hallway example to use a combined sensor sweep and visual observation
    input. This will require you to modify the learning brain configuration by adding
    a camera, and possibly updating some hyperparameters.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改走廊示例，使用组合的传感器扫描和视觉观察输入。这将要求你通过添加相机来修改学习大脑配置，并可能需要更新一些超参数。
- en: Modify other visual observation environments to use some form of vector observation.
    A good example to try this on is the VisualPushBlock example.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改其他视觉观测环境，以使用某种形式的向量观测。一个不错的例子是尝试在VisualPushBlock示例中应用此功能。
- en: Modify the visual observation camera space to be larger or smaller than 84 x
    84 pixels, and to use, or not use, gray scaling. This is a good exercise to play
    with when testing more complex or simpler CNN network architectures.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改视觉观测摄像头的空间，使其比84 x 84像素更大或更小，并选择是否使用灰度化。这是测试更复杂或更简单的CNN网络架构时一个很好的练习。
- en: Modify the `create_visual_observation_encoder` convolutional encoding function
    so that it can use different CNN architectures. These architectures may be as
    simple or complex as you want.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`create_visual_observation_encoder`卷积编码函数，使其能够使用不同的CNN架构。这些架构可以根据你的需求是简单还是复杂。
- en: Modify the `create_visual_observation_encoder` convolutional encoding function to
    use different levels and amounts of pooling layers. Try and use pooling after
    every convolutional layer to explore its effect on training.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`create_visual_observation_encoder`卷积编码函数，以使用不同级别和数量的池化层。尝试在每个卷积层后使用池化，探索其对训练的影响。
- en: Disable and enable recurrent networks on one or two of the other example's environments
    and explore the effect this has.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在其他一些示例环境中禁用并重新启用递归网络，探索其对结果的影响。
- en: Play with the `sequence_length` and `memory_size` parameters with recurrent
    enabled to see the effect that different sequence lengths have on agent performance.
    Be sure to increase the `memory_size` parameter if you increase the `sequence_length`.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在启用递归网络的情况下，调整`sequence_length`和`memory_size`参数，观察不同序列长度对智能体表现的影响。如果增加`sequence_length`，请务必相应地增加`memory_size`参数。
- en: Consider adding additional vector or visual observations to the agent. After
    all, an agent doesn't have to have only a single form of sensory input. An agent
    could always detect the direction it is in, or perhaps it may have other forms
    of sensory input, such as being able to listen. We will give an agent the ability
    to listen in a later chapter, but try and implement this yourself.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑为智能体添加额外的向量或视觉观测。毕竟，智能体不一定只能有单一形式的感官输入。智能体可以始终检测其所处的方向，或者可能有其他感官输入方式，例如能够听到声音。我们将在后续章节中为智能体提供听觉能力，但也可以尝试自己实现这一功能。
- en: Remember, these exercises are provided for your benefit and enjoyment, so be
    sure to try at least a couple.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这些练习是为你的利益和享受而提供的，因此确保至少尝试几个。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a very close look at how the agents in ML-Agents perceive
    their environment and process input. An agent's perception of the environment
    is completely in control by the developer, and it is often a fine balance of how
    much or how little input/state you want to give an agent. We played with many
    examples in this chapter and started by taking an in-depth look at the Hallway
    sample and how an agent uses rays to perceive objects in the environment. Then,
    we looked at how an agent can use visual observations, not unlike us humans, as
    input or state that it may learn from. From this, we delved into the CNN architecture
    that ML-Agents uses to encode the visual observations it provides to the agent.
    We then learned how to modify this architecture by adding or removing convolution
    or pooling layers. Finally, we looked at the role of memory, or how recurrent
    sequencing of input state can be used to help with agent training. Recurrent networks
    allow an agent to add more value to action sequences that provide a reward.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们详细探讨了ML-Agents中的智能体如何感知环境并处理输入。智能体对环境的感知完全由开发者控制，这通常是关于给予智能体多少或多少输入/状态的微妙平衡。在本章中，我们进行了许多示例，并从深入分析Hallway示例及智能体如何使用射线感知环境中的物体开始。接着，我们研究了智能体如何使用视觉观测作为输入或状态，类似于我们人类，从中学习。随后，我们探讨了ML-Agents使用的卷积神经网络（CNN）架构，该架构用于编码提供给智能体的视觉观测。我们学习了如何通过添加或删除卷积层或池化层来修改这一架构。最后，我们研究了记忆的作用，或者说如何通过输入状态的递归序列化来帮助智能体训练。递归网络使得智能体能够为提供奖励的动作序列增加更多的价值。
- en: In the next chapter, we will take a closer look at RL and how agents use the
    PPO algorithm. We will learn more about the foundations of RL along the way, as
    well as learn about the importance of the many hyperparameters used in training.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更详细地探讨强化学习（RL）以及智能体如何使用PPO算法。我们将在过程中深入学习RL的基础知识，并了解在训练中使用的许多超参数的重要性。
