- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Adversarial Robustness
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗鲁棒性
- en: '**Adversarial attacks** on LLMs are designed to manipulate the model’s output
    by making small, often imperceptible changes to the input. These attacks can expose
    vulnerabilities in LLMs and potentially lead to security risks or unintended behaviors
    in real-world applications.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs上的**对抗攻击**旨在通过在输入中做出微小、通常难以察觉的改变来操纵模型的输出。这些攻击可以暴露LLMs中的漏洞，并可能导致现实应用中的安全风险或意外行为。
- en: In this chapter, we’ll discover techniques for creating and defending against
    **adversarial examples** in LLMs. Adversarial examples are carefully crafted inputs
    designed to intentionally mislead the model into producing incorrect or unexpected
    outputs. You’ll learn about textual adversarial attacks, methods to generate these
    examples, and techniques to make your models more robust. We’ll also cover evaluation
    methods and discuss the real-world implications of adversarial attacks on LLMs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在LLMs中创建和防御**对抗样本**的技术。对抗样本是精心设计的输入，旨在故意误导模型产生错误或意外的输出。您将了解文本对抗攻击、生成这些示例的方法以及使您的模型更具鲁棒性的技术。我们还将介绍评估方法，并讨论对抗攻击对LLMs在现实世界中的影响。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Types of textual adversarial attacks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本对抗攻击的类型
- en: Adversarial training techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗训练技术
- en: Evaluating robustness
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估鲁棒性
- en: Trade-offs in the adversarial training of LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs对抗训练中的权衡
- en: Real-world implications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界的影响
- en: Types of textual adversarial attacks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本对抗攻击的类型
- en: Textual adversarial attacks involve subtly modifying text inputs to mislead
    models into making incorrect predictions, often with changes imperceptible to
    humans.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 文本对抗攻击涉及微妙地修改文本输入以误导模型做出错误预测，通常这些变化对人类来说是不可察觉的。
- en: 'There are three levels of attacks to keep in mind:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个级别的攻击需要考虑：
- en: '**Character-level attacks**: Modifying individual characters in words'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符级攻击**：修改单词中的单个字符'
- en: '**Word-level attacks**: Replacing words with synonyms or similar words'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词级攻击**：用同义词或类似词替换单词'
- en: '**Sentence-level attacks**: Modifying sentence structure while preserving meaning'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子级攻击**：修改句子结构同时保留意义'
- en: In addition, a fourth type of attack to consider involves **universal adversarial
    triggers**, which are specific phrases that, when added, cause the model to behave
    incorrectly across various inputs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种需要考虑的第四种攻击类型，涉及**通用对抗触发器**，这些是特定的短语，当添加时，会导致模型在各种输入中表现不正确。
- en: 'Here are some illustrative examples of universal adversarial triggers:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些通用的对抗触发器的示例：
- en: '**For sentiment analysis**: Imagine a sentiment analysis model trained to classify
    movie reviews as “positive” or “negative.” A universal adversarial trigger might
    be a seemingly nonsensical phrase such as “invisible carpet notebook”. If this
    trigger is appended to various positive reviews, such as “This movie was fantastic,”
    resulting in “This movie was fantastic invisible carpet notebook,” the model might
    incorrectly classify them as “negative.” Similarly, adding the same trigger to
    other positive reviews such as “I really enjoyed the acting” could also flip its
    prediction to “negative.”'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于情感分析**：想象一个训练有素的情感分析模型，用于将电影评论分类为“正面”或“负面”。一个通用的对抗触发器可能是一个看似无意义的短语，例如“无形地毯笔记本”。如果将此触发器添加到各种正面评论中，例如“这部电影太棒了”，得到“这部电影太棒了无形地毯笔记本”，模型可能会错误地将它们分类为“负面”。同样，将相同的触发器添加到其他正面评论，例如“我真的很喜欢表演”，也可能将其预测翻转到“负面”。'
- en: '**For text classification (e.g., topic identification)**: Consider a model
    classifying news articles into categories such as “Sports,” “Politics,” or “Technology.”
    A universal adversarial trigger could be “the the grand elephant”. Appending this
    trigger to a “Sports” article such as “The team won the championship” to get “The
    team won the championship the the grand elephant” might cause the model to misclassify
    it as “Politics.” Likewise, adding it to a “Technology” article about a new gadget
    could also lead to an incorrect classification.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于文本分类（例如，主题识别）**：考虑一个将新闻文章分类为“体育”、“政治”或“科技”等类别的模型。一个通用的对抗触发器可能是“the the
    grand elephant”。将此触发器添加到一篇“体育”文章，例如“该队赢得了冠军”，得到“该队赢得了冠军 the the grand elephant”，可能会使模型将其误分类为“政治”。同样，将其添加到一篇关于新小工具的“科技”文章也可能导致错误的分类。'
- en: '**For natural language inference (NLI)**: In NLI, a model determines the relationship
    between two sentences (e.g., *entailment*, *contradiction*, *neutral*). A trigger
    such as “always dance quickly” might be designed to consistently force an “entailment”
    prediction. For example, given the premise “A person is walking,” and the hypothesis
    “A person is moving,” the model would correctly predict “entailment.” However,
    if the trigger is added to the premise, “A person is walking always dance quickly,”
    the model might be tricked into still predicting “entailment” even if the relationship
    becomes less clear or even nonsensical.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于自然语言推理（NLI）**：在NLI中，模型确定两个句子之间的关系（例如，*蕴涵*、*矛盾*、*中立*）。一个触发器，如“总是快速跳舞”，可能被设计成始终强制进行“蕴涵”预测。例如，给定前提“一个人在走路”，和假设“一个人在移动”，模型会正确预测“蕴涵”。然而，如果触发器被添加到前提中，“一个人在走路总是快速跳舞”，模型可能会被欺骗仍然预测“蕴涵”，即使关系变得不那么清晰甚至荒谬。'
- en: 'For simplicity, in this section, we will discuss two types of attacks. First,
    let’s implement a simple character-level attack:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，在本节中，我们将讨论两种类型的攻击。首先，让我们实现一个简单的字符级攻击：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code defines a `character_level_attack` function that aims to create a
    slightly altered version of an input text by randomly modifying individual characters.
    For each character in the input text, there is a probability (set by the `prob`
    parameter, defaulting to `0.1`) that it will be changed. If a character is selected
    for modification and it is an alphabetic character, it will be replaced by a random
    lowercase or uppercase letter. Non-alphabetic characters (such as spaces and punctuation)
    are left unchanged. The function then joins the potentially modified characters
    back into a string, producing the “attacked” text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个`character_level_attack`函数，旨在通过随机修改单个字符来创建输入文本的略微修改版本。对于输入文本中的每个字符，都有一个概率（由`prob`参数设置，默认为`0.1`），它将被更改。如果一个字符被选中进行修改，并且它是一个字母字符，它将被替换为一个随机的小写或大写字母。非字母字符（如空格和标点符号）保持不变。然后函数将可能被修改的字符重新组合成一个字符串，生成“攻击”文本。
- en: 'The output of this code will display two lines. The first line, labeled `"Original:"`,
    will show the initial input text: `"The quick brown fox jumps over the lazy dog."`.
    The second line, labeled `"Attacked:"`, will present the modified text. Due to
    the random nature of the character replacement based on the `prob` value, the
    `"Attacked:"` text will likely have some of its alphabetic characters replaced
    by other random letters. For example, “The” might become “Tge”, “quick” could
    be “quicj”, and so on. The number and specific locations of these changes will
    vary each time the code is executed because of the random selection process.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出将显示两行。第一行，标记为“原始：”，将显示初始输入文本：“The quick brown fox jumps over the lazy
    dog。”。第二行，标记为“攻击：”，将展示修改后的文本。由于基于`prob`值的字符替换是随机的，因此“攻击：”文本可能会将其一些字母字符替换为其他随机字母。例如，“The”可能变成“Tge”，“quick”可能是“quicj”，等等。这些变化的数量和具体位置每次执行代码时都会变化，因为随机选择过程。
- en: 'Next, as another example, let’s implement a more sophisticated word-level attack
    using synonym replacement:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，作为另一个例子，让我们实现一个更复杂的词级攻击，使用同义词替换：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This function retrieves synonyms for a given word based on its part of speech.
    It uses WordNet, a lexical database for the English language, to find synonyms
    while ensuring they are different from the original word.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数根据给定单词的词性检索其同义词。它使用WordNet，一个英语语言的词汇数据库，来查找同义词，同时确保它们与原词不同。
- en: 'Now, let’s implement a word-level attack:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个词级攻击：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code snippet defines a function `word_level_attack` that attempts to create
    a subtly altered version of an input text by randomly replacing some words with
    their synonyms. It first tokenizes the input text into individual words and then
    determines the part-of-speech (POS) tag for each word. For each word, there’s
    a probability (set by the `prob` parameter, defaulting to `0.2`) that the word
    will be targeted for replacement. If a word is chosen, its POS tag is used to
    find potential synonyms from the WordNet lexical database. If synonyms are found,
    a random synonym replaces the original word in the output; otherwise, the original
    word is kept.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段定义了一个函数 `word_level_attack`，该函数尝试通过随机替换一些单词的同义词来创建输入文本的微妙修改版本。它首先将输入文本标记化成单个单词，然后为每个单词确定词性（POS）标签。对于每个单词，有一个概率（由
    `prob` 参数设置，默认为 `0.2`）表示该单词将被选中进行替换。如果选中单词，则使用其词性标签从 WordNet 词汇数据库中查找潜在的同义词。如果找到同义词，则随机同义词替换输出中的原始单词；否则，保留原始单词。
- en: 'The output of this code will display two lines. The first line, labeled `"Original:"`,
    will show the initial input text: `"The intelligent scientist conducted groundbreaking
    research."`. The second line, labeled `"Attacked:"`, will present the modified
    text. Due to the random nature of the word replacement based on the prob value,
    the `"Attacked:"` text will likely have some words replaced by their synonyms.
    For instance, “intelligent” might be replaced by “smart” or “clever,” “conducted”
    by “carried_out” or “did,” and “groundbreaking” by “innovative” or “pioneering.”
    The specific changes will vary each time the code is executed because of the random
    selection of words and their synonyms.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出将显示两行。第一行，标记为 `"Original:"`，将显示初始输入文本："The intelligent scientist conducted
    groundbreaking research."。第二行，标记为 `"Attacked:"`，将展示修改后的文本。由于基于概率值进行单词替换的随机性，`"Attacked:"`
    文本可能会用同义词替换一些单词。例如，"intelligent" 可能会被替换为 "smart" 或 "clever"，"conducted" 可能会被替换为
    "carried_out" 或 "did"，而 "groundbreaking" 可能会被替换为 "innovative" 或 "pioneering"。具体的更改每次执行代码时都会有所不同，因为单词及其同义词的选择是随机的。
- en: Adversarial training techniques
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗训练技术
- en: 'Adversarial training involves exposing the model to adversarial examples during
    the training process to improve its robustness. Here’s a simplified example of
    how you might implement adversarial training for an LLM:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练涉及在训练过程中向模型展示对抗性示例，以提高其鲁棒性。以下是一个简化的示例，说明您如何为 LLM 实现对抗训练：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function performs a single step of adversarial training. It generates adversarial
    perturbations using the **Fast Gradient Sign Method** (**FGSM**) and combines
    the loss from both clean and adversarial inputs. FGSM is a single-step adversarial
    attack that efficiently generates adversarial examples by calculating the gradient
    of the loss function with respect to the input data and then adding a small perturbation
    in the direction of the gradient’s sign. This perturbation, scaled by a small
    epsilon, aims to maximize the model’s prediction error, causing misclassification
    while being almost imperceptible to humans.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数执行一次对抗训练步骤。它使用 **Fast Gradient Sign Method**（**FGSM**）生成对抗性扰动，并合并干净和对抗输入的损失。FGSM
    是一种单步对抗攻击，通过计算损失函数相对于输入数据的梯度并添加一个小的扰动（在梯度的符号方向上）来有效地生成对抗性示例。这个扰动通过一个小 epsilon
    缩放，旨在最大化模型的预测误差，导致错误分类，同时对人类几乎不可察觉。
- en: 'To use this in a full training loop, employ the following function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要在完整训练循环中使用此功能，请使用以下函数：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function iterates over the training data, performing adversarial training
    steps for each batch. It updates the model parameters using the combined loss
    from clean and adversarial inputs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数遍历训练数据，为每个批次执行对抗训练步骤。它使用干净和对抗输入的合并损失来更新模型参数。
- en: Evaluating robustness
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估鲁棒性
- en: 'To evaluate the robustness of an LLM, we can measure its performance on both
    clean and adversarial inputs:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 LLM 的鲁棒性，我们可以测量其在干净和对抗输入上的性能：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This function evaluates the model’s performance on both clean and adversarially
    attacked inputs. It processes each item in the test dataset, generating predictions
    for both the original and attacked versions of the input.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数评估模型在干净和对抗攻击输入上的性能。它处理测试数据集中的每个项目，为输入的原始版本和攻击版本生成预测。
- en: 'You should also calculate the evaluation metrics:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该计算评估指标：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The provided Python code defines a function called `calculate_metrics` that
    takes three arguments: the true labels of the test data, the model’s predictions
    on the original (clean) test data, and the model’s predictions on the adversarially
    attacked versions of the test data. Inside the function, it utilizes the `accuracy_score`
    and `f1_score` functions from the `sklearn.metrics` library to calculate four
    key evaluation metrics:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的 Python 代码定义了一个名为 `calculate_metrics` 的函数，该函数接受三个参数：测试数据的真实标签、模型在原始（干净）测试数据上的预测，以及模型在对抗攻击版本的测试数据上的预测。在函数内部，它使用来自
    `sklearn.metrics` 库的 `accuracy_score` 和 `f1_score` 函数来计算四个关键评估指标：
- en: The accuracy of the model’s predictions on the clean data (`clean_accuracy`)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在干净数据上的预测准确性 (`clean_accuracy`)
- en: The accuracy on the adversarial data (`adv_accuracy`)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对抗数据上的准确性 (`adv_accuracy`)
- en: The weighted F1 score on the clean data (`clean_f1`)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干净数据上的加权 F1 分数 (`clean_f1`)
- en: The weighted F1 score on the adversarial data (`adv_f1`)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗数据上的加权 F1 分数 (`adv_f1`)
- en: The function then returns these four scores as a dictionary, where each metric’s
    name is the key and its calculated value is the corresponding value.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 函数随后将这四个分数作为字典返回，其中每个指标的名称是键，其计算值是对应的值。
- en: Each of the calculated scores provides a different perspective on the model’s
    performance. Accuracy represents the overall proportion of correctly classified
    instances out of the total number of instances. A high accuracy on clean data
    indicates the model performs well on original, unperturbed inputs, while a low
    accuracy suggests poor general performance. Conversely, a high accuracy on adversarial
    data implies the model is robust against the specific type of attack used, meaning
    the attacks are not very effective at fooling the model. A low accuracy on adversarial
    data, despite potentially high clean accuracy, highlights the model’s vulnerability
    to these attacks. The F1 score, particularly the weighted version used here to
    account for potential class imbalance, provides a balanced measure of precision
    and recall. A high F1 score on clean data signifies good performance in terms
    of both correctly identifying positive instances and avoiding false positives.
    Similarly, a high F1 score on adversarial data indicates robustness, as the model
    maintains good precision and recall even under attack. A low F1 score on either
    clean or adversarial data suggests the model struggles with either precision or
    recall, or both, in those respective conditions. Comparing the clean and adversarial
    scores reveals the extent to which the attacks degrade the model’s performance;
    a significant drop indicates a lack of robustness.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个计算出的分数都从不同角度反映了模型的表现。准确性表示在总实例数中正确分类的实例比例。干净数据上的高准确性表明模型在原始、未受干扰的输入上表现良好，而低准确性则表明整体性能较差。相反，对抗数据上的高准确性意味着模型对所使用的特定攻击具有鲁棒性，这意味着攻击在欺骗模型方面不是很有效。尽管干净准确性可能很高，但对抗数据上的低准确性突显了模型对这些攻击的脆弱性。F1
    分数，尤其是这里使用的加权版本，用于考虑潜在的类别不平衡，提供了一个平衡的精确度和召回率的度量。干净数据上的高 F1 分数表示在正确识别正实例和避免假阳性方面表现良好。同样，对抗数据上的高
    F1 分数表明鲁棒性，因为即使在攻击下，模型也保持了良好的精确度和召回率。干净或对抗数据上的低 F1 分数表明模型在这些相应条件下在精确度或召回率，或两者方面都有所挣扎。比较干净和对抗分数揭示了攻击降低模型性能的程度；显著的下降表明鲁棒性不足。
- en: Trade-offs in the adversarial training of LLMs
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 对抗训练中的权衡
- en: 'Adversarial training can improve model robustness, but it often comes with
    trade-offs:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练可以提高模型的鲁棒性，但通常伴随着权衡：
- en: '**Increased computational cost**: Generating adversarial examples during training
    is computationally expensive'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加的计算成本**：在训练期间生成对抗示例是计算密集型的。'
- en: '**Potential decrease in clean accuracy**: Focusing on adversarial robustness
    might slightly reduce performance on clean inputs'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在降低的干净准确性**：专注于对抗鲁棒性可能会略微降低干净输入的性能。'
- en: '**Generalization to unseen attacks**: Models might become robust to specific
    types of attacks but remain vulnerable to others'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推广到未见过的攻击**：模型可能对特定类型的攻击具有鲁棒性，但对其他攻击仍然脆弱。'
- en: 'To visualize these trade-offs, you could create a plot comparing clean and
    adversarial accuracy across different levels of adversarial training:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这些权衡，你可以创建一个比较不同对抗训练水平下的干净和对抗准确性的图表：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This function creates a plot to visualize how increasing the strength of adversarial
    training (epsilon) affects both clean and adversarial accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数创建一个图表，可视化增加对抗性训练强度（epsilon）如何影响干净和对抗性准确性。
- en: Real-world implications
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界影响
- en: 'Understanding the real-world implications of adversarial attacks on LLMs is
    crucial for responsible deployment:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理解对抗性攻击对LLM的现实世界影响对于负责任地部署至关重要：
- en: '**Security risks**: Adversarial attacks could be used to bypass content filters
    or manipulate model outputs in security-critical applications'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全风险**：对抗性攻击可能被用于绕过内容过滤器或在安全关键应用中操纵模型输出'
- en: '**Misinformation**: Attackers could potentially use adversarial techniques
    to generate fake news or misleading content that evades detection systems'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚假信息**：攻击者可能使用对抗性技术生成虚假新闻或误导性内容，从而逃避检测系统'
- en: '**User trust**: If LLMs are easily fooled by adversarial inputs, it could erode
    user trust in AI systems'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户信任**：如果LLM容易被对抗性输入欺骗，可能会侵蚀用户对AI系统的信任'
- en: '**Legal and ethical concerns**: The ability to manipulate LLM outputs raises
    ethical questions about responsibility and accountability in AI-driven decision-making'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律和伦理问题**：操纵LLM输出的能力引发了关于AI驱动决策中责任和问责的伦理问题'
- en: '**Robustness in diverse environments**: Real-world deployment of LLMs requires
    evaluating their performance under diverse adverse conditions, rather than relying
    solely on clean laboratory settings'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在多样化环境中的鲁棒性**：真实世界部署LLM需要评估它们在多样化不利条件下的性能，而不仅仅是依赖于干净的实验室环境'
- en: 'To address these implications, consider implementing robust deployment practices
    and red teaming exercises:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些影响，考虑实施鲁棒的部署实践和红队演习：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This class encapsulates best practices for deploying robust LLMs, including
    input validation, attack detection, and output post-processing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此类封装了部署鲁棒LLM的最佳实践，包括输入验证、攻击检测和输出后处理。
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Addressing adversarial robustness in LLMs is crucial for their safe and reliable
    deployment in real-world applications. By implementing the techniques and considerations
    discussed in this chapter, you can work toward developing LLMs that are more resilient
    to adversarial attacks while maintaining high performance on clean inputs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM中解决对抗性鲁棒性对于它们在现实世界应用中的安全可靠部署至关重要。通过实施本章讨论的技术和考虑因素，你可以朝着开发出对对抗性攻击更具弹性同时保持对干净输入高性能的LLM迈进。
- en: In the upcoming chapter, we will explore **Reinforcement Learning from Human
    Feedback** (**RLHF**) for LLM training.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨LLM训练中的**人类反馈强化学习**（**RLHF**）。
