- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Distributional Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: In this chapter, we will learn about distributional reinforcement learning.
    We will begin the chapter by understanding what exactly distributional reinforcement
    learning is and why it is useful. Next, we will learn about one of the most popular
    distributional reinforcement learning algorithms called **categorical DQN**. We
    will understand what a categorical DQN is and how it differs from the DQN we learned
    in *Chapter 9*, *Deep Q Networks and Its Variants*, and then we will explore the
    categorical DQN algorithm in detail.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习分布式强化学习。我们将通过理解分布式强化学习究竟是什么以及它为何有用来开始本章。接下来，我们将学习一种非常流行的分布式强化学习算法，叫做
    **分类 DQN**。我们将了解分类 DQN 是什么，它与我们在 *第 9 章*，*深度 Q 网络及其变体* 中学习的 DQN 有什么不同，然后我们将详细探讨分类
    DQN 算法。
- en: Following this, we will learn another interesting algorithm called **Quantile
    Regression DQN** (**QR-DQN**). We will understand what a QR-DQN is and how it
    differs from a categorical DQN, and then we will explore the QR-DQN algorithm
    in detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习另一个有趣的算法，叫做 **分位数回归 DQN**（**QR-DQN**）。我们将了解 QR-DQN 是什么，它与分类 DQN 有什么不同，然后我们将详细探讨
    QR-DQN 算法。
- en: At the end of the chapter, we will learn about the policy gradient algorithm
    called the **Distributed Distributional Deep Deterministic Policy Gradient** (**D4PG**).
    We will learn what the D4PG is and how it differs from the DDPG we covered in *Chapter
    12*, *Learning DDPG, TD3, and SAC*, in detail
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将学习一种名为 **分布式分布式深度确定性策略梯度**（**D4PG**）的策略梯度算法。我们将详细了解 D4PG 是什么，以及它与我们在
    *第 12 章*，*学习 DDPG、TD3 和 SAC* 中学习的 DDPG 有什么不同。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Why distributional reinforcement learning?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么使用分布式强化学习？
- en: Categorical DQN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类 DQN
- en: Quantile regression DQN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分位数回归 DQN
- en: Distributed distributional deep deterministic policy gradient
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式分布式深度确定性策略梯度
- en: Let's begin the chapter by understanding what distributional reinforcement learning
    is and why we need it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过理解什么是分布式强化学习以及我们为什么需要它来开始本章。
- en: Why distributional reinforcement learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用分布式强化学习？
- en: Say we are in state *s* and we have two possible actions to perform in this
    state. Let the actions be *up* and *down*. How do we decide which action to perform
    in the state? We compute Q values for all actions in the state and select the
    action that has the maximum Q value. So, we compute *Q*(*s*, up) and *Q*(*s*,
    down) and select the action that has the maximum Q value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于状态 *s*，并且在这个状态下有两种可能的动作可以执行。设这两种动作分别是 *上* 和 *下*。我们如何决定在这个状态下执行哪个动作呢？我们为该状态下的所有动作计算
    Q 值，并选择 Q 值最大的动作。因此，我们计算 *Q*(*s*, 上) 和 *Q*(*s*, 下)，并选择 Q 值最大的动作。
- en: 'We learned that the Q value is the expected return an agent would obtain when
    starting from state *s* and performing an action *a* following the policy ![](img/B15558_14_001.png):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q 值是一个智能体从状态 *s* 开始并执行一个动作 *a* 后，根据策略获得的预期回报！[](img/B15558_14_001.png)：
- en: '![](img/B15558_14_002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_002.png)'
- en: But there is a small problem in computing the Q value in this manner because
    the Q value is just an expectation of the return, and the expectation does not
    include the intrinsic randomness. Let's understand exactly what this means with
    an example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但使用这种方法计算 Q 值时有一个小问题，因为 Q 值仅仅是回报的期望值，而期望值并未包含内在的随机性。让我们通过一个例子来准确理解这意味着什么。
- en: Let's suppose we want to drive from work to home and we have two routes **A**
    and **B**. Now, we have to decide which route is better, that is, which route
    helps us to reach home in the minimum amount of time. To find out which route
    is better, we can calculate the Q values and select the route that has the maximum
    Q value, that is, the route that gives us the maximum expected return.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要从工作地点开车回家，我们有两条路线 **A** 和 **B**。现在，我们需要决定哪条路线更好，也就是说，哪条路线帮助我们在最短的时间内到达家里。为了找出哪条路线更好，我们可以计算
    Q 值，并选择 Q 值最大的路线，也就是给我们最大期望回报的路线。
- en: Say the Q value of choosing route *A* is *Q*(*s*, *A*) = 31, and the Q value
    of choosing route *B* is *Q*(*s*, *B*) = 28\. Since the Q value (the expected
    return of route **A**) is higher, we can choose route **A** to travel home. But
    are we missing something here? Instead of viewing the Q value as an expectation
    over a return, can we directly look into the distribution of return and make a
    better decision?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设选择路线*Ａ*的Q值为*Q*(*s*, *A*) = 31，选择路线*B*的Q值为*Q*(*s*, *B*) = 28。由于Q值（路线**A**的期望回报）较高，我们可以选择路线**A**回家。但这里是不是遗漏了什么呢？我们能不能直接看回报的分布，而不是把Q值看作回报的期望，从而做出更好的决策？
- en: Yes!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！
- en: 'But first, let''s take a look at the distribution of route **A** and route
    **B** and understand which route is best. The following plot shows the distribution
    of route **A**. It tells us with 70% probability we reach home in 10 minutes,
    and with 30% probability we reach home in 80 minutes. That is, if we choose route
    **A** we usually reach home in 10 minutes but when there is heavy traffic we reach
    home in 80 minutes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们看看路线**A**和路线**B**的分布，并理解哪条路线更优。以下图表显示了路线**A**的分布。它告诉我们，有70%的概率在10分钟内到家，而有30%的概率需要80分钟才能到家。也就是说，如果选择路线**A**，我们通常能在10分钟内到家，但当遇到交通拥堵时，我们需要80分钟才能到家：
- en: '![](img/B15558_14_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_01.png)'
- en: 'Figure 14.1: Distribution of route A'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：路线A的分布
- en: '*Figure 14.2* shows the distribution of route **B**. It tells us that with
    80% probability we reach home in 20 minutes and with 20% probability we reach
    home in 60 minutes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.2*显示了路线**B**的分布。它告诉我们，有80%的概率在20分钟内到家，20%的概率需要60分钟才能到家。'
- en: 'That is, if we choose route **B** we usually reach home in 20 minutes but when
    there is heavy traffic we reach home in 60 minutes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果我们选择路线**B**，通常在20分钟内到家，但当遇到交通拥堵时，我们需要60分钟才能到家：
- en: '![](img/B15558_14_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_02.png)'
- en: 'Figure 14.2: Distribution of route B'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：路线B的分布
- en: After looking at these two distributions, it makes more sense to choose route
    **B** instead of choosing route **A**. With route **B**, even in the worst case,
    that is, even when there is heavy traffic, we can reach home in 60 minutes. But
    with route **A**, when there is heavy traffic, we reach home in 80 minutes. So,
    it is a wise decision to choose route **B** rather than **A**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 看过这两种分布后，我们会发现选择路线**B**比选择路线**A**更有意义。选择路线**B**时，即使在最糟糕的情况下，也就是遇到交通拥堵时，我们能在60分钟内到家。但选择路线**A**时，在交通繁忙时，我们需要80分钟才能到家。因此，选择路线**B**而不是**A**是明智的决定。
- en: Similarly, if we can observe the distribution of return of route **A** and route
    **B**, we can understand more information and we will miss out on these details
    when we take actions just based on the maximum expected return, that is, the maximum
    Q value. So, instead of using the expected return to select an action, we use
    the distribution of return and then select optimal action based on the distribution.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们观察路线**A**和路线**B**的回报分布，我们就能获得更多信息。如果我们仅仅根据最大期望回报（即最大Q值）来采取行动，那么这些细节将会被忽略。因此，我们不是用期望回报来选择行动，而是使用回报的分布，然后根据分布选择最优的行动。
- en: This is the basic idea and motivation behind distributional reinforcement learning.
    In the next section, we will learn one of the most popular distributional reinforcement
    learning algorithms, called categorical DQN, which is also known as the C51 algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分布式强化学习的基本思想和动机。在下一节中，我们将学习一种最流行的分布式强化学习算法——分类DQN，也叫做C51算法。
- en: Categorical DQN
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类DQN
- en: In the last section, we learned why it is more beneficial to choose an action
    based on the distribution of return than to choose an action based on the Q value,
    which is just the expected return. In this section, we will understand how to
    compute the distribution of return using an algorithm called categorical DQN.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了为什么基于回报分布来选择行动比仅仅基于Q值（即期望回报）来选择行动更有利。在这一节中，我们将学习如何使用一种名为分类DQN的算法来计算回报的分布。
- en: The distribution of return is often called the value distribution or return
    distribution. Let *Z* be the random variable and *Z*(*s*, *a*) denote the value
    distribution of a state *s* and an action *a*. We know that the Q function is
    represented by *Q*(*s*, *a*) and it gives the value of a state-action pair. Similarly,
    now we have *Z*(*s*, *a*) and it gives the value distribution (return distribution)
    of the state-action pair.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回报的分布通常被称为值分布或回报分布。设*Z*为随机变量，*Z*(*s*, *a*)表示状态*s*和动作*a*的值分布。我们知道Q函数表示为*Q*(*s*,
    *a*)，它给出一个状态-动作对的值。同样，现在我们有了*Z*(*s*, *a*)，它给出状态-动作对的值分布（回报分布）。
- en: Okay, how can we compute *Z*(*s*, *a*)? First, let's recollect how we compute
    *Q*(*s*, *a*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何计算*Z*(*s*, *a*)？首先，让我们回顾一下如何计算*Q*(*s*, *a*)。
- en: In DQN, we learned that we use a neural network to approximate the Q function,
    *Q*(*s*, *a),* Since we use a neural network to approximate the Q function, we
    can represent the Q function by ![](img/B15558_12_331.png), where ![](img/B15558_14_004.png)
    is the parameter of the network. Given a state as an input to the network, it
    outputs the Q values of all the actions that can be performed in that state, and
    then we select the action that has the maximum Q value.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN中，我们学习到使用神经网络来逼近Q函数，*Q*(*s*, *a*)。由于我们使用神经网络来逼近Q函数，我们可以通过![](img/B15558_12_331.png)来表示Q函数，其中![](img/B15558_14_004.png)是网络的参数。给定一个状态作为输入到网络，它输出所有可以在该状态下执行的动作的Q值，然后我们选择具有最大Q值的动作。
- en: Similarly, in categorical DQN, we use a neural network to approximate the value
    of *Z*(*s*, *a*). We can represent this by ![](img/B15558_14_005.png), where ![](img/B15558_09_087.png)
    is the parameter of the network. Given a state as an input to the network, it
    outputs the value distribution (return distribution) of all the actions that can
    be performed in that state as an output and then we select an action based on
    this value distribution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在分类DQN中，我们使用神经网络来逼近*Z*(*s*, *a*)的值。我们可以通过![](img/B15558_14_005.png)来表示这一点，其中![](img/B15558_09_087.png)是网络的参数。给定一个状态作为输入到网络，它输出所有可以在该状态下执行的动作的值分布（回报分布），然后我们根据这个值分布选择一个动作。
- en: 'Let''s understand the difference between the DQN and categorical DQN with an
    example. Suppose we are in the state *s* and say our action space has two actions
    *a* and *b*. Now, as shown in *Figure 14.3*, given the state *s* as an input to
    the DQN, it returns the Q value of all the actions, then we select the action
    that has the maximum Q value, whereas in the categorical DQN, given the state
    *s* as an input, it returns the value distribution of all the actions, then we
    select the action based on this value distribution:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解DQN和分类DQN之间的区别。假设我们处于状态*s*，并且假设我们的动作空间有两个动作*a*和*b*。现在，如*图14.3*所示，给定状态*s*作为输入，DQN返回所有动作的Q值，然后我们选择具有最大Q值的动作，而在分类DQN中，给定状态*s*作为输入，它返回所有动作的值分布，然后我们根据这个值分布选择一个动作：
- en: '![](img/B15558_14_03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_03.png)'
- en: 'Figure 14.3: DQN vs categorical DQN'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：DQN与分类DQN
- en: Okay, how can we train the network? In DQN, we learned that we train the network
    by minimizing the loss between the target Q value and the Q value predicted by
    the network. We learned that the target Q value is obtained by the Bellman optimality
    equation. Thus, we minimize the loss between the target value (the optimal Bellman
    Q value) and the predicted value (the Q value predicted by the network) and train
    the network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何训练网络？在DQN中，我们学习到通过最小化目标Q值与网络预测的Q值之间的损失来训练网络。我们知道目标Q值是通过贝尔曼最优方程获得的。因此，我们最小化目标值（最优贝尔曼Q值）与预测值（网络预测的Q值）之间的损失并训练网络。
- en: Similarly, in categorical DQN, we train the network by minimizing the loss between
    the target value distribution and the value distribution predicted by the network.
    Okay, how can we obtain the target value distribution? In DQN, we obtained the
    target Q value using the Bellman equation; similarly in categorical DQN, we can
    obtain the target value distribution using the distributional Bellman equation.
    What's the distributional Bellman equation? First, let's recall the Bellman equation
    before learning about the distributional Bellman equation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在分类DQN中，我们通过最小化目标值分布与网络预测的值分布之间的损失来训练网络。好的，我们如何获得目标值分布？在DQN中，我们使用贝尔曼方程来获得目标Q值；类似地，在分类DQN中，我们可以使用分布式贝尔曼方程来获得目标值分布。那么，什么是分布式贝尔曼方程？首先，在学习分布式贝尔曼方程之前，让我们回顾一下贝尔曼方程。
- en: 'We learned that the Bellman equation for the Q function *Q*(*s*, *a*) is given
    as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q函数*Q*(*s*, *a*)的贝尔曼方程表示为：
- en: '![](img/B15558_14_007.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_007.png)'
- en: 'Similarly, the Bellman equation for the value distribution *Z*(*s*, *a*) is
    given as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，值分布*Z*(*s*, *a*)的贝尔曼方程表示为：
- en: '![](img/B15558_14_008.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_008.png)'
- en: This equation is called the distributional Bellman equation. Thus, in categorical
    DQN, we train the network by minimizing the loss between the target value distribution,
    which is given by the distributional Bellman equation, and the value distribution
    predicted by the network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为分布式贝尔曼方程。因此，在分类DQN中，我们通过最小化目标值分布（由分布式贝尔曼方程给出）和网络预测的值分布之间的损失来训练网络。
- en: Okay, what loss function should we use? In DQN, we use the **mean squared error**
    (**MSE**) as our loss function. Unlike a DQN, we cannot use the MSE as the loss
    function in the categorical DQN because in categorical DQN, we predict the probability
    distribution and not the Q value. Since we are dealing with the distribution we
    use the cross entropy loss as our loss function. Thus, in categorical DQN, we
    train the network by minimizing the cross entropy loss between the target value
    distribution and the value distribution predicted by the network.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们应该使用什么样的损失函数呢？在DQN中，我们使用**均方误差**（**MSE**）作为我们的损失函数。与DQN不同的是，在分类DQN中，我们不能使用MSE作为损失函数，因为在分类DQN中，我们预测的是概率分布，而不是Q值。由于我们处理的是分布，因此我们使用交叉熵损失作为我们的损失函数。因此，在分类DQN中，我们通过最小化目标值分布与网络预测的值分布之间的交叉熵损失来训练网络。
- en: In a nutshell, a categorical DQN is similar to DQN, except that in a categorical
    DQN, we predict the value distribution whereas in a DQN we predict the Q value.
    Thus, given a state as an input, a categorical DQN returns the value distribution
    of each action in that state. We train the network by minimizing the cross entropy
    loss between the target value distribution, which is given by the distributional
    Bellman equation, and the value distribution predicted by the network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，分类DQN与DQN相似，唯一不同的是在分类DQN中，我们预测的是值分布，而在DQN中，我们预测的是Q值。因此，给定一个状态作为输入，分类DQN返回该状态下每个动作的值分布。我们通过最小化目标值分布（由分布式贝尔曼方程给出）和网络预测的值分布之间的交叉熵损失来训练网络。
- en: Now that we have understood what a categorical DQN is and how it differs from
    a DQN, in the next section we will learn how exactly the categorical DQN predicts
    the value distribution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了分类DQN是什么以及它与DQN的区别，在接下来的部分中，我们将学习分类DQN是如何准确预测值分布的。
- en: Predicting the value distribution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测值分布
- en: '*Figure 14.4* shows a simple value distribution:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.4*展示了一个简单的值分布：'
- en: '![](img/B15558_14_04.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_04.png)'
- en: 'Figure 14.4: Value distribution'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：值分布
- en: The horizontal axis values are called support or atoms and the vertical axis
    values are the probability. We denote the support by *Z* and the probability by
    *P*. In order to predict the value distribution, along with the state, our network
    takes the support of the distribution as input and it returns the probability
    of each value in the support.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴的值称为支持或原子，垂直轴的值则是概率。我们用*Z*表示支持，用*P*表示概率。为了预测值的分布以及状态，我们的网络将分布的支持作为输入，并返回支持中每个值的概率。
- en: So, now, we will see how to compute the support of the distribution. To compute
    support, first, we need to decide the number of values of the support *N*, the
    minimum value of the support ![](img/B15558_14_009.png), and the maximum value
    of the support ![](img/B15558_14_010.png). Given a number of support *N*, we divide
    them into *N* equal parts from ![](img/B15558_14_009.png) to ![](img/B15558_14_012.png).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在我们将看到如何计算分布的支持。为了计算支持，首先我们需要决定支持的值的数量*N*、支持的最小值![](img/B15558_14_009.png)和支持的最大值![](img/B15558_14_010.png)。给定支持的数量*N*，我们将其从![](img/B15558_14_009.png)到![](img/B15558_14_012.png)分成*N*个相等的部分。
- en: 'Let''s understand this with an example. Say the number of support *N* = 5,
    the minimum value of support ![](img/B15558_14_013.png), and the maximum value
    of the support ![](img/B15558_14_014.png). Now, how can we find the values of
    the support? In order to find the values of the support, first, we will compute
    the step size called ![](img/B15558_14_015.png). The value of ![](img/B15558_14_016.png)
    can be computed as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设支撑值数量 *N* = 5，支撑值的最小值为 ![](img/B15558_14_013.png)，最大值为 ![](img/B15558_14_014.png)。现在，如何找到支撑值呢？为了找到支撑值，首先，我们需要计算一个步长，记作
    ![](img/B15558_14_015.png)。值 ![](img/B15558_14_016.png) 可以通过以下公式计算：
- en: '![](img/B15558_14_017.png)![](img/B15558_14_018.png)![](img/B15558_14_019.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_017.png)![](img/B15558_14_018.png)![](img/B15558_14_019.png)'
- en: 'Now, to compute the values of support, we start with the minimum value of support
    ![](img/B15558_14_009.png) and add ![](img/B15558_14_021.png) to every value until
    we reach the number of support *N*. In our example, we start with ![](img/B15558_14_022.png),
    which is 2, and we add ![](img/B15558_14_023.png) to every value until we reach
    the number of support *N*. Thus, the support values become:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算支撑值，我们从支撑值的最小值开始 ![](img/B15558_14_009.png)，并将 ![](img/B15558_14_021.png)
    加到每个值上，直到我们达到支撑值数量 *N*。在我们的示例中，我们从 ![](img/B15558_14_022.png) 开始，它是2，然后我们将 ![](img/B15558_14_023.png)
    加到每个值上，直到我们达到支撑值数量 *N*。因此，支撑值变为：
- en: '![](img/B15558_14_024.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_024.png)'
- en: 'Thus, we can write the value of support as ![](img/B15558_14_025.png). The
    following Python snippet gives us more clarity on how to obtain the support values:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将支撑值表示为 ![](img/B15558_14_025.png)。以下Python代码片段可以帮助我们更清楚地了解如何获得支撑值：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Okay, we have learned how to compute the support of the distribution, now how
    does the neural network take this support as input and return the probabilities?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经了解了如何计算分布的支撑值，现在神经网络是如何将这个支撑值作为输入并返回概率的呢？
- en: In order to predict the value distribution, along with the state, we also need
    to give the support of the distribution as input and then the network returns
    the probabilities of our value distribution as output. Let's understand this with
    an example. Say we are in a state *s* and we have two actions to perform in this
    state, and let the actions be *up* and *down*. Say our calculated support values
    are *z*[1], *z*[2], and *z*[3].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测价值分布，除了状态外，我们还需要将分布的支撑值作为输入，然后网络返回我们价值分布的概率作为输出。让我们通过一个例子来理解这一点。假设我们处于状态
    *s*，并且在这个状态下有两个动作可供选择，分别是 *上* 和 *下*。假设我们计算得到的支撑值是 *z*[1]、*z*[2] 和 *z*[3]。
- en: 'As *Figure 14.5* shows, along with giving the state *s* as input to the network,
    we also give the support of our distribution *z*[1], *z*[2], and *z*[3]. Then
    our network returns the probabilities *p*[i](*s*, *a*) of the given support for
    the distribution of action *up* and distribution of action *down*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 *图14.5* 所示，在将状态 *s* 作为输入传入网络的同时，我们还将分布的支撑值 *z*[1]、*z*[2] 和 *z*[3] 输入。然后，网络返回给定支撑值对应的动作
    *上* 和动作 *下* 的分布概率 *p*[i](*s*, *a*)：
- en: '![](img/B15558_14_05.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_05.png)'
- en: 'Figure 14.5: A categorical DQN'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：分类DQN
- en: The authors of the categorical DQN paper (see the *Further reading* section
    for more details) suggest that it will be efficient to set the number of support
    *N* as 51, and so the categorical DQN is also known as the C51 algorithm. Thus,
    we have learned how categorical DQN predicts the value distribution. In the next
    section, we will learn how to select the action based on this predicted value
    distribution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分类DQN论文的作者（详见 *进一步阅读* 部分）建议将支撑值数量 *N* 设置为51，因此分类DQN也被称为C51算法。因此，我们已经了解了分类DQN如何预测价值分布。在下一部分，我们将学习如何基于这个预测的价值分布选择动作。
- en: Selecting an action based on the value distribution
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于价值分布选择动作
- en: We have learned that a categorical DQN returns the value distribution of each
    action in the given state. But how can we select the best action based on the
    value distribution predicted by the network?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学过分类DQN返回给定状态下每个动作的价值分布。但我们如何根据网络预测的价值分布来选择最佳动作呢？
- en: We generally select an action based on the Q value, that is, we usually select
    the action that has the maximum Q value. But now we don't have a Q value; instead,
    we have a value distribution. How can we select an action based on the value distribution?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常基于Q值选择动作，也就是说，我们通常选择具有最大Q值的动作。但现在我们没有Q值，而是有一个价值分布。那我们如何基于价值分布选择动作呢？
- en: 'First, we will extract the Q value from the value distribution and then we
    select the action as the one that has the maximum Q value. Okay, how can we extract
    the Q value? We can compute the Q value by just taking the expectation of the
    value distribution. The expectation of the distribution is given as the sum of
    support *z*[i] multiplied by their corresponding probability *p*[i]. So the expectation
    of the value distribution *Z* is given as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从价值分布中提取Q值，然后选择具有最大Q值的动作。好的，如何提取Q值呢？我们可以通过计算价值分布的期望值来获得Q值。分布的期望值可以表示为支持度*z*[i]与它们相应的概率*p*[i]的乘积的总和。因此，价值分布*Z*的期望值为：
- en: '![](img/B15558_14_026.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_026.png)'
- en: Where *z*[i] is the support and *p*[i] is the probability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*z*[i]是支持度，*p*[i]是概率。
- en: 'Thus, the Q value of the value distribution can be computed as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，价值分布的Q值可以计算为：
- en: '![](img/B15558_14_027.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_027.png)'
- en: 'After computing the Q value, we select the best action as the one that has
    the maximum Q value:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算Q值后，我们选择Q值最大的动作作为最佳动作：
- en: '![](img/B15558_14_028.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_028.png)'
- en: Let's understand how this works exactly. Suppose we are in the state *s* and
    say we have two actions in the state. Let the actions be *up* and *down*. First,
    we need to compute support. Let the number of support *N* = 3, the minimum value
    of the support ![](img/B15558_14_013.png), and the maximum value of the support
    ![](img/B15558_14_030.png). Then, our computed support values will be [2,3,4].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们准确理解这一过程。假设我们处于状态*s*，并且在该状态下有两个动作。假设这两个动作分别为*up*和*down*。首先，我们需要计算支持度。假设支持度的数量*N*
    = 3，支持度的最小值为 ![](img/B15558_14_013.png)，最大值为 ![](img/B15558_14_030.png)。然后，我们计算出的支持度值为[2,3,4]。
- en: 'Now, along with the state *s*, we feed the support, then the categorical DQN
    returns the probabilities *p*[i](*s*, *a*) of the given support for the value
    distribution of action *up* and distribution of action *down* as shown here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将支持度与状态*s*一起输入，然后类别DQN返回给定支持度下的动作*up*和动作*down*的价值分布概率*p*[i](*s*, *a*)，如图所示：
- en: '![](img/B15558_14_06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_06.png)'
- en: 'Figure 14.6: Categorical DQN'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：类别DQN
- en: Now, how can we select the best action, based on these two value distributions?
    First, we will extract the Q value from the value distributions and then we select
    the action that has the maximum Q value.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如何根据这两个价值分布选择最佳动作呢？首先，我们将从价值分布中提取Q值，然后选择具有最大Q值的动作。
- en: 'We learned that the Q value can be extracted from the value distribution as
    the sum of support multiplied by their probabilities:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q值可以通过将支持度与相应的概率相乘的和从价值分布中提取出来：
- en: '![](img/B15558_14_027.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_027.png)'
- en: 'So, we can compute the Q value of action *up* in state *s* as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以计算在状态*s*中动作*up*的Q值，如下所示：
- en: '![](img/B15558_14_032.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_032.png)'
- en: 'Now, we can compute the Q value of action *down* in state *s* as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算在状态*s*中动作*down*的Q值，如下所示：
- en: '![](img/B15558_14_033.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_033.png)'
- en: Now, we select the action that has the maximum Q value. Since the action *up*
    has the high Q value, we select the action *up* as the best action.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们选择具有最大Q值的动作。由于动作*up*具有较高的Q值，因此我们选择动作*up*作为最佳动作。
- en: Wait! What makes categorical DQN special then? Because just like DQN, we are
    selecting the action based on the Q value at the end. One important point we have
    to note is that, in DQN, we compute the Q value based on the expectation of the
    return directly, but in categorical DQN, first, we learn the return distribution
    and then we compute the Q value based on the expectation of the return distribution,
    which captures the intrinsic randomness.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！那么，类别DQN究竟有什么特别之处？因为像DQN一样，我们最终也是根据Q值选择动作。一个我们必须注意的重要点是，在DQN中，我们是直接根据回报的期望来计算Q值的，但在类别DQN中，我们首先学习回报分布，然后基于回报分布的期望来计算Q值，这样能够捕捉到内在的随机性。
- en: We have learned that the categorical DQN outputs the value distribution of all
    the actions in the given state and then we extract the Q value from the value
    distribution and select the action that has the maximum Q value as the best action.
    But the question is how exactly does our categorical DQN learn? How do we train
    the categorical DQN to predict the accurate value distribution? Let's discuss
    this in the next section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，类别DQN输出给定状态下所有动作的价值分布，然后从中提取Q值并选择具有最大Q值的动作作为最佳动作。但问题是，我们的类别DQN到底是如何学习的？我们如何训练类别DQN以预测准确的价值分布？我们将在下一节讨论这个问题。
- en: Training the categorical DQN
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练分类DQN
- en: 'We train the categorical DQN by minimizing the cross entropy loss between the
    target value distribution and the predicted value distribution. How can we compute
    the target distribution? We can compute the target distribution using the distributional
    Bellman equation given as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最小化目标值分布和预测值分布之间的交叉熵损失来训练分类DQN。我们如何计算目标分布呢？我们可以通过以下的分布贝尔曼方程来计算目标分布：
- en: '![](img/B15558_14_034.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_034.png)'
- en: 'Where ![](img/B15558_14_035.png) represents the immediate reward *r*, which
    we obtain while performing an action *a* in the state *s* and moving to the next
    state ![](img/B15558_14_036.png), so we can just denote ![](img/B15558_14_035.png)
    by *r*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_14_035.png) 表示即时奖励*r*，这是在执行动作*a*并从状态*s*移动到下一个状态 ![](img/B15558_14_036.png)
    时获得的奖励，因此我们可以将 ![](img/B15558_14_035.png) 简单表示为*r*：
- en: '![](img/B15558_14_038.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_038.png)'
- en: Remember in DQN we computed the target value using the target network parameterized
    by ![](img/B15558_14_039.png)? Similarly, here, we use the target categorical
    DQN parameterized by ![](img/B15558_14_040.png) to compute the target distribution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在DQN中，我们使用由 ![](img/B15558_14_039.png) 参数化的目标网络计算目标值吗？同样地，这里我们使用由 ![](img/B15558_14_040.png)
    参数化的目标分类DQN来计算目标分布。
- en: After computing the target distribution, we train the network by minimizing
    the cross entropy loss between the target value distribution and the predicted
    value distribution. One important point we need to note here is that we can apply
    the cross entropy loss between any two distributions only when their supports
    are equal; when their supports are not equal we cannot apply the cross entropy
    loss.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 计算目标分布后，我们通过最小化目标值分布与预测值分布之间的交叉熵损失来训练网络。这里有一个重要的点需要注意：只有在目标分布和预测分布的支持相等时，才能应用交叉熵损失；如果它们的支持不相等，我们就无法应用交叉熵损失。
- en: 'For instance, *Figure 14.7* shows the support of both the target and predicted
    distribution is the same, (1,2,3,4). Thus, in this case, we can apply the cross
    entropy loss:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图 14.7* 显示了目标分布和预测分布的支持相同，(1,2,3,4)。因此，在这种情况下，我们可以应用交叉熵损失：
- en: '![](img/B15558_14_07.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_07.png)'
- en: 'Figure 14.7: Target and predicted distribution'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：目标分布与预测分布
- en: In *Figure 14.8*, we can see that the target distribution support (1,3,4,5)
    and the predicted distribution support (1,2,3,4) are different, so in this case,
    we cannot apply the cross entropy loss.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 14.8*中，我们可以看到目标分布的支持（1,3,4,5）和预测分布的支持（1,2,3,4）是不同的，因此在这种情况下，我们无法应用交叉熵损失。
- en: '![](img/B15558_14_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_08.png)'
- en: 'Figure 14.8: Target and predicted distribution'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：目标分布与预测分布
- en: So, when the support of the target and prediction distribution is different,
    we perform a special step called the projection step using which we can make the
    support of the target and prediction distribution equal. Once we make the support
    of the target and prediction distribution equal then we can apply the cross entropy loss.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当目标分布和预测分布的支持不同时，我们会执行一个特殊的步骤，称为投影步骤，借助这个步骤，我们可以使目标分布和预测分布的支持相等。一旦我们使目标和预测分布的支持相等，就可以应用交叉熵损失。
- en: In the next section, we will learn how exactly the projection works and how
    it makes the support of the target and prediction distribution equal.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习投影步骤是如何工作的，以及它如何使目标分布和预测分布的支持相等。
- en: Projection step
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 投影步骤
- en: Let's understand how exactly the projection step works with an example. Suppose
    the input support is *z* = [1, 2].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解投影步骤是如何工作的。假设输入支持是*z* = [1, 2]。
- en: 'Let the probability of predicted distribution be *p* = [0.5, 0.5]. *Figure
    14.9* shows the predicted distribution:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 设预测分布的概率为*p* = [0.5, 0.5]。*图 14.9* 显示了预测分布：
- en: '![](img/B15558_14_09.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_09.png)'
- en: 'Figure 14.9: Predicted distribution'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：预测分布
- en: 'Let the probability of target distribution be *p* = [0.3, 0.7]. Let the reward
    *r* = 0.1 and the discount factor ![](img/B15558_14_041.png). The target distribution
    support value is computed as![](img/B15558_14_042.png), so, we can write:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设目标分布的概率为*p* = [0.3, 0.7]。设奖励*r* = 0.1，折扣因子为 ![](img/B15558_14_041.png)。目标分布的支持值计算为
    ![](img/B15558_14_042.png)，因此，我们可以写为：
- en: '![](img/B15558_14_043.png)![](img/B15558_14_044.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_043.png)![](img/B15558_14_044.png)'
- en: 'Thus, the target distribution becomes:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标分布变为：
- en: '![](img/B15558_14_10.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_10.png)'
- en: 'Figure 14.10: Target distribution'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：目标分布
- en: As we can observe from the preceding plots, the supports of the predicted distribution
    and target distribution are different. The predicted distribution has the support
    [1, 2] while the target distribution has the support [1, 1.9], so in this case,
    we cannot apply the cross entropy loss.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中我们可以看到，预测分布和目标分布的支持范围不同。预测分布的支持范围是[1, 2]，而目标分布的支持范围是[1, 1.9]，因此在这种情况下，我们无法直接应用交叉熵损失函数。
- en: Now, using the projection step we can convert the support of our target distribution
    to be the same support as the predicted distribution. Once the supports of the
    predicted and target distribution are the same then we can apply the cross entropy
    loss.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用投影步骤，我们可以将目标分布的支持范围转换为与预测分布相同的支持范围。一旦预测分布和目标分布的支持范围一致，我们就可以应用交叉熵损失函数。
- en: Okay, what's that projection step exactly? How can we apply it and convert the
    support of the target distribution to match the support of the predicted distribution?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么这个投影步骤到底是什么呢？我们该如何应用它，将目标分布的支持范围转换为与预测分布的支持范围一致呢？
- en: Let's understand this with the same example. As the following shows, we have
    the target distribution support [1, 1.9] and we need to make it equal to the predicted
    distribution support [1, 2], how can we do that?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过相同的例子来理解这个问题。如下面所示，我们的目标分布支持范围是[1, 1.9]，而我们需要将其调整为预测分布支持范围[1, 2]，我们该如何操作呢？
- en: '![](img/B15558_14_11.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_11.png)'
- en: 'Figure 14.11: Target distribution'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：目标分布
- en: 'So, what we can do is that we can distribute the probability 0.7 from the support
    1.9 to the support 1 and 2:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以做的是将0.7的概率从支持1.9分配到支持1和2：
- en: '![](img/B15558_14_12.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_12.png)'
- en: 'Figure 14.12: Target distribution'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：目标分布
- en: Okay, but how can we distribute the probabilities from the support 1.9 to the
    support 1 and 2? Should it be an equal distribution? Of course not. Since 2 is
    closer to 1.9, we distribute more probability to 2 and less to 1\.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但我们如何将概率从支持1.9分配到支持1和2呢？是不是应该均匀分配呢？当然不是。由于2比1.9更接近，我们将更多的概率分配给2，较少的分配给1。
- en: As shown in *Figure 14.13*, from 0.7, we will distribute 0.63 to support 2 and
    0.07 to support 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图14.13*所示，从0.7开始，我们将0.63的概率分配给支持2，0.07的概率分配给支持1。
- en: '![](img/B15558_14_13.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_13.png)'
- en: 'Figure 14.13: Target distribution'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.13：目标分布
- en: 'Thus, now our target distribution will look like:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的目标分布将变为：
- en: '![](img/B15558_14_14.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_14.png)'
- en: 'Figure 14.14: Target distribution'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14：目标分布
- en: From *Figure 14.14*, we can see that support of the target distribution is changed
    from [1, 1.9] to [1, 2] and now it matches the support of the predicted distribution.
    This step is called the projection step.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图14.14*中我们可以看到，目标分布的支持范围已从[1, 1.9]变化为[1, 2]，现在它与预测分布的支持范围一致。这一步被称为投影步骤。
- en: What we learned is just a simple example, consider a case where our target and
    predicted distribution support varies very much. In this case, we cannot manually
    determine the amount of probability we have to distribute across the supports
    to make them equal. So, we introduce a set of steps to perform the projection,
    as the following shows. After performing these steps, our target distribution
    support will match our predicted distribution by distributing the probabilities
    across the support.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所学到的只是一个简单的例子，假设目标分布和预测分布的支持范围差异很大。在这种情况下，我们无法手动确定该将多少概率分配到各个支持范围，使其相等。因此，我们引入了一套步骤来进行投影，具体步骤如下。执行这些步骤后，我们的目标分布支持范围将通过将概率分配到支持范围上，从而与预测分布支持范围匹配。
- en: First, we initialize an array *m* with its shape as the number of support with
    zero values. The *m* denotes the distributed probability of the target distribution
    after the projection step.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个数组*m*，其形状为支持范围的数量，值为零。*m*表示在投影步骤后目标分布的分配概率。
- en: 'For *j* in range of the number of support:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于* j *，遍历支持范围的数量：
- en: 'Compute the target support value: ![](img/B15558_14_045.png)'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支持值：![](img/B15558_14_045.png)
- en: 'Compute the value of *b*: ![](img/B15558_14_046.png)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*b*的值：![](img/B15558_14_046.png)
- en: 'Compute the lower bound and the upper bound: ![](img/B15558_14_047.png)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算下限和上限：![](img/B15558_14_047.png)
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_048.png)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下限上分配概率：![](img/B15558_14_048.png)
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_049.png)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上限上分配概率：![](img/B15558_14_049.png)
- en: Understanding how exactly these projection steps work is a little tricky! So,
    let's understand this by considering the same example we used earlier. Let z =
    [1, 2], *N* = 2, ![](img/B15558_14_050.png), and ![](img/B15558_14_051.png).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些投影步骤如何工作有点棘手！所以，让我们通过考虑之前使用的相同示例来理解这一过程。设 z = [1, 2]，*N* = 2，![](img/B15558_14_050.png)，和
    ![](img/B15558_14_051.png)。
- en: 'Let the probability of predicted distribution be *p* = [0.5, 0.5]. *Figure
    14.15* shows the predicted distribution:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 设预测分布的概率为 *p* = [0.5, 0.5]。*图14.15* 显示了预测分布：
- en: '![](img/B15558_14_15.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_15.png)'
- en: 'Figure 14.15: Predicted distribution'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15：预测分布
- en: 'Let the probability of target distribution be *p* = [0.3, 0.7]. Let the reward
    *r* = 0.1 and the discount factor ![](img/B15558_14_041.png), and we know ![](img/B15558_14_053.png),
    thus, the target distribution becomes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 设目标分布的概率为 *p* = [0.3, 0.7]。设奖励 *r* = 0.1 和折扣因子 ![](img/B15558_14_041.png)，我们知道
    ![](img/B15558_14_053.png)，因此目标分布变为：
- en: '![](img/B15558_14_16.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_16.png)'
- en: 'Figure 14.16: Target distribution'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.16：目标分布
- en: From *Figure 14.16*, we can infer that support in target distribution is different
    from the predicted distribution. Now, we will learn how to perform the projection
    using the preceding steps.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图14.16* 中，我们可以推断目标分布的支持集与预测分布不同。现在，我们将学习如何使用前述步骤进行投影。
- en: First, we initialize an array *m* with its shape as the number of support with
    zero values. Thus, *m* = [0, 0].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个数组 *m*，其形状为支持集的大小，并将其值设置为零。即 *m* = [0, 0]。
- en: '**Iteration, j=0**:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代，j=0**：'
- en: Compute the target support value:![](img/B15558_14_054.png)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支持值：![](img/B15558_14_054.png)
- en: Compute the value of *b*:![](img/B15558_14_055.png)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *b* 的值：![](img/B15558_14_055.png)
- en: Compute the lower and upper bound:![](img/B15558_14_056.png)
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算下界和上界：![](img/B15558_14_056.png)
- en: Distribute the probability on the lower bound:![](img/B15558_14_057.png)![](img/B15558_14_058.png)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下界上分配概率：![](img/B15558_14_057.png)![](img/B15558_14_058.png)
- en: Distribute the probability on the upper bound:![](img/B15558_14_059.png)![](img/B15558_14_060.png)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上界上分配概率：![](img/B15558_14_059.png)![](img/B15558_14_060.png)
- en: After the 1st iteration, the value of *m* becomes [0, 0].
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代后，*m* 的值变为 [0, 0]。
- en: '**Iteration, j=1**:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代，j=1**：'
- en: Compute the target support value:![](img/B15558_14_061.png)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支持值：![](img/B15558_14_061.png)
- en: Compute the value of *b*:![](img/B15558_14_062.png)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *b* 的值：![](img/B15558_14_062.png)
- en: Compute the lower and upper bound of *b*:![](img/B15558_14_063.png)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *b* 的下界和上界：![](img/B15558_14_063.png)
- en: Distribute the probability on the lower bound:![](img/B15558_14_064.png)![](img/B15558_14_065.png)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下界上分配概率：![](img/B15558_14_064.png)![](img/B15558_14_065.png)
- en: Distribute the probability on the upper bound:![](img/B15558_14_066.png)![](img/B15558_14_067.png)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上界上分配概率：![](img/B15558_14_066.png)![](img/B15558_14_067.png)
- en: 'After the second iteration, the value of *m* becomes [0.07, 0,63]. The number
    of iterations = the length of our support. Since the length of our support is
    2, we will stop here and thus the value of *m* becomes our new distributed probability
    for the modified support, as *Figure 14.17* shows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次迭代之后，*m* 的值变为 [0.07, 0.63]。迭代次数等于我们的支持集的长度。由于支持集的长度为 2，我们将在此停止，因此 *m* 的值成为我们为修改后的支持集分配的新概率，如
    *图14.17* 所示：
- en: '![](img/B15558_14_17.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_17.png)'
- en: 'Figure 14.17: Target distribution'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.17：目标分布
- en: 'The following snippet will give us more clarity on how exactly the projection
    step works:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将使我们更清楚投影步骤是如何工作的：
- en: '[PRE1]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we have understood how to compute the target value distribution and
    how we can make the support of target value distribution equal to the support
    of predicted value distribution using the projection step, we will learn how to
    compute the cross entropy loss. Cross entropy loss is given as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何计算目标值分布以及如何通过投影步骤使目标值分布的支持集等于预测值分布的支持集，接下来我们将学习如何计算交叉熵损失。交叉熵损失的计算公式为：
- en: '![](img/B15558_14_068.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_068.png)'
- en: 'Where *y* is the actual value and ![](img/B15558_14_069.png) is the predicted
    value. Thus, we can write:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y* 是实际值，![](img/B15558_14_069.png) 是预测值。因此，我们可以写出：
- en: '![](img/B15558_14_070.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_070.png)'
- en: Where *m* is the target probabilities from the target value distribution and
    *p*(*s*, *a*) is the predicted probabilities from the predicted value distribution.
    We train our network by minimizing the cross entropy loss.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m* 是目标值分布的目标概率，*p*(*s*, *a*) 是预测值分布的预测概率。我们通过最小化交叉熵损失来训练网络。
- en: Thus, using a categorical DQN, we select the action based on the distribution
    of the return (value distribution). In the next section, we will put all these
    concepts together and see how a categorical DQN works.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用分类DQN时，我们根据回报的分布（价值分布）选择动作。在下一节中，我们将把所有这些概念整合在一起，看看分类DQN是如何工作的。
- en: Putting it all together
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容汇总
- en: First, we initialize the main network parameter ![](img/B15558_09_106.png) with
    random values, and we initialize the target network parameter ![](img/B15558_14_039.png)
    by just copying the main network parameter ![](img/B15558_09_054.png). We also
    initialize the replay buffer ![](img/B15558_09_124.png).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将主网络参数![](img/B15558_09_106.png)初始化为随机值，并将目标网络参数![](img/B15558_14_039.png)初始化为仅通过复制主网络参数![](img/B15558_09_054.png)来实现。我们还初始化了重放缓冲区![](img/B15558_09_124.png)。
- en: 'Now, for each step in the episode, we feed the state of the environment and
    support values to the main categorical DQN parameterized by ![](img/B15558_09_098.png).
    The main network takes the support and state of the environment as input and returns
    the probability value for each support. Then the Q value of the value distribution
    can be computed as the sum of support multiplied by their probabilities:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于每个步骤，在该回合中，我们将环境状态和支持值输入主分类DQN，该网络由![](img/B15558_09_098.png)参数化。主网络将支持和环境状态作为输入，并返回每个支持的概率值。然后，可以计算价值分布的Q值，作为支持与其概率的乘积之和：
- en: '![](img/B15558_14_075.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_075.png)'
- en: 'After computing the Q value of all the actions in the state, we select the
    best action in the state *s* as the one that has the maximum Q value:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算状态中所有动作的Q值后，我们选择状态*s*中Q值最大的最佳动作：
- en: '![](img/B15558_14_028.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_028.png)'
- en: However, instead of selecting the action that has the maximum Q value all the
    time, we select the action using the epsilon-greedy policy. With the epsilon-greedy
    policy, we select a random action with probability epsilon and with the probability
    1-epsilon, we select the best action that has the maximum Q value. We perform
    the selected action, move to the next state, obtain the reward, and store this
    transition information ![](img/B15558_14_077.png) in the replay buffer ![](img/B15558_09_124.png).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不是始终选择Q值最大的动作，我们使用epsilon-greedy策略选择动作。在epsilon-greedy策略中，我们以epsilon的概率选择一个随机动作，而以1-epsilon的概率选择Q值最大的最佳动作。我们执行选择的动作，移动到下一个状态，获得奖励，并将此转移信息![](img/B15558_14_077.png)存储在重放缓冲区![](img/B15558_09_124.png)中。
- en: Now, we sample a transition ![](img/B15558_14_077.png) from the replay buffer
    ![](img/B15558_09_124.png) and feed the next state ![](img/B15558_12_376.png)
    and support values to the target categorical DQN parameterized by ![](img/B15558_12_025.png).
    The target network takes the support and next state ![](img/B15558_12_376.png)
    as input and returns the probability value for each support.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从重放缓冲区![](img/B15558_09_124.png)中采样一个转移![](img/B15558_14_077.png)，并将下一个状态![](img/B15558_12_376.png)和支持值输入目标分类DQN，该网络由![](img/B15558_12_025.png)参数化。目标网络将支持和下一个状态![](img/B15558_12_376.png)作为输入，并返回每个支持的概率值。
- en: 'Then the Q value can be computed as the sum of support multiplied by their
    probabilities:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Q值可以作为支持与其概率的乘积之和来计算：
- en: '![](img/B15558_14_082.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_082.png)'
- en: 'After computing the Q value of all next state-action pairs, we select the best
    action in the state ![](img/B15558_14_083.png) as the one that has the maximum
    Q value:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算所有下一个状态-动作对的Q值后，我们选择在状态![](img/B15558_14_083.png)中Q值最大的最佳动作：
- en: '![](img/B15558_14_084.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_084.png)'
- en: Now, we perform the projection step. The *m* denotes the distributed probability
    of the target distribution after the projection step.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们执行投影步骤。*m*表示投影步骤后目标分布的分布概率。
- en: 'For *j* in range of the number of support:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于* j *，在支持数量的范围内：
- en: 'Compute the target support value: ![](img/B15558_14_085.png)'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支持值：![](img/B15558_14_085.png)
- en: 'Compute the value of *b*: ![](img/B15558_14_046.png)'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*b*的值：![](img/B15558_14_046.png)
- en: 'Compute the lower bound and the upper bound: ![](img/B15558_14_087.png)'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算下界和上界：![](img/B15558_14_087.png)
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_088.png)'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下界上分配概率：![](img/B15558_14_088.png)
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_089.png)'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上界上分配概率：![](img/B15558_14_089.png)
- en: 'After performing the projection step, compute the cross entropy loss:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 执行投影步骤后，计算交叉熵损失：
- en: '![](img/B15558_14_090.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_090.png)'
- en: Where *m* is the target probabilities from the target value distribution and
    *p*(*s*, *a*) is the predicted probabilities from the predicted value distribution.
    We train our network by minimizing the cross entropy loss.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*m*是目标值分布的目标概率，*p*(*s*, *a*)是从预测值分布中得到的预测概率。我们通过最小化交叉熵损失来训练网络。
- en: We don't update the target network parameter ![](img/B15558_14_040.png) in every
    time step. We freeze the target network parameter ![](img/B15558_14_040.png) for
    several time steps, and then we copy the main network parameter ![](img/B15558_10_037.png)
    to the target network parameter ![](img/B15558_14_039.png). We keep repeating
    the preceding steps for several episodes to approximate the optimal value distribution.
    To give us a more detailed understanding, the categorical DQN algorithm is given
    in the next section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不会在每个时间步更新目标网络参数！[](img/B15558_14_040.png)。我们会在几个时间步内冻结目标网络参数！[](img/B15558_14_040.png)，然后将主网络参数！[](img/B15558_10_037.png)复制到目标网络参数！[](img/B15558_14_039.png)。我们会在多个回合中不断重复上述步骤，以逼近最优值分布。为了让我们有更详细的理解，接下来会介绍类别化DQN算法。
- en: Algorithm – categorical DQN
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – 类别化DQN
- en: 'The categorical DQN algorithm is given in the following steps:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 类别化DQN算法的步骤如下：
- en: Initialize the main network parameter ![](img/B15558_10_095.png) with random
    values
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化主网络参数！[](img/B15558_10_095.png)
- en: Initialize the target network parameter ![](img/B15558_14_039.png) by copying
    the main network parameter ![](img/B15558_10_037.png)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数！[](img/B15558_10_037.png)来初始化目标网络参数！[](img/B15558_14_039.png)
- en: Initialize the replay buffer ![](img/B15558_14_098.png), the number of support
    (atoms), and also ![](img/B15558_14_022.png) and ![](img/B15558_14_100.png)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区！[](img/B15558_14_098.png)，支撑数量（原子数），以及！[](img/B15558_14_022.png)和！[](img/B15558_14_100.png)
- en: For *N* number of episodes perform *step 5*
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*N*个回合，执行*步骤 5*
- en: 'For each step in the episode, that is, for ![](img/B15558_14_101.png):'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步，也就是，对于！[](img/B15558_14_101.png)：
- en: Feed the state *s* and support values to the main categorical DQN parameterized
    by ![](img/B15558_09_054.png) and get the probability value for each support.
    Then compute the Q value as ![](img/B15558_14_103.png)
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态*s*和支撑值输入到主类别化DQN中，由！[](img/B15558_09_054.png)参数化，得到每个支撑的概率值。然后计算Q值为！[](img/B15558_14_103.png)
- en: After computing the Q value, select an action using the epsilon-greedy policy,
    that is, with the probability epsilon, select random action *a* and with probability
    1-epsilon, select the action as ![](img/B15558_14_028.png)
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算完Q值后，使用epsilon-greedy策略选择动作，即以概率epsilon选择随机动作*a*，以概率1-epsilon选择动作！[](img/B15558_14_028.png)
- en: Perform the selected action and move to the next state ![](img/B15558_14_105.png)
    and obtain the reward *r*
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选择的动作并转移到下一个状态！[](img/B15558_14_105.png)，并获得奖励*r*
- en: Store the transition information in the replay buffer ![](img/B15558_14_098.png)
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转移信息存储在重放缓冲区！[](img/B15558_14_098.png)中
- en: Randomly sample a transition from the replay buffer ![](img/B15558_12_259.png)
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机从重放缓冲区中采样一个转移！[](img/B15558_12_259.png)
- en: Feed the next state ![](img/B15558_12_376.png) and support values to the target
    categorical DQN parameterized by ![](img/B15558_14_040.png) and get the probability
    value for each support. Then compute the value as ![](img/B15558_14_110.png)
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下一个状态！[](img/B15558_12_376.png)和支撑值输入到目标类别化DQN中，由！[](img/B15558_14_040.png)参数化，得到每个支撑的概率值。然后计算该值为！[](img/B15558_14_110.png)
- en: After computing the Q value, we select the best action in the state ![](img/B15558_14_105.png)
    as the one that has the maximum Q value ![](img/B15558_14_112.png)
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算完Q值后，我们选择状态！[](img/B15558_14_105.png)中的最佳动作，即选择具有最大Q值的动作！[](img/B15558_14_112.png)
- en: Initialize the array *m* with zero values with its shape as the number of support
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用零值初始化数组*m*，其形状为支撑的数量
- en: 'For *j* in range of the number of support:'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*j*，遍历支撑的数量：
- en: 'Compute the target support value: ![](img/B15558_14_113.png)'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支撑值：！[](img/B15558_14_113.png)
- en: 'Compute the value of b: ![](img/B15558_14_114.png)'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算b的值：！[](img/B15558_14_114.png)
- en: 'Compute the lower bound and upper bound: ![](img/B15558_14_047.png)'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算下界和上界：！[](img/B15558_14_047.png)
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_116.png)'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下界上分配概率：！[](img/B15558_14_116.png)
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_117.png)'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上界上分配概率：！[](img/B15558_14_117.png)
- en: 'Compute the cross entropy loss: ![](img/B15558_14_118.png)'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算交叉熵损失：！[](img/B15558_14_118.png)
- en: Minimize the loss using gradient descent and update the parameter of the main
    network
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降最小化损失，并更新主网络的参数：
- en: Freeze the target network parameter ![](img/B15558_14_040.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结目标网络参数 ![](img/B15558_14_040.png) 若干时间步长，然后通过简单地复制主网络参数 ![](img/B15558_09_098.png)
    来更新它：
- en: Now that we have learned the categorical DQN algorithm, to understand how a categorical
    DQN works, we will implement it in the next section.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经学习了类别 DQN 算法，为了理解类别 DQN 如何工作，我们将在下一节实现它。
- en: Playing Atari games using a categorical DQN
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用类别 DQN 玩 Atari 游戏：
- en: Let's implement the categorical DQN algorithm to play Atari games. The code
    used in this section is adapted from an open-source categorical DQN implementation,
    [https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo](https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo),
    provided by Prince Wen.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现类别 DQN 算法来玩 Atari 游戏。本节中使用的代码改编自开源的类别 DQN 实现，[https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo](https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo)，由
    Prince Wen 提供。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE2]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Defining the variables
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义变量：
- en: Now, let's define some of the important variables.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一些重要的变量：
- en: 'Initialize the ![](img/B15558_14_022.png) and ![](img/B15558_14_012.png):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 ![](img/B15558_14_022.png) 和 ![](img/B15558_14_012.png)：
- en: '[PRE3]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize the number of atoms (supports):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化原子数（支持）：
- en: '[PRE4]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set the discount factor, ![](img/B15558_03_190.png):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 设置折扣因子，![](img/B15558_03_190.png)：
- en: '[PRE5]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Set the batch size:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 设置批处理大小：
- en: '[PRE6]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Set the time step at which we want to update the target network:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 设置我们希望更新目标网络的时间步长：
- en: '[PRE7]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set the epsilon value that is used in the epsilon-greedy policy:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 设置在 epsilon-贪婪策略中使用的 epsilon 值：
- en: '[PRE8]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining the replay buffer
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义重放缓冲区：
- en: 'First, let''s define the buffer length:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义缓冲区长度：
- en: '[PRE9]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the replay buffer as a deque structure:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 将重放缓冲区定义为 deque 结构：
- en: '[PRE10]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We define a function called `sample_transitions` that returns the randomly
    sampled minibatch of transitions from the replay buffer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为 `sample_transitions` 的函数，它返回从重放缓冲区中随机采样的小批量转换：
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Defining the categorical DQN class
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义类别 DQN 类：
- en: Let's define a class called `Categorical_DQN` where we will implement the categorical
    DQN algorithm. Instead of looking into the whole code at once, we will look into
    only the important parts. The complete code used in this section is available
    in the GitHub repo of the book.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为 `Categorical_DQN` 的类，我们将在其中实现类别 DQN 算法。我们不会一次性查看整个代码，而是只查看重要部分。本节中使用的完整代码可以在本书的
    GitHub 仓库中找到。
- en: 'For a clear understanding, let''s take a look into the code line by line:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解，我们逐行查看代码：
- en: '[PRE12]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Defining the init method
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义初始化方法：
- en: 'First, let''s define the init method:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义初始化方法：
- en: '[PRE13]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Start the TensorFlow session:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 会话：
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Initialize the ![](img/B15558_14_022.png) and ![](img/B15558_14_012.png):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 ![](img/B15558_14_022.png) 和 ![](img/B15558_14_012.png)：
- en: '[PRE15]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Initialize the number of atoms:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化原子数：
- en: '[PRE16]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Initialize the epsilon value:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 epsilon 值：
- en: '[PRE17]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Get the state shape of the environment:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的状态形状：
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Get the action shape of the environment:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的动作形状：
- en: '[PRE19]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize the time step:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化时间步长：
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Initialize the target state shape:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化目标状态形状：
- en: '[PRE21]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the placeholder for the state:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态的占位符：
- en: '[PRE22]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the placeholder for the action:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 定义动作的占位符：
- en: '[PRE23]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the placeholder for the *m* value (the distributed probability of the
    target distribution):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 *m* 值的占位符（目标分布的分布式概率）：
- en: '[PRE24]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Compute the value of ![](img/B15558_14_015.png) as ![](img/B15558_14_127.png):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 ![](img/B15558_14_015.png) 的值，作为 ![](img/B15558_14_127.png)：
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Compute the support values as ![](img/B15558_14_025.png):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 计算支持值，作为 ![](img/B15558_14_025.png)：
- en: '[PRE26]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Build the categorical DQN:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 构建类别 DQN：
- en: '[PRE27]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有 TensorFlow 变量：
- en: '[PRE28]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Building the categorical DQN
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建类别 DQN：
- en: 'Let''s define a function called `build_network` for building a deep network.
    Since we are dealing with Atari games, we use the convolutional neural network:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为 `build_network` 的函数，用于构建深度网络。由于我们处理的是 Atari 游戏，我们使用卷积神经网络：
- en: '[PRE29]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the first convolutional layer:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一个卷积层：
- en: '[PRE30]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the second convolutional layer:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二卷积层：
- en: '[PRE31]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Flatten the feature maps obtained as a result of the second convolutional layer:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 将第二卷积层得到的特征图展平：
- en: '[PRE32]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the first dense layer:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一个全连接层：
- en: '[PRE33]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the second dense layer:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二个全连接层：
- en: '[PRE34]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Concatenate the second dense layer with the action:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将第二个全连接层与动作连接：
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the third layer and apply the softmax function to the result of the
    third layer and obtain the probabilities for each of the atoms:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第三层，并对第三层的结果应用 softmax 函数，获得每个原子的概率：
- en: '[PRE36]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s define a function called `build_categorical_DQN` for building the
    main and target categorical DQNs:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个名为 `build_categorical_DQN` 的函数，用于构建主类别和目标类别 DQN：
- en: '[PRE37]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define the main categorical DQN and obtain the probabilities:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 定义主类别 DQN 并获取概率：
- en: '[PRE38]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the target categorical DQN and obtain the probabilities:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标类别 DQN 并获取概率：
- en: '[PRE39]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Compute the main Q value with the probabilities obtained from the main categorical
    DQN as ![](img/B15558_14_129.png):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从主类别 DQN 获得的概率计算主要 Q 值，如 ![](img/B15558_14_129.png) 所示：
- en: '[PRE40]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Similarly, compute the target Q value with probabilities obtained from the
    target categorical DQN as ![](img/B15558_14_130.png):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，使用从目标类别 DQN 获得的概率计算目标 Q 值，如 ![](img/B15558_14_130.png) 所示：
- en: '[PRE41]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the cross entropy loss as ![](img/B15558_14_131.png):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 定义交叉熵损失，如 ![](img/B15558_14_131.png) 所示：
- en: '[PRE42]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the optimizer and minimize the cross entropy loss using the Adam optimizer:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 定义优化器，并使用 Adam 优化器最小化交叉熵损失：
- en: '[PRE43]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Get the main network parameters:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 获取主网络参数：
- en: '[PRE44]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Get the target network parameters:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标网络参数：
- en: '[PRE45]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define the `update_target_net` operation for updating the target network parameters
    by copying the parameters of the main network:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 `update_target_net` 操作，通过复制主网络的参数来更新目标网络参数：
- en: '[PRE46]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Defining the train function
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义训练函数：
- en: 'Let''s define a function called `train` to train the network:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为 `train` 的函数来训练网络：
- en: '[PRE47]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Increment the time step:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 增加时间步数：
- en: '[PRE48]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Get the target Q values:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标 Q 值：
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Select the next state action ![](img/B15558_14_132.png) as the one that has
    the maximum Q value:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 选择下一个状态的动作 ![](img/B15558_14_132.png)，该动作具有最大 Q 值：
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Initialize an array *m* with its shape as the number of support with zero values.
    The *m* denotes the distributed probability of the target distribution after the
    projection step:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个数组 *m*，其形状为支持的数量，并将其值设为零。*m* 表示在投影步骤后目标分布的分布概率：
- en: '[PRE51]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Get the probability for each atom using the target categorical DQN:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标类别 DQN 获取每个原子的概率：
- en: '[PRE52]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Perform the projection step:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 执行投影步骤：
- en: '[PRE53]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Train the network by minimizing the loss:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化损失来训练网络：
- en: '[PRE54]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Update the target network parameters by copying the main network parameters:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 通过复制主网络参数来更新目标网络参数：
- en: '[PRE55]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Selecting the action
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择动作：
- en: 'Let''s define a function called `select_action` for selecting the action:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为 `select_action` 的函数，用于选择动作：
- en: '[PRE56]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We generate a random number, and if the number is less than epsilon we select
    the random action, else we select the action that has the maximum Q value:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成一个随机数，如果该数字小于 epsilon，则选择随机动作，否则选择具有最大 Q 值的动作：
- en: '[PRE57]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Training the network
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练网络：
- en: 'Now, let''s start training the network. First, create the Atari game environment
    using `gym`. Let''s create a Tennis game environment:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练网络。首先，使用 `gym` 创建 Atari 游戏环境。让我们创建一个网球游戏环境：
- en: '[PRE58]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Create an object to our `Categorical_DQN` class:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `Categorical_DQN` 类的对象：
- en: '[PRE59]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Set the number of episodes:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数：
- en: '[PRE60]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For each episode:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 对每一回合：
- en: '[PRE61]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Set `done` to `False`:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `done` 设置为 `False`：
- en: '[PRE62]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Initialize the return:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化回报：
- en: '[PRE63]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE64]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'While the episode is not over:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 当回合尚未结束时：
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Render the environment:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Select an action:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个动作：
- en: '[PRE67]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Perform the selected action:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选定的动作：
- en: '[PRE68]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Update the return:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 更新回报：
- en: '[PRE69]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Store the transition information in the replay buffer:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 将过渡信息存储在回放缓冲区中：
- en: '[PRE70]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'If the length of the replay buffer is greater than or equal to the buffer size
    then start training the network by sampling transitions from the replay buffer:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回放缓冲区的长度大于或等于缓冲区大小，则开始通过从回放缓冲区中采样过渡来训练网络：
- en: '[PRE71]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Update the state to the next state:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态更新为下一个状态：
- en: '[PRE72]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Print the return obtained in the episode:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 打印回合中获得的回报：
- en: '[PRE73]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now that we have learned how a categorical DQN works and how to implement it,
    in the next section, we will learn about another interesting algorithm.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了类别 DQN 的工作原理及其实现方法，在接下来的章节中，我们将学习另一个有趣的算法。
- en: Quantile Regression DQN
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分位回归 DQN：
- en: In this section, we will look into another interesting distributional RL algorithm
    called QR-DQN. It is a distributional DQN algorithm similar to the categorical
    DQN; however, it has several features that make it more advantageous than a categorical
    DQN.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究另一种有趣的分布式强化学习算法，称为 QR-DQN。它是一种与类别 DQN 相似的分布式 DQN 算法；然而，它具有一些使其优于类别
    DQN 的特性。
- en: Math essentials
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学基础
- en: 'Before going ahead, let''s recap two important concepts that we use in QR-DQN:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们回顾一下在 QR-DQN 中使用的两个重要概念：
- en: '**Quantile**'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分位数**'
- en: '**Inverse cumulative distribution function** (**Inverse CDF**)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向累积分布函数**（**Inverse CDF**）'
- en: Quantile
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分位数
- en: 'When we divide our distribution into equal areas of probability, they are called
    quantiles. For instance, as *Figure 14.18* shows, we have divided our distribution
    into two equal areas of probabilities and we have two quantiles with 50% probability
    each:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将分布划分为均等的概率区域时，它们被称为分位数。例如，如*图 14.18*所示，我们将分布划分为两个均等的概率区域，并且我们有两个分位数，每个分位数的概率为
    50%：
- en: '![](img/B15558_14_18.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_18.png)'
- en: 'Figure 14.18: 2-quantile plot'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.18：2-分位数图
- en: Inverse CDF (quantile function)
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向 CDF（分位函数）
- en: To understand an **inverse cumulative distribution function** (**inverse CDF**),
    first, let's learn what a **cumulative distribution function** (**CDF**) is.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 **反向累积分布函数**（**Inverse CDF**），首先，让我们了解什么是 **累积分布函数**（**CDF**）。
- en: 'Consider a random variable *X*, and *P*(*X*) denotes the probability distribution
    of *X*. Then the cumulative distribution function is expressed as:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个随机变量 *X*，并且 *P*(*X*) 表示 *X* 的概率分布。那么，累积分布函数可以表示为：
- en: '![](img/B15558_14_133.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_133.png)'
- en: It basically implies that *F*(*x*) can be obtained by adding up all the probabilities
    that are less than or equal to *x*.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上意味着 *F*(*x*) 可以通过将所有小于或等于 *x* 的概率相加得到。
- en: 'Let''s look at the following CDF:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下面的 CDF：
- en: '![](img/B15558_14_19.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_19.png)'
- en: 'Figure 14.19: CDF'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.19：CDF
- en: In the preceding plot, ![](img/B15558_14_134.png) represents the cumulative
    probability, that is, ![](img/B15558_14_135.png). Say *i* =1, then ![](img/B15558_14_136.png).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，![](img/B15558_14_134.png) 表示累积概率，即 ![](img/B15558_14_135.png)。假设 *i*
    = 1，那么 ![](img/B15558_14_136.png)。
- en: 'The CDF takes *x* as an input and returns the cumulative probability ![](img/B15558_14_137.png).
    Hence, we can write:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: CDF 以 *x* 作为输入，返回累积概率 ![](img/B15558_14_137.png)。因此，我们可以写为：
- en: '![](img/B15558_14_138.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_138.png)'
- en: Say *x* = 2, then we get ![](img/B15558_14_139.png).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *x* = 2，那么我们得到 ![](img/B15558_14_139.png)。
- en: 'Now, we will look at the inverse CDF. Inverse CDF, as the name suggests, is
    the inverse of the CDF. That is, in CDF, given the support *x*, we obtain the
    cumulative probability ![](img/B15558_14_140.png), whereas in inverse CDF, given
    the cumulative probability ![](img/B15558_10_026.png), we obtain the support *x*.
    Inverse CDF can be expressed as:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看看反向 CDF。顾名思义，反向 CDF 是 CDF 的逆函数。也就是说，在 CDF 中，给定支持 *x*，我们可以得到累积概率 ![](img/B15558_14_140.png)，而在反向
    CDF 中，给定累积概率 ![](img/B15558_10_026.png)，我们可以得到支持 *x*。反向 CDF 可以表示为：
- en: '![](img/B15558_14_142.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_142.png)'
- en: 'The following plot shows the inverse CDF:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了反向 CDF：
- en: '![](img/B15558_14_20.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_20.png)'
- en: 'Figure 14.20: Inverse CDF'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.20：反向 CDF
- en: As shown in *Figure 14.20*, given the cumulative probability ![](img/B15558_14_143.png),
    we obtain the support *x*.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 14.20*所示，给定累积概率 ![](img/B15558_14_143.png)，我们可以得到支持 *x*。
- en: Say ![](img/B15558_14_144.png), then we get *x* = 2.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/B15558_14_144.png)，那么我们得到 *x* = 2。
- en: We have learned that the quantiles are equally divided probabilities. As *Figure
    14.20* shows, we have three quantiles *q*[1] to *q*[3] with equally divided probabilities
    and the quantile values are [0.3,0.6,1.0], which are just our cumulative probabilities.
    Hence, we can say that the inverse CDF (quantile function) helps us to obtain
    the value of support given the equally divided probabilities. Note that in inverse
    CDF, the support should always be increasing as it is based on the cumulative
    probability.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学过分位数是均匀分布的概率。如*图 14.20*所示，我们有三个分位数 *q*[1] 到 *q*[3]，它们的概率均等，分位数的值分别为 [0.3,0.6,1.0]，这些正是我们的累积概率。因此，我们可以说，反向
    CDF（分位函数）帮助我们在给定均等分布概率时，得到支持 *x* 的值。请注意，在反向 CDF 中，支持应该是递增的，因为它是基于累积概率的。
- en: Now that we have learned what the quantile function is, we will gain an understanding
    of how we can make use of the quantile function in the distributional RL setting
    using an algorithm called QR-DQN.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了分位数函数，我们将进一步了解如何使用名为 QR-DQN 的算法在分布式强化学习中利用分位数函数。
- en: Understanding QR-DQN
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 QR-DQN
- en: In categorical DQN (C51), we learned that in order to predict the value distribution,
    the network takes the support of the distribution as input and returns the probabilities.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在类别 DQN（C51）中，我们了解到，为了预测价值分布，网络将分布的支持集作为输入并返回概率值。
- en: To compute the support, we also need to decide the number of support *N*, the
    minimum value of support ![](img/B15558_14_022.png), and the maximum value of
    support ![](img/B15558_14_146.png).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算支持集，我们还需要决定支持集的数量 *N*、支持集的最小值 ![](img/B15558_14_022.png) 和支持集的最大值 ![](img/B15558_14_146.png)。
- en: 'If you recollect in C51, our support values are equally spaced at fixed locations
    ![](img/B15558_14_147.png) and we feed this equally spaced support as input and
    obtained the non-uniform probabilities ![](img/B15558_14_148.png). As *Figure
    14.21* shows, in C51, we feed the equally spaced support ![](img/B15558_14_149.png)
    as input to the network along with the state(s) and obtain the non-uniform probabilities
    ![](img/B15558_14_150.png) as output:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得在 C51 中，我们的支持值是等间隔固定位置的 ![](img/B15558_14_147.png)，我们将这个等间隔的支持集作为输入，并得到非均匀概率
    ![](img/B15558_14_148.png)。正如 *图 14.21* 所示，在 C51 中，我们将等间隔的支持集 ![](img/B15558_14_149.png)
    与状态(s)一起输入到网络，输出非均匀概率 ![](img/B15558_14_150.png)：
- en: '![](img/B15558_14_21.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_21.png)'
- en: 'Figure 14.21: Categorical DQN'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.21：类别 DQN
- en: 'QR-DQN can be viewed just as the opposite of C51\. In QR-DQN, to estimate the
    value distribution, we feed the uniform probabilities ![](img/B15558_14_151.png)
    and the network outputs the supports at variable locations ![](img/B15558_14_152.png).
    As shown in the following figure, we feed the uniform probabilities ![](img/B15558_14_153.png)
    as input to the network along with the state(s) and obtain the support ![](img/B15558_14_154.png)
    placed at variable locations as output:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: QR-DQN 可以看作是 C51 的反向操作。在 QR-DQN 中，为了估算价值分布，我们输入均匀概率 ![](img/B15558_14_151.png)，网络输出在不同位置（不等间隔位置）的支持集
    ![](img/B15558_14_152.png)。如以下图所示，我们将均匀概率 ![](img/B15558_14_153.png) 与状态(s)一起输入到网络，输出位于不同位置的支持集
    ![](img/B15558_14_154.png)：
- en: '![](img/B15558_14_22.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_22.png)'
- en: 'Figure 14.22: QR-DQN'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.22：QR-DQN
- en: Thus, from the two preceding figures we can observe that, in a categorical DQN,
    along with the state, we feed the fixed support at equally spaced intervals as
    input to the network and it returns the non-uniform probabilities, whereas in
    a QR-DQN, along with the state, we feed the fixed uniform probabilities as input
    to the network and it returns the support at variable locations (unequally spaced
    support).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从前两幅图中我们可以观察到，在类别 DQN 中，我们将等间距的固定支持集与状态一起输入到网络，网络返回非均匀概率；而在 QR-DQN 中，我们将均匀的概率与状态一起输入到网络，网络返回在不同位置（不等间隔位置）的支持集。
- en: Okay, but what's the use of this? How does a QR-DQN work exactly? Let's explore
    this in detail.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但这有什么用呢？QR-DQN 究竟是如何工作的？我们来详细探讨一下。
- en: We understood that a QR-DQN takes the uniform probabilities as input and returns
    the support values for estimating the value distribution. Can we make use of the
    quantile function to estimate the value distribution? Yes! We learned that the
    quantile function helps us to obtain the values of support given the equally divided
    probabilities. Thus, in QR-DQN, we estimate the value distribution by estimating
    the quantile function.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解到，QR-DQN 以均匀概率作为输入，并返回支持值，用于估算价值分布。我们能否利用分位数函数来估算价值分布呢？是的！我们了解到，分位数函数帮助我们根据等分概率获得支持值。因此，在
    QR-DQN 中，我们通过估算分位数函数来估算价值分布。
- en: 'The quantile function is given as:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数函数表示为：
- en: '![](img/B15558_14_155.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_155.png)'
- en: Where *z* is the support and ![](img/B15558_14_143.png) is the equally divided
    cumulative probability. Thus, we can obtain the support *z* given ![](img/B15558_14_157.png).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *z* 是支持集，![](img/B15558_14_143.png) 是等分的累积概率。因此，我们可以通过给定 ![](img/B15558_14_157.png)
    来获得支持集 *z*。
- en: 'Let *N* be the number of quantiles, then the probability can be obtained as:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *N* 为分位数的数量，则概率可以表示为：
- en: '![](img/B15558_14_158.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_158.png)'
- en: For example, if *N* = 4, then *p* = [0.25, 0.25\. 0.25, 0.25]. If *N* = 5, then
    p = [0.20, 0.20, 0.20, 0.20, 0.20].
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 *N* = 4，则 *p* = [0.25, 0.25, 0.25, 0.25]。如果 *N* = 5，则 p = [0.20, 0.20,
    0.20, 0.20, 0.20]。
- en: 'Once we decide the number of quantiles *N*, the cumulative probabilities ![](img/B15558_14_143.png)
    (quantile values) can be obtained as:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定了量化数 *N*，累积概率 ![](img/B15558_14_143.png)（量化值）可以通过以下方式获得：
- en: '![](img/B15558_14_160.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_160.png)'
- en: For example, if *N* = 4, then ![](img/B15558_14_161.png). If *N* = 5, then![](img/B15558_14_162.png).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 *N* = 4，则 ![](img/B15558_14_161.png)。如果 *N* = 5，则 ![](img/B15558_14_162.png)。
- en: We just feed this equally divided cumulative probability ![](img/B15558_14_163.png)
    (quantile values) as input to the QR-DQN and it returns the support value. That
    is, we have learned that the QR-DQN estimates the value distribution as the quantile
    function, so we just feed the ![](img/B15558_14_164.png) and obtain the support
    values *z* of the value distribution.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将这个均分的累积概率 ![](img/B15558_14_163.png)（量化值）作为输入馈送给 QR-DQN，它返回支持值。也就是说，我们已经知道
    QR-DQN 将值分布估计为量化函数，因此我们只需输入 ![](img/B15558_14_164.png) 就能获得值分布的支持值 *z*。
- en: 'Let''s understand this with a simple example. Say we are in a state *s* and
    we have two possible actions *up* and *down* to perform in the state. As shown
    in the following figure, along with giving the state *s* as input to the network,
    we also feed the quantile value ![](img/B15558_14_165.png), which is just the
    equally divided cumulative probability. Then our network returns the support for
    the distribution of action *up* and the distribution of action *down*:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个简单的例子来理解这个问题。假设我们处于状态 *s*，并且在该状态下有两个可能的动作 *上* 和 *下*。如图所示，除了将状态 *s* 作为输入馈送给网络外，我们还将量化值
    ![](img/B15558_14_165.png) 作为输入，量化值就是均分的累积概率。然后，我们的网络返回 *上* 动作的分布支持和 *下* 动作的分布支持：
- en: '![](img/B15558_14_23.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_23.png)'
- en: 'Figure 14.23: QR-DQN'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.23：QR-DQN
- en: If you recollect, in C51, we computed the probability *p*(*s*, *a*) for the
    given state and action, whereas here in QR-DQN, we compute the support *z*(*s*,
    *a*) for the given state and action.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回想一下，在 C51 中，我们计算了给定状态和动作的概率 *p*(*s*, *a*)，而在 QR-DQN 中，我们计算了给定状态和动作的支持 *z*(*s*,
    *a*)。
- en: Note that we use capital *Z*(*s*, *a*) to represent the value distribution and
    small *z*(*s*, *a*) to represent the support of the distribution.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用大写的 *Z*(*s*, *a*) 来表示值分布，小写的 *z*(*s*, *a*) 来表示分布的支持。
- en: Similarly, we can also compute the target value distribution using the quantile
    function. Then we train our network by minimizing the distance between the predicted
    quantile and the target quantile distribution.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以使用量化函数来计算目标值分布。然后，我们通过最小化预测量化分布与目标量化分布之间的距离来训练我们的网络。
- en: 'Still, the fundamental question is why are we doing this? How it is more beneficial
    than C51? There are several advantages of quantile regression DQN over categorical
    DQN. In quantile regression DQN:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 但根本问题是，我们为什么要这样做？它比 C51 更有利吗？量化回归 DQN 相比于类别 DQN 有几个优点。在量化回归 DQN 中：
- en: We don't have to choose the number of supports and the bounds of support, which
    is ![](img/B15558_14_009.png) and ![](img/B15558_14_167.png).
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要选择支持的数量和支持的界限，分别是 ![](img/B15558_14_009.png) 和 ![](img/B15558_14_167.png)。
- en: There are no limitations on the bounds of support, thus the range of returns
    can vary across states.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的界限没有限制，因此回报的范围可以在不同状态间变化。
- en: We can also get rid of the projection step that we performed in the C51 to match
    the supports of the target and predicted distribution.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以摆脱在 C51 中进行的投影步骤，这一步是为了匹配目标分布和预测分布的支持。
- en: One more important advantage of a QR-DQN is that it minimizes the p-Wasserstein
    distance between the predicted and target distribution. But why is this important?
    Minimizing the Wasserstein distance between the target and predicted distribution
    helps us in attaining convergence better than minimizing the cross entropy.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: QR-DQN 的另一个重要优点是它最小化了预测分布和目标分布之间的 p-Wasserstein 距离。但为什么这很重要呢？最小化目标和预测分布之间的 Wasserstein
    距离，帮助我们比最小化交叉熵更好地达到收敛。
- en: 'Okay, what exactly is the p-Wasserstein distance? The p-Wasserstein distance,
    *W*[p], is characterized as the *L*^p metric on inverse CDF. Say we have two distributions
    *U* and *V*, then the p-Wasserstein metric between these two distributions is
    given as:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，p-Wasserstein 距离到底是什么？p-Wasserstein 距离，*W*[p]，是逆 CDF 上的 *L*^p 度量。假设我们有两个分布
    *U* 和 *V*，则这两个分布之间的 p-Wasserstein 度量为：
- en: '![](img/B15558_14_168.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_168.png)'
- en: Where ![](img/B15558_14_169.png) and ![](img/B15558_14_170.png) denote the inverse
    CDF of the distributions *U* and *V* respectively. Thus, minimizing the distance
    between two inverse CDFs implies that we minimize the Wasserstein distance.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B15558_14_169.png) 和 ![](img/B15558_14_170.png) 分别表示分布 *U* 和 *V*
    的逆 CDF。因此，最小化两个逆 CDF 之间的距离意味着我们最小化 Wasserstein 距离。
- en: We learned that in QR-DQN, we train the network by minimizing the distance between
    the predicted and target distribution, and both of them are quantile functions
    (inverse CDF). Thus, minimizing the distance between the predicted and target
    distribution (inverse CDFs) implies that we minimize the Wasserstein distance.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在 QR-DQN 中，我们通过最小化预测分布和目标分布之间的距离来训练网络，而这两者都是分位数函数（逆 CDF）。因此，最小化预测分布和目标分布（逆
    CDF）之间的距离意味着我们最小化 Wasserstein 距离。
- en: 'The authors of the QR-DQN paper (see the *Further reading* section for more
    details) also highlighted that instead of computing the support for the quantile
    values ![](img/B15558_14_157.png), they suggest using the quantile midpoint values
    ![](img/B15558_14_172.png). The quantile midpoint can be computed as:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: QR-DQN 论文的作者（详见*进一步阅读*部分）还指出，与其计算分位数值 ![](img/B15558_14_157.png) 的支持值，他们建议使用分位数中点值
    ![](img/B15558_14_172.png)。分位数中点可以计算为：
- en: '![](img/B15558_14_173.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_173.png)'
- en: That is, the value of the support *z* can be obtained using quantile midpoint
    values as ![](img/B15558_14_174.png) instead of obtaining support using the quantile
    values as ![](img/B15558_14_175.png).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 即，可以使用分位数中点值来获得支持* z *，如 ![](img/B15558_14_174.png)，而不是使用分位数值来获得支持，如 ![](img/B15558_14_175.png)。
- en: 'But why the quantile midpoint? The quantile midpoint acts as a unique minimizer,
    that is, the Wasserstein distance between two inverse CDFs will be less when we
    use quantile midpoint values ![](img/B15558_14_176.png) instead of quantile values
    ![](img/B15558_14_157.png). Since we are trying to minimize the Wasserstein distance
    between the target and predicted distribution, we can use quantile midpoints ![](img/B15558_14_178.png)
    so that the distance between them will be less. For instance, as *Figure 14.24*
    shows, the Wasserstein distance is less when we use the quantile midpoint values
    ![](img/B15558_14_179.png) instead of quantile values ![](img/B15558_14_164.png):'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么使用分位数中点呢？分位数中点作为一个独特的最小化值，即，当我们使用分位数中点值 ![](img/B15558_14_176.png) 而不是分位数值
    ![](img/B15558_14_157.png) 时，两个逆 CDF 之间的 Wasserstein 距离会更小。由于我们正在尝试最小化目标分布和预测分布之间的
    Wasserstein 距离，我们可以使用分位数中点 ![](img/B15558_14_178.png)，使它们之间的距离更小。例如，如 *图 14.24*
    所示，当我们使用分位数中点值 ![](img/B15558_14_179.png) 而不是分位数值 ![](img/B15558_14_164.png) 时，Wasserstein
    距离会更小：
- en: '![](img/B15558_14_24.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_24.png)'
- en: 'Figure 14.24: Using quantile midpoint values instead of quantile values'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.24：使用分位数中点值代替分位数值
- en: Source ([https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf))
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 ([https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf))
- en: In a nutshell, in QR-DQNs, we compute the value distribution as a quantile function.
    So, we just feed the cumulative probabilities that are equally divided probabilities
    into the network and obtain the support values of the distribution and we train
    the network by minimizing the Wasserstein distance between the target and predicted
    distribution.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在 QR-DQN 中，我们将值分布计算为分位数函数。因此，我们只需将均等分配的累积概率输入网络，并获得分布的支持值，通过最小化目标分布和预测分布之间的
    Wasserstein 距离来训练网络。
- en: Action selection
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作选择
- en: Action selection in QR-DQN is just the same as in C51\. First, we extract Q
    value from the predicted value distribution and then we select the action as the
    one that has the maximum Q value. We can extract the Q value by just taking the
    expectation of the value distribution. The expectation of distribution is given
    as a sum of support multiplied by their corresponding probability.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在 QR-DQN 中，动作选择与 C51 相同。首先，我们从预测的值分布中提取 Q 值，然后选择具有最大 Q 值的动作。我们可以通过对值分布进行期望操作来提取
    Q 值。分布的期望值为支持值与其对应概率的乘积之和。
- en: 'In C51, we computed the Q value as:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C51 中，我们计算 Q 值的方法是：
- en: '![](img/B15558_14_027.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_027.png)'
- en: Where *p*[i](*s*, *a*) is the probability given by the network for state *s*
    and action *a* and *z*[i] is the support.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*p*[i](*s*, *a*) 是网络为状态 *s* 和动作 *a* 给出的概率，*z*[i] 是支持。
- en: 'Whereas in a QR-DQN, our network outputs the support instead of the probability.
    So, the Q value in the QR-DQN can be computed as:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 而在QR-DQN中，我们的网络输出的是支撑而不是概率。所以，QR-DQN中的Q值可以计算为：
- en: '![](img/B15558_14_182.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_182.png)'
- en: Where *z*[i](*s*, *a*) is the support given by the network for state *s* and
    action *a* and *p*[i] is the probability.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*z*[i](*s*, *a*)是网络为状态*s*和动作*a*提供的支撑，*p*[i]是概率。
- en: 'After computing the Q value, we select the action that has the maximum Q value.
    For instance, let''s say, we have a state *s* and two actions in the state, let
    them be *up* and *down*.The Q value for action *up* in the state *s* is computed
    as:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完Q值后，我们选择具有最大Q值的动作。例如，假设我们有一个状态*s*，并且在该状态下有两个动作，分别是*up*和*down*。在状态*s*下，动作*up*的Q值计算为：
- en: '![](img/B15558_14_183.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_183.png)'
- en: 'The Q value for action *down* in the state *s* is computed as:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态*s*下，动作*down*的Q值计算为：
- en: '![](img/B15558_14_184.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_184.png)'
- en: 'After computing the Q value, we select the optimal action as the one that has
    the maximum Q value:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完Q值后，我们选择具有最大Q值的最优动作：
- en: '![](img/B15558_14_185.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_185.png)'
- en: Now that we have learned how to select actions in QR-DQN, in the next section,
    we will look into the loss function of QR-DQN.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在QR-DQN中选择动作，在接下来的部分，我们将探讨QR-DQN的损失函数。
- en: Loss function
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: In C51, we used cross entropy loss as our loss function because our network
    predicts the probability of the value distribution. So we used the cross entropy
    loss to minimize the probabilities between the target and predicted distribution.
    But in QR-DQN, we predict the support of the distribution instead of the probabilities.
    That is, in QR-DQN, we feed the probabilities as input and predict the support
    as output. So, how can we define the loss function for a QR-DQN?
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在C51中，我们使用交叉熵损失作为我们的损失函数，因为我们的网络预测的是值分布的概率。因此，我们使用交叉熵损失来最小化目标分布和预测分布之间的概率。但在QR-DQN中，我们预测的是分布的支撑，而不是概率。也就是说，在QR-DQN中，我们将概率作为输入，并预测支撑作为输出。那么，我们如何定义QR-DQN的损失函数呢？
- en: We can use the quantile regression loss to minimize the distance between the
    target support and the predicted support. But first, let's understand how to calculate
    the target support value.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用分位回归损失来最小化目标支撑和预测支撑之间的距离。但首先，让我们理解如何计算目标支撑值。
- en: 'Before going ahead, let''s recall how we compute the target value in a DQN.
    In DQN, we use the Bellman equation and compute the target value as:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们回顾一下如何在DQN中计算目标值。在DQN中，我们使用贝尔曼方程并计算目标值为：
- en: '![](img/B15558_14_186.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_186.png)'
- en: In the preceding equation, we select action ![](img/B15558_14_187.png) by taking
    the maximum Q value over all possible next state-action pairs.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们通过对所有可能的下一个状态-动作对取最大Q值来选择动作！[](img/B15558_14_187.png)。
- en: 'Similarly, in QR-DQN, to compute the target value, we can use the distributional
    Bellman equation. The distributional Bellman equation can be given as:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在QR-DQN中，计算目标值时，我们可以使用分布式贝尔曼方程。分布式贝尔曼方程可以表示为：
- en: '![](img/B15558_14_188.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_188.png)'
- en: 'So, the target support *z*[j] can be computed as:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，目标支撑*z*[j]可以计算为：
- en: '![](img/B15558_14_189.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_189.png)'
- en: 'To compute support *z*[j] for the state ![](img/B15558_14_036.png), we also
    need to select some action ![](img/B15558_14_132.png). How can we select an action?
    We just compute the return distribution of all next state-action pairs using the
    target network and select the action ![](img/B15558_14_192.png) that has the maximum
    Q value:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算状态！[](img/B15558_14_036.png)的支撑*z*[j]，我们还需要选择某个动作！[](img/B15558_14_132.png)。我们如何选择一个动作？我们只需使用目标网络计算所有下一个状态-动作对的回报分布，并选择具有最大Q值的动作！[](img/B15558_14_192.png)：
- en: '![](img/B15558_14_193.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_193.png)'
- en: Now that we have learned how to compute the target support value, let's see
    how to compute the quantile regression loss. The advantage of using the quantile
    regression loss is that it adds a penalty to the overestimation and underestimation
    error. Let's understand this with an example.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何计算目标支撑值，让我们来看一下如何计算分位回归损失。使用分位回归损失的优点在于它对过高估计和过低估计的误差添加了惩罚。让我们通过一个例子来理解这一点。
- en: Let's say the target support value is [1, 5, 10, 15, 20] and the predicted support
    value is [100, 5, 10, 15, 20]. As we can see, our predicted support has a very
    high value in the initial quantile and then it is decreasing. In the inverse CDF
    section, we learned that support should always be increasing as it is based on
    the cumulative probability. But if you look at the predicted values the support
    starts from 100 and then decreases.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 假设目标支撑值为[1, 5, 10, 15, 20]，而预测支撑值为[100, 5, 10, 15, 20]。如我们所见，预测的支撑值在初始量化时有一个非常高的值，然后开始递减。在逆CDF部分，我们学到支撑值应该始终递增，因为它是基于累积概率的。但是，如果你看预测值，支撑值从100开始，然后递减。
- en: Let's consider another case. Suppose the target support value is [1, 5, 10,
    15, 20] and the predicted support value is [1, 5, 10, 15, 4]. As we can see, our
    predicted support value is increasing from the initial quantile and then it is
    decreasing to 4 in the final quantile. But this should not happen. Since we are
    using inverse CDF, our support values should always be increasing.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一种情况。假设目标支撑值为[1, 5, 10, 15, 20]，而预测支撑值为[1, 5, 10, 15, 4]。如我们所见，预测的支撑值从初始量化值开始增加，然后在最后一个量化值时减少到4。但这种情况不应该发生。因为我们使用的是逆CDF，我们的支撑值应该始终是递增的。
- en: Thus, we need to make sure that our support should be increasing and not decreasing.
    So, if the initial quantile values are overestimated with high values and if the
    later quantile values are underestimated with low values, we can penalize them.
    That is, we multiply the overestimated value by ![](img/B15558_14_163.png) and
    the underestimated value by ![](img/B15558_14_195.png). Okay, how can we determine
    if the value is overestimated or underestimated?
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要确保我们的支撑值应该是递增的，而不是递减的。所以，如果初始量化值被高估，而后续量化值被低估，我们可以对其进行惩罚。也就是说，我们将高估的值乘以![](img/B15558_14_163.png)，将低估的值乘以![](img/B15558_14_195.png)。好吧，我们如何判断一个值是高估还是低估的呢？
- en: First, we compute the difference between the target and the predicted value.
    Let *u* be the difference between the target support value and the predicted support
    value. Then, if the value of *u* is less than 0, we multiply *u* by ![](img/B15558_14_196.png),
    else we multiply *u* by ![](img/B15558_10_071.png). This is known as **quantile
    regression loss**.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算目标值与预测值之间的差异。设*u*为目标支撑值与预测支撑值之间的差异。然后，如果*u*的值小于0，我们将*u*乘以![](img/B15558_14_196.png)，否则我们将*u*乘以![](img/B15558_10_071.png)。这就是**量化回归损失**。
- en: But the problem with quantile regression loss is that it will not be smooth
    at 0 and it makes the gradient stay constant. So, instead of using quantile regression
    loss, we use a new modified version of loss called quantile Huber loss.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 但是量化回归损失的问题在于它在0处不会平滑，并且会使梯度保持常数。因此，我们不使用量化回归损失，而是使用一种称为量化Huber损失的新修改版损失函数。
- en: 'To understand how exactly quantile Huber loss works, first, let''s look into
    the Huber loss. Let''s denote the difference between our actual and predicted
    values as *u*. Then the Huber loss ![](img/B15558_14_198.png) can be given as:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解量化Huber损失是如何工作的，我们首先来看一下Huber损失。设实际值与预测值之间的差异为*u*。那么Huber损失！[](img/B15558_14_198.png)可以表示为：
- en: '![](img/B15558_14_199.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_199.png)'
- en: Let ![](img/B15558_14_200.png), then, when the absolute value, ![](img/B15558_14_201.png),
    is less than or equal to ![](img/B15558_14_202.png), the Huber loss is given as
    the quadratic loss, ![](img/B15558_14_203.png), else it is a linear loss, ![](img/B15558_14_204.png).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 设！[](img/B15558_14_200.png)，那么，当绝对值！[](img/B15558_14_201.png)小于或等于！[](img/B15558_14_202.png)时，Huber损失给出的是二次损失！[](img/B15558_14_203.png)，否则它是线性损失！[](img/B15558_14_204.png)。
- en: 'The following Python snippet helps us to understand Huber loss better:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码片段有助于我们更好地理解Huber损失：
- en: '[PRE74]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now that we have understood what the Huber loss ![](img/B15558_14_198.png) is,
    let's look into the quantile Huber loss. In the quantile Huber loss, when the
    value of *u* (the difference between target and predicted support) is less than
    0, then we multiply the Huber loss ![](img/B15558_14_198.png) by ![](img/B15558_14_207.png),
    and when the value of *u* is greater than or equal to 0, we multiply the Huber
    loss ![](img/B15558_14_198.png) by ![](img/B15558_14_157.png).
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了Huber损失！[](img/B15558_14_198.png)，让我们来看看量化Huber损失。在量化Huber损失中，当*u*（目标支撑值与预测支撑值之间的差异）的值小于0时，我们将Huber损失！[](img/B15558_14_198.png)乘以![](img/B15558_14_207.png)，当*u*的值大于或等于0时，我们将Huber损失！[](img/B15558_14_198.png)乘以![](img/B15558_14_157.png)。
- en: Now that we have understood how a QR-DQN works, in the next section, we will
    look into another interesting algorithm called D4PG.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 QR-DQN 的工作原理，在下一节中，我们将介绍另一种有趣的算法——D4PG。
- en: Distributed Distributional DDPG
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式分布式 DDPG
- en: '**D4PG**, which stands for **D**istributed **D**istributional **D**eep **D**eterministic
    **P**olicy **G**radient, is one of the most interesting policy gradient algorithms.
    We can make a guess about how D4PG works just by its name. As the name suggests,
    D4PG is basically a combination of **deep deterministic policy gradient** (**DDPG**)
    and distributional reinforcement learning, and it works in a distributed fashion.
    Confused? Let''s go deeper and understand how D4PG works in detail.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '**D4PG**，即**分布式分布式深度确定性策略梯度**，是最有趣的策略梯度算法之一。仅凭其名称，我们就可以猜测 D4PG 的工作原理。正如名字所示，D4PG
    本质上是 **深度确定性策略梯度**（**DDPG**）与分布式强化学习的结合，并且以分布式方式工作。感到困惑？让我们更深入地探讨，了解 D4PG 的详细工作原理。'
- en: To understand how D4PG works, it is highly recommended to revise the DDPG algorithm
    we covered in *Chapter 12*, *Learning DDPG, TD3, and SAC*. We learned that DDPG
    is an actor critic method where the actor tries to learn the policy while the
    critic tries to evaluate the policy produced by the actor using the Q function.
    The critic uses the deep Q network for estimating the Q function and the actor
    uses the policy network for computing the policy. Thus, the actor performs an
    action while the critic gives feedback to the action performed by the actor and,
    based on the critic feedback, the actor network will be updated.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 D4PG 是如何工作的，强烈建议复习我们在*第 12 章*中讲解的 DDPG 算法，*学习 DDPG、TD3 和 SAC*。我们了解到，DDPG
    是一种演员-评论员方法，其中演员尝试学习策略，而评论员则尝试通过 Q 函数评估演员产生的策略。评论员使用深度 Q 网络来估计 Q 函数，演员则使用策略网络来计算策略。因此，演员执行一个动作，而评论员对演员执行的动作给出反馈，并且根据评论员的反馈，演员网络会更新。
- en: D4PG works just like DDPG but in the critic network, instead of using a DQN
    for estimating the Q function, we can use our distributional DQN to estimate the
    value distribution. That is, in the previous sections, we have learned several
    distributional DQN algorithms, such as C51 and QR-DQN. So, in the critic network,
    instead of using a regular DQN, we can use any distributional DQN algorithm, say
    C51.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG 的工作原理与 DDPG 类似，但在评论员网络中，我们不使用 DQN 来估计 Q 函数，而是可以使用分布式 DQN 来估计值分布。也就是说，在之前的章节中，我们已经学习了几种分布式
    DQN 算法，如 C51 和 QR-DQN。因此，在评论员网络中，我们可以使用任何分布式 DQN 算法，例如 C51，而不是常规的 DQN。
- en: 'Apart from this, D4PG also proposes several changes to the DDPG architecture.
    So, we will get into the details and learn how exactly D4PG differs from DDPG.
    Before going ahead, let''s be clear with the notation:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，D4PG 还对 DDPG 架构提出了若干改进。因此，我们将深入探讨并了解 D4PG 与 DDPG 的具体差异。在继续之前，让我们先明确一下符号：
- en: The policy network parameter is represented by ![](img/B15558_13_234.png) and
    the target policy network parameter is represented by ![](img/B15558_12_210.png).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略网络参数由 ![](img/B15558_13_234.png) 表示，目标策略网络参数由 ![](img/B15558_12_210.png) 表示。
- en: The critic network parameter is represented by ![](img/B15558_09_098.png) and
    the target critic network parameter is represented by ![](img/B15558_14_039.png).
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论员网络参数由 ![](img/B15558_09_098.png) 表示，目标评论员网络参数由 ![](img/B15558_14_039.png)
    表示。
- en: Since we are talking about a deterministic policy, let's represent it by ![](img/B15558_14_214.png),
    and our policy is parameterized by the policy network, so we can denote the policy
    by ![](img/B15558_14_215.png).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们讨论的是确定性策略，假设它由 ![](img/B15558_14_214.png) 表示，并且我们的策略是通过策略网络参数化的，因此我们可以用
    ![](img/B15558_14_215.png) 来表示该策略。
- en: Now, we will understand how exactly the critic and actor network in D4PG works.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将了解 D4PG 中评论员和演员网络的具体工作原理。
- en: Critic network
  id: totrans-503
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评论网络
- en: In DDPG, we learned that we use the critic network to estimate the Q function.
    Thus, given a state and action, the critic network estimates the Q function as
    ![](img/B15558_14_216.png). To train the critic network we minimize the MSE between
    the target Q value given by the Bellman optimality equation and the Q value predicted
    by the network.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DDPG 中，我们了解到我们使用评论员网络来估计 Q 函数。因此，给定一个状态和动作，评论员网络通过 ![](img/B15558_14_216.png)
    来估计 Q 函数。为了训练评论员网络，我们最小化目标 Q 值（由贝尔曼最优方程给出）和网络预测的 Q 值之间的均方误差（MSE）。
- en: 'The target value in DDPG is computed as:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DDPG 中，目标值是通过以下公式计算的：
- en: '![](img/B15558_14_217.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_217.png)'
- en: 'Once we compute the target value, we compute the loss as the MSE between the
    target value and the predicted value as:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出目标值，我们计算目标值和预测值之间的均方误差（MSE）损失，公式为：
- en: '![](img/B15558_12_047.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_047.png)'
- en: 'Where *K* denotes the number of transitions randomly sampled from the replay
    buffer. After computing the loss, we compute the gradients ![](img/B15558_14_219.png)
    and update the critic network parameter using gradient descent:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*K*表示从重放缓冲区随机采样的转移数量。计算损失后，我们计算梯度！[](img/B15558_14_219.png)，并使用梯度下降法更新评论员网络参数：
- en: '![](img/B15558_12_052.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_052.png)'
- en: Now, let's talk about the critic in D4PG. As we learned in D4PG, we use the
    distributional DQN to estimate the Q value. Thus, given a state and action, the
    critic network estimates the value distribution as ![](img/B15558_14_221.png).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈D4PG中的评论员。正如我们在D4PG中学到的那样，我们使用分布式DQN来估计Q值。因此，给定一个状态和动作，评论员网络估计价值分布，表示为！[](img/B15558_14_221.png)。
- en: To train the critic network, we minimize the distance between the target value
    distribution given by the distributional Bellman equation and the value distribution
    predicted by the network.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练评论员网络，我们最小化目标值分布（由分布式Bellman方程给出）与网络预测的值分布之间的距离。
- en: 'The target value distribution in D4PG is computed as:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG中的目标值分布计算公式为：
- en: '![](img/B15558_14_222.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_222.png)'
- en: 'As you can observe, equation (2) is similar to (1) except that we just replaced
    *Q* with *Z*, indicating that we are computing the target value distribution.
    D4PG proposes one more change to the target value computation (2). Instead of
    using the one-step return *r*, we use the **N-step return**, and it can be expressed
    as:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，方程（2）与（1）相似，唯一的不同是我们将*Q*替换为*Z*，这表明我们正在计算目标值分布。D4PG对目标值计算（2）提出了一个小的改动。我们不再使用一步回报*r*，而是使用**N步回报**，其表达式为：
- en: '![](img/B15558_14_223.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_223.png)'
- en: Where *N* is the length of the transition, which we sample from the replay buffer.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*N*是转移的长度，我们从重放缓冲区中采样。
- en: 'After computing the target value distribution, we can compute the distance
    between the target value distribution and the predicted value distribution as:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算目标值分布后，我们可以计算目标值分布与预测值分布之间的距离，计算公式为：
- en: '![](img/B15558_14_224.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_224.png)'
- en: 'Where *d* denotes any distance measure for measuring the distance between two
    distributions. Say we are using C51, then *d* denotes the cross entropy and *K*
    denotes the number of transitions sampled from the replay buffer. After computing
    the loss, we calculate the gradients and update the critic network parameter.
    The gradients can be computed as:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*d*表示用于衡量两个分布之间距离的任何距离度量。假设我们使用C51，那么*d*表示交叉熵，*K*表示从重放缓冲区采样的转移数量。计算损失后，我们计算梯度并更新评论员网络参数。梯度的计算公式为：
- en: '![](img/B15558_14_225.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_225.png)'
- en: D4PG proposes a small change to our gradient updates. In D4PG, we use a **prioritized
    experience replay.** Let's say we have an experience replay buffer of size *R*.
    Each transition in the replay buffer will have a non-uniform probability *p*[i].
    The non-uniform probability helps us to give more importance to one transition
    than the other. Say we have a sample *i*, then its probability can be given as
    ![](img/B15558_14_226.png) or ![](img/B15558_14_227.png). While updating the critic
    network, we weight the updates using ![](img/B15558_14_228.png), which gives importance
    to the updates.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG对我们的梯度更新提出了一个小的改动。在D4PG中，我们使用**优先经验重放**。假设我们的经验重放缓冲区大小为*R*。重放缓冲区中的每个转移都有一个非均匀的概率*p*[i]。这种非均匀概率帮助我们赋予某个转移比其他转移更高的权重。假设我们有一个样本*i*，那么它的概率可以表示为！[](img/B15558_14_226.png)
    或者！[](img/B15558_14_227.png)。在更新评论员网络时，我们使用！[](img/B15558_14_228.png)对更新进行加权，从而赋予某些更新更大的重要性。
- en: 'Thus our gradient computation becomes:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的梯度计算公式为：
- en: '![](img/B15558_14_229.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_229.png)'
- en: After computing the gradient, we can update the critic network parameter using
    gradient descent as ![](img/B15558_12_052.png). Now that we have understood how
    the critic network works in D4PG, let's look into the actor network in the next
    section.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出梯度后，我们可以使用梯度下降法更新评论员网络参数，如！[](img/B15558_12_052.png)。现在我们已经理解了D4PG中评论员网络的工作原理，接下来让我们看看下一部分的演员网络。
- en: Actor network
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演员网络
- en: 'First, let''s quickly recap how the actor network in DDPG works. In DDPG, we
    learned that the actor network takes the state as input and returns the action:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速回顾一下DDPG中的演员网络是如何工作的。在DDPG中，我们了解到，演员网络将状态作为输入，返回动作：
- en: '![](img/B15558_14_231.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_231.png)'
- en: Note that we are using the deterministic policy in the continuous action space,
    and to explore new actions we just add some noise ![](img/B15558_14_232.png) to
    the action produced by the actor network since the action is a continuous value.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在连续动作空间中使用确定性策略，并且为了探索新动作，我们只需向actor网络生成的动作中添加一些噪声 ![](img/B15558_14_232.png)，因为动作是连续值。
- en: 'So, our modified action can be represented as:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们修改后的动作可以表示为：
- en: '![](img/B15558_14_233.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_233.png)'
- en: 'Thus, the objective function of the actor is to generate an action that maximizes
    the Q value produced by the citric network:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，actor的目标函数是生成一个最大化评论员网络生成的Q值的动作：
- en: '![](img/B15558_14_234.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_234.png)'
- en: Where ![](img/B15558_14_235.png).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_14_235.png)。
- en: We learned that to maximize the objective, we compute the gradients of our objective
    function ![](img/B15558_11_014.png) and update the actor network parameter by
    performing gradient ascent.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，为了最大化目标，我们计算目标函数的梯度 ![](img/B15558_11_014.png)，并通过执行梯度上升来更新actor网络参数。
- en: 'Now let''s come to D4PG. In D4PG we perform the same steps with a little difference.
    Note that here we are not using the Q function in the critic. Instead, we are
    computing the value distribution and thus our objective function becomes:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈D4PG。在D4PG中，我们执行相同的步骤，唯一的区别是。请注意，在这里我们没有使用评论员中的Q函数。相反，我们计算价值分布，因此我们的目标函数变为：
- en: '![](img/B15558_14_237.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_237.png)'
- en: 'Where the action, ![](img/B15558_14_235.png) and just like we saw in DDPG,
    to maximize the objective, first, we compute the gradients of our objective function
    ![](img/B15558_14_238.png). After computing the gradients we update the actor
    network parameter by performing gradient ascent:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，动作为 ![](img/B15558_14_235.png)，就像我们在DDPG中看到的那样，为了最大化目标，我们首先计算目标函数的梯度 ![](img/B15558_14_238.png)。在计算梯度之后，我们通过执行梯度上升来更新actor网络参数：
- en: '![](img/B15558_12_068.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_068.png)'
- en: We learned that D4PG is a **distributed** algorithm, meaning that instead of
    using one actor, we use *L* **number of actors**, each of which acts parallel
    and is independent of the environment, collects experience, and stores the experience
    in the replay buffer. Then we update the network parameter to the actors periodically.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，D4PG是一个**分布式**算法，这意味着我们不是使用一个actor，而是使用*L* **个actor**，每个actor并行工作，独立于环境，收集经验，并将经验存储在重放缓冲区中。然后，我们定期更新网络参数给这些actor。
- en: 'Thus, to summarize, D4PG is similar to DDPG except for the following:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，D4PG与DDPG相似，除了以下几点：
- en: We use the distributional DQN in the critic network instead of using the regular
    DQN to estimate the Q values.
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在评论员网络中使用分布式DQN，而不是使用常规的DQN来估计Q值。
- en: We calculate *N*-step returns in the target instead of calculating the one-step
    return.
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在目标中计算*N*步的回报，而不是计算一步回报。
- en: We use a prioritized experience replay and add importance to the gradient update
    in the critic network.
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用优先经验重放，并在评论员网络中为梯度更新增加重要性。
- en: Instead of using one actor, we use **L** independent actors, each of which acts
    in parallel, collects experience, and stores the experience in the replay buffer.
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用**L**个独立的actor，而不是一个actor，每个actor并行工作，收集经验，并将经验存储在重放缓冲区中。
- en: Now that we have understood how D4PG works, putting together all the concepts
    we have learned, let's look into the algorithm of D4PG in the next section.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了D4PG是如何工作的，将我们学到的所有概念结合起来，接下来让我们看看D4PG的算法。
- en: Algorithm – D4PG
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – D4PG
- en: Let ![](img/B15558_14_240.png) denote the time steps at which we want to update
    the target critic and actor network parameters. We set ![](img/B15558_14_241.png),
    which states that we update the target critic network and target actor network
    parameter for every 2 steps of the episode. Similarly, let ![](img/B15558_14_242.png)denote
    thetime steps at which we want to replicate the network weights to the **L** actors.
    We set ![](img/B15558_14_243.png), which states that we replicate the network
    weights to the actors on every 2 steps of the episode.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 令 ![](img/B15558_14_240.png) 表示我们希望更新目标评论员和actor网络参数的时间步骤。我们设置 ![](img/B15558_14_241.png)，表示我们每2步更新一次目标评论员网络和目标actor网络参数。类似地，令
    ![](img/B15558_14_242.png) 表示我们希望将网络权重复制到**L**个actor的时间步骤。我们设置 ![](img/B15558_14_243.png)，表示我们每2步将网络权重复制到actor。
- en: 'The algorithm of D4PG is given as follows:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG的算法如下所示：
- en: Initialize the critic network parameter ![](img/B15558_09_056.png) and actor
    network parameter ![](img/B15558_14_245.png)
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化评论员网络参数 ![](img/B15558_09_056.png) 和演员网络参数 ![](img/B15558_14_245.png)
- en: Initialize target critic network parameter ![](img/B15558_14_246.png) and target
    actor network parameter ![](img/B15558_14_247.png) by copying from ![](img/B15558_09_054.png)
    and ![](img/B15558_14_249.png) respectively
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化目标评论员网络参数 ![](img/B15558_14_246.png) 和目标演员网络参数 ![](img/B15558_14_247.png)，方法是从
    ![](img/B15558_09_054.png) 和 ![](img/B15558_14_249.png) 复制
- en: Initialize the replay buffer ![](img/B15558_14_098.png)
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_14_098.png)
- en: Launch the *L* number of actors
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 *L* 个演员
- en: For *N* number of episodes, repeat *step 6*
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 个回合，重复 *步骤 6*
- en: 'For each step in the episode, that is, for ![](img/B15558_14_101.png):'
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合中的每一步，即，！[](img/B15558_14_101.png)：
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_266.png)
  id: totrans-556
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区随机抽取 *K* 个过渡的迷你批次 ![](img/B15558_12_266.png)
- en: Compute the target value distribution of the critic, that is,![](img/B15558_14_253.png)
  id: totrans-557
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员的目标值分布，即，！[](img/B15558_14_253.png)
- en: Compute the loss of the critic network and calculate the gradient as ![](img/B15558_14_229.png)
  id: totrans-558
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员网络的损失并计算梯度：![](img/B15558_14_229.png)
- en: 'After computing the gradients, update the critic network parameter using gradient
    descent: ![](img/B15558_14_255.png)'
  id: totrans-559
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算梯度后，使用梯度下降更新评论员网络参数：![](img/B15558_14_255.png)
- en: Compute the gradient of the actor network ![](img/B15558_11_014.png)
  id: totrans-560
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算演员网络的梯度：![](img/B15558_11_014.png)
- en: Update the actor network parameter by gradient ascent:![](img/B15558_14_257.png)
  id: totrans-561
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过梯度上升更新演员网络参数：![](img/B15558_14_257.png)
- en: 'If *t* mod ![](img/B15558_14_258.png), then:'
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* 对 ![](img/B15558_14_258.png) 取模，则：
- en: Update the target critic and target actor network parameter using soft replacement
    as ![](img/B15558_14_259.png) and ![](img/B15558_14_260.png) respectively
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用软替换更新目标评论员和目标演员网络参数，分别为 ![](img/B15558_14_259.png) 和 ![](img/B15558_14_260.png)
- en: 'If *t* mod ![](img/B15558_14_261.png), then:'
  id: totrans-564
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* 对 ![](img/B15558_14_261.png) 取模，则：
- en: Replicate the network weights to the actors
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将网络权重复制到演员中
- en: 'And we perform the following steps in the actor network:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在演员网络中执行以下步骤：
- en: Select action *a* based on the policy ![](img/B15558_14_262.png) and exploration
    noise, that is,![](img/B15558_14_233.png)
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于策略 ![](img/B15558_14_262.png) 和探索噪声选择动作 *a*，即，！[](img/B15558_14_233.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_14_264.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_092.png)
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a*，移动到下一个状态 ![](img/B15558_14_264.png)，获得奖励 *r*，并将转换信息存储到重放缓冲区 ![](img/B15558_09_092.png)
- en: Repeat *steps 1* to *2* until the learner finishes
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 1* 至 *步骤 2*，直到学习者完成
- en: Thus, we have learned how D4PG works.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经了解了 D4PG 的工作原理。
- en: Summary
  id: totrans-571
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding how distributional reinforcement learning
    works. We learned that in distributional reinforcement learning, instead of selecting
    an action based on the expected return, we select the action based on the distribution
    of return, which is often called the value distribution or return distribution.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过理解分布式强化学习的工作原理开始了本章的学习。我们了解到，在分布式强化学习中，不是基于期望回报来选择动作，而是根据回报的分布来选择动作，这通常被称为价值分布或回报分布。
- en: Next, we learned about the categorical DQN algorithm, also known as C51, where
    we feed the state and support of the distribution as the input and the network
    returns the probabilities of the value distribution. We also learned how the projection
    step matches the support of the target and predicted the value distribution so
    that we can apply the cross entropy loss.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了分类 DQN 算法，也称为 C51，其中我们将状态和分布的支持作为输入，网络返回价值分布的概率。我们还学习了投影步骤如何匹配目标和预测值分布的支持，以便我们可以应用交叉熵损失。
- en: Going ahead, we learned about quantile regression DQNs, where we feed the state
    and also the equally divided cumulative probabilities ![](img/B15558_14_157.png)
    as input to the network and it returns the support value of the distribution.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了分位回归 DQN，在这种方法中，我们将状态和等分的累积概率 ![](img/B15558_14_157.png) 作为输入馈送到网络中，网络返回分布的支持值。
- en: At the end of the chapter, we learned about how D4PG works, and we also learned
    how it varies from DDPG.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们了解了 D4PG 的工作原理，也了解了它与 DDPG 的不同之处。
- en: Questions
  id: totrans-576
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s test our knowledge of distributional reinforcement learning by answering
    the following questions:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来测试我们对分布式强化学习的知识：
- en: What is distributional reinforcement learning?
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是分布式强化学习？
- en: What is a categorical DQN?
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是分类DQN？
- en: Why is the categorical DQN called the C51 algorithm?
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么分类DQN被称为C51算法？
- en: What is the quantile function?
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是分位数函数？
- en: How does a QR-DQN differ from a categorical DQN?
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: QR-DQN与分类DQN有何不同？
- en: How does D4PG differ from DDPG?
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D4PG与DDPG有何不同？
- en: Further reading
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, refer to the following papers:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下论文：
- en: '**A Distributional Perspective on Reinforcement Learning** by *Marc G. Bellemare*,
    *Will Dabney*, *Remi Munos*, [https://arxiv.org/pdf/1707.06887.pdf](https://arxiv.org/pdf/1707.06887.pdf)'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从分布的角度看强化学习**，作者为*Marc G. Bellemare*、*Will Dabney*、*Remi Munos*，[https://arxiv.org/pdf/1707.06887.pdf](https://arxiv.org/pdf/1707.06887.pdf)'
- en: '**Distributional Reinforcement Learning with Quantile Regression** by *Will
    Dabney*, *Mark Rowland*, *Marc G. Bellemare*, *Rémi Munos*, [https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf)'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用分位回归的分布式强化学习**，作者为*Will Dabney*、*Mark Rowland*、*Marc G. Bellemare*、*Rémi
    Munos*，[https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf)'
- en: '**Distributed Distributional Deep Deterministic Policy Gradient** by *Gabriel
    Barth-Maron*, *et al*., [https://arxiv.org/pdf/1804.08617.pdf](https://arxiv.org/pdf/1804.08617.pdf)'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式分布深度确定性策略梯度**，作者为*Gabriel Barth-Maron*、*等*，[https://arxiv.org/pdf/1804.08617.pdf](https://arxiv.org/pdf/1804.08617.pdf)'
