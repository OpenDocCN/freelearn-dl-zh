- en: Generative Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: 'Generative models are the most promising push toward enabling computers to
    have an understanding of the world. They are true unsupervised models, and are
    able to perform those tasks that many today consider to be at the cutting edge
    of **artificial intelligence **(**AI**). Generative models are different for precisely
    the reason as it sounds: they generate data. Centered mostly around computer vision
    tasks, this class of network has the power to create new faces, new handwriting,
    or even paintings.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是推动计算机理解世界的最有前景的方向。它们是真正的无监督模型，能够执行许多当前被认为是**人工智能**（**AI**）前沿的任务。生成模型之所以不同，正如其名称所示：它们生成数据。主要集中在计算机视觉任务上，这类网络有能力创造新的人脸、新的手写字，甚至是绘画作品。
- en: In this section, we'll introduce generative models and their foundations, focusing
    specifically on the two most popular types of model, the **variational autoencoder **(**VAE**),
    and the **generative adversarial network** (**GAN**), where you'll learn ...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍生成模型及其基础，特别关注两种最受欢迎的模型类型：**变分自编码器**（**VAE**）和**生成对抗网络**（**GAN**），你将学习到...
- en: Technical requirements
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be utilizing Python 3\. You''ll need the following
    packages to be installed:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python 3。你需要安装以下包：
- en: NumPy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: It will be helpful if your machine is GPU enabled, as discussed in [Chapter
    3](69346214-320e-487f-b4cf-bd5c469dc75e.xhtml), *Platforms and Other Essentials*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器支持GPU，参考[第3章](69346214-320e-487f-b4cf-bd5c469dc75e.xhtml)《平台与其他基本要素》，这将非常有帮助。
- en: Getting to AI – generative models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 走向AI – 生成模型
- en: Generative models are a class of neural networks that are wholly different from
    what we have discussed thus far. The networks that we've discussed hitherto are
    feedforward networks. CNNs and RNNs are all discriminatory networks, in that they
    try to classify data. Given a specific input, they can predict classes or other
    labels. Generative models, on the other hand, try to predict features given a
    certain label. They do this by having a parameter set that is much smaller than
    the amount of data they are learning, which forces them to comprehend the general
    essence of the data in an efficient manner.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是一类完全不同于我们目前所讨论的神经网络。我们到目前为止讨论的网络都是前馈网络。CNN和RNN都是判别性网络，它们试图对数据进行分类。给定特定的输入，它们可以预测类别或其他标签。生成模型则相反，它们尝试在给定标签的情况下预测特征。它们通过拥有一个比学习的数据量小得多的参数集来实现这一点，这迫使它们以高效的方式理解数据的基本本质。
- en: There are two main types of generative model, VAE and GAN. First, we'll start
    with the motivations for generative ...
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型有两种主要类型，VAE和GAN。首先，我们将从生成模型的动机开始...
- en: Autoencoders
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: Autoencoders, and their encoder/decoder frameworks, are the inspiration behind
    generative models. They are a self-supervised technique for representation learning,
    where our network learns about its input so that it may generate new data just
    as input. In this section, we'll learn about their architecture and uses as an
    introduction to the generative networks that they inspire.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器及其编码器/解码器框架是生成模型的灵感来源。它们是一种自监督的表示学习技术，通过该技术，网络学习输入内容，以便像输入数据一样生成新的数据。在这一部分，我们将了解它们的架构和用途，作为对它们启发的生成网络的入门介绍。
- en: Network architecture
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络架构
- en: Autoencoders work by taking an input and generating a smaller vector representation
    for later *reconstructing its own input*. They do this by using an encoder to
    impose an information bottleneck on incoming data, and then utilizing a decoder
    to recreate the input data based on that representation. This is based on the
    idea that there are *structures* within data (that is, correlations, and so on)
    that exist, but that are not readily apparent. Autoencoders are a means of automatically
    learning these relationships without explicitly doing so.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通过接受输入并生成一个较小的向量表示，来*重构其自身输入*。它们通过使用编码器对传入数据施加信息瓶颈，然后利用解码器基于该表示重建输入数据。这个方法基于这样一个理念：数据中存在*结构*（即关联等），这些结构是存在的，但并不容易被察觉。自编码器是自动学习这些关系的一种方式，而无需显式地进行学习。
- en: 'Structurally, autoencoders consist of an **input layer**, a **hidden layer**,
    and an **output** **layer**, as demonstrated in the following diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构上，自编码器由**输入层**、**隐藏层**和**输出层**组成，如下图所示：
- en: The encoder learns to preserve as much of the relevant ...
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器学习保留尽可能多的相关信息……
- en: Building an autoencoder
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自编码器
- en: If you're thinking that the task of reconstructing an output doesn't appear
    that useful, you're not alone. What exactly do we use these networks for? Autoencoders
    help to extract features when there are no known labeled features at hand. To
    illustrate how this works, let's walk through an example using TensorFlow. We're
    going to reconstruct the MNIST dataset here, and, later on, we will compare the
    performance of the standard autoencoder against the variational autoencoder in
    relation to the same task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为重建输出的任务似乎没有什么用处，那你不是唯一一个这样想的人。我们究竟用这些网络做什么呢？自编码器帮助在没有已知标签特征的情况下提取特征。为了说明这一点，下面我们通过一个使用
    TensorFlow 的示例来演示。我们将在这里重建 MNIST 数据集，稍后我们将比较标准自编码器和变分自编码器在同一任务上的表现。
- en: 'Let''s get started with our imports and data. MNIST is contained natively within
    TensorFlow, so we can easily import it:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入库和数据开始。MNIST 数据集本身就包含在 TensorFlow 中，所以我们可以很方便地导入它：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For ease, we can build the auto-encoder with the `tf.layers` library. We'll
    want our Autoencoder architecture to follow the convolutional/de-convolutional
    pattern, where the input layer of the decoder matches the size of the input and
    the subsequent layer squash the data into a smaller and smaller representation.
    The decoder will be the same architecture reversed, starting with the small representation
    and working larger.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们可以使用 `tf.layers` 库来构建自编码器。我们希望自编码器的架构遵循卷积/反卷积模式，其中解码器的输入层与输入大小相匹配，随后的层将数据压缩成越来越小的表示。解码器将是相同架构的反转，首先从小的表示开始，然后逐步变大。
- en: 'All together, we want it to look something like the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有代码整合在一起后，它应该看起来像下面这样：
- en: '![](img/ba95c19c-4ed2-4962-a28b-b99d4faef25e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba95c19c-4ed2-4962-a28b-b99d4faef25e.png)'
- en: 'Let''s start with the encoder; we''ll define an initializer for the the weight
    and bias factors first, and then define the encoder as a function that takes and
    input, x. we''ll then use the `tf.layers.dense` function to create standard, fully
    connected neural network layers. The encoder will have three layers, with the
    first layer size matching the input dimensions of the input data (`784`), with
    the subsequent layers getting continually smaller:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从编码器开始；我们首先定义一个初始化器来初始化权重和偏置项，然后将编码器定义为一个接受输入 x 的函数。接着，我们将使用 `tf.layers.dense`
    函数来创建标准的全连接神经网络层。编码器将有三层，第一层的大小与输入数据的维度（`784`）相匹配，随后每一层会逐渐变小：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let's let's build our decoder; it will be using the same layer type and
    initializer as the encoder, only now we invert the layers, so that the first layer
    of the decoder is the smallest and the last is the largest.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建解码器；它将使用与编码器相同的层类型和初始化器，只不过这次我们反转了这些层，解码器的第一层最小，最后一层最大。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Before we get to training, let's define some hyper-parameters that will be needed
    during the training cycle. We'll define the size of our input, the learning rate,
    number of training steps, the batch size for the training cycle, as well as how
    often we want to display information about our training progress.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，让我们定义一些在训练循环中需要用到的超参数。我们将定义输入的大小、学习率、训练步数、训练周期的批次大小，以及我们希望多频繁地显示训练进度信息。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll then define the placeholder for our input data so that we can compile
    the model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将定义输入数据的占位符，以便能够编译模型：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And subsequently, we compile the model and the optimizer as you''ve seen before
    in previous chapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译模型和优化器，就像你在上一章中看到的那样：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lastly, we''ll code up the training cycle. By this point, most of this should
    be fairly familiar to you; start a TensorFlow session, and iterate over the epochs/batches,
    computing the loss and accuracy at each point:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写训练循环的代码。到这一点为止，这些内容对你来说应该比较熟悉了；启动 TensorFlow 会话，并在每个 epoch/批次中迭代，计算每个点的损失和准确率：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For this particular example, we''ll add in a little something more to this
    process; a way to plot the reconstructed images alongside their original versions.
    Keep in mind that this code is still contained within the training session, just
    outside of the training loop:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个具体示例，我们将额外添加一些内容；一个方法来将重建的图像与其原始版本并排显示。请记住，这段代码仍然包含在训练会话中，只是位于训练循环之外：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After training, you should end up with a result along the lines of the following,
    with the actual digits on the left, and the reconstructed digits on the right:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，您应该得到类似以下的结果，左侧是实际数字，右侧是重建的数字：
- en: '![](img/9a2dfa95-fd49-4543-90a7-ed79f9de4aab.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a2dfa95-fd49-4543-90a7-ed79f9de4aab.png)'
- en: 'So what have we done here? By training the autoencoder on unlabeled digits,
    we''ve done the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们在这里做了什么？通过在无标签的数字上训练自编码器，我们完成了以下工作：
- en: Learned the latent features of the dataset without having explicit labels
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习了数据集的潜在特征，且无需明确的标签
- en: Successfully learned the distribution of the data and reconstructed the image
    from scratch, from that distribution
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功地学习了数据的分布，并从该分布中从头重建了图像
- en: 'Now, let''s say that we wanted to take this further and generate or classify
    new digits that we haven''t seen yet. To do this, we could remove the decoder
    and attach a classifier or generator network:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想进一步开展工作，生成或分类我们还没有见过的新数字。为此，我们可以去掉解码器并附加一个分类器或生成器网络：
- en: '![](img/58fbd830-412a-4784-b3b7-08eb64a140ee.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58fbd830-412a-4784-b3b7-08eb64a140ee.png)'
- en: The encoder therefore becomes a means of initializing a supervised training
    model. Standard autoencoders have been used in a variety of tasks. In the supplementary
    code for this chapter, we'll walk through an example where we utilize autoencoders
    for visual anomaly detection.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编码器变成了初始化监督训练模型的一种方式。标准自编码器已在各种任务中得到应用。在本章的补充代码中，我们将演示一个示例，展示如何使用自编码器进行视觉异常检测。
- en: Variational autoencoders
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: '**Variational autoencoders** (**VAEs**) are built on the idea of the standard
    autoencoder, and are powerful generative models and one of the most popular means
    of learning a complicated distribution in an unsupervised fashion. VAEs are **probabilistic
    models **rooted in Bayesian inference. A probabilistic model is exactly as it
    sounds:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAE**）基于标准自编码器的思想构建，是强大的生成模型，也是学习复杂分布的一种最流行的无监督方法。VAE是基于贝叶斯推断的**概率模型**。概率模型正如其名所示：'
- en: '*Probabilistic models incorporate random variables and probability distributions
    into the model of an event or phenomenon.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率模型将随机变量和概率分布纳入事件或现象的模型中。*'
- en: VAEs, and other generative models, are probabilistic in that they seek to learn
    a distribution that they utilize for subsequent sampling. While all generative
    models are probabilistic models, not all probabilistic models are generative models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）和其他生成模型是概率性的，因为它们试图学习一个分布，并利用该分布进行后续的采样。尽管所有生成模型都是概率模型，但并非所有概率模型都是生成模型。
- en: The probabilistic structure of ...
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 概率结构的...
- en: Structure
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构
- en: 'Like standard autoencoders, VAEs utilize the same encoder/decoder framework,
    but, that aside, they are mathematically different from their namesake. VAEs take
    a probabilistic perspective in terms of guiding the network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像标准自编码器一样，变分自编码器（VAE）采用相同的编码器/解码器框架，但除了这一点，它们在数学上与其名称所示的有所不同。VAE在指导网络时采取了概率的视角：
- en: '![](img/763b5004-ecd3-4102-99ab-f55907db8e60.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/763b5004-ecd3-4102-99ab-f55907db8e60.png)'
- en: Both our **encoder** and **decoder** networks are generating distributions from
    their input data. The encoder generates a distribution from its training data,
    **Z**, which then becomes the input distribution for the decoder. The decoder
    takes this distribution, **Z**, and tries to replicate the original distribution, **X**,
    from it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的**编码器**和**解码器**网络都在从其输入数据中生成分布。编码器从其训练数据生成一个分布，**Z**，然后这个分布成为解码器的输入分布。解码器接收这个分布，**Z**，并尝试从中重建原始分布，**X**。
- en: Encoder
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The encoder generates its distribution by first defining its prior as a standard
    normal distribution. Then, during training, this distribution becomes updated,
    and the decoder can easily sample from this distribution later on. Both the encoder
    and the decoder are unique in terms of VAEs in that they output two vectors instead
    of one: a vector of means, *μ*, and another vector of standard deviation, *σ*.
    These help to define the limits for our generated distributions. Intuitively,
    the mean vector controls where the encoding of an input should be centered, while
    the standard deviation controls the extent to which the encoding may vary from
    the mean. This constraint on the encoder forces the network to learn a distribution,
    thereby taking ...'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器通过首先将其先验定义为标准正态分布来生成其分布。然后，在训练期间，这个分布会更新，解码器稍后可以轻松地从这个分布中进行采样。VAEs中编码器和解码器在输出两个向量而不是一个方面是独特的：一个均值向量*μ*，另一个标准差向量*σ*。这些帮助定义了我们生成分布的限制。直观地说，均值向量控制输入的编码应该在哪里集中，而标准差控制编码可能从均值中变化的程度。编码器的这种约束迫使网络学习一个分布，从而...
- en: Decoder
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: Like the standard autoencoder, the decoder in the VAE is a backward convolutional
    network, or a deconvolutional network. In processing the decoding, data is sampled
    from the generation stochastically (randomly), making the VAE one of the few models
    that can directly sample a probability distribution without a Markov chain Monte
    Carlo method. As a result of the stochastic generation process, the encoding that
    we generate from each pass will be a different representation of the data, all
    while maintaining the same mean and standard deviation. This helps with the decoder's
    sampling technique; because all encodings are generated from the same distribution,
    the decoder learns that a latent data point and its surrounding points are all
    members of the same class. This allows the decoder to learn how to generate from
    similar, but slightly varying, encodings.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 像标准自动编码器一样，VAE中的解码器是一个反向卷积网络，或者是一个反卷积网络。在处理解码时，数据是从生成过程中随机采样的，使得VAE成为少数可以直接从概率分布中采样而无需马尔可夫链蒙特卡洛方法的模型之一。由于随机生成过程的结果，我们从每次通过生成的编码将是数据的不同表示，同时保持相同的均值和标准差。这有助于解码器的采样技术；因为所有编码都是从相同的分布生成的，解码器学习到一个潜在数据点及其周围点都是同一类的成员。这使得解码器学会如何从类似但略有不同的编码中生成。
- en: Training and optimizing VAEs
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和优化VAEs
- en: VAEs utilize a negative log-likelihood loss as their reconstruction loss to
    measure how much information is lost during the reconstruction phase of the decoder.
    If the decoder does not reconstruct the input satisfactorily, it will incur a
    large reconstruction loss. VAEs also introduce something called **Kullback**–**Leibler** (**KL**)
    divergence into their loss functions. KL divergence simply measures how much two
    probability distributions diverge; in other words, how different they are from
    one another. We want to minimize the KL distance between the mean and standard
    deviation of the target distribution and that of a standard normal. It is properly
    minimized when the mean is zero and the standard deviation is one. The log-likelihood
    ...
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs利用负对数似然损失作为它们的重构损失，以衡量在解码器重构阶段丢失了多少信息。如果解码器不能令输入令人满意地重构，则会产生大的重构损失。VAEs还引入了称为**Kullback**–**Leibler**（**KL**）散度的东西到它们的损失函数中。KL散度简单地衡量两个概率分布的差异程度；换句话说，它们彼此有多不同。我们希望最小化目标分布的均值和标准差与标准正态分布的KL距离。当均值为零且标准差为一时，它被正确地最小化。对数似然...
- en: Utilizing a VAE
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用VAE
- en: We can construct a variational autoencoder in TensorFlow to see how it compares
    to it's simpler, standard autoencoder cousin. In this section, we'll be using
    the same MNIST dataset so that we can standardize our comparison across methods. Let's
    walk through how to construct a VAE by utilizing it to generate handwriting based
    on the `MNIST` dataset. Think of *x* as being the individual written characters
    and *z* as the latent features in each of the individual characters that we are
    trying to learn.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在TensorFlow中构建一个变分自动编码器，以查看它与它更简单的标准自动编码器表亲的比较。在本节中，我们将使用相同的MNIST数据集，以便我们可以跨方法标准化我们的比较。让我们通过利用它来生成基于`MNIST`数据集的手写体来构建一个VAE。将*x*视为每个我们试图学习的个体字符中的潜在特征，*z*为这些字符中的潜在特征。
- en: 'First, let''s start with our imports:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从我们的导入开始：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As before, we can import the `''MNIST_data''` directly from the TensorFlow
    library:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们可以直接从TensorFlow库中导入`'MNIST_data'`：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we can start to build the encoder. We''re going to be utilizing the same
    `tf.layers` package as we did before. Here, our encoder will look fairly similar
    to how it did in the previous example, our layers will take in an input and gradually
    compress that input until we generate a latent distribution, *z*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始构建编码器。我们将继续使用之前相同的`tf.layers`包。在这里，我们的编码器看起来与之前的示例相似，我们的层会接收输入并逐渐压缩该输入，直到生成潜在的分布，*z*：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here''s where we start to diverge from the standard autoencoder, however. While
    the last layer in the encoder will give us the potential z-distribution that represents
    our data, we''ll need to calculate the values of ![](img/f91f9b49-ee69-4ac5-b432-07f3b201f54f.png) and ![](img/0a07d995-2583-4465-b9a0-78b667741e97.png) that
    will help define that distribution. We can do that by creating two new layers
    that take in the potential distribution z, and output out values of `mu` and `sigma`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这里我们开始偏离标准自编码器了。虽然编码器中的最后一层会给出代表我们数据的潜在z分布，但我们需要计算![](img/f91f9b49-ee69-4ac5-b432-07f3b201f54f.png)和![](img/0a07d995-2583-4465-b9a0-78b667741e97.png)的值，这些值将有助于定义该分布。我们可以通过创建两个新的层来实现这一点，这两个层接收潜在的分布z，并输出`mu`和`sigma`的值：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we''ll use these values to go ahead and calculate the KL divergence for
    the encoder, which will eventually go into constructing our final loss function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这些值来计算编码器的KL散度，最终将其用于构建我们的最终损失函数：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s go ahead and create the decoder portion of the variational autoencoder
    now; we''ll create a deconvolutional pattern that reverses the dimensions of the
    encoder. All of this will be contained under a function called `decoder(z)`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来创建变分自编码器的解码器部分；我们将创建一个反卷积模式，反转编码器的维度。所有这些将包含在一个名为`decoder(z)`的函数中：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Also under the decoder function, we''ll use the decoder output to calculate
    the reconstruction loss:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器函数下，我们将使用解码器的输出计算重建损失：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As usual, we''ll prepare our training parameters before we start initializing
    the model. We''ll define a learning rate, batch size for our training, the number
    of training epochs, dimension of the input, and the size of our total training
    sample:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将在初始化模型之前准备好训练参数。我们将定义学习率、训练批量大小、训练轮数、输入维度和总训练样本的大小：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We''ll also define the placeholder for our input data, `x`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将定义输入数据的占位符`x`：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Before we start training, we''ll initialize the model, loss, and `optimizer`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，我们将初始化模型、损失函数和`optimizer`：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we can run the actual training process. This we be similar to the
    training processes that we''ve already built and experienced:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以运行实际的训练过程。这将类似于我们已经构建和体验过的训练过程：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Lastly, we can use the bit of code following code to generate new samples from
    our newly trained model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下代码生成从新训练的模型中生成的新样本：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Ultimately, you should end up with an image such as the following, with the
    original digits on the left and the generated digits on the right:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你应该能得到如下所示的图像，左侧是原始数字，右侧是生成的数字：
- en: '![](img/c0587722-c356-4abe-afbd-5d0f2a9e727c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0587722-c356-4abe-afbd-5d0f2a9e727c.png)'
- en: 'Observe how much clearer the digits are compared to the original autoencoder:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 观察一下生成的数字比原始自编码器清晰了多少：
- en: '![](img/3867cf5a-5047-4f9a-8f3f-ca2c310363ac.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3867cf5a-5047-4f9a-8f3f-ca2c310363ac.png)'
- en: Now, let's see how we can take this further with GANs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何通过GAN进一步推进这个过程。
- en: Generative adversarial networks
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: 'Generative adversarial networks (**GANs**) are a class of networks that were
    introduced by Ian Goodfellow in 2014\. In GANs, two neural networks play off against
    one another as adversaries in an **actor**-**critic model**, where one is the
    creator and the other is the scrutinizer. The creator, referred to as the **generator
    network**, tries to create samples that will fool the scrutinizer, the discriminator
    network. These two increasingly play off against one another, with the generator
    network creating increasingly believable samples and the discriminator network
    getting increasingly good at spotting the samples. In summary:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（**GAN**）是一类由Ian Goodfellow在2014年提出的网络。在GAN中，两个神经网络相互对抗作为敌人，采用**演员**-**评论员模型**，其中一个是创造者，另一个是审查者。创造者，称为**生成器网络**，试图创建能欺骗审查者（判别器网络）的样本。这两者之间相互博弈，生成器网络创造越来越逼真的样本，而判别器网络则变得越来越擅长识别这些样本。总结来说：
- en: The generator tries to maximize the probability of the discriminator passing
    its outputs as real, ...
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器试图最大化判别器将其输出判断为真实的概率，...
- en: Discriminator network
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器网络
- en: 'The discriminator network in image-related GANs is a standard convolutional
    neural network. It takes in an image and outputs a single number that tells us
    whether the image is *real* or *fake*. The discriminator takes in an image, and
    learns the attributes of that image so that it may be a good *judge* vis-à-vis
    the outputs of the generator. In TensorFlow, we can create the `discriminator`
    as a function that we will then run in a TensorFlow session later on. This framework
    is more or less the same as you''ve seen in the previous sections with autoencoder
    and variational autoencoders; we''ll use the higher level `tf.layers` api to create
    three main network layers and an output layer. After each of the main network
    layers, we''ll add a dropout layer for regularization. The last layer will be
    slightly different, as we''ll want to squash the output. For this, we''ll use
    a sigmoid activation function that will give us a final output saying if an image
    is believed to be fake or not:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图像相关的GAN中的判别器网络是一个标准的卷积神经网络。它接收一张图片并输出一个数字，告诉我们这张图片是*真实*的还是*伪造*的。判别器接收图像并学习该图像的属性，以便它能成为一个良好的*判定者*，来判断生成器的输出。在TensorFlow中，我们可以将`discriminator`创建为一个函数，然后在后续的TensorFlow会话中运行它。这个框架和你在前面章节中看到的自编码器和变分自编码器差不多；我们将使用更高级的`tf.layers`
    API来创建三个主要网络层和一个输出层。每个主网络层之后，我们会添加一个dropout层来进行正则化。最后一层会稍微不同，因为我们想对输出进行压缩。为此，我们将使用一个sigmoid激活函数，给出最终的输出，判断一张图片是否被认为是伪造的：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that we have this discriminator defined, let's go ahead and move on to the
    generator.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了判别器，接下来我们继续讨论生成器。
- en: Generator network
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器网络
- en: You can think of the `generator` portion of the GAN as a reverse convolutional
    neural network. Like a VAE, it uses generic normal distribution, the only difference
    being that it up samples the distribution to form an image. This distribution
    represents our prior, and is updated during training as the GAN improves at producing
    images that the discriminator is unable to determine whether they are fake.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将GAN中的`generator`部分看作是一个逆卷积神经网络。和VAE一样，它使用通用的正态分布，唯一的区别是它通过上采样该分布来生成图像。这个分布代表我们的先验，并且在训练过程中随着GAN生成的图像变得越来越真实，判别器也无法判断其真假时，分布会不断更新。
- en: In between each layer, we utilize a `ReLu` activation function and `batch _normalization`
    to stabilize each layer's outputs. As the discriminator starts inspecting the
    outputs of `generator`, `generator` will continually adjust the distribution from
    which it's drawing to closely match the target distribution. The code will look
    fairly ...
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层之间，我们使用`ReLu`激活函数和`batch_normalization`来稳定每一层的输出。随着判别器开始检查`generator`的输出，`generator`会不断调整它抽样的分布，以便与目标分布更好地匹配。代码看起来会相当...
- en: Training GANs
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练GAN
- en: GANs are easy to train, but difficult to optimize due to a number of unstable
    dynamics in their training processes. To train a GAN, we train the generator on
    sub samples of a high-dimensional training distribution; since this does not innately
    exist, we initially sample from a standard normal (Gaussian) distribution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: GAN（生成对抗网络）容易训练，但由于其训练过程中存在许多不稳定的动态，优化起来较为困难。为了训练一个GAN，我们在高维训练分布的子样本上训练生成器；由于这种分布本身并不存在，我们最初从标准正态（高斯）分布中进行采样。
- en: 'Both the generator and the discriminator are trained jointly in a minimax game
    using an objective function, also referred to as the `minimax` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和判别器在一个最小最大游戏中共同训练，使用一个目标函数，也称为`minimax`函数：
- en: '![](img/6d9f4c89-a4aa-4edb-81d3-d11a43fa60c9.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d9f4c89-a4aa-4edb-81d3-d11a43fa60c9.png)'
- en: 'Let''s break this down a bit. The function is telling us what happens where.
    Let''s look at the initial bit of the first expression:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微解析一下。这个函数告诉我们发生了什么。首先来看一下第一个表达式的初始部分：
- en: '![](img/08265ab7-a501-4899-b033-57c9e012c274.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08265ab7-a501-4899-b033-57c9e012c274.png)'
- en: 'The ![](img/c45f3372-9119-4fb7-a267-37a3a3ba0936.png) notation means expectation,
    so we are saying that the expected output, *x*, of the discriminator for real
    images drawn from the actual distribution of will be:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/c45f3372-9119-4fb7-a267-37a3a3ba0936.png)符号表示期望值，所以我们说判别器对于从实际分布中提取的真实图像的期望输出*x*将是：'
- en: '![](img/e2652248-ec02-47e7-9085-72afbd140ce5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2652248-ec02-47e7-9085-72afbd140ce5.png)'
- en: 'Likewise, here''s the second expression:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，下面是第二个表达式：
- en: '![](img/cc93119c-59d1-4f43-902c-1d724c5f3745.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc93119c-59d1-4f43-902c-1d724c5f3745.png)'
- en: 'It''s telling us that the expected value of the output of the discriminator
    for fake images drawn from the generated distribution will be as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它告诉我们，从生成分布中抽取的假图像，判别器的期望输出将是：
- en: '![](img/db7c7441-db5a-49a5-ac30-b18d81adc99d.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db7c7441-db5a-49a5-ac30-b18d81adc99d.png)'
- en: The discriminator wants to maximize (![](img/92648cd3-1ae5-41ef-92ba-1dc9222594bd.png))
    the objective so that its output for real data *D(x)* is as close to one as possible,
    while its output for fake data *D(G(z))* is as close to zero as possible. Meanwhile,
    the generator seeks the opposite, to minimize (![](img/6f3a5e8a-ee08-497c-b37c-5fd1625f0352.png))
    the objective function so that *D(x)* is as close to zero as possible, while *D(G(z))*
    is as close to one as possible. Mathematically, this is how the generator and
    the discriminator play off against one another.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器希望最大化(![](img/92648cd3-1ae5-41ef-92ba-1dc9222594bd.png))目标函数，使得其对于真实数据*D(x)*的输出尽可能接近1，而对于假数据*D(G(z))*的输出尽可能接近0。与此同时，生成器则寻求相反的目标，最小化(![](img/6f3a5e8a-ee08-497c-b37c-5fd1625f0352.png))目标函数，使得*D(x)*的输出尽可能接近0，而*D(G(z))*的输出尽可能接近1。数学上，这是生成器和判别器相互对抗的方式。
- en: 'When training GANs, we train to minimize the objective function so that the
    generator can win. We want the generator to be able to create examples that are
    realistic enough to fool the discriminator. To do this, we train and optimize
    the discriminator and the generator in parallel using gradient ascent. For each
    iteration of training, we are going to train the discriminator network in small
    batches, and then train the generator network in small batches, alternating between
    the two paradigms. Gradient ascent for the discriminator computes the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练GAN时，我们训练以最小化目标函数，使生成器能够获胜。我们希望生成器能够创建足够真实的例子来欺骗判别器。为此，我们并行训练和优化判别器和生成器，使用梯度上升法。在每次训练迭代中，我们会先在小批次中训练判别器网络，然后再在小批次中训练生成器网络，在这两种模式之间交替进行。判别器的梯度上升计算如下：
- en: '![](img/f8919266-0d1b-42b1-ba3d-135b54ea748f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8919266-0d1b-42b1-ba3d-135b54ea748f.png)'
- en: 'Training both the discriminator and the generator jointly can be challenging.
    If we tried to actually minimize the loss function for the generator, as follows,
    we would run into some issues:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同时训练判别器和生成器可能会具有挑战性。如果我们尝试实际最小化生成器的损失函数，如下所示，我们将遇到一些问题：
- en: '![](img/958a783f-cfb4-4b21-9153-0e600f597d17.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/958a783f-cfb4-4b21-9153-0e600f597d17.png)'
- en: 'If we look at a plot of the `minimax` function, we can see why this is:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`minimax`函数的图示，我们可以理解这是为什么：
- en: '![](img/1c65fc91-c973-4cdf-bb15-e4ba4455e415.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c65fc91-c973-4cdf-bb15-e4ba4455e415.png)'
- en: 'Optimization procedures look for gradient signals, which more or less tell
    gradient descent which way to go. In the `minimax` function, the biggest signals
    for gradient descent are to the right, but we actually want to to learn values
    to the left of the function, where it''s minimized to zero and the generator is
    fooling the discriminator. However, as the generator optimizes, it will move away
    from its optimal point, taking us away from where it should be. To solve this,
    we can flip the paradigm of the generator. Instead of focusing on what it did
    right, we can make it focus on what it did wrong:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程寻找梯度信号，这些信号或多或少地告诉梯度下降该走哪条路。在`minimax`函数中，梯度下降的最大信号位于右侧，但我们实际上希望它学习位于函数左侧的值，在那里它被最小化为零，并且生成器成功欺骗了判别器。然而，随着生成器的优化，它将偏离最优点，导致我们远离应该到达的位置。为了解决这个问题，我们可以反转生成器的范式。我们可以让它专注于它做错的事情，而不是专注于它做对的事情：
- en: '![](img/efaa5ce7-54a4-4f9d-a6c6-0a45966a7db8.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efaa5ce7-54a4-4f9d-a6c6-0a45966a7db8.png)'
- en: By taking the maximum of the generator's objective, we're maximizing the likelihood
    of being wrong. This parallelized training process can still be unstable, however,
    and stabilizing GANs is a very active area of research at the moment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化生成器的目标，我们实际上是在最大化错误的可能性。然而，这种并行训练过程仍然可能不稳定，稳定GAN仍然是当前非常活跃的研究领域。
- en: 'Let''s get back to the TensorFlow process. We''ll start by defining our network''s
    training parameters:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到TensorFlow的过程。我们将从定义网络的训练参数开始：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then need to define our placeholders, both for the input `x`, as well as
    the `z` distribution which the generator will generate from:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义我们的占位符，既包括输入`x`，也包括生成器将从中生成的`z`分布：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Like before, we''ll create a Glorot `Initializer` that will initialize our
    weight and bias values for us:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将创建一个Glorot `Initializer`来初始化我们的权重和偏置值：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have all of this, we can go ahead and actually define our network pieces.
    You''ll notice that for the discriminator, we''re using something called a scope.
    Scopes allow us to reuse items from the TensorFlow graph without generating an
    error - in this case, we want to use the variables from the discriminator function
    twice in a row, so we use the `tf.variable_scope` function that TensorFlow provides
    us. Between the two, we simply use the `scope.reuse_variables()` function to tell
    TensorFlow what we''re doing:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了所有这些内容，我们就可以开始实际定义我们的网络部分了。你会注意到，在判别器部分，我们使用了一个叫做作用域（scope）的东西。作用域允许我们重复使用TensorFlow图中的项而不会产生错误——在这种情况下，我们想要在连续两次中使用判别器函数中的变量，所以我们使用了TensorFlow提供的`tf.variable_scope`函数。两者之间，我们只需使用`scope.reuse_variables()`函数来告诉TensorFlow我们在做什么：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Lastly, we''ll define the loss functions for both the generator and discriminator,
    and set the optimizer:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将为生成器和判别器定义损失函数，并设置优化器：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can the run the training cycle just as we have in the previous two examples.
    The only two differences you''ll see here is that we run two optimization processes,
    one for the generator and one for the discriminator:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像前两个示例那样运行训练周期。你会看到的唯一两个不同点是，我们运行了两个优化过程，一个针对生成器，一个针对判别器：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: GANs are fairly computational expensive, so training this network may take a
    while unless you scale with a web services platform.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的计算开销相当大，因此训练该网络可能需要一段时间，除非你使用Web服务平台进行扩展。
- en: As you can see, all of the models that we've run thus far have built upon each
    other. Even with advanced generative models like GANs, we can use certain recipes
    to create powerful neural networks, and larger AI applications, quickly and efficiently.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们迄今为止运行的所有模型都是相互依赖的。即使是像GANs这样的先进生成模型，我们也可以使用某些方法来快速高效地创建强大的神经网络和更大的AI应用。
- en: Other forms of generative models
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他形式的生成模型
- en: 'While we''ve only covered two types of generative model, there are many different
    types that you may encounter in the literature. The following chart is not exhaustive,
    but does provide a general overview of the types of generative models out there:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们只介绍了两种生成模型，但在文献中你可能会遇到许多不同类型的生成模型。以下图表并非详尽无遗，但它提供了关于现有生成模型类型的一个总体概览：
- en: '![](img/37d3c00a-b73f-4f97-a532-4a468b6f1171.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37d3c00a-b73f-4f97-a532-4a468b6f1171.png)'
- en: 'Let''s break this down:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细分析一下：
- en: '**Explicit density models**: Model our data directly from a probability distribution.
    We explicitly define the probability and solve for it'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显式密度模型**：直接从概率分布建模我们的数据。我们显式地定义概率并求解它。'
- en: '**Implicit density models**: Learn to sample from a probability distribution
    without defining what that distribution is'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐式密度模型**：学会从概率分布中采样，而无需定义该分布是什么'
- en: Within explicit density models, we have **tractable density** models and **approximate
    density ...**
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在显式密度模型中，我们有**可解密度**模型和**近似密度...**
- en: Fully visible belief nets
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全可见的信念网络
- en: 'Fully visible belief networks are a class of explicit density models and a
    form of deep belief network. They use the chain rule to decompose a probability
    distribution ![](img/f74fdadd-39f8-4b3b-a292-aea743773a22.png) over a vector,
    into a product over each of the members of the vector, represented between by
    ![](img/1b1c3bcf-cf33-4ca8-88e2-8be9f01139e0.png). All together, it''s formula
    is:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 完全可见的信念网络是一类显式密度模型，也是深度信念网络的一种形式。它们使用链式法则将概率分布！[](img/f74fdadd-39f8-4b3b-a292-aea743773a22.png)
    分解成一个向量上的每个成员的乘积，如下所示：！[](img/1b1c3bcf-cf33-4ca8-88e2-8be9f01139e0.png)。总的来说，它的公式是：
- en: '![](img/44925bb5-891f-4ab8-afb4-8f1a68efef10.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44925bb5-891f-4ab8-afb4-8f1a68efef10.png)'
- en: 'The most popular model in this family is PixelCNN, an **autoregressive** generative
    model. Pixels approach image generation problems by turning them into a sequence
    modeling problem, where the next pixel value is determined by all the previously
    generated pixel values. The network scans an image one pixel at a time, and predicts
    conditional distributions over the possible pixel values. We want to assign a
    probability to every pixel image based on the last pixels that the network saw.
    For instance, if we''re looking at the same horse images as in the previous example,
    we would be consistently predicting what the next anticipated pixel looks such
    as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个家族中最流行的模型是PixelCNN，它是一个**自回归**生成模型。像素通过将图像生成问题转化为序列建模问题来处理图像生成问题，在这种方法中，下一像素的值由所有先前生成的像素值决定。网络一次扫描一个像素，并预测可能的像素值的条件分布。我们希望根据网络看到的最后几个像素给每个像素图像分配一个概率。例如，如果我们看的是与之前例子相同的马匹图像，我们将持续预测接下来预期的像素值如下所示：
- en: '![](img/ea5df82a-188b-4d1d-8600-342f9995cee1.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea5df82a-188b-4d1d-8600-342f9995cee1.png)'
- en: Based on the features that we've seen, will the next pixel still contain the
    horse's ear, or will it be background? While their training cycles are more stable
    than GANs, the biggest issue with the networks is that they generate new samples
    extremely slowly; the model must be run again fully in order to generate a new
    sample. They also block the execution, meaning that their processes cannot be
    run in parallel.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们已看到的特征，下一像素是否仍会包含马耳朵，还是会是背景？虽然它们的训练周期比GAN更稳定，但这些网络的最大问题是它们生成新样本的速度极慢；模型必须完全重新运行才能生成新样本。它们还会阻塞执行，意味着它们的过程不能并行运行。
- en: Hidden Markov models
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: A hidden Markov model is a type of **Markov model**, which is itself a subclass
    of **Dynamic Bayesian Networks**.Markov models are used to model randomly changing
    systems called **Markov processes** also called **Markov chains**. Simply put,
    a Markov process is a sequence of events where the probability of an event happening
    solely depends on the previous event.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型是一种**马尔可夫模型**，它本身是**动态贝叶斯网络**的一个子类。马尔可夫模型用于建模随机变化的系统，称为**马尔可夫过程**，也称为**马尔可夫链**。简而言之，马尔可夫过程是一系列事件，其中事件发生的概率仅依赖于前一个事件。
- en: 'Markov chains appear as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链如下所示：
- en: '![](img/747b092b-e6c7-4ffd-8b43-bcb1d522d7fc.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/747b092b-e6c7-4ffd-8b43-bcb1d522d7fc.png)'
- en: In this simple chain, there are three states, represented by the circles. We
    then have probabilities for transitioning to another state, as well as probabilities
    of staying in a current state. The classic example of a Markov chain is that of
    the ...
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的链中，有三个状态，通过圆圈表示。我们接着有从一个状态过渡到另一个状态的概率，以及停留在当前状态的概率。马尔可夫链的经典例子是...
- en: Boltzmann machines
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 博尔兹曼机
- en: 'Boltzmann machines are a general class of models that contain take binary vectors
    as input and units that assign a probability distribution to each of those binary
    vectors. As you can see in the following diagram, each unit is dependent on every
    other unit:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 博尔兹曼机是一类通用模型，输入为二进制向量，并且单元为每个二进制向量分配一个概率分布。正如下图所示，每个单元都依赖于其他所有单元：
- en: '![](img/7692c3f7-5955-49d3-afe4-bc5d0595b3cf.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7692c3f7-5955-49d3-afe4-bc5d0595b3cf.png)'
- en: A Boltzmann machine uses something called an **energy function**, which is similar
    to a loss function. For any given vector, the probability of a particular state
    is proportional to each of the energy function values. To convert this to an actual
    probability distribution, it's necessary to renormalize the distribution, but
    this problem becomes another intractable problem. Monte Carlo methods are again
    used here for sampling as a workaround, hence making Boltzmann machines a Monte
    Carlo-based method.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机使用一种叫做**能量函数**的东西，这与损失函数类似。对于任何给定的向量，特定状态的概率与每个能量函数值成正比。为了将其转换为实际的概率分布，需要对分布进行重新归一化，但这个问题又变成了另一个难以解决的问题。蒙特卡洛方法在这里再次被用作抽样的替代方案，从而使得
    Boltzmann 机成为一种基于蒙特卡洛的方法。
- en: Let's say we have documents that are represented by binary features. A Boltzmann
    machine can help us determine whether a particular word or phrase came from a
    particular document. We can also use Boltzmann machines for anomaly detection
    in large, complex systems. They work well up to a point, although this method
    does not work well in high dimensional spaces.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有通过二进制特征表示的文档。Boltzmann 机可以帮助我们判断某个特定的单词或短语是否来自特定的文档。我们还可以使用 Boltzmann 机进行大规模复杂系统中的异常检测。它们在一定程度上表现良好，尽管在高维空间中这种方法效果不佳。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about some of the most exciting networks in AI,
    variational autoencoders and GANs. Each of these relies on the same fundamental
    concepts of condensing data, and then generating from again from that condensed
    form of data. You will recall that both of these networks are probabilistic models,
    meaning that they rely on inference from probability distributions in order to
    generate data. We worked through examples of both of these networks, and showed
    how we can use them to generate new images.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了人工智能中一些最令人兴奋的网络：变分自编码器和生成对抗网络（GANs）。这些网络都依赖于相同的基本概念——压缩数据，然后再从压缩后的数据中生成内容。你会记得这两种网络都是概率模型，意味着它们依赖于从概率分布中推断来生成数据。我们通过示例展示了这两种网络，并展示了如何使用它们来生成新图像。
- en: In addition to learning about these exciting new techniques, most importantly
    you learned that the building blocks of advanced networks can be broken down into
    smaller, simpler, and repetitive parts. When you think about ...
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习这些令人兴奋的新技术外，最重要的是你学到了高级网络的构建块可以被分解成更小、更简单且重复的部分。当你思考……
- en: References
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[statisticshowto.com](http://www.statisticshowto.com/)'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[statisticshowto.com](http://www.statisticshowto.com/)'
- en: Figure adapted from Ian Goodfellow, Tutorial on GANs, 2017
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图片摘自 Ian Goodfellow, 《GANs 教程》，2017
