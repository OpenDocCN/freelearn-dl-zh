- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Understanding Text with Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自然语言处理理解文本
- en: '**Natural Language Processing** (**NLP**) is the field of machine learning
    that deals with the understanding of language, in the form of text data. It is
    one of the fields that has seen a strong evolution in the last few years, achieving
    great results in the areas of sentiment analysis, chatbots, text summarization,
    and machine translation. NLP is at the core of the assistants developed by Amazon
    (Alexa), Google, and Apple (Siri), as well as modern assistants such as ChatGPT
    and Llama 2.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是机器学习的一个领域，专注于理解语言（以文本数据形式呈现）。在过去几年中，它是发展最快的领域之一，取得了在情感分析、聊天机器人、文本摘要和机器翻译等方面的显著成果。NLP是亚马逊（Alexa）、谷歌和苹果（Siri）等开发的助手的核心，也是像ChatGPT和Llama
    2这样的现代助手的核心。'
- en: In this chapter, we will learn how to use GluonNLP, an MXNet Gluon library specific
    to NLP, how to build our own networks, and how to use its Model Zoo API for several
    applications of pre-trained models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用GluonNLP，这是一个专门用于NLP的MXNet Gluon库，如何构建我们自己的网络，以及如何使用其模型库API进行几个预训练模型的应用。
- en: 'Specifically, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主题：
- en: Introducing NLP networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍NLP网络
- en: Classifying news highlights with topic modeling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主题建模分类新闻要点
- en: Analyzing sentiment in movie reviews
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析电影评论中的情感
- en: Translating text from Vietnamese to English
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从越南语翻译到英语
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply to this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*前言*中指定的技术要求外，本章还适用以下技术要求：
- en: Ensure that you have completed *Recipe 1*, *Installing MXNet, Gluon, GluonCV
    and GluonNLP*, from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running*
    *with MXNet*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了*食谱 1*，*安装MXNet、Gluon、GluonCV和GluonNLP*，来自[*第1章*](B16591_01.xhtml#_idTextAnchor016)，*与MXNet一起启动并运行*。
- en: 'Ensure that you have completed *Recipe 4*, *Toy dataset for text classification:
    Load, manage, and visualize a spam email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了*食谱 4*，*文本分类的玩具数据集：加载、管理和可视化垃圾邮件数据集*，来自[*第2章*](B16591_02.xhtml#_idTextAnchor029)，*使用MXNet和可视化数据集：Gluon*
    *和DataLoader*。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub URL找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06)
- en: 'Furthermore, you can access each recipe directly from Google Colab; for example,
    use the following link for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接从Google Colab访问每个食谱；例如，使用以下链接访问本章的第一个食谱：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb)。
- en: Introducing NLP networks
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍NLP网络
- en: In the previous chapters, we saw how different architectures, such as **Multi-Layer
    Perceptrons** (**MLPs**) and **Convolutional Neural Networks** (**CNNs**), deal
    with numerical data and images, respectively. In this recipe, we will analyze
    the most important architectures to process natural language expressed as text
    data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们了解了不同的架构，如**多层感知器**（**MLPs**）和**卷积神经网络**（**CNNs**），分别如何处理数值数据和图像。在本章节中，我们将分析处理自然语言（以文本数据形式表达）的最重要架构。
- en: The most important characteristic of natural language is that it is a list of
    words of variable length, and the order of those words matters; it is a sequence.
    The previous architectures that we have analyzed are not suited for variable-length
    data inputs and also do not exploit the relationships among words effectively.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言最重要的特点是它是一个可变长度的单词列表，而这些单词的顺序很重要；它是一个序列。我们分析的之前的架构并不适合可变长度的数据输入，并且没有有效地利用单词之间的关系。
- en: 'In this recipe, we will introduce neural networks that have been developed
    to process sequences of words:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍为处理单词序列而开发的神经网络：
- en: We will start by applying the network introduced in the previous chapter, that
    is, CNNs for text processing, called **TextCNNs**.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从应用前一章中介绍的网络开始，即用于文本处理的 CNN，称为 **TextCNNs**。
- en: Afterward, we will introduce **Recurrent Neural Networks** (**RNNs**) and their
    vanilla implementation. Then, we will continue with an improved version known
    as **Long Short-Term** **Memory** (**LSTM**).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将介绍 **递归神经网络**（**RNNs**）及其基础实现。接着，我们将继续介绍改进版，即 **长短期记忆**（**LSTM**）。
- en: Then, as we did with computer vision, we will introduce **GluonNLP Model Zoo**,
    one of the most value-adding features of MXNet. We will leverage these libraries
    MXNet and GluonNLP to understand and implement **transformers** and their self-attention
    mechanisms, and how these networks deal with sequences of variable length.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，正如我们在计算机视觉中所做的那样，我们将介绍 **GluonNLP 模型库**，这是 MXNet 最具价值的功能之一。我们将利用这些库 MXNet
    和 GluonNLP 来理解和实现 **transformers** 及其自注意力机制，并研究这些网络如何处理变长的序列。
- en: Getting ready
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在这个配方中，我们将使用一些矩阵运算和线性代数，但这些操作不会很难。
- en: How to do it...
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be doing the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将执行以下操作：
- en: Applying CNNs for text processing (TextCNNs)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 CNNs 进行文本处理（TextCNNs）
- en: Introducing RNNs
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 RNNs
- en: Improving RNNs with LSTM
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进 RNNs 使用 LSTM
- en: Introducing GluonNLP Model Zoo
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍 GluonNLP 模型库
- en: Paying attention to Transformers
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关注 Transformers
- en: Let’s go through each of these network architectures in detail next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细介绍这些网络架构。
- en: Applying CNNs for text processing
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用 CNNs 进行文本处理
- en: CNNs were introduced in the previous chapter and are typically used for working
    with images. However, with some slight changes, CNNs can work very efficiently
    with text data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 在前一章中被介绍，通常用于处理图像。然而，通过一些轻微的修改，CNNs 也能非常高效地处理文本数据。
- en: 'Images are 2D data and, as shown in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, we worked with two layers on this data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是二维数据，如在 [*第 5 章*](B16591_05.xhtml#_idTextAnchor098) 中所示，*计算机视觉中的图像分析*，我们在这些数据上使用了两个层：
- en: 2D convolutions layers
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D 卷积层
- en: Max pooling layers
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大池化层
- en: 'These operations are slightly modified to work with text data, which can be
    seen as a 1D sequence. Therefore, for 1D convolution layers, we have the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作经过轻微修改以适应文本数据，文本数据可以看作是 1D 序列。因此，对于一维卷积层，我们有以下内容：
- en: '![Figure 6.1 – 1D convolution](img/B16591_06_1.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 一维卷积](img/B16591_06_1.jpg)'
- en: Figure 6.1 – 1D convolution
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 一维卷积
- en: As can be seen in *Figure 6**.1*, the sequence combined with the kernel varies
    over time, yielding a new sequence as output. Please note how the number of words
    that are analyzed at the same time is the kernel size (3 in the preceding figure).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 6.1* 所示，序列与卷积核随时间变化，输出产生新的序列。请注意，同时分析的词汇数目是卷积核大小（前图中为 3）。
- en: 'For the max pooling layers, as we only have one dimension, which corresponds
    to time, these layers are known as **max-over-time** **pooling** layers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最大池化层，由于我们只有一个维度（即时间），这些层被称为 **时间最大池化** 层：
- en: '![Figure 6.2 – Max-over-time pooling](img/B16591_06_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 时间最大池化](img/B16591_06_2.jpg)'
- en: Figure 6.2 – Max-over-time pooling
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 时间最大池化
- en: As can be seen from *Figure 6**.2*, the maximum value from the sequence is selected.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 6.2* 所示，序列中的最大值被选中。
- en: 'Using MXNet Gluon, we can define the 1D convolution and max-over-time layers
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MXNet Gluon，我们可以如下定义一维卷积和时间最大池化层：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: An example of how to work with these layers can be found in the GitHub code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用这些层的示例可以在 GitHub 代码中找到。
- en: As with the CNN architecture shown in the previous chapter (see *Figure 5**.6*),
    typically, after a feature learning phase, we have a classifier. As an example
    application, this kind of architecture will help us later with sentiment analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如同前一章中展示的 CNN 架构（参见 *图 5.6*），通常，在特征学习阶段之后，我们会有一个分类器。作为示例应用，这种架构将帮助我们后续进行情感分析。
- en: Introducing Recurrent Neural Networks (RNNs)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍递归神经网络（RNNs）
- en: As discussed in the recipe introduction, RNNs are architectures that deal with
    sequences of data of variable length. For NLP, these data points are sentences,
    composed of words, but they can also be utilized for sequences of images (video),
    for example.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如食谱介绍中所讨论，RNN是处理可变长度数据序列的架构。对于NLP，这些数据点是由单词组成的句子，但它们也可以用于图像序列（视频）等。
- en: RNN’s history is a series of step-by-step attempts to improve the recurrent
    processing of different inputs of a sequence. There have been several important
    contributions, the most notable being Hopfield (1982), Jordan (1986), and Elman
    (1990).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的历史是一系列逐步尝试，旨在改进对序列中不同输入的递归处理。已经有几个重要的贡献，最著名的包括Hopfield（1982）、Jordan（1986）和Elman（1990）。
- en: The idea behind RNNs is that once the output is processed from the input, that
    output is fed again to the model (in a recurrent manner), in combination with
    the new incoming input. This mechanism allows the model to have memory (can access
    past data) and process the new input, taking into account that past information
    as well. This basic architecture is typically referred to as a **vanilla RNN**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的理念是，一旦从输入中处理出输出，这个输出会再次被送入模型（以递归的方式），并与新的输入结合。这个机制使得模型能够具有记忆（可以访问过去的数据）并处理新的输入，同时考虑到过去的信息。这个基本架构通常被称为**基础RNN**。
- en: 'Please note that, as introduced in *Recipe 4*, *Understanding text datasets
    – loading, managing, and visualizing the Enron Email dataset*, in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, the inputs
    to NLP networks, including RNNs, are not the words obtained from the dataset,
    but numerical representations of those words, such as one-hot encoding or word
    embeddings.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如*食谱4*中所介绍的，*理解文本数据集——加载、管理和可视化Enron邮件数据集*，在[*第2章*](B16591_02.xhtml#_idTextAnchor029)中，*使用MXNet并可视化数据集：Gluon和DataLoader*，NLP网络的输入，包括RNN，不是从数据集中获得的单词，而是这些单词的数字表示形式，如独热编码或词嵌入。
- en: 'If a sequence of data is modeled as successive inputs, x(1), x(2), .... x(t),
    the architecture of an RNN can be visualized as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将数据序列建模为连续的输入x(1)，x(2)，... x(t)，则RNN的架构可以如下可视化：
- en: '![Figure 6.3 – RNN architecture](img/B16591_06_3.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – RNN架构](img/B16591_06_3.jpg)'
- en: Figure 6.3 – RNN architecture
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – RNN架构
- en: 'In *Figure 6**.3*, we can see how every input *x(t)* is processed over time,
    and how the processed input (hidden state) is looped back for the next iterations.
    Let’s take a deeper look at what is happening at each step, as in the preceding
    figure, activation functions and biases are not shown for simplicity. The actual
    equations of a vanilla RNN cell are as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.3*中，我们可以看到每个输入*x(t)*如何随时间处理，以及处理后的输入（隐状态）如何循环回传递给下一次迭代。让我们更深入地了解每个步骤发生的事情，如前图所示，为了简化，激活函数和偏置未显示。基础RNN单元的实际方程式如下：
- en: '![Figure 6.4 – Equations for a vanilla RNN cell](img/B16591_06_4.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 基础RNN单元的方程式](img/B16591_06_4.jpg)'
- en: Figure 6.4 – Equations for a vanilla RNN cell
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 基础RNN单元的方程式
- en: As can be seen in *Figure 6**.4*, for each step, the input, *x(t)*, is multiplied
    by a weight matrix, *U*, and a bias vector, *b*, is added, which yields the value
    *a(t)*, assuming there was no previous input, *h(t – 1) = 0*. The state value,
    *h(t)*, is computed as the output of the activation function, *tanh*, of that
    value. When there is a previous input, the previous state value, *h(t – 1)*, is
    multiplied by a weight matrix, *W*, and added to the computations of values *a(t)*
    and *h(t)*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6.4*所示，对于每个步骤，输入*x(t)*乘以权重矩阵*U*，并加上偏置向量*b*，得出值*a(t)*，假设没有先前的输入*h(t – 1) =
    0*。状态值*h(t)*作为该值的激活函数*tanh*的输出计算。当有先前的输入时，先前的状态值*h(t – 1)*与权重矩阵*W*相乘，并加到值*a(t)*和*h(t)*的计算中。
- en: The state value, *h(t)*, is then multiplied by a weight matrix, *V*, and a bias
    vector, *c*, is added which yields the value *o(t)*. The output of the cell is
    computed as the output of the activation function, *softmax*, of that value, yielding
    the final output value *y(t)*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值*h(t)*然后乘以权重矩阵*V*，并加上偏置向量*c*，得出值*o(t)*。单元的输出作为该值的激活函数*softmax*的输出计算，最终得到输出值*y(t)*。
- en: These cells can be stacked together to produce a complete RNN.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些单元可以堆叠在一起，形成完整的RNN。
- en: 'Using MXNet and Gluon, we can easily create our own custom RNN networks:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MXNet和Gluon，我们可以轻松创建我们自己的自定义RNN网络：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can use this code to define a custom RNN:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码定义一个自定义RNN：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Furthermore, we use the following to process a sequential input:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用以下方法处理顺序输入：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The previous code shown runs a custom RNN. Feel free to play with the notebook
    available in the GitHub repository accompanying this book.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前面展示的代码运行的是一个自定义的RNN。欢迎在本书附带的GitHub仓库中的笔记本上进行实验。
- en: To conclude, one advantage of RNNs is that the information present in previous
    inputs is kept stored in the states transferred along time steps. However, this
    information is constantly multiplied by different weight matrices and passed through
    non-linear functions (*tanh*), and the outcome is that, after several time steps,
    the state information is modified and does not work anymore as memory; the information
    stored has changed too much. Long-term memory storage is a problem for RNNs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RNN的一个优点是，先前输入中存在的信息会保存在随着时间步传递的状态中。然而，这些信息会不断地被不同的权重矩阵乘以并通过非线性函数（*tanh*）传递，结果是，在多个时间步后，状态信息被修改，不再作为记忆起作用；存储的信息发生了过大的变化。长时记忆存储是RNN的一个问题。
- en: Training RNNs
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练RNN
- en: In the previous chapters, we saw how to train networks using supervised learning,
    by computing the loss function between the expected outputs, the **ground truth**,
    and the actual outputs of the network. This error could then be back-propagated
    from the outer layers of the network to the inner layers of the network and update
    the weights of these layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到如何通过使用监督学习来训练网络，通过计算期望输出（**真实值**）与网络实际输出之间的损失函数。然后，这个误差可以从网络的外层向内层进行反向传播，并更新这些层的权重。
- en: With RNNs, at each time step, the network is shown a sequence of inputs and
    the expected sequence in the outputs. Errors are computed for each time step and
    back-propagated from the outer layers of the network to the inner layers of the
    network. This variation, suitable for RNNs, is called **Back-Propagation Through**
    **Time** (**BPTT**).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RNN，在每一个时间步，网络会接收一个输入序列和期望的输出序列。每个时间步的误差都会被计算出来，并从网络的外层反向传播到内层。这个适用于RNN的变化称为**时间反向传播**（**BPTT**）。
- en: 'As with other networks, computing the different gradients involves the iterative
    multiplication of matrices. This operation is exponential, which means that, after
    several occurrences, the values will either shrink or blow up. This leads to a
    problem we have already discussed: **vanishing gradients** and **exploding gradients**.
    These issues make the training of RNNs very unstable. Another important drawback
    of BPTT is that as it is a sequential computation, it cannot be parallelized.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他网络一样，计算不同梯度涉及矩阵的迭代乘法。这个操作是指数级的，这意味着，在多次出现后，数值要么会收缩，要么会爆炸。这导致了我们之前讨论过的问题：**梯度消失**和**梯度爆炸**。这些问题使得RNN的训练非常不稳定。BPTT的另一个重要缺点是，由于它是顺序计算，无法并行化。
- en: Improving RNNs with Long Short-Term Memory (LSTM)
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用长短期记忆（LSTM）改进RNN
- en: 'LSTMs were introduced by Hochreiter and Schmidhuber in 1997, as a mechanism
    to solve the problems described previously (lack of long-term memory and vanishing/exploding
    gradients). In LSTMs, instead of having the previous state multiplied and passed
    through the non-linear function, the connection is much more straightforward.
    To provide this mechanism, each LSTM cell receives two inputs from the previous
    cell: the **hidden state** (**ht**) and the **cell** **state** (**ct**):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM由Hochreiter和Schmidhuber于1997年提出，作为解决前述问题（缺乏长期记忆和梯度消失/爆炸）的一种机制。在LSTM中，与其让前一个状态被乘以并通过非线性函数传递，连接变得更加直接。为了提供这一机制，每个LSTM单元从前一个单元接收两个输入：**隐藏状态**（**ht**）和**单元状态**（**ct**）：
- en: '![Figure 6.5 – LSTM network](img/B16591_06_5.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – LSTM网络](img/B16591_06_5.jpg)'
- en: Figure 6.5 – LSTM network
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – LSTM网络
- en: 'The key components of LSTMs are called gates, which define how a certain input
    is modified to become a part of the outputs. These vectors have values between
    0 and 1 and help activate/deactivate the information from the input. They are,
    therefore, a sigmoid operation (depicted in *Figure 6**.5* as *σ*) of a weighted
    sum of the inputs, followed by a multiplication operation. To emphasize, each
    of the three gates present in an LSTM cell allows how much of the input or the
    state passes through each of the outputs. Taking this into account, the following
    equations define the LSTM behavior:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的关键组件被称为门，它们定义了某个输入如何被修改并成为输出的一部分。这些向量的值介于0和1之间，帮助激活/停用来自输入的信息。因此，它们是对加权输入和的sigmoid操作（在*图
    6**.5*中表示为*σ*），接着进行乘法运算。值得强调的是，LSTM单元中存在的三个门决定了输入或状态中有多少信息可以通过每个输出。考虑到这一点，以下方程定义了LSTM的行为：
- en: "![Figure 6.6 – Equations for \uFEFFan LSTM cell](img/B16591_06_6.jpg)"
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – LSTM单元的方程](img/B16591_06_6.jpg)'
- en: Figure 6.6 – Equations for an LSTM cell
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – LSTM单元的方程
- en: 'The equations in *Figure 6**.6* can be explained as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.6* 中的方程可以解释如下：'
- en: '**Input gate (it)**: This is the gate that decides how the information from
    the previous state and the current input will be updated.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门 (it)**：这是决定前一状态信息和当前输入信息如何被更新的门。'
- en: '**Forget gate (ft)**: This is the gate that decides how the information from
    the previous state and the current input will become part of the long-term memory
    (cell state). This is how much of the current step we want to forget.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门 (ft)**：这是决定前一状态信息和当前输入信息如何成为长期记忆（单元状态）一部分的门。它决定了我们想忘记当前步骤的多少。'
- en: '**Memory cell candidate (gt)**: This is the computation that decides how the
    information from the previous state and the current input will become part of
    the memory cell. It must allow for positive and negative values; therefore, *tanh*
    is the activation function selected.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆单元候选值 (gt)**：这是决定前一状态信息和当前输入信息如何成为记忆单元一部分的计算过程。它必须允许正负值，因此，*tanh* 是选用的激活函数。'
- en: '**Output gate (ot)**: This is the gate that decides how the information from
    the previous state and the current input will become part of the output.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门 (ot)**：这是决定前一状态信息和当前输入信息如何成为输出一部分的门。'
- en: '**Memory cell (ct)**: This is the computation that combines the previous state
    (*c*t-1) and the current memory cell candidate (*g*t) into the new cell state'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆单元 (ct)**：这是将前一状态（*c*t-1）和当前记忆单元候选值（*g*t）合并为新的单元状态的计算过程。'
- en: '**Output state (ht)**: This is the computation that combines the memory cell
    with the output gate value.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出状态 (ht)**：这是将记忆单元与输出门值结合的计算过程。'
- en: 'Using MXNet and Gluon, we can easily create our own custom LSTM networks:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MXNet和Gluon，我们可以轻松创建自定义LSTM网络：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use the following code to define a custom RNN:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来定义一个自定义RNN：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Furthermore, we can use the following to process a sequential input:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用以下方法来处理顺序输入：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The code shown here runs a custom LSTM network. Feel free to play with the notebook
    available in the GitHub repository accompanying this book.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的代码运行的是一个自定义的LSTM网络。你可以随意修改这个GitHub仓库中的笔记本，来进行实验。
- en: To conclude, LSTMs allow RNNs to be trained more optimally and have been implemented
    to solve a large number of tasks in NLP, such as sentiment analysis and language
    modeling.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LSTM使得RNN能够更优化地进行训练，并已被应用于解决大量NLP任务，如情感分析和语言建模。
- en: Introducing GluonNLP Model Zoo
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍GluonNLP模型库
- en: One of the best features that MXNet GluonCV provides is its large pool of pre-trained
    models, readily available for its users to use and deploy in their own applications.
    This model library is called **Model Zoo**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet GluonCV提供的最优秀的功能之一是其庞大的预训练模型库，用户可以轻松使用并部署到自己的应用中。这个模型库被称为**Model Zoo**。
- en: 'In Model Zoo, models have been pre-trained for the following tasks:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在Model Zoo中，已对以下任务进行了预训练的模型：
- en: Language models
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型
- en: Sentiment analysis
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Machine translation
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Sentence classification
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子分类
- en: Question answering
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Named entity recognition
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Joint intent classification and slot labeling
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合意图分类和槽位标注
- en: In this chapter, we will examine in detail the pre-trained models included for
    sentiment analysis and machine translation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细讨论情感分析和机器翻译的预训练模型。
- en: Paying attention with Transformers
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Transformers进行注意力机制
- en: Although LSTMs have been proven to work well for a lot of applications, they
    also have significant drawbacks, including taking longer and requiring more memory
    to train, as well as being sensitive to random initialization. New architectures
    have been developed that overcome these limitations. One of the most important
    examples is **Transformers**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LSTM已被证明在许多应用中表现良好，但它们也有显著的缺点，包括训练时间较长、需要更多内存以及对随机初始化非常敏感。为克服这些限制，已经开发出了新的架构。其中最重要的一个例子就是**Transformer**。
- en: Transformers were introduced by Google Brain in 2017\. It is a novel approach
    of an encoder-decoder architecture (as seen in *Recipe 4*, *Segmenting objects
    in images with PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*) with a repurposed mechanism to process
    sequences of data, called **attention**. The largest improvement of this architecture
    is that it does not depend on processing the data sequentially. All the data can
    be processed in parallel, allowing for faster training and inference. This improvement
    allowed a very large amount of text, the **corpus**, to be processed, yielding
    **Large Language Models** (**LLMs**) such as **Bidirectional Encoder Representations
    from** **Transformers** (**BERT**).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是由Google Brain于2017年提出的。它是一种新的编码器-解码器架构（如在*食谱4*、*使用PSPNet和DeepLab-v3对图像中的物体进行分割*，见[
    *第5章*](B16591_05.xhtml#_idTextAnchor098)，*用计算机视觉分析图像*中所见），采用了一种重新设计的机制来处理数据序列，称为**注意力**。这种架构的最大改进在于它不依赖于顺序处理数据。所有数据可以并行处理，从而实现更快的训练和推理。这一改进使得可以处理非常大量的文本，**语料库**，并产生了**大规模语言模型**（**LLM**），例如**双向编码器表示的Transformer**（**BERT**）。
- en: 'The architecture of Transformers can be represented as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的架构可以表示如下：
- en: '![Figure 6.7 – Transformer architecture](img/B16591_06_7.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – Transformer架构](img/B16591_06_7.jpg)'
- en: Figure 6.7 – Transformer architecture
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – Transformer架构
- en: 'In *Figure 6**.7*, we can distinguish several components:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.7*中，我们可以区分出几个组件：
- en: '**Input and output preprocessing**: Embeddings and positional encodings are
    computed before inputting the data into the network.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入与输出预处理**：在将数据输入网络之前，嵌入和位置编码会被计算出来。'
- en: '**Encoder-decoder architecture**: The left part corresponds to the encoder
    and the right part corresponds to the decoder. Feed-forward processing, residual
    connections, and normalization are parts of this component.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器-解码器架构**：左侧部分对应编码器，右侧部分对应解码器。前馈处理、残差连接和归一化是该组件的一部分。'
- en: '**Attention heads**: The sequence inputs the path from the encoder to the decoder
    and the sequence outputs are all processed via this mechanism.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力头**：序列输入编码器到解码器的路径，序列输出都通过这一机制处理。'
- en: Let’s see each of these components in more detail.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些组件。
- en: Input and output preprocessing
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入与输出预处理
- en: In the original paper, the inputs and outputs, before being fed into the network,
    are processed through **learned embeddings**. These embedding vectors typically
    have a size of 512 elements.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，输入和输出在被送入网络之前，会通过**学习到的嵌入**进行处理。这些嵌入向量通常具有512个元素的大小。
- en: Furthermore, as we will see afterward, these embeddings are passed through a
    *softmax* function, combining several pieces of information into one number, but
    also losing the positional information in the process. To maintain the positional
    information through the whole encoding and decoding, for both inputs and outputs,
    the input embeddings are added as a vector with information on position. These
    are called **positional encodings**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们接下来所看到的，这些嵌入会通过一个*softmax*函数，将多个信息片段组合成一个数值，但在此过程中也会丢失位置信息。为了在整个编码和解码过程中保持位置信息，无论是输入还是输出，输入嵌入都会作为一个包含位置信息的向量添加。这些被称为**位置编码**。
- en: Encoder-decoder architecture
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: 'As noted in the original *Google Brain’s original Transformer* paper: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf
    ) the encoder and decoder are composed of six identical layers (in *Figure 6**.5*,
    *N = 6* in the left diagram and right diagram, respectively). Each encoder layer
    has two components, a multi-head self-attention mechanism followed by a fully
    connected feed-forward network. For each decoded layer, another attention component
    is added, a masked multi-head self-attention mechanism. Attention heads will be
    explained in the following section.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如原始*Google Brain 的 Transformer* 论文中所提到的：[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)，编码器和解码器由六个相同的层组成（在*图
    6.5*中，左图和右图中的*N = 6*）。每个编码器层有两个组件，一个是多头自注意力机制，后面接着一个全连接的前馈网络。对于每个解码层，另加一个注意力组件，即掩蔽的多头自注意力机制。注意力头将在后续部分进行解释。
- en: Residual connections are also added, similar to what we saw for ResNets in *Recipe
    2*, *Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet*,
    in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing Images with* *Computer
    Vision*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接也被加入，类似于我们在*Recipe 2* 中看到的 ResNets，[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)中*使用
    MXNet 进行图像分类 – GluonCV 模型库, AlexNet 和 ResNet*，以及在*计算机视觉分析*中。
- en: This information together is normalized using layer normalization, which is
    similar to batch normalization, introduced in *Recipe 3*, *Training for regression
    models*, in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression
    Problems*. The most important difference is that, as described in the paper introducing
    layer normalization ([https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)),
    with layer normalization, all the hidden units in a layer share the same normalization
    terms, but different input data can have different normalization terms. Layer
    normalization has been proven to work better than batch normalization for processing
    sequences.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息一起经过层归一化处理，类似于*Recipe 3*中介绍的批归一化，在[*第 3 章*](B16591_03.xhtml#_idTextAnchor052)中，*回归模型训练*，*解决回归问题*。最重要的区别是，如介绍层归一化的论文所述（[https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)），层归一化使得一个层中的所有隐藏单元共享相同的归一化项，但不同的输入数据可以有不同的归一化项。层归一化已被证明在处理序列时比批归一化效果更好。
- en: By combining all these layers across the different encoder and decoder steps
    (including the embeddings), the vectors will all have a dimension of 512.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合不同编码器和解码步骤中的所有这些层（包括嵌入层），这些向量将具有 512 的维度。
- en: Attention heads
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力头
- en: In Transformers, how each word in a sequence is connected to each of the words
    in another sequence is done via the attention mechanism. For example, if we have
    an input sequence with *N* words in English (*I love you very much*) and its translation
    to French has *M* words (*Je t’aime beaucoup*), the attention matrix of weights
    between these two sequences will have *NxM* dimensions. Connecting sequences using
    this mechanism has a strong advantage over a recurrent mechanism (such as the
    one used in RNNs), which is parallelization. Attention is a matrix operation and
    therefore can be optimally parallelized.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformers 中，一个序列中的每个单词如何与另一个序列中的每个单词相连接，是通过注意力机制实现的。例如，如果我们有一个包含*N*个单词的英语输入序列（*I
    love you very much*），并且它的法语翻译有*M*个单词（*Je t’aime beaucoup*），那么这两个序列之间的注意力权重矩阵将具有*NxM*
    的维度。使用这种机制连接序列相比于递归机制（例如 RNN 中使用的机制）有一个强大的优势——并行化。注意力是一个矩阵操作，因此可以被最佳化地并行化。
- en: '|  | I | love | you | very | much |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | I | love | you | very | much |'
- en: '| je | 0.90 | 0.02 | 0.06 | 0.01 | 0.01 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| je | 0.90 | 0.02 | 0.06 | 0.01 | 0.01 |'
- en: '| t’ | 0.11 | 0.01 | 0.80 | 0.03 | 0.05 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| t’ | 0.11 | 0.01 | 0.80 | 0.03 | 0.05 |'
- en: '| aime | 0.03 | 0.92 | 0.03 | 0.01 | 0.01 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| aime | 0.03 | 0.92 | 0.03 | 0.01 | 0.01 |'
- en: '| beaucoup | 0.02 | 0.02 | 0.02 | 0.41 | 0.53 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| beaucoup | 0.02 | 0.02 | 0.02 | 0.41 | 0.53 |'
- en: Figure 6.8 – Example of attention matrix
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 注意力矩阵示例
- en: 'In the Transformer paper, three of the matrices shown in *Figure 6**.9* were
    introduced, *Query (Q), Key (K), and Value (V).* The following is an explanation
    of each of these matrices:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 论文中，*图 6.9* 中显示了三个矩阵，*查询（Q），键（K）和值（V）*。以下是对这些矩阵的解释：
- en: '**Query**: This is the input representation of each input word'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**：这是每个输入单词的输入表示'
- en: '**Key and value**: Similar to a **hash map** that maps keys into values, these
    matrices are used for indexing (key) and providing a representation (value) of
    the word (different from the query)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键和值**：类似于一个**哈希映射**，将键映射到值，这些矩阵用于索引（键）和提供单词的表示（值）（与查询不同）'
- en: 'The combination of the operations carried out by these three matrices is called
    the dot-product attention function and can be described as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种矩阵操作的组合被称为点积注意力函数，可以描述如下：
- en: '![Figure 6.9 – Attention function](img/B16591_06_9.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 注意力函数](img/B16591_06_9.jpg)'
- en: Figure 6.9 – Attention function
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 注意力函数
- en: When the output is a representation of the input sequence, this mechanism is
    called a **self-attention head**. In the architecture diagram shown earlier in
    *Figure 6**.7*, the two attention mechanisms closer to the input sequence and
    the output sequence (lower part of the diagram) are self-attention mechanisms,
    as the sequence outputted by the function is the same as the sequence that is
    inputted. When this is not the case, such as in the attention mechanism that connects
    the encoder to the decoder, this is known as a **cross-attention head**. The self-attention
    head for the output vectors is masked, meaning that only past information is available
    in the training of the network. This allows Transformer models to generate text
    from a limited input (auto-regressive models such as GPT-3 or BLOOM).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出是输入序列的表示时，这个机制被称为**自注意力头**。在之前的架构图*图6.7*中，靠近输入序列和输出序列的两个注意力机制（图的下部）是自注意力机制，因为由该函数输出的序列与输入的序列相同。当情况不同，例如在连接编码器和解码器的注意力机制中，这被称为**交叉注意力头**。输出向量的自注意力头是被屏蔽的，这意味着在网络训练中只能使用过去的信息。这使得Transformer模型能够从有限的输入生成文本（如GPT-3或BLOOM等自回归模型）。
- en: 'Instead of processing all the input data in parallel, in Google Brain’s Attention
    is All You Need original paper ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    eight attention heads are used in parallel. As the output is expected to have
    the same dimensionality (512), each head works with a reduced vector (with a dimensionality
    of 64). This is described in the paper as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Brain的论文《Attention is All You Need》中（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)），并不是并行处理所有输入数据，而是使用了八个注意力头并行工作。由于期望输出具有相同的维度（512），每个头处理的是一个较小的向量（维度为64）。该论文描述如下：
- en: '"Multi-head attention allows the model to jointly attend to information from
    different representation subspaces at different positions."'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: “多头注意力使模型能够同时关注不同表示子空间中不同位置的信息。”
- en: Reducing the dimensionality allows for the total computation cost to be similar
    to using a complete (full-dimensionality) attention head.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 降低维度可以使总计算成本与使用完整（全维度）注意力头的成本相似。
- en: Implementation in GluonNLP
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在GluonNLP中的实现
- en: 'GluonNLP has its own implementation of a Transformer model and therefore, getting
    our encoder and decoder is straightforward, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: GluonNLP有自己实现的Transformer模型，因此获取我们的编码器和解码器是直接的，具体如下：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can use the encoder to process the inputs; however, with Transformers,
    we can process the whole input at the same time:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用编码器来处理输入数据；然而，使用Transformers时，我们可以同时处理整个输入：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Large-scale Transformers are the current state of the art for most NLP tasks,
    such as topic modeling, sentiment analysis, or question answering, and the encoder
    and decoder architectures are also used separately for different tasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模Transformer是目前大多数NLP任务的最先进技术，如主题建模、情感分析或问答，并且编码器和解码器架构也会单独用于不同的任务。
- en: How it works...
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we have introduced several networks to work with NLP using
    MXNet, Gluon, and GluonNLP:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们介绍了几个网络，通过MXNet、Gluon和GluonNLP来处理NLP任务：
- en: RNNs
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNNs
- en: LSTM
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM
- en: Transformers
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers
- en: We have reviewed how each of these architectures works and analyzed its advantages
    and disadvantages, as well as how each one has improved on the previous one, exploring
    concepts such as sequences, BPTT, memory cells, and attention.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经回顾了这些架构的工作原理，并分析了它们的优缺点，以及每个架构如何改进前一个架构，探索了序列、BPTT、内存单元和注意力等概念。
- en: In the following recipes, we will explore how to use these architectures to
    solve practical problems such as topic modeling, sentiment analysis, and machine
    translation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的方案中，我们将探索如何使用这些架构来解决实际问题，如主题建模、情感分析和机器翻译。
- en: There’s more...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Some of the concepts explored in this recipe are too advanced to cover in detail
    in this book. I strongly suggest taking a look at the following references if
    you would like to get a deeper understanding:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中讨论的一些概念过于先进，无法在本书中详细介绍。如果你希望更深入地理解这些内容，我强烈建议查看以下参考资料：
- en: '**TextCNNs (****Paper):** [https://aclanthology.org/D14-1181.pdf](https://aclanthology.org/D14-1181.pdf)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TextCNNs (****论文):** [https://aclanthology.org/D14-1181.pdf](https://aclanthology.org/D14-1181.pdf)'
- en: '**RNNs (Intuitive explanation):** [https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740](https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RNNs (直观解释):** [https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740](https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740)'
- en: '**RNNs (Backpropagation Through** **Time):** [https://d2l.ai/chapter_recurrentneural-networks/bptt.html](https://d2l.ai/chapter_recurrentneural-networks/bptt.html)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RNNs (时间反向传播):** [https://d2l.ai/chapter_recurrentneural-networks/bptt.html](https://d2l.ai/chapter_recurrentneural-networks/bptt.html)'
- en: '**Vanishing/exploding gradients research papers (Learning Long-Term Dependencies
    with Gradient Descent is** **Difficult):** [http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf](http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失/爆炸研究论文 (使用梯度下降学习长期依赖关系是** **困难的):** [http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf](http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf)'
- en: '**Vanishing/exploding gradients research papers: (On the difficulty of training
    Recurrent Neural** **Networks):** [https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失/爆炸研究论文：（递归神经** **网络训练的困难）:** [https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf)'
- en: '**Vanishing/exploding gradients research papers (The exploding gradient problem
    demystified - definition, prevalence, impact, origin, tradeoffs,and** **solutions):**
    [https://arxiv.org/pdf/1712.05577.pdf](https://arxiv.org/pdf/1712.05577.pdf)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失/爆炸研究论文（爆炸梯度问题揭示 —— 定义、普遍性、影响、起源、权衡和** **解决方案）:** [https://arxiv.org/pdf/1712.05577.pdf](https://arxiv.org/pdf/1712.05577.pdf)'
- en: '**LSTMs (****Paper):** [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTMs (****论文):** [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
- en: '**LSTMs (Intuitive** **explanation):** [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTMs (直观** **解释):** [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: '**Transformers (Original paper – Attention Is All You** **Need):** [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers (原始论文 – Attention Is All You** **Need):** [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)'
- en: '**Transformers (Intuitive** **explanation):** [https://towardsdatascience.com/transformers-89034557de14](https://towardsdatascience.com/transformers-89034557de14)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers (直观** **解释):** [https://towardsdatascience.com/transformers-89034557de14](https://towardsdatascience.com/transformers-89034557de14)'
- en: '**Transformers (Layer** **Normalization):** [https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers (层** **归一化):** [https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)'
- en: '**State-of-the-art models in NLP (GPT-3** **paper):** [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLP中的最先进模型 (GPT-3** **论文):** [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)'
- en: '**State-of-the-art models in NLP (BLOOM (open source** **alternative):** [https://huggingface.co/blog/bloom](https://huggingface.co/blog/bloom)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLP中的最先进模型 (BLOOM（开源** **替代方案）:** [https://huggingface.co/blog/bloom](https://huggingface.co/blog/bloom)'
- en: 'In this chapter, we are going to analyze in detail the following tasks: topic
    modeling, sentiment analysis, and text translation. However, MXNet GluonNLP Model
    Zoo contains lots of models pre-trained for a large number of tasks. You are encouraged
    to explore the different examples included at [https://nlp.gluon.ai/model_zoo/index.html](https://nlp.gluon.ai/model_zoo/index.html).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细分析以下任务：主题建模、情感分析和文本翻译。然而，MXNet GluonNLP模型库包含了许多预训练的模型，适用于大量任务。我们鼓励你探索在[https://nlp.gluon.ai/model_zoo/index.html](https://nlp.gluon.ai/model_zoo/index.html)上提供的不同示例。
- en: Classifying news highlights with topic modeling
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主题建模对新闻亮点进行分类
- en: In this recipe, we are going to study one of the most interesting tasks in NLP,
    topic modeling. In this task, the user must find the number of topics given a
    set of documents. Sometimes, the topics (and the number of topics) are known beforehand
    and the supervised learning techniques that we have seen in previous chapters
    can be applied. However, in a typical scenario, topic modeling datasets do not
    provide ground truth and are therefore unsupervised learning problems.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将研究自然语言处理（NLP）中最有趣的任务之一——主题建模。在这个任务中，用户必须在给定文档集的情况下找到主题数。 有时，主题（以及主题的数量）是已知的，此时可以应用我们在前几章中看到的监督学习技术。然而，在典型的场景中，主题建模数据集没有提供真实标签，因此是无监督学习问题。
- en: 'To achieve this, we will use a pre-trained model from GluonNLP Model Zoo and
    apply its word embeddings to feed a clustering algorithm, which will yield the
    clustered topics. We will apply this process to a new dataset: *1 Million* *News
    Headlines*.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用来自GluonNLP Model Zoo的预训练模型，并将其词嵌入应用于聚类算法，从而得出聚类的主题。我们将把这一过程应用于一个新的数据集：*100万*
    *新闻头条*。
- en: Getting ready
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在本食谱中，我们将使用一些矩阵运算和线性代数，但它并不难。
- en: 'Furthermore, we will be working with text datasets. Therefore, we will revisit
    some concepts already seen in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将处理文本数据集。因此，我们将重新回顾一些已经在*Recipe 4*中提到的概念，*理解文本数据集——加载、管理和可视化恩隆邮件数据集*，来自[*第
    2 章*](B16591_02.xhtml#_idTextAnchor029)，*与MXNet协作并可视化数据集：Gluon* *和DataLoader*。
- en: How to do it...
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将执行以下步骤：
- en: Exploring the *1 Million News* *Headlines* dataset
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索*100万新闻* *头条*数据集
- en: Applying word embeddings
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用词嵌入
- en: Clustering the topics using K-means
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用K-means聚类主题
- en: Putting everything together
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: Let’s go through each of these steps in the following subsections.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的子章节中逐步了解这些步骤。
- en: Exploring the 1 Million News Headlines dataset
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索100万新闻头条数据集
- en: This dataset is one of the most well-known datasets used for topic modeling.
    It contains 19 years of noteworthy news headlines published by the **Australian
    Broadcasting Corporation** (**ABC**) website, from 2003 to 2021 (inclusive). The
    topic of each news headline is not included.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是主题建模中最著名的数据集之一。它包含澳大利亚广播公司（**ABC**）网站从2003年到2021年（含）发布的19年的重要新闻头条。每个新闻头条的主题没有包括在内。
- en: 'As expected from a real-world dataset, the corpus contains a large number of
    words. Therefore, before going further, we will proceed to clean the data and
    follow the process described in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，真实世界的数据集包含大量的词汇。因此，在进一步处理之前，我们将开始清洗数据，并按照*Recipe 4*中描述的过程进行，*理解文本数据集——加载、管理和可视化恩隆邮件数据集*，来自[*第
    2 章*](B16591_02.xhtml#_idTextAnchor029)，*与MXNet协作并可视化数据集：Gluon* *和DataLoader*：
- en: Tokenizing
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Removing stop words
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Stemming
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatization
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原
- en: 'Furthermore, this dataset contains more than 1 million headlines (1.2 million,
    actually). In order to be able to process them in a time-efficient manner, we
    will work with a reduced subset of 5%:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个数据集包含超过100万个头条新闻（实际上是120万个）。为了能够高效地处理它们，我们将使用一个包含5%的子集：
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we analyze this subset to compute the number of words each headline has,
    we can plot the following histogram:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析这个子集来计算每个头条新闻的词数，我们可以绘制出以下的直方图：
- en: '![Figure 6.10 – Distribution of headlines by number of words](img/B16591_06_10.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 按词数分布的头条新闻](img/B16591_06_10.jpg)'
- en: Figure 6.10 – Distribution of headlines by number of words
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 按词数分布的头条新闻
- en: As can be seen in *Figure 6**.10*, most of the headlines have between 4 and
    8 words.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*图 6.10*中所示，大多数头条新闻包含4到8个词。
- en: Applying word embeddings
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用词嵌入
- en: 'In *Recipe 4*, *Understanding text datasets – loading, managing, and visualizing
    the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we introduced
    two embedding models from GluonNLP Model Zoo: Google’s **word2vec** and **GloVe**
    from Stanford University. For this use case, we are going to work with word2vec,
    because it was trained on a dataset called Google News, composed of 3 million
    words and phrases from a corpus of 100 billion words. As the corpus is composed
    of news information, this embedding model is very well suited for our use case.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在*食谱 4*中，*理解文本数据集 – 加载、管理和可视化 Enron 邮件数据集*，来自于 [*第二章*](B16591_02.xhtml#_idTextAnchor029)，*使用
    MXNet 和可视化数据集：Gluon 和 DataLoader*，我们介绍了来自 GluonNLP 模型库的两个嵌入模型：谷歌的**word2vec**和斯坦福大学的**GloVe**。在这个使用案例中，我们将使用
    word2vec，因为它是在一个名为 Google News 的数据集上训练的，该数据集由 3 百万单词和短语组成，来源于一个包含 1000 亿个单词的语料库。由于该语料库由新闻信息组成，这个嵌入模型非常适合我们的使用案例。
- en: '![Figure 6.11 – 2D representation of word2vec embeddings](img/B16591_06_11.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – word2vec 嵌入的 2D 表示](img/B16591_06_11.jpg)'
- en: Figure 6.11 – 2D representation of word2vec embeddings
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – word2vec 嵌入的 2D 表示
- en: By using this model, each word is transformed into a vector with 300 components.
    However, for our application with headlines (full sentences), we are more interested
    in a representation of the complete headline and not just its independent words.
    A simple but effective method to accomplish this for our application is to compute
    the average vector of each preprocessed word.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个模型时，每个单词都会被转换为一个包含 300 个分量的向量。然而，在我们的应用中，处理的是标题（完整句子），我们更关心的是整个标题的表示，而不仅仅是其独立的单词。对于我们的应用，完成这一目标的一个简单有效的方法是计算每个预处理单词的平均向量。
- en: Clustering the topics using K-means
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 K-means 聚类主题
- en: With our headline embeddings ready, the last step to classify our headlines
    is to cluster them. There are many clustering algorithms, such as **expectation
    maximization clustering** and **mean shift clustering**. However, for our application,
    we will use my favorite one, **K-means**, which is implemented in a Python package
    we have already talked about, **scikit-learn**.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到标题嵌入后，分类标题的最后一步就是聚类它们。有很多聚类算法，比如**期望最大化聚类**和**均值漂移聚类**。然而，对于我们的应用，我们将使用我最喜欢的算法——**K-means**，它在我们之前提到的
    Python 包 **scikit-learn** 中已经实现。
- en: '![Figure 6.12 – K-means visualization](img/B16591_06_12.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – K-means 可视化](img/B16591_06_12.jpg)'
- en: Figure 6.12 – K-means visualization
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – K-means 可视化
- en: The intuitive idea behind K-means is that given a number of clusters, *K*, it
    will assign the clusters’ centroids randomly, and add each newly seen vector to
    the closest centroid (assignment step). Then, it will compute the new centroid
    as the mean of the vectors that belong to the cluster (update step) and iterate.
    This process is repeated until there is no significant change in the centroids’
    positions. Therefore, the full dataset can be iterated several times. For large
    datasets, other criteria can be added for convergence and stopping criteria.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 背后的直观思想是，给定一个聚类数量 *K*，它会随机分配聚类的质心，并将每个新看到的向量分配给距离最近的质心（分配步骤）。然后，它会计算新质心，作为属于该聚类的向量的均值（更新步骤），并迭代此过程。这个过程会一直重复，直到质心位置没有显著变化。因此，整个数据集可以多次迭代。对于大型数据集，可以添加其他标准来确定收敛和停止标准。
- en: One important drawback of K-means is that the number of clusters is an input
    parameter, and therefore, it requires some intuition or knowledge about the dataset.
    In practice, knowing this information beforehand is difficult. Therefore, I strongly
    recommend running the algorithm for several different values of *K*. For our use
    case, the number of clusters chosen is 4.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 的一个重要缺点是聚类的数量是一个输入参数，因此需要对数据集有一定的直觉或知识。实际上，事先知道这个信息是很困难的。因此，我强烈建议针对不同的
    *K* 值运行算法。对于我们的使用案例，选择的聚类数量是 4。
- en: Putting everything together
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有步骤整合在一起
- en: After this three-step process (cleaning the data, embedding, and clustering),
    we are ready to analyze some results.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这三步处理（数据清理、嵌入和聚类），我们就可以开始分析一些结果了。
- en: 'The most interesting output of topic modeling is identifying which headline
    topics are associated with each cluster. A helpful approach to doing this is to
    visualize the most important words for each cluster and come up with the connecting
    topic. Therefore, we can plot the following for the first identified cluster (cluster
    0):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模中最有趣的输出是识别哪些头条新闻主题与每个聚类相关。一个有用的方法是可视化每个聚类中最重要的词汇，并提出相关的主题。因此，我们可以为第一个识别的聚类（聚类0）绘制以下内容：
- en: '![Figure 6.13 – Top 15 words in order of importance for the first cluster](img/B16591_06_13.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 第一个聚类中按重要性排序的前15个词](img/B16591_06_13.jpg)'
- en: Figure 6.13 – Top 15 words in order of importance for the first cluster
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 第一个聚类中按重要性排序的前15个词
- en: In *Figure 6**.13*, we can see that the most important words are `win`, `world`,
    `cup`, and `final`. All these words are sports-related, and therefore the topic
    for cluster 0 is *sports*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.13*中，我们可以看到最重要的词是`win`（胜利）、`world`（世界）、`cup`（杯赛）和`final`（决赛）。这些词都是与体育相关的，因此聚类0的主题是*体育*。
- en: 'We can put together the most important words per cluster:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为每个聚类整理出最重要的词汇：
- en: '[PRE10]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From the preceding information, we can conclude the topics for each cluster:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的信息中，我们可以得出每个聚类的主题：
- en: '`Cluster` `0`: Sports'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cluster` `0`: 体育'
- en: '`Cluster 1`: Global affairs'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cluster 1`: 全球事务'
- en: '`Cluster` `2`: Economy'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cluster` `2`: 经济'
- en: '`Cluster 3`: Crime/current happenings'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cluster 3`: 犯罪/时事新闻'
- en: 'With this information, now we can take any of the headlines that were not used
    during the clustering process and predict their topic:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们现在可以选择任何未在聚类过程中使用的头条新闻，并预测它的主题：
- en: '[PRE11]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This piece of code will yield a similar statement to the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会生成类似于以下的输出：
- en: '[PRE12]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding sentence is a clear reference to sports; therefore, we would expect
    the predicted cluster to be `0`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 前面这句话明显与体育相关，因此我们预期预测的聚类应为`0`。
- en: 'In order to predict the cluster group, we need to follow following steps: Define
    the function, run it and verify the result.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测聚类组，我们需要按照以下步骤操作：定义函数、运行函数并验证结果。
- en: 'Let’s code a prediction function that will perform the necessary cleaning and
    preprocessing, embedding, and clustering steps:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写一个预测函数，执行必要的清理和预处理、嵌入以及聚类步骤：
- en: '[PRE13]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s run this function with the following code:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过以下代码运行这个函数：
- en: '[PRE14]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is the actual predicted cluster:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出是实际预测的聚类：
- en: '[PRE15]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Great work!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！
- en: How it works...
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we have explored the NLP task known as topic modeling. This
    task tries to come up with the topics associated with a given set of documents.
    Typically, no answer is given (no ground truth), and so this task is better solved
    via unsupervised learning. We attempted to solve this task with ABC’s *1 Million
    News* *Headlines* dataset.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们探索了被称为主题建模的自然语言处理任务。这个任务试图找出与一组给定文档相关的主题。通常，不提供明确的答案（没有标准答案），因此这个任务更适合通过无监督学习来解决。我们尝试使用ABC的*100万新闻*
    *头条* 数据集来解决这个问题。
- en: 'We followed a three-step approach:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了三步法：
- en: Data processing and cleaning
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理与清洗
- en: Word embeddings
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Clustering
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类
- en: 'For the first step, we followed a typical pipeline for any NLP problem:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一步，我们遵循了任何自然语言处理问题的典型流程：
- en: Data cleaning
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据清理
- en: Tokenizing
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: Removing stop words
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去除停用词
- en: Stemming
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatization
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词形还原
- en: For the second step, we applied Google’s word2vec to compute embeddings for
    each word, and each headline embedding was computed as the average of the embeddings
    of each one of its words.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二步，我们应用了Google的word2vec来计算每个单词的嵌入，每个头条新闻的嵌入是其每个单词嵌入的平均值。
- en: In the third step, we explored the unsupervised learning algorithm K-means,
    selected four clusters, and computed its centroids. We generated the following
    topic clusters sports, global affairs, economy and crime, and current happenings.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步中，我们探索了无监督学习算法K-means，选择了四个聚类，并计算了它们的中心点。我们生成了以下主题聚类：体育、全球事务、经济与犯罪、时事新闻。
- en: With this information, we selected a random headline and accurately predicted
    the topic it was related to.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们选择了一个随机的头条新闻，并准确地预测了它所关联的主题。
- en: There’s more...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Unsupervised learning is a very wide topic and an active field of research.
    To learn more, a good starting point is its Wikipedia article: [https://en.wikipedia.org/wiki/Unsupervised_learning](https://en.wikipedia.org/wiki/Unsupervised_learning).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一个非常广泛的主题，也是一个活跃的研究领域。要了解更多，好的起点是它的维基百科文章：[https://en.wikipedia.org/wiki/Unsupervised_learning](https://en.wikipedia.org/wiki/Unsupervised_learning)。
- en: Apart from the *1 Million News Headlines* dataset, another well-known reference
    dataset for topic modeling is the 20 Newsgroups dataset. I recommend working with
    the larger 6 Newsgroups choice as many Newsgroups had a lot of themes in common.
    More information can be found at [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*100万新闻标题*数据集，另一个著名的主题建模参考数据集是20 Newsgroups数据集。我建议使用更大的6 Newsgroups选项，因为许多Newsgroups有很多相似的主题。更多信息可以在[http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)找到。
- en: One simplification we followed during the processing of embeddings is that the
    computation of our headline embedding was done by averaging each of the corresponding
    word embeddings. However, there are other approaches, known as document embeddings
    or sentence embeddings, with models such as **Doc2Vec** or **Sentence-BERT**,
    which can be more useful for other applications. An analysis comparing some of
    these approaches can be found at [https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理嵌入时，我们遵循的一个简化方法是通过平均每个对应的词嵌入来计算标题嵌入。然而，还有其他方法，称为文档嵌入或句子嵌入，使用像**Doc2Vec**或**Sentence-BERT**这样的模型，这些方法对于其他应用可能更有用。关于这些方法的对比分析可以在[https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)找到。
- en: For a detailed explanation of how the K-means algorithm works, it is suggested
    you review [https://towardsdatascience.com/k-means-clustering-explained-4528df86a120](https://towardsdatascience.com/k-means-clustering-explained-4528df86a120).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 有关K-means算法如何工作的详细解释，建议你查看[https://towardsdatascience.com/k-means-clustering-explained-4528df86a120](https://towardsdatascience.com/k-means-clustering-explained-4528df86a120)。
- en: When predicting the topic of a given headline, K-means is equivalent to another
    algorithm, called 1-nearest neighbor, which is the specific case of K-nearest
    neighbor with K = 1\. More information regarding this supervised learning algorithm
    can be found at [https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测给定标题的主题时，K-means算法等同于另一个算法，称为1-最近邻，它是K最近邻的特例，其中K = 1。关于这个监督学习算法的更多信息可以在[https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)找到。
- en: Analyzing sentiment in movie reviews
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析电影评论的情感
- en: '**Sentiment analysis** is the use of several different techniques, including
    NLP, to identify the emotional state associated with human-generated information,
    text in our case. In this recipe, we are going to perform sentiment analysis on
    real-world movie reviews. We will classify the reviews into two sentiments: positive
    or negative.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感分析**是使用多种不同的技术，包括自然语言处理（NLP），来识别与人类生成的信息（在我们这里是文本）相关的情感状态。在这个配方中，我们将对现实世界的电影评论进行情感分析。我们将评论分为两种情感：正面或负面。'
- en: 'To achieve this, we will use several pre-trained models from GluonNLP Model
    Zoo, and apply its word embeddings to feed a classifier, which will output the
    predicted sentiment. We will apply this process to a new dataset: **IMDb** **Movie
    Reviews**.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用来自GluonNLP模型库的多个预训练模型，并将其词嵌入应用于分类器，分类器将输出预测的情感。我们将这一过程应用于一个新的数据集：**IMDb**
    **电影评论**。
- en: Getting ready
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 和前几章一样，在这个配方中，我们将使用一些矩阵运算和线性代数，但这并不难。
- en: 'Furthermore, we will be classifying text datasets. Therefore, we will revisit
    some concepts already seen in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将对文本数据集进行分类。因此，我们将回顾在*配方4*中已经看到的一些概念，*理解文本数据集 – 加载、管理和可视化Enron邮件数据集*，来自[*第2章*](B16591_02.xhtml#_idTextAnchor029)，*与MXNet协作并可视化数据集：Gluon*
    *和DataLoader*。
- en: How to do it...
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将执行以下步骤：
- en: Exploring the *IMDb Movie* *Reviews* dataset
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索*IMDb电影* *评论* 数据集
- en: Combining TextCNNs with word embeddings
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将TextCNN与词嵌入结合
- en: Introducing BERT
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍BERT
- en: Putting everything together
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整合一切
- en: Exploring the IMDb Movie Reviews dataset
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索IMDb电影评论数据集
- en: This dataset was collected in 2011 by researchers from Stanford University.
    It is split into a training set and a test set, with each of the sets having 25,000
    reviews. They included at most 30 reviews per movie. The sentiments are quite
    polar, with negative reviews with values between [1, 4] and positive reviews with
    values between [7, 10].
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集由斯坦福大学的研究人员在 2011 年收集。它被划分为训练集和测试集，每个集包含 25,000 条评论。每部电影最多包含 30 条评论。评论的情感极为两极化，负面评论的值在
    [1, 4] 之间，正面评论的值在 [7, 10] 之间。
- en: '![Figure 6.14 – Histogram of movie reviews (imbalanced dataset)](img/B16591_06_14.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 电影评论的直方图（不平衡数据集）](img/B16591_06_14.jpg)'
- en: Figure 6.14 – Histogram of movie reviews (imbalanced dataset)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 电影评论的直方图（不平衡数据集）
- en: For our analysis, we will simplify the sentiment values to a binary sentiment
    classification task. Therefore, negative reviews are assigned a 0 value and positive
    reviews a 1 value. As a by-product of this simplification, the dataset becomes
    balanced.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们将情感值简化为二分类情感分析任务。因此，负面评论被分配为 0 值，正面评论为 1 值。通过这种简化，数据集变得平衡。
- en: '![Figure 6.15 – Histogram of binarized movie reviews (balanced dataset)](img/B16591_06_15.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 二值化电影评论的直方图（平衡数据集）](img/B16591_06_15.jpg)'
- en: Figure 6.15 – Histogram of binarized movie reviews (balanced dataset)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 二值化电影评论的直方图（平衡数据集）
- en: 'Another point to take into account, noted by the paper’s authors, is that as
    nuances in the language can contain information regarding the sentiment, the preprocessing
    of the reviews must not include usual stop words and stemming. We took this remark
    into account in our preprocessing:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 论文作者指出的另一个要考虑的点是，由于语言中的细微差别可能包含关于情感的信息，因此评论的预处理不能包括常见的停用词和词干化。我们在预处理时考虑了这一点：
- en: '[PRE16]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The file can be accessed from [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可以通过[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py)访问。
- en: This function is applied to all samples in the dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数应用于数据集中的所有样本。
- en: Combining TextCNNs with word embeddings
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 TextCNN 与词嵌入结合使用
- en: 'After processing the dataset, we are now ready to use it with any architecture
    of our choice. In the first recipe of this chapter, we showed how we can use CNNs
    with sequences. In order to provide language information, TextCNNs can use pre-trained
    token representations as input. For this recipe, we will use two word embeddings
    that will generate inputs for our model:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完数据集后，我们现在可以使用它来搭建任何我们选择的架构。在本章的第一个食谱中，我们展示了如何使用 CNN 处理序列。为了提供语言信息，TextCNN
    可以使用预训练的词元表示作为输入。对于这个食谱，我们将使用两个词嵌入，它们将为我们的模型生成输入：
- en: '**word2vec**: These embeddings were introduced in *Recipe 4*, *Understanding
    text datasets – loading, managing, and visualizing the Enron Email dataset*, from
    [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029), *Working with MXNet and Visualizing
    Datasets: Gluon* *and DataLoader*'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**word2vec**：这些词嵌入在*食谱 4*中介绍，*理解文本数据集 – 加载、管理和可视化 Enron 邮件数据集*，来自[*第 2 章*](B16591_02.xhtml#_idTextAnchor029)，*与
    MXNet 一起工作并可视化数据集：Gluon 和 DataLoader*'
- en: '**BERT**: A language model introduced by Google in 2018'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT**：Google 于 2018 年推出的语言模型'
- en: Introducing BERT
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍 BERT
- en: RNNs and Transformers can work with large sequences of text. However, one of
    the largest disadvantages is that the data is processed in a single direction,
    from left to right. BERT provides a mechanism so that every word representation
    (token) can jointly use information from both directions, to the left and to the
    right of that specific word.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 和 Transformer 可以处理大规模的文本序列。然而，最大的缺点之一是数据仅单向处理，从左到右。BERT 提供了一种机制，使得每个单词的表示（词元）可以同时使用来自左侧和右侧的信息。
- en: Another distinctive characteristic of BERT is that its attention mechanisms
    are solely based on self-attention layers; no cross-attention layers are used.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的另一个独特特性是，它的注意力机制完全基于自注意力层；没有使用交叉注意力层。
- en: '![Figure 6.16 – BERT architecture: Comparison of BERT bidirectional approach
    with Transformers, such as GPT-1; left-to-right-only approach](img/B16591_06_16.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – BERT 架构：比较 BERT 双向方法与如 GPT-1 等 Transformer；仅从左到右的方法](img/B16591_06_16.jpg)'
- en: 'Figure 6.16 – BERT architecture: Comparison of BERT bidirectional approach
    with Transformers, such as GPT-1; left-to-right-only approach'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – BERT架构：将BERT的双向方法与GPT-1等Transformer模型的左到右方法进行比较
- en: 'BERT was trained in an unsupervised manner, using two task objectives:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是在无监督的方式下训练的，使用了两个任务目标：
- en: '**Masked language model**: The word under analysis is not shown, and therefore
    the model needs to understand its meaning from context alone'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩蔽语言模型**：待分析的单词不可见，因此模型需要仅凭上下文理解其含义'
- en: '**Next-sentence prediction**: Given two sentences, this task predicts whether
    they have a connection (one could happen after the other in a longer text) or
    they are not related'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一句预测**：给定两句话，任务是预测它们是否相关（在长文本中，一个可能会在另一个后面出现）或它们不相关'
- en: This training methodology, combined with the BERT architecture, proved to be
    very successful, beating state-of-the-art on 11 NLP tasks at the time the paper
    was published.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练方法与BERT架构的结合，证明非常成功，在论文发表时超过了当时11个NLP任务的最先进技术。
- en: 'GluonNLP provides two pre-trained BERT models with the following features:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: GluonNLP提供了两个预训练的BERT模型，具有以下特征：
- en: '`BERT_12_768_12`: 12 layers, 768-dimensional embedding vectors, 12 self-attention
    heads. This model is known as **BERT base**.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BERT_12_768_12`：12层、768维嵌入向量、12个自注意力头。这个模型被称为**BERT base**。'
- en: '`BERT_24_1024_16`: 24 layers, 1,024-dimensional embedding vectors, 16 self-attention
    heads. This model is known as **BERT large**.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BERT_24_1024_16`：24层、1,024维嵌入向量、16个自注意力头。这个模型被称为**BERT large**。'
- en: 'For our experiments, we will use the BERT base model, which can be easily loaded
    with the following code statement:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将使用BERT base模型，可以通过以下代码轻松加载：
- en: '[PRE17]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With the preceding function, we can easily obtain a BERT model (`bert_model`)
    and its vocabulary (`vocab`), based on the architecture of 12 layers, 768-dimensional
    embedding vectors, 12 self-attention heads, and a dataset from English Wikipedia
    (`book_corpus_wiki_en_uncased`).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述函数，我们可以轻松获得一个BERT模型（`bert_model`）及其词汇表（`vocab`），该模型基于12层架构、768维嵌入向量、12个自注意力头，并且使用来自英文维基百科的数据集（`book_corpus_wiki_en_uncased`）。
- en: Putting everything together
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容汇总在一起
- en: Let’s summarize all the steps we have seen so far.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止看到的所有步骤。
- en: 'Our *IMDb Movie Reviews* dataset is composed of 25,000 training samples and
    25,000 test samples. For cost and compute optimization purposes, we work with
    the following datasets:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的*IMDb电影评论*数据集由25,000个训练样本和25,000个测试样本组成。为了成本和计算优化，我们使用以下数据集：
- en: '**Training set**: 5,000 samples (from the original training set)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：5,000个样本（来自原始训练集）'
- en: '**Validation set**: 1,250 samples (from the original training set; no overlap
    with our 5,000-sample training set)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：1,250个样本（来自原始训练集；与我们的5,000个训练样本没有重叠）'
- en: '**Test set**: 25,000 samples'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：25,000个样本'
- en: 'We use two embedding models as inputs to TextCNN:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个嵌入模型作为TextCNN的输入：
- en: '**word2vec**: Vectors with 300 components'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**word2vec**：300维向量'
- en: '**BERT base**: Vectors with 768 components'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT base**：768维向量'
- en: The TextCNN architecture is very similar for both approaches. The kernel sizes
    for TextCNN are 3, 4, and 5, that is, analyzing 3, 4, and 5 words at the same
    time, and the number of channels is equivalent to the embedding components. Furthermore,
    as we have a binary output (*negative* or *positive*), the classifier is a fully
    connected layer with one sigmoid output (the sigmoid activation function is included
    in the loss function due to computational optimizations).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: TextCNN架构在两种方法中非常相似。TextCNN的卷积核大小为3、4和5，即同时分析3、4和5个单词，通道数等于嵌入组件数。此外，由于我们有二分类输出（*负面*或*正面*），分类器是一个完全连接层，具有一个sigmoid输出（sigmoid激活函数已包含在损失函数中，以进行计算优化）。
- en: 'For the training, equivalent parameters are used for both embedding models:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，两个嵌入模型使用等效的参数：
- en: '**Optimizer**: Adam'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：Adam'
- en: '**Learning** **rate**: 10-3'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：10-3'
- en: '**Loss function**: Sigmoid cross-entropy'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：Sigmoid交叉熵'
- en: '**Epochs**: 5'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Epochs**：5'
- en: '**Batch** **size**: 4'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批次大小**：4'
- en: 'With these parameters, we have the following results using a word2vec embedding
    model:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们通过word2vec嵌入模型得到了以下结果：
- en: '![Figure 6.17 – Training loss/validation loss and accuracy using word2vec](img/B16591_06_17.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 使用word2vec的训练损失/验证损失和准确度](img/B16591_06_17.jpg)'
- en: Figure 6.17 – Training loss/validation loss and accuracy using word2vec
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 使用word2vec的训练损失/验证损失和准确度
- en: In *Figure 6**.17*, we can see how the training improved with the epochs, yielding
    the best validation accuracy of 0.89 at the end of the training process.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.17*中，我们可以看到随着训练轮次的增加，训练效果有所提升，最终在训练结束时获得了最佳验证准确率 0.89。
- en: 'We can check the results qualitatively, selecting a movie review from our test
    set (unseen samples) and seeing the output of our sentiment analysis algorithm.
    The example movie review is from [https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset](https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset)
    with license CC BY 4.0):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定性地检查结果，从我们的测试集中选择一条电影评论（未见过的样本），并查看我们的情感分析算法的输出。该示例电影评论来自 [https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset](https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset)，并具有
    CC BY 4.0 许可证：
- en: '[PRE18]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can format this input as embeddings expected by our TextCNN network:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个输入格式化为我们的 TextCNN 网络所期望的嵌入：
- en: '[PRE19]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can pass it through our best model from training:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过我们从训练中获得的最佳模型来传递它：
- en: '[PRE20]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'These commands yield the following output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令生成了以下输出：
- en: '[PRE21]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As can be seen in the preceding output, our algorithm has classified the review
    correctly as *positive*.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述输出所示，我们的算法正确地将评论分类为*积极*。
- en: 'However, for a more thorough and formal analysis, we can quantitatively process
    the full test set and compute the final accuracy:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了进行更全面和正式的分析，我们可以定量处理完整的测试集并计算最终准确率：
- en: '[PRE22]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'However, that number alone does not provide the full information about *Type
    I* and *Type II* errors. Therefore, we can also display the results as a confusion
    matrix (introduced in *Recipe 4*, *Evaluating classification models*, in [*Chapter
    4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification Problems*):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅凭这个数字无法提供关于*I型*和*II型*错误的完整信息。因此，我们还可以将结果显示为混淆矩阵（在[*食谱 4*](B16591_04.xhtml#_idTextAnchor075)中介绍过，*评估分类模型*，*解决*
    *分类问题*）：
- en: '![Figure 6.18 – Confusion matrix using word2vec](img/B16591_06_18.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 使用 word2vec 的混淆矩阵](img/B16591_06_18.jpg)'
- en: Figure 6.18 – Confusion matrix using word2vec
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 使用 word2vec 的混淆矩阵
- en: 'When following the same approach, but this time using our BERT model for embeddings,
    we have the following results:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当采用相同的方法，但这次使用我们的 BERT 模型进行嵌入时，我们得到了以下结果：
- en: '![Figure 6.19 – Training loss/validation loss and accuracy using BERT](img/B16591_06_19.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – 使用 BERT 的训练损失/验证损失和准确率](img/B16591_06_19.jpg)'
- en: Figure 6.19 – Training loss/validation loss and accuracy using BERT
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 使用 BERT 的训练损失/验证损失和准确率
- en: In *Figure 6**.19*, we can see how the training improved with the epochs, yielding
    the best validation accuracy of 0.91 at the end of the training process. This
    figure is higher than with word2vec, as expected, as BERT is able to build more
    contextual relationships between the words in the reviews.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.19*中，我们可以看到随着训练轮次的增加，训练效果有所提升，最终在训练结束时获得了最佳验证准确率 0.91。这个数字比使用 word2vec
    时要高，正如预期的那样，因为 BERT 能够在评论中的单词之间建立更多的上下文关系。
- en: 'We can also pass the same review from the test set through our best model from
    training, using the same code statements as previously, obtaining the following
    output:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将测试集中相同的评论通过我们从训练中获得的最佳模型，使用之前相同的代码语句，得到以下输出：
- en: '[PRE23]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This experiment produces the same result (positive review) as the previous experiment
    with *word2vec*.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 该实验与之前使用*word2vec*的实验产生了相同的结果（积极评论）。
- en: 'For the test set accuracy, we have the following:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试集准确率，我们得到了以下结果：
- en: '[PRE24]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Compared to the word2vec result, BERT provides a 3% higher accuracy.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 与 word2vec 结果相比，BERT 提供了 3% 更高的准确率。
- en: 'The confusion matrix is as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下所示：
- en: '![Figure 6.20 – Confusion matrix using BERT](img/B16591_06_20.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – 使用 BERT 的混淆矩阵](img/B16591_06_20.jpg)'
- en: Figure 6.20 – Confusion matrix using BERT
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – 使用 BERT 的混淆矩阵
- en: As we can see from these results, BERT clearly outperforms word2vec. Another
    important advantage to mention is that, as Transformers allow for better parallelization,
    the training process is also faster.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果可以看出，BERT 显著优于 word2vec。另一个重要的优势是，由于 Transformer 允许更好的并行化，训练过程也更快。
- en: How it works...
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we tackled the sentiment analysis problem. We analyzed an architecture,
    TextCNN, to solve this task and explored how it can be applied to different word
    embedding models.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们解决了情感分析问题。我们分析了 TextCNN 架构，来解决这个任务，并探讨了如何将其应用于不同的词嵌入模型。
- en: We explored a new dataset, *IMDb Movie Reviews*, and made adequate transformations
    so that we could work with the dataset in a constrained computation environment
    and simplify it to a binary classification task.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了一个新的数据集，*IMDb 电影评论*，并进行了适当的转换，以便在受限计算环境中使用该数据集，并将其简化为二分类任务。
- en: 'We introduced BERT, a new word embedding model introduced by Google in 2018,
    and compared it to a previously explored model, word2vec, understanding its differences,
    advantages, and constraints. We understood the two most important advantages of
    BERT: using bidirectional information for each word and masking each word in training
    so that the information about each word is purely based on its context.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 BERT，这是 Google 在 2018 年推出的一个新的词嵌入模型，并将其与之前探索过的模型 word2vec 进行了比较，理解了它们的差异、优势和局限性。我们理解了
    BERT 的两个最重要的优势：对每个词使用双向信息，并在训练中对每个词进行掩蔽，以便每个词的信息完全基于其上下文。
- en: We ran experiments to compare these two word embedding approaches and saw that
    despite both approaches solving the problem rather well (the test accuracy for
    word2vec and BERT being 88% and 91%, respectively), BERT performed better.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了实验，比较了这两种词嵌入方法，结果发现尽管两者都能够很好地解决问题（word2vec 和 BERT 的测试准确率分别为 88% 和 91%），但
    BERT 表现更好。
- en: There’s more...
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Sentiment analysis is a well-researched task in the literature. To learn more,
    it is recommended to read this: [https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/](https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是文献中研究得较为深入的任务。想了解更多，建议阅读这篇文章：[https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/](https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/)。
- en: 'The paper introducing the *IMDb Movie Reviews* dataset, which also proposed
    a model for sentiment analysis, can be found here: *Learning Word Vectors for
    Sentiment* *Analysis*, [https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍 *IMDb 电影评论* 数据集的论文，同时提出了一个情感分析模型，可以在这里找到：*Learning Word Vectors for Sentiment*
    *Analysis*，[https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)。
- en: 'BERT was introduced in the paper [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf).
    However, a more intuitive explanation can be found here: [https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101).
    Reading the preceding article is strongly encouraged due to its analysis of how
    data can embed biases in our models.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在论文 [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)
    中进行了介绍。然而，以下链接提供了更直观的解释：[https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101)。强烈建议阅读上一篇文章，因为它分析了数据如何将偏见嵌入到我们的模型中。
- en: BERT is very powerful and can be complemented with even better language models,
    such as RoBERTa (improved version) or DistilBERT (smaller model with similar performance),
    and lots of models fine-tuned for specific tasks. A list of the pre-trained models
    available in MXNet GluonNLP can be found at [https://nlp.gluon.ai/model_zoo/bert/index.html](https://nlp.gluon.ai/model_zoo/bert/index.html).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 非常强大，并且可以与更好的语言模型进行互补，例如 RoBERTa（改进版）或 DistilBERT（性能相似但更小的模型），还有很多为特定任务微调的模型。可以在
    [https://nlp.gluon.ai/model_zoo/bert/index.html](https://nlp.gluon.ai/model_zoo/bert/index.html)
    找到 MXNet GluonNLP 中可用的预训练模型列表。
- en: Translating text from Vietnamese to English
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从越南语翻译到英语
- en: Translating text automatically (machine translation) has been a very interesting
    and useful use case for NLP since its inception, as breaking language barriers
    has lots of applications, including chatbots and automated subtitles in multiple
    languages.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 自动翻译文本（机器翻译）自诞生以来一直是自然语言处理（NLP）中一个非常有趣和有用的应用案例，因为打破语言障碍有很多应用，包括聊天机器人和多语言的自动字幕。
- en: Before deep learning, machine translation was typically approached as a statistical
    problem. Even after deep learning, it was not until Google, in 2016, applied deep
    learning to machine translation that the area of **Neural Machine Translation**
    (**NMT**) was born. This model set the foundation for translating tasks now available
    in LLMs, such as **OpenAI GPT** and **Google Bard**.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习之前，机器翻译通常被视为一个统计问题。即使在深度学习之后，直到 2016 年 Google 将深度学习应用于机器翻译，**神经机器翻译**（**NMT**）这一领域才诞生。这个模型为现在在大语言模型（LLM）中可用的翻译任务奠定了基础，如
    **OpenAI GPT** 和 **Google Bard**。
- en: In this recipe, we will apply these techniques to translate sentences from Vietnamese
    to English, using pre-trained models from GluonNLP Model Zoo.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将应用这些技术，将越南语句子翻译成英语，使用来自GluonNLP模型库的预训练模型。
- en: Getting ready
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在这个教程中，我们将使用一点矩阵运算和线性代数，但这绝对不难。
- en: 'Furthermore, we will be classifying text datasets. Therefore, we will revisit
    some concepts already seen in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将对文本数据集进行分类。因此，我们将重新回顾在[*第2章*](B16591_02.xhtml#_idTextAnchor029)中已经见过的一些概念，*理解文本数据集——加载、管理和可视化Enron邮件数据集*，*与MXNet一起工作并可视化数据集：Gluon和DataLoader*。
- en: How to do it...
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将执行以下步骤：
- en: Exploring the *IWSLT2015* dataset
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索*IWSLT2015*数据集
- en: Evaluating machine translators (BLEU)
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估机器翻译器（BLEU）
- en: Introducing the GNMT model and exploring Transformers for this task
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍GNMT模型并探索适用于此任务的Transformers
- en: Putting everything together
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 综合所有内容
- en: Let’s look at these steps in detail next.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们详细看一下这些步骤。
- en: Exploring the IWSLT2015 dataset
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索IWSLT2015数据集
- en: 'The **International Workshop on Spoken Language Translation** (**IWSLT**) is
    a yearly scientific workshop focused on all forms of translation (not necessarily
    machine translation). They have generated several very important datasets and
    benchmarks that have helped the field of machine translation evolve. In 2015,
    an English-Vietnamese dataset was published, composed of a training set of 130,000+
    sentence pairs and validation/test sets with 1,000+ sentence pairs. This dataset
    is publicly available with MXNet GluonNLP and can be easily retrieved, as follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**国际口语语言翻译研讨会**（**IWSLT**）是一个每年举办的科学研讨会，专注于所有形式的翻译（不一定是机器翻译）。他们生成了几个非常重要的数据集和基准，帮助机器翻译领域不断发展。2015年，发布了一个英语-越南语数据集，包含超过130,000对句子的训练集和1000多对句子的验证/测试集。这个数据集是公开的，可以通过MXNet
    GluonNLP轻松获取，如下所示：'
- en: '[PRE25]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This version of the dataset provides the following data:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 此版本的数据集提供了以下数据：
- en: '[PRE26]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preprocessing is similar to previous pipelines we have already seen, and
    includes the following steps:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理过程与我们之前看到的管道类似，包括以下步骤：
- en: Sentence clipping (to define maximum values)
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子剪辑（定义最大值）
- en: Tokenizing
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: Adding **End-of-Sentence** (**EOS**) tokens to the source sentence (Vietnamese)
    and **Beginning-of-Sentence** (**BOS**) and EOS tokens to the target sentence
    (English)
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向源语言句子（越南语）添加**句子结束**（**EOS**）标记，向目标语言句子（英语）添加**句子开始**（**BOS**）和EOS标记
- en: 'Furthermore, to optimize training, a bucketing process is applied, where sentences
    are grouped by similar length:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了优化训练，应用了一个分桶过程，其中句子根据相似的长度进行分组：
- en: '![Figure 6.21 – Fixed bucket sampler](img/B16591_06_21.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – 固定桶采样器](img/B16591_06_21.jpg)'
- en: Figure 6.21 – Fixed bucket sampler
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 固定桶采样器
- en: 'The example in *Figure 6**.21* shows this strategy with 10 buckets, yielding
    a minimal amount of padding (required so that all sentences in 1 batch can be
    processed in parallel). The size of the buckets is also exponentially increased.
    Use MXNet GluonNLP, as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.21*中的示例展示了这种策略，使用了10个桶，产生了最小的填充量（为了使1个批次中的所有句子可以并行处理）。桶的大小也呈指数增长。使用MXNet
    GluonNLP，如下所示：'
- en: '[PRE27]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding example, the size (width) of each bucket is augmented by 20%
    (1.2 increments).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，每个桶的大小（宽度）增加了20%（1.2倍的增量）。
- en: Evaluating machine translators (BLEU)
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估机器翻译器（BLEU）
- en: Evaluating how successful machine translation systems are is very difficult.
    For example, using a single number to measure the quality of a translation is
    inherently subjective. For our use case, we will work with a widely used metric
    called **BiLingual Evaluation** **Understudy** (**BLEU**).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器翻译系统的效果是非常困难的。例如，使用一个单一的数字来衡量翻译质量本质上是主观的。对于我们的应用场景，我们将使用一个广泛使用的指标，称为**双语评估**
    **辅助评估**（**BLEU**）。
- en: With BLEU, several reference translations are provided, and it tries to measure
    how close the automated translation is to its reference translations. To do so,
    it compares the different N-grams (size 1 to 4) of the automated translation to
    the N-grams of the reference translations.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BLEU 指标中，会提供多个参考翻译，并尝试衡量自动翻译与参考翻译的相似度。为此，它比较自动翻译与参考翻译中不同的 N-gram（大小从 1 到 4）。
- en: '![Figure 6.22 – BLEU metric](img/B16591_06_22.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.22 – BLEU 指标](img/B16591_06_22.jpg)'
- en: Figure 6.22 – BLEU metric
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22 – BLEU 指标
- en: As can be seen in *Figure 6**.22*, BLEU tries to minimize the subjectivity associated
    with translations.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 6.22* 所示，BLEU 尝试最小化翻译中的主观性。
- en: Another metric is **Perplexity**, which defines approximately how “surprised”
    the model is to see a translated word. When the model is not surprised, it means
    it is performing well; therefore, for Perplexity, a lower value is better. Computing
    Perplexity is much faster than BLEU, and so it is used more as a checking metric
    during per-batch computations in training, leaving BLEU for per-epoch computations.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个指标是 **困惑度**，它大致定义了模型在看到翻译后的单词时的“惊讶”程度。当模型不再感到惊讶时，说明它表现良好；因此，困惑度越低越好。计算困惑度比计算
    BLEU 指标要快，因此通常作为每批次训练中的检查性指标，而 BLEU 用于每轮计算。
- en: Introducing the GNMT model and exploring Transformers for this task
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍 GNMT 模型并探索 Transformer 在此任务中的应用
- en: As mentioned, the largest improvement in the field of machine translation was
    introduced by Google in 2016 with their **Google Neural Machine Translator** (**GNMT**)
    model ([https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Google 在 2016 年引入了机器翻译领域的最大突破，即 **Google 神经网络机器翻译**（**GNMT**）模型 ([https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html))。
- en: '![Figure 6.23 – GNMT architecture](img/B16591_06_23.jpg)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.23 – GNMT 架构](img/B16591_06_23.jpg)'
- en: Figure 6.23 – GNMT architecture
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – GNMT 架构
- en: GNMT is a pioneer of transformers and makes use of attention with an encoder-decoder
    architecture as well. The encoder and the decoder are LSTM RNNs, with eight layers
    in the encoder and another eight layers in the decoder. The attention mechanism
    implemented in the model is a cross-attention layer.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: GNMT 是 Transformer 的先驱，并且也采用了带有编码器-解码器架构的注意力机制。编码器和解码器都是 LSTM RNN，编码器有 8 层，解码器也有
    8 层。模型中实现的注意力机制是交叉注意力层。
- en: At the end of the model, a beam-search sampler is chained to generate new translations
    to maximize the trained conditional probability of the translations. As in the
    paper, in our implementation, the scoring function includes a length penalty so
    that all words in the translation are covered.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型的最后，链式的束搜索采样器用于生成新的翻译，以最大化训练过的条件概率翻译。如同论文中所述，在我们的实现中，评分函数包括一个长度惩罚项，以确保翻译中的所有单词都被覆盖。
- en: 'We’ll compare GNMT with Transformers for our use case of Vietnamese-to-English
    machine translation. For our application, these are the most important parameters
    for each model:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把 GNMT 和 Transformer 在越南语到英语的机器翻译任务中进行比较。对于我们的应用，以下是每个模型最重要的参数：
- en: '**GNMT**:'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GNMT**：'
- en: 'Number of layers for the encoder: 2'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的层数：2
- en: 'Number of layers for the decoder: 2'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的层数：2
- en: 'Number of units: 512'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元数：512
- en: '**Transformer**:'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer**：'
- en: 'Number of layers for the encoder: 4'
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的层数：4
- en: 'Number of layers for the decoder: 4'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的层数：4
- en: 'Number of units: 512'
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元数：512
- en: In the next sections, we will compare both architectures for the same translation
    task.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将对两种架构进行比较，以完成相同的翻译任务。
- en: Putting everything together
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 综合所有内容
- en: Let’s summarize all the steps we have seen so far.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止的所有步骤。
- en: Our *IWSLT2015* Vietnamese-to-English dataset is composed of 133,000+ training
    samples and 1,000+ validation/test samples. We’ll work with the complete datasets.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 *IWSLT2015* 越南语到英语的数据集包含 133,000 多个训练样本和 1,000 多个验证/测试样本。我们将使用完整的数据集。
- en: 'We’ll use two models for our machine translation use case:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为机器翻译任务使用两个模型：
- en: GNMT
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GNMT
- en: Transformer
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer
- en: 'For the training, equivalent parameters are used for both architectures:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，两种架构使用相同的参数：
- en: '**Optimizer**: Adam'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：Adam'
- en: '**Learning rate**: 10-3, with a learning rate schedule of a step decay, halving
    the learning rate every epoch after half training'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：10^-3，采用阶梯衰减的学习率调度，每经过半轮训练，学习率减半'
- en: '**Loss function**: Masked softmax cross-entropy, similar to the cross-entropy
    loss functions we have already explored with the added feature that when predictions
    are longer than their valid length, the redundant words are masked out'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**: 掩蔽的softmax交叉熵，类似于我们之前探讨过的交叉熵损失函数，唯一的不同是当预测长度超过其有效长度时，冗余的词会被掩蔽。'
- en: '**Epochs**: 12'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代次数**: 12'
- en: '**Batch** **size**: 128'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批次** **大小**: 128'
- en: 'With these parameters, we have the following evolution in the training using
    the GNMT model:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们在训练过程中得到了以下结果，采用GNMT模型：
- en: '![Figure 6.24 – GNMT training evolution (training loss and validation loss,
    Perplexity, and BLEU)](img/B16591_06_24.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – GNMT训练演变（训练损失和验证损失、困惑度和BLEU）](img/B16591_06_24.jpg)'
- en: Figure 6.24 – GNMT training evolution (training loss and validation loss, Perplexity,
    and BLEU)
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – GNMT训练演变（训练损失和验证损失、困惑度和BLEU）
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的损失、困惑度和BLEU分数（乘以100）如下：
- en: '[PRE28]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Current state-of-the-art models can yield above 30 points in their BLEU score,
    but this score is certainly very high.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的最先进模型在BLEU评分上可以达到30分以上，但这个分数已经非常高了。
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose `I like to read books`, and this can be verified
    with the following code:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在定性上，我们也可以通过一个句子示例来检查模型的表现。在我们的案例中，我们选择了`I like to read books`，并可以通过以下代码验证：
- en: '[PRE29]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'These code statements will give us the following output:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码语句将产生以下输出：
- en: '[PRE30]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As can be seen from the results, the text has been correctly translated from
    Vietnamese to English.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，文本已从越南语正确地翻译为英语。
- en: 'Now, we are going to repeat the same exercises with our Transformer model.
    With the parameters defined previously for its training, we have the following
    evolution in the training:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用我们的Transformer模型重复相同的实验。根据之前定义的训练参数，我们在训练中得到了以下演变：
- en: '![Figure 6.25 – Transformer training evolution (training loss and validation
    loss, Perplexity, and BLEU)](img/B16591_06_25.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – Transformer训练演变（训练损失和验证损失、困惑度和BLEU）](img/B16591_06_25.jpg)'
- en: Figure 6.25 – Transformer training evolution (training loss and validation loss,
    Perplexity, and BLEU)
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – Transformer训练演变（训练损失和验证损失、困惑度和BLEU）
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的损失、困惑度和BLEU分数（乘以100）如下：
- en: '[PRE31]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we can see, the Transformer architecture yields around ~0.015 higher BLEU
    score points.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，Transformer架构的BLEU得分大约高出~0.015分。
- en: 'As done for GNMT, we can also check how well our model is performing qualitatively
    with the same sentence example and code. The output is as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 如同GNMT所做的那样，我们也可以通过相同的句子示例和代码检查模型在定性上的表现。输出结果如下：
- en: '[PRE32]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As can be seen from the results, the text has been correctly translated from
    Vietnamese to English.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，文本已从越南语正确地翻译为英语。
- en: How it works...
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: In this recipe, we solved one of the most useful tasks in NLP, machine translation.
    We introduced a new architecture, GNMT, the precursor of the Transformer, and
    compared both models.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们解决了自然语言处理（NLP）中最有用的任务之一——机器翻译。我们介绍了一种新的架构——GNMT，它是Transformer的前身，并对这两种模型进行了比较。
- en: We explored a new dataset, *IWSLT2015*, which, among other language pairs, supports
    translations between Vietnamese and English. We introduced the Perplexity and
    BLEU metrics, which are widely used to evaluate translation models.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了一个新的数据集，*IWSLT2015*，该数据集支持包括越南语和英语在内的多种语言对之间的翻译。我们引入了广泛用于评估翻译模型的困惑度（Perplexity）和BLEU评分标准。
- en: We ran experiments to compare these two models and saw that, despite both approaches
    solving the problem rather well (the BLEU scores in the test set for GNMT and
    the Transformer were 23.15 and 24.34, respectively), the Transformer performed
    better.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了实验，比较了这两种模型，发现尽管两种方法都能很好地解决问题（GNMT和Transformer模型在测试集上的BLEU分数分别为23.15和24.34），但是Transformer表现得更好。
- en: There’s more...
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Machine translation is a difficult problem to tackle. Two very good official
    guides from MXNet GluonNLP where this problem is solved are the following:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译是一个难以攻克的问题。MXNet GluonNLP提供了两个非常好的官方指南，在这些指南中，我们解决了这个问题：
- en: '**Official machine translation tutorials of MXNet** **GluonNLP:** [https://nlp.gluon.ai/examples/machine_translation/index.html](https://nlp.gluon.ai/examples/machine_translation/index.html)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MXNet官方机器翻译教程** **GluonNLP**：[https://nlp.gluon.ai/examples/machine_translation/index.html](https://nlp.gluon.ai/examples/machine_translation/index.html)'
- en: '**AMLC19-GluonNLP:** [https://github.com/eric-haibin-lin/AMLC19-GluonNLP](https://github.com/eric-haibin-lin/AMLC19-GluonNLP)'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AMLC19-GluonNLP**：[https://github.com/eric-haibin-lin/AMLC19-GluonNLP](https://github.com/eric-haibin-lin/AMLC19-GluonNLP)'
- en: This recipe used code from the previous references. I would like to kindly thank
    the contributors.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱使用了前面提到的代码。我要特别感谢贡献者们。
- en: 'The IWSLT conference takes place every year. For more info, please visit their
    official site: [https://iwslt.org/](https://iwslt.org/).'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: IWSLT大会每年都会举行。欲了解更多信息，请访问其官方网站：[https://iwslt.org/](https://iwslt.org/)。
- en: 'We introduced two new metrics for translation problems, Perplexity and BLEU.
    Work is actively being carried out to improve these metrics, with new metrics
    being developed recently, such as **SacreBLEU**. Some references that tackle this
    very important topic are as follows:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了两个新的翻译问题度量标准，Perplexity和BLEU。当前正在积极进行改进这些度量标准的工作，最近还开发了新度量标准，如**SacreBLEU**。以下是一些处理这一重要话题的参考资料：
- en: '**Perplexity**: [http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/](http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/)'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Perplexity**：[http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/](http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/)'
- en: '**BLEU**: [https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1](https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1)'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BLEU**：[https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1](https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1)'
- en: '**Improving BLEU with** **SacreBLEU**: [https://aclanthology.org/W18-6319.pdf](https://aclanthology.org/W18-6319.pdf)'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用** **SacreBLEU**改善BLEU：[https://aclanthology.org/W18-6319.pdf](https://aclanthology.org/W18-6319.pdf)'
- en: 'We also discussed GNMT for the first time, which was one of the first real-world
    systems that used deep learning for translation (NMT), developed by Google in
    2016\. The blog post where this was announced is worth reading: [https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还首次讨论了GNMT，这是第一个使用深度学习进行翻译（NMT）的实际系统，2016年由Google开发。有关此公告的博客文章值得一读：[https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)。
- en: There are many translation models that have used the *IWSLT2015* dataset. The
    results can be found at [https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1](https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多翻译模型使用了*IWSLT2015*数据集。结果可以在[https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1](https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1)找到。
- en: Furthermore, in this recipe, we analyzed language-to-language translation, which
    has been the de facto approach of the industry for a long time, using English
    as a bridge language for multilingual translation. This is an active area of research,
    and recently, Meta, formerly known as Facebook, has developed the **No Language
    Left Behind** (**NLLB-200**) model. More information about this breakthrough can
    be found at [https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/](https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/).
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在本食谱中，我们分析了语言对语言的翻译，这一直是该行业的事实标准方法，使用英语作为多语言翻译的桥梁语言。这是一个活跃的研究领域，最近，Meta（前身为Facebook）开发了**No
    Language Left Behind**（**NLLB-200**）模型。关于这一突破的更多信息，可以在[https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/](https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/)找到。
