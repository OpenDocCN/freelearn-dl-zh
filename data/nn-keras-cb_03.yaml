- en: Applications of Deep Feedforward Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度前馈神经网络的应用
- en: 'In this chapter, we will be covering the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将覆盖以下方案：
- en: Predicting credit default
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测信用违约
- en: Predicting house prices
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测房价
- en: Categorizing news articles into topics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新闻文章分类到不同主题
- en: Classifying common audio
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类常见音频
- en: Predicting stock prices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股票价格
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapters, we learned about building a neural network and the
    various parameters that need to be tweaked to ensure that the model built generalizes
    well. Additionally, we learned about how neural networks can be leveraged to perform
    image analysis using MNIST data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何构建神经网络及需要调整的各种参数，以确保模型能够很好地泛化。此外，我们还学习了如何利用神经网络在MNIST数据集上进行图像分析。
- en: 'In this chapter, we will learn how neural networks can be used for prediction
    on top of the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何利用神经网络进行预测，基于以下内容：
- en: Structured dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化数据集
- en: Categorical output prediction
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类输出预测
- en: Continuous output prediction
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续输出预测
- en: Text analysis
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析
- en: Audio analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频分析
- en: 'Additionally, we will also be learning about the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将学习以下内容：
- en: Implementing a custom loss function
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自定义损失函数
- en: Assigning higher weights for certain classes of output over others
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对某些类别的输出赋予比其他类别更高的权重
- en: Assigning higher weights for certain rows of a dataset over others
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据集中某些行赋予比其他行更高的权重
- en: Leveraging a functional API to integrate multiple sources of data
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用功能性API整合多个数据源
- en: 'We will learn about all the preceding by going through the following recipes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下方案来学习上述内容：
- en: Predicting a credit default
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测信用违约
- en: Predicting house prices
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测房价
- en: Categorizing news articles
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类新闻文章
- en: Predicting stock prices
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股票价格
- en: Classifying common audio
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类常见音频
- en: However, you should note that these applications are provided only for you to
    understand how neural networks can be leveraged to analyze a variety of input
    data. Advanced ways of analyzing text, audio, and time-series data will be provided
    in later chapters about the  Convolutional Neural Network and the Recurrent Neural
    Network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你应该注意，这些应用仅用于帮助你理解如何利用神经网络分析各种输入数据。关于卷积神经网络和循环神经网络的高级文本、音频和时间序列数据分析方法将会在后续章节中提供。
- en: Predicting credit default
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测信用违约
- en: In the financial services industry, one of the major sources of losing out on
    revenues is the default of certain customers. However, a very small percentage
    of the total customers default. Hence, this becomes a problem of classification
    and, more importantly, identifying rare events.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融服务行业，客户违约是造成收入损失的主要原因之一。然而，实际发生违约的客户占总客户的比例非常小。因此，这成为一个分类问题，更重要的是，识别出稀有事件。
- en: In this case study, we will analyze a dataset that tracks certain key attributes
    of a customer at a given point in time and tries to predict whether the customer
    is likely to default.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将分析一个跟踪客户在某一时间点的关键属性的数据库，并尝试预测客户是否可能发生违约。
- en: Let's consider the way in which you might operationalize the predictions from
    the model we build. Businesses might want to have a special focus on the customers
    who are more likely to default—potentially giving them alternative payment options
    or  a way to reduce the credit limit, and so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下如何将我们构建的模型的预测结果投入到实际应用中。企业可能会特别关注那些更可能违约的客户——可能会为他们提供替代的付款选项或减少信用额度等方式。
- en: Getting ready
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The strategy that we''ll adopt to predict default of a customer is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下策略来预测客户违约：
- en: '**Objective**: Assign a high probability to the customers who are more likely
    to default.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：为更可能违约的客户分配高概率。'
- en: '**Mea****surement** **criterion**: Maximize the number of customers who have
    actually defaulted when we consider only the top 10% of members by decreasing
    the default probability.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量** **标准**：通过降低违约概率，最大化仅考虑前10%成员时实际违约的客户数量。'
- en: 'The strategy we will be adopting to assign a probability of default for each
    member will be as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下策略为每个成员分配违约概率：
- en: Consider the historic data of all members.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑所有成员的历史数据。
- en: 'Understand the variables that can help us to identify a customer who is likely
    to default:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解哪些变量可以帮助我们识别可能违约的客户：
- en: Income-to-debt ratio is a very good indicator of whether a member is likely
    to default.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收入与债务比率是一个非常好的指示器，能够判断成员是否可能违约。
- en: We will be extracting a few other variables similar to that.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将提取一些与此类似的其他变量。
- en: 'In the previous step, we created the input variables; now, let''s go ahead
    and create the dependent variable:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前一步中，我们创建了输入变量；现在，让我们继续创建因变量：
- en: We will extract the members who have actually defaulted in the next 2 years
    by first going back in history and then looking at whether members defaulted in
    the next 2 years
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将通过回顾历史数据并查看成员是否在接下来的两年内发生违约，来提取那些实际违约的成员。
- en: It is important to have a time lag, as it might not give us any levers to change
    the outcome if we do not have a time gap between when a member is likely to default
    and the date of prediction.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置时间滞后非常重要，因为如果我们在预测日期与成员可能违约之间没有时间间隔，可能无法为我们提供改变结果的杠杆。
- en: Given that the  outcome is binary, we will minimize the binary cross-entropy
    loss.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于结果是二元的，我们将最小化二元交叉熵损失。
- en: The model shall have a hidden layer that connects the input layer and the output
    layer.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型将有一个隐藏层，连接输入层和输出层。
- en: We shall calculate the number of the top 10% probability members who have actually
    defaulted, in the test dataset.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将计算在测试数据集中，实际违约的前10%概率成员的数量。
- en: Note that we assume that test data is representative here, as we are not in
    a position to assess the performance of a model on unseen dataset without productionalizing
    the model. We shall assume that the model's performance on an unseen dataset is
    a good indicator of how well the model will perform on future data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们假设测试数据具有代表性，因为在没有将模型投入生产的情况下，我们无法评估模型在未见数据上的表现。我们假设模型在未见数据上的表现是预测模型在未来数据上表现的良好指标。
- en: How to do it...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We''ll code up the strategy as follows (Please refer to the `Credit default
    prediction.ipynb` file in GitHub while implementing the code):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下策略编写代码（在实现代码时，请参考GitHub中的`Credit default prediction.ipynb`文件）：
- en: 'Import the relevant packages and the dataset:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包和数据集：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first three rows of the dataset we downloaded are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载的数据集的前三行如下：
- en: '![](img/f2c32cdc-9ecb-4a70-89f9-1460501a5bc3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2c32cdc-9ecb-4a70-89f9-1460501a5bc3.png)'
- en: The preceding screenshot is a subset of variables in the original dataset. The
    variable named `Defaultin2yrs` is the output variable that we need to predict,
    based on the rest of the variables present in the dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示的是原始数据集中一部分变量。名为`Defaultin2yrs`的变量是我们需要预测的输出变量，基于数据集中其他存在的变量。
- en: 'Summarize the dataset to understand the variables better:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结数据集以更好地理解变量：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once you look at the output you will notice the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦查看输出，您将会注意到以下几点：
- en: Certain variables have a small range (`age`), while others have a much bigger
    range (`Income`).
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些变量的范围较小（`age`），而其他变量的范围则大得多（`Income`）。
- en: Certain variables have missing values (`Income`).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些变量存在缺失值（`Income`）。
- en: Certain variables have outlier values (`Debt_income_ratio`). In the next steps,
    we will go ahead and correct all the issues flagged previously.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些变量存在异常值（`Debt_income_ratio`）。在接下来的步骤中，我们将纠正所有之前标记的问题。
- en: 'Impute missing values in a variable with the variable''s median value:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用变量的中位数值填充缺失值：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we excluded the first variable, as it is the variable
    that we are trying to predict, and then we imputed the missing values in the rest
    of the variables (provided the variable does have a missing value).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们排除了第一个变量，因为它是我们要预测的变量，然后对其余变量中的缺失值进行填充（前提是该变量确实存在缺失值）。
- en: 'Cap each variable to its corresponding 95^(th) percentile value so that we
    do not have outliers in our input variables:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个变量限制在其对应的95^(th)百分位值，以避免输入变量中出现异常值：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, we have identified the 95^(th) percentile value of each
    variable, created a new variable that has a value of one if the row contains an
    outlier in the given variable, and zero otherwise. Additionally, we have capped
    the variable values to the 95^(th) percentile value of the original value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们已识别出每个变量的95^(th)百分位值，创建了一个新变量，如果行中存在给定变量的异常值，则该新变量的值为1，否则为0。此外，我们还将变量值限制为原始值的95^(th)百分位值。
- en: 'Once we summarize the modified data, we notice that except for the `Debt_income_ratio`
    variable every other variable does not seem to have outliers anymore. Hence, let''s
    constrain `Debt_income_ratio` further to have a limited range of output, by capping
    it at the 80^(th) percentile value:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们总结了修改后的数据，我们注意到，除了`Debt_income_ratio`变量外，其他所有变量似乎都没有异常值了。因此，我们进一步限制`Debt_income_ratio`，将其输出范围限制在80^(th)百分位值：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Normalize all variables to the same scale for a value between zero and one:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有变量标准化到相同的尺度，使其值介于零和一之间：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we are limiting all the variables to a similar range
    of output, which is between zero and one, by dividing each input variable value
    with the input variable column's maximum value.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过将每个输入变量的值除以该输入变量列的最大值，将所有变量限制在一个相似的输出范围内，该范围介于零和一之间。
- en: 'Create the input and the output dataset:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Split the datasets into train and test datasets:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和测试集：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding step, we use the `train_test_split` method to split the input
    and output arrays into train and test datasets where the test dataset has 30%
    of the total number of data points in the input and the corresponding output arrays.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们使用`train_test_split`方法将输入和输出数组拆分为训练集和测试集，其中测试集占输入和对应输出数组总数据点的30%。
- en: 'Now that the datasets are created, let''s define the neural network model,
    as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据集已经创建，让我们定义神经网络模型，如下所示：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A summary of the model is as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/faf052a6-457c-4e04-9b97-131f47334ab3.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/faf052a6-457c-4e04-9b97-131f47334ab3.png)'
- en: In the previous architecture, we connect the input variables to a hidden layer
    that has 1,000 hidden units.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的架构中，我们将输入变量连接到一个包含1,000个隐藏单元的隐藏层。
- en: 'Compile the model. We shall employ binary cross entropy, as the output variable
    has only two classes. Additionally, we will specify that `optimizer` is an `adam`
    optimization:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型。我们将使用二元交叉熵，因为输出变量只有两个类别。此外，我们将指定`optimizer`为`adam`优化器：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Fit the model:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The variation of training and test loss, accuracy over increasing epochs is
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试损失、准确率随着训练轮次增加的变化如下所示：
- en: '![](img/e6314265-8680-40d4-a66a-75d2da8f9876.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6314265-8680-40d4-a66a-75d2da8f9876.png)'
- en: 'Make predictions on the test dataset:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试数据集进行预测：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Check for the number of actual defaulters that are captured in the top 10%
    of the test dataset when ranked in order of decreasing probability:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查在按概率递减排序时，测试数据集前10%中实际违约者的数量：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we concatenated the predicted values with actual values
    and then sorted the dataset by probability. We checked the actual number of defaulters
    that are captured in the top 10% of the test dataset (which is the first 4,500
    rows).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将预测值与实际值连接起来，然后按概率对数据集进行排序。我们检查了在按概率递减排序时，测试数据集前10%（即前4,500行）中捕获的实际违约者的数量。
- en: We should note that there are 1,580 actual defaulters that we have captured
    by going through the 4,500 high-probability customers. This is a good prediction,
    as on average only 6% of the total customers default. Hence, in this case, ~35%
    of customers who have a high probability of default actually defaulted.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应注意，我们通过筛选4,500个高概率客户，捕获了1,580个实际违约者。这是一个不错的预测，因为平均而言，只有6%的客户会违约。因此，在这种情况下，约35%的高违约概率客户实际上发生了违约。
- en: How it works...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we have learned about the following concepts:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了以下概念：
- en: '**Imputin****g** **m****issing** **values**: We have learned that one of the
    ways to impute the missing values of a variable is by replacing the missing values
    with the median of the corresponding variable. Other ways to deal with the missing
    values is by replacing them with the mean value, and also by replacing the missing
    value with the mean of the variable''s value in the rows that are most similar
    to the row that contains a missing value (this technique is called i**dentifying
    the K-Nearest Neighbours**).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填补** **缺失** **值**：我们了解到填补变量缺失值的一种方法是用该变量的中位数替换缺失值。处理缺失值的其他方法包括用均值替代缺失值，或者通过将缺失值替换为与包含缺失值的行最相似的行中该变量的均值（这一技术称为**识别最近邻**）。'
- en: '**Capping the outlier values**: We have also learned that one way to cap the
    outliers is by replacing values that are above the 95^(th) percentile value with
    the 95^(th) percentile value. The reason we performed this exercise is to ensure
    that the input variable does not have all the values clustered around a small
    value (when the variable is scaled by the maximum value, which is an outlier).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制异常值**：我们还学到，一种限制异常值的方法是将大于95^(th)百分位数的值替换为95^(th)百分位数的值。我们执行此操作的原因是为了确保输入变量不会出现所有值都集中在一个小值附近的情况（当变量按最大值缩放时，该最大值为异常值）。'
- en: '**Scaling dataset**: Finally, we scaled the dataset so that it can then be
    passed to a neural network.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放数据集**：最后，我们对数据集进行了缩放，以便它可以传递给神经网络。'
- en: Assigning weights for classes
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为类别分配权重
- en: When we assign equal weightage to the rows that belong to a defaulter and the
    rows that belong to a non-defaulter, potentially the model can fine-tune for the
    non-defaulters. In this section, we will look into ways of assigning a higher
    weightage so that our model classifies defaulters better.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为属于违约者的行和属于非违约者的行分配相同的权重时，模型可能会对非违约者进行微调。在本节中，我们将探讨如何分配更高的权重，以便我们的模型能更好地分类违约者。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In the previous section, we assigned the same weightage for each class; that
    is, the categorical cross entropy loss is the same if the magnitude of difference
    between actual and predicted is the same, irrespective of whether it is for the
    prediction of a default or not a default.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们为每个类别分配了相同的权重；也就是说，当实际值和预测值之间的差异大小相同时，分类交叉熵损失是相同的，无论它是用于预测违约还是非违约。
- en: 'To understand the scenario further, let''s consider the following example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解这个情境，让我们考虑以下示例：
- en: '| **Scenario** | **Probability of default** | **Actual value of default** |
    **Cross entropy loss** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **情境** | **违约概率** | **实际违约值** | **交叉熵损失** |'
- en: '| 1 | *0.2* | *1* | *1*log(0.2)* |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 1 | *0.2* | *1* | *1*log(0.2)* |'
- en: '| 2 | *0.8* | *0* | *(1-0)*log(1-0.8)* |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 2 | *0.8* | *0* | *(1-0)*log(1-0.8)* |'
- en: In the preceding scenario, the cross-entropy loss value is just the same, irrespective
    of the actual value of default.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的情境中，交叉熵损失值是相同的，无论实际违约值如何。
- en: However, we know that our objective is to capture as many actual defaulters
    as possible in the top 10% of predictions when ranked by probability.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们知道我们的目标是尽可能多地捕获实际违约者，在通过概率排序后的前10%预测中。
- en: Hence, let's go ahead and assign a higher weight of loss (a weight of *100*)
    when the actual value of default is *1* and a lower weightage (a weight of *1*)
    when the actual value of default is *0*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们继续分配更高的损失权重（权重为*100*），当实际违约值为*1*时，而当实际违约值为*0*时，分配较低的权重（权重为*1*）。
- en: 'The previous scenario now changes as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的情境现在发生了如下变化：
- en: '| **Scenario** | **Probability of default** | **Actual value of default** |
    **Cross entropy loss** |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **情境** | **违约概率** | **实际违约值** | **交叉熵损失** |'
- en: '| 1 | *0.2* | *1* | *100*1*log(0.2)* |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 1 | *0.2* | *1* | *100*1*log(0.2)* |'
- en: '| 2 | *0.8* | *0* | *1*(1-0)*log(1-0.8)* |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 2 | *0.8* | *0* | *1*(1-0)*log(1-0.8)* |'
- en: Now, if we notice the cross entropy loss, it is much higher when the predictions
    are wrong when the actual value of default is *1* compared to the predictions
    when the actual value of default is *0*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们注意到交叉熵损失，当预测错误时，实际违约值为*1*的情况相比实际违约值为*0*时的预测，交叉熵损失要高得多。
- en: Now that we have understood the intuition of assigning weightages to classes,
    let's go ahead and assign weights to output classes in the credit default dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了为类别分配权重的直觉，让我们继续在信用违约数据集中为输出类别分配权重。
- en: All the steps performed to build the dataset and model remain the same as in
    the previous section, except for the model-fitting process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建数据集和模型执行的所有步骤与上一节相同，除了模型拟合过程。
- en: How to do it...
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'The model fitting process is done by following these steps (Please refer to
    `Credit default prediction.ipynb` file in GitHub while implementing the code):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 模型拟合过程通过以下步骤完成（在实现代码时，请参考GitHub中的`Credit default prediction.ipynb`文件）：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that, in the preceding code snippet, we created a dictionary with the weights
    that correspond to the distinct classes in output that is then passed as an input
    to the `class_weight` parameter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码片段中，我们创建了一个字典，其中包含与输出中的不同类别对应的权重，然后将其作为输入传递给`class_weight`参数。
- en: The preceding step ensures that we assign a weightage of `100` to calculating
    the loss value when the actual outcome is `1` and a weightage of `1` when calculating
    the loss value when the actual outcome is `0`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前一步骤确保我们在计算实际结果为`1`时，给损失值赋予权重`100`，而在计算实际结果为`0`时，给损失值赋予权重`1`。
- en: 'The variation of accuracy and loss values over increasing epochs is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练轮次的增加，准确率和损失值的变化如下：
- en: '![](img/319b3dc9-49bd-46b1-b089-16157ec61b44.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/319b3dc9-49bd-46b1-b089-16157ec61b44.png)'
- en: Note that the accuracy values are much lower in this iteration, as we are predicting
    more number of data points to have a value 1 than in the scenario of equal weightage
    to both classes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此迭代中，准确度值显著降低，因为我们预测的数据点值为1的数量比在两个类别具有相等权重的场景中更多。
- en: 'Once the model is fitted, let''s proceed and check for the number of actual
    defaulters that are captured in the top 10% of predictions, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以检查在前10%的预测中捕获到的实际违约者数量，如下所示：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You notice that compared to the previous scenario of 1,580 customers being captured
    in the the top 10%, we have 1,640 customers captured in the top 10% in this scenario,
    and thus a better outcome for the objective we have set where we have captured
    36% of all defaulters in top 10% of high probable customers in this scenario,
    when compared to 35% in the previous scenario.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，与之前捕获了1,580名客户的前10%的情况相比，在这个场景下，我们捕获了1,640名客户进入前10%，因此在我们设定的目标上取得了更好的结果，在这个场景中，我们捕获了36%的所有违约者进入前10%的高概率客户，而在之前的场景中是35%。
- en: It is not always necessary that accuracy improves as we increase class weights.
    Assigning class weights is a mechanism to give higher weightage to the prediction
    of our interest.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是所有情况下随着类权重的增加，准确率都会提高。分配类权重是一种机制，用于给我们关注的预测赋予更高的权重。
- en: Predicting house prices
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测房价
- en: In the previous case study, we had an output that was categorical. In this case
    study, we shall look into an output that is continuous in nature, by trying to
    predict the price of a house where 13 variables that are likely to impact the
    house price are provided as input.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个案例研究中，我们的输出是分类的。在本案例研究中，我们将探讨一个连续的输出，通过尝试预测房价，其中提供了13个可能影响房价的变量作为输入。
- en: The objective is to minimize the error by which we predict the price of a house.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最小化预测房价的误差。
- en: Getting ready
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: Given that the objective is to minimize error, let's define the error that we
    shall be minimizing—we should ensure that a positive error and a negative error
    do not cancel out each other. Hence, we shall minimize the absolute error. An
    alternative of this is to minimize the squared error.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于目标是最小化误差，让我们定义我们将要最小化的误差——我们应该确保正误差和负误差不会相互抵消。因此，我们将最小化绝对误差。另一种选择是最小化平方误差。
- en: 'Now that we have fine-tuned our objective, let''s define our strategy of solving
    this problem:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经微调了目标，让我们定义解决这个问题的策略：
- en: Normalize the input dataset so that all variables range between zero to one.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入数据集进行归一化处理，使所有变量的范围都在0到1之间。
- en: Split the given data to train and test datasets.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将给定数据拆分为训练集和测试集。
- en: Initialize the hidden layer that connects the input of 13 variables to the output
    of one variable.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化隐藏层，将13个输入变量连接到一个输出变量。
- en: Compile the model with the Adam optimizer, and define the loss function to minimize
    as the mean absolute error value.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Adam优化器编译模型，并定义损失函数为最小化平均绝对误差值。
- en: Fit the model.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Make a prediction on the test dataset.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对测试数据集进行预测。
- en: Calculate the error in the prediction on the test dataset.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算在测试数据集上预测的误差。
- en: Now that we have defined our approach, let's go ahead and perform it in code
    in the next section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了方法，让我们在下一节中使用代码来执行它。
- en: How to do it...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Import the relevant dataset (Please refer to the `Predicting house price.ipynb`
    file in GitHub while implementing the code and for the recommended dataset):'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关数据集（在实施代码时，请参考GitHub中的`Predicting house price.ipynb`文件以及推荐的数据集）：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Normalize the input and output dataset so that all variables have a range from
    zero to one:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入和输出数据集进行归一化处理，使所有变量的范围都在0到1之间：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we have normalized the test dataset with the maximum value in the
    train dataset itself, as we should not be using any of the values from the test
    dataset in the model-building process. Additionally, note that we have normalized
    both the input and the output values.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经使用训练数据集中的最大值对测试数据集进行了归一化，因为我们不应在模型构建过程中使用测试数据集中的任何值。此外，请注意，我们已对输入和输出值都进行了归一化处理。
- en: 'Now that the input and output datasets are prepared, let''s proceed and define
    the model:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在输入和输出数据集已经准备好，让我们继续并定义模型：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A summary of the model is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/096001a6-4571-48cb-9e30-93b7c2c7ef5f.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/096001a6-4571-48cb-9e30-93b7c2c7ef5f.png)'
- en: Note that we performed  `L1`  regularization in the model-building process so
    that the model does not overfit on the training data (as the number of data points
    in the training data is small).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在模型构建过程中，我们执行了`L1`正则化，以防止模型在训练数据上过拟合（因为训练数据点数量较少）。
- en: 'Compile the model to minimize the mean absolute error value:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型以最小化平均绝对误差值：
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Fit the model:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Calculate the mean absolute error on the test dataset:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算测试数据集上的平均绝对误差：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We should note that the mean absolute error is *~6.7* units.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，平均绝对误差为*~6.7*单位。
- en: In the next section, we will vary the loss function and add custom weights to
    see whether we can improve upon the mean absolute error values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将改变损失函数并添加自定义权重，看看是否能够改进平均绝对误差值。
- en: Defining the custom loss function
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义自定义损失函数
- en: In the previous section, we used the predefined mean absolute error `loss` function
    to perform the optimization. In this section, we will learn about defining a custom
    loss function to perform optimization.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们使用了预定义的平均绝对误差`loss`函数进行优化。在本节中，我们将学习如何定义自定义损失函数来进行优化。
- en: The custom loss function that we shall build is a modified mean squared error
    value, where the error is the difference between the square root of the actual
    value and the square root of the predicted value.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建的自定义损失函数是一个修改过的均方误差值，其中误差是实际值的平方根与预测值的平方根之间的差异。
- en: 'The custom loss function is defined as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义损失函数定义如下：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now that we have defined the `loss` function, we will be reusing the same input
    and output datasets that we prepared in previous section, and we will also be
    using the same model that we defined earlier.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了`loss`函数，我们将重新使用前面准备的输入和输出数据集，并且我们将使用我们之前定义的相同模型。
- en: 'Now, let''s compile the model:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译模型：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding code, note that we defined the `loss` value as the custom loss
    function that we defined earlier—`loss_function`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，请注意我们将`loss`值定义为我们之前定义的自定义损失函数—`loss_function`。
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once we fit the model, we will note that the mean absolute error is *~6.5* units,
    which is slightly less than the previous iteration where we used the `mean_absolute_error` loss
    function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合了模型，我们会注意到平均绝对误差约为*~6.5*单位，略低于我们在前一次迭代中使用`mean_absolute_error`损失函数时的误差。
- en: Categorizing news articles into topics
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将新闻文章分类为主题
- en: In the previous case studies, we analyzed datasets that were structured, that
    is, contained variables and their corresponding values. In this case study, we
    will be working on a dataset that has text as input, and the expected output is
    one of the 46 possible topics that the text is related to.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的案例研究中，我们分析了结构化的数据集，也就是包含变量及其对应值的数据集。在这个案例研究中，我们将处理一个以文本作为输入的数据集，预期的输出是与该文本相关的46个可能主题之一。
- en: Getting ready
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: To understand the intuition of performing text analysis, let's consider the
    Reuters dataset, where each news article is classified into one of the 46 possible
    topics.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解执行文本分析的直觉，我们可以考虑Reuters数据集，其中每篇新闻文章被分类为46个可能主题之一。
- en: 'We will adopt the following strategy to perform our analysis:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下策略来执行我们的分析：
- en: Given that a dataset could contain thousands of unique words, we will shortlist
    the words that we shall consider.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集可能包含数千个独特的单词，我们将筛选出我们要考虑的单词。
- en: For this specific exercise, we shall consider the top 10,000 most frequent words.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这个特定的练习，我们将考虑最常见的前10,000个单词。
- en: An alternative approach would be to consider the words that cumulatively constitute
    80% of all words within a dataset. This ensures that all the rare words are excluded.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是考虑那些累积起来占数据集所有单词80%的单词。这确保了所有稀有单词被排除在外。
- en: Once the words are shortlisted, we shall one-hot-encode the article based on
    the constituent frequent words.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦选定词汇，我们将根据组成的常见词汇对文章进行独热编码。
- en: Similarly, we shall one-hot-encode the output label.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，我们将对输出标签进行独热编码。
- en: 'Each input now is a 10,000-dimensional vector, and the output is a 46-dimensional
    vector:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输入现在是一个10,000维的向量，输出是一个46维的向量：
- en: We will divide the dataset into train and test datasets. However, in code, you
    will notice that we will be using the in-built dataset of `reuters` in Keras that
    has built-in function to identify the top `n` frequent words and split the dataset
    into train and test datasets.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将数据集分为训练集和测试集。然而，在代码中，你会注意到我们将使用Keras中内置的`reuters`数据集，该数据集具有内置函数，可以识别最常见的`n`个词汇，并将数据集拆分为训练集和测试集。
- en: Map the input and output with a hidden layer in between.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在中间插入一个隐藏层，将输入和输出进行映射。
- en: We will perform softmax at the output layer to obtain the probability of the
    input belonging to one of the 46 classes.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在输出层执行softmax操作，以获得输入属于46个类别之一的概率。
- en: Given that we have multiple possible outputs, we shall employ a categorical
    cross entropy loss function.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们有多个可能的输出，因此我们将使用分类交叉熵损失函数。
- en: We shall compile and fit the model to measure its accuracy on a test dataset.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将编译并训练模型，以衡量其在测试数据集上的准确性。
- en: How to do it...
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We''ll code up the strategy defined previously as follows (please refer to
    the `Categorizing news articles into topics.ipynb` file in GitHub while implementing
    the code):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按如下方式实现之前定义的策略（实现代码时请参考GitHub上的`Categorizing news articles into topics.ipynb`文件）：
- en: 'Import the dataset :'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code snippet, we loaded data from the `reuters` dataset that
    is available  in Keras. Additionally, we consider only the `10000` most frequent
    words in the dataset.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，我们加载了Keras中可用的`reuters`数据集的数据。此外，我们只考虑数据集中最常见的`10000`个词汇。
- en: 'Inspect the dataset:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A sample of the loaded training dataset is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的训练数据集示例如下：
- en: '![](img/40f57912-d7fe-41a3-8840-03871083ff15.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40f57912-d7fe-41a3-8840-03871083ff15.png)'
- en: Note that the numbers in the preceding output represent the index of words that
    are present in the output.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述输出中的数字表示输出中出现的单词的索引。
- en: 'We can extract the index of values as follows:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以按如下方式提取值的索引：
- en: '[PRE26]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Vectorize the input. We will convert the text into a vector in the following
    way:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量化输入。我们将以以下方式将文本转换为向量：
- en: One-hot-encode the input words—resulting in a total of `10000` columns in the
    input dataset.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入词汇进行独热编码——最终生成输入数据集中总共`10000`列。
- en: If a word is present in the given text, the column corresponding to the word
    index shall have a value of one and every other column shall have a value of zero.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果文本中存在某个词汇，则对应词汇索引的列将显示为1，其他列将显示为0。
- en: 'Repeat the preceding step for all the unique words in a text. If a text has
    two unique words, there will be a total of two columns that have a value of one,
    and every other column will have a value of zero:'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本中的所有唯一词汇执行上述步骤。如果一篇文本有两个唯一词汇，那么将有两列值为1，其他所有列的值为0：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding function, we initialized a variable that is a zero matrix and
    imputed it with a value of one, based on the index values present in the input
    sequence.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述函数中，我们初始化了一个零矩阵，并根据输入序列中的索引值将其填充为1。
- en: In the following code, we are converting the words into IDs.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将单词转换为ID。
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'One-hot-encode the output:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输出进行独热编码：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The preceding code converts each output label into a vector that is `46` in
    length, where one of the `46` values is one and the rest are zero, depending on
    the label's index value.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将每个输出标签转换为一个长度为`46`的向量，其中`46`个值中的一个为1，其他值为0，具体取决于标签的索引值。
- en: 'Define the model and compile it:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型并编译：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/849f6764-c6d9-4993-8ffd-7319ebba6bf7.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/849f6764-c6d9-4993-8ffd-7319ebba6bf7.png)'
- en: Note that while compiling, we defined `loss` as `categorical_crossentropy` as
    the output in this case is categorical (multiple classes in output).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在编译时，我们将`loss`定义为`categorical_crossentropy`，因为此处的输出是分类的（输出有多个类别）。
- en: 'Fit the model:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code results in a model that has 80% accuracy in classifying
    the input text into the right topic, as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了一个模型，该模型在将输入文本分类到正确的主题时准确率为80%，如下所示：
- en: '![](img/2ef23cd1-8063-4ac8-8614-e647abd39c7f.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ef23cd1-8063-4ac8-8614-e647abd39c7f.png)'
- en: Classifying common audio
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类常见音频
- en: In the previous sections, we have understood the strategy to perform modeling
    on a structured dataset and also on unstructured text data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经了解了如何在结构化数据集和非结构化文本数据上执行建模策略。
- en: In this section, we will be learning about performing a classification exercise
    where the input is raw audio.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何进行一个分类任务，输入是原始音频。
- en: The strategy we will be adopting is that we will be extracting features from
    the input audio, where each audio signal is represented as a vector of a fixed
    number of features.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略是从输入音频中提取特征，每个音频信号都表示为一个固定数量特征的向量。
- en: There are multiple ways of extracting features from an audio—however, for this
    exercise, we will be extracting the **Mel Frequency Cepstral Coefficients** (**MFCC**)
    corresponding to the audio file.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 提取音频特征的方式有多种，然而在这个练习中，我们将提取与音频文件对应的**梅尔频率倒谱系数**（**MFCC**）。
- en: Once we extract the features, we shall perform the classification exercise in
    a way that is very similar to how we built a model for MNIST dataset classification—where
    we had hidden layers connecting the input and output layers.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提取了特征，我们将执行分类任务，这与我们为MNIST数据集分类构建模型时非常相似——在那时我们有隐藏层连接输入层和输出层。
- en: In the following section, we will be performing classification on top of an
    audio dataset where there are ten possible classes of output.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将在音频数据集上执行分类任务，输出有十个可能的类别。
- en: How to do it...
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The strategy that we defined previously is coded as follows (Please refer to
    the `Audio classification.ipynb` file in GitHub while implementing the code):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前定义的策略的代码如下（在实现代码时，请参考GitHub上的`Audio classification.ipynb`文件）：
- en: 'Import the dataset:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集：
- en: '[PRE32]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Extract features for each audio input:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每个音频输入的特征：
- en: '[PRE33]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we defined a function that takes `file_name` as input,
    extracts the `40` MFCC corresponding to the audio file, and returns the same.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个函数，该函数以`file_name`作为输入，提取与音频文件对应的`40`个MFCC，并返回这些特征。
- en: 'Create the input and the output dataset:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集：
- en: '[PRE34]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the preceding code, we loop through one audio file at a time, extracting
    its features and storing it in the input list. Similarly, we will be storing the
    output class in the output list. Additionally, we will convert the output list
    into a categorical value that is one-hot-encoded:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们逐个处理音频文件，提取其特征并将其存储在输入列表中。类似地，我们将输出类别存储在输出列表中。此外，我们还将把输出列表转换为一个独热编码的类别值：
- en: '[PRE35]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `pd.get_dummies` method works very similar to the `to_categorical` method
    we used earlier; however, `to_categorical` does not work on text classes (it works
    on numeric values only, which get converted to one-hot-encoded values).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.get_dummies`方法的作用与我们之前使用的`to_categorical`方法非常相似；然而，`to_categorical`不适用于文本类别（它只适用于数值类型，将其转换为独热编码值）。'
- en: 'Build the model and compile it:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE36]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The summary of the preceding model is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 前述模型的总结如下：
- en: '![](img/23de187e-b380-4973-9db6-ef8af7130d91.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23de187e-b380-4973-9db6-ef8af7130d91.png)'
- en: 'Create the train and test datasets and then fit the model:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集，然后拟合模型：
- en: '[PRE37]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Once the model is fitted, you will notice that the model has 91% accuracy in
    classifying audio in the right class.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型拟合完成，您会注意到模型在将音频正确分类到相应类别时的准确率为91%。
- en: Stock price prediction
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 股票价格预测
- en: In the previous sections, we learned about performing audio, text, and structured
    data analysis using neural networks. In this section, we will learn about performing
    a time-series analysis using a case study of predicting a stock price.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何使用神经网络进行音频、文本和结构化数据分析。在这一节中，我们将学习如何通过预测股票价格的案例研究来进行时间序列分析。
- en: Getting ready
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: 'To predict a stock price, we will perform the following steps:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测股票价格，我们将执行以下步骤：
- en: Order the dataset from the oldest to the newest date.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照从最旧到最新的顺序排列数据集。
- en: Take the first five stock prices as input and the sixth stock price as output.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前五个股票价格作为输入，第六个股票价格作为输出。
- en: Slide it across so that in the next data point the second to the sixth data
    points are input and the seventh data point is the output, and so on, till we
    reach the final data point.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将窗口向前滑动，这样在下一个数据点中，第二个到第六个数据点作为输入，第七个数据点作为输出，依此类推，直到达到最后一个数据点。
- en: Given that it is a continuous number that we are predicting, the `loss` function
    this time shall be the mean squared error value.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们预测的是一个连续数值，本次的`loss`函数将是均方误差值。
- en: Additionally, we will also try out the scenario where we integrate the text
    data into the historic numeric data to predict the next day's stock price.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将尝试将文本数据与历史数值数据整合，以预测第二天的股票价格。
- en: How to do it...
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The above strategy is coded as follows (please refer to `Chapter 3  - stock
    price prediction.ipynb` file in GitHub while implementing the code and for the
    recommended dataset):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略的代码如下（在实现代码并推荐数据集时，请参阅GitHub中的`Chapter 3 - stock price prediction.ipynb`文件）：
- en: 'Import the relevant packages and the dataset:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集：
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Prepare the dataset where the input is the previous five days'' stock price
    value and the output is the stock price value on the sixth day:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据集，其中输入是过去五天的股票价格，输出是第六天的股票价格：
- en: '[PRE39]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Prepare the train and test datasets, build the model, compile it, and fit it:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练和测试数据集，构建模型，编译并拟合它：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Build the model and compile it:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型并编译它：
- en: '[PRE41]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The previous code results in a summary of model as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码结果总结如下：
- en: '![](img/19ad486e-b866-4549-8fc7-a842143634a8.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19ad486e-b866-4549-8fc7-a842143634a8.png)'
- en: '[PRE42]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Once we fit the model, we should note that the mean squared error value *~$360*
    in predicting the stock price or ~$18 in predicting the stock price.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合了模型，我们应该注意到，预测股票价格的均方误差值为*~$360*，或者预测股票价格时的误差为 ~$18。
- en: Note that there is a pitfall in predicting a stock price this way. However,
    that will be dealt with in the chapter on RNN applications.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以这种方式预测股票价格存在一个陷阱。然而，这将在RNN应用章节中处理。
- en: For now, we will focus on learning how neural networks can be useful in a variety
    of different scenarios.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将专注于学习神经网络在不同场景中的多种用途。
- en: In the next section, we will understand the ways in which we can integrate the
    numeric data with the text data of news headlines in a single model.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将了解如何将数值数据与新闻标题的文本数据整合到一个模型中。
- en: Leveraging a functional API
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用功能性API
- en: In this section, we will continue to improve the accuracy of the stock price
    prediction by integrating historical price points data with the most-recent headlines
    of the company for which we are predicting the stock price.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过将历史价格数据与我们预测的公司最新头条数据结合，继续提高股票价格预测的准确性。
- en: 'The strategy that we will adopt to integrate data from multiple sources—structured
    (historical price) data and unstructured (headline) data is as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下策略来整合来自多个来源的数据——结构化（历史价格）数据和非结构化（头条）数据：
- en: We will convert the unstructured text into a structured format in a manner that
    is similar to the way we categorized news articles into topics.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将以类似于将新闻文章分类为主题的方式，将非结构化文本转换为结构化格式。
- en: We will pass the structured format of text through a neural network and extract
    the hidden layer output.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将通过神经网络传递结构化文本格式，并提取隐藏层输出。
- en: Finally, we pass the hidden layer output to the output layer, where the output
    layer has one node.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将隐藏层的输出传递到输出层，输出层有一个节点。
- en: In a similar manner, we pass the input historical price data through the neural
    network to extract the hidden layer values, which then get passed to the output
    layer that has one unit in output.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以类似的方式，我们将输入的历史价格数据传递给神经网络，提取隐藏层值，然后将其传递到输出层，输出层有一个输出单元。
- en: We multiply the output of each of the individual neural network operations to
    extract the final output.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将每个单独神经网络操作的输出相乘，以提取最终输出。
- en: The squared error value of the final output shall now be minimized.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终输出的平方误差值现在将被最小化。
- en: How to do it...
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The previous strategy is coded as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 前述策略的代码如下：
- en: 'Let''s fetch the headline data from the API provided by the Guardian, as follows:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从《卫报》提供的API获取头条数据，如下所示：
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once `titles` and `dates` are extracted, we shall preprocess the data to convert
    the `date` values to a `date` format, as follows:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦`titles`和`dates`被提取出来，我们将预处理数据，将`date`值转换为`date`格式，如下所示：
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now that we have the most recent headline for every date on which we are trying
    to predict the stock price, we will integrate the two data sources, as follows:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然我们已经得到了每个日期的最新头条数据，我们将整合这两个数据源，如下所示：
- en: '[PRE45]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Once the datasets are merged, we will go ahead and normalize the text data
    so that we remove the following:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集合并，我们将继续对文本数据进行归一化处理，以去除以下内容：
- en: Convert all words in a text into lowercase so that the words like `Text` and
    `text` are treated the same.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本中的所有单词转换为小写，以便像`Text`和`text`这样的单词被视为相同。
- en: Remove punctuation so that words such as `text.` and `text` are treated the
    same.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除标点符号，以便像`text.`和`text`这样的单词被视为相同。
- en: 'Remove stop words such as `a`, `and`, `the`, which do not add much context
    to the text:'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除如`a`、`and`、`the`等停用词，因为这些词对文本的上下文贡献不大：
- en: '[PRE46]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Replace all the null values in the `title` column with a hyphen `-`:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用连字符`-`替换`title`列中的所有空值：
- en: '[PRE47]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now that we have preprocessed the text data, let''s assign an ID to each word.
    Once we have finished this assignment, we can perform text analysis in a way that
    is very similar to what we did in the *Categorizing news articles into topics*
    section, as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预处理了文本数据，接下来为每个单词分配一个ID。一旦我们完成了这个分配，就可以进行类似于在*新闻文章分类*部分中所做的文本分析，具体如下：
- en: '[PRE48]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Given that we have encoded all the words, let''s replace them with their corresponding
    text in the original text:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们已经对所有单词进行了编码，接下来让我们用它们在原文中的对应文本替换：
- en: '[PRE49]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now that we have encoded the texts, we understand the way in which we will integrate
    the two data sources.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对文本进行了编码，理解了将如何整合这两个数据源。
- en: 'First, we shall prepare the training and test datasets, as follows:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将准备训练集和测试集，如下所示：
- en: '[PRE50]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Typically, we would use a functional API when there are multiple inputs or multiple
    outputs expected. In this case, given that there are multiple inputs, we will
    be leveraging a functional API.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当期望有多个输入或多个输出时，我们会使用功能性API。在这种情况下，由于有多个输入，我们将利用功能性API。
- en: 'Essentially, a functional API takes out the sequential process of building
    the model and is performed as follows. Take the input of the vectorized documents
    and extract the output from it:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本质上，功能性API去除了构建模型的顺序过程，具体操作如下。以向量化的文档作为输入，并从中提取输出：
- en: '[PRE51]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the preceding code, note that we have not used the sequential modeling process
    but defined the various connections using the `Dense` layer.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，请注意我们没有使用顺序建模过程，而是通过`Dense`层定义了各种连接。
- en: Note that the input has a shape of `2406`, as there are `2406` unique words
    that remain after the filtering process.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入的形状是`2406`，因为过滤过程后剩下了`2406`个唯一单词。
- en: 'Take the input of the previous `5` stock prices and build the model:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以之前的`5`个股票价格作为输入，构建模型：
- en: '[PRE52]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will multiply the output of the two inputs:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对两个输入的输出进行相乘：
- en: '[PRE53]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now that we have defined the output, we will build the model as follows:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了输出，接下来将按如下方式构建模型：
- en: '[PRE54]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Note that, in the preceding step, we used the `Model` layer to define the input
    (passed as a list) and the output:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的步骤中，我们使用了`Model`层来定义输入（作为列表传递）和输出：
- en: '![](img/67dff3a9-1e83-4591-9096-e379635afd33.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67dff3a9-1e83-4591-9096-e379635afd33.png)'
- en: 'A visualization of the preceding output is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出的可视化结果如下：
- en: '![](img/e66fdee9-4b95-4101-b450-29712e649413.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e66fdee9-4b95-4101-b450-29712e649413.png)'
- en: 'Compile and fit the model:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并拟合模型：
- en: '[PRE55]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The preceding code results in a mean squared error of *~5000* and clearly shows
    that the model overfits, as the training dataset loss is much lower than the test
    dataset loss.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果是平均平方误差为*~5000*，并清楚地表明模型存在过拟合现象，因为训练集的损失远低于测试集的损失。
- en: Potentially, the overfitting is a result of a very high number of dimensions
    in the vectorized text data. We will look at how we can improve upon this in [Chapter
    11](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml), *Building a Recurrent Neural
    Network*.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 可能，过拟合是由于向量化文本数据中维度过高所导致的。我们将在[第11章](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)中探讨如何改进这一点，*构建递归神经网络*。
- en: Defining weights for rows
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为行定义权重
- en: In the *Predicting house prices* recipe, we learned about defining a custom
    loss function. However, we are not in a position yet to assign a higher weightage
    for certain rows over others. (We did a similar exercise for  a credit default
    prediction case study where we assigned higher weightage to one class over the
    other; however, that was a classification problem, and the current problem that
    we are solving is a continuous variable-prediction problem.)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在*预测房价*食谱中，我们了解了如何定义自定义损失函数。然而，我们目前还无法为某些行分配更高的权重。 (我们曾在一个信用违约预测案例研究中做过类似的练习，当时我们为一个类别分配了较高的权重；然而那是一个分类问题，而我们当前解决的问题是一个连续变量预测问题。)
- en: In this section, we will define weights for each row and then pass them to the
    `custom_loss` function that we will define.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为每一行定义权重，并将其传递给我们将要定义的`custom_loss`函数。
- en: We will continue working on the same dataset that we analyzed in the *Stock
    price prediction* recipe.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用在*股票价格预测*食谱中分析过的相同数据集。
- en: How to do it...
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'To perform specifying weightages at a row level, we will modify our train and
    test datasets in such a way that the first `2100` data points after ordering the
    dataset are in the train dataset and the rest are in the test dataset:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在行级别上指定权重，我们将修改训练和测试数据集，使得按顺序排列后的前`2100`个数据点属于训练数据集，其余的数据点属于测试数据集：
- en: '[PRE56]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'A row in input shall have a higher weight if it occurred more recently and
    less weightage otherwise:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入中的一行如果较为近期发生，则会有较高的权重，反之则权重较低：
- en: '[PRE57]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The preceding code block assigns lower weightage to initial data points and
    a higher weightage to data points that occurred more recently.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块为初始数据点分配了较低的权重，为最近发生的数据点分配了较高的权重。
- en: Now that we have defined the weights for each row, we will include them in the
    custom loss function. Note that in this case our custom loss function shall include
    both the predicted and actual values of output as well as the weight that needs
    to be assigned to each row.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每一行定义了权重，我们将在自定义损失函数中包含它们。请注意，在这种情况下，我们的自定义损失函数将包括输出的预测值和实际值，以及需要为每一行分配的权重。
- en: 'The partial method enables us to pass more variables than just the actual and
    predicted values to the custom loss function:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部分方法使我们能够传递比实际值和预测值更多的变量给自定义损失函数：
- en: '[PRE58]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To pass `weights` to the `custom_loss` function, we shall be using the partial
    function to pass both `custom_loss` and `weights` as a parameter in step 7\. In
    the code that follows, we are defining the  `custom_loss`  function:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将`weights`传递给`custom_loss`函数，我们将使用部分函数，将`custom_loss`和`weights`作为参数传递给第7步。在接下来的代码中，我们定义了`custom_loss`函数：
- en: '[PRE59]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Given that the model we are building has two inputs, input variables and weights
    corresponding to each row, we will first define the `shape` input of the two as
    follows:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到我们构建的模型有两个输入——输入变量和每一行对应的权重，我们将首先定义这两个输入的`shape`如下：
- en: '[PRE60]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now that we have defined the inputs, let''s initialize `model` that accepts
    the two inputs as follows:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了输入，让我们按以下方式初始化接受两个输入的`model`：
- en: '[PRE61]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now that we have initialized `model`, we will define the optimization function
    as follows:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经初始化了`model`，我们将按以下方式定义优化函数：
- en: '[PRE62]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In the preceding scenario, we specify that we need to minimize the `custom_loss_4`
    function and also that we provide an additional variable (`weights_tensor`) to
    the custom loss function.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述场景中，我们指定需要最小化`custom_loss_4`函数，并且向自定义损失函数提供了一个额外的变量（`weights_tensor`）。
- en: 'Finally, before fitting the model, we will also provide `weights` for each
    row corresponding to the test dataset. Given that we are predicting these values,
    it is of no use to provide a low weightage to certain rows over others, as the
    test dataset is not provided to model. However, we will only specify this to make
    a prediction using the model we defined (which accepts two inputs):'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在拟合模型之前，我们还将为每一行提供对应测试数据集的`weights`。考虑到我们正在预测这些值，给某些行提供较低的权重是没有意义的，因为测试数据集并没有提供给模型。然而，我们只会在使用我们定义的模型进行预测时指定这一点（该模型接受两个输入）：
- en: '[PRE63]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once we specify the `weights`  of test data, we will go ahead and fit the model
    as follows:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们指定了测试数据的`weights`，我们将继续按照以下方式拟合模型：
- en: '[PRE64]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The preceding results in a test dataset loss that is very different to what
    we saw in the previous section. We will look at the reason for this in more detail
    in the [Chapter 11](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml), *Building a Recurrent
    Neural Network* chapter.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果导致测试数据集的损失与我们在前一节中看到的结果大不相同。我们将在[第11章](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)，《构建循环神经网络》一章中更详细地探讨这一原因。
- en: You need to be extremely careful while implementing the preceding model, as
    it has a few pitfalls. However, in general, it is advised to implement models
    to predict stock price movements only after sufficient due diligence.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现上述模型时，必须格外小心，因为它存在一些陷阱。然而，通常建议在进行充分的尽职调查后再实现预测股价波动的模型。
