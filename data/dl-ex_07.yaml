- en: Introduction to Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络简介
- en: In data science, a **convolutional neural network** (**CNN**) is specific kind
    of deep learning architecture that uses the convolution operation to extract relevant
    explanatory features for the input image. CNN layers are connected as a feed-forward
    neural network while using this convolution operation to mimic how the human brain
    functions while trying to recognize objects. Individual cortical neurons respond
    to stimuli in a restricted region of space known as the receptive field. In particular,
    biomedical imaging problems could be challenging sometimes, but in this chapter,
    we'll see how to use CNN in order to discover patterns in this image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，**卷积神经网络**（**CNN**）是一种特定的深度学习架构，它使用卷积操作来提取输入图像的相关解释特征。CNN 层以前馈神经网络的方式连接，同时使用此卷积操作来模拟人类大脑在试图识别物体时的工作方式。个别皮层神经元对在一个限制区域内的刺激做出反应，这个区域被称为感受野。特别地，生物医学成像问题有时可能会很有挑战性，但在本章中，我们将看到如何使用
    CNN 来发现图像中的模式。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The convolution operation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积操作
- en: Motivation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动机
- en: Different layers of CNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN的不同层
- en: 'CNN basic example: MNIST digit classification'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 基本示例：MNIST 数字分类
- en: The convolution operation
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积操作
- en: CNNs are widely used in the area of computer vision and they outperform most
    of the traditional computer vision techniques that we have been using. CNNs combine
    the famous convolution operation and neural networks, hence the name convolutional
    neural network. So, before diving into the neural network aspect of CNNs, we are
    going to introduce the convolution operation and see how it works.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 在计算机视觉领域得到了广泛应用，并且它们在很多方面超越了我们一直在使用的传统计算机视觉技术。CNN 结合了著名的卷积操作和神经网络，因此得名卷积神经网络。因此，在深入探讨
    CNN 的神经网络部分之前，我们将介绍卷积操作并了解它的工作原理。
- en: 'The main purpose of the convolution operation is to extract information or
    features from an image. Any image could be considered as a matrix of values and
    a specific group of values in this matrix will form a feature. The purpose of
    the convolution operation is to scan this matrix and try to extract relevant or
    explanatory features for that image. For example, consider a 5 by 5 image whose
    corresponding intensity or pixel values are shown as zeros and ones:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作的主要目的是从图像中提取信息或特征。任何图像都可以看作是一个值矩阵，其中矩阵中的特定值组将形成一个特征。卷积操作的目的是扫描这个矩阵，尝试提取与该图像相关或具有解释性的特征。例如，考虑一个
    5x5 的图像，其对应的强度或像素值显示为零和一：
- en: '![](img/1c7834a6-2ae2-474e-ad2b-6d51782c9525.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c7834a6-2ae2-474e-ad2b-6d51782c9525.png)'
- en: 'Figure 9.1: Matrix of pixel values'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：像素值矩阵
- en: 'And consider the following 3 x 3 matrix:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 并考虑以下 3 x 3 的矩阵：
- en: '![](img/d9763afb-ab0d-4025-b397-defa753e1707.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9763afb-ab0d-4025-b397-defa753e1707.png)'
- en: 'Figure 9.2: Matrix of pixel values'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：像素值矩阵
- en: 'We can convolve the 5 x 5 image using a 3 x 3 one as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 3 x 3 的卷积核对 5 x 5 的图像进行卷积，方法如下：
- en: '![](img/03dbfb76-d1b6-4116-96b5-1f20a7da595f.png)![](img/bc1d23c0-2eaa-4a9b-bed7-3760beeafefd.png)![](img/02ab2c65-cad7-4eb1-a4cb-58461dd8083c.png)![](img/6d592c61-4681-4347-9da7-ab30c8857ad7.png)![](img/27277f69-a023-44dd-bc20-9d4258e910a8.png)![](img/bfe47520-e728-47a6-8970-26a08c2ad7e2.png)![](img/2bacdd70-eb6a-4a11-b8ff-c5d1b70511d3.png)![](img/d4cb9f3d-e099-49b0-852f-1d6d8b1205c2.png)![](img/3a6b2050-5870-4bd4-9f29-ebbb1a88e937.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03dbfb76-d1b6-4116-96b5-1f20a7da595f.png)![](img/bc1d23c0-2eaa-4a9b-bed7-3760beeafefd.png)![](img/02ab2c65-cad7-4eb1-a4cb-58461dd8083c.png)![](img/6d592c61-4681-4347-9da7-ab30c8857ad7.png)![](img/27277f69-a023-44dd-bc20-9d4258e910a8.png)![](img/bfe47520-e728-47a6-8970-26a08c2ad7e2.png)![](img/2bacdd70-eb6a-4a11-b8ff-c5d1b70511d3.png)![](img/d4cb9f3d-e099-49b0-852f-1d6d8b1205c2.png)![](img/3a6b2050-5870-4bd4-9f29-ebbb1a88e937.png)'
- en: 'Figure 9.3: The convolution operation. The output matrix is called a convolved
    feature or feature map'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：卷积操作。输出矩阵称为卷积特征或特征图
- en: 'The preceding figure could be summarized as follows. In order to convolve the
    original 5 by 5 image using the 3 x 3 convolution kernel, we need to do the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图可以总结如下。为了使用 3 x 3 的卷积核对原始 5 x 5 图像进行卷积，我们需要执行以下操作：
- en: Scan the original green image using the orange matrix and each time move by
    only 1 pixel (stride)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用橙色矩阵扫描原始绿色图像，每次只移动 1 像素（步幅）
- en: For every position of the orange image, we do element-wise multiplication between
    the orange matrix and the corresponding pixel values in the green matrix
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个橙色图像的位置，我们在橙色矩阵和绿色矩阵中对应的像素值之间执行逐元素相乘操作
- en: Add the results of these element-wise multiplication operations together to
    get a single integer which will form a single value in the output pink matrix
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些逐元素相乘的结果加起来得到一个单一整数，这个整数将构成输出粉色矩阵中的单一值。
- en: As you can see from the preceding figure, the orange 3 by 3 matrix only operates
    on one part of the original green image at a time in each move (stride), or it
    only sees a part at a time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，橙色的 3 x 3 矩阵每次只对原始绿色图像的一个部分进行操作（步幅），或者它每次只看到图像的一部分。
- en: 'So, let''s put the previous explanation in the context of CNN terminology:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们将前面的解释放到 CNN 术语的背景下：
- en: The orange 3 x 3 matrix is called a **kernel**, **feature detector**, or **filter**
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 橙色的 3 x 3 矩阵被称为**核**、**特征检测器**或**滤波器**。
- en: The output pink matrix that contain the results of the element-wise multiplications
    is called the **feature map**
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的粉色矩阵，其中包含逐元素相乘的结果，称为**特征图**。
- en: Because of the fact that we are getting the feature map based on the element-wise
    multiplication between the kernel and the corresponding pixels in the original
    input image, changing the values of the kernel or the filter will give different
    feature maps each time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们是通过核与原始输入图像中对应像素的逐元素相乘来获得特征图，所以改变核或滤波器的值每次都会生成不同的特征图。
- en: So, we might think that we need to figure out the values of the feature detectors
    ourselves during the training of the convolution neural networks, but this is
    not the case here. CNNs figure out these numbers during the learning process.
    So, if we have more filters, it means that we can extract more features from the
    image.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可能会认为，在卷积神经网络的训练过程中，我们需要自己确定特征检测器的值，但事实并非如此。CNN 在学习过程中自动确定这些值。所以，如果我们有更多的滤波器，就意味着我们可以从图像中提取更多的特征。
- en: 'Before jumping to the next section, let''s introduce some terminology that
    is usually used in the context of CNNs:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一部分之前，让我们介绍一些在 CNN 上下文中通常使用的术语：
- en: '**Stride**: We mentioned this term briefly earlier. In general, stride is the
    number of pixels by which we move our feature detector or filter over the pixels
    of the input matrix. For example, stride 1 means moving the filter one pixel at
    a time while convolving the input image and stride 2 means moving the filter two
    pixels at a time while convolving the input image. The more stride we have, the
    smaller the generated feature maps are.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步幅**：我们之前简要提到了这个术语。一般来说，步幅是指我们在卷积输入矩阵时，特征检测器或滤波器在输入矩阵上移动的像素数。例如，步幅为 1 意味着每次移动一个像素，而步幅为
    2 意味着每次移动两个像素。步幅越大，生成的特征图就越小。'
- en: '**Zero-padding**: If we wanted to include the border pixels of the input image,
    then part of our filter will be outside the input image. Zero-padding solves this
    problem by padding the input matrix with zeros around the borders.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零填充**：如果我们想包含输入图像的边缘像素，那么部分滤波器将超出输入图像的范围。零填充通过在输入矩阵的边缘周围填充零来解决这个问题。'
- en: Motivation
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: 'Traditional computer vision techniques were used to perform most computer vision
    tasks, such as object detection and segmentation. The performance of these traditional
    computer vision techniques was good but it was never close to being usable in
    real time, for example by autonomous cars. In 2012, Alex Krizhevsky introduced
    CNNs, which made a breakthrough on the ImageNet competition by enhancing the object
    classification error from 26% to 15%. CNNs have been widely used since then and
    different variations have been discovered. It has even outperformed the human
    classification error over the ImageNet competition, as shown in the following
    diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的计算机视觉技术用于执行大多数计算机视觉任务，如物体检测和分割。尽管这些传统计算机视觉技术的性能不错，但始终无法接近实时使用的要求，例如自动驾驶汽车。2012
    年，Alex Krizhevsky 推出了 CNN，凭借其在 ImageNet 竞赛中的突破性表现，将物体分类错误率从 26% 降至 15%。从那时起，CNN
    被广泛应用，并且发现了不同的变种。它甚至在 ImageNet 竞赛中超越了人类分类错误，如下图所示：
- en: '![](img/20ae5e5f-425e-44c5-9001-b1b25ca200d6.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20ae5e5f-425e-44c5-9001-b1b25ca200d6.png)'
- en: 'Figure 9.4: Classification error over time with human level error marked in
    red'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：随着时间推移的分类错误，其中人类级别的错误用红色标出
- en: Applications of CNNs
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 的应用
- en: 'Since the breakthrough the CNNs achieved in different domains of computer vision
    and even natural language processing, most companies have integrated this deep
    learning solution into their computer vision echo system. For example, Google
    uses this architecture for its image search engine, and Facebook uses it for doing
    automatic tagging and more:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自从CNN在计算机视觉甚至自然语言处理的不同领域取得突破以来，大多数公司已经将这一深度学习解决方案集成到他们的计算机视觉生态系统中。例如，谷歌在其图像搜索引擎中使用该架构，Facebook则在自动标记等功能中使用它：
- en: '![](img/4d938629-dc53-44fa-b7fe-7185e92f773d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d938629-dc53-44fa-b7fe-7185e92f773d.png)'
- en: 'Figure 9.5: A typical CNN general architecture for object recognition'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：典型的用于物体识别的CNN一般架构
- en: CNNs achieved this breakthrough because of their architecture, which intuitively
    uses the convolution operation to extract features from the images. Later on,
    you will see that it's very similar to the way the human brain works.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CNN之所以能取得突破，正是因为它们的架构，直观地使用卷积操作从图像中提取特征。稍后你会发现，这与人脑的工作方式非常相似。
- en: Different layers of CNNs
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的不同层
- en: A typical CNN architecture consists of multiple layers that do different tasks,
    as shown in the preceding diagram. In this section, we are going to go through
    them in detail and will see the benefits of having all of them connected in a
    special way to make such a breakthrough in computer vision.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的CNN架构由多个执行不同任务的层组成，如上图所示。在本节中，我们将详细了解它们，并看到将所有这些层以特定方式连接起来的好处，这使得计算机视觉取得了这样的突破。
- en: Input layer
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入层
- en: 'This is the first layer in any CNN architecture. All the subsequent convolution
    and pooling layers expect the input to be in a specific format. The input variables
    will tensors, that has the following shape:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是任何CNN架构中的第一层。所有后续的卷积层和池化层都期望输入以特定格式出现。输入变量将是张量，具有以下形状：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '`batch_size` is a random sample from the original training set that''s used
    during applying stochastic gradient descent.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`是从原始训练集中的一个随机样本，用于应用随机梯度下降时。'
- en: '`image_width` is the width of the input images to the network.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_width`是输入到网络中的图像宽度。'
- en: '`image_height` is the height of the input images to the network.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_height`是输入到网络中的图像高度。'
- en: '`channels` are the number of color channels of the input images. This number
    could be 3 for RGB images or 1 for binary images.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channels`是输入图像的颜色通道数。这个数字对于RGB图像可能是3，对于二值图像则是1。'
- en: For example, consider our famous MNIST dataset. Let's say we are going to perform
    digit classification using CNNs using this dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们著名的MNIST数据集。假设我们将使用CNN进行数字分类，使用这个数据集。
- en: 'If the dataset is composed of monochrome 28 x 28 pixel images like the MNIST
    dataset, then the desired shape for our input layer is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集由28 x 28像素的单色图像组成，如MNIST数据集，那么我们输入层所需的形状如下：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To change the shape of our input features, we can do the following reshaping
    operation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改变输入特征的形状，我们可以执行以下重塑操作：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, we have specified the batch size to be -1, which means that
    this number should be determined dynamically based on the input values in the
    features. By doing this, we will be able to fine-tune our CNN model by controlling
    the batch size.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经将批量大小指定为-1，这意味着这个数字应根据特征中的输入值动态确定。通过这样做，我们将能够通过控制批量大小来微调我们的CNN模型。
- en: 'As an example for the reshape operation, suppose that we divided our input
    samples into batches of five and our feature `["x"]` array will hold 3,920 `values()`
    of the input images, where each value of this array corresponds to a pixel in
    an image. For this case, the input layer will have the following shape:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为重塑操作的示例，假设我们将输入样本分成五个一批，并且我们的特征`["x"]`数组将包含3,920个输入图像的`values()`，其中该数组的每个值对应于图像中的一个像素。对于这种情况，输入层将具有以下形状：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Convolution step
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积步骤
- en: As mentioned earlier, the convolution step got its name from the convolution
    operation. The main purpose of having these convolution steps is to extract features
    from the input images and then feed them to a linear classifier.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，卷积步骤得名于卷积操作。进行这些卷积步骤的主要目的是从输入图像中提取特征，然后将这些特征输入到线性分类器中。
- en: In natural images, features could be anywhere in the image. For example, edges
    could be in the middle or at the corner of the images, so the whole idea of stacking
    a bunch of convolution steps is to be able to detect these features anywhere in
    the image.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然图像中，特征可能出现在图像的任何位置。例如，边缘可能出现在图像的中间或角落，因此堆叠一系列卷积步骤的整个目的是能够在图像的任何地方检测到这些特征。
- en: 'It''s very easy to define a convolution step in TensorFlow. For example, if
    we wanted to apply 20 filters each of size 5 by 5 to the input layer with a ReLU
    activation function, then we can use the following line of code to do that:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中定义卷积步骤非常简单。例如，如果我们想对输入层应用20个大小为5x5的滤波器，并使用ReLU激活函数，那么可以使用以下代码来实现：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first parameter for this `conv2d` function is the input layer that we have
    defined in the preceding code, which has the appropriate shape, and the second
    argument is the filters argument which specifies the number of filters to be applied
    to the image where the higher the number of filters, the more features are extracted
    from the input image. The third parameter is the `kernel_size`, which represents
    the size of the filter or the feature detector. The padding parameters specifies
    where we use `"same"` here to introduce zero-padding to the corner pixels of the
    input image. The last argument specifies the activation function that should be
    used for the output of the convolution operation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`conv2d`函数的第一个参数是我们在前面的代码中定义的输入层，它具有合适的形状，第二个参数是滤波器参数，指定要应用于图像的滤波器数量，滤波器数量越多，从输入图像中提取的特征就越多。第三个参数是`kernel_size`，表示滤波器或特征探测器的大小。padding参数指定了使用零填充的方法，这里我们使用“same”来给输入图像的角落像素添加零填充。最后一个参数指定了应该应用于卷积操作输出的激活函数。
- en: 'So, in our MNIST example, the input tensor will have the following shape:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的MNIST示例中，输入张量将具有以下形状：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And the output tensor for this convolution step will have the following shape:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该卷积步骤的输出张量将具有以下形状：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output tensor has the same dimensions as the input images, but now we have
    20 channels that represent applying the 20 filters to the input image.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出张量的维度与输入图像相同，但现在我们有20个通道，表示应用了20个滤波器到输入图像。
- en: Introducing non-linearity
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入非线性
- en: 'In the convolution step, we talked about feeding the output of the convolution
    step to a ReLU activation function to introduce non-linearity:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积步骤中，我们提到过将卷积步骤的输出传递给ReLU激活函数以引入非线性：
- en: '![](img/655f87c8-366b-4717-a6bc-8b55c6f1c4c4.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/655f87c8-366b-4717-a6bc-8b55c6f1c4c4.png)'
- en: 'Figure 9.6: ReLU activation function'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.6: ReLU 激活函数'
- en: 'The ReLU activation function replaces all the negative pixel values with zeros
    and the whole purpose of feeding the output of the convolution step to this activation
    function is to introduce non-linearity in the output image because this will be
    useful for the training process as the data that we are using is usually non-linear.
    To clearly understand the benefit of ReLU activation function, have a look at
    the following figure, which shows the row output of the convolution step and the
    rectified version of it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数将所有负的像素值替换为零，而将卷积步骤的输出传递给该激活函数的目的就是为了引入非线性，因为我们使用的数据通常是非线性的，这对训练过程非常有用。为了清楚地理解ReLU激活函数的好处，看看下面的图，它展示了卷积步骤的行输出及其经过修正后的版本：
- en: '![](img/fdcf199b-f53c-4b9f-ac13-201474a7f2f4.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdcf199b-f53c-4b9f-ac13-201474a7f2f4.png)'
- en: 'Figure 9.7: The result of applying ReLU to the input feature map'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.7: 对输入特征图应用ReLU的结果'
- en: The pooling step
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化步骤
- en: One of the important steps for our learning process is the pooling step, which
    is sometimes called the subsampling or downsampling step. This step is mainly
    for reducing the dimensionality of the output of the convolution step (feature
    map). The advantage of this pooling step is reducing the size of the feature map
    while keeping the important information in the newly reduced version.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习过程中的一个重要步骤是池化步骤，有时也叫做下采样或子采样步骤。这个步骤主要是为了减少卷积步骤输出的特征图（feature map）的维度。池化步骤的优点是，在减小特征图的大小的同时，保留了新版本中重要的信息。
- en: 'The following figure shows this step by scanning the image with a 2 by 2 filter
    and stride 2 while applying the max operation. This kind of pooling operation
    is called **max pool**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了通过一个2x2滤波器和步幅2扫描图像，并应用最大池化操作的步骤。这种池化操作称为**最大池化**：
- en: '![](img/a1278534-f054-4594-b1f5-9e534c5d0c2d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1278534-f054-4594-b1f5-9e534c5d0c2d.png)'
- en: 'Figure 9.8: An example of a max pooling operation on a rectified feature map
    (obtained after convolution and ReLU operation) by using a 2 x 2 window (source:
    http://textminingonline.com/wp-content/uploads/2016/10/max_polling-300x256.png)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：使用2 x 2窗口在经过卷积和ReLU操作后的修正特征图上进行最大池化操作的示例（来源：http://textminingonline.com/wp-content/uploads/2016/10/max_polling-300x256.png）
- en: 'We can connect the output of the convolution step to the pooling layer by using
    the following line of code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码行将卷积步骤的输出连接到池化层：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The pooling layer receives the input from the convolution step with the following
    shape:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层接收来自卷积步骤的输入，形状如下：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For example, in our digit classification task, the input to the pooling layer
    will have the following shape:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的数字分类任务中，池化层的输入将具有以下形状：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the pooling operation will have the following shape:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作的输出将具有以下形状：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this example, we have reduced the size of the output of the convolution step
    by 50%. This step is very useful because it keeps only the important information
    and it also reduces the model's complexity and hence avoids overfitting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将卷积步骤的输出大小减少了50%。这个步骤非常有用，因为它只保留了重要的信息，同时还减少了模型的复杂度，从而避免了过拟合。
- en: Fully connected layer
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: 'After stacking up a bunch of convolution and pooling steps, we follow them
    with a fully connected layer where we feed the extracted high-level features that
    we got from the input image to this fully connected layer to use them and do the
    actual classification based on these features:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆叠了多个卷积和池化步骤之后，我们使用一个全连接层，在这个层中，我们将从输入图像中提取的高级特征输入到全连接层，以便利用这些特征进行实际的分类：
- en: '![](img/49cba1b5-f082-4e6a-87b4-a24e8cbcad72.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49cba1b5-f082-4e6a-87b4-a24e8cbcad72.png)'
- en: 'Figure 9.9: Fully connected layer -each node is connected to every other node
    in the adjacent layer'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：全连接层 - 每个节点都与相邻层的所有其他节点相连接
- en: 'For example, in the case of the digit classification task, we can follow the
    convolution and pooling step with a fully connected layer that has 1,024 neurons
    and ReLU activation to perform the actual classification. This fully connected
    layer accepts the input in the following format:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在数字分类任务中，我们可以在卷积和池化步骤之后使用一个具有1,024个神经元和ReLU激活函数的全连接层来执行实际的分类。这个全连接层接受以下格式的输入：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, we need to reshape or flatten our input feature map from `pool_layer2`
    to match this format. We can use the following line of code to reshape the output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要重新调整或展平来自`pool_layer2`的输入特征图，以匹配这种格式。我们可以使用以下代码行来重新调整输出：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this reshape function, we have used `-1` to indicate that the batch size
    will be dynamically determined and each example from the `pool_layer1` output
    will have a width of `14` and a height of `14` with `20` channels each.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个reshape函数中，我们使用`-1`表示批量大小将动态确定，并且`pool_layer1`输出中的每个示例将具有宽度为`14`、高度为`14`且有`20`个通道。
- en: 'So the final output of this reshape operation will be as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个重塑操作的最终输出将如下所示：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we can use the `dense()` function of TensorFlow to define our fully
    connected layer with the required number of neurons (units) and the final activation
    function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用TensorFlow的`dense()`函数来定义我们的全连接层，设定所需的神经元（单位）数量和最终的激活函数：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Logits layer
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Logits层
- en: 'Finally, we need the logits layer, which will take the output of the fully
    connected layer and then produce the raw prediction values. For example, in the
    case of the digit classification, the output will be a tensor of 10 values, where
    each value represents the score of one class from 0-9\. So, let''s define this
    logit layer for the digit classification example, where we need 10 outputs only,
    and with linear activation, which is the default for the `dense()` function of
    TensorFlow:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要logits层，它将接受全连接层的输出并生成原始的预测值。例如，在数字分类任务中，输出将是一个包含10个值的张量，每个值代表0-9类中的一个类别的分数。因此，让我们为数字分类示例定义这个logits层，其中我们只需要10个输出，并且使用线性激活函数，这是TensorFlow的`dense()`函数的默认值：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/983d2783-0a86-4586-acac-3d52882d8cd0.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/983d2783-0a86-4586-acac-3d52882d8cd0.png)'
- en: 'Figure 9.10: Training the ConvNet'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：训练ConvNet
- en: 'The final output of this logits layer will be a tensor of the following shape:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个logits层的最终输出将是一个具有以下形状的张量：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As mentioned previously, the logits layer of the model will return the raw
    predictions our our batch. But we need to convert these values to interpretable
    format:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，模型的logits层将返回我们批次的原始预测值。但我们需要将这些值转换为可解释的格式：
- en: The predicted class for the input sample 0-9.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入样本0-9的预测类别。
- en: The scores or probabilities for each possible class. For example, the probability
    that the sample is 0, is 1, and so on.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个可能类别的得分或概率。例如，样本属于类别0的概率是1，依此类推。
- en: '![](img/d26f9bb1-357c-445c-932f-b8af5ec6d556.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d26f9bb1-357c-445c-932f-b8af5ec6d556.jpeg)'
- en: 'Figure 9.11: A visualization of the different layers of a CNN (source: http://cs231n.github.io/assets/cnn/convnet.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：CNN不同层的可视化（来源：http://cs231n.github.io/assets/cnn/convnet.jpeg）
- en: 'So, our predicted class will be the one that has the highest value in the 10
    probabilities. We can get this value by using the `argmax` function as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的预测类别将是10个概率中最大值对应的类别。我们可以通过使用`argmax`函数如下获取这个值：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Remember that the `logits_layer` has the following shape:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`logits_layer`的形状是这样的：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So, we need to find the max values along our predictions, which is the dimension
    that has an index of 1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要沿着预测结果的维度（即索引为1的维度）找到最大值：
- en: 'Finally, we can get our next value, which represents the probabilities of each
    target class, by applying `softmax` activation to the output of the `logits_layer`,
    which will squash each value to be between 0 and 1:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过对`logits_layer`的输出应用`softmax`激活函数来得到下一个值，该值表示每个目标类别的概率，将每个值压缩到0和1之间：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: CNN basic example – MNIST digit classification
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN基础示例 – MNIST数字分类
- en: In this section, we will do a complete example of implementing a CNN for digit
    classification using the MNIST dataset. We will build a simple model of two convolution
    layers and fully connected layers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用MNIST数据集实现数字分类的完整CNN示例。我们将构建一个包含两个卷积层和全连接层的简单模型。
- en: 'Let''s start off by importing the libraries that will be needed for this implementation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入实现中所需的库：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will use TensorFlow helper functions to download and preprocess the
    MNIST dataset as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用TensorFlow的辅助函数下载并预处理MNIST数据集，如下所示：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The dataset is split into three disjoint sets: training, validation, and testing.
    So, let''s print the number of images in each set:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被分为三个不重叠的集合：训练集、验证集和测试集。因此，让我们打印出每个集合中的图像数量：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The actual labels of the images are stored in a one-hot encoding format, so
    we have an array of 10 values of zeros except for the index of the class that
    this image represents. For later use, we need to get the class numbers of the
    dataset as integers:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的实际标签以独热编码格式存储，所以我们有一个包含10个值的数组，除了表示该图像所属类别的索引外，其余值均为零。为了后续使用，我们需要将数据集中的类别号转换为整数：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s define some known variables to be used later in our implementation:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一些已知的变量，以便在后续实现中使用：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we need to define a helper function to plot some images from the dataset.
    This helper function will plot the images in a grid of nine subplots:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一个辅助函数，用于从数据集中绘制一些图像。这个辅助函数将以九个子图的网格方式绘制图像：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s plot some images from the test set and see what it looks like:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从测试集绘制一些图像，看看它们长什么样：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![](img/2f03662a-f7fb-4e0d-a0cb-d2f1da0cbfc0.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f03662a-f7fb-4e0d-a0cb-d2f1da0cbfc0.png)'
- en: 'Figure 9.12: A visualization of some examples from the MNIST dataset'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：来自MNIST数据集的一些示例的可视化
- en: Building the model
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Now, it''s time to build the core of the model. The computational graph includes
    all the layers we mentioned earlier in this chapter. We''ll start by defining
    some functions that will be used to define variables of a specific shape and randomly
    initialize them:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了构建模型核心部分的时候。计算图包含我们在本章前面提到的所有层。我们将从定义一些用于定义特定形状变量并随机初始化它们的函数开始：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s define the function that will be responsible for creating a new
    convolution layer based on some input layer, input channels, filter size, number
    of filters, and whether to use pooling parameters or not:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个函数，该函数负责根据某些输入层、输入通道、滤波器大小、滤波器数量以及是否使用池化参数来创建一个新的卷积层：
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As we mentioned previously, the pooling layer produces a 4D tensor. We need
    to flatten this 4D tensor to a 2D one to be fed to the fully connected layer:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，池化层生成一个4D张量。我们需要将这个4D张量展平为2D张量，以便传递到全连接层：
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This function creates a fully connected layer which assumes that the input
    is a 2D tensor:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数创建一个全连接层，假设输入是一个2D张量：
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Before building the network, let''s define a placeholder for the input images
    where the first dimension is `None` to represent an arbitrary number of images:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建网络之前，让我们定义一个占位符用于输入图像，其中第一维是`None`，表示可以输入任意数量的图像：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As we mentioned previously, the convolution step expects the input images to
    be in the shape of a 4D tensor. So, we need to reshape the input images to be
    in the following shape:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，卷积步骤期望输入图像的形状是4D张量。因此，我们需要将输入图像调整为以下形状：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'So, let''s reshape the input values to match this format:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们重新调整输入值的形状以匹配这种格式：
- en: '[PRE36]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we need to define another placeholder for the actual class values, which
    will in one-hot encoding format:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义另一个占位符用于实际类别的值，格式为独热编码：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Also, we need to define a placeholder to hold the integer values of the actual
    class:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要定义一个占位符来保存实际类别的整数值：
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'So, let''s start off by building the first CNN:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从构建第一个卷积神经网络开始：
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s check the shape of the output tensor that will be produced by the first
    convolution layer:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查第一卷积层将产生的输出张量的形状：
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we will create the second convolution network and feed the output of
    the first one to it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建第二个卷积神经网络，并将第一个网络的输出作为输入：
- en: '[PRE42]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Also, we need to double-check the shape of the output tensor of the second convolution
    layer. The shape should be `(?, 7, 7, 36)`, where the `?` mark means an arbitrary
    number of images.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要再次检查第二卷积层输出张量的形状。形状应该是`(?, 7, 7, 36)`，其中`?`表示任意数量的图像。
- en: 'Next, we need to flatten the 4D tensor to match the expected format for the
    fully connected layer, which is a 2D tensor:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将4D张量展平，以匹配全连接层所期望的格式，即2D张量：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We need to double-check the shape of the output tensor of the flattened layer:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要再次检查展平层输出张量的形状：
- en: '[PRE44]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we will create a fully connected layer and feed the output of the flattened
    layer to it. We will also feed the output of the fully connected layer to a ReLU
    activation function before feeding it to the second fully connected layer:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个全连接层，并将展平层的输出传递给它。我们还将把全连接层的输出输入到ReLU激活函数中，然后再传递给第二个全连接层：
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s double-check the shape of the output tensor of the first fully connected
    layer:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次检查第一个全连接层输出张量的形状：
- en: '[PRE47]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, we need to add another fully connected layer, which will take the output
    of the first fully connected layer and produce an array of size 10 for each image
    that represents the scores for each target class being the correct one:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要添加另一个全连接层，它将接收第一个全连接层的输出，并为每张图像生成一个大小为10的数组，表示每个目标类别是正确类别的得分：
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we''ll normalize these scores from the second fully connected layer and
    feed it to a `softmax` activation function, which will squash the values to be
    between 0 and 1:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对第二个全连接层的得分进行归一化，并将其输入到`softmax`激活函数中，这样它会将值压缩到0到1之间：
- en: '[PRE52]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, we need to choose the target class that has the highest probability by
    using the `argmax` function of TensorFlow:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要使用 TensorFlow 的`argmax`函数选择具有最高概率的目标类别：
- en: '[PRE53]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Cost function
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: 'Next, we need to define our performance measure, which is the cross-entropy.
    The value of the cross-entropy will be 0 if the predicted class is correct:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义我们的性能衡量标准，即交叉熵。如果预测的类别是正确的，那么交叉熵的值为0：
- en: '[PRE54]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we need to average all the cross-entropy values that we got from the
    previous step to be able to get a single performance measure over the test set:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将之前步骤得到的所有交叉熵值求平均，以便得到一个单一的性能衡量标准：
- en: '[PRE55]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we have a cost function that needs to be optimized/minimized, so we will
    be using `AdamOptimizer`, which is an optimization method like gradient descent
    but a bit more advanced:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个需要优化/最小化的成本函数，因此我们将使用`AdamOptimizer`，它是一种优化方法，类似于梯度下降，但更为先进：
- en: '[PRE56]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Performance measures
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能衡量标准
- en: 'For showing the output, let''s define a variable to check whether the predicted
    class is equal to the true one:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显示输出，让我们定义一个变量来检查预测的类别是否等于真实类别：
- en: '[PRE57]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Calculate the model accuracy by casting the boolean values then averaging them
    to sum the correctly classified ones:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将布尔值转换并求平均，计算模型的准确性，进而统计正确分类的数量：
- en: '[PRE58]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Model training
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Let''s kick off the training process by creating a session variable that will
    be responsible for executing the computational graph that we defined earlier:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建一个会负责执行先前定义的计算图的会话变量来启动训练过程：
- en: '[PRE59]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Also, we need to initialize the variables that we have defined so far:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要初始化到目前为止已定义的变量：
- en: '[PRE60]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We are going to feed the images in batches to avoid an out-of-memory error:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按批次输入图像，以避免出现内存溢出错误：
- en: '[PRE61]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Before kicking the training process, we are going to define a helper function
    that will perform the optimization process by iterating through the training batches:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练过程之前，我们将定义一个辅助函数，该函数通过遍历训练批次来执行优化过程：
- en: '[PRE62]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'And we''ll define some helper functions to help us visualize the results of
    the model and to see which images are misclassified by the model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将定义一些辅助函数，帮助我们可视化模型的结果，并查看哪些图像被模型误分类：
- en: '[PRE63]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can also plot the confusion matrix of the predicted results compared to
    the actual true classes:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制预测结果与实际类别的混淆矩阵：
- en: '[PRE64]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we are going to define a helper function to help us measure the accuracy
    of the trained model over the test set:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将定义一个辅助函数，帮助我们测量训练模型在测试集上的准确率：
- en: '[PRE65]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let''s print the accuracy of the created model over the test set without doing
    any optimization:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出未经任何优化的模型在测试集上的准确率：
- en: '[PRE66]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let''s get a sense of the optimization process actually enhancing the model
    capability to classify images to their correct class by running the optimization
    process for one iteration:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行一次优化过程来感受优化过程如何增强模型的能力，将图像正确分类到对应的类别：
- en: '[PRE68]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, let''s get down to business and kick off a long optimization process of
    10,000 iterations:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始进行一项长时间的优化过程，进行 10,000 次迭代：
- en: '[PRE69]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'At the end of the output, you should be getting something very close to the
    following output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出的最后，您应该看到与以下输出非常接近的结果：
- en: '[PRE70]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, let''s check how the model will generalize over the test:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查模型在测试集上的泛化能力：
- en: '[PRE71]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '![](img/cea9722b-34a1-4a22-a216-9083a559579a.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cea9722b-34a1-4a22-a216-9083a559579a.png)'
- en: 'Figure 9.13: Accuracy over the test'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：测试集上的准确率
- en: '[PRE73]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The following is the output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/71ec6e79-46a4-4511-b063-c5985c75d61d.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71ec6e79-46a4-4511-b063-c5985c75d61d.png)'
- en: 'Figure 9.14: Confusion matrix of the test set.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：测试集的混淆矩阵。
- en: It was interesting that we actually got almost 93% accuracy over the test while
    using a basic convolution network. This implementation and the results show you
    what a simple convolution network can do.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，实际上在使用基础卷积网络时，我们在测试集上的准确率几乎达到了 93%。这个实现和结果展示了一个简单的卷积网络能做些什么。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered the intuition and the technical details of
    how CNNs work. we also had a look at how to implement a basic architecture of
    a CNN in TensorFlow.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了卷积神经网络（CNN）的直觉和技术细节，同时也了解了如何在 TensorFlow 中实现一个基本的 CNN 架构。
- en: In the next chapter we'll demonstrate more advanced architectures that could
    be used for detecting objects in one of the image datasets widely used by data
    scientists. We'll also see the beauty of CNNs and how they come to mimic human
    understanding of objects by first realizing the basic features of objects and
    then building up more advanced semantic features on them to come up with a classification
    for them. Although this process happens very quickly in our minds, it is what
    actually happens when we recognize objects.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将展示一些更先进的架构，这些架构可以用于检测数据科学家广泛使用的图像数据集中的物体。我们还将看到卷积神经网络（CNN）的魅力，它们是如何通过首先识别物体的基本特征，再在这些特征基础上构建更高级的语义特征，从而模拟人类对物体的理解，最终得出对物体的分类的。尽管这个过程在人类大脑中发生得非常迅速，但它实际上是我们识别物体时的运作方式。
