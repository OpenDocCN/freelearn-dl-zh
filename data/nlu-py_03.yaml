- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Approaches to Natural Language Understanding – Rule-Based Systems, Machine Learning,
    and Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言理解方法 – 基于规则的系统、机器学习和深度学习
- en: This chapter will review the most common approaches to **natural language understanding**
    (**NLU**) and discuss both the benefits and drawbacks of each approach, including
    rule-based techniques, statistical techniques, and deep learning. It will also
    discuss popular pre-trained models such as **Bidirectional Encoder Representations
    from Transformers** (**BERT**) and its variants. We will learn that NLU is not
    a single technology; it includes a range of techniques, which are applicable to
    different goals.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将回顾最常见的自然语言理解（**NLU**）方法，并讨论每种方法的优缺点，包括基于规则的技术、统计技术和深度学习。还将讨论流行的预训练模型，如**双向编码器表示来自变换器（BERT）**及其变体。我们将了解到，自然语言理解并非单一的技术，它包括一系列可用于不同目标的技术。
- en: 'In this chapter, we cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Rule-based approaches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于规则的方法
- en: Traditional machine-learning approaches
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的机器学习方法
- en: Deep learning approaches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习方法
- en: Pre-trained models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Considerations for selecting technologies
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择技术时的考虑因素
- en: Let’s begin!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Rule-based approaches
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于规则的方法
- en: The basic idea behind **rule-based approaches** is that language obeys rules
    about how words are related to their meanings. For example, when we learn foreign
    languages, we typically learn specific rules about what words mean, how they’re
    ordered in sentences, and how prefixes and suffixes change the meanings of words.
    The rule-based approach to NLU operates on the premise that these kinds of rules
    can be provided to an NLU system so that the system can determine the meanings
    of sentences in the same way that a person does.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法的基本思想是，语言遵循关于单词与其含义之间关系的规则。例如，当我们学习外语时，我们通常会学习有关单词含义的特定规则，了解它们在句子中的排列方式，以及前缀和后缀如何改变单词的意义。基于规则的自然语言理解方法的运作前提是，可以为自然语言理解系统提供这些类型的规则，从而使系统能够像人类一样确定句子的含义。
- en: The rule-based approach was widely used in NLU from the mid-1950s through the
    mid-1990s until machine-learning-based approaches became popular. However, there
    are still NLU problems where rule-based approaches are useful, either on their
    own or when combined with other techniques.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法在自然语言理解（NLU）中广泛应用，从1950年代中期到1990年代中期，直到基于机器学习的方法变得流行。然而，仍然有一些自然语言理解问题，基于规则的方法仍然有用，无论是单独使用，还是与其他技术结合使用。
- en: We will begin by reviewing the rules and data that are relevant to various aspects
    of language.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回顾与语言各个方面相关的规则和数据。
- en: Words and lexicons
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词与词汇表
- en: Nearly everyone is familiar with the idea of words, which are usually defined
    as units of language that can be spoken individually. As we saw in [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016),
    in most, but not all, languages, words are separated by white space. The set of
    words in a language is referred to as the language’s **lexicon**. The idea of
    a lexicon corresponds to what we think of as a dictionary – a list of the words
    in a language. A computational lexicon also includes other information about each
    word. In particular, it includes its part or parts of speech. Depending on the
    language, it could also include information on whether the word has irregular
    forms (such as, for the irregular English verb “*eat*,” the past tense “*ate*”
    and the past participle “*eaten*” are irregular). Some lexicons also include semantic
    information such as words that are related in meaning to each word.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个人都熟悉单词的概念，单词通常被定义为可以单独发音的语言单位。正如我们在[*第一章*](B19005_01.xhtml#_idTextAnchor016)中看到的，在大多数但并非所有语言中，单词通过空格分隔。语言中的单词集合被称为该语言的**词汇表**。词汇表的概念与我们所认为的字典相对应——它是语言中单词的列表。计算机词汇表还包括有关每个单词的其他信息，特别是它的词性。根据语言的不同，它还可能包括该单词是否有不规则形式的信息（例如，对于不规则的英语动词“*eat*”，其过去式“*ate*”和过去分词“*eaten*”是不规则的）。有些词汇表还包括语义信息，如与每个单词含义相关的词语。
- en: Part-of-speech tagging
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词性标注
- en: Traditional parts of speech as taught in schools include categories such as
    “*noun*”, “*verb*”, “*adjective*”, *preposition*, and so on. The parts of speech
    used in computational lexicons are usually more detailed than these since they
    need to express more specific information than what could be captured by traditional
    categories. For example, the traditional *verb* category in English is usually
    broken down into several different parts of speech corresponding to the different
    forms of the verb, such as the past tense and past participle forms. A commonly
    used set of parts of speech for English is the parts of speech from the Penn Treebank
    ([https://catalog.ldc.upenn.edu/LDC99T42](https://catalog.ldc.upenn.edu/LDC99T42)).
    Different languages will have different parts of speech categories in their computational
    lexicons.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 学校中教授的传统词性包括“*名词*”、“*动词*”、“*形容词*”、*介词*等类别。计算词典中使用的词性通常比这些更为详细，因为它们需要表达比传统类别能够捕捉到的更具体的信息。例如，英语中传统的*动词*类别通常被细分为几个不同的词性，分别对应动词的不同形式，如过去式和过去分词。一个常用的英语词性集是来自Penn
    Treebank的词性（[https://catalog.ldc.upenn.edu/LDC99T42](https://catalog.ldc.upenn.edu/LDC99T42)）。不同的语言在其计算词典中会有不同的词性类别。
- en: A very useful task in processing natural language is to assign parts of speech
    to the words in a text. This is called **part-of-speech tagging** (**POS tagging**).
    *Table 3.1* shows an example of the Penn Treebank part-of-speech tags for the
    sentence “*We would like to book a flight from Boston* *to London*:”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 处理自然语言时，一个非常有用的任务是为文本中的单词分配词性。这被称为**词性标注**（**POS标注**）。*表3.1*展示了句子“*We would
    like to book a flight from Boston to London*”中，Penn Treebank词性标签的一个示例：
- en: '| **Word** | **Part** **of speech** | **Meaning of part of** **speech label**
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **单词** | **词性** | **词性标签的含义** |'
- en: '| --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| we | PRP | Personal pronoun |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| we | PRP | 人称代词 |'
- en: '| would | MD | Modal verb |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| would | MD | 情态动词 |'
- en: '| like | VB | Verb, base form |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| like | VB | 动词，基本形式 |'
- en: '| to | TO | To (this word has its own part of speech) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| to | TO | 不定式（该词有其独立的词性） |'
- en: '| book | VB | Verb, base form |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| book | VB | 动词，基本形式 |'
- en: '| a | DT | Determiner (article) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| a | DT | 限定词（冠词） |'
- en: '| flight | NN | Singular noun |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| flight | NN | 单数名词 |'
- en: '| from | IN | Preposition |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| from | IN | 介词 |'
- en: '| Boston | NNP | Proper noun |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Boston | NNP | 专有名词 |'
- en: '| to | TO | To |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| to | TO | 不定式 |'
- en: '| London | NNP | Proper noun |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| London | NNP | 专有名词 |'
- en: Table 3.1 – Part-of-speech tags for “We would like to book a flight from Boston
    to London”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 – 句子“We would like to book a flight from Boston to London”的词性标注
- en: POS tagging is not just a matter of looking words up in a dictionary and labeling
    them with their parts of speech because many words have more than one part of
    speech. In our example, one of these is “*book*,” which is used as a verb in the
    example but is also commonly used as a noun. POS tagging algorithms have to not
    only look at the word itself but also consider its context in order to determine
    the correct part of speech. In this example, “*book*” follows “*to*,” which often
    indicates that the next word is a verb.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注不仅仅是查字典并为单词标注词性，因为许多单词有多个词性。在我们的示例中，其中一个就是“*book*”，在这个例子中它作为动词使用，但也常作为名词使用。词性标注算法不仅要看单词本身，还要考虑它的上下文，以确定正确的词性。在这个例子中，“*book*”跟随在“*to*”之后，而“*to*”通常表示下一个单词是动词。
- en: Grammar
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语法
- en: Grammar rules are the rules that describe how words are ordered in sentences
    so that the sentences can be understood and also so that they can correctly convey
    the author’s meaning. They can be written in the form of rules describing part-whole
    relationships between sentences and their components. For example, a common grammar
    rule for English says that a sentence consists of a noun phrase followed by a
    verb phrase. A full computational grammar for any natural language usually consists
    of hundreds of rules and is very complex. It isn’t very common now to build grammar
    from scratch; rather, grammar is already included in commonly used Python NLP
    libraries such as **natural language toolkit** (**NLTK**) and **spaCy**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 语法规则是描述单词在句子中如何排列的规则，以便句子能够被理解，并且能够正确传达作者的意思。它们可以用描述句子及其成分之间部分-整体关系的规则来书写。例如，英语中的一个常见语法规则是句子由名词短语后跟动词短语组成。任何自然语言的完整计算语法通常由数百条规则组成，非常复杂。现在从零开始构建语法并不常见；相反，语法通常已经包含在常用的Python自然语言处理库中，如**自然语言工具包**（**NLTK**）和**spaCy**。
- en: Parsing
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析
- en: Finding the relationships between parts of a sentence is known as **parsing**.
    This involves applying the grammar rules to a specific sentence to show how the
    parts of the sentence are related to each other. *Figure 3**.1* shows the parsing
    of the sentence “*We would like to book a flight*.” In this style of parsing,
    known as **dependency parsing**, the relationships between the words are shown
    as arcs between the words. For example, the fact that “*we*” is the subject of
    the verb “*like*” is shown by an arc labeled **nsubj** connecting “*like*” to
    “*we*”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 找出句子各部分之间的关系称为**解析**。这涉及应用语法规则来分析特定句子，展示句子各部分是如何相互关联的。*图 3.1*展示了句子“We would
    like to book a flight”的解析。在这种解析风格中，称为**依存解析**，单词之间的关系通过连接单词的弧线来表示。例如，“*we*”是动词“*like*”的主语，这一事实通过标记为**nsubj**的弧线将“*like*”与“*we*”连接起来。
- en: '![Figure 3.1 – Parsing for “We would like to book a flight”](img/B19005_03_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – “We would like to book a flight”句子的解析](img/B19005_03_01.jpg)'
- en: Figure 3.1 – Parsing for “We would like to book a flight”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – “We would like to book a flight”句子的解析
- en: At this point, it isn’t necessary to worry about the details of parsing – we
    will discuss it in more detail in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，不必担心解析的细节——我们将在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中详细讨论。
- en: Semantic analysis
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分析
- en: Parsing involves determining how words are structurally related to each other
    in sentences, but it doesn’t say anything about their meanings or how their meanings
    are related. This kind of processing is done through **semantic analysis**. There
    are many approaches to semantic analysis—this is an active research field—but
    one way to think of semantic analysis is that it starts with the main verb of
    the sentence and looks at the relationships between the verb and other parts of
    the sentence, such as the subject, direct object, and related prepositional phrases.
    For example, the subject of “*like*” in *Figure 3**.1* is “*We*.” “*We*” could
    be described as the “*experiencer*” of “*like*” since it is described as experiencing
    “*liking.*” Similarly, the thing that is liked, “*to book a flight*,” could be
    described as the “*patient*” of “*like.*” Semantic analysis is most frequently
    done through the application of rules, but it can also be done with machine learning
    techniques, as described in the next sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 解析涉及确定单词在句子中的结构性关系，但它不涉及单词的意义或意义之间的关系。这类处理通过**语义分析**来完成。语义分析有很多方法——这是一个活跃的研究领域——但一种思路是，语义分析从句子的主要动词开始，查看动词与句子中其他部分（如主语、直接宾语和相关介词短语）之间的关系。例如，*图
    3.1*中，“*like*”的主语是“*We*”。“*We*”可以描述为“*like*”的“*experiencer*”，因为它被描述为经历“*liking*”的动作。类似地，被喜欢的事物“*to
    book a flight*”可以描述为“*like*”的“*patient*”。语义分析最常见的做法是应用规则，但也可以通过机器学习技术来完成，下一节将进行介绍。
- en: Finding semantic relationships between the concepts denoted by words, independently
    of their roles in sentences, can also be useful. For example, we can think of
    a “*dog*” as a kind of “*animal*,” or we can think of “*eating*” as a kind of
    action. One helpful resource for finding these kinds of relationships is Wordnet
    ([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)), which is a
    large, manually prepared database describing the relationships between thousands
    of English words. *Figure 3**.2* shows part of the Wordnet information for the
    word “*airplane*,” indicating that an airplane is a kind of “*heavier-than-aircraft*,”
    which is a kind of “*aircraft*,” and so on, going all the way up to the very general
    category “*entity*:”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在不考虑词语在句子中的角色的情况下，找到表示词汇概念之间的语义关系也是有用的。例如，我们可以将“*dog*”看作是“*animal*”的一种，或者我们可以将“*eating*”看作是一种动作。一个有助于找到这类关系的资源是Wordnet（[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)），它是一个大型的手工构建数据库，描述了成千上万的英语单词之间的关系。*图
    3.2*展示了“*airplane*”这个词的部分Wordnet信息，表明飞机是“*heavier-than-aircraft*”的一种，而“*heavier-than-aircraft*”是“*aircraft*”的一种，依此类推，一直到非常一般的类别“*entity*”。
- en: '![Figure 3.2 – Wordnet semantic hierarchy for the word “airplane”](img/B19005_03_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – “airplane”一词的Wordnet语义层级](img/B19005_03_02.jpg)'
- en: Figure 3.2 – Wordnet semantic hierarchy for the word “airplane”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – “airplane”一词的Wordnet语义层级
- en: Pragmatic analysis
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用分析
- en: '**Pragmatic analysis** determines the meanings of words and phrases in context.
    For example, in long texts, different words can be used to refer to the same thing,
    or different things can be referred to with the same word. This is called **coreference**.
    For example, the sentence “*We want to book a flight from Boston to London*” could
    be followed by “*the flight needs to leave before 10 a.m.*” Pragmatic analysis
    determines that the flight that needs to leave before 10 a.m. is the same flight
    that we want to book.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**语用分析**确定词语和短语在上下文中的意义。例如，在长篇文本中，不同的词可以用来指代相同的事物，或者不同的事物可以用相同的词来指代。这被称为**共指**。例如，句子“*We
    want to book a flight from Boston to London*”后面可能跟着“*the flight needs to leave
    before 10 a.m.*”语用分析确定需要在上午10点之前起飞的航班就是我们想要预订的航班。'
- en: 'A very important type of pragmatic analysis is **named entity recognition**
    (**NER**), which links up references that occur in texts to corresponding entities
    in the real world. *Figure 3**.3* shows NER for the sentence “*Book a flight to
    London on United for less than 1,000 dollars*.” “*London*” is a named entity,
    which is labeled as a geographical location, “*United*” is labeled as an organization,
    and “*less than 1,000 dollars*” is labeled as a monetary amount:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常重要的语用分析方法是**命名实体识别**（**NER**），它将文本中出现的引用与现实世界中对应的实体关联起来。*图3.3*展示了对句子“*Book
    a flight to London on United for less than 1,000 dollars*”的NER分析。“*London*”是一个命名实体，被标记为地理位置，“*United*”被标记为组织，“*less
    than 1,000 dollars*”被标记为货币金额：
- en: '![Figure 3.3 – NER for “Book a flight to London on United for less than 1,000
    dollars”](img/B19005_03_03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – “Book a flight to London on United for less than 1,000 dollars” 的NER分析](img/B19005_03_03.jpg)'
- en: Figure 3.3 – NER for “Book a flight to London on United for less than 1,000
    dollars”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – “Book a flight to London on United for less than 1,000 dollars” 的NER分析
- en: Pipelines
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: 'In NLP applications, the steps we have just described are most often implemented
    as a **pipeline**; that is, a sequence of steps where the results of one step
    are the input to the next step. For example, a typical NLP pipeline might be as
    follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理应用中，我们刚才描述的步骤通常作为**管道**来实现；即一系列步骤，其中一个步骤的结果是下一个步骤的输入。例如，一个典型的NLP管道可能如下所示：
- en: '**Lexical lookup**: Look up the words in the application’s dictionary'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇查找**：在应用程序的字典中查找词语'
- en: '**POS tagging**: Assign parts of speech to each word in context'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注**：为每个词在上下文中分配词性'
- en: '**Parsing**: Determine how the words are related to each other'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句法分析**：确定词语之间的关系'
- en: '**Semantic analysis**: Determine the meanings of the words and the overall
    meaning of the sentence'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分析**：确定单词的意义及句子的整体意义'
- en: '**Pragmatic analysis**: Determine aspects of the meaning that depend on a broader
    context, such as the interpretations of pronouns'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语用分析**：确定依赖于更广泛上下文的意义方面，例如代词的解释'
- en: One advantage of using pipelines is that each step can be implemented with different
    technologies, as long as the output of that step is in the format expected by
    the next step. So, pipelines are not only useful in rule-based approaches but
    also in the other techniques, which we will describe in the next sections.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道的一个优点是每个步骤可以使用不同的技术来实现，只要该步骤的输出格式符合下一个步骤的预期格式。因此，管道不仅在基于规则的方法中有用，也在我们将在接下来的部分描述的其他技术中有用。
- en: More details on rule-based techniques will be provided in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有关基于规则技术的更多细节将在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中提供。
- en: We will now turn to techniques that rely less on the rules of the language and
    more on machine learning with existing data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向那些不那么依赖语言规则，而更多依赖机器学习与现有数据的技术。
- en: Traditional machine learning approaches
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统的机器学习方法
- en: 'While rule-based approaches provide very fine-grained and specific information
    about language, there are some drawbacks to these approaches, which has motivated
    the development of alternatives. There are two major drawbacks:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于规则的方法提供了非常精细和具体的语言信息，但这些方法也存在一些缺点，这促使了替代方法的发展。主要有两个缺点：
- en: Developing the rules used in rule-based approaches can be a laborious process.
    Rule development can either be done by experts directly writing rules based on
    their knowledge of the language or, more commonly, the rules can be derived from
    examples of text that have been annotated with a correct analysis. Both of these
    approaches can be expensive and time-consuming.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发用于基于规则的方法的规则可能是一个繁琐的过程。规则开发可以由专家直接编写规则，基于他们对语言的理解，或者更常见的是，规则可以从标注了正确分析的文本示例中推导出来。这两种方法都可能是昂贵且耗时的。
- en: Rules are not likely to be universally applicable to every text that the system
    encounters. The experts who developed the rules might have overlooked some cases,
    the annotated data might not have examples of every case, and speakers can make
    errors such as false starts, which need to be analyzed although they aren’t covered
    by any rule. Written language can include spelling errors, which results in words
    that aren’t in the lexicon. Finally, the language itself can change, resulting
    in new words and new phrases that aren’t covered by existing rules.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则不太可能普遍适用于系统遇到的每个文本。制定这些规则的专家可能忽视了一些情况，标注的数据可能没有覆盖每种情况的示例，且说话者可能会犯错，如错误的开始，尽管这些错误没有被任何规则覆盖，但仍需分析。书面语言可能包含拼写错误，从而导致一些词汇不在词汇表中。最后，语言本身可能会发生变化，产生新的单词和短语，而这些新词和新短语并不在现有规则中涵盖。
- en: For these reasons, rule-based approaches are primarily used as part of NLU pipelines,
    supplementing other techniques.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，基于规则的方法主要作为自然语言理解（NLU）管道的一部分使用，补充其他技术。
- en: 'Traditional machine learning approaches were motivated by problems in classification,
    where documents that are similar in meaning can be grouped. Two problems have
    to be solved in classification:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习方法受到分类问题的驱动，在这些问题中，相似意思的文档可以被分组。分类问题中需要解决两个问题：
- en: Representing the documents in a training set in such a way that documents in
    the same categories have similar representations
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以某种方式表示训练集中的文档，使得同一类别中的文档具有相似的表示
- en: Deciding how new, previously unseen documents should be classified based on
    their similarity to the documents in the training set
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定如何根据与训练集中文档的相似度对新的、先前未见过的文档进行分类
- en: Representing documents
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示文档
- en: Representations of documents are based on words. A very simple approach is to
    assume that the document should be represented simply as the set of words that
    it contains. This is called the **bag of words** (**BoW**) approach. The simplest
    way of using a BoW for document representation is to make a list of all the words
    in the corpus, and for each document and each word, state whether that word occurs
    in that document.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的表示是基于单词的。一种非常简单的方法是假设文档应该仅仅通过它包含的单词集合来表示。这种方法被称为**词袋模型**（**BoW**）。使用BoW进行文档表示的最简单方法是列出语料库中的所有单词，然后对于每个文档和每个单词，指出该单词是否出现在该文档中。
- en: 'For example, suppose we have a corpus consisting of the three documents in
    *Figure 3**.4*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个由三个文档组成的语料库，如*图3.4*所示：
- en: '![Figure 3.4 – A small corpus of restaurant queries](img/B19005_03_04New.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 小型餐厅查询语料库](img/B19005_03_04New.jpg)'
- en: Figure 3.4 – A small corpus of restaurant queries
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 小型餐厅查询语料库
- en: 'The entire vocabulary for this toy corpus is 29 words, so each document is
    associated with a list 29 items long that states, for each word, whether or not
    it appears in the document. The list represents *occurs* as `1` and *does not
    occur* as `0`, as shown in *Table 3.2*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个玩具语料库的整个词汇表包含29个词，因此每个文档都与一个长度为29的列表相关联，列表中指出了每个单词是否出现在该文档中。这个列表将*出现*表示为`1`，将*未出现*表示为`0`，如*表3.2*所示：
- en: '|  | **a** | **an** | **any** | **are** | **aren’t** | **away** | **Chinese**
    | **Eastern** | **…** |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | **a** | **an** | **any** | **are** | **aren’t** | **away** | **Chinese**
    | **Eastern** | **…** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | … |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | … |'
- en: '| 2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | … |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | … |'
- en: '| 3 | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 1 | … |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 1 | … |'
- en: Table 3.2 – BoW for a small corpus
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 – 小语料库的词袋模型（BoW）
- en: '*Table 3.2* shows the BoW lists for the first eight words in the vocabulary
    for these three documents. Each row in *Table 3.2* represents one document. For
    example, the word “*a*” occurs once in the first document, but the word “*an*”
    does not occur, so its entry is *0*. This representation is mathematically a *vector*.
    Vectors are a powerful tool in NLU, and we will discuss them later in much more
    detail. The BoW representation might seem very simplistic (for example, it doesn’t
    take into account any information about the order of words). However, other variations
    on this concept are more powerful, which will be discussed later on in *Chapters
    9* to *12*.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.2* 显示了这三份文档中词汇表前八个单词的词袋（BoW）列表。*表3.2*中的每一行代表一个文档。例如，单词“*a*”在第一个文档中出现一次，但单词“*an*”没有出现，因此其条目是*0*。这种表示在数学上是一个*向量*。向量是NLU中的一种强大工具，我们将在后面详细讨论它们。BoW表示法可能看起来非常简单（例如，它没有考虑单词的顺序信息）。然而，这一概念的其他变种更为强大，稍后将在*第9章*至*第12章*中讨论。'
- en: Classification
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: The assumption behind the BoW approach is that the more words that two documents
    have in common, the more similar they are in meaning. This is not a hard-and-fast
    rule, but it turns out to be useful in practice.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: BoW方法背后的假设是，两个文档中共享的单词越多，它们的意义就越相似。这不是一条硬性规则，但事实证明，在实践中非常有用。
- en: For many applications, we want to group documents that are similar in meaning
    into different categories. This is the process of **classification**. If we want
    to classify a new document into one of these categories, we need to find out how
    similar its vector is to the vectors of the other documents in each category.
    For example, the sentiment analysis task discussed in [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016)
    is the task of classifying documents into one of two categories – positive or
    negative sentiment regarding the topic of the text.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用，我们希望将含义相似的文档分组到不同的类别中。这就是**分类**的过程。如果我们想将一个新文档分类到这些类别中的某一类，我们需要找出它的向量与每个类别中其他文档的向量的相似度。例如，在[*第1章*](B19005_01.xhtml#_idTextAnchor016)中讨论的情感分析任务，就是将文档分类为两类——关于文本主题的积极或消极情感。
- en: Many algorithms have been used for the classification of text documents. Naïve
    Bayes and **support vector machines** (**SVMs**), to be discussed in [*Chapter
    9*](B19005_09.xhtml#_idTextAnchor173), are two of the most popular. Neural networks,
    especially **recurrent neural networks** (**RNNs**), are also popular. Neural
    networks are briefly discussed in the next section and will be discussed in detail
    in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 已经使用了许多算法来对文本文件进行分类。朴素贝叶斯和**支持向量机**（**SVMs**），将在[*第9章*](B19005_09.xhtml#_idTextAnchor173)中讨论，是其中最受欢迎的两种。神经网络，特别是**递归神经网络**（**RNNs**），也很受欢迎。神经网络将在下一节简要讨论，并将在[*第10章*](B19005_10.xhtml#_idTextAnchor184)中详细讨论。
- en: In this section, we’ve summarized some approaches that are used in traditional
    machine learning. Now, we will turn our attention to the new approaches based
    on deep learning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了一些在传统机器学习中使用的方法。现在，我们将把注意力转向基于深度学习的新方法。
- en: Deep learning approaches
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法
- en: Neural networks, and especially the large neural networks generally referred
    to as **deep learning**, have become very popular for NLU in the past few years
    because they significantly improve the accuracy of earlier methods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，特别是通常称为**深度学习**的大型神经网络，近年来在自然语言理解（NLU）中变得非常流行，因为它们显著提高了早期方法的准确性。
- en: The basic concept behind neural networks is that they consist of layers of connected
    units, called **neurons** in analogy to the neurons in animal nervous systems.
    Each neuron in a neural net is connected to other neurons in the neural net. If
    a neuron receives the appropriate inputs from other neurons, it will fire, or
    send input to another neuron, which will in turn fire or not fire depending on
    other inputs that it receives. During the training process, weights on the neurons
    are adjusted to maximize classification accuracy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本概念是，它们由多个连接单元的层组成，这些单元被称为**神经元**，类似于动物神经系统中的神经元。神经网络中的每个神经元都与其他神经元相连接。如果一个神经元从其他神经元接收到适当的输入，它将被激活，或者将输入发送到另一个神经元，而后者则会根据接收到的其他输入决定是否激活。在训练过程中，会调整神经元上的权重，以最大化分类准确性。
- en: '*Figure 3**.5* shows an example of a four-layer neural net performing a sentiment
    analysis task. The neurons are circles connected by lines. The first layer, on
    the left, receives a text input. Two hidden layers of neurons then process the
    input, and the result (*positive*) is produced by the single neuron in the final
    output layer:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.5* 显示了一个四层神经网络执行情感分析任务的示例。神经元是由线连接的圆圈。最左边的第一层接收文本输入，接着两个隐藏层的神经元处理该输入，最终的输出层中的单个神经元给出结果（*正面*）：'
- en: '![Figure 3.5 – A four-layer neural network trained to perform sentiment analysis
    on product reviews](img/B19005_03_05.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 一个四层神经网络，用于对产品评论进行情感分析](img/B19005_03_05.jpg)'
- en: Figure 3.5 – A four-layer neural network trained to perform sentiment analysis
    on product reviews
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 一个四层神经网络，用于对产品评论进行情感分析
- en: Although the concepts behind neural networks have existed for many years, the
    implementation of neural networks large enough to perform significant tasks has
    only been possible within the last few years because of the limitations of earlier
    computing resources. Their current popularity is largely due to the fact that
    they are often more accurate than earlier approaches, especially given sufficient
    training data. However, the process of training a neural net for large-scale tasks
    can be complex and time-consuming and can require the services of expert data
    scientists. In some cases, the additional accuracy that a neural net provides
    is not enough to justify the additional expense of developing the system.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络背后的概念已经存在多年，但能够执行重大任务的大型神经网络的实现直到最近几年才得以实现，这得益于之前计算资源的限制。它们目前的流行性主要是因为它们通常比早期的方法更准确，特别是在有足够的训练数据的情况下。然而，训练一个神经网络以执行大规模任务的过程可能非常复杂且耗时，并且可能需要专家数据科学家的帮助。在某些情况下，神经网络所提供的额外准确性不足以证明开发该系统的额外成本是合理的。
- en: Deep learning and neural networks will be discussed in detail in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和神经网络将在[*第10章*](B19005_10.xhtml#_idTextAnchor184)中详细讨论。
- en: Pre-trained models
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练模型
- en: The most recent approach to NLU is based on the idea that much of the information
    required to understand natural language can be made available to many different
    applications by processing generic text (such as internet text) to create a baseline
    model for the language. Some of these models are very large and are based on tremendous
    amounts of data. To apply these models to a specific application, the generic
    model is adapted to the application through the use of application-specific training
    data, through a process called **fine-tuning**. Because the baseline model already
    contains a vast amount of general information about the language, the amount of
    training data can be considerably less than the training data required for some
    of the traditional approaches. These popular technologies include BERT and its
    many variations and **Generative Pre-trained Transformers** (**GPTs**) and their
    variations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的自然语言理解方法基于这样的理念：理解自然语言所需的许多信息，可以通过处理通用文本（如互联网文本）来为多个不同的应用提供，从而创建一个语言的基准模型。这些模型中的一些非常庞大，基于大量的数据。为了将这些模型应用于特定的应用，通用模型通过使用特定应用的训练数据进行适应，这一过程被称为**微调**。由于基准模型已经包含了大量关于语言的通用信息，因此所需的训练数据量通常比某些传统方法所需的训练数据少得多。这些流行的技术包括BERT及其许多变种，以及**生成预训练变换器**（**GPTs**）及其变种。
- en: Pre-trained models will be discussed in detail in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型将在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中详细讨论。
- en: Considerations for selecting technologies
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择技术时的考虑因素
- en: 'This chapter has introduced four classes of NLU technologies:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了四类自然语言理解技术：
- en: Rule-based
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于规则的
- en: Statistical machine learning
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计机器学习
- en: Deep learning and neural networks
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习和神经网络
- en: Pre-trained models
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型
- en: How should we decide which technology or technologies should be employed to
    solve a specific problem? The considerations are largely practical and have to
    do with the costs and effort required to create a working solution. Let’s look
    at the characteristics of each approach.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何决定应该采用哪种技术或技术组合来解决特定问题呢？这些考虑因素主要是实际性的，涉及到创建可行解决方案所需的成本和努力。让我们来看一下每种方法的特点。
- en: '*Table 3.3* lists the four approaches to NLU that we’ve reviewed in this chapter
    and how they compare with respect to developer expertise, the amount of data required,
    the training time, accuracy, and cost. As *Table 3.3* shows, every approach has
    advantages and disadvantages. For small or simple problems that don’t require
    large amounts of data, the rule-based, deep learning, or pre-trained approaches
    should be strongly considered, at least for part of the pipeline. While pre-trained
    models are accurate and have relatively low development costs, developers may
    prefer to avoid the costs of cloud services or the costs of managing large models
    on their local computer resources:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 3.3* 列出了我们在本章中回顾的四种自然语言理解（NLU）方法，并对它们在开发者专业知识、所需数据量、训练时间、准确性和成本方面进行了比较。如
    *表 3.3* 所示，每种方法都有优点和缺点。对于那些不需要大量数据的小型或简单问题，基于规则的、深度学习或预训练方法应该被强烈考虑，至少可以作为管道的一部分。虽然预训练模型准确性高，且开发成本相对较低，但开发者可能更倾向于避免使用云服务或管理本地计算资源上的大型模型的成本。'
- en: '|  | **Developer** **expertise** | **Amount of** **data required** | **Training
    time** | **Accuracy** | **Cost** |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | **开发者** **专业知识** | **所需数据量** | **训练时间** | **准确性** | **成本** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Rule-based | High (linguists or domain experts) | Small amount of domain-specific
    data | Large amount of time for experts to write rules | High if rules are accurate
    | Rule development may be costly; computer time costs are low |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 基于规则的 | 高（语言学家或领域专家） | 少量领域特定数据 | 专家写规则需要大量时间 | 如果规则准确，成本高 | 规则开发可能成本较高；计算机时间成本较低
    |'
- en: '| Statistical | Medium – use standard tools; some NLP/data science expertise
    required | Medium amount of domain-specific data | Large amount of time for annotation
    | Medium | Data annotation may be costly; computer time costs are low |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 统计方法 | 中等 - 使用标准工具；需要一些NLP/数据科学专业知识 | 中等量的领域特定数据 | 大量的标注时间 | 中等 | 数据标注可能成本较高；计算机时间成本较低
    |'
- en: '| Deep learning | High (data scientists) | Large amount of domain-specific
    data | Large amount of time for annotation; additional computer time for training
    models | Medium-high | Charges for some cloud services or local computer resources
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | 高（数据科学家） | 大量领域特定数据 | 大量标注时间；训练模型需要额外的计算时间 | 中等偏高 | 一些云服务或本地计算资源的费用
    |'
- en: '| Pre-trained | Medium – use standard tools with some data science expertise
    | Small amount of domain-specific data | Medium amount of time to label data for
    fine-tuning models | High | Charges for some cloud services or local computer
    resources |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 预训练模型 | 中等 - 使用标准工具，具备一定的数据科学专业知识 | 少量领域特定数据 | 中等量的时间标注数据以微调模型 | 高 | 一些云服务或本地计算资源的费用
    |'
- en: Table 3.3 – Comparison between general approaches to NLU
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.3 – NLU一般方法的比较
- en: The most important considerations are the problem that’s being addressed and
    what the acceptable costs are. It should also be kept in mind that choosing one
    or another technology isn’t a permanent commitment, especially for approaches
    that rely on annotated data, which can be used for more than one approach.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的考虑因素是所要解决的问题以及可接受的成本。还应记住，选择某种技术并不是一个永久性的承诺，特别是对于那些依赖标注数据的方法，这些数据可以被用于多种方法。
- en: Summary
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we surveyed the various techniques that can be used in NLU
    applications and learned several important skills.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了可以在NLU应用中使用的各种技术，并学到了几个重要的技能。
- en: We learned about what rule-based approaches are and the major rule-based techniques,
    including topics such as POS tagging and parsing. We then learned about the important
    traditional machine learning techniques, especially the ways that text documents
    can be represented numerically. Next, we focused on the benefits and drawbacks
    of the more modern deep learning techniques and the advantages of pre-trained
    models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了基于规则的方法是什么，以及主要的基于规则的技术，包括词性标注和句法分析等主题。接着，我们学习了重要的传统机器学习技术，特别是文本文件如何进行数字化表示的方法。然后，我们关注了现代深度学习技术的优缺点以及预训练模型的优势。
- en: In the next chapter, we will review the basics of getting started with NLU –
    installing Python, using Jupyter Labs and GitHub, using NLU libraries such as
    NLTK and spaCy, and how to choose between libraries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾开始进行自然语言理解（NLU）所需的基础知识——安装Python，使用Jupyter Labs和GitHub，使用NLU库（如NLTK和spaCy），以及如何在这些库之间进行选择。
