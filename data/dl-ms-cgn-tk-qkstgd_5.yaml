- en: Working with Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理图像
- en: In this chapter, we're going to explore some more deep learning models with
    CNTK. Specifically, we're going to look at using neural networks for classifying
    image data. All that you've learned in the past chapters will come back in this
    chapter as we discuss how to train convolutional neural networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些使用 CNTK 的深度学习模型。具体来说，我们将重点研究使用神经网络进行图像数据分类。你在前几章中学到的所有内容将在本章中派上用场，因为我们将讨论如何训练卷积神经网络。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Convolutional neural network architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络架构
- en: How to build a convolutional neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建卷积神经网络
- en: How to feed image data into a convolutional network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将图像数据输入卷积网络
- en: How to improve network performance with data augmentation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过数据增强提高网络性能
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We assume you have a recent version of Anaconda installed on your computer
    and have followed the steps in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, to install CNTK on your computer. The sample code
    for this chapter can be found in our GitHub repository at: [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你已经在计算机上安装了最新版本的 Anaconda，并按照[第 1 章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)《*使用
    CNTK 入门*》中的步骤安装了 CNTK。本章的示例代码可以在我们的 GitHub 仓库找到：[https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5)。
- en: 'In this chapter, we''ll work on an example stored in a Jupyter notebook. To
    access the sample code, run the following commands inside an Anaconda prompt in
    the directory where you''ve downloaded the code:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用存储在 Jupyter notebook 中的一个示例。要访问示例代码，请在下载代码的目录中，在 Anaconda 提示符下运行以下命令：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We'll mention relevant notebooks in each of the sections so you can follow along
    and try out the different techniques yourself.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每一部分提到相关的 notebook，以便你可以跟着进行并亲自尝试不同的技术。
- en: The dataset for this chapter is not available in the GitHub repository. It would
    be too big to store there. Please open the `Prepare the dataset.ipynb` notebook
    and follow the instructions there to obtain the data for this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的数据集在 GitHub 仓库中无法获取，因为它太大，无法存储在那里。请打开 `Prepare the dataset.ipynb` notebook，并按照其中的说明获取本章的数据。
- en: 'Check out the following video to see the code in action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码的实际效果：
- en: '[http://bit.ly/2Wm6U49](http://bit.ly/2Wm6U49)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2Wm6U49](http://bit.ly/2Wm6U49)'
- en: Convolutional neural network architecture
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络架构
- en: In previous chapters, we've learned how to use regular feed-forward network
    architectures to build neural networks. In a feed-forward neural network, we assume
    that there are interactions between the different input features. But we don't
    make any assumptions about the nature of these interactions. This is, however,
    not always the right thing to do.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何使用常规的前馈网络架构来构建神经网络。在前馈神经网络中，我们假设不同输入特征之间存在相互作用，但我们并未对这些相互作用的性质做出假设。然而，这并不总是正确的做法。
- en: When you work with complex data such as images, a feed-forward neural network
    won't do a very good job. This comes from the fact that we assume that there's
    an interaction between the inputs of our network. But we don't account for the
    fact that they are organized in a spatial way. When you look at the pixels in
    an image, there's a horizontal and vertical relationship between them. There's
    also a relationship between the colors in an image and the position of certain
    colored pixels in that image.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理复杂数据（如图像）时，前馈神经网络并不能很好地完成任务。这是因为我们假设网络输入之间存在相互作用，但我们没有考虑到它们是空间上组织的这一事实。当你观察图像中的像素时，它们之间存在水平和垂直关系。图像中的颜色与某些颜色像素的位置之间也有关系。
- en: A convolutional network is a special kind of neural network that makes the explicit
    assumption that we're dealing with data that has a spatial relationship to it.
    This makes it really good at recognizing images. But other spatially organized
    data will work too. Let's explore the architecture of a convolutional neural network
    used for image classification tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络是一种特殊的神经网络，它明确假设我们处理的数据具有空间关系。这使得它在图像识别方面非常有效。不过，其他具有空间组织的数据也同样适用。让我们来探索一种用于图像分类任务的卷积神经网络的架构。
- en: Network architecture used for image classification
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于图像分类的网络架构
- en: 'Convolutional networks used for image classification typically contain one
    or more convolution layers followed by pooling layers, and usually end in regular
    fully-connected layers to provide the final output as shown in the following screenshot:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像分类的卷积网络通常包含一个或多个卷积层，后跟池化层，并且通常以常规的全连接层结束，以提供最终的输出，如下图所示：
- en: '![](img/3bf0afaf-b58a-413f-ab2f-4caa7b6c68ee.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bf0afaf-b58a-413f-ab2f-4caa7b6c68ee.png)'
- en: 'This image is from: https://en.wikipedia.org/wiki/File:Typical_cnn.png'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此图片来源于：https://en.wikipedia.org/wiki/File:Typical_cnn.png
- en: When you take a closer look at the structure of a convolutional network, you'll
    see that it starts with a set of **convolution** and **pooling** layers. You can
    consider this part a complex, trainable photo filter. The **convolution layers**
    filter out interesting details that are needed to classify the image, and the
    **pooling** **layers** summarize these features so that there are fewer data points
    to crunch near the end of the network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你仔细观察卷积网络的结构时，你会发现它以一组**卷积**层和**池化**层开始。你可以将这一部分看作是一个复杂的、可训练的照片滤镜。**卷积层**过滤出分类图像所需的有趣细节，而**池化**层则总结这些特征，以便网络后端处理的数据点更少。
- en: Usually, you will find several sets of convolution layers and pooling layers
    in a neural network for image classification. This is done to be able to extract
    more complex details from the image. The first layer of the network extracts simple
    details, such as lines, from the image. The next set of layers then combines the
    output of the previous set of layers to learn more complex features, such as corners
    or curves. As you can imagine, the layers after that are used to learn increasingly
    more complex features.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在用于图像分类的神经网络中，你会发现多个卷积层和池化层的组合。这样做是为了能够从图像中提取更多复杂的细节。网络的第一层提取简单的细节，比如线条。接下来的层会将前一层的输出结合起来，学习更复杂的特征，比如角落或曲线。正如你所想的那样，后续的层会用来学习越来越复杂的特征。
- en: Often, when you build a neural network, you want to classify what's in the image.
    This is where the classic dense layers play an important role. Typically, a model
    used for image recognition will end in one output layer and one or more dense
    layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你构建神经网络时，你希望对图像中的内容进行分类。这时，经典的全连接层就发挥了重要作用。通常，用于图像识别的模型会以一个输出层和一个或多个全连接层结束。
- en: Let's take a look at how to work with convolutional and pooling layers to create
    a convolutional neural network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用卷积层和池化层来创建一个卷积神经网络。
- en: Working with convolution layers
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积层
- en: 'Now that you''ve seen what a convolutional network looks like, let''s look
    at the convolution layer that is used in the convolutional network:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你已经了解了卷积网络的基本结构，让我们来看看卷积网络中使用的卷积层：
- en: '![](img/ad85d578-6750-4eaf-8626-068c79382417.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad85d578-6750-4eaf-8626-068c79382417.png)'
- en: 'This image is from: https://en.wikipedia.org/wiki/File:Conv_layer.png'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此图片来源于：https://en.wikipedia.org/wiki/File:Conv_layer.png
- en: The convolution layer is the core building block of the convolutional network.
    You can consider the convolution layer a trainable filter that you can use to
    extract important details from the input and remove data that is considered noise.
    A convolution layer contains a set of weights that cover a small area (width and
    height) but cover all channels of the input given to the layer. When you create
    a convolution layer, you need to specify its depth in neurons. You'll find that
    most frameworks, including CNTK, talk about filters when talking about the depth
    of the layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是卷积网络的核心构建块。你可以将卷积层看作一个可训练的滤镜，用来从输入中提取重要细节并去除被认为是噪声的数据。一个卷积层包含一组权重，这些权重覆盖一个小区域（宽度和高度），但涵盖了输入的所有通道。当你创建卷积层时，你需要指定其神经元的深度。你会发现，大多数框架，包括CNTK，在讨论层的深度时，会提到滤镜。
- en: When we perform a forward pass, we slide the filters of the layer across the
    input and perform a dot product operation between the input and the weights for
    each of the filters. The sliding motion is controlled by a stride setting. When
    you specify a stride of 1, you will end up with an output matrix that has the
    same width and height as the input, but with the same depth as the number of filters
    in the layer. You can set a different stride, which will reduce the width and
    height of the output matrix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行前向传播时，我们会将该层的卷积核滑动过输入数据，并在每个卷积核上执行输入数据与权重之间的点积运算。滑动的过程由步幅设置来控制。当你指定步幅为
    1 时，最终得到的输出矩阵将具有与输入相同的宽度和高度，但深度与该层的卷积核数量相同。你可以设置不同的步幅，这会减少输出矩阵的宽度和高度。
- en: In addition to the input size and number of filters, you can configure the padding
    of the layer. Adding padding to a convolution layer will add a border of zeros
    around the processed input data. This may sound like a weird thing to do but can
    come in quite handy in some situations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了输入大小和卷积核数量外，您还可以配置该层的填充。给卷积层添加填充会在处理过的输入数据周围添加零的边框。虽然这听起来像是一件奇怪的事，但在某些情况下，它是非常有用的。
- en: 'When you look at the output size of a convolution layer, it will be determined
    based on the input size, the number of filters, the stride, and padding. The formula
    looks like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看卷积层的输出大小时，它将基于输入大小、卷积核数量、步幅和填充来决定。公式如下：
- en: '![](img/29379922-0057-4588-808c-60406285ff02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29379922-0057-4588-808c-60406285ff02.png)'
- en: '*W* is the input size, *F* is the number of filters or depth of the layer,
    *P* the padding, and *S* the stride.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*W* 是输入大小，*F* 是卷积核数量或层深，*P* 是填充，*S* 是步幅。'
- en: Not all combinations of input size, filters, stride, and padding are valid,
    though. For example, when you have an input size *W* = 10 and a layer depth *F*
    = 3 and a stride *S* = 2, then you'll end up with an output volume of 5.5\. Not
    all inputs will perfectly map to this output size, so CNTK will raise an exception.
    This is where the padding setting comes in. By specifying padding, we can make
    sure that all inputs are mapped to output neurons.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有输入大小、卷积核、步幅和填充的组合都是有效的。例如，当输入大小 *W* = 10，层深 *F* = 3，步幅 *S* = 2 时，最终得到的输出体积是
    5.5。并不是所有输入都能完美地映射到这个输出大小，因此 CNTK 会抛出一个异常。这就是填充设置的作用。通过指定填充，我们可以确保所有输入都映射到输出神经元。
- en: 'The settings for input size and filters we''ve just discussed may feel a little
    abstract, but there''s sense to them. Setting a larger input size will cause the
    layer to capture coarser patterns from the input. Setting a smaller input size
    will make the layer better at detecting finer patterns. The depth or number of
    filters controls how many different patterns can be detected in the input image.
    At a high level, you could say that a convolutional filter with one filter detects
    one pattern; for example, horizontal lines. A layer with two filters can detect
    two different patterns: horizontal and vertical lines.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才讨论的输入大小和卷积核数量可能感觉有些抽象，但它们是有道理的。设置较大的输入大小会使得该层捕捉到输入数据中的较粗糙的模式。设置较小的输入大小则使得该层能够更好地检测更精细的模式。卷积核的深度或数量决定了能够检测到多少种不同的模式。在高级别上，可以说一个卷积核使用一个滤波器来检测一种模式；例如，水平线。而一个拥有两个滤波器的层则能够检测两种不同的模式：水平线和垂直线。
- en: Coming up with the right settings for a convolutional network can be quite a
    bit of work. Luckily, CNTK has settings that help make this process a little less
    complex.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为卷积网络设定正确的参数可能需要不少工作。幸运的是，CNTK 提供了一些设置，帮助简化这个过程。
- en: Training a convolutional layer is done in the same way as a regular dense layer.
    This means that we'll perform a forward pass, calculate the gradients, and use
    the learner to come up with better values for the parameters in a backward pass.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练卷积层的方式与训练常规的密集层相同。这意味着我们将执行一次前向传播，计算梯度，并使用学习器在反向传播时更新参数的值。
- en: Convolution layers are often followed by a pooling layer to compress the features
    learned by the convolutional layer. Let's look at pooling layers next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层后面通常会跟着一个池化层，用于压缩卷积层学习到的特征。接下来我们来看池化层。
- en: Working with pooling layers
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理池化层
- en: In the previous section we've looked at convolutional layers and how they can
    be used to extract details from pixel data. Pooling layers are used to summarize
    the extracted details. Pooling layers help reduce the volume of data so that it
    becomes easier to classify this data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们讨论了卷积层以及它们如何用于从像素数据中提取细节。池化层用于总结这些提取的细节。池化层有助于减少数据量，使得分类这些数据变得更加容易。
- en: 'It''s important to understand that neural networks have a harder time to classify
    a sample when the sample has a lot of different input features. That''s why we
    use a combination of convolution layers and pooling layers to extract details
    and summarize them:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 理解神经网络在面对具有大量不同输入特征的样本时，分类难度较大的问题非常重要。这就是为什么我们使用卷积层和池化层的组合来提取细节并进行总结的原因：
- en: '![](img/14a89752-593c-4dcb-a9ad-ecbc785bfe33.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14a89752-593c-4dcb-a9ad-ecbc785bfe33.png)'
- en: 'This image is from: https://en.wikipedia.org/wiki/File:Max_pooling.png'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此图像来自： https://en.wikipedia.org/wiki/File:Max_pooling.png
- en: A pooling layer features a downsampling algorithm that you configure with an
    input size and stride. We will feed the output of each filter in the previous
    convolution layer into the pooling layer. The pooling layer moves across the slices
    of data and takes small windows equal to the configured input size. It takes the
    small areas of values and grabs the highest value from them as the output for
    that area. Just like with the convolution layer, it uses the stride to control
    how fast it moves across the input. For example, a size of 1 combined with a stride
    of 2 will reduce the data dimensions by half. By using only the highest input
    value, it discards 75% of the input data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层具有一个下采样算法，你可以通过输入大小和步幅来进行配置。我们将把前一卷积层的每个滤波器的输出传递到池化层。池化层会跨越数据片段，并提取与配置的输入大小相等的小窗口。它从这些小区域的值中选取最大的值作为该区域的输出。就像卷积层一样，它使用步幅来控制它在输入中移动的速度。例如，步幅为2，大小为1时，会将数据的维度减半。通过仅使用最高的输入值，它丢弃了75%的输入数据。
- en: Max-sampling, as this pooling technique is called, is not the only way in which
    a pooling layer can reduce the dimensionality of the input data. You can also
    use average-pooling. In this case, the average value of the input area is used
    as output in the pooling layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种池化技术被称为最大池化，它并不是池化层减少输入数据维度的唯一方式。你还可以使用平均池化。在这种情况下，池化层会使用输入区域的平均值作为输出。
- en: Note that pooling layers only reduce the size of the input along the width and
    height. The depth remains the same as before, so you can rest assured that features
    are only downsampled and not removed completely.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，池化层仅减少输入在宽度和高度方向上的大小。深度保持不变，因此你可以放心，特征只是被下采样，并没有完全丢弃。
- en: Since pooling layers have a fixed algorithm to downsample input data, there
    are no trainable parameters in them. This means that it takes no time to train
    pooling layers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于池化层使用固定算法来下采样输入数据，因此它们没有可训练的参数。这意味着训练池化层几乎不需要时间。
- en: Other uses for convolutional networks
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积网络的其他用途
- en: 'We''re focusing our efforts on using convolutional networks for image classification,
    but you can use this kind of neural network for many more scenarios, for example:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点放在使用卷积网络进行图像分类上，但你也可以将这种神经网络应用于更多的场景，例如：
- en: 'Object detection in images. The CNTK website includes a nice example that shows
    how to build an object detection model: [https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN](https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中的目标检测。CNTK 网站提供了一个很好的示例，展示了如何构建一个目标检测模型：[https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN](https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN)
- en: Detect faces in photos and predict the age of the person in the photo
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在照片中检测面部并预测照片中人的年龄
- en: Caption images using a combination of convolutional and recurrent neural networks
    that we discuss in [Chapter 6](a5da9ef2-399a-4c30-b751-318d64939369.xhtml), *Working
    with Time Series Data*
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络和递归神经网络的结合，按照[第六章](a5da9ef2-399a-4c30-b751-318d64939369.xhtml)的内容进行图像标题生成，*处理时间序列数据*
- en: Predict the distance to the bottom of a lake from sonar images
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测来自声纳图像的湖底距离
- en: When you start to combine convolutional networks for different tasks, you can
    build some pretty powerful applications; for example, a security camera that detects
    people in the videostream and warns the security guard about trespassers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始将卷积网络组合用于不同的任务时，你可以构建一些非常强大的应用；例如，一个安全摄像头，能够检测视频流中的人物，并警告保安有非法入侵者。
- en: Countries such as China are investing heavily in this kind of technology. Convolutional
    networks are used in smart city applications to monitor crossings. Using a deep
    learning model, the authorities can detect accidents at traffic lights and reroute
    the traffic automatically so that the police have an easier job.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 中国等国家正在大力投资这种技术。卷积网络被应用于智能城市项目中，用于监控过路口。通过深度学习模型，相关部门能够检测交通信号灯的事故，并自动重新规划交通路线，从而使警察的工作变得更加轻松。
- en: Building convolutional networks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建卷积网络
- en: Now that you've seen the basics behind convolutional networks and some common
    use cases for them, let's take a look at how to build one with CNTK.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了卷积网络的基本原理以及一些常见的应用场景，让我们来看看如何使用CNTK来构建一个卷积网络。
- en: We're going to build a model that can recognize handwritten digits in images.
    There's a free dataset available called the MNIST dataset that contains 60,000
    samples of handwritten digits. There's also a test set available with 10,000 samples
    for the MNIST dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个能够识别手写数字的模型。MNIST数据集是一个免费的数据集，包含了60,000个手写数字样本。还有一个包含10,000个样本的测试集。
- en: Let's get started and see what building a convolutional network looks like in
    CNTK. First, we'll look at how to put together the structure of the convolutional
    neural network, we then will take a look at how to train the parameters of a convolutional
    neural network. Finally, we'll explore how to improve the neural network by changing
    it's structure with different layer setups.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始并看看在CNTK中构建卷积网络是什么样子。首先，我们将了解如何构建卷积神经网络的结构，然后我们会了解如何训练卷积神经网络的参数。最后，我们将探讨如何通过更改网络结构和不同层设置来改进神经网络。
- en: Building the network structure
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络结构
- en: Typically, when you build a neural network for recognizing patterns in images,
    you will use a combination of convolution and pooling layers. The end of the network
    should contain one or more hidden layers, ending with a softmax layer for classification
    purposes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你构建一个用于识别图像中模式的神经网络时，你会使用卷积层和池化层的组合。网络的最后应该包含一个或多个隐藏层，并以softmax层结束，用于分类目的。
- en: 'Let''s build the network structure:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来构建网络结构：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Follow the given steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤进行操作：
- en: First, import the required layers for the neural network.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入神经网络所需的层。
- en: Then, import the activation functions for the network.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，导入网络的激活函数。
- en: Next, import the `glorot_uniform initializer` function to initialize the convolutional
    layers later.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入`glorot_uniform initializer`函数，以便稍后初始化卷积层。
- en: After that, import the `input_variable` function to create input variables and
    the `default_options` function to make configuration of the neural network a little
    easier.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，导入`input_variable`函数来创建输入变量，以及`default_options`函数，使得神经网络的配置更加简便。
- en: Create a new `input_variable` store the input images, they will contain `3`
    channels (red, green, and blue) and have a size of `28` by `28` pixels.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`input_variable`来存储输入图像，这些图像将包含`3`个通道（红、绿、蓝），尺寸为`28` x `28` 像素。
- en: Create another `input_variable` to store the labels to predict.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个`input_variable`来存储待预测的标签。
- en: Next, create the `default_options` for the network and use the `glorot_uniform`
    as the initialization function.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建网络的`default_options`并使用`glorot_uniform`作为初始化函数。
- en: Then, create a new `Sequential` layer set to structure the neural network
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的`Sequential`层集来构建神经网络的结构
- en: Within the `Sequential` layer set, add a `Convolutional2D` layer with a `filter_shape`
    of `5` and a `strides` setting of `1` and set the number of filters to `8`. Enable
    `padding` so the image is padded to retain the original dimensions.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Sequential`层集中，添加一个`Convolutional2D`层，`filter_shape`为`5`，`strides`设置为`1`，并将过滤器数量设置为`8`。启用`padding`，以便填充图像以保留原始尺寸。
- en: Add a `MaxPooling` layer with a `filter_shape` of `2` and a `strides` setting
    of `2` to compress the image by half.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个`MaxPooling`层，`filter_shape`为`2`，`strides`设置为`2`，以将图像压缩一半。
- en: Add another `Convolution2D` layer with a `filter_shape` of 5 and a `strides`
    setting of 1, use 16 filters. Add `padding` to retain the size of the image produced
    by the previous pooling layer.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个`Convolution2D`层，`filter_shape`为5，`strides`设置为1，使用16个过滤器。添加`padding`以保持由前一池化层产生的图像尺寸。
- en: Next, add another `MaxPooling` layer with a `filter_shape` of 3 and a `strides`
    setting of 3 to reduce the image to a third.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，添加另一个`MaxPooling`层，`filter_shape`为3，`strides`设置为3，将图像尺寸缩小到原来的三分之一。
- en: Finally, add a `Dense` layer with 10 neurons for the 10 possible classes the
    network can predict. Use a `log_softmax` activation function to turn the network
    into a classification model.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，添加一个`Dense`层，包含10个神经元，用于网络可以预测的10个类别。使用`log_softmax`激活函数，将网络转换为分类模型。
- en: We're using images of 28x28 pixels as the input for the model. This size is
    fixed, so when you want to make a prediction with this model, you need to provide
    the same size images as input.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用28x28像素的图像作为模型的输入。这个尺寸是固定的，因此当你想用这个模型进行预测时，你需要提供相同尺寸的图像作为输入。
- en: Note that this model is still very basic and will not produce perfect results,
    but it is a good start. Later on, we can start to tune it should we need to.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个模型仍然非常基础，不会产生完美的结果，但它是一个良好的开始。稍后如果需要，我们可以开始调整它。
- en: Training the network with images
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图像训练网络
- en: 'Now that we have the structure of the convolutional neural network, let''s
    explore how to train it. Training a neural network that works with images requires
    more memory than most computers have available. This is where the minibatch sources
    from [Chapter 3](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml), *Getting Data into
    Your Neural Network*, come into play. We''re going to set up a set of two minibatch
    sources to train and evaluate the neural network we''ve just created. Let''s first
    take a look at how to construct a minibatch source for images:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了卷积神经网络的结构，接下来让我们探讨如何训练它。训练一个处理图像的神经网络需要比大多数计算机可用内存更多的内存。这时，来自[第3章](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml)的minibatch数据源，*将数据输入到神经网络中*，就派上用场了。我们将设置一组包含两个minibatch数据源，用于训练和评估我们刚刚创建的神经网络。让我们首先看看如何为图像构建一个minibatch数据源：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the given steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, Import the `os` package to get access to some useful filesystem functions.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`os`包，以便访问一些有用的文件系统功能。
- en: Next, import the necessary components to create a new `MinibatchSource`.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入必要的组件来创建一个新的`MinibatchSource`。
- en: Create a new function `create_datasource` which takes the path to an input folder
    and a `max_sweeps` setting to control how often we can iterate over the dataset.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的函数`create_datasource`，该函数接收输入文件夹的路径和一个`max_sweeps`设置，用来控制我们遍历数据集的频率。
- en: Within the `create_datasource` function, locate the mapping.bin file within
    the source folder. This file will contain a mapping between the image on disk
    and its associated label.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`create_datasource`函数中，找到源文件夹中的`mapping.bin`文件。这个文件包含磁盘上的图像和其关联标签之间的映射。
- en: Then create a set of stream definitions to read from the mapping.bin file.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后创建一组流定义，用来从`mapping.bin`文件中读取数据。
- en: Add a `StreamDef` for the image file. Make sure to include the `transforms`
    keyword argument and initialize it with an empty array.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为图像文件添加一个`StreamDef`。确保包括`transforms`关键字参数，并初始化为空数组。
- en: Add another `StreamDef` for the labels field with 10 features.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为标签字段添加另一个`StreamDef`，该字段包含10个特征。
- en: Create a new `ImageDeserializer` and provide it the `mapping_file` and the `stream_definitions`
    variables.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`ImageDeserializer`，并为其提供`mapping_file`和`stream_definitions`变量。
- en: Finally, create a `MinibatchSource` and provide it with the deserializer and
    the `max_sweeps` setting.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，创建一个`MinibatchSource`并为其提供反序列化器和`max_sweeps`设置。
- en: Note, you can create the files necessary for training using the code in the
    `Preparing the dataset.ipynb` Python notebook. Make sure you have enough room
    on your hard drive to store the images. 1 GB of hard drive space is enough to
    store all samples for training and validation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以使用`Preparing the dataset.ipynb` Python笔记本中的代码创建训练所需的文件。确保你的硬盘上有足够的空间来存储图像。1GB的硬盘空间足够存储所有训练和验证样本。
- en: 'Once we have the `create_datasource` function, we can create two separate data
    sources to train the model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`create_datasource`函数，就可以创建两个独立的数据源来训练模型：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: First, call the `create_datasource` function with the `mnist_train` folder to
    create the data source for training.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，调用`create_datasource`函数，并传入`mnist_train`文件夹，以创建训练数据源。
- en: Next, call the `create_datasource` function with the `mnist_test` folder and
    set the `max_sweeps` to 1 to create the datasource for validating the neural network.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`create_datasource`函数，并使用`mnist_test`文件夹，将`max_sweeps`设置为1，以创建用于验证神经网络的数据源。
- en: 'Once you''ve prepared the images, it''s time to start training the neural network.
    We can use the `train` method on the `loss` function to kick off the training
    process:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备好图像，就可以开始训练神经网络了。我们可以使用`train`方法在`loss`函数上启动训练过程：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Follow the given steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤进行：
- en: First, import the Function decorator from the cntk package.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从cntk包中导入Function装饰器。
- en: Next, import the `cross_entropy_with_softmax` function from the losses module.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从losses模块中导入`cross_entropy_with_softmax`函数。
- en: Then, import the `classification_error` function from the metrics module.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从metrics模块中导入`classification_error`函数。
- en: After that, import the `sgd` learner from the learners module.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此之后，从learners模块导入`sgd`学习器。
- en: Create a new function `criterion_factory` with two parameters, output and targets.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新函数`criterion_factory`，带有两个参数：output和targets。
- en: Mark the function with the `@Function` decorator to turn it into a CNTK function
    object.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`@Function`装饰器标记该函数，将其转换为CNTK函数对象。
- en: Within the function, create a new instance of the `cross_entropy_with_softmax`
    function.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内部，创建一个新的`cross_entropy_with_softmax`函数的实例。
- en: Next, create a new instance of the `classification_error` metric.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个新的`classification_error`指标的实例。
- en: Return both the loss and metric as a result of the function.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失和指标作为函数的结果返回。
- en: After creating the `criterion_factory` function, initialize a new loss with
    it.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建`criterion_factory`函数后，用它初始化一个新的损失。
- en: Finally, setup the `sgd` learner with the parameters of the model and a learning
    rate of 0.2.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用模型的参数和学习率0.2设置`sgd`学习器。
- en: 'Now that we''ve setup the loss and learner for the neural network, let''s look
    at how to train and validate the neural network:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为神经网络设置了损失和学习器，让我们看看如何训练和验证神经网络：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Follow the given steps:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤进行：
- en: First import the `ProgressPrinter` class from the `logging` module
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先从`logging`模块中导入`ProgressPrinter`类。
- en: Next, import the `TestConfig` class from the `train` module.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从`train`模块中导入`TestConfig`类。
- en: Create a new instance of the `ProgressPrinter` so we can log the output of the
    training process.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`ProgressPrinter`实例，以便我们可以记录训练过程的输出。
- en: Then, create the `TestConfig` for the neural network using the `test_datasource`
    that we made earlier as input.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用前面创建的`test_datasource`作为输入，为神经网络创建`TestConfig`。
- en: Create a new dictionary to map the data streams from the `train_datasource`
    to the input variables of the neural network.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的字典，将`train_datasource`的数据流映射到神经网络的输入变量。
- en: Finally, call the `train` method on the `loss` and provide the `train_datasource`,
    the settings for the trainer, the `learner`, `input_map` and callbacks to use
    during training.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在`loss`上调用`train`方法，并提供`train_datasource`、训练器的设置、`learner`、`input_map`和训练期间要使用的回调函数。
- en: 'When you execute the python code, you will get back output that looks similar
    to this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '当您执行Python代码时，您将获得类似于以下输出：  '
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice how the loss decreases over time. It does take quite a long time to reach
    a low enough value for the model to be usable. Training an image classification
    model will take a very long time, so this is one of the cases where using GPU
    will make a big difference to the amount of time it takes to train the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意损失随时间减少的情况。达到足够低的值使模型可用确实需要相当长的时间。训练图像分类模型将需要很长时间，因此这是使用GPU将大大减少训练时间的情况之一。
- en: Picking the right combination of layers
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择正确的层次组合
- en: In previous sections we've seen how to use convolutional and pooling layers
    to build a neural network.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们已经看到如何使用卷积层和池化层构建神经网络。
- en: We've just seen that it takes quite a long time to train a model used for image
    recognition. Aside from the long training time, picking the right setup for a
    convolutional network is very hard and takes a long time. Often you will need
    hours of running experiments to find a network structure that works. This can
    be very demotivating for aspiring AI developers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到，训练用于图像识别的模型需要相当长的时间。除了长时间的训练时间外，选择卷积网络的正确设置也非常困难，需要很长时间。通常，您需要运行数小时的实验来找到有效的网络结构。这对于有抱负的AI开发者来说可能非常沮丧。
- en: 'Lucky for us, there are several research groups working on finding the best
    architecture for neural networks used in image classification tasks. There are
    several different architectures that have been used successfully in competitions
    and real-life scenarios:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，许多研究团队正在致力于寻找用于图像分类任务的最佳神经网络架构。已有几种不同的架构在竞赛和现实场景中取得了成功：
- en: VGG-16
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG-16
- en: ResNet
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: Inception
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception
- en: And there are several more. While we can't go into detail on how to build each
    of these architectures work, let's explore them on a functional level to see how
    they work so that you can make a more informed choice about which network architecture
    to try in your own application.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多的架构。虽然我们不能详细讨论每种架构的构建方式，但我们可以从功能层面探讨它们的工作原理，这样你可以做出更有根据的选择，决定在自己的应用中尝试哪种网络架构。
- en: The VGG network architecture was invented by the Visual Geometry Group as a
    way to classify images in 1,000 different categories. This is quite hard to do,
    but the team managed to get an accuracy of 70.1%, which is quite good, considering
    how hard it is to differentiate between 1,000 different categories
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络架构是由视觉几何组（Visual Geometry Group）发明的，用于将图像分类为1000个不同的类别。这项任务非常困难，但该团队成功达到了70.1%的准确率，考虑到区分1000个不同类别的难度，这个结果相当不错。
- en: The VGG network architecture uses stacks of convolution layers with an input
    size of 3x3\. The layers get an ever-increasing depth. Starting at layers with
    32 filters, continuing with 48 filters, all the way up to 512 filters. The reduction
    of the data volume is done using 2x2 pooling filters. The VGG network architecture
    was state-of-the-art when it was invented in 2015, as it had much better accuracy
    than the models invented previously.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络架构使用了堆叠的卷积层，输入大小为3x3。层的深度逐渐增加，从32个滤波器开始，继续使用48个滤波器，一直到512个滤波器。数据量的减少是通过使用2x2的池化滤波器完成的。VGG网络架构在2015年被发明时是当时的最先进技术，因为它的准确率比之前发明的模型要高得多。
- en: 'There are other ways to build neural networks for image recognition, though.
    The ResNet architecture uses what''s called a micro-architecture. It still uses
    convolution layers, but this time they are arranged in blocks. The architecture
    is still very similar to other convolutional networks, but where the VGG network
    has long chain layers, the ResNet architecture has skip connections around blocks
    of convolution layers:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，构建用于图像识别的神经网络还有其他方法。ResNet架构使用了所谓的微架构。它仍然使用卷积层，但这次它们被安排成块。该架构与其他卷积网络非常相似，只是VGG网络使用了长链式层，而ResNet架构则在卷积层块之间使用了跳跃连接：
- en: '![](img/b8cff3e4-456c-49b2-934b-f9e9cc603f31.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8cff3e4-456c-49b2-934b-f9e9cc603f31.png)'
- en: ResNet architecture
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构
- en: This is where the term micro-architecture comes from. Each of the blocks is
    a micro-network capable of learning patterns from the input. Each block has several
    convolution layers and a residual connection. This connection bypasses the block
    of convolution layers, and the data coming from this residual connection is added
    to the output of the convolution layers. The idea behind this is that the residual
    connection shakes up the learning process in the network so that it learns better
    and faster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是“微架构”这个术语的来源。每一个块都是一个微型网络，能够从输入中学习模式。每个块有若干卷积层和一个残差连接。这个连接绕过卷积层块，来自残差连接的数据会加到卷积层的输出上。这个设计的理念是，残差连接能够打破网络中的学习过程，使其学习得更好、更快。
- en: Compared to the VGG network architecture, the ResNet architecture is deeper
    but easier to train since it has fewer parameters that you need to optimize. The
    VGG network architecture takes up 599 MB of memory, while the ResNet architecture
    takes only 102 MB.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与VGG网络架构相比，ResNet架构更深，但更易于训练，因为它需要优化的参数较少。VGG网络架构占用599 MB的内存，而ResNet架构只需要102
    MB。
- en: 'The final network architecture that we''ll explore is the Inception architecture.
    This architecture is also one from the micro-architecture category. Instead of
    the residual blocks that are used in the ResNet architecture, the inception network
    uses inception blocks:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探讨的最终网络架构是Inception架构。这个架构同样属于微架构类别。与ResNet架构中使用的残差块不同，Inception网络使用了Inception块：
- en: '![](img/d0d66db2-9de6-4355-a709-34d0e87c754d.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0d66db2-9de6-4355-a709-34d0e87c754d.png)'
- en: Inception Network
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Inception网络
- en: The inception blocks in the Inception architecture use convolution layers with
    different input sizes of 1x1, 3x3, and 5x5, which are then concatenated along
    the channel axis. This generates a matrix that has the same width and height as
    the input but has more channels than the input. The idea is that when you do this,
    you have a much better spread of features extracted from the input and thus much
    better quality data to perform the classification task on. The Inception architecture
    depicted here is very shallow; the full version used normally can have more than
    two inception blocks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Inception架构中的Inception块使用不同输入大小（1x1、3x3和5x5）的卷积层，然后沿着通道轴进行拼接。这会生成一个矩阵，其宽度和高度与输入相同，但通道数比输入更多。其思想是，当你这样做时，输入中提取的特征会有更好的分布，从而为执行分类任务提供更高质量的数据。这里展示的Inception架构非常浅，通常使用的完整版本可以有超过两个Inception块。
- en: 'When you start to work on the other convolutional network architectures you
    will quickly find that you need a lot more computational power to train them.
    Often, the dataset won''t fit into memory and your computer will just be too slow
    to train the model within a reasonable amount of time. This is where distributed
    training can help out. If you''re interested in training models using multiple
    machines, you should definitely take a look at this chapter in the CNTK manual:
    [https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines](https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始处理其他卷积神经网络架构时，你会很快发现你需要更多的计算能力来训练它们。通常，数据集无法完全加载到内存中，且你的计算机可能会因为训练模型所需的时间过长而变得太慢。此时，分布式训练可以提供帮助。如果你对使用多台机器训练模型感兴趣，绝对应该查看CNTK手册中的这一章节：[https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines](https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines)。
- en: Improving model performance with data augmentation
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过数据增强提升模型性能
- en: Neural networks used for image recognition not only are difficult to set up
    and train, they also require a lot of data to train. Also, they tend to overfit
    on the images used during training. For example, when you only use photos of faces
    in an upright position, your model will have a hard time recognizing faces that
    are rotated in another direction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像识别的神经网络不仅难以设置和训练，还需要大量数据来进行训练。此外，它们往往会在训练过程中对图像过拟合。例如，当你只使用直立姿势的面部照片时，模型很难识别以其他方向旋转的面部。
- en: To help overcome problems with rotation and shifts in certain directions, you
    can use image augmentation. CNTK supports specific transforms when creating a
    minibatch source for images.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助克服旋转和某些方向上的偏移问题，你可以使用图像增强。CNTK在创建图像的小批量源时，支持特定的变换。
- en: We've included an additional notebook for this chapter that demonstrates how
    to use the transformations. You can find the sample code for this section in the
    `Recognizing hand-written digits with augmented data.ipynb` file in the samples
    for this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本章节提供了一个额外的笔记本，演示了如何使用这些变换。你可以在本章节的示例中找到该部分的示例代码，文件名为`Recognizing hand-written
    digits with augmented data.ipynb`。
- en: 'There are several transformations that you can use. For example, you can randomly
    crop images used for training with just a few lines of code. Other transformations
    you can use are scale and color. You can find more information about these transformations
    on the CNTK website: [https://cntk.ai/pythondocs/cntk.io.transforms.html](https://cntk.ai/pythondocs/cntk.io.transforms.html).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用多种变换。例如，你可以只用几行代码随机裁剪用于训练的图像。你还可以使用的其他变换包括缩放和颜色变换。你可以在CNTK网站上找到有关这些变换的更多信息：[https://cntk.ai/pythondocs/cntk.io.transforms.html](https://cntk.ai/pythondocs/cntk.io.transforms.html)。
- en: 'Within the function used to create the minibatch source earlier in this chapter,
    we can change the list of transforms by including a cropping transform as shown
    in the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面用于创建小批量源的函数中，我们可以通过加入裁剪变换来修改变换列表，代码如下所示：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We've enhanced the function to include a set of image transforms. When we're
    training, we will randomly crop the image so we get more variations of the image.
    This changes the dimensions of the image, however, so we need to also include
    a scale transformation to make sure that it fits the size expected by the input
    layer of our neural network.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改进了函数，加入了一组图像变换。在训练时，我们将随机裁剪图像，以获得更多图像的变化。然而，这也会改变图像的尺寸，因此我们还需要加入一个缩放变换，确保图像符合神经网络输入层所期望的大小。
- en: Using these transforms during training will increase the variation in the training
    data, which reduces the chance that your neural network gets stuck on images that
    have a slightly different color, rotation, or size.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中使用这些变换将增加训练数据的变化性，从而减少神经网络因图像的颜色、旋转或大小稍有不同而卡住的几率。
- en: Be aware, though, that these transforms don't generate new samples. They simply
    change the data before it is fed into the trainer. You will want to increase the
    maximum number of epochs to allow for enough random samples to be generated with
    these transforms applied. How many extra epochs of training you need will depend
    on the size of your dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 但是需要注意的是，这些变换不会生成新的样本。它们仅仅是在数据输入到训练器之前对其进行更改。你需要增加最大训练轮次，以便在应用这些变换时生成足够的随机样本。需要额外的训练轮次数量将取决于数据集的大小。
- en: It's also important to keep in mind that the dimensions of the input layer and
    intermediate layers have a large impact on the capabilities of the convolutional
    network. Larger images will naturally work better when you want to detect small
    objects. Scaling images back to a much smaller size will make the smaller object
    disappear or lose too much detail to be recognizable by the network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，需要牢记输入层和中间层的维度对卷积网络的能力有很大影响。较大的图像在检测小物体时自然会表现得更好。将图像缩小到一个更小的尺寸会使较小的物体消失，或者丧失过多细节，以至于网络无法识别。
- en: Convolutional networks that support larger images will, however, take a lot
    more computation power to optimize, so it will take longer to train them and it
    will be harder to get optimal results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，支持较大图像的卷积网络需要更多的计算能力来进行优化，因此训练这些网络将花费更长的时间，而且更难得到最佳结果。
- en: Ultimately, you will need to balance a combination of image size, layer dimensions,
    and what data augmentation you use to get optimal results.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你需要平衡图像大小、层的维度以及使用的数据增强方法，以获得最佳结果。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've looked at using neural networks to classify images. It's
    very different from working with normal data. Not only do we need a lot more training
    data to get the right result, we also need a different architecture to work with
    images that is better suited for the job.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用神经网络进行图像分类。与处理普通数据有很大的不同。我们不仅需要更多的训练数据来得到正确的结果，还需要一种更适合图像处理的不同架构。
- en: We've seen how convolution layers and pooling layers can be used to essentially
    create an advanced photo filter that extracts important details from the data
    and summarize these details to reduce the dimensionality of the input to a manageable
    size.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了卷积层和池化层如何被用来本质上创建一个高级的照片滤镜，从数据中提取重要细节，并总结这些细节，以减少输入的维度，使其变得可管理。
- en: Once we have used the advanced properties of the convolution filters and pooling
    filters, it's back to business as usual with dense layers to produce a classification
    network.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用了卷积滤波器和池化滤波器的高级特性，接下来就是常规的工作了，通过密集层来构建分类网络。
- en: It can be quite hard to come up with a good structure for an image classification
    model, so it's always a good idea to check out one of the existing architectures
    before venturing into image classification. Also, using the right kind of augmentation
    techniques can help quite a bit to get better performance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为图像分类模型设计一个良好的结构可能相当困难，因此在开始进行图像分类之前，查看现有的架构总是一个不错的主意。另外，使用合适的增强技术可以在提升性能方面起到相当大的作用。
- en: Working with images is just one of the scenarios where deep learning is powerful.
    In the next chapter, we'll look at how to use deep learning to train models on
    time-series data, such as stock exchange information, or course information for
    things such as Bitcoin. We'll learn how to use sequences in CNTK and how to build
    a neural network that can reason over time. See you in the next chapter.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 处理图像只是深度学习强大功能的一个应用场景。在下一章中，我们将探讨如何使用深度学习在时间序列数据上训练模型，如股票信息或比特币等课程信息。我们将学习如何在CNTK中使用序列，并构建一个可以在时间上推理的神经网络。下一章见。
