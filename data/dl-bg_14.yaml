- en: Deep and Wide Neural Networks
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 深度与宽神经网络
- en: So far, we have covered a variety of unsupervised deep learning methodologies
    that can lead to many interesting applications, such as feature extraction, information
    compression, and data augmentation. However, as we move toward supervised deep
    learning methodologies that can perform classification or regression, for example,
    we have to begin by addressing an important question related to neural networks
    that might be in your mind already: *what is the difference between wide and deep
    neural networks?*
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了多种无监督深度学习方法，这些方法可以应用于许多有趣的领域，例如特征提取、信息压缩和数据增强。然而，当我们转向可以执行分类或回归等任务的监督式深度学习方法时，我们必须首先解决一个与神经网络相关的重要问题，这个问题你可能已经在思考了：*宽神经网络和深神经网络之间有什么区别？*
- en: In this chapter, you will implement deep and wide neural networks to see the
    difference in the performance and complexities of both. As a bonus, we will cover
    the concepts of dense networks and sparse networks in terms of the connections
    between neurons. We will also optimize the dropout rates in our networks to maximize
    the generalization ability of the network, which is a critical skill to have today.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将实现深度神经网络和宽神经网络，以观察两者在性能和复杂性上的差异。作为额外内容，我们还将讨论稠密网络和稀疏网络之间在神经元连接方面的概念。我们还将优化网络中的丢弃率，以最大化网络的泛化能力，这是今天必须掌握的一项关键技能。
- en: 'This chapter is organized as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结构如下：
- en: Wide neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽神经网络
- en: Dense deep neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稠密深度神经网络
- en: Sparse deep neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏深度神经网络
- en: Hyperparameter optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Wide neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 宽神经网络
- en: Before we discuss the types of neural networks covered in this chapter, it might
    be appropriate to revisit the definition of deep learning and then continue addressing
    all these types.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论本章涉及的神经网络类型之前，可能有必要重新审视一下深度学习的定义，然后继续介绍所有这些类型。
- en: Deep learning revisited
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习回顾
- en: Recently, on February 9, 2020, Turing Award winner Yann LeCun gave an interesting
    talk at the AAAI-20 conference in New York City. In his talk, he provided clarity
    with respect to what deep learning is, and before we give this definition here,
    let me remind you that LeCun (along with J. Bengio, and G. Hinton) is considered
    one of the fathers of deep learning, and received the Turing Award for precisely
    his achievements in the area. Therefore, what he has to say is important. Secondly,
    throughout this book, we have not given a strong definition of what deep learning
    is; people might be thinking that it refers to deep neural networks, but that
    is not factually correct – it is much more than that, so let's set the record
    straight once and for all.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在2020年2月9日，图灵奖得主Yann LeCun在纽约市的AAAI-20会议上做了一个有趣的演讲。在演讲中，他清晰地阐明了深度学习的定义，在我们在此给出这个定义之前，让我提醒你，LeCun（与J.
    Bengio和G. Hinton一起）被认为是深度学习的奠基人之一，并且正是因其在该领域的成就而获得了图灵奖。因此，他的观点非常重要。其次，在本书中，我们没有给出深度学习的明确定义；人们可能认为它指的是深度神经网络，但这并不准确——它远不止于此，所以让我们彻底澄清这个问题。
- en: '"It is not just supervised learning, it is not just neural networks, **Deep
    Learning** is the idea of building a system by assembling parametrized modules
    into a (possibly dynamic) computation graph, and training it to perform a task
    by optimizing the parameters using a gradient-based method." - Yann LeCun'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “这不仅仅是监督学习，这不仅仅是神经网络，**深度学习**是通过将参数化模块组装成（可能是动态的）计算图，并通过使用基于梯度的方法优化参数来训练它执行任务的一种理念。”
    —— Yann LeCun
- en: Most of the models we have covered so far fit this definition, with the exception
    of the simple introductory models that we used to explain the more complex ones.
    The only reason why those introductory models are not included as deep learning
    is that they are not necessarily part of a computation graph; we are referring
    specifically to the perceptron (Rosenblatt, F. (1958)*)*, and the corresponding
    **Perceptron Learning Algorithm** (**PLA**) (Muselli, M. (1997)*)*. However, from
    the **Multilayer Perceptron** (**MLP**) and forward, all algorithms presented
    so far are, in fact, deep learning algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们覆盖的大多数模型都符合这个定义，除了我们用来解释更复杂模型的简单入门模型。之所以这些入门模型不被视为深度学习，是因为它们不一定是计算图的一部分；我们具体指的是感知器（Rosenblatt,
    F. (1958)）*，以及相应的**感知器学习算法**（**PLA**）（Muselli, M. (1997)）*。然而，从**多层感知器**（**MLP**）开始，到目前为止展示的所有算法实际上都是深度学习算法。
- en: This is an important distinction to make at this point since this is a deep
    learning book, and you are *learning* about deep learning. We are about to learn
    some of the most interesting topics in deep learning and we need to keep a focus
    on what deep learning is. We will talk about deep networks and wide networks;
    however, both are deep learning. In fact, all the models we will be discussing
    here are deep learning models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上做出这个重要的区分是必要的，因为这是一本深度学习书籍，而你正在*学习*深度学习。我们即将学习一些深度学习中最有趣的主题，我们需要聚焦于深度学习是什么。我们将讨论深度网络和宽网络；然而，两者都是深度学习。实际上，我们将在这里讨论的所有模型都是深度学习模型。
- en: With this clarification in mind, let's define what a wide network is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个澄清之后，让我们定义一下什么是宽网络。
- en: Wide layers
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宽层
- en: 'What makes a neural network **wide** is a relatively large number of neurons
    in a relatively small number of hidden layers. Recent developments in deep learning
    have even made possible the computational treatment of wide networks with an infinite
    amount of neural units (Novak, R., et al. (2019)*)*. Although this is a very nice
    advance in the field, we will limit our layers to have a reasonable number of
    units. To make our comparison with a *less wide* network, we will create a wide
    network for the CIFAR-10 dataset. We will create the architecture shown in the
    following figure:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使神经网络**变宽**的原因是在相对较少的隐藏层中拥有相对较多的神经元。深度学习的最新发展甚至使得计算处理具有无限数量神经单元的宽网络成为可能（Novak,
    R., 等人，2019年）。虽然这是该领域的一个重要进展，但我们会将层数限制为具有合理数量的单元。为了与*较不宽*的网络进行比较，我们将为CIFAR-10数据集创建一个宽网络。我们将创建如下图所示的架构：
- en: '![](img/556ac356-0a15-48e3-91b0-68750f573abf.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/556ac356-0a15-48e3-91b0-68750f573abf.png)'
- en: Figure 11.1 – Network architecture of a wide network for CIFAR-10
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – CIFAR-10的宽网络架构
- en: One important aspect of neural networks that we will consider from now on is
    the number of **parameters**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从现在开始要考虑神经网络的一个重要方面是**参数**的数量。
- en: In deep learning, the number of **parameters** is defined as the number of variables
    that the learning algorithm needs to estimate through gradient descent techniques
    in order to minimize the loss function. The great majority of parameters are,
    usually, the weights of a network; however, other parameters might include biases,
    mean and standard deviation for *batch normalization*, filters for convolutional
    networks, memory vectors for recurrent networks, and many others.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，**参数**的数量定义为学习算法需要通过梯度下降技术估计的变量数量，以便最小化损失函数。大多数参数通常是网络的权重；然而，其他参数可能包括偏置、*批归一化*的均值和标准差、卷积网络的滤波器、循环网络的记忆向量以及许多其他内容。
- en: Knowing the number of parameters has been of particular importance because,
    in an ideal world, you want to have more data samples than variables you want
    to learn. In other words, an ideal learning scenario includes more data than parameters.
    If you think about it, it is intuitive; imagine having a matrix with two rows
    and three columns. The three columns describe the color representation in red,
    green, and blue of a fruit. The two rows correspond to one sample for an orange
    and another one for an apple. If you want to build a linear regression system
    to determine the probability of the data being from an orange, you certainly would
    like to have a lot more data! Especially since there are many apples that may
    have a color that is close to the color of an orange. More data is better! But
    if you have more parameters, like in linear regression where you have as many
    parameters as columns, then your problem is usually described as an *ill-posed *problem.
    In deep learning, this phenomenon is known as **over-parametrization**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 了解参数的数量特别重要，因为在理想情况下，你希望数据样本的数量大于你想要学习的变量数。换句话说，理想的学习场景应该包含比参数更多的数据。如果你仔细想想，这是直观的；假设有一个矩阵，包含两行三列。三列描述的是水果的红、绿、蓝三色的表示。两行对应的是一个橙子的样本和一个苹果的样本。如果你想建立一个线性回归系统来判断数据是否来自橙子，你肯定希望有更多的数据！尤其是因为有很多苹果，它们的颜色可能与橙子的颜色相似。更多的数据更好！但如果你有更多的参数，比如在线性回归中，参数的数量与列数相同，那么你的问题通常会被描述为*不适定*问题。在深度学习中，这种现象被称为**过度参数化**。
- en: Only in deep learning do over-parametrized models work really well. There is
    research that has shown that in the particular case of neural networks, given
    the redundancy of data as it flows in non-linear relationships, the loss functions
    can produce smooth landscapes (Soltanolkotabi, M., et al. (2018)). This is particularly
    interesting because then we could prove that over-parametrized deep learning models
    will converge to very good solutions using gradient descent (Du, S. S., et al.
    (2018)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在深度学习中，过度参数化的模型才会表现得非常好。有研究表明，在神经网络的特定情况下，考虑到数据在非线性关系中的冗余性，损失函数可以生成平滑的地形（Soltanolkotabi,
    M., et al. (2018)）。这尤其有趣，因为我们可以证明，过度参数化的深度学习模型在使用梯度下降时会收敛到非常好的解决方案（Du, S. S.,
    et al. (2018)）。
- en: Summaries
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In Keras, there is a function called `summary()` that, called from a `Model`
    object, can give the total number of parameters to be estimated. For example,
    let''s create the wide network in *Figure 11.1*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，有一个名为 `summary()` 的函数，当从 `Model` 对象调用时，可以显示需要估算的总参数数量。例如，让我们创建图 *Figure
    11.1* 中的宽网络：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code produces the following output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会产生以下输出：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The summary produced here indicates that the total number of parameters in the
    model is 18,911,242\. This is to show that a simple wide network can have nearly
    19 million parameters for a problem with 3,072 features. This is clearly an over-parametrized
    model on which we will perform gradient descent to learn those parameters; in
    other words, this is a deep learning model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此处生成的摘要表明模型中的参数总数为 18,911,242。这是为了说明，简单的宽网络在具有 3,072 个特征的问题中可以有近 1900 万个参数。这显然是一个过度参数化的模型，我们将在其上执行梯度下降来学习这些参数；换句话说，这是一个深度学习模型。
- en: Names
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 名称
- en: Another new thing that we will introduce in this chapter is the use of **names**
    for individual pieces of the Keras models. You should have noticed that in the
    preceding code, the script contains a new argument with a string assigned to it;
    for example, `Dropout(0.5, **name**='d1')`. This is used internally to keep track
    of the names of pieces in the model. This can be good practice; however, it is
    not required. If you do not provide names, Keras will automatically assign generic
    names to each individual piece. Assigning names to elements can be helpful when
    saving or restoring models (we will do that soon enough – be patient), or can
    be useful when printing summaries, like the preceding one.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍的另一个新内容是使用 **名称** 来标识 Keras 模型中的各个组件。你应该注意到，在之前的代码中，脚本包含了一个新的参数并为其分配了一个字符串值；例如，`Dropout(0.5,
    **name**='d1')`。这在内部用于跟踪模型中各个部分的名称。这是一个好的实践，但并不是强制要求的。如果不提供名称，Keras 会自动为每个组件分配一个通用名称。为元素分配名称在保存或恢复模型时（我们很快就会做这件事——请耐心等待），或在打印摘要时（如前所述）可能会很有帮助。
- en: Now, let's look at the dataset that we will load. Precisely, the data mentioned
    earlier that has 3,072 dimensions, called CIFAR-10.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看将要加载的数据集。准确地说，就是之前提到的具有 3,072 个维度的数据集，称为 CIFAR-10。
- en: The CIFAR-10 dataset
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR-10 数据集
- en: The dataset we will work with in this chapter is called **CIFAR-10**. It comes
    from the acronym **Canadian Institute For Advanced Research** (**CIFAR**). The
    number 10 comes from the number of classes with which the dataset is organized.
    It is a dataset of color images that also has an alternative database with 100
    different objects, known as CIFAR-100; however, we will focus on CIFAR-10 for
    now. Each color image is ![](img/4de98ef3-ecc3-425b-8729-23683bac81a5.png) pixels.
    Its total dimensions, considering the color channels, is ![](img/7cc49236-3995-41c3-a6b9-2a8a255a67c9.png).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将使用的数据集称为 **CIFAR-10**。它来源于 **Canadian Institute For Advanced Research**（**CIFAR**）的缩写。数字
    10 来自数据集所包含的类别数量。它是一个彩色图像数据集，还有一个包含 100 种不同对象的替代数据库，称为 CIFAR-100；然而，我们现在将重点关注
    CIFAR-10。每个彩色图像是 ![](img/4de98ef3-ecc3-425b-8729-23683bac81a5.png) 像素。考虑到颜色通道，它的总维度为
    ![](img/7cc49236-3995-41c3-a6b9-2a8a255a67c9.png)。
- en: 'The diagram in *Figure 11.1* has one image sample and *Figure 11.2* has an
    example for each class within the test set:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *Figure 11.1* 中的示意图有一个图像样本，而图 *Figure 11.2* 中则展示了测试集内每个类别的示例：
- en: '![](img/3fa9732f-d25a-4a1b-b7f4-81561e948eaf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fa9732f-d25a-4a1b-b7f4-81561e948eaf.png)'
- en: Figure 11.2 – Sample color images for each class in the CIFAR-10 dataset
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – CIFAR-10 数据集中每个类别的样本彩色图像
- en: 'This dataset can be loaded by executing the following commands:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过执行以下命令来加载该数据集：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This downloads the data automatically and produces the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这会自动下载数据并产生以下输出：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These things are nothing new except for the dataset. For more information about
    how this dataset was prepared, please check [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing
    Data*, where we go over how to convert data into usable data by normalizing it
    and converting targets to one-hot encoding.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容除了数据集外，没什么新鲜的。关于数据集的准备方式，请参考[第3章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)，*数据准备*，我们将在其中讲解如何通过标准化数据并将目标转换为独热编码来将数据转化为可用数据。
- en: The output we receive by printing the shape of the dataset, using the `.shape`
    attribute of NumPy arrays, tells us that we have 50,000 samples to train with,
    and 10,000 samples on which to test our training performance. This is the standard
    split in the deep learning community and helps the comparison among methodologies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过打印数据集的形状，使用NumPy数组的`.shape`属性得到的输出显示，我们有50,000个样本用于训练，另外还有10,000个样本用于测试训练效果。这是深度学习社区的标准数据集划分方式，便于不同方法间的比较。
- en: New training tools
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的训练工具
- en: 'With the code we have so far, we can easily begin the training process by invoking
    the `fit()` method as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过到目前为止的代码，我们可以很容易地开始训练过程，只需像下面这样调用`fit()`方法：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is nothing new; we covered all these details in [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational
    Autoencoders*. However, we want to introduce new important tools into the mix
    that will help us train a better model, much more efficiently, and preserve our
    best-trained model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些并不新颖，我们在[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自编码器*中已经讲解过这些细节。然而，我们希望引入一些新的重要工具，这些工具将帮助我们更高效地训练更好的模型，并保存我们最优训练的模型。
- en: Saving or loading models
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存或加载模型
- en: 'Saving our trained model is important if we want to sell a product, or distribute
    a working architecture, or to control versions of models. These models can be
    saved by calling on either of the following methods:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 保存训练好的模型很重要，如果我们想销售产品、分发工作架构，或控制模型的版本，这样的模型可以通过调用以下任一方法来保存：
- en: '`save()`, used to save the whole model, including optimizer states such as
    the gradient descent algorithm, the number of epochs, the learning rate, and others.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save()`，用于保存整个模型，包括优化器的状态，例如梯度下降算法、迭代次数、学习率等。'
- en: '`save_weights()`, used to save only the parameters of the model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_weights()`，用于仅保存模型的参数。'
- en: 'For example, we can save our model''s weights as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以按以下方式保存模型的权重：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create a file in your local disk called `widenet.hdf5`. This type
    of file extension is for a standard file format called **Hierarchical Data Format**
    (**HDF**), which enables consistency across common platforms, and, therefore,
    the easy sharing of data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会在本地磁盘上创建一个名为`widenet.hdf5`的文件。这种文件扩展名对应一种叫做**层次数据格式**（**HDF**）的标准文件格式，它可以确保跨平台的一致性，因此，便于数据共享。
- en: 'You can re-load a saved model later on by simply executing the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以稍后通过执行以下命令重新加载保存的模型：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that doing this relies on you building the model first, that is, creating **exactly**
    all the layers of the model in the exact same order and with the exact same names.
    An alternative, to save all the effort of reconstructing the model exactly, is
    to use the `save()` method.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，执行此操作前你必须先构建好模型，即按照**精确**的顺序创建所有模型层，并使用完全相同的名称。另一种替代方法是使用`save()`方法来避免重新构建模型。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'However, the downside of using the `save()` method is that to load the model
    you will need to import an additional library, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用`save()`方法的缺点是，为了加载模型，你将需要导入一个额外的库，如下所示：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This essentially removes the need to re-create the model. Throughout this chapter,
    we will be saving our model weights simply so that you get used to it. Now let's
    take a look at how to use **callbacks**, which are interesting ways to monitor
    the learning process. We will start with a **callback **for reducing the learning
    rate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这样就不需要重新创建模型了。在本章中，我们将通过保存模型权重来让你习惯这一过程。现在，让我们来看一下如何使用**回调函数**，这是一种监控学习过程的有趣方法。我们将从一个**用于降低学习率的回调**开始。
- en: Reducing the learning rate on the fly
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态降低学习率
- en: Keras has a superclass for **callbacks**, found in `tensorflow.keras.callbacks`, where
    we have, among other nice things, a class for reducing the learning rate of the
    learning algorithm. If you don't remember what the **learning rate** is, feel
    free to go back to [Chapter 6](a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml), *Training
    Multiple Layers of Neurons*, to review the concept. But, as a quick recap, the
    learning rate controls how big the steps that are taken to update the parameters
    of the model in the direction of the gradient are.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Keras有一个`callbacks`的父类，位于`tensorflow.keras.callbacks`中，在这里我们有其他有用的工具，其中包括一个用于减少学习率的类。如果你不记得**学习率**是什么，可以回到[第六章](a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml)，*训练多层神经网络*，复习一下这个概念。不过，简而言之，学习率控制了更新模型参数时沿梯度方向所采取的步长大小。
- en: The problem is that, many times, you will encounter that certain types of deep
    learning models *get stuck* in the learning process. By *getting stuck* we mean
    that there is no progress being made toward reducing the loss function either
    on the training or validation set. The technical term the *professionals *use
    is that the learning looks like a **plateau**. It is a problem that is evident
    when you look at how the loss function is minimized across epochs because it looks
    like a *plateau*, that is, a flat line. Ideally, we want to see the loss decreasing
    at every epoch, and it is usually the case for the first few epochs, but there
    can be a time when reducing the learning rate can help the learning algorithm
    to *focus* by making small changes to the existing acquired knowledge, that is,
    the learned parameters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，很多时候，你会遇到某些深度学习模型在学习过程中*陷入困境*。我们所说的*陷入困境*是指在训练集或验证集上，损失函数没有取得任何进展。专业人士使用的技术术语是，学习看起来像是**平台期**。这是一个显而易见的问题，尤其是当你查看损失函数在多个训练轮次（epochs）中的变化时，因为它看起来像是一个*平台期*，也就是一条平坦的线。理想情况下，我们希望看到损失在每个训练轮次中都在下降，通常在前几个训练轮次中是这样的，但有时降低学习率能帮助学习算法通过对现有已学知识（即已学得的参数）做出小的调整来*集中注意力*，从而继续进步。
- en: 'The class we are discussing here is called `ReduceLROnPlateau`. You can load
    it as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的类叫做`ReduceLROnPlateau`。你可以按如下方式加载它：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To use this library, you will have to use the `callbacks` argument in the `fit()`
    function after defining it like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个库，你需要在定义后，在`fit()`函数中使用`callbacks`参数，如下所示：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this code fragment, we call `ReduceLROnPlateau` with the following arguments:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用以下参数调用`ReduceLROnPlateau`：
- en: '`monitor=''val_loss''`, this is the default value, but you can change it to
    look for a plateau in the `''loss''` curve.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitor=''val_loss''`，这是默认值，但你可以更改它以查看‘loss’曲线中的平台期。'
- en: '`factor=0.1`, this is the default value, and it is the rate by which the learning
    rate will be reduced. For example, the default learning rate for the Adam optimizer
    is 0.001, but when a plateau is detected, it will be multiplied by 0.1, leading
    to a new updated learning rate of 0.0001.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factor=0.1`，这是默认值，表示学习率将被减少的比例。例如，Adam优化器的默认学习率是0.001，但当检测到平台期时，它会乘以0.1，从而得到新的学习率0.0001。'
- en: '`patience=20`, the default value is 10, and is the number of epochs with no
    improvement in the monitored loss, which will be considered a plateau.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patience=20`，默认值是10，表示在监视的损失没有改善的情况下，连续多少轮次会被认为是平台期。'
- en: There are other arguments that you can use in this method, but these are the
    most popular, in my opinion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法中还有其他参数可以使用，但在我看来，这些是最常用的。
- en: 'Next, let''s look at another important callback: *early stopping*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看另一个重要的回调：*提前停止*。
- en: Stopping the learning process early
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提前停止学习过程
- en: 'This next callback is interesting because it allows you to stop the training
    if there is no progress being made and **it allows you to keep the best version
    of the model **during the learning process. It is found in the same class as the
    preceding one and is called `EarlyStopping()`, and you can load it as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个回调很有趣，因为它允许你在没有进展的情况下停止训练，并且**它允许你在学习过程中保留模型的最佳版本**。它与前一个回调在同一个类中，名为`EarlyStopping()`，你可以按如下方式加载它：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The early stopping callback essentially lets you stop the training process
    if there has been no progress in the last few epochs, as specified in the `patience`
    parameter. You can define and use the early stopping callback as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止回调本质上让你在指定的`patience`参数指定的若干训练轮次内没有进展时停止训练过程。你可以按如下方式定义并使用提前停止回调：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is a short explanation of each of the arguments used in `EarlyStopping()`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 `EarlyStopping()` 中每个参数的简短解释：
- en: '`monitor=''val_loss''`, this is the default value, but you can change it to
    look for changes in the `''loss''` curve.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitor=''val_loss''`，这是默认值，但你可以更改为查看 `''loss''` 曲线的变化。'
- en: '`patience=100`, the default value is 10 and is the number of epochs with no
    improvement in the monitored loss. I personally like to set this to a larger number
    than the patience in `ReduceLROnPlateau`, because I like to let the learning rate
    produce an improvement in the learning process (hopefully) before I terminate
    the learning process because there was no improvement.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patience=100`，默认值为 10，是指在监控的损失没有改善的轮数。我个人喜欢将这个值设置为比 `ReduceLROnPlateau` 中的耐心值更大的数值，因为我喜欢在终止学习过程之前，先让学习率在学习过程中产生一些改善（希望如此），而不是因为没有改善就提前结束。'
- en: '`restore_best_weights=True`, the default value is `False`. If `False`, the
    model weights obtained at the last epoch are preserved. However, if set to `True`,
    it will preserve and return the best weights at the end of the learning process.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`restore_best_weights=True`，默认值为 `False`。如果为 `False`，则会保存最后一轮训练得到的模型权重。但是，如果设置为
    `True`，它将保存并返回学习过程中最好的权重。'
- en: This last argument is my personal favorite because I can set the number of epochs
    to a large number, within reason, and let the training go for as long as it needs.
    In the preceding example, if we set the number of epochs to 1,000, it does not
    necessarily mean that the learning process will go for 1,000 epochs, but if there
    is no progress within 50 epochs, the process can stop early. If the process gets
    to a point at which it has learned good parameters, it can get to a point at which
    there is no progress, then stop after 50 epochs and still return the best model
    that was ever recorded during the learning process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的参数是我个人最喜欢的，因为我可以将训练的轮数设置为一个合理的较大数值，让训练持续进行，直到它需要的时间为止。在前面的示例中，如果我们将训练轮数设置为
    1,000，这并不意味着学习过程会进行 1,000 轮，但如果在 50 轮内没有进展，过程可以提前终止。如果过程已经达到了一个点，模型学到了良好的参数，但没有进一步的进展，那么可以在
    50 轮后停止，并仍然返回在学习过程中记录下的最佳模型。
- en: 'We can combine all the preceding callbacks and the saving methodology as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有前面的回调函数和保存方法结合如下：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Notice that the callbacks have been combined into a list of callbacks that
    will monitor the learning process, looking for plateaus to decrease the learning
    rate, or looking to stop the process if there has been no improvement in a few
    epochs. Also, notice that we created a new variable, `hist`. This variable contains
    a dictionary with logs of the learning process, such as the losses across epochs.
    We can plot such losses to see how the training takes place as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，回调函数已经被合并成一个回调列表，监控学习过程，寻找平稳期来降低学习率，或在几轮内没有改善时终止过程。同时，注意我们创建了一个新变量 `hist`。这个变量包含一个字典，记录了学习过程中的日志，例如每轮的损失。我们可以绘制这些损失曲线，看看训练过程是如何进行的，如下所示：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the curves in *Figure 11.3*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成 *图 11.3* 中的曲线：
- en: '![](img/fe6ae368-567e-4d3c-9a03-630d7eedfdaf.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe6ae368-567e-4d3c-9a03-630d7eedfdaf.png)'
- en: Figure 11.3 – Model loss of widenet across epochs using callbacks
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 使用回调函数的 widenet 模型损失随轮次变化的曲线
- en: From the figure, we can clearly see the evidence of the learning rate reduction
    around epoch 85 where the learning is adjusted after the plateau in the validation
    loss (that is, the loss over the test set); however, this has little effect on
    the validation loss, and therefore training is terminated early around epoch 190
    since there was no improvement in the validation loss.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以清楚地看到，在第 85 轮左右，学习率下降的证据，在验证损失（即测试集上的损失）平稳后进行调整；然而，这对验证损失几乎没有影响，因此训练在大约第
    190 轮时提前终止，因为验证损失没有改善。
- en: In the next section, we will analyze the performance of the `widenet` model
    in a quantitative manner that will allow comparison later on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将以定量的方式分析 `widenet` 模型的性能，以便稍后进行比较。
- en: Results
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Here, we want to simply explain the performance of the network in terms that
    are easy to understand and to communicate to others. We will be focusing on analyzing
    the confusion matrix of the model, precision, recall, F1-score, accuracy, and
    balanced error rate. If you do not recall what these terms mean, please go back
    and quickly review [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning
    from Data*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是想以易于理解并能与他人沟通的方式来解释网络的性能。我们将重点分析模型的混淆矩阵、精度、召回率、F1分数、准确率和均衡误差率。如果你不记得这些术语的含义，请返回并快速复习[第4章](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml)，*数据学习*。
- en: 'One of the nice things about scikit-learn is that it has a nice automated process
    for calculating a report for classification performance that includes most of
    the terms mentioned above. It is simply called a **classification report**. This
    and the other libraries that we will need can be found in the `sklearn.metrics`
    class and can be imported as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的一个优点是它有一个自动化过程，可以生成一个分类性能报告，其中包括上述大部分术语。这个报告被称为**分类报告**。我们将需要的其他库可以在`sklearn.metrics`类中找到，并可以按如下方式导入：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'These three libraries operate in a similar way – they take the ground truth
    and the predictions to evaluate performance:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种库的工作方式类似——它们使用真实标签和预测值来评估性能：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This code outputs something like this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出类似如下内容：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The part on the top indicates the output of `classification_report()`. It gives
    the precision, recall, f1-score, and accuracy of the model. Ideally, we want to
    have all of those numbers as close to 1.0 as possible. Intuitively, the accuracy
    needs to be 100% (or 1.0); however, the rest of the numbers require careful study.
    From this report, we can observe that the total accuracy is 54%. From the rest
    of the report, we can determine that the classes that are more accurately classified
    are 1 and 8, corresponding to *automobile* and *ship*. Similarly, we can see that
    the two classes most poorly classified are 3 and 5, corresponding to *cats *and *dogs*,
    respectively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部部分显示了`classification_report()`的输出。它提供了模型的精度、召回率、F1分数和准确率。理想情况下，我们希望这些数字尽可能接近1.0。直观上，准确率需要达到100%（或1.0）；然而，其他的数字则需要仔细研究。从这个报告中，我们可以观察到总准确率为54%。从报告的其余部分，我们可以确定分类准确性较高的类别是1和8，分别对应*汽车*和*船只*。类似地，我们可以看到分类最差的两个类别是3和5，分别对应*猫*和*狗*。
- en: 'While these numbers are informative, we could look into what is the source
    of the confusion, by looking at the confusion matrix, which is the group of numbers
    produced by `confusion_matrix()`. If we inspect the confusion matrix on row number
    four (corresponding to label 3, *cats*), we see that it correctly classifies 370
    cats as cats, but 143 cats were classified as dogs, and 160 cats were classified
    as frogs, just to name the most serious areas of confusions. Another way to look
    at it is visually, as shown in the following figure:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些数字提供了信息，但我们可以通过查看混淆矩阵来探究混淆的来源，混淆矩阵是由`confusion_matrix()`生成的一组数字。如果我们检查第四行（对应标签3，*猫*）的混淆矩阵，我们可以看到它正确地将370只猫分类为猫，但143只猫被分类为狗，160只猫被分类为青蛙，这些是最严重的混淆区域。另一种视觉化查看的方式如下图所示：
- en: '![](img/4316c4aa-3eac-45dd-84a5-b7025c5926b2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4316c4aa-3eac-45dd-84a5-b7025c5926b2.png)'
- en: Figure 11.4 – Confusion matrix visualization for the widenet model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – widenet模型的混淆矩阵可视化
- en: Ideally, we want to see a confusion matrix that is diagonal; however, in this
    case, we don't see that effect. After visual inspection, from *Figure 11.4*, we
    can observe which classes have the lowest correct predictions and confirm, visually,
    where the confusion is.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望看到一个对角线的混淆矩阵；然而，在这种情况下，我们并未看到这种效果。通过目视检查，从*图11.4*中，我们可以观察到哪些类别的正确预测最少，并在视觉上确认混淆的地方。
- en: Finally, it is important to note that while **classification accuracy** (**ACC**)
    is 54%, we still need to verify the **balanced error rate** (**BER**) to complement
    what we know about the accuracy. This is particularly important when the classes
    are not evenly distributed, that is, when there are more samples for some classes
    compared to others. As explained in [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning
    from Data*, we can simply calculate the balanced accuracy and subtract it from
    one. This reveals that the BER is 0.4567, or 45.67%. In an ideal world, we want
    to lower the BER to zero, and definitely stay away from a BER of 50%, which would
    imply that the model is no better than random chance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是要注意，虽然**分类准确率**（**ACC**）为 54%，我们仍然需要验证**平衡误差率**（**BER**），以补充我们对准确率的了解。当类别分布不均时，特别重要，即某些类别的样本多于其他类别。如[第
    4 章](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml)《从数据中学习》中所解释，我们可以简单地计算平衡准确率并从中减去
    1。这显示出 BER 为 0.4567，即 45.67%。在理想情况下，我们希望将 BER 降到零，绝对避免 BER 为 50%，这意味着模型的表现与随机猜测没有区别。
- en: In this case, the accuracy of the model is not impressive; however, this is
    a very challenging classification problem for fully connected networks, thus,
    this performance is not surprising. Now, we will try to do a similar experiment,
    changing from a relatively wide network, to a deep network, and compare the results.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型的准确性并不令人印象深刻；然而，这是一个非常具有挑战性的分类问题，尤其对于全连接网络而言，因此，这一表现并不令人惊讶。接下来，我们将尝试做一个类似的实验，将网络从相对宽的网络改为深度网络，并对比结果。
- en: Dense deep neural networks
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稠密深度神经网络
- en: 'It is widely known that deeper networks can offer good performance in classification
    tasks (Liao, Q., et al. (2018)). In this section, we want to build a deep dense
    neural network and see how it performs in the CIFAR-10 dataset. We will be building
    the model shown in the following figure:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，深度网络在分类任务中可以提供良好的性能（Liao, Q., 等，2018）。在本节中，我们希望构建一个深度稠密神经网络，并观察它在 CIFAR-10
    数据集上的表现。我们将构建如下图所示的模型：
- en: '![](img/2063cb67-f41e-4a7b-b26d-9cab8a459240.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2063cb67-f41e-4a7b-b26d-9cab8a459240.png)'
- en: Figure 11.5 – Network architecture of a deep dense network for CIFAR-10
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – CIFAR-10 深度稠密网络的网络架构
- en: One of the aims of this model is to have the same number of neural units as
    the model in *Figure 11.1*, for the wide network. This model has a bottleneck
    architecture, where the number of neurons decreases as the network gets deeper.
    This can be coded programmatically using the Keras functional approach, as we
    discuss next.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的目标之一是与*图 11.1*中的宽网络具有相同数量的神经单元。该模型具有瓶颈架构，其中神经元的数量随着网络加深而减少。我们可以使用 Keras
    的函数式方法以编程方式实现这一点，接下来我们将讨论。
- en: Building and training the model
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练模型
- en: One interesting fact about Keras' functional approach is that we can **recycle** variable
    names as we build the model and that we can even build a model using a loop. For
    example, let's say that I would like to create dense layers with dropout rates
    that exponentially decrease along with the number of neurons by a factor of 1.5
    and 2, respectively.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keras 的函数式方法，一个有趣的事实是，在构建模型时，我们可以**重复使用**变量名，甚至可以通过循环构建模型。例如，假设我想创建具有 dropout
    比例的稠密层，这些比例会随着神经元数量的增加而指数下降，下降的因子分别为 1.5 和 2。
- en: 'We could achieve this by having a cycle that uses an initial dropout rate,
    `dr`, and an initial number of neural units, `units`, and decreases both by a
    factor of 1.5 and 2, respectively, every time, as long as the number of neural
    units is always greater than 10; we stop at 10 because the last layer will contain
    10 neurons, one for each class. It looks something like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个循环来实现这一点，循环使用初始的 dropout 比率 `dr` 和初始的神经单元数量 `units`，每次分别按因子 1.5 和 2
    递减，前提是神经单元的数量始终大于 10；我们在 10 停止，因为最后一层将包含 10 个神经元，每个类别一个。大致如下所示：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding code snippet illustrates that we can reuse variables without confusing
    Python since TensorFlow operates over a computational graph that has no problem
    in resolving parts of the graph in the correct sequence. The code also shows that
    we can create a bottleneck-type of network very easily with an exponentially decaying
    number of units and dropout rate.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段说明了我们可以重用变量而不至于让 Python 混淆，因为 TensorFlow 在计算图上操作，能够正确解决图中各部分的顺序。代码还显示了我们可以非常轻松地创建一个瓶颈型网络，其中单元数和
    dropout 比例按指数衰减。
- en: 'The full code to build this model looks like this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 构建此模型的完整代码如下：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Compiling and training the model goes like this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并训练模型的过程如下：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This produces the following output, caused by `deepnet.summary()` in the preceding
    code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出，这是由前面的`deepnet.summary()`产生的：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As shown in the preceding summary, and also in *Figure 11.5*, the total number
    of parameters of this model is **15,734,806**. This confirms that this is an over-parametrized
    model. The printed summary also depicts how each part of the model is named when
    no specific name is provided; that is, they all receive a generic name based on
    the name of the class and a consecutive number.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的总结所示，且在*图 11.5*中也展示了，该模型的总参数数量为**15,734,806**。这确认了该模型是一个过度参数化的模型。打印的总结还展示了在没有特定名称的情况下，模型的各个部分是如何命名的；即它们都基于类名和连续的编号获得一个通用名称。
- en: 'The `fit()` method trains the deep model and when we plot the training logged
    in the `hist` variable, as we did earlier for *Figure 11.3*, we obtain the following
    figure:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 方法用于训练深度模型，当我们像之前在*图 11.3*中一样绘制训练记录在`hist`变量中的内容时，我们得到以下图像：'
- en: '![](img/719dd863-8eec-447e-91f8-a95203ded62a.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/719dd863-8eec-447e-91f8-a95203ded62a.png)'
- en: Figure 11.6 – Model loss of deepnet across epochs using callbacks
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 使用回调函数在不同epoch下的深度网络损失
- en: From *Figure 11.6*, we see that the deep network stops training after about
    200 epochs and the training and test sets cross paths around epoch 70, after which,
    the model begins to overfit the training set. If we compare this result to the
    one in *Figure 11.3* for the wide network, we can see that the model starts overfitting
    around epoch 55.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 11.6*中可以看出，深度网络在大约200个epoch后停止训练，训练集和测试集在大约第70个epoch交叉，之后模型开始对训练集过拟合。如果我们将这个结果与*图
    11.3*中的宽网络结果进行比较，可以看到模型大约在第55个epoch开始过拟合。
- en: Let's now discuss the quantitative results of this model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论该模型的定量结果。
- en: Results
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'If we generate a classification report in the same manner as we did for the
    wide network, we obtain the results shown here:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以与宽网络相同的方式生成分类报告，我们得到如下结果：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This suggests comparable results to the wide model, in which we obtained a 0.4567
    BER, which represents a difference of 0.0089 in favor of the wide model, which
    does not represent a significant difference in this case. We can verify that the
    models are also comparable with respect to their classification performance on
    particular classes by looking at the preceding results or at the confusion matrix
    shown in the following figure:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明与宽网络的结果相当，其中我们得到了0.4567的BER，这代表了0.0089的差异，偏向于宽网络，这在此情况下并不代表显著差异。我们可以通过查看前面的结果或下图所示的混淆矩阵，验证模型在特定类别上的分类性能也是可比的：
- en: '![](img/f5ce5c8d-8be0-496f-a076-8fd401f0ea24.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5ce5c8d-8be0-496f-a076-8fd401f0ea24.png)'
- en: Figure 11.7 – Confusion matrix visualization for the deepnet model
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 深度网络模型的混淆矩阵可视化
- en: From the preceding results, we can confirm that the toughest class to classify
    is number 3, *cats*, which are often confused with dogs. Similarly, the easiest
    to classify is number 1, *ship**s*, which are often confused with airplanes. But
    once again, this is consistent with the results from the wide network.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的结果可以确认，最难分类的类别是第3类，*猫*，它们经常与狗混淆。同样，最容易分类的是第1类，*船*，它们经常与飞机混淆。但再一次，这与宽网络的结果一致。
- en: One more type of deep network that we can experiment with is one that promotes
    sparsity among the weights of the network, which we'll discuss next.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种我们可以尝试的深度网络类型是促进网络权重稀疏性的网络，我们将在接下来讨论。
- en: Sparse deep neural networks
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏深度神经网络
- en: A sparse network can be defined as *sparse* in different aspects of its architecture
    (Gripon, V., and Berrou, C., 2011). However, the specific type of sparseness we'll
    look into in this section is the sparseness obtained with respect to the weights
    of the network, that is, its parameters. We will be looking at each specific parameter
    to see if it is relatively close to zero (computationally speaking).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏网络可以在其架构的不同方面定义为*sparse*（Gripon, V., 和 Berrou, C., 2011）。然而，本节中我们将研究的特定稀疏性是指与网络的权重相关的稀疏性，即其参数。我们将检查每个具体的参数，看它是否接近于零（从计算的角度来看）。
- en: 'Currently, there are three ways of imposing weight sparseness in Keras over
    Tensorflow, and they are related to the concept of a vector norm. If we look at
    the Manhattan norm, ![](img/0393ae08-e2cd-4212-bdd7-09a6d5ebb397.png), or the
    Euclidean norm, ![](img/17601231-bf41-41e7-8008-03f7ad3924e4.png), they are defined
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，在Keras上通过TensorFlow施加权重稀疏性有三种方式，它们与向量范数的概念相关。如果我们看曼哈顿范数![](img/0393ae08-e2cd-4212-bdd7-09a6d5ebb397.png)，或者欧几里得范数![](img/17601231-bf41-41e7-8008-03f7ad3924e4.png)，它们的定义如下：
- en: '![](img/1566d40d-fd4b-423a-b31e-1ef070ce041c.png),'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/1566d40d-fd4b-423a-b31e-1ef070ce041c.png),'
- en: '![](img/d216abc0-69b5-487b-97f4-b6265cc2c2e6.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d216abc0-69b5-487b-97f4-b6265cc2c2e6.png)'
- en: Here, *n* is the number of elements in the vector ![](img/3b4165c3-c661-408a-b9f9-0f0d5a43b31d.png).
    As you can see, in simple terms, the ![](img/14b8cca1-5449-4bdb-9f76-d7839f733dc0.png)-norm
    adds up all elements in terms of their absolute value, while the ![](img/0a7be022-f165-4047-b381-1a597f96a9ed.png)-norm
    does it in terms of their squared values. It is evident that if both norms are
    close to zero, ![](img/51bcc6df-b0d4-421a-963b-a3d2127e4284.png), the chances
    are that most of its elements are zero or close to zero. As a matter of personal
    choice here, we will use the ![](img/6cc9b5e1-9ac6-4f3a-8fd4-59b862fb22e4.png)-norm
    because, as opposed to ![](img/ffda2abd-d60c-46db-8190-2425c9dec01d.png), very
    large vectors are quadratically penalized so as to avoid specific neurons dominating
    specific terms.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n* 是向量中元素的数量 ![](img/3b4165c3-c661-408a-b9f9-0f0d5a43b31d.png)。正如你所看到的，简而言之，![](img/14b8cca1-5449-4bdb-9f76-d7839f733dc0.png)-norm
    会根据元素的绝对值将所有元素相加，而![](img/0a7be022-f165-4047-b381-1a597f96a9ed.png)-norm 会根据元素的平方值来相加。显然，如果两个范数都接近零，![](img/51bcc6df-b0d4-421a-963b-a3d2127e4284.png)，大多数元素很可能是零或接近零。在这里，个人选择是使用![](img/6cc9b5e1-9ac6-4f3a-8fd4-59b862fb22e4.png)-norm，因为与![](img/ffda2abd-d60c-46db-8190-2425c9dec01d.png)相比，较大的向量会受到平方惩罚，从而避免特定神经元主导某些项。
- en: 'Keras contains these tools in the `regularizers` class: `tf.keras.regularizers`.
    We can import them as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包含这些工具在`regularizers`类中：`tf.keras.regularizers`。我们可以按如下方式导入它们：
- en: '![](img/c66b2062-de8b-428f-93fc-60a69e3c6d59.png)-norm: `tf.keras.regularizers.l1(l=0.01)`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/c66b2062-de8b-428f-93fc-60a69e3c6d59.png)-norm: `tf.keras.regularizers.l1(l=0.01)`'
- en: '![](img/0bbc9e82-8105-4829-9d6a-5f98b9c233d3.png)-norm: `tf.keras.regularizers.l2(l=0.01)`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/0bbc9e82-8105-4829-9d6a-5f98b9c233d3.png)-norm: `tf.keras.regularizers.l2(l=0.01)`'
- en: These regularizers are applied to the loss function of the network in order
    to minimize the norm of the weights.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些正则化器应用于网络的损失函数，以最小化权重的范数。
- en: A **regularizer** is a term that is used in machine learning to denote a term
    or function that provides elements to an objective (loss) function, or to a general
    optimization problem (such as gradient descent), in order to provide numerical
    stability or promote the feasibility of the problem. In this case, the regularizer
    promotes the stability of the weights by preventing the explosion of some weight
    values, while at the same time promoting general sparsity.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化器**是机器学习中用来表示一个术语或函数，它将元素提供给目标（损失）函数，或一般的优化问题（如梯度下降），以提供数值稳定性或促进问题的可行性。在这种情况下，正则化器通过防止某些权重值的爆炸，同时促进一般稀疏性，从而促进权重的稳定性。'
- en: 'The parameter `l=0.01` is a penalty factor that directly determines the importance
    of minimizing weight norms. In other words, the penalty is applied as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`l=0.01`是一个惩罚因子，它直接决定了最小化权重范数的重要性。换句话说，惩罚按如下方式应用：
- en: '![](img/c469f7b0-3678-4ec0-ba10-72f886eaafcf.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c469f7b0-3678-4ec0-ba10-72f886eaafcf.png)'
- en: 'Therefore, using a very small value, such as `l=0.0000001` will pay little
    attention to the norm, and `l=0.01` will pay a lot of attention to the norm during
    the minimization of the loss function. Here''s the catch: this parameter needs
    to be tuned up because if the network is too big, there might be several millions
    of parameters, which can make the norm look very large, and so a small penalty
    is in order; whereas if the network is relatively small, a larger penalty is recommended.
    Since this exercise is on a very deep network with 15+ million parameters, we
    will use a value of `l=0.0001`.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用非常小的值，如`l=0.0000001`，将对范数关注较少，而`l=0.01`将在最小化损失函数时对范数给予更多关注。关键是：这个参数需要调整，因为如果网络太大，可能会有几百万个参数，这会导致范数看起来非常大，因此需要一个小的惩罚；而如果网络相对较小，则建议使用更大的惩罚。由于这个练习是在一个拥有超过1500万个参数的非常深的网络上进行的，我们将使用`l=0.0001`的值。
- en: Let's go ahead and build a sparse network.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建一个稀疏网络。
- en: Building a sparse network and training it
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建稀疏网络并训练它
- en: 'To build this network, we will use the exact same architecture shown in *Figure
    11.5*, except that the declaration of each individual dense layer will contain
    a specification that we want to consider the minimization of the norm of the weights
    associated with that layer. Please look at the code of the previous section and
    compare it to the following code, where we highlight the differences:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这个网络，我们将使用与*图11.5*中显示的完全相同的架构，只是每个单独的密集层的声明将包含一个我们希望考虑该层权重的范数最小化的规范。请查看前一节的代码，并将其与以下代码进行比较，其中我们突出显示了差异：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Compiling and training the model goes the same, like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 编译和训练模型是相同的，像这样：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of `sparsenet.summary()` is identical to the one shown in the previous
    section for `deepnet.summary()`, so we will not repeat it here. However, we can
    look at the training curve as the loss is minimized – see the following figure:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparsenet.summary()` 的输出与前一节中的 `deepnet.summary()` 完全相同，因此我们这里不再重复。但是，我们可以看一下当损失最小化时的训练曲线
    - 请参见以下图：'
- en: '![](img/a48a28cd-af02-4fcb-bc98-03cf53e4c9df.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a48a28cd-af02-4fcb-bc98-03cf53e4c9df.png)'
- en: Figure 11.8 – Loss function optimization across epochs for the sparsenet model
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 - 稀疏网络模型跨时期的损失函数优化
- en: From the figure, we can see that both curves, the training and test set, are
    minimized closely together up to around epoch 120, where both start to digress,
    and the model begins to overfit after that. In comparison to previous models in
    *Figure 11.3* and *Figure 11.6*, we can see that this model can be trained a bit
    more slowly and still achieve relative convergence. Note, however, that while
    the loss function still remains the binary cross-entropy, the model is also minimizing
    the ![](img/c35d10cc-5624-4614-9a2e-457897f8f7cb.png)-norm, making this particular
    loss not directly comparable to the previous ones.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，训练集和测试集的两条曲线在大约第120个时期之前非常接近最小值，之后开始偏离，模型开始过拟合。与*图11.3*和*图11.6*中的先前模型相比，我们可以看到这个模型的训练速度可以稍慢一些，仍然可以实现相对收敛。然而，请注意，虽然损失函数仍然是二元交叉熵，但模型也在最小化![](img/c35d10cc-5624-4614-9a2e-457897f8f7cb.png)-范数，使得这种特定的损失函数与先前的不可直接比较。
- en: Let's now discuss the quantitative results of this model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论该模型的定量结果。
- en: Results
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'When we look at a quantitative analysis of performance, we can tell that the
    model is comparable to the previous models. There is a slight gain in terms of
    a BER; however, it is not enough to declare victory and the problem solved by
    any means – see the following analysis:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行性能的定量分析时，我们可以看出该模型与先前的模型相比是可比的。在BER方面有轻微的增益；然而，这并不足以宣布胜利，并且问题解决的任何手段都不足以解决问题
    - 请参阅以下分析：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'What we can clearly conclude is that the model is not worse in terms of performance
    when compared to other models discussed in this chapter. In fact, close inspection
    of the confusion matrix shown in the following figure indicates that similar errors
    are made with this network as well in terms of objects that are similar in nature:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地得出结论，与本章讨论的其他模型相比，该模型在性能上并不更差。事实上，对以下图中显示的混淆矩阵的仔细检查表明，这个网络在性质相似的对象方面也会产生类似的错误：
- en: '![](img/a4731f7e-bd2c-482c-ae01-db80b399522c.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4731f7e-bd2c-482c-ae01-db80b399522c.png)'
- en: Figure 11.9 – Confusion matrix for the sparsenet model
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9 - 稀疏网络模型的混淆矩阵
- en: 'Now, since it is difficult to appreciate the differences between the models
    we have discussed so far – wide, deep, and sparse – we can calculate and plot
    the norm of the weights of each trained model, as shown in the following figure:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于很难理解到目前为止我们讨论的模型之间的差异 - 广泛、深入和稀疏，我们可以计算并绘制每个训练模型权重的范数，如下图所示：
- en: '![](img/4bc9b510-5899-4937-ac67-37f1a60f9456.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bc9b510-5899-4937-ac67-37f1a60f9456.png)'
- en: Figure 11.10 – Cumulative norm weights of the trained models
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10 - 训练模型的累积范数权重
- en: This figure shows the calculation in terms of the ![](img/2ab2673c-164f-45b9-89d9-d6fc42d48046.png)-norm
    so as to have values close enough to appreciate them; on the horizontal axis we
    have the number of layers, and on the vertical axis, we have the cumulative norm
    as we progress in the layers of the networks. This is where we can appreciate
    how different the networks are with respect to their parameters. In a sparse network,
    the cumulative norm is much smaller (about four to five times) in comparison to
    the other networks. This can be an interesting and important characteristic for
    those networks that might be implemented on a chip or other applications in which
    zero-weights can lead to efficient computations in production (Wang, P., et al.
    2018).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了按![](img/2ab2673c-164f-45b9-89d9-d6fc42d48046.png)-范数进行的计算，从而使得数值足够接近以便进行评估；在横轴上是层数，纵轴上是随着网络层数增加的累积范数。这是我们可以欣赏到不同网络在其参数上的差异之处。在稀疏网络中，累积范数比其他网络要小得多（大约是其他网络的四到五倍）。对于那些可能被实现到芯片上或其他零权重能在生产中带来高效计算的应用来说，这可能是一个有趣且重要的特征（Wang,
    P. 等，2018）。
- en: While the level at which the network weights are affected by the norm can be
    determined experimentally through hyperparameter optimization techniques, it is
    often more common to determine other parameters such as the dropout rate, the
    number of neural units, and others we discuss in the next section.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网络权重受范数影响的程度可以通过超参数优化技术进行实验确定，但通常更常见的是确定其他参数，例如丢弃率、神经单元数量等，这些将在下一节讨论。
- en: Hyperparameter optimization
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: There are a few methodologies out there for optimizing parameters; for example,
    some are gradient-based (Rivas, P., et al. 2014; Maclaurin, D., et al. 2015*)*,
    others are Bayesian (Feurer, M., et al. 2015*)*. However, it has been difficult
    to have a generalized method that works extremely well and that is efficient at
    the same time – usually, you get one or the other. You can read more about other
    algorithms here (Bergstra, J. S., et al. 2011*)*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以用来优化参数；例如，有些是基于梯度的（Rivas, P. 等，2014；Maclaurin, D. 等，2015*），而其他则是贝叶斯方法（Feurer,
    M. 等，2015*）。然而，至今还没有一种通用的方法能够既极其有效又高效——通常你只能获得其中之一。你可以在这里阅读更多关于其他算法的内容（Bergstra,
    J. S. 等，2011*）。
- en: For any beginner in this field, it might be better to get started with something
    simple and easy to remember, such as random search (Bergstra, J., & Bengio, Y.
    2012*)* or grid search. These two methods are very similar and while we will focus
    here on **grid search**, the implementations of both are very similar.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该领域的任何初学者来说，最好从一些简单且容易记住的东西入手，例如随机搜索（Bergstra, J. & Bengio, Y. 2012*）或网格搜索。这两种方法非常相似，虽然我们这里重点讨论**网格搜索**，但这两者的实现非常相似。
- en: Libraries and parameters
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库和参数
- en: 'We will need to use two major libraries that we have not covered before: `GridSearchCV`,
    for executing the grid search with cross-validation, and `KerasClassifier`, to
    create a Keras classifier that can communicate with scikit-learn.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要使用两个我们之前未涉及过的主要库：`GridSearchCV`，用于执行带有交叉验证的网格搜索，和`KerasClassifier`，用于创建可以与scikit-learn通信的Keras分类器。
- en: 'Both libraries can be imported as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 两个库可以如下导入：
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The hyperparameters that we will optimize (and their possible values) are the
    following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将优化的超参数（及其可能的值）如下：
- en: '**Dropout rate**: `0.2`, `0.5`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃率**：`0.2`，`0.5`'
- en: '**Optimizer**: `rmsprop`, `adam`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：`rmsprop`，`adam`'
- en: '**Learning rate**: `0.01`, `0.0001`'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：`0.01`，`0.0001`'
- en: '**Neurons in hidden layers**: `1024`, `512`, `256`'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层中的神经元数量**：`1024`，`512`，`256`'
- en: 'In total, the possible combination of hyperparameters is 2x2x2x3=24\. This
    is the total number of options in the four-dimensional grid. The number of alternatives
    can be much larger and more comprehensive but remember: we want to keep things
    simple for this example. Furthermore, since we will be applying cross-validation,
    you will multiply the possible combinations by the number of splits in cross-validation
    and that would be how many complete, end-to-end training sessions will be executed
    to determine the best combination of hyperparameters.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，超参数的可能组合是2x2x2x3=24。这是四维网格中的选项总数。替代方案的数量可以更大、更全面，但请记住：为了简单起见，我们在这个例子中保持简单。此外，由于我们将应用交叉验证，你需要将可能的组合数乘以交叉验证中的划分数量，这样就能得出为确定最佳超参数组合所执行的完整端到端训练会话的数量。
- en: Be mindful of the number of options you will try in the grid search, since all
    of them will be tested, and this can take a lot of time for larger networks and
    for larger datasets. When you gain more experience, you will be able to choose
    a smaller set of parameters just by thinking about the architecture that you define.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意你将在网格搜索中尝试的选项数量，因为所有这些选项都会被测试，对于较大的网络和数据集，这可能会花费很多时间。随着经验的积累，你将能够通过思考所定义的架构来选择更小的参数集。
- en: The full implementation is discussed next.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的实现将在下一节讨论。
- en: Implementation and results
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现与结果
- en: 'The complete code for the grid search is shown here, but consider that most
    of these things are repetitive since this is modeled on the wide network model
    discussed earlier in this chapter:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了网格搜索的完整代码，但请注意，这些内容大多是重复的，因为它是基于本章前面讨论的宽网络模型进行建模的：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We declare a method to build a model and return it like so:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明一个方法来构建模型并返回它，代码如下：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then we put the pieces together, searching for parameters, and training as
    follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将各部分拼接在一起，搜索参数，并按如下方式进行训练：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will print out several lines, one for each time the cross-validation runs.
    We will omit a lot of the output here, just to show you what it looks like, but
    you can tune the level of verbosity manually if you want:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出几行，每一行对应交叉验证运行一次。我们在这里省略了很多输出，只是为了向你展示它的样子，但如果你愿意，可以手动调整输出的详细程度：
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This last line is the most precious information you need since it is the best
    combination of parameters that give the best results. Now you can go ahead and
    change your original implementation of the wide network with these **optimized **parameters
    and see how the performance changes. You should receive a boost in the average
    accuracy of around 5%, which is not bad!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行是你最宝贵的信息，因为它是最佳参数组合，能带来最佳的结果。现在，你可以使用这些**优化后的**参数来更改你原始的宽网络实现，看看性能变化。你应该会看到平均准确率提升约5%，这并不差！
- en: 'Alternatively, you can try out a larger set of parameters or increase the number
    of splits for cross-validation. The possibilities are endless. You should always
    try to optimize the number of parameters in your models for the following reasons:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以尝试更大范围的参数集，或者增加交叉验证的分割数。可能性是无穷的。你应该始终尝试优化模型中的参数数量，原因如下：
- en: It gives you confidence in your model.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它让你对自己的模型充满信心。
- en: It gives your clients confidence in you.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它让你的客户对你充满信心。
- en: It tells the world that you are a professional.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它向世界表明你是一个专业人士。
- en: Good work! It is time to wrap up.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！是时候总结一下了。
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed different implementations of neural networks, namely,
    wide, deep, and sparse implementations. After reading this chapter, you should
    appreciate the differences in design and how they may affect performance or training
    time. At this point, you should be able to appreciate the simplicity of these
    architectures and how they present new alternatives to other things we've discussed
    so far. In this chapter, you also learned to optimize the hyperparameters of your
    models, for example, the dropout rates, aiming to maximize the generalization
    ability of the network.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了神经网络的不同实现方式，即宽网络、深度网络和稀疏网络实现。阅读完本章后，你应该理解设计的差异以及它们如何影响性能或训练时间。此时，你应该能够理解这些架构的简洁性以及它们如何为我们迄今为止讨论的其他方法提供新的替代方案。本章中，你还学习了如何优化模型的超参数，例如，丢弃率，以最大化网络的泛化能力。
- en: I am sure you noticed that these models achieved accuracies beyond random chance,
    that is, > 50%; however, the problem we discussed is a very difficult problem
    to solve, and you might not be surprised that a general neural architecture, like
    the ones we studied here, does not perform extraordinarily well. In order to achieve
    better performance, we can use a more specialized type of architecture designed
    to solve problems with a high spatial correlation of the input, such as image
    processing. One type of specialized architecture is known as a **Convolutional
    Neural Network** (**CNN**).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你注意到这些模型的准确率超过了随机机会，也就是说，> 50%；然而，我们讨论的问题是一个非常难以解决的问题，你可能不会感到惊讶，像我们在这里研究的一般神经网络架构并没有表现得特别出色。为了获得更好的性能，我们可以使用一种更为专门的架构，旨在解决输入具有高度空间相关性的问题，比如图像处理。一种专门的架构类型被称为**卷积神经网络**（**CNN**）。
- en: Our next station, [Chapter 12](c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml),
    *Convolutional Neural Networks,* will discuss precisely that*.* You will be able
    to see how much a difference can make when you move from a general-purpose model
    to a more field-specific model. You cannot miss this upcoming chapter. But before
    you go, please try to quiz yourself with the following questions.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个章节，[第12章](c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml)，*卷积神经网络*，将详细讨论这一点*。*你将能够看到从通用模型转向更具领域特定性的模型所带来的差异有多大。你不能错过即将到来的这一章。但在你继续之前，请尝试用以下问题进行自我测验。
- en: Questions and answers
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Was there a significant difference in performance between a wide or deep
    network?**'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**宽网络和深网络之间的性能有显著差异吗？**'
- en: Not much in the case, we studied here. However, one thing you must remember
    is that both networks learned fundamentally different things or aspects of the
    input. Therefore, in other applications, the performance might vary.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这里研究的案例中并没有太多。然而，你必须记住的一点是，两个网络学到的输入的东西或方面是根本不同的。因此，在其他应用中，性能可能会有所不同。
- en: '**Is deep learning the same as a deep neural network? **'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度学习是否等同于深度神经网络？**'
- en: No. Deep learning is the area of machine learning focused on all algorithms
    that train over-parametrized models using novel gradient descent techniques. Deep
    neural networks are networks with many hidden layers. Therefore, a deep network
    is deep learning. But deep learning is not uniquely specific to deep networks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 不，深度学习是机器学习的一个领域，专注于使用新颖的梯度下降技术训练过参数化模型的所有算法。深度神经网络是具有多个隐藏层的网络。因此，深度网络就是深度学习。但是，深度学习并不专属于深度网络。
- en: '**Could you give an example of when sparse networks are desired?**'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**你能举一个稀疏网络被需要的例子吗？**'
- en: Let's think about robotics. In this field, most things run on microchips that
    have memory constraints and storage constraints and computational power constraints;
    finding neural architectures whose weights are mostly zero would mean you do not
    have to calculate those products. This implies having weights that can be stored
    in less space, loaded quickly, and computed faster. Other possibilities include
    IoT devices, smartphones, smart vehicles, smart cities, law enforcement, and so
    on.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下机器人技术。在这个领域，大多数东西运行在具有内存约束、存储约束和计算能力约束的微芯片上；找到大部分权重为零的神经架构意味着你不必计算这些乘积。这意味着拥有可以存储在更小空间中、加载更快、计算更快的权重。其他可能的应用包括物联网设备、智能手机、智能车辆、智慧城市、执法等。
- en: '**How can we make these models perform better?**'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们如何使这些模型表现得更好？**'
- en: We can further optimize the hyperparameters by including more options. We can
    use autoencoders to preprocess the input. But the most effective thing would be
    to switch to CNNs to solve this problem since CNNs are particularly good at the
    classification of images. See the next chapter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过包括更多选项来进一步优化超参数。我们可以使用自编码器来预处理输入。但最有效的做法是切换到CNN来解决这个问题，因为CNN在图像分类上尤其擅长。请参见下一章。
- en: References
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological review*, 65(6), 386.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenblatt, F. (1958). 感知机：大脑中信息存储与组织的概率模型。*心理学评论*，65(6)，386。
- en: Muselli, M. (1997). On convergence properties of the pocket algorithm. *IEEE
    Transactions on Neural Networks*, 8(3), 623-629.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muselli, M. (1997). 关于口袋算法收敛性的性质。*IEEE神经网络学报*，8(3)，623-629。
- en: 'Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., &
    Schoenholz, S. S. (2019). Neural Tangents: Fast and Easy Infinite Neural Networks
    in Python. *arXiv preprint* arXiv:1912.02803.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., &
    Schoenholz, S. S. (2019). 神经切线：Python中快速且简单的无限神经网络。*arXiv预印本* arXiv:1912.02803。
- en: Soltanolkotabi, M., Javanmard, A., & Lee, J. D. (2018). Theoretical insights
    into the optimization landscape of over-parameterized shallow neural networks. *IEEE
    Transactions on Information Theory*, 65(2), 742-769.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soltanolkotabi, M., Javanmard, A., & Lee, J. D. (2018). 关于过参数化浅层神经网络优化景观的理论见解。*IEEE信息论学报*，65(2)，742-769。
- en: Du, S. S., Zhai, X., Poczos, B., & Singh, A. (2018). Gradient descent provably
    optimizes over-parameterized neural networks. *arXiv preprint* arXiv:1810.02054.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du, S. S., Zhai, X., Poczos, B., & Singh, A. (2018). 梯度下降可以证明优化过参数化神经网络。*arXiv预印本*
    arXiv:1810.02054。
- en: Liao, Q., Miranda, B., Banburski, A., Hidary, J., & Poggio, T. (2018). A surprising
    linear relationship predicts test performance in deep networks. *arXiv preprint*
    arXiv:1807.09659.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao, Q., Miranda, B., Banburski, A., Hidary, J., & Poggio, T. (2018). 一个令人惊讶的线性关系预测深度网络的测试性能。*arXiv预印本*
    arXiv:1807.09659。
- en: Gripon, V., & Berrou, C. (2011). Sparse neural networks with large learning
    diversity. *IEEE transactions on neural networks*, 22(7), 1087-1096.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gripon, V., & Berrou, C. (2011). 具有大规模学习多样性的稀疏神经网络。《*IEEE神经网络学报*》，22(7)，1087-1096。
- en: 'Wang, P., Ji, Y., Hong, C., Lyu, Y., Wang, D., & Xie, Y. (2018, June). SNrram:
    an efficient sparse neural network computation architecture based on resistive
    random-access memory. In *2018 55th ACM/ESDA/IEEE Design Automation Conference*
    (DAC) (pp. 1-6). IEEE.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, P., Ji, Y., Hong, C., Lyu, Y., Wang, D., & Xie, Y. (2018年6月). SNrram：一种基于电阻式随机存取存储器的高效稀疏神经网络计算架构。载于*2018年第55届ACM/ESDA/IEEE设计自动化大会*（DAC）（第1-6页）。IEEE。
- en: Rivas-Perea, P., Cota-Ruiz, J., & Rosiles, J. G. (2014). A nonlinear least squares
    quasi-newton strategy for lp-svr hyper-parameters selection. *International Journal
    of Machine Learning and Cybernetics*, 5(4), 579-597.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivas-Perea, P., Cota-Ruiz, J., & Rosiles, J. G. (2014). 一种用于lp-svr超参数选择的非线性最小二乘准牛顿策略。《*机器学习与网络科学国际期刊*》，5(4)，579-597。
- en: Maclaurin, D., Duvenaud, D., & Adams, R. (2015, June). Gradient-based hyperparameter
    optimization through reversible learning. In *International Conference on Machine
    Learning* (pp. 2113-2122).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maclaurin, D., Duvenaud, D., & Adams, R. (2015年6月). 通过可逆学习进行基于梯度的超参数优化。载于*国际机器学习大会*（第2113-2122页）。
- en: Feurer, M., Springenberg, J. T., & Hutter, F. (2015, February). Initializing
    Bayesian hyperparameter optimization via meta-learning. In *Twenty-Ninth AAAI
    Conference on Artificial Intelligence*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feurer, M., Springenberg, J. T., & Hutter, F. (2015年2月). 通过元学习初始化贝叶斯超参数优化。载于*第二十九届AAAI人工智能大会*。
- en: Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization.
    The *Journal of Machine Learning Research*, 13(1), 281-305.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J., & Bengio, Y. (2012). 随机搜索超参数优化。《*机器学习研究期刊*》，13(1)，281-305。
- en: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. (2011). Algorithms for
    hyper-parameter optimization. In *Advances in neural information processing systems*
    (pp. 2546-2554).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. (2011). 超参数优化算法。载于*神经信息处理系统进展*（第2546-2554页）。
